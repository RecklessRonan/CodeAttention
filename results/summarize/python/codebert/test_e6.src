9915	def update ( self , instance , validated_data ) : is_primary = validated_data . pop ( "is_primary" , False ) instance = super ( EmailSerializer , self ) . update ( instance , validated_data ) if is_primary : instance . set_primary ( ) return instance
1654	def CheckPrintf ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] match = Search ( r'snprintf\s*\(([^,]*),\s*([0-9]*)\s*,' , line ) if match and match . group ( 2 ) != '0' : error ( filename , linenum , 'runtime/printf' , 3 , 'If you can, use sizeof(%s) instead of %s as the 2nd arg ' 'to snprintf.' % ( match . group ( 1 ) , match . group ( 2 ) ) ) if Search ( r'\bsprintf\s*\(' , line ) : error ( filename , linenum , 'runtime/printf' , 5 , 'Never use sprintf. Use snprintf instead.' ) match = Search ( r'\b(strcpy|strcat)\s*\(' , line ) if match : error ( filename , linenum , 'runtime/printf' , 4 , 'Almost always, snprintf is better than %s' % match . group ( 1 ) )
3142	def create ( self , data ) : if 'name' not in data : raise KeyError ( 'The file must have a name' ) if 'file_data' not in data : raise KeyError ( 'The file must have file_data' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . file_id = response [ 'id' ] else : self . file_id = None return response
12326	def init ( globalvars = None , show = False ) : global config profileini = getprofileini ( ) if os . path . exists ( profileini ) : config = configparser . ConfigParser ( ) config . read ( profileini ) mgr = plugins_get_mgr ( ) mgr . update_configs ( config ) if show : for source in config : print ( "[%s] :" % ( source ) ) for k in config [ source ] : print ( " %s : %s" % ( k , config [ source ] [ k ] ) ) else : print ( "Profile does not exist. So creating one" ) if not show : update ( globalvars ) print ( "Complete init" )
3422	def get_context ( obj ) : try : return obj . _contexts [ - 1 ] except ( AttributeError , IndexError ) : pass try : return obj . _model . _contexts [ - 1 ] except ( AttributeError , IndexError ) : pass return None
6861	def prep_root_password ( self , password = None , ** kwargs ) : r = self . database_renderer ( ** kwargs ) r . env . root_password = password or r . genv . get ( 'db_root_password' ) r . sudo ( "DEBIAN_FRONTEND=noninteractive dpkg --configure -a" ) r . sudo ( "debconf-set-selections <<< 'mysql-server mysql-server/root_password password {root_password}'" ) r . sudo ( "debconf-set-selections <<< 'mysql-server mysql-server/root_password_again password {root_password}'" )
10202	def register_aggregations ( ) : return [ dict ( aggregation_name = 'file-download-agg' , templates = 'invenio_stats.contrib.aggregations.aggr_file_download' , aggregator_class = StatAggregator , aggregator_config = dict ( client = current_search_client , event = 'file-download' , aggregation_field = 'unique_id' , aggregation_interval = 'day' , copy_fields = dict ( file_key = 'file_key' , bucket_id = 'bucket_id' , file_id = 'file_id' , ) , metric_aggregation_fields = { 'unique_count' : ( 'cardinality' , 'unique_session_id' , { 'precision_threshold' : 1000 } ) , 'volume' : ( 'sum' , 'size' , { } ) , } , ) ) , dict ( aggregation_name = 'record-view-agg' , templates = 'invenio_stats.contrib.aggregations.aggr_record_view' , aggregator_class = StatAggregator , aggregator_config = dict ( client = current_search_client , event = 'record-view' , aggregation_field = 'unique_id' , aggregation_interval = 'day' , copy_fields = dict ( record_id = 'record_id' , pid_type = 'pid_type' , pid_value = 'pid_value' , ) , metric_aggregation_fields = { 'unique_count' : ( 'cardinality' , 'unique_session_id' , { 'precision_threshold' : 1000 } ) , } , ) ) ]
1528	def pick_unused_port ( self ) : s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) s . bind ( ( '127.0.0.1' , 0 ) ) _ , port = s . getsockname ( ) s . close ( ) return port
10643	def Nu ( L : float , h : float , k : float ) -> float : return h * L / k
489	def close ( self ) : self . _logger . info ( "Closing" ) if self . _conn is not None : self . _conn . close ( ) self . _conn = None else : self . _logger . warning ( "close() called, but connection policy was alredy closed" ) return
1203	def tf_step ( self , x , iteration , conjugate , residual , squared_residual ) : x , next_iteration , conjugate , residual , squared_residual = super ( ConjugateGradient , self ) . tf_step ( x , iteration , conjugate , residual , squared_residual ) A_conjugate = self . fn_x ( conjugate ) if self . damping > 0.0 : A_conjugate = [ A_conj + self . damping * conj for A_conj , conj in zip ( A_conjugate , conjugate ) ] conjugate_A_conjugate = tf . add_n ( inputs = [ tf . reduce_sum ( input_tensor = ( conj * A_conj ) ) for conj , A_conj in zip ( conjugate , A_conjugate ) ] ) alpha = squared_residual / tf . maximum ( x = conjugate_A_conjugate , y = util . epsilon ) next_x = [ t + alpha * conj for t , conj in zip ( x , conjugate ) ] next_residual = [ res - alpha * A_conj for res , A_conj in zip ( residual , A_conjugate ) ] next_squared_residual = tf . add_n ( inputs = [ tf . reduce_sum ( input_tensor = ( res * res ) ) for res in next_residual ] ) beta = next_squared_residual / tf . maximum ( x = squared_residual , y = util . epsilon ) next_conjugate = [ res + beta * conj for res , conj in zip ( next_residual , conjugate ) ] return next_x , next_iteration , next_conjugate , next_residual , next_squared_residual
12575	def apply_mask ( self , mask_img ) : self . set_mask ( mask_img ) return self . get_data ( masked = True , smoothed = True , safe_copy = True )
2461	def set_file_name ( self , doc , name ) : if self . has_package ( doc ) : doc . package . files . append ( file . File ( name ) ) self . reset_file_stat ( ) return True else : raise OrderError ( 'File::Name' )
914	def lscsum ( lx , epsilon = None ) : lx = numpy . asarray ( lx ) base = lx . max ( ) if numpy . isinf ( base ) : return base if ( epsilon is not None ) and ( base < epsilon ) : return epsilon x = numpy . exp ( lx - base ) ssum = x . sum ( ) result = numpy . log ( ssum ) + base return result
4336	def overdrive ( self , gain_db = 20.0 , colour = 20.0 ) : if not is_number ( gain_db ) : raise ValueError ( 'db_level must be a number.' ) if not is_number ( colour ) : raise ValueError ( 'colour must be a number.' ) effect_args = [ 'overdrive' , '{:f}' . format ( gain_db ) , '{:f}' . format ( colour ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'overdrive' ) return self
5525	def grab ( self , bbox = None ) : w = Gdk . get_default_root_window ( ) if bbox is not None : g = [ bbox [ 0 ] , bbox [ 1 ] , bbox [ 2 ] - bbox [ 0 ] , bbox [ 3 ] - bbox [ 1 ] ] else : g = w . get_geometry ( ) pb = Gdk . pixbuf_get_from_window ( w , * g ) if pb . get_bits_per_sample ( ) != 8 : raise ValueError ( 'Expected 8 bits per pixel.' ) elif pb . get_n_channels ( ) != 3 : raise ValueError ( 'Expected RGB image.' ) pixel_bytes = pb . read_pixel_bytes ( ) . get_data ( ) width , height = g [ 2 ] , g [ 3 ] return Image . frombytes ( 'RGB' , ( width , height ) , pixel_bytes , 'raw' , 'RGB' , pb . get_rowstride ( ) , 1 )
8647	def accept_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'accept' , } endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotAcceptedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
7516	def init_arrays ( data ) : co5 = h5py . File ( data . clust_database , 'r' ) io5 = h5py . File ( data . database , 'w' ) maxlen = data . _hackersonly [ "max_fragment_length" ] + 20 chunks = co5 [ "seqs" ] . attrs [ "chunksize" ] [ 0 ] nloci = co5 [ "seqs" ] . shape [ 0 ] snps = io5 . create_dataset ( "snps" , ( nloci , maxlen , 2 ) , dtype = np . bool , chunks = ( chunks , maxlen , 2 ) , compression = 'gzip' ) snps . attrs [ "chunksize" ] = chunks snps . attrs [ "names" ] = [ "-" , "*" ] filters = io5 . create_dataset ( "filters" , ( nloci , 6 ) , dtype = np . bool ) filters . attrs [ "filters" ] = [ "duplicates" , "max_indels" , "max_snps" , "max_shared_hets" , "min_samps" , "max_alleles" ] edges = io5 . create_dataset ( "edges" , ( nloci , 5 ) , dtype = np . uint16 , chunks = ( chunks , 5 ) , compression = "gzip" ) edges . attrs [ "chunksize" ] = chunks edges . attrs [ "names" ] = [ "R1_L" , "R1_R" , "R2_L" , "R2_R" , "sep" ] edges [ : , 4 ] = co5 [ "splits" ] [ : ] filters [ : , 0 ] = co5 [ "duplicates" ] [ : ] io5 . close ( ) co5 . close ( )
10135	def dump ( grids , mode = MODE_ZINC ) : if isinstance ( grids , Grid ) : return dump_grid ( grids , mode = mode ) _dump = functools . partial ( dump_grid , mode = mode ) if mode == MODE_ZINC : return '\n' . join ( map ( _dump , grids ) ) elif mode == MODE_JSON : return '[%s]' % ',' . join ( map ( _dump , grids ) ) else : raise NotImplementedError ( 'Format not implemented: %s' % mode )
12310	def pull_stream ( self , uri , ** kwargs ) : return self . protocol . execute ( 'pullStream' , uri = uri , ** kwargs )
12226	def on_pref_update ( * args , ** kwargs ) : Preference . update_prefs ( * args , ** kwargs ) Preference . read_prefs ( get_prefs ( ) )
7469	def concatclusts ( outhandle , alignbits ) : with gzip . open ( outhandle , 'wb' ) as out : for fname in alignbits : with open ( fname ) as infile : out . write ( infile . read ( ) + "//\n//\n" )
9080	def get_providers ( self , ** kwargs ) : if 'ids' in kwargs : ids = [ self . concept_scheme_uri_map . get ( id , id ) for id in kwargs [ 'ids' ] ] providers = [ self . providers [ k ] for k in self . providers . keys ( ) if k in ids ] else : providers = list ( self . providers . values ( ) ) if 'subject' in kwargs : providers = [ p for p in providers if kwargs [ 'subject' ] in p . metadata [ 'subject' ] ] return providers
1974	def sys_allocate ( self , cpu , length , isX , addr ) : if addr not in cpu . memory : logger . info ( "ALLOCATE: addr points to invalid address. Returning EFAULT" ) return Decree . CGC_EFAULT perms = [ 'rw ' , 'rwx' ] [ bool ( isX ) ] try : result = cpu . memory . mmap ( None , length , perms ) except Exception as e : logger . info ( "ALLOCATE exception %s. Returning ENOMEM %r" , str ( e ) , length ) return Decree . CGC_ENOMEM cpu . write_int ( addr , result , 32 ) logger . info ( "ALLOCATE(%d, %s, 0x%08x) -> 0x%08x" % ( length , perms , addr , result ) ) self . syscall_trace . append ( ( "_allocate" , - 1 , length ) ) return 0
13683	def get ( self , url , params = { } ) : params . update ( { 'api_key' : self . api_key } ) try : response = requests . get ( self . host + url , params = params ) except RequestException as e : response = e . args return self . json_parse ( response . content )
11416	def record_add_subfield_into ( rec , tag , subfield_code , value , subfield_position = None , field_position_global = None , field_position_local = None ) : subfields = record_get_subfields ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) if subfield_position is None : subfields . append ( ( subfield_code , value ) ) else : subfields . insert ( subfield_position , ( subfield_code , value ) )
6662	def generate_csr ( self , domain = '' , r = None ) : r = r or self . local_renderer r . env . domain = domain or r . env . domain role = self . genv . ROLE or ALL site = self . genv . SITE or self . genv . default_site print ( 'self.genv.default_site:' , self . genv . default_site , file = sys . stderr ) print ( 'site.csr0:' , site , file = sys . stderr ) ssl_dst = 'roles/%s/ssl' % ( role , ) print ( 'ssl_dst:' , ssl_dst ) if not os . path . isdir ( ssl_dst ) : os . makedirs ( ssl_dst ) for site , site_data in self . iter_sites ( ) : print ( 'site.csr1:' , site , file = sys . stderr ) assert r . env . domain , 'No SSL domain defined.' r . env . ssl_base_dst = '%s/%s' % ( ssl_dst , r . env . domain . replace ( '*.' , '' ) ) r . env . ssl_csr_year = date . today ( ) . year r . local ( 'openssl req -nodes -newkey rsa:{ssl_length} ' '-subj "/C={ssl_country}/ST={ssl_state}/L={ssl_city}/O={ssl_organization}/CN={ssl_domain}" ' '-keyout {ssl_base_dst}.{ssl_csr_year}.key -out {ssl_base_dst}.{ssl_csr_year}.csr' )
8805	def build_full_day_ips ( query , period_start , period_end ) : ip_list = query . filter ( models . IPAddress . version == 4L ) . filter ( models . IPAddress . network_id == PUBLIC_NETWORK_ID ) . filter ( models . IPAddress . used_by_tenant_id is not None ) . filter ( models . IPAddress . allocated_at != null ( ) ) . filter ( models . IPAddress . allocated_at < period_start ) . filter ( or_ ( models . IPAddress . _deallocated is False , models . IPAddress . deallocated_at == null ( ) , models . IPAddress . deallocated_at >= period_end ) ) . all ( ) return ip_list
7320	def make_message_multipart ( message ) : if not message . is_multipart ( ) : multipart_message = email . mime . multipart . MIMEMultipart ( 'alternative' ) for header_key in set ( message . keys ( ) ) : values = message . get_all ( header_key , failobj = [ ] ) for value in values : multipart_message [ header_key ] = value original_text = message . get_payload ( ) multipart_message . attach ( email . mime . text . MIMEText ( original_text ) ) message = multipart_message message = _create_boundary ( message ) return message
195	def MotionBlur ( k = 5 , angle = ( 0 , 360 ) , direction = ( - 1.0 , 1.0 ) , order = 1 , name = None , deterministic = False , random_state = None ) : k_param = iap . handle_discrete_param ( k , "k" , value_range = ( 3 , None ) , tuple_to_uniform = True , list_to_choice = True , allow_floats = False ) angle_param = iap . handle_continuous_param ( angle , "angle" , value_range = None , tuple_to_uniform = True , list_to_choice = True ) direction_param = iap . handle_continuous_param ( direction , "direction" , value_range = ( - 1.0 - 1e-6 , 1.0 + 1e-6 ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( image , nb_channels , random_state_func ) : from . import geometric as iaa_geometric k_sample = int ( k_param . draw_sample ( random_state = random_state_func ) ) angle_sample = angle_param . draw_sample ( random_state = random_state_func ) direction_sample = direction_param . draw_sample ( random_state = random_state_func ) k_sample = k_sample if k_sample % 2 != 0 else k_sample + 1 direction_sample = np . clip ( direction_sample , - 1.0 , 1.0 ) direction_sample = ( direction_sample + 1.0 ) / 2.0 matrix = np . zeros ( ( k_sample , k_sample ) , dtype = np . float32 ) matrix [ : , k_sample // 2 ] = np . linspace ( float ( direction_sample ) , 1.0 - float ( direction_sample ) , num = k_sample ) rot = iaa_geometric . Affine ( rotate = angle_sample , order = order ) matrix = ( rot . augment_image ( ( matrix * 255 ) . astype ( np . uint8 ) ) / 255.0 ) . astype ( np . float32 ) return [ matrix / np . sum ( matrix ) ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return iaa_convolutional . Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
332	def model_stoch_vol ( data , samples = 2000 , progressbar = True ) : from pymc3 . distributions . timeseries import GaussianRandomWalk with pm . Model ( ) as model : nu = pm . Exponential ( 'nu' , 1. / 10 , testval = 5. ) sigma = pm . Exponential ( 'sigma' , 1. / .02 , testval = .1 ) s = GaussianRandomWalk ( 's' , sigma ** - 2 , shape = len ( data ) ) volatility_process = pm . Deterministic ( 'volatility_process' , pm . math . exp ( - 2 * s ) ) pm . StudentT ( 'r' , nu , lam = volatility_process , observed = data ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace
7807	def verify_client ( self , client_jid = None , domains = None ) : jids = [ jid for jid in self . get_jids ( ) if jid . local ] if not jids : return None if client_jid is not None and client_jid in jids : return client_jid if domains is None : return jids [ 0 ] for jid in jids : for domain in domains : if are_domains_equal ( jid . domain , domain ) : return jid return None
5757	def get_homogeneous ( package_descriptors , targets , repos_data ) : homogeneous = { } for package_descriptor in package_descriptors . values ( ) : pkg_name = package_descriptor . pkg_name debian_pkg_name = package_descriptor . debian_pkg_name versions = [ ] for repo_data in repos_data : versions . append ( set ( [ ] ) ) for target in targets : version = _strip_version_suffix ( repo_data . get ( target , { } ) . get ( debian_pkg_name , None ) ) versions [ - 1 ] . add ( version ) homogeneous [ pkg_name ] = max ( [ len ( v ) for v in versions ] ) == 1 return homogeneous
6369	def recall ( self ) : r if self . _tp + self . _fn == 0 : return float ( 'NaN' ) return self . _tp / ( self . _tp + self . _fn )
9540	def number_range_exclusive ( min , max , type = float ) : def checker ( v ) : if type ( v ) <= min or type ( v ) >= max : raise ValueError ( v ) return checker
3117	def oauth2_authorize ( request ) : return_url = request . GET . get ( 'return_url' , None ) if not return_url : return_url = request . META . get ( 'HTTP_REFERER' , '/' ) scopes = request . GET . getlist ( 'scopes' , django_util . oauth2_settings . scopes ) if django_util . oauth2_settings . storage_model : if not request . user . is_authenticated ( ) : return redirect ( '{0}?next={1}' . format ( settings . LOGIN_URL , parse . quote ( request . get_full_path ( ) ) ) ) else : user_oauth = django_util . UserOAuth2 ( request , scopes , return_url ) if user_oauth . has_credentials ( ) : return redirect ( return_url ) flow = _make_flow ( request = request , scopes = scopes , return_url = return_url ) auth_url = flow . step1_get_authorize_url ( ) return shortcuts . redirect ( auth_url )
7266	def run_matcher ( self , subject , * expected , ** kw ) : self . expected = expected _args = ( subject , ) if self . kind == OperatorTypes . MATCHER : _args += expected try : result = self . match ( * _args , ** kw ) except Exception as error : return self . _make_error ( error = error ) reasons = [ ] if isinstance ( result , tuple ) : result , reasons = result if result is False and self . ctx . negate : return True if result is True and not self . ctx . negate : return True return self . _make_error ( reasons = reasons )
8915	def fetch_by_name ( self , name ) : service = self . name_index . get ( name ) if not service : raise ServiceNotFound return Service ( service )
4155	def save_file ( self ) : with open ( self . write_file , 'w' ) as out_nb : json . dump ( self . work_notebook , out_nb , indent = 2 )
11322	def generate_dirlist_html ( FS , filepath ) : yield '<table class="dirlist">' if filepath == '/' : filepath = '' for name in FS . listdir ( filepath ) : full_path = pathjoin ( filepath , name ) if FS . isdir ( full_path ) : full_path = full_path + '/' yield u'<tr><td><a href="{0}">{0}</a></td></tr>' . format ( cgi . escape ( full_path ) ) yield '</table>'
2600	def unset_logging ( self ) : if self . logger_flag is True : return root_logger = logging . getLogger ( ) for hndlr in root_logger . handlers : if hndlr not in self . prior_loghandlers : hndlr . setLevel ( logging . ERROR ) self . logger_flag = True
12986	def toBytes ( self , value ) : if type ( value ) == bytes : return value return value . encode ( self . getEncoding ( ) )
5903	def glob_parts ( prefix , ext ) : if ext . startswith ( '.' ) : ext = ext [ 1 : ] files = glob . glob ( prefix + '.' + ext ) + glob . glob ( prefix + '.part[0-9][0-9][0-9][0-9].' + ext ) files . sort ( ) return files
4692	def env ( ) : ipmi = cij . env_to_dict ( PREFIX , REQUIRED ) if ipmi is None : ipmi [ "USER" ] = "admin" ipmi [ "PASS" ] = "admin" ipmi [ "HOST" ] = "localhost" ipmi [ "PORT" ] = "623" cij . info ( "ipmi.env: USER: %s, PASS: %s, HOST: %s, PORT: %s" % ( ipmi [ "USER" ] , ipmi [ "PASS" ] , ipmi [ "HOST" ] , ipmi [ "PORT" ] ) ) cij . env_export ( PREFIX , EXPORTED , ipmi ) return 0
5776	def _bcrypt_sign ( private_key , data , hash_algorithm , rsa_pss_padding = False ) : if hash_algorithm == 'raw' : digest = data else : hash_constant = { 'md5' : BcryptConst . BCRYPT_MD5_ALGORITHM , 'sha1' : BcryptConst . BCRYPT_SHA1_ALGORITHM , 'sha256' : BcryptConst . BCRYPT_SHA256_ALGORITHM , 'sha384' : BcryptConst . BCRYPT_SHA384_ALGORITHM , 'sha512' : BcryptConst . BCRYPT_SHA512_ALGORITHM } [ hash_algorithm ] digest = getattr ( hashlib , hash_algorithm ) ( data ) . digest ( ) padding_info = null ( ) flags = 0 if private_key . algorithm == 'rsa' : if rsa_pss_padding : hash_length = { 'md5' : 16 , 'sha1' : 20 , 'sha256' : 32 , 'sha384' : 48 , 'sha512' : 64 } [ hash_algorithm ] flags = BcryptConst . BCRYPT_PAD_PSS padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_PSS_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) hash_buffer = buffer_from_unicode ( hash_constant ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info_struct . cbSalt = hash_length else : flags = BcryptConst . BCRYPT_PAD_PKCS1 padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_PKCS1_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) if hash_algorithm == 'raw' : padding_info_struct . pszAlgId = null ( ) else : hash_buffer = buffer_from_unicode ( hash_constant ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info = cast ( bcrypt , 'void *' , padding_info_struct_pointer ) if private_key . algorithm == 'dsa' and private_key . bit_size > 1024 and hash_algorithm in set ( [ 'md5' , 'sha1' ] ) : raise ValueError ( pretty_message ( ) ) out_len = new ( bcrypt , 'DWORD *' ) res = bcrypt . BCryptSignHash ( private_key . key_handle , padding_info , digest , len ( digest ) , null ( ) , 0 , out_len , flags ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) if private_key . algorithm == 'rsa' : padding_info = cast ( bcrypt , 'void *' , padding_info_struct_pointer ) res = bcrypt . BCryptSignHash ( private_key . key_handle , padding_info , digest , len ( digest ) , buffer , buffer_len , out_len , flags ) handle_error ( res ) signature = bytes_from_buffer ( buffer , deref ( out_len ) ) if private_key . algorithm != 'rsa' : signature = algos . DSASignature . from_p1363 ( signature ) . dump ( ) return signature
6291	def add_texture_dir ( self , directory ) : dirs = list ( self . TEXTURE_DIRS ) dirs . append ( directory ) self . TEXTURE_DIRS = dirs
10076	def commit ( self , * args , ** kwargs ) : return super ( Deposit , self ) . commit ( * args , ** kwargs )
6515	def output ( self , msg , newline = True ) : click . echo ( text_type ( msg ) , nl = newline , file = self . output_file )
13354	def _pipepager ( text , cmd , color ) : import subprocess env = dict ( os . environ ) cmd_detail = cmd . rsplit ( '/' , 1 ) [ - 1 ] . split ( ) if color is None and cmd_detail [ 0 ] == 'less' : less_flags = os . environ . get ( 'LESS' , '' ) + ' ' . join ( cmd_detail [ 1 : ] ) if not less_flags : env [ 'LESS' ] = '-R' color = True elif 'r' in less_flags or 'R' in less_flags : color = True if not color : text = strip_ansi ( text ) c = subprocess . Popen ( cmd , shell = True , stdin = subprocess . PIPE , env = env ) encoding = get_best_encoding ( c . stdin ) try : c . stdin . write ( text . encode ( encoding , 'replace' ) ) c . stdin . close ( ) except ( IOError , KeyboardInterrupt ) : pass while True : try : c . wait ( ) except KeyboardInterrupt : pass else : break
11915	def render ( self , template , ** data ) : dct = self . global_data . copy ( ) dct . update ( data ) try : html = self . env . get_template ( template ) . render ( ** dct ) except TemplateNotFound : raise JinjaTemplateNotFound return html
1012	def _cleanUpdatesList ( self , col , cellIdx , seg ) : for key , updateList in self . segmentUpdates . iteritems ( ) : c , i = key [ 0 ] , key [ 1 ] if c == col and i == cellIdx : for update in updateList : if update [ 1 ] . segment == seg : self . _removeSegmentUpdate ( update )
8523	def add_float ( self , name , min , max , warp = None ) : min , max = map ( float , ( min , max ) ) if not min < max : raise ValueError ( 'variable %s: min >= max error' % name ) if warp not in ( None , 'log' ) : raise ValueError ( 'variable %s: warp=%s is not supported. use ' 'None or "log",' % ( name , warp ) ) if min <= 0 and warp == 'log' : raise ValueError ( 'variable %s: log-warping requires min > 0' ) self . variables [ name ] = FloatVariable ( name , min , max , warp )
12867	def cleanup ( self , app ) : if hasattr ( self . database . obj , 'close_all' ) : self . database . close_all ( )
8376	def parse ( svg , cached = False , _copy = True ) : if not cached : dom = parser . parseString ( svg ) paths = parse_node ( dom , [ ] ) else : id = _cache . id ( svg ) if not _cache . has_key ( id ) : dom = parser . parseString ( svg ) _cache . save ( id , parse_node ( dom , [ ] ) ) paths = _cache . load ( id , _copy ) return paths
8029	def sizeClassifier ( path , min_size = DEFAULTS [ 'min_size' ] ) : filestat = _stat ( path ) if stat . S_ISLNK ( filestat . st_mode ) : return if filestat . st_size < min_size : return return filestat . st_size
3399	def add_switches_and_objective ( self ) : constraints = list ( ) big_m = max ( max ( abs ( b ) for b in r . bounds ) for r in self . model . reactions ) prob = self . model . problem for rxn in self . model . reactions : if not hasattr ( rxn , 'gapfilling_type' ) : continue indicator = prob . Variable ( name = 'indicator_{}' . format ( rxn . id ) , lb = 0 , ub = 1 , type = 'binary' ) if rxn . id in self . penalties : indicator . cost = self . penalties [ rxn . id ] else : indicator . cost = self . penalties [ rxn . gapfilling_type ] indicator . rxn_id = rxn . id self . indicators . append ( indicator ) constraint_lb = prob . Constraint ( rxn . flux_expression - big_m * indicator , ub = 0 , name = 'constraint_lb_{}' . format ( rxn . id ) , sloppy = True ) constraint_ub = prob . Constraint ( rxn . flux_expression + big_m * indicator , lb = 0 , name = 'constraint_ub_{}' . format ( rxn . id ) , sloppy = True ) constraints . extend ( [ constraint_lb , constraint_ub ] ) self . model . add_cons_vars ( self . indicators ) self . model . add_cons_vars ( constraints , sloppy = True ) self . model . objective = prob . Objective ( Zero , direction = 'min' , sloppy = True ) self . model . objective . set_linear_coefficients ( { i : 1 for i in self . indicators } ) self . update_costs ( )
906	def anomalyProbability ( self , value , anomalyScore , timestamp = None ) : if timestamp is None : timestamp = self . _iteration dataPoint = ( timestamp , value , anomalyScore ) if self . _iteration < self . _probationaryPeriod : likelihood = 0.5 else : if ( ( self . _distribution is None ) or ( self . _iteration % self . _reestimationPeriod == 0 ) ) : numSkipRecords = self . _calcSkipRecords ( numIngested = self . _iteration , windowSize = self . _historicalScores . maxlen , learningPeriod = self . _learningPeriod ) _ , _ , self . _distribution = estimateAnomalyLikelihoods ( self . _historicalScores , skipRecords = numSkipRecords ) likelihoods , _ , self . _distribution = updateAnomalyLikelihoods ( [ dataPoint ] , self . _distribution ) likelihood = 1.0 - likelihoods [ 0 ] self . _historicalScores . append ( dataPoint ) self . _iteration += 1 return likelihood
1071	def getaddrspec ( self ) : aslist = [ ] self . gotonext ( ) while self . pos < len ( self . field ) : if self . field [ self . pos ] == '.' : aslist . append ( '.' ) self . pos += 1 elif self . field [ self . pos ] == '"' : aslist . append ( '"%s"' % self . getquote ( ) ) elif self . field [ self . pos ] in self . atomends : break else : aslist . append ( self . getatom ( ) ) self . gotonext ( ) if self . pos >= len ( self . field ) or self . field [ self . pos ] != '@' : return '' . join ( aslist ) aslist . append ( '@' ) self . pos += 1 self . gotonext ( ) return '' . join ( aslist ) + self . getdomain ( )
10818	def can_invite_others ( self , user ) : if self . is_managed : return False elif self . is_admin ( user ) : return True elif self . subscription_policy != SubscriptionPolicy . CLOSED : return True else : return False
2090	def _disassoc ( self , url_fragment , me , other ) : url = self . endpoint + '%d/%s/' % ( me , url_fragment ) r = client . get ( url , params = { 'id' : other } ) . json ( ) if r [ 'count' ] == 0 : return { 'changed' : False } r = client . post ( url , data = { 'disassociate' : True , 'id' : other } ) return { 'changed' : True }
2076	def main ( loader , name ) : scores = [ ] raw_scores_ds = { } X , y , mapping = loader ( ) clf = linear_model . LogisticRegression ( solver = 'lbfgs' , multi_class = 'auto' , max_iter = 200 , random_state = 0 ) encoders = ( set ( category_encoders . __all__ ) - { 'WOEEncoder' } ) for encoder_name in encoders : encoder = getattr ( category_encoders , encoder_name ) start_time = time . time ( ) score , stds , raw_scores , dim = score_models ( clf , X , y , encoder ) scores . append ( [ encoder_name , name , dim , score , stds , time . time ( ) - start_time ] ) raw_scores_ds [ encoder_name ] = raw_scores gc . collect ( ) results = pd . DataFrame ( scores , columns = [ 'Encoding' , 'Dataset' , 'Dimensionality' , 'Avg. Score' , 'Score StDev' , 'Elapsed Time' ] ) raw = pd . DataFrame . from_dict ( raw_scores_ds ) ax = raw . plot ( kind = 'box' , return_type = 'axes' ) plt . title ( 'Scores for Encodings on %s Dataset' % ( name , ) ) plt . ylabel ( 'Score (higher is better)' ) for tick in ax . get_xticklabels ( ) : tick . set_rotation ( 90 ) plt . grid ( ) plt . tight_layout ( ) plt . show ( ) return results , raw
13663	def get_item ( filename , uuid ) : with open ( os . fsencode ( str ( filename ) ) , "r" ) as f : data = json . load ( f ) results = [ i for i in data if i [ "uuid" ] == str ( uuid ) ] if results : return results return None
8916	def _retrieve_certificate ( self , access_token , timeout = 3 ) : logger . debug ( "Retrieve certificate with token." ) key_pair = crypto . PKey ( ) key_pair . generate_key ( crypto . TYPE_RSA , 2048 ) private_key = crypto . dump_privatekey ( crypto . FILETYPE_PEM , key_pair ) . decode ( "utf-8" ) cert_request = crypto . X509Req ( ) cert_request . set_pubkey ( key_pair ) cert_request . sign ( key_pair , 'md5' ) der_cert_req = crypto . dump_certificate_request ( crypto . FILETYPE_ASN1 , cert_request ) encoded_cert_req = base64 . b64encode ( der_cert_req ) token = { 'access_token' : access_token , 'token_type' : 'Bearer' } client = OAuth2Session ( token = token ) response = client . post ( self . certificate_url , data = { 'certificate_request' : encoded_cert_req } , verify = False , timeout = timeout , ) if response . ok : content = "{} {}" . format ( response . text , private_key ) with open ( self . esgf_credentials , 'w' ) as fh : fh . write ( content ) logger . debug ( 'Fetched certificate successfully.' ) else : msg = "Could not get certificate: {} {}" . format ( response . status_code , response . reason ) raise Exception ( msg ) return True
10123	def rotate ( self , angle , center = None ) : args = [ angle ] if center is not None : args . extend ( center ) self . poly . rotate ( * args ) return self
5371	def load_file ( file_path , credentials = None ) : if file_path . startswith ( 'gs://' ) : return _load_file_from_gcs ( file_path , credentials ) else : return open ( file_path , 'r' )
2599	def uncan ( obj , g = None ) : import_needed = False for cls , uncanner in iteritems ( uncan_map ) : if isinstance ( cls , string_types ) : import_needed = True break elif isinstance ( obj , cls ) : return uncanner ( obj , g ) if import_needed : _import_mapping ( uncan_map , _original_uncan_map ) return uncan ( obj , g ) return obj
794	def getActiveJobCountForClientInfo ( self , clientInfo ) : with ConnectionFactory . get ( ) as conn : query = 'SELECT count(job_id) ' 'FROM %s ' 'WHERE client_info = %%s ' ' AND status != %%s' % self . jobsTableName conn . cursor . execute ( query , [ clientInfo , self . STATUS_COMPLETED ] ) activeJobCount = conn . cursor . fetchone ( ) [ 0 ] return activeJobCount
3902	def _input_filter ( self , keys , _ ) : if keys == [ self . _keys [ 'menu' ] ] : if self . _urwid_loop . widget == self . _tabbed_window : self . _show_menu ( ) else : self . _hide_menu ( ) elif keys == [ self . _keys [ 'quit' ] ] : self . _coroutine_queue . put ( self . _client . disconnect ( ) ) else : return keys
3493	def production_envelope ( model , reactions , objective = None , carbon_sources = None , points = 20 , threshold = None ) : reactions = model . reactions . get_by_any ( reactions ) objective = model . solver . objective if objective is None else objective data = dict ( ) if carbon_sources is None : c_input = find_carbon_sources ( model ) else : c_input = model . reactions . get_by_any ( carbon_sources ) if c_input is None : data [ 'carbon_source' ] = None elif hasattr ( c_input , 'id' ) : data [ 'carbon_source' ] = c_input . id else : data [ 'carbon_source' ] = ', ' . join ( rxn . id for rxn in c_input ) threshold = normalize_cutoff ( model , threshold ) size = points ** len ( reactions ) for direction in ( 'minimum' , 'maximum' ) : data [ 'flux_{}' . format ( direction ) ] = full ( size , nan , dtype = float ) data [ 'carbon_yield_{}' . format ( direction ) ] = full ( size , nan , dtype = float ) data [ 'mass_yield_{}' . format ( direction ) ] = full ( size , nan , dtype = float ) grid = pd . DataFrame ( data ) with model : model . objective = objective objective_reactions = list ( sutil . linear_reaction_coefficients ( model ) ) if len ( objective_reactions ) != 1 : raise ValueError ( 'cannot calculate yields for objectives with ' 'multiple reactions' ) c_output = objective_reactions [ 0 ] min_max = fva ( model , reactions , fraction_of_optimum = 0 ) min_max [ min_max . abs ( ) < threshold ] = 0.0 points = list ( product ( * [ linspace ( min_max . at [ rxn . id , "minimum" ] , min_max . at [ rxn . id , "maximum" ] , points , endpoint = True ) for rxn in reactions ] ) ) tmp = pd . DataFrame ( points , columns = [ rxn . id for rxn in reactions ] ) grid = pd . concat ( [ grid , tmp ] , axis = 1 , copy = False ) add_envelope ( model , reactions , grid , c_input , c_output , threshold ) return grid
8603	def create_user ( self , user ) : data = self . _create_user_dict ( user = user ) response = self . _perform_request ( url = '/um/users' , method = 'POST' , data = json . dumps ( data ) ) return response
530	def getInputNames ( self ) : inputs = self . getSpec ( ) . inputs return [ inputs . getByIndex ( i ) [ 0 ] for i in xrange ( inputs . getCount ( ) ) ]
13007	def _from_parts ( cls , args , init = True ) : if args : args = list ( args ) if isinstance ( args [ 0 ] , WindowsPath2 ) : args [ 0 ] = args [ 0 ] . path elif args [ 0 ] . startswith ( "\\\\?\\" ) : args [ 0 ] = args [ 0 ] [ 4 : ] args = tuple ( args ) return super ( WindowsPath2 , cls ) . _from_parts ( args , init )
9048	def gradient ( self ) : grad = { } for i , f in enumerate ( self . _covariances ) : for varname , g in f . gradient ( ) . items ( ) : grad [ f"{self._name}[{i}].{varname}" ] = g return grad
13020	def _execute ( self , query , commit = False , working_columns = None ) : log . debug ( "RawlBase._execute()" ) result = [ ] if working_columns is None : working_columns = self . columns with RawlConnection ( self . dsn ) as conn : query_id = random . randrange ( 9999 ) curs = conn . cursor ( ) try : log . debug ( "Executing(%s): %s" % ( query_id , query . as_string ( curs ) ) ) except : log . exception ( "LOGGING EXCEPTION LOL" ) curs . execute ( query ) log . debug ( "Executed" ) if commit == True : log . debug ( "COMMIT(%s)" % query_id ) conn . commit ( ) log . debug ( "curs.rowcount: %s" % curs . rowcount ) if curs . rowcount > 0 : result_rows = curs . fetchall ( ) for row in result_rows : i = 0 row_dict = { } for col in working_columns : try : col = col . replace ( '.' , '_' ) row_dict [ col ] = row [ i ] except IndexError : pass i += 1 log . debug ( "Appending dict to result: %s" % row_dict ) rr = RawlResult ( working_columns , row_dict ) result . append ( rr ) curs . close ( ) return result
4168	def ss2zpk ( a , b , c , d , input = 0 ) : import scipy . signal z , p , k = scipy . signal . ss2zpk ( a , b , c , d , input = input ) return z , p , k
9656	def get_the_node_dict ( G , name ) : for node in G . nodes ( data = True ) : if node [ 0 ] == name : return node [ 1 ]
10232	def list_abundance_cartesian_expansion ( graph : BELGraph ) -> None : for u , v , k , d in list ( graph . edges ( keys = True , data = True ) ) : if CITATION not in d : continue if isinstance ( u , ListAbundance ) and isinstance ( v , ListAbundance ) : for u_member , v_member in itt . product ( u . members , v . members ) : graph . add_qualified_edge ( u_member , v_member , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( u , ListAbundance ) : for member in u . members : graph . add_qualified_edge ( member , v , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( v , ListAbundance ) : for member in v . members : graph . add_qualified_edge ( u , member , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) _remove_list_abundance_nodes ( graph )
5068	def traverse_pagination ( response , endpoint ) : results = response . get ( 'results' , [ ] ) next_page = response . get ( 'next' ) while next_page : querystring = parse_qs ( urlparse ( next_page ) . query , keep_blank_values = True ) response = endpoint . get ( ** querystring ) results += response . get ( 'results' , [ ] ) next_page = response . get ( 'next' ) return results
7600	def get_popular_tournaments ( self , ** params : keys ) : url = self . api . POPULAR + '/tournament' return self . _get_model ( url , PartialTournament , ** params )
4034	def ib64_patched ( self , attrsD , contentparams ) : if attrsD . get ( "mode" , "" ) == "base64" : return 0 if self . contentparams [ "type" ] . startswith ( "text/" ) : return 0 if self . contentparams [ "type" ] . endswith ( "+xml" ) : return 0 if self . contentparams [ "type" ] . endswith ( "/xml" ) : return 0 if self . contentparams [ "type" ] . endswith ( "/json" ) : return 0 return 0
4307	def _validate_file_formats ( input_filepath_list , combine_type ) : _validate_sample_rates ( input_filepath_list , combine_type ) if combine_type == 'concatenate' : _validate_num_channels ( input_filepath_list , combine_type )
6482	def _load_class ( class_path , default ) : if class_path is None : return default component = class_path . rsplit ( '.' , 1 ) result_processor = getattr ( importlib . import_module ( component [ 0 ] ) , component [ 1 ] , default ) if len ( component ) > 1 else default return result_processor
12976	def compat_convertHashedIndexes ( self , objs , conn = None ) : if conn is None : conn = self . _get_connection ( ) fields = [ ] for indexedField in self . indexedFields : origField = self . fields [ indexedField ] if 'hashIndex' not in origField . __class__ . __new__ . __code__ . co_varnames : continue if indexedField . hashIndex is True : hashingField = origField regField = origField . copy ( ) regField . hashIndex = False else : regField = origField hashingField = origField . copy ( ) hashingField . hashIndex = True fields . append ( ( origField , regField , hashingField ) ) objDicts = [ obj . asDict ( True , forStorage = True ) for obj in objs ] for objDict in objDicts : pipeline = conn . pipeline ( ) pk = objDict [ '_id' ] for origField , regField , hashingField in fields : val = objDict [ indexedField ] self . _rem_id_from_index ( regField , pk , val , pipeline ) self . _rem_id_from_index ( hashingField , pk , val , pipeline ) self . _add_id_to_index ( origField , pk , val , pipeline ) pipeline . execute ( )
3013	def locked_delete ( self ) : filters = { self . key_name : self . key_value } self . session . query ( self . model_class ) . filter_by ( ** filters ) . delete ( )
1498	def process_incoming_tuples ( self ) : if self . output_helper . is_out_queue_available ( ) : self . _read_tuples_and_execute ( ) self . output_helper . send_out_tuples ( ) else : self . bolt_metrics . update_out_queue_full_count ( )
422	def delete_validation_log ( self , ** kwargs ) : self . _fill_project_info ( kwargs ) self . db . ValidLog . delete_many ( kwargs ) logging . info ( "[Database] Delete ValidLog SUCCESS" )
13627	def Timestamp ( value , _divisor = 1. , tz = UTC , encoding = None ) : value = Float ( value , encoding ) if value is not None : value = value / _divisor return datetime . fromtimestamp ( value , tz ) return None
716	def __loadHyperSearchJobID ( cls , permWorkDir , outputLabel ) : filePath = cls . __getHyperSearchJobIDFilePath ( permWorkDir = permWorkDir , outputLabel = outputLabel ) jobID = None with open ( filePath , "r" ) as jobIdPickleFile : jobInfo = pickle . load ( jobIdPickleFile ) jobID = jobInfo [ "hyperSearchJobID" ] return jobID
13703	def expand_words ( self , line , width = 60 ) : if not line . strip ( ) : return line wordi = 1 while len ( strip_codes ( line ) ) < width : wordendi = self . find_word_end ( line , wordi ) if wordendi < 0 : wordi = 1 wordendi = self . find_word_end ( line , wordi ) if wordendi < 0 : line = '' . join ( ( ' ' , line ) ) else : line = ' ' . join ( ( line [ : wordendi ] , line [ wordendi : ] ) ) wordi += 1 if ' ' not in strip_codes ( line ) . strip ( ) : return line . replace ( ' ' , '' ) return line
5717	def push_datapackage ( descriptor , backend , ** backend_options ) : warnings . warn ( 'Functions "push/pull_datapackage" are deprecated. ' 'Please use "Package" class' , UserWarning ) tables = [ ] schemas = [ ] datamap = { } mapping = { } model = Package ( descriptor ) plugin = import_module ( 'jsontableschema.plugins.%s' % backend ) storage = plugin . Storage ( ** backend_options ) for resource in model . resources : if not resource . tabular : continue name = resource . descriptor . get ( 'name' , None ) table = _convert_path ( resource . descriptor [ 'path' ] , name ) schema = resource . descriptor [ 'schema' ] data = resource . table . iter ( keyed = True ) def values ( schema , data ) : for item in data : row = [ ] for field in schema [ 'fields' ] : row . append ( item . get ( field [ 'name' ] , None ) ) yield tuple ( row ) tables . append ( table ) schemas . append ( schema ) datamap [ table ] = values ( schema , data ) if name is not None : mapping [ name ] = table schemas = _convert_schemas ( mapping , schemas ) for table in tables : if table in storage . buckets : storage . delete ( table ) storage . create ( tables , schemas ) for table in storage . buckets : if table in datamap : storage . write ( table , datamap [ table ] ) return storage
3854	def add_color_to_scheme ( scheme , name , foreground , background , palette_colors ) : if foreground is None and background is None : return scheme new_scheme = [ ] for item in scheme : if item [ 0 ] == name : if foreground is None : foreground = item [ 1 ] if background is None : background = item [ 2 ] if palette_colors > 16 : new_scheme . append ( ( name , '' , '' , '' , foreground , background ) ) else : new_scheme . append ( ( name , foreground , background ) ) else : new_scheme . append ( item ) return new_scheme
245	def get_low_liquidity_transactions ( transactions , market_data , last_n_days = None ) : txn_daily_w_bar = daily_txns_with_bar_data ( transactions , market_data ) txn_daily_w_bar . index . name = 'date' txn_daily_w_bar = txn_daily_w_bar . reset_index ( ) if last_n_days is not None : md = txn_daily_w_bar . date . max ( ) - pd . Timedelta ( days = last_n_days ) txn_daily_w_bar = txn_daily_w_bar [ txn_daily_w_bar . date > md ] bar_consumption = txn_daily_w_bar . assign ( max_pct_bar_consumed = ( txn_daily_w_bar . amount / txn_daily_w_bar . volume ) * 100 ) . sort_values ( 'max_pct_bar_consumed' , ascending = False ) max_bar_consumption = bar_consumption . groupby ( 'symbol' ) . first ( ) return max_bar_consumption [ [ 'date' , 'max_pct_bar_consumed' ] ]
12341	def _set_path ( self , path ) : "Set self.path, self.dirname and self.basename." import os . path self . path = os . path . abspath ( path ) self . dirname = os . path . dirname ( path ) self . basename = os . path . basename ( path )
2631	def _status ( self ) : job_id_list = ' ' . join ( self . resources . keys ( ) ) cmd = "condor_q {0} -af:jr JobStatus" . format ( job_id_list ) retcode , stdout , stderr = super ( ) . execute_wait ( cmd ) for line in stdout . strip ( ) . split ( '\n' ) : parts = line . split ( ) job_id = parts [ 0 ] status = translate_table . get ( parts [ 1 ] , 'UNKNOWN' ) self . resources [ job_id ] [ 'status' ] = status
1650	def _DropCommonSuffixes ( filename ) : for suffix in itertools . chain ( ( '%s.%s' % ( test_suffix . lstrip ( '_' ) , ext ) for test_suffix , ext in itertools . product ( _test_suffixes , GetNonHeaderExtensions ( ) ) ) , ( '%s.%s' % ( suffix , ext ) for suffix , ext in itertools . product ( [ 'inl' , 'imp' , 'internal' ] , GetHeaderExtensions ( ) ) ) ) : if ( filename . endswith ( suffix ) and len ( filename ) > len ( suffix ) and filename [ - len ( suffix ) - 1 ] in ( '-' , '_' ) ) : return filename [ : - len ( suffix ) - 1 ] return os . path . splitext ( filename ) [ 0 ]
6312	def from_separate ( cls , meta : ProgramDescription , vertex_source , geometry_source = None , fragment_source = None , tess_control_source = None , tess_evaluation_source = None ) : instance = cls ( meta ) instance . vertex_source = ShaderSource ( VERTEX_SHADER , meta . path or meta . vertex_shader , vertex_source , ) if geometry_source : instance . geometry_source = ShaderSource ( GEOMETRY_SHADER , meta . path or meta . geometry_shader , geometry_source , ) if fragment_source : instance . fragment_source = ShaderSource ( FRAGMENT_SHADER , meta . path or meta . fragment_shader , fragment_source , ) if tess_control_source : instance . tess_control_source = ShaderSource ( TESS_CONTROL_SHADER , meta . path or meta . tess_control_shader , tess_control_source , ) if tess_evaluation_source : instance . tess_evaluation_source = ShaderSource ( TESS_EVALUATION_SHADER , meta . path or meta . tess_control_shader , tess_evaluation_source , ) return instance
12424	def loads ( s , separator = DEFAULT , index_separator = DEFAULT , cls = dict , list_cls = list ) : if isinstance ( s , six . text_type ) : io = StringIO ( s ) else : io = BytesIO ( s ) return load ( fp = io , separator = separator , index_separator = index_separator , cls = cls , list_cls = list_cls , )
5083	def get_learner_data_records ( self , enterprise_enrollment , completed_date = None , grade = None , is_passing = False ) : completed_timestamp = None course_completed = False if completed_date is not None : completed_timestamp = parse_datetime_to_epoch_millis ( completed_date ) course_completed = is_passing sapsf_user_id = enterprise_enrollment . enterprise_customer_user . get_remote_id ( ) if sapsf_user_id is not None : SapSuccessFactorsLearnerDataTransmissionAudit = apps . get_model ( 'sap_success_factors' , 'SapSuccessFactorsLearnerDataTransmissionAudit' ) return [ SapSuccessFactorsLearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , sapsf_user_id = sapsf_user_id , course_id = parse_course_key ( enterprise_enrollment . course_id ) , course_completed = course_completed , completed_timestamp = completed_timestamp , grade = grade , ) , SapSuccessFactorsLearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , sapsf_user_id = sapsf_user_id , course_id = enterprise_enrollment . course_id , course_completed = course_completed , completed_timestamp = completed_timestamp , grade = grade , ) , ] else : LOGGER . debug ( 'No learner data was sent for user [%s] because an SAP SuccessFactors user ID could not be found.' , enterprise_enrollment . enterprise_customer_user . username )
11113	def get_repository ( self , path , info = None , verbose = True ) : if path . strip ( ) in ( '' , '.' ) : path = os . getcwd ( ) realPath = os . path . realpath ( os . path . expanduser ( path ) ) if not os . path . isdir ( realPath ) : os . makedirs ( realPath ) if not self . is_repository ( realPath ) : self . create_repository ( realPath , info = info , verbose = verbose ) else : self . load_repository ( realPath )
6388	def _sb_ends_in_short_syllable ( self , term ) : if not term : return False if len ( term ) == 2 : if term [ - 2 ] in self . _vowels and term [ - 1 ] not in self . _vowels : return True elif len ( term ) >= 3 : if ( term [ - 3 ] not in self . _vowels and term [ - 2 ] in self . _vowels and term [ - 1 ] in self . _codanonvowels ) : return True return False
8551	def delete_image ( self , image_id ) : response = self . _perform_request ( url = '/images/' + image_id , method = 'DELETE' ) return response
12876	def many ( parser ) : results = [ ] terminate = object ( ) while local_ps . value : result = optional ( parser , terminate ) if result == terminate : break results . append ( result ) return results
5270	def _get_word_start_index ( self , idx ) : i = 0 for _idx in self . word_starts [ 1 : ] : if idx < _idx : return i else : i += 1 return i
11896	def _create_index_file ( root_dir , location , image_files , dirs , force_no_processing = False ) : header_text = 'imageMe: ' + location + ' [' + str ( len ( image_files ) ) + ' image(s)]' html = [ '<!DOCTYPE html>' , '<html>' , ' <head>' , ' <title>imageMe</title>' ' <style>' , ' html, body {margin: 0;padding: 0;}' , ' .header {text-align: right;}' , ' .content {' , ' padding: 3em;' , ' padding-left: 4em;' , ' padding-right: 4em;' , ' }' , ' .image {max-width: 100%; border-radius: 0.3em;}' , ' td {width: ' + str ( 100.0 / IMAGES_PER_ROW ) + '%;}' , ' </style>' , ' </head>' , ' <body>' , ' <div class="content">' , ' <h2 class="header">' + header_text + '</h2>' ] directories = [ ] if root_dir != location : directories = [ '..' ] directories += dirs if len ( directories ) > 0 : html . append ( '<hr>' ) for directory in directories : link = directory + '/' + INDEX_FILE_NAME html += [ ' <h3 class="header">' , ' <a href="' + link + '">' + directory + '</a>' , ' </h3>' ] table_row_count = 1 html += [ '<hr>' , '<table>' ] for image_file in image_files : if table_row_count == 1 : html . append ( '<tr>' ) img_src = _get_thumbnail_src_from_file ( location , image_file , force_no_processing ) link_target = _get_image_link_target_from_file ( location , image_file , force_no_processing ) html += [ ' <td>' , ' <a href="' + link_target + '">' , ' <img class="image" src="' + img_src + '">' , ' </a>' , ' </td>' ] if table_row_count == IMAGES_PER_ROW : table_row_count = 0 html . append ( '</tr>' ) table_row_count += 1 html += [ '</tr>' , '</table>' ] html += [ ' </div>' , ' </body>' , '</html>' ] index_file_path = _get_index_file_path ( location ) print ( 'Creating index file %s' % index_file_path ) index_file = open ( index_file_path , 'w' ) index_file . write ( '\n' . join ( html ) ) index_file . close ( ) return index_file_path
7106	def export ( self , model_name , export_folder ) : for transformer in self . transformers : if isinstance ( transformer , MultiLabelBinarizer ) : joblib . dump ( transformer , join ( export_folder , "label.transformer.bin" ) , protocol = 2 ) if isinstance ( transformer , TfidfVectorizer ) : joblib . dump ( transformer , join ( export_folder , "tfidf.transformer.bin" ) , protocol = 2 ) if isinstance ( transformer , CountVectorizer ) : joblib . dump ( transformer , join ( export_folder , "count.transformer.bin" ) , protocol = 2 ) if isinstance ( transformer , NumberRemover ) : joblib . dump ( transformer , join ( export_folder , "number.transformer.bin" ) , protocol = 2 ) model = [ model for model in self . models if model . name == model_name ] [ 0 ] e = Experiment ( self . X , self . y , model . estimator , None ) model_filename = join ( export_folder , "model.bin" ) e . export ( model_filename )
3987	def _move_temp_binary_to_path ( tmp_binary_path ) : binary_path = _get_binary_location ( ) if not binary_path . endswith ( constants . DUSTY_BINARY_NAME ) : raise RuntimeError ( 'Refusing to overwrite binary {}' . format ( binary_path ) ) st = os . stat ( binary_path ) permissions = st . st_mode owner = st . st_uid group = st . st_gid shutil . move ( tmp_binary_path , binary_path ) os . chown ( binary_path , owner , group ) os . chmod ( binary_path , permissions ) return binary_path
7850	def add_feature ( self , var ) : if self . has_feature ( var ) : return n = self . xmlnode . newChild ( None , "feature" , None ) n . setProp ( "var" , to_utf8 ( var ) )
4383	def is_denied ( self , role , method , resource ) : return ( role , method , resource ) in self . _denied
12888	def call ( self , path , extra = None ) : try : if not self . __webfsapi : self . __webfsapi = yield from self . get_fsapi_endpoint ( ) if not self . sid : self . sid = yield from self . create_session ( ) if not isinstance ( extra , dict ) : extra = dict ( ) params = dict ( pin = self . pin , sid = self . sid ) params . update ( ** extra ) req_url = ( '%s/%s' % ( self . __webfsapi , path ) ) result = yield from self . __session . get ( req_url , params = params , timeout = self . timeout ) if result . status == 200 : text = yield from result . text ( encoding = 'utf-8' ) else : self . sid = yield from self . create_session ( ) params = dict ( pin = self . pin , sid = self . sid ) params . update ( ** extra ) result = yield from self . __session . get ( req_url , params = params , timeout = self . timeout ) text = yield from result . text ( encoding = 'utf-8' ) return objectify . fromstring ( text ) except Exception as e : logging . info ( 'AFSAPI Exception: ' + traceback . format_exc ( ) ) return None
7858	def make_error_response ( self , cond ) : if self . stanza_type in ( "result" , "error" ) : raise ValueError ( "Errors may not be generated for" " 'result' and 'error' iq" ) stanza = Iq ( stanza_type = "error" , from_jid = self . to_jid , to_jid = self . from_jid , stanza_id = self . stanza_id , error_cond = cond ) if self . _payload is None : self . decode_payload ( ) for payload in self . _payload : Stanza . add_payload ( stanza , payload ) return stanza
2389	def f7 ( seq ) : seen = set ( ) seen_add = seen . add return [ x for x in seq if x not in seen and not seen_add ( x ) ]
11979	def set ( self , ip , netmask = None ) : if isinstance ( ip , str ) and netmask is None : ipnm = ip . split ( '/' ) if len ( ipnm ) != 2 : raise ValueError ( 'set: invalid CIDR: "%s"' % ip ) ip = ipnm [ 0 ] netmask = ipnm [ 1 ] if isinstance ( ip , IPv4Address ) : self . _ip = ip else : self . _ip = IPv4Address ( ip ) if isinstance ( netmask , IPv4NetMask ) : self . _nm = netmask else : self . _nm = IPv4NetMask ( netmask ) ipl = int ( self . _ip ) nml = int ( self . _nm ) base_add = ipl & nml self . _ip_num = 0xFFFFFFFF - 1 - nml if self . _ip_num in ( - 1 , 0 ) : if self . _ip_num == - 1 : self . _ip_num = 1 else : self . _ip_num = 2 self . _net_ip = None self . _bc_ip = None self . _first_ip_dec = base_add self . _first_ip = IPv4Address ( self . _first_ip_dec , notation = IP_DEC ) if self . _ip_num == 1 : last_ip_dec = self . _first_ip_dec else : last_ip_dec = self . _first_ip_dec + 1 self . _last_ip = IPv4Address ( last_ip_dec , notation = IP_DEC ) return self . _net_ip = IPv4Address ( base_add , notation = IP_DEC ) self . _bc_ip = IPv4Address ( base_add + self . _ip_num + 1 , notation = IP_DEC ) self . _first_ip_dec = base_add + 1 self . _first_ip = IPv4Address ( self . _first_ip_dec , notation = IP_DEC ) self . _last_ip = IPv4Address ( base_add + self . _ip_num , notation = IP_DEC )
10895	def set_filter ( self , slices , values ) : self . filters = [ [ sl , values [ sl ] ] for sl in slices ]
10179	def delete ( self , start_date = None , end_date = None ) : aggs_query = Search ( using = self . client , index = self . aggregation_alias , doc_type = self . aggregation_doc_type ) . extra ( _source = False ) range_args = { } if start_date : range_args [ 'gte' ] = self . _format_range_dt ( start_date . replace ( microsecond = 0 ) ) if end_date : range_args [ 'lte' ] = self . _format_range_dt ( end_date . replace ( microsecond = 0 ) ) if range_args : aggs_query = aggs_query . filter ( 'range' , timestamp = range_args ) bookmarks_query = Search ( using = self . client , index = self . aggregation_alias , doc_type = self . bookmark_doc_type ) . sort ( { 'date' : { 'order' : 'desc' } } ) if range_args : bookmarks_query = bookmarks_query . filter ( 'range' , date = range_args ) def _delete_actions ( ) : for query in ( aggs_query , bookmarks_query ) : affected_indices = set ( ) for doc in query . scan ( ) : affected_indices . add ( doc . meta . index ) yield dict ( _index = doc . meta . index , _op_type = 'delete' , _id = doc . meta . id , _type = doc . meta . doc_type ) current_search_client . indices . flush ( index = ',' . join ( affected_indices ) , wait_if_ongoing = True ) bulk ( self . client , _delete_actions ( ) , refresh = True )
1643	def CheckSectionSpacing ( filename , clean_lines , class_info , linenum , error ) : if ( class_info . last_line - class_info . starting_linenum <= 24 or linenum <= class_info . starting_linenum ) : return matched = Match ( r'\s*(public|protected|private):' , clean_lines . lines [ linenum ] ) if matched : prev_line = clean_lines . lines [ linenum - 1 ] if ( not IsBlankLine ( prev_line ) and not Search ( r'\b(class|struct)\b' , prev_line ) and not Search ( r'\\$' , prev_line ) ) : end_class_head = class_info . starting_linenum for i in range ( class_info . starting_linenum , linenum ) : if Search ( r'\{\s*$' , clean_lines . lines [ i ] ) : end_class_head = i break if end_class_head < linenum - 1 : error ( filename , linenum , 'whitespace/blank_line' , 3 , '"%s:" should be preceded by a blank line' % matched . group ( 1 ) )
6335	def sim ( self , src , tar , * args , ** kwargs ) : return 1.0 - self . dist ( src , tar , * args , ** kwargs )
4273	def url ( self ) : url = self . name . encode ( 'utf-8' ) return url_quote ( url ) + '/' + self . url_ext
2146	def _separate ( self , kwargs ) : self . _pop_none ( kwargs ) result = { } for field in Resource . config_fields : if field in kwargs : result [ field ] = kwargs . pop ( field ) if field in Resource . json_fields : if not isinstance ( result [ field ] , six . string_types ) : continue try : data = json . loads ( result [ field ] ) result [ field ] = data except ValueError : raise exc . TowerCLIError ( 'Provided json file format ' 'invalid. Please recheck.' ) return result
12811	def lineReceived ( self , line ) : while self . _in_header : if line : self . _headers . append ( line ) else : http , status , message = self . _headers [ 0 ] . split ( " " , 2 ) status = int ( status ) if status == 200 : self . factory . get_stream ( ) . connected ( ) else : self . factory . continueTrying = 0 self . transport . loseConnection ( ) self . factory . get_stream ( ) . disconnected ( RuntimeError ( status , message ) ) return self . _in_header = False break else : try : self . _len_expected = int ( line , 16 ) self . setRawMode ( ) except : pass
7986	def registration_success ( self , stanza ) : _unused = stanza self . lock . acquire ( ) try : self . state_change ( "registered" , self . registration_form ) if ( 'FORM_TYPE' in self . registration_form and self . registration_form [ 'FORM_TYPE' ] . value == 'jabber:iq:register' ) : if 'username' in self . registration_form : self . my_jid = JID ( self . registration_form [ 'username' ] . value , self . my_jid . domain , self . my_jid . resource ) if 'password' in self . registration_form : self . password = self . registration_form [ 'password' ] . value self . registration_callback = None self . _post_connect ( ) finally : self . lock . release ( )
685	def getTotalw ( self ) : w = sum ( [ field . w for field in self . fields ] ) return w
4568	def dumps ( data , use_yaml = None , safe = True , ** kwds ) : if use_yaml is None : use_yaml = ALWAYS_DUMP_YAML if use_yaml : dumps = yaml . safe_dump if safe else yaml . dump else : dumps = json . dumps kwds . update ( indent = 4 , sort_keys = True ) if not safe : kwds . update ( default = repr ) return dumps ( data , ** kwds )
2294	def integral_approx_estimator ( x , y ) : a , b = ( 0. , 0. ) x = np . array ( x ) y = np . array ( y ) idx , idy = ( np . argsort ( x ) , np . argsort ( y ) ) for x1 , x2 , y1 , y2 in zip ( x [ [ idx ] ] [ : - 1 ] , x [ [ idx ] ] [ 1 : ] , y [ [ idx ] ] [ : - 1 ] , y [ [ idx ] ] [ 1 : ] ) : if x1 != x2 and y1 != y2 : a = a + np . log ( np . abs ( ( y2 - y1 ) / ( x2 - x1 ) ) ) for x1 , x2 , y1 , y2 in zip ( x [ [ idy ] ] [ : - 1 ] , x [ [ idy ] ] [ 1 : ] , y [ [ idy ] ] [ : - 1 ] , y [ [ idy ] ] [ 1 : ] ) : if x1 != x2 and y1 != y2 : b = b + np . log ( np . abs ( ( x2 - x1 ) / ( y2 - y1 ) ) ) return ( a - b ) / len ( x )
7630	def values ( ns_key ) : if ns_key not in __NAMESPACE__ : raise NamespaceError ( 'Unknown namespace: {:s}' . format ( ns_key ) ) if 'enum' not in __NAMESPACE__ [ ns_key ] [ 'value' ] : raise NamespaceError ( 'Namespace {:s} is not enumerated' . format ( ns_key ) ) return copy . copy ( __NAMESPACE__ [ ns_key ] [ 'value' ] [ 'enum' ] )
6901	def _parse_xmatch_catalog_header ( xc , xk ) : catdef = [ ] if xc . endswith ( '.gz' ) : infd = gzip . open ( xc , 'rb' ) else : infd = open ( xc , 'rb' ) for line in infd : if line . decode ( ) . startswith ( '#' ) : catdef . append ( line . decode ( ) . replace ( '#' , '' ) . strip ( ) . rstrip ( '\n' ) ) if not line . decode ( ) . startswith ( '#' ) : break if not len ( catdef ) > 0 : LOGERROR ( "catalog definition not parseable " "for catalog: %s, skipping..." % xc ) return None catdef = ' ' . join ( catdef ) catdefdict = json . loads ( catdef ) catdefkeys = [ x [ 'key' ] for x in catdefdict [ 'columns' ] ] catdefdtypes = [ x [ 'dtype' ] for x in catdefdict [ 'columns' ] ] catdefnames = [ x [ 'name' ] for x in catdefdict [ 'columns' ] ] catdefunits = [ x [ 'unit' ] for x in catdefdict [ 'columns' ] ] catcolinds = [ ] catcoldtypes = [ ] catcolnames = [ ] catcolunits = [ ] for xkcol in xk : if xkcol in catdefkeys : xkcolind = catdefkeys . index ( xkcol ) catcolinds . append ( xkcolind ) catcoldtypes . append ( catdefdtypes [ xkcolind ] ) catcolnames . append ( catdefnames [ xkcolind ] ) catcolunits . append ( catdefunits [ xkcolind ] ) return ( infd , catdefdict , catcolinds , catcoldtypes , catcolnames , catcolunits )
6001	def pix_to_sub ( self ) : pix_to_sub = [ [ ] for _ in range ( self . pixels ) ] for regular_pixel , pix_pixel in enumerate ( self . sub_to_pix ) : pix_to_sub [ pix_pixel ] . append ( regular_pixel ) return pix_to_sub
3431	def remove_reactions ( self , reactions , remove_orphans = False ) : if isinstance ( reactions , string_types ) or hasattr ( reactions , "id" ) : warn ( "need to pass in a list" ) reactions = [ reactions ] context = get_context ( self ) for reaction in reactions : try : reaction = self . reactions [ self . reactions . index ( reaction ) ] except ValueError : warn ( '%s not in %s' % ( reaction , self ) ) else : forward = reaction . forward_variable reverse = reaction . reverse_variable if context : obj_coef = reaction . objective_coefficient if obj_coef != 0 : context ( partial ( self . solver . objective . set_linear_coefficients , { forward : obj_coef , reverse : - obj_coef } ) ) context ( partial ( self . _populate_solver , [ reaction ] ) ) context ( partial ( setattr , reaction , '_model' , self ) ) context ( partial ( self . reactions . add , reaction ) ) self . remove_cons_vars ( [ forward , reverse ] ) self . reactions . remove ( reaction ) reaction . _model = None for met in reaction . _metabolites : if reaction in met . _reaction : met . _reaction . remove ( reaction ) if context : context ( partial ( met . _reaction . add , reaction ) ) if remove_orphans and len ( met . _reaction ) == 0 : self . remove_metabolites ( met ) for gene in reaction . _genes : if reaction in gene . _reaction : gene . _reaction . remove ( reaction ) if context : context ( partial ( gene . _reaction . add , reaction ) ) if remove_orphans and len ( gene . _reaction ) == 0 : self . genes . remove ( gene ) if context : context ( partial ( self . genes . add , gene ) ) associated_groups = self . get_associated_groups ( reaction ) for group in associated_groups : group . remove_members ( reaction )
4607	def blacklist ( self , account ) : assert callable ( self . blockchain . account_whitelist ) return self . blockchain . account_whitelist ( account , lists = [ "black" ] , account = self )
729	def prettyPrintPattern ( self , bits , verbosity = 1 ) : numberMap = self . numberMapForBits ( bits ) text = "" numberList = [ ] numberItems = sorted ( numberMap . iteritems ( ) , key = lambda ( number , bits ) : len ( bits ) , reverse = True ) for number , bits in numberItems : if verbosity > 2 : strBits = [ str ( n ) for n in bits ] numberText = "{0} (bits: {1})" . format ( number , "," . join ( strBits ) ) elif verbosity > 1 : numberText = "{0} ({1} bits)" . format ( number , len ( bits ) ) else : numberText = str ( number ) numberList . append ( numberText ) text += "[{0}]" . format ( ", " . join ( numberList ) ) return text
728	def numberMapForBits ( self , bits ) : numberMap = dict ( ) for bit in bits : numbers = self . numbersForBit ( bit ) for number in numbers : if not number in numberMap : numberMap [ number ] = set ( ) numberMap [ number ] . add ( bit ) return numberMap
7349	def parse_token ( response ) : items = response . split ( "&" ) items = [ item . split ( "=" ) for item in items ] return { key : value for key , value in items }
3773	def set_user_methods ( self , user_methods , forced = False ) : r if isinstance ( user_methods , str ) : user_methods = [ user_methods ] self . user_methods = user_methods self . forced = forced if set ( self . user_methods ) . difference ( self . all_methods ) : raise Exception ( "One of the given methods is not available for this chemical" ) if not self . user_methods and self . forced : raise Exception ( 'Only user specified methods are considered when forced is True, but no methods were provided' ) self . method = None self . sorted_valid_methods = [ ] self . T_cached = None
1793	def MUL ( cpu , src ) : size = src . size reg_name_low , reg_name_high = { 8 : ( 'AL' , 'AH' ) , 16 : ( 'AX' , 'DX' ) , 32 : ( 'EAX' , 'EDX' ) , 64 : ( 'RAX' , 'RDX' ) } [ size ] res = ( Operators . ZEXTEND ( cpu . read_register ( reg_name_low ) , 256 ) * Operators . ZEXTEND ( src . read ( ) , 256 ) ) cpu . write_register ( reg_name_low , Operators . EXTRACT ( res , 0 , size ) ) cpu . write_register ( reg_name_high , Operators . EXTRACT ( res , size , size ) ) cpu . OF = Operators . EXTRACT ( res , size , size ) != 0 cpu . CF = cpu . OF
11726	def format_seconds ( self , n_seconds ) : func = self . ok if n_seconds >= 60 : n_minutes , n_seconds = divmod ( n_seconds , 60 ) return "%s minutes %s seconds" % ( func ( "%d" % n_minutes ) , func ( "%.3f" % n_seconds ) ) else : return "%s seconds" % ( func ( "%.3f" % n_seconds ) )
3749	def calculate_P ( self , T , P , method ) : r if method == COOLPROP : mu = PropsSI ( 'V' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : mu = self . interpolate_P ( T , P , method ) return mu
7324	def create_sample_input_files ( template_filename , database_filename , config_filename ) : print ( "Creating sample template email {}" . format ( template_filename ) ) if os . path . exists ( template_filename ) : print ( "Error: file exists: " + template_filename ) sys . exit ( 1 ) with io . open ( template_filename , "w" ) as template_file : template_file . write ( u"TO: {{email}}\n" u"SUBJECT: Testing mailmerge\n" u"FROM: My Self <myself@mydomain.com>\n" u"\n" u"Hi, {{name}},\n" u"\n" u"Your number is {{number}}.\n" ) print ( "Creating sample database {}" . format ( database_filename ) ) if os . path . exists ( database_filename ) : print ( "Error: file exists: " + database_filename ) sys . exit ( 1 ) with io . open ( database_filename , "w" ) as database_file : database_file . write ( u'email,name,number\n' u'myself@mydomain.com,"Myself",17\n' u'bob@bobdomain.com,"Bob",42\n' ) print ( "Creating sample config file {}" . format ( config_filename ) ) if os . path . exists ( config_filename ) : print ( "Error: file exists: " + config_filename ) sys . exit ( 1 ) with io . open ( config_filename , "w" ) as config_file : config_file . write ( u"# Example: GMail\n" u"[smtp_server]\n" u"host = smtp.gmail.com\n" u"port = 465\n" u"security = SSL/TLS\n" u"username = YOUR_USERNAME_HERE\n" u"#\n" u"# Example: Wide open\n" u"# [smtp_server]\n" u"# host = open-smtp.example.com\n" u"# port = 25\n" u"# security = Never\n" u"# username = None\n" u"#\n" u"# Example: University of Michigan\n" u"# [smtp_server]\n" u"# host = smtp.mail.umich.edu\n" u"# port = 465\n" u"# security = SSL/TLS\n" u"# username = YOUR_USERNAME_HERE\n" u"#\n" u"# Example: University of Michigan EECS Dept., with STARTTLS security\n" u"# [smtp_server]\n" u"# host = newman.eecs.umich.edu\n" u"# port = 25\n" u"# security = STARTTLS\n" u"# username = YOUR_USERNAME_HERE\n" u"#\n" u"# Example: University of Michigan EECS Dept., with no encryption\n" u"# [smtp_server]\n" u"# host = newman.eecs.umich.edu\n" u"# port = 25\n" u"# security = Never\n" u"# username = YOUR_USERNAME_HERE\n" ) print ( "Edit these files, and then run mailmerge again" )
3369	def set_objective ( model , value , additive = False ) : interface = model . problem reverse_value = model . solver . objective . expression reverse_value = interface . Objective ( reverse_value , direction = model . solver . objective . direction , sloppy = True ) if isinstance ( value , dict ) : if not model . objective . is_Linear : raise ValueError ( 'can only update non-linear objectives ' 'additively using object of class ' 'model.problem.Objective, not %s' % type ( value ) ) if not additive : model . solver . objective = interface . Objective ( Zero , direction = model . solver . objective . direction ) for reaction , coef in value . items ( ) : model . solver . objective . set_linear_coefficients ( { reaction . forward_variable : coef , reaction . reverse_variable : - coef } ) elif isinstance ( value , ( Basic , optlang . interface . Objective ) ) : if isinstance ( value , Basic ) : value = interface . Objective ( value , direction = model . solver . objective . direction , sloppy = False ) if not _valid_atoms ( model , value . expression ) : value = interface . Objective . clone ( value , model = model . solver ) if not additive : model . solver . objective = value else : model . solver . objective += value . expression else : raise TypeError ( '%r is not a valid objective for %r.' % ( value , model . solver ) ) context = get_context ( model ) if context : def reset ( ) : model . solver . objective = reverse_value model . solver . objective . direction = reverse_value . direction context ( reset )
11649	def transform ( self , X ) : n = self . train_ . shape [ 0 ] if X . ndim != 2 or X . shape [ 1 ] != n : msg = "X should have {} columns, the number of samples at fit time" raise TypeError ( msg . format ( n ) ) if self . copy : X = X . copy ( ) if self . shift_ != 0 and X is self . train_ or ( X . shape == self . train_ . shape and np . allclose ( X , self . train_ ) ) : X [ xrange ( n ) , xrange ( n ) ] += self . shift_ return X
2663	def scale_out ( self , blocks = 1 ) : r = [ ] for i in range ( blocks ) : if self . provider : external_block_id = str ( len ( self . blocks ) ) launch_cmd = self . launch_cmd . format ( block_id = external_block_id ) internal_block = self . provider . submit ( launch_cmd , 1 , 1 ) logger . debug ( "Launched block {}->{}" . format ( external_block_id , internal_block ) ) if not internal_block : raise ( ScalingFailed ( self . provider . label , "Attempts to provision nodes via provider has failed" ) ) r . extend ( [ external_block_id ] ) self . blocks [ external_block_id ] = internal_block else : logger . error ( "No execution provider available" ) r = None return r
4785	def starts_with ( self , prefix ) : if prefix is None : raise TypeError ( 'given prefix arg must not be none' ) if isinstance ( self . val , str_types ) : if not isinstance ( prefix , str_types ) : raise TypeError ( 'given prefix arg must be a string' ) if len ( prefix ) == 0 : raise ValueError ( 'given prefix arg must not be empty' ) if not self . val . startswith ( prefix ) : self . _err ( 'Expected <%s> to start with <%s>, but did not.' % ( self . val , prefix ) ) elif isinstance ( self . val , Iterable ) : if len ( self . val ) == 0 : raise ValueError ( 'val must not be empty' ) first = next ( iter ( self . val ) ) if first != prefix : self . _err ( 'Expected %s to start with <%s>, but did not.' % ( self . val , prefix ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
13444	def _rindex ( mylist : Sequence [ T ] , x : T ) -> int : return len ( mylist ) - mylist [ : : - 1 ] . index ( x ) - 1
8215	def show_variables_window ( self ) : if self . var_window is None and self . bot . _vars : self . var_window = VarWindow ( self , self . bot , '%s variables' % ( self . title or 'Shoebot' ) ) self . var_window . window . connect ( "destroy" , self . var_window_closed )
1197	def nested ( * managers ) : warn ( "With-statements now directly support multiple context managers" , DeprecationWarning , 3 ) exits = [ ] vars = [ ] exc = ( None , None , None ) try : for mgr in managers : exit = mgr . __exit__ enter = mgr . __enter__ vars . append ( enter ( ) ) exits . append ( exit ) yield vars except : exc = sys . exc_info ( ) finally : while exits : exit = exits . pop ( ) try : if exit ( * exc ) : exc = ( None , None , None ) except : exc = sys . exc_info ( ) if exc != ( None , None , None ) : raise exc [ 0 ] , exc [ 1 ] , exc [ 2 ]
5958	def break_array ( a , threshold = numpy . pi , other = None ) : assert len ( a . shape ) == 1 , "Only 1D arrays supported" if other is not None and a . shape != other . shape : raise ValueError ( "arrays must be of identical shape" ) breaks = numpy . where ( numpy . abs ( numpy . diff ( a ) ) >= threshold ) [ 0 ] breaks += 1 m = len ( breaks ) b = numpy . empty ( ( len ( a ) + m ) ) b_breaks = breaks + numpy . arange ( m ) mask = numpy . zeros_like ( b , dtype = numpy . bool ) mask [ b_breaks ] = True b [ ~ mask ] = a b [ mask ] = numpy . NAN if other is not None : c = numpy . empty_like ( b ) c [ ~ mask ] = other c [ mask ] = numpy . NAN ma_c = numpy . ma . array ( c , mask = mask ) else : ma_c = None return numpy . ma . array ( b , mask = mask ) , ma_c
2790	def get_snapshots ( self ) : data = self . get_data ( "volumes/%s/snapshots/" % self . id ) snapshots = list ( ) for jsond in data [ u'snapshots' ] : snapshot = Snapshot ( ** jsond ) snapshot . token = self . token snapshots . append ( snapshot ) return snapshots
5079	def strip_html_tags ( text , allowed_tags = None ) : if text is None : return if allowed_tags is None : allowed_tags = ALLOWED_TAGS return bleach . clean ( text , tags = allowed_tags , attributes = [ 'id' , 'class' , 'style' , 'href' , 'title' ] , strip = True )
13883	def GetFileLines ( filename , newline = None , encoding = None ) : return GetFileContents ( filename , binary = False , encoding = encoding , newline = newline , ) . split ( '\n' )
1171	def format_option_strings ( self , option ) : if option . takes_value ( ) : metavar = option . metavar or option . dest . upper ( ) short_opts = [ self . _short_opt_fmt % ( sopt , metavar ) for sopt in option . _short_opts ] long_opts = [ self . _long_opt_fmt % ( lopt , metavar ) for lopt in option . _long_opts ] else : short_opts = option . _short_opts long_opts = option . _long_opts if self . short_first : opts = short_opts + long_opts else : opts = long_opts + short_opts return ", " . join ( opts )
13590	def sigma_prime ( self ) : return _np . sqrt ( self . emit / self . beta ( self . E ) )
7030	def specwindow_lsp_value ( times , mags , errs , omega ) : norm_times = times - times . min ( ) tau = ( ( 1.0 / ( 2.0 * omega ) ) * nparctan ( npsum ( npsin ( 2.0 * omega * norm_times ) ) / npsum ( npcos ( 2.0 * omega * norm_times ) ) ) ) lspval_top_cos = ( npsum ( 1.0 * npcos ( omega * ( norm_times - tau ) ) ) * npsum ( 1.0 * npcos ( omega * ( norm_times - tau ) ) ) ) lspval_bot_cos = npsum ( ( npcos ( omega * ( norm_times - tau ) ) ) * ( npcos ( omega * ( norm_times - tau ) ) ) ) lspval_top_sin = ( npsum ( 1.0 * npsin ( omega * ( norm_times - tau ) ) ) * npsum ( 1.0 * npsin ( omega * ( norm_times - tau ) ) ) ) lspval_bot_sin = npsum ( ( npsin ( omega * ( norm_times - tau ) ) ) * ( npsin ( omega * ( norm_times - tau ) ) ) ) lspval = 0.5 * ( ( lspval_top_cos / lspval_bot_cos ) + ( lspval_top_sin / lspval_bot_sin ) ) return lspval
664	def makeCloneMap ( columnsShape , outputCloningWidth , outputCloningHeight = - 1 ) : if outputCloningHeight < 0 : outputCloningHeight = outputCloningWidth columnsHeight , columnsWidth = columnsShape numDistinctMasters = outputCloningWidth * outputCloningHeight a = numpy . empty ( ( columnsHeight , columnsWidth ) , 'uint32' ) for row in xrange ( columnsHeight ) : for col in xrange ( columnsWidth ) : a [ row , col ] = ( col % outputCloningWidth ) + ( row % outputCloningHeight ) * outputCloningWidth return a , numDistinctMasters
7584	def _get_binary ( self ) : backup_binaries = [ "raxmlHPC-PTHREADS" , "raxmlHPC-PTHREADS-SSE3" ] for binary in [ self . params . binary ] + backup_binaries : proc = subprocess . Popen ( [ "which" , self . params . binary ] , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) . communicate ( ) if proc : self . params . binary = binary if not proc [ 0 ] : raise Exception ( BINARY_ERROR . format ( self . params . binary ) )
440	def print_layers ( self ) : for i , layer in enumerate ( self . all_layers ) : logging . info ( " layer {:3}: {:20} {:15} {}" . format ( i , layer . name , str ( layer . get_shape ( ) ) , layer . dtype . name ) )
4080	def get_languages ( ) -> set : try : languages = cache [ 'languages' ] except KeyError : languages = LanguageTool . _get_languages ( ) cache [ 'languages' ] = languages return languages
12942	def hasUnsavedChanges ( self , cascadeObjects = False ) : if not self . _id or not self . _origData : return True for thisField in self . FIELDS : thisVal = object . __getattribute__ ( self , thisField ) if self . _origData . get ( thisField , '' ) != thisVal : return True if cascadeObjects is True and issubclass ( thisField . __class__ , IRForeignLinkFieldBase ) : if thisVal . objHasUnsavedChanges ( ) : return True return False
10920	def do_levmarq_particles ( s , particles , damping = 1.0 , decrease_damp_factor = 10. , run_length = 4 , collect_stats = False , max_iter = 2 , ** kwargs ) : lp = LMParticles ( s , particles , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , max_iter = max_iter , ** kwargs ) lp . do_run_2 ( ) if collect_stats : return lp . get_termination_stats ( )
2799	def convert_convtranspose ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting transposed convolution ...' ) if names == 'short' : tf_name = 'C' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) bias_name = '{0}.bias' . format ( w_name ) weights_name = '{0}.weight' . format ( w_name ) if len ( weights [ weights_name ] . numpy ( ) . shape ) == 4 : W = weights [ weights_name ] . numpy ( ) . transpose ( 2 , 3 , 1 , 0 ) height , width , n_filters , channels = W . shape n_groups = params [ 'group' ] if n_groups > 1 : raise AssertionError ( 'Cannot convert conv1d with groups != 1' ) if params [ 'dilations' ] [ 0 ] > 1 : raise AssertionError ( 'Cannot convert conv1d with dilation_rate != 1' ) if bias_name in weights : biases = weights [ bias_name ] . numpy ( ) has_bias = True else : biases = None has_bias = False input_name = inputs [ 0 ] if has_bias : weights = [ W , biases ] else : weights = [ W ] conv = keras . layers . Conv2DTranspose ( filters = n_filters , kernel_size = ( height , width ) , strides = ( params [ 'strides' ] [ 0 ] , params [ 'strides' ] [ 1 ] ) , padding = 'valid' , output_padding = 0 , weights = weights , use_bias = has_bias , activation = None , dilation_rate = params [ 'dilations' ] [ 0 ] , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , name = tf_name ) layers [ scope_name ] = conv ( layers [ input_name ] ) layers [ scope_name ] . set_shape ( layers [ scope_name ] . _keras_shape ) pads = params [ 'pads' ] if pads [ 0 ] > 0 : assert ( len ( pads ) == 2 or ( pads [ 2 ] == pads [ 0 ] and pads [ 3 ] == pads [ 1 ] ) ) crop = keras . layers . Cropping2D ( pads [ : 2 ] , name = tf_name + '_crop' ) layers [ scope_name ] = crop ( layers [ scope_name ] ) else : raise AssertionError ( 'Layer is not supported for now' )
6117	def circular ( cls , shape , pixel_scale , radius_arcsec , centre = ( 0. , 0. ) , invert = False ) : mask = mask_util . mask_circular_from_shape_pixel_scale_and_radius ( shape , pixel_scale , radius_arcsec , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
10514	def windowuptime ( self , window_name ) : tmp_time = self . _remote_windowuptime ( window_name ) if tmp_time : tmp_time = tmp_time . split ( '-' ) start_time = tmp_time [ 0 ] . split ( ' ' ) end_time = tmp_time [ 1 ] . split ( ' ' ) _start_time = datetime . datetime ( int ( start_time [ 0 ] ) , int ( start_time [ 1 ] ) , int ( start_time [ 2 ] ) , int ( start_time [ 3 ] ) , int ( start_time [ 4 ] ) , int ( start_time [ 5 ] ) ) _end_time = datetime . datetime ( int ( end_time [ 0 ] ) , int ( end_time [ 1 ] ) , int ( end_time [ 2 ] ) , int ( end_time [ 3 ] ) , int ( end_time [ 4 ] ) , int ( end_time [ 5 ] ) ) return _start_time , _end_time return None
13120	def argument_count ( self ) : arguments , _ = self . argparser . parse_known_args ( ) return self . count ( ** vars ( arguments ) )
790	def jobSetStatus ( self , jobID , status , useConnectionID = True , ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET status=%%s, ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE job_id=%%s' % ( self . jobsTableName , ) sqlParams = [ status , jobID ] if useConnectionID : query += ' AND _eng_cjm_conn_id=%s' sqlParams . append ( self . _connectionID ) result = conn . cursor . execute ( query , sqlParams ) if result != 1 : raise RuntimeError ( "Tried to change the status of job %d to %s, but " "this job belongs to some other CJM" % ( jobID , status ) )
5393	def _make_environment ( self , inputs , outputs , mounts ) : env = { } env . update ( providers_util . get_file_environment_variables ( inputs ) ) env . update ( providers_util . get_file_environment_variables ( outputs ) ) env . update ( providers_util . get_file_environment_variables ( mounts ) ) return env
10053	def post ( self , pid , record ) : uploaded_file = request . files [ 'file' ] key = secure_filename ( request . form . get ( 'name' ) or uploaded_file . filename ) if key in record . files : raise FileAlreadyExists ( ) record . files [ key ] = uploaded_file . stream record . commit ( ) db . session . commit ( ) return self . make_response ( obj = record . files [ key ] . obj , pid = pid , record = record , status = 201 )
5941	def transform_args ( self , * args , ** kwargs ) : newargs = self . _combineargs ( * args , ** kwargs ) return self . _build_arg_list ( ** newargs )
4297	def dump_config_file ( filename , args , parser = None ) : config = ConfigParser ( ) config . add_section ( SECTION ) if parser is None : for attr in args : config . set ( SECTION , attr , args . attr ) else : keys_empty_values_not_pass = ( '--extra-settings' , '--languages' , '--requirements' , '--template' , '--timezone' ) for action in parser . _actions : if action . dest in ( 'help' , 'config_file' , 'config_dump' , 'project_name' ) : continue keyp = action . option_strings [ 0 ] option_name = keyp . lstrip ( '-' ) option_value = getattr ( args , action . dest ) if any ( [ i for i in keys_empty_values_not_pass if i in action . option_strings ] ) : if action . dest == 'languages' : if len ( option_value ) == 1 and option_value [ 0 ] == 'en' : config . set ( SECTION , option_name , '' ) else : config . set ( SECTION , option_name , ',' . join ( option_value ) ) else : config . set ( SECTION , option_name , option_value if option_value else '' ) elif action . choices == ( 'yes' , 'no' ) : config . set ( SECTION , option_name , 'yes' if option_value else 'no' ) elif action . dest == 'templates' : config . set ( SECTION , option_name , option_value if option_value else 'no' ) elif action . dest == 'cms_version' : version = ( 'stable' if option_value == CMS_VERSION_MATRIX [ 'stable' ] else option_value ) config . set ( SECTION , option_name , version ) elif action . dest == 'django_version' : version = ( 'stable' if option_value == DJANGO_VERSION_MATRIX [ 'stable' ] else option_value ) config . set ( SECTION , option_name , version ) elif action . const : config . set ( SECTION , option_name , 'true' if option_value else 'false' ) else : config . set ( SECTION , option_name , str ( option_value ) ) with open ( filename , 'w' ) as fp : config . write ( fp )
2992	def mutualFundSymbolsDF ( token = '' , version = '' ) : df = pd . DataFrame ( mutualFundSymbols ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
6521	def directories ( self , filters = None , containing = None ) : filters = compile_masks ( filters or [ r'.*' ] ) contains = compile_masks ( containing ) for dirname , files in iteritems ( self . _found ) : relpath = text_type ( Path ( dirname ) . relative_to ( self . base_path ) ) if matches_masks ( relpath , filters ) : if not contains or self . _contains ( files , contains ) : yield dirname
5280	def sklearn2pmml ( pipeline , pmml , user_classpath = [ ] , with_repr = False , debug = False , java_encoding = "UTF-8" ) : if debug : java_version = _java_version ( java_encoding ) if java_version is None : java_version = ( "java" , "N/A" ) print ( "python: {0}" . format ( platform . python_version ( ) ) ) print ( "sklearn: {0}" . format ( sklearn . __version__ ) ) print ( "sklearn.externals.joblib: {0}" . format ( joblib . __version__ ) ) print ( "pandas: {0}" . format ( pandas . __version__ ) ) print ( "sklearn_pandas: {0}" . format ( sklearn_pandas . __version__ ) ) print ( "sklearn2pmml: {0}" . format ( __version__ ) ) print ( "{0}: {1}" . format ( java_version [ 0 ] , java_version [ 1 ] ) ) if not isinstance ( pipeline , PMMLPipeline ) : raise TypeError ( "The pipeline object is not an instance of " + PMMLPipeline . __name__ + ". Use the 'sklearn2pmml.make_pmml_pipeline(obj)' utility function to translate a regular Scikit-Learn estimator or pipeline to a PMML pipeline" ) estimator = pipeline . _final_estimator cmd = [ "java" , "-cp" , os . pathsep . join ( _classpath ( user_classpath ) ) , "org.jpmml.sklearn.Main" ] dumps = [ ] try : if with_repr : pipeline . repr_ = repr ( pipeline ) if hasattr ( estimator , "download_mojo" ) : estimator_mojo = estimator . download_mojo ( ) dumps . append ( estimator_mojo ) estimator . _mojo_path = estimator_mojo pipeline_pkl = _dump ( pipeline , "pipeline" ) cmd . extend ( [ "--pkl-pipeline-input" , pipeline_pkl ] ) dumps . append ( pipeline_pkl ) cmd . extend ( [ "--pmml-output" , pmml ] ) if debug : print ( "Executing command:\n{0}" . format ( " " . join ( cmd ) ) ) try : process = Popen ( cmd , stdout = PIPE , stderr = PIPE , bufsize = 1 ) except OSError : raise RuntimeError ( "Java is not installed, or the Java executable is not on system path" ) output , error = process . communicate ( ) retcode = process . poll ( ) if debug or retcode : if ( len ( output ) > 0 ) : print ( "Standard output:\n{0}" . format ( _decode ( output , java_encoding ) ) ) else : print ( "Standard output is empty" ) if ( len ( error ) > 0 ) : print ( "Standard error:\n{0}" . format ( _decode ( error , java_encoding ) ) ) else : print ( "Standard error is empty" ) if retcode : raise RuntimeError ( "The JPMML-SkLearn conversion application has failed. The Java executable should have printed more information about the failure into its standard output and/or standard error streams" ) finally : if debug : print ( "Preserved joblib dump file(s): {0}" . format ( " " . join ( dumps ) ) ) else : for dump in dumps : os . remove ( dump )
2378	def report ( self , linenumber , filename , severity , message , rulename , char ) : if self . _print_filename is not None : print ( "+ " + self . _print_filename ) self . _print_filename = None if severity in ( WARNING , ERROR ) : self . counts [ severity ] += 1 else : self . counts [ "other" ] += 1 print ( self . args . format . format ( linenumber = linenumber , filename = filename , severity = severity , message = message . encode ( 'utf-8' ) , rulename = rulename , char = char ) )
10	def save_policy ( self , path ) : with open ( path , 'wb' ) as f : pickle . dump ( self . policy , f )
8151	def _addvar ( self , v ) : oldvar = self . _oldvars . get ( v . name ) if oldvar is not None : if isinstance ( oldvar , Variable ) : if oldvar . compliesTo ( v ) : v . value = oldvar . value else : v . value = v . sanitize ( oldvar ) else : for listener in VarListener . listeners : listener . var_added ( v ) self . _vars [ v . name ] = v self . _namespace [ v . name ] = v . value self . _oldvars [ v . name ] = v return v
11028	def _sse_content_with_protocol ( response , handler , ** sse_kwargs ) : protocol = SseProtocol ( handler , ** sse_kwargs ) finished = protocol . when_finished ( ) response . deliverBody ( protocol ) return finished , protocol
13854	def append_main_thread ( self ) : thread = MainThread ( main_queue = self . main_queue , main_spider = self . main_spider , branch_spider = self . branch_spider ) thread . daemon = True thread . start ( )
9824	def update ( ctx , name , description , tags , private ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description if private is not None : update_dict [ 'is_public' ] = not private tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the project.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . project . update_project ( user , project_name , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Project updated." ) get_project_details ( response )
2801	def convert_reduce_sum ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting reduce_sum ...' ) keepdims = params [ 'keepdims' ] > 0 axis = params [ 'axes' ] def target_layer ( x , keepdims = keepdims , axis = axis ) : import keras . backend as K return K . sum ( x , keepdims = keepdims , axis = axis ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
13730	def value_to_bool ( config_val , evar ) : if not config_val : return False if config_val . strip ( ) . lower ( ) == 'true' : return True else : return False
1130	def urljoin ( base , url , allow_fragments = True ) : if not base : return url if not url : return base bscheme , bnetloc , bpath , bparams , bquery , bfragment = urlparse ( base , '' , allow_fragments ) scheme , netloc , path , params , query , fragment = urlparse ( url , bscheme , allow_fragments ) if scheme != bscheme or scheme not in uses_relative : return url if scheme in uses_netloc : if netloc : return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) netloc = bnetloc if path [ : 1 ] == '/' : return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) if not path and not params : path = bpath params = bparams if not query : query = bquery return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) segments = bpath . split ( '/' ) [ : - 1 ] + path . split ( '/' ) if segments [ - 1 ] == '.' : segments [ - 1 ] = '' while '.' in segments : segments . remove ( '.' ) while 1 : i = 1 n = len ( segments ) - 1 while i < n : if ( segments [ i ] == '..' and segments [ i - 1 ] not in ( '' , '..' ) ) : del segments [ i - 1 : i + 1 ] break i = i + 1 else : break if segments == [ '' , '..' ] : segments [ - 1 ] = '' elif len ( segments ) >= 2 and segments [ - 1 ] == '..' : segments [ - 2 : ] = [ '' ] return urlunparse ( ( scheme , netloc , '/' . join ( segments ) , params , query , fragment ) )
9528	def pbkdf2 ( password , salt , iterations , dklen = 0 , digest = None ) : if digest is None : digest = settings . CRYPTOGRAPHY_DIGEST if not dklen : dklen = digest . digest_size password = force_bytes ( password ) salt = force_bytes ( salt ) kdf = PBKDF2HMAC ( algorithm = digest , length = dklen , salt = salt , iterations = iterations , backend = settings . CRYPTOGRAPHY_BACKEND ) return kdf . derive ( password )
6003	def sub_to_pix ( self ) : return mapper_util . voronoi_sub_to_pix_from_grids_and_geometry ( sub_grid = self . grid_stack . sub , regular_to_nearest_pix = self . grid_stack . pix . regular_to_nearest_pix , sub_to_regular = self . grid_stack . sub . sub_to_regular , pixel_centres = self . geometry . pixel_centres , pixel_neighbors = self . geometry . pixel_neighbors , pixel_neighbors_size = self . geometry . pixel_neighbors_size ) . astype ( 'int' )
670	def createSensorToClassifierLinks ( network , sensorRegionName , classifierRegionName ) : network . link ( sensorRegionName , classifierRegionName , "UniformLink" , "" , srcOutput = "bucketIdxOut" , destInput = "bucketIdxIn" ) network . link ( sensorRegionName , classifierRegionName , "UniformLink" , "" , srcOutput = "actValueOut" , destInput = "actValueIn" ) network . link ( sensorRegionName , classifierRegionName , "UniformLink" , "" , srcOutput = "categoryOut" , destInput = "categoryIn" )
2686	def curated ( name ) : return cached_download ( 'https://docs.mikeboers.com/pyav/samples/' + name , os . path . join ( 'pyav-curated' , name . replace ( '/' , os . path . sep ) ) )
8511	def load ( self ) : from pylearn2 . config import yaml_parse from pylearn2 . datasets import Dataset dataset = yaml_parse . load ( self . yaml_string ) assert isinstance ( dataset , Dataset ) data = dataset . iterator ( mode = 'sequential' , num_batches = 1 , data_specs = dataset . data_specs , return_tuple = True ) . next ( ) if len ( data ) == 2 : X , y = data y = np . squeeze ( y ) if self . one_hot : y = np . argmax ( y , axis = 1 ) else : X = data y = None return X , y
6751	def unregister ( self ) : for k in list ( env . keys ( ) ) : if k . startswith ( self . env_prefix ) : del env [ k ] try : del all_satchels [ self . name . upper ( ) ] except KeyError : pass try : del manifest_recorder [ self . name ] except KeyError : pass try : del manifest_deployers [ self . name . upper ( ) ] except KeyError : pass try : del manifest_deployers_befores [ self . name . upper ( ) ] except KeyError : pass try : del required_system_packages [ self . name . upper ( ) ] except KeyError : pass
10432	def selectrowindex ( self , window_name , object_name , row_index ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) count = len ( object_handle . AXRows ) if row_index < 0 or row_index > count : raise LdtpServerException ( 'Row index out of range: %d' % row_index ) cell = object_handle . AXRows [ row_index ] if not cell . AXSelected : object_handle . activate ( ) cell . AXSelected = True else : pass return 1
3136	def get ( self , ** queryparams ) : return self . _mc_client . _get ( url = self . _build_path ( ) , ** queryparams )
12598	def read_xl ( xl_path : str ) : xl_path , choice = _check_xl_path ( xl_path ) reader = XL_READERS [ choice ] return reader ( xl_path )
5026	def get_result ( self , course_grade ) : return Result ( score = Score ( scaled = course_grade . percent , raw = course_grade . percent * 100 , min = MIN_SCORE , max = MAX_SCORE , ) , success = course_grade . passed , completion = course_grade . passed )
11727	def ppdict ( dict_to_print , br = '\n' , html = False , key_align = 'l' , sort_keys = True , key_preffix = '' , key_suffix = '' , value_prefix = '' , value_suffix = '' , left_margin = 3 , indent = 2 ) : if dict_to_print : if sort_keys : dic = dict_to_print . copy ( ) keys = list ( dic . keys ( ) ) keys . sort ( ) dict_to_print = OrderedDict ( ) for k in keys : dict_to_print [ k ] = dic [ k ] tmp = [ '{' ] ks = [ type ( x ) == str and "'%s'" % x or x for x in dict_to_print . keys ( ) ] vs = [ type ( x ) == str and "'%s'" % x or x for x in dict_to_print . values ( ) ] max_key_len = max ( [ len ( str ( x ) ) for x in ks ] ) for i in range ( len ( ks ) ) : k = { 1 : str ( ks [ i ] ) . ljust ( max_key_len ) , key_align == 'r' : str ( ks [ i ] ) . rjust ( max_key_len ) } [ 1 ] v = vs [ i ] tmp . append ( ' ' * indent + '{}{}{}:{}{}{},' . format ( key_preffix , k , key_suffix , value_prefix , v , value_suffix ) ) tmp [ - 1 ] = tmp [ - 1 ] [ : - 1 ] tmp . append ( '}' ) if left_margin : tmp = [ ' ' * left_margin + x for x in tmp ] if html : return '<code>{}</code>' . format ( br . join ( tmp ) . replace ( ' ' , '&nbsp;' ) ) else : return br . join ( tmp ) else : return '{}'
7119	def filter_dict ( unfiltered , filter_keys ) : filtered = DotDict ( ) for k in filter_keys : filtered [ k ] = unfiltered [ k ] return filtered
9067	def _lml_optimal_scale ( self ) : assert self . _optimal [ "scale" ] n = len ( self . _y ) lml = - self . _df * log2pi - self . _df - n * log ( self . scale ) lml -= sum ( npsum ( log ( D ) ) for D in self . _D ) return lml / 2
3770	def mixing_simple ( fracs , props ) : r if not none_and_length_check ( [ fracs , props ] ) : return None result = sum ( frac * prop for frac , prop in zip ( fracs , props ) ) return result
10251	def remove_highlight_nodes ( graph : BELGraph , nodes : Optional [ Iterable [ BaseEntity ] ] = None ) -> None : for node in graph if nodes is None else nodes : if is_node_highlighted ( graph , node ) : del graph . node [ node ] [ NODE_HIGHLIGHT ]
7720	def set_history ( self , parameters ) : for child in xml_element_iter ( self . xmlnode . children ) : if get_node_ns_uri ( child ) == MUC_NS and child . name == "history" : child . unlinkNode ( ) child . freeNode ( ) break if parameters . maxchars and parameters . maxchars < 0 : raise ValueError ( "History parameter maxchars must be positive" ) if parameters . maxstanzas and parameters . maxstanzas < 0 : raise ValueError ( "History parameter maxstanzas must be positive" ) if parameters . maxseconds and parameters . maxseconds < 0 : raise ValueError ( "History parameter maxseconds must be positive" ) hnode = self . xmlnode . newChild ( self . xmlnode . ns ( ) , "history" , None ) if parameters . maxchars is not None : hnode . setProp ( "maxchars" , str ( parameters . maxchars ) ) if parameters . maxstanzas is not None : hnode . setProp ( "maxstanzas" , str ( parameters . maxstanzas ) ) if parameters . maxseconds is not None : hnode . setProp ( "maxseconds" , str ( parameters . maxseconds ) ) if parameters . since is not None : hnode . setProp ( "since" , parameters . since . strftime ( "%Y-%m-%dT%H:%M:%SZ" ) )
1567	def invoke_hook_bolt_execute ( self , heron_tuple , execute_latency_ns ) : if len ( self . task_hooks ) > 0 : bolt_execute_info = BoltExecuteInfo ( heron_tuple = heron_tuple , executing_task_id = self . get_task_id ( ) , execute_latency_ms = execute_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . bolt_execute ( bolt_execute_info )
10611	def _calculate_H_coal ( self , T ) : m_C = 0 m_H = 0 m_O = 0 m_N = 0 m_S = 0 H = 0.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) if stoich . element_mass_fraction ( compound , 'C' ) == 1.0 : m_C += self . _compound_masses [ index ] elif stoich . element_mass_fraction ( compound , 'H' ) == 1.0 : m_H += self . _compound_masses [ index ] elif stoich . element_mass_fraction ( compound , 'O' ) == 1.0 : m_O += self . _compound_masses [ index ] elif stoich . element_mass_fraction ( compound , 'N' ) == 1.0 : m_N += self . _compound_masses [ index ] elif stoich . element_mass_fraction ( compound , 'S' ) == 1.0 : m_S += self . _compound_masses [ index ] else : dH = thermo . H ( compound , T , self . _compound_masses [ index ] ) H += dH m_total = y_C + y_H + y_O + y_N + y_S y_C = m_C / m_total y_H = m_H / m_total y_O = m_O / m_total y_N = m_N / m_total y_S = m_S / m_total hmodel = coals . DafHTy ( ) H = hmodel . calculate ( T = T + 273.15 , y_C = y_C , y_H = y_H , y_O = y_O , y_N = y_N , y_S = y_S ) / 3.6e6 H298 = hmodel . calculate ( T = 298.15 , y_C = y_C , y_H = y_H , y_O = y_O , y_N = y_N , y_S = y_S ) / 3.6e6 Hdaf = H - H298 + self . _DH298 Hdaf *= m_total H += Hdaf return H
7218	def delete ( self , task_name ) : r = self . gbdx_connection . delete ( self . _base_url + '/' + task_name ) raise_for_status ( r ) return r . text
9941	def collect ( self ) : if self . symlink and not self . local : raise CommandError ( "Can't symlink to a remote destination." ) if self . clear : self . clear_dir ( '' ) if self . symlink : handler = self . link_file else : handler = self . copy_file found_files = OrderedDict ( ) for finder in get_finders ( ) : for path , storage in finder . list ( self . ignore_patterns ) : if getattr ( storage , 'prefix' , None ) : prefixed_path = os . path . join ( storage . prefix , path ) else : prefixed_path = path if prefixed_path not in found_files : found_files [ prefixed_path ] = ( storage , path ) handler ( path , prefixed_path , storage ) if self . post_process and hasattr ( self . storage , 'post_process' ) : processor = self . storage . post_process ( found_files , dry_run = self . dry_run ) for original_path , processed_path , processed in processor : if isinstance ( processed , Exception ) : self . stderr . write ( "Post-processing '%s' failed!" % original_path ) self . stderr . write ( "" ) raise processed if processed : self . log ( "Post-processed '%s' as '%s'" % ( original_path , processed_path ) , level = 1 ) self . post_processed_files . append ( original_path ) else : self . log ( "Skipped post-processing '%s'" % original_path ) return { 'modified' : self . copied_files + self . symlinked_files , 'unmodified' : self . unmodified_files , 'post_processed' : self . post_processed_files , }
9325	def _validate_server ( self ) : if not self . _title : msg = "No 'title' in Server Discovery for request '{}'" raise ValidationError ( msg . format ( self . url ) )
2307	def reset_parameters ( self ) : stdv = 1. / math . sqrt ( self . weight . size ( 1 ) ) self . weight . data . uniform_ ( - stdv , stdv ) if self . bias is not None : self . bias . data . uniform_ ( - stdv , stdv )
118	def imap_batches_unordered ( self , batches , chunksize = 1 ) : assert ia . is_generator ( batches ) , ( "Expected to get a generator as 'batches', got type %s. " + "Call map_batches() if you use lists." ) % ( type ( batches ) , ) gen = self . pool . imap_unordered ( _Pool_starworker , self . _handle_batch_ids_gen ( batches ) , chunksize = chunksize ) for batch in gen : yield batch
735	def sort ( filename , key , outputFile , fields = None , watermark = 1024 * 1024 * 100 ) : if fields is not None : assert set ( key ) . issubset ( set ( [ f [ 0 ] for f in fields ] ) ) with FileRecordStream ( filename ) as f : if fields : fieldNames = [ ff [ 0 ] for ff in fields ] indices = [ f . getFieldNames ( ) . index ( name ) for name in fieldNames ] assert len ( indices ) == len ( fields ) else : fileds = f . getFields ( ) fieldNames = f . getFieldNames ( ) indices = None key = [ fieldNames . index ( name ) for name in key ] chunk = 0 records = [ ] for i , r in enumerate ( f ) : if indices : temp = [ ] for i in indices : temp . append ( r [ i ] ) r = temp records . append ( r ) available_memory = psutil . avail_phymem ( ) if available_memory < watermark : _sortChunk ( records , key , chunk , fields ) records = [ ] chunk += 1 if len ( records ) > 0 : _sortChunk ( records , key , chunk , fields ) chunk += 1 _mergeFiles ( key , chunk , outputFile , fields )
13790	def MessageSetItemDecoder ( extensions_by_number ) : type_id_tag_bytes = encoder . TagBytes ( 2 , wire_format . WIRETYPE_VARINT ) message_tag_bytes = encoder . TagBytes ( 3 , wire_format . WIRETYPE_LENGTH_DELIMITED ) item_end_tag_bytes = encoder . TagBytes ( 1 , wire_format . WIRETYPE_END_GROUP ) local_ReadTag = ReadTag local_DecodeVarint = _DecodeVarint local_SkipField = SkipField def DecodeItem ( buffer , pos , end , message , field_dict ) : message_set_item_start = pos type_id = - 1 message_start = - 1 message_end = - 1 while 1 : ( tag_bytes , pos ) = local_ReadTag ( buffer , pos ) if tag_bytes == type_id_tag_bytes : ( type_id , pos ) = local_DecodeVarint ( buffer , pos ) elif tag_bytes == message_tag_bytes : ( size , message_start ) = local_DecodeVarint ( buffer , pos ) pos = message_end = message_start + size elif tag_bytes == item_end_tag_bytes : break else : pos = SkipField ( buffer , pos , end , tag_bytes ) if pos == - 1 : raise _DecodeError ( 'Missing group end tag.' ) if pos > end : raise _DecodeError ( 'Truncated message.' ) if type_id == - 1 : raise _DecodeError ( 'MessageSet item missing type_id.' ) if message_start == - 1 : raise _DecodeError ( 'MessageSet item missing message.' ) extension = extensions_by_number . get ( type_id ) if extension is not None : value = field_dict . get ( extension ) if value is None : value = field_dict . setdefault ( extension , extension . message_type . _concrete_class ( ) ) if value . _InternalParse ( buffer , message_start , message_end ) != message_end : raise _DecodeError ( 'Unexpected end-group tag.' ) else : if not message . _unknown_fields : message . _unknown_fields = [ ] message . _unknown_fields . append ( ( MESSAGE_SET_ITEM_TAG , buffer [ message_set_item_start : pos ] ) ) return pos return DecodeItem
1957	def _init_arm_kernel_helpers ( self ) : page_data = bytearray ( b'\xf1\xde\xfd\xe7' * 1024 ) preamble = binascii . unhexlify ( 'ff0300ea' + '650400ea' + 'f0ff9fe5' + '430400ea' + '220400ea' + '810400ea' + '000400ea' + '870400ea' ) __kuser_cmpxchg64 = binascii . unhexlify ( '30002de9' + '08c09de5' + '30009ce8' + '010055e1' + '00005401' + '0100a013' + '0000a003' + '0c008c08' + '3000bde8' + '1eff2fe1' ) __kuser_dmb = binascii . unhexlify ( '5bf07ff5' + '1eff2fe1' ) __kuser_cmpxchg = binascii . unhexlify ( '003092e5' + '000053e1' + '0000a003' + '00108205' + '0100a013' + '1eff2fe1' ) self . _arm_tls_memory = self . current . memory . mmap ( None , 4 , 'rw ' ) __kuser_get_tls = binascii . unhexlify ( '04009FE5' + '010090e8' + '1eff2fe1' ) + struct . pack ( '<I' , self . _arm_tls_memory ) tls_area = b'\x00' * 12 version = struct . pack ( '<I' , 5 ) def update ( address , code ) : page_data [ address : address + len ( code ) ] = code update ( 0x000 , preamble ) update ( 0xf60 , __kuser_cmpxchg64 ) update ( 0xfa0 , __kuser_dmb ) update ( 0xfc0 , __kuser_cmpxchg ) update ( 0xfe0 , __kuser_get_tls ) update ( 0xff0 , tls_area ) update ( 0xffc , version ) self . current . memory . mmap ( 0xffff0000 , len ( page_data ) , 'r x' , page_data )
9409	def _extract ( data , session = None ) : if isinstance ( data , list ) : return [ _extract ( d , session ) for d in data ] if not isinstance ( data , np . ndarray ) : return data if isinstance ( data , MatlabObject ) : cls = session . _get_user_class ( data . classname ) return cls . from_value ( data ) if data . dtype . names : if data . size == 1 : return _create_struct ( data , session ) return StructArray ( data , session ) if data . dtype . kind == 'O' : return Cell ( data , session ) if data . size == 1 : return data . item ( ) if data . size == 0 : if data . dtype . kind in 'US' : return '' return [ ] return data
1188	def roundfrac ( intpart , fraction , digs ) : f = len ( fraction ) if f <= digs : return intpart , fraction + '0' * ( digs - f ) i = len ( intpart ) if i + digs < 0 : return '0' * - digs , '' total = intpart + fraction nextdigit = total [ i + digs ] if nextdigit >= '5' : n = i + digs - 1 while n >= 0 : if total [ n ] != '9' : break n = n - 1 else : total = '0' + total i = i + 1 n = 0 total = total [ : n ] + chr ( ord ( total [ n ] ) + 1 ) + '0' * ( len ( total ) - n - 1 ) intpart , fraction = total [ : i ] , total [ i : ] if digs >= 0 : return intpart , fraction [ : digs ] else : return intpart [ : digs ] + '0' * - digs , ''
11162	def size ( self ) : try : return self . _stat . st_size except : self . _stat = self . stat ( ) return self . size
4707	def power_on ( self , interval = 200 ) : if self . __power_on_port is None : cij . err ( "cij.usb.relay: Invalid USB_RELAY_POWER_ON" ) return 1 return self . __press ( self . __power_on_port , interval = interval )
667	def logProbability ( self , distn ) : x = numpy . asarray ( distn ) n = x . sum ( ) return ( logFactorial ( n ) - numpy . sum ( [ logFactorial ( k ) for k in x ] ) + numpy . sum ( x * numpy . log ( self . dist . pmf ) ) )
10946	def do_internal_run ( self ) : if not self . save_J : raise RuntimeError ( 'self.save_J=True required for do_internal_run()' ) if not np . all ( self . _has_saved_J ) : raise RuntimeError ( 'J, JTJ have not been pre-computed. Call do_run_1 or do_run_2' ) self . _do_run ( mode = 'internal' )
9162	def processor ( ) : registry = get_current_registry ( ) settings = registry . settings connection_string = settings [ CONNECTION_STRING ] channels = _get_channels ( settings ) with psycopg2 . connect ( connection_string ) as conn : conn . set_isolation_level ( ISOLATION_LEVEL_AUTOCOMMIT ) with conn . cursor ( ) as cursor : for channel in channels : cursor . execute ( 'LISTEN {}' . format ( channel ) ) logger . debug ( 'Waiting for notifications on channel "{}"' . format ( channel ) ) registry . notify ( ChannelProcessingStartUpEvent ( ) ) rlist = [ conn ] wlist = [ ] xlist = [ ] timeout = 5 while True : if select . select ( rlist , wlist , xlist , timeout ) != ( [ ] , [ ] , [ ] ) : conn . poll ( ) while conn . notifies : notif = conn . notifies . pop ( 0 ) logger . debug ( 'Got NOTIFY: pid={} channel={} payload={}' . format ( notif . pid , notif . channel , notif . payload ) ) event = create_pg_notify_event ( notif ) try : registry . notify ( event ) except Exception : logger . exception ( 'Logging an uncaught exception' )
13562	def save ( self , * args , ** kwargs ) : rerank = kwargs . pop ( 'rerank' , True ) if rerank : if not self . id : self . _process_new_rank_obj ( ) elif self . rank == self . _rank_at_load : pass else : self . _process_moved_rank_obj ( ) super ( RankedModel , self ) . save ( * args , ** kwargs )
9305	def handle_date_mismatch ( self , req ) : req_datetime = self . get_request_date ( req ) new_key_date = req_datetime . strftime ( '%Y%m%d' ) self . regenerate_signing_key ( date = new_key_date )
5756	def _strip_version_suffix ( version ) : global version_regex if not version : return version match = version_regex . search ( version ) return match . group ( 0 ) if match else version
4133	def codestr2rst ( codestr , lang = 'python' ) : code_directive = "\n.. code-block:: {0}\n\n" . format ( lang ) indented_block = indent ( codestr , ' ' * 4 ) return code_directive + indented_block
6199	def simulate_diffusion ( self , save_pos = False , total_emission = True , radial = False , rs = None , seed = 1 , path = './' , wrap_func = wrap_periodic , chunksize = 2 ** 19 , chunkslice = 'times' , verbose = True ) : if rs is None : rs = np . random . RandomState ( seed = seed ) self . open_store_traj ( chunksize = chunksize , chunkslice = chunkslice , radial = radial , path = path ) self . traj_group . _v_attrs [ 'init_random_state' ] = rs . get_state ( ) em_store = self . emission_tot if total_emission else self . emission print ( '- Start trajectories simulation - %s' % ctime ( ) , flush = True ) if verbose : print ( '[PID %d] Diffusion time:' % os . getpid ( ) , end = '' ) i_chunk = 0 t_chunk_size = self . emission . chunkshape [ 1 ] chunk_duration = t_chunk_size * self . t_step par_start_pos = self . particles . positions prev_time = 0 for time_size in iter_chunksize ( self . n_samples , t_chunk_size ) : if verbose : curr_time = int ( chunk_duration * ( i_chunk + 1 ) ) if curr_time > prev_time : print ( ' %ds' % curr_time , end = '' , flush = True ) prev_time = curr_time POS , em = self . _sim_trajectories ( time_size , par_start_pos , rs , total_emission = total_emission , save_pos = save_pos , radial = radial , wrap_func = wrap_func ) em_store . append ( em ) if save_pos : self . position . append ( np . vstack ( POS ) . astype ( 'float32' ) ) i_chunk += 1 self . store . h5file . flush ( ) self . traj_group . _v_attrs [ 'last_random_state' ] = rs . get_state ( ) self . store . h5file . flush ( ) print ( '\n- End trajectories simulation - %s' % ctime ( ) , flush = True )
10533	def update_project ( project ) : try : project_id = project . id project = _forbidden_attributes ( project ) res = _pybossa_req ( 'put' , 'project' , project_id , payload = project . data ) if res . get ( 'id' ) : return Project ( res ) else : return res except : raise
6215	def load_glb ( self ) : with open ( self . path , 'rb' ) as fd : magic = fd . read ( 4 ) if magic != GLTF_MAGIC_HEADER : raise ValueError ( "{} has incorrect header {} != {}" . format ( self . path , magic , GLTF_MAGIC_HEADER ) ) version = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] if version != 2 : raise ValueError ( "{} has unsupported version {}" . format ( self . path , version ) ) _ = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] chunk_0_length = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] chunk_0_type = fd . read ( 4 ) if chunk_0_type != b'JSON' : raise ValueError ( "Expected JSON chunk, not {} in file {}" . format ( chunk_0_type , self . path ) ) json_meta = fd . read ( chunk_0_length ) . decode ( ) chunk_1_length = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] chunk_1_type = fd . read ( 4 ) if chunk_1_type != b'BIN\x00' : raise ValueError ( "Expected BIN chunk, not {} in file {}" . format ( chunk_1_type , self . path ) ) self . meta = GLTFMeta ( self . path , json . loads ( json_meta ) , binary_buffer = fd . read ( chunk_1_length ) )
11673	def make_stacked ( self ) : "If unstacked, convert to stacked. If stacked, do nothing." if self . stacked : return self . _boundaries = bounds = np . r_ [ 0 , np . cumsum ( self . n_pts ) ] self . stacked_features = stacked = np . vstack ( self . features ) self . features = np . array ( [ stacked [ bounds [ i - 1 ] : bounds [ i ] ] for i in xrange ( 1 , len ( bounds ) ) ] , dtype = object ) self . stacked = True
10974	def new ( ) : form = GroupForm ( request . form ) if form . validate_on_submit ( ) : try : group = Group . create ( admins = [ current_user ] , ** form . data ) flash ( _ ( 'Group "%(name)s" created' , name = group . name ) , 'success' ) return redirect ( url_for ( ".index" ) ) except IntegrityError : flash ( _ ( 'Group creation failure' ) , 'error' ) return render_template ( "invenio_groups/new.html" , form = form , )
2069	def get_splice_data ( ) : df = pd . read_csv ( 'source_data/splice/splice.csv' ) X = df . reindex ( columns = [ x for x in df . columns . values if x != 'class' ] ) X [ 'dna' ] = X [ 'dna' ] . map ( lambda x : list ( str ( x ) . strip ( ) ) ) for idx in range ( 60 ) : X [ 'dna_%d' % ( idx , ) ] = X [ 'dna' ] . map ( lambda x : x [ idx ] ) del X [ 'dna' ] y = df . reindex ( columns = [ 'class' ] ) y = preprocessing . LabelEncoder ( ) . fit_transform ( y . values . reshape ( - 1 , ) ) mapping = None return X , y , mapping
8882	def predict_proba ( self , X ) : check_is_fitted ( self , [ 'inverse_influence_matrix' ] ) X = check_array ( X ) return self . __find_leverages ( X , self . inverse_influence_matrix )
6648	def _loadConfig ( self ) : config_dicts = [ self . additional_config , self . app_config ] + [ t . getConfig ( ) for t in self . hierarchy ] config_blame = [ _mirrorStructure ( self . additional_config , 'command-line config' ) , _mirrorStructure ( self . app_config , 'application\'s config.json' ) , ] + [ _mirrorStructure ( t . getConfig ( ) , t . getName ( ) ) for t in self . hierarchy ] self . config = _mergeDictionaries ( * config_dicts ) self . config_blame = _mergeDictionaries ( * config_blame )
4982	def get_available_course_modes ( self , request , course_run_id , enterprise_catalog ) : modes = EnrollmentApiClient ( ) . get_course_modes ( course_run_id ) if not modes : LOGGER . warning ( 'Unable to get course modes for course run id {course_run_id}.' . format ( course_run_id = course_run_id ) ) messages . add_generic_info_message_for_error ( request ) if enterprise_catalog : modes = [ mode for mode in modes if mode [ 'slug' ] in enterprise_catalog . enabled_course_modes ] modes . sort ( key = lambda course_mode : enterprise_catalog . enabled_course_modes . index ( course_mode [ 'slug' ] ) ) if not modes : LOGGER . info ( 'No matching course modes found for course run {course_run_id} in ' 'EnterpriseCustomerCatalog [{enterprise_catalog_uuid}]' . format ( course_run_id = course_run_id , enterprise_catalog_uuid = enterprise_catalog , ) ) messages . add_generic_info_message_for_error ( request ) return modes
12329	def compute_sha256 ( filename ) : try : h = sha256 ( ) fd = open ( filename , 'rb' ) while True : buf = fd . read ( 0x1000000 ) if buf in [ None , "" ] : break h . update ( buf . encode ( 'utf-8' ) ) fd . close ( ) return h . hexdigest ( ) except : output = run ( [ "sha256sum" , "-b" , filename ] ) return output . split ( " " ) [ 0 ]
6877	def _gunzip_sqlitecurve ( sqlitecurve ) : cmd = 'gunzip -k %s' % sqlitecurve try : subprocess . check_output ( cmd , shell = True ) return sqlitecurve . replace ( '.gz' , '' ) except subprocess . CalledProcessError : return None
3411	def add_moma ( model , solution = None , linear = True ) : r if 'moma_old_objective' in model . solver . variables : raise ValueError ( 'model is already adjusted for MOMA' ) if not linear : model . solver = sutil . choose_solver ( model , qp = True ) if solution is None : solution = pfba ( model ) prob = model . problem v = prob . Variable ( "moma_old_objective" ) c = prob . Constraint ( model . solver . objective . expression - v , lb = 0.0 , ub = 0.0 , name = "moma_old_objective_constraint" ) to_add = [ v , c ] model . objective = prob . Objective ( Zero , direction = "min" , sloppy = True ) obj_vars = [ ] for r in model . reactions : flux = solution . fluxes [ r . id ] if linear : components = sutil . add_absolute_expression ( model , r . flux_expression , name = "moma_dist_" + r . id , difference = flux , add = False ) to_add . extend ( components ) obj_vars . append ( components . variable ) else : dist = prob . Variable ( "moma_dist_" + r . id ) const = prob . Constraint ( r . flux_expression - dist , lb = flux , ub = flux , name = "moma_constraint_" + r . id ) to_add . extend ( [ dist , const ] ) obj_vars . append ( dist ** 2 ) model . add_cons_vars ( to_add ) if linear : model . objective . set_linear_coefficients ( { v : 1.0 for v in obj_vars } ) else : model . objective = prob . Objective ( add ( obj_vars ) , direction = "min" , sloppy = True )
1308	def IsProcess64Bit ( processId : int ) -> bool : try : func = ctypes . windll . ntdll . ZwWow64ReadVirtualMemory64 except Exception as ex : return False try : IsWow64Process = ctypes . windll . kernel32 . IsWow64Process IsWow64Process . argtypes = ( ctypes . c_void_p , ctypes . POINTER ( ctypes . c_int ) ) except Exception as ex : return False hProcess = ctypes . windll . kernel32 . OpenProcess ( 0x1000 , 0 , processId ) if hProcess : is64Bit = ctypes . c_int32 ( ) if IsWow64Process ( hProcess , ctypes . byref ( is64Bit ) ) : ctypes . windll . kernel32 . CloseHandle ( ctypes . c_void_p ( hProcess ) ) return False if is64Bit . value else True else : ctypes . windll . kernel32 . CloseHandle ( ctypes . c_void_p ( hProcess ) )
11685	def cli ( id ) : ch = Analyse ( id ) ch . full_analysis ( ) click . echo ( 'Created: %s. Modified: %s. Deleted: %s' % ( ch . create , ch . modify , ch . delete ) ) if ch . is_suspect : click . echo ( 'The changeset {} is suspect! Reasons: {}' . format ( id , ', ' . join ( ch . suspicion_reasons ) ) ) else : click . echo ( 'The changeset %s is not suspect!' % id )
4204	def rlevinson ( a , efinal ) : a = numpy . array ( a ) realdata = numpy . isrealobj ( a ) assert a [ 0 ] == 1 , 'First coefficient of the prediction polynomial must be unity' p = len ( a ) if p < 2 : raise ValueError ( 'Polynomial should have at least two coefficients' ) if realdata == True : U = numpy . zeros ( ( p , p ) ) else : U = numpy . zeros ( ( p , p ) , dtype = complex ) U [ : , p - 1 ] = numpy . conj ( a [ - 1 : : - 1 ] ) p = p - 1 e = numpy . zeros ( p ) e [ - 1 ] = efinal for k in range ( p - 1 , 0 , - 1 ) : [ a , e [ k - 1 ] ] = levdown ( a , e [ k ] ) U [ : , k ] = numpy . concatenate ( ( numpy . conj ( a [ - 1 : : - 1 ] . transpose ( ) ) , [ 0 ] * ( p - k ) ) ) e0 = e [ 0 ] / ( 1. - abs ( a [ 1 ] ** 2 ) ) U [ 0 , 0 ] = 1 kr = numpy . conj ( U [ 0 , 1 : ] ) kr = kr . transpose ( ) R = numpy . zeros ( 1 , dtype = complex ) k = 1 R0 = e0 R [ 0 ] = - numpy . conj ( U [ 0 , 1 ] ) * R0 for k in range ( 1 , p ) : r = - sum ( numpy . conj ( U [ k - 1 : : - 1 , k ] ) * R [ - 1 : : - 1 ] ) - kr [ k ] * e [ k - 1 ] R = numpy . insert ( R , len ( R ) , r ) R = numpy . insert ( R , 0 , e0 ) return R , U , kr , e
12034	def kernel_gaussian ( self , sizeMS , sigmaMS = None , forwardOnly = False ) : sigmaMS = sizeMS / 10 if sigmaMS is None else sigmaMS size , sigma = sizeMS * self . pointsPerMs , sigmaMS * self . pointsPerMs self . kernel = swhlab . common . kernel_gaussian ( size , sigma , forwardOnly ) return self . kernel
10322	def spanning_1d_chain ( length ) : ret = nx . grid_graph ( dim = [ int ( length + 2 ) ] ) ret . node [ 0 ] [ 'span' ] = 0 ret [ 0 ] [ 1 ] [ 'span' ] = 0 ret . node [ length + 1 ] [ 'span' ] = 1 ret [ length ] [ length + 1 ] [ 'span' ] = 1 return ret
812	def _fixupRandomEncoderParams ( params , minVal , maxVal , minResolution ) : encodersDict = ( params [ "modelConfig" ] [ "modelParams" ] [ "sensorParams" ] [ "encoders" ] ) for encoder in encodersDict . itervalues ( ) : if encoder is not None : if encoder [ "type" ] == "RandomDistributedScalarEncoder" : resolution = max ( minResolution , ( maxVal - minVal ) / encoder . pop ( "numBuckets" ) ) encodersDict [ "c1" ] [ "resolution" ] = resolution
2696	def get_tiles ( graf , size = 3 ) : keeps = list ( filter ( lambda w : w . word_id > 0 , graf ) ) keeps_len = len ( keeps ) for i in iter ( range ( 0 , keeps_len - 1 ) ) : w0 = keeps [ i ] for j in iter ( range ( i + 1 , min ( keeps_len , i + 1 + size ) ) ) : w1 = keeps [ j ] if ( w1 . idx - w0 . idx ) <= size : yield ( w0 . root , w1 . root , )
414	def delete_model ( self , ** kwargs ) : self . _fill_project_info ( kwargs ) self . db . Model . delete_many ( kwargs ) logging . info ( "[Database] Delete Model SUCCESS" )
13556	def all_comments ( self ) : ctype = ContentType . objects . get ( app_label__exact = "happenings" , model__exact = 'event' ) update_ctype = ContentType . objects . get ( app_label__exact = "happenings" , model__exact = 'update' ) update_ids = self . update_set . values_list ( 'id' , flat = True ) return Comment . objects . filter ( Q ( content_type = ctype . id , object_pk = self . id ) | Q ( content_type = update_ctype . id , object_pk__in = update_ids ) )
3506	def loopless_fva_iter ( model , reaction , solution = False , zero_cutoff = None ) : zero_cutoff = normalize_cutoff ( model , zero_cutoff ) current = model . objective . value sol = get_solution ( model ) objective_dir = model . objective . direction if reaction . boundary : if solution : return sol else : return current with model : _add_cycle_free ( model , sol . fluxes ) model . slim_optimize ( ) if abs ( reaction . flux - current ) < zero_cutoff : if solution : return sol return current ll_sol = get_solution ( model ) . fluxes reaction . bounds = ( current , current ) model . slim_optimize ( ) almost_ll_sol = get_solution ( model ) . fluxes with model : for rxn in model . reactions : rid = rxn . id if ( ( abs ( ll_sol [ rid ] ) < zero_cutoff ) and ( abs ( almost_ll_sol [ rid ] ) > zero_cutoff ) ) : rxn . bounds = max ( 0 , rxn . lower_bound ) , min ( 0 , rxn . upper_bound ) if solution : best = model . optimize ( ) else : model . slim_optimize ( ) best = reaction . flux model . objective . direction = objective_dir return best
11918	def get_dataframe ( self ) : assert self . dataframe is not None , ( "'%s' should either include a `dataframe` attribute, " "or override the `get_dataframe()` method." % self . __class__ . __name__ ) dataframe = self . dataframe return dataframe
7655	def update ( self , ** kwargs ) : for name , value in six . iteritems ( kwargs ) : setattr ( self , name , value )
9855	def _read_header ( self , ccp4file ) : bsaflag = self . _detect_byteorder ( ccp4file ) nheader = struct . calcsize ( self . _headerfmt ) names = [ r . key for r in self . _header_struct ] bintopheader = ccp4file . read ( 25 * 4 ) def decode_header ( header , bsaflag = '@' ) : h = dict ( zip ( names , struct . unpack ( bsaflag + self . _headerfmt , header ) ) ) h [ 'bsaflag' ] = bsaflag return h header = decode_header ( bintopheader , bsaflag ) for rec in self . _header_struct : if not rec . is_legal_dict ( header ) : warnings . warn ( "Key %s: Illegal value %r" % ( rec . key , header [ rec . key ] ) ) if ( header [ 'lskflg' ] ) : skewmatrix = np . fromfile ( ccp4file , dtype = np . float32 , count = 9 ) header [ 'skwmat' ] = skewmatrix . reshape ( ( 3 , 3 ) ) header [ 'skwtrn' ] = np . fromfile ( ccp4file , dtype = np . float32 , count = 3 ) else : header [ 'skwmat' ] = header [ 'skwtrn' ] = None ccp4file . seek ( 12 * 4 , 1 ) ccp4file . seek ( 15 * 4 , 1 ) ccp4file . seek ( 4 , 1 ) endiancode = struct . unpack ( bsaflag + '4b' , ccp4file . read ( 4 ) ) header [ 'endianness' ] = 'little' if endiancode == ( 0x44 , 0x41 , 0 , 0 ) else 'big' header [ 'arms' ] = struct . unpack ( bsaflag + 'f' , ccp4file . read ( 4 ) ) [ 0 ] header [ 'nlabl' ] = struct . unpack ( bsaflag + 'I' , ccp4file . read ( 4 ) ) [ 0 ] if header [ 'nlabl' ] : binlabel = ccp4file . read ( 80 * header [ 'nlabl' ] ) flag = bsaflag + str ( 80 * header [ 'nlabl' ] ) + 's' label = struct . unpack ( flag , binlabel ) [ 0 ] header [ 'label' ] = label . decode ( 'utf-8' ) . rstrip ( '\x00' ) else : header [ 'label' ] = None ccp4file . seek ( 256 * 4 ) return header
1820	def SETP ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . PF , 1 , 0 ) )
2805	def convert_elementwise_add ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting elementwise_add ...' ) if 'broadcast' in params : model0 = layers [ inputs [ 0 ] ] model1 = layers [ inputs [ 1 ] ] if names == 'short' : tf_name = 'A' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) def target_layer ( x ) : layer = tf . add ( x [ 0 ] , x [ 1 ] ) return layer lambda_layer = keras . layers . Lambda ( target_layer , name = tf_name ) layers [ scope_name ] = lambda_layer ( [ layers [ inputs [ 0 ] ] , layers [ inputs [ 1 ] ] ] ) else : model0 = layers [ inputs [ 0 ] ] model1 = layers [ inputs [ 1 ] ] if names == 'short' : tf_name = 'A' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) add = keras . layers . Add ( name = tf_name ) layers [ scope_name ] = add ( [ model0 , model1 ] )
7853	def identity_is ( self , item_category , item_type = None ) : if not item_category : raise ValueError ( "bad category" ) if not item_type : type_expr = u"" elif '"' not in item_type : type_expr = u' and @type="%s"' % ( item_type , ) elif "'" not in type : type_expr = u" and @type='%s'" % ( item_type , ) else : raise ValueError ( "Invalid type name" ) if '"' not in item_category : expr = u'd:identity[@category="%s"%s]' % ( item_category , type_expr ) elif "'" not in item_category : expr = u"d:identity[@category='%s'%s]" % ( item_category , type_expr ) else : raise ValueError ( "Invalid category name" ) l = self . xpath_ctxt . xpathEval ( to_utf8 ( expr ) ) if l : return True else : return False
7962	def _close ( self ) : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" ) if self . _socket is None : return try : self . _socket . shutdown ( socket . SHUT_RDWR ) except socket . error : pass self . _socket . close ( ) self . _socket = None self . _write_queue . clear ( ) self . _write_queue_cond . notify ( )
5134	def generate_pagerank_graph ( num_vertices = 250 , ** kwargs ) : g = minimal_random_graph ( num_vertices , ** kwargs ) r = np . zeros ( num_vertices ) for k , pr in nx . pagerank ( g ) . items ( ) : r [ k ] = pr g = set_types_rank ( g , rank = r , ** kwargs ) return g
8166	def reload_functions ( self ) : with LiveExecution . lock : if self . edited_source : tree = ast . parse ( self . edited_source ) for f in [ n for n in ast . walk ( tree ) if isinstance ( n , ast . FunctionDef ) ] : self . ns [ f . name ] . __code__ = meta . decompiler . compile_func ( f , self . filename , self . ns ) . __code__
3247	def get_users ( group , ** conn ) : group_details = get_group_api ( group [ 'GroupName' ] , ** conn ) user_list = [ ] for user in group_details . get ( 'Users' , [ ] ) : user_list . append ( user [ 'UserName' ] ) return user_list
3616	def _should_really_index ( self , instance ) : if self . _should_index_is_method : is_method = inspect . ismethod ( self . should_index ) try : count_args = len ( inspect . signature ( self . should_index ) . parameters ) except AttributeError : count_args = len ( inspect . getargspec ( self . should_index ) . args ) if is_method or count_args is 1 : return self . should_index ( instance ) else : return self . should_index ( ) else : attr_type = type ( self . should_index ) if attr_type is DeferredAttribute : attr_value = self . should_index . __get__ ( instance , None ) elif attr_type is str : attr_value = getattr ( instance , self . should_index ) elif attr_type is property : attr_value = self . should_index . __get__ ( instance ) else : raise AlgoliaIndexError ( '{} should be a boolean attribute or a method that returns a boolean.' . format ( self . should_index ) ) if type ( attr_value ) is not bool : raise AlgoliaIndexError ( "%s's should_index (%s) should be a boolean" % ( instance . __class__ . __name__ , self . should_index ) ) return attr_value
5773	def dsa_sign ( private_key , data , hash_algorithm ) : if private_key . algorithm != 'dsa' : raise ValueError ( 'The key specified is not a DSA private key' ) return _sign ( private_key , data , hash_algorithm )
12813	def styles ( self ) : styles = get_all_styles ( ) whitelist = self . app . config . get ( 'CSL_STYLES_WHITELIST' ) if whitelist : return { k : v for k , v in styles . items ( ) if k in whitelist } return styles
8615	def wait_for_completion ( self , response , timeout = 3600 , initial_wait = 5 , scaleup = 10 ) : if not response : return logger = logging . getLogger ( __name__ ) wait_period = initial_wait next_increase = time . time ( ) + wait_period * scaleup if timeout : timeout = time . time ( ) + timeout while True : request = self . get_request ( request_id = response [ 'requestId' ] , status = True ) if request [ 'metadata' ] [ 'status' ] == 'DONE' : break elif request [ 'metadata' ] [ 'status' ] == 'FAILED' : raise PBFailedRequest ( 'Request {0} failed to complete: {1}' . format ( response [ 'requestId' ] , request [ 'metadata' ] [ 'message' ] ) , response [ 'requestId' ] ) current_time = time . time ( ) if timeout and current_time > timeout : raise PBTimeoutError ( 'Timed out waiting for request {0}.' . format ( response [ 'requestId' ] ) , response [ 'requestId' ] ) if current_time > next_increase : wait_period *= 2 next_increase = time . time ( ) + wait_period * scaleup scaleup *= 2 logger . info ( "Request %s is in state '%s'. Sleeping for %i seconds..." , response [ 'requestId' ] , request [ 'metadata' ] [ 'status' ] , wait_period ) time . sleep ( wait_period )
7381	def simplified_edges ( self ) : for group , edgelist in self . edges . items ( ) : for u , v , d in edgelist : yield ( u , v )
8321	def parse_important ( self , markup ) : important = [ ] table_titles = [ table . title for table in self . tables ] m = re . findall ( self . re [ "bold" ] , markup ) for bold in m : bold = self . plain ( bold ) if not bold in table_titles : important . append ( bold . lower ( ) ) return important
747	def anomalyGetLabels ( self , start , end ) : return self . _getAnomalyClassifier ( ) . getSelf ( ) . getLabels ( start , end )
13698	def wait ( self , timeout = None ) : if timeout is None : timeout = self . _timeout while self . _process . check_readable ( timeout ) : self . _flush ( )
617	def parseTimestamp ( s ) : s = s . strip ( ) for pattern in DATETIME_FORMATS : try : return datetime . datetime . strptime ( s , pattern ) except ValueError : pass raise ValueError ( 'The provided timestamp %s is malformed. The supported ' 'formats are: [%s]' % ( s , ', ' . join ( DATETIME_FORMATS ) ) )
3706	def COSTALD ( T , Tc , Vc , omega ) : r Tr = T / Tc V_delta = ( - 0.296123 + 0.386914 * Tr - 0.0427258 * Tr ** 2 - 0.0480645 * Tr ** 3 ) / ( Tr - 1.00001 ) V_0 = 1 - 1.52816 * ( 1 - Tr ) ** ( 1 / 3. ) + 1.43907 * ( 1 - Tr ) ** ( 2 / 3. ) - 0.81446 * ( 1 - Tr ) + 0.190454 * ( 1 - Tr ) ** ( 4 / 3. ) return Vc * V_0 * ( 1 - omega * V_delta )
9936	def list ( self , ignore_patterns ) : for prefix , root in self . locations : storage = self . storages [ root ] for path in utils . get_files ( storage , ignore_patterns ) : yield path , storage
13121	def get_pipe ( self , object_type ) : for line in sys . stdin : try : data = json . loads ( line . strip ( ) ) obj = object_type ( ** data ) yield obj except ValueError : yield self . id_to_object ( line . strip ( ) )
2714	def load ( self ) : tags = self . get_data ( "tags/%s" % self . name ) tag = tags [ 'tag' ] for attr in tag . keys ( ) : setattr ( self , attr , tag [ attr ] ) return self
8884	def fit ( self , X , y = None ) : X = check_array ( X ) self . _x_min = X . min ( axis = 0 ) self . _x_max = X . max ( axis = 0 ) return self
5418	def format_logging_uri ( uri , job_metadata , task_metadata ) : fmt = str ( uri ) if '{' not in fmt : if uri . endswith ( '.log' ) : fmt = os . path . splitext ( uri ) [ 0 ] else : fmt = os . path . join ( uri , '{job-id}' ) if task_metadata . get ( 'task-id' ) is not None : fmt += '.{task-id}' if task_metadata . get ( 'task-attempt' ) is not None : fmt += '.{task-attempt}' fmt += '.log' return _format_task_uri ( fmt , job_metadata , task_metadata )
7385	def group_theta ( self , group ) : for i , g in enumerate ( self . nodes . keys ( ) ) : if g == group : break return i * self . major_angle
6083	def deflections_of_galaxies_from_sub_grid ( sub_grid , galaxies ) : if galaxies : return sum ( map ( lambda galaxy : galaxy . deflections_from_grid ( sub_grid ) , galaxies ) ) else : return np . full ( ( sub_grid . shape [ 0 ] , 2 ) , 0.0 )
6839	def distrib_release ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : kernel = ( run ( 'uname -s' ) or '' ) . strip ( ) . lower ( ) if kernel == LINUX : return run ( 'lsb_release -r --short' ) elif kernel == SUNOS : return run ( 'uname -v' )
6864	def normalized_flux_to_mag ( lcdict , columns = ( 'sap.sap_flux' , 'sap.sap_flux_err' , 'sap.sap_bkg' , 'sap.sap_bkg_err' , 'pdc.pdcsap_flux' , 'pdc.pdcsap_flux_err' ) ) : tess_mag = lcdict [ 'objectinfo' ] [ 'tessmag' ] for key in columns : k1 , k2 = key . split ( '.' ) if 'err' not in k2 : lcdict [ k1 ] [ k2 . replace ( 'flux' , 'mag' ) ] = ( tess_mag - 2.5 * np . log10 ( lcdict [ k1 ] [ k2 ] ) ) else : lcdict [ k1 ] [ k2 . replace ( 'flux' , 'mag' ) ] = ( - 2.5 * np . log10 ( 1.0 - lcdict [ k1 ] [ k2 ] ) ) return lcdict
7820	def flush ( self , dispatch = True ) : if dispatch : while True : event = self . dispatch ( False ) if event in ( None , QUIT ) : return event else : while True : try : self . queue . get ( False ) except Queue . Empty : return None
1883	def new_symbolic_value ( self , nbits , label = None , taint = frozenset ( ) ) : assert nbits in ( 1 , 4 , 8 , 16 , 32 , 64 , 128 , 256 ) avoid_collisions = False if label is None : label = 'val' avoid_collisions = True expr = self . _constraints . new_bitvec ( nbits , name = label , taint = taint , avoid_collisions = avoid_collisions ) self . _input_symbols . append ( expr ) return expr
1936	def constructor_abi ( self ) -> Dict [ str , Any ] : item = self . _constructor_abi_item if item : return dict ( item ) return { 'inputs' : [ ] , 'payable' : False , 'stateMutability' : 'nonpayable' , 'type' : 'constructor' }
12665	def apply_mask ( image , mask_img ) : img = check_img ( image ) mask = check_img ( mask_img ) check_img_compatibility ( img , mask ) vol = img . get_data ( ) mask_data , _ = load_mask_data ( mask ) return vol [ mask_data ] , mask_data
1288	def tf_baseline_loss ( self , states , internals , reward , update , reference = None ) : if self . baseline_mode == 'states' : loss = self . baseline . loss ( states = states , internals = internals , reward = reward , update = update , reference = reference ) elif self . baseline_mode == 'network' : loss = self . baseline . loss ( states = self . network . apply ( x = states , internals = internals , update = update ) , internals = internals , reward = reward , update = update , reference = reference ) regularization_loss = self . baseline . regularization_loss ( ) if regularization_loss is not None : loss += regularization_loss return loss
2976	def cmd_status ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) containers = b . status ( ) print_containers ( containers , opts . json )
7289	def has_digit ( string_or_list , sep = "_" ) : if isinstance ( string_or_list , ( tuple , list ) ) : list_length = len ( string_or_list ) if list_length : return six . text_type ( string_or_list [ - 1 ] ) . isdigit ( ) else : return False else : return has_digit ( string_or_list . split ( sep ) )
8546	def delete_datacenter ( self , datacenter_id ) : response = self . _perform_request ( url = '/datacenters/%s' % ( datacenter_id ) , method = 'DELETE' ) return response
1938	def get_func_argument_types ( self , hsh : bytes ) : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) sig = self . _function_signatures_by_selector . get ( hsh ) return '()' if sig is None else sig [ sig . find ( '(' ) : ]
4437	def build ( self , track , requester ) : try : self . track = track [ 'track' ] self . identifier = track [ 'info' ] [ 'identifier' ] self . can_seek = track [ 'info' ] [ 'isSeekable' ] self . author = track [ 'info' ] [ 'author' ] self . duration = track [ 'info' ] [ 'length' ] self . stream = track [ 'info' ] [ 'isStream' ] self . title = track [ 'info' ] [ 'title' ] self . uri = track [ 'info' ] [ 'uri' ] self . requester = requester return self except KeyError : raise InvalidTrack ( 'An invalid track was passed.' )
13065	def expose_ancestors_or_children ( self , member , collection , lang = None ) : x = { "id" : member . id , "label" : str ( member . get_label ( lang ) ) , "model" : str ( member . model ) , "type" : str ( member . type ) , "size" : member . size , "semantic" : self . semantic ( member , parent = collection ) } if isinstance ( member , ResourceCollection ) : x [ "lang" ] = str ( member . lang ) return x
8058	def do_play ( self , line ) : if self . pause_speed is None : self . bot . _speed = self . pause_speed self . pause_speed = None self . print_response ( "Play" )
13528	def printoptions ( ) : x = json . dumps ( environment . options , indent = 4 , sort_keys = True , skipkeys = True , cls = MyEncoder ) print ( x )
12061	def processArgs ( ) : if len ( sys . argv ) < 2 : print ( "\n\nERROR:" ) print ( "this script requires arguments!" ) print ( 'try "python command.py info"' ) return if sys . argv [ 1 ] == 'info' : print ( "import paths:\n " , "\n " . join ( sys . path ) ) print ( ) print ( "python version:" , sys . version ) print ( "SWHLab path:" , __file__ ) print ( "SWHLab version:" , swhlab . __version__ ) return if sys . argv [ 1 ] == 'glanceFolder' : abfFolder = swhlab . common . gui_getFolder ( ) if not abfFolder or not os . path . isdir ( abfFolder ) : print ( "bad path" ) return fnames = sorted ( glob . glob ( abfFolder + "/*.abf" ) ) outFolder = tempfile . gettempdir ( ) + "/swhlab/" if os . path . exists ( outFolder ) : shutil . rmtree ( outFolder ) os . mkdir ( outFolder ) outFile = outFolder + "/index.html" out = '<html><body>' out += '<h2>%s</h2>' % abfFolder for i , fname in enumerate ( fnames ) : print ( "\n\n### PROCESSING %d of %d" % ( i , len ( fnames ) ) ) saveAs = os . path . join ( os . path . dirname ( outFolder ) , os . path . basename ( fname ) ) + ".png" out += '<br><br><br><code>%s</code><br>' % os . path . abspath ( fname ) out += '<a href="%s"><img src="%s"></a><br>' % ( saveAs , saveAs ) swhlab . analysis . glance . processAbf ( fname , saveAs ) out += '</body></html>' with open ( outFile , 'w' ) as f : f . write ( out ) webbrowser . open_new_tab ( outFile ) return print ( "\n\nERROR:\nI'm not sure how to process these arguments!" ) print ( sys . argv )
11647	def fit_transform ( self , X , y = None ) : n = X . shape [ 0 ] if X . shape != ( n , n ) : raise TypeError ( "Input must be a square matrix." ) memory = get_memory ( self . memory ) discard_X = not self . copy and self . negatives_likely vals , vecs = memory . cache ( scipy . linalg . eigh , ignore = [ 'overwrite_a' ] ) ( X , overwrite_a = discard_X ) vals = vals [ : , None ] self . clip_ = np . dot ( vecs , np . sign ( vals ) * vecs . T ) if discard_X or vals [ 0 , 0 ] < 0 : del X np . abs ( vals , out = vals ) X = np . dot ( vecs , vals * vecs . T ) del vals , vecs X = Symmetrize ( copy = False ) . fit_transform ( X ) return X
2542	def set_file_chksum ( self , doc , chk_sum ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_chksum_set : self . file_chksum_set = True self . file ( doc ) . chk_sum = checksum . Algorithm ( 'SHA1' , chk_sum ) return True else : raise CardinalityError ( 'File::CheckSum' ) else : raise OrderError ( 'File::CheckSum' )
6436	def gen_fibonacci ( ) : num_a , num_b = 1 , 2 while True : yield num_a num_a , num_b = num_b , num_a + num_b
6940	def _gaussian ( x , amp , loc , std ) : return amp * np . exp ( - ( ( x - loc ) * ( x - loc ) ) / ( 2.0 * std * std ) )
9024	def write ( self , string ) : bytes_ = string . encode ( self . _encoding ) self . _file . write ( bytes_ )
9566	def pack_into ( fmt , buf , offset , * args , ** kwargs ) : return CompiledFormat ( fmt ) . pack_into ( buf , offset , * args , ** kwargs )
5953	def stop_logging ( ) : from . import log logger = logging . getLogger ( "gromacs" ) logger . info ( "GromacsWrapper %s STOPPED logging" , get_version ( ) ) log . clear_handlers ( logger )
13465	def __register_library ( self , module_name : str , attr : str , fallback : str = None ) : try : module = importlib . import_module ( module_name ) except ImportError : if fallback is not None : module = importlib . import_module ( fallback ) self . __logger . warn ( module_name + " not available: Replaced with " + fallback ) else : self . __logger . warn ( module_name + " not available: No Replacement Specified" ) if not attr in dir ( self . __sketch ) : setattr ( self . __sketch , attr , module ) else : self . __logger . warn ( attr + " could not be imported as it's label is already used in the sketch" )
10530	def get_project ( project_id ) : try : res = _pybossa_req ( 'get' , 'project' , project_id ) if res . get ( 'id' ) : return Project ( res ) else : return res except : raise
10958	def update_from_model_change ( self , oldmodel , newmodel , tile ) : self . _loglikelihood -= self . _calc_loglikelihood ( oldmodel , tile = tile ) self . _loglikelihood += self . _calc_loglikelihood ( newmodel , tile = tile ) self . _residuals [ tile . slicer ] = self . _data [ tile . slicer ] - newmodel
5115	def copy ( self ) : net = QueueNetwork ( None ) net . g = self . g . copy ( ) net . max_agents = copy . deepcopy ( self . max_agents ) net . nV = copy . deepcopy ( self . nV ) net . nE = copy . deepcopy ( self . nE ) net . num_agents = copy . deepcopy ( self . num_agents ) net . num_events = copy . deepcopy ( self . num_events ) net . _t = copy . deepcopy ( self . _t ) net . _initialized = copy . deepcopy ( self . _initialized ) net . _prev_edge = copy . deepcopy ( self . _prev_edge ) net . _blocking = copy . deepcopy ( self . _blocking ) net . colors = copy . deepcopy ( self . colors ) net . out_edges = copy . deepcopy ( self . out_edges ) net . in_edges = copy . deepcopy ( self . in_edges ) net . edge2queue = copy . deepcopy ( self . edge2queue ) net . _route_probs = copy . deepcopy ( self . _route_probs ) if net . _initialized : keys = [ q . _key ( ) for q in net . edge2queue if q . _time < np . infty ] net . _fancy_heap = PriorityQueue ( keys , net . nE ) return net
7065	def delete_ec2_nodes ( instance_id_list , client = None ) : if not client : client = boto3 . client ( 'ec2' ) resp = client . terminate_instances ( InstanceIds = instance_id_list ) return resp
972	def _setStatePointers ( self ) : if not self . allocateStatesInCPP : self . cells4 . setStatePointers ( self . infActiveState [ "t" ] , self . infActiveState [ "t-1" ] , self . infPredictedState [ "t" ] , self . infPredictedState [ "t-1" ] , self . colConfidence [ "t" ] , self . colConfidence [ "t-1" ] , self . cellConfidence [ "t" ] , self . cellConfidence [ "t-1" ] )
6780	def manifest_filename ( self ) : r = self . local_renderer tp_fn = r . format ( r . env . data_dir + '/manifest.yaml' ) return tp_fn
12388	def set ( self , target , value ) : if not self . _set : return if self . path is None : self . set = lambda * a : None return None if self . _segments [ target . __class__ ] : self . get ( target ) if self . _segments [ target . __class__ ] : return parent_getter = compose ( * self . _getters [ target . __class__ ] [ : - 1 ] ) target = parent_getter ( target ) func = self . _make_setter ( self . path . split ( '.' ) [ - 1 ] , target . __class__ ) func ( target , value ) def setter ( target , value ) : func ( parent_getter ( target ) , value ) self . set = setter
4450	def info ( self ) : res = self . redis . execute_command ( 'FT.INFO' , self . index_name ) it = six . moves . map ( to_string , res ) return dict ( six . moves . zip ( it , it ) )
136	def change_first_point_by_index ( self , point_idx ) : ia . do_assert ( 0 <= point_idx < len ( self . exterior ) ) if point_idx == 0 : return self . deepcopy ( ) exterior = np . concatenate ( ( self . exterior [ point_idx : , : ] , self . exterior [ : point_idx , : ] ) , axis = 0 ) return self . deepcopy ( exterior = exterior )
10377	def one_sided ( value : float , distribution : List [ float ] ) -> float : assert distribution return sum ( value < element for element in distribution ) / len ( distribution )
11445	def _clean_xml ( self , path_to_xml ) : try : if os . path . isfile ( path_to_xml ) : tree = ET . parse ( path_to_xml ) root = tree . getroot ( ) else : root = ET . fromstring ( path_to_xml ) except Exception , e : self . logger . error ( "Could not read OAI XML, aborting filter!" ) raise e strip_xml_namespace ( root ) return root
3077	def has_credentials ( self ) : if not self . credentials : return False elif ( self . credentials . access_token_expired and not self . credentials . refresh_token ) : return False else : return True
3505	def loopless_solution ( model , fluxes = None ) : if fluxes is None : sol = model . optimize ( objective_sense = None ) fluxes = sol . fluxes with model : prob = model . problem loopless_obj_constraint = prob . Constraint ( model . objective . expression , lb = - 1e32 , name = "loopless_obj_constraint" ) model . add_cons_vars ( [ loopless_obj_constraint ] ) _add_cycle_free ( model , fluxes ) solution = model . optimize ( objective_sense = None ) solution . objective_value = loopless_obj_constraint . primal return solution
494	def acquireConnection ( self ) : self . _logger . debug ( "Acquiring connection" ) dbConn = SteadyDB . connect ( ** _getCommonSteadyDBArgsDict ( ) ) connWrap = ConnectionWrapper ( dbConn = dbConn , cursor = dbConn . cursor ( ) , releaser = self . _releaseConnection , logger = self . _logger ) return connWrap
7861	def _make_tls_connection ( self ) : logger . debug ( "Preparing TLS connection" ) if self . settings [ "tls_verify_peer" ] : cert_reqs = ssl . CERT_REQUIRED else : cert_reqs = ssl . CERT_NONE self . stream . transport . starttls ( keyfile = self . settings [ "tls_key_file" ] , certfile = self . settings [ "tls_cert_file" ] , server_side = not self . stream . initiator , cert_reqs = cert_reqs , ssl_version = ssl . PROTOCOL_TLSv1 , ca_certs = self . settings [ "tls_cacert_file" ] , do_handshake_on_connect = False , )
11802	def conflicted_vars ( self , current ) : "Return a list of variables in current assignment that are in conflict" return [ var for var in self . vars if self . nconflicts ( var , current [ var ] , current ) > 0 ]
4059	def _citation_processor ( self , retrieved ) : items = [ ] for cit in retrieved . entries : items . append ( cit [ "content" ] [ 0 ] [ "value" ] ) self . url_params = None return items
9806	def deploy ( file , manager_path , check , dry_run ) : config = read_deployment_config ( file ) manager = DeployManager ( config = config , filepath = file , manager_path = manager_path , dry_run = dry_run ) exception = None if check : manager . check ( ) Printer . print_success ( 'Polyaxon deployment file is valid.' ) else : try : manager . install ( ) except Exception as e : Printer . print_error ( 'Polyaxon could not be installed.' ) exception = e if exception : Printer . print_error ( 'Error message `{}`.' . format ( exception ) )
9270	def get_temp_tag_for_repo_creation ( self ) : tag_date = self . tag_times_dict . get ( REPO_CREATED_TAG_NAME , None ) if not tag_date : tag_name , tag_date = self . fetcher . fetch_repo_creation_date ( ) self . tag_times_dict [ tag_name ] = timestring_to_datetime ( tag_date ) return REPO_CREATED_TAG_NAME
4899	def parse_arguments ( * args , ** options ) : days = options . get ( 'days' , 1 ) enterprise_customer_uuid = options . get ( 'enterprise_customer_uuid' ) enterprise_customer = None if enterprise_customer_uuid : try : enterprise_customer = EnterpriseCustomer . objects . get ( uuid = enterprise_customer_uuid ) except EnterpriseCustomer . DoesNotExist : raise CommandError ( 'Enterprise customer with uuid "{enterprise_customer_uuid}" does not exist.' . format ( enterprise_customer_uuid = enterprise_customer_uuid ) ) return days , enterprise_customer
1358	def get_argument_component ( self ) : try : component = self . get_argument ( constants . PARAM_COMPONENT ) return component except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
5523	def upload_stream ( self , destination , * , offset = 0 ) : return self . get_stream ( "STOR " + str ( destination ) , "1xx" , offset = offset , )
57	def project ( self , from_shape , to_shape ) : coords_proj = project_coords ( [ ( self . x1 , self . y1 ) , ( self . x2 , self . y2 ) ] , from_shape , to_shape ) return self . copy ( x1 = coords_proj [ 0 ] [ 0 ] , y1 = coords_proj [ 0 ] [ 1 ] , x2 = coords_proj [ 1 ] [ 0 ] , y2 = coords_proj [ 1 ] [ 1 ] , label = self . label )
7055	def register_lcformat ( formatkey , fileglob , timecols , magcols , errcols , readerfunc_module , readerfunc , readerfunc_kwargs = None , normfunc_module = None , normfunc = None , normfunc_kwargs = None , magsarefluxes = False , overwrite_existing = False , lcformat_dir = '~/.astrobase/lcformat-jsons' ) : LOGINFO ( 'adding %s to LC format registry...' % formatkey ) lcformat_dpath = os . path . abspath ( os . path . expanduser ( lcformat_dir ) ) if not os . path . exists ( lcformat_dpath ) : os . makedirs ( lcformat_dpath ) lcformat_jsonpath = os . path . join ( lcformat_dpath , '%s.json' % formatkey ) if os . path . exists ( lcformat_jsonpath ) and not overwrite_existing : LOGERROR ( 'There is an existing lcformat JSON: %s ' 'for this formatkey: %s and ' 'overwrite_existing = False, skipping...' % ( lcformat_jsonpath , formatkey ) ) return None readermodule = _check_extmodule ( readerfunc_module , formatkey ) if not readermodule : LOGERROR ( "could not import the required " "module: %s to read %s light curves" % ( readerfunc_module , formatkey ) ) return None try : getattr ( readermodule , readerfunc ) readerfunc_in = readerfunc except AttributeError : LOGEXCEPTION ( 'Could not get the specified reader ' 'function: %s for lcformat: %s ' 'from module: %s' % ( formatkey , readerfunc_module , readerfunc ) ) raise if normfunc_module : normmodule = _check_extmodule ( normfunc_module , formatkey ) if not normmodule : LOGERROR ( "could not import the required " "module: %s to normalize %s light curves" % ( normfunc_module , formatkey ) ) return None else : normmodule = None if normfunc_module and normfunc : try : getattr ( normmodule , normfunc ) normfunc_in = normfunc except AttributeError : LOGEXCEPTION ( 'Could not get the specified norm ' 'function: %s for lcformat: %s ' 'from module: %s' % ( normfunc , formatkey , normfunc_module ) ) raise else : normfunc_in = None formatdict = { 'fileglob' : fileglob , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'magsarefluxes' : magsarefluxes , 'lcreader_module' : readerfunc_module , 'lcreader_func' : readerfunc_in , 'lcreader_kwargs' : readerfunc_kwargs , 'lcnorm_module' : normfunc_module , 'lcnorm_func' : normfunc_in , 'lcnorm_kwargs' : normfunc_kwargs } with open ( lcformat_jsonpath , 'w' ) as outfd : json . dump ( formatdict , outfd , indent = 4 ) return lcformat_jsonpath
580	def getSpec ( cls ) : spec = { "description" : IdentityRegion . __doc__ , "singleNodeOnly" : True , "inputs" : { "in" : { "description" : "The input vector." , "dataType" : "Real32" , "count" : 0 , "required" : True , "regionLevel" : False , "isDefaultInput" : True , "requireSplitterMap" : False } , } , "outputs" : { "out" : { "description" : "A copy of the input vector." , "dataType" : "Real32" , "count" : 0 , "regionLevel" : True , "isDefaultOutput" : True } , } , "parameters" : { "dataWidth" : { "description" : "Size of inputs" , "accessMode" : "Read" , "dataType" : "UInt32" , "count" : 1 , "constraints" : "" } , } , } return spec
6009	def load_background_noise_map ( background_noise_map_path , background_noise_map_hdu , pixel_scale , convert_background_noise_map_from_weight_map , convert_background_noise_map_from_inverse_noise_map ) : background_noise_map_options = sum ( [ convert_background_noise_map_from_weight_map , convert_background_noise_map_from_inverse_noise_map ] ) if background_noise_map_options == 0 and background_noise_map_path is not None : return NoiseMap . from_fits_with_pixel_scale ( file_path = background_noise_map_path , hdu = background_noise_map_hdu , pixel_scale = pixel_scale ) elif convert_background_noise_map_from_weight_map and background_noise_map_path is not None : weight_map = Array . from_fits ( file_path = background_noise_map_path , hdu = background_noise_map_hdu ) return NoiseMap . from_weight_map ( weight_map = weight_map , pixel_scale = pixel_scale ) elif convert_background_noise_map_from_inverse_noise_map and background_noise_map_path is not None : inverse_noise_map = Array . from_fits ( file_path = background_noise_map_path , hdu = background_noise_map_hdu ) return NoiseMap . from_inverse_noise_map ( inverse_noise_map = inverse_noise_map , pixel_scale = pixel_scale ) else : return None
8099	def copy ( self , graph ) : s = styles ( graph ) s . guide = self . guide . copy ( graph ) dict . __init__ ( s , [ ( v . name , v . copy ( ) ) for v in self . values ( ) ] ) return s
10800	def _newcall ( self , rvecs ) : sigma = 1 * self . filter_size out = self . _eval_firstorder ( rvecs , self . d , sigma ) ondata = self . _eval_firstorder ( self . x , self . d , sigma ) for i in range ( self . iterations ) : out += self . _eval_firstorder ( rvecs , self . d - ondata , sigma ) ondata += self . _eval_firstorder ( self . x , self . d - ondata , sigma ) sigma *= self . damp return out
684	def getTotaln ( self ) : n = sum ( [ field . n for field in self . fields ] ) return n
11190	def show ( dataset_uri ) : try : dataset = dtoolcore . ProtoDataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) except dtoolcore . DtoolCoreTypeError : dataset = dtoolcore . DataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) readme_content = dataset . get_readme_content ( ) click . secho ( readme_content )
10198	def anonymize_user ( doc ) : ip = doc . pop ( 'ip_address' , None ) if ip : doc . update ( { 'country' : get_geoip ( ip ) } ) user_id = doc . pop ( 'user_id' , '' ) session_id = doc . pop ( 'session_id' , '' ) user_agent = doc . pop ( 'user_agent' , '' ) timestamp = arrow . get ( doc . get ( 'timestamp' ) ) timeslice = timestamp . strftime ( '%Y%m%d%H' ) salt = get_anonymization_salt ( timestamp ) visitor_id = hashlib . sha224 ( salt . encode ( 'utf-8' ) ) if user_id : visitor_id . update ( user_id . encode ( 'utf-8' ) ) elif session_id : visitor_id . update ( session_id . encode ( 'utf-8' ) ) elif ip and user_agent : vid = '{}|{}|{}' . format ( ip , user_agent , timeslice ) visitor_id . update ( vid . encode ( 'utf-8' ) ) else : pass unique_session_id = hashlib . sha224 ( salt . encode ( 'utf-8' ) ) if user_id : sid = '{}|{}' . format ( user_id , timeslice ) unique_session_id . update ( sid . encode ( 'utf-8' ) ) elif session_id : sid = '{}|{}' . format ( session_id , timeslice ) unique_session_id . update ( sid . encode ( 'utf-8' ) ) elif ip and user_agent : sid = '{}|{}|{}' . format ( ip , user_agent , timeslice ) unique_session_id . update ( sid . encode ( 'utf-8' ) ) doc . update ( dict ( visitor_id = visitor_id . hexdigest ( ) , unique_session_id = unique_session_id . hexdigest ( ) ) ) return doc
1246	def disconnect ( self ) : if not self . socket : logging . warning ( "No active socket to close!" ) return self . socket . close ( ) self . socket = None
113	def postprocess ( self , images , augmenter , parents ) : if self . postprocessor is None : return images else : return self . postprocessor ( images , augmenter , parents )
5856	def create_dataset_version ( self , dataset_id ) : failure_message = "Failed to create dataset version for dataset {}" . format ( dataset_id ) number = self . _get_success_json ( self . _post_json ( routes . create_dataset_version ( dataset_id ) , data = { } , failure_message = failure_message ) ) [ 'dataset_scoped_id' ] return DatasetVersion ( number = number )
1264	def sanity_check_states ( states_spec ) : states = copy . deepcopy ( states_spec ) is_unique = ( 'shape' in states ) if is_unique : states = dict ( state = states ) for name , state in states . items ( ) : if isinstance ( state [ 'shape' ] , int ) : state [ 'shape' ] = ( state [ 'shape' ] , ) if 'type' not in state : state [ 'type' ] = 'float' return states , is_unique
8723	def display ( content ) : if isinstance ( content , gp . GPServer ) : IPython . display . display ( GPAuthWidget ( content ) ) elif isinstance ( content , gp . GPTask ) : IPython . display . display ( GPTaskWidget ( content ) ) elif isinstance ( content , gp . GPJob ) : IPython . display . display ( GPJobWidget ( content ) ) else : IPython . display . display ( content )
12864	def days_in_month ( year , month ) : eom = _days_per_month [ month - 1 ] if is_leap_year ( year ) and month == 2 : eom += 1 return eom
7186	def maybe_replace_any_if_equal ( name , expected , actual ) : is_equal = expected == actual if not is_equal and Config . replace_any : actual_str = minimize_whitespace ( str ( actual ) ) if actual_str and actual_str [ 0 ] in { '"' , "'" } : actual_str = actual_str [ 1 : - 1 ] is_equal = actual_str in { 'Any' , 'typing.Any' , 't.Any' } if not is_equal : expected_annotation = minimize_whitespace ( str ( expected ) ) actual_annotation = minimize_whitespace ( str ( actual ) ) raise ValueError ( f"incompatible existing {name}. " + f"Expected: {expected_annotation!r}, actual: {actual_annotation!r}" ) return expected or actual
598	def finishLearning ( self ) : if self . _tfdr is None : raise RuntimeError ( "Temporal memory has not been initialized" ) if hasattr ( self . _tfdr , 'finishLearning' ) : self . resetSequenceStates ( ) self . _tfdr . finishLearning ( )
8752	def is_isonet_vif ( vif ) : nicira_iface_id = vif . record . get ( 'other_config' ) . get ( 'nicira-iface-id' ) if nicira_iface_id : return True return False
1347	def clone ( git_uri ) : hash_digest = sha256_hash ( git_uri ) local_path = home_directory_path ( FOLDER , hash_digest ) exists_locally = path_exists ( local_path ) if not exists_locally : _clone_repo ( git_uri , local_path ) else : logging . info ( "Git repository already exists locally." ) return local_path
4185	def window_flattop ( N , mode = 'symmetric' , precision = None ) : r assert mode in [ 'periodic' , 'symmetric' ] t = arange ( 0 , N ) if mode == 'periodic' : x = 2 * pi * t / float ( N ) else : if N == 1 : return ones ( 1 ) x = 2 * pi * t / float ( N - 1 ) a0 = 0.21557895 a1 = 0.41663158 a2 = 0.277263158 a3 = 0.083578947 a4 = 0.006947368 if precision == 'octave' : d = 4.6402 a0 = 1. / d a1 = 1.93 / d a2 = 1.29 / d a3 = 0.388 / d a4 = 0.0322 / d w = a0 - a1 * cos ( x ) + a2 * cos ( 2 * x ) - a3 * cos ( 3 * x ) + a4 * cos ( 4 * x ) return w
4213	def pass_from_pipe ( cls ) : is_pipe = not sys . stdin . isatty ( ) return is_pipe and cls . strip_last_newline ( sys . stdin . read ( ) )
9544	def add_value_check ( self , field_name , value_check , code = VALUE_CHECK_FAILED , message = MESSAGES [ VALUE_CHECK_FAILED ] , modulus = 1 ) : assert field_name in self . _field_names , 'unexpected field name: %s' % field_name assert callable ( value_check ) , 'value check must be a callable function' t = field_name , value_check , code , message , modulus self . _value_checks . append ( t )
13138	def http_get_provider ( provider , request_url , params , token_secret , token_cookie = None ) : if not validate_provider ( provider ) : raise InvalidUsage ( 'Provider not supported' ) klass = getattr ( socialauth . providers , provider . capitalize ( ) ) provider = klass ( request_url , params , token_secret , token_cookie ) if provider . status == 302 : ret = dict ( status = 302 , redirect = provider . redirect ) tc = getattr ( provider , 'set_token_cookie' , None ) if tc is not None : ret [ 'set_token_cookie' ] = tc return ret if provider . status == 200 and provider . user_id is not None : ret = dict ( status = 200 , provider_user_id = provider . user_id ) if provider . user_name is not None : ret [ 'provider_user_name' ] = provider . user_name return ret raise InvalidUsage ( 'Invalid request' )
13486	def minify ( self , css ) : css = css . replace ( "\r\n" , "\n" ) for rule in _REPLACERS [ self . level ] : css = re . compile ( rule [ 0 ] , re . MULTILINE | re . UNICODE | re . DOTALL ) . sub ( rule [ 1 ] , css ) return css
7985	def submit_registration_form ( self , form ) : self . lock . acquire ( ) try : if form and form . type != "cancel" : self . registration_form = form iq = Iq ( stanza_type = "set" ) iq . set_content ( self . __register . submit_form ( form ) ) self . set_response_handlers ( iq , self . registration_success , self . registration_error ) self . send ( iq ) else : self . __register = None finally : self . lock . release ( )
4476	def slice_clip ( filename , start , stop , n_samples , sr , mono = True ) : with psf . SoundFile ( str ( filename ) , mode = 'r' ) as soundf : n_target = stop - start soundf . seek ( start ) y = soundf . read ( n_target ) . T if mono : y = librosa . to_mono ( y ) y = librosa . resample ( y , soundf . samplerate , sr ) y = librosa . util . fix_length ( y , n_samples ) return y
4762	def contents_of ( f , encoding = 'utf-8' ) : try : contents = f . read ( ) except AttributeError : try : with open ( f , 'r' ) as fp : contents = fp . read ( ) except TypeError : raise ValueError ( 'val must be file or path, but was type <%s>' % type ( f ) . __name__ ) except OSError : if not isinstance ( f , str_types ) : raise ValueError ( 'val must be file or path, but was type <%s>' % type ( f ) . __name__ ) raise if sys . version_info [ 0 ] == 3 and type ( contents ) is bytes : return contents . decode ( encoding , 'replace' ) elif sys . version_info [ 0 ] == 2 and encoding == 'ascii' : return contents . encode ( 'ascii' , 'replace' ) else : try : return contents . decode ( encoding , 'replace' ) except AttributeError : pass return contents
8212	def draw_freehand ( self ) : if _ctx . _ns [ "mousedown" ] : x , y = mouse ( ) if self . show_grid : x , y = self . grid . snap ( x , y ) if self . freehand_move == True : cmd = MOVETO self . freehand_move = False else : cmd = LINETO pt = PathElement ( ) if cmd != MOVETO : pt . freehand = True else : pt . freehand = False pt . cmd = cmd pt . x = x pt . y = y pt . ctrl1 = Point ( x , y ) pt . ctrl2 = Point ( x , y ) self . _points . append ( pt ) r = 4 _ctx . nofill ( ) _ctx . stroke ( self . handle_color ) _ctx . oval ( pt . x - r , pt . y - r , r * 2 , r * 2 ) _ctx . fontsize ( 9 ) _ctx . fill ( self . handle_color ) _ctx . text ( " (" + str ( int ( pt . x ) ) + ", " + str ( int ( pt . y ) ) + ")" , pt . x + r , pt . y ) self . _dirty = True else : self . freehand_move = True if self . _dirty : self . _points [ - 1 ] . freehand = False self . export_svg ( ) self . _dirty = False
8802	def do_notify ( context , event_type , payload ) : LOG . debug ( 'IP_BILL: notifying {}' . format ( payload ) ) notifier = n_rpc . get_notifier ( 'network' ) notifier . info ( context , event_type , payload )
5513	def bytes_per_second ( ftp , retr = True ) : tot_bytes = 0 if retr : def request_file ( ) : ftp . voidcmd ( 'TYPE I' ) conn = ftp . transfercmd ( "retr " + TESTFN ) return conn with contextlib . closing ( request_file ( ) ) as conn : register_memory ( ) stop_at = time . time ( ) + 1.0 while stop_at > time . time ( ) : chunk = conn . recv ( BUFFER_LEN ) if not chunk : a = time . time ( ) ftp . voidresp ( ) conn . close ( ) conn = request_file ( ) stop_at += time . time ( ) - a tot_bytes += len ( chunk ) try : while chunk : chunk = conn . recv ( BUFFER_LEN ) ftp . voidresp ( ) conn . close ( ) except ( ftplib . error_temp , ftplib . error_perm ) : pass else : ftp . voidcmd ( 'TYPE I' ) with contextlib . closing ( ftp . transfercmd ( "STOR " + TESTFN ) ) as conn : register_memory ( ) chunk = b'x' * BUFFER_LEN stop_at = time . time ( ) + 1 while stop_at > time . time ( ) : tot_bytes += conn . send ( chunk ) ftp . voidresp ( ) return tot_bytes
2619	def create_session ( self ) : session = None if self . key_file is not None : credfile = os . path . expandvars ( os . path . expanduser ( self . key_file ) ) try : with open ( credfile , 'r' ) as f : creds = json . load ( f ) except json . JSONDecodeError as e : logger . error ( "EC2Provider '{}': json decode error in credential file {}" . format ( self . label , credfile ) ) raise e except Exception as e : logger . debug ( "EC2Provider '{0}' caught exception while reading credential file: {1}" . format ( self . label , credfile ) ) raise e logger . debug ( "EC2Provider '{}': Using credential file to create session" . format ( self . label ) ) session = boto3 . session . Session ( region_name = self . region , ** creds ) elif self . profile is not None : logger . debug ( "EC2Provider '{}': Using profile name to create session" . format ( self . label ) ) session = boto3 . session . Session ( profile_name = self . profile , region_name = self . region ) else : logger . debug ( "EC2Provider '{}': Using environment variables to create session" . format ( self . label ) ) session = boto3 . session . Session ( region_name = self . region ) return session
1524	def dereference_symlinks ( src ) : while os . path . islink ( src ) : src = os . path . join ( os . path . dirname ( src ) , os . readlink ( src ) ) return src
426	def check_unfinished_task ( self , task_name = None , ** kwargs ) : if not isinstance ( task_name , str ) : raise Exception ( "task_name should be string" ) self . _fill_project_info ( kwargs ) kwargs . update ( { '$or' : [ { 'status' : 'pending' } , { 'status' : 'running' } ] } ) task = self . db . Task . find ( kwargs ) task_id_list = task . distinct ( '_id' ) n_task = len ( task_id_list ) if n_task == 0 : logging . info ( "[Database] No unfinished task - task_name: {}" . format ( task_name ) ) return False else : logging . info ( "[Database] Find {} unfinished task - task_name: {}" . format ( n_task , task_name ) ) return True
4698	def fmt ( lbaf = 3 ) : if env ( ) : cij . err ( "cij.nvme.exists: Invalid NVMe ENV." ) return 1 nvme = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) cmd = [ "nvme" , "format" , nvme [ "DEV_PATH" ] , "-l" , str ( lbaf ) ] rcode , _ , _ = cij . ssh . command ( cmd , shell = True ) return rcode
2516	def p_file_notice ( self , f_term , predicate ) : try : for _ , _ , notice in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_notice ( self . doc , six . text_type ( notice ) ) except CardinalityError : self . more_than_one_error ( 'file notice' )
3516	def woopra ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return WoopraNode ( )
12305	def get_module_class ( class_path ) : mod_name , cls_name = class_path . rsplit ( '.' , 1 ) try : mod = import_module ( mod_name ) except ImportError as ex : raise EvoStreamException ( 'Error importing module %s: ' '"%s"' % ( mod_name , ex ) ) return getattr ( mod , cls_name )
2475	def set_lic_name ( self , doc , name ) : if self . has_extr_lic ( doc ) : if not self . extr_lic_name_set : self . extr_lic_name_set = True if validations . validate_extr_lic_name ( name ) : self . extr_lic ( doc ) . full_name = name return True else : raise SPDXValueError ( 'ExtractedLicense::Name' ) else : raise CardinalityError ( 'ExtractedLicense::Name' ) else : raise OrderError ( 'ExtractedLicense::Name' )
12788	def load ( self , reload = False , require_load = False ) : if reload : self . config = None if self . config : self . _log . debug ( 'Returning cached config instance. Use ' '``reload=True`` to avoid caching!' ) return path = self . _effective_path ( ) config_filename = self . _effective_filename ( ) self . _active_path = [ join ( _ , config_filename ) for _ in path ] for dirname in path : conf_name = join ( dirname , config_filename ) readable = self . check_file ( conf_name ) if readable : action = 'Updating' if self . _loaded_files else 'Loading initial' self . _log . info ( '%s config from %s' , action , conf_name ) self . read ( conf_name ) if conf_name == expanduser ( "~/.%s/%s/%s" % ( self . group_name , self . app_name , self . filename ) ) : self . _log . warning ( "DEPRECATION WARNING: The file " "'%s/.%s/%s/app.ini' was loaded. The XDG " "Basedir standard requires this file to be in " "'%s/.config/%s/%s/app.ini'! This location " "will no longer be parsed in a future version of " "config_resolver! You can already (and should) move " "the file!" , expanduser ( "~" ) , self . group_name , self . app_name , expanduser ( "~" ) , self . group_name , self . app_name ) self . _loaded_files . append ( conf_name ) if not self . _loaded_files and not require_load : self . _log . warning ( "No config file named %s found! Search path was %r" , config_filename , path ) elif not self . _loaded_files and require_load : raise IOError ( "No config file named %s found! Search path " "was %r" % ( config_filename , path ) )
5526	def grab ( bbox = None , childprocess = None , backend = None ) : if childprocess is None : childprocess = childprocess_default_value ( ) return _grab ( to_file = False , childprocess = childprocess , backend = backend , bbox = bbox )
10478	def _waitFor ( self , timeout , notification , ** kwargs ) : callback = self . _matchOther retelem = None callbackArgs = None callbackKwargs = None if 'callback' in kwargs : callback = kwargs [ 'callback' ] del kwargs [ 'callback' ] if 'args' in kwargs : if not isinstance ( kwargs [ 'args' ] , tuple ) : errStr = 'Notification callback args not given as a tuple' raise TypeError ( errStr ) callbackArgs = kwargs [ 'args' ] del kwargs [ 'args' ] if 'kwargs' in kwargs : if not isinstance ( kwargs [ 'kwargs' ] , dict ) : errStr = 'Notification callback kwargs not given as a dict' raise TypeError ( errStr ) callbackKwargs = kwargs [ 'kwargs' ] del kwargs [ 'kwargs' ] if kwargs : if callbackKwargs : callbackKwargs . update ( kwargs ) else : callbackKwargs = kwargs else : callbackArgs = ( retelem , ) callbackKwargs = kwargs return self . _setNotification ( timeout , notification , callback , callbackArgs , callbackKwargs )
8636	def get_milestone_by_id ( session , milestone_id , user_details = None ) : endpoint = 'milestones/{}' . format ( milestone_id ) response = make_get_request ( session , endpoint , params_data = user_details ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MilestonesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
8328	def _lastRecursiveChild ( self ) : "Finds the last element beneath this object to be parsed." lastChild = self while hasattr ( lastChild , 'contents' ) and lastChild . contents : lastChild = lastChild . contents [ - 1 ] return lastChild
11046	def init_logging ( log_level ) : log_level_filter = LogLevelFilterPredicate ( LogLevel . levelWithName ( log_level ) ) log_level_filter . setLogLevelForNamespace ( 'twisted.web.client._HTTP11ClientFactory' , LogLevel . warn ) log_observer = FilteringLogObserver ( textFileLogObserver ( sys . stdout ) , [ log_level_filter ] ) globalLogPublisher . addObserver ( log_observer )
11045	def init_storage_dir ( storage_dir ) : storage_path = FilePath ( storage_dir ) default_cert_path = storage_path . child ( 'default.pem' ) if not default_cert_path . exists ( ) : default_cert_path . setContent ( generate_wildcard_pem_bytes ( ) ) unmanaged_certs_path = storage_path . child ( 'unmanaged-certs' ) if not unmanaged_certs_path . exists ( ) : unmanaged_certs_path . createDirectory ( ) certs_path = storage_path . child ( 'certs' ) if not certs_path . exists ( ) : certs_path . createDirectory ( ) return storage_path , certs_path
9010	def index_of_first_produced_mesh_in_row ( self ) : index = 0 for instruction in self . row_instructions : if instruction is self : break index += instruction . number_of_produced_meshes else : self . _raise_not_found_error ( ) return index
28	def mpi_fork ( n , extra_mpi_args = [ ] ) : if n <= 1 : return "child" if os . getenv ( "IN_MPI" ) is None : env = os . environ . copy ( ) env . update ( MKL_NUM_THREADS = "1" , OMP_NUM_THREADS = "1" , IN_MPI = "1" ) args = [ "mpirun" , "-np" , str ( n ) ] + extra_mpi_args + [ sys . executable ] args += sys . argv subprocess . check_call ( args , env = env ) return "parent" else : install_mpi_excepthook ( ) return "child"
772	def shift ( self , modelResult ) : inferencesToWrite = { } if self . _inferenceBuffer is None : maxDelay = InferenceElement . getMaxDelay ( modelResult . inferences ) self . _inferenceBuffer = collections . deque ( maxlen = maxDelay + 1 ) self . _inferenceBuffer . appendleft ( copy . deepcopy ( modelResult . inferences ) ) for inferenceElement , inference in modelResult . inferences . iteritems ( ) : if isinstance ( inference , dict ) : inferencesToWrite [ inferenceElement ] = { } for key , _ in inference . iteritems ( ) : delay = InferenceElement . getTemporalDelay ( inferenceElement , key ) if len ( self . _inferenceBuffer ) > delay : prevInference = self . _inferenceBuffer [ delay ] [ inferenceElement ] [ key ] inferencesToWrite [ inferenceElement ] [ key ] = prevInference else : inferencesToWrite [ inferenceElement ] [ key ] = None else : delay = InferenceElement . getTemporalDelay ( inferenceElement ) if len ( self . _inferenceBuffer ) > delay : inferencesToWrite [ inferenceElement ] = ( self . _inferenceBuffer [ delay ] [ inferenceElement ] ) else : if type ( inference ) in ( list , tuple ) : inferencesToWrite [ inferenceElement ] = [ None ] * len ( inference ) else : inferencesToWrite [ inferenceElement ] = None shiftedResult = ModelResult ( rawInput = modelResult . rawInput , sensorInput = modelResult . sensorInput , inferences = inferencesToWrite , metrics = modelResult . metrics , predictedFieldIdx = modelResult . predictedFieldIdx , predictedFieldName = modelResult . predictedFieldName ) return shiftedResult
10300	def get_names_including_errors ( graph : BELGraph ) -> Mapping [ str , Set [ str ] ] : return { namespace : get_names_including_errors_by_namespace ( graph , namespace ) for namespace in get_namespaces ( graph ) }
7251	def batch_workflow_status ( self , batch_workflow_id ) : self . logger . debug ( 'Get status of batch workflow: ' + batch_workflow_id ) url = '%(base_url)s/batch_workflows/%(batch_id)s' % { 'base_url' : self . base_url , 'batch_id' : batch_workflow_id } r = self . gbdx_connection . get ( url ) return r . json ( )
12891	def handle_int ( self , item ) : doc = yield from self . handle_get ( item ) if doc is None : return None return int ( doc . value . u8 . text ) or None
7644	def can_convert ( annotation , target_namespace ) : if annotation . namespace == target_namespace : return True if target_namespace in __CONVERSION__ : for source in __CONVERSION__ [ target_namespace ] : if annotation . search ( namespace = source ) : return True return False
8910	def owsproxy_delegate ( request ) : twitcher_url = request . registry . settings . get ( 'twitcher.url' ) protected_path = request . registry . settings . get ( 'twitcher.ows_proxy_protected_path' , '/ows' ) url = twitcher_url + protected_path + '/proxy' if request . matchdict . get ( 'service_name' ) : url += '/' + request . matchdict . get ( 'service_name' ) if request . matchdict . get ( 'access_token' ) : url += '/' + request . matchdict . get ( 'service_name' ) url += '?' + urlparse . urlencode ( request . params ) LOGGER . debug ( "delegate to owsproxy: %s" , url ) resp = requests . request ( method = request . method . upper ( ) , url = url , data = request . body , headers = request . headers , verify = False ) return Response ( resp . content , status = resp . status_code , headers = resp . headers )
84	def CoarsePepper ( p = 0 , size_px = None , size_percent = None , per_channel = False , min_size = 4 , name = None , deterministic = False , random_state = None ) : mask = iap . handle_probability_param ( p , "p" , tuple_to_uniform = True , list_to_choice = True ) if size_px is not None : mask_low = iap . FromLowerResolution ( other_param = mask , size_px = size_px , min_size = min_size ) elif size_percent is not None : mask_low = iap . FromLowerResolution ( other_param = mask , size_percent = size_percent , min_size = min_size ) else : raise Exception ( "Either size_px or size_percent must be set." ) replacement01 = iap . ForceSign ( iap . Beta ( 0.5 , 0.5 ) - 0.5 , positive = False , mode = "invert" ) + 0.5 replacement = replacement01 * 255 if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return ReplaceElementwise ( mask = mask_low , replacement = replacement , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
210	def pad_to_aspect_ratio ( self , aspect_ratio , mode = "constant" , cval = 0.0 , return_pad_amounts = False ) : arr_0to1_padded , pad_amounts = ia . pad_to_aspect_ratio ( self . arr_0to1 , aspect_ratio = aspect_ratio , mode = mode , cval = cval , return_pad_amounts = True ) heatmaps = HeatmapsOnImage . from_0to1 ( arr_0to1_padded , shape = self . shape , min_value = self . min_value , max_value = self . max_value ) if return_pad_amounts : return heatmaps , pad_amounts else : return heatmaps
5930	def _canonicalize ( self , filename ) : path , ext = os . path . splitext ( filename ) if not ext : ext = ".collection" return path + ext
7064	def sqs_delete_item ( queue_url , receipt_handle , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 'sqs' ) try : client . delete_message ( QueueUrl = queue_url , ReceiptHandle = receipt_handle ) except Exception as e : LOGEXCEPTION ( 'could not delete message with receipt handle: ' '%s from queue: %s' % ( receipt_handle , queue_url ) ) if raiseonfail : raise
4865	def validate_username ( self , value ) : try : self . user = User . objects . get ( username = value ) except User . DoesNotExist : raise serializers . ValidationError ( "User does not exist" ) return value
4849	def _serialize_items ( self , channel_metadata_items ) : return json . dumps ( self . _prepare_items_for_transmission ( channel_metadata_items ) , sort_keys = True ) . encode ( 'utf-8' )
5996	def plot_border ( mask , should_plot_border , units , kpc_per_arcsec , pointsize , zoom_offset_pixels ) : if should_plot_border and mask is not None : plt . gca ( ) border_pixels = mask . masked_grid_index_to_pixel [ mask . border_pixels ] if zoom_offset_pixels is not None : border_pixels -= zoom_offset_pixels border_arcsec = mask . grid_pixels_to_grid_arcsec ( grid_pixels = border_pixels ) border_units = convert_grid_units ( array = mask , grid_arcsec = border_arcsec , units = units , kpc_per_arcsec = kpc_per_arcsec ) plt . scatter ( y = border_units [ : , 0 ] , x = border_units [ : , 1 ] , s = pointsize , c = 'y' )
12413	def serialize ( self , data , format = None ) : return self . _resource . serialize ( data , response = self , format = format )
5089	def dropHistoricalTable ( apps , schema_editor ) : table_name = 'sap_success_factors_historicalsapsuccessfactorsenterprisecus80ad' if table_name in connection . introspection . table_names ( ) : migrations . DeleteModel ( name = table_name , )
6514	def get_gender ( self , name , country = None ) : if not self . case_sensitive : name = name . lower ( ) if name not in self . names : return self . unknown_value elif not country : def counter ( country_values ) : country_values = map ( ord , country_values . replace ( " " , "" ) ) return ( len ( country_values ) , sum ( map ( lambda c : c > 64 and c - 55 or c - 48 , country_values ) ) ) return self . _most_popular_gender ( name , counter ) elif country in self . __class__ . COUNTRIES : index = self . __class__ . COUNTRIES . index ( country ) counter = lambda e : ( ord ( e [ index ] ) - 32 , 0 ) return self . _most_popular_gender ( name , counter ) else : raise NoCountryError ( "No such country: %s" % country )
6490	def _process_filters ( filter_dictionary ) : def filter_item ( field ) : if filter_dictionary [ field ] is not None : return { "or" : [ _get_filter_field ( field , filter_dictionary [ field ] ) , { "missing" : { "field" : field } } ] } return { "missing" : { "field" : field } } return [ filter_item ( field ) for field in filter_dictionary ]
409	def _tf_batch_map_coordinates ( self , inputs , coords ) : input_shape = inputs . get_shape ( ) coords_shape = coords . get_shape ( ) batch_channel = tf . shape ( inputs ) [ 0 ] input_h = int ( input_shape [ 1 ] ) input_w = int ( input_shape [ 2 ] ) kernel_n = int ( coords_shape [ 3 ] ) n_coords = input_h * input_w * kernel_n coords_lt = tf . cast ( tf . floor ( coords ) , 'int32' ) coords_rb = tf . cast ( tf . ceil ( coords ) , 'int32' ) coords_lb = tf . stack ( [ coords_lt [ : , : , : , : , 0 ] , coords_rb [ : , : , : , : , 1 ] ] , axis = - 1 ) coords_rt = tf . stack ( [ coords_rb [ : , : , : , : , 0 ] , coords_lt [ : , : , : , : , 1 ] ] , axis = - 1 ) idx = self . _tf_repeat ( tf . range ( batch_channel ) , n_coords ) vals_lt = self . _get_vals_by_coords ( inputs , coords_lt , idx , ( batch_channel , input_h , input_w , kernel_n ) ) vals_rb = self . _get_vals_by_coords ( inputs , coords_rb , idx , ( batch_channel , input_h , input_w , kernel_n ) ) vals_lb = self . _get_vals_by_coords ( inputs , coords_lb , idx , ( batch_channel , input_h , input_w , kernel_n ) ) vals_rt = self . _get_vals_by_coords ( inputs , coords_rt , idx , ( batch_channel , input_h , input_w , kernel_n ) ) coords_offset_lt = coords - tf . cast ( coords_lt , 'float32' ) vals_t = vals_lt + ( vals_rt - vals_lt ) * coords_offset_lt [ : , : , : , : , 0 ] vals_b = vals_lb + ( vals_rb - vals_lb ) * coords_offset_lt [ : , : , : , : , 0 ] mapped_vals = vals_t + ( vals_b - vals_t ) * coords_offset_lt [ : , : , : , : , 1 ] return mapped_vals
2543	def set_file_license_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_license_comment_set : self . file_license_comment_set = True self . file ( doc ) . license_comment = text return True else : raise CardinalityError ( 'File::LicenseComment' ) else : raise OrderError ( 'File::LicenseComment' )
7787	def _try_backup_item ( self ) : if not self . _backup_state : return False item = self . cache . get_item ( self . address , self . _backup_state ) if item : self . _object_handler ( item . address , item . value , item . state ) return True else : False
8665	def _prettify_list ( items ) : assert isinstance ( items , list ) keys_list = 'Available Keys:' for item in items : keys_list += '\n - {0}' . format ( item ) return keys_list
1721	def limited ( func ) : def f ( standard = False , ** args ) : insert_pos = len ( inline_stack . names ) res = func ( ** args ) if len ( res ) > LINE_LEN_LIMIT : name = inline_stack . require ( 'LONG' ) inline_stack . names . pop ( ) inline_stack . names . insert ( insert_pos , name ) res = 'def %s(var=var):\n return %s\n' % ( name , res ) inline_stack . define ( name , res ) return name + '()' else : return res f . __dict__ [ 'standard' ] = func return f
6709	def check ( self ) : self . _validate_settings ( ) r = self . local_renderer r . env . alias = r . env . aliases [ 0 ] r . sudo ( r . env . check_command_template )
13550	def _get_resource ( self , url , data_key = None ) : headers = { "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = WhenIWork_DAO ( ) . getURL ( url , headers ) if response . status != 200 : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
2323	def forward ( self , pred , target ) : loss = th . FloatTensor ( [ 0 ] ) for i in range ( 1 , self . moments ) : mk_pred = th . mean ( th . pow ( pred , i ) , 0 ) mk_tar = th . mean ( th . pow ( target , i ) , 0 ) loss . add_ ( th . mean ( ( mk_pred - mk_tar ) ** 2 ) ) return loss
13174	def next ( self , name = None ) : if self . parent is None or self . index is None : return None for idx in xrange ( self . index + 1 , len ( self . parent ) ) : if name is None or self . parent [ idx ] . tagname == name : return self . parent [ idx ]
13836	def _MergeMessageField ( self , tokenizer , message , field ) : is_map_entry = _IsMapEntry ( field ) if tokenizer . TryConsume ( '<' ) : end_token = '>' else : tokenizer . Consume ( '{' ) end_token = '}' if field . label == descriptor . FieldDescriptor . LABEL_REPEATED : if field . is_extension : sub_message = message . Extensions [ field ] . add ( ) elif is_map_entry : sub_message = field . message_type . _concrete_class ( ) else : sub_message = getattr ( message , field . name ) . add ( ) else : if field . is_extension : sub_message = message . Extensions [ field ] else : sub_message = getattr ( message , field . name ) sub_message . SetInParent ( ) while not tokenizer . TryConsume ( end_token ) : if tokenizer . AtEnd ( ) : raise tokenizer . ParseErrorPreviousToken ( 'Expected "%s".' % ( end_token , ) ) self . _MergeField ( tokenizer , sub_message ) if is_map_entry : value_cpptype = field . message_type . fields_by_name [ 'value' ] . cpp_type if value_cpptype == descriptor . FieldDescriptor . CPPTYPE_MESSAGE : value = getattr ( message , field . name ) [ sub_message . key ] value . MergeFrom ( sub_message . value ) else : getattr ( message , field . name ) [ sub_message . key ] = sub_message . value
4460	def geo ( lat , lon , radius , unit = 'km' ) : return GeoValue ( lat , lon , radius , unit )
13521	def send_zip ( self , exercise , file , params ) : resp = self . post ( exercise . return_url , params = params , files = { "submission[file]" : ( 'submission.zip' , file ) } , data = { "commit" : "Submit" } ) return self . _to_json ( resp )
11372	def get_temporary_file ( prefix = "tmp_" , suffix = "" , directory = None ) : try : file_fd , filepath = mkstemp ( prefix = prefix , suffix = suffix , dir = directory ) os . close ( file_fd ) except IOError , e : try : os . remove ( filepath ) except Exception : pass raise e return filepath
8767	def _validate_allocation_pools ( self ) : ip_pools = self . _alloc_pools subnet_cidr = self . _subnet_cidr LOG . debug ( _ ( "Performing IP validity checks on allocation pools" ) ) ip_sets = [ ] for ip_pool in ip_pools : try : start_ip = netaddr . IPAddress ( ip_pool [ 'start' ] ) end_ip = netaddr . IPAddress ( ip_pool [ 'end' ] ) except netaddr . AddrFormatError : LOG . info ( _ ( "Found invalid IP address in pool: " "%(start)s - %(end)s:" ) , { 'start' : ip_pool [ 'start' ] , 'end' : ip_pool [ 'end' ] } ) raise n_exc_ext . InvalidAllocationPool ( pool = ip_pool ) if ( start_ip . version != self . _subnet_cidr . version or end_ip . version != self . _subnet_cidr . version ) : LOG . info ( _ ( "Specified IP addresses do not match " "the subnet IP version" ) ) raise n_exc_ext . InvalidAllocationPool ( pool = ip_pool ) if end_ip < start_ip : LOG . info ( _ ( "Start IP (%(start)s) is greater than end IP " "(%(end)s)" ) , { 'start' : ip_pool [ 'start' ] , 'end' : ip_pool [ 'end' ] } ) raise n_exc_ext . InvalidAllocationPool ( pool = ip_pool ) if ( start_ip < self . _subnet_first_ip or end_ip > self . _subnet_last_ip ) : LOG . info ( _ ( "Found pool larger than subnet " "CIDR:%(start)s - %(end)s" ) , { 'start' : ip_pool [ 'start' ] , 'end' : ip_pool [ 'end' ] } ) raise n_exc_ext . OutOfBoundsAllocationPool ( pool = ip_pool , subnet_cidr = subnet_cidr ) ip_sets . append ( netaddr . IPSet ( netaddr . IPRange ( ip_pool [ 'start' ] , ip_pool [ 'end' ] ) . cidrs ( ) ) ) LOG . debug ( _ ( "Checking for overlaps among allocation pools " "and gateway ip" ) ) ip_ranges = ip_pools [ : ] for l_cursor in xrange ( len ( ip_sets ) ) : for r_cursor in xrange ( l_cursor + 1 , len ( ip_sets ) ) : if ip_sets [ l_cursor ] & ip_sets [ r_cursor ] : l_range = ip_ranges [ l_cursor ] r_range = ip_ranges [ r_cursor ] LOG . info ( _ ( "Found overlapping ranges: %(l_range)s and " "%(r_range)s" ) , { 'l_range' : l_range , 'r_range' : r_range } ) raise n_exc_ext . OverlappingAllocationPools ( pool_1 = l_range , pool_2 = r_range , subnet_cidr = subnet_cidr )
167	def is_fully_within_image ( self , image , default = False ) : if len ( self . coords ) == 0 : return default return np . all ( self . get_pointwise_inside_image_mask ( image ) )
1973	def sys_openat ( self , dirfd , buf , flags , mode ) : if issymbolic ( dirfd ) : logger . debug ( "Ask to read from a symbolic directory file descriptor!!" ) self . constraints . add ( dirfd >= 0 ) self . constraints . add ( dirfd <= len ( self . files ) ) raise ConcretizeArgument ( self , 0 ) if issymbolic ( buf ) : logger . debug ( "Ask to read to a symbolic buffer" ) raise ConcretizeArgument ( self , 1 ) return super ( ) . sys_openat ( dirfd , buf , flags , mode )
6807	def create_raspbian_vagrant_box ( self ) : r = self . local_renderer r . sudo ( 'adduser --disabled-password --gecos "" vagrant' ) r . sudo ( 'echo "vagrant ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/vagrant' ) r . sudo ( 'chmod 0440 /etc/sudoers.d/vagrant' ) r . sudo ( 'apt-get update' ) r . sudo ( 'apt-get install -y openssh-server' ) r . sudo ( 'mkdir -p /home/vagrant/.ssh' ) r . sudo ( 'chmod 0700 /home/vagrant/.ssh' ) r . sudo ( 'wget --no-check-certificate https://raw.github.com/mitchellh/vagrant/master/keys/vagrant.pub -O /home/vagrant/.ssh/authorized_keys' ) r . sudo ( 'chmod 0600 /home/vagrant/.ssh/authorized_keys' ) r . sudo ( 'chown -R vagrant /home/vagrant/.ssh' ) r . sudo ( "sed -i '/AuthorizedKeysFile/s/^#//g' /etc/ssh/sshd_config" ) r . sudo ( "sed -i '/PasswordAuthentication/s/^#//g' /etc/ssh/sshd_config" ) r . sudo ( "sed -i 's/PasswordAuthentication yes/PasswordAuthentication no/g' /etc/ssh/sshd_config" ) r . sudo ( 'apt-get upgrade' ) r . sudo ( 'apt-get install -y gcc build-essential' ) r . sudo ( 'mkdir /tmp/test' ) r . sudo ( 'cp {libvirt_images_dir}/{raspbian_image} /tmp/test' ) r . sudo ( 'cp {libvirt_boot_dir}/{raspbian_kernel} /tmp/test' ) r . render_to_file ( 'rpi/metadata.json' , '/tmp/test/metadata.json' ) r . render_to_file ( 'rpi/Vagrantfile' , '/tmp/test/Vagrantfile' ) r . sudo ( 'qemu-img convert -f raw -O qcow2 {libvirt_images_dir}/{raspbian_image} {libvirt_images_dir}/{raspbian_image}.qcow2' ) r . sudo ( 'mv {libvirt_images_dir}/{raspbian_image}.qcow2 {libvirt_images_dir}/box.img' ) r . sudo ( 'cd /tmp/test; tar cvzf custom_box.box ./metadata.json ./Vagrantfile ./{raspbian_kernel} ./box.img' )
6842	def force_stop ( self ) : r = self . local_renderer with self . settings ( warn_only = True ) : r . sudo ( 'pkill -9 -f celery' ) r . sudo ( 'rm -f /tmp/celery*.pid' )
13466	def set_moments ( self , sx , sxp , sxxp ) : self . _sx = sx self . _sxp = sxp self . _sxxp = sxxp emit = _np . sqrt ( sx ** 2 * sxp ** 2 - sxxp ** 2 ) self . _store_emit ( emit = emit )
5246	def update_missing ( ** kwargs ) : data_path = os . environ . get ( BBG_ROOT , '' ) . replace ( '\\' , '/' ) if not data_path : return if len ( kwargs ) == 0 : return log_path = f'{data_path}/Logs/{missing_info(**kwargs)}' cnt = len ( files . all_files ( log_path ) ) + 1 files . create_folder ( log_path ) open ( f'{log_path}/{cnt}.log' , 'a' ) . close ( )
845	def _calcDistance ( self , inputPattern , distanceNorm = None ) : if distanceNorm is None : distanceNorm = self . distanceNorm if self . useSparseMemory : if self . _protoSizes is None : self . _protoSizes = self . _Memory . rowSums ( ) overlapsWithProtos = self . _Memory . rightVecSumAtNZ ( inputPattern ) inputPatternSum = inputPattern . sum ( ) if self . distanceMethod == "rawOverlap" : dist = inputPattern . sum ( ) - overlapsWithProtos elif self . distanceMethod == "pctOverlapOfInput" : dist = inputPatternSum - overlapsWithProtos if inputPatternSum > 0 : dist /= inputPatternSum elif self . distanceMethod == "pctOverlapOfProto" : overlapsWithProtos /= self . _protoSizes dist = 1.0 - overlapsWithProtos elif self . distanceMethod == "pctOverlapOfLarger" : maxVal = numpy . maximum ( self . _protoSizes , inputPatternSum ) if maxVal . all ( ) > 0 : overlapsWithProtos /= maxVal dist = 1.0 - overlapsWithProtos elif self . distanceMethod == "norm" : dist = self . _Memory . vecLpDist ( self . distanceNorm , inputPattern ) distMax = dist . max ( ) if distMax > 0 : dist /= distMax else : raise RuntimeError ( "Unimplemented distance method %s" % self . distanceMethod ) else : if self . distanceMethod == "norm" : dist = numpy . power ( numpy . abs ( self . _M - inputPattern ) , self . distanceNorm ) dist = dist . sum ( 1 ) dist = numpy . power ( dist , 1.0 / self . distanceNorm ) dist /= dist . max ( ) else : raise RuntimeError ( "Not implemented yet for dense storage...." ) return dist
4001	def get_port_spec_document ( expanded_active_specs , docker_vm_ip ) : forwarding_port = 65000 port_spec = { 'docker_compose' : { } , 'nginx' : [ ] , 'hosts_file' : [ ] } host_full_addresses , host_names , stream_host_ports = set ( ) , set ( ) , set ( ) for app_name in sorted ( expanded_active_specs [ 'apps' ] . keys ( ) ) : app_spec = expanded_active_specs [ 'apps' ] [ app_name ] if 'host_forwarding' not in app_spec : continue port_spec [ 'docker_compose' ] [ app_name ] = [ ] for host_forwarding_spec in app_spec [ 'host_forwarding' ] : _add_full_addresses ( host_forwarding_spec , host_full_addresses ) if host_forwarding_spec [ 'type' ] == 'stream' : _add_stream_host_port ( host_forwarding_spec , stream_host_ports ) port_spec [ 'docker_compose' ] [ app_name ] . append ( _docker_compose_port_spec ( host_forwarding_spec , forwarding_port ) ) port_spec [ 'nginx' ] . append ( _nginx_port_spec ( host_forwarding_spec , forwarding_port , docker_vm_ip ) ) _add_host_names ( host_forwarding_spec , docker_vm_ip , port_spec , host_names ) forwarding_port += 1 return port_spec
10277	def get_neurommsig_scores_prestratified ( subgraphs : Mapping [ str , BELGraph ] , genes : List [ Gene ] , ora_weight : Optional [ float ] = None , hub_weight : Optional [ float ] = None , top_percent : Optional [ float ] = None , topology_weight : Optional [ float ] = None , ) -> Optional [ Mapping [ str , float ] ] : return { name : get_neurommsig_score ( graph = subgraph , genes = genes , ora_weight = ora_weight , hub_weight = hub_weight , top_percent = top_percent , topology_weight = topology_weight , ) for name , subgraph in subgraphs . items ( ) }
3639	def squad ( self , squad_id = 0 , persona_id = None ) : method = 'GET' url = 'squad/%s/user/%s' % ( squad_id , persona_id or self . persona_id ) events = [ self . pin . event ( 'page_view' , 'Hub - Squads' ) ] self . pin . send ( events ) rc = self . __request__ ( method , url ) events = [ self . pin . event ( 'page_view' , 'Squad Details' ) , self . pin . event ( 'page_view' , 'Squads - Squad Overview' ) ] self . pin . send ( events ) return [ itemParse ( i ) for i in rc . get ( 'players' , ( ) ) ]
850	def setParameter ( self , parameterName , index , parameterValue ) : if parameterName == 'topDownMode' : self . topDownMode = parameterValue elif parameterName == 'predictedField' : self . predictedField = parameterValue else : raise Exception ( 'Unknown parameter: ' + parameterName )
10373	def node_has_namespaces ( node : BaseEntity , namespaces : Set [ str ] ) -> bool : ns = node . get ( NAMESPACE ) return ns is not None and ns in namespaces
8938	def confluence ( ctx , no_publish = False , clean = False , opts = '' ) : cfg = config . load ( ) if clean : ctx . run ( "invoke clean --docs" ) cmd = [ 'sphinx-build' , '-b' , 'confluence' ] cmd . extend ( [ '-E' , '-a' ] ) if opts : cmd . append ( opts ) cmd . extend ( [ '.' , ctx . rituals . docs . build + '_cf' ] ) if no_publish : cmd . extend ( [ '-Dconfluence_publish=False' ] ) notify . info ( "Starting Sphinx build..." ) with pushd ( ctx . rituals . docs . sources ) : ctx . run ( ' ' . join ( cmd ) , pty = True )
3069	def wrap_http_for_jwt_access ( credentials , http ) : orig_request_method = http . request wrap_http_for_auth ( credentials , http ) authenticated_request_method = http . request def new_request ( uri , method = 'GET' , body = None , headers = None , redirections = httplib2 . DEFAULT_MAX_REDIRECTS , connection_type = None ) : if 'aud' in credentials . _kwargs : if ( credentials . access_token is None or credentials . access_token_expired ) : credentials . refresh ( None ) return request ( authenticated_request_method , uri , method , body , headers , redirections , connection_type ) else : headers = _initialize_headers ( headers ) _apply_user_agent ( headers , credentials . user_agent ) uri_root = uri . split ( '?' , 1 ) [ 0 ] token , unused_expiry = credentials . _create_token ( { 'aud' : uri_root } ) headers [ 'Authorization' ] = 'Bearer ' + token return request ( orig_request_method , uri , method , body , clean_headers ( headers ) , redirections , connection_type ) http . request = new_request http . request . credentials = credentials
6609	def failed_runids ( self , runids ) : for i in runids : try : self . clusterprocids_finished . remove ( i ) except ValueError : pass
5797	def _get_func_info ( docstring , def_lineno , code_lines , prefix ) : def_index = def_lineno - 1 definition = code_lines [ def_index ] definition = definition . rstrip ( ) while not definition . endswith ( ':' ) : def_index += 1 definition += '\n' + code_lines [ def_index ] . rstrip ( ) definition = textwrap . dedent ( definition ) . rstrip ( ':' ) definition = definition . replace ( '\n' , '\n' + prefix ) description = '' found_colon = False params = '' for line in docstring . splitlines ( ) : if line and line [ 0 ] == ':' : found_colon = True if not found_colon : if description : description += '\n' description += line else : if params : params += '\n' params += line description = description . strip ( ) description_md = '' if description : description_md = "%s%s" % ( prefix , description . replace ( '\n' , '\n' + prefix ) ) description_md = re . sub ( '\n>(\\s+)\n' , '\n>\n' , description_md ) params = params . strip ( ) if params : definition += ( ':\n%s ' % prefix ) definition = re . sub ( '\n>(\\s+)\n' , '\n>\n' , definition ) for search , replace in definition_replacements . items ( ) : definition = definition . replace ( search , replace ) return ( definition , description_md )
1969	def awake ( self , procid ) : logger . debug ( f"Remove procid:{procid} from waitlists and reestablish it in the running list" ) for wait_list in self . rwait : if procid in wait_list : wait_list . remove ( procid ) for wait_list in self . twait : if procid in wait_list : wait_list . remove ( procid ) self . timers [ procid ] = None self . running . append ( procid ) if self . _current is None : self . _current = procid
7524	def _collapse_outgroup ( tree , taxdicts ) : outg = taxdicts [ 0 ] [ "p4" ] if not all ( [ i [ "p4" ] == outg for i in taxdicts ] ) : raise Exception ( "no good" ) tre = ete . Tree ( tree . write ( format = 1 ) ) alltax = [ i for i in tre . get_leaf_names ( ) if i not in outg ] alltax += [ outg [ 0 ] ] tre . prune ( alltax ) tre . search_nodes ( name = outg [ 0 ] ) [ 0 ] . name = "outgroup" tre . ladderize ( ) taxd = copy . deepcopy ( taxdicts ) newtaxdicts = [ ] for test in taxd : test [ "p4" ] = [ "outgroup" ] newtaxdicts . append ( test ) return tre , newtaxdicts
8323	def isList ( l ) : return hasattr ( l , '__iter__' ) or ( type ( l ) in ( types . ListType , types . TupleType ) )
2106	def _echo_setting ( key ) : value = getattr ( settings , key ) secho ( '%s: ' % key , fg = 'magenta' , bold = True , nl = False ) secho ( six . text_type ( value ) , bold = True , fg = 'white' if isinstance ( value , six . text_type ) else 'cyan' , )
4642	def _haveKey ( self , key ) : query = ( "SELECT {} FROM {} WHERE {}=?" . format ( self . __value__ , self . __tablename__ , self . __key__ ) , ( key , ) , ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( * query ) return True if cursor . fetchone ( ) else False
13125	def id_to_object ( self , line ) : user = User . get ( line , ignore = 404 ) if not user : user = User ( username = line ) user . save ( ) return user
4076	def cfg_to_args ( config ) : kwargs = { } opts_to_args = { 'metadata' : [ ( 'name' , 'name' ) , ( 'author' , 'author' ) , ( 'author-email' , 'author_email' ) , ( 'maintainer' , 'maintainer' ) , ( 'maintainer-email' , 'maintainer_email' ) , ( 'home-page' , 'url' ) , ( 'summary' , 'description' ) , ( 'description' , 'long_description' ) , ( 'download-url' , 'download_url' ) , ( 'classifier' , 'classifiers' ) , ( 'platform' , 'platforms' ) , ( 'license' , 'license' ) , ( 'keywords' , 'keywords' ) , ] , 'files' : [ ( 'packages_root' , 'package_dir' ) , ( 'packages' , 'packages' ) , ( 'modules' , 'py_modules' ) , ( 'scripts' , 'scripts' ) , ( 'package_data' , 'package_data' ) , ( 'data_files' , 'data_files' ) , ] , } opts_to_args [ 'metadata' ] . append ( ( 'requires-dist' , 'install_requires' ) ) if IS_PY2K and not which ( '3to2' ) : kwargs [ 'setup_requires' ] = [ '3to2' ] kwargs [ 'zip_safe' ] = False for section in opts_to_args : for option , argname in opts_to_args [ section ] : value = get_cfg_value ( config , section , option ) if value : kwargs [ argname ] = value if 'long_description' not in kwargs : kwargs [ 'long_description' ] = read_description_file ( config ) if 'package_dir' in kwargs : kwargs [ 'package_dir' ] = { '' : kwargs [ 'package_dir' ] } if 'keywords' in kwargs : kwargs [ 'keywords' ] = split_elements ( kwargs [ 'keywords' ] ) if 'package_data' in kwargs : kwargs [ 'package_data' ] = get_package_data ( kwargs [ 'package_data' ] ) if 'data_files' in kwargs : kwargs [ 'data_files' ] = get_data_files ( kwargs [ 'data_files' ] ) kwargs [ 'version' ] = get_version ( ) if not IS_PY2K : kwargs [ 'test_suite' ] = 'test' return kwargs
9931	def get_repr ( self , obj , referent = None ) : objtype = type ( obj ) typename = str ( objtype . __module__ ) + "." + objtype . __name__ prettytype = typename . replace ( "__builtin__." , "" ) name = getattr ( obj , "__name__" , "" ) if name : prettytype = "%s %r" % ( prettytype , name ) key = "" if referent : key = self . get_refkey ( obj , referent ) url = reverse ( 'dowser_trace_object' , args = ( typename , id ( obj ) ) ) return ( '<a class="objectid" href="%s">%s</a> ' '<span class="typename">%s</span>%s<br />' '<span class="repr">%s</span>' % ( url , id ( obj ) , prettytype , key , get_repr ( obj , 100 ) ) )
8622	def get_user_by_id ( session , user_id , user_details = None ) : if user_details : user_details [ 'compact' ] = True response = make_get_request ( session , 'users/{}' . format ( user_id ) , params_data = user_details ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise UserNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
8046	def parse_definitions ( self , class_ , all = False ) : while self . current is not None : self . log . debug ( "parsing definition list, current token is %r (%s)" , self . current . kind , self . current . value , ) self . log . debug ( "got_newline: %s" , self . stream . got_logical_newline ) if all and self . current . value == "__all__" : self . parse_all ( ) elif ( self . current . kind == tk . OP and self . current . value == "@" and self . stream . got_logical_newline ) : self . consume ( tk . OP ) self . parse_decorators ( ) elif self . current . value in [ "def" , "class" ] : yield self . parse_definition ( class_ . _nest ( self . current . value ) ) elif self . current . kind == tk . INDENT : self . consume ( tk . INDENT ) for definition in self . parse_definitions ( class_ ) : yield definition elif self . current . kind == tk . DEDENT : self . consume ( tk . DEDENT ) return elif self . current . value == "from" : self . parse_from_import_statement ( ) else : self . stream . move ( )
8025	def multiglob_compile ( globs , prefix = False ) : if not globs : return re . compile ( '^$' ) elif prefix : globs = [ x + '*' for x in globs ] return re . compile ( '|' . join ( fnmatch . translate ( x ) for x in globs ) )
3406	def ast2str ( expr , level = 0 , names = None ) : if isinstance ( expr , Expression ) : return ast2str ( expr . body , 0 , names ) if hasattr ( expr , "body" ) else "" elif isinstance ( expr , Name ) : return names . get ( expr . id , expr . id ) if names else expr . id elif isinstance ( expr , BoolOp ) : op = expr . op if isinstance ( op , Or ) : str_exp = " or " . join ( ast2str ( i , level + 1 , names ) for i in expr . values ) elif isinstance ( op , And ) : str_exp = " and " . join ( ast2str ( i , level + 1 , names ) for i in expr . values ) else : raise TypeError ( "unsupported operation " + op . __class__ . __name ) return "(" + str_exp + ")" if level else str_exp elif expr is None : return "" else : raise TypeError ( "unsupported operation " + repr ( expr ) )
6159	def IIR_sos_header ( fname_out , SOS_mat ) : Ns , Mcol = SOS_mat . shape f = open ( fname_out , 'wt' ) f . write ( '//define a IIR SOS CMSIS-DSP coefficient array\n\n' ) f . write ( '#include <stdint.h>\n\n' ) f . write ( '#ifndef STAGES\n' ) f . write ( '#define STAGES %d\n' % Ns ) f . write ( '#endif\n' ) f . write ( '/*********************************************************/\n' ) f . write ( '/* IIR SOS Filter Coefficients */\n' ) f . write ( 'float32_t ba_coeff[%d] = { //b0,b1,b2,a1,a2,... by stage\n' % ( 5 * Ns ) ) for k in range ( Ns ) : if ( k < Ns - 1 ) : f . write ( ' %+-13e, %+-13e, %+-13e,\n' % ( SOS_mat [ k , 0 ] , SOS_mat [ k , 1 ] , SOS_mat [ k , 2 ] ) ) f . write ( ' %+-13e, %+-13e,\n' % ( - SOS_mat [ k , 4 ] , - SOS_mat [ k , 5 ] ) ) else : f . write ( ' %+-13e, %+-13e, %+-13e,\n' % ( SOS_mat [ k , 0 ] , SOS_mat [ k , 1 ] , SOS_mat [ k , 2 ] ) ) f . write ( ' %+-13e, %+-13e\n' % ( - SOS_mat [ k , 4 ] , - SOS_mat [ k , 5 ] ) ) f . write ( '};\n' ) f . write ( '/*********************************************************/\n' ) f . close ( )
3860	def _wrap_event ( event_ ) : cls = conversation_event . ConversationEvent if event_ . HasField ( 'chat_message' ) : cls = conversation_event . ChatMessageEvent elif event_ . HasField ( 'otr_modification' ) : cls = conversation_event . OTREvent elif event_ . HasField ( 'conversation_rename' ) : cls = conversation_event . RenameEvent elif event_ . HasField ( 'membership_change' ) : cls = conversation_event . MembershipChangeEvent elif event_ . HasField ( 'hangout_event' ) : cls = conversation_event . HangoutEvent elif event_ . HasField ( 'group_link_sharing_modification' ) : cls = conversation_event . GroupLinkSharingModificationEvent return cls ( event_ )
3306	def _run__cherrypy ( app , config , mode ) : assert mode == "cherrypy-wsgiserver" try : from cherrypy import wsgiserver from cherrypy . wsgiserver . ssl_builtin import BuiltinSSLAdapter _logger . warning ( "WARNING: cherrypy.wsgiserver is deprecated." ) _logger . warning ( " Starting with CherryPy 9.0 the functionality from cherrypy.wsgiserver" ) _logger . warning ( " was moved to the cheroot project." ) _logger . warning ( " Consider using --server=cheroot." ) except ImportError : _logger . error ( "*" * 78 ) _logger . error ( "ERROR: Could not import cherrypy.wsgiserver." ) _logger . error ( "Try `pip install cherrypy` or specify another server using the --server option." ) _logger . error ( "Note that starting with CherryPy 9.0, the server was moved to" ) _logger . error ( "the cheroot project, so it is recommended to use `-server=cheroot`" ) _logger . error ( "and run `pip install cheroot` instead." ) _logger . error ( "*" * 78 ) raise server_name = "WsgiDAV/{} {} Python/{}" . format ( __version__ , wsgiserver . CherryPyWSGIServer . version , util . PYTHON_VERSION ) wsgiserver . CherryPyWSGIServer . version = server_name ssl_certificate = _get_checked_path ( config . get ( "ssl_certificate" ) , config ) ssl_private_key = _get_checked_path ( config . get ( "ssl_private_key" ) , config ) ssl_certificate_chain = _get_checked_path ( config . get ( "ssl_certificate_chain" ) , config ) protocol = "http" if ssl_certificate : assert ssl_private_key wsgiserver . CherryPyWSGIServer . ssl_adapter = BuiltinSSLAdapter ( ssl_certificate , ssl_private_key , ssl_certificate_chain ) protocol = "https" _logger . info ( "SSL / HTTPS enabled." ) _logger . info ( "Running {}" . format ( server_name ) ) _logger . info ( "Serving on {}://{}:{} ..." . format ( protocol , config [ "host" ] , config [ "port" ] ) ) server_args = { "bind_addr" : ( config [ "host" ] , config [ "port" ] ) , "wsgi_app" : app , "server_name" : server_name , } server_args . update ( config . get ( "server_args" , { } ) ) server = wsgiserver . CherryPyWSGIServer ( ** server_args ) startup_event = config . get ( "startup_event" ) if startup_event : def _patched_tick ( ) : server . tick = org_tick org_tick ( ) _logger . info ( "CherryPyWSGIServer is ready" ) startup_event . set ( ) org_tick = server . tick server . tick = _patched_tick try : server . start ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) finally : server . stop ( ) return
3152	def delete ( self , list_id , webhook_id ) : self . list_id = list_id self . webhook_id = webhook_id return self . _mc_client . _delete ( url = self . _build_path ( list_id , 'webhooks' , webhook_id ) )
13488	def create ( self , server ) : for chunk in self . __cut_to_size ( ) : server . post ( 'tasks_admin' , chunk . as_payload ( ) , replacements = { 'slug' : chunk . challenge . slug } )
8293	def unique ( list ) : unique = [ ] [ unique . append ( x ) for x in list if x not in unique ] return unique
8764	def get_public_net_id ( self ) : for id , net_params in self . strategy . iteritems ( ) : if id == CONF . QUARK . public_net_id : return id return None
7771	def payload_class_for_element_name ( element_name ) : logger . debug ( " looking up payload class for element: {0!r}" . format ( element_name ) ) logger . debug ( " known: {0!r}" . format ( STANZA_PAYLOAD_CLASSES ) ) if element_name in STANZA_PAYLOAD_CLASSES : return STANZA_PAYLOAD_CLASSES [ element_name ] else : return XMLPayload
1205	def target_optimizer_arguments ( self ) : variables = self . target_network . get_variables ( ) + [ variable for name in sorted ( self . target_distributions ) for variable in self . target_distributions [ name ] . get_variables ( ) ] source_variables = self . network . get_variables ( ) + [ variable for name in sorted ( self . distributions ) for variable in self . distributions [ name ] . get_variables ( ) ] arguments = dict ( time = self . global_timestep , variables = variables , source_variables = source_variables ) if self . global_model is not None : arguments [ 'global_variables' ] = self . global_model . target_network . get_variables ( ) + [ variable for name in sorted ( self . global_model . target_distributions ) for variable in self . global_model . target_distributions [ name ] . get_variables ( ) ] return arguments
3317	def digest_auth_user ( self , realm , user_name , environ ) : user = self . _get_realm_entry ( realm , user_name ) if user is None : return False password = user . get ( "password" ) environ [ "wsgidav.auth.roles" ] = user . get ( "roles" , [ ] ) return self . _compute_http_digest_a1 ( realm , user_name , password )
909	def __advancePhase ( self ) : self . __currentPhase = self . __phaseCycler . next ( ) self . __currentPhase . enterPhase ( ) return
9141	def filter_labels_by_language ( labels , language , broader = False ) : if language == 'any' : return labels if broader : language = tags . tag ( language ) . language . format return [ l for l in labels if tags . tag ( l . language ) . language . format == language ] else : language = tags . tag ( language ) . format return [ l for l in labels if tags . tag ( l . language ) . format == language ]
2729	def get_kernel_available ( self ) : kernels = list ( ) data = self . get_data ( "droplets/%s/kernels/" % self . id ) while True : for jsond in data [ u'kernels' ] : kernel = Kernel ( ** jsond ) kernel . token = self . token kernels . append ( kernel ) try : url = data [ u'links' ] [ u'pages' ] . get ( u'next' ) if not url : break data = self . get_data ( url ) except KeyError : break return kernels
12200	def from_jsonfile ( cls , fp , selector_handler = None , strict = False , debug = False ) : return cls . _from_jsonlines ( fp , selector_handler = selector_handler , strict = strict , debug = debug )
1349	def write_error_response ( self , message ) : self . set_status ( 404 ) response = self . make_error_response ( str ( message ) ) now = time . time ( ) spent = now - self . basehandler_starttime response [ constants . RESPONSE_KEY_EXECUTION_TIME ] = spent self . write_json_response ( response )
8950	def error ( msg ) : _flush ( ) sys . stderr . write ( "\033[1;37;41mERROR: {}\033[0m\n" . format ( msg ) ) sys . stderr . flush ( )
8457	def _needs_new_cc_config_for_update ( old_template , old_version , new_template , new_version ) : if old_template != new_template : return True else : return _cookiecutter_configs_have_changed ( new_template , old_version , new_version )
4854	def _create_transmissions ( self , content_metadata_item_map ) : ContentMetadataItemTransmission = apps . get_model ( 'integrated_channel' , 'ContentMetadataItemTransmission' ) transmissions = [ ] for content_id , channel_metadata in content_metadata_item_map . items ( ) : transmissions . append ( ContentMetadataItemTransmission ( enterprise_customer = self . enterprise_configuration . enterprise_customer , integrated_channel_code = self . enterprise_configuration . channel_code ( ) , content_id = content_id , channel_metadata = channel_metadata ) ) ContentMetadataItemTransmission . objects . bulk_create ( transmissions )
12881	def next ( self ) : self . index += 1 t = self . peek ( ) if not self . depth : self . _cut ( ) return t
11928	def get_files_stat ( self ) : if not exists ( Post . src_dir ) : logger . error ( SourceDirectoryNotFound . __doc__ ) sys . exit ( SourceDirectoryNotFound . exit_code ) paths = [ ] for fn in ls ( Post . src_dir ) : if fn . endswith ( src_ext ) : paths . append ( join ( Post . src_dir , fn ) ) if exists ( config . filepath ) : paths . append ( config . filepath ) files = dict ( ( p , stat ( p ) . st_mtime ) for p in paths ) return files
4153	def rst2md ( text ) : top_heading = re . compile ( r'^=+$\s^([\w\s-]+)^=+$' , flags = re . M ) text = re . sub ( top_heading , r'# \1' , text ) math_eq = re . compile ( r'^\.\. math::((?:.+)?(?:\n+^ .+)*)' , flags = re . M ) text = re . sub ( math_eq , lambda match : r'$${0}$$' . format ( match . group ( 1 ) . strip ( ) ) , text ) inline_math = re . compile ( r':math:`(.+)`' ) text = re . sub ( inline_math , r'$\1$' , text ) return text
6674	def get_owner ( self , path , use_sudo = False ) : func = use_sudo and run_as_root or self . run with self . settings ( hide ( 'running' , 'stdout' ) , warn_only = True ) : result = func ( 'stat -c %%U "%(path)s"' % locals ( ) ) if result . failed and 'stat: illegal option' in result : return func ( 'stat -f %%Su "%(path)s"' % locals ( ) ) return result
7031	def specwindow_lsp ( times , mags , errs , magsarefluxes = False , startp = None , endp = None , stepsize = 1.0e-4 , autofreq = True , nbestpeaks = 5 , periodepsilon = 0.1 , sigclip = 10.0 , nworkers = None , glspfunc = _glsp_worker_specwindow , verbose = True ) : lspres = pgen_lsp ( times , mags , errs , magsarefluxes = magsarefluxes , startp = startp , endp = endp , autofreq = autofreq , nbestpeaks = nbestpeaks , periodepsilon = periodepsilon , stepsize = stepsize , nworkers = nworkers , sigclip = sigclip , glspfunc = glspfunc , verbose = verbose ) lspres [ 'method' ] = 'win' if lspres [ 'lspvals' ] is not None : lspmax = npnanmax ( lspres [ 'lspvals' ] ) if npisfinite ( lspmax ) : lspres [ 'lspvals' ] = lspres [ 'lspvals' ] / lspmax lspres [ 'nbestlspvals' ] = [ x / lspmax for x in lspres [ 'nbestlspvals' ] ] lspres [ 'bestlspval' ] = lspres [ 'bestlspval' ] / lspmax return lspres
10349	def lint_file ( in_file , out_file = None ) : for line in in_file : print ( line . strip ( ) , file = out_file )
12927	def _parse_allele_data ( self ) : return [ Allele ( sequence = x ) for x in [ self . ref_allele ] + self . alt_alleles ]
3479	def _clip ( sid , prefix ) : return sid [ len ( prefix ) : ] if sid . startswith ( prefix ) else sid
5261	def parse_10qk ( self , response ) : loader = ReportItemLoader ( response = response ) item = loader . load_item ( ) if 'doc_type' in item : doc_type = item [ 'doc_type' ] if doc_type in ( '10-Q' , '10-K' ) : return item return None
249	def get_txn_vol ( transactions ) : txn_norm = transactions . copy ( ) txn_norm . index = txn_norm . index . normalize ( ) amounts = txn_norm . amount . abs ( ) prices = txn_norm . price values = amounts * prices daily_amounts = amounts . groupby ( amounts . index ) . sum ( ) daily_values = values . groupby ( values . index ) . sum ( ) daily_amounts . name = "txn_shares" daily_values . name = "txn_volume" return pd . concat ( [ daily_values , daily_amounts ] , axis = 1 )
11428	def record_strip_empty_fields ( rec , tag = None ) : if tag is None : tags = rec . keys ( ) for tag in tags : record_strip_empty_fields ( rec , tag ) elif tag in rec : if tag [ : 2 ] == '00' : if len ( rec [ tag ] ) == 0 or not rec [ tag ] [ 0 ] [ 3 ] : del rec [ tag ] else : fields = [ ] for field in rec [ tag ] : subfields = [ ] for subfield in field [ 0 ] : if subfield [ 1 ] : subfield = ( subfield [ 0 ] , subfield [ 1 ] . strip ( ) ) subfields . append ( subfield ) if len ( subfields ) > 0 : new_field = create_field ( subfields , field [ 1 ] , field [ 2 ] , field [ 3 ] ) fields . append ( new_field ) if len ( fields ) > 0 : rec [ tag ] = fields else : del rec [ tag ]
6880	def _parse_csv_header ( header ) : headerlines = header . split ( '\n' ) headerlines = [ x . lstrip ( '# ' ) for x in headerlines ] objectstart = headerlines . index ( 'OBJECT' ) metadatastart = headerlines . index ( 'METADATA' ) camfilterstart = headerlines . index ( 'CAMFILTERS' ) photaperturestart = headerlines . index ( 'PHOTAPERTURES' ) columnstart = headerlines . index ( 'COLUMNS' ) lcstart = headerlines . index ( 'LIGHTCURVE' ) objectinfo = headerlines [ objectstart + 1 : metadatastart - 1 ] metadatainfo = headerlines [ metadatastart + 1 : camfilterstart - 1 ] camfilterinfo = headerlines [ camfilterstart + 1 : photaperturestart - 1 ] photapertureinfo = headerlines [ photaperturestart + 1 : columnstart - 1 ] columninfo = headerlines [ columnstart + 1 : lcstart - 1 ] metadict = { 'objectinfo' : { } } objectinfo = [ x . split ( ';' ) for x in objectinfo ] for elem in objectinfo : for kvelem in elem : key , val = kvelem . split ( ' = ' , 1 ) metadict [ 'objectinfo' ] [ key . strip ( ) ] = ( _smartcast ( val , METAKEYS [ key . strip ( ) ] ) ) metadict [ 'objectid' ] = metadict [ 'objectinfo' ] [ 'objectid' ] [ : ] del metadict [ 'objectinfo' ] [ 'objectid' ] metadatainfo = [ x . split ( ';' ) for x in metadatainfo ] for elem in metadatainfo : for kvelem in elem : try : key , val = kvelem . split ( ' = ' , 1 ) if key . strip ( ) == 'lcbestaperture' : val = json . loads ( val ) if key . strip ( ) in ( 'datarelease' , 'lcversion' ) : val = int ( val ) if key . strip ( ) == 'lastupdated' : val = float ( val ) metadict [ key . strip ( ) ] = val except Exception as e : LOGWARNING ( 'could not understand header element "%s",' ' skipped.' % kvelem ) metadict [ 'filters' ] = [ ] for row in camfilterinfo : filterid , filtername , filterdesc = row . split ( ' - ' ) metadict [ 'filters' ] . append ( ( int ( filterid ) , filtername , filterdesc ) ) metadict [ 'lcapertures' ] = { } for row in photapertureinfo : apnum , appix = row . split ( ' - ' ) appix = float ( appix . rstrip ( ' px' ) ) metadict [ 'lcapertures' ] [ apnum . strip ( ) ] = appix metadict [ 'columns' ] = [ ] for row in columninfo : colnum , colname , coldesc = row . split ( ' - ' ) metadict [ 'columns' ] . append ( colname ) return metadict
12102	def _launch_process_group ( self , process_commands , streams_path ) : processes = { } def check_complete_processes ( wait = False ) : result = False for proc in list ( processes ) : if wait : proc . wait ( ) if proc . poll ( ) is not None : self . debug ( "Process %d exited with code %d." % ( processes [ proc ] [ 'tid' ] , proc . poll ( ) ) ) processes [ proc ] [ 'stdout' ] . close ( ) processes [ proc ] [ 'stderr' ] . close ( ) del processes [ proc ] result = True return result for cmd , tid in process_commands : self . debug ( "Starting process %d..." % tid ) job_timestamp = time . strftime ( '%H%M%S' ) basename = "%s_%s_tid_%d" % ( self . batch_name , job_timestamp , tid ) stdout_handle = open ( os . path . join ( streams_path , "%s.o.%d" % ( basename , tid ) ) , "wb" ) stderr_handle = open ( os . path . join ( streams_path , "%s.e.%d" % ( basename , tid ) ) , "wb" ) proc = subprocess . Popen ( cmd , stdout = stdout_handle , stderr = stderr_handle ) processes [ proc ] = { 'tid' : tid , 'stdout' : stdout_handle , 'stderr' : stderr_handle } if self . max_concurrency : while len ( processes ) >= self . max_concurrency : if not check_complete_processes ( len ( processes ) == 1 ) : time . sleep ( 0.1 ) while len ( processes ) > 0 : if not check_complete_processes ( True ) : time . sleep ( 0.1 )
8180	def add_node ( self , id , radius = 8 , style = style . DEFAULT , category = "" , label = None , root = False , properties = { } ) : if self . has_key ( id ) : return self [ id ] if not isinstance ( style , str ) and style . __dict__ . has_key [ "name" ] : style = style . name n = node ( self , id , radius , style , category , label , properties ) self [ n . id ] = n self . nodes . append ( n ) if root : self . root = n return n
10071	def pid ( self ) : pid = self . deposit_fetcher ( self . id , self ) return PersistentIdentifier . get ( pid . pid_type , pid . pid_value )
8242	def outline ( path , colors , precision = 0.4 , continuous = True ) : def _point_count ( path , precision ) : return max ( int ( path . length * precision * 0.5 ) , 10 ) n = sum ( [ _point_count ( contour , precision ) for contour in path . contours ] ) contour_i = 0 contour_n = len ( path . contours ) - 1 if contour_n == 0 : continuous = False i = 0 for contour in path . contours : if not continuous : i = 0 j = _point_count ( contour , precision ) first = True for pt in contour . points ( j ) : if first : first = False else : if not continuous : clr = float ( i ) / j * len ( colors ) else : clr = float ( i ) / n * len ( colors ) - 1 * contour_i / contour_n _ctx . stroke ( colors [ int ( clr ) ] ) _ctx . line ( x0 , y0 , pt . x , pt . y ) x0 = pt . x y0 = pt . y i += 1 pt = contour . point ( 0.9999999 ) _ctx . line ( x0 , y0 , pt . x , pt . y ) contour_i += 1
5306	def rgb_to_ansi256 ( r , g , b ) : if r == g and g == b : if r < 8 : return 16 if r > 248 : return 231 return round ( ( ( r - 8 ) / 247.0 ) * 24 ) + 232 ansi_r = 36 * round ( r / 255.0 * 5.0 ) ansi_g = 6 * round ( g / 255.0 * 5.0 ) ansi_b = round ( b / 255.0 * 5.0 ) ansi = 16 + ansi_r + ansi_g + ansi_b return ansi
3603	def _build_endpoint_url ( self , url , name = None ) : if not url . endswith ( self . URL_SEPERATOR ) : url = url + self . URL_SEPERATOR if name is None : name = '' return '%s%s%s' % ( urlparse . urljoin ( self . dsn , url ) , name , self . NAME_EXTENSION )
6735	def write_temp_file_or_dryrun ( content , * args , ** kwargs ) : dryrun = get_dryrun ( kwargs . get ( 'dryrun' ) ) if dryrun : fd , tmp_fn = tempfile . mkstemp ( ) os . remove ( tmp_fn ) cmd_run = 'local' cmd = 'cat <<EOT >> %s\n%s\nEOT' % ( tmp_fn , content ) if BURLAP_COMMAND_PREFIX : print ( '%s %s: %s' % ( render_command_prefix ( ) , cmd_run , cmd ) ) else : print ( cmd ) else : fd , tmp_fn = tempfile . mkstemp ( ) fout = open ( tmp_fn , 'w' ) fout . write ( content ) fout . close ( ) return tmp_fn
2116	def convert ( self , value , param , ctx ) : if not isinstance ( value , str ) : return value if isinstance ( value , six . binary_type ) : value = value . decode ( 'UTF-8' ) if value . startswith ( '@' ) : filename = os . path . expanduser ( value [ 1 : ] ) file_obj = super ( Variables , self ) . convert ( filename , param , ctx ) if hasattr ( file_obj , 'read' ) : return file_obj . read ( ) return file_obj return value
4525	def get ( self , position = 0 ) : n = len ( self ) if n == 1 : return self [ 0 ] pos = position if self . length and self . autoscale : pos *= len ( self ) pos /= self . length pos *= self . scale pos += self . offset if not self . continuous : if not self . serpentine : return self [ int ( pos % n ) ] m = ( 2 * n ) - 2 pos %= m if pos < n : return self [ int ( pos ) ] else : return self [ int ( m - pos ) ] if self . serpentine : pos %= ( 2 * n ) if pos > n : pos = ( 2 * n ) - pos else : pos %= n pos *= n - 1 pos /= n index = int ( pos ) fade = pos - index if not fade : return self [ index ] r1 , g1 , b1 = self [ index ] r2 , g2 , b2 = self [ ( index + 1 ) % len ( self ) ] dr , dg , db = r2 - r1 , g2 - g1 , b2 - b1 return r1 + fade * dr , g1 + fade * dg , b1 + fade * db
5148	def write ( self , name , path = './' ) : byte_object = self . generate ( ) file_name = '{0}.tar.gz' . format ( name ) if not path . endswith ( '/' ) : path += '/' f = open ( '{0}{1}' . format ( path , file_name ) , 'wb' ) f . write ( byte_object . getvalue ( ) ) f . close ( )
7275	def seek ( self , relative_position ) : self . _player_interface . Seek ( Int64 ( 1000.0 * 1000 * relative_position ) ) self . seekEvent ( self , relative_position )
652	def sameSegment ( seg1 , seg2 ) : result = True for field in [ 1 , 2 , 3 , 4 , 5 , 6 ] : if abs ( seg1 [ 0 ] [ field ] - seg2 [ 0 ] [ field ] ) > 0.001 : result = False if len ( seg1 [ 1 : ] ) != len ( seg2 [ 1 : ] ) : result = False for syn in seg2 [ 1 : ] : if syn [ 2 ] <= 0 : print "A synapse with zero permanence encountered" result = False if result == True : for syn in seg1 [ 1 : ] : if syn [ 2 ] <= 0 : print "A synapse with zero permanence encountered" result = False res = sameSynapse ( syn , seg2 [ 1 : ] ) if res == False : result = False return result
2774	def save ( self ) : forwarding_rules = [ rule . __dict__ for rule in self . forwarding_rules ] data = { 'name' : self . name , 'region' : self . region [ 'slug' ] , 'forwarding_rules' : forwarding_rules , 'redirect_http_to_https' : self . redirect_http_to_https } if self . tag : data [ 'tag' ] = self . tag else : data [ 'droplet_ids' ] = self . droplet_ids if self . algorithm : data [ "algorithm" ] = self . algorithm if self . health_check : data [ 'health_check' ] = self . health_check . __dict__ if self . sticky_sessions : data [ 'sticky_sessions' ] = self . sticky_sessions . __dict__ return self . get_data ( "load_balancers/%s/" % self . id , type = PUT , params = data )
9307	def get_canonical_request ( self , req , cano_headers , signed_headers ) : url = urlparse ( req . url ) path = self . amz_cano_path ( url . path ) split = req . url . split ( '?' , 1 ) qs = split [ 1 ] if len ( split ) == 2 else '' qs = self . amz_cano_querystring ( qs ) payload_hash = req . headers [ 'x-amz-content-sha256' ] req_parts = [ req . method . upper ( ) , path , qs , cano_headers , signed_headers , payload_hash ] cano_req = '\n' . join ( req_parts ) return cano_req
3874	def _add_conversation ( self , conversation , events = [ ] , event_cont_token = None ) : conv_id = conversation . conversation_id . id logger . debug ( 'Adding new conversation: {}' . format ( conv_id ) ) conv = Conversation ( self . _client , self . _user_list , conversation , events , event_cont_token ) self . _conv_dict [ conv_id ] = conv return conv
6519	def is_excluded_dir ( self , path ) : if self . is_excluded ( path ) : return True return matches_masks ( path . name , ALWAYS_EXCLUDED_DIRS )
7618	def beat ( ref , est , ** kwargs ) : r namespace = 'beat' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_times , _ = ref . to_event_values ( ) est_times , _ = est . to_event_values ( ) return mir_eval . beat . evaluate ( ref_times , est_times , ** kwargs )
3375	def add_absolute_expression ( model , expression , name = "abs_var" , ub = None , difference = 0 , add = True ) : Components = namedtuple ( 'Components' , [ 'variable' , 'upper_constraint' , 'lower_constraint' ] ) variable = model . problem . Variable ( name , lb = 0 , ub = ub ) upper_constraint = model . problem . Constraint ( expression - variable , ub = difference , name = "abs_pos_" + name ) , lower_constraint = model . problem . Constraint ( expression + variable , lb = difference , name = "abs_neg_" + name ) to_add = Components ( variable , upper_constraint , lower_constraint ) if add : add_cons_vars_to_problem ( model , to_add ) return to_add
9199	def reversals ( series , left = False , right = False ) : series = iter ( series ) x_last , x = next ( series ) , next ( series ) d_last = ( x - x_last ) if left : yield x_last for x_next in series : if x_next == x : continue d_next = x_next - x if d_last * d_next < 0 : yield x x_last , x = x , x_next d_last = d_next if right : yield x_next
12990	def overview ( ) : range_search = RangeSearch ( ) ranges = range_search . get_ranges ( ) if ranges : formatted_ranges = [ ] tags_lookup = { } for r in ranges : formatted_ranges . append ( { 'mask' : r . range } ) tags_lookup [ r . range ] = r . tags search = Host . search ( ) search = search . filter ( 'term' , status = 'up' ) search . aggs . bucket ( 'hosts' , 'ip_range' , field = 'address' , ranges = formatted_ranges ) response = search . execute ( ) print_line ( "{0:<18} {1:<6} {2}" . format ( "Range" , "Count" , "Tags" ) ) print_line ( "-" * 60 ) for entry in response . aggregations . hosts . buckets : print_line ( "{0:<18} {1:<6} {2}" . format ( entry . key , entry . doc_count , tags_lookup [ entry . key ] ) ) else : print_error ( "No ranges defined." )
10982	def locate_spheres ( image , feature_rad , dofilter = False , order = ( 3 , 3 , 3 ) , trim_edge = True , ** kwargs ) : m = models . SmoothFieldModel ( ) I = ilms . LegendrePoly2P1D ( order = order , constval = image . get_image ( ) . mean ( ) ) s = states . ImageState ( image , [ I ] , pad = 0 , mdl = m ) if dofilter : opt . do_levmarq ( s , s . params ) pos = addsub . feature_guess ( s , feature_rad , trim_edge = trim_edge , ** kwargs ) [ 0 ] return pos
5454	def task_view_generator ( job_descriptor ) : for task_descriptor in job_descriptor . task_descriptors : jd = JobDescriptor ( job_descriptor . job_metadata , job_descriptor . job_params , job_descriptor . job_resources , [ task_descriptor ] ) yield jd
3685	def set_from_PT ( self , Vs ) : good_roots = [ ] bad_roots = [ ] for i in Vs : j = i . real if abs ( i . imag ) > 1E-9 or j < 0 : bad_roots . append ( i ) else : good_roots . append ( j ) if len ( bad_roots ) == 2 : V = good_roots [ 0 ] self . phase = self . set_properties_from_solution ( self . T , self . P , V , self . b , self . delta , self . epsilon , self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 ) if self . phase == 'l' : self . V_l = V else : self . V_g = V else : self . V_l , self . V_g = min ( good_roots ) , max ( good_roots ) [ self . set_properties_from_solution ( self . T , self . P , V , self . b , self . delta , self . epsilon , self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 ) for V in [ self . V_l , self . V_g ] ] self . phase = 'l/g'
1960	def sys_rename ( self , oldnamep , newnamep ) : oldname = self . current . read_string ( oldnamep ) newname = self . current . read_string ( newnamep ) ret = 0 try : os . rename ( oldname , newname ) except OSError as e : ret = - e . errno return ret
5351	def __autorefresh_studies ( self , cfg ) : if 'studies' not in self . conf [ self . backend_section ] or 'enrich_areas_of_code:git' not in self . conf [ self . backend_section ] [ 'studies' ] : logger . debug ( "Not doing autorefresh for studies, Areas of Code study is not active." ) return aoc_index = self . conf [ 'enrich_areas_of_code:git' ] . get ( 'out_index' , GitEnrich . GIT_AOC_ENRICHED ) if not aoc_index : aoc_index = GitEnrich . GIT_AOC_ENRICHED logger . debug ( "Autorefresh for Areas of Code study index: %s" , aoc_index ) es = Elasticsearch ( [ self . conf [ 'es_enrichment' ] [ 'url' ] ] , timeout = 100 , verify_certs = self . _get_enrich_backend ( ) . elastic . requests . verify ) if not es . indices . exists ( index = aoc_index ) : logger . debug ( "Not doing autorefresh, index doesn't exist for Areas of Code study" ) return logger . debug ( "Doing autorefresh for Areas of Code study" ) aoc_backend = GitEnrich ( self . db_sh , None , cfg [ 'projects' ] [ 'projects_file' ] , self . db_user , self . db_password , self . db_host ) aoc_backend . mapping = None aoc_backend . roles = [ 'author' ] elastic_enrich = get_elastic ( self . conf [ 'es_enrichment' ] [ 'url' ] , aoc_index , clean = False , backend = aoc_backend ) aoc_backend . set_elastic ( elastic_enrich ) self . __autorefresh ( aoc_backend , studies = True )
6090	def transform_grid ( func ) : @ wraps ( func ) def wrapper ( profile , grid , * args , ** kwargs ) : if not isinstance ( grid , TransformedGrid ) : return func ( profile , profile . transform_grid_to_reference_frame ( grid ) , * args , ** kwargs ) else : return func ( profile , grid , * args , ** kwargs ) return wrapper
13110	def lookup ( cls , key , get = False ) : if get : item = cls . _item_dict . get ( key ) return item . name if item else key return cls . _item_dict [ key ] . name
4664	def detail ( self , * args , ** kwargs ) : prefix = kwargs . pop ( "prefix" , default_prefix ) kwargs [ "votes" ] = list ( set ( kwargs [ "votes" ] ) ) return OrderedDict ( [ ( "memo_key" , PublicKey ( kwargs [ "memo_key" ] , prefix = prefix ) ) , ( "voting_account" , ObjectId ( kwargs [ "voting_account" ] , "account" ) ) , ( "num_witness" , Uint16 ( kwargs [ "num_witness" ] ) ) , ( "num_committee" , Uint16 ( kwargs [ "num_committee" ] ) ) , ( "votes" , Array ( [ VoteId ( o ) for o in kwargs [ "votes" ] ] ) ) , ( "extensions" , Set ( [ ] ) ) , ] )
5249	def bopen ( ** kwargs ) : con = BCon ( ** kwargs ) con . start ( ) try : yield con finally : con . stop ( )
6685	def update ( kernel = False ) : manager = MANAGER cmds = { 'yum -y --color=never' : { False : '--exclude=kernel* update' , True : 'update' } } cmd = cmds [ manager ] [ kernel ] run_as_root ( "%(manager)s %(cmd)s" % locals ( ) )
695	def _loadDescriptionFile ( descriptionPyPath ) : global g_descriptionImportCount if not os . path . isfile ( descriptionPyPath ) : raise RuntimeError ( ( "Experiment description file %s does not exist or " + "is not a file" ) % ( descriptionPyPath , ) ) mod = imp . load_source ( "pf_description%d" % g_descriptionImportCount , descriptionPyPath ) g_descriptionImportCount += 1 if not hasattr ( mod , "descriptionInterface" ) : raise RuntimeError ( "Experiment description file %s does not define %s" % ( descriptionPyPath , "descriptionInterface" ) ) if not isinstance ( mod . descriptionInterface , exp_description_api . DescriptionIface ) : raise RuntimeError ( ( "Experiment description file %s defines %s but it " + "is not DescriptionIface-based" ) % ( descriptionPyPath , name ) ) return mod
8542	def _get_username ( self , username = None , use_config = True , config_filename = None ) : if not username and use_config : if self . _config is None : self . _read_config ( config_filename ) username = self . _config . get ( "credentials" , "username" , fallback = None ) if not username : username = input ( "Please enter your username: " ) . strip ( ) while not username : username = input ( "No username specified. Please enter your username: " ) . strip ( ) if 'credendials' not in self . _config : self . _config . add_section ( 'credentials' ) self . _config . set ( "credentials" , "username" , username ) self . _save_config ( ) return username
8968	def decryptMessage ( self , ciphertext , header , ad = None ) : if ad == None : ad = self . __ad plaintext = self . __decryptSavedMessage ( ciphertext , header , ad ) if plaintext : return plaintext if self . triggersStep ( header . dh_pub ) : self . __saveMessageKeys ( header . pn ) self . step ( header . dh_pub ) self . __saveMessageKeys ( header . n ) return self . __decrypt ( ciphertext , self . __skr . nextDecryptionKey ( ) , header , ad )
556	def getCompletedSwarms ( self ) : swarmIds = [ ] for swarmId , info in self . _state [ 'swarms' ] . iteritems ( ) : if info [ 'status' ] == 'completed' : swarmIds . append ( swarmId ) return swarmIds
11208	def get_config ( jid ) : acls = getattr ( settings , 'XMPP_HTTP_UPLOAD_ACCESS' , ( ( '.*' , False ) , ) ) for regex , config in acls : if isinstance ( regex , six . string_types ) : regex = [ regex ] for subex in regex : if re . search ( subex , jid ) : return config return False
9293	def db_value ( self , value ) : value = self . transform_value ( value ) return self . hhash . encrypt ( value , salt_size = self . salt_size , rounds = self . rounds )
4552	def fill_triangle ( setter , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : a = b = y = last = 0 if y0 > y1 : y0 , y1 = y1 , y0 x0 , x1 = x1 , x0 if y1 > y2 : y2 , y1 = y1 , y2 x2 , x1 = x1 , x2 if y0 > y1 : y0 , y1 = y1 , y0 x0 , x1 = x1 , x0 if y0 == y2 : a = b = x0 if x1 < a : a = x1 elif x1 > b : b = x1 if x2 < a : a = x2 elif x2 > b : b = x2 _draw_fast_hline ( setter , a , y0 , b - a + 1 , color , aa ) dx01 = x1 - x0 dy01 = y1 - y0 dx02 = x2 - x0 dy02 = y2 - y0 dx12 = x2 - x1 dy12 = y2 - y1 sa = 0 sb = 0 if y1 == y2 : last = y1 else : last = y1 - 1 for y in range ( y , last + 1 ) : a = x0 + sa / dy01 b = x0 + sb / dy02 sa += dx01 sb += dx02 if a > b : a , b = b , a _draw_fast_hline ( setter , a , y , b - a + 1 , color , aa ) sa = dx12 * ( y - y1 ) sb = dx02 * ( y - y0 ) for y in range ( y , y2 + 1 ) : a = x1 + sa / dy12 b = x0 + sb / dy02 sa += dx12 sb += dx02 if a > b : a , b = b , a _draw_fast_hline ( setter , a , y , b - a + 1 , color , aa )
10633	def get_compound_afrs ( self ) : result = self . _compound_mfrs * 1.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) result [ index ] = stoich . amount ( compound , result [ index ] ) return result
8948	def info ( msg ) : _flush ( ) sys . stdout . write ( msg + '\n' ) sys . stdout . flush ( )
10798	def _weight ( self , rsq , sigma = None ) : sigma = sigma or self . filter_size if not self . clip : o = np . exp ( - rsq / ( 2 * sigma ** 2 ) ) else : o = np . zeros ( rsq . shape , dtype = 'float' ) m = ( rsq < self . clipsize ** 2 ) o [ m ] = np . exp ( - rsq [ m ] / ( 2 * sigma ** 2 ) ) return o
7279	def play ( self ) : if not self . is_playing ( ) : self . play_pause ( ) self . _is_playing = True self . playEvent ( self )
4015	def consume ( consumer_id ) : global _consumers consumer = _consumers [ consumer_id ] client = get_docker_client ( ) try : status = client . inspect_container ( consumer . container_id ) [ 'State' ] [ 'Status' ] except Exception as e : status = 'unknown' new_logs = client . logs ( consumer . container_id , stdout = True , stderr = True , stream = False , timestamps = False , since = calendar . timegm ( consumer . offset . timetuple ( ) ) ) updated_consumer = Consumer ( consumer . container_id , datetime . utcnow ( ) ) _consumers [ str ( consumer_id ) ] = updated_consumer response = jsonify ( { 'logs' : new_logs , 'status' : status } ) response . headers [ 'Access-Control-Allow-Origin' ] = '*' response . headers [ 'Access-Control-Allow-Methods' ] = 'GET, POST' return response
13298	def find_repos ( self , depth = 10 ) : repos = [ ] for root , subdirs , files in walk_dn ( self . root , depth = depth ) : if 'modules' in root : continue if '.git' in subdirs : repos . append ( root ) return repos
7525	def draw ( self , show_tip_labels = True , show_node_support = False , use_edge_lengths = False , orient = "right" , print_args = False , * args , ** kwargs ) : self . _decompose_tree ( orient = orient , use_edge_lengths = use_edge_lengths ) dwargs = { } dwargs [ "show_tip_labels" ] = show_tip_labels dwargs [ "show_node_support" ] = show_node_support dwargs . update ( kwargs ) canvas , axes , panel = tree_panel_plot ( self , print_args , ** dwargs ) return canvas , axes , panel
10175	def _format_range_dt ( self , d ) : if not isinstance ( d , six . string_types ) : d = d . isoformat ( ) return '{0}||/{1}' . format ( d , self . dt_rounding_map [ self . aggregation_interval ] )
10622	def get_element_mass ( self , element ) : result = numpy . zeros ( 1 ) for compound in self . material . compounds : result += self . get_compound_mass ( compound ) * numpy . array ( stoich . element_mass_fractions ( compound , [ element ] ) ) return result [ 0 ]
8480	def get ( name , default = None , allow_default = True ) : return Config ( ) . get ( name , default , allow_default = allow_default )
2268	def to_dict ( self ) : return self . _base ( ( key , ( value . to_dict ( ) if isinstance ( value , AutoDict ) else value ) ) for key , value in self . items ( ) )
127	def area ( self ) : if len ( self . exterior ) < 3 : raise Exception ( "Cannot compute the polygon's area because it contains less than three points." ) poly = self . to_shapely_polygon ( ) return poly . area
2921	def _restart ( self , my_task ) : if not my_task . _has_state ( Task . WAITING ) : raise WorkflowException ( my_task , "Cannot refire a task that is not" "in WAITING state" ) if my_task . _get_internal_data ( 'task_id' ) is not None : if not hasattr ( my_task , 'async_call' ) : task_id = my_task . _get_internal_data ( 'task_id' ) my_task . async_call = default_app . AsyncResult ( task_id ) my_task . deserialized = True my_task . async_call . state async_call = my_task . async_call if async_call . state == 'FAILED' : pass elif async_call . state in [ 'RETRY' , 'PENDING' , 'STARTED' ] : async_call . revoke ( ) LOG . info ( "Celery task '%s' was in %s state and was revoked" % ( async_call . state , async_call ) ) elif async_call . state == 'SUCCESS' : LOG . warning ( "Celery task '%s' succeeded, but a refire was " "requested" % async_call ) self . _clear_celery_task_data ( my_task ) return self . _start ( my_task )
8419	def is_close_to_int ( x ) : if not np . isfinite ( x ) : return False return abs ( x - nearest_int ( x ) ) < 1e-10
13662	def atomic_write ( filename ) : f = _tempfile ( os . fsencode ( filename ) ) try : yield f finally : f . close ( ) os . replace ( f . name , filename )
8822	def main ( notify , hour , minute ) : config_opts = [ '--config-file' , '/etc/neutron/neutron.conf' ] config . init ( config_opts ) network_strategy . STRATEGY . load ( ) billing . PUBLIC_NETWORK_ID = network_strategy . STRATEGY . get_public_net_id ( ) config . setup_logging ( ) context = neutron_context . get_admin_context ( ) query = context . session . query ( models . IPAddress ) ( period_start , period_end ) = billing . calc_periods ( hour , minute ) full_day_ips = billing . build_full_day_ips ( query , period_start , period_end ) partial_day_ips = billing . build_partial_day_ips ( query , period_start , period_end ) if notify : for ipaddress in full_day_ips : click . echo ( 'start: {}, end: {}' . format ( period_start , period_end ) ) payload = billing . build_payload ( ipaddress , billing . IP_EXISTS , start_time = period_start , end_time = period_end ) billing . do_notify ( context , billing . IP_EXISTS , payload ) for ipaddress in partial_day_ips : click . echo ( 'start: {}, end: {}' . format ( period_start , period_end ) ) payload = billing . build_payload ( ipaddress , billing . IP_EXISTS , start_time = ipaddress . allocated_at , end_time = period_end ) billing . do_notify ( context , billing . IP_EXISTS , payload ) else : click . echo ( 'Case 1 ({}):\n' . format ( len ( full_day_ips ) ) ) for ipaddress in full_day_ips : pp ( billing . build_payload ( ipaddress , billing . IP_EXISTS , start_time = period_start , end_time = period_end ) ) click . echo ( '\n===============================================\n' ) click . echo ( 'Case 2 ({}):\n' . format ( len ( partial_day_ips ) ) ) for ipaddress in partial_day_ips : pp ( billing . build_payload ( ipaddress , billing . IP_EXISTS , start_time = ipaddress . allocated_at , end_time = period_end ) )
1492	def register_timer_task_in_sec ( self , task , second ) : second_in_float = float ( second ) expiration = time . time ( ) + second_in_float heappush ( self . timer_tasks , ( expiration , task ) )
13303	def nmse ( a , b ) : return np . square ( a - b ) . mean ( ) / ( a . mean ( ) * b . mean ( ) )
12556	def save_varlist ( filename , varnames , varlist ) : variables = { } for i , vn in enumerate ( varnames ) : variables [ vn ] = varlist [ i ] ExportData . save_variables ( filename , variables )
4217	def delete_password ( self , service , username ) : if not self . connected ( service ) : raise PasswordDeleteError ( "Cancelled by user" ) if not self . iface . hasEntry ( self . handle , service , username , self . appid ) : raise PasswordDeleteError ( "Password not found" ) self . iface . removeEntry ( self . handle , service , username , self . appid )
4922	def retrieve ( self , request , pk = None ) : catalog_api = CourseCatalogApiClient ( request . user ) catalog = catalog_api . get_catalog ( pk ) self . ensure_data_exists ( request , catalog , error_message = ( "Unable to fetch API response for given catalog from endpoint '/catalog/{pk}/'. " "The resource you are looking for does not exist." . format ( pk = pk ) ) ) serializer = self . serializer_class ( catalog ) return Response ( serializer . data )
2408	def extract_features_and_generate_model ( essays , algorithm = util_functions . AlgorithmTypes . regression ) : f = feature_extractor . FeatureExtractor ( ) f . initialize_dictionaries ( essays ) train_feats = f . gen_feats ( essays ) set_score = numpy . asarray ( essays . _score , dtype = numpy . int ) if len ( util_functions . f7 ( list ( set_score ) ) ) > 5 : algorithm = util_functions . AlgorithmTypes . regression else : algorithm = util_functions . AlgorithmTypes . classification clf , clf2 = get_algorithms ( algorithm ) cv_error_results = get_cv_error ( clf2 , train_feats , essays . _score ) try : clf . fit ( train_feats , set_score ) except ValueError : log . exception ( "Not enough classes (0,1,etc) in sample." ) set_score [ 0 ] = 1 set_score [ 1 ] = 0 clf . fit ( train_feats , set_score ) return f , clf , cv_error_results
896	def read ( cls , proto ) : tm = object . __new__ ( cls ) tm . columnDimensions = tuple ( proto . columnDimensions ) tm . cellsPerColumn = int ( proto . cellsPerColumn ) tm . activationThreshold = int ( proto . activationThreshold ) tm . initialPermanence = round ( proto . initialPermanence , EPSILON_ROUND ) tm . connectedPermanence = round ( proto . connectedPermanence , EPSILON_ROUND ) tm . minThreshold = int ( proto . minThreshold ) tm . maxNewSynapseCount = int ( proto . maxNewSynapseCount ) tm . permanenceIncrement = round ( proto . permanenceIncrement , EPSILON_ROUND ) tm . permanenceDecrement = round ( proto . permanenceDecrement , EPSILON_ROUND ) tm . predictedSegmentDecrement = round ( proto . predictedSegmentDecrement , EPSILON_ROUND ) tm . maxSegmentsPerCell = int ( proto . maxSegmentsPerCell ) tm . maxSynapsesPerSegment = int ( proto . maxSynapsesPerSegment ) tm . connections = Connections . read ( proto . connections ) tm . _random = Random ( ) tm . _random . read ( proto . random ) tm . activeCells = [ int ( x ) for x in proto . activeCells ] tm . winnerCells = [ int ( x ) for x in proto . winnerCells ] flatListLength = tm . connections . segmentFlatListLength ( ) tm . numActiveConnectedSynapsesForSegment = [ 0 ] * flatListLength tm . numActivePotentialSynapsesForSegment = [ 0 ] * flatListLength tm . lastUsedIterationForSegment = [ 0 ] * flatListLength tm . activeSegments = [ ] tm . matchingSegments = [ ] for protoSegment in proto . activeSegments : tm . activeSegments . append ( tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) ) for protoSegment in proto . matchingSegments : tm . matchingSegments . append ( tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) ) for protoSegment in proto . numActivePotentialSynapsesForSegment : segment = tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) tm . numActivePotentialSynapsesForSegment [ segment . flatIdx ] = ( int ( protoSegment . number ) ) tm . iteration = long ( proto . iteration ) for protoSegment in proto . lastUsedIterationForSegment : segment = tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) tm . lastUsedIterationForSegment [ segment . flatIdx ] = ( long ( protoSegment . number ) ) return tm
11307	def map_attr ( self , mapping , attr , obj ) : if attr not in mapping and hasattr ( self , attr ) : if not callable ( getattr ( self , attr ) ) : mapping [ attr ] = getattr ( self , attr ) else : mapping [ attr ] = getattr ( self , attr ) ( obj )
12459	def main ( * args ) : r with disable_error_handler ( ) : args = parse_args ( args or sys . argv [ 1 : ] ) config = read_config ( args . config , args ) if config is None : return True bootstrap = config [ __script__ ] if not check_pre_requirements ( bootstrap [ 'pre_requirements' ] ) : return True env_args = prepare_args ( config [ 'virtualenv' ] , bootstrap ) if not create_env ( bootstrap [ 'env' ] , env_args , bootstrap [ 'recreate' ] , bootstrap [ 'ignore_activated' ] , bootstrap [ 'quiet' ] ) : return True pip_args = prepare_args ( config [ 'pip' ] , bootstrap ) if not install ( bootstrap [ 'env' ] , bootstrap [ 'requirements' ] , pip_args , bootstrap [ 'ignore_activated' ] , bootstrap [ 'install_dev_requirements' ] , bootstrap [ 'quiet' ] ) : return True run_hook ( bootstrap [ 'hook' ] , bootstrap , bootstrap [ 'quiet' ] ) if not bootstrap [ 'quiet' ] : print_message ( 'All OK!' ) return False
4200	def _thumbnail_div ( full_dir , fname , snippet , is_backref = False ) : thumb = os . path . join ( full_dir , 'images' , 'thumb' , 'sphx_glr_%s_thumb.png' % fname [ : - 3 ] ) ref_name = os . path . join ( full_dir , fname ) . replace ( os . path . sep , '_' ) template = BACKREF_THUMBNAIL_TEMPLATE if is_backref else THUMBNAIL_TEMPLATE return template . format ( snippet = snippet , thumbnail = thumb , ref_name = ref_name )
7368	async def read ( response , loads = loads , encoding = None ) : ctype = response . headers . get ( 'Content-Type' , "" ) . lower ( ) try : if "application/json" in ctype : logger . info ( "decoding data as json" ) return await response . json ( encoding = encoding , loads = loads ) if "text" in ctype : logger . info ( "decoding data as text" ) return await response . text ( encoding = encoding ) except ( UnicodeDecodeError , json . JSONDecodeError ) as exc : data = await response . read ( ) raise exceptions . PeonyDecodeError ( response = response , data = data , exception = exc ) return await response . read ( )
382	def drop ( x , keep = 0.5 ) : if len ( x . shape ) == 3 : if x . shape [ - 1 ] == 3 : img_size = x . shape mask = np . random . binomial ( n = 1 , p = keep , size = x . shape [ : - 1 ] ) for i in range ( 3 ) : x [ : , : , i ] = np . multiply ( x [ : , : , i ] , mask ) elif x . shape [ - 1 ] == 1 : img_size = x . shape x = np . multiply ( x , np . random . binomial ( n = 1 , p = keep , size = img_size ) ) else : raise Exception ( "Unsupported shape {}" . format ( x . shape ) ) elif len ( x . shape ) == 2 or 1 : img_size = x . shape x = np . multiply ( x , np . random . binomial ( n = 1 , p = keep , size = img_size ) ) else : raise Exception ( "Unsupported shape {}" . format ( x . shape ) ) return x
9017	def _pattern ( self , base ) : rows = self . _rows ( base . get ( ROWS , [ ] ) ) self . _finish_inheritance ( ) self . _finish_instructions ( ) self . _connect_rows ( base . get ( CONNECTIONS , [ ] ) ) id_ = self . _to_id ( base [ ID ] ) name = base [ NAME ] return self . new_pattern ( id_ , name , rows )
13484	def ghpages ( ) : opts = options docroot = path ( opts . get ( 'docroot' , 'docs' ) ) if not docroot . exists ( ) : raise BuildFailure ( "Sphinx documentation root (%s) does not exist." % docroot ) builddir = docroot / opts . get ( "builddir" , ".build" ) builddir = builddir / 'html' if not builddir . exists ( ) : raise BuildFailure ( "Sphinx build directory (%s) does not exist." % builddir ) nojekyll = path ( builddir ) / '.nojekyll' nojekyll . touch ( ) sh ( 'ghp-import -p %s' % ( builddir ) )
3401	def find_external_compartment ( model ) : if model . boundary : counts = pd . Series ( tuple ( r . compartments ) [ 0 ] for r in model . boundary ) most = counts . value_counts ( ) most = most . index [ most == most . max ( ) ] . to_series ( ) else : most = None like_external = compartment_shortlist [ "e" ] + [ "e" ] matches = pd . Series ( [ co in like_external for co in model . compartments ] , index = model . compartments ) if matches . sum ( ) == 1 : compartment = matches . index [ matches ] [ 0 ] LOGGER . info ( "Compartment `%s` sounds like an external compartment. " "Using this one without counting boundary reactions" % compartment ) return compartment elif most is not None and matches . sum ( ) > 1 and matches [ most ] . sum ( ) == 1 : compartment = most [ matches [ most ] ] [ 0 ] LOGGER . warning ( "There are several compartments that look like an " "external compartment but `%s` has the most boundary " "reactions, so using that as the external " "compartment." % compartment ) return compartment elif matches . sum ( ) > 1 : raise RuntimeError ( "There are several compartments (%s) that look " "like external compartments but we can't tell " "which one to use. Consider renaming your " "compartments please." ) if most is not None : return most [ 0 ] LOGGER . warning ( "Could not identify an external compartment by name and" " choosing one with the most boundary reactions. That " "might be complete nonsense or change suddenly. " "Consider renaming your compartments using " "`Model.compartments` to fix this." ) raise RuntimeError ( "The heuristic for discovering an external compartment " "relies on names and boundary reactions. Yet, there " "are neither compartments with recognized names nor " "boundary reactions in the model." )
6202	def merge_da ( ts_d , ts_par_d , ts_a , ts_par_a ) : ts = np . hstack ( [ ts_d , ts_a ] ) ts_par = np . hstack ( [ ts_par_d , ts_par_a ] ) a_ch = np . hstack ( [ np . zeros ( ts_d . shape [ 0 ] , dtype = bool ) , np . ones ( ts_a . shape [ 0 ] , dtype = bool ) ] ) index_sort = ts . argsort ( ) return ts [ index_sort ] , a_ch [ index_sort ] , ts_par [ index_sort ]
11993	def set_encryption_passphrases ( self , encryption_passphrases ) : self . encryption_passphrases = self . _update_dict ( encryption_passphrases , { } , replace_data = True )
1700	def consume ( self , consume_function ) : from heronpy . streamlet . impl . consumebolt import ConsumeStreamlet consume_streamlet = ConsumeStreamlet ( consume_function , self ) self . _add_child ( consume_streamlet ) return
4484	def copyfileobj ( fsrc , fdst , total , length = 16 * 1024 ) : with tqdm ( unit = 'bytes' , total = total , unit_scale = True ) as pbar : while 1 : buf = fsrc . read ( length ) if not buf : break fdst . write ( buf ) pbar . update ( len ( buf ) )
11515	def search_item_by_name_and_folder ( self , name , folder_id , token = None ) : parameters = dict ( ) parameters [ 'name' ] = name parameters [ 'folderId' ] = folder_id if token : parameters [ 'token' ] = token response = self . request ( 'midas.item.searchbynameandfolder' , parameters ) return response [ 'items' ]
3024	def _in_gce_environment ( ) : if SETTINGS . env_name is not None : return SETTINGS . env_name == 'GCE_PRODUCTION' if NO_GCE_CHECK != 'True' and _detect_gce_environment ( ) : SETTINGS . env_name = 'GCE_PRODUCTION' return True return False
7047	def _parallel_bls_worker ( task ) : try : return _bls_runner ( * task ) except Exception as e : LOGEXCEPTION ( 'BLS failed for task %s' % repr ( task [ 2 : ] ) ) return { 'power' : nparray ( [ npnan for x in range ( task [ 2 ] ) ] ) , 'bestperiod' : npnan , 'bestpower' : npnan , 'transdepth' : npnan , 'transduration' : npnan , 'transingressbin' : npnan , 'transegressbin' : npnan }
8756	def delete_tenant_quota ( context , tenant_id ) : tenant_quotas = context . session . query ( Quota ) tenant_quotas = tenant_quotas . filter_by ( tenant_id = tenant_id ) tenant_quotas . delete ( )
13086	def get ( self , section , key ) : try : return self . config . get ( section , key ) except configparser . NoSectionError : pass except configparser . NoOptionError : pass return self . defaults [ section ] [ key ]
6095	def voronoi_regular_to_pix_from_grids_and_geometry ( regular_grid , regular_to_nearest_pix , pixel_centres , pixel_neighbors , pixel_neighbors_size ) : regular_to_pix = np . zeros ( ( regular_grid . shape [ 0 ] ) ) for regular_index in range ( regular_grid . shape [ 0 ] ) : nearest_pix_pixel_index = regular_to_nearest_pix [ regular_index ] while True : nearest_pix_pixel_center = pixel_centres [ nearest_pix_pixel_index ] sub_to_nearest_pix_distance = ( regular_grid [ regular_index , 0 ] - nearest_pix_pixel_center [ 0 ] ) ** 2 + ( regular_grid [ regular_index , 1 ] - nearest_pix_pixel_center [ 1 ] ) ** 2 closest_separation_from_pix_neighbor = 1.0e8 for neighbor_index in range ( pixel_neighbors_size [ nearest_pix_pixel_index ] ) : neighbor = pixel_neighbors [ nearest_pix_pixel_index , neighbor_index ] separation_from_neighbor = ( regular_grid [ regular_index , 0 ] - pixel_centres [ neighbor , 0 ] ) ** 2 + ( regular_grid [ regular_index , 1 ] - pixel_centres [ neighbor , 1 ] ) ** 2 if separation_from_neighbor < closest_separation_from_pix_neighbor : closest_separation_from_pix_neighbor = separation_from_neighbor closest_neighbor_index = neighbor_index neighboring_pix_pixel_index = pixel_neighbors [ nearest_pix_pixel_index , closest_neighbor_index ] sub_to_neighboring_pix_distance = closest_separation_from_pix_neighbor if sub_to_nearest_pix_distance <= sub_to_neighboring_pix_distance : regular_to_pix [ regular_index ] = nearest_pix_pixel_index break else : nearest_pix_pixel_index = neighboring_pix_pixel_index return regular_to_pix
7736	def map ( self , data ) : result = [ ] for char in data : ret = None for lookup in self . mapping : ret = lookup ( char ) if ret is not None : break if ret is not None : result . append ( ret ) else : result . append ( char ) return result
10921	def do_levmarq_all_particle_groups ( s , region_size = 40 , max_iter = 2 , damping = 1.0 , decrease_damp_factor = 10. , run_length = 4 , collect_stats = False , ** kwargs ) : lp = LMParticleGroupCollection ( s , region_size = region_size , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , get_cos = collect_stats , max_iter = max_iter , ** kwargs ) lp . do_run_2 ( ) if collect_stats : return lp . stats
12950	def connectAlt ( cls , redisConnectionParams ) : if not isinstance ( redisConnectionParams , dict ) : raise ValueError ( 'redisConnectionParams must be a dictionary!' ) hashVal = hashDictOneLevel ( redisConnectionParams ) modelDictCopy = copy . deepcopy ( dict ( cls . __dict__ ) ) modelDictCopy [ 'REDIS_CONNECTION_PARAMS' ] = redisConnectionParams ConnectedIndexedRedisModel = type ( 'AltConnect' + cls . __name__ + str ( hashVal ) , cls . __bases__ , modelDictCopy ) return ConnectedIndexedRedisModel
2642	def submit ( self , func , * args , ** kwargs ) : task_id = uuid . uuid4 ( ) logger . debug ( "Pushing function {} to queue with args {}" . format ( func , args ) ) self . tasks [ task_id ] = Future ( ) fn_buf = pack_apply_message ( func , args , kwargs , buffer_threshold = 1024 * 1024 , item_threshold = 1024 ) msg = { "task_id" : task_id , "buffer" : fn_buf } self . outgoing_q . put ( msg ) return self . tasks [ task_id ]
2977	def cmd_kill ( opts ) : kill_signal = opts . signal if hasattr ( opts , 'signal' ) else "SIGKILL" __with_containers ( opts , Blockade . kill , signal = kill_signal )
12414	def flush ( self ) : self . require_not_closed ( ) chunk = self . _stream . getvalue ( ) self . _stream . truncate ( 0 ) self . _stream . seek ( 0 ) self . body = chunk if ( self . _body is None ) else ( self . _body + chunk ) if self . asynchronous : self . streaming = True
13442	def cmd_init_pull_from_cloud ( args ) : ( lcat , ccat ) = ( args . local_catalog , args . cloud_catalog ) logging . info ( "[init-pull-from-cloud]: %s => %s" % ( ccat , lcat ) ) if isfile ( lcat ) : args . error ( "[init-pull-from-cloud] The local catalog already exist: %s" % lcat ) if not isfile ( ccat ) : args . error ( "[init-pull-from-cloud] The cloud catalog does not exist: %s" % ccat ) ( lmeta , cmeta ) = ( "%s.lrcloud" % lcat , "%s.lrcloud" % ccat ) if isfile ( lmeta ) : args . error ( "[init-pull-from-cloud] The local meta-data already exist: %s" % lmeta ) if not isfile ( cmeta ) : args . error ( "[init-pull-from-cloud] The cloud meta-data does not exist: %s" % cmeta ) logging . info ( "Locking local catalog: %s" % ( lcat ) ) if not lock_file ( lcat ) : raise RuntimeError ( "The catalog %s is locked!" % lcat ) util . copy ( ccat , lcat ) cloudDAG = ChangesetDAG ( ccat ) path = cloudDAG . path ( cloudDAG . root . hash , cloudDAG . leafs [ 0 ] . hash ) util . apply_changesets ( args , path , lcat ) mfile = MetaFile ( lmeta ) utcnow = datetime . utcnow ( ) . strftime ( DATETIME_FORMAT ) [ : - 4 ] mfile [ 'catalog' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'catalog' ] [ 'modification_utc' ] = utcnow mfile [ 'catalog' ] [ 'filename' ] = lcat mfile [ 'last_push' ] [ 'filename' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'filename' ] mfile [ 'last_push' ] [ 'hash' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'hash' ] mfile [ 'last_push' ] [ 'modification_utc' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'modification_utc' ] mfile . flush ( ) if not args . no_smart_previews : copy_smart_previews ( lcat , ccat , local2cloud = False ) logging . info ( "Unlocking local catalog: %s" % ( lcat ) ) unlock_file ( lcat ) logging . info ( "[init-pull-from-cloud]: Success!" )
4106	def MINEIGVAL ( T0 , T , TOL ) : M = len ( T ) eigval = 10 eigvalold = 1 eigvec = numpy . zeros ( M + 1 , dtype = complex ) for k in range ( 0 , M + 1 ) : eigvec [ k ] = 1 + 0j it = 0 maxit = 15 while abs ( eigvalold - eigval ) > TOL * eigvalold and it < maxit : it = it + 1 eigvalold = eigval eig = toeplitz . HERMTOEP ( T0 , T , eigvec ) SUM = 0 save = 0. + 0j for k in range ( 0 , M + 1 ) : SUM = SUM + eig [ k ] . real ** 2 + eig [ k ] . imag ** 2 save = save + eig [ k ] * eigvec [ k ] . conjugate ( ) SUM = 1. / SUM eigval = save . real * SUM for k in range ( 0 , M + 1 ) : eigvec [ k ] = SUM * eig [ k ] if it == maxit : print ( 'warning reached max number of iteration (%s)' % maxit ) return eigval , eigvec
13694	def main ( ) : global DEBUG argd = docopt ( USAGESTR , version = VERSIONSTR , script = SCRIPT ) DEBUG = argd [ '--debug' ] width = parse_int ( argd [ '--width' ] or DEFAULT_WIDTH ) or 1 indent = parse_int ( argd [ '--indent' ] or ( argd [ '--INDENT' ] or 0 ) ) prepend = ' ' * ( indent * 4 ) if prepend and argd [ '--indent' ] : width -= len ( prepend ) userprepend = argd [ '--prepend' ] or ( argd [ '--PREPEND' ] or '' ) prepend = '' . join ( ( prepend , userprepend ) ) if argd [ '--prepend' ] : width -= len ( userprepend ) userappend = argd [ '--append' ] or ( argd [ '--APPEND' ] or '' ) if argd [ '--append' ] : width -= len ( userappend ) if argd [ 'WORDS' ] : argd [ 'WORDS' ] = ( ( try_read_file ( w ) if len ( w ) < 256 else w ) for w in argd [ 'WORDS' ] ) words = ' ' . join ( ( w for w in argd [ 'WORDS' ] if w ) ) else : words = read_stdin ( ) block = FormatBlock ( words ) . iter_format_block ( chars = argd [ '--chars' ] , fill = argd [ '--fill' ] , prepend = prepend , strip_first = argd [ '--stripfirst' ] , append = userappend , strip_last = argd [ '--striplast' ] , width = width , newlines = argd [ '--newlines' ] , lstrip = argd [ '--lstrip' ] , ) for i , line in enumerate ( block ) : if argd [ '--enumerate' ] : print ( '{: >3}: {}' . format ( i + 1 , line ) ) else : print ( line ) return 0
1298	def SetClipboardText ( text : str ) -> bool : if ctypes . windll . user32 . OpenClipboard ( 0 ) : ctypes . windll . user32 . EmptyClipboard ( ) textByteLen = ( len ( text ) + 1 ) * 2 hClipboardData = ctypes . windll . kernel32 . GlobalAlloc ( 0 , textByteLen ) hDestText = ctypes . windll . kernel32 . GlobalLock ( hClipboardData ) ctypes . cdll . msvcrt . wcsncpy ( ctypes . c_wchar_p ( hDestText ) , ctypes . c_wchar_p ( text ) , textByteLen // 2 ) ctypes . windll . kernel32 . GlobalUnlock ( hClipboardData ) ctypes . windll . user32 . SetClipboardData ( 13 , hClipboardData ) ctypes . windll . user32 . CloseClipboard ( ) return True return False
4365	def decode ( rawstr , json_loads = default_json_loads ) : decoded_msg = { } try : rawstr = rawstr . decode ( 'utf-8' ) except AttributeError : pass split_data = rawstr . split ( ":" , 3 ) msg_type = split_data [ 0 ] msg_id = split_data [ 1 ] endpoint = split_data [ 2 ] data = '' if msg_id != '' : if "+" in msg_id : msg_id = msg_id . split ( '+' ) [ 0 ] decoded_msg [ 'id' ] = int ( msg_id ) decoded_msg [ 'ack' ] = 'data' else : decoded_msg [ 'id' ] = int ( msg_id ) decoded_msg [ 'ack' ] = True msg_type_id = int ( msg_type ) if msg_type_id in MSG_VALUES : decoded_msg [ 'type' ] = MSG_VALUES [ int ( msg_type ) ] else : raise Exception ( "Unknown message type: %s" % msg_type ) decoded_msg [ 'endpoint' ] = endpoint if len ( split_data ) > 3 : data = split_data [ 3 ] if msg_type == "0" : pass elif msg_type == "1" : decoded_msg [ 'qs' ] = data elif msg_type == "2" : pass elif msg_type == "3" : decoded_msg [ 'data' ] = data elif msg_type == "4" : decoded_msg [ 'data' ] = json_loads ( data ) elif msg_type == "5" : try : data = json_loads ( data ) except ValueError : print ( "Invalid JSON event message" , data ) decoded_msg [ 'args' ] = [ ] else : decoded_msg [ 'name' ] = data . pop ( 'name' ) if 'args' in data : decoded_msg [ 'args' ] = data [ 'args' ] else : decoded_msg [ 'args' ] = [ ] elif msg_type == "6" : if '+' in data : ackId , data = data . split ( '+' ) decoded_msg [ 'ackId' ] = int ( ackId ) decoded_msg [ 'args' ] = json_loads ( data ) else : decoded_msg [ 'ackId' ] = int ( data ) decoded_msg [ 'args' ] = [ ] elif msg_type == "7" : if '+' in data : reason , advice = data . split ( '+' ) decoded_msg [ 'reason' ] = REASONS_VALUES [ int ( reason ) ] decoded_msg [ 'advice' ] = ADVICES_VALUES [ int ( advice ) ] else : decoded_msg [ 'advice' ] = '' if data != '' : decoded_msg [ 'reason' ] = REASONS_VALUES [ int ( data ) ] else : decoded_msg [ 'reason' ] = '' elif msg_type == "8" : pass return decoded_msg
8026	def getPaths ( roots , ignores = None ) : paths , count , ignores = [ ] , 0 , ignores or [ ] ignore_re = multiglob_compile ( ignores , prefix = False ) for root in roots : root = os . path . realpath ( root ) if os . path . isfile ( root ) : paths . append ( root ) continue for fldr in os . walk ( root ) : out . write ( "Gathering file paths to compare... (%d files examined)" % count ) for subdir in fldr [ 1 ] : dirpath = os . path . join ( fldr [ 0 ] , subdir ) if ignore_re . match ( dirpath ) : fldr [ 1 ] . remove ( subdir ) for filename in fldr [ 2 ] : filepath = os . path . join ( fldr [ 0 ] , filename ) if ignore_re . match ( filepath ) : continue paths . append ( filepath ) count += 1 out . write ( "Found %s files to be compared for duplication." % ( len ( paths ) ) , newline = True ) return paths
5452	def ensure_task_params_are_complete ( task_descriptors ) : for task_desc in task_descriptors : for param in [ 'labels' , 'envs' , 'inputs' , 'outputs' , 'input-recursives' , 'output-recursives' ] : if not task_desc . task_params . get ( param ) : task_desc . task_params [ param ] = set ( )
10261	def _collapse_variants_by_function ( graph : BELGraph , func : str ) -> None : for parent_node , variant_node , data in graph . edges ( data = True ) : if data [ RELATION ] == HAS_VARIANT and parent_node . function == func : collapse_pair ( graph , from_node = variant_node , to_node = parent_node )
5094	def refresh_robots ( self ) : resp = requests . get ( urljoin ( self . ENDPOINT , 'dashboard' ) , headers = self . _headers ) resp . raise_for_status ( ) for robot in resp . json ( ) [ 'robots' ] : if robot [ 'mac_address' ] is None : continue try : self . _robots . add ( Robot ( name = robot [ 'name' ] , serial = robot [ 'serial' ] , secret = robot [ 'secret_key' ] , traits = robot [ 'traits' ] , endpoint = robot [ 'nucleo_url' ] ) ) except requests . exceptions . HTTPError : print ( "Your '{}' robot is offline." . format ( robot [ 'name' ] ) ) continue self . refresh_persistent_maps ( ) for robot in self . _robots : robot . has_persistent_maps = robot . serial in self . _persistent_maps
1204	def from_spec ( spec , kwargs = None ) : layer = util . get_object ( obj = spec , predefined_objects = tensorforce . core . networks . layers , kwargs = kwargs ) assert isinstance ( layer , Layer ) return layer
9830	def write ( self , filename ) : maxcol = 80 with open ( filename , 'w' ) as outfile : for line in self . comments : comment = '# ' + str ( line ) outfile . write ( comment [ : maxcol ] + '\n' ) for component , object in self . sorted_components ( ) : object . write ( outfile ) DXclass . write ( self , outfile , quote = True ) for component , object in self . sorted_components ( ) : outfile . write ( 'component "%s" value %s\n' % ( component , str ( object . id ) ) )
11429	def record_strip_controlfields ( rec ) : for tag in rec . keys ( ) : if tag [ : 2 ] == '00' and rec [ tag ] [ 0 ] [ 3 ] : del rec [ tag ]
4283	def generate_video ( source , outname , settings , options = None ) : logger = logging . getLogger ( __name__ ) converter = settings [ 'video_converter' ] w_src , h_src = video_size ( source , converter = converter ) w_dst , h_dst = settings [ 'video_size' ] logger . debug ( 'Video size: %i, %i -> %i, %i' , w_src , h_src , w_dst , h_dst ) base , src_ext = splitext ( source ) base , dst_ext = splitext ( outname ) if dst_ext == src_ext and w_src <= w_dst and h_src <= h_dst : logger . debug ( 'Video is smaller than the max size, copying it instead' ) shutil . copy ( source , outname ) return if h_dst * w_src < h_src * w_dst : resize_opt = [ '-vf' , "scale=trunc(oh*a/2)*2:%i" % h_dst ] else : resize_opt = [ '-vf' , "scale=%i:trunc(ow/a/2)*2" % w_dst ] if w_src <= w_dst and h_src <= h_dst : resize_opt = [ ] cmd = [ converter , '-i' , source , '-y' ] if options is not None : cmd += options cmd += resize_opt + [ outname ] logger . debug ( 'Processing video: %s' , ' ' . join ( cmd ) ) check_subprocess ( cmd , source , outname )
10091	def setup ( app ) : if 'http' not in app . domains : httpdomain . setup ( app ) app . add_directive ( 'autopyramid' , RouteDirective )
2638	def submit ( self , command , blocksize , tasks_per_node , job_name = "parsl.auto" ) : wrapped_cmd = self . launcher ( command , tasks_per_node , 1 ) instance , name = self . create_instance ( command = wrapped_cmd ) self . provisioned_blocks += 1 self . resources [ name ] = { "job_id" : name , "status" : translate_table [ instance [ 'status' ] ] } return name
12763	def load_attachments ( self , source , skeleton ) : self . targets = { } self . offsets = { } filename = source if isinstance ( source , str ) : source = open ( source ) else : filename = '(file-{})' . format ( id ( source ) ) for i , line in enumerate ( source ) : tokens = line . split ( '#' ) [ 0 ] . strip ( ) . split ( ) if not tokens : continue label = tokens . pop ( 0 ) if label not in self . channels : logging . info ( '%s:%d: unknown marker %s' , filename , i , label ) continue if not tokens : continue name = tokens . pop ( 0 ) bodies = [ b for b in skeleton . bodies if b . name == name ] if len ( bodies ) != 1 : logging . info ( '%s:%d: %d skeleton bodies match %s' , filename , i , len ( bodies ) , name ) continue b = self . targets [ label ] = bodies [ 0 ] o = self . offsets [ label ] = np . array ( list ( map ( float , tokens ) ) ) * b . dimensions / 2 logging . info ( '%s < , label , b . name , o )
12564	def _partition_data ( datavol , roivol , roivalue , maskvol = None , zeroe = True ) : if maskvol is not None : indices = ( roivol == roivalue ) * ( maskvol > 0 ) else : indices = roivol == roivalue if datavol . ndim == 4 : ts = datavol [ indices , : ] else : ts = datavol [ indices ] if zeroe : if datavol . ndim == 4 : ts = ts [ ts . sum ( axis = 1 ) != 0 , : ] return ts
3012	def locked_put ( self , credentials ) : filters = { self . key_name : self . key_value } query = self . session . query ( self . model_class ) . filter_by ( ** filters ) entity = query . first ( ) if not entity : entity = self . model_class ( ** filters ) setattr ( entity , self . property_name , credentials ) self . session . add ( entity )
13513	def reynolds_number ( length , speed , temperature = 25 ) : kinematic_viscosity = interpolate . interp1d ( [ 0 , 10 , 20 , 25 , 30 , 40 ] , np . array ( [ 18.54 , 13.60 , 10.50 , 9.37 , 8.42 , 6.95 ] ) / 10 ** 7 ) Re = length * speed / kinematic_viscosity ( temperature ) return Re
2802	def convert_concat ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting concat ...' ) concat_nodes = [ layers [ i ] for i in inputs ] if len ( concat_nodes ) == 1 : layers [ scope_name ] = concat_nodes [ 0 ] return if names == 'short' : tf_name = 'CAT' + random_string ( 5 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) cat = keras . layers . Concatenate ( name = tf_name , axis = params [ 'axis' ] ) layers [ scope_name ] = cat ( concat_nodes )
5470	def _prepare_summary_table ( rows ) : if not rows : return [ ] key_field = 'job-name' if key_field not in rows [ 0 ] : key_field = 'job-id' grouped = collections . defaultdict ( lambda : collections . defaultdict ( lambda : [ ] ) ) for row in rows : grouped [ row . get ( key_field , '' ) ] [ row . get ( 'status' , '' ) ] += [ row ] new_rows = [ ] for job_key in sorted ( grouped . keys ( ) ) : group = grouped . get ( job_key , None ) canonical_status = [ 'RUNNING' , 'SUCCESS' , 'FAILURE' , 'CANCEL' ] for status in canonical_status + sorted ( group . keys ( ) ) : if status not in group : continue task_count = len ( group [ status ] ) del group [ status ] if task_count : summary_row = collections . OrderedDict ( ) summary_row [ key_field ] = job_key summary_row [ 'status' ] = status summary_row [ 'task-count' ] = task_count new_rows . append ( summary_row ) return new_rows
7383	def plot_axis ( self , rs , theta ) : xs , ys = get_cartesian ( rs , theta ) self . ax . plot ( xs , ys , 'black' , alpha = 0.3 )
5174	def events ( self , ** kwargs ) : return self . __api . events ( query = EqualsOperator ( "report" , self . hash_ ) , ** kwargs )
11966	def _bin_to_dec ( ip , check = True ) : if check and not is_bin ( ip ) : raise ValueError ( '_bin_to_dec: invalid IP: "%s"' % ip ) if isinstance ( ip , int ) : ip = str ( ip ) return int ( str ( ip ) , 2 )
9050	def bernoulli_sample ( offset , G , heritability = 0.5 , causal_variants = None , causal_variance = 0 , random_state = None , ) : r link = LogitLink ( ) mean , cov = _mean_cov ( offset , G , heritability , causal_variants , causal_variance , random_state ) lik = BernoulliProdLik ( link ) sampler = GGPSampler ( lik , mean , cov ) return sampler . sample ( random_state )
8358	def shoebot_example ( ** shoebot_kwargs ) : def decorator ( f ) : def run ( ) : from shoebot import ShoebotInstallError print ( " Shoebot - %s:" % f . __name__ . replace ( "_" , " " ) ) try : import shoebot outputfile = "/tmp/shoebot-%s.png" % f . __name__ bot = shoebot . create_bot ( outputfile = outputfile ) f ( bot ) bot . finish ( ) print ( ' [passed] : %s' % outputfile ) print ( '' ) except ShoebotInstallError as e : print ( ' [failed]' , e . args [ 0 ] ) print ( '' ) except Exception : print ( ' [failed] - traceback:' ) for line in traceback . format_exc ( ) . splitlines ( ) : print ( ' %s' % line ) print ( '' ) return run return decorator
4324	def convert ( self , samplerate = None , n_channels = None , bitdepth = None ) : bitdepths = [ 8 , 16 , 24 , 32 , 64 ] if bitdepth is not None : if bitdepth not in bitdepths : raise ValueError ( "bitdepth must be one of {}." . format ( str ( bitdepths ) ) ) self . output_format . extend ( [ '-b' , '{}' . format ( bitdepth ) ] ) if n_channels is not None : if not isinstance ( n_channels , int ) or n_channels <= 0 : raise ValueError ( "n_channels must be a positive integer." ) self . output_format . extend ( [ '-c' , '{}' . format ( n_channels ) ] ) if samplerate is not None : if not is_number ( samplerate ) or samplerate <= 0 : raise ValueError ( "samplerate must be a positive number." ) self . rate ( samplerate ) return self
12491	def as_float_array ( X , copy = True , force_all_finite = True ) : if isinstance ( X , np . matrix ) or ( not isinstance ( X , np . ndarray ) and not sp . issparse ( X ) ) : return check_array ( X , [ 'csr' , 'csc' , 'coo' ] , dtype = np . float64 , copy = copy , force_all_finite = force_all_finite , ensure_2d = False ) elif sp . issparse ( X ) and X . dtype in [ np . float32 , np . float64 ] : return X . copy ( ) if copy else X elif X . dtype in [ np . float32 , np . float64 ] : return X . copy ( 'F' if X . flags [ 'F_CONTIGUOUS' ] else 'C' ) if copy else X else : return X . astype ( np . float32 if X . dtype == np . int32 else np . float64 )
4704	def memcopy ( self , stream , offset = 0 , length = float ( "inf" ) ) : data = [ ord ( i ) for i in list ( stream ) ] size = min ( length , len ( data ) , self . m_size ) buff = cast ( self . m_buf , POINTER ( c_uint8 ) ) for i in range ( size ) : buff [ offset + i ] = data [ i ]
10833	def query_by_admin ( cls , admin ) : return cls . query . filter_by ( admin_type = resolve_admin_type ( admin ) , admin_id = admin . get_id ( ) )
12279	def add ( repo , args , targetdir , execute = False , generator = False , includes = [ ] , script = False , source = None ) : if not execute : files = add_files ( args = args , targetdir = targetdir , source = source , script = script , generator = generator ) else : files = run_executable ( repo , args , includes ) if files is None or len ( files ) == 0 : return repo filtered_files = [ ] package = repo . package for h in files : found = False for i , r in enumerate ( package [ 'resources' ] ) : if h [ 'relativepath' ] == r [ 'relativepath' ] : found = True if h [ 'sha256' ] == r [ 'sha256' ] : change = False for attr in [ 'source' ] : if h [ attr ] != r [ attr ] : r [ attr ] = h [ attr ] change = True if change : filtered_files . append ( h ) continue else : filtered_files . append ( h ) package [ 'resources' ] [ i ] = h break if not found : filtered_files . append ( h ) package [ 'resources' ] . append ( h ) if len ( filtered_files ) == 0 : return 0 repo . manager . add_files ( repo , filtered_files ) rootdir = repo . rootdir with cd ( rootdir ) : datapath = "datapackage.json" with open ( datapath , 'w' ) as fd : fd . write ( json . dumps ( package , indent = 4 ) ) return len ( filtered_files )
8974	def new_knitting_pattern_set_loader ( specification = DefaultSpecification ( ) ) : parser = specification . new_parser ( specification ) loader = specification . new_loader ( parser . knitting_pattern_set ) return loader
7578	def _get_evanno_table ( self , kpops , max_var_multiple , quiet ) : kpops = sorted ( kpops ) replnliks = [ ] for kpop in kpops : reps , excluded = _concat_reps ( self , kpop , max_var_multiple , quiet ) if excluded : if not quiet : sys . stderr . write ( "[K{}] {} reps excluded (not converged) see 'max_var_multiple'.\n" . format ( kpop , excluded ) ) if reps : ninds = reps [ 0 ] . inds nreps = len ( reps ) else : ninds = nreps = 0 if not reps : print "no result files found" replnliks . append ( [ i . est_lnlik for i in reps ] ) if len ( replnliks ) > 1 : lnmean = [ np . mean ( i ) for i in replnliks ] lnstds = [ np . std ( i , ddof = 1 ) for i in replnliks ] else : lnmean = replnliks lnstds = np . nan tab = pd . DataFrame ( index = kpops , data = { "Nreps" : [ len ( i ) for i in replnliks ] , "lnPK" : [ 0 ] * len ( kpops ) , "lnPPK" : [ 0 ] * len ( kpops ) , "deltaK" : [ 0 ] * len ( kpops ) , "estLnProbMean" : lnmean , "estLnProbStdev" : lnstds , } ) for kpop in kpops [ 1 : ] : tab . loc [ kpop , "lnPK" ] = tab . loc [ kpop , "estLnProbMean" ] - tab . loc [ kpop - 1 , "estLnProbMean" ] for kpop in kpops [ 1 : - 1 ] : tab . loc [ kpop , "lnPPK" ] = abs ( tab . loc [ kpop + 1 , "lnPK" ] - tab . loc [ kpop , "lnPK" ] ) tab . loc [ kpop , "deltaK" ] = ( abs ( tab . loc [ kpop + 1 , "estLnProbMean" ] - 2.0 * tab . loc [ kpop , "estLnProbMean" ] + tab . loc [ kpop - 1 , "estLnProbMean" ] ) / tab . loc [ kpop , "estLnProbStdev" ] ) return tab
702	def getResultsPerChoice ( self , swarmId , maxGenIdx , varName ) : results = dict ( ) ( allParticles , _ , resultErrs , _ , _ ) = self . getParticleInfos ( swarmId , genIdx = None , matured = True ) for particleState , resultErr in itertools . izip ( allParticles , resultErrs ) : if maxGenIdx is not None : if particleState [ 'genIdx' ] > maxGenIdx : continue if resultErr == numpy . inf : continue position = Particle . getPositionFromState ( particleState ) varPosition = position [ varName ] varPositionStr = str ( varPosition ) if varPositionStr in results : results [ varPositionStr ] [ 1 ] . append ( resultErr ) else : results [ varPositionStr ] = ( varPosition , [ resultErr ] ) return results
8235	def split_complementary ( clr ) : clr = color ( clr ) colors = colorlist ( clr ) clr = clr . complement colors . append ( clr . rotate_ryb ( - 30 ) . lighten ( 0.1 ) ) colors . append ( clr . rotate_ryb ( 30 ) . lighten ( 0.1 ) ) return colors
2034	def MSTORE8 ( self , address , value ) : if istainted ( self . pc ) : for taint in get_taints ( self . pc ) : value = taint_with ( value , taint ) self . _allocate ( address , 1 ) self . _store ( address , Operators . EXTRACT ( value , 0 , 8 ) , 1 )
6073	def mass_within_circle_in_units ( self , radius , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : if self . has_mass_profile : return sum ( map ( lambda p : p . mass_within_circle_in_units ( radius = radius , unit_mass = unit_mass , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) , self . mass_profiles ) ) else : return None
10370	def build_edge_data_filter ( annotations : Mapping , partial_match : bool = True ) -> EdgePredicate : @ edge_predicate def annotation_dict_filter ( data : EdgeData ) -> bool : return subdict_matches ( data , annotations , partial_match = partial_match ) return annotation_dict_filter
12091	def proto_02_01_MT70 ( abf = exampleABF ) : standard_overlayWithAverage ( abf ) swhlab . memtest . memtest ( abf ) swhlab . memtest . checkSweep ( abf ) swhlab . plot . save ( abf , tag = 'check' , resize = False )
4346	def swap ( self ) : effect_args = [ 'swap' ] self . effects . extend ( effect_args ) self . effects_log . append ( 'swap' ) return self
1752	def _reg_name ( self , reg_id ) : if reg_id >= X86_REG_ENDING : logger . warning ( "Trying to get register name for a non-register" ) return None cs_reg_name = self . cpu . instruction . reg_name ( reg_id ) if cs_reg_name is None or cs_reg_name . lower ( ) == '(invalid)' : return None return self . cpu . _regfile . _alias ( cs_reg_name . upper ( ) )
4959	def get_course_runs_from_program ( program ) : course_runs = set ( ) for course in program . get ( "courses" , [ ] ) : for run in course . get ( "course_runs" , [ ] ) : if "key" in run and run [ "key" ] : course_runs . add ( run [ "key" ] ) return course_runs
13713	def run ( self ) : self . log . debug ( 'consumer is running...' ) self . running = True while self . running : self . upload ( ) self . log . debug ( 'consumer exited.' )
11636	def refresh_access_token ( self , ) : logger . debug ( "REFRESHING TOKEN" ) self . token_time = time . time ( ) credentials = { 'token_time' : self . token_time } if self . oauth_version == 'oauth1' : self . access_token , self . access_token_secret = self . oauth . get_access_token ( self . access_token , self . access_token_secret , params = { "oauth_session_handle" : self . session_handle } ) credentials . update ( { 'access_token' : self . access_token , 'access_token_secret' : self . access_token_secret , 'session_handle' : self . session_handle , 'token_time' : self . token_time } ) else : headers = self . generate_oauth2_headers ( ) raw_access = self . oauth . get_raw_access_token ( data = { "refresh_token" : self . refresh_token , 'redirect_uri' : self . callback_uri , 'grant_type' : 'refresh_token' } , headers = headers ) credentials . update ( self . oauth2_access_parser ( raw_access ) ) return credentials
11713	def instance ( self , counter = None ) : if not counter : history = self . history ( ) if not history : return history else : return Response . _from_json ( history [ 'pipelines' ] [ 0 ] ) return self . _get ( '/instance/{counter:d}' . format ( counter = counter ) )
11814	def score ( self , plaintext ) : "Return a score for text based on how common letters pairs are." s = 1.0 for bi in bigrams ( plaintext ) : s = s * self . P2 [ bi ] return s
926	def generateDataset ( aggregationInfo , inputFilename , outputFilename = None ) : inputFullPath = resource_filename ( "nupic.datafiles" , inputFilename ) inputObj = FileRecordStream ( inputFullPath ) aggregator = Aggregator ( aggregationInfo = aggregationInfo , inputFields = inputObj . getFields ( ) ) if aggregator . isNullAggregation ( ) : return inputFullPath if outputFilename is None : outputFilename = 'agg_%s' % os . path . splitext ( os . path . basename ( inputFullPath ) ) [ 0 ] timePeriods = 'years months weeks days ' 'hours minutes seconds milliseconds microseconds' for k in timePeriods . split ( ) : if aggregationInfo . get ( k , 0 ) > 0 : outputFilename += '_%s_%d' % ( k , aggregationInfo [ k ] ) outputFilename += '.csv' outputFilename = os . path . join ( os . path . dirname ( inputFullPath ) , outputFilename ) lockFilePath = outputFilename + '.please_wait' if os . path . isfile ( outputFilename ) or os . path . isfile ( lockFilePath ) : while os . path . isfile ( lockFilePath ) : print 'Waiting for %s to be fully written by another process' % lockFilePath time . sleep ( 1 ) return outputFilename lockFD = open ( lockFilePath , 'w' ) outputObj = FileRecordStream ( streamID = outputFilename , write = True , fields = inputObj . getFields ( ) ) while True : inRecord = inputObj . getNextRecord ( ) ( aggRecord , aggBookmark ) = aggregator . next ( inRecord , None ) if aggRecord is None and inRecord is None : break if aggRecord is not None : outputObj . appendRecord ( aggRecord ) return outputFilename
9912	def send ( self ) : context = { "verification_url" : app_settings . EMAIL_VERIFICATION_URL . format ( key = self . key ) } email_utils . send_email ( context = context , from_email = settings . DEFAULT_FROM_EMAIL , recipient_list = [ self . email . email ] , subject = _ ( "Please Verify Your Email Address" ) , template_name = "rest_email_auth/emails/verify-email" , ) logger . info ( "Sent confirmation email to %s for user #%d" , self . email . email , self . email . user . id , )
810	def _finishLearning ( self ) : if self . _doSphering : self . _finishSphering ( ) self . _knn . finishLearning ( ) self . _accuracy = None
11210	def _set_tzdata ( self , tzobj ) : for attr in _tzfile . attrs : setattr ( self , '_' + attr , getattr ( tzobj , attr ) )
1378	def check_release_file_exists ( ) : release_file = get_heron_release_file ( ) if not os . path . isfile ( release_file ) : Log . error ( "Required file not found: %s" % release_file ) return False return True
5561	def init_bounds ( self ) : if self . _raw [ "init_bounds" ] is None : return self . bounds else : return Bounds ( * _validate_bounds ( self . _raw [ "init_bounds" ] ) )
1844	def JO ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . OF , target . read ( ) , cpu . PC )
13847	def splitext_files_only ( filepath ) : "Custom version of splitext that doesn't perform splitext on directories" return ( ( filepath , '' ) if os . path . isdir ( filepath ) else os . path . splitext ( filepath ) )
9532	def dumps ( obj , key = None , salt = 'django.core.signing' , serializer = JSONSerializer , compress = False ) : data = serializer ( ) . dumps ( obj ) is_compressed = False if compress : compressed = zlib . compress ( data ) if len ( compressed ) < ( len ( data ) - 1 ) : data = compressed is_compressed = True base64d = b64_encode ( data ) if is_compressed : base64d = b'.' + base64d return TimestampSigner ( key , salt = salt ) . sign ( base64d )
5990	def constant_regularization_matrix_from_pixel_neighbors ( coefficients , pixel_neighbors , pixel_neighbors_size ) : pixels = len ( pixel_neighbors ) regularization_matrix = np . zeros ( shape = ( pixels , pixels ) ) regularization_coefficient = coefficients [ 0 ] ** 2.0 for i in range ( pixels ) : regularization_matrix [ i , i ] += 1e-8 for j in range ( pixel_neighbors_size [ i ] ) : neighbor_index = pixel_neighbors [ i , j ] regularization_matrix [ i , i ] += regularization_coefficient regularization_matrix [ i , neighbor_index ] -= regularization_coefficient return regularization_matrix
5910	def delete_frames ( self ) : for frame in glob . glob ( self . frameglob ) : os . unlink ( frame )
13157	def count ( cls , cur , table : str , where_keys : list = None ) : if where_keys : where_clause , values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _count_query_where . format ( table , where_clause ) q , t = query , values else : query = cls . _count_query . format ( table ) q , t = query , ( ) yield from cur . execute ( q , t ) result = yield from cur . fetchone ( ) return int ( result [ 0 ] )
6210	def print_children ( data_file , group = '/' ) : base = data_file . get_node ( group ) print ( 'Groups in:\n %s\n' % base ) for node in base . _f_walk_groups ( ) : if node is not base : print ( ' %s' % node ) print ( '\nLeaf-nodes in %s:' % group ) for node in base . _v_leaves . itervalues ( ) : info = node . shape if len ( info ) == 0 : info = node . read ( ) print ( '\t%s, %s' % ( node . name , info ) ) if len ( node . title ) > 0 : print ( '\t %s' % node . title )
11048	def dataReceived ( self , data ) : self . resetTimeout ( ) lines = ( self . _buffer + data ) . splitlines ( ) if data . endswith ( b'\n' ) or data . endswith ( b'\r' ) : self . _buffer = b'' else : self . _buffer = lines . pop ( - 1 ) for line in lines : if self . transport . disconnecting : return if len ( line ) > self . _max_length : self . lineLengthExceeded ( line ) return else : self . lineReceived ( line ) if len ( self . _buffer ) > self . _max_length : self . lineLengthExceeded ( self . _buffer ) return
8869	def read_bgen ( filepath , metafile_filepath = None , samples_filepath = None , verbose = True ) : r assert_file_exist ( filepath ) assert_file_readable ( filepath ) metafile_filepath = _get_valid_metafile_filepath ( filepath , metafile_filepath ) if not os . path . exists ( metafile_filepath ) : if verbose : print ( f"We will create the metafile `{metafile_filepath}`. This file will " "speed up further\nreads and only need to be created once. So, please, " "bear with me." ) create_metafile ( filepath , metafile_filepath , verbose ) samples = get_samples ( filepath , samples_filepath , verbose ) variants = map_metadata ( filepath , metafile_filepath ) genotype = map_genotype ( filepath , metafile_filepath , verbose ) return dict ( variants = variants , samples = samples , genotype = genotype )
4065	def fields_types ( self , tname , qstring , itemtype ) : template_name = tname + itemtype query_string = qstring . format ( i = itemtype ) if self . templates . get ( template_name ) and not self . _updated ( query_string , self . templates [ template_name ] , template_name ) : return self . templates [ template_name ] [ "tmplt" ] retrieved = self . _retrieve_data ( query_string ) return self . _cache ( retrieved , template_name )
5871	def fetch_course_organizations ( course_key ) : queryset = internal . OrganizationCourse . objects . filter ( course_id = text_type ( course_key ) , active = True ) . select_related ( 'organization' ) return [ serializers . serialize_organization_with_course ( organization ) for organization in queryset ]
6608	def wait ( self ) : sleep = 5 while True : if self . clusterprocids_outstanding : self . poll ( ) if not self . clusterprocids_outstanding : break time . sleep ( sleep ) return self . clusterprocids_finished
12643	def set_config_value ( name , value ) : cli_config = CLIConfig ( SF_CLI_CONFIG_DIR , SF_CLI_ENV_VAR_PREFIX ) cli_config . set_value ( 'servicefabric' , name , value )
9470	def conference_list_members ( self , call_params ) : path = '/' + self . api_version + '/ConferenceListMembers/' method = 'POST' return self . request ( path , method , call_params )
5863	def add_organization_course ( organization_data , course_key ) : _validate_course_key ( course_key ) _validate_organization_data ( organization_data ) data . create_organization_course ( organization = organization_data , course_key = course_key )
1213	def _run_single ( self , thread_id , agent , environment , deterministic = False , max_episode_timesteps = - 1 , episode_finished = None , testing = False , sleep = None ) : old_episode_finished = False if episode_finished is not None and len ( getargspec ( episode_finished ) . args ) == 1 : old_episode_finished = True episode = 0 while not self . should_stop : state = environment . reset ( ) agent . reset ( ) self . global_timestep , self . global_episode = agent . timestep , agent . episode episode_reward = 0 time_step = 0 time_start = time . time ( ) while True : action , internals , states = agent . act ( states = state , deterministic = deterministic , buffered = False ) reward = 0 for repeat in xrange ( self . repeat_actions ) : state , terminal , step_reward = environment . execute ( action = action ) reward += step_reward if terminal : break if not testing : agent . atomic_observe ( states = state , actions = action , internals = internals , reward = reward , terminal = terminal ) if sleep is not None : time . sleep ( sleep ) time_step += 1 episode_reward += reward if terminal or time_step == max_episode_timesteps : break if self . should_stop : return self . global_timestep += time_step self . episode_list_lock . acquire ( ) self . episode_rewards . append ( episode_reward ) self . episode_timesteps . append ( time_step ) self . episode_times . append ( time . time ( ) - time_start ) self . episode_list_lock . release ( ) if episode_finished is not None : if old_episode_finished : summary_data = { "thread_id" : thread_id , "episode" : episode , "timestep" : time_step , "episode_reward" : episode_reward } if not episode_finished ( summary_data ) : return elif not episode_finished ( self , thread_id ) : return episode += 1
4454	def group_by ( self , fields , * reducers ) : group = Group ( fields , reducers ) self . _groups . append ( group ) return self
3654	def start_image_acquisition ( self ) : if not self . _create_ds_at_connection : self . _setup_data_streams ( ) num_required_buffers = self . _num_buffers for data_stream in self . _data_streams : try : num_buffers = data_stream . buffer_announce_min if num_buffers < num_required_buffers : num_buffers = num_required_buffers except InvalidParameterException as e : num_buffers = num_required_buffers self . _logger . debug ( e , exc_info = True ) if data_stream . defines_payload_size ( ) : buffer_size = data_stream . payload_size else : buffer_size = self . device . node_map . PayloadSize . value raw_buffers = self . _create_raw_buffers ( num_buffers , buffer_size ) buffer_tokens = self . _create_buffer_tokens ( raw_buffers ) self . _announced_buffers = self . _announce_buffers ( data_stream = data_stream , _buffer_tokens = buffer_tokens ) self . _queue_announced_buffers ( data_stream = data_stream , buffers = self . _announced_buffers ) try : acq_mode = self . device . node_map . AcquisitionMode . value if acq_mode == 'Continuous' : num_images_to_acquire = - 1 elif acq_mode == 'SingleFrame' : num_images_to_acquire = 1 elif acq_mode == 'MultiFrame' : num_images_to_acquire = self . device . node_map . AcquisitionFrameCount . value else : num_images_to_acquire = - 1 except LogicalErrorException as e : num_images_to_acquire = - 1 self . _logger . debug ( e , exc_info = True ) self . _num_images_to_acquire = num_images_to_acquire try : self . device . node_map . TLParamsLocked . value = 1 except LogicalErrorException : pass self . _is_acquiring_images = True for data_stream in self . _data_streams : data_stream . start_acquisition ( ACQ_START_FLAGS_LIST . ACQ_START_FLAGS_DEFAULT , self . _num_images_to_acquire ) if self . thread_image_acquisition : self . thread_image_acquisition . start ( ) self . device . node_map . AcquisitionStart . execute ( ) self . _logger . info ( '{0} started image acquisition.' . format ( self . _device . id_ ) ) if self . _profiler : self . _profiler . print_diff ( )
3519	def matomo ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return MatomoNode ( )
12381	def delete ( self , request , response ) : if self . slug is None : raise http . exceptions . NotImplemented ( ) self . assert_operations ( 'destroy' ) self . destroy ( ) self . response . status = http . client . NO_CONTENT self . make_response ( )
12085	def folderScan ( self , abfFolder = None ) : if abfFolder is None and 'abfFolder' in dir ( self ) : abfFolder = self . abfFolder else : self . abfFolder = abfFolder self . abfFolder = os . path . abspath ( self . abfFolder ) self . log . info ( "scanning [%s]" , self . abfFolder ) if not os . path . exists ( self . abfFolder ) : self . log . error ( "path doesn't exist: [%s]" , abfFolder ) return self . abfFolder2 = os . path . abspath ( self . abfFolder + "/swhlab/" ) if not os . path . exists ( self . abfFolder2 ) : self . log . error ( "./swhlab/ doesn't exist. creating it..." ) os . mkdir ( self . abfFolder2 ) self . fnames = os . listdir ( self . abfFolder ) self . fnames2 = os . listdir ( self . abfFolder2 ) self . log . debug ( "./ has %d files" , len ( self . fnames ) ) self . log . debug ( "./swhlab/ has %d files" , len ( self . fnames2 ) ) self . fnamesByExt = filesByExtension ( self . fnames ) if not "abf" in self . fnamesByExt . keys ( ) : self . log . error ( "no ABF files found" ) self . log . debug ( "found %d ABFs" , len ( self . fnamesByExt [ "abf" ] ) ) self . cells = findCells ( self . fnames ) self . log . debug ( "found %d cells" % len ( self . cells ) ) self . fnamesByCell = filesByCell ( self . fnames , self . cells ) self . log . debug ( "grouped cells by number of source files: %s" % str ( [ len ( self . fnamesByCell [ elem ] ) for elem in self . fnamesByCell ] ) )
3917	def from_conversation_event ( conversation , conv_event , prev_conv_event , datetimefmt , watermark_users = None ) : user = conversation . get_user ( conv_event . user_id ) if prev_conv_event is not None : is_new_day = ( conv_event . timestamp . astimezone ( tz = None ) . date ( ) != prev_conv_event . timestamp . astimezone ( tz = None ) . date ( ) ) else : is_new_day = False if isinstance ( conv_event , hangups . ChatMessageEvent ) : return MessageWidget ( conv_event . timestamp , conv_event . text , datetimefmt , user , show_date = is_new_day , watermark_users = watermark_users ) elif isinstance ( conv_event , hangups . RenameEvent ) : if conv_event . new_name == '' : text = ( '{} cleared the conversation name' . format ( user . first_name ) ) else : text = ( '{} renamed the conversation to {}' . format ( user . first_name , conv_event . new_name ) ) return MessageWidget ( conv_event . timestamp , text , datetimefmt , show_date = is_new_day , watermark_users = watermark_users ) elif isinstance ( conv_event , hangups . MembershipChangeEvent ) : event_users = [ conversation . get_user ( user_id ) for user_id in conv_event . participant_ids ] names = ', ' . join ( [ user . full_name for user in event_users ] ) if conv_event . type_ == hangups . MEMBERSHIP_CHANGE_TYPE_JOIN : text = ( '{} added {} to the conversation' . format ( user . first_name , names ) ) else : text = ( '{} left the conversation' . format ( names ) ) return MessageWidget ( conv_event . timestamp , text , datetimefmt , show_date = is_new_day , watermark_users = watermark_users ) elif isinstance ( conv_event , hangups . HangoutEvent ) : text = { hangups . HANGOUT_EVENT_TYPE_START : ( 'A Hangout call is starting.' ) , hangups . HANGOUT_EVENT_TYPE_END : ( 'A Hangout call ended.' ) , hangups . HANGOUT_EVENT_TYPE_ONGOING : ( 'A Hangout call is ongoing.' ) , } . get ( conv_event . event_type , 'Unknown Hangout call event.' ) return MessageWidget ( conv_event . timestamp , text , datetimefmt , show_date = is_new_day , watermark_users = watermark_users ) elif isinstance ( conv_event , hangups . GroupLinkSharingModificationEvent ) : status_on = hangups . GROUP_LINK_SHARING_STATUS_ON status_text = ( 'on' if conv_event . new_status == status_on else 'off' ) text = '{} turned {} joining by link.' . format ( user . first_name , status_text ) return MessageWidget ( conv_event . timestamp , text , datetimefmt , show_date = is_new_day , watermark_users = watermark_users ) else : text = 'Unknown conversation event' return MessageWidget ( conv_event . timestamp , text , datetimefmt , show_date = is_new_day , watermark_users = watermark_users )
499	def _deleteRecordsFromKNN ( self , recordsToDelete ) : prototype_idx = self . _knnclassifier . getParameter ( 'categoryRecencyList' ) idsToDelete = ( [ r . ROWID for r in recordsToDelete if not r . setByUser and r . ROWID in prototype_idx ] ) nProtos = self . _knnclassifier . _knn . _numPatterns self . _knnclassifier . _knn . removeIds ( idsToDelete ) assert self . _knnclassifier . _knn . _numPatterns == nProtos - len ( idsToDelete )
8707	def exec_file ( self , path ) : filename = os . path . basename ( path ) log . info ( 'Execute %s' , filename ) content = from_file ( path ) . replace ( '\r' , '' ) . split ( '\n' ) res = '> ' for line in content : line = line . rstrip ( '\n' ) retlines = ( res + self . __exchange ( line ) ) . splitlines ( ) res = retlines . pop ( ) for lin in retlines : log . info ( lin ) log . info ( res )
2616	def initialize_boto_client ( self ) : self . session = self . create_session ( ) self . client = self . session . client ( 'ec2' ) self . ec2 = self . session . resource ( 'ec2' ) self . instances = [ ] self . instance_states = { } self . vpc_id = 0 self . sg_id = 0 self . sn_ids = [ ]
13071	def r_first_passage ( self , objectId ) : collection , reffs = self . get_reffs ( objectId = objectId , export_collection = True ) first , _ = reffs [ 0 ] return redirect ( url_for ( ".r_passage_semantic" , objectId = objectId , subreference = first , semantic = self . semantic ( collection ) ) )
7733	def make_kick_request ( self , nick , reason ) : self . clear_muc_child ( ) self . muc_child = MucAdminQuery ( parent = self . xmlnode ) item = MucItem ( "none" , "none" , nick = nick , reason = reason ) self . muc_child . add_item ( item ) return self . muc_child
8840	def missing_some ( data , min_required , args ) : if min_required < 1 : return [ ] found = 0 not_found = object ( ) ret = [ ] for arg in args : if get_var ( data , arg , not_found ) is not_found : ret . append ( arg ) else : found += 1 if found >= min_required : return [ ] return ret
7514	def enter_pairs ( iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ) : LOGGER . info ( "edges in enter_pairs %s" , edg ) seq1 = aseqs [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] snp1 = asnps [ iloc , edg [ 0 ] : edg [ 1 ] + 1 , ] seq2 = aseqs [ iloc , : , edg [ 2 ] : edg [ 3 ] + 1 ] snp2 = asnps [ iloc , edg [ 2 ] : edg [ 3 ] + 1 , ] nalln = np . all ( seq1 == "N" , axis = 1 ) nsidx = nalln + smask LOGGER . info ( "nsidx %s, nalln %s, smask %s" , nsidx , nalln , smask ) samplecov = samplecov + np . invert ( nsidx ) . astype ( np . int32 ) LOGGER . info ( "samplecov %s" , samplecov ) idx = np . sum ( np . invert ( nsidx ) . astype ( np . int32 ) ) LOGGER . info ( "idx %s" , idx ) locuscov [ idx ] += 1 seq1 = seq1 [ ~ nsidx , ] seq2 = seq2 [ ~ nsidx , ] names = pnames [ ~ nsidx ] outstr = "\n" . join ( [ name + s1 . tostring ( ) + "nnnn" + s2 . tostring ( ) for name , s1 , s2 in zip ( names , seq1 , seq2 ) ] ) snpstring1 = [ "-" if snp1 [ i , 0 ] else "*" if snp1 [ i , 1 ] else " " for i in range ( len ( snp1 ) ) ] snpstring2 = [ "-" if snp2 [ i , 0 ] else "*" if snp2 [ i , 1 ] else " " for i in range ( len ( snp2 ) ) ] outstr += "\n" + snppad + "" . join ( snpstring1 ) + " " + "" . join ( snpstring2 ) + "|{}|" . format ( iloc + start ) return outstr , samplecov , locuscov
8018	async def disconnect ( self , code ) : try : await asyncio . wait ( self . application_futures . values ( ) , return_when = asyncio . ALL_COMPLETED , timeout = self . application_close_timeout ) except asyncio . TimeoutError : pass
9149	def count_relations ( self ) -> int : if self . edge_model is ... : raise Bio2BELMissingEdgeModelError ( 'edge_edge model is undefined/count_bel_relations is not overridden' ) elif isinstance ( self . edge_model , list ) : return sum ( self . _count_model ( m ) for m in self . edge_model ) else : return self . _count_model ( self . edge_model )
6548	def move_to ( self , ypos , xpos ) : xpos -= 1 ypos -= 1 self . exec_command ( "MoveCursor({0}, {1})" . format ( ypos , xpos ) . encode ( "ascii" ) )
3039	def has_scopes ( self , scopes ) : scopes = _helpers . string_to_scopes ( scopes ) return set ( scopes ) . issubset ( self . scopes )
12612	def search_unique ( self , table_name , sample , unique_fields = None ) : return search_unique ( table = self . table ( table_name ) , sample = sample , unique_fields = unique_fields )
12719	def axes ( self ) : return [ np . array ( self . ode_obj . getAxis ( i ) ) for i in range ( self . ADOF or self . LDOF ) ]
9907	def send_confirmation ( self ) : confirmation = EmailConfirmation . objects . create ( email = self ) confirmation . send ( )
2883	def to_html_string ( self ) : html = ET . Element ( 'html' ) head = ET . SubElement ( html , 'head' ) title = ET . SubElement ( head , 'title' ) title . text = self . description body = ET . SubElement ( html , 'body' ) h1 = ET . SubElement ( body , 'h1' ) h1 . text = self . description span = ET . SubElement ( body , 'span' ) span . text = ' CONTENT ' html_text = ET . tostring ( html ) svg_content = '' svg_done = set ( ) for spec in self . get_specs_depth_first ( ) : if spec . svg and spec . svg not in svg_done : svg_content += '<p>' + spec . svg + "</p>" svg_done . add ( spec . svg ) return html_text . replace ( ' CONTENT ' , svg_content )
13042	def create_query ( section ) : query = { } if 'ports' in section : query [ 'ports' ] = [ section [ 'ports' ] ] if 'up' in section : query [ 'up' ] = bool ( section [ 'up' ] ) if 'search' in section : query [ 'search' ] = [ section [ 'search' ] ] if 'tags' in section : query [ 'tags' ] = [ section [ 'tags' ] ] if 'groups' in section : query [ 'groups' ] = [ section [ 'groups' ] ] return query
9989	def import_funcs ( self , module ) : newcells = self . _impl . new_cells_from_module ( module ) return get_interfaces ( newcells )
9201	def extract_cycles ( series , left = False , right = False ) : points = deque ( ) for x in reversals ( series , left = left , right = right ) : points . append ( x ) while len ( points ) >= 3 : X = abs ( points [ - 2 ] - points [ - 1 ] ) Y = abs ( points [ - 3 ] - points [ - 2 ] ) if X < Y : break elif len ( points ) == 3 : yield points [ 0 ] , points [ 1 ] , 0.5 points . popleft ( ) else : yield points [ - 3 ] , points [ - 2 ] , 1.0 last = points . pop ( ) points . pop ( ) points . pop ( ) points . append ( last ) else : while len ( points ) > 1 : yield points [ 0 ] , points [ 1 ] , 0.5 points . popleft ( )
9115	def fs_cleansed_attachments ( self ) : if exists ( self . fs_cleansed_attachment_container ) : return [ join ( self . fs_cleansed_attachment_container , attachment ) for attachment in listdir ( self . fs_cleansed_attachment_container ) ] else : return [ ]
9812	def revoke ( username ) : try : PolyaxonClient ( ) . user . revoke_superuser ( username ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not revoke superuser role from user `{}`.' . format ( username ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Superuser role was revoked successfully from user `{}`." . format ( username ) )
285	def plot_drawdown_underwater ( returns , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . percentage ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) df_cum_rets = ep . cum_returns ( returns , starting_value = 1.0 ) running_max = np . maximum . accumulate ( df_cum_rets ) underwater = - 100 * ( ( running_max - df_cum_rets ) / running_max ) ( underwater ) . plot ( ax = ax , kind = 'area' , color = 'coral' , alpha = 0.7 , ** kwargs ) ax . set_ylabel ( 'Drawdown' ) ax . set_title ( 'Underwater plot' ) ax . set_xlabel ( '' ) return ax
5893	def handle_upload ( self , request ) : if request . method != 'POST' : raise Http404 if request . is_ajax ( ) : try : filename = request . GET [ 'quillUploadFile' ] data = request is_raw = True except KeyError : return HttpResponseBadRequest ( "Invalid file upload." ) else : if len ( request . FILES ) != 1 : return HttpResponseBadRequest ( "Can only upload 1 file at a time." ) try : data = request . FILES [ 'quillUploadFile' ] filename = data . name is_raw = False except KeyError : return HttpResponseBadRequest ( 'Missing image `quillUploadFile`.' ) url = save_file ( data , filename , is_raw , default_storage ) response_data = { } response_data [ 'url' ] = url return HttpResponse ( json . dumps ( response_data ) , content_type = "text/html; charset=utf-8" )
9925	def get_queryset ( self ) : oldest = timezone . now ( ) - app_settings . PASSWORD_RESET_EXPIRATION queryset = super ( ValidPasswordResetTokenManager , self ) . get_queryset ( ) return queryset . filter ( created_at__gt = oldest )
889	def _leastUsedCell ( cls , random , cells , connections ) : leastUsedCells = [ ] minNumSegments = float ( "inf" ) for cell in cells : numSegments = connections . numSegments ( cell ) if numSegments < minNumSegments : minNumSegments = numSegments leastUsedCells = [ ] if numSegments == minNumSegments : leastUsedCells . append ( cell ) i = random . getUInt32 ( len ( leastUsedCells ) ) return leastUsedCells [ i ]
7597	def get_top_war_clans ( self , country_key = '' , ** params : keys ) : url = self . api . TOP + '/war/' + str ( country_key ) return self . _get_model ( url , PartialClan , ** params )
4236	def login ( self ) : if not self . force_login_v2 : v1_result = self . login_v1 ( ) if v1_result : return v1_result return self . login_v2 ( )
9949	def new_space ( self , name = None , bases = None , formula = None , * , refs = None , source = None , is_derived = False , prefix = "" ) : from modelx . core . space import StaticSpaceImpl if name is None : name = self . spacenamer . get_next ( self . namespace , prefix ) if name in self . namespace : raise ValueError ( "Name '%s' already exists." % name ) if not prefix and not is_valid_name ( name ) : raise ValueError ( "Invalid name '%s'." % name ) space = self . _new_space ( name = name , formula = formula , refs = refs , source = source , is_derived = is_derived , ) self . _set_space ( space ) self . model . spacegraph . add_space ( space ) if bases is not None : if isinstance ( bases , StaticSpaceImpl ) : bases = [ bases ] space . add_bases ( bases ) return space
11109	def walk_directories_info ( self , relativePath = "" ) : def walk_directories ( directory , relativePath ) : directories = dict . __getitem__ ( directory , 'directories' ) for fname in sorted ( directories ) : info = dict . __getitem__ ( directories , fname ) yield os . path . join ( relativePath , fname ) , info for k in sorted ( dict . keys ( directories ) ) : path = os . path . join ( relativePath , k ) dir = dict . __getitem__ ( directories , k ) for e in walk_directories ( dir , path ) : yield e dir , errorMessage = self . get_directory_info ( relativePath ) assert dir is not None , errorMessage return walk_directories ( dir , relativePath = '' )
6775	def force_stop_and_purge ( self ) : r = self . local_renderer self . stop ( ) with settings ( warn_only = True ) : r . sudo ( 'killall rabbitmq-server' ) with settings ( warn_only = True ) : r . sudo ( 'killall beam.smp' ) r . sudo ( 'rm -Rf /var/lib/rabbitmq/mnesia/*' )
10771	def contour ( self , level ) : if not isinstance ( level , numbers . Number ) : raise TypeError ( ( "'_level' must be of type 'numbers.Number' but is " "'{:s}'" ) . format ( type ( level ) ) ) vertices = self . _contour_generator . create_contour ( level ) return self . formatter ( level , vertices )
13539	def get_locations ( self ) : url = "/2/locations" data = self . _get_resource ( url ) locations = [ ] for entry in data [ 'locations' ] : locations . append ( self . location_from_json ( entry ) ) return locations
1511	def start_heron_tools ( masters , cl_args ) : single_master = list ( masters ) [ 0 ] wait_for_master_to_start ( single_master ) cmd = "%s run %s >> /tmp/heron_tools_start.log 2>&1 &" % ( get_nomad_path ( cl_args ) , get_heron_tools_job_file ( cl_args ) ) Log . info ( "Starting Heron Tools on %s" % single_master ) if not is_self ( single_master ) : cmd = ssh_remote_execute ( cmd , single_master , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) if return_code != 0 : Log . error ( "Failed to start Heron Tools on %s with error:\n%s" % ( single_master , output [ 1 ] ) ) sys . exit ( - 1 ) wait_for_job_to_start ( single_master , "heron-tools" ) Log . info ( "Done starting Heron Tools" )
8560	def get_lan_members ( self , datacenter_id , lan_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/lans/%s/nics?depth=%s' % ( datacenter_id , lan_id , str ( depth ) ) ) return response
5473	def trim_display_field ( self , value , max_length ) : if not value : return '' if len ( value ) > max_length : return value [ : max_length - 3 ] + '...' return value
6552	def check ( self , solution ) : return self . func ( * ( solution [ v ] for v in self . variables ) )
10543	def create_task ( project_id , info , n_answers = 30 , priority_0 = 0 , quorum = 0 ) : try : task = dict ( project_id = project_id , info = info , calibration = 0 , priority_0 = priority_0 , n_answers = n_answers , quorum = quorum ) res = _pybossa_req ( 'post' , 'task' , payload = task ) if res . get ( 'id' ) : return Task ( res ) else : return res except : raise
4430	async def _remove ( self , ctx , index : int ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue : return await ctx . send ( 'Nothing queued.' ) if index > len ( player . queue ) or index < 1 : return await ctx . send ( f'Index has to be **between** 1 and {len(player.queue)}' ) index -= 1 removed = player . queue . pop ( index ) await ctx . send ( f'Removed **{removed.title}** from the queue.' )
7148	def with_payment_id ( self , payment_id = 0 ) : payment_id = numbers . PaymentID ( payment_id ) if not payment_id . is_short ( ) : raise TypeError ( "Payment ID {0} has more than 64 bits and cannot be integrated" . format ( payment_id ) ) prefix = 54 if self . is_testnet ( ) else 25 if self . is_stagenet ( ) else 19 data = bytearray ( [ prefix ] ) + self . _decoded [ 1 : 65 ] + struct . pack ( '>Q' , int ( payment_id ) ) checksum = bytearray ( keccak_256 ( data ) . digest ( ) [ : 4 ] ) return IntegratedAddress ( base58 . encode ( hexlify ( data + checksum ) ) )
8102	def open_socket ( self ) : self . socket = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) self . socket . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) self . socket . setblocking ( 0 ) self . socket . bind ( ( self . host , self . port ) )
1496	def get_sub_parts ( self , query ) : parts = [ ] num_open_braces = 0 delimiter = ',' last_starting_index = 0 for i in range ( len ( query ) ) : if query [ i ] == '(' : num_open_braces += 1 elif query [ i ] == ')' : num_open_braces -= 1 elif query [ i ] == delimiter and num_open_braces == 0 : parts . append ( query [ last_starting_index : i ] . strip ( ) ) last_starting_index = i + 1 parts . append ( query [ last_starting_index : ] . strip ( ) ) return parts
2347	def phrase_to_filename ( self , phrase ) : name = re . sub ( r"[^\w\s\.]" , '' , phrase . strip ( ) . lower ( ) ) name = re . sub ( r"\s+" , '_' , name ) return name + '.png'
9001	def build_SVG_dict ( self ) : zoom = self . _zoom layout = self . _layout builder = self . _builder bbox = list ( map ( lambda f : f * zoom , layout . bounding_box ) ) builder . bounding_box = bbox flip_x = bbox [ 2 ] + bbox [ 0 ] * 2 flip_y = bbox [ 3 ] + bbox [ 1 ] * 2 instructions = list ( layout . walk_instructions ( lambda i : ( flip_x - ( i . x + i . width ) * zoom , flip_y - ( i . y + i . height ) * zoom , i . instruction ) ) ) instructions . sort ( key = lambda x_y_i : x_y_i [ 2 ] . render_z ) for x , y , instruction in instructions : render_z = instruction . render_z z_id = ( "" if not render_z else "-{}" . format ( render_z ) ) layer_id = "row-{}{}" . format ( instruction . row . id , z_id ) def_id = self . _register_instruction_in_defs ( instruction ) scale = self . _symbol_id_to_scale [ def_id ] group = { "@class" : "instruction" , "@id" : "instruction-{}" . format ( instruction . id ) , "@transform" : "translate({},{}),scale({})" . format ( x , y , scale ) } builder . place_svg_use ( def_id , layer_id , group ) builder . insert_defs ( self . _instruction_type_color_to_symbol . values ( ) ) return builder . get_svg_dict ( )
2465	def set_file_chksum ( self , doc , chksum ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_chksum_set : self . file_chksum_set = True self . file ( doc ) . chk_sum = checksum_from_sha1 ( chksum ) return True else : raise CardinalityError ( 'File::CheckSum' ) else : raise OrderError ( 'File::CheckSum' )
3417	def _cell ( x ) : x_no_none = [ i if i is not None else "" for i in x ] return array ( x_no_none , dtype = np_object )
616	def expGenerator ( args ) : parser = OptionParser ( ) parser . set_usage ( "%prog [options] --description='{json object with args}'\n" + "%prog [options] --descriptionFromFile='{filename}'\n" + "%prog [options] --showSchema" ) parser . add_option ( "--description" , dest = "description" , help = "Tells ExpGenerator to generate an experiment description.py and " "permutations.py file using the given JSON formatted experiment " "description string." ) parser . add_option ( "--descriptionFromFile" , dest = 'descriptionFromFile' , help = "Tells ExpGenerator to open the given filename and use it's " "contents as the JSON formatted experiment description." ) parser . add_option ( "--claDescriptionTemplateFile" , dest = 'claDescriptionTemplateFile' , default = 'claDescriptionTemplate.tpl' , help = "The file containing the template description file for " " ExpGenerator [default: %default]" ) parser . add_option ( "--showSchema" , action = "store_true" , dest = "showSchema" , help = "Prints the JSON schemas for the --description arg." ) parser . add_option ( "--version" , dest = 'version' , default = 'v2' , help = "Generate the permutations file for this version of hypersearch." " Possible choices are 'v1' and 'v2' [default: %default]." ) parser . add_option ( "--outDir" , dest = "outDir" , default = None , help = "Where to generate experiment. If not specified, " "then a temp directory will be created" ) ( options , remainingArgs ) = parser . parse_args ( args ) if len ( remainingArgs ) > 0 : raise _InvalidCommandArgException ( _makeUsageErrorStr ( "Unexpected command-line args: <%s>" % ( ' ' . join ( remainingArgs ) , ) , parser . get_usage ( ) ) ) activeOptions = filter ( lambda x : getattr ( options , x ) != None , ( 'description' , 'showSchema' ) ) if len ( activeOptions ) > 1 : raise _InvalidCommandArgException ( _makeUsageErrorStr ( ( "The specified command options are " + "mutually-exclusive: %s" ) % ( activeOptions , ) , parser . get_usage ( ) ) ) if options . showSchema : _handleShowSchemaOption ( ) elif options . description : _handleDescriptionOption ( options . description , options . outDir , parser . get_usage ( ) , hsVersion = options . version , claDescriptionTemplateFile = options . claDescriptionTemplateFile ) elif options . descriptionFromFile : _handleDescriptionFromFileOption ( options . descriptionFromFile , options . outDir , parser . get_usage ( ) , hsVersion = options . version , claDescriptionTemplateFile = options . claDescriptionTemplateFile ) else : raise _InvalidCommandArgException ( _makeUsageErrorStr ( "Error in validating command options. No option " "provided:\n" , parser . get_usage ( ) ) )
11306	def store_providers ( self , provider_data ) : if not hasattr ( provider_data , '__iter__' ) : raise OEmbedException ( 'Autodiscovered response not iterable' ) provider_pks = [ ] for provider in provider_data : if 'endpoint' not in provider or 'matches' not in provider : continue resource_type = provider . get ( 'type' ) if resource_type not in RESOURCE_TYPES : continue stored_provider , created = StoredProvider . objects . get_or_create ( wildcard_regex = provider [ 'matches' ] ) if created : stored_provider . endpoint_url = relative_to_full ( provider [ 'endpoint' ] , provider [ 'matches' ] ) stored_provider . resource_type = resource_type stored_provider . save ( ) provider_pks . append ( stored_provider . pk ) return StoredProvider . objects . filter ( pk__in = provider_pks )
4426	async def _play ( self , ctx , * , query : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) query = query . strip ( '<>' ) if not url_rx . match ( query ) : query = f'ytsearch:{query}' tracks = await self . bot . lavalink . get_tracks ( query ) if not tracks : return await ctx . send ( 'Nothing found!' ) embed = discord . Embed ( color = discord . Color . blurple ( ) ) if 'list' in query and 'ytsearch:' not in query : for track in tracks : player . add ( requester = ctx . author . id , track = track ) embed . title = 'Playlist enqueued!' embed . description = f'Imported {len(tracks)} tracks from the playlist!' await ctx . send ( embed = embed ) else : track_title = tracks [ 0 ] [ "info" ] [ "title" ] track_uri = tracks [ 0 ] [ "info" ] [ "uri" ] embed . title = "Track enqueued!" embed . description = f'[{track_title}]({track_uri})' player . add ( requester = ctx . author . id , track = tracks [ 0 ] ) if not player . is_playing : await player . play ( )
6454	def stem ( self , word ) : word = normalize ( 'NFC' , text_type ( word . lower ( ) ) ) word = word . translate ( self . _accents ) wlen = len ( word ) - 1 if wlen > 4 and word [ - 3 : ] == 'ern' : word = word [ : - 3 ] elif wlen > 3 and word [ - 2 : ] in { 'em' , 'en' , 'er' , 'es' } : word = word [ : - 2 ] elif wlen > 2 and ( word [ - 1 ] == 'e' or ( word [ - 1 ] == 's' and word [ - 2 ] in self . _st_ending ) ) : word = word [ : - 1 ] wlen = len ( word ) - 1 if wlen > 4 and word [ - 3 : ] == 'est' : word = word [ : - 3 ] elif wlen > 3 and ( word [ - 2 : ] in { 'er' , 'en' } or ( word [ - 2 : ] == 'st' and word [ - 3 ] in self . _st_ending ) ) : word = word [ : - 2 ] return word
11501	def list_communities ( self , token = None ) : parameters = dict ( ) if token : parameters [ 'token' ] = token response = self . request ( 'midas.community.list' , parameters ) return response
5301	def parse_colors ( path ) : if path . endswith ( ".txt" ) : return parse_rgb_txt_file ( path ) elif path . endswith ( ".json" ) : return parse_json_color_file ( path ) raise TypeError ( "colorful only supports .txt and .json files for colors" )
4114	def rc2lar ( k ) : assert numpy . isrealobj ( k ) , 'Log area ratios not defined for complex reflection coefficients.' if max ( numpy . abs ( k ) ) >= 1 : raise ValueError ( 'All reflection coefficients should have magnitude less than unity.' ) return - 2 * numpy . arctanh ( - numpy . array ( k ) )
12707	def rotation ( self , rotation ) : if isinstance ( rotation , np . ndarray ) : rotation = rotation . ravel ( ) self . ode_body . setRotation ( tuple ( rotation ) )
419	def save_training_log ( self , ** kwargs ) : self . _fill_project_info ( kwargs ) kwargs . update ( { 'time' : datetime . utcnow ( ) } ) _result = self . db . TrainLog . insert_one ( kwargs ) _log = self . _print_dict ( kwargs ) logging . info ( "[Database] train log: " + _log )
3918	def _handle_event ( self , conv_event ) : if not self . _is_scrolling : self . set_focus ( conv_event . id_ ) else : self . _modified ( )
11806	def viterbi_segment ( text , P ) : n = len ( text ) words = [ '' ] + list ( text ) best = [ 1.0 ] + [ 0.0 ] * n for i in range ( n + 1 ) : for j in range ( 0 , i ) : w = text [ j : i ] if P [ w ] * best [ i - len ( w ) ] >= best [ i ] : best [ i ] = P [ w ] * best [ i - len ( w ) ] words [ i ] = w sequence = [ ] i = len ( words ) - 1 while i > 0 : sequence [ 0 : 0 ] = [ words [ i ] ] i = i - len ( words [ i ] ) return sequence , best [ - 1 ]
9645	def _get_detail_value ( var , attr ) : value = getattr ( var , attr ) kls = getattr ( getattr ( value , '__class__' , '' ) , '__name__' , '' ) if kls in ( 'ManyRelatedManager' , 'RelatedManager' , 'EmptyManager' ) : return kls if callable ( value ) : return 'routine' return value
9430	def _extract_members ( self , members , targetpath , pwd ) : archive = unrarlib . RAROpenArchiveDataEx ( self . filename , mode = constants . RAR_OM_EXTRACT ) handle = self . _open ( archive ) password = pwd or self . pwd if password is not None : unrarlib . RARSetPassword ( handle , b ( password ) ) try : rarinfo = self . _read_header ( handle ) while rarinfo is not None : if rarinfo . filename in members : self . _process_current ( handle , constants . RAR_EXTRACT , targetpath ) else : self . _process_current ( handle , constants . RAR_SKIP ) rarinfo = self . _read_header ( handle ) except unrarlib . MissingPassword : raise RuntimeError ( "File is encrypted, password required" ) except unrarlib . BadPassword : raise RuntimeError ( "Bad password for File" ) except unrarlib . BadDataError : raise RuntimeError ( "File CRC Error" ) except unrarlib . UnrarException as e : raise BadRarFile ( "Bad RAR archive data: %s" % str ( e ) ) finally : self . _close ( handle )
9090	def _get_default_namespace ( self ) -> Optional [ Namespace ] : return self . _get_query ( Namespace ) . filter ( Namespace . url == self . _get_namespace_url ( ) ) . one_or_none ( )
773	def generateStats ( filename , maxSamples = None , ) : statsCollectorMapping = { 'float' : FloatStatsCollector , 'int' : IntStatsCollector , 'string' : StringStatsCollector , 'datetime' : DateTimeStatsCollector , 'bool' : BoolStatsCollector , } filename = resource_filename ( "nupic.datafiles" , filename ) print "*" * 40 print "Collecting statistics for file:'%s'" % ( filename , ) dataFile = FileRecordStream ( filename ) statsCollectors = [ ] for fieldName , fieldType , fieldSpecial in dataFile . getFields ( ) : statsCollector = statsCollectorMapping [ fieldType ] ( fieldName , fieldType , fieldSpecial ) statsCollectors . append ( statsCollector ) if maxSamples is None : maxSamples = 500000 for i in xrange ( maxSamples ) : record = dataFile . getNextRecord ( ) if record is None : break for i , value in enumerate ( record ) : statsCollectors [ i ] . addValue ( value ) stats = { } for statsCollector in statsCollectors : statsCollector . getStats ( stats ) if dataFile . getResetFieldIdx ( ) is not None : resetFieldName , _ , _ = dataFile . getFields ( ) [ dataFile . reset ] stats . pop ( resetFieldName ) if VERBOSITY > 0 : pprint . pprint ( stats ) return stats
6762	def write_pgpass ( self , name = None , site = None , use_sudo = 0 , root = 0 ) : r = self . database_renderer ( name = name , site = site ) root = int ( root ) use_sudo = int ( use_sudo ) r . run ( 'touch {pgpass_path}' ) if '~' in r . env . pgpass_path : r . run ( 'chmod {pgpass_chmod} {pgpass_path}' ) else : r . sudo ( 'chmod {pgpass_chmod} {pgpass_path}' ) if root : r . env . shell_username = r . env . get ( 'db_root_username' , 'postgres' ) r . env . shell_password = r . env . get ( 'db_root_password' , 'password' ) else : r . env . shell_username = r . env . db_user r . env . shell_password = r . env . db_password r . append ( '{db_host}:{port}:*:{shell_username}:{shell_password}' , r . env . pgpass_path , use_sudo = use_sudo )
4845	def _load_data ( self , resource , default = DEFAULT_VALUE_SAFEGUARD , ** kwargs ) : default_val = default if default != self . DEFAULT_VALUE_SAFEGUARD else { } try : return get_edx_api_data ( api_config = CatalogIntegration . current ( ) , resource = resource , api = self . client , ** kwargs ) or default_val except ( SlumberBaseException , ConnectionError , Timeout ) as exc : LOGGER . exception ( 'Failed to load data from resource [%s] with kwargs [%s] due to: [%s]' , resource , kwargs , str ( exc ) ) return default_val
5230	def to_hour ( num ) -> str : to_str = str ( int ( num ) ) return pd . Timestamp ( f'{to_str[:-2]}:{to_str[-2:]}' ) . strftime ( '%H:%M' )
4046	def num_tagitems ( self , tag ) : query = "/{t}/{u}/tags/{ta}/items" . format ( u = self . library_id , t = self . library_type , ta = tag ) return self . _totals ( query )
9362	def text ( what = "sentence" , * args , ** kwargs ) : if what == "character" : return character ( * args , ** kwargs ) elif what == "characters" : return characters ( * args , ** kwargs ) elif what == "word" : return word ( * args , ** kwargs ) elif what == "words" : return words ( * args , ** kwargs ) elif what == "sentence" : return sentence ( * args , ** kwargs ) elif what == "sentences" : return sentences ( * args , ** kwargs ) elif what == "paragraph" : return paragraph ( * args , ** kwargs ) elif what == "paragraphs" : return paragraphs ( * args , ** kwargs ) elif what == "title" : return title ( * args , ** kwargs ) else : raise NameError ( 'No such method' )
4724	def main ( conf ) : fpath = yml_fpath ( conf [ "OUTPUT" ] ) if os . path . exists ( fpath ) : cij . err ( "main:FAILED { fpath: %r }, exists" % fpath ) return 1 trun = trun_setup ( conf ) if not trun : return 1 trun_to_file ( trun ) trun_emph ( trun ) tr_err = 0 tr_ent_err = trun_enter ( trun ) for tsuite in ( ts for ts in trun [ "testsuites" ] if not tr_ent_err ) : ts_err = 0 ts_ent_err = tsuite_enter ( trun , tsuite ) for tcase in ( tc for tc in tsuite [ "testcases" ] if not ts_ent_err ) : tc_err = tcase_enter ( trun , tsuite , tcase ) if not tc_err : tc_err += script_run ( trun , tcase ) tc_err += tcase_exit ( trun , tsuite , tcase ) tcase [ "status" ] = "FAIL" if tc_err else "PASS" trun [ "progress" ] [ tcase [ "status" ] ] += 1 trun [ "progress" ] [ "UNKN" ] -= 1 ts_err += tc_err trun_to_file ( trun ) if not ts_ent_err : ts_err += tsuite_exit ( trun , tsuite ) ts_err += ts_ent_err tr_err += ts_err tsuite [ "status" ] = "FAIL" if ts_err else "PASS" cij . emph ( "rnr:tsuite %r" % tsuite [ "status" ] , tsuite [ "status" ] != "PASS" ) if not tr_ent_err : trun_exit ( trun ) tr_err += tr_ent_err trun [ "status" ] = "FAIL" if tr_err else "PASS" trun [ "stamp" ] [ "end" ] = int ( time . time ( ) ) + 1 trun_to_file ( trun ) cij . emph ( "rnr:main:progress %r" % trun [ "progress" ] ) cij . emph ( "rnr:main:trun %r" % trun [ "status" ] , trun [ "status" ] != "PASS" ) return trun [ "progress" ] [ "UNKN" ] + trun [ "progress" ] [ "FAIL" ]
7507	def _renamer ( self , tre ) : names = tre . get_leaves ( ) for name in names : name . name = self . samples [ int ( name . name ) ] return tre . write ( format = 9 )
3280	def resolve_provider ( self , path ) : share = None lower_path = path . lower ( ) for r in self . sorted_share_list : if r == "/" : share = r break elif lower_path == r or lower_path . startswith ( r + "/" ) : share = r break if share is None : return None , None return share , self . provider_map . get ( share )
10506	def main ( port = 4118 , parentpid = None ) : if "LDTP_DEBUG" in os . environ : _ldtp_debug = True else : _ldtp_debug = False _ldtp_debug_file = os . environ . get ( 'LDTP_DEBUG_FILE' , None ) if _ldtp_debug : print ( "Parent PID: {}" . format ( int ( parentpid ) ) ) if _ldtp_debug_file : with open ( unicode ( _ldtp_debug_file ) , "a" ) as fp : fp . write ( "Parent PID: {}" . format ( int ( parentpid ) ) ) server = LDTPServer ( ( '' , port ) , allow_none = True , logRequests = _ldtp_debug , requestHandler = RequestHandler ) server . register_introspection_functions ( ) server . register_multicall_functions ( ) ldtp_inst = core . Core ( ) server . register_instance ( ldtp_inst ) if parentpid : thread . start_new_thread ( notifyclient , ( parentpid , ) ) try : server . serve_forever ( ) except KeyboardInterrupt : pass except : if _ldtp_debug : print ( traceback . format_exc ( ) ) if _ldtp_debug_file : with open ( _ldtp_debug_file , "a" ) as fp : fp . write ( traceback . format_exc ( ) )
4672	def newWallet ( self , pwd ) : if self . created ( ) : raise WalletExists ( "You already have created a wallet!" ) self . store . unlock ( pwd )
10467	def getAnyAppWithWindow ( cls ) : apps = cls . _getRunningApps ( ) for app in apps : pid = app . processIdentifier ( ) ref = cls . getAppRefByPid ( pid ) if hasattr ( ref , 'windows' ) and len ( ref . windows ( ) ) > 0 : return ref raise ValueError ( 'No GUI application found.' )
4728	def gen_to_dev ( self , address ) : cmd = [ "nvm_addr gen2dev" , self . envs [ "DEV_PATH" ] , "0x{:x}" . format ( address ) ] status , stdout , _ = cij . ssh . command ( cmd , shell = True ) if status : raise RuntimeError ( "cij.liblight.gen_to_dev: cmd fail" ) return int ( re . findall ( r"dev: ([0-9a-fx]+)" , stdout ) [ 0 ] , 16 )
11248	def median ( data ) : ordered = sorted ( data ) length = len ( ordered ) if length % 2 == 0 : return ( ordered [ math . floor ( length / 2 ) - 1 ] + ordered [ math . floor ( length / 2 ) ] ) / 2.0 elif length % 2 != 0 : return ordered [ math . floor ( length / 2 ) ]
3872	def get_all ( self , include_archived = False ) : return [ conv for conv in self . _conv_dict . values ( ) if not conv . is_archived or include_archived ]
7411	def sample_loci ( self ) : idxs = np . random . choice ( self . idxs , self . ntests ) with open ( self . data ) as indata : liter = ( indata . read ( ) . strip ( ) . split ( "|\n" ) ) seqdata = { i : "" for i in self . samples } for idx , loc in enumerate ( liter ) : if idx in idxs : lines = loc . split ( "\n" ) [ : - 1 ] names = [ i . split ( ) [ 0 ] for i in lines ] seqs = [ i . split ( ) [ 1 ] for i in lines ] dd = { i : j for i , j in zip ( names , seqs ) } for name in seqdata : if name in names : seqdata [ name ] += dd [ name ] else : seqdata [ name ] += "N" * len ( seqs [ 0 ] ) return seqdata
4284	def generate_thumbnail ( source , outname , box , delay , fit = True , options = None , converter = 'ffmpeg' ) : logger = logging . getLogger ( __name__ ) tmpfile = outname + ".tmp.jpg" cmd = [ converter , '-i' , source , '-an' , '-r' , '1' , '-ss' , delay , '-vframes' , '1' , '-y' , tmpfile ] logger . debug ( 'Create thumbnail for video: %s' , ' ' . join ( cmd ) ) check_subprocess ( cmd , source , outname ) image . generate_thumbnail ( tmpfile , outname , box , fit = fit , options = options ) os . unlink ( tmpfile )
10013	def parse_env_config ( config , env_name ) : all_env = get ( config , 'app.all_environments' , { } ) env = get ( config , 'app.environments.' + str ( env_name ) , { } ) return merge_dict ( all_env , env )
9336	def get ( self , Q ) : while self . Errors . empty ( ) : try : return Q . get ( timeout = 1 ) except queue . Empty : if not self . is_alive ( ) : try : return Q . get ( timeout = 0 ) except queue . Empty : raise StopProcessGroup else : continue else : raise StopProcessGroup
6797	def get_media_timestamp ( self , last_timestamp = None ) : r = self . local_renderer _latest_timestamp = - 1e9999999999999999 for path in self . iter_static_paths ( ) : path = r . env . static_root + '/' + path self . vprint ( 'checking timestamp of path:' , path ) if not os . path . isfile ( path ) : continue _latest_timestamp = max ( _latest_timestamp , get_last_modified_timestamp ( path ) or _latest_timestamp ) if last_timestamp is not None and _latest_timestamp > last_timestamp : break self . vprint ( 'latest_timestamp:' , _latest_timestamp ) return _latest_timestamp
12725	def cfms ( self , cfms ) : _set_params ( self . ode_obj , 'CFM' , cfms , self . ADOF + self . LDOF )
6428	def encode ( self , word , lang = 'en' ) : if lang == 'es' : return self . _phonetic_spanish . encode ( self . _spanish_metaphone . encode ( word ) ) word = self . _soundex . encode ( self . _metaphone . encode ( word ) ) word = word [ 0 ] . translate ( self . _trans ) + word [ 1 : ] return word
1173	def insort_right ( a , x , lo = 0 , hi = None ) : if lo < 0 : raise ValueError ( 'lo must be non-negative' ) if hi is None : hi = len ( a ) while lo < hi : mid = ( lo + hi ) // 2 if x < a [ mid ] : hi = mid else : lo = mid + 1 a . insert ( lo , x )
774	def main ( ) : initLogging ( verbose = True ) initExperimentPrng ( ) @ staticmethod def _mockCreate ( * args , ** kwargs ) : kwargs . pop ( 'implementation' , None ) return SDRClassifierDiff ( * args , ** kwargs ) SDRClassifierFactory . create = _mockCreate runExperiment ( sys . argv [ 1 : ] )
1332	def gradient ( self , image = None , label = None , strict = True ) : assert self . has_gradient ( ) if image is None : image = self . __original_image if label is None : label = self . __original_class assert not strict or self . in_bounds ( image ) self . _total_gradient_calls += 1 gradient = self . __model . gradient ( image , label ) assert gradient . shape == image . shape return gradient
11403	def create_record ( marcxml = None , verbose = CFG_BIBRECORD_DEFAULT_VERBOSE_LEVEL , correct = CFG_BIBRECORD_DEFAULT_CORRECT , parser = '' , sort_fields_by_indicators = False , keep_singletons = CFG_BIBRECORD_KEEP_SINGLETONS ) : if marcxml is None : return { } try : rec = _create_record_lxml ( marcxml , verbose , correct , keep_singletons = keep_singletons ) except InvenioBibRecordParserError as ex1 : return ( None , 0 , str ( ex1 ) ) if sort_fields_by_indicators : _record_sort_by_indicators ( rec ) errs = [ ] if correct : errs = _correct_record ( rec ) return ( rec , int ( not errs ) , errs )
11404	def filter_field_instances ( field_instances , filter_subcode , filter_value , filter_mode = 'e' ) : matched = [ ] if filter_mode == 'e' : to_match = ( filter_subcode , filter_value ) for instance in field_instances : if to_match in instance [ 0 ] : matched . append ( instance ) elif filter_mode == 's' : for instance in field_instances : for subfield in instance [ 0 ] : if subfield [ 0 ] == filter_subcode and subfield [ 1 ] . find ( filter_value ) > - 1 : matched . append ( instance ) break elif filter_mode == 'r' : reg_exp = re . compile ( filter_value ) for instance in field_instances : for subfield in instance [ 0 ] : if subfield [ 0 ] == filter_subcode and reg_exp . match ( subfield [ 1 ] ) is not None : matched . append ( instance ) break return matched
4897	def get_course_duration ( self , obj ) : duration = obj . end - obj . start if obj . start and obj . end else None if duration : return strfdelta ( duration , '{W} weeks {D} days.' ) return ''
2247	def _make_signature_key ( args , kwargs ) : kwitems = kwargs . items ( ) if ( sys . version_info . major , sys . version_info . minor ) < ( 3 , 7 ) : kwitems = sorted ( kwitems ) kwitems = tuple ( kwitems ) try : key = _hashable ( args ) , _hashable ( kwitems ) except TypeError : raise TypeError ( 'Signature is not hashable: args={} kwargs{}' . format ( args , kwargs ) ) return key
12754	def joint_distances ( self ) : return [ ( ( np . array ( j . anchor ) - j . anchor2 ) ** 2 ) . sum ( ) for j in self . joints ]
5667	def add_walk_distances_to_db_python ( gtfs , osm_path , cutoff_distance_m = 1000 ) : if isinstance ( gtfs , str ) : gtfs = GTFS ( gtfs ) assert ( isinstance ( gtfs , GTFS ) ) print ( "Reading in walk network" ) walk_network = create_walk_network_from_osm ( osm_path ) print ( "Matching stops to the OSM network" ) stop_I_to_nearest_osm_node , stop_I_to_nearest_osm_node_distance = match_stops_to_nodes ( gtfs , walk_network ) transfers = gtfs . get_straight_line_transfer_distances ( ) from_I_to_to_stop_Is = { stop_I : set ( ) for stop_I in stop_I_to_nearest_osm_node } for transfer_tuple in transfers . itertuples ( ) : from_I = transfer_tuple . from_stop_I to_I = transfer_tuple . to_stop_I from_I_to_to_stop_Is [ from_I ] . add ( to_I ) print ( "Computing walking distances" ) for from_I , to_stop_Is in from_I_to_to_stop_Is . items ( ) : from_node = stop_I_to_nearest_osm_node [ from_I ] from_dist = stop_I_to_nearest_osm_node_distance [ from_I ] shortest_paths = networkx . single_source_dijkstra_path_length ( walk_network , from_node , cutoff = cutoff_distance_m - from_dist , weight = "distance" ) for to_I in to_stop_Is : to_distance = stop_I_to_nearest_osm_node_distance [ to_I ] to_node = stop_I_to_nearest_osm_node [ to_I ] osm_distance = shortest_paths . get ( to_node , float ( 'inf' ) ) total_distance = from_dist + osm_distance + to_distance from_stop_I_transfers = transfers [ transfers [ 'from_stop_I' ] == from_I ] straigth_distance = from_stop_I_transfers [ from_stop_I_transfers [ "to_stop_I" ] == to_I ] [ "d" ] . values [ 0 ] assert ( straigth_distance < total_distance + 2 ) if total_distance <= cutoff_distance_m : gtfs . conn . execute ( "UPDATE stop_distances " "SET d_walk = " + str ( int ( total_distance ) ) + " WHERE from_stop_I=" + str ( from_I ) + " AND to_stop_I=" + str ( to_I ) ) gtfs . conn . commit ( )
12839	def init_async ( self , loop ) : super ( PooledAIODatabase , self ) . init_async ( loop ) self . _waiters = collections . deque ( )
3837	async def set_focus ( self , set_focus_request ) : response = hangouts_pb2 . SetFocusResponse ( ) await self . _pb_request ( 'conversations/setfocus' , set_focus_request , response ) return response
4715	def trun_emph ( trun ) : if trun [ "conf" ] [ "VERBOSE" ] > 1 : cij . emph ( "rnr:CONF {" ) for cvar in sorted ( trun [ "conf" ] . keys ( ) ) : cij . emph ( " % 16s: %r" % ( cvar , trun [ "conf" ] [ cvar ] ) ) cij . emph ( "}" ) if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:INFO {" ) cij . emph ( " OUTPUT: %r" % trun [ "conf" ] [ "OUTPUT" ] ) cij . emph ( " yml_fpath: %r" % yml_fpath ( trun [ "conf" ] [ "OUTPUT" ] ) ) cij . emph ( "}" )
1106	def get_matching_blocks ( self ) : if self . matching_blocks is not None : return self . matching_blocks la , lb = len ( self . a ) , len ( self . b ) queue = [ ( 0 , la , 0 , lb ) ] matching_blocks = [ ] while queue : alo , ahi , blo , bhi = queue . pop ( ) i , j , k = x = self . find_longest_match ( alo , ahi , blo , bhi ) if k : matching_blocks . append ( x ) if alo < i and blo < j : queue . append ( ( alo , i , blo , j ) ) if i + k < ahi and j + k < bhi : queue . append ( ( i + k , ahi , j + k , bhi ) ) matching_blocks . sort ( ) i1 = j1 = k1 = 0 non_adjacent = [ ] for i2 , j2 , k2 in matching_blocks : if i1 + k1 == i2 and j1 + k1 == j2 : k1 += k2 else : if k1 : non_adjacent . append ( ( i1 , j1 , k1 ) ) i1 , j1 , k1 = i2 , j2 , k2 if k1 : non_adjacent . append ( ( i1 , j1 , k1 ) ) non_adjacent . append ( ( la , lb , 0 ) ) self . matching_blocks = map ( Match . _make , non_adjacent ) return self . matching_blocks
4908	def _post ( self , url , data , scope ) : self . _create_session ( scope ) response = self . session . post ( url , data = data ) return response . status_code , response . text
10546	def get_taskruns ( project_id , limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) params [ 'project_id' ] = project_id try : res = _pybossa_req ( 'get' , 'taskrun' , params = params ) if type ( res ) . __name__ == 'list' : return [ TaskRun ( taskrun ) for taskrun in res ] else : raise TypeError except : raise
3792	def solve_T ( self , P , V , quick = True ) : r self . Tc = sum ( self . Tcs ) / self . N return super ( type ( self ) . __mro__ [ - 3 ] , self ) . solve_T ( P = P , V = V , quick = quick )
6857	def create_user ( name , password , host = 'localhost' , ** kwargs ) : with settings ( hide ( 'running' ) ) : query ( "CREATE USER '%(name)s'@'%(host)s' IDENTIFIED BY '%(password)s';" % { 'name' : name , 'password' : password , 'host' : host } , ** kwargs ) puts ( "Created MySQL user '%s'." % name )
3940	async def listen ( self ) : retries = 0 need_new_sid = True while retries <= self . _max_retries : if retries > 0 : backoff_seconds = self . _retry_backoff_base ** retries logger . info ( 'Backing off for %s seconds' , backoff_seconds ) await asyncio . sleep ( backoff_seconds ) if need_new_sid : await self . _fetch_channel_sid ( ) need_new_sid = False self . _chunk_parser = ChunkParser ( ) try : await self . _longpoll_request ( ) except ChannelSessionError as err : logger . warning ( 'Long-polling interrupted: %s' , err ) need_new_sid = True except exceptions . NetworkError as err : logger . warning ( 'Long-polling request failed: %s' , err ) else : retries = 0 continue retries += 1 logger . info ( 'retry attempt count is now %s' , retries ) if self . _is_connected : self . _is_connected = False await self . on_disconnect . fire ( ) logger . error ( 'Ran out of retries for long-polling request' )
7138	def get_type_info ( obj ) : if isinstance ( obj , primitive_types ) : return ( 'primitive' , type ( obj ) . __name__ ) if isinstance ( obj , sequence_types ) : return ( 'sequence' , type ( obj ) . __name__ ) if isinstance ( obj , array_types ) : return ( 'array' , type ( obj ) . __name__ ) if isinstance ( obj , key_value_types ) : return ( 'key-value' , type ( obj ) . __name__ ) if isinstance ( obj , types . ModuleType ) : return ( 'module' , type ( obj ) . __name__ ) if isinstance ( obj , ( types . FunctionType , types . MethodType ) ) : return ( 'function' , type ( obj ) . __name__ ) if isinstance ( obj , type ) : if hasattr ( obj , '__dict__' ) : return ( 'class' , obj . __name__ ) if isinstance ( type ( obj ) , type ) : if hasattr ( obj , '__dict__' ) : cls_name = type ( obj ) . __name__ if cls_name == 'classobj' : cls_name = obj . __name__ return ( 'class' , '{}' . format ( cls_name ) ) if cls_name == 'instance' : cls_name = obj . __class__ . __name__ return ( 'instance' , '{} instance' . format ( cls_name ) ) return ( 'unknown' , type ( obj ) . __name__ )
9320	def _validate_collection ( self ) : if not self . _id : msg = "No 'id' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if not self . _title : msg = "No 'title' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _can_read is None : msg = "No 'can_read' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _can_write is None : msg = "No 'can_write' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _id not in self . url : msg = "The collection '{}' does not match the url for queries '{}'" raise ValidationError ( msg . format ( self . _id , self . url ) )
9179	def _validate_license ( model ) : license_mapping = obtain_licenses ( ) try : license_url = model . metadata [ 'license_url' ] except KeyError : raise exceptions . MissingRequiredMetadata ( 'license_url' ) try : license = license_mapping [ license_url ] except KeyError : raise exceptions . InvalidLicense ( license_url ) if not license [ 'is_valid_for_publication' ] : raise exceptions . InvalidLicense ( license_url )
11041	def get_single_header ( headers , key ) : raw_headers = headers . getRawHeaders ( key ) if raw_headers is None : return None header , _ = cgi . parse_header ( raw_headers [ - 1 ] ) return header
8655	def search_messages ( session , thread_id , query , limit = 20 , offset = 0 , message_context_details = None , window_above = None , window_below = None ) : query = { 'thread_id' : thread_id , 'query' : query , 'limit' : limit , 'offset' : offset } if message_context_details : query [ 'message_context_details' ] = message_context_details if window_above : query [ 'window_above' ] = window_above if window_below : query [ 'window_below' ] = window_below response = make_get_request ( session , 'messages/search' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MessagesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
2581	def _load_checkpoints ( self , checkpointDirs ) : memo_lookup_table = { } for checkpoint_dir in checkpointDirs : logger . info ( "Loading checkpoints from {}" . format ( checkpoint_dir ) ) checkpoint_file = os . path . join ( checkpoint_dir , 'tasks.pkl' ) try : with open ( checkpoint_file , 'rb' ) as f : while True : try : data = pickle . load ( f ) memo_fu = Future ( ) if data [ 'exception' ] : memo_fu . set_exception ( data [ 'exception' ] ) else : memo_fu . set_result ( data [ 'result' ] ) memo_lookup_table [ data [ 'hash' ] ] = memo_fu except EOFError : break except FileNotFoundError : reason = "Checkpoint file was not found: {}" . format ( checkpoint_file ) logger . error ( reason ) raise BadCheckpoint ( reason ) except Exception : reason = "Failed to load checkpoint: {}" . format ( checkpoint_file ) logger . error ( reason ) raise BadCheckpoint ( reason ) logger . info ( "Completed loading checkpoint:{0} with {1} tasks" . format ( checkpoint_file , len ( memo_lookup_table . keys ( ) ) ) ) return memo_lookup_table
10011	def get ( vals , key , default_val = None ) : val = vals for part in key . split ( '.' ) : if isinstance ( val , dict ) : val = val . get ( part , None ) if val is None : return default_val else : return default_val return val
11401	def create_field ( subfields = None , ind1 = ' ' , ind2 = ' ' , controlfield_value = '' , global_position = - 1 ) : if subfields is None : subfields = [ ] ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) field = ( subfields , ind1 , ind2 , controlfield_value , global_position ) _check_field_validity ( field ) return field
4968	def _validate_program ( self ) : program = self . cleaned_data . get ( self . Fields . PROGRAM ) if not program : return course_runs = get_course_runs_from_program ( program ) try : client = CourseCatalogApiClient ( self . _user , self . _enterprise_customer . site ) available_modes = client . get_common_course_modes ( course_runs ) course_mode = self . cleaned_data . get ( self . Fields . COURSE_MODE ) except ( HttpClientError , HttpServerError ) : raise ValidationError ( ValidationMessages . FAILED_TO_OBTAIN_COURSE_MODES . format ( program_title = program . get ( "title" ) ) ) if not course_mode : raise ValidationError ( ValidationMessages . COURSE_WITHOUT_COURSE_MODE ) if course_mode not in available_modes : raise ValidationError ( ValidationMessages . COURSE_MODE_NOT_AVAILABLE . format ( mode = course_mode , program_title = program . get ( "title" ) , modes = ", " . join ( available_modes ) ) )
4637	def claim ( self , account = None , ** kwargs ) : if not account : if "default_account" in self . blockchain . config : account = self . blockchain . config [ "default_account" ] if not account : raise ValueError ( "You need to provide an account" ) account = self . account_class ( account , blockchain_instance = self . blockchain ) pubkeys = self . blockchain . wallet . getPublicKeys ( ) addresses = dict ( ) for p in pubkeys : if p [ : len ( self . blockchain . prefix ) ] != self . blockchain . prefix : continue pubkey = self . publickey_class ( p , prefix = self . blockchain . prefix ) addresses [ str ( self . address_class . from_pubkey ( pubkey , compressed = False , version = 0 , prefix = self . blockchain . prefix , ) ) ] = pubkey addresses [ str ( self . address_class . from_pubkey ( pubkey , compressed = True , version = 0 , prefix = self . blockchain . prefix , ) ) ] = pubkey addresses [ str ( self . address_class . from_pubkey ( pubkey , compressed = False , version = 56 , prefix = self . blockchain . prefix , ) ) ] = pubkey addresses [ str ( self . address_class . from_pubkey ( pubkey , compressed = True , version = 56 , prefix = self . blockchain . prefix , ) ) ] = pubkey if self [ "owner" ] not in addresses . keys ( ) : raise MissingKeyError ( "Need key for address {}" . format ( self [ "owner" ] ) ) op = self . operations . Balance_claim ( ** { "fee" : { "amount" : 0 , "asset_id" : "1.3.0" } , "deposit_to_account" : account [ "id" ] , "balance_to_claim" : self [ "id" ] , "balance_owner_key" : addresses [ self [ "owner" ] ] , "total_claimed" : self [ "balance" ] , "prefix" : self . blockchain . prefix , } ) signers = [ account [ "name" ] , addresses . get ( self [ "owner" ] ) , ] return self . blockchain . finalizeOp ( op , signers , "active" , ** kwargs )
8623	def get_self_user_id ( session ) : response = make_get_request ( session , 'self' ) if response . status_code == 200 : return response . json ( ) [ 'result' ] [ 'id' ] else : raise UserIdNotRetrievedException ( 'Error retrieving user id: %s' % response . text , response . text )
4765	def is_not_equal_to ( self , other ) : if self . val == other : self . _err ( 'Expected <%s> to be not equal to <%s>, but was.' % ( self . val , other ) ) return self
5487	def send_payload ( self , params ) : data = json . dumps ( { 'jsonrpc' : self . version , 'method' : self . service_name , 'params' : params , 'id' : text_type ( uuid . uuid4 ( ) ) } ) data_binary = data . encode ( 'utf-8' ) url_request = Request ( self . service_url , data_binary , headers = self . headers ) return urlopen ( url_request ) . read ( )
9527	def to_boulderio ( infile , outfile ) : seq_reader = sequences . file_reader ( infile ) f_out = utils . open_file_write ( outfile ) for sequence in seq_reader : print ( "SEQUENCE_ID=" + sequence . id , file = f_out ) print ( "SEQUENCE_TEMPLATE=" + sequence . seq , file = f_out ) print ( "=" , file = f_out ) utils . close ( f_out )
5127	def stop_collecting_data ( self , queues = None , edge = None , edge_type = None ) : queues = _get_queues ( self . g , queues , edge , edge_type ) for k in queues : self . edge2queue [ k ] . collect_data = False
8405	def _censor_with ( x , range , value = None ) : return [ val if range [ 0 ] <= val <= range [ 1 ] else value for val in x ]
8020	async def websocket_accept ( self , message , stream_name ) : is_first = not self . applications_accepting_frames self . applications_accepting_frames . add ( stream_name ) if is_first : await self . accept ( )
11120	def get_file_relative_path_by_id ( self , id ) : for path , info in self . walk_files_info ( ) : if info [ 'id' ] == id : return path return None
5071	def get_configuration_value_for_site ( site , key , default = None ) : if hasattr ( site , 'configuration' ) : return site . configuration . get_value ( key , default ) return default
6265	def translate_buffer_format ( vertex_format ) : buffer_format = [ ] attributes = [ ] mesh_attributes = [ ] if "T2F" in vertex_format : buffer_format . append ( "2f" ) attributes . append ( "in_uv" ) mesh_attributes . append ( ( "TEXCOORD_0" , "in_uv" , 2 ) ) if "C3F" in vertex_format : buffer_format . append ( "3f" ) attributes . append ( "in_color" ) mesh_attributes . append ( ( "NORMAL" , "in_color" , 3 ) ) if "N3F" in vertex_format : buffer_format . append ( "3f" ) attributes . append ( "in_normal" ) mesh_attributes . append ( ( "NORMAL" , "in_normal" , 3 ) ) buffer_format . append ( "3f" ) attributes . append ( "in_position" ) mesh_attributes . append ( ( "POSITION" , "in_position" , 3 ) ) return " " . join ( buffer_format ) , attributes , mesh_attributes
2129	def configure_display ( self , data , kwargs = None , write = False ) : if settings . format != 'human' : return if write : obj , obj_type , res , res_type = self . obj_res ( kwargs ) data [ 'type' ] = kwargs [ 'type' ] data [ obj_type ] = obj data [ res_type ] = res self . set_display_columns ( set_false = [ 'team' if obj_type == 'user' else 'user' ] , set_true = [ 'target_team' if res_type == 'team' else res_type ] ) else : self . set_display_columns ( set_false = [ 'user' , 'team' ] , set_true = [ 'resource_name' , 'resource_type' ] ) if 'results' in data : for i in range ( len ( data [ 'results' ] ) ) : self . populate_resource_columns ( data [ 'results' ] [ i ] ) else : self . populate_resource_columns ( data )
4248	def netspeed_by_addr ( self , addr ) : if self . _databaseType == const . NETSPEED_EDITION : return const . NETSPEED_NAMES [ self . id_by_addr ( addr ) ] elif self . _databaseType in ( const . NETSPEED_EDITION_REV1 , const . NETSPEED_EDITION_REV1_V6 ) : ipnum = util . ip2long ( addr ) return self . _get_org ( ipnum ) raise GeoIPError ( 'Invalid database type, expected NetSpeed or NetSpeedCell' )
5684	def day_start_ut ( self , ut ) : old_tz = self . set_current_process_time_zone ( ) ut = time . mktime ( time . localtime ( ut ) [ : 3 ] + ( 12 , 00 , 0 , 0 , 0 , - 1 ) ) - 43200 set_process_timezone ( old_tz ) return ut
3715	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : Vms = [ i ( T , P ) for i in self . VolumeSolids ] return mixing_simple ( zs , Vms ) else : raise Exception ( 'Method not valid' )
2824	def convert_sigmoid ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting sigmoid ...' ) if names == 'short' : tf_name = 'SIGM' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) sigmoid = keras . layers . Activation ( 'sigmoid' , name = tf_name ) layers [ scope_name ] = sigmoid ( layers [ inputs [ 0 ] ] )
4521	def get ( self , ring , angle ) : pixel = self . angleToPixel ( angle , ring ) return self . _get_base ( pixel )
655	def _fillInOnTimes ( vector , durations ) : nonzeros = numpy . array ( vector ) . nonzero ( ) [ 0 ] if len ( nonzeros ) == 0 : return if len ( nonzeros ) == 1 : durations [ nonzeros [ 0 ] ] = 1 return prev = nonzeros [ 0 ] onTime = 1 onStartIdx = prev endIdx = nonzeros [ - 1 ] for idx in nonzeros [ 1 : ] : if idx != prev + 1 : durations [ onStartIdx : onStartIdx + onTime ] = range ( 1 , onTime + 1 ) onTime = 1 onStartIdx = idx else : onTime += 1 prev = idx durations [ onStartIdx : onStartIdx + onTime ] = range ( 1 , onTime + 1 )
5628	def hook ( self , event_type = 'push' ) : def decorator ( func ) : self . _hooks [ event_type ] . append ( func ) return func return decorator
4125	def data_two_freqs ( N = 200 ) : nn = arange ( N ) xx = cos ( 0.257 * pi * nn ) + sin ( 0.2 * pi * nn ) + 0.01 * randn ( nn . size ) return xx
10361	def rewire_targets ( graph , rewiring_probability ) : if not all_edges_consistent ( graph ) : raise ValueError ( '{} is not consistent' . format ( graph ) ) result = graph . copy ( ) nodes = result . nodes ( ) for u , v in result . edges ( ) : if random . random ( ) < rewiring_probability : continue w = random . choice ( nodes ) while w == u or result . has_edge ( u , w ) : w = random . choice ( nodes ) result . add_edge ( w , v ) result . remove_edge ( u , v ) return result
3501	def assess_precursors ( model , reaction , flux_coefficient_cutoff = 0.001 , solver = None ) : warn ( 'use assess_component instead' , DeprecationWarning ) return assess_component ( model , reaction , 'reactants' , flux_coefficient_cutoff , solver )
3963	def start_local_env ( recreate_containers ) : assembled_spec = spec_assembler . get_assembled_specs ( ) required_absent_assets = virtualbox . required_absent_assets ( assembled_spec ) if required_absent_assets : raise RuntimeError ( 'Assets {} are specified as required but are not set. Set them with `dusty assets set`' . format ( required_absent_assets ) ) docker_ip = virtualbox . get_docker_vm_ip ( ) if os . path . exists ( constants . COMPOSEFILE_PATH ) : try : stop_apps_or_services ( rm_containers = recreate_containers ) except CalledProcessError as e : log_to_client ( "WARNING: docker-compose stop failed" ) log_to_client ( str ( e ) ) daemon_warnings . clear_namespace ( 'disk' ) df_info = virtualbox . get_docker_vm_disk_info ( as_dict = True ) if 'M' in df_info [ 'free' ] or 'K' in df_info [ 'free' ] : warning_msg = 'VM is low on disk. Available disk: {}' . format ( df_info [ 'free' ] ) daemon_warnings . warn ( 'disk' , warning_msg ) log_to_client ( warning_msg ) log_to_client ( "Compiling together the assembled specs" ) active_repos = spec_assembler . get_all_repos ( active_only = True , include_specs_repo = False ) log_to_client ( "Compiling the port specs" ) port_spec = port_spec_compiler . get_port_spec_document ( assembled_spec , docker_ip ) log_to_client ( "Compiling the nginx config" ) docker_bridge_ip = virtualbox . get_docker_bridge_ip ( ) nginx_config = nginx_compiler . get_nginx_configuration_spec ( port_spec , docker_bridge_ip ) log_to_client ( "Creating setup and script bash files" ) make_up_command_files ( assembled_spec , port_spec ) log_to_client ( "Compiling docker-compose config" ) compose_config = compose_compiler . get_compose_dict ( assembled_spec , port_spec ) log_to_client ( "Saving port forwarding to hosts file" ) hosts . update_hosts_file_from_port_spec ( port_spec ) log_to_client ( "Configuring NFS" ) nfs . configure_nfs ( ) log_to_client ( "Saving updated nginx config to the VM" ) nginx . update_nginx_from_config ( nginx_config ) log_to_client ( "Saving Docker Compose config and starting all containers" ) compose . update_running_containers_from_spec ( compose_config , recreate_containers = recreate_containers ) log_to_client ( "Your local environment is now started!" )
13321	def add_active_module ( module ) : modules = set ( get_active_modules ( ) ) modules . add ( module ) new_modules_path = os . pathsep . join ( [ m . path for m in modules ] ) os . environ [ 'CPENV_ACTIVE_MODULES' ] = str ( new_modules_path )
12433	def dasherize ( value ) : value = value . strip ( ) value = re . sub ( r'([A-Z])' , r'-\1' , value ) value = re . sub ( r'[-_\s]+' , r'-' , value ) value = re . sub ( r'^-' , r'' , value ) value = value . lower ( ) return value
12826	def flush_buffer ( self ) : self . code_builder . add_line ( '{0}.extend([{1}])' , self . result_var , ',' . join ( self . buffered ) ) self . buffered = [ ]
6530	def purge_config_cache ( location = None ) : cache_path = get_cache_path ( location ) if location : os . remove ( cache_path ) else : shutil . rmtree ( cache_path )
8157	def close ( self ) : self . _con . commit ( ) self . _cur . close ( ) self . _con . close ( )
2650	def send ( self , message_type , task_id , message ) : x = 0 try : buffer = pickle . dumps ( ( self . source_id , int ( time . time ( ) ) , message_type , message ) ) except Exception as e : print ( "Exception during pickling {}" . format ( e ) ) return try : x = self . sock . sendto ( buffer , ( self . ip , self . port ) ) except socket . timeout : print ( "Could not send message within timeout limit" ) return False return x
5423	def _wait_after ( provider , job_ids , poll_interval , stop_on_failure ) : job_ids_to_check = { j for j in job_ids if j != dsub_util . NO_JOB } error_messages = [ ] while job_ids_to_check and ( not error_messages or not stop_on_failure ) : print ( 'Waiting for: %s.' % ( ', ' . join ( job_ids_to_check ) ) ) jobs_left = _wait_for_any_job ( provider , job_ids_to_check , poll_interval ) jobs_completed = job_ids_to_check . difference ( jobs_left ) tasks_completed = provider . lookup_job_tasks ( { '*' } , job_ids = jobs_completed ) dominant_job_tasks = _dominant_task_for_jobs ( tasks_completed ) if len ( dominant_job_tasks ) != len ( jobs_completed ) : jobs_found = dsub_util . tasks_to_job_ids ( dominant_job_tasks ) jobs_not_found = jobs_completed . difference ( jobs_found ) for j in jobs_not_found : error = '%s: not found' % j print_error ( ' %s' % error ) error_messages += [ error ] for t in dominant_job_tasks : job_id = t . get_field ( 'job-id' ) status = t . get_field ( 'task-status' ) print ( ' %s: %s' % ( str ( job_id ) , str ( status ) ) ) if status in [ 'FAILURE' , 'CANCELED' ] : error_messages += [ provider . get_tasks_completion_messages ( [ t ] ) ] job_ids_to_check = jobs_left return error_messages
11526	def create_small_thumbnail ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'itemId' ] = item_id response = self . request ( 'midas.thumbnailcreator.create.small.thumbnail' , parameters ) return response
13200	def format_title ( self , format = 'html5' , deparagraph = True , mathjax = False , smart = True , extra_args = None ) : if self . title is None : return None output_text = convert_lsstdoc_tex ( self . title , format , deparagraph = deparagraph , mathjax = mathjax , smart = smart , extra_args = extra_args ) return output_text
4403	def mget ( self , keys , * args ) : args = list_or_args ( keys , args ) server_keys = { } ret_dict = { } for key in args : server_name = self . get_server_name ( key ) server_keys [ server_name ] = server_keys . get ( server_name , [ ] ) server_keys [ server_name ] . append ( key ) for server_name , sub_keys in iteritems ( server_keys ) : values = self . connections [ server_name ] . mget ( sub_keys ) ret_dict . update ( dict ( zip ( sub_keys , values ) ) ) result = [ ] for key in args : result . append ( ret_dict . get ( key , None ) ) return result
9894	def _uptime_solaris ( ) : global __boottime try : kstat = ctypes . CDLL ( 'libkstat.so' ) except ( AttributeError , OSError ) : return None KSTAT_STRLEN = 31 class anon_union ( ctypes . Union ) : _fields_ = [ ( 'c' , ctypes . c_char * 16 ) , ( 'time' , ctypes . c_int ) ] class kstat_named_t ( ctypes . Structure ) : _fields_ = [ ( 'name' , ctypes . c_char * KSTAT_STRLEN ) , ( 'data_type' , ctypes . c_char ) , ( 'value' , anon_union ) ] kstat . kstat_open . restype = ctypes . c_void_p kstat . kstat_lookup . restype = ctypes . c_void_p kstat . kstat_lookup . argtypes = [ ctypes . c_void_p , ctypes . c_char_p , ctypes . c_int , ctypes . c_char_p ] kstat . kstat_read . restype = ctypes . c_int kstat . kstat_read . argtypes = [ ctypes . c_void_p , ctypes . c_void_p , ctypes . c_void_p ] kstat . kstat_data_lookup . restype = ctypes . POINTER ( kstat_named_t ) kstat . kstat_data_lookup . argtypes = [ ctypes . c_void_p , ctypes . c_char_p ] kc = kstat . kstat_open ( ) if not kc : return None ksp = kstat . kstat_lookup ( kc , 'unix' , 0 , 'system_misc' ) if ksp and kstat . kstat_read ( kc , ksp , None ) != - 1 : data = kstat . kstat_data_lookup ( ksp , 'boot_time' ) if data : __boottime = data . contents . value . time kstat . kstat_close ( kc ) if __boottime is not None : return time . time ( ) - __boottime return None
187	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , color_lines = None , color_points = None , alpha = 1.0 , alpha_lines = None , alpha_points = None , size = 1 , size_lines = None , size_points = None , antialiased = True , raise_if_out_of_image = False ) : for ls in self . line_strings : image = ls . draw_on_image ( image , color = color , color_lines = color_lines , color_points = color_points , alpha = alpha , alpha_lines = alpha_lines , alpha_points = alpha_points , size = size , size_lines = size_lines , size_points = size_points , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) return image
10986	def get_particles_featuring ( feature_rad , state_name = None , im_name = None , use_full_path = False , actual_rad = None , invert = True , featuring_params = { } , ** kwargs ) : state_name , im_name = _pick_state_im_name ( state_name , im_name , use_full_path = use_full_path ) s = states . load ( state_name ) if actual_rad is None : actual_rad = np . median ( s . obj_get_radii ( ) ) im = util . RawImage ( im_name , tile = s . image . tile ) pos = locate_spheres ( im , feature_rad , invert = invert , ** featuring_params ) _ = s . obj_remove_particle ( np . arange ( s . obj_get_radii ( ) . size ) ) s . obj_add_particle ( pos , np . ones ( pos . shape [ 0 ] ) * actual_rad ) s . set_image ( im ) _translate_particles ( s , invert = invert , ** kwargs ) return s
2972	def from_dict ( name , values ) : count = 1 count_value = values . get ( 'count' , 1 ) if isinstance ( count_value , int ) : count = max ( count_value , 1 ) def with_index ( name , idx ) : if name and idx : return '%s_%d' % ( name , idx ) return name def get_instance ( n , idx = None ) : return BlockadeContainerConfig ( with_index ( n , idx ) , values [ 'image' ] , command = values . get ( 'command' ) , links = values . get ( 'links' ) , volumes = values . get ( 'volumes' ) , publish_ports = values . get ( 'ports' ) , expose_ports = values . get ( 'expose' ) , environment = values . get ( 'environment' ) , hostname = values . get ( 'hostname' ) , dns = values . get ( 'dns' ) , start_delay = values . get ( 'start_delay' , 0 ) , neutral = values . get ( 'neutral' , False ) , holy = values . get ( 'holy' , False ) , container_name = with_index ( values . get ( 'container_name' ) , idx ) , cap_add = values . get ( 'cap_add' ) ) if count == 1 : yield get_instance ( name ) else : for idx in range ( 1 , count + 1 ) : yield get_instance ( name , idx )
9669	def is_valid_sound ( sound , ts ) : if isinstance ( sound , ( Marker , UnknownSound ) ) : return False s1 = ts [ sound . name ] s2 = ts [ sound . s ] return s1 . name == s2 . name and s1 . s == s2 . s
50	def copy ( self , x = None , y = None ) : return self . deepcopy ( x = x , y = y )
10043	def create_blueprint ( endpoints ) : from invenio_records_ui . views import create_url_rule blueprint = Blueprint ( 'invenio_deposit_ui' , __name__ , static_folder = '../static' , template_folder = '../templates' , url_prefix = '' , ) @ blueprint . errorhandler ( PIDDeletedError ) def tombstone_errorhandler ( error ) : return render_template ( current_app . config [ 'DEPOSIT_UI_TOMBSTONE_TEMPLATE' ] , pid = error . pid , record = error . record or { } , ) , 410 for endpoint , options in ( endpoints or { } ) . items ( ) : options = deepcopy ( options ) options . pop ( 'jsonschema' , None ) options . pop ( 'schemaform' , None ) blueprint . add_url_rule ( ** create_url_rule ( endpoint , ** options ) ) @ blueprint . route ( '/deposit' ) @ login_required def index ( ) : return render_template ( current_app . config [ 'DEPOSIT_UI_INDEX_TEMPLATE' ] ) @ blueprint . route ( '/deposit/new' ) @ login_required def new ( ) : deposit_type = request . values . get ( 'type' ) return render_template ( current_app . config [ 'DEPOSIT_UI_NEW_TEMPLATE' ] , record = { '_deposit' : { 'id' : None } } , jsonschema = current_deposit . jsonschemas [ deposit_type ] , schemaform = current_deposit . schemaforms [ deposit_type ] , ) return blueprint
5436	def parse_pair_args ( labels , argclass ) : label_data = set ( ) for arg in labels : name , value = split_pair ( arg , '=' , nullable_idx = 1 ) label_data . add ( argclass ( name , value ) ) return label_data
6183	def git_path_valid ( git_path = None ) : if git_path is None and GIT_PATH is None : return False if git_path is None : git_path = GIT_PATH try : call ( [ git_path , '--version' ] ) return True except OSError : return False
8906	def save_service ( self , service , overwrite = True ) : name = namesgenerator . get_sane_name ( service . name ) if not name : name = namesgenerator . get_random_name ( ) if self . collection . count_documents ( { 'name' : name } ) > 0 : name = namesgenerator . get_random_name ( retry = True ) if self . collection . count_documents ( { 'name' : name } ) > 0 : if overwrite : self . collection . delete_one ( { 'name' : name } ) else : raise Exception ( "service name already registered." ) self . collection . insert_one ( Service ( name = name , url = baseurl ( service . url ) , type = service . type , purl = service . purl , public = service . public , auth = service . auth , verify = service . verify ) ) return self . fetch_by_name ( name = name )
13440	def unlock_file ( filename ) : lockfile = "%s.lock" % filename if isfile ( lockfile ) : os . remove ( lockfile ) return True else : return False
5522	def parse_list_line_windows ( self , b ) : line = b . decode ( encoding = self . encoding ) . rstrip ( "\r\n" ) date_time_end = line . index ( "M" ) date_time_str = line [ : date_time_end + 1 ] . strip ( ) . split ( " " ) date_time_str = " " . join ( [ x for x in date_time_str if len ( x ) > 0 ] ) line = line [ date_time_end + 1 : ] . lstrip ( ) with setlocale ( "C" ) : strptime = datetime . datetime . strptime date_time = strptime ( date_time_str , "%m/%d/%Y %I:%M %p" ) info = { } info [ "modify" ] = self . format_date_time ( date_time ) next_space = line . index ( " " ) if line . startswith ( "<DIR>" ) : info [ "type" ] = "dir" else : info [ "type" ] = "file" info [ "size" ] = line [ : next_space ] . replace ( "," , "" ) if not info [ "size" ] . isdigit ( ) : raise ValueError filename = line [ next_space : ] . lstrip ( ) if filename == "." or filename == ".." : raise ValueError return pathlib . PurePosixPath ( filename ) , info
8790	def pop ( self , model ) : tags = self . _pop ( model ) if tags : for tag in tags : value = self . deserialize ( tag ) try : self . validate ( value ) return value except TagValidationError : continue
11608	def add ( self , addend_mat , axis = 1 ) : if self . finalized : if axis == 0 : raise NotImplementedError ( 'The method is not yet implemented for the axis.' ) elif axis == 1 : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] + addend_mat elif axis == 2 : raise NotImplementedError ( 'The method is not yet implemented for the axis.' ) else : raise RuntimeError ( 'The axis should be 0, 1, or 2.' ) else : raise RuntimeError ( 'The original matrix must be finalized.' )
4057	def _csljson_processor ( self , retrieved ) : items = [ ] json_kwargs = { } if self . preserve_json_order : json_kwargs [ "object_pairs_hook" ] = OrderedDict for csl in retrieved . entries : items . append ( json . loads ( csl [ "content" ] [ 0 ] [ "value" ] , ** json_kwargs ) ) self . url_params = None return items
1219	def restore ( self , sess , save_path ) : if self . _saver is None : raise TensorForceError ( "register_saver_ops should be called before restore" ) self . _saver . restore ( sess = sess , save_path = save_path )
6897	def _periodicfeatures_worker ( task ) : pfpickle , lcbasedir , outdir , starfeatures , kwargs = task try : return get_periodicfeatures ( pfpickle , lcbasedir , outdir , starfeatures = starfeatures , ** kwargs ) except Exception as e : LOGEXCEPTION ( 'failed to get periodicfeatures for %s' % pfpickle )
11522	def add_condor_job ( self , token , batchmaketaskid , jobdefinitionfilename , outputfilename , errorfilename , logfilename , postfilename ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'batchmaketaskid' ] = batchmaketaskid parameters [ 'jobdefinitionfilename' ] = jobdefinitionfilename parameters [ 'outputfilename' ] = outputfilename parameters [ 'errorfilename' ] = errorfilename parameters [ 'logfilename' ] = logfilename parameters [ 'postfilename' ] = postfilename response = self . request ( 'midas.batchmake.add.condor.job' , parameters ) return response
12948	def reload ( self , cascadeObjects = True ) : _id = self . _id if not _id : raise KeyError ( 'Object has never been saved! Cannot reload.' ) currentData = self . asDict ( False , forStorage = False ) newDataObj = self . objects . get ( _id ) if not newDataObj : raise KeyError ( 'Object with id=%d is not in database. Cannot reload.' % ( _id , ) ) newData = newDataObj . asDict ( False , forStorage = False ) if currentData == newData and not self . foreignFields : return [ ] updatedFields = { } for thisField , newValue in newData . items ( ) : defaultValue = thisField . getDefaultValue ( ) currentValue = currentData . get ( thisField , defaultValue ) fieldIsUpdated = False if currentValue != newValue : fieldIsUpdated = True elif cascadeObjects is True and issubclass ( thisField . __class__ , IRForeignLinkFieldBase ) : if currentValue . isFetched ( ) : oldObjs = currentValue . getObjs ( ) newObjs = newValue . getObjs ( ) if oldObjs != newObjs : fieldIsUpdated = True else : for i in range ( len ( oldObjs ) ) : if not oldObjs [ i ] . hasSameValues ( newObjs [ i ] , cascadeObjects = True ) : fieldIsUpdated = True break if fieldIsUpdated is True : updatedFields [ thisField ] = ( currentValue , newValue ) setattr ( self , thisField , newValue ) self . _origData [ thisField ] = newDataObj . _origData [ thisField ] return updatedFields
9995	def del_attr ( self , name ) : if name in self . namespace : if name in self . cells : self . del_cells ( name ) elif name in self . spaces : self . del_space ( name ) elif name in self . refs : self . del_ref ( name ) else : raise RuntimeError ( "Must not happen" ) else : raise KeyError ( "'%s' not found in Space '%s'" % ( name , self . name ) )
9469	def conference_list ( self , call_params ) : path = '/' + self . api_version + '/ConferenceList/' method = 'POST' return self . request ( path , method , call_params )
3530	def get_identity ( context , prefix = None , identity_func = None , user = None ) : if prefix is not None : try : return context [ '%s_identity' % prefix ] except KeyError : pass try : return context [ 'analytical_identity' ] except KeyError : pass if getattr ( settings , 'ANALYTICAL_AUTO_IDENTIFY' , True ) : try : if user is None : user = get_user_from_context ( context ) if get_user_is_authenticated ( user ) : if identity_func is not None : return identity_func ( user ) else : return user . get_username ( ) except ( KeyError , AttributeError ) : pass return None
10060	def jsonschemas ( self ) : _jsonschemas = { k : v [ 'jsonschema' ] for k , v in self . app . config [ 'DEPOSIT_RECORDS_UI_ENDPOINTS' ] . items ( ) if 'jsonschema' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT_DEFAULT_JSONSCHEMA' ] , _jsonschemas )
194	def AssertLambda ( func_images = None , func_heatmaps = None , func_keypoints = None , func_polygons = None , name = None , deterministic = False , random_state = None ) : def func_images_assert ( images , random_state , parents , hooks ) : ia . do_assert ( func_images ( images , random_state , parents , hooks ) , "Input images did not fulfill user-defined assertion in AssertLambda." ) return images def func_heatmaps_assert ( heatmaps , random_state , parents , hooks ) : ia . do_assert ( func_heatmaps ( heatmaps , random_state , parents , hooks ) , "Input heatmaps did not fulfill user-defined assertion in AssertLambda." ) return heatmaps def func_keypoints_assert ( keypoints_on_images , random_state , parents , hooks ) : ia . do_assert ( func_keypoints ( keypoints_on_images , random_state , parents , hooks ) , "Input keypoints did not fulfill user-defined assertion in AssertLambda." ) return keypoints_on_images def func_polygons_assert ( polygons_on_images , random_state , parents , hooks ) : ia . do_assert ( func_polygons ( polygons_on_images , random_state , parents , hooks ) , "Input polygons did not fulfill user-defined assertion in AssertLambda." ) return polygons_on_images if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Lambda ( func_images_assert if func_images is not None else None , func_heatmaps_assert if func_heatmaps is not None else None , func_keypoints_assert if func_keypoints is not None else None , func_polygons_assert if func_polygons is not None else None , name = name , deterministic = deterministic , random_state = random_state )
8685	def _encrypt ( self , value ) : value = json . dumps ( value ) with warnings . catch_warnings ( ) : warnings . simplefilter ( "ignore" ) encrypted_value = self . cipher . encrypt ( value . encode ( 'utf8' ) ) hexified_value = binascii . hexlify ( encrypted_value ) . decode ( 'ascii' ) return hexified_value
5555	def _element_at_zoom ( name , element , zoom ) : if isinstance ( element , dict ) : if "format" in element : return element out_elements = { } for sub_name , sub_element in element . items ( ) : out_element = _element_at_zoom ( sub_name , sub_element , zoom ) if name == "input" : out_elements [ sub_name ] = out_element elif out_element is not None : out_elements [ sub_name ] = out_element if len ( out_elements ) == 1 and name != "input" : return next ( iter ( out_elements . values ( ) ) ) if len ( out_elements ) == 0 : return None return out_elements elif isinstance ( name , str ) : if name . startswith ( "zoom" ) : return _filter_by_zoom ( conf_string = name . strip ( "zoom" ) . strip ( ) , zoom = zoom , element = element ) else : return element else : return element
2501	def value_error ( self , key , bad_value ) : msg = ERROR_MESSAGES [ key ] . format ( bad_value ) self . logger . log ( msg ) self . error = True
10414	def node_exclusion_filter_builder ( nodes : Iterable [ BaseEntity ] ) -> NodePredicate : node_set = set ( nodes ) def exclusion_filter ( _ : BELGraph , node : BaseEntity ) -> bool : return node not in node_set return exclusion_filter
6114	def resized_scaled_array_from_array ( self , new_shape , new_centre_pixels = None , new_centre_arcsec = None ) : if new_centre_pixels is None and new_centre_arcsec is None : new_centre = ( - 1 , - 1 ) elif new_centre_pixels is not None and new_centre_arcsec is None : new_centre = new_centre_pixels elif new_centre_pixels is None and new_centre_arcsec is not None : new_centre = self . arc_second_coordinates_to_pixel_coordinates ( arc_second_coordinates = new_centre_arcsec ) else : raise exc . DataException ( 'You have supplied two centres (pixels and arc-seconds) to the resize scaled' 'array function' ) return self . new_with_array ( array = array_util . resized_array_2d_from_array_2d_and_resized_shape ( array_2d = self , resized_shape = new_shape , origin = new_centre ) )
12501	def _smooth_data_array ( arr , affine , fwhm , copy = True ) : if arr . dtype . kind == 'i' : if arr . dtype == np . int64 : arr = arr . astype ( np . float64 ) else : arr = arr . astype ( np . float32 ) if copy : arr = arr . copy ( ) arr [ np . logical_not ( np . isfinite ( arr ) ) ] = 0 try : affine = affine [ : 3 , : 3 ] fwhm_sigma_ratio = np . sqrt ( 8 * np . log ( 2 ) ) vox_size = np . sqrt ( np . sum ( affine ** 2 , axis = 0 ) ) sigma = fwhm / ( fwhm_sigma_ratio * vox_size ) for n , s in enumerate ( sigma ) : ndimage . gaussian_filter1d ( arr , s , output = arr , axis = n ) except : raise ValueError ( 'Error smoothing the array.' ) else : return arr
1119	def listdir ( path ) : try : cached_mtime , list = cache [ path ] del cache [ path ] except KeyError : cached_mtime , list = - 1 , [ ] mtime = os . stat ( path ) . st_mtime if mtime != cached_mtime : list = os . listdir ( path ) list . sort ( ) cache [ path ] = mtime , list return list
472	def build_reverse_dictionary ( word_to_id ) : reverse_dictionary = dict ( zip ( word_to_id . values ( ) , word_to_id . keys ( ) ) ) return reverse_dictionary
3800	def Bahadori_gas ( T , MW ) : r A = [ 4.3931323468E-1 , - 3.88001122207E-2 , 9.28616040136E-4 , - 6.57828995724E-6 ] B = [ - 2.9624238519E-3 , 2.67956145820E-4 , - 6.40171884139E-6 , 4.48579040207E-8 ] C = [ 7.54249790107E-6 , - 6.46636219509E-7 , 1.5124510261E-8 , - 1.0376480449E-10 ] D = [ - 6.0988433456E-9 , 5.20752132076E-10 , - 1.19425545729E-11 , 8.0136464085E-14 ] X , Y = T , MW a = A [ 0 ] + B [ 0 ] * X + C [ 0 ] * X ** 2 + D [ 0 ] * X ** 3 b = A [ 1 ] + B [ 1 ] * X + C [ 1 ] * X ** 2 + D [ 1 ] * X ** 3 c = A [ 2 ] + B [ 2 ] * X + C [ 2 ] * X ** 2 + D [ 2 ] * X ** 3 d = A [ 3 ] + B [ 3 ] * X + C [ 3 ] * X ** 2 + D [ 3 ] * X ** 3 return a + b * Y + c * Y ** 2 + d * Y ** 3
6345	def stem ( self , word ) : terminate = False intact = True while not terminate : for n in range ( 6 , 0 , - 1 ) : if word [ - n : ] in self . _rule_table [ n ] : accept = False if len ( self . _rule_table [ n ] [ word [ - n : ] ] ) < 4 : for rule in self . _rule_table [ n ] [ word [ - n : ] ] : ( word , accept , intact , terminate , ) = self . _apply_rule ( word , rule , intact , terminate ) if accept : break else : rule = self . _rule_table [ n ] [ word [ - n : ] ] ( word , accept , intact , terminate ) = self . _apply_rule ( word , rule , intact , terminate ) if accept : break else : break return word
13512	def is_colour ( value ) : global PREDEFINED , HEX_MATCH , RGB_MATCH , RGBA_MATCH , HSL_MATCH , HSLA_MATCH value = value . strip ( ) if HEX_MATCH . match ( value ) or RGB_MATCH . match ( value ) or RGBA_MATCH . match ( value ) or HSL_MATCH . match ( value ) or HSLA_MATCH . match ( value ) or value in PREDEFINED : return True return False
4298	def _convert_config_to_stdin ( config , parser ) : keys_empty_values_not_pass = ( '--extra-settings' , '--languages' , '--requirements' , '--template' , '--timezone' ) args = [ ] for key , val in config . items ( SECTION ) : keyp = '--{0}' . format ( key ) action = parser . _option_string_actions [ keyp ] if action . const : try : if config . getboolean ( SECTION , key ) : args . append ( keyp ) except ValueError : args . extend ( [ keyp , val ] ) elif any ( [ i for i in keys_empty_values_not_pass if i in action . option_strings ] ) : if val != '' : args . extend ( [ keyp , val ] ) else : args . extend ( [ keyp , val ] ) return args
11843	def step ( self ) : if not self . is_done ( ) : actions = [ agent . program ( self . percept ( agent ) ) for agent in self . agents ] for ( agent , action ) in zip ( self . agents , actions ) : self . execute_action ( agent , action ) self . exogenous_change ( )
8982	def _set_pixel ( self , x , y , color ) : if not self . is_in_bounds ( x , y ) : return rgb = self . _convert_rrggbb_to_image_color ( color ) x -= self . _min_x y -= self . _min_y self . _image . putpixel ( ( x , y ) , rgb )
8083	def transform ( self , mode = None ) : if mode : self . _canvas . mode = mode return self . _canvas . mode
9719	async def take_control ( self , password ) : cmd = "takecontrol %s" % password return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
12608	def _query_sample ( sample , operators = '__eq__' ) : if isinstance ( operators , str ) : operators = [ operators ] * len ( sample ) if len ( sample ) != len ( operators ) : raise ValueError ( 'Expected `operators` to be a string or a list with the same' ' length as `field_names` ({}), got {}.' . format ( len ( sample ) , operators ) ) queries = [ ] for i , fn in enumerate ( sample ) : fv = sample [ fn ] op = operators [ i ] queries . append ( _build_query ( field_name = fn , field_value = fv , operator = op ) ) return _concat_queries ( queries , operators = '__and__' )
5161	def __intermediate_addresses ( self , interface ) : address_list = self . get_copy ( interface , 'addresses' ) if not address_list : return [ { 'proto' : 'none' } ] result = [ ] static = { } dhcp = [ ] for address in address_list : family = address . get ( 'family' ) if address [ 'proto' ] == 'dhcp' : address [ 'proto' ] = 'dhcp' if family == 'ipv4' else 'dhcpv6' dhcp . append ( self . __intermediate_address ( address ) ) continue if 'gateway' in address : uci_key = 'gateway' if family == 'ipv4' else 'ip6gw' interface [ uci_key ] = address [ 'gateway' ] address_key = 'ipaddr' if family == 'ipv4' else 'ip6addr' static . setdefault ( address_key , [ ] ) static [ address_key ] . append ( '{address}/{mask}' . format ( ** address ) ) static . update ( self . __intermediate_address ( address ) ) if static : if len ( static . get ( 'ipaddr' , [ ] ) ) == 1 : network = ip_interface ( six . text_type ( static [ 'ipaddr' ] [ 0 ] ) ) static [ 'ipaddr' ] = str ( network . ip ) static [ 'netmask' ] = str ( network . netmask ) if len ( static . get ( 'ip6addr' , [ ] ) ) == 1 : static [ 'ip6addr' ] = static [ 'ip6addr' ] [ 0 ] result . append ( static ) if dhcp : result += dhcp return result
1774	def invalidate_cache ( cpu , address , size ) : cache = cpu . instruction_cache for offset in range ( size ) : if address + offset in cache : del cache [ address + offset ]
1040	def line ( self ) : line , column = self . source_buffer . decompose_position ( self . begin_pos ) return line
1662	def FilesBelongToSameModule ( filename_cc , filename_h ) : fileinfo_cc = FileInfo ( filename_cc ) if not fileinfo_cc . Extension ( ) . lstrip ( '.' ) in GetNonHeaderExtensions ( ) : return ( False , '' ) fileinfo_h = FileInfo ( filename_h ) if not fileinfo_h . Extension ( ) . lstrip ( '.' ) in GetHeaderExtensions ( ) : return ( False , '' ) filename_cc = filename_cc [ : - ( len ( fileinfo_cc . Extension ( ) ) ) ] matched_test_suffix = Search ( _TEST_FILE_SUFFIX , fileinfo_cc . BaseName ( ) ) if matched_test_suffix : filename_cc = filename_cc [ : - len ( matched_test_suffix . group ( 1 ) ) ] filename_cc = filename_cc . replace ( '/public/' , '/' ) filename_cc = filename_cc . replace ( '/internal/' , '/' ) filename_h = filename_h [ : - ( len ( fileinfo_h . Extension ( ) ) ) ] if filename_h . endswith ( '-inl' ) : filename_h = filename_h [ : - len ( '-inl' ) ] filename_h = filename_h . replace ( '/public/' , '/' ) filename_h = filename_h . replace ( '/internal/' , '/' ) files_belong_to_same_module = filename_cc . endswith ( filename_h ) common_path = '' if files_belong_to_same_module : common_path = filename_cc [ : - len ( filename_h ) ] return files_belong_to_same_module , common_path
1537	def add_spec ( self , * specs ) : for spec in specs : if not isinstance ( spec , HeronComponentSpec ) : raise TypeError ( "Argument to add_spec needs to be HeronComponentSpec, given: %s" % str ( spec ) ) if spec . name is None : raise ValueError ( "TopologyBuilder cannot take a spec without name" ) if spec . name == "config" : raise ValueError ( "config is a reserved name" ) if spec . name in self . _specs : raise ValueError ( "Attempting to add duplicate spec name: %r %r" % ( spec . name , spec ) ) self . _specs [ spec . name ] = spec
13450	def field_value ( self , admin_model , instance , field_name ) : _ , _ , value = lookup_field ( field_name , instance , admin_model ) return value
13575	def select ( course = False , tid = None , auto = False ) : if course : update ( course = True ) course = None try : course = Course . get_selected ( ) except NoCourseSelected : pass ret = { } if not tid : ret = Menu . launch ( "Select a course" , Course . select ( ) . execute ( ) , course ) else : ret [ "item" ] = Course . get ( Course . tid == tid ) if "item" in ret : ret [ "item" ] . set_select ( ) update ( ) if ret [ "item" ] . path == "" : select_a_path ( auto = auto ) skip ( ) return else : print ( "You can select the course with `tmc select --course`" ) return else : selected = None try : selected = Exercise . get_selected ( ) except NoExerciseSelected : pass ret = { } if not tid : ret = Menu . launch ( "Select an exercise" , Course . get_selected ( ) . exercises , selected ) else : ret [ "item" ] = Exercise . byid ( tid ) if "item" in ret : ret [ "item" ] . set_select ( ) print ( "Selected {}" . format ( ret [ "item" ] ) )
4680	def getAccountsFromPublicKey ( self , pub ) : names = self . rpc . get_key_references ( [ str ( pub ) ] ) [ 0 ] for name in names : yield name
3978	def _get_referenced_libs ( specs ) : active_libs = set ( ) for app_spec in specs [ 'apps' ] . values ( ) : for lib in app_spec [ 'depends' ] [ 'libs' ] : active_libs . add ( lib ) return active_libs
12674	def subset ( * args ) : if args and isinstance ( args [ 0 ] , dataframe . DataFrame ) : return args [ 0 ] . subset ( * args [ 1 : ] ) elif not args : raise ValueError ( "No arguments provided" ) else : return pipeable . Pipeable ( pipeable . PipingMethod . SUBSET , * args )
2492	def create_review_node ( self , review ) : review_node = BNode ( ) type_triple = ( review_node , RDF . type , self . spdx_namespace . Review ) self . graph . add ( type_triple ) reviewer_node = Literal ( review . reviewer . to_value ( ) ) self . graph . add ( ( review_node , self . spdx_namespace . reviewer , reviewer_node ) ) reviewed_date_node = Literal ( review . review_date_iso_format ) reviewed_triple = ( review_node , self . spdx_namespace . reviewDate , reviewed_date_node ) self . graph . add ( reviewed_triple ) if review . has_comment : comment_node = Literal ( review . comment ) comment_triple = ( review_node , RDFS . comment , comment_node ) self . graph . add ( comment_triple ) return review_node
13560	def decorate ( msg = "" , waitmsg = "Please wait" ) : def decorator ( func ) : @ functools . wraps ( func ) def wrapper ( * args , ** kwargs ) : spin = Spinner ( msg = msg , waitmsg = waitmsg ) spin . start ( ) a = None try : a = func ( * args , ** kwargs ) except Exception as e : spin . msg = "Something went wrong: " spin . stop_spinning ( ) spin . join ( ) raise e spin . stop_spinning ( ) spin . join ( ) return a return wrapper return decorator
945	def _checkpointLabelFromCheckpointDir ( checkpointDir ) : assert checkpointDir . endswith ( g_defaultCheckpointExtension ) lastSegment = os . path . split ( checkpointDir ) [ 1 ] checkpointLabel = lastSegment [ 0 : - len ( g_defaultCheckpointExtension ) ] return checkpointLabel
6930	def xmatch_cplist_external_catalogs ( cplist , xmatchpkl , xmatchradiusarcsec = 2.0 , updateexisting = True , resultstodir = None ) : with open ( xmatchpkl , 'rb' ) as infd : xmd = pickle . load ( infd ) status_dict = { } for cpf in cplist : cpd = _read_checkplot_picklefile ( cpf ) try : xmatch_external_catalogs ( cpd , xmd , xmatchradiusarcsec = xmatchradiusarcsec , updatexmatch = updateexisting ) for xmi in cpd [ 'xmatch' ] : if cpd [ 'xmatch' ] [ xmi ] [ 'found' ] : LOGINFO ( 'checkplot %s: %s matched to %s, ' 'match dist: %s arcsec' % ( os . path . basename ( cpf ) , cpd [ 'objectid' ] , cpd [ 'xmatch' ] [ xmi ] [ 'name' ] , cpd [ 'xmatch' ] [ xmi ] [ 'distarcsec' ] ) ) if not resultstodir : outcpf = _write_checkplot_picklefile ( cpd , outfile = cpf ) else : xcpf = os . path . join ( resultstodir , os . path . basename ( cpf ) ) outcpf = _write_checkplot_picklefile ( cpd , outfile = xcpf ) status_dict [ cpf ] = outcpf except Exception as e : LOGEXCEPTION ( 'failed to match objects for %s' % cpf ) status_dict [ cpf ] = None return status_dict
11106	def get_pickling_errors ( obj , seen = None ) : if seen == None : seen = [ ] if hasattr ( obj , "__getstate__" ) : state = obj . __getstate__ ( ) else : return None if state == None : return 'object state is None' if isinstance ( state , tuple ) : if not isinstance ( state [ 0 ] , dict ) : state = state [ 1 ] else : state = state [ 0 ] . update ( state [ 1 ] ) result = { } for i in state : try : pickle . dumps ( state [ i ] , protocol = 2 ) except pickle . PicklingError as e : if not state [ i ] in seen : seen . append ( state [ i ] ) result [ i ] = get_pickling_errors ( state [ i ] , seen ) return result
13537	def _child_allowed ( self , child_rule ) : num_kids = self . node . children . count ( ) num_kids_allowed = len ( self . rule . children ) if not self . rule . multiple_paths : num_kids_allowed = 1 if num_kids >= num_kids_allowed : raise AttributeError ( 'Rule %s only allows %s children' % ( self . rule_name , self . num_kids_allowed ) ) for node in self . node . children . all ( ) : if node . data . rule_label == child_rule . class_label : raise AttributeError ( 'Child rule already exists' ) if child_rule not in self . rule . children : raise AttributeError ( 'Rule %s is not a valid child of Rule %s' % ( child_rule . __name__ , self . rule_name ) )
2490	def create_file_node ( self , doc_file ) : file_node = URIRef ( 'http://www.spdx.org/files#{id}' . format ( id = str ( doc_file . spdx_id ) ) ) type_triple = ( file_node , RDF . type , self . spdx_namespace . File ) self . graph . add ( type_triple ) name_triple = ( file_node , self . spdx_namespace . fileName , Literal ( doc_file . name ) ) self . graph . add ( name_triple ) if doc_file . has_optional_field ( 'comment' ) : comment_triple = ( file_node , RDFS . comment , Literal ( doc_file . comment ) ) self . graph . add ( comment_triple ) if doc_file . has_optional_field ( 'type' ) : ftype = self . spdx_namespace [ self . FILE_TYPES [ doc_file . type ] ] ftype_triple = ( file_node , self . spdx_namespace . fileType , ftype ) self . graph . add ( ftype_triple ) self . graph . add ( ( file_node , self . spdx_namespace . checksum , self . create_checksum_node ( doc_file . chk_sum ) ) ) conc_lic_node = self . license_or_special ( doc_file . conc_lics ) conc_lic_triple = ( file_node , self . spdx_namespace . licenseConcluded , conc_lic_node ) self . graph . add ( conc_lic_triple ) license_info_nodes = map ( self . license_or_special , doc_file . licenses_in_file ) for lic in license_info_nodes : triple = ( file_node , self . spdx_namespace . licenseInfoInFile , lic ) self . graph . add ( triple ) if doc_file . has_optional_field ( 'license_comment' ) : comment_triple = ( file_node , self . spdx_namespace . licenseComments , Literal ( doc_file . license_comment ) ) self . graph . add ( comment_triple ) cr_text_node = self . to_special_value ( doc_file . copyright ) cr_text_triple = ( file_node , self . spdx_namespace . copyrightText , cr_text_node ) self . graph . add ( cr_text_triple ) if doc_file . has_optional_field ( 'notice' ) : notice_triple = ( file_node , self . spdx_namespace . noticeText , doc_file . notice ) self . graph . add ( notice_triple ) contrib_nodes = map ( lambda c : Literal ( c ) , doc_file . contributors ) contrib_triples = [ ( file_node , self . spdx_namespace . fileContributor , node ) for node in contrib_nodes ] for triple in contrib_triples : self . graph . add ( triple ) return file_node
8174	def goal ( self , x , y , z , d = 50.0 ) : return ( x - self . x ) / d , ( y - self . y ) / d , ( z - self . z ) / d
11554	def disable_digital_reporting ( self , pin ) : port = pin // 8 command = [ self . _command_handler . REPORT_DIGITAL + port , self . REPORTING_DISABLE ] self . _command_handler . send_command ( command )
7758	def send ( self , stanza ) : if self . uplink : self . uplink . send ( stanza ) else : raise NoRouteError ( "No route for stanza" )
9333	def full_like ( array , value , dtype = None ) : shared = empty_like ( array , dtype ) shared [ : ] = value return shared
6777	def iter_dict_differences ( a , b ) : common_keys = set ( a ) . union ( b ) for k in common_keys : a_value = a . get ( k ) b_value = b . get ( k ) if a_value != b_value : yield k , ( a_value , b_value )
6696	def is_installed ( pkg_name ) : with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : res = run ( "dpkg -s %(pkg_name)s" % locals ( ) ) for line in res . splitlines ( ) : if line . startswith ( "Status: " ) : status = line [ 8 : ] if "installed" in status . split ( ' ' ) : return True return False
11313	def update_cnum ( self ) : if "ConferencePaper" not in self . collections : cnums = record_get_field_values ( self . record , '773' , code = "w" ) for cnum in cnums : cnum_subs = [ ( "9" , "INSPIRE-CNUM" ) , ( "a" , cnum ) ] record_add_field ( self . record , "035" , subfields = cnum_subs )
3683	def calculate ( self , T , method ) : r if method == WAGNER_MCGARRY : Psat = Wagner_original ( T , self . WAGNER_MCGARRY_Tc , self . WAGNER_MCGARRY_Pc , * self . WAGNER_MCGARRY_coefs ) elif method == WAGNER_POLING : Psat = Wagner ( T , self . WAGNER_POLING_Tc , self . WAGNER_POLING_Pc , * self . WAGNER_POLING_coefs ) elif method == ANTOINE_EXTENDED_POLING : Psat = TRC_Antoine_extended ( T , * self . ANTOINE_EXTENDED_POLING_coefs ) elif method == ANTOINE_POLING : A , B , C = self . ANTOINE_POLING_coefs Psat = Antoine ( T , A , B , C , base = 10.0 ) elif method == DIPPR_PERRY_8E : Psat = EQ101 ( T , * self . Perrys2_8_coeffs ) elif method == VDI_PPDS : Psat = Wagner ( T , self . VDI_PPDS_Tc , self . VDI_PPDS_Pc , * self . VDI_PPDS_coeffs ) elif method == COOLPROP : Psat = PropsSI ( 'P' , 'T' , T , 'Q' , 0 , self . CASRN ) elif method == BOILING_CRITICAL : Psat = boiling_critical_relation ( T , self . Tb , self . Tc , self . Pc ) elif method == LEE_KESLER_PSAT : Psat = Lee_Kesler ( T , self . Tc , self . Pc , self . omega ) elif method == AMBROSE_WALTON : Psat = Ambrose_Walton ( T , self . Tc , self . Pc , self . omega ) elif method == SANJARI : Psat = Sanjari ( T , self . Tc , self . Pc , self . omega ) elif method == EDALAT : Psat = Edalat ( T , self . Tc , self . Pc , self . omega ) elif method == EOS : Psat = self . eos [ 0 ] . Psat ( T ) elif method in self . tabular_data : Psat = self . interpolate ( T , method ) return Psat
4440	async def _playat ( self , ctx , index : int ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if index < 1 : return await ctx . send ( 'Invalid specified index.' ) if len ( player . queue ) < index : return await ctx . send ( 'This index exceeds the queue\'s length.' ) await player . play_at ( index - 1 )
11575	def encoder_data ( self , data ) : prev_val = self . digital_response_table [ data [ self . RESPONSE_TABLE_MODE ] ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] val = int ( ( data [ self . MSB ] << 7 ) + data [ self . LSB ] ) if val > 8192 : val -= 16384 pin = data [ 0 ] with self . pymata . data_lock : self . digital_response_table [ data [ self . RESPONSE_TABLE_MODE ] ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] = val if prev_val != val : callback = self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_CALLBACK ] if callback is not None : callback ( [ self . pymata . ENCODER , pin , self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] ] )
12404	def bump ( self , bump_reqs = None , ** kwargs ) : bumps = { } for existing_req in sorted ( self . requirements ( ) , key = lambda r : r . project_name ) : if bump_reqs and existing_req . project_name not in bump_reqs : continue bump_reqs . check ( existing_req ) try : bump = self . _bump ( existing_req , bump_reqs . get ( existing_req . project_name ) ) if bump : bumps [ bump . name ] = bump bump_reqs . check ( bump ) except Exception as e : if bump_reqs and bump_reqs . get ( existing_req . project_name ) and all ( r . required_by is None for r in bump_reqs . get ( existing_req . project_name ) ) : raise else : log . warn ( e ) for reqs in bump_reqs . required_requirements ( ) . values ( ) : name = reqs [ 0 ] . project_name if name not in bumps and self . should_add ( name ) : try : bump = self . _bump ( None , reqs ) if bump : bumps [ bump . name ] = bump bump_reqs . check ( bump ) except Exception as e : if all ( r . required_by is None for r in reqs ) : raise else : log . warn ( e ) self . bumps . update ( bumps . values ( ) ) return bumps . values ( )
12821	def _str_to_path ( s , result_type ) : assert isinstance ( s , str ) if isinstance ( s , bytes ) and result_type is text_type : return s . decode ( 'ascii' ) elif isinstance ( s , text_type ) and result_type is bytes : return s . encode ( 'ascii' ) return s
9981	def is_funcdef ( src ) : module_node = ast . parse ( dedent ( src ) ) if len ( module_node . body ) == 1 and isinstance ( module_node . body [ 0 ] , ast . FunctionDef ) : return True else : return False
8492	def main ( ) : parser = argparse . ArgumentParser ( description = "Helper for working with " "pyconfigs" ) target_group = parser . add_mutually_exclusive_group ( ) target_group . add_argument ( '-f' , '--filename' , help = "parse an individual file or directory" , metavar = 'F' ) target_group . add_argument ( '-m' , '--module' , help = "parse a package or module, recursively looking inside it" , metavar = 'M' ) parser . add_argument ( '-v' , '--view-call' , help = "show the actual pyconfig call made (default: show namespace)" , action = 'store_true' ) parser . add_argument ( '-l' , '--load-configs' , help = "query the currently set value for each key found" , action = 'store_true' ) key_group = parser . add_mutually_exclusive_group ( ) key_group . add_argument ( '-a' , '--all' , help = "show keys which don't have defaults set" , action = 'store_true' ) key_group . add_argument ( '-k' , '--only-keys' , help = "show a list of discovered keys without values" , action = 'store_true' ) parser . add_argument ( '-n' , '--natural-sort' , help = "sort by filename and line (default: alphabetical by key)" , action = 'store_true' ) parser . add_argument ( '-s' , '--source' , help = "show source annotations (implies --natural-sort)" , action = 'store_true' ) parser . add_argument ( '-c' , '--color' , help = "toggle output colors (default: %s)" % bool ( pygments ) , action = 'store_const' , default = bool ( pygments ) , const = ( not bool ( pygments ) ) ) args = parser . parse_args ( ) if args . color and not pygments : _error ( "Pygments is required for color output.\n" " pip install pygments" ) if args . module : _handle_module ( args ) if args . filename : _handle_file ( args )
5106	def _current_color ( self , which = 0 ) : if which == 1 : color = self . colors [ 'edge_loop_color' ] elif which == 2 : color = self . colors [ 'vertex_color' ] else : div = self . coloring_sensitivity * self . num_servers + 1. tmp = 1. - min ( self . num_system / div , 1 ) if self . edge [ 0 ] == self . edge [ 1 ] : color = [ i * tmp for i in self . colors [ 'vertex_fill_color' ] ] color [ 3 ] = 1.0 else : color = [ i * tmp for i in self . colors [ 'edge_color' ] ] color [ 3 ] = 1 / 2. return color
8231	def speed ( self , framerate = None ) : if framerate is not None : self . _speed = framerate self . _dynamic = True else : return self . _speed
10098	def update_template_version ( self , name , subject , template_id , version_id , text = '' , html = None , timeout = None ) : if ( html ) : payload = { 'name' : name , 'subject' : subject , 'html' : html , 'text' : text } else : payload = { 'name' : name , 'subject' : subject , 'text' : text } return self . _api_request ( self . TEMPLATES_VERSION_ENDPOINT % ( template_id , version_id ) , self . HTTP_PUT , payload = payload , timeout = timeout )
8908	def fetch_by_name ( self , name ) : service = self . collection . find_one ( { 'name' : name } ) if not service : raise ServiceNotFound return Service ( service )
4874	def validate_lms_user_id ( self , value ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) try : return models . EnterpriseCustomerUser . objects . get ( user_id = value , enterprise_customer = enterprise_customer ) except models . EnterpriseCustomerUser . DoesNotExist : pass return None
11060	def stop ( self ) : if self . webserver is not None : self . webserver . stop ( ) if not self . test_mode : self . plugins . save_state ( )
618	def parseBool ( s ) : l = s . lower ( ) if l in ( "true" , "t" , "1" ) : return True if l in ( "false" , "f" , "0" ) : return False raise Exception ( "Unable to convert string '%s' to a boolean value" % s )
7732	def get_join_info ( self ) : x = self . get_muc_child ( ) if not x : return None if not isinstance ( x , MucX ) : return None return x
7045	def all_nonperiodic_features ( times , mags , errs , magsarefluxes = False , stetson_weightbytimediff = True ) : finiteind = npisfinite ( times ) & npisfinite ( mags ) & npisfinite ( errs ) ftimes , fmags , ferrs = times [ finiteind ] , mags [ finiteind ] , errs [ finiteind ] nzind = npnonzero ( ferrs ) ftimes , fmags , ferrs = ftimes [ nzind ] , fmags [ nzind ] , ferrs [ nzind ] xfeatures = nonperiodic_lightcurve_features ( times , mags , errs , magsarefluxes = magsarefluxes ) stetj = stetson_jindex ( ftimes , fmags , ferrs , weightbytimediff = stetson_weightbytimediff ) stetk = stetson_kindex ( fmags , ferrs ) xfeatures . update ( { 'stetsonj' : stetj , 'stetsonk' : stetk } ) return xfeatures
5922	def create ( logger_name , logfile = 'gromacs.log' ) : logger = logging . getLogger ( logger_name ) logger . setLevel ( logging . DEBUG ) logfile = logging . FileHandler ( logfile ) logfile_formatter = logging . Formatter ( '%(asctime)s %(name)-12s %(levelname)-8s %(message)s' ) logfile . setFormatter ( logfile_formatter ) logger . addHandler ( logfile ) console = logging . StreamHandler ( ) console . setLevel ( logging . INFO ) formatter = logging . Formatter ( '%(name)-12s: %(levelname)-8s %(message)s' ) console . setFormatter ( formatter ) logger . addHandler ( console ) return logger
1116	def _convert_flags ( self , fromlist , tolist , flaglist , context , numlines ) : toprefix = self . _prefix [ 1 ] next_id = [ '' ] * len ( flaglist ) next_href = [ '' ] * len ( flaglist ) num_chg , in_change = 0 , False last = 0 for i , flag in enumerate ( flaglist ) : if flag : if not in_change : in_change = True last = i i = max ( [ 0 , i - numlines ] ) next_id [ i ] = ' id="difflib_chg_%s_%d"' % ( toprefix , num_chg ) num_chg += 1 next_href [ last ] = '<a href="#difflib_chg_%s_%d">n</a>' % ( toprefix , num_chg ) else : in_change = False if not flaglist : flaglist = [ False ] next_id = [ '' ] next_href = [ '' ] last = 0 if context : fromlist = [ '<td></td><td>&nbsp;No Differences Found&nbsp;</td>' ] tolist = fromlist else : fromlist = tolist = [ '<td></td><td>&nbsp;Empty File&nbsp;</td>' ] if not flaglist [ 0 ] : next_href [ 0 ] = '<a href="#difflib_chg_%s_0">f</a>' % toprefix next_href [ last ] = '<a href="#difflib_chg_%s_top">t</a>' % ( toprefix ) return fromlist , tolist , flaglist , next_href , next_id
13414	def removeLayout ( self , layout ) : for cnt in reversed ( range ( layout . count ( ) ) ) : item = layout . takeAt ( cnt ) widget = item . widget ( ) if widget is not None : widget . deleteLater ( ) else : self . removeLayout ( item . layout ( ) )
10301	def count_defaultdict ( dict_of_lists : Mapping [ X , List [ Y ] ] ) -> Mapping [ X , typing . Counter [ Y ] ] : return { k : Counter ( v ) for k , v in dict_of_lists . items ( ) }
8649	def delete_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'delete' , } endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
9094	def add_namespace_to_graph ( self , graph : BELGraph ) -> Namespace : namespace = self . upload_bel_namespace ( ) graph . namespace_url [ namespace . keyword ] = namespace . url self . _add_annotation_to_graph ( graph ) return namespace
3329	def acquire_read ( self , timeout = None ) : if timeout is not None : endtime = time ( ) + timeout me = currentThread ( ) self . __condition . acquire ( ) try : if self . __writer is me : self . __writercount += 1 return while True : if self . __writer is None : if self . __upgradewritercount or self . __pendingwriters : if me in self . __readers : self . __readers [ me ] += 1 return else : self . __readers [ me ] = self . __readers . get ( me , 0 ) + 1 return if timeout is not None : remaining = endtime - time ( ) if remaining <= 0 : raise RuntimeError ( "Acquiring read lock timed out" ) self . __condition . wait ( remaining ) else : self . __condition . wait ( ) finally : self . __condition . release ( )
2607	def update_memo ( self , task_id , task , r ) : if not self . memoize or not task [ 'memoize' ] : return if task [ 'hashsum' ] in self . memo_lookup_table : logger . info ( 'Updating appCache entry with latest %s:%s call' % ( task [ 'func_name' ] , task_id ) ) self . memo_lookup_table [ task [ 'hashsum' ] ] = r else : self . memo_lookup_table [ task [ 'hashsum' ] ] = r
10367	def complex_increases_activity ( graph : BELGraph , u : BaseEntity , v : BaseEntity , key : str ) -> bool : return ( isinstance ( u , ( ComplexAbundance , NamedComplexAbundance ) ) and complex_has_member ( graph , u , v ) and part_has_modifier ( graph [ u ] [ v ] [ key ] , OBJECT , ACTIVITY ) )
3544	def compute_exit_code ( config , exception = None ) : code = 0 if exception is not None : code = code | 1 if config . surviving_mutants > 0 : code = code | 2 if config . surviving_mutants_timeout > 0 : code = code | 4 if config . suspicious_mutants > 0 : code = code | 8 return code
3597	def list ( self , cat , ctr = None , nb_results = None , offset = None ) : path = LIST_URL + "?c=3&cat={}" . format ( requests . utils . quote ( cat ) ) if ctr is not None : path += "&ctr={}" . format ( requests . utils . quote ( ctr ) ) if nb_results is not None : path += "&n={}" . format ( requests . utils . quote ( str ( nb_results ) ) ) if offset is not None : path += "&o={}" . format ( requests . utils . quote ( str ( offset ) ) ) data = self . executeRequestApi2 ( path ) clusters = [ ] docs = [ ] if ctr is None : for pf in data . preFetch : for cluster in pf . response . payload . listResponse . doc : clusters . extend ( cluster . child ) return [ c . docid for c in clusters ] else : apps = [ ] for d in data . payload . listResponse . doc : for c in d . child : for a in c . child : apps . append ( utils . parseProtobufObj ( a ) ) return apps
9053	def posteriori_mean ( self ) : r from numpy_sugar . linalg import rsolve Sigma = self . posteriori_covariance ( ) eta = self . _ep . _posterior . eta return dot ( Sigma , eta + rsolve ( GLMM . covariance ( self ) , self . mean ( ) ) )
3720	def ionic_strength ( mis , zis ) : r return 0.5 * sum ( [ mi * zi * zi for mi , zi in zip ( mis , zis ) ] )
9630	def render_to_message ( self , extra_context = None , ** kwargs ) : if extra_context is None : extra_context = { } kwargs . setdefault ( 'headers' , { } ) . update ( self . headers ) context = self . get_context_data ( ** extra_context ) return self . message_class ( subject = self . render_subject ( context ) , body = self . render_body ( context ) , ** kwargs )
1124	def Loc ( kind , loc = None ) : @ llrule ( loc , lambda parser : [ kind ] ) def rule ( parser ) : result = parser . _accept ( kind ) if result is unmatched : return result return result . loc return rule
9756	def update ( ctx , name , description , tags ) : user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the experiment.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . experiment . update_experiment ( user , project_name , _experiment , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiment updated." ) get_experiment_details ( response )
12208	def add ( self , * entries ) : for entry in entries : if isinstance ( entry , string_types ) : self . _add_entries ( database . parse_string ( entry , bib_format = 'bibtex' ) ) else : self . _add_entries ( entry )
6160	def eye_plot ( x , L , S = 0 ) : plt . figure ( figsize = ( 6 , 4 ) ) idx = np . arange ( 0 , L + 1 ) plt . plot ( idx , x [ S : S + L + 1 ] , 'b' ) k_max = int ( ( len ( x ) - S ) / L ) - 1 for k in range ( 1 , k_max ) : plt . plot ( idx , x [ S + k * L : S + L + 1 + k * L ] , 'b' ) plt . grid ( ) plt . xlabel ( 'Time Index - n' ) plt . ylabel ( 'Amplitude' ) plt . title ( 'Eye Plot' ) return 0
5896	def render_toolbar ( context , config ) : quill_config = getattr ( quill_app , config ) t = template . loader . get_template ( quill_config [ 'toolbar_template' ] ) return t . render ( context )
4243	def _get_region ( self , ipnum ) : region_code = None country_code = None seek_country = self . _seek_country ( ipnum ) def get_region_code ( offset ) : region1 = chr ( offset // 26 + 65 ) region2 = chr ( offset % 26 + 65 ) return '' . join ( [ region1 , region2 ] ) if self . _databaseType == const . REGION_EDITION_REV0 : seek_region = seek_country - const . STATE_BEGIN_REV0 if seek_region >= 1000 : country_code = 'US' region_code = get_region_code ( seek_region - 1000 ) else : country_code = const . COUNTRY_CODES [ seek_region ] elif self . _databaseType == const . REGION_EDITION_REV1 : seek_region = seek_country - const . STATE_BEGIN_REV1 if seek_region < const . US_OFFSET : pass elif seek_region < const . CANADA_OFFSET : country_code = 'US' region_code = get_region_code ( seek_region - const . US_OFFSET ) elif seek_region < const . WORLD_OFFSET : country_code = 'CA' region_code = get_region_code ( seek_region - const . CANADA_OFFSET ) else : index = ( seek_region - const . WORLD_OFFSET ) // const . FIPS_RANGE if index < len ( const . COUNTRY_CODES ) : country_code = const . COUNTRY_CODES [ index ] elif self . _databaseType in const . CITY_EDITIONS : rec = self . _get_record ( ipnum ) region_code = rec . get ( 'region_code' ) country_code = rec . get ( 'country_code' ) return { 'country_code' : country_code , 'region_code' : region_code }
3948	def _timezone_format ( value ) : return timezone . make_aware ( value , timezone . get_current_timezone ( ) ) if getattr ( settings , 'USE_TZ' , False ) else value
604	def _addBase ( self , position , xlabel = None , ylabel = None ) : ax = self . _fig . add_subplot ( position ) ax . set_xlabel ( xlabel ) ax . set_ylabel ( ylabel ) return ax
749	def _removeUnlikelyPredictions ( cls , likelihoodsDict , minLikelihoodThreshold , maxPredictionsPerStep ) : maxVal = ( None , None ) for ( k , v ) in likelihoodsDict . items ( ) : if len ( likelihoodsDict ) <= 1 : break if maxVal [ 0 ] is None or v >= maxVal [ 1 ] : if maxVal [ 0 ] is not None and maxVal [ 1 ] < minLikelihoodThreshold : del likelihoodsDict [ maxVal [ 0 ] ] maxVal = ( k , v ) elif v < minLikelihoodThreshold : del likelihoodsDict [ k ] likelihoodsDict = dict ( sorted ( likelihoodsDict . iteritems ( ) , key = itemgetter ( 1 ) , reverse = True ) [ : maxPredictionsPerStep ] ) return likelihoodsDict
2356	def is_element_present ( self , strategy , locator ) : return self . driver_adapter . is_element_present ( strategy , locator , root = self . root )
13212	def build_jsonld ( self , url = None , code_url = None , ci_url = None , readme_url = None , license_id = None ) : jsonld = { '@context' : [ "https://raw.githubusercontent.com/codemeta/codemeta/2.0-rc/" "codemeta.jsonld" , "http://schema.org" ] , '@type' : [ 'Report' , 'SoftwareSourceCode' ] , 'language' : 'TeX' , 'reportNumber' : self . handle , 'name' : self . plain_title , 'description' : self . plain_abstract , 'author' : [ { '@type' : 'Person' , 'name' : author_name } for author_name in self . plain_authors ] , 'dateModified' : self . revision_datetime } try : jsonld [ 'articleBody' ] = self . plain_content jsonld [ 'fileFormat' ] = 'text/plain' except RuntimeError : self . _logger . exception ( 'Could not convert latex body to plain ' 'text for articleBody.' ) self . _logger . warning ( 'Falling back to tex source for articleBody' ) jsonld [ 'articleBody' ] = self . _tex jsonld [ 'fileFormat' ] = 'text/plain' if url is not None : jsonld [ '@id' ] = url jsonld [ 'url' ] = url else : jsonld [ '@id' ] = self . handle if code_url is not None : jsonld [ 'codeRepository' ] = code_url if ci_url is not None : jsonld [ 'contIntegration' ] = ci_url if readme_url is not None : jsonld [ 'readme' ] = readme_url if license_id is not None : jsonld [ 'license_id' ] = None return jsonld
4775	def contains_duplicates ( self ) : try : if len ( self . val ) != len ( set ( self . val ) ) : return self except TypeError : raise TypeError ( 'val is not iterable' ) self . _err ( 'Expected <%s> to contain duplicates, but did not.' % self . val )
10339	def spia_matrices_to_excel ( spia_matrices : Mapping [ str , pd . DataFrame ] , path : str ) -> None : writer = pd . ExcelWriter ( path , engine = 'xlsxwriter' ) for relation , df in spia_matrices . items ( ) : df . to_excel ( writer , sheet_name = relation , index = False ) writer . save ( )
1897	def _assert ( self , expression : Bool ) : assert isinstance ( expression , Bool ) smtlib = translate_to_smtlib ( expression ) self . _send ( '(assert %s)' % smtlib )
8922	def localize_datetime ( dt , tz_name = 'UTC' ) : tz_aware_dt = dt if dt . tzinfo is None : utc = pytz . timezone ( 'UTC' ) aware = utc . localize ( dt ) timezone = pytz . timezone ( tz_name ) tz_aware_dt = aware . astimezone ( timezone ) else : logger . warn ( 'tzinfo already set' ) return tz_aware_dt
1270	def _fire ( self , layers , the_plot ) : if the_plot . get ( 'last_marauder_shot' ) == the_plot . frame : return the_plot [ 'last_marauder_shot' ] = the_plot . frame col = np . random . choice ( np . nonzero ( layers [ 'X' ] . sum ( axis = 0 ) ) [ 0 ] ) row = np . nonzero ( layers [ 'X' ] [ : , col ] ) [ 0 ] [ - 1 ] + 1 self . _teleport ( ( row , col ) )
1407	def emit ( self , tup , tup_id = None , stream = Stream . DEFAULT_STREAM_ID , direct_task = None , need_task_ids = False ) : self . pplan_helper . check_output_schema ( stream , tup ) custom_target_task_ids = self . pplan_helper . choose_tasks_for_custom_grouping ( stream , tup ) self . pplan_helper . context . invoke_hook_emit ( tup , stream , None ) data_tuple = tuple_pb2 . HeronDataTuple ( ) data_tuple . key = 0 if direct_task is not None : if not isinstance ( direct_task , int ) : raise TypeError ( "direct_task argument needs to be an integer, given: %s" % str ( type ( direct_task ) ) ) data_tuple . dest_task_ids . append ( direct_task ) elif custom_target_task_ids is not None : for task_id in custom_target_task_ids : data_tuple . dest_task_ids . append ( task_id ) if tup_id is not None : tuple_info = TupleHelper . make_root_tuple_info ( stream , tup_id ) if self . acking_enabled : root = data_tuple . roots . add ( ) root . taskid = self . pplan_helper . my_task_id root . key = tuple_info . key self . in_flight_tuples [ tuple_info . key ] = tuple_info else : self . immediate_acks . append ( tuple_info ) tuple_size_in_bytes = 0 start_time = time . time ( ) for obj in tup : serialized = self . serializer . serialize ( obj ) data_tuple . values . append ( serialized ) tuple_size_in_bytes += len ( serialized ) serialize_latency_ns = ( time . time ( ) - start_time ) * system_constants . SEC_TO_NS self . spout_metrics . serialize_data_tuple ( stream , serialize_latency_ns ) super ( SpoutInstance , self ) . admit_data_tuple ( stream_id = stream , data_tuple = data_tuple , tuple_size_in_bytes = tuple_size_in_bytes ) self . total_tuples_emitted += 1 self . spout_metrics . update_emit_count ( stream ) if need_task_ids : sent_task_ids = custom_target_task_ids or [ ] if direct_task is not None : sent_task_ids . append ( direct_task ) return sent_task_ids
818	def grow ( self , rows , cols ) : if not self . hist_ : self . hist_ = SparseMatrix ( rows , cols ) self . rowSums_ = numpy . zeros ( rows , dtype = dtype ) self . colSums_ = numpy . zeros ( cols , dtype = dtype ) self . hack_ = None else : oldRows = self . hist_ . nRows ( ) oldCols = self . hist_ . nCols ( ) nextRows = max ( oldRows , rows ) nextCols = max ( oldCols , cols ) if ( oldRows < nextRows ) or ( oldCols < nextCols ) : self . hist_ . resize ( nextRows , nextCols ) if oldRows < nextRows : oldSums = self . rowSums_ self . rowSums_ = numpy . zeros ( nextRows , dtype = dtype ) self . rowSums_ [ 0 : len ( oldSums ) ] = oldSums self . hack_ = None if oldCols < nextCols : oldSums = self . colSums_ self . colSums_ = numpy . zeros ( nextCols , dtype = dtype ) self . colSums_ [ 0 : len ( oldSums ) ] = oldSums self . hack_ = None
3298	def string_to_xml ( text ) : try : return etree . XML ( text ) except Exception : _logger . error ( "Error parsing XML string. " "If lxml is not available, and unicode is involved, then " "installing lxml _may_ solve this issue." ) _logger . error ( "XML source: {}" . format ( text ) ) raise
7805	def verify_jid_against_common_name ( self , jid ) : if not self . common_names : return False for name in self . common_names : try : cn_jid = JID ( name ) except ValueError : continue if jid == cn_jid : return True return False
1186	def check_charset ( self , ctx , char ) : self . set_dispatcher . reset ( char ) save_position = ctx . code_position result = None while result is None : result = self . set_dispatcher . dispatch ( ctx . peek_code ( ) , ctx ) ctx . code_position = save_position return result
8543	def _get_password ( self , password , use_config = True , config_filename = None , use_keyring = HAS_KEYRING ) : if not password and use_config : if self . _config is None : self . _read_config ( config_filename ) password = self . _config . get ( "credentials" , "password" , fallback = None ) if not password and use_keyring : logger = logging . getLogger ( __name__ ) question = ( "Please enter your password for {} on {}: " . format ( self . username , self . host_base ) ) if HAS_KEYRING : password = keyring . get_password ( self . keyring_identificator , self . username ) if password is None : password = getpass . getpass ( question ) try : keyring . set_password ( self . keyring_identificator , self . username , password ) except keyring . errors . PasswordSetError as error : logger . warning ( "Storing password in keyring '%s' failed: %s" , self . keyring_identificator , error ) else : logger . warning ( "Install the 'keyring' Python module to store your password " "securely in your keyring!" ) password = self . _config . get ( "credentials" , "password" , fallback = None ) if password is None : password = getpass . getpass ( question ) store_plaintext_passwords = self . _config . get ( "preferences" , "store-plaintext-passwords" , fallback = None ) if store_plaintext_passwords != "no" : question = ( "Do you want to store your password in plain text in " + self . _config_filename ( ) ) answer = ask ( question , [ "yes" , "no" , "never" ] , "no" ) if answer == "yes" : self . _config . set ( "credentials" , "password" , password ) self . _save_config ( ) elif answer == "never" : if "preferences" not in self . _config : self . _config . add_section ( "preferences" ) self . _config . set ( "preferences" , "store-plaintext-passwords" , "no" ) self . _save_config ( ) return password
2207	def truepath ( path , real = False ) : path = expanduser ( path ) path = expandvars ( path ) if real : path = realpath ( path ) else : path = abspath ( path ) path = normpath ( path ) return path
2759	def get_all_load_balancers ( self ) : data = self . get_data ( "load_balancers" ) load_balancers = list ( ) for jsoned in data [ 'load_balancers' ] : load_balancer = LoadBalancer ( ** jsoned ) load_balancer . token = self . token load_balancer . health_check = HealthCheck ( ** jsoned [ 'health_check' ] ) load_balancer . sticky_sessions = StickySesions ( ** jsoned [ 'sticky_sessions' ] ) forwarding_rules = list ( ) for rule in jsoned [ 'forwarding_rules' ] : forwarding_rules . append ( ForwardingRule ( ** rule ) ) load_balancer . forwarding_rules = forwarding_rules load_balancers . append ( load_balancer ) return load_balancers
8934	def auto_detect ( workdir ) : if os . path . isdir ( os . path . join ( workdir , '.git' ) ) and os . path . isfile ( os . path . join ( workdir , '.git' , 'HEAD' ) ) : return 'git' return 'unknown'
5730	def read ( self , count ) : new_index = self . index + count if new_index > self . len : buf = self . raw_text [ self . index : ] else : buf = self . raw_text [ self . index : new_index ] self . index = new_index return buf
5008	def _create_session ( self ) : session = requests . Session ( ) session . timeout = self . SESSION_TIMEOUT oauth_access_token , expires_at = SAPSuccessFactorsAPIClient . get_oauth_access_token ( self . enterprise_configuration . sapsf_base_url , self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . sapsf_company_id , self . enterprise_configuration . sapsf_user_id , self . enterprise_configuration . user_type ) session . headers [ 'Authorization' ] = 'Bearer {}' . format ( oauth_access_token ) session . headers [ 'content-type' ] = 'application/json' self . session = session self . expires_at = expires_at
7390	def add_edges ( self ) : for group , edgelist in self . edges . items ( ) : for ( u , v , d ) in edgelist : self . draw_edge ( u , v , d , group )
1095	def escape ( pattern ) : "Escape all non-alphanumeric characters in pattern." s = list ( pattern ) alphanum = _alphanum for i , c in enumerate ( pattern ) : if c not in alphanum : if c == "\000" : s [ i ] = "\\000" else : s [ i ] = "\\" + c return pattern [ : 0 ] . join ( s )
8097	def path ( s , graph , path ) : def end ( n ) : r = n . r * 0.35 s . _ctx . oval ( n . x - r , n . y - r , r * 2 , r * 2 ) if path and len ( path ) > 1 and s . stroke : s . _ctx . nofill ( ) s . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a ) if s . name != DEFAULT : s . _ctx . strokewidth ( s . strokewidth ) else : s . _ctx . strokewidth ( s . strokewidth * 2 ) first = True for id in path : n = graph [ id ] if first : first = False s . _ctx . beginpath ( n . x , n . y ) end ( n ) else : s . _ctx . lineto ( n . x , n . y ) s . _ctx . endpath ( ) end ( n )
12241	def camel ( theta ) : x , y = theta obj = 2 * x ** 2 - 1.05 * x ** 4 + x ** 6 / 6 + x * y + y ** 2 grad = np . array ( [ 4 * x - 4.2 * x ** 3 + x ** 5 + y , x + 2 * y ] ) return obj , grad
9818	def upgrade ( self ) : if not self . is_valid : raise PolyaxonDeploymentConfigError ( 'Deployment type `{}` not supported' . format ( self . deployment_type ) ) if self . is_kubernetes : self . upgrade_on_kubernetes ( ) elif self . is_docker_compose : self . upgrade_on_docker_compose ( ) elif self . is_docker : self . upgrade_on_docker ( ) elif self . is_heroku : self . upgrade_on_heroku ( )
1756	def emulate_until ( self , target : int ) : self . _concrete = True self . _break_unicorn_at = target if self . emu : self . emu . _stop_at = target
454	def initialize_rnn_state ( state , feed_dict = None ) : if isinstance ( state , LSTMStateTuple ) : c = state . c . eval ( feed_dict = feed_dict ) h = state . h . eval ( feed_dict = feed_dict ) return c , h else : new_state = state . eval ( feed_dict = feed_dict ) return new_state
11865	def pointwise_product ( self , other , bn ) : "Multiply two factors, combining their variables." vars = list ( set ( self . vars ) | set ( other . vars ) ) cpt = dict ( ( event_values ( e , vars ) , self . p ( e ) * other . p ( e ) ) for e in all_events ( vars , bn , { } ) ) return Factor ( vars , cpt )
3509	def constraint_matrices ( model , array_type = 'dense' , include_vars = False , zero_tol = 1e-6 ) : if array_type not in ( 'DataFrame' , 'dense' ) and not dok_matrix : raise ValueError ( 'Sparse matrices require scipy' ) array_builder = { 'dense' : np . array , 'dok' : dok_matrix , 'lil' : lil_matrix , 'DataFrame' : pd . DataFrame , } [ array_type ] Problem = namedtuple ( "Problem" , [ "equalities" , "b" , "inequalities" , "bounds" , "variable_fixed" , "variable_bounds" ] ) equality_rows = [ ] inequality_rows = [ ] inequality_bounds = [ ] b = [ ] for const in model . constraints : lb = - np . inf if const . lb is None else const . lb ub = np . inf if const . ub is None else const . ub equality = ( ub - lb ) < zero_tol coefs = const . get_linear_coefficients ( model . variables ) coefs = [ coefs [ v ] for v in model . variables ] if equality : b . append ( lb if abs ( lb ) > zero_tol else 0.0 ) equality_rows . append ( coefs ) else : inequality_rows . append ( coefs ) inequality_bounds . append ( [ lb , ub ] ) var_bounds = np . array ( [ [ v . lb , v . ub ] for v in model . variables ] ) fixed = var_bounds [ : , 1 ] - var_bounds [ : , 0 ] < zero_tol results = Problem ( equalities = array_builder ( equality_rows ) , b = np . array ( b ) , inequalities = array_builder ( inequality_rows ) , bounds = array_builder ( inequality_bounds ) , variable_fixed = np . array ( fixed ) , variable_bounds = array_builder ( var_bounds ) ) return results
731	def _getW ( self ) : w = self . _w if type ( w ) is list : return w [ self . _random . getUInt32 ( len ( w ) ) ] else : return w
3822	async def easter_egg ( self , easter_egg_request ) : response = hangouts_pb2 . EasterEggResponse ( ) await self . _pb_request ( 'conversations/easteregg' , easter_egg_request , response ) return response
269	def detect_intraday ( positions , transactions , threshold = 0.25 ) : daily_txn = transactions . copy ( ) daily_txn . index = daily_txn . index . date txn_count = daily_txn . groupby ( level = 0 ) . symbol . nunique ( ) . sum ( ) daily_pos = positions . drop ( 'cash' , axis = 1 ) . replace ( 0 , np . nan ) return daily_pos . count ( axis = 1 ) . sum ( ) / txn_count < threshold
4358	def send_packet ( self , pkt ) : self . put_client_msg ( packet . encode ( pkt , self . json_dumps ) )
3993	def _load_ssh_auth_post_yosemite ( mac_username ) : user_id = subprocess . check_output ( [ 'id' , '-u' , mac_username ] ) ssh_auth_sock = subprocess . check_output ( [ 'launchctl' , 'asuser' , user_id , 'launchctl' , 'getenv' , 'SSH_AUTH_SOCK' ] ) . rstrip ( ) _set_ssh_auth_sock ( ssh_auth_sock )
10493	def clickMouseButtonLeft ( self , coord , interval = None ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags ) if interval : self . _postQueuedEvents ( interval = interval ) else : self . _postQueuedEvents ( )
11531	def _hashkey ( self , method , url , ** kwa ) : to_hash = '' . join ( [ str ( method ) , str ( url ) , str ( kwa . get ( 'data' , '' ) ) , str ( kwa . get ( 'params' , '' ) ) ] ) return hashlib . md5 ( to_hash . encode ( ) ) . hexdigest ( )
3797	def setup_a_alpha_and_derivatives ( self , i , T = None ) : r self . a , self . Tc , self . omega = self . ais [ i ] , self . Tcs [ i ] , self . omegas [ i ]
8831	def send ( self , s ) : self . _socket . send ( s . encode ( ) ) return self . read ( )
7704	def add_item ( self , item , replace = False ) : if item . jid in self . _jids : if replace : self . remove_item ( item . jid ) else : raise ValueError ( "JID already in the roster" ) index = len ( self . _items ) self . _items . append ( item ) self . _jids [ item . jid ] = index
5137	def add ( self , string , start , end , line ) : if string . strip ( ) : self . start_lineno = min ( self . start_lineno , start [ 0 ] ) self . end_lineno = max ( self . end_lineno , end [ 0 ] )
8373	def widget_changed ( self , widget , v ) : if v . type is NUMBER : self . bot . _namespace [ v . name ] = widget . get_value ( ) self . bot . _vars [ v . name ] . value = widget . get_value ( ) publish_event ( VARIABLE_UPDATED_EVENT , v ) elif v . type is BOOLEAN : self . bot . _namespace [ v . name ] = widget . get_active ( ) self . bot . _vars [ v . name ] . value = widget . get_active ( ) publish_event ( VARIABLE_UPDATED_EVENT , v ) elif v . type is TEXT : self . bot . _namespace [ v . name ] = widget . get_text ( ) self . bot . _vars [ v . name ] . value = widget . get_text ( ) publish_event ( VARIABLE_UPDATED_EVENT , v )
8051	def _darkest ( self ) : rgb , n = ( 1.0 , 1.0 , 1.0 ) , 3.0 for r , g , b in self : if r + g + b < n : rgb , n = ( r , g , b ) , r + g + b return rgb
2834	def platform_detect ( ) : pi = pi_version ( ) if pi is not None : return RASPBERRY_PI plat = platform . platform ( ) if plat . lower ( ) . find ( 'armv7l-with-debian' ) > - 1 : return BEAGLEBONE_BLACK elif plat . lower ( ) . find ( 'armv7l-with-ubuntu' ) > - 1 : return BEAGLEBONE_BLACK elif plat . lower ( ) . find ( 'armv7l-with-glibc2.4' ) > - 1 : return BEAGLEBONE_BLACK elif plat . lower ( ) . find ( 'tegra-aarch64-with-ubuntu' ) > - 1 : return JETSON_NANO try : import mraa if mraa . getPlatformName ( ) == 'MinnowBoard MAX' : return MINNOWBOARD except ImportError : pass return UNKNOWN
2330	def computeGaussKernel ( x ) : xnorm = np . power ( euclidean_distances ( x , x ) , 2 ) return np . exp ( - xnorm / ( 2.0 ) )
2667	def write_then_readinto ( self , out_buffer , in_buffer , * , out_start = 0 , out_end = None , in_start = 0 , in_end = None , stop = True ) : if out_end is None : out_end = len ( out_buffer ) if in_end is None : in_end = len ( in_buffer ) if hasattr ( self . i2c , 'writeto_then_readfrom' ) : if self . _debug : print ( "i2c_device.writeto_then_readfrom.out_buffer:" , [ hex ( i ) for i in out_buffer [ out_start : out_end ] ] ) self . i2c . writeto_then_readfrom ( self . device_address , out_buffer , in_buffer , out_start = out_start , out_end = out_end , in_start = in_start , in_end = in_end , stop = stop ) if self . _debug : print ( "i2c_device.writeto_then_readfrom.in_buffer:" , [ hex ( i ) for i in in_buffer [ in_start : in_end ] ] ) else : self . write ( out_buffer , start = out_start , end = out_end , stop = stop ) if self . _debug : print ( "i2c_device.write_then_readinto.write.out_buffer:" , [ hex ( i ) for i in out_buffer [ out_start : out_end ] ] ) self . readinto ( in_buffer , start = in_start , end = in_end ) if self . _debug : print ( "i2c_device.write_then_readinto.readinto.in_buffer:" , [ hex ( i ) for i in in_buffer [ in_start : in_end ] ] )
11692	def set_fields ( self , changeset ) : self . id = int ( changeset . get ( 'id' ) ) self . user = changeset . get ( 'user' ) self . uid = changeset . get ( 'uid' ) self . editor = changeset . get ( 'created_by' , None ) self . review_requested = changeset . get ( 'review_requested' , False ) self . host = changeset . get ( 'host' , 'Not reported' ) self . bbox = changeset . get ( 'bbox' ) . wkt self . comment = changeset . get ( 'comment' , 'Not reported' ) self . source = changeset . get ( 'source' , 'Not reported' ) self . imagery_used = changeset . get ( 'imagery_used' , 'Not reported' ) self . date = datetime . strptime ( changeset . get ( 'created_at' ) , '%Y-%m-%dT%H:%M:%SZ' ) self . suspicion_reasons = [ ] self . is_suspect = False self . powerfull_editor = False
12143	async def _push ( self , * args , ** kwargs ) : self . _data . append ( ( args , kwargs ) ) if self . _future is not None : future , self . _future = self . _future , None future . set_result ( True )
3050	def from_stream ( credential_filename ) : if credential_filename and os . path . isfile ( credential_filename ) : try : return _get_application_default_credential_from_file ( credential_filename ) except ( ApplicationDefaultCredentialsError , ValueError ) as error : extra_help = ( ' (provided as parameter to the ' 'from_stream() method)' ) _raise_exception_for_reading_json ( credential_filename , extra_help , error ) else : raise ApplicationDefaultCredentialsError ( 'The parameter passed to the from_stream() ' 'method should point to a file.' )
1403	def getTopologyInfo ( self , topologyName , cluster , role , environ ) : for ( topology_name , _ ) , topologyInfo in self . topologyInfos . items ( ) : executionState = topologyInfo [ "execution_state" ] if ( topologyName == topology_name and cluster == executionState [ "cluster" ] and environ == executionState [ "environ" ] ) : if not role or executionState . get ( "role" ) == role : return topologyInfo if role is not None : Log . info ( "Could not find topology info for topology: %s," "cluster: %s, role: %s, and environ: %s" , topologyName , cluster , role , environ ) else : Log . info ( "Could not find topology info for topology: %s," "cluster: %s and environ: %s" , topologyName , cluster , environ ) raise Exception ( "No topology found" )
6487	def _translate_hits ( es_response ) : def translate_result ( result ) : translated_result = copy . copy ( result ) data = translated_result . pop ( "_source" ) translated_result . update ( { "data" : data , "score" : translated_result [ "_score" ] } ) return translated_result def translate_facet ( result ) : terms = { term [ "term" ] : term [ "count" ] for term in result [ "terms" ] } return { "terms" : terms , "total" : result [ "total" ] , "other" : result [ "other" ] , } results = [ translate_result ( hit ) for hit in es_response [ "hits" ] [ "hits" ] ] response = { "took" : es_response [ "took" ] , "total" : es_response [ "hits" ] [ "total" ] , "max_score" : es_response [ "hits" ] [ "max_score" ] , "results" : results , } if "facets" in es_response : response [ "facets" ] = { facet : translate_facet ( es_response [ "facets" ] [ facet ] ) for facet in es_response [ "facets" ] } return response
7802	def _decode_asn1_string ( data ) : if isinstance ( data , BMPString ) : return bytes ( data ) . decode ( "utf-16-be" ) else : return bytes ( data ) . decode ( "utf-8" )
10461	def _glob_match ( self , pattern , string ) : return bool ( re . match ( fnmatch . translate ( pattern ) , string , re . M | re . U | re . L ) )
4031	def _decrypt ( self , value , encrypted_value ) : if sys . platform == 'win32' : return self . _decrypt_windows_chrome ( value , encrypted_value ) if value or ( encrypted_value [ : 3 ] != b'v10' ) : return value encrypted_value = encrypted_value [ 3 : ] encrypted_value_half_len = int ( len ( encrypted_value ) / 2 ) cipher = pyaes . Decrypter ( pyaes . AESModeOfOperationCBC ( self . key , self . iv ) ) decrypted = cipher . feed ( encrypted_value [ : encrypted_value_half_len ] ) decrypted += cipher . feed ( encrypted_value [ encrypted_value_half_len : ] ) decrypted += cipher . feed ( ) return decrypted . decode ( "utf-8" )
10401	def get_final_score ( self ) -> float : if not self . done_chomping ( ) : raise ValueError ( 'algorithm has not yet completed' ) return self . graph . nodes [ self . target_node ] [ self . tag ]
10660	def mass_fractions ( amounts ) : m = masses ( amounts ) m_total = sum ( m . values ( ) ) return { compound : m [ compound ] / m_total for compound in m . keys ( ) }
9256	def issues_to_log ( self , issues , pull_requests ) : log = "" sections_a , issues_a = self . parse_by_sections ( issues , pull_requests ) for section , s_issues in sections_a . items ( ) : log += self . generate_sub_section ( s_issues , section ) log += self . generate_sub_section ( issues_a , self . options . issue_prefix ) return log
5265	def backslashcase ( string ) : str1 = re . sub ( r"_" , r"\\" , snakecase ( string ) ) return str1
620	def parseSdr ( s ) : assert isinstance ( s , basestring ) sdr = [ int ( c ) for c in s if c in ( "0" , "1" ) ] if len ( sdr ) != len ( s ) : raise ValueError ( "The provided string %s is malformed. The string should " "have only 0's and 1's." ) return sdr
13026	def combine_files ( self , f1 , f2 , f3 ) : with open ( os . path . join ( self . datadir , f3 ) , 'wb' ) as new_file : with open ( os . path . join ( self . datadir , f1 ) , 'rb' ) as file_1 : new_file . write ( file_1 . read ( ) ) with open ( os . path . join ( self . datadir , f2 ) , 'rb' ) as file_2 : new_file . write ( file_2 . read ( ) )
13447	def authorize ( self ) : response = self . client . login ( username = self . USERNAME , password = self . PASSWORD ) self . assertTrue ( response ) self . authed = True
10659	def masses ( amounts ) : return { compound : mass ( compound , amounts [ compound ] ) for compound in amounts . keys ( ) }
9322	def refresh ( self , accept = MEDIA_TYPE_TAXII_V20 ) : self . refresh_information ( accept ) self . refresh_collections ( accept )
11025	def _get_container_port_mappings ( app ) : container = app [ 'container' ] port_mappings = container . get ( 'portMappings' ) if port_mappings is None and 'docker' in container : port_mappings = container [ 'docker' ] . get ( 'portMappings' ) return port_mappings
10040	def deposit_fetcher ( record_uuid , data ) : return FetchedPID ( provider = DepositProvider , pid_type = DepositProvider . pid_type , pid_value = str ( data [ '_deposit' ] [ 'id' ] ) , )
11278	def parse_address_list ( addrs ) : for addr in addrs . split ( ',' ) : elem = addr . split ( '-' ) if len ( elem ) == 1 : yield int ( elem [ 0 ] ) elif len ( elem ) == 2 : start , end = list ( map ( int , elem ) ) for i in range ( start , end + 1 ) : yield i else : raise ValueError ( 'format error in %s' % addr )
3456	def build_hugo_md ( filename , tag , bump ) : header = [ '+++\n' , 'date = "{}"\n' . format ( date . today ( ) . isoformat ( ) ) , 'title = "{}"\n' . format ( tag ) , 'author = "The COBRApy Team"\n' , 'release = "{}"\n' . format ( bump ) , '+++\n' , '\n' ] with open ( filename , "r" ) as file_h : content = insert_break ( file_h . readlines ( ) ) header . extend ( content ) with open ( filename , "w" ) as file_h : file_h . writelines ( header )
9061	def beta_covariance ( self ) : from numpy_sugar . linalg import ddot tX = self . _X [ "tX" ] Q = concatenate ( self . _QS [ 0 ] , axis = 1 ) S0 = self . _QS [ 1 ] D = self . v0 * S0 + self . v1 D = D . tolist ( ) + [ self . v1 ] * ( len ( self . _y ) - len ( D ) ) D = asarray ( D ) A = inv ( tX . T @ ( Q @ ddot ( 1 / D , Q . T @ tX ) ) ) VT = self . _X [ "VT" ] H = lstsq ( VT , A , rcond = None ) [ 0 ] return lstsq ( VT , H . T , rcond = None ) [ 0 ]
10212	def count_subgraph_sizes ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Counter [ int ] : return count_dict_values ( group_nodes_by_annotation ( graph , annotation ) )
741	def radiusForSpeed ( self , speed ) : overlap = 1.5 coordinatesPerTimestep = speed * self . timestep / self . scale radius = int ( round ( float ( coordinatesPerTimestep ) / 2 * overlap ) ) minRadius = int ( math . ceil ( ( math . sqrt ( self . w ) - 1 ) / 2 ) ) return max ( radius , minRadius )
547	def __flushPredictionCache ( self ) : if not self . __predictionCache : return if self . _predictionLogger is None : self . _createPredictionLogger ( ) startTime = time . time ( ) self . _predictionLogger . writeRecords ( self . __predictionCache , progressCB = self . __writeRecordsCallback ) self . _logger . info ( "Flushed prediction cache; numrows=%s; elapsed=%s sec." , len ( self . __predictionCache ) , time . time ( ) - startTime ) self . __predictionCache . clear ( )
248	def make_transaction_frame ( transactions ) : transaction_list = [ ] for dt in transactions . index : txns = transactions . loc [ dt ] if len ( txns ) == 0 : continue for txn in txns : txn = map_transaction ( txn ) transaction_list . append ( txn ) df = pd . DataFrame ( sorted ( transaction_list , key = lambda x : x [ 'dt' ] ) ) df [ 'txn_dollars' ] = - df [ 'amount' ] * df [ 'price' ] df . index = list ( map ( pd . Timestamp , df . dt . values ) ) return df
8662	def migrate ( src_path , src_passphrase , src_backend , dst_path , dst_passphrase , dst_backend ) : src_storage = STORAGE_MAPPING [ src_backend ] ( ** _parse_path_string ( src_path ) ) dst_storage = STORAGE_MAPPING [ dst_backend ] ( ** _parse_path_string ( dst_path ) ) src_stash = Stash ( src_storage , src_passphrase ) dst_stash = Stash ( dst_storage , dst_passphrase ) keys = src_stash . export ( ) dst_stash . load ( src_passphrase , keys = keys )
12540	def is_dicom_file ( filepath ) : if not os . path . exists ( filepath ) : raise IOError ( 'File {} not found.' . format ( filepath ) ) filename = os . path . basename ( filepath ) if filename == 'DICOMDIR' : return False try : _ = dicom . read_file ( filepath ) except Exception as exc : log . debug ( 'Checking if {0} was a DICOM, but returned ' 'False.' . format ( filepath ) ) return False return True
6922	def autocorr_magseries ( times , mags , errs , maxlags = 1000 , func = _autocorr_func3 , fillgaps = 0.0 , filterwindow = 11 , forcetimebin = None , sigclip = 3.0 , magsarefluxes = False , verbose = True ) : interpolated = fill_magseries_gaps ( times , mags , errs , fillgaps = fillgaps , forcetimebin = forcetimebin , sigclip = sigclip , magsarefluxes = magsarefluxes , filterwindow = filterwindow , verbose = verbose ) if not interpolated : print ( 'failed to interpolate light curve to minimum cadence!' ) return None itimes , imags = interpolated [ 'itimes' ] , interpolated [ 'imags' ] , if maxlags : lags = nparange ( 0 , maxlags ) else : lags = nparange ( itimes . size ) series_stdev = 1.483 * npmedian ( npabs ( imags ) ) if func != _autocorr_func3 : autocorr = nparray ( [ func ( imags , x , imags . size , 0.0 , series_stdev ) for x in lags ] ) else : autocorr = _autocorr_func3 ( imags , lags [ 0 ] , imags . size , 0.0 , series_stdev ) if maxlags is not None : autocorr = autocorr [ : maxlags ] interpolated . update ( { 'minitime' : itimes . min ( ) , 'lags' : lags , 'acf' : autocorr } ) return interpolated
13518	def maximum_deck_area ( self , water_plane_coef = 0.88 ) : AD = self . beam * self . length * water_plane_coef return AD
9655	def run_commands ( commands , settings ) : sprint = settings [ "sprint" ] quiet = settings [ "quiet" ] error = settings [ "error" ] enhanced_errors = True the_shell = None if settings [ "no_enhanced_errors" ] : enhanced_errors = False if "shell" in settings : the_shell = settings [ "shell" ] windows_p = sys . platform == "win32" STDOUT = None STDERR = None if quiet : STDOUT = PIPE STDERR = PIPE commands = commands . rstrip ( ) sprint ( "About to run commands '{}'" . format ( commands ) , level = "verbose" ) if not quiet : sprint ( commands ) if the_shell : tmp = shlex . split ( the_shell ) the_shell = tmp [ 0 ] tmp = tmp [ 1 : ] if enhanced_errors and not windows_p : tmp . append ( "-e" ) tmp . append ( commands ) commands = tmp else : if enhanced_errors and not windows_p : commands = [ "-e" , commands ] p = Popen ( commands , shell = True , stdout = STDOUT , stderr = STDERR , executable = the_shell ) out , err = p . communicate ( ) if p . returncode : if quiet : error ( err . decode ( locale . getpreferredencoding ( ) ) ) error ( "Command failed to run" ) sys . exit ( 1 )
2445	def reset_package ( self ) : self . package_set = False self . package_vers_set = False self . package_file_name_set = False self . package_supplier_set = False self . package_originator_set = False self . package_down_location_set = False self . package_home_set = False self . package_verif_set = False self . package_chk_sum_set = False self . package_source_info_set = False self . package_conc_lics_set = False self . package_license_declared_set = False self . package_license_comment_set = False self . package_cr_text_set = False self . package_summary_set = False self . package_desc_set = False
12503	def _smooth_array ( arr , affine , fwhm = None , ensure_finite = True , copy = True , ** kwargs ) : if arr . dtype . kind == 'i' : if arr . dtype == np . int64 : arr = arr . astype ( np . float64 ) else : arr = arr . astype ( np . float32 ) if copy : arr = arr . copy ( ) if ensure_finite : arr [ np . logical_not ( np . isfinite ( arr ) ) ] = 0 if fwhm == 'fast' : arr = _fast_smooth_array ( arr ) elif fwhm is not None : affine = affine [ : 3 , : 3 ] fwhm_over_sigma_ratio = np . sqrt ( 8 * np . log ( 2 ) ) vox_size = np . sqrt ( np . sum ( affine ** 2 , axis = 0 ) ) sigma = fwhm / ( fwhm_over_sigma_ratio * vox_size ) for n , s in enumerate ( sigma ) : ndimage . gaussian_filter1d ( arr , s , output = arr , axis = n , ** kwargs ) return arr
12402	def require ( self , req ) : reqs = req if isinstance ( req , list ) else [ req ] for req in reqs : if not isinstance ( req , BumpRequirement ) : req = BumpRequirement ( req ) req . required = True req . required_by = self self . requirements . append ( req )
9909	def set_primary ( self ) : query = EmailAddress . objects . filter ( is_primary = True , user = self . user ) query = query . exclude ( pk = self . pk ) with transaction . atomic ( ) : query . update ( is_primary = False ) self . is_primary = True self . save ( ) logger . info ( "Set %s as the primary email address for %s." , self . email , self . user , )
9466	def conference_record_stop ( self , call_params ) : path = '/' + self . api_version + '/ConferenceRecordStop/' method = 'POST' return self . request ( path , method , call_params )
7860	def _request_tls ( self ) : self . requested = True element = ElementTree . Element ( STARTTLS_TAG ) self . stream . write_element ( element )
7396	def get_publication_list ( context , list , template = 'publications/publications.html' ) : list = List . objects . filter ( list__iexact = list ) if not list : return '' list = list [ 0 ] publications = list . publication_set . all ( ) publications = publications . order_by ( '-year' , '-month' , '-id' ) if not publications : return '' populate ( publications ) return render_template ( template , context [ 'request' ] , { 'list' : list , 'publications' : publications } )
3166	def cancel ( self , campaign_id ) : self . campaign_id = campaign_id return self . _mc_client . _post ( url = self . _build_path ( campaign_id , 'actions/cancel-send' ) )
13709	def is_threat ( self , result = None , harmless_age = None , threat_score = None , threat_type = None ) : harmless_age = harmless_age if harmless_age is not None else settings . CACHED_HTTPBL_HARMLESS_AGE threat_score = threat_score if threat_score is not None else settings . CACHED_HTTPBL_THREAT_SCORE threat_type = threat_type if threat_type is not None else - 1 result = result if result is not None else self . _last_result threat = False if result is not None : if result [ 'age' ] < harmless_age and result [ 'threat' ] > threat_score : threat = True if threat_type > - 1 : if result [ 'type' ] & threat_type : threat = True else : threat = False return threat
266	def format_asset ( asset ) : try : import zipline . assets except ImportError : return asset if isinstance ( asset , zipline . assets . Asset ) : return asset . symbol else : return asset
297	def plot_return_quantiles ( returns , live_start_date = None , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) is_returns = returns if live_start_date is None else returns . loc [ returns . index < live_start_date ] is_weekly = ep . aggregate_returns ( is_returns , 'weekly' ) is_monthly = ep . aggregate_returns ( is_returns , 'monthly' ) sns . boxplot ( data = [ is_returns , is_weekly , is_monthly ] , palette = [ "#4c72B0" , "#55A868" , "#CCB974" ] , ax = ax , ** kwargs ) if live_start_date is not None : oos_returns = returns . loc [ returns . index >= live_start_date ] oos_weekly = ep . aggregate_returns ( oos_returns , 'weekly' ) oos_monthly = ep . aggregate_returns ( oos_returns , 'monthly' ) sns . swarmplot ( data = [ oos_returns , oos_weekly , oos_monthly ] , ax = ax , color = "red" , marker = "d" , ** kwargs ) red_dots = matplotlib . lines . Line2D ( [ ] , [ ] , color = "red" , marker = "d" , label = "Out-of-sample data" , linestyle = '' ) ax . legend ( handles = [ red_dots ] , frameon = True , framealpha = 0.5 ) ax . set_xticklabels ( [ 'Daily' , 'Weekly' , 'Monthly' ] ) ax . set_title ( 'Return quantiles' ) return ax
9317	def refresh ( self , accept = MEDIA_TYPE_TAXII_V20 ) : response = self . __raw = self . _conn . get ( self . url , headers = { "Accept" : accept } ) self . _populate_fields ( ** response )
9241	def fetch_tags_dates ( self ) : if self . options . verbose : print ( "Fetching dates for {} tags..." . format ( len ( self . filtered_tags ) ) ) def worker ( tag ) : self . get_time_of_tag ( tag ) threads = [ ] max_threads = 50 cnt = len ( self . filtered_tags ) for i in range ( 0 , ( cnt // max_threads ) + 1 ) : for j in range ( max_threads ) : idx = i * 50 + j if idx == cnt : break t = threading . Thread ( target = worker , args = ( self . filtered_tags [ idx ] , ) ) threads . append ( t ) t . start ( ) if self . options . verbose > 2 : print ( "." , end = "" ) for t in threads : t . join ( ) if self . options . verbose > 2 : print ( "." ) if self . options . verbose > 1 : print ( "Fetched dates for {} tags." . format ( len ( self . tag_times_dict ) ) )
11242	def indent_css ( f , output ) : line_count = get_line_count ( f ) f = open ( f , 'r+' ) output = open ( output , 'r+' ) for line in range ( line_count ) : string = f . readline ( ) . rstrip ( ) if len ( string ) > 0 : if string [ - 1 ] == ";" : output . write ( " " + string + "\n" ) else : output . write ( string + "\n" ) output . close ( ) f . close ( )
3351	def get_by_any ( self , iterable ) : def get_item ( item ) : if isinstance ( item , int ) : return self [ item ] elif isinstance ( item , string_types ) : return self . get_by_id ( item ) elif item in self : return item else : raise TypeError ( "item in iterable cannot be '%s'" % type ( item ) ) if not isinstance ( iterable , list ) : iterable = [ iterable ] return [ get_item ( item ) for item in iterable ]
13740	def connect ( self ) : if not self . connected ( ) : self . _ws = create_connection ( self . WS_URI ) message = { 'type' : self . WS_TYPE , 'product_id' : self . WS_PRODUCT_ID } self . _ws . send ( dumps ( message ) ) with self . _lock : if not self . _thread : thread = Thread ( target = self . _keep_alive_thread , args = [ ] ) thread . start ( )
12055	def ftp_login ( folder = None ) : pwDir = os . path . realpath ( __file__ ) for i in range ( 3 ) : pwDir = os . path . dirname ( pwDir ) pwFile = os . path . join ( pwDir , "passwd.txt" ) print ( " -- looking for login information in:\n [%s]" % pwFile ) try : with open ( pwFile ) as f : lines = f . readlines ( ) username = lines [ 0 ] . strip ( ) password = lines [ 1 ] . strip ( ) print ( " -- found a valid username/password" ) except : print ( " -- password lookup FAILED." ) username = TK_askPassword ( "FTP LOGIN" , "enter FTP username" ) password = TK_askPassword ( "FTP LOGIN" , "enter password for %s" % username ) if not username or not password : print ( " !! failed getting login info. aborting FTP effort." ) return print ( " username:" , username ) print ( " password:" , "*" * ( len ( password ) ) ) print ( " -- logging in to FTP ..." ) try : ftp = ftplib . FTP ( "swharden.com" ) ftp . login ( username , password ) if folder : ftp . cwd ( folder ) return ftp except : print ( " !! login failure !!" ) return False
5594	def intersecting ( self , tile ) : return [ self . tile ( * intersecting_tile . id ) for intersecting_tile in self . tile_pyramid . intersecting ( tile ) ]
10793	def separate_particles_into_groups ( s , region_size = 40 , bounds = None ) : imtile = ( s . oshape . translate ( - s . pad ) if bounds is None else util . Tile ( bounds [ 0 ] , bounds [ 1 ] ) ) region = util . Tile ( region_size , dim = s . dim ) trange = np . ceil ( imtile . shape . astype ( 'float' ) / region . shape ) translations = util . Tile ( trange ) . coords ( form = 'vector' ) translations = translations . reshape ( - 1 , translations . shape [ - 1 ] ) groups = [ ] positions = s . obj_get_positions ( ) for v in translations : tmptile = region . copy ( ) . translate ( region . shape * v - s . pad ) groups . append ( find_particles_in_tile ( positions , tmptile ) ) return [ g for g in groups if len ( g ) > 0 ]
4563	def to_type_constructor ( value , python_path = None ) : if not value : return value if callable ( value ) : return { 'datatype' : value } value = to_type ( value ) typename = value . get ( 'typename' ) if typename : r = aliases . resolve ( typename ) try : value [ 'datatype' ] = importer . import_symbol ( r , python_path = python_path ) del value [ 'typename' ] except Exception as e : value [ '_exception' ] = e return value
8276	def swatch ( self , x , y , w = 35 , h = 35 , padding = 4 , roundness = 0 , n = 12 , d = 0.035 , grouped = None ) : if grouped is None : grouped = self . group_swatches if not grouped : s = sum ( [ wgt for clr , rng , wgt in self . ranges ] ) for clr , rng , wgt in self . ranges : cols = max ( 1 , int ( wgt / s * n ) ) for i in _range ( cols ) : rng . colors ( clr , n = n , d = d ) . swatch ( x , y , w , h , padding = padding , roundness = roundness ) x += w + padding return x , y + n * ( h + padding ) grouped = self . _weight_by_hue ( ) for total_weight , normalized_weight , hue , ranges in grouped : dy = y rc = 0 for clr , rng , weight in ranges : dx = x cols = int ( normalized_weight * n ) cols = max ( 1 , min ( cols , n - len ( grouped ) ) ) if clr . name == "black" : rng = rng . black if clr . name == "white" : rng = rng . white for i in _range ( cols ) : rows = int ( weight / total_weight * n ) rows = max ( 1 , rows ) if ( clr , rng , weight ) == ranges [ - 1 ] and rc + rows < n : rows += 1 rng . colors ( clr , n = rows , d = d ) . swatch ( dx , dy , w , h , padding = padding , roundness = roundness ) dx += w + padding dy += ( w + padding ) * rows rc = rows x += ( w + padding ) * cols + padding return x , dy
3517	def spring_metrics ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return SpringMetricsNode ( )
1517	def start_master_nodes ( masters , cl_args ) : pids = [ ] for master in masters : Log . info ( "Starting master on %s" % master ) cmd = "%s agent -config %s >> /tmp/nomad_server_log 2>&1 &" % ( get_nomad_path ( cl_args ) , get_nomad_master_config_file ( cl_args ) ) if not is_self ( master ) : cmd = ssh_remote_execute ( cmd , master , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) pids . append ( { "pid" : pid , "dest" : master } ) errors = [ ] for entry in pids : pid = entry [ "pid" ] return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) if return_code != 0 : errors . append ( "Failed to start master on %s with error:\n%s" % ( entry [ "dest" ] , output [ 1 ] ) ) if errors : for error in errors : Log . error ( error ) sys . exit ( - 1 ) Log . info ( "Done starting masters" )
1210	def table ( self , header , body ) : table = '\n.. list-table::\n' if header and not header . isspace ( ) : table = ( table + self . indent + ':header-rows: 1\n\n' + self . _indent_block ( header ) + '\n' ) else : table = table + '\n' table = table + self . _indent_block ( body ) + '\n\n' return table
4019	def _dusty_vm_exists ( ) : existing_vms = check_output_demoted ( [ 'VBoxManage' , 'list' , 'vms' ] ) for line in existing_vms . splitlines ( ) : if '"{}"' . format ( constants . VM_MACHINE_NAME ) in line : return True return False
12578	def apply_smoothing ( self , smooth_fwhm ) : if smooth_fwhm <= 0 : return old_smooth_fwhm = self . _smooth_fwhm self . _smooth_fwhm = smooth_fwhm try : data = self . get_data ( smoothed = True , masked = True , safe_copy = True ) except ValueError as ve : self . _smooth_fwhm = old_smooth_fwhm raise else : self . _smooth_fwhm = smooth_fwhm return data
5282	def construct_formset ( self ) : formset_class = self . get_formset ( ) if hasattr ( self , 'get_extra_form_kwargs' ) : klass = type ( self ) . __name__ raise DeprecationWarning ( 'Calling {0}.get_extra_form_kwargs is no longer supported. ' 'Set `form_kwargs` in {0}.formset_kwargs or override ' '{0}.get_formset_kwargs() directly.' . format ( klass ) , ) return formset_class ( ** self . get_formset_kwargs ( ) )
7209	def cancel ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot cancel.' ) if self . batch_values : self . workflow . batch_workflow_cancel ( self . id ) else : self . workflow . cancel ( self . id )
1078	def isoformat ( self ) : return "%s-%s-%s" % ( str ( self . _year ) . zfill ( 4 ) , str ( self . _month ) . zfill ( 2 ) , str ( self . _day ) . zfill ( 2 ) )
7287	def set_form_fields ( self , form_field_dict , parent_key = None , field_type = None ) : for form_key , field_value in form_field_dict . items ( ) : form_key = make_key ( parent_key , form_key ) if parent_key is not None else form_key if isinstance ( field_value , tuple ) : set_list_class = False base_key = form_key if ListField in ( field_value . field_type , field_type ) : if parent_key is None or ListField == field_value . field_type : if field_type != EmbeddedDocumentField : field_value . widget . attrs [ 'class' ] += ' listField {0}' . format ( form_key ) set_list_class = True else : field_value . widget . attrs [ 'class' ] += ' listField' list_keys = [ field_key for field_key in self . form . fields . keys ( ) if has_digit ( field_key ) ] key_int = 0 while form_key in list_keys : key_int += 1 form_key = make_key ( form_key , key_int ) if parent_key is not None : valid_base_keys = [ model_key for model_key in self . model_map_dict . keys ( ) if not model_key . startswith ( "_" ) ] while base_key not in valid_base_keys and base_key : base_key = make_key ( base_key , exclude_last_string = True ) embedded_key_class = None if set_list_class : field_value . widget . attrs [ 'class' ] += " listField" . format ( base_key ) embedded_key_class = make_key ( field_key , exclude_last_string = True ) field_value . widget . attrs [ 'class' ] += " embeddedField" if base_key == parent_key : field_value . widget . attrs [ 'class' ] += ' {0}' . format ( base_key ) else : field_value . widget . attrs [ 'class' ] += ' {0} {1}' . format ( base_key , parent_key ) if embedded_key_class is not None : field_value . widget . attrs [ 'class' ] += ' {0}' . format ( embedded_key_class ) default_value = self . get_field_value ( form_key ) if isinstance ( default_value , list ) and len ( default_value ) > 0 : key_index = int ( form_key . split ( "_" ) [ - 1 ] ) new_base_key = make_key ( form_key , exclude_last_string = True ) for list_value in default_value : list_widget = deepcopy ( field_value . widget ) new_key = make_key ( new_base_key , six . text_type ( key_index ) ) list_widget . attrs [ 'class' ] += " {0}" . format ( make_key ( base_key , key_index ) ) self . set_form_field ( list_widget , field_value . document_field , new_key , list_value ) key_index += 1 else : self . set_form_field ( field_value . widget , field_value . document_field , form_key , default_value ) elif isinstance ( field_value , dict ) : self . set_form_fields ( field_value , form_key , field_value . get ( "_field_type" , None ) )
6216	def buffers_exist ( self ) : for buff in self . buffers : if not buff . is_separate_file : continue path = self . path . parent / buff . uri if not os . path . exists ( path ) : raise FileNotFoundError ( "Buffer {} referenced in {} not found" . format ( path , self . path ) )
9212	def get_channel_image ( self , channel , img_size = 300 , skip_cache = False ) : from bs4 import BeautifulSoup from wikipedia . exceptions import PageError import re import wikipedia wikipedia . set_lang ( 'fr' ) if not channel : _LOGGER . error ( 'Channel is not set. Could not retrieve image.' ) return if channel in self . _cache_channel_img and not skip_cache : img = self . _cache_channel_img [ channel ] _LOGGER . debug ( 'Cache hit: %s -> %s' , channel , img ) return img channel_info = self . get_channel_info ( channel ) query = channel_info [ 'wiki_page' ] if not query : _LOGGER . debug ( 'Wiki page is not set for channel %s' , channel ) return _LOGGER . debug ( 'Query: %s' , query ) if 'max_img_size' in channel_info : if img_size > channel_info [ 'max_img_size' ] : _LOGGER . info ( 'Requested image size is bigger than the max, ' 'setting it to %s' , channel_info [ 'max_img_size' ] ) img_size = channel_info [ 'max_img_size' ] try : page = wikipedia . page ( query ) _LOGGER . debug ( 'Wikipedia article title: %s' , page . title ) soup = BeautifulSoup ( page . html ( ) , 'html.parser' ) images = soup . find_all ( 'img' ) img_src = None for i in images : if i [ 'alt' ] . startswith ( 'Image illustrative' ) : img_src = re . sub ( r'\d+px' , '{}px' . format ( img_size ) , i [ 'src' ] ) img = 'https:{}' . format ( img_src ) if img_src else None self . _cache_channel_img [ channel ] = img return img except PageError : _LOGGER . error ( 'Could not fetch channel image for %s' , channel )
3511	def sample ( model , n , method = "optgp" , thinning = 100 , processes = 1 , seed = None ) : if method == "optgp" : sampler = OptGPSampler ( model , processes , thinning = thinning , seed = seed ) elif method == "achr" : sampler = ACHRSampler ( model , thinning = thinning , seed = seed ) else : raise ValueError ( "method must be 'optgp' or 'achr'!" ) return pandas . DataFrame ( columns = [ rxn . id for rxn in model . reactions ] , data = sampler . sample ( n ) )
6511	def _eat_name_line ( self , line ) : if line [ 0 ] not in "#=" : parts = line . split ( ) country_values = line [ 30 : - 1 ] name = map_name ( parts [ 1 ] ) if not self . case_sensitive : name = name . lower ( ) if parts [ 0 ] == "M" : self . _set ( name , u"male" , country_values ) elif parts [ 0 ] == "1M" or parts [ 0 ] == "?M" : self . _set ( name , u"mostly_male" , country_values ) elif parts [ 0 ] == "F" : self . _set ( name , u"female" , country_values ) elif parts [ 0 ] == "1F" or parts [ 0 ] == "?F" : self . _set ( name , u"mostly_female" , country_values ) elif parts [ 0 ] == "?" : self . _set ( name , self . unknown_value , country_values ) else : raise "Not sure what to do with a sex of %s" % parts [ 0 ]
2575	def launch_task ( self , task_id , executable , * args , ** kwargs ) : self . tasks [ task_id ] [ 'time_submitted' ] = datetime . datetime . now ( ) hit , memo_fu = self . memoizer . check_memo ( task_id , self . tasks [ task_id ] ) if hit : logger . info ( "Reusing cached result for task {}" . format ( task_id ) ) return memo_fu executor_label = self . tasks [ task_id ] [ "executor" ] try : executor = self . executors [ executor_label ] except Exception : logger . exception ( "Task {} requested invalid executor {}: config is\n{}" . format ( task_id , executor_label , self . _config ) ) if self . monitoring is not None and self . monitoring . resource_monitoring_enabled : executable = self . monitoring . monitor_wrapper ( executable , task_id , self . monitoring . monitoring_hub_url , self . run_id , self . monitoring . resource_monitoring_interval ) with self . submitter_lock : exec_fu = executor . submit ( executable , * args , ** kwargs ) self . tasks [ task_id ] [ 'status' ] = States . launched if self . monitoring is not None : task_log_info = self . _create_task_log_info ( task_id , 'lazy' ) self . monitoring . send ( MessageType . TASK_INFO , task_log_info ) exec_fu . retries_left = self . _config . retries - self . tasks [ task_id ] [ 'fail_count' ] logger . info ( "Task {} launched on executor {}" . format ( task_id , executor . label ) ) return exec_fu
6727	def get_name ( ) : if env . vm_type == EC2 : for instance in get_all_running_ec2_instances ( ) : if env . host_string == instance . public_dns_name : name = instance . tags . get ( env . vm_name_tag ) return name else : raise NotImplementedError
5450	def validate_bucket_name ( bucket ) : if not bucket . startswith ( 'gs://' ) : raise ValueError ( 'Invalid bucket path "%s". Must start with "gs://".' % bucket ) bucket_name = bucket [ len ( 'gs://' ) : ] if not re . search ( r'^\w[\w_\.-]{1,61}\w$' , bucket_name ) : raise ValueError ( 'Invalid bucket name: %s' % bucket )
2479	def datetime_iso_format ( date ) : return "{0:0>4}-{1:0>2}-{2:0>2}T{3:0>2}:{4:0>2}:{5:0>2}Z" . format ( date . year , date . month , date . day , date . hour , date . minute , date . second )
9144	def drop ( connection , skip ) : for idx , name , manager in _iterate_managers ( connection , skip ) : click . secho ( f'dropping {name}' , fg = 'cyan' , bold = True ) manager . drop_all ( )
6761	def configure ( self ) : lm = self . last_manifest for tracker in self . get_trackers ( ) : self . vprint ( 'Checking tracker:' , tracker ) last_thumbprint = lm [ '_tracker_%s' % tracker . get_natural_key_hash ( ) ] self . vprint ( 'last thumbprint:' , last_thumbprint ) has_changed = tracker . is_changed ( last_thumbprint ) self . vprint ( 'Tracker changed:' , has_changed ) if has_changed : self . vprint ( 'Change detected!' ) tracker . act ( )
5492	def write_config ( self ) : with open ( self . config_file , "w" ) as config_file : self . cfg . write ( config_file )
13682	def get_json_tuples ( self , prettyprint = False , translate = True ) : j = self . get_json ( prettyprint , translate ) if len ( j ) > 2 : if prettyprint : j = j [ 1 : - 2 ] + ",\n" else : j = j [ 1 : - 1 ] + "," else : j = "" return j
4678	def getActiveKeyForAccount ( self , name ) : account = self . rpc . get_account ( name ) for authority in account [ "active" ] [ "key_auths" ] : try : return self . getPrivateKeyForPublicKey ( authority [ 0 ] ) except Exception : pass return False
5804	def detect_client_auth_request ( server_handshake_bytes ) : for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type == b'\x0d' : return True return False
7963	def _feed_reader ( self , data ) : IN_LOGGER . debug ( "IN: %r" , data ) if data : self . lock . release ( ) try : self . _reader . feed ( data ) finally : self . lock . acquire ( ) else : self . _eof = True self . lock . release ( ) try : self . _stream . stream_eof ( ) finally : self . lock . acquire ( ) if not self . _serializer : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" )
256	def gen_round_trip_stats ( round_trips ) : stats = { } stats [ 'pnl' ] = agg_all_long_short ( round_trips , 'pnl' , PNL_STATS ) stats [ 'summary' ] = agg_all_long_short ( round_trips , 'pnl' , SUMMARY_STATS ) stats [ 'duration' ] = agg_all_long_short ( round_trips , 'duration' , DURATION_STATS ) stats [ 'returns' ] = agg_all_long_short ( round_trips , 'returns' , RETURN_STATS ) stats [ 'symbols' ] = round_trips . groupby ( 'symbol' ) [ 'returns' ] . agg ( RETURN_STATS ) . T return stats
3777	def calculate_integral ( self , T1 , T2 , method ) : r return float ( quad ( self . calculate , T1 , T2 , args = ( method ) ) [ 0 ] )
10721	def get_command ( namespace ) : cmd = [ "pylint" , namespace . package ] + arg_map [ namespace . package ] if namespace . ignore : cmd . append ( "--ignore=%s" % namespace . ignore ) return cmd
3893	def _get_parser ( extra_args ) : parser = argparse . ArgumentParser ( formatter_class = argparse . ArgumentDefaultsHelpFormatter , ) dirs = appdirs . AppDirs ( 'hangups' , 'hangups' ) default_token_path = os . path . join ( dirs . user_cache_dir , 'refresh_token.txt' ) parser . add_argument ( '--token-path' , default = default_token_path , help = 'path used to store OAuth refresh token' ) parser . add_argument ( '-d' , '--debug' , action = 'store_true' , help = 'log detailed debugging messages' ) for extra_arg in extra_args : parser . add_argument ( extra_arg , required = True ) return parser
8753	def partition_vifs ( xapi_client , interfaces , security_group_states ) : added = [ ] updated = [ ] removed = [ ] for vif in interfaces : if ( 'floating_ip' in CONF . QUARK . environment_capabilities and is_isonet_vif ( vif ) ) : continue vif_has_groups = vif in security_group_states if vif . tagged and vif_has_groups and security_group_states [ vif ] [ sg_cli . SECURITY_GROUP_ACK ] : continue if vif . tagged : if vif_has_groups : updated . append ( vif ) else : removed . append ( vif ) else : if vif_has_groups : added . append ( vif ) return added , updated , removed
1928	def load_overrides ( path = None ) : if path is not None : names = [ path ] else : possible_names = [ 'mcore.yml' , 'manticore.yml' ] names = [ os . path . join ( '.' , '' . join ( x ) ) for x in product ( [ '' , '.' ] , possible_names ) ] for name in names : try : with open ( name , 'r' ) as yml_f : logger . info ( f'Reading configuration from {name}' ) parse_config ( yml_f ) break except FileNotFoundError : pass else : if path is not None : raise FileNotFoundError ( f"'{path}' not found for config overrides" )
8135	def down ( self ) : i = self . index ( ) if i != None : del self . canvas . layers [ i ] i = max ( 0 , i - 1 ) self . canvas . layers . insert ( i , self )
505	def _getStateAnomalyVector ( self , state ) : vector = numpy . zeros ( self . _anomalyVectorLength ) vector [ state . anomalyVector ] = 1 return vector
8983	def get_instruction_id ( self , instruction_or_id ) : if isinstance ( instruction_or_id , tuple ) : return _InstructionId ( instruction_or_id ) return _InstructionId ( instruction_or_id . type , instruction_or_id . hex_color )
3388	def _bounds_dist ( self , p ) : prob = self . problem lb_dist = ( p - prob . variable_bounds [ 0 , ] ) . min ( ) ub_dist = ( prob . variable_bounds [ 1 , ] - p ) . min ( ) if prob . bounds . shape [ 0 ] > 0 : const = prob . inequalities . dot ( p ) const_lb_dist = ( const - prob . bounds [ 0 , ] ) . min ( ) const_ub_dist = ( prob . bounds [ 1 , ] - const ) . min ( ) lb_dist = min ( lb_dist , const_lb_dist ) ub_dist = min ( ub_dist , const_ub_dist ) return np . array ( [ lb_dist , ub_dist ] )
6846	def is_present ( self , host = None ) : r = self . local_renderer r . env . host = host or self . genv . host_string ret = r . _local ( "getent hosts {host} | awk '{{ print $1 }}'" , capture = True ) or '' if self . verbose : print ( 'ret:' , ret ) ret = ret . strip ( ) if self . verbose : print ( 'Host %s %s present.' % ( r . env . host , 'IS' if bool ( ret ) else 'IS NOT' ) ) ip = ret ret = bool ( ret ) if not ret : return False r . env . ip = ip with settings ( warn_only = True ) : ret = r . _local ( 'ping -c 1 {ip}' , capture = True ) or '' packet_loss = re . findall ( r'([0-9]+)% packet loss' , ret ) ip_accessible = packet_loss and int ( packet_loss [ 0 ] ) < 100 if self . verbose : print ( 'IP %s accessible: %s' % ( ip , ip_accessible ) ) return bool ( ip_accessible )
11910	def bump_version ( version , which = None ) : try : parts = [ int ( n ) for n in version . split ( '.' ) ] except ValueError : fail ( 'Current version is not numeric' ) if len ( parts ) != 3 : fail ( 'Current version is not semantic versioning' ) PARTS = { 'major' : 0 , 'minor' : 1 , 'patch' : 2 } index = PARTS [ which ] if which in PARTS else 2 before , middle , after = parts [ : index ] , parts [ index ] , parts [ index + 1 : ] middle += 1 return '.' . join ( str ( n ) for n in before + [ middle ] + after )
204	def from_heatmaps ( heatmaps , class_indices = None , nb_classes = None ) : if class_indices is None : return SegmentationMapOnImage ( heatmaps . arr_0to1 , shape = heatmaps . shape ) else : ia . do_assert ( nb_classes is not None ) ia . do_assert ( min ( class_indices ) >= 0 ) ia . do_assert ( max ( class_indices ) < nb_classes ) ia . do_assert ( len ( class_indices ) == heatmaps . arr_0to1 . shape [ 2 ] ) arr_0to1 = heatmaps . arr_0to1 arr_0to1_full = np . zeros ( ( arr_0to1 . shape [ 0 ] , arr_0to1 . shape [ 1 ] , nb_classes ) , dtype = np . float32 ) for heatmap_channel , mapped_channel in enumerate ( class_indices ) : arr_0to1_full [ : , : , mapped_channel ] = arr_0to1 [ : , : , heatmap_channel ] return SegmentationMapOnImage ( arr_0to1_full , shape = heatmaps . shape )
3264	def md_link ( node ) : mimetype = node . find ( "type" ) mdtype = node . find ( "metadataType" ) content = node . find ( "content" ) if None in [ mimetype , mdtype , content ] : return None else : return ( mimetype . text , mdtype . text , content . text )
8385	def write_main ( argv ) : if len ( argv ) != 1 : print ( "Please provide the name of a file to write." ) return 1 filename = argv [ 0 ] resource_name = "files/" + filename tweaks_name = amend_filename ( filename , "_tweaks" ) if not pkg_resources . resource_exists ( "edx_lint" , resource_name ) : print ( u"Don't have file %r to write." % filename ) return 2 if os . path . exists ( filename ) : print ( u"Checking existing copy of %s" % filename ) tef = TamperEvidentFile ( filename ) if not tef . validate ( ) : bak_name = amend_filename ( filename , "_backup" ) print ( u"Your copy of %s seems to have been edited, renaming it to %s" % ( filename , bak_name ) ) if os . path . exists ( bak_name ) : print ( u"A previous %s exists, deleting it" % bak_name ) os . remove ( bak_name ) os . rename ( filename , bak_name ) print ( u"Reading edx_lint/files/%s" % filename ) cfg = configparser . RawConfigParser ( ) resource_string = pkg_resources . resource_string ( "edx_lint" , resource_name ) . decode ( "utf8" ) if six . PY2 : cfg . readfp ( cStringIO ( resource_string ) , resource_name ) else : cfg . read_string ( resource_string , resource_name ) if os . path . exists ( tweaks_name ) : print ( u"Applying local tweaks from %s" % tweaks_name ) cfg_tweaks = configparser . RawConfigParser ( ) cfg_tweaks . read ( [ tweaks_name ] ) merge_configs ( cfg , cfg_tweaks ) print ( u"Writing %s" % filename ) output_text = cStringIO ( ) output_text . write ( WARNING_HEADER . format ( filename = filename , tweaks_name = tweaks_name ) ) cfg . write ( output_text ) out_tef = TamperEvidentFile ( filename ) if six . PY2 : output_bytes = output_text . getvalue ( ) else : output_bytes = output_text . getvalue ( ) . encode ( "utf8" ) out_tef . write ( output_bytes ) return 0
2071	def col_transform ( self , col , digits ) : if col is None or float ( col ) < 0.0 : return None else : col = self . number_to_base ( int ( col ) , self . base , digits ) if len ( col ) == digits : return col else : return [ 0 for _ in range ( digits - len ( col ) ) ] + col
2400	def get_good_pos_ngrams ( self ) : if ( os . path . isfile ( NGRAM_PATH ) ) : good_pos_ngrams = pickle . load ( open ( NGRAM_PATH , 'rb' ) ) elif os . path . isfile ( ESSAY_CORPUS_PATH ) : essay_corpus = open ( ESSAY_CORPUS_PATH ) . read ( ) essay_corpus = util_functions . sub_chars ( essay_corpus ) good_pos_ngrams = util_functions . regenerate_good_tokens ( essay_corpus ) pickle . dump ( good_pos_ngrams , open ( NGRAM_PATH , 'wb' ) ) else : good_pos_ngrams = [ 'NN PRP' , 'NN PRP .' , 'NN PRP . DT' , 'PRP .' , 'PRP . DT' , 'PRP . DT NNP' , '. DT' , '. DT NNP' , '. DT NNP NNP' , 'DT NNP' , 'DT NNP NNP' , 'DT NNP NNP NNP' , 'NNP NNP' , 'NNP NNP NNP' , 'NNP NNP NNP NNP' , 'NNP NNP NNP .' , 'NNP NNP .' , 'NNP NNP . TO' , 'NNP .' , 'NNP . TO' , 'NNP . TO NNP' , '. TO' , '. TO NNP' , '. TO NNP NNP' , 'TO NNP' , 'TO NNP NNP' ] return set ( good_pos_ngrams )
38	def dict_gather ( comm , d , op = 'mean' , assert_all_have_data = True ) : if comm is None : return d alldicts = comm . allgather ( d ) size = comm . size k2li = defaultdict ( list ) for d in alldicts : for ( k , v ) in d . items ( ) : k2li [ k ] . append ( v ) result = { } for ( k , li ) in k2li . items ( ) : if assert_all_have_data : assert len ( li ) == size , "only %i out of %i MPI workers have sent '%s'" % ( len ( li ) , size , k ) if op == 'mean' : result [ k ] = np . mean ( li , axis = 0 ) elif op == 'sum' : result [ k ] = np . sum ( li , axis = 0 ) else : assert 0 , op return result
11772	def NaiveBayesLearner ( dataset ) : targetvals = dataset . values [ dataset . target ] target_dist = CountingProbDist ( targetvals ) attr_dists = dict ( ( ( gv , attr ) , CountingProbDist ( dataset . values [ attr ] ) ) for gv in targetvals for attr in dataset . inputs ) for example in dataset . examples : targetval = example [ dataset . target ] target_dist . add ( targetval ) for attr in dataset . inputs : attr_dists [ targetval , attr ] . add ( example [ attr ] ) def predict ( example ) : def class_probability ( targetval ) : return ( target_dist [ targetval ] * product ( attr_dists [ targetval , attr ] [ example [ attr ] ] for attr in dataset . inputs ) ) return argmax ( targetvals , class_probability ) return predict
9358	def paragraph ( separator = '\n\n' , wrap_start = '' , wrap_end = '' , html = False , sentences_quantity = 3 ) : return paragraphs ( quantity = 1 , separator = separator , wrap_start = wrap_start , wrap_end = wrap_end , html = html , sentences_quantity = sentences_quantity )
9613	def element ( self , using , value ) : return self . _execute ( Command . FIND_CHILD_ELEMENT , { 'using' : using , 'value' : value } )
12328	def init_repo ( self , gitdir ) : hooksdir = os . path . join ( gitdir , 'hooks' ) content = postreceive_template % { 'client' : self . client , 'bucket' : self . bucket , 's3cfg' : self . s3cfg , 'prefix' : self . prefix } postrecv_filename = os . path . join ( hooksdir , 'post-receive' ) with open ( postrecv_filename , 'w' ) as fd : fd . write ( content ) self . make_hook_executable ( postrecv_filename ) print ( "Wrote to" , postrecv_filename )
1773	def pop ( cpu , size ) : assert size in ( 16 , cpu . address_bit_size ) base , _ , _ = cpu . get_descriptor ( cpu . SS ) address = cpu . STACK + base value = cpu . read_int ( address , size ) cpu . STACK = cpu . STACK + size // 8 return value
1703	def outer_left_join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . OUTER_LEFT , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
13861	def to_datetime ( when ) : if when is None or is_datetime ( when ) : return when if is_time ( when ) : return datetime . combine ( epoch . date ( ) , when ) if is_date ( when ) : return datetime . combine ( when , time ( 0 ) ) raise TypeError ( "unable to convert {} to datetime" . format ( when . __class__ . __name__ ) )
11827	def boggle_neighbors ( n2 , cache = { } ) : if cache . get ( n2 ) : return cache . get ( n2 ) n = exact_sqrt ( n2 ) neighbors = [ None ] * n2 for i in range ( n2 ) : neighbors [ i ] = [ ] on_top = i < n on_bottom = i >= n2 - n on_left = i % n == 0 on_right = ( i + 1 ) % n == 0 if not on_top : neighbors [ i ] . append ( i - n ) if not on_left : neighbors [ i ] . append ( i - n - 1 ) if not on_right : neighbors [ i ] . append ( i - n + 1 ) if not on_bottom : neighbors [ i ] . append ( i + n ) if not on_left : neighbors [ i ] . append ( i + n - 1 ) if not on_right : neighbors [ i ] . append ( i + n + 1 ) if not on_left : neighbors [ i ] . append ( i - 1 ) if not on_right : neighbors [ i ] . append ( i + 1 ) cache [ n2 ] = neighbors return neighbors
3064	def update_query_params ( uri , params ) : parts = urllib . parse . urlparse ( uri ) query_params = parse_unique_urlencoded ( parts . query ) query_params . update ( params ) new_query = urllib . parse . urlencode ( query_params ) new_parts = parts . _replace ( query = new_query ) return urllib . parse . urlunparse ( new_parts )
10258	def count_top_centrality ( graph : BELGraph , number : Optional [ int ] = 30 ) -> Mapping [ BaseEntity , int ] : dd = nx . betweenness_centrality ( graph ) dc = Counter ( dd ) return dict ( dc . most_common ( number ) )
7379	def _get ( self , text ) : if self . strict : match = self . prog . match ( text ) if match : cmd = match . group ( ) if cmd in self : return cmd else : words = self . prog . findall ( text ) for word in words : if word in self : return word
258	def perf_attrib ( returns , positions , factor_returns , factor_loadings , transactions = None , pos_in_dollars = True ) : ( returns , positions , factor_returns , factor_loadings ) = _align_and_warn ( returns , positions , factor_returns , factor_loadings , transactions = transactions , pos_in_dollars = pos_in_dollars ) positions = _stack_positions ( positions , pos_in_dollars = pos_in_dollars ) return ep . perf_attrib ( returns , positions , factor_returns , factor_loadings )
9391	def aggregate_count_over_time ( self , metric_store , groupby_name , aggregate_timestamp ) : all_qps = metric_store [ 'qps' ] qps = all_qps [ groupby_name ] if aggregate_timestamp in qps : qps [ aggregate_timestamp ] += 1 else : qps [ aggregate_timestamp ] = 1 return None
677	def getDescription ( self ) : description = { 'name' : self . name , 'fields' : [ f . name for f in self . fields ] , 'numRecords by field' : [ f . numRecords for f in self . fields ] } return description
7480	def sub_build_clustbits ( data , usort , nseeds ) : LOGGER . info ( "loading full _catcons file into memory" ) allcons = { } conshandle = os . path . join ( data . dirs . across , data . name + "_catcons.tmp" ) with gzip . open ( conshandle , 'rb' ) as iocons : cons = itertools . izip ( * [ iter ( iocons ) ] * 2 ) for namestr , seq in cons : nnn , sss = [ i . strip ( ) for i in namestr , seq ] allcons [ nnn [ 1 : ] ] = sss optim = ( ( nseeds // ( data . cpus * 4 ) ) + ( nseeds % ( data . cpus * 4 ) ) ) LOGGER . info ( "building clustbits, optim=%s, nseeds=%s, cpus=%s" , optim , nseeds , data . cpus ) with open ( usort , 'rb' ) as insort : isort = iter ( insort ) loci = 0 lastseed = 0 fseqs = [ ] seqlist = [ ] seqsize = 0 while 1 : try : hit , seed , ori = isort . next ( ) . strip ( ) . split ( ) except StopIteration : break try : if seed != lastseed : if fseqs : seqlist . append ( "\n" . join ( fseqs ) ) seqsize += 1 fseqs = [ ] if seqsize >= optim : if seqlist : loci += seqsize with open ( os . path . join ( data . tmpdir , data . name + ".chunk_{}" . format ( loci ) ) , 'w' ) as clustsout : LOGGER . debug ( "writing chunk - seqsize {} loci {} {}" . format ( seqsize , loci , clustsout . name ) ) clustsout . write ( "\n//\n//\n" . join ( seqlist ) + "\n//\n//\n" ) seqlist = [ ] seqsize = 0 fseqs . append ( ">{}\n{}" . format ( seed , allcons [ seed ] ) ) lastseed = seed seq = allcons [ hit ] if ori == "-" : seq = fullcomp ( seq ) [ : : - 1 ] fseqs . append ( ">{}\n{}" . format ( hit , seq ) ) except KeyError as inst : LOGGER . error ( "Bad Seed/Hit: seqsize {}\tloci {}\tseed {}\thit {}" . format ( seqsize , loci , seed , hit ) ) if fseqs : seqlist . append ( "\n" . join ( fseqs ) ) seqsize += 1 loci += seqsize if seqlist : with open ( os . path . join ( data . tmpdir , data . name + ".chunk_{}" . format ( loci ) ) , 'w' ) as clustsout : clustsout . write ( "\n//\n//\n" . join ( seqlist ) + "\n//\n//\n" ) del allcons clustbits = glob . glob ( os . path . join ( data . tmpdir , data . name + ".chunk_*" ) ) return clustbits , loci
4570	def load ( file , use_yaml = None ) : if isinstance ( file , str ) : fp = open ( file ) filename = file else : fp = file filename = getattr ( fp , 'name' , '' ) try : return loads ( fp . read ( ) , use_yaml , filename ) except Exception as e : e . args = ( 'There was a error in the data file' , filename ) + e . args raise
5543	def clip ( self , array , geometries , inverted = False , clip_buffer = 0 ) : return commons_clip . clip_array_with_vector ( array , self . tile . affine , geometries , inverted = inverted , clip_buffer = clip_buffer * self . tile . pixel_x_size )
3599	def delivery ( self , packageName , versionCode = None , offerType = 1 , downloadToken = None , expansion_files = False ) : if versionCode is None : versionCode = self . details ( packageName ) . get ( 'versionCode' ) params = { 'ot' : str ( offerType ) , 'doc' : packageName , 'vc' : str ( versionCode ) } headers = self . getHeaders ( ) if downloadToken is not None : params [ 'dtok' ] = downloadToken response = requests . get ( DELIVERY_URL , headers = headers , params = params , verify = ssl_verify , timeout = 60 , proxies = self . proxies_config ) response = googleplay_pb2 . ResponseWrapper . FromString ( response . content ) if response . commands . displayErrorMessage != "" : raise RequestError ( response . commands . displayErrorMessage ) elif response . payload . deliveryResponse . appDeliveryData . downloadUrl == "" : raise RequestError ( 'App not purchased' ) else : result = { } result [ 'docId' ] = packageName result [ 'additionalData' ] = [ ] downloadUrl = response . payload . deliveryResponse . appDeliveryData . downloadUrl cookie = response . payload . deliveryResponse . appDeliveryData . downloadAuthCookie [ 0 ] cookies = { str ( cookie . name ) : str ( cookie . value ) } result [ 'file' ] = self . _deliver_data ( downloadUrl , cookies ) if not expansion_files : return result for obb in response . payload . deliveryResponse . appDeliveryData . additionalFile : a = { } if obb . fileType == 0 : obbType = 'main' else : obbType = 'patch' a [ 'type' ] = obbType a [ 'versionCode' ] = obb . versionCode a [ 'file' ] = self . _deliver_data ( obb . downloadUrl , None ) result [ 'additionalData' ] . append ( a ) return result
1577	def make_shell_logfiles_url ( host , shell_port , _ , instance_id = None ) : if not shell_port : return None if not instance_id : return "http://%s:%d/browse/log-files" % ( host , shell_port ) else : return "http://%s:%d/file/log-files/%s.log.0" % ( host , shell_port , instance_id )
5765	def _decrypt_encrypted_data ( encryption_algorithm_info , encrypted_content , password ) : decrypt_func = crypto_funcs [ encryption_algorithm_info . encryption_cipher ] if encryption_algorithm_info . kdf == 'pbkdf2' : if encryption_algorithm_info . encryption_cipher == 'rc5' : raise ValueError ( pretty_message ( ) ) enc_key = pbkdf2 ( encryption_algorithm_info . kdf_hmac , password , encryption_algorithm_info . kdf_salt , encryption_algorithm_info . kdf_iterations , encryption_algorithm_info . key_length ) enc_iv = encryption_algorithm_info . encryption_iv plaintext = decrypt_func ( enc_key , encrypted_content , enc_iv ) elif encryption_algorithm_info . kdf == 'pbkdf1' : derived_output = pbkdf1 ( encryption_algorithm_info . kdf_hmac , password , encryption_algorithm_info . kdf_salt , encryption_algorithm_info . kdf_iterations , encryption_algorithm_info . key_length + 8 ) enc_key = derived_output [ 0 : 8 ] enc_iv = derived_output [ 8 : 16 ] plaintext = decrypt_func ( enc_key , encrypted_content , enc_iv ) elif encryption_algorithm_info . kdf == 'pkcs12_kdf' : enc_key = pkcs12_kdf ( encryption_algorithm_info . kdf_hmac , password , encryption_algorithm_info . kdf_salt , encryption_algorithm_info . kdf_iterations , encryption_algorithm_info . key_length , 1 ) if encryption_algorithm_info . encryption_cipher == 'rc4' : plaintext = decrypt_func ( enc_key , encrypted_content ) else : enc_iv = pkcs12_kdf ( encryption_algorithm_info . kdf_hmac , password , encryption_algorithm_info . kdf_salt , encryption_algorithm_info . kdf_iterations , encryption_algorithm_info . encryption_block_size , 2 ) plaintext = decrypt_func ( enc_key , encrypted_content , enc_iv ) return plaintext
10103	def send ( self , email_id , recipient , email_data = None , sender = None , cc = None , bcc = None , tags = [ ] , headers = { } , esp_account = None , locale = None , email_version_name = None , inline = None , files = [ ] , timeout = None ) : if not email_data : email_data = { } if isinstance ( recipient , string_types ) : warnings . warn ( "Passing email directly for recipient is deprecated" , DeprecationWarning ) recipient = { 'address' : recipient } payload = { 'email_id' : email_id , 'recipient' : recipient , 'email_data' : email_data } if sender : payload [ 'sender' ] = sender if cc : if not type ( cc ) == list : logger . error ( 'kwarg cc must be type(list), got %s' % type ( cc ) ) payload [ 'cc' ] = cc if bcc : if not type ( bcc ) == list : logger . error ( 'kwarg bcc must be type(list), got %s' % type ( bcc ) ) payload [ 'bcc' ] = bcc if tags : if not type ( tags ) == list : logger . error ( 'kwarg tags must be type(list), got %s' % ( type ( tags ) ) ) payload [ 'tags' ] = tags if headers : if not type ( headers ) == dict : logger . error ( 'kwarg headers must be type(dict), got %s' % ( type ( headers ) ) ) payload [ 'headers' ] = headers if esp_account : if not isinstance ( esp_account , string_types ) : logger . error ( 'kwarg esp_account must be a string, got %s' % ( type ( esp_account ) ) ) payload [ 'esp_account' ] = esp_account if locale : if not isinstance ( locale , string_types ) : logger . error ( 'kwarg locale must be a string, got %s' % ( type ( locale ) ) ) payload [ 'locale' ] = locale if email_version_name : if not isinstance ( email_version_name , string_types ) : logger . error ( 'kwarg email_version_name must be a string, got %s' % ( type ( email_version_name ) ) ) payload [ 'version_name' ] = email_version_name if inline : payload [ 'inline' ] = self . _make_file_dict ( inline ) if files : payload [ 'files' ] = [ self . _make_file_dict ( f ) for f in files ] return self . _api_request ( self . SEND_ENDPOINT , self . HTTP_POST , payload = payload , timeout = timeout )
7099	def child_added ( self , child ) : if child . widget : self . parent ( ) . init_info_window_adapter ( ) super ( AndroidMapMarker , self ) . child_added ( child )
11311	def get_record ( self ) : self . recid = self . get_recid ( ) self . remove_controlfields ( ) self . update_system_numbers ( ) self . add_systemnumber ( "Inspire" , recid = self . recid ) self . add_control_number ( "003" , "SzGeCERN" ) self . update_collections ( ) self . update_languages ( ) self . update_reportnumbers ( ) self . update_authors ( ) self . update_journals ( ) self . update_subject_categories ( "INSPIRE" , "SzGeCERN" , "categories_cds" ) self . update_pagenumber ( ) self . update_notes ( ) self . update_experiments ( ) self . update_isbn ( ) self . update_dois ( ) self . update_links_and_ffts ( ) self . update_date ( ) self . update_date_year ( ) self . update_hidden_notes ( ) self . update_oai_info ( ) self . update_cnum ( ) self . update_conference_info ( ) self . fields_list = [ "909" , "541" , "961" , "970" , "690" , "695" , "981" , ] self . strip_fields ( ) if "ANNOUNCEMENT" in self . collections : self . update_conference_111 ( ) self . update_conference_links ( ) record_add_field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CONFERENCE" ) ] ) if "THESIS" in self . collections : self . update_thesis_information ( ) self . update_thesis_supervisors ( ) if "PROCEEDINGS" in self . collections : self . update_title_to_proceeding ( ) self . update_author_to_proceeding ( ) record_add_field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CONFERENCE" ) ] ) if self . tag_as_cern : record_add_field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CERN" ) ] ) return self . record
9111	def size_attachments ( self ) : total_size = 0 for attachment in self . fs_cleansed_attachments : total_size += stat ( attachment ) . st_size return total_size
2205	def userhome ( username = None ) : if username is None : if 'HOME' in os . environ : userhome_dpath = os . environ [ 'HOME' ] else : if sys . platform . startswith ( 'win32' ) : if 'USERPROFILE' in os . environ : userhome_dpath = os . environ [ 'USERPROFILE' ] elif 'HOMEPATH' in os . environ : drive = os . environ . get ( 'HOMEDRIVE' , '' ) userhome_dpath = join ( drive , os . environ [ 'HOMEPATH' ] ) else : raise OSError ( "Cannot determine the user's home directory" ) else : import pwd userhome_dpath = pwd . getpwuid ( os . getuid ( ) ) . pw_dir else : if sys . platform . startswith ( 'win32' ) : c_users = dirname ( userhome ( ) ) userhome_dpath = join ( c_users , username ) if not exists ( userhome_dpath ) : raise KeyError ( 'Unknown user: {}' . format ( username ) ) else : import pwd try : pwent = pwd . getpwnam ( username ) except KeyError : raise KeyError ( 'Unknown user: {}' . format ( username ) ) userhome_dpath = pwent . pw_dir return userhome_dpath
5257	def block_to_fork ( block_number ) : forks_by_block = { 0 : "frontier" , 1150000 : "homestead" , 2463000 : "tangerine_whistle" , 2675000 : "spurious_dragon" , 4370000 : "byzantium" , 7280000 : "petersburg" , 9999999 : "serenity" } fork_names = list ( forks_by_block . values ( ) ) fork_blocks = list ( forks_by_block . keys ( ) ) return fork_names [ bisect ( fork_blocks , block_number ) - 1 ]
4470	def _get_param_names ( cls ) : init = cls . __init__ args , varargs = inspect . getargspec ( init ) [ : 2 ] if varargs is not None : raise RuntimeError ( 'BaseTransformer objects cannot have varargs' ) args . pop ( 0 ) args . sort ( ) return args
11767	def weighted_sample_with_replacement ( seq , weights , n ) : sample = weighted_sampler ( seq , weights ) return [ sample ( ) for s in range ( n ) ]
3958	def resolve ( cls , all_known_repos , name ) : match = None for repo in all_known_repos : if repo . remote_path == name : return repo if name == repo . short_name : if match is None : match = repo else : raise RuntimeError ( 'Short repo name {} is ambiguous. It matches both {} and {}' . format ( name , match . remote_path , repo . remote_path ) ) if match is None : raise RuntimeError ( 'Short repo name {} does not match any known repos' . format ( name ) ) return match
12202	def _from_jsonlines ( cls , lines , selector_handler = None , strict = False , debug = False ) : return cls ( json . loads ( "\n" . join ( [ l for l in lines if not cls . REGEX_COMMENT_LINE . match ( l ) ] ) ) , selector_handler = selector_handler , strict = strict , debug = debug )
5389	def _datetime_in_range ( self , dt , dt_min = None , dt_max = None ) : dt = dt . replace ( microsecond = 0 ) if dt_min : dt_min = dt_min . replace ( microsecond = 0 ) else : dt_min = dsub_util . replace_timezone ( datetime . datetime . min , pytz . utc ) if dt_max : dt_max = dt_max . replace ( microsecond = 0 ) else : dt_max = dsub_util . replace_timezone ( datetime . datetime . max , pytz . utc ) return dt_min <= dt <= dt_max
9176	def with_db_cursor ( func ) : @ functools . wraps ( func ) def wrapped ( * args , ** kwargs ) : if 'cursor' in kwargs or func . func_code . co_argcount == len ( args ) : return func ( * args , ** kwargs ) with db_connect ( ) as db_connection : with db_connection . cursor ( ) as cursor : kwargs [ 'cursor' ] = cursor return func ( * args , ** kwargs ) return wrapped
11639	def json_write_data ( json_data , filename ) : with open ( filename , 'w' ) as fp : json . dump ( json_data , fp , indent = 4 , sort_keys = True , ensure_ascii = False ) return True return False
5177	def resources ( self , type_ = None , title = None , ** kwargs ) : if type_ is None : resources = self . __api . resources ( query = EqualsOperator ( "certname" , self . name ) , ** kwargs ) elif type_ is not None and title is None : resources = self . __api . resources ( type_ = type_ , query = EqualsOperator ( "certname" , self . name ) , ** kwargs ) else : resources = self . __api . resources ( type_ = type_ , title = title , query = EqualsOperator ( "certname" , self . name ) , ** kwargs ) return resources
11934	def auto_widget ( field ) : info = { 'widget' : field . field . widget . __class__ . __name__ , 'field' : field . field . __class__ . __name__ , 'name' : field . name , } return [ fmt . format ( ** info ) for fmt in ( '{field}_{widget}_{name}' , '{field}_{name}' , '{widget}_{name}' , '{field}_{widget}' , '{name}' , '{widget}' , '{field}' , ) ]
1747	def _in_range ( self , index ) : if isinstance ( index , slice ) : in_range = index . start < index . stop and index . start >= self . start and index . stop <= self . end else : in_range = index >= self . start and index <= self . end return in_range
5504	def relative_datetime ( self ) : now = datetime . now ( timezone . utc ) tense = "from now" if self . created_at > now else "ago" return "{0} {1}" . format ( humanize . naturaldelta ( now - self . created_at ) , tense )
6068	def tabulate_integral ( self , grid , tabulate_bins ) : eta_min = 1.0e-4 eta_max = 1.05 * np . max ( self . grid_to_elliptical_radii ( grid ) ) minimum_log_eta = np . log10 ( eta_min ) maximum_log_eta = np . log10 ( eta_max ) bin_size = ( maximum_log_eta - minimum_log_eta ) / ( tabulate_bins - 1 ) return eta_min , eta_max , minimum_log_eta , maximum_log_eta , bin_size
11309	def map_to_dictionary ( self , url , obj , ** kwargs ) : maxwidth = kwargs . get ( 'maxwidth' , None ) maxheight = kwargs . get ( 'maxheight' , None ) provider_url , provider_name = self . provider_from_url ( url ) mapping = { 'version' : '1.0' , 'url' : url , 'provider_name' : provider_name , 'provider_url' : provider_url , 'type' : self . resource_type } self . preprocess ( obj , mapping , ** kwargs ) if self . resource_type == 'photo' and self . get_image ( obj ) : self . resize_photo ( obj , mapping , maxwidth , maxheight ) elif self . resource_type in ( 'video' , 'rich' , 'photo' ) : width , height = size_to_nearest ( maxwidth , maxheight , self . _meta . valid_sizes , self . _meta . force_fit ) mapping . update ( width = width , height = height ) if self . get_image ( obj ) : self . thumbnail ( obj , mapping ) for attr in ( 'title' , 'author_name' , 'author_url' , 'html' ) : self . map_attr ( mapping , attr , obj ) if 'url' in mapping : mapping [ 'url' ] = relative_to_full ( mapping [ 'url' ] , url ) if 'thumbnail_url' in mapping : mapping [ 'thumbnail_url' ] = relative_to_full ( mapping [ 'thumbnail_url' ] , url ) if 'html' not in mapping and mapping [ 'type' ] in ( 'video' , 'rich' ) : mapping [ 'html' ] = self . render_html ( obj , context = Context ( mapping ) ) self . postprocess ( obj , mapping , ** kwargs ) return mapping
10932	def get_termination_stats ( self , get_cos = True ) : delta_vals = self . _last_vals - self . param_vals delta_err = self . _last_error - self . error frac_err = delta_err / self . error to_return = { 'delta_vals' : delta_vals , 'delta_err' : delta_err , 'num_iter' : 1 * self . _num_iter , 'frac_err' : frac_err , 'error' : self . error , 'exp_err' : self . _exp_err } if get_cos : model_cosine = self . calc_model_cosine ( ) to_return . update ( { 'model_cosine' : model_cosine } ) return to_return
8507	def _get_param_names ( self ) : template = Template ( self . yaml_string ) names = [ 'yaml_string' ] for match in re . finditer ( template . pattern , template . template ) : name = match . group ( 'named' ) or match . group ( 'braced' ) assert name is not None names . append ( name ) return names
5738	def enqueue ( self , f , * args , ** kwargs ) : task = Task ( uuid4 ( ) . hex , f , args , kwargs ) self . storage . put_task ( task ) return self . enqueue_task ( task )
12863	def quoted ( parser = any_token ) : quote_char = quote ( ) value , _ = many_until ( parser , partial ( one_of , quote_char ) ) return build_string ( value )
12124	def to_table ( args , vdims = [ ] ) : "Helper function to convet an Args object to a HoloViews Table" if not Table : return "HoloViews Table not available" kdims = [ dim for dim in args . constant_keys + args . varying_keys if dim not in vdims ] items = [ tuple ( [ spec [ k ] for k in kdims + vdims ] ) for spec in args . specs ] return Table ( items , kdims = kdims , vdims = vdims )
4392	def adsSyncWriteControlReqEx ( port , address , ads_state , device_state , data , plc_data_type ) : sync_write_control_request = _adsDLL . AdsSyncWriteControlReqEx ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) ads_state_c = ctypes . c_ulong ( ads_state ) device_state_c = ctypes . c_ulong ( device_state ) if plc_data_type == PLCTYPE_STRING : data = ctypes . c_char_p ( data . encode ( "utf-8" ) ) data_pointer = data data_length = len ( data_pointer . value ) + 1 else : data = plc_data_type ( data ) data_pointer = ctypes . pointer ( data ) data_length = ctypes . sizeof ( data ) error_code = sync_write_control_request ( port , ams_address_pointer , ads_state_c , device_state_c , data_length , data_pointer , ) if error_code : raise ADSError ( error_code )
8928	def prep ( ctx , commit = True ) : cfg = config . load ( ) scm = scm_provider ( cfg . project_root , commit = commit , ctx = ctx ) if not scm . workdir_is_clean ( ) : notify . failure ( "You have uncommitted changes, please commit or stash them!" ) setup_cfg = cfg . rootjoin ( 'setup.cfg' ) if os . path . exists ( setup_cfg ) : with io . open ( setup_cfg , encoding = 'utf-8' ) as handle : data = handle . readlines ( ) changed = False for i , line in enumerate ( data ) : if any ( line . startswith ( i ) for i in ( 'tag_build' , 'tag_date' ) ) : data [ i ] = '#' + data [ i ] changed = True if changed and commit : notify . info ( "Rewriting 'setup.cfg'..." ) with io . open ( setup_cfg , 'w' , encoding = 'utf-8' ) as handle : handle . write ( '' . join ( data ) ) scm . add_file ( 'setup.cfg' ) elif changed : notify . warning ( "WOULD rewrite 'setup.cfg', but --no-commit was passed" ) else : notify . warning ( "Cannot rewrite 'setup.cfg', none found!" ) ctx . run ( 'python setup.py -q develop -U' ) version = capture ( 'python setup.py --version' ) ctx . run ( 'invoke clean --all build --docs release.dist' ) for distfile in os . listdir ( 'dist' ) : trailer = distfile . split ( '-' + version ) [ 1 ] trailer , _ = os . path . splitext ( trailer ) if trailer and trailer [ 0 ] not in '.-' : notify . failure ( "The version found in 'dist' seems to be" " a pre-release one! [{}{}]" . format ( version , trailer ) ) scm . commit ( ctx . rituals . release . commit . message . format ( version = version ) ) scm . tag ( ctx . rituals . release . tag . name . format ( version = version ) , ctx . rituals . release . tag . message . format ( version = version ) )
4885	def allow_request ( self , request , view ) : service_users = get_service_usernames ( ) if request . user . username in service_users : self . update_throttle_scope ( ) return super ( ServiceUserThrottle , self ) . allow_request ( request , view )
13773	def init_logs ( path = None , target = None , logger_name = 'root' , level = logging . DEBUG , maxBytes = 1 * 1024 * 1024 , backupCount = 5 , application_name = 'default' , server_hostname = None , fields = None ) : log_file = os . path . abspath ( os . path . join ( path , target ) ) logger = logging . getLogger ( logger_name ) logger . setLevel ( level ) handler = logging . handlers . RotatingFileHandler ( log_file , maxBytes = maxBytes , backupCount = backupCount ) handler . setLevel ( level ) handler . setFormatter ( JsonFormatter ( application_name = application_name , server_hostname = server_hostname , fields = fields ) ) logger . addHandler ( handler )
397	def cross_entropy ( output , target , name = None ) : if name is None : raise Exception ( "Please give a unique name to tl.cost.cross_entropy for TF1.0+" ) return tf . reduce_mean ( tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = target , logits = output ) , name = name )
12600	def concat_sheets ( xl_path : str , sheetnames = None , add_tab_names = False ) : xl_path , choice = _check_xl_path ( xl_path ) if sheetnames is None : sheetnames = get_sheet_list ( xl_path ) sheets = pd . read_excel ( xl_path , sheetname = sheetnames ) if add_tab_names : for tab in sheets : sheets [ tab ] [ 'Tab' ] = [ tab ] * len ( sheets [ tab ] ) return pd . concat ( [ sheets [ tab ] for tab in sheets ] )
11464	def download ( self , source_file , target_folder = '' ) : current_folder = self . _ftp . pwd ( ) if not target_folder . startswith ( '/' ) : target_folder = join ( getcwd ( ) , target_folder ) folder = os . path . dirname ( source_file ) self . cd ( folder ) if folder . startswith ( "/" ) : folder = folder [ 1 : ] destination_folder = join ( target_folder , folder ) if not os . path . exists ( destination_folder ) : print ( "Creating folder" , destination_folder ) os . makedirs ( destination_folder ) source_file = os . path . basename ( source_file ) destination = join ( destination_folder , source_file ) try : with open ( destination , 'wb' ) as result : self . _ftp . retrbinary ( 'RETR %s' % ( source_file , ) , result . write ) except error_perm as e : print ( e ) remove ( join ( target_folder , source_file ) ) raise self . _ftp . cwd ( current_folder )
9729	def get_analog_single ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . device_count ) : component_position , device = QRTPacket . _get_exact ( RTAnalogDeviceSingle , data , component_position ) RTAnalogDeviceSamples . format = struct . Struct ( RTAnalogDeviceSamples . format_str % device . channel_count ) component_position , sample = QRTPacket . _get_tuple ( RTAnalogDeviceSamples , data , component_position ) append_components ( ( device , sample ) ) return components
6678	def uncommented_lines ( self , filename , use_sudo = False ) : func = run_as_root if use_sudo else self . run res = func ( 'cat %s' % quote ( filename ) , quiet = True ) if res . succeeded : return [ line for line in res . splitlines ( ) if line and not line . startswith ( '#' ) ] return [ ]
11127	def update_file ( self , value , relativePath , name = None , description = False , klass = False , dump = False , pull = False , ACID = None , verbose = False ) : if ACID is None : ACID = self . __ACID assert isinstance ( ACID , bool ) , "ACID must be boolean" relativePath = os . path . normpath ( relativePath ) if relativePath == '.' : relativePath = '' assert name != '.pyrepinfo' , "'.pyrepinfo' is not allowed as file name in main repository directory" assert name != '.pyrepstate' , "'.pyrepstate' is not allowed as file name in main repository directory" assert name != '.pyreplock' , "'.pyreplock' is not allowed as file name in main repository directory" if name is None : assert len ( relativePath ) , "name must be given when relative path is given as empty string or as a simple dot '.'" relativePath , name = os . path . split ( relativePath ) fileInfoDict , errorMessage = self . get_file_info ( relativePath , name ) assert fileInfoDict is not None , errorMessage realPath = os . path . join ( self . __path , relativePath ) if verbose : if not os . path . isfile ( os . path . join ( realPath , name ) ) : warnings . warn ( "file '%s' is in repository but does not exist in the system. It is therefore being recreated." % os . path . join ( realPath , name ) ) if not dump : dump = fileInfoDict [ "dump" ] if not pull : pull = fileInfoDict [ "pull" ] if ACID : savePath = os . path . join ( tempfile . gettempdir ( ) , name ) else : savePath = os . path . join ( realPath , name ) try : exec ( dump . replace ( "$FILE_PATH" , str ( savePath ) ) ) except Exception as e : message = "unable to dump the file (%s)" % e if 'pickle.dump(' in dump : message += '\nmore info: %s' % str ( get_pickling_errors ( value ) ) raise Exception ( message ) if ACID : try : shutil . copyfile ( savePath , os . path . join ( realPath , name ) ) except Exception as e : os . remove ( savePath ) if verbose : warnings . warn ( e ) return os . remove ( savePath ) fileInfoDict [ "timestamp" ] = datetime . utcnow ( ) if description is not False : fileInfoDict [ "description" ] = description if klass is not False : assert inspect . isclass ( klass ) , "klass must be a class definition" fileInfoDict [ "class" ] = klass self . save ( )
7230	def create ( self , vectors ) : if type ( vectors ) is dict : vectors = [ vectors ] for vector in vectors : if not 'properties' in list ( vector . keys ( ) ) : raise Exception ( 'Vector does not contain "properties" field.' ) if not 'item_type' in list ( vector [ 'properties' ] . keys ( ) ) : raise Exception ( 'Vector does not contain "item_type".' ) if not 'ingest_source' in list ( vector [ 'properties' ] . keys ( ) ) : raise Exception ( 'Vector does not contain "ingest_source".' ) r = self . gbdx_connection . post ( self . create_url , data = json . dumps ( vectors ) ) r . raise_for_status ( ) return r . json ( )
3220	def get_client ( service , service_type = 'client' , ** conn_args ) : client_details = choose_client ( service ) user_agent = get_user_agent ( ** conn_args ) if client_details : if client_details [ 'client_type' ] == 'cloud' : client = get_gcp_client ( mod_name = client_details [ 'module_name' ] , pkg_name = conn_args . get ( 'pkg_name' , 'google.cloud' ) , key_file = conn_args . get ( 'key_file' , None ) , project = conn_args [ 'project' ] , user_agent = user_agent ) else : client = get_google_client ( mod_name = client_details [ 'module_name' ] , key_file = conn_args . get ( 'key_file' , None ) , user_agent = user_agent , api_version = conn_args . get ( 'api_version' , 'v1' ) ) else : try : client = get_google_client ( mod_name = service , key_file = conn_args . get ( 'key_file' , None ) , user_agent = user_agent , api_version = conn_args . get ( 'api_version' , 'v1' ) ) except Exception as e : raise e return client_details , client
5400	def _map ( self , event ) : description = event . get ( 'description' , '' ) start_time = google_base . parse_rfc3339_utc_string ( event . get ( 'timestamp' , '' ) ) for name , regex in _EVENT_REGEX_MAP . items ( ) : match = regex . match ( description ) if match : return { 'name' : name , 'start-time' : start_time } , match return { 'name' : description , 'start-time' : start_time } , None
7435	def _zbufcountlines ( filename , gzipped ) : if gzipped : cmd1 = [ "gunzip" , "-c" , filename ] else : cmd1 = [ "cat" , filename ] cmd2 = [ "wc" ] proc1 = sps . Popen ( cmd1 , stdout = sps . PIPE , stderr = sps . PIPE ) proc2 = sps . Popen ( cmd2 , stdin = proc1 . stdout , stdout = sps . PIPE , stderr = sps . PIPE ) res = proc2 . communicate ( ) [ 0 ] if proc2 . returncode : raise IPyradWarningExit ( "error zbufcountlines {}:" . format ( res ) ) LOGGER . info ( res ) nlines = int ( res . split ( ) [ 0 ] ) return nlines
2545	def add_review_comment ( self , doc , comment ) : if len ( doc . reviews ) != 0 : if not self . review_comment_set : self . review_comment_set = True doc . reviews [ - 1 ] . comment = comment return True else : raise CardinalityError ( 'ReviewComment' ) else : raise OrderError ( 'ReviewComment' )
4005	def streaming_to_client ( ) : for handler in client_logger . handlers : if hasattr ( handler , 'append_newlines' ) : break else : handler = None old_propagate = client_logger . propagate client_logger . propagate = False if handler is not None : old_append = handler . append_newlines handler . append_newlines = False yield client_logger . propagate = old_propagate if handler is not None : handler . append_newlines = old_append
11123	def remove_directory ( self , relativePath , removeFromSystem = False ) : relativePath = os . path . normpath ( relativePath ) parentDirInfoDict , errorMessage = self . get_parent_directory_info ( relativePath ) assert parentDirInfoDict is not None , errorMessage path , name = os . path . split ( relativePath ) if dict . __getitem__ ( parentDirInfoDict , 'directories' ) . get ( name , None ) is None : raise Exception ( "'%s' is not a registered directory in repository relative path '%s'" % ( name , path ) ) if removeFromSystem : for rp in self . walk_files_relative_path ( relativePath = relativePath ) : ap = os . path . join ( self . __path , relativePath , rp ) if not os . path . isfile ( ap ) : continue if not os . path . exists ( ap ) : continue if os . path . isfile ( ap ) : os . remove ( ap ) for rp in self . walk_directories_relative_path ( relativePath = relativePath ) : ap = os . path . join ( self . __path , relativePath , rp ) if not os . path . isdir ( ap ) : continue if not os . path . exists ( ap ) : continue if not len ( os . listdir ( ap ) ) : os . rmdir ( ap ) dict . __getitem__ ( parentDirInfoDict , 'directories' ) . pop ( name , None ) ap = os . path . join ( self . __path , relativePath ) if not os . path . isdir ( ap ) : if not len ( os . listdir ( ap ) ) : os . rmdir ( ap ) self . save ( )
13143	def _transform_triple_numpy ( x ) : return np . array ( [ x . head , x . relation , x . tail ] , dtype = np . int64 )
9939	def find_in_app ( self , app , path ) : storage = self . storages . get ( app , None ) if storage : if storage . exists ( path ) : matched_path = storage . path ( path ) if matched_path : return matched_path
431	def save_image ( image , image_path = '_temp.png' ) : try : imageio . imwrite ( image_path , image ) except Exception : imageio . imwrite ( image_path , image [ : , : , 0 ] )
4554	def pointOnCircle ( cx , cy , radius , angle ) : angle = math . radians ( angle ) - ( math . pi / 2 ) x = cx + radius * math . cos ( angle ) if x < cx : x = math . ceil ( x ) else : x = math . floor ( x ) y = cy + radius * math . sin ( angle ) if y < cy : y = math . ceil ( y ) else : y = math . floor ( y ) return ( int ( x ) , int ( y ) )
11495	def list_users ( self , limit = 20 ) : parameters = dict ( ) parameters [ 'limit' ] = limit response = self . request ( 'midas.user.list' , parameters ) return response
2643	def filepath ( self ) : if hasattr ( self , 'local_path' ) : return self . local_path if self . scheme in [ 'ftp' , 'http' , 'https' , 'globus' ] : return self . filename elif self . scheme in [ 'file' ] : return self . path else : raise Exception ( 'Cannot return filepath for unknown scheme {}' . format ( self . scheme ) )
8244	def shader ( x , y , dx , dy , radius = 300 , angle = 0 , spread = 90 ) : if angle != None : radius *= 2 d = sqrt ( ( dx - x ) ** 2 + ( dy - y ) ** 2 ) a = degrees ( atan2 ( dy - y , dx - x ) ) + 180 if d <= radius : d1 = 1.0 * d / radius else : d1 = 1.0 if angle is None : return 1 - d1 angle = 360 - angle % 360 spread = max ( 0 , min ( spread , 360 ) ) if spread == 0 : return 0.0 d = abs ( a - angle ) if d <= spread / 2 : d2 = d / spread + d1 else : d2 = 1.0 if 360 - angle <= spread / 2 : d = abs ( 360 - angle + a ) if d <= spread / 2 : d2 = d / spread + d1 if angle < spread / 2 : d = abs ( 360 + angle - a ) if d <= spread / 2 : d2 = d / spread + d1 return 1 - max ( 0 , min ( d2 , 1 ) )
834	def run ( self ) : print "-" * 80 + "Computing the SDR" + "-" * 80 self . sp . compute ( self . inputArray , True , self . activeArray ) print self . activeArray . nonzero ( )
1649	def GetLineWidth ( line ) : if isinstance ( line , unicode ) : width = 0 for uc in unicodedata . normalize ( 'NFC' , line ) : if unicodedata . east_asian_width ( uc ) in ( 'W' , 'F' ) : width += 2 elif not unicodedata . combining ( uc ) : width += 1 return width else : return len ( line )
11816	def score ( self , code ) : text = permutation_decode ( self . ciphertext , code ) logP = ( sum ( [ log ( self . Pwords [ word ] ) for word in words ( text ) ] ) + sum ( [ log ( self . P1 [ c ] ) for c in text ] ) + sum ( [ log ( self . P2 [ b ] ) for b in bigrams ( text ) ] ) ) return exp ( logP )
12829	def parse_conll ( self , texts : List [ str ] , retry_count : int = 0 ) -> List [ str ] : post_data = { 'texts' : texts , 'output_type' : 'conll' } try : response = requests . post ( f'http://{self.hostname}:{self.port}' , json = post_data , headers = { 'Connection' : 'close' } ) response . raise_for_status ( ) except ( requests . exceptions . ConnectionError , requests . exceptions . Timeout ) as server_error : raise ServerError ( server_error , self . hostname , self . port ) except requests . exceptions . HTTPError as http_error : raise http_error else : try : return response . json ( ) except json . JSONDecodeError as json_exception : if retry_count == self . retries : self . log_error ( response . text ) raise Exception ( 'Json Decoding error cannot parse this ' f':\n{response.text}' ) return self . parse_conll ( texts , retry_count + 1 )
8216	def hide_variables_window ( self ) : if self . var_window is not None : self . var_window . window . destroy ( ) self . var_window = None
13081	def register_filters ( self ) : for _filter , instance in self . _filters : if not instance : self . app . jinja_env . filters [ _filter . replace ( "f_" , "" ) ] = getattr ( flask_nemo . filters , _filter ) else : self . app . jinja_env . filters [ _filter . replace ( "f_" , "" ) ] = getattr ( instance , _filter . replace ( "_{}" . format ( instance . name ) , "" ) )
10314	def canonical_circulation ( elements : T , key : Optional [ Callable [ [ T ] , bool ] ] = None ) -> T : return min ( get_circulations ( elements ) , key = key )
2074	def convert_input_vector ( y , index ) : if y is None : return None if isinstance ( y , pd . Series ) : return y elif isinstance ( y , np . ndarray ) : if len ( np . shape ( y ) ) == 1 : return pd . Series ( y , name = 'target' , index = index ) elif len ( np . shape ( y ) ) == 2 and np . shape ( y ) [ 0 ] == 1 : return pd . Series ( y [ 0 , : ] , name = 'target' , index = index ) elif len ( np . shape ( y ) ) == 2 and np . shape ( y ) [ 1 ] == 1 : return pd . Series ( y [ : , 0 ] , name = 'target' , index = index ) else : raise ValueError ( 'Unexpected input shape: %s' % ( str ( np . shape ( y ) ) ) ) elif np . isscalar ( y ) : return pd . Series ( [ y ] , name = 'target' , index = index ) elif isinstance ( y , list ) : if len ( y ) == 0 or ( len ( y ) > 0 and not isinstance ( y [ 0 ] , list ) ) : return pd . Series ( y , name = 'target' , index = index ) elif len ( y ) > 0 and isinstance ( y [ 0 ] , list ) and len ( y [ 0 ] ) == 1 : flatten = lambda y : [ item for sublist in y for item in sublist ] return pd . Series ( flatten ( y ) , name = 'target' , index = index ) elif len ( y ) == 1 and isinstance ( y [ 0 ] , list ) : return pd . Series ( y [ 0 ] , name = 'target' , index = index ) else : raise ValueError ( 'Unexpected input shape' ) elif isinstance ( y , pd . DataFrame ) : if len ( list ( y ) ) == 0 : return pd . Series ( y , name = 'target' ) if len ( list ( y ) ) == 1 : return y . iloc [ : , 0 ] else : raise ValueError ( 'Unexpected input shape: %s' % ( str ( y . shape ) ) ) else : return pd . Series ( y , name = 'target' , index = index )
3332	def init_logging ( config ) : verbose = config . get ( "verbose" , 3 ) enable_loggers = config . get ( "enable_loggers" , [ ] ) if enable_loggers is None : enable_loggers = [ ] logger_date_format = config . get ( "logger_date_format" , "%Y-%m-%d %H:%M:%S" ) logger_format = config . get ( "logger_format" , "%(asctime)s.%(msecs)03d - <%(thread)d> %(name)-27s %(levelname)-8s: %(message)s" , ) formatter = logging . Formatter ( logger_format , logger_date_format ) consoleHandler = logging . StreamHandler ( sys . stdout ) consoleHandler . setFormatter ( formatter ) logger = logging . getLogger ( BASE_LOGGER_NAME ) if verbose >= 4 : logger . setLevel ( logging . DEBUG ) elif verbose == 3 : logger . setLevel ( logging . INFO ) elif verbose == 2 : logger . setLevel ( logging . WARN ) elif verbose == 1 : logger . setLevel ( logging . ERROR ) else : logger . setLevel ( logging . CRITICAL ) logger . propagate = False for hdlr in logger . handlers [ : ] : try : hdlr . flush ( ) hdlr . close ( ) except Exception : pass logger . removeHandler ( hdlr ) logger . addHandler ( consoleHandler ) if verbose >= 3 : for e in enable_loggers : if not e . startswith ( BASE_LOGGER_NAME + "." ) : e = BASE_LOGGER_NAME + "." + e lg = logging . getLogger ( e . strip ( ) ) lg . setLevel ( logging . DEBUG )
656	def averageOnTimePerTimestep ( vectors , numSamples = None ) : if vectors . ndim == 1 : vectors . shape = ( - 1 , 1 ) numTimeSteps = len ( vectors ) numElements = len ( vectors [ 0 ] ) if numSamples is not None : import pdb pdb . set_trace ( ) countOn = numpy . random . randint ( 0 , numElements , numSamples ) vectors = vectors [ : , countOn ] durations = numpy . zeros ( vectors . shape , dtype = 'int32' ) for col in xrange ( vectors . shape [ 1 ] ) : _fillInOnTimes ( vectors [ : , col ] , durations [ : , col ] ) sums = vectors . sum ( axis = 1 ) sums . clip ( min = 1 , max = numpy . inf , out = sums ) avgDurations = durations . sum ( axis = 1 , dtype = 'float64' ) / sums avgOnTime = avgDurations . sum ( ) / ( avgDurations > 0 ) . sum ( ) freqCounts = _accumulateFrequencyCounts ( avgDurations ) return ( avgOnTime , freqCounts )
6219	def interleaves ( self , info ) : return info . byte_offset == self . component_type . size * self . components
3804	def calculate ( self , T , method ) : r if method == GHARAGHEIZI_G : kg = Gharagheizi_gas ( T , self . MW , self . Tb , self . Pc , self . omega ) elif method == DIPPR_9B : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T ) if hasattr ( self . mug , '__call__' ) else self . mug kg = DIPPR9B ( T , self . MW , Cvgm , mug , self . Tc ) elif method == CHUNG : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T ) if hasattr ( self . mug , '__call__' ) else self . mug kg = Chung ( T , self . MW , self . Tc , self . omega , Cvgm , mug ) elif method == ELI_HANLEY : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm kg = eli_hanley ( T , self . MW , self . Tc , self . Vc , self . Zc , self . omega , Cvgm ) elif method == EUCKEN_MOD : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T ) if hasattr ( self . mug , '__call__' ) else self . mug kg = Eucken_modified ( self . MW , Cvgm , mug ) elif method == EUCKEN : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T ) if hasattr ( self . mug , '__call__' ) else self . mug kg = Eucken ( self . MW , Cvgm , mug ) elif method == DIPPR_PERRY_8E : kg = EQ102 ( T , * self . Perrys2_314_coeffs ) elif method == VDI_PPDS : kg = horner ( self . VDI_PPDS_coeffs , T ) elif method == BAHADORI_G : kg = Bahadori_gas ( T , self . MW ) elif method == COOLPROP : kg = CoolProp_T_dependent_property ( T , self . CASRN , 'L' , 'g' ) elif method in self . tabular_data : kg = self . interpolate ( T , method ) return kg
1143	def _bytelist2longBigEndian ( list ) : "Transform a list of characters into a list of longs." imax = len ( list ) // 4 hl = [ 0 ] * imax j = 0 i = 0 while i < imax : b0 = ord ( list [ j ] ) << 24 b1 = ord ( list [ j + 1 ] ) << 16 b2 = ord ( list [ j + 2 ] ) << 8 b3 = ord ( list [ j + 3 ] ) hl [ i ] = b0 | b1 | b2 | b3 i = i + 1 j = j + 4 return hl
7114	def fit ( self , X , y ) : word_vector_transformer = WordVectorTransformer ( padding = 'max' ) X = word_vector_transformer . fit_transform ( X ) X = LongTensor ( X ) self . word_vector_transformer = word_vector_transformer y_transformer = LabelEncoder ( ) y = y_transformer . fit_transform ( y ) y = torch . from_numpy ( y ) self . y_transformer = y_transformer dataset = CategorizedDataset ( X , y ) dataloader = DataLoader ( dataset , batch_size = self . batch_size , shuffle = True , num_workers = 4 ) KERNEL_SIZES = self . kernel_sizes NUM_KERNEL = self . num_kernel EMBEDDING_DIM = self . embedding_dim model = TextCNN ( vocab_size = word_vector_transformer . get_vocab_size ( ) , embedding_dim = EMBEDDING_DIM , output_size = len ( self . y_transformer . classes_ ) , kernel_sizes = KERNEL_SIZES , num_kernel = NUM_KERNEL ) if USE_CUDA : model = model . cuda ( ) EPOCH = self . epoch LR = self . lr loss_function = nn . CrossEntropyLoss ( ) optimizer = optim . Adam ( model . parameters ( ) , lr = LR ) for epoch in range ( EPOCH ) : losses = [ ] for i , data in enumerate ( dataloader ) : X , y = data X , y = Variable ( X ) , Variable ( y ) optimizer . zero_grad ( ) model . train ( ) output = model ( X ) loss = loss_function ( output , y ) losses . append ( loss . data . tolist ( ) [ 0 ] ) loss . backward ( ) optimizer . step ( ) if i % 100 == 0 : print ( "[%d/%d] mean_loss : %0.2f" % ( epoch , EPOCH , np . mean ( losses ) ) ) losses = [ ] self . model = model
10028	def describe_events ( self , environment_name , next_token = None , start_time = None ) : events = self . ebs . describe_events ( application_name = self . app_name , environment_name = environment_name , next_token = next_token , start_time = start_time + 'Z' ) return ( events [ 'DescribeEventsResponse' ] [ 'DescribeEventsResult' ] [ 'Events' ] , events [ 'DescribeEventsResponse' ] [ 'DescribeEventsResult' ] [ 'NextToken' ] )
12644	def cert_info ( ) : sec_type = security_type ( ) if sec_type == 'pem' : return get_config_value ( 'pem_path' , fallback = None ) if sec_type == 'cert' : cert_path = get_config_value ( 'cert_path' , fallback = None ) key_path = get_config_value ( 'key_path' , fallback = None ) return cert_path , key_path return None
1862	def SCAS ( cpu , dest , src ) : dest_reg = dest . reg mem_reg = src . mem . base size = dest . size arg0 = dest . read ( ) arg1 = src . read ( ) res = arg0 - arg1 cpu . _calculate_CMP_flags ( size , res , arg0 , arg1 ) increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( mem_reg , cpu . read_register ( mem_reg ) + increment )
6643	def getExtraIncludes ( self ) : if 'extraIncludes' in self . description : return [ os . path . normpath ( x ) for x in self . description [ 'extraIncludes' ] ] else : return [ ]
7990	def transport_connected ( self ) : with self . lock : if self . initiator : if self . _output_state is None : self . _initiate ( )
6619	def poll ( self ) : finished_procs = [ p for p in self . running_procs if p . poll ( ) is not None ] self . running_procs = collections . deque ( [ p for p in self . running_procs if p not in finished_procs ] ) for proc in finished_procs : stdout , stderr = proc . communicate ( ) finished_pids = [ p . pid for p in finished_procs ] self . finished_pids . extend ( finished_pids ) logger = logging . getLogger ( __name__ ) messages = 'Running: {}, Finished: {}' . format ( len ( self . running_procs ) , len ( self . finished_pids ) ) logger . info ( messages ) return finished_pids
0	def save_act ( self , path = None ) : if path is None : path = os . path . join ( logger . get_dir ( ) , "model.pkl" ) with tempfile . TemporaryDirectory ( ) as td : save_variables ( os . path . join ( td , "model" ) ) arc_name = os . path . join ( td , "packed.zip" ) with zipfile . ZipFile ( arc_name , 'w' ) as zipf : for root , dirs , files in os . walk ( td ) : for fname in files : file_path = os . path . join ( root , fname ) if file_path != arc_name : zipf . write ( file_path , os . path . relpath ( file_path , td ) ) with open ( arc_name , "rb" ) as f : model_data = f . read ( ) with open ( path , "wb" ) as f : cloudpickle . dump ( ( model_data , self . _act_params ) , f )
2102	def configure_model ( self , attrs , field_name ) : self . relationship = field_name self . _set_method_names ( relationship = field_name ) if self . res_name is None : self . res_name = grammar . singularize ( attrs . get ( 'endpoint' , 'unknown' ) . strip ( '/' ) )
3054	def from_string ( cls , key_pem , is_x509_cert ) : key_pem = _helpers . _to_bytes ( key_pem ) if is_x509_cert : der = rsa . pem . load_pem ( key_pem , 'CERTIFICATE' ) asn1_cert , remaining = decoder . decode ( der , asn1Spec = Certificate ( ) ) if remaining != b'' : raise ValueError ( 'Unused bytes' , remaining ) cert_info = asn1_cert [ 'tbsCertificate' ] [ 'subjectPublicKeyInfo' ] key_bytes = _bit_list_to_bytes ( cert_info [ 'subjectPublicKey' ] ) pubkey = rsa . PublicKey . load_pkcs1 ( key_bytes , 'DER' ) else : pubkey = rsa . PublicKey . load_pkcs1 ( key_pem , 'PEM' ) return cls ( pubkey )
8119	def transform_path ( self , path ) : p = path . __class__ ( ) for pt in path : if pt . cmd == "close" : p . closepath ( ) elif pt . cmd == "moveto" : p . moveto ( * self . apply ( pt . x , pt . y ) ) elif pt . cmd == "lineto" : p . lineto ( * self . apply ( pt . x , pt . y ) ) elif pt . cmd == "curveto" : vx1 , vy1 = self . apply ( pt . ctrl1 . x , pt . ctrl1 . y ) vx2 , vy2 = self . apply ( pt . ctrl2 . x , pt . ctrl2 . y ) x , y = self . apply ( pt . x , pt . y ) p . curveto ( vx1 , vy1 , vx2 , vy2 , x , y ) return p
475	def sentence_to_token_ids ( sentence , vocabulary , tokenizer = None , normalize_digits = True , UNK_ID = 3 , _DIGIT_RE = re . compile ( br"\d" ) ) : if tokenizer : words = tokenizer ( sentence ) else : words = basic_tokenizer ( sentence ) if not normalize_digits : return [ vocabulary . get ( w , UNK_ID ) for w in words ] return [ vocabulary . get ( re . sub ( _DIGIT_RE , b"0" , w ) , UNK_ID ) for w in words ]
2741	def remove_tags ( self , tags ) : return self . get_data ( "firewalls/%s/tags" % self . id , type = DELETE , params = { "tags" : tags } )
11155	def print_big_dir ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table = sorted ( [ ( p , p . dirsize ) for p in self . select_dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p , size in size_table [ : top_n ] : print ( "{:<9} {:<9}" . format ( repr_data_size ( size ) , p . abspath ) )
1941	def map_memory_callback ( self , address , size , perms , name , offset , result ) : logger . info ( ' ' . join ( ( "Mapping Memory @" , hex ( address ) if type ( address ) is int else "0x??" , hr_size ( size ) , "-" , perms , "-" , f"{name}:{hex(offset) if name else ''}" , "->" , hex ( result ) ) ) ) self . _emu . mem_map ( address , size , convert_permissions ( perms ) ) self . copy_memory ( address , size )
9313	def sign_sha256 ( key , msg ) : if isinstance ( msg , text_type ) : msg = msg . encode ( 'utf-8' ) return hmac . new ( key , msg , hashlib . sha256 ) . digest ( )
5998	def plot_grid ( grid_arcsec , array , units , kpc_per_arcsec , pointsize , zoom_offset_arcsec ) : if grid_arcsec is not None : if zoom_offset_arcsec is not None : grid_arcsec -= zoom_offset_arcsec grid_units = convert_grid_units ( grid_arcsec = grid_arcsec , array = array , units = units , kpc_per_arcsec = kpc_per_arcsec ) plt . scatter ( y = np . asarray ( grid_units [ : , 0 ] ) , x = np . asarray ( grid_units [ : , 1 ] ) , s = pointsize , c = 'k' )
11550	def setup ( self , configuration = "ModbusSerialClient(method='rtu',port='/dev/cu.usbmodem14101',baudrate=9600)" ) : from pymodbus3 . client . sync import ModbusSerialClient , ModbusUdpClient , ModbusTcpClient self . _client = eval ( configuration ) self . _client . connect ( )
1110	def _dump ( self , tag , x , lo , hi ) : for i in xrange ( lo , hi ) : yield '%s %s' % ( tag , x [ i ] )
9956	def tracemessage ( self , maxlen = 6 ) : result = "" for i , value in enumerate ( self ) : result += "{0}: {1}\n" . format ( i , get_node_repr ( value ) ) result = result . strip ( "\n" ) lines = result . split ( "\n" ) if maxlen and len ( lines ) > maxlen : i = int ( maxlen / 2 ) lines = lines [ : i ] + [ "..." ] + lines [ - ( maxlen - i ) : ] result = "\n" . join ( lines ) return result
7123	def write_config ( config , app_dir , filename = 'configuration.json' ) : path = os . path . join ( app_dir , filename ) with open ( path , 'w' ) as f : json . dump ( config , f , indent = 4 , cls = DetectMissingEncoder , separators = ( ',' , ': ' ) )
8393	def parse_pylint_output ( pylint_output ) : for line in pylint_output : if not line . strip ( ) : continue if line [ 0 : 5 ] in ( "-" * 5 , "*" * 5 ) : continue parsed = PYLINT_PARSEABLE_REGEX . search ( line ) if parsed is None : LOG . warning ( u"Unable to parse %r. If this is a lint failure, please re-run pylint with the " u"--output-format=parseable option, otherwise, you can ignore this message." , line ) continue parsed_dict = parsed . groupdict ( ) parsed_dict [ 'linenum' ] = int ( parsed_dict [ 'linenum' ] ) yield PylintError ( ** parsed_dict )
11658	def transform ( self , X ) : X = check_array ( X , copy = self . copy ) X *= self . scale_ X += self . min_ if self . truncate : np . maximum ( self . feature_range [ 0 ] , X , out = X ) np . minimum ( self . feature_range [ 1 ] , X , out = X ) return X
6108	def yticks ( self ) : return np . linspace ( np . amin ( self . grid_stack . regular [ : , 0 ] ) , np . amax ( self . grid_stack . regular [ : , 0 ] ) , 4 )
4451	def search ( self , query ) : args , query = self . _mk_query_args ( query ) st = time . time ( ) res = self . redis . execute_command ( self . SEARCH_CMD , * args ) return Result ( res , not query . _no_content , duration = ( time . time ( ) - st ) * 1000.0 , has_payload = query . _with_payloads )
686	def getEncoding ( self , n ) : assert ( all ( field . numEncodings > n for field in self . fields ) ) encoding = np . concatenate ( [ field . encodings [ n ] for field in self . fields ] ) return encoding
11545	def set_pwm_frequency ( self , frequency , pin = None ) : if pin is None : self . _set_pwm_frequency ( frequency , None ) else : pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : self . _set_pwm_frequency ( frequency , pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
7957	def _continue_tls_handshake ( self ) : try : logger . debug ( " do_handshake()" ) self . _socket . do_handshake ( ) except ssl . SSLError , err : if err . args [ 0 ] == ssl . SSL_ERROR_WANT_READ : self . _tls_state = "want_read" logger . debug ( " want_read" ) self . _state_cond . notify ( ) return elif err . args [ 0 ] == ssl . SSL_ERROR_WANT_WRITE : self . _tls_state = "want_write" logger . debug ( " want_write" ) self . _write_queue . appendleft ( TLSHandshake ) return else : raise self . _tls_state = "connected" self . _set_state ( "connected" ) self . _auth_properties [ 'security-layer' ] = "TLS" if "tls-unique" in CHANNEL_BINDING_TYPES : try : tls_unique = self . _socket . get_channel_binding ( "tls-unique" ) except ValueError : pass else : self . _auth_properties [ 'channel-binding' ] = { "tls-unique" : tls_unique } try : cipher = self . _socket . cipher ( ) except AttributeError : cipher = "unknown" cert = get_certificate_from_ssl_socket ( self . _socket ) self . event ( TLSConnectedEvent ( cipher , cert ) )
9012	def _start ( self ) : self . _instruction_library = self . _spec . new_default_instructions ( ) self . _as_instruction = self . _instruction_library . as_instruction self . _id_cache = { } self . _pattern_set = None self . _inheritance_todos = [ ] self . _instruction_todos = [ ]
5187	def inventory ( self , ** kwargs ) : inventory = self . _query ( 'inventory' , ** kwargs ) for inv in inventory : yield Inventory ( node = inv [ 'certname' ] , time = inv [ 'timestamp' ] , environment = inv [ 'environment' ] , facts = inv [ 'facts' ] , trusted = inv [ 'trusted' ] )
11212	def _hash ( secret : bytes , data : bytes , alg : str ) -> bytes : algorithm = get_algorithm ( alg ) return hmac . new ( secret , msg = data , digestmod = algorithm ) . digest ( )
11354	def record_add_field ( rec , tag , ind1 = '' , ind2 = '' , subfields = [ ] , controlfield_value = '' ) : if controlfield_value : doc = etree . Element ( "controlfield" , attrib = { "tag" : tag , } ) doc . text = unicode ( controlfield_value ) else : doc = etree . Element ( "datafield" , attrib = { "tag" : tag , "ind1" : ind1 , "ind2" : ind2 , } ) for code , value in subfields : field = etree . SubElement ( doc , "subfield" , attrib = { "code" : code } ) field . text = value rec . append ( doc ) return rec
10185	def _events_config ( self ) : result = { } for ep in iter_entry_points ( group = self . entry_point_group_events ) : for cfg in ep . load ( ) ( ) : if cfg [ 'event_type' ] not in self . enabled_events : continue elif cfg [ 'event_type' ] in result : raise DuplicateEventError ( 'Duplicate event {0} in entry point ' '{1}' . format ( cfg [ 'event_type' ] , ep . name ) ) cfg . update ( self . enabled_events [ cfg [ 'event_type' ] ] or { } ) result [ cfg [ 'event_type' ] ] = cfg return result
12995	def round_arr_teff_luminosity ( arr ) : arr [ 'temp' ] = np . around ( arr [ 'temp' ] , - 1 ) arr [ 'lum' ] = np . around ( arr [ 'lum' ] , 3 ) return arr
4716	def tcase_setup ( trun , parent , tcase_fname ) : case = copy . deepcopy ( TESTCASE ) case [ "fname" ] = tcase_fname case [ "fpath_orig" ] = os . sep . join ( [ trun [ "conf" ] [ "TESTCASES" ] , case [ "fname" ] ] ) if not os . path . exists ( case [ "fpath_orig" ] ) : cij . err ( 'rnr:tcase_setup: !case["fpath_orig"]: %r' % case [ "fpath_orig" ] ) return None case [ "name" ] = os . path . splitext ( case [ "fname" ] ) [ 0 ] case [ "ident" ] = "/" . join ( [ parent [ "ident" ] , case [ "fname" ] ] ) case [ "res_root" ] = os . sep . join ( [ parent [ "res_root" ] , case [ "fname" ] ] ) case [ "aux_root" ] = os . sep . join ( [ case [ "res_root" ] , "_aux" ] ) case [ "log_fpath" ] = os . sep . join ( [ case [ "res_root" ] , "run.log" ] ) case [ "fpath" ] = os . sep . join ( [ case [ "res_root" ] , case [ "fname" ] ] ) case [ "evars" ] . update ( copy . deepcopy ( parent [ "evars" ] ) ) os . makedirs ( case [ "res_root" ] ) os . makedirs ( case [ "aux_root" ] ) shutil . copyfile ( case [ "fpath_orig" ] , case [ "fpath" ] ) case [ "hooks" ] = hooks_setup ( trun , case , parent . get ( "hooks_pr_tcase" ) ) return case
11878	def getProcessOwner ( pid ) : try : ownerUid = os . stat ( '/proc/' + str ( pid ) ) . st_uid except : return None try : ownerName = pwd . getpwuid ( ownerUid ) . pw_name except : ownerName = None return { 'uid' : ownerUid , 'name' : ownerName }
2096	def status ( self , pk = None , detail = False , ** kwargs ) : self . _pop_none ( kwargs ) if not pk : job = self . get ( include_debug_header = True , ** kwargs ) else : debug . log ( 'Asking for job status.' , header = 'details' ) finished_endpoint = '%s%s/' % ( self . endpoint , pk ) job = client . get ( finished_endpoint ) . json ( ) if detail : return job return { 'elapsed' : job [ 'elapsed' ] , 'failed' : job [ 'failed' ] , 'status' : job [ 'status' ] , }
2944	def accept_message ( self , message ) : assert not self . read_only self . refresh_waiting_tasks ( ) self . do_engine_steps ( ) for my_task in Task . Iterator ( self . task_tree , Task . WAITING ) : my_task . task_spec . accept_message ( my_task , message )
8690	def put ( self , key ) : self . _consul_request ( 'PUT' , self . _key_url ( key [ 'name' ] ) , json = key ) return key [ 'name' ]
7210	def stdout ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get stdout.' ) if self . batch_values : raise NotImplementedError ( "Query Each Workflow Id within the Batch Workflow for stdout." ) wf = self . workflow . get ( self . id ) stdout_list = [ ] for task in wf [ 'tasks' ] : stdout_list . append ( { 'id' : task [ 'id' ] , 'taskType' : task [ 'taskType' ] , 'name' : task [ 'name' ] , 'stdout' : self . workflow . get_stdout ( self . id , task [ 'id' ] ) } ) return stdout_list
490	def acquireConnection ( self ) : self . _logger . debug ( "Acquiring connection" ) self . _conn . _ping_check ( ) connWrap = ConnectionWrapper ( dbConn = self . _conn , cursor = self . _conn . cursor ( ) , releaser = self . _releaseConnection , logger = self . _logger ) return connWrap
6191	def volume ( self ) : return ( self . x2 - self . x1 ) * ( self . y2 - self . y1 ) * ( self . z2 - self . z1 )
1198	def tf_loss ( self , states , internals , reward , update , reference = None ) : prediction = self . predict ( states = states , internals = internals , update = update ) return tf . nn . l2_loss ( t = ( prediction - reward ) )
8847	def mouseMoveEvent ( self , e ) : super ( PyInteractiveConsole , self ) . mouseMoveEvent ( e ) cursor = self . cursorForPosition ( e . pos ( ) ) assert isinstance ( cursor , QtGui . QTextCursor ) p = cursor . positionInBlock ( ) usd = cursor . block ( ) . userData ( ) if usd and usd . start_pos_in_block <= p <= usd . end_pos_in_block : if QtWidgets . QApplication . overrideCursor ( ) is None : QtWidgets . QApplication . setOverrideCursor ( QtGui . QCursor ( QtCore . Qt . PointingHandCursor ) ) else : if QtWidgets . QApplication . overrideCursor ( ) is not None : QtWidgets . QApplication . restoreOverrideCursor ( )
6237	def draw_buffers ( self , near , far ) : self . ctx . disable ( moderngl . DEPTH_TEST ) helper . draw ( self . gbuffer . color_attachments [ 0 ] , pos = ( 0.0 , 0.0 ) , scale = ( 0.25 , 0.25 ) ) helper . draw ( self . gbuffer . color_attachments [ 1 ] , pos = ( 0.5 , 0.0 ) , scale = ( 0.25 , 0.25 ) ) helper . draw_depth ( self . gbuffer . depth_attachment , near , far , pos = ( 1.0 , 0.0 ) , scale = ( 0.25 , 0.25 ) ) helper . draw ( self . lightbuffer . color_attachments [ 0 ] , pos = ( 1.5 , 0.0 ) , scale = ( 0.25 , 0.25 ) )
377	def pixel_value_scale ( im , val = 0.9 , clip = None , is_random = False ) : clip = clip if clip is not None else ( - np . inf , np . inf ) if is_random : scale = 1 + np . random . uniform ( - val , val ) im = im * scale else : im = im * val if len ( clip ) == 2 : im = np . clip ( im , clip [ 0 ] , clip [ 1 ] ) else : raise Exception ( "clip : tuple of 2 numbers" ) return im
13609	def contact ( request ) : form = ContactForm ( request . POST or None ) if form . is_valid ( ) : subject = form . cleaned_data [ 'subject' ] message = form . cleaned_data [ 'message' ] sender = form . cleaned_data [ 'sender' ] cc_myself = form . cleaned_data [ 'cc_myself' ] recipients = settings . CONTACTFORM_RECIPIENTS if cc_myself : recipients . append ( sender ) send_mail ( getattr ( settings , "CONTACTFORM_SUBJECT_PREFIX" , '' ) + subject , message , sender , recipients ) return render ( request , 'contactform/thanks.html' ) return render ( request , 'contactform/contact.html' , { 'form' : form } )
5291	def get ( self , request , * args , ** kwargs ) : form_class = self . get_form_class ( ) form = self . get_form ( form_class ) inlines = self . construct_inlines ( ) return self . render_to_response ( self . get_context_data ( form = form , inlines = inlines , ** kwargs ) )
9267	def get_time_of_tag ( self , tag ) : if not tag : raise ChangelogGeneratorError ( "tag is nil" ) name_of_tag = tag [ "name" ] time_for_name = self . tag_times_dict . get ( name_of_tag , None ) if time_for_name : return time_for_name else : time_string = self . fetcher . fetch_date_of_tag ( tag ) try : self . tag_times_dict [ name_of_tag ] = timestring_to_datetime ( time_string ) except UnicodeWarning : print ( "ERROR ERROR:" , tag ) self . tag_times_dict [ name_of_tag ] = timestring_to_datetime ( time_string ) return self . tag_times_dict [ name_of_tag ]
7369	def doc ( func ) : stripped_chars = " \t" if hasattr ( func , '__doc__' ) : docstring = func . __doc__ . lstrip ( " \n\t" ) if "\n" in docstring : i = docstring . index ( "\n" ) return docstring [ : i ] . rstrip ( stripped_chars ) elif docstring : return docstring . rstrip ( stripped_chars ) return ""
8326	def setup ( self , parent = None , previous = None ) : self . parent = parent self . previous = previous self . next = None self . previousSibling = None self . nextSibling = None if self . parent and self . parent . contents : self . previousSibling = self . parent . contents [ - 1 ] self . previousSibling . nextSibling = self
3909	async def consume ( self ) : while True : coro = await self . _queue . get ( ) assert asyncio . iscoroutine ( coro ) await coro
4832	def course_discovery_api_client ( user , catalog_url ) : if JwtBuilder is None : raise NotConnectedToOpenEdX ( _ ( "To get a Catalog API client, this package must be " "installed in an Open edX environment." ) ) jwt = JwtBuilder . create_jwt_for_user ( user ) return EdxRestApiClient ( catalog_url , jwt = jwt )
10934	def check_terminate ( self ) : if not self . _has_run : return False else : terminate = self . check_completion ( ) terminate |= ( self . _num_iter >= self . max_iter ) return terminate
2404	def gen_prompt_feats ( self , e_set ) : prompt_toks = nltk . word_tokenize ( e_set . _prompt ) expand_syns = [ ] for word in prompt_toks : synonyms = util_functions . get_wordnet_syns ( word ) expand_syns . append ( synonyms ) expand_syns = list ( chain . from_iterable ( expand_syns ) ) prompt_overlap = [ ] prompt_overlap_prop = [ ] for j in e_set . _tokens : tok_length = len ( j ) if ( tok_length == 0 ) : tok_length = 1 prompt_overlap . append ( len ( [ i for i in j if i in prompt_toks ] ) ) prompt_overlap_prop . append ( prompt_overlap [ len ( prompt_overlap ) - 1 ] / float ( tok_length ) ) expand_overlap = [ ] expand_overlap_prop = [ ] for j in e_set . _tokens : tok_length = len ( j ) if ( tok_length == 0 ) : tok_length = 1 expand_overlap . append ( len ( [ i for i in j if i in expand_syns ] ) ) expand_overlap_prop . append ( expand_overlap [ len ( expand_overlap ) - 1 ] / float ( tok_length ) ) prompt_arr = numpy . array ( ( prompt_overlap , prompt_overlap_prop , expand_overlap , expand_overlap_prop ) ) . transpose ( ) return prompt_arr . copy ( )
5606	def extract_from_array ( in_raster = None , in_affine = None , out_tile = None ) : if isinstance ( in_raster , ReferencedRaster ) : in_affine = in_raster . affine in_raster = in_raster . data minrow , maxrow , mincol , maxcol = bounds_to_ranges ( out_bounds = out_tile . bounds , in_affine = in_affine , in_shape = in_raster . shape ) if ( minrow >= 0 and mincol >= 0 and maxrow <= in_raster . shape [ - 2 ] and maxcol <= in_raster . shape [ - 1 ] ) : return in_raster [ ... , minrow : maxrow , mincol : maxcol ] else : raise ValueError ( "extraction fails if output shape is not within input" )
2565	def async_process ( fn ) : def run ( * args , ** kwargs ) : proc = mp . Process ( target = fn , args = args , kwargs = kwargs ) proc . start ( ) return proc return run
7810	def from_der_data ( cls , data ) : logger . debug ( "Decoding DER certificate: {0!r}" . format ( data ) ) if cls . _cert_asn1_type is None : cls . _cert_asn1_type = Certificate ( ) cert = der_decoder . decode ( data , asn1Spec = cls . _cert_asn1_type ) [ 0 ] result = cls ( ) tbs_cert = cert . getComponentByName ( 'tbsCertificate' ) subject = tbs_cert . getComponentByName ( 'subject' ) logger . debug ( "Subject: {0!r}" . format ( subject ) ) result . _decode_subject ( subject ) validity = tbs_cert . getComponentByName ( 'validity' ) result . _decode_validity ( validity ) extensions = tbs_cert . getComponentByName ( 'extensions' ) if extensions : for extension in extensions : logger . debug ( "Extension: {0!r}" . format ( extension ) ) oid = extension . getComponentByName ( 'extnID' ) logger . debug ( "OID: {0!r}" . format ( oid ) ) if oid != SUBJECT_ALT_NAME_OID : continue value = extension . getComponentByName ( 'extnValue' ) logger . debug ( "Value: {0!r}" . format ( value ) ) if isinstance ( value , Any ) : value = der_decoder . decode ( value , asn1Spec = OctetString ( ) ) [ 0 ] alt_names = der_decoder . decode ( value , asn1Spec = GeneralNames ( ) ) [ 0 ] logger . debug ( "SubjectAltName: {0!r}" . format ( alt_names ) ) result . _decode_alt_names ( alt_names ) return result
3552	def _state_changed ( self , state ) : logger . debug ( 'Adapter state change: {0}' . format ( state ) ) if state == 5 : self . _powered_off . clear ( ) self . _powered_on . set ( ) elif state == 4 : self . _powered_on . clear ( ) self . _powered_off . set ( )
3287	def _get_log ( self , limit = None ) : self . ui . pushbuffer ( ) commands . log ( self . ui , self . repo , limit = limit , date = None , rev = None , user = None ) res = self . ui . popbuffer ( ) . strip ( ) logList = [ ] for logentry in res . split ( "\n\n" ) : log = { } logList . append ( log ) for line in logentry . split ( "\n" ) : k , v = line . split ( ":" , 1 ) assert k in ( "changeset" , "tag" , "user" , "date" , "summary" ) log [ k . strip ( ) ] = v . strip ( ) log [ "parsed_date" ] = util . parse_time_string ( log [ "date" ] ) local_id , unid = log [ "changeset" ] . split ( ":" ) log [ "local_id" ] = int ( local_id ) log [ "unid" ] = unid return logList
7317	def sendmail ( self , msg_from , msg_to , msg ) : SMTP_dummy . msg_from = msg_from SMTP_dummy . msg_to = msg_to SMTP_dummy . msg = msg
13762	def _check_next ( self ) : if self . is_initial : return True if self . before : if self . before_cursor : return True else : return False else : if self . after_cursor : return True else : return False
12176	def plot_shaded_data ( X , Y , variances , varianceX ) : plt . plot ( X , Y , color = 'k' , lw = 2 ) nChunks = int ( len ( Y ) / CHUNK_POINTS ) for i in range ( 0 , 100 , PERCENT_STEP ) : varLimitLow = np . percentile ( variances , i ) varLimitHigh = np . percentile ( variances , i + PERCENT_STEP ) varianceIsAboveMin = np . where ( variances >= varLimitLow ) [ 0 ] varianceIsBelowMax = np . where ( variances <= varLimitHigh ) [ 0 ] varianceIsRange = [ chunkNumber for chunkNumber in range ( nChunks ) if chunkNumber in varianceIsAboveMin and chunkNumber in varianceIsBelowMax ] for chunkNumber in varianceIsRange : t1 = chunkNumber * CHUNK_POINTS / POINTS_PER_SEC t2 = t1 + CHUNK_POINTS / POINTS_PER_SEC plt . axvspan ( t1 , t2 , alpha = .3 , color = COLORMAP ( i / 100 ) , lw = 0 )
9533	def unsign ( self , signed_value , ttl = None ) : h_size , d_size = struct . calcsize ( '>cQ' ) , self . digest . digest_size fmt = '>cQ%ds%ds' % ( len ( signed_value ) - h_size - d_size , d_size ) try : version , timestamp , value , sig = struct . unpack ( fmt , signed_value ) except struct . error : raise BadSignature ( 'Signature is not valid' ) if version != self . version : raise BadSignature ( 'Signature version not supported' ) if ttl is not None : if isinstance ( ttl , datetime . timedelta ) : ttl = ttl . total_seconds ( ) age = abs ( time . time ( ) - timestamp ) if age > ttl + _MAX_CLOCK_SKEW : raise SignatureExpired ( 'Signature age %s > %s seconds' % ( age , ttl ) ) try : self . signature ( signed_value [ : - d_size ] ) . verify ( sig ) except InvalidSignature : raise BadSignature ( 'Signature "%s" does not match' % binascii . b2a_base64 ( sig ) ) return value
7034	def import_apikey ( lcc_server , apikey_text_json ) : USERHOME = os . path . expanduser ( '~' ) APIKEYFILE = os . path . join ( USERHOME , '.astrobase' , 'lccs' , 'apikey-%s' % lcc_server . replace ( 'https://' , 'https-' ) . replace ( 'http://' , 'http-' ) ) respdict = json . loads ( apikey_text_json ) apikey = respdict [ 'apikey' ] expires = respdict [ 'expires' ] if not os . path . exists ( os . path . dirname ( APIKEYFILE ) ) : os . makedirs ( os . path . dirname ( APIKEYFILE ) ) with open ( APIKEYFILE , 'w' ) as outfd : outfd . write ( '%s %s\n' % ( apikey , expires ) ) os . chmod ( APIKEYFILE , 0o100600 ) LOGINFO ( 'key fetched successfully from: %s. expires on: %s' % ( lcc_server , expires ) ) LOGINFO ( 'written to: %s' % APIKEYFILE ) return apikey , expires
12163	def _check_limit ( self , event ) : if self . count ( event ) > self . max_listeners : warnings . warn ( 'Too many listeners for event {}' . format ( event ) , ResourceWarning , )
2396	def confusion_matrix ( rater_a , rater_b , min_rating = None , max_rating = None ) : assert ( len ( rater_a ) == len ( rater_b ) ) rater_a = [ int ( a ) for a in rater_a ] rater_b = [ int ( b ) for b in rater_b ] min_rating = int ( min_rating ) max_rating = int ( max_rating ) if min_rating is None : min_rating = min ( rater_a ) if max_rating is None : max_rating = max ( rater_a ) num_ratings = int ( max_rating - min_rating + 1 ) conf_mat = [ [ 0 for i in range ( num_ratings ) ] for j in range ( num_ratings ) ] for a , b in zip ( rater_a , rater_b ) : conf_mat [ int ( a - min_rating ) ] [ int ( b - min_rating ) ] += 1 return conf_mat
8947	def run_elective ( self , cmd , * args , ** kwargs ) : if self . _commit : return self . run ( cmd , * args , ** kwargs ) else : notify . warning ( "WOULD RUN: {}" . format ( cmd ) ) kwargs = kwargs . copy ( ) kwargs [ 'echo' ] = False return self . run ( 'true' , * args , ** kwargs )
5236	def file_modified_time ( file_name ) -> pd . Timestamp : return pd . to_datetime ( time . ctime ( os . path . getmtime ( filename = file_name ) ) )
2234	def _proc_async_iter_stream ( proc , stream , buffersize = 1 ) : from six . moves import queue from threading import Thread def enqueue_output ( proc , stream , stream_queue ) : while proc . poll ( ) is None : line = stream . readline ( ) stream_queue . put ( line ) for line in _textio_iterlines ( stream ) : stream_queue . put ( line ) stream_queue . put ( None ) stream_queue = queue . Queue ( maxsize = buffersize ) _thread = Thread ( target = enqueue_output , args = ( proc , stream , stream_queue ) ) _thread . daemon = True _thread . start ( ) return stream_queue
8093	def node_label ( s , node , alpha = 1.0 ) : if s . text : s . _ctx . font ( s . font ) s . _ctx . fontsize ( s . fontsize ) s . _ctx . nostroke ( ) s . _ctx . fill ( s . text . r , s . text . g , s . text . b , s . text . a * alpha ) try : p = node . _textpath except : txt = node . label try : txt = unicode ( txt ) except : try : txt = txt . decode ( "utf-8" ) except : pass dx , dy = 0 , 0 if s . align == 2 : dx = - s . _ctx . textwidth ( txt , s . textwidth ) / 2 dy = s . _ctx . textheight ( txt ) / 2 node . _textpath = s . _ctx . textpath ( txt , dx , dy , width = s . textwidth ) p = node . _textpath if s . depth : try : __colors . shadow ( dx = 2 , dy = 4 , blur = 5 , alpha = 0.3 * alpha ) except : pass s . _ctx . push ( ) s . _ctx . translate ( node . x , node . y ) s . _ctx . scale ( alpha ) s . _ctx . drawpath ( p . copy ( ) ) s . _ctx . pop ( )
8913	def save_service ( self , service , overwrite = True ) : name = namesgenerator . get_sane_name ( service . name ) if not name : name = namesgenerator . get_random_name ( ) if name in self . name_index : name = namesgenerator . get_random_name ( retry = True ) if name in self . name_index : if overwrite : self . _delete ( name = name ) else : raise Exception ( "service name already registered." ) self . _insert ( Service ( name = name , url = baseurl ( service . url ) , type = service . type , purl = service . purl , public = service . public , auth = service . auth , verify = service . verify ) ) return self . fetch_by_name ( name = name )
1138	def _splitext ( p , sep , altsep , extsep ) : sepIndex = p . rfind ( sep ) if altsep : altsepIndex = p . rfind ( altsep ) sepIndex = max ( sepIndex , altsepIndex ) dotIndex = p . rfind ( extsep ) if dotIndex > sepIndex : filenameIndex = sepIndex + 1 while filenameIndex < dotIndex : if p [ filenameIndex ] != extsep : return p [ : dotIndex ] , p [ dotIndex : ] filenameIndex += 1 return p , ''
11785	def attrnum ( self , attr ) : "Returns the number used for attr, which can be a name, or -n .. n-1." if attr < 0 : return len ( self . attrs ) + attr elif isinstance ( attr , str ) : return self . attrnames . index ( attr ) else : return attr
2458	def set_pkg_license_comment ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_license_comment_set : self . package_license_comment_set = True if validations . validate_pkg_lics_comment ( text ) : doc . package . license_comment = str_from_text ( text ) return True else : raise SPDXValueError ( 'Package::LicenseComment' ) else : raise CardinalityError ( 'Package::LicenseComment' )
8570	def get_location ( self , location_id , depth = 0 ) : response = self . _perform_request ( '/locations/%s?depth=%s' % ( location_id , depth ) ) return response
10214	def summarize_subgraph_node_overlap ( graph : BELGraph , node_predicates = None , annotation : str = 'Subgraph' ) : r1 = group_nodes_by_annotation_filtered ( graph , node_predicates = node_predicates , annotation = annotation ) return calculate_tanimoto_set_distances ( r1 )
1817	def SETNS ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . SF == False , 1 , 0 ) )
1764	def push_int ( self , value , force = False ) : self . STACK -= self . address_bit_size // 8 self . write_int ( self . STACK , value , force = force ) return self . STACK
1737	def parse_exponent ( source , start ) : if not source [ start ] in { 'e' , 'E' } : if source [ start ] in IDENTIFIER_PART : raise SyntaxError ( 'Invalid number literal!' ) return start start += 1 if source [ start ] in { '-' , '+' } : start += 1 FOUND = False while source [ start ] in NUMS : FOUND = True start += 1 if not FOUND or source [ start ] in IDENTIFIER_PART : raise SyntaxError ( 'Invalid number literal!' ) return start
6115	def fit_lens_data_with_sensitivity_tracers ( lens_data , tracer_normal , tracer_sensitive ) : if ( tracer_normal . has_light_profile and tracer_sensitive . has_light_profile ) and ( not tracer_normal . has_pixelization and not tracer_sensitive . has_pixelization ) : return SensitivityProfileFit ( lens_data = lens_data , tracer_normal = tracer_normal , tracer_sensitive = tracer_sensitive ) elif ( not tracer_normal . has_light_profile and not tracer_sensitive . has_light_profile ) and ( tracer_normal . has_pixelization and tracer_sensitive . has_pixelization ) : return SensitivityInversionFit ( lens_data = lens_data , tracer_normal = tracer_normal , tracer_sensitive = tracer_sensitive ) else : raise exc . FittingException ( 'The sensitivity_fit routine did not call a SensitivityFit class - check the ' 'properties of the tracers' )
6105	def masses_of_galaxies_within_circles_in_units ( self , radius : dim . Length , unit_mass = 'angular' , critical_surface_density = None ) : return list ( map ( lambda galaxy : galaxy . mass_within_circle_in_units ( radius = radius , unit_mass = unit_mass , kpc_per_arcsec = self . kpc_per_arcsec , critical_surface_density = critical_surface_density ) , self . galaxies ) )
13824	def ToJsonString ( self ) : if self . seconds < 0 or self . nanos < 0 : result = '-' seconds = - self . seconds + int ( ( 0 - self . nanos ) // 1e9 ) nanos = ( 0 - self . nanos ) % 1e9 else : result = '' seconds = self . seconds + int ( self . nanos // 1e9 ) nanos = self . nanos % 1e9 result += '%d' % seconds if ( nanos % 1e9 ) == 0 : return result + 's' if ( nanos % 1e6 ) == 0 : return result + '.%03ds' % ( nanos / 1e6 ) if ( nanos % 1e3 ) == 0 : return result + '.%06ds' % ( nanos / 1e3 ) return result + '.%09ds' % nanos
13197	def open511_convert ( input_doc , output_format , serialize = True , ** kwargs ) : try : output_format_info = FORMATS [ output_format ] except KeyError : raise ValueError ( "Unrecognized output format %s" % output_format ) input_doc = ensure_format ( input_doc , output_format_info . input_format ) result = output_format_info . func ( input_doc , ** kwargs ) if serialize : result = output_format_info . serializer ( result ) return result
6770	def install_yum ( self , fn = None , package_name = None , update = 0 , list_only = 0 ) : assert self . genv [ ROLE ] yum_req_fn = fn or self . find_template ( self . genv . yum_requirments_fn ) if not yum_req_fn : return [ ] assert os . path . isfile ( yum_req_fn ) update = int ( update ) if list_only : return [ _ . strip ( ) for _ in open ( yum_req_fn ) . readlines ( ) if _ . strip ( ) and not _ . strip . startswith ( '#' ) and ( not package_name or _ . strip ( ) == package_name ) ] if update : self . sudo_or_dryrun ( 'yum update --assumeyes' ) if package_name : self . sudo_or_dryrun ( 'yum install --assumeyes %s' % package_name ) else : if self . genv . is_local : self . put_or_dryrun ( local_path = yum_req_fn ) yum_req_fn = self . genv . put_remote_fn self . sudo_or_dryrun ( 'yum install --assumeyes $(cat %(yum_req_fn)s)' % yum_req_fn )
13067	def make_parents ( self , collection , lang = None ) : return [ { "id" : member . id , "label" : str ( member . get_label ( lang ) ) , "model" : str ( member . model ) , "type" : str ( member . type ) , "size" : member . size } for member in collection . parents if member . get_label ( ) ]
12383	def create_project ( self ) : if os . path . exists ( self . _py ) : prj_dir = os . path . join ( self . _app_dir , self . _project_name ) if os . path . exists ( prj_dir ) : if self . _force : logging . warn ( 'Removing existing project' ) shutil . rmtree ( prj_dir ) else : logging . warn ( 'Found existing project; not creating (use --force to overwrite)' ) return logging . info ( 'Creating project' ) p = subprocess . Popen ( 'cd {0} ; {1} startproject {2} > /dev/null' . format ( self . _app_dir , self . _ve_dir + os . sep + self . _project_name + os . sep + 'bin' + os . sep + 'django-admin.py' , self . _project_name ) , shell = True ) os . waitpid ( p . pid , 0 ) else : logging . error ( 'Unable to find Python interpreter in virtualenv' ) return
2726	def create ( self , * args , ** kwargs ) : for attr in kwargs . keys ( ) : setattr ( self , attr , kwargs [ attr ] ) if not self . size_slug and self . size : self . size_slug = self . size ssh_keys_id = Droplet . __get_ssh_keys_id_or_fingerprint ( self . ssh_keys , self . token , self . name ) data = { "name" : self . name , "size" : self . size_slug , "image" : self . image , "region" : self . region , "ssh_keys" : ssh_keys_id , "backups" : bool ( self . backups ) , "ipv6" : bool ( self . ipv6 ) , "private_networking" : bool ( self . private_networking ) , "volumes" : self . volumes , "tags" : self . tags , "monitoring" : bool ( self . monitoring ) , } if self . user_data : data [ "user_data" ] = self . user_data data = self . get_data ( "droplets/" , type = POST , params = data ) if data : self . id = data [ 'droplet' ] [ 'id' ] action_id = data [ 'links' ] [ 'actions' ] [ 0 ] [ 'id' ] self . action_ids = [ ] self . action_ids . append ( action_id )
1877	def MOVSD ( cpu , dest , src ) : assert dest . type != 'memory' or src . type != 'memory' value = Operators . EXTRACT ( src . read ( ) , 0 , 64 ) if dest . size > src . size : value = Operators . ZEXTEND ( value , dest . size ) dest . write ( value )
2286	def graph_evaluation ( data , adj_matrix , gpu = None , gpu_id = 0 , ** kwargs ) : gpu = SETTINGS . get_default ( gpu = gpu ) device = 'cuda:{}' . format ( gpu_id ) if gpu else 'cpu' obs = th . FloatTensor ( data ) . to ( device ) cgnn = CGNN_model ( adj_matrix , data . shape [ 0 ] , gpu_id = gpu_id , ** kwargs ) . to ( device ) cgnn . reset_parameters ( ) return cgnn . run ( obs , ** kwargs )
7160	def next_question ( self ) : for key , questions in self . questions . items ( ) : if key in self . answers : continue for question in questions : if self . check_condition ( question . _condition ) : return question return None
4589	def serpentine_x ( x , y , matrix ) : if y % 2 : return matrix . columns - 1 - x , y return x , y
4625	def change_password ( self , newpassword ) : if not self . unlocked ( ) : raise WalletLocked self . password = newpassword self . _save_encrypted_masterpassword ( )
3065	def _add_query_parameter ( url , name , value ) : if value is None : return url else : return update_query_params ( url , { name : value } )
224	async def receive ( self ) -> Message : if self . client_state == WebSocketState . CONNECTING : message = await self . _receive ( ) message_type = message [ "type" ] assert message_type == "websocket.connect" self . client_state = WebSocketState . CONNECTED return message elif self . client_state == WebSocketState . CONNECTED : message = await self . _receive ( ) message_type = message [ "type" ] assert message_type in { "websocket.receive" , "websocket.disconnect" } if message_type == "websocket.disconnect" : self . client_state = WebSocketState . DISCONNECTED return message else : raise RuntimeError ( 'Cannot call "receive" once a disconnect message has been received.' )
10565	def exclude_filepaths ( filepaths , exclude_patterns = None ) : if not exclude_patterns : return filepaths , [ ] exclude_re = re . compile ( "|" . join ( pattern for pattern in exclude_patterns ) ) included_songs = [ ] excluded_songs = [ ] for filepath in filepaths : if exclude_patterns and exclude_re . search ( filepath ) : excluded_songs . append ( filepath ) else : included_songs . append ( filepath ) return included_songs , excluded_songs
5465	def _get_action_by_name ( op , name ) : actions = get_actions ( op ) for action in actions : if action . get ( 'name' ) == name : return action
5132	def generate_transition_matrix ( g , seed = None ) : g = _test_graph ( g ) if isinstance ( seed , numbers . Integral ) : np . random . seed ( seed ) nV = g . number_of_nodes ( ) mat = np . zeros ( ( nV , nV ) ) for v in g . nodes ( ) : ind = [ e [ 1 ] for e in sorted ( g . out_edges ( v ) ) ] deg = len ( ind ) if deg == 1 : mat [ v , ind ] = 1 elif deg > 1 : probs = np . ceil ( np . random . rand ( deg ) * 100 ) / 100. if np . isclose ( np . sum ( probs ) , 0 ) : probs [ np . random . randint ( deg ) ] = 1 mat [ v , ind ] = probs / np . sum ( probs ) return mat
1092	def sub ( pattern , repl , string , count = 0 , flags = 0 ) : return _compile ( pattern , flags ) . sub ( repl , string , count )
1621	def FindNextMultiLineCommentEnd ( lines , lineix ) : while lineix < len ( lines ) : if lines [ lineix ] . strip ( ) . endswith ( '*/' ) : return lineix lineix += 1 return len ( lines )
8917	def _get_param ( self , param , allowed_values = None , optional = False ) : request_params = self . _request_params ( ) if param in request_params : value = request_params [ param ] . lower ( ) if allowed_values is not None : if value in allowed_values : self . params [ param ] = value else : raise OWSInvalidParameterValue ( "%s %s is not supported" % ( param , value ) , value = param ) elif optional : self . params [ param ] = None else : raise OWSMissingParameterValue ( 'Parameter "%s" is missing' % param , value = param ) return self . params [ param ]
11612	def report_read_counts ( self , filename , grp_wise = False , reorder = 'as-is' , notes = None ) : expected_read_counts = self . probability . sum ( axis = APM . Axis . READ ) if grp_wise : lname = self . probability . gname expected_read_counts = expected_read_counts * self . grp_conv_mat else : lname = self . probability . lname total_read_counts = expected_read_counts . sum ( axis = 0 ) if reorder == 'decreasing' : report_order = np . argsort ( total_read_counts . flatten ( ) ) report_order = report_order [ : : - 1 ] elif reorder == 'increasing' : report_order = np . argsort ( total_read_counts . flatten ( ) ) elif reorder == 'as-is' : report_order = np . arange ( len ( lname ) ) cntdata = np . vstack ( ( expected_read_counts , total_read_counts ) ) fhout = open ( filename , 'w' ) fhout . write ( "locus\t" + "\t" . join ( self . probability . hname ) + "\ttotal" ) if notes is not None : fhout . write ( "\tnotes" ) fhout . write ( "\n" ) for locus_id in report_order : lname_cur = lname [ locus_id ] fhout . write ( "\t" . join ( [ lname_cur ] + map ( str , cntdata [ : , locus_id ] . ravel ( ) ) ) ) if notes is not None : fhout . write ( "\t%s" % notes [ lname_cur ] ) fhout . write ( "\n" ) fhout . close ( )
2648	def make_rundir ( path ) : try : if not os . path . exists ( path ) : os . makedirs ( path ) prev_rundirs = glob ( os . path . join ( path , "[0-9]*" ) ) current_rundir = os . path . join ( path , '000' ) if prev_rundirs : x = sorted ( [ int ( os . path . basename ( x ) ) for x in prev_rundirs ] ) [ - 1 ] current_rundir = os . path . join ( path , '{0:03}' . format ( x + 1 ) ) os . makedirs ( current_rundir ) logger . debug ( "Parsl run initializing in rundir: {0}" . format ( current_rundir ) ) return os . path . abspath ( current_rundir ) except Exception as e : logger . error ( "Failed to create a run directory" ) logger . error ( "Error: {0}" . format ( e ) ) raise
9933	def walk ( self , maxresults = 100 , maxdepth = None ) : log . debug ( "step" ) self . seen = { } self . ignore ( self , self . __dict__ , self . obj , self . seen , self . _ignore ) self . ignore_caller ( ) self . maxdepth = maxdepth count = 0 log . debug ( "will iterate results" ) for result in self . _gen ( self . obj ) : log . debug ( "will yeld" ) yield result count += 1 if maxresults and count >= maxresults : yield 0 , 0 , "==== Max results reached ====" return
11695	def verify_words ( self ) : if self . comment : if find_words ( self . comment , self . suspect_words , self . excluded_words ) : self . label_suspicious ( 'suspect_word' ) if self . source : for word in self . illegal_sources : if word in self . source . lower ( ) : self . label_suspicious ( 'suspect_word' ) break if self . imagery_used : for word in self . illegal_sources : if word in self . imagery_used . lower ( ) : self . label_suspicious ( 'suspect_word' ) break self . suspicion_reasons = list ( set ( self . suspicion_reasons ) )
12965	def allOnlyFields ( self , fields , cascadeFetch = False ) : matchedKeys = self . getPrimaryKeys ( ) if matchedKeys : return self . getMultipleOnlyFields ( matchedKeys , fields , cascadeFetch = cascadeFetch ) return IRQueryableList ( [ ] , mdl = self . mdl )
8625	def set_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_put_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise UserJobsNotSetException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
1253	def print_state ( self ) : def tile_string ( value ) : if value > 0 : return '% 5d' % ( 2 ** value , ) return " " separator_line = '-' * 25 print ( separator_line ) for row in range ( 4 ) : print ( "|" + "|" . join ( [ tile_string ( v ) for v in self . _state [ row , : ] ] ) + "|" ) print ( separator_line )
709	def runWithPermutationsScript ( permutationsFilePath , options , outputLabel , permWorkDir ) : global g_currentVerbosityLevel if "verbosityCount" in options : g_currentVerbosityLevel = options [ "verbosityCount" ] del options [ "verbosityCount" ] else : g_currentVerbosityLevel = 1 _setupInterruptHandling ( ) options [ "permutationsScriptPath" ] = permutationsFilePath options [ "outputLabel" ] = outputLabel options [ "outDir" ] = permWorkDir options [ "permWorkDir" ] = permWorkDir runOptions = _injectDefaultOptions ( options ) _validateOptions ( runOptions ) return _runAction ( runOptions )
9558	def _apply_unique_checks ( self , i , r , unique_sets , summarize = False , context = None ) : for key , code , message in self . _unique_checks : value = None values = unique_sets [ key ] if isinstance ( key , basestring ) : fi = self . _field_names . index ( key ) if fi >= len ( r ) : continue value = r [ fi ] else : value = [ ] for f in key : fi = self . _field_names . index ( f ) if fi >= len ( r ) : break value . append ( r [ fi ] ) value = tuple ( value ) if value in values : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'key' ] = key p [ 'value' ] = value if context is not None : p [ 'context' ] = context yield p values . add ( value )
3619	def register ( self , model , index_cls = AlgoliaIndex , auto_indexing = None ) : if self . is_registered ( model ) : raise RegistrationError ( '{} is already registered with Algolia engine' . format ( model ) ) if not issubclass ( index_cls , AlgoliaIndex ) : raise RegistrationError ( '{} should be a subclass of AlgoliaIndex' . format ( index_cls ) ) index_obj = index_cls ( model , self . client , self . __settings ) self . __registered_models [ model ] = index_obj if ( isinstance ( auto_indexing , bool ) and auto_indexing ) or self . __auto_indexing : post_save . connect ( self . __post_save_receiver , model ) pre_delete . connect ( self . __pre_delete_receiver , model ) logger . info ( 'REGISTER %s' , model )
8257	def _average ( self ) : r , g , b , a = 0 , 0 , 0 , 0 for clr in self : r += clr . r g += clr . g b += clr . b a += clr . alpha r /= len ( self ) g /= len ( self ) b /= len ( self ) a /= len ( self ) return color ( r , g , b , a , mode = "rgb" )
6190	def set_sim_params ( self , nparams , attr_params ) : for name , value in nparams . items ( ) : val = value [ 0 ] if value [ 0 ] is not None else 'none' self . h5file . create_array ( '/parameters' , name , obj = val , title = value [ 1 ] ) for name , value in attr_params . items ( ) : self . h5file . set_node_attr ( '/parameters' , name , value )
402	def maxnorm_regularizer ( scale = 1.0 ) : if isinstance ( scale , numbers . Integral ) : raise ValueError ( 'scale cannot be an integer: %s' % scale ) if isinstance ( scale , numbers . Real ) : if scale < 0. : raise ValueError ( 'Setting a scale less than 0 on a regularizer: %g' % scale ) if scale == 0. : tl . logging . info ( 'Scale of 0 disables regularizer.' ) return lambda _ , name = None : None def mn ( weights , name = 'max_regularizer' ) : with tf . name_scope ( name ) as scope : my_scale = ops . convert_to_tensor ( scale , dtype = weights . dtype . base_dtype , name = 'scale' ) standard_ops_fn = standard_ops . multiply return standard_ops_fn ( my_scale , standard_ops . reduce_max ( standard_ops . abs ( weights ) ) , name = scope ) return mn
10163	def load ( self ) : ret = { } with open ( self . get_path ( ) , 'r' ) as f : lines = f . readlines ( ) ret [ 'personalities' ] = self . get_personalities ( lines [ 0 ] ) ret [ 'arrays' ] = self . get_arrays ( lines [ 1 : - 1 ] , ret [ 'personalities' ] ) self . content = reduce ( lambda x , y : x + y , lines ) return ret
1414	def get_pplan ( self , topologyName , callback = None ) : isWatching = False ret = { "result" : None } if callback : isWatching = True else : def callback ( data ) : ret [ "result" ] = data self . _get_pplan_with_watch ( topologyName , callback , isWatching ) return ret [ "result" ]
2554	def setdocument ( self , doc ) : if self . document != doc : self . document = doc for i in self . children : if not isinstance ( i , dom_tag ) : return i . setdocument ( doc )
13835	def _ParseOrMerge ( self , lines , message ) : tokenizer = _Tokenizer ( lines ) while not tokenizer . AtEnd ( ) : self . _MergeField ( tokenizer , message )
6526	def parse ( cls , content , is_pyproject = False ) : parsed = pytoml . loads ( content ) if is_pyproject : parsed = parsed . get ( 'tool' , { } ) parsed = parsed . get ( 'tidypy' , { } ) return parsed
12469	def smart_str ( value , encoding = 'utf-8' , errors = 'strict' ) : if not IS_PY3 and isinstance ( value , unicode ) : return value . encode ( encoding , errors ) return str ( value )
9351	def number ( type = None , length = None , prefixes = None ) : if type and type in CARDS : card = type else : card = random . choice ( list ( CARDS . keys ( ) ) ) if not prefixes : prefixes = CARDS [ card ] [ 'prefixes' ] prefix = random . choice ( prefixes ) if not length : length = CARDS [ card ] [ 'length' ] result = str ( prefix ) for d in range ( length - len ( str ( prefix ) ) ) : result += str ( basic . number ( ) ) last_digit = check_digit ( int ( result ) ) return int ( result [ : - 1 ] + str ( last_digit ) )
361	def exists_or_mkdir ( path , verbose = True ) : if not os . path . exists ( path ) : if verbose : logging . info ( "[*] creates %s ..." % path ) os . makedirs ( path ) return False else : if verbose : logging . info ( "[!] %s exists ..." % path ) return True
20	def pretty_eta ( seconds_left ) : minutes_left = seconds_left // 60 seconds_left %= 60 hours_left = minutes_left // 60 minutes_left %= 60 days_left = hours_left // 24 hours_left %= 24 def helper ( cnt , name ) : return "{} {}{}" . format ( str ( cnt ) , name , ( 's' if cnt > 1 else '' ) ) if days_left > 0 : msg = helper ( days_left , 'day' ) if hours_left > 0 : msg += ' and ' + helper ( hours_left , 'hour' ) return msg if hours_left > 0 : msg = helper ( hours_left , 'hour' ) if minutes_left > 0 : msg += ' and ' + helper ( minutes_left , 'minute' ) return msg if minutes_left > 0 : return helper ( minutes_left , 'minute' ) return 'less than a minute'
10291	def enrich_unqualified ( graph : BELGraph ) : enrich_complexes ( graph ) enrich_composites ( graph ) enrich_reactions ( graph ) enrich_variants ( graph )
6325	def encode ( self , text ) : text = text_type ( text ) if '\x00' in text : text = text . replace ( '\x00' , ' ' ) minval = Fraction ( 0 ) maxval = Fraction ( 1 ) for char in text + '\x00' : prob_range = self . _probs [ char ] delta = maxval - minval maxval = minval + prob_range [ 1 ] * delta minval = minval + prob_range [ 0 ] * delta delta = ( maxval - minval ) / 2 nbits = long ( 0 ) while delta < 1 : nbits += 1 delta *= 2 if nbits == 0 : return 0 , 0 avg = ( maxval + minval ) * 2 ** ( nbits - 1 ) return avg . numerator // avg . denominator , nbits
5592	def tiles_from_bbox ( self , geometry , zoom ) : for tile in self . tile_pyramid . tiles_from_bbox ( geometry , zoom ) : yield self . tile ( * tile . id )
251	def get_turnover ( positions , transactions , denominator = 'AGB' ) : txn_vol = get_txn_vol ( transactions ) traded_value = txn_vol . txn_volume if denominator == 'AGB' : AGB = positions . drop ( 'cash' , axis = 1 ) . abs ( ) . sum ( axis = 1 ) denom = AGB . rolling ( 2 ) . mean ( ) denom . iloc [ 0 ] = AGB . iloc [ 0 ] / 2 elif denominator == 'portfolio_value' : denom = positions . sum ( axis = 1 ) else : raise ValueError ( "Unexpected value for denominator '{}'. The " "denominator parameter must be either 'AGB'" " or 'portfolio_value'." . format ( denominator ) ) denom . index = denom . index . normalize ( ) turnover = traded_value . div ( denom , axis = 'index' ) turnover = turnover . fillna ( 0 ) return turnover
2032	def MLOAD ( self , address ) : self . _allocate ( address , 32 ) value = self . _load ( address , 32 ) return value
4120	def onesided_2_twosided ( data ) : psd = np . concatenate ( ( data [ 0 : - 1 ] , cshift ( data [ - 1 : 0 : - 1 ] , - 1 ) ) ) / 2. psd [ 0 ] *= 2. psd [ - 1 ] *= 2. return psd
8263	def swatch ( self , x , y , w = 35 , h = 35 , padding = 0 , roundness = 0 ) : for clr in self : clr . swatch ( x , y , w , h , roundness ) y += h + padding
10495	def clickMouseButtonRightWithMods ( self , coord , modifiers ) : modFlags = self . _pressModifiers ( modifiers ) self . _queueMouseButton ( coord , Quartz . kCGMouseButtonRight , modFlags ) self . _releaseModifiers ( modifiers ) self . _postQueuedEvents ( )
6331	def encode ( self , word , terminator = '\0' ) : r if word : if terminator in word : raise ValueError ( 'Specified terminator, {}, already in word.' . format ( terminator if terminator != '\0' else '\\0' ) ) else : word += terminator wordlist = sorted ( word [ i : ] + word [ : i ] for i in range ( len ( word ) ) ) return '' . join ( [ w [ - 1 ] for w in wordlist ] ) else : return terminator
4436	def destroy ( self ) : self . ws . destroy ( ) self . bot . remove_listener ( self . on_socket_response ) self . hooks . clear ( )
10203	def register_queries ( ) : return [ dict ( query_name = 'bucket-file-download-histogram' , query_class = ESDateHistogramQuery , query_config = dict ( index = 'stats-file-download' , doc_type = 'file-download-day-aggregation' , copy_fields = dict ( bucket_id = 'bucket_id' , file_key = 'file_key' , ) , required_filters = dict ( bucket_id = 'bucket_id' , file_key = 'file_key' , ) ) ) , dict ( query_name = 'bucket-file-download-total' , query_class = ESTermsQuery , query_config = dict ( index = 'stats-file-download' , doc_type = 'file-download-day-aggregation' , copy_fields = dict ( ) , required_filters = dict ( bucket_id = 'bucket_id' , ) , aggregated_fields = [ 'file_key' ] ) ) , ]
1893	def _reset ( self , constraints = None ) : if self . _proc is None : self . _start_proc ( ) else : if self . support_reset : self . _send ( "(reset)" ) for cfg in self . _init : self . _send ( cfg ) else : self . _stop_proc ( ) self . _start_proc ( ) if constraints is not None : self . _send ( constraints )
165	def compute_distance ( self , other , default = None ) : distances = self . compute_pointwise_distances ( other , default = [ ] ) if len ( distances ) == 0 : return default return min ( distances )
7669	def slice ( self , start_time , end_time , strict = False ) : sliced_array = AnnotationArray ( ) for ann in self : sliced_array . append ( ann . slice ( start_time , end_time , strict = strict ) ) return sliced_array
5497	def discover ( cls , * args , ** kwargs ) : file = os . path . join ( Cache . cache_dir , Cache . cache_name ) return cls . from_file ( file , * args , ** kwargs )
13774	def format ( self , record ) : record_fields = record . __dict__ . copy ( ) self . _set_exc_info ( record_fields ) event_name = 'default' if record_fields . get ( 'event_name' ) : event_name = record_fields . pop ( 'event_name' ) log_level = 'INFO' if record_fields . get ( 'log_level' ) : log_level = record_fields . pop ( 'log_level' ) [ record_fields . pop ( k ) for k in record_fields . keys ( ) if k not in self . fields ] defaults = self . defaults . copy ( ) fields = self . fields . copy ( ) fields . update ( record_fields ) filtered_fields = { } for k , v in fields . iteritems ( ) : if v is not None : filtered_fields [ k ] = v defaults . update ( { 'event_timestamp' : self . _get_now ( ) , 'event_name' : event_name , 'log_level' : log_level , 'fields' : filtered_fields } ) return json . dumps ( defaults , default = self . json_default )
1009	def compute ( self , bottomUpInput , enableLearn , enableInference = None ) : if enableInference is None : if enableLearn : enableInference = False else : enableInference = True assert ( enableLearn or enableInference ) activeColumns = bottomUpInput . nonzero ( ) [ 0 ] if enableLearn : self . lrnIterationIdx += 1 self . iterationIdx += 1 if self . verbosity >= 3 : print "\n==== PY Iteration: %d =====" % ( self . iterationIdx ) print "Active cols:" , activeColumns if enableLearn : if self . lrnIterationIdx in Segment . dutyCycleTiers : for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : for segment in self . cells [ c ] [ i ] : segment . dutyCycle ( ) if self . avgInputDensity is None : self . avgInputDensity = len ( activeColumns ) else : self . avgInputDensity = ( 0.99 * self . avgInputDensity + 0.01 * len ( activeColumns ) ) if enableInference : self . _updateInferenceState ( activeColumns ) if enableLearn : self . _updateLearningState ( activeColumns ) if self . globalDecay > 0.0 and ( ( self . lrnIterationIdx % self . maxAge ) == 0 ) : for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : segsToDel = [ ] for segment in self . cells [ c ] [ i ] : age = self . lrnIterationIdx - segment . lastActiveIteration if age <= self . maxAge : continue synsToDel = [ ] for synapse in segment . syns : synapse [ 2 ] = synapse [ 2 ] - self . globalDecay if synapse [ 2 ] <= 0 : synsToDel . append ( synapse ) if len ( synsToDel ) == segment . getNumSynapses ( ) : segsToDel . append ( segment ) elif len ( synsToDel ) > 0 : for syn in synsToDel : segment . syns . remove ( syn ) for seg in segsToDel : self . _cleanUpdatesList ( c , i , seg ) self . cells [ c ] [ i ] . remove ( seg ) if self . collectStats : if enableInference : predictedState = self . infPredictedState [ 't-1' ] else : predictedState = self . lrnPredictedState [ 't-1' ] self . _updateStatsInferEnd ( self . _internalStats , activeColumns , predictedState , self . colConfidence [ 't-1' ] ) output = self . _computeOutput ( ) self . printComputeEnd ( output , learn = enableLearn ) self . resetCalled = False return output
11761	def refresh ( self ) : args = [ ( obj . name , obj . value ) for obj in self . queryset . all ( ) ] super ( SettingDict , self ) . update ( args ) self . empty_cache = False
2629	def scale_out ( self , blocks = 1 , block_size = 1 ) : self . config [ 'sites.jetstream.{0}' . format ( self . pool ) ] [ 'flavor' ] count = 0 if blocks == 1 : block_id = len ( self . blocks ) self . blocks [ block_id ] = [ ] for instance_id in range ( 0 , block_size ) : instances = self . server_manager . create ( 'parsl-{0}-{1}' . format ( block_id , instance_id ) , self . client . images . get ( '87e08a17-eae2-4ce4-9051-c561d9a54bde' ) , self . client . flavors . list ( ) [ 0 ] , min_count = 1 , max_count = 1 , userdata = setup_script . format ( engine_config = self . engine_config ) , key_name = 'TG-MCB090174-api-key' , security_groups = [ 'global-ssh' ] , nics = [ { "net-id" : '724a50cf-7f11-4b3b-a884-cd7e6850e39e' , "net-name" : 'PARSL-priv-net' , "v4-fixed-ip" : '' } ] ) self . blocks [ block_id ] . extend ( [ instances ] ) count += 1 return count
2531	def parse_doc_fields ( self , doc_term ) : try : self . builder . set_doc_spdx_id ( self . doc , doc_term ) except SPDXValueError : self . value_error ( 'DOC_SPDX_ID_VALUE' , doc_term ) try : if doc_term . count ( '#' , 0 , len ( doc_term ) ) <= 1 : doc_namespace = doc_term . split ( '#' ) [ 0 ] self . builder . set_doc_namespace ( self . doc , doc_namespace ) else : self . value_error ( 'DOC_NAMESPACE_VALUE' , doc_term ) except SPDXValueError : self . value_error ( 'DOC_NAMESPACE_VALUE' , doc_term ) for _s , _p , o in self . graph . triples ( ( doc_term , self . spdx_namespace [ 'specVersion' ] , None ) ) : try : self . builder . set_doc_version ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'DOC_VERS_VALUE' , o ) except CardinalityError : self . more_than_one_error ( 'specVersion' ) break for _s , _p , o in self . graph . triples ( ( doc_term , self . spdx_namespace [ 'dataLicense' ] , None ) ) : try : self . builder . set_doc_data_lic ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'DOC_D_LICS' , o ) except CardinalityError : self . more_than_one_error ( 'dataLicense' ) break for _s , _p , o in self . graph . triples ( ( doc_term , self . spdx_namespace [ 'name' ] , None ) ) : try : self . builder . set_doc_name ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'name' ) break for _s , _p , o in self . graph . triples ( ( doc_term , RDFS . comment , None ) ) : try : self . builder . set_doc_comment ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'Document comment' ) break
9968	def copy ( self , space = None , name = None ) : return Cells ( space = space , name = name , formula = self . formula )
956	def getArgumentDescriptions ( f ) : argspec = inspect . getargspec ( f ) docstring = f . __doc__ descriptions = { } if docstring : lines = docstring . split ( '\n' ) i = 0 while i < len ( lines ) : stripped = lines [ i ] . lstrip ( ) if not stripped : i += 1 continue indentLevel = lines [ i ] . index ( stripped [ 0 ] ) firstWord = stripped . split ( ) [ 0 ] if firstWord . endswith ( ':' ) : firstWord = firstWord [ : - 1 ] if firstWord in argspec . args : argName = firstWord restOfLine = stripped [ len ( firstWord ) + 1 : ] . strip ( ) argLines = [ restOfLine ] i += 1 while i < len ( lines ) : stripped = lines [ i ] . lstrip ( ) if not stripped : break if lines [ i ] . index ( stripped [ 0 ] ) <= indentLevel : break argLines . append ( lines [ i ] . strip ( ) ) i += 1 descriptions [ argName ] = ' ' . join ( argLines ) else : i += 1 args = [ ] if argspec . defaults : defaultCount = len ( argspec . defaults ) else : defaultCount = 0 nonDefaultArgCount = len ( argspec . args ) - defaultCount for i , argName in enumerate ( argspec . args ) : if i >= nonDefaultArgCount : defaultValue = argspec . defaults [ i - nonDefaultArgCount ] args . append ( ( argName , descriptions . get ( argName , "" ) , defaultValue ) ) else : args . append ( ( argName , descriptions . get ( argName , "" ) ) ) return args
6036	def array_2d_from_array_1d ( self , array_1d ) : return mapping_util . map_masked_1d_array_to_2d_array_from_array_1d_shape_and_one_to_two ( array_1d = array_1d , shape = self . mask . shape , one_to_two = self . mask . masked_grid_index_to_pixel )
13658	def _addRoute ( self , f , matcher ) : self . _routes . append ( ( f . func_name , f , matcher ) )
4278	def process_dir ( self , album , force = False ) : for f in album : if isfile ( f . dst_path ) and not force : self . logger . info ( "%s exists - skipping" , f . filename ) self . stats [ f . type + '_skipped' ] += 1 else : self . stats [ f . type ] += 1 yield ( f . type , f . path , f . filename , f . src_path , album . dst_path , self . settings )
730	def _generate ( self ) : candidates = np . array ( range ( self . _n ) , np . uint32 ) for i in xrange ( self . _num ) : self . _random . shuffle ( candidates ) pattern = candidates [ 0 : self . _getW ( ) ] self . _patterns [ i ] = set ( pattern )
8346	def findAll ( self , name = None , attrs = { } , recursive = True , text = None , limit = None , ** kwargs ) : generator = self . recursiveChildGenerator if not recursive : generator = self . childGenerator return self . _findAll ( name , attrs , text , limit , generator , ** kwargs )
13613	def combine_filenames ( filenames , max_length = 40 ) : path = None names = [ ] extension = None timestamps = [ ] shas = [ ] filenames . sort ( ) concat_names = "_" . join ( filenames ) if concat_names in COMBINED_FILENAMES_GENERATED : return COMBINED_FILENAMES_GENERATED [ concat_names ] for filename in filenames : name = os . path . basename ( filename ) if not extension : extension = os . path . splitext ( name ) [ 1 ] elif os . path . splitext ( name ) [ 1 ] != extension : raise ValueError ( "Can't combine multiple file extensions" ) for base in MEDIA_ROOTS : try : shas . append ( md5 ( os . path . join ( base , filename ) ) ) break except IOError : pass if path is None : path = os . path . dirname ( filename ) else : if len ( os . path . dirname ( filename ) ) < len ( path ) : path = os . path . dirname ( filename ) m = hashlib . md5 ( ) m . update ( "," . join ( shas ) ) new_filename = "%s-inkmd" % m . hexdigest ( ) new_filename = new_filename [ : max_length ] new_filename += extension COMBINED_FILENAMES_GENERATED [ concat_names ] = new_filename return os . path . join ( path , new_filename )
3739	def Hf_g ( CASRN , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in ATcT_g . index : methods . append ( ATCT_G ) if CASRN in TRC_gas_data . index and not np . isnan ( TRC_gas_data . at [ CASRN , 'Hf' ] ) : methods . append ( TRC ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ATCT_G : _Hfg = float ( ATcT_g . at [ CASRN , 'Hf_298K' ] ) elif Method == TRC : _Hfg = float ( TRC_gas_data . at [ CASRN , 'Hf' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' ) return _Hfg
345	def _load_mnist_dataset ( shape , path , name = 'mnist' , url = 'http://yann.lecun.com/exdb/mnist/' ) : path = os . path . join ( path , name ) def load_mnist_images ( path , filename ) : filepath = maybe_download_and_extract ( filename , path , url ) logging . info ( filepath ) with gzip . open ( filepath , 'rb' ) as f : data = np . frombuffer ( f . read ( ) , np . uint8 , offset = 16 ) data = data . reshape ( shape ) return data / np . float32 ( 256 ) def load_mnist_labels ( path , filename ) : filepath = maybe_download_and_extract ( filename , path , url ) with gzip . open ( filepath , 'rb' ) as f : data = np . frombuffer ( f . read ( ) , np . uint8 , offset = 8 ) return data logging . info ( "Load or Download {0} > {1}" . format ( name . upper ( ) , path ) ) X_train = load_mnist_images ( path , 'train-images-idx3-ubyte.gz' ) y_train = load_mnist_labels ( path , 'train-labels-idx1-ubyte.gz' ) X_test = load_mnist_images ( path , 't10k-images-idx3-ubyte.gz' ) y_test = load_mnist_labels ( path , 't10k-labels-idx1-ubyte.gz' ) X_train , X_val = X_train [ : - 10000 ] , X_train [ - 10000 : ] y_train , y_val = y_train [ : - 10000 ] , y_train [ - 10000 : ] X_train = np . asarray ( X_train , dtype = np . float32 ) y_train = np . asarray ( y_train , dtype = np . int32 ) X_val = np . asarray ( X_val , dtype = np . float32 ) y_val = np . asarray ( y_val , dtype = np . int32 ) X_test = np . asarray ( X_test , dtype = np . float32 ) y_test = np . asarray ( y_test , dtype = np . int32 ) return X_train , y_train , X_val , y_val , X_test , y_test
13760	def _format_iso_time ( self , time ) : if isinstance ( time , str ) : return time elif isinstance ( time , datetime ) : return time . strftime ( '%Y-%m-%dT%H:%M:%S.%fZ' ) else : return None
10268	def main ( output ) : from hbp_knowledge import get_graph graph = get_graph ( ) text = to_html ( graph ) print ( text , file = output )
12307	def auto_get_repo ( autooptions , debug = False ) : pluginmgr = plugins_get_mgr ( ) repomgr = pluginmgr . get ( what = 'repomanager' , name = 'git' ) repo = None try : if debug : print ( "Looking repo" ) repo = repomgr . lookup ( username = autooptions [ 'username' ] , reponame = autooptions [ 'reponame' ] ) except : try : print ( "Checking and cloning if the dataset exists on backend" ) url = autooptions [ 'remoteurl' ] if debug : print ( "Doesnt exist. trying to clone: {}" . format ( url ) ) common_clone ( url ) repo = repomgr . lookup ( username = autooptions [ 'username' ] , reponame = autooptions [ 'reponame' ] ) if debug : print ( "Cloning successful" ) except : yes = input ( "Repo doesnt exist. Should I create one? [yN]" ) if yes == 'y' : setup = "git" if autooptions [ 'remoteurl' ] . startswith ( 's3://' ) : setup = 'git+s3' repo = common_init ( username = autooptions [ 'username' ] , reponame = autooptions [ 'reponame' ] , setup = setup , force = True , options = autooptions ) if debug : print ( "Successfully inited repo" ) else : raise Exception ( "Cannot load repo" ) repo . options = autooptions return repo
12759	def load_csv ( self , filename , start_frame = 10 , max_frames = int ( 1e300 ) ) : import pandas as pd compression = None if filename . endswith ( '.gz' ) : compression = 'gzip' df = pd . read_csv ( filename , compression = compression ) . set_index ( 'time' ) . fillna ( - 1 ) assert self . world . dt == pd . Series ( df . index ) . diff ( ) . mean ( ) markers = [ ] for c in df . columns : m = re . match ( r'^marker\d\d-(.*)-c$' , c ) if m : markers . append ( m . group ( 1 ) ) self . channels = self . _map_labels_to_channels ( markers ) cols = [ c for c in df . columns if re . match ( r'^marker\d\d-.*-[xyzc]$' , c ) ] self . data = df [ cols ] . values . reshape ( ( len ( df ) , len ( markers ) , 4 ) ) [ start_frame : ] self . data [ : , : , [ 1 , 2 ] ] = self . data [ : , : , [ 2 , 1 ] ] logging . info ( '%s: loaded marker data %s' , filename , self . data . shape ) self . process_data ( ) self . create_bodies ( )
10269	def node_is_upstream_leaf ( graph : BELGraph , node : BaseEntity ) -> bool : return 0 == len ( graph . predecessors ( node ) ) and 1 == len ( graph . successors ( node ) )
12321	def send ( self , send_email = True ) : url = str ( self . api . base_url + '{code}/status/' ) . format ( code = self . code ) payload = { 'mark_as_sent' : True , 'send_email' : send_email , } stat = self . api . connection . make_put ( url , payload )
12756	def set_target_angles ( self , angles ) : j = 0 for joint in self . joints : velocities = [ ctrl ( tgt - cur , self . world . dt ) for cur , tgt , ctrl in zip ( joint . angles , angles [ j : j + joint . ADOF ] , joint . controllers ) ] joint . velocities = velocities j += joint . ADOF
1064	def isheader ( self , line ) : i = line . find ( ':' ) if i > - 1 : return line [ : i ] . lower ( ) return None
953	def closenessScores ( self , expValues , actValues , ** kwargs ) : ratio = 1.0 esum = int ( expValues . sum ( ) ) asum = int ( actValues . sum ( ) ) if asum > esum : diff = asum - esum if diff < esum : ratio = 1 - diff / float ( esum ) else : ratio = 1 / float ( diff ) olap = expValues & actValues osum = int ( olap . sum ( ) ) if esum == 0 : r = 0.0 else : r = osum / float ( esum ) r = r * ratio return numpy . array ( [ r ] )
5090	def export_as_csv_action ( description = "Export selected objects as CSV file" , fields = None , header = True ) : def export_as_csv ( modeladmin , request , queryset ) : opts = modeladmin . model . _meta if not fields : field_names = [ field . name for field in opts . fields ] else : field_names = fields response = HttpResponse ( content_type = "text/csv" ) response [ "Content-Disposition" ] = "attachment; filename={filename}.csv" . format ( filename = str ( opts ) . replace ( "." , "_" ) ) writer = unicodecsv . writer ( response , encoding = "utf-8" ) if header : writer . writerow ( field_names ) for obj in queryset : row = [ ] for field_name in field_names : field = getattr ( obj , field_name ) if callable ( field ) : value = field ( ) else : value = field if value is None : row . append ( "[Not Set]" ) elif not value and isinstance ( value , string_types ) : row . append ( "[Empty]" ) else : row . append ( value ) writer . writerow ( row ) return response export_as_csv . short_description = description return export_as_csv
4635	def child ( self , offset256 ) : a = bytes ( self . pubkey ) + offset256 s = hashlib . sha256 ( a ) . digest ( ) return self . derive_from_seed ( s )
3766	def Z_from_virial_pressure_form ( P , * args ) : r return 1 + P * sum ( [ coeff * P ** i for i , coeff in enumerate ( args ) ] )
9832	def value ( self , ascode = None ) : if ascode is None : ascode = self . code return self . cast [ ascode ] ( self . text )
398	def sigmoid_cross_entropy ( output , target , name = None ) : return tf . reduce_mean ( tf . nn . sigmoid_cross_entropy_with_logits ( labels = target , logits = output ) , name = name )
7927	def reorder_srv ( records ) : records = list ( records ) records . sort ( ) ret = [ ] tmp = [ ] for rrecord in records : if not tmp or rrecord . priority == tmp [ 0 ] . priority : tmp . append ( rrecord ) continue ret += shuffle_srv ( tmp ) tmp = [ rrecord ] if tmp : ret += shuffle_srv ( tmp ) return ret
4424	def get ( self , guild_id ) : if guild_id not in self . _players : p = self . _player ( lavalink = self . lavalink , guild_id = guild_id ) self . _players [ guild_id ] = p return self . _players [ guild_id ]
12313	def instantiate ( repo , name = None , filename = None ) : default_transformers = repo . options . get ( 'transformer' , { } ) transformers = { } if name is not None : if name in default_transformers : transformers = { name : default_transformers [ name ] } else : transformers = { name : { 'files' : [ ] , } } else : transformers = default_transformers input_matching_files = None if filename is not None : input_matching_files = repo . find_matching_files ( [ filename ] ) for t in transformers : for k in transformers [ t ] : if "files" not in k : continue if k == "files" and input_matching_files is not None : transformers [ t ] [ k ] = input_matching_files else : if transformers [ t ] [ k ] is None or len ( transformers [ t ] [ k ] ) == 0 : transformers [ t ] [ k ] = [ ] else : matching_files = repo . find_matching_files ( transformers [ t ] [ k ] ) transformers [ t ] [ k ] = matching_files return transformers
1868	def PSRLDQ ( cpu , dest , src ) : temp = Operators . EXTRACT ( src . read ( ) , 0 , 8 ) temp = Operators . ITEBV ( src . size , temp > 15 , 16 , temp ) dest . write ( dest . read ( ) >> ( temp * 8 ) )
9097	def drop_bel_namespace ( self ) -> Optional [ Namespace ] : namespace = self . _get_default_namespace ( ) if namespace is not None : for entry in tqdm ( namespace . entries , desc = f'deleting entries in {self._get_namespace_name()}' ) : self . session . delete ( entry ) self . session . delete ( namespace ) log . info ( 'committing deletions' ) self . session . commit ( ) return namespace
8159	def edit ( self , id , * args , ** kw ) : if args and kw : return if args and type ( args [ 0 ] ) == dict : fields = [ k for k in args [ 0 ] ] v = [ args [ 0 ] [ k ] for k in args [ 0 ] ] if kw : fields = [ k for k in kw ] v = [ kw [ k ] for k in kw ] sql = "update " + self . _name + " set " + "=?, " . join ( fields ) + "=? where " + self . _key + "=" + unicode ( id ) self . _db . _cur . execute ( sql , v ) self . _db . _i += 1 if self . _db . _i >= self . _db . _commit : self . _db . _i = 0 self . _db . _con . commit ( )
6686	def is_installed ( pkg_name ) : manager = MANAGER with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : res = run ( "rpm --query %(pkg_name)s" % locals ( ) ) if res . succeeded : return True return False
9555	def _apply_value_predicates ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for field_name , predicate , code , message , modulus in self . _value_predicates : if i % modulus == 0 : fi = self . _field_names . index ( field_name ) if fi < len ( r ) : value = r [ fi ] try : valid = predicate ( value ) if not valid : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field_name p [ 'value' ] = value p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field_name p [ 'value' ] = value p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( predicate . __name__ , predicate . __doc__ ) if context is not None : p [ 'context' ] = context yield p
9578	def read_numeric_array ( fd , endian , header , data_etypes ) : if header [ 'is_complex' ] : raise ParseError ( 'Complex arrays are not supported' ) data = read_elements ( fd , endian , data_etypes ) if not isinstance ( data , Sequence ) : return data rowcount = header [ 'dims' ] [ 0 ] colcount = header [ 'dims' ] [ 1 ] array = [ list ( data [ c * rowcount + r ] for c in range ( colcount ) ) for r in range ( rowcount ) ] return squeeze ( array )
8626	def delete_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_delete_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise UserJobsNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
8033	def find_dupes ( paths , exact = False , ignores = None , min_size = 0 ) : groups = { '' : getPaths ( paths , ignores ) } groups = groupBy ( groups , sizeClassifier , 'sizes' , min_size = min_size ) groups = groupBy ( groups , hashClassifier , 'header hashes' , limit = HEAD_SIZE ) if exact : groups = groupBy ( groups , groupByContent , fun_desc = 'contents' ) else : groups = groupBy ( groups , hashClassifier , fun_desc = 'hashes' ) return groups
4833	def traverse_pagination ( response , endpoint , content_filter_query , query_params ) : results = response . get ( 'results' , [ ] ) page = 1 while response . get ( 'next' ) : page += 1 response = endpoint ( ) . post ( content_filter_query , ** dict ( query_params , page = page ) ) results += response . get ( 'results' , [ ] ) return results
9088	async def _update_loop ( self ) -> None : await asyncio . sleep ( self . _update_interval ) while not self . _closed : await self . update ( ) await asyncio . sleep ( self . _update_interval )
3557	def find_device ( cls , timeout_sec = TIMEOUT_SEC ) : return get_provider ( ) . find_device ( service_uuids = cls . ADVERTISED , timeout_sec = timeout_sec )
6033	def from_shape_pixel_scale_and_sub_grid_size ( cls , shape , pixel_scale , sub_grid_size = 2 ) : regular_grid = RegularGrid . from_shape_and_pixel_scale ( shape = shape , pixel_scale = pixel_scale ) sub_grid = SubGrid . from_shape_pixel_scale_and_sub_grid_size ( shape = shape , pixel_scale = pixel_scale , sub_grid_size = sub_grid_size ) blurring_grid = np . array ( [ [ 0.0 , 0.0 ] ] ) return GridStack ( regular_grid , sub_grid , blurring_grid )
4269	def get_exif_data ( filename ) : logger = logging . getLogger ( __name__ ) img = _read_image ( filename ) try : exif = img . _getexif ( ) or { } except ZeroDivisionError : logger . warning ( 'Failed to read EXIF data.' ) return None data = { TAGS . get ( tag , tag ) : value for tag , value in exif . items ( ) } if 'GPSInfo' in data : try : data [ 'GPSInfo' ] = { GPSTAGS . get ( tag , tag ) : value for tag , value in data [ 'GPSInfo' ] . items ( ) } except AttributeError : logger = logging . getLogger ( __name__ ) logger . info ( 'Failed to get GPS Info' ) del data [ 'GPSInfo' ] return data
13706	def iter_space_block ( self , text = None , width = 60 , fmtfunc = str ) : if width < 1 : width = 1 curline = '' text = ( self . text if text is None else text ) or '' for word in text . split ( ) : possibleline = ' ' . join ( ( curline , word ) ) if curline else word codelen = sum ( len ( s ) for s in get_codes ( possibleline ) ) reallen = len ( possibleline ) - codelen if reallen > width : yield fmtfunc ( curline ) curline = word else : curline = possibleline if curline : yield fmtfunc ( curline )
13311	def site_path ( self ) : if platform == 'win' : return unipath ( self . path , 'Lib' , 'site-packages' ) py_ver = 'python{0}' . format ( sys . version [ : 3 ] ) return unipath ( self . path , 'lib' , py_ver , 'site-packages' )
193	def OneOf ( children , name = None , deterministic = False , random_state = None ) : return SomeOf ( n = 1 , children = children , random_order = False , name = name , deterministic = deterministic , random_state = random_state )
202	def resize ( self , sizes , interpolation = "cubic" ) : arr_resized = ia . imresize_single_image ( self . arr , sizes , interpolation = interpolation ) arr_resized = np . clip ( arr_resized , 0.0 , 1.0 ) segmap = SegmentationMapOnImage ( arr_resized , shape = self . shape ) segmap . input_was = self . input_was return segmap
13898	def IterHashes ( iterator_size , hash_length = 7 ) : if not isinstance ( iterator_size , int ) : raise TypeError ( 'iterator_size must be integer.' ) count = 0 while count != iterator_size : count += 1 yield GetRandomHash ( hash_length )
7127	def find_and_patch_entry ( soup , entry ) : link = soup . find ( "a" , { "class" : "headerlink" } , href = "#" + entry . anchor ) tag = soup . new_tag ( "a" ) tag [ "name" ] = APPLE_REF_TEMPLATE . format ( entry . type , entry . name ) if link : link . parent . insert ( 0 , tag ) return True elif entry . anchor . startswith ( "module-" ) : soup . h1 . parent . insert ( 0 , tag ) return True else : return False
6182	def hash ( self ) : hash_list = [ ] for key , value in sorted ( self . __dict__ . items ( ) ) : if not callable ( value ) : if isinstance ( value , np . ndarray ) : hash_list . append ( value . tostring ( ) ) else : hash_list . append ( str ( value ) ) return hashlib . md5 ( repr ( hash_list ) . encode ( ) ) . hexdigest ( )
4636	def derive_from_seed ( self , offset ) : seed = int ( hexlify ( bytes ( self ) ) . decode ( "ascii" ) , 16 ) z = int ( hexlify ( offset ) . decode ( "ascii" ) , 16 ) order = ecdsa . SECP256k1 . order secexp = ( seed + z ) % order secret = "%0x" % secexp if len ( secret ) < 64 : secret = ( "0" * ( 64 - len ( secret ) ) ) + secret return PrivateKey ( secret , prefix = self . pubkey . prefix )
1216	def from_spec ( spec , kwargs = None ) : optimizer = util . get_object ( obj = spec , predefined_objects = tensorforce . core . optimizers . optimizers , kwargs = kwargs ) assert isinstance ( optimizer , Optimizer ) return optimizer
9541	def datetime_range_inclusive ( min , max , format ) : dmin = datetime . strptime ( min , format ) dmax = datetime . strptime ( max , format ) def checker ( v ) : dv = datetime . strptime ( v , format ) if dv < dmin or dv > dmax : raise ValueError ( v ) return checker
4583	def convert_mode ( image , mode = 'RGB' ) : deprecated . deprecated ( 'util.gif.convert_model' ) return image if ( image . mode == mode ) else image . convert ( mode = mode )
4504	def put_edit ( self , f , * args , ** kwds ) : self . put_nowait ( functools . partial ( f , * args , ** kwds ) )
13588	def post_required ( method_or_options = [ ] ) : def decorator ( method ) : expected_fields = [ ] if not callable ( method_or_options ) : expected_fields = method_or_options @ wraps ( method ) def wrapper ( * args , ** kwargs ) : request = args [ 0 ] if request . method != 'POST' : logger . error ( 'POST required for this url' ) raise Http404 ( 'only POST allowed for this url' ) missing = [ ] for field in expected_fields : if field not in request . POST : missing . append ( field ) if missing : s = 'Expected fields missing in POST: %s' % missing logger . error ( s ) raise Http404 ( s ) return method ( * args , ** kwargs ) return wrapper if callable ( method_or_options ) : return decorator ( method_or_options ) return decorator
1036	def chain ( self , expanded_from ) : return Range ( self . source_buffer , self . begin_pos , self . begin_pos , expanded_from = expanded_from )
6537	def mod_sys_path ( paths ) : old_path = sys . path sys . path = paths + sys . path try : yield finally : sys . path = old_path
4651	def appendWif ( self , wif ) : if wif : try : self . privatekey_class ( wif ) self . wifs . add ( wif ) except Exception : raise InvalidWifError
10548	def delete_taskrun ( taskrun_id ) : try : res = _pybossa_req ( 'delete' , 'taskrun' , taskrun_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : raise
4122	def centerdc_2_twosided ( data ) : N = len ( data ) newpsd = np . concatenate ( ( data [ N // 2 : ] , ( cshift ( data [ 0 : N // 2 ] , - 1 ) ) ) ) return newpsd
4836	def get_paginated_catalogs ( self , querystring = None ) : return self . _load_data ( self . CATALOGS_ENDPOINT , default = [ ] , querystring = querystring , traverse_pagination = False , many = False )
4590	def serpentine_y ( x , y , matrix ) : if x % 2 : return x , matrix . rows - 1 - y return x , y
13103	def create_scan ( self , host_ips ) : now = datetime . datetime . now ( ) data = { "uuid" : self . get_template_uuid ( ) , "settings" : { "name" : "jackal-" + now . strftime ( "%Y-%m-%d %H:%M" ) , "text_targets" : host_ips } } response = requests . post ( self . url + 'scans' , data = json . dumps ( data ) , verify = False , headers = self . headers ) if response : result = json . loads ( response . text ) return result [ 'scan' ] [ 'id' ]
1525	def to_table ( result ) : max_count = 20 table , count = [ ] , 0 for role , envs_topos in result . items ( ) : for env , topos in envs_topos . items ( ) : for topo in topos : count += 1 if count > max_count : continue else : table . append ( [ role , env , topo ] ) header = [ 'role' , 'env' , 'topology' ] rest_count = 0 if count <= max_count else count - max_count return table , header , rest_count
7583	def run ( self , ipyclient = None , quiet = False , force = False , block = False , ) : if force : for key , oldfile in self . trees : if os . path . exists ( oldfile ) : os . remove ( oldfile ) if os . path . exists ( self . trees . info ) : print ( "Error: set a new name for this job or use Force flag.\nFile exists: {}" . format ( self . trees . info ) ) return if not ipyclient : proc = _call_raxml ( self . _command_list ) self . stdout = proc [ 0 ] self . stderr = proc [ 1 ] else : lbview = ipyclient . load_balanced_view ( ) self . async = lbview . apply ( _call_raxml , self . _command_list ) if not quiet : if not ipyclient : if "Overall execution time" not in self . stdout : print ( "Error in raxml run\n" + self . stdout ) else : print ( "job {} finished successfully" . format ( self . params . n ) ) else : print ( "job {} submitted to cluster" . format ( self . params . n ) )
7731	def make_join_request ( self , password = None , history_maxchars = None , history_maxstanzas = None , history_seconds = None , history_since = None ) : self . clear_muc_child ( ) self . muc_child = MucX ( parent = self . xmlnode ) if ( history_maxchars is not None or history_maxstanzas is not None or history_seconds is not None or history_since is not None ) : history = HistoryParameters ( history_maxchars , history_maxstanzas , history_seconds , history_since ) self . muc_child . set_history ( history ) if password is not None : self . muc_child . set_password ( password )
12023	def check_phase ( self ) : plus_minus = set ( [ '+' , '-' ] ) for k , g in groupby ( sorted ( [ line for line in self . lines if line [ 'line_type' ] == 'feature' and line [ 'type' ] == 'CDS' and 'Parent' in line [ 'attributes' ] ] , key = lambda x : x [ 'attributes' ] [ 'Parent' ] ) , key = lambda x : x [ 'attributes' ] [ 'Parent' ] ) : cds_list = list ( g ) strand_set = list ( set ( [ line [ 'strand' ] for line in cds_list ] ) ) if len ( strand_set ) != 1 : for line in cds_list : self . add_line_error ( line , { 'message' : 'Inconsistent CDS strand with parent: {0:s}' . format ( k ) , 'error_type' : 'STRAND' } ) continue if len ( cds_list ) == 1 : if cds_list [ 0 ] [ 'phase' ] != 0 : self . add_line_error ( cds_list [ 0 ] , { 'message' : 'Wrong phase {0:d}, should be {1:d}' . format ( cds_list [ 0 ] [ 'phase' ] , 0 ) , 'error_type' : 'PHASE' } ) continue strand = strand_set [ 0 ] if strand not in plus_minus : continue if strand == '-' : sorted_cds_list = sorted ( cds_list , key = lambda x : x [ 'end' ] , reverse = True ) else : sorted_cds_list = sorted ( cds_list , key = lambda x : x [ 'start' ] ) phase = 0 for line in sorted_cds_list : if line [ 'phase' ] != phase : self . add_line_error ( line , { 'message' : 'Wrong phase {0:d}, should be {1:d}' . format ( line [ 'phase' ] , phase ) , 'error_type' : 'PHASE' } ) phase = ( 3 - ( ( line [ 'end' ] - line [ 'start' ] + 1 - phase ) % 3 ) ) % 3
11293	def oembed_schema ( request ) : current_domain = Site . objects . get_current ( ) . domain url_schemes = [ ] endpoint = reverse ( 'oembed_json' ) providers = oembed . site . get_providers ( ) for provider in providers : if not provider . provides : continue match = None if isinstance ( provider , DjangoProvider ) : url_pattern = resolver . reverse_dict . get ( provider . _meta . named_view ) if url_pattern : regex = re . sub ( r'%\(.+?\)s' , '*' , url_pattern [ 0 ] [ 0 ] [ 0 ] ) match = 'http://%s/%s' % ( current_domain , regex ) elif isinstance ( provider , HTTPProvider ) : match = provider . url_scheme else : match = provider . regex if match : url_schemes . append ( { 'type' : provider . resource_type , 'matches' : match , 'endpoint' : endpoint } ) url_schemes . sort ( key = lambda item : item [ 'matches' ] ) response = HttpResponse ( mimetype = 'application/json' ) response . write ( simplejson . dumps ( url_schemes ) ) return response
11734	def get_resample_data ( self ) : if self . data is not None : if self . _pvpc_mean_daily is None : self . _pvpc_mean_daily = self . data [ 'data' ] . resample ( 'D' ) . mean ( ) if self . _pvpc_mean_monthly is None : self . _pvpc_mean_monthly = self . data [ 'data' ] . resample ( 'MS' ) . mean ( ) return self . _pvpc_mean_daily , self . _pvpc_mean_monthly
6472	def color_ramp ( self , size ) : color = PALETTE . get ( self . option . palette , { } ) color = color . get ( self . term . colors , None ) color_ramp = [ ] if color is not None : ratio = len ( color ) / float ( size ) for i in range ( int ( size ) ) : color_ramp . append ( self . term . color ( color [ int ( ratio * i ) ] ) ) return color_ramp
10038	def pick_coda_from_letter ( letter ) : try : __ , __ , coda = split_phonemes ( letter , onset = False , nucleus = False , coda = True ) except ValueError : return None else : return coda
11220	def compare ( self , jwt : 'Jwt' , compare_dates : bool = False ) -> bool : if self . secret != jwt . secret : return False if self . payload != jwt . payload : return False if self . alg != jwt . alg : return False if self . header != jwt . header : return False expected_claims = self . registered_claims actual_claims = jwt . registered_claims if not compare_dates : strip = [ 'exp' , 'nbf' , 'iat' ] expected_claims = { k : { v if k not in strip else None } for k , v in expected_claims . items ( ) } actual_claims = { k : { v if k not in strip else None } for k , v in actual_claims . items ( ) } if expected_claims != actual_claims : return False return True
7011	def plot_periodbase_lsp ( lspinfo , outfile = None , plotdpi = 100 ) : if isinstance ( lspinfo , str ) and os . path . exists ( lspinfo ) : LOGINFO ( 'loading LSP info from pickle %s' % lspinfo ) with open ( lspinfo , 'rb' ) as infd : lspinfo = pickle . load ( infd ) try : periods = lspinfo [ 'periods' ] lspvals = lspinfo [ 'lspvals' ] bestperiod = lspinfo [ 'bestperiod' ] lspmethod = lspinfo [ 'method' ] plt . plot ( periods , lspvals ) plt . xscale ( 'log' , basex = 10 ) plt . xlabel ( 'Period [days]' ) plt . ylabel ( PLOTYLABELS [ lspmethod ] ) plottitle = '%s best period: %.6f d' % ( METHODSHORTLABELS [ lspmethod ] , bestperiod ) plt . title ( plottitle ) for bestperiod , bestpeak in zip ( lspinfo [ 'nbestperiods' ] , lspinfo [ 'nbestlspvals' ] ) : plt . annotate ( '%.6f' % bestperiod , xy = ( bestperiod , bestpeak ) , xycoords = 'data' , xytext = ( 0.0 , 25.0 ) , textcoords = 'offset points' , arrowprops = dict ( arrowstyle = "->" ) , fontsize = 'x-small' ) plt . grid ( color = '#a9a9a9' , alpha = 0.9 , zorder = 0 , linewidth = 1.0 , linestyle = ':' ) if outfile and isinstance ( outfile , str ) : if outfile . endswith ( '.png' ) : plt . savefig ( outfile , bbox_inches = 'tight' , dpi = plotdpi ) else : plt . savefig ( outfile , bbox_inches = 'tight' ) plt . close ( ) return os . path . abspath ( outfile ) elif dispok : plt . show ( ) plt . close ( ) return else : LOGWARNING ( 'no output file specified and no $DISPLAY set, ' 'saving to lsp-plot.png in current directory' ) outfile = 'lsp-plot.png' plt . savefig ( outfile , bbox_inches = 'tight' , dpi = plotdpi ) plt . close ( ) return os . path . abspath ( outfile ) except Exception as e : LOGEXCEPTION ( 'could not plot this LSP, appears to be empty' ) return
13738	def _real_time_thread ( self ) : while self . ws_client . connected ( ) : if self . die : break if self . pause : sleep ( 5 ) continue message = self . ws_client . receive ( ) if message is None : break message_type = message [ 'type' ] if message_type == 'error' : continue if message [ 'sequence' ] <= self . sequence : continue if message_type == 'open' : self . _handle_open ( message ) elif message_type == 'match' : self . _handle_match ( message ) elif message_type == 'done' : self . _handle_done ( message ) elif message_type == 'change' : self . _handle_change ( message ) else : continue self . ws_client . disconnect ( )
8162	def publish_event ( event_t , data = None , extra_channels = None , wait = None ) : event = Event ( event_t , data ) pubsub . publish ( "shoebot" , event ) for channel_name in extra_channels or [ ] : pubsub . publish ( channel_name , event ) if wait is not None : channel = pubsub . subscribe ( wait ) channel . listen ( wait )
8211	def insert_point ( self , x , y ) : try : bezier = _ctx . ximport ( "bezier" ) except : from nodebox . graphics import bezier n = 100 closest = None dx0 = float ( "inf" ) dy0 = float ( "inf" ) for i in range ( n ) : t = float ( i ) / n pt = self . path . point ( t ) dx = abs ( pt . x - x ) dy = abs ( pt . y - y ) if dx + dy <= dx0 + dy0 : dx0 = dx dy0 = dy closest = t decimals = [ 3 , 4 ] for d in decimals : d = 1.0 / pow ( 10 , d ) for i in range ( 20 ) : t = closest - d + float ( i ) * d * 0.1 if t < 0.0 : t = 1.0 + t if t > 1.0 : t = t - 1.0 pt = self . path . point ( t ) dx = abs ( pt . x - x ) dy = abs ( pt . y - y ) if dx <= dx0 and dy <= dy0 : dx0 = dx dy0 = dy closest_precise = t closest = closest_precise p = bezier . insert_point ( self . path , closest_precise ) i , t , pt = bezier . _locate ( self . path , closest_precise ) i += 1 pt = PathElement ( ) pt . cmd = p [ i ] . cmd pt . x = p [ i ] . x pt . y = p [ i ] . y pt . ctrl1 = Point ( p [ i ] . ctrl1 . x , p [ i ] . ctrl1 . y ) pt . ctrl2 = Point ( p [ i ] . ctrl2 . x , p [ i ] . ctrl2 . y ) pt . freehand = False self . _points . insert ( i , pt ) self . _points [ i - 1 ] . ctrl1 = Point ( p [ i - 1 ] . ctrl1 . x , p [ i - 1 ] . ctrl1 . y ) self . _points [ i + 1 ] . ctrl1 = Point ( p [ i + 1 ] . ctrl1 . x , p [ i + 1 ] . ctrl1 . y ) self . _points [ i + 1 ] . ctrl2 = Point ( p [ i + 1 ] . ctrl2 . x , p [ i + 1 ] . ctrl2 . y )
9692	def send ( self , data ) : while len ( self . senders ) >= self . window : pass self . senders [ self . new_seq_no ] = self . Sender ( self . write , self . send_lock , data , self . new_seq_no , timeout = self . sending_timeout , callback = self . send_callback , ) self . senders [ self . new_seq_no ] . start ( ) self . new_seq_no = ( self . new_seq_no + 1 ) % HDLController . MAX_SEQ_NO
13367	def register_proxy_type ( cls , real_type , proxy_type ) : if distob . engine is None : cls . _initial_proxy_types [ real_type ] = proxy_type elif isinstance ( distob . engine , ObjectHub ) : distob . engine . _runtime_reg_proxy_type ( real_type , proxy_type ) else : distob . engine . _singleeng_reg_proxy_type ( real_type , proxy_type ) pass
1397	def extract_execution_state ( self , topology ) : execution_state = topology . execution_state executionState = { "cluster" : execution_state . cluster , "environ" : execution_state . environ , "role" : execution_state . role , "jobname" : topology . name , "submission_time" : execution_state . submission_time , "submission_user" : execution_state . submission_user , "release_username" : execution_state . release_state . release_username , "release_tag" : execution_state . release_state . release_tag , "release_version" : execution_state . release_state . release_version , "has_physical_plan" : None , "has_tmaster_location" : None , "has_scheduler_location" : None , "extra_links" : [ ] , } for extra_link in self . config . extra_links : link = extra_link . copy ( ) link [ "url" ] = self . config . get_formatted_url ( executionState , link [ EXTRA_LINK_FORMATTER_KEY ] ) executionState [ "extra_links" ] . append ( link ) return executionState
54	def shift ( self , x = 0 , y = 0 ) : keypoints = [ keypoint . shift ( x = x , y = y ) for keypoint in self . keypoints ] return self . deepcopy ( keypoints )
12697	def _parse_string ( self , xml ) : if not isinstance ( xml , HTMLElement ) : xml = dhtmlparser . parseString ( str ( xml ) ) record = xml . find ( "record" ) if not record : raise ValueError ( "There is no <record> in your MARC XML document!" ) record = record [ 0 ] self . oai_marc = len ( record . find ( "oai_marc" ) ) > 0 if not self . oai_marc : leader = record . find ( "leader" ) if len ( leader ) >= 1 : self . leader = leader [ 0 ] . getContent ( ) if self . oai_marc : self . _parse_control_fields ( record . find ( "fixfield" ) , "id" ) self . _parse_data_fields ( record . find ( "varfield" ) , "id" , "label" ) else : self . _parse_control_fields ( record . find ( "controlfield" ) , "tag" ) self . _parse_data_fields ( record . find ( "datafield" ) , "tag" , "code" ) if self . oai_marc and "LDR" in self . controlfields : self . leader = self . controlfields [ "LDR" ]
6689	def groupuninstall ( group , options = None ) : manager = MANAGER if options is None : options = [ ] elif isinstance ( options , str ) : options = [ options ] options = " " . join ( options ) run_as_root ( '%(manager)s %(options)s groupremove "%(group)s"' % locals ( ) )
4362	def _heartbeat ( self ) : interval = self . config [ 'heartbeat_interval' ] while self . connected : gevent . sleep ( interval ) self . put_client_msg ( "2::" )
3884	def from_conv_part_data ( conv_part_data , self_user_id ) : user_id = UserID ( chat_id = conv_part_data . id . chat_id , gaia_id = conv_part_data . id . gaia_id ) return User ( user_id , conv_part_data . fallback_name , None , None , [ ] , ( self_user_id == user_id ) or ( self_user_id is None ) )
1686	def Split ( self ) : googlename = self . RepositoryName ( ) project , rest = os . path . split ( googlename ) return ( project , ) + os . path . splitext ( rest )
9618	def UnPlug ( self , force = False ) : if force : _xinput . UnPlugForce ( c_uint ( self . id ) ) else : _xinput . UnPlug ( c_uint ( self . id ) ) while self . id not in self . available_ids ( ) : if self . id == 0 : break
4333	def noisered ( self , profile_path , amount = 0.5 ) : if not os . path . exists ( profile_path ) : raise IOError ( "profile_path {} does not exist." . format ( profile_path ) ) if not is_number ( amount ) or amount < 0 or amount > 1 : raise ValueError ( "amount must be a number between 0 and 1." ) effect_args = [ 'noisered' , profile_path , '{:f}' . format ( amount ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'noisered' ) return self
3518	def kiss_insights ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return KissInsightsNode ( )
7070	def precision ( ntp , nfp ) : if ( ntp + nfp ) > 0 : return ntp / ( ntp + nfp ) else : return np . nan
170	def draw_mask ( self , image_shape , size_lines = 1 , size_points = 0 , raise_if_out_of_image = False ) : heatmap = self . draw_heatmap_array ( image_shape , alpha_lines = 1.0 , alpha_points = 1.0 , size_lines = size_lines , size_points = size_points , antialiased = False , raise_if_out_of_image = raise_if_out_of_image ) return heatmap > 0.5
4703	def compare ( buf_a , buf_b , ignore ) : for field in getattr ( buf_a , '_fields_' ) : name , types = field [ 0 ] , field [ 1 ] if name in ignore : continue val_a = getattr ( buf_a , name ) val_b = getattr ( buf_b , name ) if isinstance ( types , ( type ( Union ) , type ( Structure ) ) ) : if compare ( val_a , val_b , ignore ) : return 1 elif isinstance ( types , type ( Array ) ) : for i , _ in enumerate ( val_a ) : if isinstance ( types , ( type ( Union ) , type ( Structure ) ) ) : if compare ( val_a [ i ] , val_b [ i ] , ignore ) : return 1 else : if val_a [ i ] != val_b [ i ] : return 1 else : if val_a != val_b : return 1 return 0
9328	def get ( self , url , headers = None , params = None ) : merged_headers = self . _merge_headers ( headers ) if "Accept" not in merged_headers : merged_headers [ "Accept" ] = MEDIA_TYPE_TAXII_V20 accept = merged_headers [ "Accept" ] resp = self . session . get ( url , headers = merged_headers , params = params ) resp . raise_for_status ( ) content_type = resp . headers [ "Content-Type" ] if not self . valid_content_type ( content_type = content_type , accept = accept ) : msg = "Unexpected Response. Got Content-Type: '{}' for Accept: '{}'" raise TAXIIServiceException ( msg . format ( content_type , accept ) ) return _to_json ( resp )
10316	def relation_set_has_contradictions ( relations : Set [ str ] ) -> bool : has_increases = any ( relation in CAUSAL_INCREASE_RELATIONS for relation in relations ) has_decreases = any ( relation in CAUSAL_DECREASE_RELATIONS for relation in relations ) has_cnc = any ( relation == CAUSES_NO_CHANGE for relation in relations ) return 1 < sum ( [ has_cnc , has_decreases , has_increases ] )
3322	def clear ( self ) : self . _lock . acquire_write ( ) try : was_closed = self . _dict is None if was_closed : self . open ( ) if len ( self . _dict ) : self . _dict . clear ( ) self . _dict . sync ( ) if was_closed : self . close ( ) finally : self . _lock . release ( )
13312	def _pre_activate ( self ) : if 'CPENV_CLEAN_ENV' not in os . environ : if platform == 'win' : os . environ [ 'PROMPT' ] = '$P$G' else : os . environ [ 'PS1' ] = '\\u@\\h:\\w\\$' clean_env_path = utils . get_store_env_tmp ( ) os . environ [ 'CPENV_CLEAN_ENV' ] = clean_env_path utils . store_env ( path = clean_env_path ) else : utils . restore_env_from_file ( os . environ [ 'CPENV_CLEAN_ENV' ] )
10284	def count_targets ( edge_iter : EdgeIterator ) -> Counter : return Counter ( v for _ , v , _ in edge_iter )
13173	def parents ( self , name = None ) : p = self . parent while p is not None : if name is None or p . tagname == name : yield p p = p . parent
12331	def get_tree ( gitdir = "." ) : cmd = [ "git" , "log" , "--all" , "--branches" , '--pretty=format:{ "commit": "%H", "abbreviated_commit": "%h", "tree": "%T", "abbreviated_tree": "%t", "parent": "%P", "abbreviated_parent": "%p", "refs": "%d", "encoding": "%e", "subject": "%s", "sanitized_subject_line": "%f", "commit_notes": "", "author": { "name": "%aN", "email": "%aE", "date": "%ai" }, "commiter": { "name": "%cN", "email": "%cE", "date": "%ci" }},' ] output = run ( cmd ) lines = output . split ( "\n" ) content = "" history = [ ] for l in lines : try : revisedcontent = content + l if revisedcontent . count ( '"' ) % 2 == 0 : j = json . loads ( revisedcontent [ : - 1 ] ) if "Notes added by" in j [ 'subject' ] : content = "" continue history . append ( j ) content = "" else : content = revisedcontent except Exception as e : print ( "Error while parsing record" ) print ( revisedcontent ) content = "" history . reverse ( ) changes = get_change ( ) for i in range ( len ( history ) ) : abbrev_commit = history [ i ] [ 'abbreviated_commit' ] if abbrev_commit not in changes : raise Exception ( "Missing changes for " + abbrev_commit ) history [ i ] [ 'changes' ] = changes [ abbrev_commit ] [ 'changes' ] return history
12123	def validate_activatable_models ( ) : for model in get_activatable_models ( ) : activatable_field = next ( ( f for f in model . _meta . fields if f . __class__ == models . BooleanField and f . name == model . ACTIVATABLE_FIELD_NAME ) , None ) if activatable_field is None : raise ValidationError ( ( 'Model {0} is an activatable model. It must define an activatable BooleanField that ' 'has a field name of model.ACTIVATABLE_FIELD_NAME (which defaults to is_active)' . format ( model ) ) ) if not model . ALLOW_CASCADE_DELETE : for field in model . _meta . fields : if field . __class__ in ( models . ForeignKey , models . OneToOneField ) : if field . remote_field . on_delete == models . CASCADE : raise ValidationError ( ( 'Model {0} is an activatable model. All ForeignKey and OneToOneFields ' 'must set on_delete methods to something other than CASCADE (the default). ' 'If you want to explicitely allow cascade deletes, then you must set the ' 'ALLOW_CASCADE_DELETE=True class variable on your model.' ) . format ( model ) )
4350	def vol ( self , gain , gain_type = 'amplitude' , limiter_gain = None ) : if not is_number ( gain ) : raise ValueError ( 'gain must be a number.' ) if limiter_gain is not None : if ( not is_number ( limiter_gain ) or limiter_gain <= 0 or limiter_gain >= 1 ) : raise ValueError ( 'limiter gain must be a positive number less than 1' ) if gain_type in [ 'amplitude' , 'power' ] and gain < 0 : raise ValueError ( "If gain_type = amplitude or power, gain must be positive." ) effect_args = [ 'vol' ] effect_args . append ( '{:f}' . format ( gain ) ) if gain_type == 'amplitude' : effect_args . append ( 'amplitude' ) elif gain_type == 'power' : effect_args . append ( 'power' ) elif gain_type == 'db' : effect_args . append ( 'dB' ) else : raise ValueError ( 'gain_type must be one of amplitude power or db' ) if limiter_gain is not None : if gain_type in [ 'amplitude' , 'power' ] and gain > 1 : effect_args . append ( '{:f}' . format ( limiter_gain ) ) elif gain_type == 'db' and gain > 0 : effect_args . append ( '{:f}' . format ( limiter_gain ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'vol' ) return self
61	def iou ( self , other ) : inters = self . intersection ( other ) if inters is None : return 0.0 else : area_union = self . area + other . area - inters . area return inters . area / area_union if area_union > 0 else 0.0
2316	def create_graph_from_data ( self , data , ** kwargs ) : self . arguments [ '{CITEST}' ] = self . dir_CI_test [ self . CI_test ] self . arguments [ '{METHOD_INDEP}' ] = self . dir_method_indep [ self . method_indep ] self . arguments [ '{DIRECTED}' ] = 'TRUE' self . arguments [ '{ALPHA}' ] = str ( self . alpha ) self . arguments [ '{NJOBS}' ] = str ( self . nb_jobs ) self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) results = self . _run_pc ( data , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
7219	def update ( self , task_name , task_json ) : r = self . gbdx_connection . put ( self . _base_url + '/' + task_name , json = task_json ) raise_for_status ( r ) return r . json ( )
10190	def tell ( self , message , sender = no_sender ) : if sender is not no_sender and not isinstance ( sender , ActorRef ) : raise ValueError ( "Sender must be actor reference" ) self . _cell . send_message ( message , sender )
9802	def get ( keys ) : _config = GlobalConfigManager . get_config_or_default ( ) if not keys : return print_values = { } for key in keys : if hasattr ( _config , key ) : print_values [ key ] = getattr ( _config , key ) else : click . echo ( 'Key `{}` is not recognised.' . format ( key ) ) dict_tabulate ( print_values , )
5654	def makedirs ( path ) : if not os . path . isdir ( path ) : os . makedirs ( path ) return path
4682	def getKeyType ( self , account , pub ) : for authority in [ "owner" , "active" ] : for key in account [ authority ] [ "key_auths" ] : if str ( pub ) == key [ 0 ] : return authority if str ( pub ) == account [ "options" ] [ "memo_key" ] : return "memo" return None
6905	def great_circle_dist ( ra1 , dec1 , ra2 , dec2 ) : in_ra1 = ra1 % 360.0 in_ra1 = in_ra1 + 360.0 * ( in_ra1 < 0.0 ) in_ra2 = ra2 % 360.0 in_ra2 = in_ra2 + 360.0 * ( in_ra1 < 0.0 ) ra1_rad , dec1_rad = np . deg2rad ( in_ra1 ) , np . deg2rad ( dec1 ) ra2_rad , dec2_rad = np . deg2rad ( in_ra2 ) , np . deg2rad ( dec2 ) del_dec2 = ( dec2_rad - dec1_rad ) / 2.0 del_ra2 = ( ra2_rad - ra1_rad ) / 2.0 sin_dist = np . sqrt ( np . sin ( del_dec2 ) * np . sin ( del_dec2 ) + np . cos ( dec1_rad ) * np . cos ( dec2_rad ) * np . sin ( del_ra2 ) * np . sin ( del_ra2 ) ) dist_rad = 2.0 * np . arcsin ( sin_dist ) return np . rad2deg ( dist_rad ) * 3600.0
7649	def _open ( name_or_fdesc , mode = 'r' , fmt = 'auto' ) : open_map = { 'jams' : open , 'json' : open , 'jamz' : gzip . open , 'gz' : gzip . open } if hasattr ( name_or_fdesc , 'read' ) or hasattr ( name_or_fdesc , 'write' ) : yield name_or_fdesc elif isinstance ( name_or_fdesc , six . string_types ) : if fmt == 'auto' : _ , ext = os . path . splitext ( name_or_fdesc ) ext = ext [ 1 : ] else : ext = fmt try : ext = ext . lower ( ) if ext in [ 'jamz' , 'gz' ] and 't' not in mode : mode = '{:s}t' . format ( mode ) with open_map [ ext ] ( name_or_fdesc , mode = mode ) as fdesc : yield fdesc except KeyError : raise ParameterError ( 'Unknown JAMS extension ' 'format: "{:s}"' . format ( ext ) ) else : raise ParameterError ( 'Invalid filename or ' 'descriptor: {}' . format ( name_or_fdesc ) )
2875	def add_bpmn_files ( self , filenames ) : for filename in filenames : f = open ( filename , 'r' ) try : self . add_bpmn_xml ( ET . parse ( f ) , filename = filename ) finally : f . close ( )
12701	def get_subfields ( self , datafield , subfield , i1 = None , i2 = None , exception = False ) : if len ( datafield ) != 3 : raise ValueError ( "`datafield` parameter have to be exactly 3 chars long!" ) if len ( subfield ) != 1 : raise ValueError ( "Bad subfield specification - subfield have to be 1 char long!" ) if datafield not in self . datafields : if exception : raise KeyError ( datafield + " is not in datafields!" ) return [ ] output = [ ] for datafield in self . datafields [ datafield ] : if subfield not in datafield : continue for sfield in datafield [ subfield ] : if i1 and sfield . i1 != i1 : continue if i2 and sfield . i2 != i2 : continue output . append ( sfield ) if not output and exception : raise KeyError ( subfield + " couldn't be found in subfields!" ) return output
10779	def _remove_closest_particle ( self , p ) : dp = self . pos - p dist2 = ( dp * dp ) . sum ( axis = 1 ) ind = dist2 . argmin ( ) rp = self . pos [ ind ] . copy ( ) self . pos = np . delete ( self . pos , ind , axis = 0 ) return rp
10335	def build_delete_node_by_hash ( manager : Manager ) -> Callable [ [ BELGraph , str ] , None ] : @ in_place_transformation def delete_node_by_hash ( graph : BELGraph , node_hash : str ) -> None : node = manager . get_dsl_by_hash ( node_hash ) graph . remove_node ( node ) return delete_node_by_hash
13270	def all_subclasses ( cls ) : for subclass in cls . __subclasses__ ( ) : yield subclass for subc in all_subclasses ( subclass ) : yield subc
3617	def get_settings ( self ) : try : logger . info ( 'GET SETTINGS ON %s' , self . index_name ) return self . __index . get_settings ( ) except AlgoliaException as e : if DEBUG : raise e else : logger . warning ( 'ERROR DURING GET_SETTINGS ON %s: %s' , self . model , e )
9406	def _cleanup ( self ) : self . exit ( ) workspace = osp . join ( os . getcwd ( ) , 'octave-workspace' ) if osp . exists ( workspace ) : os . remove ( workspace )
8223	def main_iteration ( self ) : if self . show_vars : self . show_variables_window ( ) else : self . hide_variables_window ( ) for snapshot_f in self . scheduled_snapshots : fn = snapshot_f ( self . last_draw_ctx ) print ( "Saved snapshot: %s" % fn ) else : self . scheduled_snapshots = deque ( ) while Gtk . events_pending ( ) : Gtk . main_iteration ( )
5834	def create_ml_configuration ( self , search_template , extract_as_keys , dataset_ids ) : data = { "search_template" : search_template , "extract_as_keys" : extract_as_keys } failure_message = "ML Configuration creation failed" config_job_id = self . _get_success_json ( self . _post_json ( 'v1/descriptors/builders/simple/default/trigger' , data , failure_message = failure_message ) ) [ 'data' ] [ 'result' ] [ 'uid' ] while True : config_status = self . __get_ml_configuration_status ( config_job_id ) print ( 'Configuration status: ' , config_status ) if config_status [ 'status' ] == 'Finished' : ml_config = self . __convert_response_to_configuration ( config_status [ 'result' ] , dataset_ids ) return ml_config time . sleep ( 5 )
10928	def _run2 ( self ) : if self . check_update_J ( ) : self . update_J ( ) else : if self . check_Broyden_J ( ) : self . update_Broyden_J ( ) if self . check_update_eig_J ( ) : self . update_eig_J ( ) _last_residuals = self . calc_residuals ( ) . copy ( ) _last_error = 1 * self . error _last_vals = self . param_vals . copy ( ) delta_params_1 = self . find_LM_updates ( self . calc_grad ( ) , do_correct_damping = False ) self . decrease_damping ( ) delta_params_2 = self . find_LM_updates ( self . calc_grad ( ) , do_correct_damping = False ) self . decrease_damping ( undo_decrease = True ) er1 = self . update_function ( self . param_vals + delta_params_1 ) er2 = self . update_function ( self . param_vals + delta_params_2 ) triplet = ( self . error , er1 , er2 ) best_step = find_best_step ( triplet ) if best_step == 0 : _ = self . update_function ( self . param_vals . copy ( ) ) grad = self . calc_grad ( ) CLOG . debug ( 'Bad step, increasing damping' ) CLOG . debug ( '%f\t%f\t%f' % triplet ) for _try in range ( self . _max_inner_loop ) : self . increase_damping ( ) delta_vals = self . find_LM_updates ( grad ) er_new = self . update_function ( self . param_vals + delta_vals ) good_step = er_new < self . error if good_step : self . update_param_vals ( delta_vals , incremental = True ) self . error = er_new CLOG . debug ( 'Sufficiently increased damping' ) CLOG . debug ( '%f\t%f' % ( triplet [ 0 ] , self . error ) ) break else : CLOG . warn ( 'Stuck!' ) self . error = self . update_function ( self . param_vals . copy ( ) ) elif best_step == 1 : good_step = True CLOG . debug ( 'Good step, same damping' ) CLOG . debug ( '%f\t%f\t%f' % triplet ) er1_1 = self . update_function ( self . param_vals + delta_params_1 ) if np . abs ( er1_1 - er1 ) > 1e-6 : raise RuntimeError ( 'Function updates are not exact.' ) self . update_param_vals ( delta_params_1 , incremental = True ) self . error = er1 elif best_step == 2 : good_step = True self . error = er2 CLOG . debug ( 'Good step, decreasing damping' ) CLOG . debug ( '%f\t%f\t%f' % triplet ) self . update_param_vals ( delta_params_2 , incremental = True ) self . decrease_damping ( ) if good_step : self . _last_residuals = _last_residuals self . _last_error = _last_error self . _last_vals = _last_vals self . error self . do_internal_run ( initial_count = 1 )
11441	def _correct_record ( record ) : errors = [ ] for tag in record . keys ( ) : upper_bound = '999' n = len ( tag ) if n > 3 : i = n - 3 while i > 0 : upper_bound = '%s%s' % ( '0' , upper_bound ) i -= 1 if tag == '!' : errors . append ( ( 1 , '(field number(s): ' + str ( [ f [ 4 ] for f in record [ tag ] ] ) + ')' ) ) record [ '000' ] = record . pop ( tag ) tag = '000' elif not ( '001' <= tag <= upper_bound or tag in ( 'FMT' , 'FFT' , 'BDR' , 'BDM' ) ) : errors . append ( 2 ) record [ '000' ] = record . pop ( tag ) tag = '000' fields = [ ] for field in record [ tag ] : if field [ 0 ] == [ ] and field [ 3 ] == '' : errors . append ( ( 8 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) subfields = [ ] for subfield in field [ 0 ] : if subfield [ 0 ] == '!' : errors . append ( ( 3 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) newsub = ( '' , subfield [ 1 ] ) else : newsub = subfield subfields . append ( newsub ) if field [ 1 ] == '!' : errors . append ( ( 4 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) ind1 = " " else : ind1 = field [ 1 ] if field [ 2 ] == '!' : errors . append ( ( 5 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) ind2 = " " else : ind2 = field [ 2 ] fields . append ( ( subfields , ind1 , ind2 , field [ 3 ] , field [ 4 ] ) ) record [ tag ] = fields return errors
4774	def contains_sequence ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) else : try : for i in xrange ( len ( self . val ) - len ( items ) + 1 ) : for j in xrange ( len ( items ) ) : if self . val [ i + j ] != items [ j ] : break else : return self except TypeError : raise TypeError ( 'val is not iterable' ) self . _err ( 'Expected <%s> to contain sequence %s, but did not.' % ( self . val , self . _fmt_items ( items ) ) )
3970	def _get_build_path ( app_spec ) : if os . path . isabs ( app_spec [ 'build' ] ) : return app_spec [ 'build' ] return os . path . join ( Repo ( app_spec [ 'repo' ] ) . local_path , app_spec [ 'build' ] )
3585	def get_all ( self , cbobjects ) : try : with self . _lock : return [ self . _metadata [ x ] for x in cbobjects ] except KeyError : raise RuntimeError ( 'Failed to find expected metadata for CoreBluetooth object!' )
10062	def deposit_links_factory ( pid ) : links = default_links_factory ( pid ) def _url ( name , ** kwargs ) : endpoint = '.{0}_{1}' . format ( current_records_rest . default_endpoint_prefixes [ pid . pid_type ] , name , ) return url_for ( endpoint , pid_value = pid . pid_value , _external = True , ** kwargs ) links [ 'files' ] = _url ( 'files' ) ui_endpoint = current_app . config . get ( 'DEPOSIT_UI_ENDPOINT' ) if ui_endpoint is not None : links [ 'html' ] = ui_endpoint . format ( host = request . host , scheme = request . scheme , pid_value = pid . pid_value , ) deposit_cls = Deposit if 'pid_value' in request . view_args : deposit_cls = request . view_args [ 'pid_value' ] . data [ 1 ] . __class__ for action in extract_actions_from_class ( deposit_cls ) : links [ action ] = _url ( 'actions' , action = action ) return links
4861	def validate_username ( self , value ) : try : user = User . objects . get ( username = value ) except User . DoesNotExist : raise serializers . ValidationError ( "User does not exist" ) try : enterprise_customer_user = models . EnterpriseCustomerUser . objects . get ( user_id = user . pk ) except models . EnterpriseCustomerUser . DoesNotExist : raise serializers . ValidationError ( "User has no EnterpriseCustomerUser" ) self . enterprise_customer_user = enterprise_customer_user return value
6984	def timebinlc ( lcfile , binsizesec , outdir = None , lcformat = 'hat-sql' , lcformatdir = None , timecols = None , magcols = None , errcols = None , minbinelems = 7 ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if timecols is None : timecols = dtimecols if magcols is None : magcols = dmagcols if errcols is None : errcols = derrcols lcdict = readerfunc ( lcfile ) if ( ( isinstance ( lcdict , ( list , tuple ) ) ) and ( isinstance ( lcdict [ 0 ] , dict ) ) ) : lcdict = lcdict [ 0 ] if 'binned' in lcdict : LOGERROR ( 'this light curve appears to be binned already, skipping...' ) return None lcdict [ 'binned' ] = { } for tcol , mcol , ecol in zip ( timecols , magcols , errcols ) : if '.' in tcol : tcolget = tcol . split ( '.' ) else : tcolget = [ tcol ] times = _dict_get ( lcdict , tcolget ) if '.' in mcol : mcolget = mcol . split ( '.' ) else : mcolget = [ mcol ] mags = _dict_get ( lcdict , mcolget ) if '.' in ecol : ecolget = ecol . split ( '.' ) else : ecolget = [ ecol ] errs = _dict_get ( lcdict , ecolget ) if normfunc is None : ntimes , nmags = normalize_magseries ( times , mags , magsarefluxes = magsarefluxes ) times , mags , errs = ntimes , nmags , errs binned = time_bin_magseries_with_errs ( times , mags , errs , binsize = binsizesec , minbinelems = minbinelems ) lcdict [ 'binned' ] [ mcol ] = { 'times' : binned [ 'binnedtimes' ] , 'mags' : binned [ 'binnedmags' ] , 'errs' : binned [ 'binnederrs' ] , 'nbins' : binned [ 'nbins' ] , 'timebins' : binned [ 'jdbins' ] , 'binsizesec' : binsizesec } if outdir is None : outdir = os . path . dirname ( lcfile ) outfile = os . path . join ( outdir , '%s-binned%.1fsec-%s.pkl' % ( squeeze ( lcdict [ 'objectid' ] ) . replace ( ' ' , '-' ) , binsizesec , lcformat ) ) with open ( outfile , 'wb' ) as outfd : pickle . dump ( lcdict , outfd , protocol = pickle . HIGHEST_PROTOCOL ) return outfile
630	def binSearch ( arr , val ) : i = bisect_left ( arr , val ) if i != len ( arr ) and arr [ i ] == val : return i return - 1
4731	def terminate ( self ) : if self . __thread : cmd = [ "who am i" ] status , output , _ = cij . util . execute ( cmd , shell = True , echo = True ) if status : cij . warn ( "cij.dmesg.terminate: who am i failed" ) return 1 tty = output . split ( ) [ 1 ] cmd = [ "pkill -f '{}' -t '{}'" . format ( " " . join ( self . __prefix ) , tty ) ] status , _ , _ = cij . util . execute ( cmd , shell = True , echo = True ) if status : cij . warn ( "cij.dmesg.terminate: pkill failed" ) return 1 self . __thread . join ( ) self . __thread = None return 0
4543	def compose_events ( events , condition = all ) : events = list ( events ) master_event = threading . Event ( ) def changed ( ) : if condition ( e . is_set ( ) for e in events ) : master_event . set ( ) else : master_event . clear ( ) def add_changed ( f ) : @ functools . wraps ( f ) def wrapped ( ) : f ( ) changed ( ) return wrapped for e in events : e . set = add_changed ( e . set ) e . clear = add_changed ( e . clear ) changed ( ) return master_event
7016	def concat_write_pklc ( lcbasedir , objectid , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , outdir = None , recursive = True ) : concatlcd = concatenate_textlcs_for_objectid ( lcbasedir , objectid , aperture = aperture , sortby = sortby , normalize = normalize , recursive = recursive ) if not outdir : outdir = 'pklcs' if not os . path . exists ( outdir ) : os . mkdir ( outdir ) outfpath = os . path . join ( outdir , '%s-%s-pklc.pkl' % ( concatlcd [ 'objectid' ] , aperture ) ) pklc = lcdict_to_pickle ( concatlcd , outfile = outfpath ) return pklc
12345	def stitch ( self , folder = None ) : debug ( 'stitching ' + self . __str__ ( ) ) if not folder : folder = self . path macros = [ ] files = [ ] for well in self . wells : f , m = stitch_macro ( well , folder ) macros . extend ( m ) files . extend ( f ) chopped_arguments = zip ( chop ( macros , _pools ) , chop ( files , _pools ) ) chopped_filenames = Parallel ( n_jobs = _pools ) ( delayed ( fijibin . macro . run ) ( macro = arg [ 0 ] , output_files = arg [ 1 ] ) for arg in chopped_arguments ) return [ f for list_ in chopped_filenames for f in list_ ]
981	def _initializeBucketMap ( self , maxBuckets , offset ) : self . _maxBuckets = maxBuckets self . minIndex = self . _maxBuckets / 2 self . maxIndex = self . _maxBuckets / 2 self . _offset = offset self . bucketMap = { } def _permutation ( n ) : r = numpy . arange ( n , dtype = numpy . uint32 ) self . random . shuffle ( r ) return r self . bucketMap [ self . minIndex ] = _permutation ( self . n ) [ 0 : self . w ] self . numTries = 0
6076	def einstein_mass_in_units ( self , unit_mass = 'angular' , critical_surface_density = None ) : if self . has_mass_profile : return sum ( map ( lambda p : p . einstein_mass_in_units ( unit_mass = unit_mass , critical_surface_density = critical_surface_density ) , self . mass_profiles ) ) else : return None
2964	def insert_rule ( self , chain , src = None , dest = None , target = None ) : if not chain : raise ValueError ( "Invalid chain" ) if not target : raise ValueError ( "Invalid target" ) if not ( src or dest ) : raise ValueError ( "Need src, dest, or both" ) args = [ "-I" , chain ] if src : args += [ "-s" , src ] if dest : args += [ "-d" , dest ] args += [ "-j" , target ] self . call ( * args )
7659	def append_columns ( self , columns ) : self . append_records ( [ dict ( time = t , duration = d , value = v , confidence = c ) for ( t , d , v , c ) in six . moves . zip ( columns [ 'time' ] , columns [ 'duration' ] , columns [ 'value' ] , columns [ 'confidence' ] ) ] )
12549	def spatial_map ( icc , thr , mode = '+' ) : return thr_img ( icc_img_to_zscore ( icc ) , thr = thr , mode = mode ) . get_data ( )
1345	def predictions ( self , image ) : return np . squeeze ( self . batch_predictions ( image [ np . newaxis ] ) , axis = 0 )
6058	def bin_up_array_2d_using_mean ( array_2d , bin_up_factor ) : padded_array_2d = pad_2d_array_for_binning_up_with_bin_up_factor ( array_2d = array_2d , bin_up_factor = bin_up_factor ) binned_array_2d = np . zeros ( shape = ( padded_array_2d . shape [ 0 ] // bin_up_factor , padded_array_2d . shape [ 1 ] // bin_up_factor ) ) for y in range ( binned_array_2d . shape [ 0 ] ) : for x in range ( binned_array_2d . shape [ 1 ] ) : value = 0.0 for y1 in range ( bin_up_factor ) : for x1 in range ( bin_up_factor ) : padded_y = y * bin_up_factor + y1 padded_x = x * bin_up_factor + x1 value += padded_array_2d [ padded_y , padded_x ] binned_array_2d [ y , x ] = value / ( bin_up_factor ** 2.0 ) return binned_array_2d
3791	def refractive_index ( CASRN , T = None , AvailableMethods = False , Method = None , full_info = True ) : r def list_methods ( ) : methods = [ ] if CASRN in CRC_RI_organic . index : methods . append ( CRC ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == CRC : _RI = float ( CRC_RI_organic . at [ CASRN , 'RI' ] ) if full_info : _T = float ( CRC_RI_organic . at [ CASRN , 'RIT' ] ) elif Method == NONE : _RI , _T = None , None else : raise Exception ( 'Failure in in function' ) if full_info : return _RI , _T else : return _RI
3802	def calculate_P ( self , T , P , method ) : r if method == DIPPR_9G : kl = self . T_dependent_property ( T ) kl = DIPPR9G ( T , P , self . Tc , self . Pc , kl ) elif method == MISSENARD : kl = self . T_dependent_property ( T ) kl = Missenard ( T , P , self . Tc , self . Pc , kl ) elif method == COOLPROP : kl = PropsSI ( 'L' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : kl = self . interpolate_P ( T , P , method ) return kl
10942	def update_function ( self , param_vals ) : self . opt_obj . update_function ( param_vals ) return self . opt_obj . get_error ( )
12035	def sweepYfiltered ( self ) : assert self . kernel is not None return swhlab . common . convolve ( self . sweepY , self . kernel )
10226	def get_correlation_triangles ( graph : BELGraph ) -> SetOfNodeTriples : return { tuple ( sorted ( [ n , u , v ] , key = str ) ) for n in graph for u , v in itt . combinations ( graph [ n ] , 2 ) if graph . has_edge ( u , v ) }
7271	def load ( ) : for operator in operators : module , symbols = operator [ 0 ] , operator [ 1 : ] path = 'grappa.operators.{}' . format ( module ) operator = __import__ ( path , None , None , symbols ) for symbol in symbols : Engine . register ( getattr ( operator , symbol ) )
8275	def recombine ( self , other , d = 0.7 ) : a , b = self , other d1 = max ( 0 , min ( d , 1 ) ) d2 = d1 c = ColorTheme ( name = a . name [ : int ( len ( a . name ) * d1 ) ] + b . name [ int ( len ( b . name ) * d2 ) : ] , ranges = a . ranges [ : int ( len ( a . ranges ) * d1 ) ] + b . ranges [ int ( len ( b . ranges ) * d2 ) : ] , top = a . top , cache = os . path . join ( DEFAULT_CACHE , "recombined" ) , blue = a . blue , length = a . length * d1 + b . length * d2 ) c . tags = a . tags [ : int ( len ( a . tags ) * d1 ) ] c . tags += b . tags [ int ( len ( b . tags ) * d2 ) : ] return c
5666	def _run ( self ) : if self . _has_run : raise RuntimeError ( "This spreader instance has already been run: " "create a new Spreader object for a new run." ) i = 1 while self . event_heap . size ( ) > 0 and len ( self . _uninfected_stops ) > 0 : event = self . event_heap . pop_next_event ( ) this_stop = self . _stop_I_to_spreading_stop [ event . from_stop_I ] if event . arr_time_ut > self . start_time_ut + self . max_duration_ut : break if this_stop . can_infect ( event ) : target_stop = self . _stop_I_to_spreading_stop [ event . to_stop_I ] already_visited = target_stop . has_been_visited ( ) target_stop . visit ( event ) if not already_visited : self . _uninfected_stops . remove ( event . to_stop_I ) print ( i , self . event_heap . size ( ) ) transfer_distances = self . gtfs . get_straight_line_transfer_distances ( event . to_stop_I ) self . event_heap . add_walk_events_to_heap ( transfer_distances , event , self . start_time_ut , self . walk_speed , self . _uninfected_stops , self . max_duration_ut ) i += 1 self . _has_run = True
7595	def get_clan ( self , * tags : crtag , ** params : keys ) : url = self . api . CLAN + '/' + ',' . join ( tags ) return self . _get_model ( url , FullClan , ** params )
6509	def set_search_enviroment ( cls , ** kwargs ) : initializer = _load_class ( getattr ( settings , "SEARCH_INITIALIZER" , None ) , cls ) ( ) return initializer . initialize ( ** kwargs )
5906	def create_portable_topology ( topol , struct , ** kwargs ) : _topoldir , _topol = os . path . split ( topol ) processed = kwargs . pop ( 'processed' , os . path . join ( _topoldir , 'pp_' + _topol ) ) grompp_kwargs , mdp_kwargs = filter_grompp_options ( ** kwargs ) mdp_kwargs = add_mdp_includes ( topol , mdp_kwargs ) with tempfile . NamedTemporaryFile ( suffix = '.mdp' ) as mdp : mdp . write ( '; empty mdp file\ninclude = {include!s}\n' . format ( ** mdp_kwargs ) ) mdp . flush ( ) grompp_kwargs [ 'p' ] = topol grompp_kwargs [ 'pp' ] = processed grompp_kwargs [ 'f' ] = mdp . name grompp_kwargs [ 'c' ] = struct grompp_kwargs [ 'v' ] = False try : gromacs . grompp ( ** grompp_kwargs ) finally : utilities . unlink_gmx ( 'topol.tpr' , 'mdout.mdp' ) return utilities . realpath ( processed )
7680	def beat_position ( annotation , ** kwargs ) : times , values = annotation . to_interval_values ( ) labels = [ _ [ 'position' ] for _ in values ] return mir_eval . display . events ( times , labels = labels , ** kwargs )
5388	def _task_sort_function ( task ) : return ( task . get_field ( 'create-time' ) , int ( task . get_field ( 'task-id' , 0 ) ) , int ( task . get_field ( 'task-attempt' , 0 ) ) )
9728	def get_analog ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . device_count ) : component_position , device = QRTPacket . _get_exact ( RTAnalogDevice , data , component_position ) if device . sample_count > 0 : component_position , sample_number = QRTPacket . _get_exact ( RTSampleNumber , data , component_position ) RTAnalogChannel . format = struct . Struct ( RTAnalogChannel . format_str % device . sample_count ) for _ in range ( device . channel_count ) : component_position , channel = QRTPacket . _get_tuple ( RTAnalogChannel , data , component_position ) append_components ( ( device , sample_number , channel ) ) return components
4926	def transform_title ( self , content_metadata_item ) : title_with_locales = [ ] for locale in self . enterprise_configuration . get_locales ( ) : title_with_locales . append ( { 'locale' : locale , 'value' : content_metadata_item . get ( 'title' , '' ) } ) return title_with_locales
7245	def _tile_coords ( self , bounds ) : tfm = partial ( pyproj . transform , pyproj . Proj ( init = "epsg:3857" ) , pyproj . Proj ( init = "epsg:4326" ) ) bounds = ops . transform ( tfm , box ( * bounds ) ) . bounds west , south , east , north = bounds epsilon = 1.0e-10 if east != west and north != south : west += epsilon south += epsilon east -= epsilon north -= epsilon params = [ west , south , east , north , [ self . zoom_level ] ] tile_coords = [ ( tile . x , tile . y ) for tile in mercantile . tiles ( * params ) ] xtiles , ytiles = zip ( * tile_coords ) minx = min ( xtiles ) miny = min ( ytiles ) maxx = max ( xtiles ) maxy = max ( ytiles ) return minx , miny , maxx , maxy
8052	def parse_theme ( self , xml ) : kt = KulerTheme ( ) kt . author = xml . getElementsByTagName ( "author" ) [ 0 ] kt . author = kt . author . childNodes [ 1 ] . childNodes [ 0 ] . nodeValue kt . id = int ( self . parse_tag ( xml , "id" ) ) kt . label = self . parse_tag ( xml , "label" ) mode = self . parse_tag ( xml , "mode" ) for swatch in xml . getElementsByTagName ( "swatch" ) : c1 = float ( self . parse_tag ( swatch , "c1" ) ) c2 = float ( self . parse_tag ( swatch , "c2" ) ) c3 = float ( self . parse_tag ( swatch , "c3" ) ) c4 = float ( self . parse_tag ( swatch , "c4" ) ) if mode == "rgb" : kt . append ( ( c1 , c2 , c3 ) ) if mode == "cmyk" : kt . append ( cmyk_to_rgb ( c1 , c2 , c3 , c4 ) ) if mode == "hsv" : kt . append ( colorsys . hsv_to_rgb ( c1 , c2 , c3 ) ) if mode == "hex" : kt . append ( hex_to_rgb ( c1 ) ) if mode == "lab" : kt . append ( lab_to_rgb ( c1 , c2 , c3 ) ) if self . _cache . exists ( self . id_string + str ( kt . id ) ) : xml = self . _cache . read ( self . id_string + str ( kt . id ) ) xml = minidom . parseString ( xml ) for tags in xml . getElementsByTagName ( "tag" ) : tags = self . parse_tag ( tags , "label" ) tags = tags . split ( " " ) kt . tags . extend ( tags ) return kt
12366	def update ( self , id , name ) : return super ( Keys , self ) . update ( id , name = name )
8150	def _frame_limit ( self , start_time ) : if self . _speed : completion_time = time ( ) exc_time = completion_time - start_time sleep_for = ( 1.0 / abs ( self . _speed ) ) - exc_time if sleep_for > 0 : sleep ( sleep_for )
4011	def get_dusty_containers ( services , include_exited = False ) : client = get_docker_client ( ) if services : containers = [ get_container_for_app_or_service ( service , include_exited = include_exited ) for service in services ] return [ container for container in containers if container ] else : return [ container for container in client . containers ( all = include_exited ) if any ( name . startswith ( '/dusty' ) for name in container . get ( 'Names' , [ ] ) ) ]
8828	def sg_gather_associated_ports ( context , group ) : if not group : return None if not hasattr ( group , "ports" ) or len ( group . ports ) <= 0 : return [ ] return group . ports
2560	def heartbeat ( self ) : heartbeat = ( HEARTBEAT_CODE ) . to_bytes ( 4 , "little" ) r = self . task_incoming . send ( heartbeat ) logger . debug ( "Return from heartbeat : {}" . format ( r ) )
2863	def ping ( self ) : self . _idle ( ) self . _transaction_start ( ) self . _i2c_start ( ) self . _i2c_write_bytes ( [ self . _address_byte ( False ) ] ) self . _i2c_stop ( ) response = self . _transaction_end ( ) if len ( response ) != 1 : raise RuntimeError ( 'Expected 1 response byte but received {0} byte(s).' . format ( len ( response ) ) ) return ( ( response [ 0 ] & 0x01 ) == 0x00 )
1667	def IsBlockInNameSpace ( nesting_state , is_forward_declaration ) : if is_forward_declaration : return len ( nesting_state . stack ) >= 1 and ( isinstance ( nesting_state . stack [ - 1 ] , _NamespaceInfo ) ) return ( len ( nesting_state . stack ) > 1 and nesting_state . stack [ - 1 ] . check_namespace_indentation and isinstance ( nesting_state . stack [ - 2 ] , _NamespaceInfo ) )
11887	def receive ( self ) : try : buffer = self . _socket . recv ( BUFFER_SIZE ) except socket . timeout as error : _LOGGER . error ( "Error receiving: %s" , error ) return "" buffering = True response = '' while buffering : if '\n' in buffer . decode ( "utf8" ) : response = buffer . decode ( "utf8" ) . split ( '\n' ) [ 0 ] buffering = False else : try : more = self . _socket . recv ( BUFFER_SIZE ) except socket . timeout : more = None if not more : buffering = False response = buffer . decode ( "utf8" ) else : buffer += more return response
5772	def _bcrypt_verify ( certificate_or_public_key , signature , data , hash_algorithm , rsa_pss_padding = False ) : if hash_algorithm == 'raw' : digest = data else : hash_constant = { 'md5' : BcryptConst . BCRYPT_MD5_ALGORITHM , 'sha1' : BcryptConst . BCRYPT_SHA1_ALGORITHM , 'sha256' : BcryptConst . BCRYPT_SHA256_ALGORITHM , 'sha384' : BcryptConst . BCRYPT_SHA384_ALGORITHM , 'sha512' : BcryptConst . BCRYPT_SHA512_ALGORITHM } [ hash_algorithm ] digest = getattr ( hashlib , hash_algorithm ) ( data ) . digest ( ) padding_info = null ( ) flags = 0 if certificate_or_public_key . algorithm == 'rsa' : if rsa_pss_padding : flags = BcryptConst . BCRYPT_PAD_PSS padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_PSS_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) hash_buffer = buffer_from_unicode ( hash_constant ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info_struct . cbSalt = len ( digest ) else : flags = BcryptConst . BCRYPT_PAD_PKCS1 padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_PKCS1_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) if hash_algorithm == 'raw' : padding_info_struct . pszAlgId = null ( ) else : hash_buffer = buffer_from_unicode ( hash_constant ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info = cast ( bcrypt , 'void *' , padding_info_struct_pointer ) else : try : signature = algos . DSASignature . load ( signature ) . to_p1363 ( ) except ( ValueError , OverflowError , TypeError ) : raise SignatureError ( 'Signature is invalid' ) res = bcrypt . BCryptVerifySignature ( certificate_or_public_key . key_handle , padding_info , digest , len ( digest ) , signature , len ( signature ) , flags ) failure = res == BcryptConst . STATUS_INVALID_SIGNATURE failure = failure or res == BcryptConst . STATUS_INVALID_PARAMETER if failure : raise SignatureError ( 'Signature is invalid' ) handle_error ( res )
651	def sameTMParams ( tp1 , tp2 ) : result = True for param in [ "numberOfCols" , "cellsPerColumn" , "initialPerm" , "connectedPerm" , "minThreshold" , "newSynapseCount" , "permanenceInc" , "permanenceDec" , "permanenceMax" , "globalDecay" , "activationThreshold" , "doPooling" , "segUpdateValidDuration" , "burnIn" , "pamLength" , "maxAge" ] : if getattr ( tp1 , param ) != getattr ( tp2 , param ) : print param , "is different" print getattr ( tp1 , param ) , "vs" , getattr ( tp2 , param ) result = False return result
10651	def get_activity ( self , name ) : return [ a for a in self . activities if a . name == name ] [ 0 ]
2486	def licenses_from_tree ( self , tree ) : licenses = set ( ) self . licenses_from_tree_helper ( tree , licenses ) return licenses
3934	def _get_session_cookies ( session , access_token ) : headers = { 'Authorization' : 'Bearer {}' . format ( access_token ) } try : r = session . get ( ( 'https://accounts.google.com/accounts/OAuthLogin' '?source=hangups&issueuberauth=1' ) , headers = headers ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'OAuthLogin request failed: {}' . format ( e ) ) uberauth = r . text try : r = session . get ( ( 'https://accounts.google.com/MergeSession?' 'service=mail&' 'continue=http://www.google.com&uberauth={}' ) . format ( uberauth ) , headers = headers ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'MergeSession request failed: {}' . format ( e ) ) cookies = session . cookies . get_dict ( domain = '.google.com' ) if cookies == { } : raise GoogleAuthError ( 'Failed to find session cookies' ) return cookies
5970	def MD_restrained ( dirname = 'MD_POSRES' , ** kwargs ) : logger . info ( "[{dirname!s}] Setting up MD with position restraints..." . format ( ** vars ( ) ) ) kwargs . setdefault ( 'struct' , 'em/em.pdb' ) kwargs . setdefault ( 'qname' , 'PR_GMX' ) kwargs . setdefault ( 'define' , '-DPOSRES' ) kwargs . setdefault ( 'nstxout' , '50000' ) kwargs . setdefault ( 'nstvout' , '50000' ) kwargs . setdefault ( 'nstfout' , '0' ) kwargs . setdefault ( 'nstlog' , '500' ) kwargs . setdefault ( 'nstenergy' , '2500' ) kwargs . setdefault ( 'nstxtcout' , '5000' ) kwargs . setdefault ( 'refcoord_scaling' , 'com' ) kwargs . setdefault ( 'Pcoupl' , "Berendsen" ) new_kwargs = _setup_MD ( dirname , ** kwargs ) new_kwargs . pop ( 'define' , None ) new_kwargs . pop ( 'refcoord_scaling' , None ) new_kwargs . pop ( 'Pcoupl' , None ) return new_kwargs
7117	def smush_config ( sources , initial = None ) : if initial is None : initial = { } config = DotDict ( initial ) for fn in sources : log . debug ( 'Merging %s' , fn ) mod = get_config_module ( fn ) config = mod . update ( config ) log . debug ( 'Current config:\n%s' , json . dumps ( config , indent = 4 , cls = LenientJSONEncoder ) ) return config
12792	def post ( self , url = None , post_data = { } , parse_data = False , key = None , parameters = None , listener = None ) : return self . _fetch ( "POST" , url , post_data = post_data , parse_data = parse_data , key = key , parameters = parameters , listener = listener , full_return = True )
8663	def generate_passphrase ( size = 12 ) : chars = string . ascii_lowercase + string . ascii_uppercase + string . digits return str ( '' . join ( random . choice ( chars ) for _ in range ( size ) ) )
6196	def compact_name ( self , hashsize = 6 ) : s = self . compact_name_core ( hashsize , t_max = True ) s += "_ID%d-%d" % ( self . ID , self . EID ) return s
4266	def set_meta ( target , keys , overwrite = False ) : if not os . path . exists ( target ) : sys . stderr . write ( "The target {} does not exist.\n" . format ( target ) ) sys . exit ( 1 ) if len ( keys ) < 2 or len ( keys ) % 2 > 0 : sys . stderr . write ( "Need an even number of arguments.\n" ) sys . exit ( 1 ) if os . path . isdir ( target ) : descfile = os . path . join ( target , 'index.md' ) else : descfile = os . path . splitext ( target ) [ 0 ] + '.md' if os . path . exists ( descfile ) and not overwrite : sys . stderr . write ( "Description file '{}' already exists. " "Use --overwrite to overwrite it.\n" . format ( descfile ) ) sys . exit ( 2 ) with open ( descfile , "w" ) as fp : for i in range ( len ( keys ) // 2 ) : k , v = keys [ i * 2 : ( i + 1 ) * 2 ] fp . write ( "{}: {}\n" . format ( k . capitalize ( ) , v ) ) print ( "{} metadata key(s) written to {}" . format ( len ( keys ) // 2 , descfile ) )
6703	def enter_password_change ( self , username = None , old_password = None ) : from fabric . state import connections from fabric . network import disconnect_all r = self . local_renderer r . genv . user = r . genv . user or username r . pc ( 'Changing password for user {user} via interactive prompts.' ) r . env . old_password = r . env . default_passwords [ self . genv . user ] r . env . new_password = self . env . passwords [ self . genv . user ] if old_password : r . env . old_password = old_password prompts = { '(current) UNIX password: ' : r . env . old_password , 'Enter new UNIX password: ' : r . env . new_password , 'Retype new UNIX password: ' : r . env . new_password , } print ( 'prompts:' , prompts ) r . env . password = r . env . old_password with self . settings ( warn_only = True ) : ret = r . _local ( "sshpass -p '{password}' ssh -o StrictHostKeyChecking=no {user}@{host_string} echo hello" , capture = True ) if ret . return_code in ( 1 , 6 ) or 'hello' in ret : self . genv . password = r . env . old_password elif self . genv . user in self . genv . user_passwords : self . genv . password = r . env . new_password else : self . genv . password = None print ( 'using password:' , self . genv . password ) with self . settings ( prompts = prompts ) : ret = r . _run ( 'echo checking for expired password' ) print ( 'ret:[%s]' % ret ) do_disconnect = 'passwd: password updated successfully' in ret print ( 'do_disconnect:' , do_disconnect ) if do_disconnect : disconnect_all ( ) self . genv . password = r . env . new_password
3933	def _make_token_request ( session , token_request_data ) : try : r = session . post ( OAUTH2_TOKEN_REQUEST_URL , data = token_request_data ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'Token request failed: {}' . format ( e ) ) else : res = r . json ( ) if 'error' in res : raise GoogleAuthError ( 'Token request error: {!r}' . format ( res [ 'error' ] ) ) return res
5030	def get_learner_data_records ( self , enterprise_enrollment , completed_date = None , is_passing = False , ** kwargs ) : completed_timestamp = completed_date . strftime ( "%F" ) if isinstance ( completed_date , datetime ) else None if enterprise_enrollment . enterprise_customer_user . get_remote_id ( ) is not None : DegreedLearnerDataTransmissionAudit = apps . get_model ( 'degreed' , 'DegreedLearnerDataTransmissionAudit' ) return [ DegreedLearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , degreed_user_email = enterprise_enrollment . enterprise_customer_user . user_email , course_id = parse_course_key ( enterprise_enrollment . course_id ) , course_completed = completed_date is not None and is_passing , completed_timestamp = completed_timestamp , ) , DegreedLearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , degreed_user_email = enterprise_enrollment . enterprise_customer_user . user_email , course_id = enterprise_enrollment . course_id , course_completed = completed_date is not None and is_passing , completed_timestamp = completed_timestamp , ) ] else : LOGGER . debug ( 'No learner data was sent for user [%s] because a Degreed user ID could not be found.' , enterprise_enrollment . enterprise_customer_user . username )
12938	def setDefaultRedisConnectionParams ( connectionParams ) : global _defaultRedisConnectionParams _defaultRedisConnectionParams . clear ( ) for key , value in connectionParams . items ( ) : _defaultRedisConnectionParams [ key ] = value clearRedisPools ( )
6383	def fingerprint ( self , word ) : word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = '' . join ( c for c in word if c in self . _letters ) start = word [ 0 : 1 ] consonant_part = '' vowel_part = '' for char in word [ 1 : ] : if char != start : if char in self . _vowels : if char not in vowel_part : vowel_part += char elif char not in consonant_part : consonant_part += char return start + consonant_part + vowel_part
1997	def sync_svc ( state ) : syscall = state . cpu . R7 name = linux_syscalls . armv7 [ syscall ] logger . debug ( f"Syncing syscall: {name}" ) try : if 'mmap' in name : returned = gdb . getR ( 'R0' ) logger . debug ( f"Syncing mmap ({returned:x})" ) state . cpu . write_register ( 'R0' , returned ) if 'exit' in name : return except ValueError : for reg in state . cpu . canonical_registers : print ( f'{reg}: {state.cpu.read_register(reg):x}' ) raise
11471	def upload ( self , filename , location = '' ) : current_folder = self . _ftp . pwd ( ) self . mkdir ( location ) self . cd ( location ) fl = open ( filename , 'rb' ) filename = filename . split ( '/' ) [ - 1 ] self . _ftp . storbinary ( 'STOR %s' % filename , fl ) fl . close ( ) self . cd ( current_folder )
12355	def delete ( self , wait = True ) : resp = self . parent . delete ( self . id ) if wait : self . wait ( ) return resp
8651	def get_jobs ( session , job_ids , seo_details , lang ) : get_jobs_data = { 'jobs[]' : job_ids , 'seo_details' : seo_details , 'lang' : lang , } response = make_get_request ( session , 'jobs' , params_data = get_jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise JobsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
13903	def coerce ( value1 , value2 , default = None ) : if value1 is not NoSet : return value1 elif value2 is not NoSet : return value2 else : return default
3086	def _is_ndb ( self ) : if isinstance ( self . _model , type ) : if _NDB_MODEL is not None and issubclass ( self . _model , _NDB_MODEL ) : return True elif issubclass ( self . _model , db . Model ) : return False raise TypeError ( 'Model class not an NDB or DB model: {0}.' . format ( self . _model ) )
128	def project ( self , from_shape , to_shape ) : if from_shape [ 0 : 2 ] == to_shape [ 0 : 2 ] : return self . copy ( ) ls_proj = self . to_line_string ( closed = False ) . project ( from_shape , to_shape ) return self . copy ( exterior = ls_proj . coords )
2346	def predict_proba ( self , a , b , device = None ) : device = SETTINGS . get_default ( device = device ) if self . model is None : print ( 'Model has to be trained before doing any predictions' ) raise ValueError if len ( np . array ( a ) . shape ) == 1 : a = np . array ( a ) . reshape ( ( - 1 , 1 ) ) b = np . array ( b ) . reshape ( ( - 1 , 1 ) ) m = np . hstack ( ( a , b ) ) m = scale ( m ) m = m . astype ( 'float32' ) m = th . from_numpy ( m ) . t ( ) . unsqueeze ( 0 ) if th . cuda . is_available ( ) : m = m . cuda ( ) return ( self . model ( m ) . data . cpu ( ) . numpy ( ) - .5 ) * 2
5567	def area_at_zoom ( self , zoom = None ) : if zoom is None : if not self . _cache_full_process_area : logger . debug ( "calculate process area ..." ) self . _cache_full_process_area = cascaded_union ( [ self . _area_at_zoom ( z ) for z in self . init_zoom_levels ] ) . buffer ( 0 ) return self . _cache_full_process_area else : if zoom not in self . init_zoom_levels : raise ValueError ( "zoom level not available with current configuration" ) return self . _area_at_zoom ( zoom )
12655	def remove_dcm2nii_underprocessed ( filepaths ) : cln_flist = [ ] len_sorted = sorted ( filepaths , key = len ) for idx , fpath in enumerate ( len_sorted ) : remove = False fname = op . basename ( fpath ) rest = len_sorted [ idx + 1 : ] for rest_fpath in rest : rest_file = op . basename ( rest_fpath ) if rest_file . endswith ( fname ) : remove = True break if not remove : cln_flist . append ( fpath ) return cln_flist
1381	def getComponentExceptionSummary ( self , tmaster , component_name , instances = [ ] , callback = None ) : if not tmaster or not tmaster . host or not tmaster . stats_port : return exception_request = tmaster_pb2 . ExceptionLogRequest ( ) exception_request . component_name = component_name if len ( instances ) > 0 : exception_request . instances . extend ( instances ) request_str = exception_request . SerializeToString ( ) port = str ( tmaster . stats_port ) host = tmaster . host url = "http://{0}:{1}/exceptionsummary" . format ( host , port ) Log . debug ( "Creating request object." ) request = tornado . httpclient . HTTPRequest ( url , body = request_str , method = 'POST' , request_timeout = 5 ) Log . debug ( 'Making HTTP call to fetch exceptionsummary url: %s' , url ) try : client = tornado . httpclient . AsyncHTTPClient ( ) result = yield client . fetch ( request ) Log . debug ( "HTTP call complete." ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) ) responseCode = result . code if responseCode >= 400 : message = "Error in getting exceptions from Tmaster, code: " + responseCode Log . error ( message ) raise tornado . gen . Return ( { "message" : message } ) exception_response = tmaster_pb2 . ExceptionLogResponse ( ) exception_response . ParseFromString ( result . body ) if exception_response . status . status == common_pb2 . NOTOK : if exception_response . status . HasField ( "message" ) : raise tornado . gen . Return ( { "message" : exception_response . status . message } ) ret = [ ] for exception_log in exception_response . exceptions : ret . append ( { 'class_name' : exception_log . stacktrace , 'lasttime' : exception_log . lasttime , 'firsttime' : exception_log . firsttime , 'count' : str ( exception_log . count ) } ) raise tornado . gen . Return ( ret )
3405	def _sample_chain ( args ) : n , idx = args center = sampler . center np . random . seed ( ( sampler . _seed + idx ) % np . iinfo ( np . int32 ) . max ) pi = np . random . randint ( sampler . n_warmup ) prev = sampler . warmup [ pi , ] prev = step ( sampler , center , prev - center , 0.95 ) n_samples = max ( sampler . n_samples , 1 ) samples = np . zeros ( ( n , center . shape [ 0 ] ) ) for i in range ( 1 , sampler . thinning * n + 1 ) : pi = np . random . randint ( sampler . n_warmup ) delta = sampler . warmup [ pi , ] - center prev = step ( sampler , prev , delta ) if sampler . problem . homogeneous and ( n_samples * sampler . thinning % sampler . nproj == 0 ) : prev = sampler . _reproject ( prev ) center = sampler . _reproject ( center ) if i % sampler . thinning == 0 : samples [ i // sampler . thinning - 1 , ] = prev center = ( ( n_samples * center ) / ( n_samples + 1 ) + prev / ( n_samples + 1 ) ) n_samples += 1 return ( sampler . retries , samples )
12563	def get_rois_centers_of_mass ( vol ) : from scipy . ndimage . measurements import center_of_mass roisvals = np . unique ( vol ) roisvals = roisvals [ roisvals != 0 ] rois_centers = OrderedDict ( ) for r in roisvals : rois_centers [ r ] = center_of_mass ( vol , vol , r ) return rois_centers
7918	def _validate_ip_address ( family , address ) : try : info = socket . getaddrinfo ( address , 0 , family , socket . SOCK_STREAM , 0 , socket . AI_NUMERICHOST ) except socket . gaierror , err : logger . debug ( "gaierror: {0} for {1!r}" . format ( err , address ) ) raise ValueError ( "Bad IP address" ) if not info : logger . debug ( "getaddrinfo result empty" ) raise ValueError ( "Bad IP address" ) addr = info [ 0 ] [ 4 ] logger . debug ( " got address: {0!r}" . format ( addr ) ) try : return socket . getnameinfo ( addr , socket . NI_NUMERICHOST ) [ 0 ] except socket . gaierror , err : logger . debug ( "gaierror: {0} for {1!r}" . format ( err , addr ) ) raise ValueError ( "Bad IP address" )
3562	def find_characteristic ( self , uuid ) : for char in self . list_characteristics ( ) : if char . uuid == uuid : return char return None
6955	def _get_value ( quantitystr , fitparams , fixedparams ) : fitparamskeys , fixedparamskeys = fitparams . keys ( ) , fixedparams . keys ( ) if quantitystr in fitparamskeys : quantity = fitparams [ quantitystr ] elif quantitystr in fixedparamskeys : quantity = fixedparams [ quantitystr ] return quantity
6847	def purge_keys ( self ) : r = self . local_renderer r . env . default_ip = self . hostname_to_ip ( self . env . default_hostname ) r . env . home_dir = '/home/%s' % getpass . getuser ( ) r . local ( 'ssh-keygen -f "{home_dir}/.ssh/known_hosts" -R {host_string}' ) if self . env . default_hostname : r . local ( 'ssh-keygen -f "{home_dir}/.ssh/known_hosts" -R {default_hostname}' ) if r . env . default_ip : r . local ( 'ssh-keygen -f "{home_dir}/.ssh/known_hosts" -R {default_ip}' )
9112	def replies ( self ) : fs_reply_path = join ( self . fs_replies_path , 'message_001.txt' ) if exists ( fs_reply_path ) : return [ load ( open ( fs_reply_path , 'r' ) ) ] else : return [ ]
6826	def clone ( self , remote_url , path = None , use_sudo = False , user = None ) : cmd = 'git clone --quiet %s' % remote_url if path is not None : cmd = cmd + ' %s' % path if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
7410	def count_var ( nex ) : arr = np . array ( [ list ( i . split ( ) [ - 1 ] ) for i in nex ] ) miss = np . any ( arr == "N" , axis = 0 ) nomiss = arr [ : , ~ miss ] nsnps = np . invert ( np . all ( nomiss == nomiss [ 0 , : ] , axis = 0 ) ) . sum ( ) return nomiss . shape [ 1 ] , nsnps
5944	def isstream ( obj ) : signature_methods = ( "close" , ) alternative_methods = ( ( "read" , "readline" , "readlines" ) , ( "write" , "writeline" , "writelines" ) ) for m in signature_methods : if not hasmethod ( obj , m ) : return False alternative_results = [ numpy . all ( [ hasmethod ( obj , m ) for m in alternatives ] ) for alternatives in alternative_methods ] return numpy . any ( alternative_results )
9987	def to_frame ( self , * args ) : if sys . version_info < ( 3 , 6 , 0 ) : from collections import OrderedDict impls = OrderedDict ( ) for name , obj in self . items ( ) : impls [ name ] = obj . _impl else : impls = get_impls ( self ) return _to_frame_inner ( impls , args )
7997	def set_peer_authenticated ( self , peer , restart_stream = False ) : with self . lock : self . peer_authenticated = True self . peer = peer if restart_stream : self . _restart_stream ( ) self . event ( AuthenticatedEvent ( self . peer ) )
4280	def watermark ( im , mark , position , opacity = 1 ) : if opacity < 1 : mark = reduce_opacity ( mark , opacity ) if im . mode != 'RGBA' : im = im . convert ( 'RGBA' ) layer = Image . new ( 'RGBA' , im . size , ( 0 , 0 , 0 , 0 ) ) if position == 'tile' : for y in range ( 0 , im . size [ 1 ] , mark . size [ 1 ] ) : for x in range ( 0 , im . size [ 0 ] , mark . size [ 0 ] ) : layer . paste ( mark , ( x , y ) ) elif position == 'scale' : ratio = min ( float ( im . size [ 0 ] ) / mark . size [ 0 ] , float ( im . size [ 1 ] ) / mark . size [ 1 ] ) w = int ( mark . size [ 0 ] * ratio ) h = int ( mark . size [ 1 ] * ratio ) mark = mark . resize ( ( w , h ) ) layer . paste ( mark , ( int ( ( im . size [ 0 ] - w ) / 2 ) , int ( ( im . size [ 1 ] - h ) / 2 ) ) ) else : layer . paste ( mark , position ) return Image . composite ( layer , im , layer )
6779	def get_deploy_funcs ( components , current_thumbprint , previous_thumbprint , preview = False ) : for component in components : funcs = manifest_deployers . get ( component , [ ] ) for func_name in funcs : if func_name . startswith ( 'burlap.' ) : print ( 'skipping %s' % func_name ) continue takes_diff = manifest_deployers_takes_diff . get ( func_name , False ) func = resolve_deployer ( func_name ) current = current_thumbprint . get ( component ) last = previous_thumbprint . get ( component ) if takes_diff : yield func_name , partial ( func , last = last , current = current ) else : yield func_name , partial ( func )
1910	def run ( self , procs = 1 , timeout = None , should_profile = False ) : assert not self . running , "Manticore is already running." self . _start_run ( ) self . _last_run_stats [ 'time_started' ] = time . time ( ) with self . shutdown_timeout ( timeout ) : self . _start_workers ( procs , profiling = should_profile ) self . _join_workers ( ) self . _finish_run ( profiling = should_profile )
13448	def authed_get ( self , url , response_code = 200 , headers = { } , follow = False ) : if not self . authed : self . authorize ( ) response = self . client . get ( url , follow = follow , ** headers ) self . assertEqual ( response_code , response . status_code ) return response
11942	def stored_messages_archive ( context , num_elements = 10 ) : if "user" in context : user = context [ "user" ] if user . is_authenticated ( ) : qs = MessageArchive . objects . select_related ( "message" ) . filter ( user = user ) return { "messages" : qs [ : num_elements ] , "count" : qs . count ( ) , }
11722	def app_class ( ) : try : pkg_resources . get_distribution ( 'invenio-files-rest' ) from invenio_files_rest . app import Flask as FlaskBase except pkg_resources . DistributionNotFound : from flask import Flask as FlaskBase class Request ( TrustedHostsMixin , FlaskBase . request_class ) : pass class Flask ( FlaskBase ) : request_class = Request return Flask
5274	def find ( self , y ) : node = self . root while True : edge = self . _edgeLabel ( node , node . parent ) if edge . startswith ( y ) : return node . idx i = 0 while ( i < len ( edge ) and edge [ i ] == y [ 0 ] ) : y = y [ 1 : ] i += 1 if i != 0 : if i == len ( edge ) and y != '' : pass else : return - 1 node = node . _get_transition_link ( y [ 0 ] ) if not node : return - 1
5062	def get_enterprise_customer_user ( user_id , enterprise_uuid ) : EnterpriseCustomerUser = apps . get_model ( 'enterprise' , 'EnterpriseCustomerUser' ) try : return EnterpriseCustomerUser . objects . get ( enterprise_customer__uuid = enterprise_uuid , user_id = user_id ) except EnterpriseCustomerUser . DoesNotExist : return None
5781	def _create_buffers ( self , number ) : buffers = new ( secur32 , 'SecBuffer[%d]' % number ) for index in range ( 0 , number ) : buffers [ index ] . cbBuffer = 0 buffers [ index ] . BufferType = Secur32Const . SECBUFFER_EMPTY buffers [ index ] . pvBuffer = null ( ) sec_buffer_desc_pointer = struct ( secur32 , 'SecBufferDesc' ) sec_buffer_desc = unwrap ( sec_buffer_desc_pointer ) sec_buffer_desc . ulVersion = Secur32Const . SECBUFFER_VERSION sec_buffer_desc . cBuffers = number sec_buffer_desc . pBuffers = buffers return ( sec_buffer_desc_pointer , buffers )
9236	def _signal_handler_map ( self ) : result = { } for signum , handler in self . signal_map . items ( ) : result [ signum ] = self . _get_signal_handler ( handler ) return result
5577	def load_input_reader ( input_params , readonly = False ) : logger . debug ( "find input reader with params %s" , input_params ) if not isinstance ( input_params , dict ) : raise TypeError ( "input_params must be a dictionary" ) if "abstract" in input_params : driver_name = input_params [ "abstract" ] [ "format" ] elif "path" in input_params : if os . path . splitext ( input_params [ "path" ] ) [ 1 ] : input_file = input_params [ "path" ] driver_name = driver_from_file ( input_file ) else : logger . debug ( "%s is a directory" , input_params [ "path" ] ) driver_name = "TileDirectory" else : raise MapcheteDriverError ( "invalid input parameters %s" % input_params ) for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : driver_ = v . load ( ) if hasattr ( driver_ , "METADATA" ) and ( driver_ . METADATA [ "driver_name" ] == driver_name ) : return v . load ( ) . InputData ( input_params , readonly = readonly ) raise MapcheteDriverError ( "no loader for driver '%s' could be found." % driver_name )
1024	def needsquoting ( c , quotetabs , header ) : if c in ' \t' : return quotetabs if c == '_' : return header return c == ESCAPE or not ( ' ' <= c <= '~' )
10673	def load_data_factsage ( path = '' ) : compounds . clear ( ) if path == '' : path = default_data_path if not os . path . exists ( path ) : warnings . warn ( 'The specified data file path does not exist. (%s)' % path ) return files = glob . glob ( os . path . join ( path , 'Compound_*.txt' ) ) for file in files : compound = Compound ( _read_compound_from_factsage_file_ ( file ) ) compounds [ compound . formula ] = compound
11387	def call_path ( self , basepath ) : rel_filepath = self . path if basepath : rel_filepath = os . path . relpath ( self . path , basepath ) basename = self . name if basename in set ( [ '__init__.py' , '__main__.py' ] ) : rel_filepath = os . path . dirname ( rel_filepath ) return rel_filepath
8438	def _patched_run_hook ( hook_name , project_dir , context ) : if hook_name == 'post_gen_project' : with temple . utils . cd ( project_dir ) : temple . utils . write_temple_config ( context [ 'cookiecutter' ] , context [ 'template' ] , context [ 'version' ] ) return cc_hooks . run_hook ( hook_name , project_dir , context )
7014	def concatenate_textlcs ( lclist , sortby = 'rjd' , normalize = True ) : lcdict = read_hatpi_textlc ( lclist [ 0 ] ) lccounter = 0 lcdict [ 'concatenated' ] = { lccounter : os . path . abspath ( lclist [ 0 ] ) } lcdict [ 'lcn' ] = np . full_like ( lcdict [ 'rjd' ] , lccounter ) if normalize : for col in MAGCOLS : if col in lcdict : thismedval = np . nanmedian ( lcdict [ col ] ) if col in ( 'ifl1' , 'ifl2' , 'ifl3' ) : lcdict [ col ] = lcdict [ col ] / thismedval else : lcdict [ col ] = lcdict [ col ] - thismedval for lcf in lclist [ 1 : ] : thislcd = read_hatpi_textlc ( lcf ) if thislcd [ 'columns' ] != lcdict [ 'columns' ] : LOGERROR ( 'file %s does not have the ' 'same columns as first file %s, skipping...' % ( lcf , lclist [ 0 ] ) ) continue else : LOGINFO ( 'adding %s (ndet: %s) to %s (ndet: %s)' % ( lcf , thislcd [ 'objectinfo' ] [ 'ndet' ] , lclist [ 0 ] , lcdict [ lcdict [ 'columns' ] [ 0 ] ] . size ) ) lccounter = lccounter + 1 lcdict [ 'concatenated' ] [ lccounter ] = os . path . abspath ( lcf ) lcdict [ 'lcn' ] = np . concatenate ( ( lcdict [ 'lcn' ] , np . full_like ( thislcd [ 'rjd' ] , lccounter ) ) ) for col in lcdict [ 'columns' ] : if normalize and col in MAGCOLS : thismedval = np . nanmedian ( thislcd [ col ] ) if col in ( 'ifl1' , 'ifl2' , 'ifl3' ) : thislcd [ col ] = thislcd [ col ] / thismedval else : thislcd [ col ] = thislcd [ col ] - thismedval lcdict [ col ] = np . concatenate ( ( lcdict [ col ] , thislcd [ col ] ) ) lcdict [ 'objectinfo' ] [ 'ndet' ] = lcdict [ lcdict [ 'columns' ] [ 0 ] ] . size lcdict [ 'objectinfo' ] [ 'stations' ] = [ 'HP%s' % x for x in np . unique ( lcdict [ 'stf' ] ) . tolist ( ) ] lcdict [ 'nconcatenated' ] = lccounter + 1 if sortby and sortby in [ x [ 0 ] for x in COLDEFS ] : LOGINFO ( 'sorting concatenated light curve by %s...' % sortby ) sortind = np . argsort ( lcdict [ sortby ] ) for col in lcdict [ 'columns' ] : lcdict [ col ] = lcdict [ col ] [ sortind ] lcdict [ 'lcn' ] = lcdict [ 'lcn' ] [ sortind ] LOGINFO ( 'done. concatenated light curve has %s detections' % lcdict [ 'objectinfo' ] [ 'ndet' ] ) return lcdict
5978	def mask_blurring_from_mask_and_psf_shape ( mask , psf_shape ) : blurring_mask = np . full ( mask . shape , True ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : for y1 in range ( ( - psf_shape [ 0 ] + 1 ) // 2 , ( psf_shape [ 0 ] + 1 ) // 2 ) : for x1 in range ( ( - psf_shape [ 1 ] + 1 ) // 2 , ( psf_shape [ 1 ] + 1 ) // 2 ) : if 0 <= x + x1 <= mask . shape [ 1 ] - 1 and 0 <= y + y1 <= mask . shape [ 0 ] - 1 : if mask [ y + y1 , x + x1 ] : blurring_mask [ y + y1 , x + x1 ] = False else : raise exc . MaskException ( "setup_blurring_mask extends beyond the sub_grid_size of the masks - pad the " "datas array before masking" ) return blurring_mask
3529	def get_user_from_context ( context ) : try : return context [ 'user' ] except KeyError : pass try : request = context [ 'request' ] return request . user except ( KeyError , AttributeError ) : pass return None
6504	def decorate_matches ( match_in , match_word ) : matches = re . finditer ( match_word , match_in , re . IGNORECASE ) for matched_string in set ( [ match . group ( ) for match in matches ] ) : match_in = match_in . replace ( matched_string , getattr ( settings , "SEARCH_MATCH_DECORATION" , u"<b>{}</b>" ) . format ( matched_string ) ) return match_in
13308	def gmv ( a , b ) : return np . exp ( np . square ( np . log ( a ) - np . log ( b ) ) . mean ( ) )
7843	def get_type ( self ) : item_type = self . xmlnode . prop ( "type" ) if not item_type : item_type = "?" return item_type . decode ( "utf-8" )
7484	def run2 ( data , samples , force , ipyclient ) : data . dirs . edits = os . path . join ( os . path . realpath ( data . paramsdict [ "project_dir" ] ) , data . name + "_edits" ) if not os . path . exists ( data . dirs . edits ) : os . makedirs ( data . dirs . edits ) subsamples = choose_samples ( samples , force ) if int ( data . paramsdict [ "filter_adapters" ] ) == 3 : if not data . _hackersonly [ "p3_adapters_extra" ] : for poly in [ "A" * 8 , "T" * 8 , "C" * 8 , "G" * 8 ] : data . _hackersonly [ "p3_adapters_extra" ] . append ( poly ) if not data . _hackersonly [ "p5_adapters_extra" ] : for poly in [ "A" * 8 , "T" * 8 , "C" * 8 , "G" * 8 ] : data . _hackersonly [ "p5_adapters_extra" ] . append ( poly ) else : data . _hackersonly [ "p5_adapters_extra" ] = [ ] data . _hackersonly [ "p3_adapters_extra" ] = [ ] subsamples = concat_reads ( data , subsamples , ipyclient ) lbview = ipyclient . load_balanced_view ( targets = ipyclient . ids [ : : 2 ] ) run_cutadapt ( data , subsamples , lbview ) assembly_cleanup ( data )
1549	def set_logging_level ( cl_args ) : if 'verbose' in cl_args and cl_args [ 'verbose' ] : configure ( logging . DEBUG ) else : configure ( logging . INFO )
9622	def buttons ( self ) : return [ name for name , value in rController . _buttons . items ( ) if self . gamepad . wButtons & value == value ]
13374	def binpath ( * paths ) : package_root = os . path . dirname ( __file__ ) return os . path . normpath ( os . path . join ( package_root , 'bin' , * paths ) )
12542	def get_attributes ( self , attributes , default = '' ) : if isinstance ( attributes , str ) : attributes = [ attributes ] attrs = [ getattr ( self , attr , default ) for attr in attributes ] if len ( attrs ) == 1 : return attrs [ 0 ] return tuple ( attrs )
3969	def _conditional_links ( assembled_specs , app_name ) : link_to_apps = [ ] potential_links = assembled_specs [ 'apps' ] [ app_name ] [ 'conditional_links' ] for potential_link in potential_links [ 'apps' ] : if potential_link in assembled_specs [ 'apps' ] : link_to_apps . append ( potential_link ) for potential_link in potential_links [ 'services' ] : if potential_link in assembled_specs [ 'services' ] : link_to_apps . append ( potential_link ) return link_to_apps
9489	def generate_bytecode_from_obb ( obb : object , previous : bytes ) -> bytes : if isinstance ( obb , pyte . superclasses . _PyteOp ) : return obb . to_bytes ( previous ) elif isinstance ( obb , ( pyte . superclasses . _PyteAugmentedComparator , pyte . superclasses . _PyteAugmentedValidator . _FakeMathematicalOP ) ) : return obb . to_bytes ( previous ) elif isinstance ( obb , pyte . superclasses . _PyteAugmentedValidator ) : obb . validate ( ) return obb . to_load ( ) elif isinstance ( obb , int ) : return obb . to_bytes ( ( obb . bit_length ( ) + 7 ) // 8 , byteorder = "little" ) or b'' elif isinstance ( obb , bytes ) : return obb else : raise TypeError ( "`{}` was not a valid bytecode-encodable item" . format ( obb ) )
10653	def run ( self , clock , generalLedger ) : for c in self . components : c . run ( clock , generalLedger ) for a in self . activities : a . run ( clock , generalLedger )
970	def _getEphemeralMembers ( self ) : e = BacktrackingTM . _getEphemeralMembers ( self ) if self . makeCells4Ephemeral : e . extend ( [ 'cells4' ] ) return e
13109	def r_annotations ( self ) : target = request . args . get ( "target" , None ) wildcard = request . args . get ( "wildcard" , "." , type = str ) include = request . args . get ( "include" ) exclude = request . args . get ( "exclude" ) limit = request . args . get ( "limit" , None , type = int ) start = request . args . get ( "start" , 1 , type = int ) expand = request . args . get ( "expand" , False , type = bool ) if target : try : urn = MyCapytain . common . reference . URN ( target ) except ValueError : return "invalid urn" , 400 count , annotations = self . __queryinterface__ . getAnnotations ( urn , wildcard = wildcard , include = include , exclude = exclude , limit = limit , start = start , expand = expand ) else : count , annotations = self . __queryinterface__ . getAnnotations ( None , limit = limit , start = start , expand = expand ) mapped = [ ] response = { "@context" : type ( self ) . JSONLD_CONTEXT , "id" : url_for ( ".r_annotations" , start = start , limit = limit ) , "type" : "AnnotationCollection" , "startIndex" : start , "items" : [ ] , "total" : count } for a in annotations : mapped . append ( { "id" : url_for ( ".r_annotation" , sha = a . sha ) , "body" : url_for ( ".r_annotation_body" , sha = a . sha ) , "type" : "Annotation" , "target" : a . target . to_json ( ) , "dc:type" : a . type_uri , "owl:sameAs" : [ a . uri ] , "nemo:slug" : a . slug } ) response [ "items" ] = mapped response = jsonify ( response ) return response
1871	def CWDE ( cpu ) : bit = Operators . EXTRACT ( cpu . AX , 15 , 1 ) cpu . EAX = Operators . SEXTEND ( cpu . AX , 16 , 32 ) cpu . EDX = Operators . SEXTEND ( bit , 1 , 32 )
11659	def inverse_transform ( self , X ) : X = check_array ( X , copy = self . copy ) X -= self . min_ X /= self . scale_ return X
1769	def concrete_emulate ( self , insn ) : if not self . emu : self . emu = ConcreteUnicornEmulator ( self ) self . emu . _stop_at = self . _break_unicorn_at try : self . emu . emulate ( insn ) except unicorn . UcError as e : if e . errno == unicorn . UC_ERR_INSN_INVALID : text_bytes = ' ' . join ( '%02x' % x for x in insn . bytes ) logger . error ( "Unimplemented instruction: 0x%016x:\t%s\t%s\t%s" , insn . address , text_bytes , insn . mnemonic , insn . op_str ) raise InstructionEmulationError ( str ( e ) )
11178	def get_separator ( self , i ) : return i and self . separator [ min ( i - 1 , len ( self . separator ) - 1 ) ] or ''
2556	def get ( self , tag = None , ** kwargs ) : if tag is None : tag = dom_tag attrs = [ ( dom_tag . clean_attribute ( attr ) , value ) for attr , value in kwargs . items ( ) ] results = [ ] for child in self . children : if ( isinstance ( tag , basestring ) and type ( child ) . __name__ == tag ) or ( not isinstance ( tag , basestring ) and isinstance ( child , tag ) ) : if all ( child . attributes . get ( attribute ) == value for attribute , value in attrs ) : results . append ( child ) if isinstance ( child , dom_tag ) : results . extend ( child . get ( tag , ** kwargs ) ) return results
6565	def xor_gate ( variables , vartype = dimod . BINARY , name = 'XOR' ) : variables = tuple ( variables ) if vartype is dimod . BINARY : configs = frozenset ( [ ( 0 , 0 , 0 ) , ( 0 , 1 , 1 ) , ( 1 , 0 , 1 ) , ( 1 , 1 , 0 ) ] ) def func ( in1 , in2 , out ) : return ( in1 != in2 ) == out else : configs = frozenset ( [ ( - 1 , - 1 , - 1 ) , ( - 1 , + 1 , + 1 ) , ( + 1 , - 1 , + 1 ) , ( + 1 , + 1 , - 1 ) ] ) def func ( in1 , in2 , out ) : return ( ( in1 > 0 ) != ( in2 > 0 ) ) == ( out > 0 ) return Constraint ( func , configs , variables , vartype = vartype , name = name )
2605	def make_hash ( self , task ) : t = [ serialize_object ( task [ 'func_name' ] ) [ 0 ] , serialize_object ( task [ 'fn_hash' ] ) [ 0 ] , serialize_object ( task [ 'args' ] ) [ 0 ] , serialize_object ( task [ 'kwargs' ] ) [ 0 ] , serialize_object ( task [ 'env' ] ) [ 0 ] ] x = b'' . join ( t ) hashedsum = hashlib . md5 ( x ) . hexdigest ( ) return hashedsum
7837	def remove ( self ) : if self . disco is None : return self . xmlnode . unlinkNode ( ) oldns = self . xmlnode . ns ( ) ns = self . xmlnode . newNs ( oldns . getContent ( ) , None ) self . xmlnode . replaceNs ( oldns , ns ) common_root . addChild ( self . xmlnode ( ) ) self . disco = None
7629	def namespace_array ( ns_key ) : obs_sch = namespace ( ns_key ) obs_sch [ 'title' ] = 'Observation' sch = copy . deepcopy ( JAMS_SCHEMA [ 'definitions' ] [ 'SparseObservationList' ] ) sch [ 'items' ] = obs_sch return sch
2658	def _create_deployment ( self , deployment ) : api_response = self . kube_client . create_namespaced_deployment ( body = deployment , namespace = self . namespace ) logger . debug ( "Deployment created. status='{0}'" . format ( str ( api_response . status ) ) )
2249	def memoize_property ( fget ) : while hasattr ( fget , 'fget' ) : fget = fget . fget attr_name = '_' + fget . __name__ @ functools . wraps ( fget ) def fget_memoized ( self ) : if not hasattr ( self , attr_name ) : setattr ( self , attr_name , fget ( self ) ) return getattr ( self , attr_name ) return property ( fget_memoized )
10846	def reorder ( self , updates_ids , offset = None , utc = None ) : url = PATHS [ 'REORDER' ] % self . profile_id order_format = "order[]=%s&" post_data = '' if offset : post_data += 'offset=%s&' % offset if utc : post_data += 'utc=%s&' % utc for update in updates_ids : post_data += order_format % update return self . api . post ( url = url , data = post_data )
9574	def read_file_header ( fd , endian ) : fields = [ ( 'description' , 's' , 116 ) , ( 'subsystem_offset' , 's' , 8 ) , ( 'version' , 'H' , 2 ) , ( 'endian_test' , 's' , 2 ) ] hdict = { } for name , fmt , num_bytes in fields : data = fd . read ( num_bytes ) hdict [ name ] = unpack ( endian , fmt , data ) hdict [ 'description' ] = hdict [ 'description' ] . strip ( ) v_major = hdict [ 'version' ] >> 8 v_minor = hdict [ 'version' ] & 0xFF hdict [ '__version__' ] = '%d.%d' % ( v_major , v_minor ) return hdict
11462	def update_subject_categories ( self , primary , secondary , kb ) : category_fields = record_get_field_instances ( self . record , tag = '650' , ind1 = '1' , ind2 = '7' ) record_delete_fields ( self . record , "650" ) for field in category_fields : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : new_value = self . get_config_item ( value , kb ) if new_value != value : new_subs = [ ( '2' , secondary ) , ( 'a' , new_value ) ] else : new_subs = [ ( '2' , primary ) , ( 'a' , value ) ] record_add_field ( self . record , "650" , ind1 = "1" , ind2 = "7" , subfields = new_subs ) break
10808	def delete ( self ) : with db . session . begin_nested ( ) : Membership . query_by_group ( self ) . delete ( ) GroupAdmin . query_by_group ( self ) . delete ( ) GroupAdmin . query_by_admin ( self ) . delete ( ) db . session . delete ( self )
7821	def challenge ( self , challenge ) : if not challenge : logger . debug ( "Empty challenge" ) return Failure ( "bad-challenge" ) if self . _server_first_message : return self . _final_challenge ( challenge ) match = SERVER_FIRST_MESSAGE_RE . match ( challenge ) if not match : logger . debug ( "Bad challenge syntax: {0!r}" . format ( challenge ) ) return Failure ( "bad-challenge" ) self . _server_first_message = challenge mext = match . group ( "mext" ) if mext : logger . debug ( "Unsupported extension received: {0!r}" . format ( mext ) ) return Failure ( "bad-challenge" ) nonce = match . group ( "nonce" ) if not nonce . startswith ( self . _c_nonce ) : logger . debug ( "Nonce does not start with our nonce" ) return Failure ( "bad-challenge" ) salt = match . group ( "salt" ) try : salt = a2b_base64 ( salt ) except ValueError : logger . debug ( "Bad base64 encoding for salt: {0!r}" . format ( salt ) ) return Failure ( "bad-challenge" ) iteration_count = match . group ( "iteration_count" ) try : iteration_count = int ( iteration_count ) except ValueError : logger . debug ( "Bad iteration_count: {0!r}" . format ( iteration_count ) ) return Failure ( "bad-challenge" ) return self . _make_response ( nonce , salt , iteration_count )
2093	def lookup_stdout ( self , pk = None , start_line = None , end_line = None , full = True ) : stdout_url = '%s%s/stdout/' % ( self . unified_job_type , pk ) payload = { 'format' : 'json' , 'content_encoding' : 'base64' , 'content_format' : 'ansi' } if start_line : payload [ 'start_line' ] = start_line if end_line : payload [ 'end_line' ] = end_line debug . log ( 'Requesting a copy of job standard output' , header = 'details' ) resp = client . get ( stdout_url , params = payload ) . json ( ) content = b64decode ( resp [ 'content' ] ) return content . decode ( 'utf-8' , 'replace' )
7674	def slice ( self , start_time , end_time , strict = False ) : if self . file_metadata . duration is None : raise JamsError ( 'Duration must be set (jam.file_metadata.duration) before ' 'slicing can be performed.' ) if ( start_time < 0 or start_time > float ( self . file_metadata . duration ) or end_time < start_time or end_time > float ( self . file_metadata . duration ) ) : raise ParameterError ( 'start_time and end_time must be within the original file ' 'duration ({:f}) and end_time cannot be smaller than ' 'start_time.' . format ( float ( self . file_metadata . duration ) ) ) jam_sliced = JAMS ( annotations = None , file_metadata = self . file_metadata , sandbox = self . sandbox ) jam_sliced . annotations = self . annotations . slice ( start_time , end_time , strict = strict ) jam_sliced . file_metadata . duration = end_time - start_time if 'slice' not in jam_sliced . sandbox . keys ( ) : jam_sliced . sandbox . update ( slice = [ { 'start_time' : start_time , 'end_time' : end_time } ] ) else : jam_sliced . sandbox . slice . append ( { 'start_time' : start_time , 'end_time' : end_time } ) return jam_sliced
5217	def check_hours ( tickers , tz_exch , tz_loc = DEFAULT_TZ ) -> pd . DataFrame : cols = [ 'Trading_Day_Start_Time_EOD' , 'Trading_Day_End_Time_EOD' ] con , _ = create_connection ( ) hours = con . ref ( tickers = tickers , flds = cols ) cur_dt = pd . Timestamp ( 'today' ) . strftime ( '%Y-%m-%d ' ) hours . loc [ : , 'local' ] = hours . value . astype ( str ) . str [ : - 3 ] hours . loc [ : , 'exch' ] = pd . DatetimeIndex ( cur_dt + hours . value . astype ( str ) ) . tz_localize ( tz_loc ) . tz_convert ( tz_exch ) . strftime ( '%H:%M' ) hours = pd . concat ( [ hours . set_index ( [ 'ticker' , 'field' ] ) . exch . unstack ( ) . loc [ : , cols ] , hours . set_index ( [ 'ticker' , 'field' ] ) . local . unstack ( ) . loc [ : , cols ] , ] , axis = 1 ) hours . columns = [ 'Exch_Start' , 'Exch_End' , 'Local_Start' , 'Local_End' ] return hours
1702	def outer_right_join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . OUTER_RIGHT , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
12747	def load ( self , source , ** kwargs ) : if hasattr ( source , 'endswith' ) and source . lower ( ) . endswith ( '.asf' ) : self . load_asf ( source , ** kwargs ) else : self . load_skel ( source , ** kwargs )
11334	def table ( * columns , ** kwargs ) : ret = [ ] prefix = kwargs . get ( 'prefix' , '' ) buf_count = kwargs . get ( 'buf_count' , 2 ) if len ( columns ) == 1 : columns = list ( columns [ 0 ] ) else : columns = list ( zip ( * columns ) ) headers = kwargs . get ( "headers" , [ ] ) if headers : columns . insert ( 0 , headers ) widths = kwargs . get ( "widths" , [ ] ) row_counts = Counter ( ) for i in range ( len ( widths ) ) : row_counts [ i ] = int ( widths [ i ] ) width = int ( kwargs . get ( "width" , 0 ) ) for row in columns : for i , c in enumerate ( row ) : if isinstance ( c , basestring ) : cl = len ( c ) else : cl = len ( str ( c ) ) if cl > row_counts [ i ] : row_counts [ i ] = cl width = int ( kwargs . get ( "width" , 0 ) ) if width : for i in row_counts : if row_counts [ i ] < width : row_counts [ i ] = width def colstr ( c ) : if isinstance ( c , basestring ) : return c return str ( c ) def rowstr ( row , prefix , row_counts ) : row_format = prefix cols = list ( map ( colstr , row ) ) for i in range ( len ( row_counts ) ) : c = cols [ i ] if re . match ( r"^\d+(?:\.\d+)?$" , c ) : if i == 0 : row_format += "{:>" + str ( row_counts [ i ] ) + "}" else : row_format += "{:>" + str ( row_counts [ i ] + buf_count ) + "}" else : row_format += "{:<" + str ( row_counts [ i ] + buf_count ) + "}" return row_format . format ( * cols ) for row in columns : ret . append ( rowstr ( row , prefix , row_counts ) ) out ( os . linesep . join ( ret ) )
2791	def get_object ( cls , api_token , cert_id ) : certificate = cls ( token = api_token , id = cert_id ) certificate . load ( ) return certificate
11576	def sonar_data ( self , data ) : val = int ( ( data [ self . MSB ] << 7 ) + data [ self . LSB ] ) pin_number = data [ 0 ] with self . pymata . data_lock : sonar_pin_entry = self . active_sonar_map [ pin_number ] self . digital_response_table [ data [ self . RESPONSE_TABLE_MODE ] ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] = val if sonar_pin_entry [ 0 ] is not None : if sonar_pin_entry [ 1 ] != val : self . active_sonar_map [ pin_number ] [ 0 ] ( [ self . pymata . SONAR , pin_number , val ] ) sonar_pin_entry [ 1 ] = val self . active_sonar_map [ pin_number ] = sonar_pin_entry
8824	def context ( self ) : if not self . _context : self . _context = context . get_admin_context ( ) return self . _context
13616	def scaffold ( ) : click . echo ( "A whole new site? Awesome." ) title = click . prompt ( "What's the title?" ) url = click . prompt ( "Great. What's url? http://" ) click . echo ( "Got it. Creating %s..." % url )
10293	def expand_internal_causal ( universe : BELGraph , graph : BELGraph ) -> None : expand_internal ( universe , graph , edge_predicates = is_causal_relation )
1993	def load_state ( self , state_id , delete = True ) : return self . _store . load_state ( f'{self._prefix}{state_id:08x}{self._suffix}' , delete = delete )
3186	def create ( self , conversation_id , data ) : self . conversation_id = conversation_id if 'from_email' not in data : raise KeyError ( 'The conversation message must have a from_email' ) check_email ( data [ 'from_email' ] ) if 'read' not in data : raise KeyError ( 'The conversation message must have a read' ) if data [ 'read' ] not in [ True , False ] : raise TypeError ( 'The conversation message read must be True or False' ) response = self . _mc_client . _post ( url = self . _build_path ( conversation_id , 'messages' ) , data = data ) if response is not None : self . message_id = response [ 'id' ] else : self . message_id = None return response
1873	def MOVLPD ( cpu , dest , src ) : value = src . read ( ) if src . size == 64 and dest . size == 128 : value = ( dest . read ( ) & 0xffffffffffffffff0000000000000000 ) | Operators . ZEXTEND ( value , 128 ) dest . write ( value )
6845	def check_ok ( self ) : import requests if not self . env . check_ok : return branch_name = self . _local ( 'git rev-parse --abbrev-ref HEAD' , capture = True ) . strip ( ) check_ok_paths = self . env . check_ok_paths or { } if branch_name in check_ok_paths : check = check_ok_paths [ branch_name ] if 'username' in check : auth = ( check [ 'username' ] , check [ 'password' ] ) else : auth = None ret = requests . get ( check [ 'url' ] , auth = auth ) passed = check [ 'text' ] in ret . content assert passed , 'Check failed: %s' % check [ 'url' ]
3810	async def connect ( self ) : proxy = os . environ . get ( 'HTTP_PROXY' ) self . _session = http_utils . Session ( self . _cookies , proxy = proxy ) try : self . _channel = channel . Channel ( self . _session , self . _max_retries , self . _retry_backoff_base ) self . _channel . on_connect . add_observer ( self . on_connect . fire ) self . _channel . on_reconnect . add_observer ( self . on_reconnect . fire ) self . _channel . on_disconnect . add_observer ( self . on_disconnect . fire ) self . _channel . on_receive_array . add_observer ( self . _on_receive_array ) self . _listen_future = asyncio . ensure_future ( self . _channel . listen ( ) ) try : await self . _listen_future except asyncio . CancelledError : self . _listen_future . cancel ( ) logger . info ( 'Client.connect returning because Channel.listen returned' ) finally : await self . _session . close ( )
6099	def luminosity_integral ( self , x , axis_ratio ) : r = x * axis_ratio return 2 * np . pi * r * self . intensities_from_grid_radii ( x )
2855	def setup ( self , pin , mode ) : self . _setup_pin ( pin , mode ) self . mpsse_write_gpio ( )
4684	def getPublicKeys ( self , current = False ) : pubkeys = self . store . getPublicKeys ( ) if not current : return pubkeys pubs = [ ] for pubkey in pubkeys : if pubkey [ : len ( self . prefix ) ] == self . prefix : pubs . append ( pubkey ) return pubs
8891	def deserialize ( cls , dict_model ) : kwargs = { } for f in cls . _meta . concrete_fields : if f . attname in dict_model : kwargs [ f . attname ] = dict_model [ f . attname ] return cls ( ** kwargs )
8990	def first_consumed_mesh ( self ) : for instruction in self . instructions : if instruction . consumes_meshes ( ) : return instruction . first_consumed_mesh raise IndexError ( "{} consumes no meshes" . format ( self ) )
3084	def oauth2decorator_from_clientsecrets ( filename , scope , message = None , cache = None ) : return OAuth2DecoratorFromClientSecrets ( filename , scope , message = message , cache = cache )
13629	def put ( self , metrics ) : if type ( metrics ) == list : for metric in metrics : self . c . put_metric_data ( ** metric ) else : self . c . put_metric_data ( ** metrics )
1687	def _CollapseStrings ( elided ) : if _RE_PATTERN_INCLUDE . match ( elided ) : return elided elided = _RE_PATTERN_CLEANSE_LINE_ESCAPES . sub ( '' , elided ) collapsed = '' while True : match = Match ( r'^([^\'"]*)([\'"])(.*)$' , elided ) if not match : collapsed += elided break head , quote , tail = match . groups ( ) if quote == '"' : second_quote = tail . find ( '"' ) if second_quote >= 0 : collapsed += head + '""' elided = tail [ second_quote + 1 : ] else : collapsed += elided break else : if Search ( r'\b(?:0[bBxX]?|[1-9])[0-9a-fA-F]*$' , head ) : match_literal = Match ( r'^((?:\'?[0-9a-zA-Z_])*)(.*)$' , "'" + tail ) collapsed += head + match_literal . group ( 1 ) . replace ( "'" , '' ) elided = match_literal . group ( 2 ) else : second_quote = tail . find ( '\'' ) if second_quote >= 0 : collapsed += head + "''" elided = tail [ second_quote + 1 : ] else : collapsed += elided break return collapsed
3345	def parse_if_header_dict ( environ ) : if "wsgidav.conditions.if" in environ : return if "HTTP_IF" not in environ : environ [ "wsgidav.conditions.if" ] = None environ [ "wsgidav.ifLockTokenList" ] = [ ] return iftext = environ [ "HTTP_IF" ] . strip ( ) if not iftext . startswith ( "<" ) : iftext = "<*>" + iftext ifDict = dict ( [ ] ) ifLockList = [ ] resource1 = "*" for ( tmpURLVar , URLVar , _tmpContentVar , contentVar ) in reIfSeparator . findall ( iftext ) : if tmpURLVar != "" : resource1 = URLVar else : listTagContents = [ ] testflag = True for listitem in reIfTagListContents . findall ( contentVar ) : if listitem . upper ( ) != "NOT" : if listitem . startswith ( "[" ) : listTagContents . append ( ( testflag , "entity" , listitem . strip ( '"[]' ) ) ) else : listTagContents . append ( ( testflag , "locktoken" , listitem . strip ( "<>" ) ) ) ifLockList . append ( listitem . strip ( "<>" ) ) testflag = listitem . upper ( ) != "NOT" if resource1 in ifDict : listTag = ifDict [ resource1 ] else : listTag = [ ] ifDict [ resource1 ] = listTag listTag . append ( listTagContents ) environ [ "wsgidav.conditions.if" ] = ifDict environ [ "wsgidav.ifLockTokenList" ] = ifLockList _logger . debug ( "parse_if_header_dict\n{}" . format ( pformat ( ifDict ) ) ) return
1684	def Begin ( self , function_name ) : self . in_a_function = True self . lines_in_function = 0 self . current_function = function_name
10216	def to_jupyter ( graph : BELGraph , chart : Optional [ str ] = None ) -> Javascript : with open ( os . path . join ( HERE , 'render_with_javascript.js' ) , 'rt' ) as f : js_template = Template ( f . read ( ) ) return Javascript ( js_template . render ( ** _get_context ( graph , chart = chart ) ) )
5016	def transmit ( self , payload , ** kwargs ) : IntegratedChannelLearnerDataTransmissionAudit = apps . get_model ( app_label = kwargs . get ( 'app_label' , 'integrated_channel' ) , model_name = kwargs . get ( 'model_name' , 'LearnerDataTransmissionAudit' ) , ) for learner_data in payload . export ( ) : serialized_payload = learner_data . serialize ( enterprise_configuration = self . enterprise_configuration ) LOGGER . debug ( 'Attempting to transmit serialized payload: %s' , serialized_payload ) enterprise_enrollment_id = learner_data . enterprise_course_enrollment_id if learner_data . completed_timestamp is None : LOGGER . info ( 'Skipping in-progress enterprise enrollment {}' . format ( enterprise_enrollment_id ) ) continue previous_transmissions = IntegratedChannelLearnerDataTransmissionAudit . objects . filter ( enterprise_course_enrollment_id = enterprise_enrollment_id , error_message = '' ) if previous_transmissions . exists ( ) : LOGGER . info ( 'Skipping previously sent enterprise enrollment {}' . format ( enterprise_enrollment_id ) ) continue try : code , body = self . client . create_course_completion ( getattr ( learner_data , kwargs . get ( 'remote_user_id' ) ) , serialized_payload ) LOGGER . info ( 'Successfully sent completion status call for enterprise enrollment {}' . format ( enterprise_enrollment_id , ) ) except RequestException as request_exception : code = 500 body = str ( request_exception ) self . handle_transmission_error ( learner_data , request_exception ) learner_data . status = str ( code ) learner_data . error_message = body if code >= 400 else '' learner_data . save ( )
8835	def less_or_equal ( a , b , * args ) : return ( less ( a , b ) or soft_equals ( a , b ) ) and ( not args or less_or_equal ( b , * args ) )
942	def _runExperimentImpl ( options , model = None ) : json_helpers . validate ( options . privateOptions , schemaDict = g_parsedPrivateCommandLineOptionsSchema ) experimentDir = options . experimentDir descriptionPyModule = helpers . loadExperimentDescriptionScriptFromDir ( experimentDir ) expIface = helpers . getExperimentDescriptionInterfaceFromModule ( descriptionPyModule ) if options . privateOptions [ 'listAvailableCheckpoints' ] : _printAvailableCheckpoints ( experimentDir ) return None experimentTasks = expIface . getModelControl ( ) . get ( 'tasks' , [ ] ) if ( len ( experimentTasks ) == 0 and expIface . getModelControl ( ) [ 'environment' ] == OpfEnvironment . Nupic ) : expIface . convertNupicEnvToOPF ( ) experimentTasks = expIface . getModelControl ( ) . get ( 'tasks' , [ ] ) expIface . normalizeStreamSources ( ) newSerialization = options . privateOptions [ 'newSerialization' ] if options . privateOptions [ 'listTasks' ] : print "Available tasks:" for label in [ t [ 'taskLabel' ] for t in experimentTasks ] : print "\t" , label return None if options . privateOptions [ 'runCheckpointName' ] : assert model is None checkpointName = options . privateOptions [ 'runCheckpointName' ] model = ModelFactory . loadFromCheckpoint ( savedModelDir = _getModelCheckpointDir ( experimentDir , checkpointName ) , newSerialization = newSerialization ) elif model is not None : print "Skipping creation of OPFExperiment instance: caller provided his own" else : modelDescription = expIface . getModelDescription ( ) model = ModelFactory . create ( modelDescription ) if options . privateOptions [ 'createCheckpointName' ] : checkpointName = options . privateOptions [ 'createCheckpointName' ] _saveModel ( model = model , experimentDir = experimentDir , checkpointLabel = checkpointName , newSerialization = newSerialization ) return model taskIndexList = range ( len ( experimentTasks ) ) customTaskExecutionLabelsList = options . privateOptions [ 'taskLabels' ] if customTaskExecutionLabelsList : taskLabelsList = [ t [ 'taskLabel' ] for t in experimentTasks ] taskLabelsSet = set ( taskLabelsList ) customTaskExecutionLabelsSet = set ( customTaskExecutionLabelsList ) assert customTaskExecutionLabelsSet . issubset ( taskLabelsSet ) , ( "Some custom-provided task execution labels don't correspond " "to actual task labels: mismatched labels: %r; actual task " "labels: %r." ) % ( customTaskExecutionLabelsSet - taskLabelsSet , customTaskExecutionLabelsList ) taskIndexList = [ taskLabelsList . index ( label ) for label in customTaskExecutionLabelsList ] print "#### Executing custom task list: %r" % [ taskLabelsList [ i ] for i in taskIndexList ] for taskIndex in taskIndexList : task = experimentTasks [ taskIndex ] taskRunner = _TaskRunner ( model = model , task = task , cmdOptions = options ) taskRunner . run ( ) del taskRunner if options . privateOptions [ 'checkpointModel' ] : _saveModel ( model = model , experimentDir = experimentDir , checkpointLabel = task [ 'taskLabel' ] , newSerialization = newSerialization ) return model
7305	def process_post_form ( self , success_message = None ) : if not hasattr ( self , 'document' ) or self . document is None : self . document = self . document_type ( ) self . form = MongoModelForm ( model = self . document_type , instance = self . document , form_post_data = self . request . POST ) . get_form ( ) self . form . is_bound = True if self . form . is_valid ( ) : self . document_map_dict = MongoModelForm ( model = self . document_type ) . create_document_dictionary ( self . document_type ) self . new_document = self . document_type self . embedded_list_docs = { } if self . new_document is None : messages . error ( self . request , u"Failed to save document" ) else : self . new_document = self . new_document ( ) for form_key in self . form . cleaned_data . keys ( ) : if form_key == 'id' and hasattr ( self , 'document' ) : self . new_document . id = self . document . id continue self . process_document ( self . new_document , form_key , None ) self . new_document . save ( ) if success_message : messages . success ( self . request , success_message ) return self . form
7567	def splitalleles ( consensus ) : allele1 = list ( consensus ) allele2 = list ( consensus ) hidx = [ i for ( i , j ) in enumerate ( consensus ) if j in "RKSWYMrkswym" ] for idx in hidx : hsite = consensus [ idx ] if hsite . isupper ( ) : allele1 [ idx ] = PRIORITY [ hsite ] allele2 [ idx ] = MINOR [ hsite ] else : allele1 [ idx ] = MINOR [ hsite . upper ( ) ] allele2 [ idx ] = PRIORITY [ hsite . upper ( ) ] allele1 = "" . join ( allele1 ) allele2 = "" . join ( allele2 ) return allele1 , allele2
8014	async def send_upstream ( self , message , stream_name = None ) : if stream_name is None : for steam_queue in self . application_streams . values ( ) : await steam_queue . put ( message ) return steam_queue = self . application_streams . get ( stream_name ) if steam_queue is None : raise ValueError ( "Invalid multiplexed frame received (stream not mapped)" ) await steam_queue . put ( message )
4614	def awaitTxConfirmation ( self , transaction , limit = 10 ) : counter = 10 for block in self . blocks ( ) : counter += 1 for tx in block [ "transactions" ] : if sorted ( tx [ "signatures" ] ) == sorted ( transaction [ "signatures" ] ) : return tx if counter > limit : raise Exception ( "The operation has not been added after 10 blocks!" )
884	def reset ( self ) : self . activeCells = [ ] self . winnerCells = [ ] self . activeSegments = [ ] self . matchingSegments = [ ]
1319	def IsTopLevel ( self ) -> bool : handle = self . NativeWindowHandle if handle : return GetAncestor ( handle , GAFlag . Root ) == handle return False
2450	def set_pkg_originator ( self , doc , entity ) : self . assert_package_exists ( ) if not self . package_originator_set : self . package_originator_set = True if validations . validate_pkg_originator ( entity ) : doc . package . originator = entity return True else : raise SPDXValueError ( 'Package::Originator' ) else : raise CardinalityError ( 'Package::Originator' )
3448	def minimal_medium ( model , min_objective_value = 0.1 , exports = False , minimize_components = False , open_exchanges = False ) : exchange_rxns = find_boundary_types ( model , "exchange" ) if isinstance ( open_exchanges , bool ) : open_bound = 1000 else : open_bound = open_exchanges with model as mod : if open_exchanges : LOGGER . debug ( "Opening exchanges for %d imports." , len ( exchange_rxns ) ) for rxn in exchange_rxns : rxn . bounds = ( - open_bound , open_bound ) LOGGER . debug ( "Applying objective value constraints." ) obj_const = mod . problem . Constraint ( mod . objective . expression , lb = min_objective_value , name = "medium_obj_constraint" ) mod . add_cons_vars ( [ obj_const ] ) mod . solver . update ( ) mod . objective = Zero LOGGER . debug ( "Adding new media objective." ) tol = mod . solver . configuration . tolerances . feasibility if minimize_components : add_mip_obj ( mod ) if isinstance ( minimize_components , bool ) : minimize_components = 1 seen = set ( ) best = num_components = mod . slim_optimize ( ) if mod . solver . status != OPTIMAL : LOGGER . warning ( "Minimization of medium was infeasible." ) return None exclusion = mod . problem . Constraint ( Zero , ub = 0 ) mod . add_cons_vars ( [ exclusion ] ) mod . solver . update ( ) media = [ ] for i in range ( minimize_components ) : LOGGER . info ( "Finding alternative medium #%d." , ( i + 1 ) ) vars = [ mod . variables [ "ind_" + s ] for s in seen ] if len ( seen ) > 0 : exclusion . set_linear_coefficients ( dict . fromkeys ( vars , 1 ) ) exclusion . ub = best - 1 num_components = mod . slim_optimize ( ) if mod . solver . status != OPTIMAL or num_components > best : break medium = _as_medium ( exchange_rxns , tol , exports = exports ) media . append ( medium ) seen . update ( medium [ medium > 0 ] . index ) if len ( media ) > 1 : medium = pd . concat ( media , axis = 1 , sort = True ) . fillna ( 0.0 ) medium . sort_index ( axis = 1 , inplace = True ) else : medium = media [ 0 ] else : add_linear_obj ( mod ) mod . slim_optimize ( ) if mod . solver . status != OPTIMAL : LOGGER . warning ( "Minimization of medium was infeasible." ) return None medium = _as_medium ( exchange_rxns , tol , exports = exports ) return medium
357	def load_ckpt ( sess = None , mode_name = 'model.ckpt' , save_dir = 'checkpoint' , var_list = None , is_latest = True , printable = False ) : if sess is None : raise ValueError ( "session is None." ) if var_list is None : var_list = [ ] if is_latest : ckpt_file = tf . train . latest_checkpoint ( save_dir ) else : ckpt_file = os . path . join ( save_dir , mode_name ) if not var_list : var_list = tf . global_variables ( ) logging . info ( "[*] load %s n_params: %d" % ( ckpt_file , len ( var_list ) ) ) if printable : for idx , v in enumerate ( var_list ) : logging . info ( " param {:3}: {:15} {}" . format ( idx , v . name , str ( v . get_shape ( ) ) ) ) try : saver = tf . train . Saver ( var_list ) saver . restore ( sess , ckpt_file ) except Exception as e : logging . info ( e ) logging . info ( "[*] load ckpt fail ..." )
918	def warning ( self , msg , * args , ** kwargs ) : self . _baseLogger . warning ( self , self . getExtendedMsg ( msg ) , * args , ** kwargs )
6499	def search ( self , query_string = None , field_dictionary = None , filter_dictionary = None , exclude_dictionary = None , facet_terms = None , exclude_ids = None , use_field_match = False , ** kwargs ) : log . debug ( "searching index with %s" , query_string ) elastic_queries = [ ] elastic_filters = [ ] if query_string : if six . PY2 : query_string = query_string . encode ( 'utf-8' ) . translate ( None , RESERVED_CHARACTERS ) else : query_string = query_string . translate ( query_string . maketrans ( '' , '' , RESERVED_CHARACTERS ) ) elastic_queries . append ( { "query_string" : { "fields" : [ "content.*" ] , "query" : query_string } } ) if field_dictionary : if use_field_match : elastic_queries . extend ( _process_field_queries ( field_dictionary ) ) else : elastic_filters . extend ( _process_field_filters ( field_dictionary ) ) if filter_dictionary : elastic_filters . extend ( _process_filters ( filter_dictionary ) ) if exclude_ids : if not exclude_dictionary : exclude_dictionary = { } if "_id" not in exclude_dictionary : exclude_dictionary [ "_id" ] = [ ] exclude_dictionary [ "_id" ] . extend ( exclude_ids ) if exclude_dictionary : elastic_filters . append ( _process_exclude_dictionary ( exclude_dictionary ) ) query_segment = { "match_all" : { } } if elastic_queries : query_segment = { "bool" : { "must" : elastic_queries } } query = query_segment if elastic_filters : filter_segment = { "bool" : { "must" : elastic_filters } } query = { "filtered" : { "query" : query_segment , "filter" : filter_segment , } } body = { "query" : query } if facet_terms : facet_query = _process_facet_terms ( facet_terms ) if facet_query : body [ "facets" ] = facet_query try : es_response = self . _es . search ( index = self . index_name , body = body , ** kwargs ) except exceptions . ElasticsearchException as ex : message = six . text_type ( ex ) if 'QueryParsingException' in message : log . exception ( "Malformed search query: %s" , message ) raise QueryParseError ( 'Malformed search query.' ) else : log . exception ( "error while searching index - %s" , str ( message ) ) raise return _translate_hits ( es_response )
10025	def get_versions ( self ) : response = self . ebs . describe_application_versions ( application_name = self . app_name ) return response [ 'DescribeApplicationVersionsResponse' ] [ 'DescribeApplicationVersionsResult' ] [ 'ApplicationVersions' ]
264	def _stack_positions ( positions , pos_in_dollars = True ) : if pos_in_dollars : positions = get_percent_alloc ( positions ) positions = positions . drop ( 'cash' , axis = 'columns' ) positions = positions . stack ( ) positions . index = positions . index . set_names ( [ 'dt' , 'ticker' ] ) return positions
3355	def union ( self , iterable ) : _dict = self . _dict append = self . append for i in iterable : if i . id not in _dict : append ( i )
12724	def erps ( self , erps ) : _set_params ( self . ode_obj , 'ERP' , erps , self . ADOF + self . LDOF )
1039	def column ( self ) : line , column = self . source_buffer . decompose_position ( self . begin_pos ) return column
1904	def _find_zero ( cpu , constrs , ptr ) : offset = 0 while True : byt = cpu . read_int ( ptr + offset , 8 ) if issymbolic ( byt ) : if not solver . can_be_true ( constrs , byt != 0 ) : break else : if byt == 0 : break offset += 1 return offset
8596	def update_group ( self , group_id , ** kwargs ) : properties = { } if 'create_datacenter' in kwargs : kwargs [ 'create_data_center' ] = kwargs . pop ( 'create_datacenter' ) for attr , value in kwargs . items ( ) : properties [ self . _underscore_to_camelcase ( attr ) ] = value data = { "properties" : properties } response = self . _perform_request ( url = '/um/groups/%s' % group_id , method = 'PUT' , data = json . dumps ( data ) ) return response
13187	def image_path ( instance , filename ) : filename , ext = os . path . splitext ( filename . lower ( ) ) instance_id_hash = hashlib . md5 ( str ( instance . id ) ) . hexdigest ( ) filename_hash = '' . join ( random . sample ( hashlib . md5 ( filename . encode ( 'utf-8' ) ) . hexdigest ( ) , 8 ) ) return '{}/{}{}' . format ( instance_id_hash , filename_hash , ext )
10107	def get_context_data ( self , ** kwargs ) : context = super ( TabView , self ) . get_context_data ( ** kwargs ) context . update ( kwargs ) process_tabs_kwargs = { 'tabs' : self . get_group_tabs ( ) , 'current_tab' : self , 'group_current_tab' : self , } context [ 'tabs' ] = self . _process_tabs ( ** process_tabs_kwargs ) context [ 'current_tab_id' ] = self . tab_id if self . tab_parent is not None : if self . tab_parent not in self . _registry : msg = '%s has no attribute _is_tab' % self . tab_parent . __class__ . __name__ raise ImproperlyConfigured ( msg ) parent = self . tab_parent ( ) process_parents_kwargs = { 'tabs' : parent . get_group_tabs ( ) , 'current_tab' : self , 'group_current_tab' : parent , } context [ 'parent_tabs' ] = self . _process_tabs ( ** process_parents_kwargs ) context [ 'parent_tab_id' ] = parent . tab_id if self . tab_id in self . _children : process_children_kwargs = { 'tabs' : [ t ( ) for t in self . _children [ self . tab_id ] ] , 'current_tab' : self , 'group_current_tab' : None , } context [ 'child_tabs' ] = self . _process_tabs ( ** process_children_kwargs ) return context
10729	def _handle_base_case ( klass , symbol ) : def the_func ( value , variant = 0 ) : ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( 0 , variant ) return ( klass ( value , variant_level = obj_level ) , func_level ) return lambda : ( the_func , symbol )
12431	def create_manage_scripts ( self ) : start = '# start script for {0}\n\n' . format ( self . _project_name ) start += 'echo \'Starting uWSGI...\'\n' start += 'sh {0}.uwsgi\n' . format ( os . path . join ( self . _conf_dir , self . _project_name ) ) start += 'sleep 1\n' start += 'echo \'Starting Nginx...\'\n' start += 'nginx -c {0}_nginx.conf\n' . format ( os . path . join ( self . _conf_dir , self . _project_name ) ) start += 'sleep 1\n' start += 'echo \'{0} started\'\n\n' . format ( self . _project_name ) stop = '# stop script for {0}\n\n' . format ( self . _project_name ) stop += 'if [ -e {0}_nginx.pid ]; then nginx -c {1}_nginx.conf -s stop ; fi\n' . format ( os . path . join ( self . _var_dir , self . _project_name ) , os . path . join ( self . _conf_dir , self . _project_name ) ) stop += 'if [ -e {0}_uwsgi.pid ]; then kill -9 `cat {0}_uwsgi.pid` ; rm {0}_uwsgi.pid 2>&1 > /dev/null ; fi\n' . format ( os . path . join ( self . _var_dir , self . _project_name ) ) stop += 'echo \'{0} stopped\'\n' . format ( self . _project_name ) start_file = '{0}_start.sh' . format ( os . path . join ( self . _script_dir , self . _project_name ) ) stop_file = '{0}_stop.sh' . format ( os . path . join ( self . _script_dir , self . _project_name ) ) f = open ( start_file , 'w' ) f . write ( start ) f . close ( ) f = open ( stop_file , 'w' ) f . write ( stop ) f . close ( ) os . chmod ( start_file , 0754 ) os . chmod ( stop_file , 0754 )
984	def mmGetMetricSequencesPredictedActiveCellsPerColumn ( self ) : self . _mmComputeTransitionTraces ( ) numCellsPerColumn = [ ] for predictedActiveCells in ( self . _mmData [ "predictedActiveCellsForSequence" ] . values ( ) ) : cellsForColumn = self . mapCellsToColumns ( predictedActiveCells ) numCellsPerColumn += [ len ( x ) for x in cellsForColumn . values ( ) ] return Metric ( self , "# predicted => active cells per column for each sequence" , numCellsPerColumn )
6023	def new_psf_with_renormalized_array ( self ) : return PSF ( array = self , pixel_scale = self . pixel_scale , renormalize = True )
1748	def _get_offset ( self , index ) : if not self . _in_range ( index ) : raise IndexError ( 'Map index out of range' ) if isinstance ( index , slice ) : index = slice ( index . start - self . start , index . stop - self . start ) else : index -= self . start return index
10383	def main ( ) : logging . basicConfig ( level = logging . INFO ) log . setLevel ( logging . INFO ) bms_base = get_bms_base ( ) neurommsig_base = get_neurommsig_base ( ) neurommsig_excel_dir = os . path . join ( neurommsig_base , 'resources' , 'excels' , 'neurommsig' ) nift_values = get_nift_values ( ) log . info ( 'Starting Alzheimers' ) ad_path = os . path . join ( neurommsig_excel_dir , 'alzheimers' , 'alzheimers.xlsx' ) ad_df = preprocess ( ad_path ) with open ( os . path . join ( bms_base , 'aetionomy' , 'alzheimers' , 'neurommsigdb_ad.bel' ) , 'w' ) as ad_file : write_neurommsig_bel ( ad_file , ad_df , mesh_alzheimer , nift_values ) log . info ( 'Starting Parkinsons' ) pd_path = os . path . join ( neurommsig_excel_dir , 'parkinsons' , 'parkinsons.xlsx' ) pd_df = preprocess ( pd_path ) with open ( os . path . join ( bms_base , 'aetionomy' , 'parkinsons' , 'neurommsigdb_pd.bel' ) , 'w' ) as pd_file : write_neurommsig_bel ( pd_file , pd_df , mesh_parkinson , nift_values )
4401	def fetch ( self ) : xml = urllib . request . urlopen ( self . URL ) tree = ET . ElementTree ( file = xml ) records = self . _parse_deputies ( tree . getroot ( ) ) df = pd . DataFrame ( records , columns = ( 'congressperson_id' , 'budget_id' , 'condition' , 'congressperson_document' , 'civil_name' , 'congressperson_name' , 'picture_url' , 'gender' , 'state' , 'party' , 'phone_number' , 'email' ) ) return self . _translate ( df )
9464	def conference_undeaf ( self , call_params ) : path = '/' + self . api_version + '/ConferenceUndeaf/' method = 'POST' return self . request ( path , method , call_params )
5712	def retrieve_descriptor ( descriptor ) : the_descriptor = descriptor if the_descriptor is None : the_descriptor = { } if isinstance ( the_descriptor , six . string_types ) : try : if os . path . isfile ( the_descriptor ) : with open ( the_descriptor , 'r' ) as f : the_descriptor = json . load ( f ) else : req = requests . get ( the_descriptor ) req . raise_for_status ( ) req . encoding = 'utf8' the_descriptor = req . json ( ) except ( IOError , requests . exceptions . RequestException ) as error : message = 'Unable to load JSON at "%s"' % descriptor six . raise_from ( exceptions . DataPackageException ( message ) , error ) except ValueError as error : message = 'Unable to parse JSON at "%s". %s' % ( descriptor , error ) six . raise_from ( exceptions . DataPackageException ( message ) , error ) if hasattr ( the_descriptor , 'read' ) : try : the_descriptor = json . load ( the_descriptor ) except ValueError as e : six . raise_from ( exceptions . DataPackageException ( str ( e ) ) , e ) if not isinstance ( the_descriptor , dict ) : msg = 'Data must be a \'dict\', but was a \'{0}\'' raise exceptions . DataPackageException ( msg . format ( type ( the_descriptor ) . __name__ ) ) return the_descriptor
13622	def many ( func ) : def _many ( result ) : if _isSequenceTypeNotText ( result ) : return map ( func , result ) return [ ] return maybe ( _many , default = [ ] )
7690	def sonify ( annotation , sr = 22050 , duration = None , ** kwargs ) : length = None if duration is None : duration = annotation . duration if duration is not None : length = int ( duration * sr ) if annotation . namespace in SONIFY_MAPPING : ann = coerce_annotation ( annotation , annotation . namespace ) return SONIFY_MAPPING [ annotation . namespace ] ( ann , sr = sr , length = length , ** kwargs ) for namespace , func in six . iteritems ( SONIFY_MAPPING ) : try : ann = coerce_annotation ( annotation , namespace ) return func ( ann , sr = sr , length = length , ** kwargs ) except NamespaceError : pass raise NamespaceError ( 'Unable to sonify annotation of namespace="{:s}"' . format ( annotation . namespace ) )
1191	def fnmatchcase ( name , pat ) : try : re_pat = _cache [ pat ] except KeyError : res = translate ( pat ) if len ( _cache ) >= _MAXCACHE : globals ( ) [ '_cache' ] = { } _cache [ pat ] = re_pat = re . compile ( res ) return re_pat . match ( name ) is not None
536	def readFromProto ( cls , proto ) : instance = cls ( ) instance . implementation = proto . implementation instance . steps = proto . steps instance . stepsList = [ int ( i ) for i in proto . steps . split ( "," ) ] instance . alpha = proto . alpha instance . verbosity = proto . verbosity instance . maxCategoryCount = proto . maxCategoryCount instance . _sdrClassifier = SDRClassifierFactory . read ( proto ) instance . learningMode = proto . learningMode instance . inferenceMode = proto . inferenceMode instance . recordNum = proto . recordNum return instance
7792	def remove_fetcher ( self , fetcher ) : self . _lock . acquire ( ) try : for t , f in list ( self . _active_fetchers ) : if f is fetcher : self . _active_fetchers . remove ( ( t , f ) ) f . _deactivated ( ) return finally : self . _lock . release ( )
3324	def lock_string ( lock_dict ) : if not lock_dict : return "Lock: None" if lock_dict [ "expire" ] < 0 : expire = "Infinite ({})" . format ( lock_dict [ "expire" ] ) else : expire = "{} (in {} seconds)" . format ( util . get_log_time ( lock_dict [ "expire" ] ) , lock_dict [ "expire" ] - time . time ( ) ) return "Lock(<{}..>, '{}', {}, {}, depth-{}, until {}" . format ( lock_dict . get ( "token" , "?" * 30 ) [ 18 : 22 ] , lock_dict . get ( "root" ) , lock_dict . get ( "principal" ) , lock_dict . get ( "scope" ) , lock_dict . get ( "depth" ) , expire , )
9839	def __gridconnections ( self ) : try : tok = self . __consume ( ) except DXParserNoTokens : return if tok . equals ( 'counts' ) : shape = [ ] try : while True : self . __peek ( ) . value ( 'INTEGER' ) tok = self . __consume ( ) shape . append ( tok . value ( 'INTEGER' ) ) except ( DXParserNoTokens , ValueError ) : pass if len ( shape ) == 0 : raise DXParseError ( 'gridconnections: no shape parameters' ) self . currentobject [ 'shape' ] = shape else : raise DXParseError ( 'gridconnections: ' + str ( tok ) + ' not recognized.' )
161	def width ( self ) : if len ( self . coords ) <= 1 : return 0 return np . max ( self . xx ) - np . min ( self . xx )
8331	def findNextSibling ( self , name = None , attrs = { } , text = None , ** kwargs ) : return self . _findOne ( self . findNextSiblings , name , attrs , text , ** kwargs )
1037	def begin ( self ) : return Range ( self . source_buffer , self . begin_pos , self . begin_pos , expanded_from = self . expanded_from )
9718	async def stream_frames_stop ( self ) : self . _protocol . set_on_packet ( None ) cmd = "streamframes stop" await self . _protocol . send_command ( cmd , callback = False )
6176	def iter_chunksize ( num_samples , chunksize ) : last_chunksize = int ( np . mod ( num_samples , chunksize ) ) chunksize = int ( chunksize ) for _ in range ( int ( num_samples ) // chunksize ) : yield chunksize if last_chunksize > 0 : yield last_chunksize
10671	def _finalise_result_ ( compound , value , mass ) : result = value / 3.6E6 result = result / compound . molar_mass result = result * mass return result
4511	def crop ( image , top_offset = 0 , left_offset = 0 , bottom_offset = 0 , right_offset = 0 ) : if bottom_offset or top_offset or left_offset or right_offset : width , height = image . size box = ( left_offset , top_offset , width - right_offset , height - bottom_offset ) image = image . crop ( box = box ) return image
13844	def process_macros ( self , content : str ) -> str : def _sub ( macro ) : name = macro . group ( 'body' ) params = self . get_options ( macro . group ( 'options' ) ) return self . options [ 'macros' ] . get ( name , '' ) . format_map ( params ) return self . pattern . sub ( _sub , content )
77	def project_coords ( coords , from_shape , to_shape ) : from_shape = normalize_shape ( from_shape ) to_shape = normalize_shape ( to_shape ) if from_shape [ 0 : 2 ] == to_shape [ 0 : 2 ] : return coords from_height , from_width = from_shape [ 0 : 2 ] to_height , to_width = to_shape [ 0 : 2 ] assert all ( [ v > 0 for v in [ from_height , from_width , to_height , to_width ] ] ) coords_proj = np . array ( coords ) . astype ( np . float32 ) coords_proj [ : , 0 ] = ( coords_proj [ : , 0 ] / from_width ) * to_width coords_proj [ : , 1 ] = ( coords_proj [ : , 1 ] / from_height ) * to_height return coords_proj
11156	def print_big_file ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table = sorted ( [ ( p , p . size ) for p in self . select_file ( recursive = True ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p , size in size_table [ : top_n ] : print ( "{:<9} {:<9}" . format ( repr_data_size ( size ) , p . abspath ) )
12070	def tryLoadingFrom ( tryPath , moduleName = 'swhlab' ) : if not 'site-packages' in swhlab . __file__ : print ( "loaded custom swhlab module from" , os . path . dirname ( swhlab . __file__ ) ) return while len ( tryPath ) > 5 : sp = tryPath + "/swhlab/" if os . path . isdir ( sp ) and os . path . exists ( sp + "/__init__.py" ) : if not os . path . dirname ( tryPath ) in sys . path : sys . path . insert ( 0 , os . path . dirname ( tryPath ) ) print ( "#" * 80 ) print ( "# WARNING: using site-packages swhlab module" ) print ( "#" * 80 ) tryPath = os . path . dirname ( tryPath ) return
6544	def terminate ( self ) : if not self . is_terminated : log . debug ( "terminal client terminated" ) try : self . exec_command ( b"Quit" ) except BrokenPipeError : pass except socket . error as e : if e . errno != errno . ECONNRESET : raise self . app . close ( ) self . is_terminated = True
13084	def add_tag ( ) : if len ( sys . argv ) > 1 : tag = sys . argv [ 1 ] doc_mapper = DocMapper ( ) if doc_mapper . is_pipe : count = 0 for obj in doc_mapper . get_pipe ( ) : obj . add_tag ( tag ) obj . update ( tags = obj . tags ) count += 1 print_success ( "Added tag '{}' to {} object(s)" . format ( tag , count ) ) else : print_error ( "Please use this script with pipes" ) else : print_error ( "Usage: jk-add-tag <tag>" ) sys . exit ( )
2668	def sixteen_oscillator_two_stimulated_ensembles_grid ( ) : "Not accurate false due to spikes are observed" parameters = legion_parameters ( ) parameters . teta_x = - 1.1 template_dynamic_legion ( 16 , 2000 , 1500 , conn_type = conn_type . GRID_FOUR , params = parameters , stimulus = [ 1 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 1 , 1 ] )
10428	def get_pmids ( graph : BELGraph , output : TextIO ) : for pmid in get_pubmed_identifiers ( graph ) : click . echo ( pmid , file = output )
5727	def _get_responses_windows ( self , timeout_sec ) : timeout_time_sec = time . time ( ) + timeout_sec responses = [ ] while True : try : self . gdb_process . stdout . flush ( ) if PYTHON3 : raw_output = self . gdb_process . stdout . readline ( ) . replace ( b"\r" , b"\n" ) else : raw_output = self . gdb_process . stdout . read ( ) . replace ( b"\r" , b"\n" ) responses += self . _get_responses_list ( raw_output , "stdout" ) except IOError : pass try : self . gdb_process . stderr . flush ( ) if PYTHON3 : raw_output = self . gdb_process . stderr . readline ( ) . replace ( b"\r" , b"\n" ) else : raw_output = self . gdb_process . stderr . read ( ) . replace ( b"\r" , b"\n" ) responses += self . _get_responses_list ( raw_output , "stderr" ) except IOError : pass if time . time ( ) > timeout_time_sec : break return responses
13912	def _AddPropertiesForExtensions ( descriptor , cls ) : extension_dict = descriptor . extensions_by_name for extension_name , extension_field in extension_dict . items ( ) : constant_name = extension_name . upper ( ) + "_FIELD_NUMBER" setattr ( cls , constant_name , extension_field . number )
1195	def calculate_transitive_deps ( modname , script , gopath ) : deps = set ( ) def calc ( modname , script ) : if modname in deps : return deps . add ( modname ) for imp in collect_imports ( modname , script , gopath ) : if imp . is_native : deps . add ( imp . name ) continue parts = imp . name . split ( '.' ) calc ( imp . name , imp . script ) if len ( parts ) == 1 : continue package_dir , filename = os . path . split ( imp . script ) if filename == '__init__.py' : package_dir = os . path . dirname ( package_dir ) for i in xrange ( len ( parts ) - 1 , 0 , - 1 ) : modname = '.' . join ( parts [ : i ] ) script = os . path . join ( package_dir , '__init__.py' ) calc ( modname , script ) package_dir = os . path . dirname ( package_dir ) calc ( modname , script ) deps . remove ( modname ) return deps
11900	def _get_thumbnail_image_from_file ( dir_path , image_file ) : img = _get_image_from_file ( dir_path , image_file ) if img is None : return None if img . format . lower ( ) == 'gif' : return None img_width , img_height = img . size scale_ratio = THUMBNAIL_WIDTH / float ( img_width ) target_height = int ( scale_ratio * img_height ) try : img . thumbnail ( ( THUMBNAIL_WIDTH , target_height ) , resample = RESAMPLE ) except IOError as exptn : print ( 'WARNING: IOError when thumbnailing %s/%s: %s' % ( dir_path , image_file , exptn ) ) return None return img
2318	def predict ( self , data , alpha = 0.01 , max_iter = 2000 , ** kwargs ) : edge_model = GraphLasso ( alpha = alpha , max_iter = max_iter ) edge_model . fit ( data . values ) return nx . relabel_nodes ( nx . DiGraph ( edge_model . get_precision ( ) ) , { idx : i for idx , i in enumerate ( data . columns ) } )
4694	def regex_find ( pattern , content ) : find = re . findall ( pattern , content ) if not find : cij . err ( "pattern <%r> is invalid, no matches!" % pattern ) cij . err ( "content: %r" % content ) return '' if len ( find ) >= 2 : cij . err ( "pattern <%r> is too simple, matched more than 2!" % pattern ) cij . err ( "content: %r" % content ) return '' return find [ 0 ]
7702	def get_items_by_name ( self , name , case_sensitive = True ) : if not case_sensitive and name : name = name . lower ( ) result = [ ] for item in self . _items : if item . name == name : result . append ( item ) elif item . name is None : continue elif not case_sensitive and item . name . lower ( ) == name : result . append ( item ) return result
1243	def sample_minibatch ( self , batch_size ) : pool_size = len ( self ) if pool_size == 0 : return [ ] delta_p = self . _memory [ 0 ] / batch_size chosen_idx = [ ] if abs ( self . _memory [ 0 ] ) < util . epsilon : chosen_idx = np . random . randint ( self . _capacity - 1 , self . _capacity - 1 + len ( self ) , size = batch_size ) . tolist ( ) else : for i in xrange ( batch_size ) : lower = max ( i * delta_p , 0 ) upper = min ( ( i + 1 ) * delta_p , self . _memory [ 0 ] ) p = random . uniform ( lower , upper ) chosen_idx . append ( self . _sample_with_priority ( p ) ) return [ ( i , self . _memory [ i ] ) for i in chosen_idx ]
9344	def read ( self , n ) : while len ( self . pool ) < n : self . cur = self . files . next ( ) self . pool = numpy . append ( self . pool , self . fetch ( self . cur ) , axis = 0 ) rt = self . pool [ : n ] if n == len ( self . pool ) : self . pool = self . fetch ( None ) else : self . pool = self . pool [ n : ] return rt
707	def _engineServicesRunning ( ) : process = subprocess . Popen ( [ "ps" , "aux" ] , stdout = subprocess . PIPE ) stdout = process . communicate ( ) [ 0 ] result = process . returncode if result != 0 : raise RuntimeError ( "Unable to check for running client job manager" ) running = False for line in stdout . split ( "\n" ) : if "python" in line and "clientjobmanager.client_job_manager" in line : running = True break return running
3006	def get_storage ( request ) : storage_model = oauth2_settings . storage_model user_property = oauth2_settings . storage_model_user_property credentials_property = oauth2_settings . storage_model_credentials_property if storage_model : module_name , class_name = storage_model . rsplit ( '.' , 1 ) module = importlib . import_module ( module_name ) storage_model_class = getattr ( module , class_name ) return storage . DjangoORMStorage ( storage_model_class , user_property , request . user , credentials_property ) else : return dictionary_storage . DictionaryStorage ( request . session , key = _CREDENTIALS_KEY )
12068	def new ( ABF , forceNewFigure = False , title = None , xlabel = None , ylabel = None ) : if len ( pylab . get_fignums ( ) ) and forceNewFigure == False : return pylab . figure ( figsize = ( 8 , 6 ) ) pylab . grid ( alpha = .5 ) pylab . title ( ABF . ID ) pylab . ylabel ( ABF . units ) pylab . xlabel ( "seconds" ) if xlabel : pylab . xlabel ( xlabel ) if ylabel : pylab . ylabel ( ylabel ) if title : pylab . title ( title ) annotate ( ABF )
2713	def get_object ( cls , api_token , snapshot_id ) : snapshot = cls ( token = api_token , id = snapshot_id ) snapshot . load ( ) return snapshot
2155	def set_or_reset_runtime_param ( self , key , value ) : if self . _runtime . has_option ( 'general' , key ) : self . _runtime = self . _new_parser ( ) if value is None : return settings . _runtime . set ( 'general' , key . replace ( 'tower_' , '' ) , six . text_type ( value ) )
13431	def admin_link_move_up ( obj , link_text = 'up' ) : if obj . rank == 1 : return '' content_type = ContentType . objects . get_for_model ( obj ) link = reverse ( 'awl-rankedmodel-move' , args = ( content_type . id , obj . id , obj . rank - 1 ) ) return '<a href="%s">%s</a>' % ( link , link_text )
4818	def parse_lms_api_datetime ( datetime_string , datetime_format = LMS_API_DATETIME_FORMAT ) : if isinstance ( datetime_string , datetime . datetime ) : date_time = datetime_string else : try : date_time = datetime . datetime . strptime ( datetime_string , datetime_format ) except ValueError : date_time = datetime . datetime . strptime ( datetime_string , LMS_API_DATETIME_FORMAT_WITHOUT_TIMEZONE ) if date_time . tzinfo is None : date_time = date_time . replace ( tzinfo = timezone . utc ) return date_time
12412	def write ( self , chunk , serialize = False , format = None ) : self . require_not_closed ( ) if chunk is None : return if serialize or format is not None : self . serialize ( chunk , format = format ) return if type ( chunk ) is six . binary_type : self . _length += len ( chunk ) self . _stream . write ( chunk ) elif isinstance ( chunk , six . string_types ) : encoding = self . encoding if encoding is not None : chunk = chunk . encode ( encoding ) else : raise exceptions . InvalidOperation ( 'Attempting to write textual data without an encoding.' ) self . _length += len ( chunk ) self . _stream . write ( chunk ) elif isinstance ( chunk , collections . Iterable ) : for section in chunk : self . write ( section ) else : raise exceptions . InvalidOperation ( 'Attempting to write something not recognized.' )
9269	def version_of_first_item ( self ) : try : sections = read_changelog ( self . options ) return sections [ 0 ] [ "version" ] except ( IOError , TypeError ) : return self . get_temp_tag_for_repo_creation ( )
6280	def clear_values ( self , red = 0.0 , green = 0.0 , blue = 0.0 , alpha = 0.0 , depth = 1.0 ) : self . clear_color = ( red , green , blue , alpha ) self . clear_depth = depth
1846	def JS ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . SF , target . read ( ) , cpu . PC )
11006	def get_active_bets ( self , project_id = None ) : url = urljoin ( self . settings [ 'bets_url' ] , 'bets?state=fresh,active,accept_end&page=1&page_size=100' ) if project_id is not None : url += '&kava_project_id={}' . format ( project_id ) bets = [ ] has_next_page = True while has_next_page : res = self . _req ( url ) bets . extend ( res [ 'bets' ] [ 'results' ] ) url = res [ 'bets' ] . get ( 'next' ) has_next_page = bool ( url ) return bets
2355	def find_elements ( self , strategy , locator ) : return self . driver_adapter . find_elements ( strategy , locator , root = self . root )
10446	def launchapp ( self , cmd , args = [ ] , delay = 0 , env = 1 , lang = "C" ) : try : atomac . NativeUIElement . launchAppByBundleId ( cmd ) return 1 except RuntimeError : if atomac . NativeUIElement . launchAppByBundlePath ( cmd , args ) : try : time . sleep ( int ( delay ) ) except ValueError : time . sleep ( 5 ) return 1 else : raise LdtpServerException ( u"Unable to find app '%s'" % cmd )
3225	def iter_project ( projects , key_file = None ) : def decorator ( func ) : @ wraps ( func ) def decorated_function ( * args , ** kwargs ) : item_list = [ ] exception_map = { } for project in projects : if isinstance ( project , string_types ) : kwargs [ 'project' ] = project if key_file : kwargs [ 'key_file' ] = key_file elif isinstance ( project , dict ) : kwargs [ 'project' ] = project [ 'project' ] kwargs [ 'key_file' ] = project [ 'key_file' ] itm , exc = func ( * args , ** kwargs ) item_list . extend ( itm ) exception_map . update ( exc ) return ( item_list , exception_map ) return decorated_function return decorator
7546	def make_chunks ( data , samples , lbview ) : start = time . time ( ) printstr = " chunking clusters | {} | s5 |" elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 10 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) lasyncs = { } for sample in samples : lasyncs [ sample . name ] = lbview . apply ( chunk_clusters , * ( data , sample ) ) while 1 : ready = [ i . ready ( ) for i in lasyncs . values ( ) ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( "" ) break for sample in samples : if not lasyncs [ sample . name ] . successful ( ) : LOGGER . error ( " sample %s failed: %s" , sample . name , lasyncs [ sample . name ] . exception ( ) ) return lasyncs
12461	def pip_cmd ( env , cmd , ignore_activated = False , ** kwargs ) : r cmd = tuple ( cmd ) dirname = safe_path ( env ) if not ignore_activated : activated_env = os . environ . get ( 'VIRTUAL_ENV' ) if hasattr ( sys , 'real_prefix' ) : dirname = sys . prefix elif activated_env : dirname = activated_env pip_path = os . path . join ( dirname , 'Scripts' if IS_WINDOWS else 'bin' , 'pip' ) if kwargs . pop ( 'return_path' , False ) : return pip_path if not os . path . isfile ( pip_path ) : raise OSError ( 'No pip found at {0!r}' . format ( pip_path ) ) if BOOTSTRAPPER_TEST_KEY in os . environ and cmd [ 0 ] == 'install' : cmd = list ( cmd ) cmd . insert ( 1 , '--disable-pip-version-check' ) cmd = tuple ( cmd ) with disable_error_handler ( ) : return run_cmd ( ( pip_path , ) + cmd , ** kwargs )
2911	def _find_ancestor_from_name ( self , name ) : if self . parent is None : return None if self . parent . get_name ( ) == name : return self . parent return self . parent . _find_ancestor_from_name ( name )
4720	def tcase_enter ( trun , tsuite , tcase ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tcase:enter" ) cij . emph ( "rnr:tcase:enter { fname: %r }" % tcase [ "fname" ] ) cij . emph ( "rnr:tcase:enter { log_fpath: %r }" % tcase [ "log_fpath" ] ) rcode = 0 for hook in tcase [ "hooks" ] [ "enter" ] : rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tcase:exit: { rcode: %r }" % rcode , rcode ) return rcode
6309	def draw ( self , time , frametime , target ) : for effect in self . effects : value = effect . rocket_timeline_track . time_value ( time ) if value > 0.5 : effect . draw ( time , frametime , target )
9032	def _place_row ( self , row , position ) : self . _rows_in_grid [ row ] = RowInGrid ( row , position )
8022	def cast ( cls , fx_spot , domestic_curve = None , foreign_curve = None ) : assert domestic_curve . origin == foreign_curve . origin return cls ( fx_spot , domestic_curve = domestic_curve , foreign_curve = foreign_curve )
10903	def examine_unexplained_noise ( state , bins = 1000 , xlim = ( - 10 , 10 ) ) : r = state . residuals q = np . fft . fftn ( r ) calc_sig = lambda x : np . sqrt ( np . dot ( x , x ) / x . size ) rh , xr = np . histogram ( r . ravel ( ) / calc_sig ( r . ravel ( ) ) , bins = bins , density = True ) bigq = np . append ( q . real . ravel ( ) , q . imag . ravel ( ) ) qh , xq = np . histogram ( bigq / calc_sig ( q . real . ravel ( ) ) , bins = bins , density = True ) xr = 0.5 * ( xr [ 1 : ] + xr [ : - 1 ] ) xq = 0.5 * ( xq [ 1 : ] + xq [ : - 1 ] ) gauss = lambda t : np . exp ( - t * t * 0.5 ) / np . sqrt ( 2 * np . pi ) plt . figure ( figsize = [ 16 , 8 ] ) axes = [ ] for a , ( x , r , lbl ) in enumerate ( [ [ xr , rh , 'Real' ] , [ xq , qh , 'Fourier' ] ] ) : ax = plt . subplot ( 1 , 2 , a + 1 ) ax . semilogy ( x , r , label = 'Data' ) ax . plot ( x , gauss ( x ) , label = 'Gauss Fit' , scalex = False , scaley = False ) ax . set_xlabel ( 'Residuals value $r/\sigma$' ) ax . set_ylabel ( 'Probability $P(r/\sigma)$' ) ax . legend ( loc = 'upper right' ) ax . set_title ( '{}-Space' . format ( lbl ) ) ax . set_xlim ( xlim ) axes . append ( ax ) return axes
11614	def export_posterior_probability ( self , filename , title = "Posterior Probability" ) : self . probability . save ( h5file = filename , title = title )
250	def adjust_returns_for_slippage ( returns , positions , transactions , slippage_bps ) : slippage = 0.0001 * slippage_bps portfolio_value = positions . sum ( axis = 1 ) pnl = portfolio_value * returns traded_value = get_txn_vol ( transactions ) . txn_volume slippage_dollars = traded_value * slippage adjusted_pnl = pnl . add ( - slippage_dollars , fill_value = 0 ) adjusted_returns = returns * adjusted_pnl / pnl return adjusted_returns
8576	def get_request ( self , request_id , status = False ) : if status : response = self . _perform_request ( '/requests/' + request_id + '/status' ) else : response = self . _perform_request ( '/requests/%s' % request_id ) return response
13551	def _put_resource ( self , url , body ) : headers = { "Content-Type" : "application/json" , "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = WhenIWork_DAO ( ) . putURL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 201 or response . status == 204 ) : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
12745	def pid ( kp = 0. , ki = 0. , kd = 0. , smooth = 0.1 ) : r state = dict ( p = 0 , i = 0 , d = 0 ) def control ( error , dt = 1 ) : state [ 'd' ] = smooth * state [ 'd' ] + ( 1 - smooth ) * ( error - state [ 'p' ] ) / dt state [ 'i' ] += error * dt state [ 'p' ] = error return kp * state [ 'p' ] + ki * state [ 'i' ] + kd * state [ 'd' ] return control
5434	def parse_tasks_file_header ( header , input_file_param_util , output_file_param_util ) : job_params = [ ] for col in header : col_type = '--env' col_value = col if col . startswith ( '-' ) : col_type , col_value = split_pair ( col , ' ' , 1 ) if col_type == '--env' : job_params . append ( job_model . EnvParam ( col_value ) ) elif col_type == '--label' : job_params . append ( job_model . LabelParam ( col_value ) ) elif col_type == '--input' or col_type == '--input-recursive' : name = input_file_param_util . get_variable_name ( col_value ) job_params . append ( job_model . InputFileParam ( name , recursive = ( col_type . endswith ( 'recursive' ) ) ) ) elif col_type == '--output' or col_type == '--output-recursive' : name = output_file_param_util . get_variable_name ( col_value ) job_params . append ( job_model . OutputFileParam ( name , recursive = ( col_type . endswith ( 'recursive' ) ) ) ) else : raise ValueError ( 'Unrecognized column header: %s' % col ) return job_params
9350	def check_digit ( num ) : sum = 0 digits = str ( num ) [ : - 1 ] [ : : - 1 ] for i , n in enumerate ( digits ) : if ( i + 1 ) % 2 != 0 : digit = int ( n ) * 2 if digit > 9 : sum += ( digit - 9 ) else : sum += digit else : sum += int ( n ) return ( ( divmod ( sum , 10 ) [ 0 ] + 1 ) * 10 - sum ) % 10
5885	def get_canonical_link ( self ) : if self . article . final_url : kwargs = { 'tag' : 'link' , 'attr' : 'rel' , 'value' : 'canonical' } meta = self . parser . getElementsByTag ( self . article . doc , ** kwargs ) if meta is not None and len ( meta ) > 0 : href = self . parser . getAttribute ( meta [ 0 ] , 'href' ) if href : href = href . strip ( ) o = urlparse ( href ) if not o . hostname : tmp = urlparse ( self . article . final_url ) domain = '%s://%s' % ( tmp . scheme , tmp . hostname ) href = urljoin ( domain , href ) return href return self . article . final_url
5223	def ccy_pair ( local , base = 'USD' ) -> CurrencyPair : ccy_param = param . load_info ( cat = 'ccy' ) if f'{local}{base}' in ccy_param : info = ccy_param [ f'{local}{base}' ] elif f'{base}{local}' in ccy_param : info = ccy_param [ f'{base}{local}' ] info [ 'factor' ] = 1. / info . get ( 'factor' , 1. ) info [ 'power' ] = - info . get ( 'power' , 1 ) elif base . lower ( ) == local . lower ( ) : info = dict ( ticker = '' ) info [ 'factor' ] = 1. if base [ - 1 ] . lower ( ) == base [ - 1 ] : info [ 'factor' ] /= 100. if local [ - 1 ] . lower ( ) == local [ - 1 ] : info [ 'factor' ] *= 100. else : logger = logs . get_logger ( ccy_pair ) logger . error ( f'incorrect currency - local {local} / base {base}' ) return CurrencyPair ( ticker = '' , factor = 1. , power = 1 ) if 'factor' not in info : info [ 'factor' ] = 1. if 'power' not in info : info [ 'power' ] = 1 return CurrencyPair ( ** info )
10463	def menuitemenabled ( self , window_name , object_name ) : try : menu_handle = self . _get_menu_handle ( window_name , object_name , False ) if menu_handle . AXEnabled : return 1 except LdtpServerException : pass return 0
10507	def server_bind ( self , * args , ** kwargs ) : self . socket . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) SimpleXMLRPCServer . server_bind ( self , * args , ** kwargs )
12565	def get_3D_from_4D ( image , vol_idx = 0 ) : img = check_img ( image ) hdr , aff = get_img_info ( img ) if len ( img . shape ) != 4 : raise AttributeError ( 'Volume in {} does not have 4 dimensions.' . format ( repr_imgs ( img ) ) ) if not 0 <= vol_idx < img . shape [ 3 ] : raise IndexError ( 'IndexError: 4th dimension in volume {} has {} volumes, ' 'not {}.' . format ( repr_imgs ( img ) , img . shape [ 3 ] , vol_idx ) ) img_data = img . get_data ( ) new_vol = img_data [ : , : , : , vol_idx ] . copy ( ) hdr . set_data_shape ( hdr . get_data_shape ( ) [ : 3 ] ) return new_vol , hdr , aff
3243	def get_rules ( security_group , ** kwargs ) : rules = security_group . pop ( 'security_group_rules' , [ ] ) for rule in rules : rule [ 'ip_protocol' ] = rule . pop ( 'protocol' ) rule [ 'from_port' ] = rule . pop ( 'port_range_max' ) rule [ 'to_port' ] = rule . pop ( 'port_range_min' ) rule [ 'cidr_ip' ] = rule . pop ( 'remote_ip_prefix' ) rule [ 'rule_type' ] = rule . pop ( 'direction' ) security_group [ 'rules' ] = sorted ( rules ) return security_group
10319	def _microcanonical_average_max_cluster_size ( max_cluster_size , alpha ) : ret = dict ( ) runs = max_cluster_size . size sqrt_n = np . sqrt ( runs ) max_cluster_size_sample_mean = max_cluster_size . mean ( ) ret [ 'max_cluster_size' ] = max_cluster_size_sample_mean max_cluster_size_sample_std = max_cluster_size . std ( ddof = 1 ) if max_cluster_size_sample_std : old_settings = np . seterr ( all = 'raise' ) ret [ 'max_cluster_size_ci' ] = scipy . stats . t . interval ( 1 - alpha , df = runs - 1 , loc = max_cluster_size_sample_mean , scale = max_cluster_size_sample_std / sqrt_n ) np . seterr ( ** old_settings ) else : ret [ 'max_cluster_size_ci' ] = ( max_cluster_size_sample_mean * np . ones ( 2 ) ) return ret
4935	def chunks ( dictionary , chunk_size ) : iterable = iter ( dictionary ) for __ in range ( 0 , len ( dictionary ) , chunk_size ) : yield { key : dictionary [ key ] for key in islice ( iterable , chunk_size ) }
785	def jobCountCancellingJobs ( self , ) : with ConnectionFactory . get ( ) as conn : query = 'SELECT COUNT(job_id) ' 'FROM %s ' 'WHERE (status<>%%s AND cancel is TRUE)' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . STATUS_COMPLETED ] ) rows = conn . cursor . fetchall ( ) return rows [ 0 ] [ 0 ]
6464	def usage_palette ( parser ) : parser . print_usage ( ) print ( '' ) print ( 'available palettes:' ) for palette in sorted ( PALETTE ) : print ( ' %-12s' % ( palette , ) ) return 0
8789	def _pop ( self , model ) : tags = [ ] for tag in model . tags : if self . is_tag ( tag ) : tags . append ( tag ) if tags : for tag in tags : model . tags . remove ( tag ) return tags
2386	def create_model_path ( model_path ) : if not model_path . startswith ( "/" ) and not model_path . startswith ( "models/" ) : model_path = "/" + model_path if not model_path . startswith ( "models" ) : model_path = "models" + model_path if not model_path . endswith ( ".p" ) : model_path += ".p" return model_path
7923	def as_unicode ( self ) : result = self . domain if self . local : result = self . local + u'@' + result if self . resource : result = result + u'/' + self . resource if not JID . cache . has_key ( result ) : JID . cache [ result ] = self return result
13552	def _post_resource ( self , url , body ) : headers = { "Content-Type" : "application/json" , "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = WhenIWork_DAO ( ) . postURL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 204 ) : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
8404	def squish ( x , range = ( 0 , 1 ) , only_finite = True ) : xtype = type ( x ) if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) finite = np . isfinite ( x ) if only_finite else True x [ np . logical_and ( x < range [ 0 ] , finite ) ] = range [ 0 ] x [ np . logical_and ( x > range [ 1 ] , finite ) ] = range [ 1 ] if not isinstance ( x , xtype ) : x = xtype ( x ) return x
551	def __checkMaturity ( self ) : if self . _currentRecordIndex + 1 < self . _MIN_RECORDS_TO_BE_BEST : return if self . _isMature : return metric = self . _getMetrics ( ) [ self . _optimizedMetricLabel ] self . _metricRegression . addPoint ( x = self . _currentRecordIndex , y = metric ) pctChange , absPctChange = self . _metricRegression . getPctChanges ( ) if pctChange is not None and absPctChange <= self . _MATURITY_MAX_CHANGE : self . _jobsDAO . modelSetFields ( self . _modelID , { 'engMatured' : True } ) self . _cmpReason = ClientJobsDAO . CMPL_REASON_STOPPED self . _isMature = True self . _logger . info ( "Model %d has matured (pctChange=%s, n=%d). \n" "Scores = %s\n" "Stopping execution" , self . _modelID , pctChange , self . _MATURITY_NUM_POINTS , self . _metricRegression . _window )
5789	def handle_openssl_error ( result , exception_class = None ) : if result > 0 : return if exception_class is None : exception_class = OSError error_num = libcrypto . ERR_get_error ( ) buffer = buffer_from_bytes ( 120 ) libcrypto . ERR_error_string ( error_num , buffer ) error_string = byte_string_from_buffer ( buffer ) raise exception_class ( _try_decode ( error_string ) )
8466	def run ( self ) : options = { } if bool ( self . config [ 'use_proxy' ] ) : options [ 'proxies' ] = { "http" : self . config [ 'proxy' ] , "https" : self . config [ 'proxy' ] } options [ "url" ] = self . config [ 'url' ] options [ "data" ] = { "issues" : json . dumps ( map ( lambda x : x . __todict__ ( ) , self . issues ) ) } if 'get' == self . config [ 'method' ] . lower ( ) : requests . get ( ** options ) else : requests . post ( ** options )
1038	def end ( self ) : return Range ( self . source_buffer , self . end_pos , self . end_pos , expanded_from = self . expanded_from )
7699	def verify_roster_push ( self , fix = False ) : self . _verify ( ( None , u"from" , u"to" , u"both" , u"remove" ) , fix )
5362	def validate_config ( self ) : for key , key_config in self . params_map . items ( ) : if key_config [ 'required' ] : if key not in self . config : raise ValueError ( "Invalid Configuration! Required parameter '%s' was not provided to Sultan." ) for key in self . config . keys ( ) : if key not in self . params_map : raise ValueError ( "Invalid Configuration! The parameter '%s' provided is not used by Sultan!" % key )
11477	def _create_or_reuse_folder ( local_folder , parent_folder_id , reuse_existing = False ) : local_folder_name = os . path . basename ( local_folder ) folder_id = None if reuse_existing : children = session . communicator . folder_children ( session . token , parent_folder_id ) folders = children [ 'folders' ] for folder in folders : if folder [ 'name' ] == local_folder_name : folder_id = folder [ 'folder_id' ] break if folder_id is None : new_folder = session . communicator . create_folder ( session . token , local_folder_name , parent_folder_id ) folder_id = new_folder [ 'folder_id' ] return folder_id
9676	def _calculate_period ( self , vals ) : if len ( vals ) < 4 : return None if self . firmware [ 'major' ] < 16 : return ( ( vals [ 3 ] << 24 ) | ( vals [ 2 ] << 16 ) | ( vals [ 1 ] << 8 ) | vals [ 0 ] ) / 12e6 else : return self . _calculate_float ( vals )
10003	def rename ( self , name ) : self . _impl . system . rename_model ( new_name = name , old_name = self . name )
9664	def construct_graph ( sakefile , settings ) : verbose = settings [ "verbose" ] sprint = settings [ "sprint" ] G = nx . DiGraph ( ) sprint ( "Going to construct Graph" , level = "verbose" ) for target in sakefile : if target == "all" : continue if "formula" not in sakefile [ target ] : for atomtarget in sakefile [ target ] : if atomtarget == "help" : continue sprint ( "Adding '{}'" . format ( atomtarget ) , level = "verbose" ) data_dict = sakefile [ target ] [ atomtarget ] data_dict [ "parent" ] = target G . add_node ( atomtarget , ** data_dict ) else : sprint ( "Adding '{}'" . format ( target ) , level = "verbose" ) G . add_node ( target , ** sakefile [ target ] ) sprint ( "Nodes are built\nBuilding connections" , level = "verbose" ) for node in G . nodes ( data = True ) : sprint ( "checking node {} for dependencies" . format ( node [ 0 ] ) , level = "verbose" ) for k , v in node [ 1 ] . items ( ) : if v is None : node [ 1 ] [ k ] = [ ] if "output" in node [ 1 ] : for index , out in enumerate ( node [ 1 ] [ 'output' ] ) : node [ 1 ] [ 'output' ] [ index ] = clean_path ( node [ 1 ] [ 'output' ] [ index ] ) if "dependencies" not in node [ 1 ] : continue sprint ( "it has dependencies" , level = "verbose" ) connects = [ ] for index , dep in enumerate ( node [ 1 ] [ 'dependencies' ] ) : dep = os . path . normpath ( dep ) shrt = "dependencies" node [ 1 ] [ 'dependencies' ] [ index ] = clean_path ( node [ 1 ] [ shrt ] [ index ] ) for node in G . nodes ( data = True ) : connects = [ ] if "dependencies" not in node [ 1 ] : continue for dep in node [ 1 ] [ 'dependencies' ] : matches = check_for_dep_in_outputs ( dep , verbose , G ) if not matches : continue for match in matches : sprint ( "Appending {} to matches" . format ( match ) , level = "verbose" ) connects . append ( match ) if connects : for connect in connects : G . add_edge ( connect , node [ 0 ] ) return G
3350	def _generate_index ( self ) : self . _dict = { v . id : k for k , v in enumerate ( self ) }
1096	def free_temp ( self , v ) : self . used_temps . remove ( v ) self . free_temps . add ( v )
4381	def exempt ( self , resource ) : if resource not in self . _exempt : self . _exempt . append ( resource )
6455	def dist_mlipns ( src , tar , threshold = 0.25 , max_mismatches = 2 ) : return MLIPNS ( ) . dist ( src , tar , threshold , max_mismatches )
9415	def from_value ( cls , value ) : instance = OctaveUserClass . __new__ ( cls ) instance . _address = '%s_%s' % ( instance . _name , id ( instance ) ) instance . _ref ( ) . push ( instance . _address , value ) return instance
11473	def parse_data ( self , text , maxwidth , maxheight , template_dir , context , urlize_all_links ) : block_parser = TextBlockParser ( ) lines = text . splitlines ( ) parsed = [ ] for line in lines : if STANDALONE_URL_RE . match ( line ) : user_url = line . strip ( ) try : resource = oembed . site . embed ( user_url , maxwidth = maxwidth , maxheight = maxheight ) context [ 'minwidth' ] = min ( maxwidth , resource . width ) context [ 'minheight' ] = min ( maxheight , resource . height ) except OEmbedException : if urlize_all_links : line = '<a href="%(LINK)s">%(LINK)s</a>' % { 'LINK' : user_url } else : context [ 'minwidth' ] = min ( maxwidth , resource . width ) context [ 'minheight' ] = min ( maxheight , resource . height ) line = self . render_oembed ( resource , user_url , template_dir = template_dir , context = context ) else : line = block_parser . parse ( line , maxwidth , maxheight , 'inline' , context , urlize_all_links ) parsed . append ( line ) return mark_safe ( '\n' . join ( parsed ) )
5038	def enroll_user ( cls , enterprise_customer , user , course_mode , * course_ids ) : enterprise_customer_user , __ = EnterpriseCustomerUser . objects . get_or_create ( enterprise_customer = enterprise_customer , user_id = user . id ) enrollment_client = EnrollmentApiClient ( ) succeeded = True for course_id in course_ids : try : enrollment_client . enroll_user_in_course ( user . username , course_id , course_mode ) except HttpClientError as exc : if cls . is_user_enrolled ( user , course_id , course_mode ) : succeeded = True else : succeeded = False default_message = 'No error message provided' try : error_message = json . loads ( exc . content . decode ( ) ) . get ( 'message' , default_message ) except ValueError : error_message = default_message logging . error ( 'Error while enrolling user %(user)s: %(message)s' , dict ( user = user . username , message = error_message ) ) if succeeded : __ , created = EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = enterprise_customer_user , course_id = course_id ) if created : track_enrollment ( 'admin-enrollment' , user . id , course_id ) return succeeded
1477	def _get_instance_plans ( self , packing_plan , container_id ) : this_container_plan = None for container_plan in packing_plan . container_plans : if container_plan . id == container_id : this_container_plan = container_plan if this_container_plan is None : return None return this_container_plan . instance_plans
10143	def decrypt_files ( file_link ) : if ENCRYPTION_DISABLED : print ( 'For decryption please install gpg' ) exit ( ) try : parsed_link = re . findall ( r'(.*/(.*))#(.{30})' , file_link ) [ 0 ] req = urllib . request . Request ( parsed_link [ 0 ] , data = None , headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) ' ' AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36' } ) file_response = urllib . request . urlopen ( req ) file_to_decrypt = file_response . read ( ) decrypt_r , decrypt_w = os . pipe ( ) cmd = 'gpg --batch --decrypt --passphrase-fd {}' . format ( decrypt_r ) decrypt_output = Popen ( shlex . split ( cmd ) , stdout = PIPE , stdin = PIPE , stderr = PIPE , pass_fds = ( decrypt_r , ) ) os . close ( decrypt_r ) open ( decrypt_w , 'w' ) . write ( parsed_link [ 2 ] ) decrypted_data , stderr = decrypt_output . communicate ( file_to_decrypt ) with open ( parsed_link [ 1 ] , 'wb' ) as decrypted_file : decrypted_file . write ( decrypted_data ) return parsed_link [ 1 ] + ' is decrypted and saved.' except IndexError : return 'Please enter valid link.'
10629	def HHV ( self , HHV ) : self . _HHV = HHV if self . isCoal : self . _DH298 = self . _calculate_DH298_coal ( )
4638	def shared_blockchain_instance ( self ) : if not self . _sharedInstance . instance : klass = self . get_instance_class ( ) self . _sharedInstance . instance = klass ( ** self . _sharedInstance . config ) return self . _sharedInstance . instance
11682	def _readline ( self ) : line = '' while 1 : readable , _ , __ = select . select ( [ self . sock ] , [ ] , [ ] , 0.5 ) if self . _stop : break if not readable : continue data = readable [ 0 ] . recv ( 1 ) if data == '\n' : break line += unicode ( data , self . encoding ) return line
4310	def _build_input_format_list ( input_filepath_list , input_volumes = None , input_format = None ) : n_inputs = len ( input_filepath_list ) input_format_list = [ ] for _ in range ( n_inputs ) : input_format_list . append ( [ ] ) if input_volumes is None : vols = [ 1 ] * n_inputs else : n_volumes = len ( input_volumes ) if n_volumes < n_inputs : logger . warning ( 'Volumes were only specified for %s out of %s files.' 'The last %s files will remain at their original volumes.' , n_volumes , n_inputs , n_inputs - n_volumes ) vols = input_volumes + [ 1 ] * ( n_inputs - n_volumes ) elif n_volumes > n_inputs : logger . warning ( '%s volumes were specified but only %s input files exist.' 'The last %s volumes will be ignored.' , n_volumes , n_inputs , n_volumes - n_inputs ) vols = input_volumes [ : n_inputs ] else : vols = [ v for v in input_volumes ] if input_format is None : fmts = [ [ ] for _ in range ( n_inputs ) ] else : n_fmts = len ( input_format ) if n_fmts < n_inputs : logger . warning ( 'Input formats were only specified for %s out of %s files.' 'The last %s files will remain unformatted.' , n_fmts , n_inputs , n_inputs - n_fmts ) fmts = [ f for f in input_format ] fmts . extend ( [ [ ] for _ in range ( n_inputs - n_fmts ) ] ) elif n_fmts > n_inputs : logger . warning ( '%s Input formats were specified but only %s input files exist' '. The last %s formats will be ignored.' , n_fmts , n_inputs , n_fmts - n_inputs ) fmts = input_format [ : n_inputs ] else : fmts = [ f for f in input_format ] for i , ( vol , fmt ) in enumerate ( zip ( vols , fmts ) ) : input_format_list [ i ] . extend ( [ '-v' , '{}' . format ( vol ) ] ) input_format_list [ i ] . extend ( fmt ) return input_format_list
2555	def add ( self , * args ) : for obj in args : if isinstance ( obj , numbers . Number ) : obj = str ( obj ) if isinstance ( obj , basestring ) : obj = escape ( obj ) self . children . append ( obj ) elif isinstance ( obj , dom_tag ) : ctx = dom_tag . _with_contexts [ _get_thread_context ( ) ] if ctx and ctx [ - 1 ] : ctx [ - 1 ] . used . add ( obj ) self . children . append ( obj ) obj . parent = self obj . setdocument ( self . document ) elif isinstance ( obj , dict ) : for attr , value in obj . items ( ) : self . set_attribute ( * dom_tag . clean_pair ( attr , value ) ) elif hasattr ( obj , '__iter__' ) : for subobj in obj : self . add ( subobj ) else : raise ValueError ( '%r not a tag or string.' % obj ) if len ( args ) == 1 : return args [ 0 ] return args
12937	def depricated_name ( newmethod ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : warnings . simplefilter ( 'always' , DeprecationWarning ) warnings . warn ( "Function {} is depricated, please use {} instead." . format ( func . __name__ , newmethod ) , category = DeprecationWarning , stacklevel = 2 ) warnings . simplefilter ( 'default' , DeprecationWarning ) return func ( * args , ** kwargs ) return wrapper return decorator
7907	def __error_message ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : return False rs . process_error_message ( stanza ) return True
2276	def _win32_dir ( path , star = '' ) : from ubelt import util_cmd import re wrapper = 'cmd /S /C "{}"' command = 'dir /-C "{}"{}' . format ( path , star ) wrapped = wrapper . format ( command ) info = util_cmd . cmd ( wrapped , shell = True ) if info [ 'ret' ] != 0 : from ubelt import util_format print ( 'Failed command:' ) print ( info [ 'command' ] ) print ( util_format . repr2 ( info , nl = 1 ) ) raise OSError ( str ( info ) ) lines = info [ 'out' ] . split ( '\n' ) [ 5 : - 3 ] splitter = re . compile ( '( +)' ) for line in lines : parts = splitter . split ( line ) date , sep , time , sep , ampm , sep , type_or_size , sep = parts [ : 8 ] name = '' . join ( parts [ 8 : ] ) if name == '.' or name == '..' : continue if type_or_size in [ '<JUNCTION>' , '<SYMLINKD>' , '<SYMLINK>' ] : pos = name . find ( ':' ) bpos = name [ : pos ] . rfind ( '[' ) name = name [ : bpos - 1 ] pointed = name [ bpos + 1 : - 1 ] yield type_or_size , name , pointed else : yield type_or_size , name , None
8086	def strokewidth ( self , w = None ) : if w is not None : self . _canvas . strokewidth = w else : return self . _canvas . strokewidth
12774	def inverse_kinematics ( self , start = 0 , end = 1e100 , states = None , max_force = 20 ) : zeros = None if max_force > 0 : self . skeleton . enable_motors ( max_force ) zeros = np . zeros ( self . skeleton . num_dofs ) for _ in self . follow_markers ( start , end , states ) : if zeros is not None : self . skeleton . set_target_angles ( zeros ) yield self . skeleton . joint_angles
12978	def deleteByPk ( self , pk ) : obj = self . mdl . objects . getOnlyIndexedFields ( pk ) if not obj : return 0 return self . deleteOne ( obj )
7438	def files ( self ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) return pd . DataFrame ( [ self . samples [ i ] . files for i in nameordered ] , index = nameordered ) . dropna ( axis = 1 , how = 'all' )
13755	def write_to_file ( file_path , contents , encoding = "utf-8" ) : with codecs . open ( file_path , "w" , encoding ) as f : f . write ( contents )
7891	def set_stream ( self , stream ) : _unused = stream if self . joined and self . handler : self . handler . user_left ( self . me , None ) self . joined = False
5188	def connect ( host = 'localhost' , port = 8080 , ssl_verify = False , ssl_key = None , ssl_cert = None , timeout = 10 , protocol = None , url_path = '/' , username = None , password = None , token = None ) : return BaseAPI ( host = host , port = port , timeout = timeout , ssl_verify = ssl_verify , ssl_key = ssl_key , ssl_cert = ssl_cert , protocol = protocol , url_path = url_path , username = username , password = password , token = token )
999	def printColConfidence ( self , aState , maxCols = 20 ) : def formatFPRow ( var ) : s = '' for c in range ( min ( maxCols , self . numberOfCols ) ) : if c > 0 and c % 10 == 0 : s += ' ' s += ' %5.3f' % var [ c ] s += ' ' return s print formatFPRow ( aState )
12819	def _file_size ( self , field ) : size = 0 try : handle = open ( self . _files [ field ] , "r" ) size = os . fstat ( handle . fileno ( ) ) . st_size handle . close ( ) except : size = 0 self . _file_lengths [ field ] = size return self . _file_lengths [ field ]
6258	def update ( self , aspect_ratio = None , fov = None , near = None , far = None ) : self . aspect_ratio = aspect_ratio or self . aspect_ratio self . fov = fov or self . fov self . near = near or self . near self . far = far or self . far self . matrix = Matrix44 . perspective_projection ( self . fov , self . aspect_ratio , self . near , self . far )
10289	def enrich_reactions ( graph : BELGraph ) : nodes = list ( get_nodes_by_function ( graph , REACTION ) ) for u in nodes : for v in u . reactants : graph . add_has_reactant ( u , v ) for v in u . products : graph . add_has_product ( u , v )
1604	def run_metrics ( command , parser , cl_args , unknown_args ) : cluster , role , env = cl_args [ 'cluster' ] , cl_args [ 'role' ] , cl_args [ 'environ' ] topology = cl_args [ 'topology-name' ] try : result = tracker_access . get_topology_info ( cluster , env , topology , role ) spouts = result [ 'physical_plan' ] [ 'spouts' ] . keys ( ) bolts = result [ 'physical_plan' ] [ 'bolts' ] . keys ( ) components = spouts + bolts cname = cl_args [ 'component' ] if cname : if cname in components : components = [ cname ] else : Log . error ( 'Unknown component: \'%s\'' % cname ) raise except Exception : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False cresult = [ ] for comp in components : try : metrics = tracker_access . get_component_metrics ( comp , cluster , env , topology , role ) except : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False stat , header = to_table ( metrics ) cresult . append ( ( comp , stat , header ) ) for i , ( comp , stat , header ) in enumerate ( cresult ) : if i != 0 : print ( '' ) print ( '\'%s\' metrics:' % comp ) print ( tabulate ( stat , headers = header ) ) return True
8071	def not_found ( url , wait = 10 ) : try : connection = open ( url , wait ) except HTTP404NotFound : return True except : return False return False
3138	def get ( self , app_id , ** queryparams ) : self . app_id = app_id return self . _mc_client . _get ( url = self . _build_path ( app_id ) , ** queryparams )
8066	def get_source ( self , doc ) : start_iter = doc . get_start_iter ( ) end_iter = doc . get_end_iter ( ) source = doc . get_text ( start_iter , end_iter , False ) return source
8072	def is_type ( url , types = [ ] , wait = 10 ) : if isinstance ( types , str ) : types = [ types ] try : connection = open ( url , wait ) except : return False type = connection . info ( ) [ "Content-Type" ] for t in types : if type . startswith ( t ) : return True return False
12683	def notice_settings ( request ) : notice_types = NoticeType . objects . all ( ) settings_table = [ ] for notice_type in notice_types : settings_row = [ ] for medium_id , medium_display in NOTICE_MEDIA : form_label = "%s_%s" % ( notice_type . label , medium_id ) setting = NoticeSetting . for_user ( request . user , notice_type , medium_id ) if request . method == "POST" : if request . POST . get ( form_label ) == "on" : if not setting . send : setting . send = True setting . save ( ) else : if setting . send : setting . send = False setting . save ( ) settings_row . append ( ( form_label , setting . send ) ) settings_table . append ( { "notice_type" : notice_type , "cells" : settings_row } ) if request . method == "POST" : next_page = request . POST . get ( "next_page" , "." ) return HttpResponseRedirect ( next_page ) settings = { "column_headers" : [ medium_display for medium_id , medium_display in NOTICE_MEDIA ] , "rows" : settings_table , } return render_to_response ( "notification/notice_settings.html" , { "notice_types" : notice_types , "notice_settings" : settings , } , context_instance = RequestContext ( request ) )
2596	def _import_mapping ( mapping , original = None ) : for key , value in list ( mapping . items ( ) ) : if isinstance ( key , string_types ) : try : cls = import_item ( key ) except Exception : if original and key not in original : print ( "ERROR: canning class not importable: %r" , key , exc_info = True ) mapping . pop ( key ) else : mapping [ cls ] = mapping . pop ( key )
9100	def write_bel_namespace_mappings ( self , file : TextIO , ** kwargs ) -> None : json . dump ( self . _get_namespace_identifier_to_name ( ** kwargs ) , file , indent = 2 , sort_keys = True )
943	def _getModelCheckpointDir ( experimentDir , checkpointLabel ) : checkpointDir = os . path . join ( getCheckpointParentDir ( experimentDir ) , checkpointLabel + g_defaultCheckpointExtension ) checkpointDir = os . path . abspath ( checkpointDir ) return checkpointDir
7781	def rfc2426 ( self ) : ret = "begin:VCARD\r\n" ret += "version:3.0\r\n" for _unused , value in self . content . items ( ) : if value is None : continue if type ( value ) is list : for v in value : ret += v . rfc2426 ( ) else : v = value . rfc2426 ( ) ret += v return ret + "end:VCARD\r\n"
3027	def _get_well_known_file ( ) : default_config_dir = os . getenv ( _CLOUDSDK_CONFIG_ENV_VAR ) if default_config_dir is None : if os . name == 'nt' : try : default_config_dir = os . path . join ( os . environ [ 'APPDATA' ] , _CLOUDSDK_CONFIG_DIRECTORY ) except KeyError : drive = os . environ . get ( 'SystemDrive' , 'C:' ) default_config_dir = os . path . join ( drive , '\\' , _CLOUDSDK_CONFIG_DIRECTORY ) else : default_config_dir = os . path . join ( os . path . expanduser ( '~' ) , '.config' , _CLOUDSDK_CONFIG_DIRECTORY ) return os . path . join ( default_config_dir , _WELL_KNOWN_CREDENTIALS_FILE )
10368	def find_activations ( graph : BELGraph ) : for u , v , key , data in graph . edges ( keys = True , data = True ) : if u != v : continue bel = graph . edge_to_bel ( u , v , data ) line = data . get ( LINE ) if line is None : continue elif has_protein_modification_increases_activity ( graph , u , v , key ) : print ( line , '- pmod changes -' , bel ) find_related ( graph , v , data ) elif has_degradation_increases_activity ( data ) : print ( line , '- degradation changes -' , bel ) find_related ( graph , v , data ) elif has_translocation_increases_activity ( data ) : print ( line , '- translocation changes -' , bel ) find_related ( graph , v , data ) elif complex_increases_activity ( graph , u , v , key ) : print ( line , '- complex changes - ' , bel ) find_related ( graph , v , data ) elif has_same_subject_object ( graph , u , v , key ) : print ( line , '- same sub/obj -' , bel ) else : print ( line , '- *** - ' , bel )
7234	def tilemap ( self , query , styles = { } , bbox = [ - 180 , - 90 , 180 , 90 ] , zoom = 16 , api_key = os . environ . get ( 'MAPBOX_API_KEY' , None ) , image = None , image_bounds = None , index = "vector-user-provided" , name = "GBDX_Task_Output" , ** kwargs ) : try : from IPython . display import display except : print ( "IPython is required to produce maps." ) return assert api_key is not None , "No Mapbox API Key found. You can either pass in a token or set the MAPBOX_API_KEY environment variable." wkt = box ( * bbox ) . wkt features = self . query ( wkt , query , index = index ) union = cascaded_union ( [ shape ( f [ 'geometry' ] ) for f in features ] ) lon , lat = union . centroid . coords [ 0 ] url = 'https://vector.geobigdata.io/insight-vector/api/mvt/{z}/{x}/{y}?' url += 'q={}&index={}' . format ( query , index ) if styles is not None and not isinstance ( styles , list ) : styles = [ styles ] map_id = "map_{}" . format ( str ( int ( time . time ( ) ) ) ) map_data = VectorTileLayer ( url , source_name = name , styles = styles , ** kwargs ) image_layer = self . _build_image_layer ( image , image_bounds ) template = BaseTemplate ( map_id , ** { "lat" : lat , "lon" : lon , "zoom" : zoom , "datasource" : json . dumps ( map_data . datasource ) , "layers" : json . dumps ( map_data . layers ) , "image_layer" : image_layer , "mbkey" : api_key , "token" : self . gbdx_connection . access_token } ) template . inject ( )
13117	def search ( self , number = None , * args , ** kwargs ) : search = self . create_search ( * args , ** kwargs ) try : if number : response = search [ 0 : number ] else : args , _ = self . core_parser . parse_known_args ( ) if args . number : response = search [ 0 : args . number ] else : response = search . scan ( ) return [ hit for hit in response ] except NotFoundError : print_error ( "The index was not found, have you initialized the index?" ) return [ ] except ( ConnectionError , TransportError ) : print_error ( "Cannot connect to elasticsearch" ) return [ ]
5374	def _prefix_exists_in_gcs ( gcs_prefix , credentials = None ) : gcs_service = _get_storage_service ( credentials ) bucket_name , prefix = gcs_prefix [ len ( 'gs://' ) : ] . split ( '/' , 1 ) request = gcs_service . objects ( ) . list ( bucket = bucket_name , prefix = prefix , maxResults = 1 ) response = request . execute ( ) return response . get ( 'items' , None )
9862	async def update_info ( self , * _ ) : query = gql ( ) res = await self . _execute ( query ) if res is None : return errors = res . get ( "errors" , [ ] ) if errors : msg = errors [ 0 ] . get ( "message" , "failed to login" ) _LOGGER . error ( msg ) raise InvalidLogin ( msg ) data = res . get ( "data" ) if not data : return viewer = data . get ( "viewer" ) if not viewer : return self . _name = viewer . get ( "name" ) homes = viewer . get ( "homes" , [ ] ) self . _home_ids = [ ] for _home in homes : home_id = _home . get ( "id" ) self . _all_home_ids += [ home_id ] subs = _home . get ( "subscriptions" ) if subs : status = subs [ 0 ] . get ( "status" , "ended" ) . lower ( ) if not home_id or status != "running" : continue self . _home_ids += [ home_id ]
2098	def relaunch ( self , pk = None , ** kwargs ) : if not pk : existing_data = self . get ( ** kwargs ) pk = existing_data [ 'id' ] relaunch_endpoint = '%s%s/relaunch/' % ( self . endpoint , pk ) data = { } answer = { } try : result = client . post ( relaunch_endpoint , data = data ) . json ( ) if 'id' in result : answer . update ( result ) answer [ 'changed' ] = True except exc . MethodNotAllowed : answer [ 'changed' ] = False return answer
7487	def concat_multiple_inputs ( data , sample ) : if len ( sample . files . fastqs ) > 1 : cmd1 = [ "cat" ] + [ i [ 0 ] for i in sample . files . fastqs ] isgzip = ".gz" if not sample . files . fastqs [ 0 ] [ 0 ] . endswith ( ".gz" ) : isgzip = "" conc1 = os . path . join ( data . dirs . edits , sample . name + "_R1_concat.fq{}" . format ( isgzip ) ) with open ( conc1 , 'w' ) as cout1 : proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = cout1 , close_fds = True ) res1 = proc1 . communicate ( ) [ 0 ] if proc1 . returncode : raise IPyradWarningExit ( "error in: {}, {}" . format ( cmd1 , res1 ) ) conc2 = 0 if "pair" in data . paramsdict [ "datatype" ] : cmd2 = [ "cat" ] + [ i [ 1 ] for i in sample . files . fastqs ] conc2 = os . path . join ( data . dirs . edits , sample . name + "_R2_concat.fq{}" . format ( isgzip ) ) with open ( conc2 , 'w' ) as cout2 : proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = cout2 , close_fds = True ) res2 = proc2 . communicate ( ) [ 0 ] if proc2 . returncode : raise IPyradWarningExit ( "Error concatenating fastq files. Make sure all " + "these files exist: {}\nError message: {}" . format ( cmd2 , proc2 . returncode ) ) sample . files . concat = [ ( conc1 , conc2 ) ] return sample . files . concat
9897	def boottime ( ) : global __boottime if __boottime is None : up = uptime ( ) if up is None : return None if __boottime is None : _boottime_linux ( ) if datetime is None : raise RuntimeError ( 'datetime module required.' ) return datetime . fromtimestamp ( __boottime or time . time ( ) - up )
7827	def stream_element_handler ( element_name , usage_restriction = None ) : def decorator ( func ) : func . _pyxmpp_stream_element_handled = element_name func . _pyxmpp_usage_restriction = usage_restriction return func return decorator
4592	def colors_no_palette ( colors = None , ** kwds ) : if isinstance ( colors , str ) : colors = _split_colors ( colors ) else : colors = to_triplets ( colors or ( ) ) colors = ( color ( c ) for c in colors or ( ) ) return palette . Palette ( colors , ** kwds )
5881	def is_highlink_density ( self , element ) : links = self . parser . getElementsByTag ( element , tag = 'a' ) if not links : return False text = self . parser . getText ( element ) words = text . split ( ' ' ) words_number = float ( len ( words ) ) link_text_parts = [ ] for link in links : link_text_parts . append ( self . parser . getText ( link ) ) link_text = '' . join ( link_text_parts ) link_words = link_text . split ( ' ' ) number_of_link_words = float ( len ( link_words ) ) number_of_links = float ( len ( links ) ) link_divisor = float ( number_of_link_words / words_number ) score = float ( link_divisor * number_of_links ) if score >= 1.0 : return True return False
10765	def url ( self ) : if self . id is None : return '' return '{}/{}' . format ( strawpoll . API . _BASE_URL , self . id )
10133	def parse_scalar ( scalar_data , version ) : try : return hs_scalar [ version ] . parseString ( scalar_data , parseAll = True ) [ 0 ] except pp . ParseException as pe : raise ZincParseException ( 'Failed to parse scalar: %s' % reformat_exception ( pe ) , scalar_data , 1 , pe . col ) except : LOG . debug ( 'Failing scalar data: %r (version %r)' , scalar_data , version )
5699	def write_stats_as_csv ( gtfs , path_to_csv , re_write = False ) : stats_dict = get_stats ( gtfs ) if re_write : os . remove ( path_to_csv ) is_new = True mode = 'r' if os . path . exists ( path_to_csv ) else 'w+' with open ( path_to_csv , mode ) as csvfile : for line in csvfile : if line : is_new = False else : is_new = True with open ( path_to_csv , 'a' ) as csvfile : if ( sys . version_info > ( 3 , 0 ) ) : delimiter = u"," else : delimiter = b"," statswriter = csv . writer ( csvfile , delimiter = delimiter ) if is_new : statswriter . writerow ( [ key for key in sorted ( stats_dict . keys ( ) ) ] ) row_to_write = [ ] for key in sorted ( stats_dict . keys ( ) ) : row_to_write . append ( stats_dict [ key ] ) statswriter . writerow ( row_to_write )
13246	async def _download_lsst_bibtex ( bibtex_names ) : blob_url_template = ( 'https://raw.githubusercontent.com/lsst/lsst-texmf/master/texmf/' 'bibtex/bib/{name}.bib' ) urls = [ blob_url_template . format ( name = name ) for name in bibtex_names ] tasks = [ ] async with ClientSession ( ) as session : for url in urls : task = asyncio . ensure_future ( _download_text ( url , session ) ) tasks . append ( task ) return await asyncio . gather ( * tasks )
8332	def findNextSiblings ( self , name = None , attrs = { } , text = None , limit = None , ** kwargs ) : return self . _findAll ( name , attrs , text , limit , self . nextSiblingGenerator , ** kwargs )
9984	def has_lambda ( src ) : module_node = ast . parse ( dedent ( src ) ) lambdaexp = [ node for node in ast . walk ( module_node ) if isinstance ( node , ast . Lambda ) ] return bool ( lambdaexp )
4661	def broadcast ( self , tx = None ) : if tx : return self . transactionbuilder_class ( tx , blockchain_instance = self ) . broadcast ( ) else : return self . txbuffer . broadcast ( )
9795	def group ( ctx , project , group ) : ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'group' ] = group
2762	def get_all_certificates ( self ) : data = self . get_data ( "certificates" ) certificates = list ( ) for jsoned in data [ 'certificates' ] : cert = Certificate ( ** jsoned ) cert . token = self . token certificates . append ( cert ) return certificates
8345	def find ( self , name = None , attrs = { } , recursive = True , text = None , ** kwargs ) : r = None l = self . findAll ( name , attrs , recursive , text , 1 , ** kwargs ) if l : r = l [ 0 ] return r
7259	def search_point ( self , lat , lng , filters = None , startDate = None , endDate = None , types = None , type = None ) : searchAreaWkt = "POLYGON ((%s %s, %s %s, %s %s, %s %s, %s %s))" % ( lng , lat , lng , lat , lng , lat , lng , lat , lng , lat ) return self . search ( searchAreaWkt = searchAreaWkt , filters = filters , startDate = startDate , endDate = endDate , types = types )
9660	def merge_from_store_and_in_mems ( from_store , in_mem_shas , dont_update_shas_of ) : if not from_store : for item in dont_update_shas_of : if item in in_mem_shas [ 'files' ] : del in_mem_shas [ 'files' ] [ item ] return in_mem_shas for key in from_store [ 'files' ] : if key not in in_mem_shas [ 'files' ] and key not in dont_update_shas_of : in_mem_shas [ 'files' ] [ key ] = from_store [ 'files' ] [ key ] for item in dont_update_shas_of : if item in in_mem_shas [ 'files' ] : del in_mem_shas [ 'files' ] [ item ] return in_mem_shas
4863	def to_representation ( self , instance ) : request = self . context [ 'request' ] enterprise_customer = instance . enterprise_customer representation = super ( EnterpriseCustomerCatalogDetailSerializer , self ) . to_representation ( instance ) paginated_content = instance . get_paginated_content ( request . GET ) count = paginated_content [ 'count' ] search_results = paginated_content [ 'results' ] for item in search_results : content_type = item [ 'content_type' ] marketing_url = item . get ( 'marketing_url' ) if marketing_url : item [ 'marketing_url' ] = utils . update_query_parameters ( marketing_url , utils . get_enterprise_utm_context ( enterprise_customer ) ) if content_type == 'course' : item [ 'enrollment_url' ] = instance . get_course_enrollment_url ( item [ 'key' ] ) if content_type == 'courserun' : item [ 'enrollment_url' ] = instance . get_course_run_enrollment_url ( item [ 'key' ] ) if content_type == 'program' : item [ 'enrollment_url' ] = instance . get_program_enrollment_url ( item [ 'uuid' ] ) previous_url = None next_url = None page = int ( request . GET . get ( 'page' , '1' ) ) request_uri = request . build_absolute_uri ( ) if paginated_content [ 'previous' ] : previous_url = utils . update_query_parameters ( request_uri , { 'page' : page - 1 } ) if paginated_content [ 'next' ] : next_url = utils . update_query_parameters ( request_uri , { 'page' : page + 1 } ) representation [ 'count' ] = count representation [ 'previous' ] = previous_url representation [ 'next' ] = next_url representation [ 'results' ] = search_results return representation
4234	def _xml_get ( e , name ) : r = e . find ( name ) if r is not None : return r . text return None
11333	def banner ( * lines , ** kwargs ) : sep = kwargs . get ( "sep" , "*" ) count = kwargs . get ( "width" , globals ( ) [ "WIDTH" ] ) out ( sep * count ) if lines : out ( sep ) for line in lines : out ( "{} {}" . format ( sep , line ) ) out ( sep ) out ( sep * count )
11304	def embed ( self , url , ** kwargs ) : try : provider = self . provider_for_url ( url ) except OEmbedMissingEndpoint : raise else : try : stored_match = StoredOEmbed . objects . filter ( match = url , maxwidth = kwargs . get ( 'maxwidth' , None ) , maxheight = kwargs . get ( 'maxheight' , None ) , date_expires__gte = datetime . datetime . now ( ) ) [ 0 ] return OEmbedResource . create_json ( stored_match . response_json ) except IndexError : params = dict ( [ ( k , v ) for k , v in kwargs . items ( ) if v ] ) resource = provider . request_resource ( url , ** params ) try : cache_age = int ( resource . cache_age ) if cache_age < MIN_OEMBED_TTL : cache_age = MIN_OEMBED_TTL except : cache_age = DEFAULT_OEMBED_TTL date_expires = datetime . datetime . now ( ) + datetime . timedelta ( seconds = cache_age ) stored_oembed , created = StoredOEmbed . objects . get_or_create ( match = url , maxwidth = kwargs . get ( 'maxwidth' , None ) , maxheight = kwargs . get ( 'maxheight' , None ) ) stored_oembed . response_json = resource . json stored_oembed . resource_type = resource . type stored_oembed . date_expires = date_expires if resource . content_object : stored_oembed . content_object = resource . content_object stored_oembed . save ( ) return resource
8806	def calc_periods ( hour = 0 , minute = 0 ) : period_end = datetime . datetime . utcnow ( ) . replace ( hour = hour , minute = minute , second = 0 , microsecond = 0 ) period_start = period_end - datetime . timedelta ( days = 1 ) period_end -= datetime . timedelta ( seconds = 1 ) return ( period_start , period_end )
309	def plot_cones ( name , bounds , oos_returns , num_samples = 1000 , ax = None , cone_std = ( 1. , 1.5 , 2. ) , random_seed = None , num_strikes = 3 ) : if ax is None : fig = figure . Figure ( figsize = ( 10 , 8 ) ) FigureCanvasAgg ( fig ) axes = fig . add_subplot ( 111 ) else : axes = ax returns = ep . cum_returns ( oos_returns , starting_value = 1. ) bounds_tmp = bounds . copy ( ) returns_tmp = returns . copy ( ) cone_start = returns . index [ 0 ] colors = [ "green" , "orange" , "orangered" , "darkred" ] for c in range ( num_strikes + 1 ) : if c > 0 : tmp = returns . loc [ cone_start : ] bounds_tmp = bounds_tmp . iloc [ 0 : len ( tmp ) ] bounds_tmp = bounds_tmp . set_index ( tmp . index ) crossing = ( tmp < bounds_tmp [ float ( - 2. ) ] . iloc [ : len ( tmp ) ] ) if crossing . sum ( ) <= 0 : break cone_start = crossing . loc [ crossing ] . index [ 0 ] returns_tmp = returns . loc [ cone_start : ] bounds_tmp = ( bounds - ( 1 - returns . loc [ cone_start ] ) ) for std in cone_std : x = returns_tmp . index y1 = bounds_tmp [ float ( std ) ] . iloc [ : len ( returns_tmp ) ] y2 = bounds_tmp [ float ( - std ) ] . iloc [ : len ( returns_tmp ) ] axes . fill_between ( x , y1 , y2 , color = colors [ c ] , alpha = 0.5 ) label = 'Cumulative returns = {:.2f}%' . format ( ( returns . iloc [ - 1 ] - 1 ) * 100 ) axes . plot ( returns . index , returns . values , color = 'black' , lw = 3. , label = label ) if name is not None : axes . set_title ( name ) axes . axhline ( 1 , color = 'black' , alpha = 0.2 ) axes . legend ( frameon = True , framealpha = 0.5 ) if ax is None : return fig else : return axes
9785	def bookmark ( ctx ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) try : PolyaxonClient ( ) . build_job . bookmark ( user , project_name , _build ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not bookmark build job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Build job bookmarked." )
2082	def lookup_stdout ( self , pk = None , start_line = None , end_line = None , full = True ) : uj_res = get_resource ( 'unified_job' ) query_params = ( ( 'unified_job_node__workflow_job' , pk ) , ( 'order_by' , 'finished' ) , ( 'status__in' , 'successful,failed,error' ) ) jobs_list = uj_res . list ( all_pages = True , query = query_params ) if jobs_list [ 'count' ] == 0 : return '' return_content = ResSubcommand ( uj_res ) . _format_human ( jobs_list ) lines = return_content . split ( '\n' ) if not full : lines = lines [ : - 1 ] N = len ( lines ) start_range = start_line if start_line is None : start_range = 0 elif start_line > N : start_range = N end_range = end_line if end_line is None or end_line > N : end_range = N lines = lines [ start_range : end_range ] return_content = '\n' . join ( lines ) if len ( lines ) > 0 : return_content += '\n' return return_content
11786	def sanitize ( self , example ) : "Return a copy of example, with non-input attributes replaced by None." return [ attr_i if i in self . inputs else None for i , attr_i in enumerate ( example ) ]
7769	def _stream_disconnected ( self , event ) : with self . lock : if event . stream != self . stream : return if self . stream is not None and event . stream == self . stream : if self . stream . transport in self . _ml_handlers : self . _ml_handlers . remove ( self . stream . transport ) self . main_loop . remove_handler ( self . stream . transport ) self . stream = None self . uplink = None
7196	def ndwi ( self ) : data = self . _read ( self [ self . _ndwi_bands , ... ] ) . astype ( np . float32 ) return ( data [ 1 , : , : ] - data [ 0 , : , : ] ) / ( data [ 0 , : , : ] + data [ 1 , : , : ] )
3814	def _get_upload_session_status ( res ) : response = json . loads ( res . body . decode ( ) ) if 'sessionStatus' not in response : try : info = ( response [ 'errorMessage' ] [ 'additionalInfo' ] [ 'uploader_service.GoogleRupioAdditionalInfo' ] [ 'completionInfo' ] [ 'customerSpecificInfo' ] ) reason = '{} : {}' . format ( info [ 'status' ] , info [ 'message' ] ) except KeyError : reason = 'unknown reason' raise exceptions . NetworkError ( 'image upload failed: {}' . format ( reason ) ) return response [ 'sessionStatus' ]
9870	def build_environ ( self , sock_file , conn ) : request = self . read_request_line ( sock_file ) environ = self . base_environ . copy ( ) for k , v in self . read_headers ( sock_file ) . items ( ) : environ [ str ( 'HTTP_' + k ) ] = v environ [ 'REQUEST_METHOD' ] = request [ 'method' ] environ [ 'PATH_INFO' ] = request [ 'path' ] environ [ 'SERVER_PROTOCOL' ] = request [ 'protocol' ] environ [ 'SERVER_PORT' ] = str ( conn . server_port ) environ [ 'REMOTE_PORT' ] = str ( conn . client_port ) environ [ 'REMOTE_ADDR' ] = str ( conn . client_addr ) environ [ 'QUERY_STRING' ] = request [ 'query_string' ] if 'HTTP_CONTENT_LENGTH' in environ : environ [ 'CONTENT_LENGTH' ] = environ [ 'HTTP_CONTENT_LENGTH' ] if 'HTTP_CONTENT_TYPE' in environ : environ [ 'CONTENT_TYPE' ] = environ [ 'HTTP_CONTENT_TYPE' ] self . request_method = environ [ 'REQUEST_METHOD' ] if conn . ssl : environ [ 'wsgi.url_scheme' ] = 'https' environ [ 'HTTPS' ] = 'on' else : environ [ 'wsgi.url_scheme' ] = 'http' if environ . get ( 'HTTP_TRANSFER_ENCODING' , '' ) == 'chunked' : environ [ 'wsgi.input' ] = ChunkedReader ( sock_file ) else : environ [ 'wsgi.input' ] = sock_file return environ
9699	def event_choices ( events ) : if events is None : msg = "Please add some events in settings.WEBHOOK_EVENTS." raise ImproperlyConfigured ( msg ) try : choices = [ ( x , x ) for x in events ] except TypeError : msg = "settings.WEBHOOK_EVENTS must be an iterable object." raise ImproperlyConfigured ( msg ) return choices
10134	def add_item ( self , key , value , after = False , index = None , pos_key = None , replace = True ) : if self . _validate_fn : self . _validate_fn ( value ) if ( index is not None ) and ( pos_key is not None ) : raise ValueError ( 'Either specify index or pos_key, not both.' ) elif pos_key is not None : try : index = self . index ( pos_key ) except ValueError : raise KeyError ( '%r not found' % pos_key ) if after and ( index is not None ) : index += 1 if key in self . _values : if not replace : raise KeyError ( '%r is duplicate' % key ) if index is not None : del self [ key ] else : self . _values [ key ] = value return if index is not None : self . _order . insert ( index , key ) else : self . _order . append ( key ) self . _values [ key ] = value
1453	def add_key ( self , key ) : if key not in self . value : self . value [ key ] = ReducedMetric ( self . reducer )
8687	def get ( self , key_name ) : result = self . db . search ( Query ( ) . name == key_name ) if not result : return { } return result [ 0 ]
399	def binary_cross_entropy ( output , target , epsilon = 1e-8 , name = 'bce_loss' ) : return tf . reduce_mean ( tf . reduce_sum ( - ( target * tf . log ( output + epsilon ) + ( 1. - target ) * tf . log ( 1. - output + epsilon ) ) , axis = 1 ) , name = name )
12131	def lexsort ( self , * order ) : if order == [ ] : raise Exception ( "Please specify the keys for sorting, use" "'+' prefix for ascending," "'-' for descending.)" ) if not set ( el [ 1 : ] for el in order ) . issubset ( set ( self . varying_keys ) ) : raise Exception ( "Key(s) specified not in the set of varying keys." ) sorted_args = copy . deepcopy ( self ) specs_param = sorted_args . params ( 'specs' ) specs_param . constant = False sorted_args . specs = self . _lexsorted_specs ( order ) specs_param . constant = True sorted_args . _lexorder = order return sorted_args
7896	def change_nick ( self , new_nick ) : new_room_jid = JID ( self . room_jid . node , self . room_jid . domain , new_nick ) p = Presence ( to_jid = new_room_jid ) self . manager . stream . send ( p )
1054	def seed ( self , a = None ) : super ( Random , self ) . seed ( a ) self . gauss_next = None
2711	def json_iter ( path ) : with open ( path , 'r' ) as f : for line in f . readlines ( ) : yield json . loads ( line )
9390	def get_aggregation_timestamp ( self , timestamp , granularity = 'second' ) : if granularity is None or granularity . lower ( ) == 'none' : return int ( timestamp ) , 1 elif granularity == 'hour' : return ( int ( timestamp ) / ( 3600 * 1000 ) ) * 3600 * 1000 , 3600 elif granularity == 'minute' : return ( int ( timestamp ) / ( 60 * 1000 ) ) * 60 * 1000 , 60 else : return ( int ( timestamp ) / 1000 ) * 1000 , 1
13399	def resourcePath ( self , relative_path ) : from os import path import sys try : base_path = sys . _MEIPASS except Exception : base_path = path . dirname ( path . abspath ( __file__ ) ) return path . join ( base_path , relative_path )
6637	def unpublish ( self , registry = None ) : return registry_access . unpublish ( self . getRegistryNamespace ( ) , self . getName ( ) , self . getVersion ( ) , registry = registry )
11152	def sha256file ( abspath , nbytes = 0 , chunk_size = DEFAULT_CHUNK_SIZE ) : return get_file_fingerprint ( abspath , hashlib . sha256 , nbytes = nbytes , chunk_size = chunk_size )
3920	def set_focus ( self , position ) : self . _focus_position = position self . _modified ( ) try : self . next_position ( position ) except IndexError : self . _is_scrolling = False else : self . _is_scrolling = True
777	def connect ( self , deleteOldVersions = False , recreate = False ) : with ConnectionFactory . get ( ) as conn : self . _initTables ( cursor = conn . cursor , deleteOldVersions = deleteOldVersions , recreate = recreate ) conn . cursor . execute ( 'SELECT CONNECTION_ID()' ) self . _connectionID = conn . cursor . fetchall ( ) [ 0 ] [ 0 ] self . _logger . info ( "clientJobsConnectionID=%r" , self . _connectionID ) return
3482	def _get_doc_from_filename ( filename ) : if isinstance ( filename , string_types ) : if ( "win" in platform ) and ( len ( filename ) < 260 ) and os . path . exists ( filename ) : doc = libsbml . readSBMLFromFile ( filename ) elif ( "win" not in platform ) and os . path . exists ( filename ) : doc = libsbml . readSBMLFromFile ( filename ) else : if "<sbml" not in filename : raise IOError ( "The file with 'filename' does not exist, " "or is not an SBML string. Provide the path to " "an existing SBML file or a valid SBML string " "representation: \n%s" , filename ) doc = libsbml . readSBMLFromString ( filename ) elif hasattr ( filename , "read" ) : doc = libsbml . readSBMLFromString ( filename . read ( ) ) else : raise CobraSBMLError ( "Input type '%s' for 'filename' is not supported." " Provide a path, SBML str, " "or file handle." , type ( filename ) ) return doc
8411	def scaled_limits ( self ) : _min = self . limits [ 0 ] / self . factor _max = self . limits [ 1 ] / self . factor return _min , _max
8616	def _b ( s , encoding = 'utf-8' ) : if six . PY2 : if isinstance ( s , str ) : return s elif isinstance ( s , unicode ) : return s . encode ( encoding ) else : if isinstance ( s , bytes ) : return s elif isinstance ( s , str ) : return s . encode ( encoding ) raise TypeError ( "Invalid argument %r for _b()" % ( s , ) )
7095	def init_map ( self ) : d = self . declaration if d . show_location : self . set_show_location ( d . show_location ) if d . show_traffic : self . set_show_traffic ( d . show_traffic ) if d . show_indoors : self . set_show_indoors ( d . show_indoors ) if d . show_buildings : self . set_show_buildings ( d . show_buildings ) mapview = self . map mid = mapview . getId ( ) mapview . onCameraChange . connect ( self . on_camera_changed ) mapview . onCameraMoveStarted . connect ( self . on_camera_move_started ) mapview . onCameraMoveCanceled . connect ( self . on_camera_move_stopped ) mapview . onCameraIdle . connect ( self . on_camera_move_stopped ) mapview . setOnCameraChangeListener ( mid ) mapview . setOnCameraMoveStartedListener ( mid ) mapview . setOnCameraMoveCanceledListener ( mid ) mapview . setOnCameraIdleListener ( mid ) mapview . onMapClick . connect ( self . on_map_clicked ) mapview . setOnMapClickListener ( mid ) mapview . onMapLongClick . connect ( self . on_map_long_clicked ) mapview . setOnMapLongClickListener ( mid ) mapview . onMarkerClick . connect ( self . on_marker_clicked ) mapview . setOnMarkerClickListener ( self . map . getId ( ) ) mapview . onMarkerDragStart . connect ( self . on_marker_drag_start ) mapview . onMarkerDrag . connect ( self . on_marker_drag ) mapview . onMarkerDragEnd . connect ( self . on_marker_drag_end ) mapview . setOnMarkerDragListener ( mid ) mapview . onInfoWindowClick . connect ( self . on_info_window_clicked ) mapview . onInfoWindowLongClick . connect ( self . on_info_window_long_clicked ) mapview . onInfoWindowClose . connect ( self . on_info_window_closed ) mapview . setOnInfoWindowClickListener ( mid ) mapview . setOnInfoWindowCloseListener ( mid ) mapview . setOnInfoWindowLongClickListener ( mid ) mapview . onPolygonClick . connect ( self . on_poly_clicked ) mapview . onPolylineClick . connect ( self . on_poly_clicked ) mapview . setOnPolygonClickListener ( mid ) mapview . setOnPolylineClickListener ( mid ) mapview . onCircleClick . connect ( self . on_circle_clicked ) mapview . setOnCircleClickListener ( mid )
12339	def compress ( images , delete_tif = False , folder = None ) : if type ( images ) == str : return [ compress_blocking ( images , delete_tif , folder ) ] filenames = copy ( images ) return Parallel ( n_jobs = _pools ) ( delayed ( compress_blocking ) ( image = image , delete_tif = delete_tif , folder = folder ) for image in filenames )
359	def load_file_list ( path = None , regx = '\.jpg' , printable = True , keep_prefix = False ) : r if path is None : path = os . getcwd ( ) file_list = os . listdir ( path ) return_list = [ ] for _ , f in enumerate ( file_list ) : if re . search ( regx , f ) : return_list . append ( f ) if keep_prefix : for i , f in enumerate ( return_list ) : return_list [ i ] = os . path . join ( path , f ) if printable : logging . info ( 'Match file list = %s' % return_list ) logging . info ( 'Number of files = %d' % len ( return_list ) ) return return_list
11800	def infer_assignment ( self ) : "Return the partial assignment implied by the current inferences." self . support_pruning ( ) return dict ( ( v , self . curr_domains [ v ] [ 0 ] ) for v in self . vars if 1 == len ( self . curr_domains [ v ] ) )
10676	def Cp ( compound_string , T , mass = 1.0 ) : formula , phase = _split_compound_string_ ( compound_string ) TK = T + 273.15 compound = compounds [ formula ] result = compound . Cp ( phase , TK ) return _finalise_result_ ( compound , result , mass )
2564	def start ( self ) : self . comm . Barrier ( ) logger . debug ( "Manager synced with workers" ) self . _kill_event = threading . Event ( ) self . _task_puller_thread = threading . Thread ( target = self . pull_tasks , args = ( self . _kill_event , ) ) self . _result_pusher_thread = threading . Thread ( target = self . push_results , args = ( self . _kill_event , ) ) self . _task_puller_thread . start ( ) self . _result_pusher_thread . start ( ) start = None result_counter = 0 task_recv_counter = 0 task_sent_counter = 0 logger . info ( "Loop start" ) while not self . _kill_event . is_set ( ) : time . sleep ( LOOP_SLOWDOWN ) timer = time . time ( ) + 0.05 counter = min ( 10 , comm . size ) while time . time ( ) < timer : info = MPI . Status ( ) if counter > 10 : logger . debug ( "Hit max mpi events per round" ) break if not self . comm . Iprobe ( status = info ) : logger . debug ( "Timer expired, processed {} mpi events" . format ( counter ) ) break else : tag = info . Get_tag ( ) logger . info ( "Message with tag {} received" . format ( tag ) ) counter += 1 if tag == RESULT_TAG : result = self . recv_result_from_workers ( ) self . pending_result_queue . put ( result ) result_counter += 1 elif tag == TASK_REQUEST_TAG : worker_rank = self . recv_task_request_from_workers ( ) self . ready_worker_queue . put ( worker_rank ) else : logger . error ( "Unknown tag {} - ignoring this message and continuing" . format ( tag ) ) available_worker_cnt = self . ready_worker_queue . qsize ( ) available_task_cnt = self . pending_task_queue . qsize ( ) logger . debug ( "[MAIN] Ready workers: {} Ready tasks: {}" . format ( available_worker_cnt , available_task_cnt ) ) this_round = min ( available_worker_cnt , available_task_cnt ) for i in range ( this_round ) : worker_rank = self . ready_worker_queue . get ( ) task = self . pending_task_queue . get ( ) comm . send ( task , dest = worker_rank , tag = worker_rank ) task_sent_counter += 1 logger . debug ( "Assigning worker:{} task:{}" . format ( worker_rank , task [ 'task_id' ] ) ) if not start : start = time . time ( ) logger . debug ( "Tasks recvd:{} Tasks dispatched:{} Results recvd:{}" . format ( task_recv_counter , task_sent_counter , result_counter ) ) self . _task_puller_thread . join ( ) self . _result_pusher_thread . join ( ) self . task_incoming . close ( ) self . result_outgoing . close ( ) self . context . term ( ) delta = time . time ( ) - start logger . info ( "mpi_worker_pool ran for {} seconds" . format ( delta ) )
739	def cPrint ( self , level , message , * args , ** kw ) : if level > self . consolePrinterVerbosity : return if len ( kw ) > 1 : raise KeyError ( "Invalid keywords for cPrint: %s" % str ( kw . keys ( ) ) ) newline = kw . get ( "newline" , True ) if len ( kw ) == 1 and 'newline' not in kw : raise KeyError ( "Invalid keyword for cPrint: %s" % kw . keys ( ) [ 0 ] ) if len ( args ) == 0 : if newline : print message else : print message , else : if newline : print message % args else : print message % args ,
10492	def doubleClickDragMouseButtonLeft ( self , coord , dest_coord , interval = 0.5 ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , dest_coord = dest_coord ) self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , dest_coord = dest_coord , clickCount = 2 ) self . _postQueuedEvents ( interval = interval )
6089	def for_data_and_tracer ( cls , lens_data , tracer , padded_tracer = None ) : if tracer . has_light_profile and not tracer . has_pixelization : return LensProfileFit ( lens_data = lens_data , tracer = tracer , padded_tracer = padded_tracer ) elif not tracer . has_light_profile and tracer . has_pixelization : return LensInversionFit ( lens_data = lens_data , tracer = tracer , padded_tracer = None ) elif tracer . has_light_profile and tracer . has_pixelization : return LensProfileInversionFit ( lens_data = lens_data , tracer = tracer , padded_tracer = None ) else : raise exc . FittingException ( 'The fit routine did not call a Fit class - check the ' 'properties of the tracer' )
5861	def _keys_to_camel_case ( self , obj ) : return dict ( ( to_camel_case ( key ) , value ) for ( key , value ) in obj . items ( ) )
1685	def RepositoryName ( self ) : r fullname = self . FullName ( ) if os . path . exists ( fullname ) : project_dir = os . path . dirname ( fullname ) if _repository : repo = FileInfo ( _repository ) . FullName ( ) root_dir = project_dir while os . path . exists ( root_dir ) : if os . path . normcase ( root_dir ) == os . path . normcase ( repo ) : return os . path . relpath ( fullname , root_dir ) . replace ( '\\' , '/' ) one_up_dir = os . path . dirname ( root_dir ) if one_up_dir == root_dir : break root_dir = one_up_dir if os . path . exists ( os . path . join ( project_dir , ".svn" ) ) : root_dir = project_dir one_up_dir = os . path . dirname ( root_dir ) while os . path . exists ( os . path . join ( one_up_dir , ".svn" ) ) : root_dir = os . path . dirname ( root_dir ) one_up_dir = os . path . dirname ( one_up_dir ) prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) return fullname [ len ( prefix ) + 1 : ] root_dir = current_dir = os . path . dirname ( fullname ) while current_dir != os . path . dirname ( current_dir ) : if ( os . path . exists ( os . path . join ( current_dir , ".git" ) ) or os . path . exists ( os . path . join ( current_dir , ".hg" ) ) or os . path . exists ( os . path . join ( current_dir , ".svn" ) ) ) : root_dir = current_dir current_dir = os . path . dirname ( current_dir ) if ( os . path . exists ( os . path . join ( root_dir , ".git" ) ) or os . path . exists ( os . path . join ( root_dir , ".hg" ) ) or os . path . exists ( os . path . join ( root_dir , ".svn" ) ) ) : prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) return fullname [ len ( prefix ) + 1 : ] return fullname
3975	def _env_vars_from_file ( filename ) : def split_env ( env ) : if '=' in env : return env . split ( '=' , 1 ) else : return env , None env = { } for line in open ( filename , 'r' ) : line = line . strip ( ) if line and not line . startswith ( '#' ) : k , v = split_env ( line ) env [ k ] = v return env
11569	def run ( self ) : while not self . is_stopped ( ) : try : if self . arduino . inWaiting ( ) : c = self . arduino . read ( ) self . command_deque . append ( ord ( c ) ) else : time . sleep ( .1 ) except OSError : pass except IOError : self . stop ( ) self . close ( )
13281	def parse ( self , source ) : command_regex = self . _make_command_regex ( self . name ) for match in re . finditer ( command_regex , source ) : self . _logger . debug ( match ) start_index = match . start ( 0 ) yield self . _parse_command ( source , start_index )
4035	def cleanwrap ( func ) : def enc ( self , * args , ** kwargs ) : return ( func ( self , item , ** kwargs ) for item in args ) return enc
10803	def tk ( self , k , x ) : weights = np . diag ( np . ones ( k + 1 ) ) [ k ] return np . polynomial . chebyshev . chebval ( self . _x2c ( x ) , weights )
7241	def aoi ( self , ** kwargs ) : g = self . _parse_geoms ( ** kwargs ) if g is None : return self else : return self [ g ]
13342	def concatenate ( tup , axis = 0 ) : from distob import engine if len ( tup ) is 0 : raise ValueError ( 'need at least one array to concatenate' ) first = tup [ 0 ] others = tup [ 1 : ] if ( hasattr ( first , 'concatenate' ) and hasattr ( type ( first ) , '__array_interface__' ) ) : return first . concatenate ( others , axis ) arrays = [ ] for ar in tup : if isinstance ( ar , DistArray ) : if axis == ar . _distaxis : arrays . extend ( ar . _subarrays ) else : arrays . append ( gather ( ar ) ) elif isinstance ( ar , RemoteArray ) : arrays . append ( ar ) elif isinstance ( ar , Remote ) : arrays . append ( _remote_to_array ( ar ) ) elif hasattr ( type ( ar ) , '__array_interface__' ) : arrays . append ( ar ) else : arrays . append ( np . array ( ar ) ) if all ( isinstance ( ar , np . ndarray ) for ar in arrays ) : return np . concatenate ( arrays , axis ) total_length = 0 commonshape = list ( arrays [ 0 ] . shape ) commonshape [ axis ] = None for ar in arrays : total_length += ar . shape [ axis ] shp = list ( ar . shape ) shp [ axis ] = None if shp != commonshape : raise ValueError ( 'incompatible shapes for concatenation' ) blocksize = ( ( total_length - 1 ) // engine . nengines ) + 1 rarrays = [ ] for ar in arrays : if isinstance ( ar , DistArray ) : rarrays . extend ( ar . _subarrays ) elif isinstance ( ar , RemoteArray ) : rarrays . append ( ar ) else : da = _scatter_ndarray ( ar , axis , blocksize ) for ra in da . _subarrays : rarrays . append ( ra ) del da del arrays eid = rarrays [ 0 ] . _id . engine if all ( ra . _id . engine == eid for ra in rarrays ) : if eid == engine . eid : return concatenate ( [ gather ( r ) for r in rarrays ] , axis ) else : return call ( concatenate , rarrays , axis ) else : return DistArray ( rarrays , axis )
2527	def get_annotation_comment ( self , r_term ) : comment_list = list ( self . graph . triples ( ( r_term , RDFS . comment , None ) ) ) if len ( comment_list ) > 1 : self . error = True msg = 'Annotation can have at most one comment.' self . logger . log ( msg ) return else : return six . text_type ( comment_list [ 0 ] [ 2 ] )
1908	def forward_events_to ( self , sink , include_source = False ) : assert isinstance ( sink , Eventful ) , f'{sink.__class__.__name__} is not Eventful' self . _forwards [ sink ] = include_source
8520	def plot_3 ( data , ss , * args ) : if len ( data ) <= 1 : warnings . warn ( "Only one datapoint. Could not compute t-SNE embedding." ) return None scores = np . array ( [ d [ 'mean_test_score' ] for d in data ] ) warped = np . array ( [ ss . point_to_unit ( d [ 'parameters' ] ) for d in data ] ) X = TSNE ( n_components = 2 ) . fit_transform ( warped ) e_scores = np . exp ( scores ) mine , maxe = np . min ( e_scores ) , np . max ( e_scores ) color = ( e_scores - mine ) / ( maxe - mine ) mapped_colors = list ( map ( rgb2hex , cm . get_cmap ( 'RdBu_r' ) ( color ) ) ) p = bk . figure ( title = 't-SNE (unsupervised)' , tools = TOOLS ) df_params = nonconstant_parameters ( data ) df_params [ 'score' ] = scores df_params [ 'x' ] = X [ : , 0 ] df_params [ 'y' ] = X [ : , 1 ] df_params [ 'color' ] = mapped_colors df_params [ 'radius' ] = 1 p . circle ( x = 'x' , y = 'y' , color = 'color' , radius = 'radius' , source = ColumnDataSource ( data = df_params ) , fill_alpha = 0.6 , line_color = None ) cp = p hover = cp . select ( dict ( type = HoverTool ) ) format_tt = [ ( s , '@%s' % s ) for s in df_params . columns ] hover . tooltips = OrderedDict ( [ ( "index" , "$index" ) ] + format_tt ) xax , yax = p . axis xax . axis_label = 't-SNE coord 1' yax . axis_label = 't-SNE coord 2' return p
10406	def canonical_averages_dtype ( spanning_cluster = True ) : fields = list ( ) fields . extend ( [ ( 'number_of_runs' , 'uint32' ) , ] ) if spanning_cluster : fields . extend ( [ ( 'percolation_probability_mean' , 'float64' ) , ( 'percolation_probability_m2' , 'float64' ) , ] ) fields . extend ( [ ( 'max_cluster_size_mean' , 'float64' ) , ( 'max_cluster_size_m2' , 'float64' ) , ( 'moments_mean' , '(5,)float64' ) , ( 'moments_m2' , '(5,)float64' ) , ] ) return _ndarray_dtype ( fields )
3095	def http ( self , * args , ** kwargs ) : return self . credentials . authorize ( transport . get_http_object ( * args , ** kwargs ) )
9224	def convergent_round ( value , ndigits = 0 ) : if sys . version_info [ 0 ] < 3 : if value < 0.0 : return - convergent_round ( - value ) epsilon = 0.0000001 integral_part , _ = divmod ( value , 1 ) if abs ( value - ( integral_part + 0.5 ) ) < epsilon : if integral_part % 2.0 < epsilon : return integral_part else : nearest_even = integral_part + 0.5 return math . ceil ( nearest_even ) return round ( value , ndigits )
10690	def render ( self , format = ReportFormat . printout ) : table = self . _generate_table_ ( ) if format == ReportFormat . printout : print ( tabulate ( table , headers = "firstrow" , tablefmt = "simple" ) ) elif format == ReportFormat . latex : self . _render_latex_ ( table ) elif format == ReportFormat . txt : self . _render_txt_ ( table ) elif format == ReportFormat . csv : self . _render_csv_ ( table ) elif format == ReportFormat . string : return str ( tabulate ( table , headers = "firstrow" , tablefmt = "simple" ) ) elif format == ReportFormat . matplotlib : self . _render_matplotlib_ ( ) elif format == ReportFormat . png : if self . output_path is None : self . _render_matplotlib_ ( ) else : self . _render_matplotlib_ ( True )
2437	def add_review_date ( self , doc , reviewed ) : if len ( doc . reviews ) != 0 : if not self . review_date_set : self . review_date_set = True date = utils . datetime_from_iso_format ( reviewed ) if date is not None : doc . reviews [ - 1 ] . review_date = date return True else : raise SPDXValueError ( 'Review::ReviewDate' ) else : raise CardinalityError ( 'Review::ReviewDate' ) else : raise OrderError ( 'Review::ReviewDate' )
9788	def bookmark ( ctx , username ) : ctx . obj = ctx . obj or { } ctx . obj [ 'username' ] = username
12133	def extract_log ( log_path , dict_type = dict ) : log_path = ( log_path if os . path . isfile ( log_path ) else os . path . join ( os . getcwd ( ) , log_path ) ) with open ( log_path , 'r' ) as log : splits = ( line . split ( ) for line in log ) uzipped = ( ( int ( split [ 0 ] ) , json . loads ( " " . join ( split [ 1 : ] ) ) ) for split in splits ) szipped = [ ( i , dict ( ( str ( k ) , v ) for ( k , v ) in d . items ( ) ) ) for ( i , d ) in uzipped ] return dict_type ( szipped )
4198	def get_short_module_name ( module_name , obj_name ) : parts = module_name . split ( '.' ) short_name = module_name for i in range ( len ( parts ) - 1 , 0 , - 1 ) : short_name = '.' . join ( parts [ : i ] ) try : exec ( 'from %s import %s' % ( short_name , obj_name ) ) except ImportError : short_name = '.' . join ( parts [ : ( i + 1 ) ] ) break return short_name
5027	def get_requirements ( requirements_file ) : lines = open ( requirements_file ) . readlines ( ) dependencies = [ ] dependency_links = [ ] for line in lines : package = line . strip ( ) if package . startswith ( '#' ) : continue if any ( package . startswith ( prefix ) for prefix in VCS_PREFIXES ) : package_link , __ , package = package . rpartition ( '#' ) package_link = re . sub ( r'(.*)(?P<dependency_link>https?.*$)' , r'\g<dependency_link>' , package_link ) package = re . sub ( r'(egg=)?(?P<package_name>.*)==.*$' , r'\g<package_name>' , package ) package_version = re . sub ( r'.*[^=]==' , '' , line . strip ( ) ) if package : dependency_links . append ( '{package_link}#egg={package}-{package_version}' . format ( package_link = package_link , package = package , package_version = package_version , ) ) else : package , __ , __ = package . partition ( '#' ) package = package . strip ( ) if package : dependencies . append ( package ) return dependencies , dependency_links
2002	def _type_size ( ty ) : if ty [ 0 ] in ( 'int' , 'uint' , 'bytesM' , 'function' ) : return 32 elif ty [ 0 ] in ( 'tuple' ) : result = 0 for ty_i in ty [ 1 ] : result += ABI . _type_size ( ty_i ) return result elif ty [ 0 ] in ( 'array' ) : rep = ty [ 1 ] result = 32 return result elif ty [ 0 ] in ( 'bytes' , 'string' ) : result = 32 return result raise ValueError
8682	def purge ( self , force = False , key_type = None ) : self . _assert_valid_stash ( ) if not force : raise GhostError ( "The `force` flag must be provided to perform a stash purge. " "I mean, you don't really want to just delete everything " "without precautionary measures eh?" ) audit ( storage = self . _storage . db_path , action = 'PURGE' , message = json . dumps ( dict ( ) ) ) for key_name in self . list ( key_type = key_type ) : self . delete ( key_name )
996	def _updateStatsInferEnd ( self , stats , bottomUpNZ , predictedState , colConfidence ) : if not self . collectStats : return stats [ 'nInfersSinceReset' ] += 1 ( numExtra2 , numMissing2 , confidences2 ) = self . _checkPrediction ( patternNZs = [ bottomUpNZ ] , output = predictedState , colConfidence = colConfidence ) predictionScore , positivePredictionScore , negativePredictionScore = ( confidences2 [ 0 ] ) stats [ 'curPredictionScore2' ] = float ( predictionScore ) stats [ 'curFalseNegativeScore' ] = 1.0 - float ( positivePredictionScore ) stats [ 'curFalsePositiveScore' ] = float ( negativePredictionScore ) stats [ 'curMissing' ] = numMissing2 stats [ 'curExtra' ] = numExtra2 if stats [ 'nInfersSinceReset' ] <= self . burnIn : return stats [ 'nPredictions' ] += 1 numExpected = max ( 1.0 , float ( len ( bottomUpNZ ) ) ) stats [ 'totalMissing' ] += numMissing2 stats [ 'totalExtra' ] += numExtra2 stats [ 'pctExtraTotal' ] += 100.0 * numExtra2 / numExpected stats [ 'pctMissingTotal' ] += 100.0 * numMissing2 / numExpected stats [ 'predictionScoreTotal2' ] += float ( predictionScore ) stats [ 'falseNegativeScoreTotal' ] += 1.0 - float ( positivePredictionScore ) stats [ 'falsePositiveScoreTotal' ] += float ( negativePredictionScore ) if self . collectSequenceStats : cc = self . cellConfidence [ 't-1' ] * self . infActiveState [ 't' ] sconf = cc . sum ( axis = 1 ) for c in range ( self . numberOfCols ) : if sconf [ c ] > 0 : cc [ c , : ] /= sconf [ c ] self . _internalStats [ 'confHistogram' ] += cc
11697	def count ( self ) : xml = get_changeset ( self . id ) actions = [ action . tag for action in xml . getchildren ( ) ] self . create = actions . count ( 'create' ) self . modify = actions . count ( 'modify' ) self . delete = actions . count ( 'delete' ) self . verify_editor ( ) try : if ( self . create / len ( actions ) > self . percentage and self . create > self . create_threshold and ( self . powerfull_editor or self . create > self . top_threshold ) ) : self . label_suspicious ( 'possible import' ) elif ( self . modify / len ( actions ) > self . percentage and self . modify > self . modify_threshold ) : self . label_suspicious ( 'mass modification' ) elif ( ( self . delete / len ( actions ) > self . percentage and self . delete > self . delete_threshold ) or self . delete > self . top_threshold ) : self . label_suspicious ( 'mass deletion' ) except ZeroDivisionError : print ( 'It seems this changeset was redacted' )
5221	def exch_info ( ticker : str ) -> pd . Series : logger = logs . get_logger ( exch_info , level = 'debug' ) if ' ' not in ticker . strip ( ) : ticker = f'XYZ {ticker.strip()} Equity' info = param . load_info ( cat = 'exch' ) . get ( market_info ( ticker = ticker ) . get ( 'exch' , '' ) , dict ( ) ) if ( 'allday' in info ) and ( 'day' not in info ) : info [ 'day' ] = info [ 'allday' ] if any ( req not in info for req in [ 'tz' , 'allday' , 'day' ] ) : logger . error ( f'required exchange info cannot be found in {ticker} ...' ) return pd . Series ( ) for ss in ValidSessions : if ss not in info : continue info [ ss ] = [ param . to_hour ( num = s ) for s in info [ ss ] ] return pd . Series ( info )
13757	def get_path_extension ( path ) : file_path , file_ext = os . path . splitext ( path ) return file_ext . lstrip ( '.' )
8825	def update_sg ( self , context , sg , rule_id , action ) : db_sg = db_api . security_group_find ( context , id = sg , scope = db_api . ONE ) if not db_sg : return None with context . session . begin ( ) : job_body = dict ( action = "%s sg rule %s" % ( action , rule_id ) , resource_id = rule_id , tenant_id = db_sg [ 'tenant_id' ] ) job_body = dict ( job = job_body ) job = job_api . create_job ( context . elevated ( ) , job_body ) rpc_client = QuarkSGAsyncProducerClient ( ) try : rpc_client . populate_subtasks ( context , sg , job [ 'id' ] ) except om_exc . MessagingTimeout : LOG . error ( "Failed to create subtasks. Rabbit running?" ) return None return { "job_id" : job [ 'id' ] }
6917	def simple_flare_find ( times , mags , errs , smoothbinsize = 97 , flare_minsigma = 4.0 , flare_maxcadencediff = 1 , flare_mincadencepoints = 3 , magsarefluxes = False , savgol_polyorder = 2 , ** savgol_kwargs ) : if errs is None : errs = 0.001 * mags finiteind = np . isfinite ( times ) & np . isfinite ( mags ) & np . isfinite ( errs ) ftimes = times [ finiteind ] fmags = mags [ finiteind ] ferrs = errs [ finiteind ] smoothed = savgol_filter ( fmags , smoothbinsize , savgol_polyorder , ** savgol_kwargs ) subtracted = fmags - smoothed series_mad = np . median ( np . abs ( subtracted ) ) series_stdev = 1.483 * series_mad if magsarefluxes : extind = np . where ( subtracted > ( flare_minsigma * series_stdev ) ) else : extind = np . where ( subtracted < ( - flare_minsigma * series_stdev ) ) if extind and extind [ 0 ] : extrema_indices = extind [ 0 ] flaregroups = [ ] for ind , extrema_index in enumerate ( extrema_indices ) : pass
4585	def animated_gif_to_colorlists ( image , container = list ) : deprecated . deprecated ( 'util.gif.animated_gif_to_colorlists' ) from PIL import ImageSequence it = ImageSequence . Iterator ( image ) return [ image_to_colorlist ( i , container ) for i in it ]
4028	def create_cookie ( host , path , secure , expires , name , value ) : return http . cookiejar . Cookie ( 0 , name , value , None , False , host , host . startswith ( '.' ) , host . startswith ( '.' ) , path , True , secure , expires , False , None , None , { } )
5164	def __intermediate_bridge ( self , interface , i ) : if interface [ 'type' ] == 'bridge' and i < 2 : bridge_members = ' ' . join ( interface . pop ( 'bridge_members' ) ) if bridge_members : interface [ 'ifname' ] = bridge_members else : interface [ 'bridge_empty' ] = True del interface [ 'ifname' ] elif interface [ 'type' ] == 'bridge' and i >= 2 : if 'br-' not in interface [ 'ifname' ] : interface [ 'ifname' ] = 'br-{ifname}' . format ( ** interface ) for attr in [ 'type' , 'bridge_members' , 'stp' , 'gateway' ] : if attr in interface : del interface [ attr ] elif interface [ 'type' ] != 'bridge' : del interface [ 'type' ] return interface
3758	def UFL ( Hc = None , atoms = { } , CASRN = '' , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in IEC_2010 . index and not np . isnan ( IEC_2010 . at [ CASRN , 'UFL' ] ) : methods . append ( IEC ) if CASRN in NFPA_2008 . index and not np . isnan ( NFPA_2008 . at [ CASRN , 'UFL' ] ) : methods . append ( NFPA ) if Hc : methods . append ( SUZUKI ) if atoms : methods . append ( CROWLLOUVAR ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IEC : return float ( IEC_2010 . at [ CASRN , 'UFL' ] ) elif Method == NFPA : return float ( NFPA_2008 . at [ CASRN , 'UFL' ] ) elif Method == SUZUKI : return Suzuki_UFL ( Hc = Hc ) elif Method == CROWLLOUVAR : return Crowl_Louvar_UFL ( atoms = atoms ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
12662	def load_mask ( image , allow_empty = True ) : img = check_img ( image , make_it_3d = True ) values = np . unique ( img . get_data ( ) ) if len ( values ) == 1 : if values [ 0 ] == 0 and not allow_empty : raise ValueError ( 'Given mask is invalid because it masks all data' ) elif len ( values ) == 2 : if 0 not in values : raise ValueError ( 'Background of the mask must be represented with 0.' ' Given mask contains: {}.' . format ( values ) ) elif len ( values ) != 2 : raise ValueError ( 'Given mask is not made of 2 values: {}. ' 'Cannot interpret as true or false' . format ( values ) ) return nib . Nifti1Image ( as_ndarray ( get_img_data ( img ) , dtype = bool ) , img . get_affine ( ) , img . get_header ( ) )
4666	def sign ( self , wifkeys , chain = None ) : if not chain : chain = self . get_default_prefix ( ) self . deriveDigest ( chain ) self . privkeys = [ ] for item in wifkeys : if item not in self . privkeys : self . privkeys . append ( item ) sigs = [ ] for wif in self . privkeys : signature = sign_message ( self . message , wif ) sigs . append ( Signature ( signature ) ) self . data [ "signatures" ] = Array ( sigs ) return self
9250	def filter_issues_for_tags ( self , newer_tag , older_tag ) : filtered_pull_requests = self . delete_by_time ( self . pull_requests , older_tag , newer_tag ) filtered_issues = self . delete_by_time ( self . issues , older_tag , newer_tag ) newer_tag_name = newer_tag [ "name" ] if newer_tag else None if self . options . filter_issues_by_milestone : filtered_issues = self . filter_by_milestone ( filtered_issues , newer_tag_name , self . issues ) filtered_pull_requests = self . filter_by_milestone ( filtered_pull_requests , newer_tag_name , self . pull_requests ) return filtered_issues , filtered_pull_requests
3611	def delete_async ( self , url , name , callback = None , params = None , headers = None ) : if not name : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) process_pool . apply_async ( make_delete_request , args = ( endpoint , params , headers ) , callback = callback )
4617	def formatTime ( t ) : if isinstance ( t , float ) : return datetime . utcfromtimestamp ( t ) . strftime ( timeFormat ) if isinstance ( t , datetime ) : return t . strftime ( timeFormat )
711	def _iterModels ( modelIDs ) : class ModelInfoIterator ( object ) : __CACHE_LIMIT = 1000 debug = False def __init__ ( self , modelIDs ) : self . __modelIDs = tuple ( modelIDs ) if self . debug : _emit ( Verbosity . DEBUG , "MODELITERATOR: __init__; numModelIDs=%s" % len ( self . __modelIDs ) ) self . __nextIndex = 0 self . __modelCache = collections . deque ( ) return def __iter__ ( self ) : return self def next ( self ) : return self . __getNext ( ) def __getNext ( self ) : if self . debug : _emit ( Verbosity . DEBUG , "MODELITERATOR: __getNext(); modelCacheLen=%s" % ( len ( self . __modelCache ) ) ) if not self . __modelCache : self . __fillCache ( ) if not self . __modelCache : raise StopIteration ( ) return self . __modelCache . popleft ( ) def __fillCache ( self ) : assert ( not self . __modelCache ) numModelIDs = len ( self . __modelIDs ) if self . __modelIDs else 0 if self . __nextIndex >= numModelIDs : return idRange = self . __nextIndex + self . __CACHE_LIMIT if idRange > numModelIDs : idRange = numModelIDs lookupIDs = self . __modelIDs [ self . __nextIndex : idRange ] self . __nextIndex += ( idRange - self . __nextIndex ) infoList = _clientJobsDB ( ) . modelsInfo ( lookupIDs ) assert len ( infoList ) == len ( lookupIDs ) , "modelsInfo returned %s elements; expected %s." % ( len ( infoList ) , len ( lookupIDs ) ) for rawInfo in infoList : modelInfo = _NupicModelInfo ( rawInfo = rawInfo ) self . __modelCache . append ( modelInfo ) assert len ( self . __modelCache ) == len ( lookupIDs ) , "Added %s elements to modelCache; expected %s." % ( len ( self . __modelCache ) , len ( lookupIDs ) ) if self . debug : _emit ( Verbosity . DEBUG , "MODELITERATOR: Leaving __fillCache(); modelCacheLen=%s" % ( len ( self . __modelCache ) , ) ) return ModelInfoIterator ( modelIDs )
2678	def get_role_name ( region , account_id , role ) : prefix = ARN_PREFIXES . get ( region , 'aws' ) return 'arn:{0}:iam::{1}:role/{2}' . format ( prefix , account_id , role )
1305	def GetConsoleOriginalTitle ( ) -> str : if IsNT6orHigher : arrayType = ctypes . c_wchar * MAX_PATH values = arrayType ( ) ctypes . windll . kernel32 . GetConsoleOriginalTitleW ( values , MAX_PATH ) return values . value else : raise RuntimeError ( 'GetConsoleOriginalTitle is not supported on Windows XP or lower.' )
12699	def _parse_data_fields ( self , fields , tag_id = "tag" , sub_id = "code" ) : for field in fields : params = field . params if tag_id not in params : continue field_repr = OrderedDict ( [ [ self . i1_name , params . get ( self . i1_name , " " ) ] , [ self . i2_name , params . get ( self . i2_name , " " ) ] , ] ) for subfield in field . find ( "subfield" ) : if sub_id not in subfield . params : continue content = MARCSubrecord ( val = subfield . getContent ( ) . strip ( ) , i1 = field_repr [ self . i1_name ] , i2 = field_repr [ self . i2_name ] , other_subfields = field_repr ) code = subfield . params [ sub_id ] if code in field_repr : field_repr [ code ] . append ( content ) else : field_repr [ code ] = [ content ] tag = params [ tag_id ] if tag in self . datafields : self . datafields [ tag ] . append ( field_repr ) else : self . datafields [ tag ] = [ field_repr ]
9727	def data_received ( self , data ) : self . _received_data += data h_size = RTheader . size data = self . _received_data size , type_ = RTheader . unpack_from ( data , 0 ) while len ( data ) >= size : self . _parse_received ( data [ h_size : size ] , type_ ) data = data [ size : ] if len ( data ) < h_size : break size , type_ = RTheader . unpack_from ( data , 0 ) self . _received_data = data
7900	def process_configuration_success ( self , stanza ) : _unused = stanza self . configured = True self . handler . room_configured ( )
3712	def calculate_P ( self , T , P , method ) : r if method == EOS : self . eos [ 0 ] = self . eos [ 0 ] . to_TP ( T = T , P = P ) Vm = self . eos [ 0 ] . V_g elif method == TSONOPOULOS_EXTENDED : B = BVirial_Tsonopoulos_extended ( T , self . Tc , self . Pc , self . omega , dipole = self . dipole ) Vm = ideal_gas ( T , P ) + B elif method == TSONOPOULOS : B = BVirial_Tsonopoulos ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == ABBOTT : B = BVirial_Abbott ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == PITZER_CURL : B = BVirial_Pitzer_Curl ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == CRC_VIRIAL : a1 , a2 , a3 , a4 , a5 = self . CRC_VIRIAL_coeffs t = 298.15 / T - 1. B = ( a1 + a2 * t + a3 * t ** 2 + a4 * t ** 3 + a5 * t ** 4 ) / 1E6 Vm = ideal_gas ( T , P ) + B elif method == IDEAL : Vm = ideal_gas ( T , P ) elif method == COOLPROP : Vm = 1. / PropsSI ( 'DMOLAR' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : Vm = self . interpolate_P ( T , P , method ) return Vm
5704	def _scan_footpaths ( self , stop_id , walk_departure_time ) : for _ , neighbor , data in self . _walk_network . edges_iter ( nbunch = [ stop_id ] , data = True ) : d_walk = data [ "d_walk" ] arrival_time = walk_departure_time + d_walk / self . _walk_speed self . _update_stop_label ( neighbor , arrival_time )
11315	def update_notes ( self ) : fields = record_get_field_instances ( self . record , '500' ) for field in fields : subs = field_get_subfields ( field ) for sub in subs . get ( 'a' , [ ] ) : sub = sub . strip ( ) if sub . startswith ( "*" ) and sub . endswith ( "*" ) : record_delete_field ( self . record , tag = "500" , field_position_global = field [ 4 ] )
62	def is_fully_within_image ( self , image ) : shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] return self . x1 >= 0 and self . x2 < width and self . y1 >= 0 and self . y2 < height
6855	def ismounted ( device ) : with settings ( hide ( 'running' , 'stdout' ) ) : res = run_as_root ( 'mount' ) for line in res . splitlines ( ) : fields = line . split ( ) if fields [ 0 ] == device : return True with settings ( hide ( 'running' , 'stdout' ) ) : res = run_as_root ( 'swapon -s' ) for line in res . splitlines ( ) : fields = line . split ( ) if fields [ 0 ] == device : return True return False
6	def cg ( f_Ax , b , cg_iters = 10 , callback = None , verbose = False , residual_tol = 1e-10 ) : p = b . copy ( ) r = b . copy ( ) x = np . zeros_like ( b ) rdotr = r . dot ( r ) fmtstr = "%10i %10.3g %10.3g" titlestr = "%10s %10s %10s" if verbose : print ( titlestr % ( "iter" , "residual norm" , "soln norm" ) ) for i in range ( cg_iters ) : if callback is not None : callback ( x ) if verbose : print ( fmtstr % ( i , rdotr , np . linalg . norm ( x ) ) ) z = f_Ax ( p ) v = rdotr / p . dot ( z ) x += v * p r -= v * z newrdotr = r . dot ( r ) mu = newrdotr / rdotr p = r + mu * p rdotr = newrdotr if rdotr < residual_tol : break if callback is not None : callback ( x ) if verbose : print ( fmtstr % ( i + 1 , rdotr , np . linalg . norm ( x ) ) ) return x
11119	def get_file_info ( self , relativePath , name = None ) : relativePath = os . path . normpath ( relativePath ) if relativePath == '.' : relativePath = '' assert name != '.pyrepinfo' , "'.pyrepinfo' can't be a file name." if name is None : assert len ( relativePath ) , "name must be given when relative path is given as empty string or as a simple dot '.'" relativePath , name = os . path . split ( relativePath ) errorMessage = "" dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) if dirInfoDict is None : return None , errorMessage fileInfo = dict . __getitem__ ( dirInfoDict , "files" ) . get ( name , None ) if fileInfo is None : errorMessage = "file %s does not exist in relative path '%s'" % ( name , relativePath ) return fileInfo , errorMessage
6744	def install_config ( local_path = None , remote_path = None , render = True , extra = None , formatter = None ) : local_path = find_template ( local_path ) if render : extra = extra or { } local_path = render_to_file ( template = local_path , extra = extra , formatter = formatter ) put_or_dryrun ( local_path = local_path , remote_path = remote_path , use_sudo = True )
673	def runHotgym ( numRecords ) : dataSource = FileRecordStream ( streamID = _INPUT_FILE_PATH ) numRecords = min ( numRecords , dataSource . getDataRowCount ( ) ) network = createNetwork ( dataSource ) network . regions [ "sensor" ] . setParameter ( "predictedField" , "consumption" ) network . regions [ "SP" ] . setParameter ( "learningMode" , 1 ) network . regions [ "TM" ] . setParameter ( "learningMode" , 1 ) network . regions [ "classifier" ] . setParameter ( "learningMode" , 1 ) network . regions [ "SP" ] . setParameter ( "inferenceMode" , 1 ) network . regions [ "TM" ] . setParameter ( "inferenceMode" , 1 ) network . regions [ "classifier" ] . setParameter ( "inferenceMode" , 1 ) results = [ ] N = 1 for iteration in range ( 0 , numRecords , N ) : network . run ( N ) predictionResults = getPredictionResults ( network , "classifier" ) oneStep = predictionResults [ 1 ] [ "predictedValue" ] oneStepConfidence = predictionResults [ 1 ] [ "predictionConfidence" ] fiveStep = predictionResults [ 5 ] [ "predictedValue" ] fiveStepConfidence = predictionResults [ 5 ] [ "predictionConfidence" ] result = ( oneStep , oneStepConfidence * 100 , fiveStep , fiveStepConfidence * 100 ) print "1-step: {:16} ({:4.4}%)\t 5-step: {:16} ({:4.4}%)" . format ( * result ) results . append ( result ) return results
4312	def _validate_volumes ( input_volumes ) : if not ( input_volumes is None or isinstance ( input_volumes , list ) ) : raise TypeError ( "input_volumes must be None or a list." ) if isinstance ( input_volumes , list ) : for vol in input_volumes : if not core . is_number ( vol ) : raise ValueError ( "Elements of input_volumes must be numbers: found {}" . format ( vol ) )
9078	def make_df_getter ( data_url : str , data_path : str , ** kwargs ) -> Callable [ [ Optional [ str ] , bool , bool ] , pd . DataFrame ] : download_function = make_downloader ( data_url , data_path ) def get_df ( url : Optional [ str ] = None , cache : bool = True , force_download : bool = False ) -> pd . DataFrame : if url is None and cache : url = download_function ( force_download = force_download ) return pd . read_csv ( url or data_url , ** kwargs ) return get_df
5136	def get_class_traits ( klass ) : source = inspect . getsource ( klass ) cb = CommentBlocker ( ) cb . process_file ( StringIO ( source ) ) mod_ast = compiler . parse ( source ) class_ast = mod_ast . node . nodes [ 0 ] for node in class_ast . code . nodes : if isinstance ( node , compiler . ast . Assign ) : name = node . nodes [ 0 ] . name rhs = unparse ( node . expr ) . strip ( ) doc = strip_comment_marker ( cb . search_for_comment ( node . lineno , default = '' ) ) yield name , rhs , doc
5755	def get_regressions ( package_descriptors , targets , building_repo_data , testing_repo_data , main_repo_data ) : regressions = { } for package_descriptor in package_descriptors . values ( ) : pkg_name = package_descriptor . pkg_name debian_pkg_name = package_descriptor . debian_pkg_name regressions [ pkg_name ] = { } for target in targets : regressions [ pkg_name ] [ target ] = False main_version = main_repo_data . get ( target , { } ) . get ( debian_pkg_name , None ) if main_version is not None : main_ver_loose = LooseVersion ( main_version ) for repo_data in [ building_repo_data , testing_repo_data ] : version = repo_data . get ( target , { } ) . get ( debian_pkg_name , None ) if not version or main_ver_loose > LooseVersion ( version ) : regressions [ pkg_name ] [ target ] = True return regressions
4496	def login ( self , username , password = None , token = None ) : self . session . basic_auth ( username , password )
1940	def get_func_signature ( self , hsh : bytes ) -> Optional [ str ] : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) return self . _function_signatures_by_selector . get ( hsh )
11855	def predictor ( self , ( i , j , A , alpha , Bb ) ) : "Add to chart any rules for B that could help extend this edge." B = Bb [ 0 ] if B in self . grammar . rules : for rhs in self . grammar . rewrites_for ( B ) : self . add_edge ( [ j , j , B , [ ] , rhs ] )
12912	def append ( self , item ) : if self . meta_type == 'dict' : raise AssertionError ( 'Cannot append to object of `dict` base type!' ) if self . meta_type == 'list' : self . _list . append ( item ) return
765	def runNetwork ( network , numRecords , writer ) : sensorRegion = network . regions [ _RECORD_SENSOR ] l1SpRegion = network . regions [ _L1_SPATIAL_POOLER ] l1TpRegion = network . regions [ _L1_TEMPORAL_MEMORY ] l1Classifier = network . regions [ _L1_CLASSIFIER ] l2SpRegion = network . regions [ _L2_SPATIAL_POOLER ] l2TpRegion = network . regions [ _L2_TEMPORAL_MEMORY ] l2Classifier = network . regions [ _L2_CLASSIFIER ] l1PreviousPredictedColumns = [ ] l2PreviousPredictedColumns = [ ] l1PreviousPrediction = None l2PreviousPrediction = None l1ErrorSum = 0.0 l2ErrorSum = 0.0 for record in xrange ( numRecords ) : network . run ( 1 ) actual = float ( sensorRegion . getOutputData ( "actValueOut" ) [ 0 ] ) l1Predictions = l1Classifier . getOutputData ( "actualValues" ) l1Probabilities = l1Classifier . getOutputData ( "probabilities" ) l1Prediction = l1Predictions [ l1Probabilities . argmax ( ) ] if l1PreviousPrediction is not None : l1ErrorSum += math . fabs ( l1PreviousPrediction - actual ) l1PreviousPrediction = l1Prediction l2Predictions = l2Classifier . getOutputData ( "actualValues" ) l2Probabilities = l2Classifier . getOutputData ( "probabilities" ) l2Prediction = l2Predictions [ l2Probabilities . argmax ( ) ] if l2PreviousPrediction is not None : l2ErrorSum += math . fabs ( l2PreviousPrediction - actual ) l2PreviousPrediction = l2Prediction l1AnomalyScore = l1TpRegion . getOutputData ( "anomalyScore" ) [ 0 ] l2AnomalyScore = l2TpRegion . getOutputData ( "anomalyScore" ) [ 0 ] writer . writerow ( ( record , actual , l1PreviousPrediction , l1AnomalyScore , l2PreviousPrediction , l2AnomalyScore ) ) l1PredictedColumns = l1TpRegion . getOutputData ( "topDownOut" ) . nonzero ( ) [ 0 ] l1PreviousPredictedColumns = copy . deepcopy ( l1PredictedColumns ) l2PredictedColumns = l2TpRegion . getOutputData ( "topDownOut" ) . nonzero ( ) [ 0 ] l2PreviousPredictedColumns = copy . deepcopy ( l2PredictedColumns ) if numRecords > 1 : print "L1 ave abs class. error: %f" % ( l1ErrorSum / ( numRecords - 1 ) ) print "L2 ave abs class. error: %f" % ( l2ErrorSum / ( numRecords - 1 ) )
9908	def send_duplicate_notification ( self ) : email_utils . send_email ( from_email = settings . DEFAULT_FROM_EMAIL , recipient_list = [ self . email ] , subject = _ ( "Registration Attempt" ) , template_name = "rest_email_auth/emails/duplicate-email" , ) logger . info ( "Sent duplicate email notification to: %s" , self . email )
12040	def checkOut ( thing , html = True ) : msg = "" for name in sorted ( dir ( thing ) ) : if not "__" in name : msg += "<b>%s</b>\n" % name try : msg += " ^-VALUE: %s\n" % getattr ( thing , name ) ( ) except : pass if html : html = '<html><body><code>' + msg + '</code></body></html>' html = html . replace ( " " , "&nbsp;" ) . replace ( "\n" , "<br>" ) fname = tempfile . gettempdir ( ) + "/swhlab/checkout.html" with open ( fname , 'w' ) as f : f . write ( html ) webbrowser . open ( fname ) print ( msg . replace ( '<b>' , '' ) . replace ( '</b>' , '' ) )
7974	def stop ( self , join = False , timeout = None ) : logger . debug ( "Closing the io handlers..." ) for handler in self . io_handlers : handler . close ( ) if self . event_thread and self . event_thread . is_alive ( ) : logger . debug ( "Sending the QUIT signal" ) self . event_queue . put ( QUIT ) logger . debug ( " sent" ) threads = self . io_threads + self . timeout_threads for thread in threads : logger . debug ( "Stopping thread: {0!r}" . format ( thread ) ) thread . stop ( ) if not join : return if self . event_thread : threads . append ( self . event_thread ) if timeout is None : for thread in threads : thread . join ( ) else : timeout1 = ( timeout * 0.01 ) / len ( threads ) threads_left = [ ] for thread in threads : logger . debug ( "Quick-joining thread {0!r}..." . format ( thread ) ) thread . join ( timeout1 ) if thread . is_alive ( ) : logger . debug ( " thread still alive" . format ( thread ) ) threads_left . append ( thread ) if threads_left : timeout2 = ( timeout * 0.99 ) / len ( threads_left ) for thread in threads_left : logger . debug ( "Joining thread {0!r}..." . format ( thread ) ) thread . join ( timeout2 ) self . io_threads = [ ] self . event_thread = None
8175	def update ( self , shuffled = True , cohesion = 100 , separation = 10 , alignment = 5 , goal = 20 , limit = 30 ) : from random import shuffle if shuffled : shuffle ( self ) m1 = 1.0 m2 = 1.0 m3 = 1.0 m4 = 1.0 if not self . scattered and _ctx . random ( ) < self . _scatter : self . scattered = True if self . scattered : m1 = - m1 m3 *= 0.25 self . _scatter_i += 1 if self . _scatter_i >= self . _scatter_t : self . scattered = False self . _scatter_i = 0 if not self . has_goal : m4 = 0 if self . flee : m4 = - m4 for b in self : if b . is_perching : if b . _perch_t > 0 : b . _perch_t -= 1 continue else : b . is_perching = False vx1 , vy1 , vz1 = b . cohesion ( cohesion ) vx2 , vy2 , vz2 = b . separation ( separation ) vx3 , vy3 , vz3 = b . alignment ( alignment ) vx4 , vy4 , vz4 = b . goal ( self . _gx , self . _gy , self . _gz , goal ) b . vx += m1 * vx1 + m2 * vx2 + m3 * vx3 + m4 * vx4 b . vy += m1 * vy1 + m2 * vy2 + m3 * vy3 + m4 * vy4 b . vz += m1 * vz1 + m2 * vz2 + m3 * vz3 + m4 * vz4 b . limit ( limit ) b . x += b . vx b . y += b . vy b . z += b . vz self . constrain ( )
11631	def _readNamelist ( currentlyIncluding , cache , namFilename , unique_glyphs ) : filename = os . path . abspath ( os . path . normcase ( namFilename ) ) if filename in currentlyIncluding : raise NamelistRecursionError ( filename ) currentlyIncluding . add ( filename ) try : result = __readNamelist ( cache , filename , unique_glyphs ) finally : currentlyIncluding . remove ( filename ) return result
6939	def checkplot_infokey_worker ( task ) : cpf , keys = task cpd = _read_checkplot_picklefile ( cpf ) resultkeys = [ ] for k in keys : try : resultkeys . append ( _dict_get ( cpd , k ) ) except Exception as e : resultkeys . append ( np . nan ) return resultkeys
12555	def sav_to_pandas_savreader ( input_file ) : from savReaderWriter import SavReader lines = [ ] with SavReader ( input_file , returnHeader = True ) as reader : header = next ( reader ) for line in reader : lines . append ( line ) return pd . DataFrame ( data = lines , columns = header )
1393	def getTopologyByClusterRoleEnvironAndName ( self , cluster , role , environ , topologyName ) : topologies = list ( filter ( lambda t : t . name == topologyName and t . cluster == cluster and ( not role or t . execution_state . role == role ) and t . environ == environ , self . topologies ) ) if not topologies or len ( topologies ) > 1 : if role is not None : raise Exception ( "Topology not found for {0}, {1}, {2}, {3}" . format ( cluster , role , environ , topologyName ) ) else : raise Exception ( "Topology not found for {0}, {1}, {2}" . format ( cluster , environ , topologyName ) ) return topologies [ 0 ]
7968	def _run_io_threads ( self , handler ) : reader = ReadingThread ( self . settings , handler , daemon = self . daemon , exc_queue = self . exc_queue ) writter = WrittingThread ( self . settings , handler , daemon = self . daemon , exc_queue = self . exc_queue ) self . io_threads += [ reader , writter ] reader . start ( ) writter . start ( )
9377	def calculate_stats ( data_list , stats_to_calculate = [ 'mean' , 'std' ] , percentiles_to_calculate = [ ] ) : stats_to_numpy_method_map = { 'mean' : numpy . mean , 'avg' : numpy . mean , 'std' : numpy . std , 'standard_deviation' : numpy . std , 'median' : numpy . median , 'min' : numpy . amin , 'max' : numpy . amax } calculated_stats = { } calculated_percentiles = { } if len ( data_list ) == 0 : return calculated_stats , calculated_percentiles for stat in stats_to_calculate : if stat in stats_to_numpy_method_map . keys ( ) : calculated_stats [ stat ] = stats_to_numpy_method_map [ stat ] ( data_list ) else : logger . error ( "Unsupported stat : " + str ( stat ) ) for percentile in percentiles_to_calculate : if isinstance ( percentile , float ) or isinstance ( percentile , int ) : calculated_percentiles [ percentile ] = numpy . percentile ( data_list , percentile ) else : logger . error ( "Unsupported percentile requested (should be int or float): " + str ( percentile ) ) return calculated_stats , calculated_percentiles
12140	def load_table ( self , table ) : items , data_keys = [ ] , None for key , filename in table . items ( ) : data_dict = self . filetype . data ( filename [ 0 ] ) current_keys = tuple ( sorted ( data_dict . keys ( ) ) ) values = [ data_dict [ k ] for k in current_keys ] if data_keys is None : data_keys = current_keys elif data_keys != current_keys : raise Exception ( "Data keys are inconsistent" ) items . append ( ( key , values ) ) return Table ( items , kdims = table . kdims , vdims = data_keys )
4647	def create ( self ) : query = ( ) . format ( self . __tablename__ , self . __key__ , self . __value__ ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( query ) connection . commit ( )
4142	def _numpy_cholesky ( A , B ) : L = numpy . linalg . cholesky ( A ) y = numpy . linalg . solve ( L , B ) x = numpy . linalg . solve ( L . transpose ( ) . conjugate ( ) , y ) return x , L
8872	def match ( self , subsetLines , offsetOfSubset , fileName ) : for ( offset , l ) in enumerate ( subsetLines ) : for t in self . regex : m = t . Regex . search ( l ) if m != None : truePosition = offset + offsetOfSubset _logger . debug ( 'Found match on line {}' . format ( str ( truePosition + 1 ) ) ) _logger . debug ( 'Line is {}' . format ( l ) ) self . failed = True self . matchLocation = CheckFileParser . FileLocation ( fileName , truePosition + 1 ) raise DirectiveException ( self )
10655	def run ( self , clock ) : if clock . timestep_ix >= self . period_count : return for c in self . components : c . run ( clock , self . gl ) self . _perform_year_end_procedure ( clock )
6590	def put ( self , package ) : pkgidx = self . workingArea . put_package ( package ) logger = logging . getLogger ( __name__ ) logger . info ( 'submitting {}' . format ( self . workingArea . package_relpath ( pkgidx ) ) ) runid = self . dispatcher . run ( self . workingArea , pkgidx ) self . runid_pkgidx_map [ runid ] = pkgidx return pkgidx
7316	def create_query ( self , attr ) : field = attr [ 0 ] operator = attr [ 1 ] value = attr [ 2 ] model = self . model if '.' in field : field_items = field . split ( '.' ) field_name = getattr ( model , field_items [ 0 ] , None ) class_name = field_name . property . mapper . class_ new_model = getattr ( class_name , field_items [ 1 ] ) return field_name . has ( OPERATORS [ operator ] ( new_model , value ) ) return OPERATORS [ operator ] ( getattr ( model , field , None ) , value )
12100	def _append_log ( self , specs ) : self . _spec_log += specs log_path = os . path . join ( self . root_directory , ( "%s.log" % self . batch_name ) ) core . Log . write_log ( log_path , [ spec for ( _ , spec ) in specs ] , allow_append = True )
6453	def dist ( self , src , tar ) : if tar == src : return 0.0 if not src or not tar : return 1.0 max_length = max ( len ( src ) , len ( tar ) ) return self . dist_abs ( src , tar ) / max_length
12106	def _launch_process_group ( self , process_commands , streams_path ) : processes = [ ] for cmd , tid in process_commands : job_timestamp = time . strftime ( '%H%M%S' ) basename = "%s_%s_tid_%d" % ( self . batch_name , job_timestamp , tid ) stdout_path = os . path . join ( streams_path , "%s.o.%d" % ( basename , tid ) ) stderr_path = os . path . join ( streams_path , "%s.e.%d" % ( basename , tid ) ) process = { 'tid' : tid , 'cmd' : cmd , 'stdout' : stdout_path , 'stderr' : stderr_path } processes . append ( process ) json_path = os . path . join ( self . root_directory , self . json_name % ( tid ) ) with open ( json_path , 'w' ) as json_file : json . dump ( processes , json_file , sort_keys = True , indent = 4 ) p = subprocess . Popen ( [ self . script_path , json_path , self . batch_name , str ( len ( processes ) ) , str ( self . max_concurrency ) ] ) if p . wait ( ) != 0 : raise EnvironmentError ( "Script command exit with code: %d" % p . poll ( ) )
11607	def social_widget_render ( parser , token ) : bits = token . split_contents ( ) tag_name = bits [ 0 ] if len ( bits ) < 2 : raise TemplateSyntaxError ( "'%s' takes at least one argument" % tag_name ) args = [ ] kwargs = { } bits = bits [ 1 : ] if len ( bits ) : for bit in bits : match = kwarg_re . match ( bit ) if not match : raise TemplateSyntaxError ( "Malformed arguments to %s tag" % tag_name ) name , value = match . groups ( ) if name : name = name . replace ( '-' , '_' ) kwargs [ name ] = parser . compile_filter ( value ) else : args . append ( parser . compile_filter ( value ) ) return SocialWidgetNode ( args , kwargs )
10693	def rgb_to_hsv ( rgb ) : r , g , b = rgb [ 0 ] / 255 , rgb [ 1 ] / 255 , rgb [ 2 ] / 255 _min = min ( r , g , b ) _max = max ( r , g , b ) v = _max delta = _max - _min if _max == 0 : return 0 , 0 , v s = delta / _max if delta == 0 : delta = 1 if r == _max : h = 60 * ( ( ( g - b ) / delta ) % 6 ) elif g == _max : h = 60 * ( ( ( b - r ) / delta ) + 2 ) else : h = 60 * ( ( ( r - g ) / delta ) + 4 ) return round ( h , 3 ) , round ( s , 3 ) , round ( v , 3 )
11370	def convert_date_from_iso_to_human ( value ) : try : year , month , day = value . split ( "-" ) except ValueError : try : year , month , day = value . split ( " " ) except ValueError : return value try : date_object = datetime ( int ( year ) , int ( month ) , int ( day ) ) except TypeError : return value return date_object . strftime ( "%d %b %Y" )
5118	def get_queue_data ( self , queues = None , edge = None , edge_type = None , return_header = False ) : queues = _get_queues ( self . g , queues , edge , edge_type ) data = np . zeros ( ( 0 , 6 ) ) for q in queues : dat = self . edge2queue [ q ] . fetch_data ( ) if len ( dat ) > 0 : data = np . vstack ( ( data , dat ) ) if return_header : return data , 'arrival,service,departure,num_queued,num_total,q_id' return data
1755	def read_register ( self , register ) : self . _publish ( 'will_read_register' , register ) value = self . _regfile . read ( register ) self . _publish ( 'did_read_register' , register , value ) return value
12322	def to_holvi_dict ( self ) : self . _jsondata [ "items" ] = [ ] for item in self . items : self . _jsondata [ "items" ] . append ( item . to_holvi_dict ( ) ) self . _jsondata [ "issue_date" ] = self . issue_date . isoformat ( ) self . _jsondata [ "due_date" ] = self . due_date . isoformat ( ) self . _jsondata [ "receiver" ] = self . receiver . to_holvi_dict ( ) return { k : v for ( k , v ) in self . _jsondata . items ( ) if k in self . _valid_keys }
7002	def parallel_pf_lcdir ( lcdir , outdir , fileglob = None , recursive = True , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , pfmethods = ( 'gls' , 'pdm' , 'mav' , 'win' ) , pfkwargs = ( { } , { } , { } , { } ) , sigclip = 10.0 , getblssnr = False , nperiodworkers = NCPUS , ncontrolworkers = 1 , liststartindex = None , listmaxobjects = None , minobservations = 500 , excludeprocessed = True ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if not fileglob : fileglob = dfileglob LOGINFO ( 'searching for %s light curves in %s ...' % ( lcformat , lcdir ) ) if recursive is False : matching = glob . glob ( os . path . join ( lcdir , fileglob ) ) else : if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( lcdir , '**' , fileglob ) , recursive = True ) else : walker = os . walk ( lcdir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , fileglob ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) if matching and len ( matching ) > 0 : matching = sorted ( matching ) LOGINFO ( 'found %s light curves, running pf...' % len ( matching ) ) return parallel_pf ( matching , outdir , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = lcformatdir , pfmethods = pfmethods , pfkwargs = pfkwargs , getblssnr = getblssnr , sigclip = sigclip , nperiodworkers = nperiodworkers , ncontrolworkers = ncontrolworkers , liststartindex = liststartindex , listmaxobjects = listmaxobjects , minobservations = minobservations , excludeprocessed = excludeprocessed ) else : LOGERROR ( 'no light curve files in %s format found in %s' % ( lcformat , lcdir ) ) return None
4471	def _transform ( self , jam , state ) : if not hasattr ( jam . sandbox , 'muda' ) : raise RuntimeError ( 'No muda state found in jams sandbox.' ) jam_w = copy . deepcopy ( jam ) jam_w . sandbox . muda [ 'history' ] . append ( { 'transformer' : self . __serialize__ , 'state' : state } ) if hasattr ( self , 'audio' ) : self . audio ( jam_w . sandbox . muda , state ) if hasattr ( self , 'metadata' ) : self . metadata ( jam_w . file_metadata , state ) for query , function_name in six . iteritems ( self . dispatch ) : function = getattr ( self , function_name ) for matched_annotation in jam_w . search ( namespace = query ) : function ( matched_annotation , state ) return jam_w
3836	async def set_conversation_notification_level ( self , set_conversation_notification_level_request ) : response = hangouts_pb2 . SetConversationNotificationLevelResponse ( ) await self . _pb_request ( 'conversations/setconversationnotificationlevel' , set_conversation_notification_level_request , response ) return response
2974	def cmd_up ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) containers = b . create ( verbose = opts . verbose , force = opts . force ) print_containers ( containers , opts . json )
11897	def _create_index_files ( root_dir , force_no_processing = False ) : created_files = [ ] for here , dirs , files in os . walk ( root_dir ) : print ( 'Processing %s' % here ) dirs = sorted ( dirs ) image_files = [ f for f in files if re . match ( IMAGE_FILE_REGEX , f ) ] image_files = sorted ( image_files ) created_files . append ( _create_index_file ( root_dir , here , image_files , dirs , force_no_processing ) ) return created_files
7844	def set_type ( self , item_type ) : if not item_type : raise ValueError ( "Type is required in DiscoIdentity" ) item_type = unicode ( item_type ) self . xmlnode . setProp ( "type" , item_type . encode ( "utf-8" ) )
7739	def check_bidi ( data ) : has_l = False has_ral = False for char in data : if stringprep . in_table_d1 ( char ) : has_ral = True elif stringprep . in_table_d2 ( char ) : has_l = True if has_l and has_ral : raise StringprepError ( "Both RandALCat and LCat characters present" ) if has_ral and ( not stringprep . in_table_d1 ( data [ 0 ] ) or not stringprep . in_table_d1 ( data [ - 1 ] ) ) : raise StringprepError ( "The first and the last character must" " be RandALCat" ) return data
3283	def read ( self , size = None ) : while size is None or len ( self . buffer ) < size : try : self . buffer += next ( self . data_stream ) except StopIteration : break sized_chunk = self . buffer [ : size ] if size is None : self . buffer = "" else : self . buffer = self . buffer [ size : ] return sized_chunk
9416	def to_value ( cls , instance ) : if not isinstance ( instance , OctaveUserClass ) or not instance . _attrs : return dict ( ) dtype = [ ] values = [ ] for attr in instance . _attrs : dtype . append ( ( str ( attr ) , object ) ) values . append ( getattr ( instance , attr ) ) struct = np . array ( [ tuple ( values ) ] , dtype ) return MatlabObject ( struct , instance . _name )
12407	def cons ( collection , value ) : if isinstance ( value , collections . Mapping ) : if collection is None : collection = { } collection . update ( ** value ) elif isinstance ( value , six . string_types ) : if collection is None : collection = [ ] collection . append ( value ) elif isinstance ( value , collections . Iterable ) : if collection is None : collection = [ ] collection . extend ( value ) else : if collection is None : collection = [ ] collection . append ( value ) return collection
4992	def transmit_content_metadata ( username , channel_code , channel_pk ) : start = time . time ( ) api_user = User . objects . get ( username = username ) integrated_channel = INTEGRATED_CHANNEL_CHOICES [ channel_code ] . objects . get ( pk = channel_pk ) LOGGER . info ( 'Transmitting content metadata to integrated channel using configuration: [%s]' , integrated_channel ) try : integrated_channel . transmit_content_metadata ( api_user ) except Exception : LOGGER . exception ( 'Transmission of content metadata failed for user [%s] and for integrated ' 'channel with code [%s] and id [%s].' , username , channel_code , channel_pk ) duration = time . time ( ) - start LOGGER . info ( 'Content metadata transmission task for integrated channel configuration [%s] took [%s] seconds' , integrated_channel , duration )
5692	def update ( self , new_labels , departure_time_backup = None ) : if self . _closed : raise RuntimeError ( "Profile is closed, no updates can be made" ) try : departure_time = next ( iter ( new_labels ) ) . departure_time except StopIteration : departure_time = departure_time_backup self . _check_dep_time_is_valid ( departure_time ) for new_label in new_labels : assert ( new_label . departure_time == departure_time ) dep_time_index = self . dep_times_to_index [ departure_time ] if dep_time_index > 0 : mod_prev_labels = [ label . get_copy_with_specified_departure_time ( departure_time ) for label in self . _label_bags [ dep_time_index - 1 ] ] else : mod_prev_labels = list ( ) mod_prev_labels += self . _label_bags [ dep_time_index ] walk_label = self . _get_label_to_target ( departure_time ) if walk_label : new_labels = new_labels + [ walk_label ] new_frontier = merge_pareto_frontiers ( new_labels , mod_prev_labels ) self . _label_bags [ dep_time_index ] = new_frontier return True
5392	def _task_directory ( self , job_id , task_id , task_attempt ) : dir_name = 'task' if task_id is None else str ( task_id ) if task_attempt : dir_name = '%s.%s' % ( dir_name , task_attempt ) return self . _provider_root ( ) + '/' + job_id + '/' + dir_name
9734	def get_image ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . image_count ) : component_position , image_info = QRTPacket . _get_exact ( RTImage , data , component_position ) append_components ( ( image_info , data [ component_position : - 1 ] ) ) return components
13478	def _sentence_to_interstitial_spacing ( self ) : not_sentence_end_chars = [ ' ' ] abbreviations = [ 'i.e.' , 'e.g.' , ' v.' , ' w.' , ' wh.' ] titles = [ 'Prof.' , 'Mr.' , 'Mrs.' , 'Messrs.' , 'Mmes.' , 'Msgr.' , 'Ms.' , 'Fr.' , 'Rev.' , 'St.' , 'Dr.' , 'Lieut.' , 'Lt.' , 'Capt.' , 'Cptn.' , 'Sgt.' , 'Sjt.' , 'Gen.' , 'Hon.' , 'Cpl.' , 'L-Cpl.' , 'Pvt.' , 'Dvr.' , 'Gnr.' , 'Spr.' , 'Col.' , 'Lt-Col' , 'Lt-Gen.' , 'Mx.' ] for abbrev in abbreviations : for x in not_sentence_end_chars : self . _str_replacement ( abbrev + x , abbrev + '\ ' ) for title in titles : for x in not_sentence_end_chars : self . _str_replacement ( title + x , title + '~' )
8173	def _angle ( self ) : from math import atan , pi , degrees a = degrees ( atan ( self . vy / self . vx ) ) + 360 if self . vx < 0 : a += 180 return a
7429	def draw ( self , axes ) : tre = toytree . tree ( newick = self . results . tree ) tre . draw ( axes = axes , use_edge_lengths = True , tree_style = 'c' , tip_labels_align = True , edge_align_style = { "stroke-width" : 1 } ) for admix in self . results . admixture : pidx , pdist , cidx , cdist , weight = admix a = _get_admix_point ( tre , pidx , pdist ) b = _get_admix_point ( tre , cidx , cdist ) mark = axes . plot ( a = ( a [ 0 ] , b [ 0 ] ) , b = ( a [ 1 ] , b [ 1 ] ) , style = { "stroke-width" : 10 * weight , "stroke-opacity" : 0.95 , "stroke-linecap" : "round" } ) axes . scatterplot ( a = ( b [ 0 ] ) , b = ( b [ 1 ] ) , size = 8 , title = "weight: {}" . format ( weight ) , ) axes . y . show = False axes . x . ticks . show = True axes . x . label . text = "Drift parameter" return axes
8184	def update ( self , iterations = 10 ) : self . alpha += 0.05 self . alpha = min ( self . alpha , 1.0 ) if self . layout . i == 0 : self . layout . prepare ( ) self . layout . i += 1 elif self . layout . i == 1 : self . layout . iterate ( ) elif self . layout . i < self . layout . n : n = min ( iterations , self . layout . i / 10 + 1 ) for i in range ( n ) : self . layout . iterate ( ) min_ , max = self . layout . bounds self . x = _ctx . WIDTH - max . x * self . d - min_ . x * self . d self . y = _ctx . HEIGHT - max . y * self . d - min_ . y * self . d self . x /= 2 self . y /= 2 return not self . layout . done
9523	def scaffolds_to_contigs ( infile , outfile , number_contigs = False ) : seq_reader = sequences . file_reader ( infile ) fout = utils . open_file_write ( outfile ) for seq in seq_reader : contigs = seq . contig_coords ( ) counter = 1 for contig in contigs : if number_contigs : name = seq . id + '.' + str ( counter ) counter += 1 else : name = '.' . join ( [ seq . id , str ( contig . start + 1 ) , str ( contig . end + 1 ) ] ) print ( sequences . Fasta ( name , seq [ contig . start : contig . end + 1 ] ) , file = fout ) utils . close ( fout )
1065	def getfirstmatchingheader ( self , name ) : name = name . lower ( ) + ':' n = len ( name ) lst = [ ] hit = 0 for line in self . headers : if hit : if not line [ : 1 ] . isspace ( ) : break elif line [ : n ] . lower ( ) == name : hit = 1 if hit : lst . append ( line ) return lst
10459	def isEmpty ( cls , datatype = None ) : if not datatype : datatype = AppKit . NSString if not isinstance ( datatype , types . ListType ) : datatype = [ datatype ] pp = pprint . PrettyPrinter ( ) logging . debug ( 'Desired datatypes: %s' % pp . pformat ( datatype ) ) opt_dict = { } logging . debug ( 'Results filter is: %s' % pp . pformat ( opt_dict ) ) try : log_msg = 'Request to verify pasteboard is empty' logging . debug ( log_msg ) pb = AppKit . NSPasteboard . generalPasteboard ( ) its_empty = not bool ( pb . canReadObjectForClasses_options_ ( datatype , opt_dict ) ) except ValueError as error : logging . error ( error ) raise return bool ( its_empty )
3368	def _valid_atoms ( model , expression ) : atoms = expression . atoms ( optlang . interface . Variable ) return all ( a . problem is model . solver for a in atoms )
13231	def get_def_macros ( tex_source ) : r macros = { } for match in DEF_PATTERN . finditer ( tex_source ) : macros [ match . group ( 'name' ) ] = match . group ( 'content' ) return macros
305	def plot_monthly_returns_timeseries ( returns , ax = None , ** kwargs ) : def cumulate_returns ( x ) : return ep . cum_returns ( x ) [ - 1 ] if ax is None : ax = plt . gca ( ) monthly_rets = returns . resample ( 'M' ) . apply ( lambda x : cumulate_returns ( x ) ) monthly_rets = monthly_rets . to_period ( ) sns . barplot ( x = monthly_rets . index , y = monthly_rets . values , color = 'steelblue' ) locs , labels = plt . xticks ( ) plt . setp ( labels , rotation = 90 ) xticks_coord = [ ] xticks_label = [ ] count = 0 for i in monthly_rets . index : if i . month == 1 : xticks_label . append ( i ) xticks_coord . append ( count ) ax . axvline ( count , color = 'gray' , ls = '--' , alpha = 0.3 ) count += 1 ax . axhline ( 0.0 , color = 'darkgray' , ls = '-' ) ax . set_xticks ( xticks_coord ) ax . set_xticklabels ( xticks_label ) return ax
8981	def _set_pixel_and_convert_color ( self , x , y , color ) : if color is None : return color = self . _convert_color_to_rrggbb ( color ) self . _set_pixel ( x , y , color )
5728	def _get_responses_unix ( self , timeout_sec ) : timeout_time_sec = time . time ( ) + timeout_sec responses = [ ] while True : select_timeout = timeout_time_sec - time . time ( ) if select_timeout <= 0 : select_timeout = 0 events , _ , _ = select . select ( self . read_list , [ ] , [ ] , select_timeout ) responses_list = None try : for fileno in events : if fileno == self . stdout_fileno : self . gdb_process . stdout . flush ( ) raw_output = self . gdb_process . stdout . read ( ) stream = "stdout" elif fileno == self . stderr_fileno : self . gdb_process . stderr . flush ( ) raw_output = self . gdb_process . stderr . read ( ) stream = "stderr" else : raise ValueError ( "Developer error. Got unexpected file number %d" % fileno ) responses_list = self . _get_responses_list ( raw_output , stream ) responses += responses_list except IOError : pass if timeout_sec == 0 : break elif responses_list and self . _allow_overwrite_timeout_times : timeout_time_sec = min ( time . time ( ) + self . time_to_check_for_additional_output_sec , timeout_time_sec , ) elif time . time ( ) > timeout_time_sec : break return responses
2776	def remove_droplets ( self , droplet_ids ) : return self . get_data ( "load_balancers/%s/droplets/" % self . id , type = DELETE , params = { "droplet_ids" : droplet_ids } )
13088	def write_config ( self , initialize_indices = False ) : if not os . path . exists ( self . config_dir ) : os . mkdir ( self . config_dir ) with open ( self . config_file , 'w' ) as configfile : self . config . write ( configfile ) if initialize_indices : index = self . get ( 'jackal' , 'index' ) from jackal import Host , Range , Service , User , Credential , Log from jackal . core import create_connection create_connection ( self ) Host . init ( index = "{}-hosts" . format ( index ) ) Range . init ( index = "{}-ranges" . format ( index ) ) Service . init ( index = "{}-services" . format ( index ) ) User . init ( index = "{}-users" . format ( index ) ) Credential . init ( index = "{}-creds" . format ( index ) ) Log . init ( index = "{}-log" . format ( index ) )
2049	def create_contract ( self , price = 0 , address = None , caller = None , balance = 0 , init = None , gas = None ) : expected_address = self . create_account ( self . new_address ( sender = caller ) ) if address is None : address = expected_address elif caller is not None and address != expected_address : raise EthereumError ( f"Error: contract created from address {hex(caller)} with nonce {self.get_nonce(caller)} was expected to be at address {hex(expected_address)}, but create_contract was called with address={hex(address)}" ) self . start_transaction ( 'CREATE' , address , price , init , caller , balance , gas = gas ) self . _process_pending_transaction ( ) return address
6465	def size ( self ) : for fd in range ( 3 ) : cr = self . _ioctl_GWINSZ ( fd ) if cr : break if not cr : try : fd = os . open ( os . ctermid ( ) , os . O_RDONLY ) cr = self . _ioctl_GWINSZ ( fd ) os . close ( fd ) except Exception : pass if not cr : env = os . environ cr = ( env . get ( 'LINES' , 25 ) , env . get ( 'COLUMNS' , 80 ) ) return int ( cr [ 1 ] ) , int ( cr [ 0 ] )
7846	def add_item ( self , jid , node = None , name = None , action = None ) : return DiscoItem ( self , jid , node , name , action )
7492	def random_product ( iter1 , iter2 ) : pool1 = tuple ( iter1 ) pool2 = tuple ( iter2 ) ind1 = random . sample ( pool1 , 2 ) ind2 = random . sample ( pool2 , 2 ) return tuple ( ind1 + ind2 )
759	def appendInputWithSimilarValues ( inputs ) : numInputs = len ( inputs ) for i in xrange ( numInputs ) : input = inputs [ i ] for j in xrange ( len ( input ) - 1 ) : if input [ j ] == 1 and input [ j + 1 ] == 0 : newInput = copy . deepcopy ( input ) newInput [ j ] = 0 newInput [ j + 1 ] = 1 inputs . append ( newInput ) break
10950	def sample ( field , inds = None , slicer = None , flat = True ) : if inds is not None : out = field . ravel ( ) [ inds ] elif slicer is not None : out = field [ slicer ] . ravel ( ) else : out = field if flat : return out . ravel ( ) return out
8607	def add_group_user ( self , group_id , user_id ) : data = { "id" : user_id } response = self . _perform_request ( url = '/um/groups/%s/users' % group_id , method = 'POST' , data = json . dumps ( data ) ) return response
3025	def _save_private_file ( filename , json_contents ) : temp_filename = tempfile . mktemp ( ) file_desc = os . open ( temp_filename , os . O_WRONLY | os . O_CREAT , 0o600 ) with os . fdopen ( file_desc , 'w' ) as file_handle : json . dump ( json_contents , file_handle , sort_keys = True , indent = 2 , separators = ( ',' , ': ' ) ) shutil . move ( temp_filename , filename )
9439	def heartbeat ( ) : print "We got a call heartbeat notification\n" if request . method == 'POST' : print request . form else : print request . args return "OK"
1094	def findall ( pattern , string , flags = 0 ) : return _compile ( pattern , flags ) . findall ( string ) def finditer ( pattern , string , flags = 0 ) : return _compile ( pattern , flags ) . finditer ( string )
1384	def trigger_watches ( self ) : to_remove = [ ] for uid , callback in self . watches . items ( ) : try : callback ( self ) except Exception as e : Log . error ( "Caught exception while triggering callback: " + str ( e ) ) Log . debug ( traceback . format_exc ( ) ) to_remove . append ( uid ) for uid in to_remove : self . unregister_watch ( uid )
8987	def last_produced_mesh ( self ) : for instruction in reversed ( self . instructions ) : if instruction . produces_meshes ( ) : return instruction . last_produced_mesh raise IndexError ( "{} produces no meshes" . format ( self ) )
13497	def clone ( self ) : t = Tag ( self . version . major , self . version . minor , self . version . patch ) if self . revision is not None : t . revision = self . revision . clone ( ) return t
4233	def autodetect_url ( ) : for url in [ "http://routerlogin.net:5000" , "https://routerlogin.net" , "http://routerlogin.net" ] : try : r = requests . get ( url + "/soap/server_sa/" , headers = _get_soap_headers ( "Test:1" , "test" ) , verify = False ) if r . status_code == 200 : return url except requests . exceptions . RequestException : pass return None
10691	def rgb_to_hex ( rgb ) : r , g , b = rgb return "#{0}{1}{2}" . format ( hex ( int ( r ) ) [ 2 : ] . zfill ( 2 ) , hex ( int ( g ) ) [ 2 : ] . zfill ( 2 ) , hex ( int ( b ) ) [ 2 : ] . zfill ( 2 ) )
3451	def find_blocked_reactions ( model , reaction_list = None , zero_cutoff = None , open_exchanges = False , processes = None ) : zero_cutoff = normalize_cutoff ( model , zero_cutoff ) with model : if open_exchanges : for reaction in model . exchanges : reaction . bounds = ( min ( reaction . lower_bound , - 1000 ) , max ( reaction . upper_bound , 1000 ) ) if reaction_list is None : reaction_list = model . reactions model . slim_optimize ( ) solution = get_solution ( model , reactions = reaction_list ) reaction_list = solution . fluxes [ solution . fluxes . abs ( ) < zero_cutoff ] . index . tolist ( ) flux_span = flux_variability_analysis ( model , fraction_of_optimum = 0. , reaction_list = reaction_list , processes = processes ) return flux_span [ flux_span . abs ( ) . max ( axis = 1 ) < zero_cutoff ] . index . tolist ( )
5072	def get_configuration_value ( val_name , default = None , ** kwargs ) : if kwargs . get ( 'type' ) == 'url' : return get_url ( val_name ) or default if callable ( get_url ) else default return configuration_helpers . get_value ( val_name , default , ** kwargs ) if configuration_helpers else default
7947	def set_target ( self , stream ) : with self . lock : if self . _stream : raise ValueError ( "Target stream already set" ) self . _stream = stream self . _reader = StreamReader ( stream )
13801	def revoke_token ( self , token , callback ) : yield Task ( self . data_store . remove , 'tokens' , token = token ) callback ( )
8586	def get_attached_cdrom ( self , datacenter_id , server_id , cdrom_id ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/cdroms/%s' % ( datacenter_id , server_id , cdrom_id ) ) return response
11165	def ctime ( self ) : try : return self . _stat . st_ctime except : self . _stat = self . stat ( ) return self . ctime
3679	def economic_status ( self ) : r if self . __economic_status : return self . __economic_status else : self . __economic_status = economic_status ( self . CAS , Method = 'Combined' ) return self . __economic_status
5361	def __execute_initial_load ( self ) : if self . conf [ 'phases' ] [ 'panels' ] : tasks_cls = [ TaskPanels , TaskPanelsMenu ] self . execute_tasks ( tasks_cls ) if self . conf [ 'phases' ] [ 'identities' ] : tasks_cls = [ TaskInitSortingHat ] self . execute_tasks ( tasks_cls ) logger . info ( "Loading projects" ) tasks_cls = [ TaskProjects ] self . execute_tasks ( tasks_cls ) logger . info ( "Done" ) return
11115	def save ( self ) : repoInfoPath = os . path . join ( self . __path , ".pyrepinfo" ) try : fdinfo = open ( repoInfoPath , 'wb' ) except Exception as e : raise Exception ( "unable to open repository info for saving (%s)" % e ) try : pickle . dump ( self , fdinfo , protocol = 2 ) except Exception as e : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) raise Exception ( "Unable to save repository info (%s)" % e ) finally : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) repoTimePath = os . path . join ( self . __path , ".pyrepstate" ) try : self . __state = ( "%.6f" % time . time ( ) ) . encode ( ) with open ( repoTimePath , 'wb' ) as fdtime : fdtime . write ( self . __state ) fdtime . flush ( ) os . fsync ( fdtime . fileno ( ) ) except Exception as e : raise Exception ( "unable to open repository time stamp for saving (%s)" % e )
8744	def get_floatingip ( context , id , fields = None ) : LOG . info ( 'get_floatingip %s for tenant %s' % ( id , context . tenant_id ) ) filters = { 'address_type' : ip_types . FLOATING , '_deallocated' : False } floating_ip = db_api . floating_ip_find ( context , id = id , scope = db_api . ONE , ** filters ) if not floating_ip : raise q_exc . FloatingIpNotFound ( id = id ) return v . _make_floating_ip_dict ( floating_ip )
7470	def fill_dups_arr ( data ) : duplefiles = glob . glob ( os . path . join ( data . tmpdir , "duples_*.tmp.npy" ) ) duplefiles . sort ( key = lambda x : int ( x . rsplit ( "_" , 1 ) [ - 1 ] [ : - 8 ] ) ) io5 = h5py . File ( data . clust_database , 'r+' ) dfilter = io5 [ "duplicates" ] init = 0 for dupf in duplefiles : end = int ( dupf . rsplit ( "_" , 1 ) [ - 1 ] [ : - 8 ] ) inarr = np . load ( dupf ) dfilter [ init : end ] = inarr init += end - init LOGGER . info ( "all duplicates: %s" , dfilter [ : ] . sum ( ) ) io5 . close ( )
13798	def handle ( self ) : while True : try : line = self . rfile . readline ( ) try : cmd = json . loads ( line ) except Exception , exc : self . wfile . write ( repr ( exc ) + NEWLINE ) continue else : handler = getattr ( self , 'handle_' + cmd [ 0 ] , None ) if not handler : self . wfile . write ( repr ( CommandNotFound ( cmd [ 0 ] ) ) + NEWLINE ) continue return_value = handler ( * cmd [ 1 : ] ) if not return_value : continue self . wfile . write ( one_lineify ( json . dumps ( return_value ) ) + NEWLINE ) except Exception , exc : self . wfile . write ( repr ( exc ) + NEWLINE ) continue
8903	def add_syncable_models ( ) : import django . apps from morango . models import SyncableModel from morango . manager import SyncableModelManager from morango . query import SyncableModelQuerySet model_list = [ ] for model_class in django . apps . apps . get_models ( ) : if issubclass ( model_class , SyncableModel ) : name = model_class . __name__ if _multiple_self_ref_fk_check ( model_class ) : raise InvalidMorangoModelConfiguration ( "Syncing models with more than 1 self referential ForeignKey is not supported." ) try : from mptt import models from morango . utils . morango_mptt import MorangoMPTTModel , MorangoMPTTTreeManager , MorangoTreeQuerySet if issubclass ( model_class , models . MPTTModel ) : if not issubclass ( model_class , MorangoMPTTModel ) : raise InvalidMorangoModelConfiguration ( "{} that inherits from MPTTModel, should instead inherit from MorangoMPTTModel." . format ( name ) ) if not isinstance ( model_class . objects , MorangoMPTTTreeManager ) : raise InvalidMPTTManager ( "Manager for {} must inherit from MorangoMPTTTreeManager." . format ( name ) ) if not isinstance ( model_class . objects . none ( ) , MorangoTreeQuerySet ) : raise InvalidMPTTQuerySet ( "Queryset for {} model must inherit from MorangoTreeQuerySet." . format ( name ) ) except ImportError : pass if not isinstance ( model_class . objects , SyncableModelManager ) : raise InvalidSyncableManager ( "Manager for {} must inherit from SyncableModelManager." . format ( name ) ) if not isinstance ( model_class . objects . none ( ) , SyncableModelQuerySet ) : raise InvalidSyncableQueryset ( "Queryset for {} model must inherit from SyncableModelQuerySet." . format ( name ) ) if model_class . _meta . many_to_many : raise UnsupportedFieldType ( "{} model with a ManyToManyField is not supported in morango." ) if not hasattr ( model_class , 'morango_model_name' ) : raise InvalidMorangoModelConfiguration ( "{} model must define a morango_model_name attribute" . format ( name ) ) if not hasattr ( model_class , 'morango_profile' ) : raise InvalidMorangoModelConfiguration ( "{} model must define a morango_profile attribute" . format ( name ) ) profile = model_class . morango_profile _profile_models [ profile ] = _profile_models . get ( profile , [ ] ) if model_class . morango_model_name is not None : _insert_model_into_profile_dict ( model_class , profile ) for profile , model_list in iteritems ( _profile_models ) : syncable_models_dict = OrderedDict ( ) for model_class in model_list : syncable_models_dict [ model_class . morango_model_name ] = model_class _profile_models [ profile ] = syncable_models_dict
9556	def _apply_record_checks ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for check , modulus in self . _record_checks : if i % modulus == 0 : rdict = self . _as_dict ( r ) try : check ( rdict ) except RecordError as e : code = e . code if e . code is not None else RECORD_CHECK_FAILED p = { 'code' : code } if not summarize : message = e . message if e . message is not None else MESSAGES [ RECORD_CHECK_FAILED ] p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context if e . details is not None : p [ 'details' ] = e . details yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( check . __name__ , check . __doc__ ) if context is not None : p [ 'context' ] = context yield p
1822	def SETPO ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . PF == False , 1 , 0 ) )
12584	def spatialimg_to_hdfgroup ( h5group , spatial_img ) : try : h5group [ 'data' ] = spatial_img . get_data ( ) h5group [ 'affine' ] = spatial_img . get_affine ( ) if hasattr ( h5group , 'get_extra' ) : h5group [ 'extra' ] = spatial_img . get_extra ( ) hdr = spatial_img . get_header ( ) for k in list ( hdr . keys ( ) ) : h5group [ 'data' ] . attrs [ k ] = hdr [ k ] except ValueError as ve : raise Exception ( 'Error creating group ' + h5group . name ) from ve
6348	def _apply_final_rules ( self , phonetic , final_rules , language_arg , strip ) : if not final_rules : return phonetic phonetic = self . _expand_alternates ( phonetic ) phonetic_array = phonetic . split ( '|' ) for k in range ( len ( phonetic_array ) ) : phonetic = phonetic_array [ k ] phonetic2 = '' phoneticx = self . _normalize_lang_attrs ( phonetic , True ) i = 0 while i < len ( phonetic ) : found = False if phonetic [ i ] == '[' : attrib_start = i i += 1 while True : if phonetic [ i ] == ']' : i += 1 phonetic2 += phonetic [ attrib_start : i ] break i += 1 continue for rule in final_rules : pattern = rule [ _PATTERN_POS ] pattern_length = len ( pattern ) lcontext = rule [ _LCONTEXT_POS ] rcontext = rule [ _RCONTEXT_POS ] right = '^' + rcontext left = lcontext + '$' if ( pattern_length > len ( phoneticx ) - i ) or phoneticx [ i : i + pattern_length ] != pattern : continue if rcontext != '' : if not search ( right , phoneticx [ i + pattern_length : ] ) : continue if lcontext != '' : if not search ( left , phoneticx [ : i ] ) : continue candidate = self . _apply_rule_if_compat ( phonetic2 , rule [ _PHONETIC_POS ] , language_arg ) if candidate is not None : phonetic2 = candidate found = True break if not found : phonetic2 += phonetic [ i ] pattern_length = 1 i += pattern_length phonetic_array [ k ] = self . _expand_alternates ( phonetic2 ) phonetic = '|' . join ( phonetic_array ) if strip : phonetic = self . _normalize_lang_attrs ( phonetic , True ) if '|' in phonetic : phonetic = '(' + self . _remove_dupes ( phonetic ) + ')' return phonetic
6422	def _synoname_strip_punct ( self , word ) : stripped = '' for char in word : if char not in set ( ',-./:;"&\'()!{|}?$%*+<=>[\\]^_`~' ) : stripped += char return stripped . strip ( )
12854	def parse ( filename ) : for event , elt in et . iterparse ( filename , events = ( 'start' , 'end' , 'comment' , 'pi' ) , huge_tree = True ) : if event == 'start' : obj = _elt2obj ( elt ) obj [ 'type' ] = ENTER yield obj if elt . text : yield { 'type' : TEXT , 'text' : elt . text } elif event == 'end' : yield { 'type' : EXIT } if elt . tail : yield { 'type' : TEXT , 'text' : elt . tail } elt . clear ( ) elif event == 'comment' : yield { 'type' : COMMENT , 'text' : elt . text } elif event == 'pi' : yield { 'type' : PI , 'text' : elt . text } else : assert False , ( event , elt )
3473	def build_reaction_string ( self , use_metabolite_names = False ) : def format ( number ) : return "" if number == 1 else str ( number ) . rstrip ( "." ) + " " id_type = 'id' if use_metabolite_names : id_type = 'name' reactant_bits = [ ] product_bits = [ ] for met in sorted ( self . _metabolites , key = attrgetter ( "id" ) ) : coefficient = self . _metabolites [ met ] name = str ( getattr ( met , id_type ) ) if coefficient >= 0 : product_bits . append ( format ( coefficient ) + name ) else : reactant_bits . append ( format ( abs ( coefficient ) ) + name ) reaction_string = ' + ' . join ( reactant_bits ) if not self . reversibility : if self . lower_bound < 0 and self . upper_bound <= 0 : reaction_string += ' <-- ' else : reaction_string += ' else : reaction_string += ' <=> ' reaction_string += ' + ' . join ( product_bits ) return reaction_string
3179	def get ( self , batch_webhook_id , ** queryparams ) : self . batch_webhook_id = batch_webhook_id return self . _mc_client . _get ( url = self . _build_path ( batch_webhook_id ) , ** queryparams )
6297	def release ( self , buffer = True ) : for key , vao in self . vaos : vao . release ( ) if buffer : for buff in self . buffers : buff . buffer . release ( ) if self . _index_buffer : self . _index_buffer . release ( )
5356	def set_param ( self , section , param , value ) : if section not in self . conf or param not in self . conf [ section ] : logger . error ( 'Config section %s and param %s not exists' , section , param ) else : self . conf [ section ] [ param ] = value
1425	def initialize ( self , config , context ) : self . logger . info ( "Initializing PulsarSpout with the following" ) self . logger . info ( "Component-specific config: \n%s" % str ( config ) ) self . logger . info ( "Context: \n%s" % str ( context ) ) self . emit_count = 0 self . ack_count = 0 self . fail_count = 0 if not PulsarSpout . serviceUrl in config or not PulsarSpout . topicName in config : self . logger . fatal ( "Need to specify both serviceUrl and topicName" ) self . pulsar_cluster = str ( config [ PulsarSpout . serviceUrl ] ) self . topic = str ( config [ PulsarSpout . topicName ] ) mode = config [ api_constants . TOPOLOGY_RELIABILITY_MODE ] if mode == api_constants . TopologyReliabilityMode . ATLEAST_ONCE : self . acking_timeout = 1000 * int ( config [ api_constants . TOPOLOGY_MESSAGE_TIMEOUT_SECS ] ) else : self . acking_timeout = 30000 if PulsarSpout . receiveTimeoutMs in config : self . receive_timeout_ms = config [ PulsarSpout . receiveTimeoutMs ] else : self . receive_timeout_ms = 10 if PulsarSpout . deserializer in config : self . deserializer = config [ PulsarSpout . deserializer ] if not callable ( self . deserializer ) : self . logger . fatal ( "Pulsar Message Deserializer needs to be callable" ) else : self . deserializer = self . default_deserializer self . logConfFileName = GenerateLogConfig ( context ) self . logger . info ( "Generated LogConf at %s" % self . logConfFileName ) self . client = pulsar . Client ( self . pulsar_cluster , log_conf_file_path = self . logConfFileName ) self . logger . info ( "Setup Client with cluster %s" % self . pulsar_cluster ) try : self . consumer = self . client . subscribe ( self . topic , context . get_topology_name ( ) , consumer_type = pulsar . ConsumerType . Failover , unacked_messages_timeout_ms = self . acking_timeout ) except Exception as e : self . logger . fatal ( "Pulsar client subscription failed: %s" % str ( e ) ) self . logger . info ( "Subscribed to topic %s" % self . topic )
11738	def attach_bp ( self , bp , description = '' ) : if not isinstance ( bp , Blueprint ) : raise InvalidBlueprint ( 'Blueprints attached to the bundle must be of type {0}' . format ( Blueprint ) ) self . blueprints . append ( ( bp , description ) )
11393	def relative_to_full ( url , example_url ) : if re . match ( 'https?:\/\/' , url ) : return url domain = get_domain ( example_url ) if domain : return '%s%s' % ( domain , url ) return url
742	def readFromFile ( cls , f , packed = True ) : schema = cls . getSchema ( ) if packed : proto = schema . read_packed ( f ) else : proto = schema . read ( f ) return cls . read ( proto )
13241	def daily_periods ( self , range_start = datetime . date . min , range_end = datetime . date . max , exclude_dates = tuple ( ) ) : tz = self . timezone period = self . period weekdays = self . weekdays current_date = max ( range_start , self . start_date ) end_date = range_end if self . end_date : end_date = min ( end_date , self . end_date ) while current_date <= end_date : if current_date . weekday ( ) in weekdays and current_date not in exclude_dates : yield Period ( tz . localize ( datetime . datetime . combine ( current_date , period . start ) ) , tz . localize ( datetime . datetime . combine ( current_date , period . end ) ) ) current_date += datetime . timedelta ( days = 1 )
1488	def save_module ( self , obj ) : self . modules . add ( obj ) self . save_reduce ( subimport , ( obj . __name__ , ) , obj = obj )
4020	def _init_docker_vm ( ) : if not _dusty_vm_exists ( ) : log_to_client ( 'Initializing new Dusty VM with Docker Machine' ) machine_options = [ '--driver' , 'virtualbox' , '--virtualbox-cpu-count' , '-1' , '--virtualbox-boot2docker-url' , constants . CONFIG_BOOT2DOCKER_URL , '--virtualbox-memory' , str ( get_config_value ( constants . CONFIG_VM_MEM_SIZE ) ) , '--virtualbox-hostonly-nictype' , constants . VM_NIC_TYPE ] check_call_demoted ( [ 'docker-machine' , 'create' ] + machine_options + [ constants . VM_MACHINE_NAME ] , redirect_stderr = True )
300	def plot_slippage_sensitivity ( returns , positions , transactions , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) avg_returns_given_slippage = pd . Series ( ) for bps in range ( 1 , 100 ) : adj_returns = txn . adjust_returns_for_slippage ( returns , positions , transactions , bps ) avg_returns = ep . annual_return ( adj_returns ) avg_returns_given_slippage . loc [ bps ] = avg_returns avg_returns_given_slippage . plot ( alpha = 1.0 , lw = 2 , ax = ax ) ax . set_title ( 'Average annual returns given additional per-dollar slippage' ) ax . set_xticks ( np . arange ( 0 , 100 , 10 ) ) ax . set_ylabel ( 'Average annual return' ) ax . set_xlabel ( 'Per-dollar slippage (bps)' ) return ax
10789	def guess_invert ( st ) : pos = st . obj_get_positions ( ) pxinds_ar = np . round ( pos ) . astype ( 'int' ) inim = st . ishape . translate ( - st . pad ) . contains ( pxinds_ar ) pxinds_tuple = tuple ( pxinds_ar [ inim ] . T ) pxvals = st . data [ pxinds_tuple ] invert = np . median ( pxvals ) < np . median ( st . data ) return invert
11439	def _get_children_by_tag_name ( node , name ) : try : return [ child for child in node . childNodes if child . nodeName == name ] except TypeError : return [ ]
3674	def draw_3d ( self , width = 300 , height = 500 , style = 'stick' , Hs = True ) : r try : import py3Dmol from IPython . display import display if Hs : mol = self . rdkitmol_Hs else : mol = self . rdkitmol AllChem . EmbedMultipleConfs ( mol ) mb = Chem . MolToMolBlock ( mol ) p = py3Dmol . view ( width = width , height = height ) p . addModel ( mb , 'sdf' ) p . setStyle ( { style : { } } ) p . zoomTo ( ) display ( p . show ( ) ) except : return 'py3Dmol, RDKit, and IPython are required for this feature.'
3330	def acquire_write ( self , timeout = None ) : if timeout is not None : endtime = time ( ) + timeout me , upgradewriter = currentThread ( ) , False self . __condition . acquire ( ) try : if self . __writer is me : self . __writercount += 1 return elif me in self . __readers : if self . __upgradewritercount : raise ValueError ( "Inevitable dead lock, denying write lock" ) upgradewriter = True self . __upgradewritercount = self . __readers . pop ( me ) else : self . __pendingwriters . append ( me ) while True : if not self . __readers and self . __writer is None : if self . __upgradewritercount : if upgradewriter : self . __writer = me self . __writercount = self . __upgradewritercount + 1 self . __upgradewritercount = 0 return elif self . __pendingwriters [ 0 ] is me : self . __writer = me self . __writercount = 1 self . __pendingwriters = self . __pendingwriters [ 1 : ] return if timeout is not None : remaining = endtime - time ( ) if remaining <= 0 : if upgradewriter : self . __readers [ me ] = self . __upgradewritercount self . __upgradewritercount = 0 else : self . __pendingwriters . remove ( me ) raise RuntimeError ( "Acquiring write lock timed out" ) self . __condition . wait ( remaining ) else : self . __condition . wait ( ) finally : self . __condition . release ( )
2085	def format_options ( self , ctx , formatter ) : field_opts = [ ] global_opts = [ ] local_opts = [ ] other_opts = [ ] for param in self . params : if param . name in SETTINGS_PARMS : opts = global_opts elif getattr ( param , 'help' , None ) and param . help . startswith ( '[FIELD]' ) : opts = field_opts param . help = param . help [ len ( '[FIELD]' ) : ] else : opts = local_opts rv = param . get_help_record ( ctx ) if rv is None : continue else : opts . append ( rv ) if self . add_help_option : help_options = self . get_help_option_names ( ctx ) if help_options : other_opts . append ( [ join_options ( help_options ) [ 0 ] , 'Show this message and exit.' ] ) if field_opts : with formatter . section ( 'Field Options' ) : formatter . write_dl ( field_opts ) if local_opts : with formatter . section ( 'Local Options' ) : formatter . write_dl ( local_opts ) if global_opts : with formatter . section ( 'Global Options' ) : formatter . write_dl ( global_opts ) if other_opts : with formatter . section ( 'Other Options' ) : formatter . write_dl ( other_opts )
13897	def DumpDirHashToStringIO ( directory , stringio , base = '' , exclude = None , include = None ) : import fnmatch import os files = [ ( os . path . join ( directory , i ) , i ) for i in os . listdir ( directory ) ] files = [ i for i in files if os . path . isfile ( i [ 0 ] ) ] for fullname , filename in files : if include is not None : if not fnmatch . fnmatch ( fullname , include ) : continue if exclude is not None : if fnmatch . fnmatch ( fullname , exclude ) : continue md5 = Md5Hex ( fullname ) if base : stringio . write ( '%s/%s=%s\n' % ( base , filename , md5 ) ) else : stringio . write ( '%s=%s\n' % ( filename , md5 ) )
429	def read_image ( image , path = '' ) : return imageio . imread ( os . path . join ( path , image ) )
1584	def yaml_config_reader ( config_path ) : if not config_path . endswith ( ".yaml" ) : raise ValueError ( "Config file not yaml" ) with open ( config_path , 'r' ) as f : config = yaml . load ( f ) return config
8075	def rect ( self , x , y , width , height , roundness = 0.0 , draw = True , ** kwargs ) : path = self . BezierPath ( ** kwargs ) path . rect ( x , y , width , height , roundness , self . rectmode ) if draw : path . draw ( ) return path
9906	def parse ( self , ping_message ) : try : if typepy . is_not_null_string ( ping_message . stdout ) : ping_message = ping_message . stdout except AttributeError : pass logger . debug ( "parsing ping result: {}" . format ( ping_message ) ) self . __parser = NullPingParser ( ) if typepy . is_null_string ( ping_message ) : logger . debug ( "ping_message is empty" ) self . __stats = PingStats ( ) return self . __stats ping_lines = _to_unicode ( ping_message ) . splitlines ( ) parser_class_list = ( LinuxPingParser , WindowsPingParser , MacOsPingParser , AlpineLinuxPingParser , ) for parser_class in parser_class_list : self . __parser = parser_class ( ) try : self . __stats = self . __parser . parse ( ping_lines ) return self . __stats except ParseError as e : if e . reason != ParseErrorReason . HEADER_NOT_FOUND : raise e except pp . ParseException : pass self . __parser = NullPingParser ( ) return self . __stats
3275	def handle_delete ( self ) : if "/by_tag/" not in self . path : raise DAVError ( HTTP_FORBIDDEN ) catType , tag , _rest = util . save_split ( self . path . strip ( "/" ) , "/" , 2 ) assert catType == "by_tag" assert tag in self . data [ "tags" ] self . data [ "tags" ] . remove ( tag ) return True
4499	def _json ( self , response , status_code ) : if isinstance ( status_code , numbers . Integral ) : status_code = ( status_code , ) if response . status_code in status_code : return response . json ( ) else : raise RuntimeError ( "Response has status " "code {} not {}" . format ( response . status_code , status_code ) )
3244	def get_security_group ( security_group , flags = FLAGS . ALL , ** kwargs ) : result = registry . build_out ( flags , start_with = security_group , pass_datastructure = True , ** kwargs ) result . pop ( 'security_group_rules' , [ ] ) return result
8617	def _underscore_to_camelcase ( value ) : def camelcase ( ) : yield str . lower while True : yield str . capitalize c = camelcase ( ) return "" . join ( next ( c ) ( x ) if x else '_' for x in value . split ( "_" ) )
4088	def asyncSlot ( * args ) : def outer_decorator ( fn ) : @ Slot ( * args ) @ functools . wraps ( fn ) def wrapper ( * args , ** kwargs ) : asyncio . ensure_future ( fn ( * args , ** kwargs ) ) return wrapper return outer_decorator
11791	def mrv ( assignment , csp ) : "Minimum-remaining-values heuristic." return argmin_random_tie ( [ v for v in csp . vars if v not in assignment ] , lambda var : num_legal_values ( csp , var , assignment ) )
11741	def _compute_first ( self ) : for terminal in self . terminals : self . _first [ terminal ] . add ( terminal ) self . _first [ END_OF_INPUT ] . add ( END_OF_INPUT ) while True : changed = False for nonterminal , productions in self . nonterminals . items ( ) : for production in productions : new_first = self . first ( production . rhs ) if new_first - self . _first [ nonterminal ] : self . _first [ nonterminal ] |= new_first changed = True if not changed : break
10052	def get ( self , pid , record ) : return self . make_response ( obj = record . files , pid = pid , record = record )
10097	def create_new_version ( self , name , subject , text = '' , template_id = None , html = None , locale = None , timeout = None ) : if ( html ) : payload = { 'name' : name , 'subject' : subject , 'html' : html , 'text' : text } else : payload = { 'name' : name , 'subject' : subject , 'text' : text } if locale : url = self . TEMPLATES_SPECIFIC_LOCALE_VERSIONS_ENDPOINT % ( template_id , locale ) else : url = self . TEMPLATES_NEW_VERSION_ENDPOINT % template_id return self . _api_request ( url , self . HTTP_POST , payload = payload , timeout = timeout )
11911	def get_version ( filename , pattern ) : with open ( filename ) as f : match = re . search ( r"^(\s*%s\s*=\s*')(.+?)(')(?sm)" % pattern , f . read ( ) ) if match : before , version , after = match . groups ( ) return version fail ( 'Could not find {} in {}' . format ( pattern , filename ) )
8442	def _code_search ( query , github_user = None ) : github_client = temple . utils . GithubClient ( ) headers = { 'Accept' : 'application/vnd.github.v3.text-match+json' } resp = github_client . get ( '/search/code' , params = { 'q' : query , 'per_page' : 100 } , headers = headers ) if resp . status_code == requests . codes . unprocessable_entity and github_user : raise temple . exceptions . InvalidGithubUserError ( 'Invalid Github user or org - "{}"' . format ( github_user ) ) resp . raise_for_status ( ) resp_data = resp . json ( ) repositories = collections . defaultdict ( dict ) while True : repositories . update ( { 'git@github.com:{}.git' . format ( repo [ 'repository' ] [ 'full_name' ] ) : repo [ 'repository' ] for repo in resp_data [ 'items' ] } ) next_url = _parse_link_header ( resp . headers ) . get ( 'next' ) if next_url : resp = requests . get ( next_url , headers = headers ) resp . raise_for_status ( ) resp_data = resp . json ( ) else : break return repositories
8056	def do_escape_nl ( self , arg ) : if arg . lower ( ) == 'off' : self . escape_nl = False else : self . escape_nl = True
2494	def package_verif_node ( self , package ) : verif_node = BNode ( ) type_triple = ( verif_node , RDF . type , self . spdx_namespace . PackageVerificationCode ) self . graph . add ( type_triple ) value_triple = ( verif_node , self . spdx_namespace . packageVerificationCodeValue , Literal ( package . verif_code ) ) self . graph . add ( value_triple ) excl_file_nodes = map ( lambda excl : Literal ( excl ) , package . verif_exc_files ) excl_predicate = self . spdx_namespace . packageVerificationCodeExcludedFile excl_file_triples = [ ( verif_node , excl_predicate , xcl_file ) for xcl_file in excl_file_nodes ] for trp in excl_file_triples : self . graph . add ( trp ) return verif_node
12698	def _parse_control_fields ( self , fields , tag_id = "tag" ) : for field in fields : params = field . params if tag_id not in params : continue self . controlfields [ params [ tag_id ] ] = field . getContent ( ) . strip ( )
8036	def lexrank ( sentences , continuous = False , sim_threshold = 0.1 , alpha = 0.9 , use_divrank = False , divrank_alpha = 0.25 ) : ranker_params = { 'max_iter' : 1000 } if use_divrank : ranker = divrank_scipy ranker_params [ 'alpha' ] = divrank_alpha ranker_params [ 'd' ] = alpha else : ranker = networkx . pagerank_scipy ranker_params [ 'alpha' ] = alpha graph = networkx . DiGraph ( ) sent_tf_list = [ ] for sent in sentences : words = tools . word_segmenter_ja ( sent ) tf = collections . Counter ( words ) sent_tf_list . append ( tf ) sent_vectorizer = DictVectorizer ( sparse = True ) sent_vecs = sent_vectorizer . fit_transform ( sent_tf_list ) sim_mat = 1 - pairwise_distances ( sent_vecs , sent_vecs , metric = 'cosine' ) if continuous : linked_rows , linked_cols = numpy . where ( sim_mat > 0 ) else : linked_rows , linked_cols = numpy . where ( sim_mat >= sim_threshold ) graph . add_nodes_from ( range ( sent_vecs . shape [ 0 ] ) ) for i , j in zip ( linked_rows , linked_cols ) : if i == j : continue weight = sim_mat [ i , j ] if continuous else 1.0 graph . add_edge ( i , j , { 'weight' : weight } ) scores = ranker ( graph , ** ranker_params ) return scores , sim_mat
8113	def age ( self , id ) : path = self . hash ( id ) if os . path . exists ( path ) : modified = datetime . datetime . fromtimestamp ( os . stat ( path ) [ 8 ] ) age = datetime . datetime . today ( ) - modified return age . days else : return 0
13075	def create_blueprint ( self ) : self . register_plugins ( ) self . blueprint = Blueprint ( self . name , "nemo" , url_prefix = self . prefix , template_folder = self . template_folder , static_folder = self . static_folder , static_url_path = self . static_url_path ) for url , name , methods , instance in self . _urls : self . blueprint . add_url_rule ( url , view_func = self . view_maker ( name , instance ) , endpoint = _plugin_endpoint_rename ( name , instance ) , methods = methods ) for url , name , methods , instance in self . _semantic_url : self . blueprint . add_url_rule ( url , view_func = self . view_maker ( name , instance ) , endpoint = _plugin_endpoint_rename ( name , instance ) + "_semantic" , methods = methods ) self . register_assets ( ) self . register_filters ( ) self . __templates_namespaces__ . extend ( self . __instance_templates__ ) for namespace , directory in self . __templates_namespaces__ [ : : - 1 ] : if namespace not in self . __template_loader__ : self . __template_loader__ [ namespace ] = [ ] self . __template_loader__ [ namespace ] . append ( jinja2 . FileSystemLoader ( op . abspath ( directory ) ) ) self . blueprint . jinja_loader = jinja2 . PrefixLoader ( { namespace : jinja2 . ChoiceLoader ( paths ) for namespace , paths in self . __template_loader__ . items ( ) } , "::" ) if self . cache is not None : for func , instance in self . cached : setattr ( instance , func . __name__ , self . cache . memoize ( ) ( func ) ) return self . blueprint
6072	def luminosity_within_ellipse_in_units ( self , major_axis : dim . Length , unit_luminosity = 'eps' , kpc_per_arcsec = None , exposure_time = None ) : if self . has_light_profile : return sum ( map ( lambda p : p . luminosity_within_ellipse_in_units ( major_axis = major_axis , unit_luminosity = unit_luminosity , kpc_per_arcsec = kpc_per_arcsec , exposure_time = exposure_time ) , self . light_profiles ) ) else : return None
11765	def k_in_row ( self , board , move , player , ( delta_x , delta_y ) ) : "Return true if there is a line through move on board for player." x , y = move n = 0 while board . get ( ( x , y ) ) == player : n += 1 x , y = x + delta_x , y + delta_y x , y = move while board . get ( ( x , y ) ) == player : n += 1 x , y = x - delta_x , y - delta_y n -= 1 return n >= self . k
7864	def main ( ) : parser = argparse . ArgumentParser ( description = 'XMPP version checker' , parents = [ XMPPSettings . get_arg_parser ( ) ] ) parser . add_argument ( 'source' , metavar = 'SOURCE' , help = 'Source JID' ) parser . add_argument ( 'target' , metavar = 'TARGET' , nargs = '?' , help = 'Target JID (default: domain of SOURCE)' ) parser . add_argument ( '--debug' , action = 'store_const' , dest = 'log_level' , const = logging . DEBUG , default = logging . INFO , help = 'Print debug messages' ) parser . add_argument ( '--quiet' , const = logging . ERROR , action = 'store_const' , dest = 'log_level' , help = 'Print only error messages' ) args = parser . parse_args ( ) settings = XMPPSettings ( ) settings . load_arguments ( args ) if settings . get ( "password" ) is None : password = getpass ( "{0!r} password: " . format ( args . source ) ) if sys . version_info . major < 3 : password = password . decode ( "utf-8" ) settings [ "password" ] = password if sys . version_info . major < 3 : args . source = args . source . decode ( "utf-8" ) source = JID ( args . source ) if args . target : if sys . version_info . major < 3 : args . target = args . target . decode ( "utf-8" ) target = JID ( args . target ) else : target = JID ( source . domain ) logging . basicConfig ( level = args . log_level ) checker = VersionChecker ( source , target , settings ) try : checker . run ( ) except KeyboardInterrupt : checker . disconnect ( )
3762	def Tt ( CASRN , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in Staveley_data . index : methods . append ( STAVELEY ) if Tm ( CASRN ) : methods . append ( MELTING ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == STAVELEY : Tt = Staveley_data . at [ CASRN , "Tt68" ] elif Method == MELTING : Tt = Tm ( CASRN ) elif Method == NONE : Tt = None else : raise Exception ( 'Failure in in function' ) return Tt
11191	def write ( proto_dataset_uri , input ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( uri = proto_dataset_uri ) _validate_and_put_readme ( proto_dataset , input . read ( ) )
3058	def _write_credentials_file ( credentials_file , credentials ) : data = { 'file_version' : 2 , 'credentials' : { } } for key , credential in iteritems ( credentials ) : credential_json = credential . to_json ( ) encoded_credential = _helpers . _from_bytes ( base64 . b64encode ( _helpers . _to_bytes ( credential_json ) ) ) data [ 'credentials' ] [ key ] = encoded_credential credentials_file . seek ( 0 ) json . dump ( data , credentials_file ) credentials_file . truncate ( )
449	def batch_normalization ( x , mean , variance , offset , scale , variance_epsilon , data_format , name = None ) : with ops . name_scope ( name , 'batchnorm' , [ x , mean , variance , scale , offset ] ) : inv = math_ops . rsqrt ( variance + variance_epsilon ) if scale is not None : inv *= scale a = math_ops . cast ( inv , x . dtype ) b = math_ops . cast ( offset - mean * inv if offset is not None else - mean * inv , x . dtype ) df = { 'channels_first' : 'NCHW' , 'channels_last' : 'NHWC' } return _bias_add ( _bias_scale ( x , a , df [ data_format ] ) , b , df [ data_format ] )
6966	def initialize ( self , executor , secret ) : self . executor = executor self . secret = secret
415	def save_dataset ( self , dataset = None , dataset_name = None , ** kwargs ) : self . _fill_project_info ( kwargs ) if dataset_name is None : raise Exception ( "dataset_name is None, please give a dataset name" ) kwargs . update ( { 'dataset_name' : dataset_name } ) s = time . time ( ) try : dataset_id = self . dataset_fs . put ( self . _serialization ( dataset ) ) kwargs . update ( { 'dataset_id' : dataset_id , 'time' : datetime . utcnow ( ) } ) self . db . Dataset . insert_one ( kwargs ) print ( "[Database] Save dataset: SUCCESS, took: {}s" . format ( round ( time . time ( ) - s , 2 ) ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) print ( "[Database] Save dataset: FAIL" ) return False
13373	def unipath ( * paths ) : return os . path . normpath ( expandpath ( os . path . join ( * paths ) ) )
2241	def modpath_to_modname ( modpath , hide_init = True , hide_main = False , check = True , relativeto = None ) : if check and relativeto is None : if not exists ( modpath ) : raise ValueError ( 'modpath={} does not exist' . format ( modpath ) ) modpath_ = abspath ( expanduser ( modpath ) ) modpath_ = normalize_modpath ( modpath_ , hide_init = hide_init , hide_main = hide_main ) if relativeto : dpath = dirname ( abspath ( expanduser ( relativeto ) ) ) rel_modpath = relpath ( modpath_ , dpath ) else : dpath , rel_modpath = split_modpath ( modpath_ , check = check ) modname = splitext ( rel_modpath ) [ 0 ] if '.' in modname : modname , abi_tag = modname . split ( '.' ) modname = modname . replace ( '/' , '.' ) modname = modname . replace ( '\\' , '.' ) return modname
2549	def include ( f ) : fl = open ( f , 'r' ) data = fl . read ( ) fl . close ( ) return raw ( data )
13687	def assert_equal_files ( self , obtained_fn , expected_fn , fix_callback = lambda x : x , binary = False , encoding = None ) : import os from zerotk . easyfs import GetFileContents , GetFileLines __tracebackhide__ = True import io def FindFile ( filename ) : data_filename = self . get_filename ( filename ) if os . path . isfile ( data_filename ) : return data_filename if os . path . isfile ( filename ) : return filename from . _exceptions import MultipleFilesNotFound raise MultipleFilesNotFound ( [ filename , data_filename ] ) obtained_fn = FindFile ( obtained_fn ) expected_fn = FindFile ( expected_fn ) if binary : obtained_lines = GetFileContents ( obtained_fn , binary = True ) expected_lines = GetFileContents ( expected_fn , binary = True ) assert obtained_lines == expected_lines else : obtained_lines = fix_callback ( GetFileLines ( obtained_fn , encoding = encoding ) ) expected_lines = GetFileLines ( expected_fn , encoding = encoding ) if obtained_lines != expected_lines : html_fn = os . path . splitext ( obtained_fn ) [ 0 ] + '.diff.html' html_diff = self . _generate_html_diff ( expected_fn , expected_lines , obtained_fn , obtained_lines ) with io . open ( html_fn , 'w' ) as f : f . write ( html_diff ) import difflib diff = [ 'FILES DIFFER:' , obtained_fn , expected_fn ] diff += [ 'HTML DIFF: %s' % html_fn ] diff += difflib . context_diff ( obtained_lines , expected_lines ) raise AssertionError ( '\n' . join ( diff ) + '\n' )
6936	def cp_objectinfo_worker ( task ) : cpf , cpkwargs = task try : newcpf = update_checkplot_objectinfo ( cpf , ** cpkwargs ) return newcpf except Exception as e : LOGEXCEPTION ( 'failed to update objectinfo for %s' % cpf ) return None
13413	def addMenu ( self ) : self . parent . multiLogLayout . addLayout ( self . logSelectLayout ) self . getPrograms ( logType , programName )
5739	def enqueue_task ( self , task ) : data = dumps ( task ) if self . _async : self . publisher_client . publish ( self . topic_path , data = data ) logger . info ( 'Task {} queued.' . format ( task . id ) ) else : unpickled_task = unpickle ( data ) logger . info ( 'Executing task {} synchronously.' . format ( unpickled_task . id ) ) with measure_time ( ) as summary , self . queue_context ( ) : unpickled_task . execute ( queue = self ) summary ( unpickled_task . summary ( ) ) return TaskResult ( task . id , self )
8628	def create_project ( session , title , description , currency , budget , jobs ) : project_data = { 'title' : title , 'description' : description , 'currency' : currency , 'budget' : budget , 'jobs' : jobs } response = make_post_request ( session , 'projects' , json_data = project_data ) json_data = response . json ( ) if response . status_code == 200 : project_data = json_data [ 'result' ] p = Project ( project_data ) p . url = urljoin ( session . url , 'projects/%s' % p . seo_url ) return p else : raise ProjectNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] , )
1149	def warnpy3k ( message , category = None , stacklevel = 1 ) : if sys . py3kwarning : if category is None : category = DeprecationWarning warn ( message , category , stacklevel + 1 )
1490	def tail ( filename , n ) : size = os . path . getsize ( filename ) with open ( filename , "rb" ) as f : fm = mmap . mmap ( f . fileno ( ) , 0 , mmap . MAP_SHARED , mmap . PROT_READ ) try : for i in xrange ( size - 1 , - 1 , - 1 ) : if fm [ i ] == '\n' : n -= 1 if n == - 1 : break return fm [ i + 1 if i else 0 : ] . splitlines ( ) finally : fm . close ( )
8428	def gradient_n_pal ( colors , values = None , name = 'gradientn' ) : if values is None : colormap = mcolors . LinearSegmentedColormap . from_list ( name , colors ) else : colormap = mcolors . LinearSegmentedColormap . from_list ( name , list ( zip ( values , colors ) ) ) def _gradient_n_pal ( vals ) : return ratios_to_colors ( vals , colormap ) return _gradient_n_pal
10751	def validate_sceneInfo ( self ) : if self . sceneInfo . prefix not in self . __prefixesValid : raise WrongSceneNameError ( 'AWS: Prefix of %s (%s) is invalid' % ( self . sceneInfo . name , self . sceneInfo . prefix ) )
12519	def mask ( self , image ) : if image is None : self . _mask = None try : mask = load_mask ( image ) except Exception as exc : raise Exception ( 'Could not load mask image {}.' . format ( image ) ) from exc else : self . _mask = mask
9095	def _add_annotation_to_graph ( self , graph : BELGraph ) -> None : if 'bio2bel' not in graph . annotation_list : graph . annotation_list [ 'bio2bel' ] = set ( ) graph . annotation_list [ 'bio2bel' ] . add ( self . module_name )
5139	def process_token ( self , kind , string , start , end , line ) : if self . current_block . is_comment : if kind == tokenize . COMMENT : self . current_block . add ( string , start , end , line ) else : self . new_noncomment ( start [ 0 ] , end [ 0 ] ) else : if kind == tokenize . COMMENT : self . new_comment ( string , start , end , line ) else : self . current_block . add ( string , start , end , line )
13134	def import_domaindump ( ) : parser = argparse . ArgumentParser ( description = "Imports users, groups and computers result files from the ldapdomaindump tool, will resolve the names from domain_computers output for IPs" ) parser . add_argument ( "files" , nargs = '+' , help = "The domaindump files to import" ) arguments = parser . parse_args ( ) domain_users_file = '' domain_groups_file = '' computer_count = 0 user_count = 0 stats = { } for filename in arguments . files : if filename . endswith ( 'domain_computers.json' ) : print_notification ( 'Parsing domain computers' ) computer_count = parse_domain_computers ( filename ) if computer_count : stats [ 'hosts' ] = computer_count print_success ( "{} hosts imported" . format ( computer_count ) ) elif filename . endswith ( 'domain_users.json' ) : domain_users_file = filename elif filename . endswith ( 'domain_groups.json' ) : domain_groups_file = filename if domain_users_file : print_notification ( "Parsing domain users" ) user_count = parse_domain_users ( domain_users_file , domain_groups_file ) if user_count : print_success ( "{} users imported" . format ( user_count ) ) stats [ 'users' ] = user_count Logger ( ) . log ( "import_domaindump" , 'Imported domaindump, found {} user, {} systems' . format ( user_count , computer_count ) , stats )
289	def plot_rolling_returns ( returns , factor_returns = None , live_start_date = None , logy = False , cone_std = None , legend_loc = 'best' , volatility_match = False , cone_function = timeseries . forecast_cone_bootstrap , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Cumulative returns' ) ax . set_yscale ( 'log' if logy else 'linear' ) if volatility_match and factor_returns is None : raise ValueError ( 'volatility_match requires passing of ' 'factor_returns.' ) elif volatility_match and factor_returns is not None : bmark_vol = factor_returns . loc [ returns . index ] . std ( ) returns = ( returns / returns . std ( ) ) * bmark_vol cum_rets = ep . cum_returns ( returns , 1.0 ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) if factor_returns is not None : cum_factor_returns = ep . cum_returns ( factor_returns [ cum_rets . index ] , 1.0 ) cum_factor_returns . plot ( lw = 2 , color = 'gray' , label = factor_returns . name , alpha = 0.60 , ax = ax , ** kwargs ) if live_start_date is not None : live_start_date = ep . utils . get_utc_timestamp ( live_start_date ) is_cum_returns = cum_rets . loc [ cum_rets . index < live_start_date ] oos_cum_returns = cum_rets . loc [ cum_rets . index >= live_start_date ] else : is_cum_returns = cum_rets oos_cum_returns = pd . Series ( [ ] ) is_cum_returns . plot ( lw = 3 , color = 'forestgreen' , alpha = 0.6 , label = 'Backtest' , ax = ax , ** kwargs ) if len ( oos_cum_returns ) > 0 : oos_cum_returns . plot ( lw = 4 , color = 'red' , alpha = 0.6 , label = 'Live' , ax = ax , ** kwargs ) if cone_std is not None : if isinstance ( cone_std , ( float , int ) ) : cone_std = [ cone_std ] is_returns = returns . loc [ returns . index < live_start_date ] cone_bounds = cone_function ( is_returns , len ( oos_cum_returns ) , cone_std = cone_std , starting_value = is_cum_returns [ - 1 ] ) cone_bounds = cone_bounds . set_index ( oos_cum_returns . index ) for std in cone_std : ax . fill_between ( cone_bounds . index , cone_bounds [ float ( std ) ] , cone_bounds [ float ( - std ) ] , color = 'steelblue' , alpha = 0.5 ) if legend_loc is not None : ax . legend ( loc = legend_loc , frameon = True , framealpha = 0.5 ) ax . axhline ( 1.0 , linestyle = '--' , color = 'black' , lw = 2 ) return ax
6822	def configure_modrpaf ( self ) : r = self . local_renderer if r . env . modrpaf_enabled : self . install_packages ( ) self . enable_mod ( 'rpaf' ) else : if self . last_manifest . modrpaf_enabled : self . disable_mod ( 'mod_rpaf' )
9713	def validate_response ( expected_responses ) : def internal_decorator ( function ) : @ wraps ( function ) async def wrapper ( * args , ** kwargs ) : response = await function ( * args , ** kwargs ) for expected_response in expected_responses : if response . startswith ( expected_response ) : return response raise QRTCommandException ( "Expected %s but got %s" % ( expected_responses , response ) ) return wrapper return internal_decorator
9462	def conference_hangup ( self , call_params ) : path = '/' + self . api_version + '/ConferenceHangup/' method = 'POST' return self . request ( path , method , call_params )
8387	def check_main ( argv ) : if len ( argv ) != 1 : print ( "Please provide the name of a file to check." ) return 1 filename = argv [ 0 ] if os . path . exists ( filename ) : print ( u"Checking existing copy of %s" % filename ) tef = TamperEvidentFile ( filename ) if tef . validate ( ) : print ( u"Your copy of %s is good" % filename ) else : print ( u"Your copy of %s seems to have been edited" % filename ) else : print ( u"You don't have a copy of %s" % filename ) return 0
11986	async def _upload_file ( self , full_path ) : rel_path = os . path . relpath ( full_path , self . folder ) key = s3_key ( os . path . join ( self . key , rel_path ) ) ct = self . content_types . get ( key . split ( '.' ) [ - 1 ] ) with open ( full_path , 'rb' ) as fp : file = fp . read ( ) try : await self . botocore . upload_file ( self . bucket , file , key = key , ContentType = ct ) except Exception as exc : LOGGER . error ( 'Could not upload "%s": %s' , key , exc ) self . failures [ key ] = self . all . pop ( full_path ) return size = self . all . pop ( full_path ) self . success [ key ] = size self . total_size += size percentage = 100 * ( 1 - len ( self . all ) / self . total_files ) message = '{0:.0f}% completed - uploaded "{1}" - {2}' . format ( percentage , key , convert_bytes ( size ) ) LOGGER . info ( message )
102	def imresize_single_image ( image , sizes , interpolation = None ) : grayscale = False if image . ndim == 2 : grayscale = True image = image [ : , : , np . newaxis ] do_assert ( len ( image . shape ) == 3 , image . shape ) rs = imresize_many_images ( image [ np . newaxis , : , : , : ] , sizes , interpolation = interpolation ) if grayscale : return np . squeeze ( rs [ 0 , : , : , 0 ] ) else : return rs [ 0 , ... ]
6351	def _phonetic_numbers ( self , phonetic ) : phonetic_array = phonetic . split ( '-' ) result = ' ' . join ( [ self . _pnums_with_leading_space ( i ) [ 1 : ] for i in phonetic_array ] ) return result
10878	def calculate_polychrome_linescan_psf ( x , y , z , normalize = False , kfki = 0.889 , sigkf = 0.1 , zint = 100. , nkpts = 3 , dist_type = 'gaussian' , wrap = True , ** kwargs ) : kfkipts , wts = get_polydisp_pts_wts ( kfki , sigkf , dist_type = dist_type , nkpts = nkpts ) if wrap : xpts = vec_to_halfvec ( x ) ypts = vec_to_halfvec ( y ) x3 , y3 , z3 = np . meshgrid ( xpts , ypts , z , indexing = 'ij' ) else : x3 , y3 , z3 = np . meshgrid ( x , y , z , indexing = 'ij' ) rho3 = np . sqrt ( x3 * x3 + y3 * y3 ) if wrap : y2 , z2 = np . meshgrid ( ypts , z , indexing = 'ij' ) hilm0 = calculate_linescan_ilm_psf ( y2 , z2 , zint = zint , ** kwargs ) if ypts [ 0 ] == 0 : hilm = np . append ( hilm0 [ - 1 : 0 : - 1 ] , hilm0 , axis = 0 ) else : hilm = np . append ( hilm0 [ : : - 1 ] , hilm0 , axis = 0 ) else : y2 , z2 = np . meshgrid ( y , z , indexing = 'ij' ) hilm = calculate_linescan_ilm_psf ( y2 , z2 , zint = zint , ** kwargs ) if wrap : func = lambda x , y , z , kfki = 1. : get_hsym_asym ( rho3 * kfki , z3 * kfki , zint = kfki * zint , get_hdet = True , ** kwargs ) [ 0 ] hdet_func = lambda kfki : wrap_and_calc_psf ( xpts , ypts , z , func , kfki = kfki ) else : hdet_func = lambda kfki : get_hsym_asym ( rho3 * kfki , z3 * kfki , zint = kfki * zint , get_hdet = True , ** kwargs ) [ 0 ] inner = [ wts [ a ] * hdet_func ( kfkipts [ a ] ) for a in range ( nkpts ) ] hdet = np . sum ( inner , axis = 0 ) if normalize : hilm /= hilm . sum ( ) hdet /= hdet . sum ( ) for a in range ( x . size ) : hdet [ a ] *= hilm return hdet if normalize else hdet / hdet . sum ( )
3107	def _retrieve_info ( self , http ) : if self . invalid : info = _metadata . get_service_account_info ( http , service_account = self . service_account_email or 'default' ) self . invalid = False self . service_account_email = info [ 'email' ] self . scopes = info [ 'scopes' ]
3063	def parse_unique_urlencoded ( content ) : urlencoded_params = urllib . parse . parse_qs ( content ) params = { } for key , value in six . iteritems ( urlencoded_params ) : if len ( value ) != 1 : msg = ( 'URL-encoded content contains a repeated value:' '%s -> %s' % ( key , ', ' . join ( value ) ) ) raise ValueError ( msg ) params [ key ] = value [ 0 ] return params
7137	def format ( obj , options ) : formatters = { float_types : lambda x : '{:.{}g}' . format ( x , options . digits ) , } for _types , fmtr in formatters . items ( ) : if isinstance ( obj , _types ) : return fmtr ( obj ) try : if six . PY2 and isinstance ( obj , six . string_types ) : return str ( obj . encode ( 'utf-8' ) ) return str ( obj ) except : return 'OBJECT'
13747	def get_item ( self , hash_key , start = 0 , extra_attrs = None ) : table = self . get_table ( ) try : item = table . get_item ( hash_key = hash_key ) except DynamoDBKeyNotFoundError : item = None if item is None : item = self . create_item ( hash_key = hash_key , start = start , extra_attrs = extra_attrs , ) return item
5414	def get_provider ( args , resources ) : provider = getattr ( args , 'provider' , 'google' ) if provider == 'google' : return google . GoogleJobProvider ( getattr ( args , 'verbose' , False ) , getattr ( args , 'dry_run' , False ) , args . project ) elif provider == 'google-v2' : return google_v2 . GoogleV2JobProvider ( getattr ( args , 'verbose' , False ) , getattr ( args , 'dry_run' , False ) , args . project ) elif provider == 'local' : return local . LocalJobProvider ( resources ) elif provider == 'test-fails' : return test_fails . FailsJobProvider ( ) else : raise ValueError ( 'Unknown provider: ' + provider )
9488	def generate_simple_call ( opcode : int , index : int ) : bs = b"" bs += opcode . to_bytes ( 1 , byteorder = "little" ) if isinstance ( index , int ) : if PY36 : bs += index . to_bytes ( 1 , byteorder = "little" ) else : bs += index . to_bytes ( 2 , byteorder = "little" ) else : bs += index return bs
10828	def delete ( cls , group , user ) : with db . session . begin_nested ( ) : cls . query . filter_by ( group = group , user_id = user . get_id ( ) ) . delete ( )
7721	def get_history ( self ) : for child in xml_element_iter ( self . xmlnode . children ) : if get_node_ns_uri ( child ) == MUC_NS and child . name == "history" : maxchars = from_utf8 ( child . prop ( "maxchars" ) ) if maxchars is not None : maxchars = int ( maxchars ) maxstanzas = from_utf8 ( child . prop ( "maxstanzas" ) ) if maxstanzas is not None : maxstanzas = int ( maxstanzas ) maxseconds = from_utf8 ( child . prop ( "maxseconds" ) ) if maxseconds is not None : maxseconds = int ( maxseconds ) since = None return HistoryParameters ( maxchars , maxstanzas , maxseconds , since )
4443	def delete ( self , string ) : return self . redis . execute_command ( AutoCompleter . SUGDEL_COMMAND , self . key , string )
12086	def html_index ( self , launch = False , showChildren = False ) : self . makePics ( ) html = '<a href="index_splash.html" target="content">./%s/</a><br>' % os . path . basename ( self . abfFolder ) for ID in smartSort ( self . fnamesByCell . keys ( ) ) : link = '' if ID + ".html" in self . fnames2 : link = 'href="%s.html" target="content"' % ID html += ( '<a %s>%s</a><br>' % ( link , ID ) ) if showChildren : for fname in self . fnamesByCell [ ID ] : thisID = os . path . splitext ( fname ) [ 0 ] files2 = [ x for x in self . fnames2 if x . startswith ( thisID ) and not x . endswith ( ".html" ) ] html += '<i>%s</i>' % thisID if len ( files2 ) : html += ' (%s)' % len ( files2 ) html += '<br>' html += "<br>" style . save ( html , self . abfFolder2 + "/index_menu.html" ) self . html_index_splash ( ) style . frames ( self . abfFolder2 + "/index.html" , launch = launch )
8386	def amend_filename ( filename , amend ) : base , ext = os . path . splitext ( filename ) amended_name = base + amend + ext return amended_name
9478	def parse_string ( self , string ) : dom = minidom . parseString ( string ) return self . parse_dom ( dom )
5365	def format ( self , record ) : if isinstance ( self . fmt , dict ) : self . _fmt = self . fmt [ record . levelname ] if sys . version_info > ( 3 , 2 ) : if self . style not in logging . _STYLES : raise ValueError ( 'Style must be one of: %s' % ',' . join ( list ( logging . _STYLES . keys ( ) ) ) ) self . _style = logging . _STYLES [ self . style ] [ 0 ] ( self . _fmt ) if sys . version_info > ( 2 , 7 ) : message = super ( LevelFormatter , self ) . format ( record ) else : message = ColoredFormatter . format ( self , record ) return message
7734	def nfkc ( data ) : if isinstance ( data , list ) : data = u"" . join ( data ) return unicodedata . normalize ( "NFKC" , data )
8443	def ls ( github_user , template = None ) : temple . check . has_env_vars ( temple . constants . GITHUB_API_TOKEN_ENV_VAR ) if template : temple . check . is_git_ssh_path ( template ) search_q = 'user:{} filename:{} {}' . format ( github_user , temple . constants . TEMPLE_CONFIG_FILE , template ) else : search_q = 'user:{} cookiecutter.json in:path' . format ( github_user ) results = _code_search ( search_q , github_user ) return collections . OrderedDict ( sorted ( results . items ( ) ) )
12171	def count ( self , event ) : return len ( self . _listeners [ event ] ) + len ( self . _once [ event ] )
9346	def adapt ( cls , source , template ) : if not isinstance ( template , packarray ) : raise TypeError ( 'template must be a packarray' ) return cls ( source , template . start , template . end )
2909	def _find_any ( self , task_spec ) : tasks = [ ] if self . task_spec == task_spec : tasks . append ( self ) for child in self : if child . task_spec != task_spec : continue tasks . append ( child ) return tasks
3165	def get ( self , workflow_id , email_id , subscriber_hash ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . workflow_id = workflow_id self . email_id = email_id self . subscriber_hash = subscriber_hash return self . _mc_client . _get ( url = self . _build_path ( workflow_id , 'emails' , email_id , 'queue' , subscriber_hash ) )
8504	def get_key ( self ) : if not isinstance ( self . key , Unparseable ) : return self . key line = self . source [ self . col_offset : ] regex = re . compile ( ) match = regex . match ( line ) if not match : return Unparseable ( ) return "<%s>" % match . group ( 1 )
723	def getTerminationCallbacks ( self , terminationFunc ) : activities = [ None ] * len ( ModelTerminator . _MILESTONES ) for index , ( iteration , _ ) in enumerate ( ModelTerminator . _MILESTONES ) : cb = functools . partial ( terminationFunc , index = index ) activities [ index ] = PeriodicActivityRequest ( repeating = False , period = iteration , cb = cb )
12970	def _doCascadeFetch ( obj ) : obj . validateModel ( ) if not obj . foreignFields : return for foreignField in obj . foreignFields : subObjsData = object . __getattribute__ ( obj , foreignField ) if not subObjsData : setattr ( obj , str ( foreignField ) , irNull ) continue subObjs = subObjsData . getObjs ( ) for subObj in subObjs : if isIndexedRedisModel ( subObj ) : IndexedRedisQuery . _doCascadeFetch ( subObj )
8382	def textpath ( self , i ) : if len ( self . _textpaths ) == i : self . _ctx . font ( self . font , self . fontsize ) txt = self . q [ i ] if len ( self . q ) > 1 : txt += " (" + str ( i + 1 ) + "/" + str ( len ( self . q ) ) + ")" p = self . _ctx . textpath ( txt , 0 , 0 , width = self . _w ) h = self . _ctx . textheight ( txt , width = self . _w ) self . _textpaths . append ( ( p , h ) ) return self . _textpaths [ i ]
10338	def update_spia_matrices ( spia_matrices : Dict [ str , pd . DataFrame ] , u : CentralDogma , v : CentralDogma , edge_data : EdgeData , ) -> None : if u . namespace . upper ( ) != 'HGNC' or v . namespace . upper ( ) != 'HGNC' : return u_name = u . name v_name = v . name relation = edge_data [ RELATION ] if relation in CAUSAL_INCREASE_RELATIONS : if v . variants and any ( isinstance ( variant , ProteinModification ) for variant in v . variants ) : for variant in v . variants : if not isinstance ( variant , ProteinModification ) : continue if variant [ IDENTIFIER ] [ NAME ] == "Ub" : spia_matrices [ "activation_ubiquination" ] [ u_name ] [ v_name ] = 1 elif variant [ IDENTIFIER ] [ NAME ] == "Ph" : spia_matrices [ "activation_phosphorylation" ] [ u_name ] [ v_name ] = 1 elif isinstance ( v , ( Gene , Rna ) ) : spia_matrices [ 'expression' ] [ u_name ] [ v_name ] = 1 else : spia_matrices [ 'activation' ] [ u_name ] [ v_name ] = 1 elif relation in CAUSAL_DECREASE_RELATIONS : if v . variants and any ( isinstance ( variant , ProteinModification ) for variant in v . variants ) : for variant in v . variants : if not isinstance ( variant , ProteinModification ) : continue if variant [ IDENTIFIER ] [ NAME ] == "Ub" : spia_matrices [ 'inhibition_ubiquination' ] [ u_name ] [ v_name ] = 1 elif variant [ IDENTIFIER ] [ NAME ] == "Ph" : spia_matrices [ "inhibition_phosphorylation" ] [ u_name ] [ v_name ] = 1 elif isinstance ( v , ( Gene , Rna ) ) : spia_matrices [ "repression" ] [ u_name ] [ v_name ] = 1 else : spia_matrices [ "inhibition" ] [ u_name ] [ v_name ] = 1 elif relation == ASSOCIATION : spia_matrices [ "binding_association" ] [ u_name ] [ v_name ] = 1
7801	def handle_authorized ( self , event ) : stream = event . stream if not stream : return if not stream . initiator : return if stream . features is None : return element = stream . features . find ( SESSION_TAG ) if element is None : return logger . debug ( "Establishing IM session" ) stanza = Iq ( stanza_type = "set" ) payload = XMLPayload ( ElementTree . Element ( SESSION_TAG ) ) stanza . set_payload ( payload ) self . stanza_processor . set_response_handlers ( stanza , self . _session_success , self . _session_error ) stream . send ( stanza )
11609	def multiply ( self , multiplier , axis = None ) : if self . finalized : if multiplier . ndim == 1 : if axis == 0 : raise NotImplementedError ( 'The method is not yet implemented for the axis.' ) elif axis == 1 : sz = len ( multiplier ) multiplier_mat = lil_matrix ( ( sz , sz ) ) multiplier_mat . setdiag ( multiplier ) for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] * multiplier_mat elif axis == 2 : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] . data *= multiplier [ self . data [ hid ] . indices ] else : raise RuntimeError ( 'The axis should be 0, 1, or 2.' ) elif multiplier . ndim == 2 : if axis == 0 : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] . data *= multiplier [ self . data [ hid ] . indices , hid ] elif axis == 1 : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] . multiply ( multiplier ) elif axis == 2 : for hid in xrange ( self . shape [ 1 ] ) : multiplier_vec = multiplier [ hid , : ] multiplier_vec = multiplier_vec . ravel ( ) self . data [ hid ] . data *= multiplier_vec . repeat ( np . diff ( self . data [ hid ] . indptr ) ) else : raise RuntimeError ( 'The axis should be 0, 1, or 2.' ) elif isinstance ( multiplier , Sparse3DMatrix ) : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] . multiply ( multiplier . data [ hid ] ) else : raise RuntimeError ( 'The multiplier should be 1, 2 dimensional numpy array or a Sparse3DMatrix object.' ) else : raise RuntimeError ( 'The original matrix must be finalized.' )
2916	def get_dump ( self , indent = 0 , recursive = True ) : dbg = ( ' ' * indent * 2 ) dbg += '%s/' % self . id dbg += '%s:' % self . thread_id dbg += ' Task of %s' % self . get_name ( ) if self . task_spec . description : dbg += ' (%s)' % self . get_description ( ) dbg += ' State: %s' % self . get_state_name ( ) dbg += ' Children: %s' % len ( self . children ) if recursive : for child in self . children : dbg += '\n' + child . get_dump ( indent + 1 ) return dbg
1053	def extract_stack ( f = None , limit = None ) : if f is None : try : raise ZeroDivisionError except ZeroDivisionError : f = sys . exc_info ( ) [ 2 ] . tb_frame . f_back if limit is None : if hasattr ( sys , 'tracebacklimit' ) : limit = sys . tracebacklimit list = [ ] n = 0 while f is not None and ( limit is None or n < limit ) : lineno = f . f_lineno co = f . f_code filename = co . co_filename name = co . co_name linecache . checkcache ( filename ) line = linecache . getline ( filename , lineno , f . f_globals ) if line : line = line . strip ( ) else : line = None list . append ( ( filename , lineno , name , line ) ) f = f . f_back n = n + 1 list . reverse ( ) return list
11130	def start ( self ) : with self . _status_lock : if self . _running : raise RuntimeError ( "Already running" ) self . _running = True self . _observer = Observer ( ) self . _observer . schedule ( self . _event_handler , self . _directory_location , recursive = True ) self . _observer . start ( ) self . _origin_mapped_data = self . _load_all_in_directory ( )
11842	def ModelBasedVacuumAgent ( ) : "An agent that keeps track of what locations are clean or dirty." model = { loc_A : None , loc_B : None } def program ( ( location , status ) ) : "Same as ReflexVacuumAgent, except if everything is clean, do NoOp." model [ location ] = status if model [ loc_A ] == model [ loc_B ] == 'Clean' : return 'NoOp' elif status == 'Dirty' : return 'Suck' elif location == loc_A : return 'Right' elif location == loc_B : return 'Left' return Agent ( program )
10228	def get_separate_unstable_correlation_triples ( graph : BELGraph ) -> Iterable [ NodeTriple ] : cg = get_correlation_graph ( graph ) for a , b , c in get_correlation_triangles ( cg ) : if POSITIVE_CORRELATION in cg [ a ] [ b ] and POSITIVE_CORRELATION in cg [ b ] [ c ] and NEGATIVE_CORRELATION in cg [ a ] [ c ] : yield b , a , c if POSITIVE_CORRELATION in cg [ a ] [ b ] and NEGATIVE_CORRELATION in cg [ b ] [ c ] and POSITIVE_CORRELATION in cg [ a ] [ c ] : yield a , b , c if NEGATIVE_CORRELATION in cg [ a ] [ b ] and POSITIVE_CORRELATION in cg [ b ] [ c ] and POSITIVE_CORRELATION in cg [ a ] [ c ] : yield c , a , b
9642	def set_trace ( context ) : try : import ipdb as pdb except ImportError : import pdb print ( "For best results, pip install ipdb." ) print ( "Variables that are available in the current context:" ) render = lambda s : template . Template ( s ) . render ( context ) availables = get_variables ( context ) pprint ( availables ) print ( 'Type `availables` to show this list.' ) print ( 'Type <variable_name> to access one.' ) print ( 'Use render("template string") to test template rendering' ) for var in availables : locals ( ) [ var ] = context [ var ] pdb . set_trace ( ) return ''
11660	def fit ( self , X , y = None ) : self . kmeans_fit_ = copy ( self . kmeans ) X = as_features ( X , stack = True ) self . kmeans_fit_ . fit ( X . stacked_features ) return self
1334	def backward ( self , gradient , image = None , strict = True ) : assert self . has_gradient ( ) assert gradient . ndim == 1 if image is None : image = self . __original_image assert not strict or self . in_bounds ( image ) self . _total_gradient_calls += 1 gradient = self . __model . backward ( gradient , image ) assert gradient . shape == image . shape return gradient
159	def Grayscale ( alpha = 0 , from_colorspace = "RGB" , name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return ChangeColorspace ( to_colorspace = ChangeColorspace . GRAY , alpha = alpha , from_colorspace = from_colorspace , name = name , deterministic = deterministic , random_state = random_state )
10584	def remove_account ( self , name ) : acc_to_remove = None for a in self . accounts : if a . name == name : acc_to_remove = a if acc_to_remove is not None : self . accounts . remove ( acc_to_remove )
3794	def setup_a_alpha_and_derivatives ( self , i , T = None ) : r self . a , self . m , self . Tc = self . ais [ i ] , self . ms [ i ] , self . Tcs [ i ]
2315	def orient_undirected_graph ( self , data , graph , ** kwargs ) : self . arguments [ '{CITEST}' ] = self . dir_CI_test [ self . CI_test ] self . arguments [ '{METHOD_INDEP}' ] = self . dir_method_indep [ self . method_indep ] self . arguments [ '{DIRECTED}' ] = 'TRUE' self . arguments [ '{ALPHA}' ] = str ( self . alpha ) self . arguments [ '{NJOBS}' ] = str ( self . nb_jobs ) self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) fe = DataFrame ( nx . adj_matrix ( graph , weight = None ) . todense ( ) ) fg = DataFrame ( 1 - fe . values ) results = self . _run_pc ( data , fixedEdges = fe , fixedGaps = fg , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
11568	def open ( self , verbose ) : if verbose : print ( '\nOpening Arduino Serial port %s ' % self . port_id ) try : self . arduino . close ( ) time . sleep ( 1 ) self . arduino . open ( ) time . sleep ( 1 ) return self . arduino except Exception : raise
8136	def duplicate ( self ) : i = self . canvas . layer ( self . img . copy ( ) , self . x , self . y , self . name ) clone = self . canvas . layers [ i ] clone . alpha = self . alpha clone . blend = self . blend
1011	def trimSegments ( self , minPermanence = None , minNumSyns = None ) : if minPermanence is None : minPermanence = self . connectedPerm if minNumSyns is None : minNumSyns = self . activationThreshold totalSegsRemoved , totalSynsRemoved = 0 , 0 for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : ( segsRemoved , synsRemoved ) = self . _trimSegmentsInCell ( colIdx = c , cellIdx = i , segList = self . cells [ c ] [ i ] , minPermanence = minPermanence , minNumSyns = minNumSyns ) totalSegsRemoved += segsRemoved totalSynsRemoved += synsRemoved if self . verbosity >= 5 : print "Cells, all segments:" self . printCells ( predictedOnly = False ) return totalSegsRemoved , totalSynsRemoved
13094	def start_processes ( self ) : self . relay = subprocess . Popen ( [ 'ntlmrelayx.py' , '-6' , '-tf' , self . targets_file , '-w' , '-l' , self . directory , '-of' , self . output_file ] , cwd = self . directory ) self . responder = subprocess . Popen ( [ 'responder' , '-I' , self . interface_name ] )
13238	def _daily_periods ( self , range_start , range_end ) : specific = set ( self . exceptions . keys ( ) ) return heapq . merge ( self . exception_periods ( range_start , range_end ) , * [ sched . daily_periods ( range_start = range_start , range_end = range_end , exclude_dates = specific ) for sched in self . _recurring_schedules ] )
9792	def is_ignored ( cls , path , patterns ) : status = None for pattern in cls . find_matching ( path , patterns ) : status = pattern . is_exclude return status
8318	def connect_table ( self , table , chunk , markup ) : k = markup . find ( chunk ) i = markup . rfind ( "\n=" , 0 , k ) j = markup . find ( "\n" , i + 1 ) paragraph_title = markup [ i : j ] . strip ( ) . strip ( "= " ) for paragraph in self . paragraphs : if paragraph . title == paragraph_title : paragraph . tables . append ( table ) table . paragraph = paragraph
579	def dictDiff ( da , db ) : different = False resultDict = dict ( ) resultDict [ 'inAButNotInB' ] = set ( da ) - set ( db ) if resultDict [ 'inAButNotInB' ] : different = True resultDict [ 'inBButNotInA' ] = set ( db ) - set ( da ) if resultDict [ 'inBButNotInA' ] : different = True resultDict [ 'differentValues' ] = [ ] for key in ( set ( da ) - resultDict [ 'inAButNotInB' ] ) : comparisonResult = da [ key ] == db [ key ] if isinstance ( comparisonResult , bool ) : isEqual = comparisonResult else : isEqual = comparisonResult . all ( ) if not isEqual : resultDict [ 'differentValues' ] . append ( key ) different = True assert ( ( ( resultDict [ 'inAButNotInB' ] or resultDict [ 'inBButNotInA' ] or resultDict [ 'differentValues' ] ) and different ) or not different ) return resultDict if different else None
1527	def is_host_port_reachable ( self ) : for hostport in self . hostportlist : try : socket . create_connection ( hostport , StateManager . TIMEOUT_SECONDS ) return True except : LOG . info ( "StateManager %s Unable to connect to host: %s port %i" % ( self . name , hostport [ 0 ] , hostport [ 1 ] ) ) continue return False
1891	def _solver_version ( self ) -> Version : self . _reset ( ) if self . _received_version is None : self . _send ( '(get-info :version)' ) self . _received_version = self . _recv ( ) key , version = shlex . split ( self . _received_version [ 1 : - 1 ] ) return Version ( * map ( int , version . split ( '.' ) ) )
10402	def calculate_score ( self , node : BaseEntity ) -> float : score = ( self . graph . nodes [ node ] [ self . tag ] if self . tag in self . graph . nodes [ node ] else self . default_score ) for predecessor , _ , d in self . graph . in_edges ( node , data = True ) : if d [ RELATION ] in CAUSAL_INCREASE_RELATIONS : score += self . graph . nodes [ predecessor ] [ self . tag ] elif d [ RELATION ] in CAUSAL_DECREASE_RELATIONS : score -= self . graph . nodes [ predecessor ] [ self . tag ] return score
7982	def auth_finish ( self , _unused ) : self . lock . acquire ( ) try : self . __logger . debug ( "Authenticated" ) self . authenticated = True self . state_change ( "authorized" , self . my_jid ) self . _post_auth ( ) finally : self . lock . release ( )
8057	def do_restart ( self , line ) : self . bot . _frame = 0 self . bot . _namespace . clear ( ) self . bot . _namespace . update ( self . bot . _initial_namespace )
13404	def prettify ( self , elem ) : from xml . etree import ElementTree from re import sub rawString = ElementTree . tostring ( elem , 'utf-8' ) parsedString = sub ( r'(?=<[^/].*>)' , '\n' , rawString ) return parsedString [ 1 : ]
11087	def sleep ( self , channel ) : self . log . info ( 'Sleeping in %s' , channel ) self . _bot . dispatcher . ignore ( channel ) self . send_message ( channel , 'Good night' )
7378	def process_keys ( func ) : @ wraps ( func ) def decorated ( self , k , * args ) : if not isinstance ( k , str ) : msg = "%s: key must be a string" % self . __class__ . __name__ raise ValueError ( msg ) if not k . startswith ( self . prefix ) : k = self . prefix + k return func ( self , k , * args ) return decorated
3083	def _parse_state_value ( state , user ) : uri , token = state . rsplit ( ':' , 1 ) if xsrfutil . validate_token ( xsrf_secret_key ( ) , token , user . user_id ( ) , action_id = uri ) : return uri else : return None
13581	def apply_types ( use_types , guess_type , line ) : new_line = { } for k , v in line . items ( ) : if use_types . has_key ( k ) : new_line [ k ] = force_type ( use_types [ k ] , v ) elif guess_type : new_line [ k ] = determine_type ( v ) else : new_line [ k ] = v return new_line
10200	def run ( self ) : return elasticsearch . helpers . bulk ( self . client , self . actionsiter ( ) , stats_only = True , chunk_size = 50 )
7975	def loop_iteration ( self , timeout = 0.1 ) : try : exc_info = self . exc_queue . get ( True , timeout ) [ 1 ] except Queue . Empty : return exc_type , exc_value , ext_stack = exc_info raise exc_type , exc_value , ext_stack
3507	def create_stoichiometric_matrix ( model , array_type = 'dense' , dtype = None ) : if array_type not in ( 'DataFrame' , 'dense' ) and not dok_matrix : raise ValueError ( 'Sparse matrices require scipy' ) if dtype is None : dtype = np . float64 array_constructor = { 'dense' : np . zeros , 'dok' : dok_matrix , 'lil' : lil_matrix , 'DataFrame' : np . zeros , } n_metabolites = len ( model . metabolites ) n_reactions = len ( model . reactions ) array = array_constructor [ array_type ] ( ( n_metabolites , n_reactions ) , dtype = dtype ) m_ind = model . metabolites . index r_ind = model . reactions . index for reaction in model . reactions : for metabolite , stoich in iteritems ( reaction . metabolites ) : array [ m_ind ( metabolite ) , r_ind ( reaction ) ] = stoich if array_type == 'DataFrame' : metabolite_ids = [ met . id for met in model . metabolites ] reaction_ids = [ rxn . id for rxn in model . reactions ] return pd . DataFrame ( array , index = metabolite_ids , columns = reaction_ids ) else : return array
2825	def convert_softmax ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting softmax ...' ) if names == 'short' : tf_name = 'SMAX' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) def target_layer ( x , dim = params [ 'dim' ] ) : import keras return keras . activations . softmax ( x , axis = dim ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
2464	def set_file_type ( self , doc , type_value ) : type_dict = { 'SOURCE' : file . FileType . SOURCE , 'BINARY' : file . FileType . BINARY , 'ARCHIVE' : file . FileType . ARCHIVE , 'OTHER' : file . FileType . OTHER } if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_type_set : self . file_type_set = True if type_value in type_dict . keys ( ) : self . file ( doc ) . type = type_dict [ type_value ] return True else : raise SPDXValueError ( 'File::Type' ) else : raise CardinalityError ( 'File::Type' ) else : raise OrderError ( 'File::Type' )
10466	def getFrontmostApp ( cls ) : apps = cls . _getRunningApps ( ) for app in apps : pid = app . processIdentifier ( ) ref = cls . getAppRefByPid ( pid ) try : if ref . AXFrontmost : return ref except ( _a11y . ErrorUnsupported , _a11y . ErrorCannotComplete , _a11y . ErrorAPIDisabled , _a11y . ErrorNotImplemented ) : pass raise ValueError ( 'No GUI application found.' )
6676	def upload_template ( self , filename , destination , context = None , use_jinja = False , template_dir = None , use_sudo = False , backup = True , mirror_local_mode = False , mode = None , mkdir = False , chown = False , user = None ) : if mkdir : remote_dir = os . path . dirname ( destination ) if use_sudo : self . sudo ( 'mkdir -p %s' % quote ( remote_dir ) , user = user ) else : self . run ( 'mkdir -p %s' % quote ( remote_dir ) ) if not self . dryrun : _upload_template ( filename = filename , destination = destination , context = context , use_jinja = use_jinja , template_dir = template_dir , use_sudo = use_sudo , backup = backup , mirror_local_mode = mirror_local_mode , mode = mode , ) if chown : if user is None : user = self . genv . user run_as_root ( 'chown %s: %s' % ( user , quote ( destination ) ) )
222	def is_not_modified ( self , response_headers : Headers , request_headers : Headers ) -> bool : try : if_none_match = request_headers [ "if-none-match" ] etag = response_headers [ "etag" ] if if_none_match == etag : return True except KeyError : pass try : if_modified_since = parsedate ( request_headers [ "if-modified-since" ] ) last_modified = parsedate ( response_headers [ "last-modified" ] ) if ( if_modified_since is not None and last_modified is not None and if_modified_since >= last_modified ) : return True except KeyError : pass return False
236	def plot_cap_exposures_net ( net_exposures , ax = None ) : if ax is None : ax = plt . gca ( ) color_list = plt . cm . gist_rainbow ( np . linspace ( 0 , 1 , 5 ) ) cap_names = CAP_BUCKETS . keys ( ) for i in range ( len ( net_exposures ) ) : ax . plot ( net_exposures [ i ] , color = color_list [ i ] , alpha = 0.8 , label = cap_names [ i ] ) ax . axhline ( 0 , color = 'k' , linestyle = '-' ) ax . set ( title = 'Net exposure to market caps' , ylabel = 'Proportion of net exposure \n in market cap buckets' ) return ax
3605	def get ( self , url , name , params = None , headers = None , connection = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) return make_get_request ( endpoint , params , headers , connection = connection )
10869	def calc_pts_hg ( npts = 20 ) : pts_hg , wts_hg = np . polynomial . hermite . hermgauss ( npts * 2 ) pts_hg = pts_hg [ npts : ] wts_hg = wts_hg [ npts : ] * np . exp ( pts_hg * pts_hg ) return pts_hg , wts_hg
11135	def copyto ( self , new_abspath = None , new_dirpath = None , new_dirname = None , new_basename = None , new_fname = None , new_ext = None , overwrite = False , makedirs = False ) : self . assert_exists ( ) p = self . change ( new_abspath = new_abspath , new_dirpath = new_dirpath , new_dirname = new_dirname , new_basename = new_basename , new_fname = new_fname , new_ext = new_ext , ) if p . is_not_exist_or_allow_overwrite ( overwrite = overwrite ) : if self . abspath != p . abspath : try : shutil . copy ( self . abspath , p . abspath ) except IOError as e : if makedirs : os . makedirs ( p . parent . abspath ) shutil . copy ( self . abspath , p . abspath ) else : raise e return p
10757	def writable_path ( path ) : if os . path . exists ( path ) : return os . access ( path , os . W_OK ) try : with open ( path , 'w' ) : pass except ( OSError , IOError ) : return False else : os . remove ( path ) return True
7762	def make_error_response ( self , cond ) : if self . stanza_type == "error" : raise ValueError ( "Errors may not be generated in response" " to errors" ) msg = Message ( stanza_type = "error" , from_jid = self . to_jid , to_jid = self . from_jid , stanza_id = self . stanza_id , error_cond = cond , subject = self . _subject , body = self . _body , thread = self . _thread ) if self . _payload is None : self . decode_payload ( ) for payload in self . _payload : msg . add_payload ( payload . copy ( ) ) return msg
4330	def gain ( self , gain_db = 0.0 , normalize = True , limiter = False , balance = None ) : if not is_number ( gain_db ) : raise ValueError ( "gain_db must be a number." ) if not isinstance ( normalize , bool ) : raise ValueError ( "normalize must be a boolean." ) if not isinstance ( limiter , bool ) : raise ValueError ( "limiter must be a boolean." ) if balance not in [ None , 'e' , 'B' , 'b' ] : raise ValueError ( "balance must be one of None, 'e', 'B', or 'b'." ) effect_args = [ 'gain' ] if balance is not None : effect_args . append ( '-{}' . format ( balance ) ) if normalize : effect_args . append ( '-n' ) if limiter : effect_args . append ( '-l' ) effect_args . append ( '{:f}' . format ( gain_db ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'gain' ) return self
6446	def _cond_bb ( self , word , suffix_len ) : return ( len ( word ) - suffix_len >= 3 and word [ - suffix_len - 3 : - suffix_len ] != 'met' and word [ - suffix_len - 4 : - suffix_len ] != 'ryst' )
5321	def get_data ( self , reset_device = False ) : try : if reset_device : self . _device . reset ( ) for interface in [ 0 , 1 ] : if self . _device . is_kernel_driver_active ( interface ) : LOGGER . debug ( 'Detaching kernel driver for interface %d ' 'of %r on ports %r' , interface , self . _device , self . _ports ) self . _device . detach_kernel_driver ( interface ) self . _device . set_configuration ( ) usb . util . claim_interface ( self . _device , INTERFACE ) self . _control_transfer ( COMMANDS [ 'temp' ] ) self . _interrupt_read ( ) self . _control_transfer ( COMMANDS [ 'temp' ] ) temp_data = self . _interrupt_read ( ) if self . _device . product == 'TEMPer1F_H1_V1.4' : humidity_data = temp_data else : humidity_data = None data = { 'temp_data' : temp_data , 'humidity_data' : humidity_data } usb . util . dispose_resources ( self . _device ) return data except usb . USBError as err : if not reset_device : LOGGER . warning ( "Encountered %s, resetting %r and trying again." , err , self . _device ) return self . get_data ( True ) if "not permitted" in str ( err ) : raise Exception ( "Permission problem accessing USB. " "Maybe I need to run as root?" ) else : LOGGER . error ( err ) raise
9339	def loadtxt2 ( fname , dtype = None , delimiter = ' ' , newline = '\n' , comment_character = '#' , skiplines = 0 ) : dtypert = [ None , None , None ] def preparedtype ( dtype ) : dtypert [ 0 ] = dtype flatten = flatten_dtype ( dtype ) dtypert [ 1 ] = flatten dtypert [ 2 ] = numpy . dtype ( [ ( 'a' , ( numpy . int8 , flatten . itemsize ) ) ] ) buf = numpy . empty ( ( ) , dtype = dtypert [ 1 ] ) converters = [ _default_conv [ flatten [ name ] . char ] for name in flatten . names ] return buf , converters , flatten . names def fileiter ( fh ) : converters = [ ] buf = None if dtype is not None : buf , converters , names = preparedtype ( dtype ) yield None for lineno , line in enumerate ( fh ) : if lineno < skiplines : continue if line [ 0 ] in comment_character : if buf is None and line [ 1 ] == '?' : ddtype = pickle . loads ( base64 . b64decode ( line [ 2 : ] ) ) buf , converters , names = preparedtype ( ddtype ) yield None continue for word , c , name in zip ( line . split ( ) , converters , names ) : buf [ name ] = c ( word ) buf2 = buf . copy ( ) . view ( dtype = dtypert [ 2 ] ) yield buf2 if isinstance ( fname , basestring ) : fh = file ( fh , 'r' ) cleanup = lambda : fh . close ( ) else : fh = iter ( fname ) cleanup = lambda : None try : i = fileiter ( fh ) i . next ( ) return numpy . fromiter ( i , dtype = dtypert [ 2 ] ) . view ( dtype = dtypert [ 0 ] ) finally : cleanup ( )
10378	def calculate_concordance_probability ( graph : BELGraph , key : str , cutoff : Optional [ float ] = None , permutations : Optional [ int ] = None , percentage : Optional [ float ] = None , use_ambiguous : bool = False , permute_type : str = 'shuffle_node_data' , ) -> Tuple [ float , List [ float ] , float ] : if permute_type == 'random_by_edges' : permute_func = partial ( random_by_edges , percentage = percentage ) elif permute_type == 'shuffle_node_data' : permute_func = partial ( shuffle_node_data , key = key , percentage = percentage ) elif permute_type == 'shuffle_relations' : permute_func = partial ( shuffle_relations , percentage = percentage ) else : raise ValueError ( 'Invalid permute_type: {}' . format ( permute_type ) ) graph : BELGraph = graph . copy ( ) collapse_to_genes ( graph ) collapse_all_variants ( graph ) score = calculate_concordance ( graph , key , cutoff = cutoff ) distribution = [ ] for _ in range ( permutations or 500 ) : permuted_graph = permute_func ( graph ) permuted_graph_scores = calculate_concordance ( permuted_graph , key , cutoff = cutoff , use_ambiguous = use_ambiguous ) distribution . append ( permuted_graph_scores ) return score , distribution , one_sided ( score , distribution )
1878	def MOVSS ( cpu , dest , src ) : if dest . type == 'register' and src . type == 'register' : assert dest . size == 128 and src . size == 128 dest . write ( dest . read ( ) & ~ 0xffffffff | src . read ( ) & 0xffffffff ) elif dest . type == 'memory' : assert src . type == 'register' dest . write ( Operators . EXTRACT ( src . read ( ) , 0 , dest . size ) ) else : assert src . type == 'memory' and dest . type == 'register' assert src . size == 32 and dest . size == 128 dest . write ( Operators . ZEXTEND ( src . read ( ) , 128 ) )
3127	def update ( self , template_id , data ) : if 'name' not in data : raise KeyError ( 'The template must have a name' ) if 'html' not in data : raise KeyError ( 'The template must have html' ) self . template_id = template_id return self . _mc_client . _patch ( url = self . _build_path ( template_id ) , data = data )
5412	def build_action ( name = None , image_uri = None , commands = None , entrypoint = None , environment = None , pid_namespace = None , flags = None , port_mappings = None , mounts = None , labels = None ) : return { 'name' : name , 'imageUri' : image_uri , 'commands' : commands , 'entrypoint' : entrypoint , 'environment' : environment , 'pidNamespace' : pid_namespace , 'flags' : flags , 'portMappings' : port_mappings , 'mounts' : mounts , 'labels' : labels , }
13096	def watch ( self ) : wm = pyinotify . WatchManager ( ) self . notifier = pyinotify . Notifier ( wm , default_proc_fun = self . callback ) wm . add_watch ( self . directory , pyinotify . ALL_EVENTS ) try : self . notifier . loop ( ) except ( KeyboardInterrupt , AttributeError ) : print_notification ( "Stopping" ) finally : self . notifier . stop ( ) self . terminate_processes ( )
11390	def register_field ( cls , field ) : FieldRegistry . add_field ( cls , field ) signals . post_save . connect ( handle_save_embeds , sender = cls , dispatch_uid = '%s.%s.%s' % ( cls . _meta . app_label , cls . _meta . module_name , field . name ) )
6477	def render ( self , stream ) : encoding = self . option . encoding or self . term . encoding or "utf8" if self . option . color : ramp = self . color_ramp ( self . size . y ) [ : : - 1 ] else : ramp = None if self . cycle >= 1 and self . lines : stream . write ( self . term . csi ( 'cuu' , self . lines ) ) zero = int ( self . null / 4 ) lines = 0 for y in range ( self . screen . size . y ) : if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) for x in range ( self . screen . size . x ) : point = Point ( ( x , y ) ) if point in self . screen : value = self . screen [ point ] if isinstance ( value , int ) : stream . write ( chr ( self . base + value ) . encode ( encoding ) ) else : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( self . term . csi_wrap ( value . encode ( encoding ) , 'bold' ) ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) else : stream . write ( b' ' ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'rmul' ) ) if ramp : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( b'\n' ) lines += 1 stream . flush ( ) self . cycle = self . cycle + 1 self . lines = lines
12054	def inspectABF ( abf = exampleABF , saveToo = False , justPlot = False ) : pylab . close ( 'all' ) print ( " ~~ inspectABF()" ) if type ( abf ) is str : abf = swhlab . ABF ( abf ) swhlab . plot . new ( abf , forceNewFigure = True ) if abf . sweepInterval * abf . sweeps < 60 * 5 : pylab . subplot ( 211 ) pylab . title ( "%s [%s]" % ( abf . ID , abf . protoComment ) ) swhlab . plot . sweep ( abf , 'all' ) pylab . subplot ( 212 ) swhlab . plot . sweep ( abf , 'all' , continuous = True ) swhlab . plot . comments ( abf ) else : print ( " -- plotting as long recording" ) swhlab . plot . sweep ( abf , 'all' , continuous = True , minutes = True ) swhlab . plot . comments ( abf , minutes = True ) pylab . title ( "%s [%s]" % ( abf . ID , abf . protoComment ) ) swhlab . plot . annotate ( abf ) if justPlot : return if saveToo : path = os . path . split ( abf . fname ) [ 0 ] basename = os . path . basename ( abf . fname ) pylab . savefig ( os . path . join ( path , "_" + basename . replace ( ".abf" , ".png" ) ) ) pylab . show ( ) return
567	def __validateExperimentControl ( self , control ) : taskList = control . get ( 'tasks' , None ) if taskList is not None : taskLabelsList = [ ] for task in taskList : validateOpfJsonValue ( task , "opfTaskSchema.json" ) validateOpfJsonValue ( task [ 'taskControl' ] , "opfTaskControlSchema.json" ) taskLabel = task [ 'taskLabel' ] assert isinstance ( taskLabel , types . StringTypes ) , "taskLabel type: %r" % type ( taskLabel ) assert len ( taskLabel ) > 0 , "empty string taskLabel not is allowed" taskLabelsList . append ( taskLabel . lower ( ) ) taskLabelDuplicates = filter ( lambda x : taskLabelsList . count ( x ) > 1 , taskLabelsList ) assert len ( taskLabelDuplicates ) == 0 , "Duplcate task labels are not allowed: %s" % taskLabelDuplicates return
13375	def ensure_path_exists ( path , * args ) : if os . path . exists ( path ) : return os . makedirs ( path , * args )
7110	def fit ( self , X , y ) : trainer = pycrfsuite . Trainer ( verbose = True ) for xseq , yseq in zip ( X , y ) : trainer . append ( xseq , yseq ) trainer . set_params ( self . params ) if self . filename : filename = self . filename else : filename = 'model.tmp' trainer . train ( filename ) tagger = pycrfsuite . Tagger ( ) tagger . open ( filename ) self . estimator = tagger
3105	def code_verifier ( n_bytes = 64 ) : verifier = base64 . urlsafe_b64encode ( os . urandom ( n_bytes ) ) . rstrip ( b'=' ) if len ( verifier ) < 43 : raise ValueError ( "Verifier too short. n_bytes must be > 30." ) elif len ( verifier ) > 128 : raise ValueError ( "Verifier too long. n_bytes must be < 97." ) else : return verifier
9687	def read_gsc_sfr ( self ) : config = [ ] data = { } self . cnxn . xfer ( [ 0x33 ] ) sleep ( 10e-3 ) for i in range ( 8 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) data [ "GSC" ] = self . _calculate_float ( config [ 0 : 4 ] ) data [ "SFR" ] = self . _calculate_float ( config [ 4 : ] ) return data
11331	def progress ( length , ** kwargs ) : quiet = False progress_class = kwargs . pop ( "progress_class" , Progress ) kwargs [ "write_method" ] = istdout . info kwargs [ "width" ] = kwargs . get ( "width" , globals ( ) [ "WIDTH" ] ) kwargs [ "length" ] = length pbar = progress_class ( ** kwargs ) pbar . update ( 0 ) yield pbar pbar . update ( length ) br ( )
3772	def phase_select_property ( phase = None , s = None , l = None , g = None , V_over_F = None ) : r if phase == 's' : return s elif phase == 'l' : return l elif phase == 'g' : return g elif phase == 'two-phase' : return None elif phase is None : return None else : raise Exception ( 'Property not recognized' )
1282	def block_html ( self , html ) : if self . options . get ( 'skip_style' ) and html . lower ( ) . startswith ( '<style' ) : return '' if self . options . get ( 'escape' ) : return escape ( html ) return html
10897	def get_scale_from_raw ( raw , scaled ) : t0 , t1 = scaled . min ( ) , scaled . max ( ) r0 , r1 = float ( raw . min ( ) ) , float ( raw . max ( ) ) rmin = ( t1 * r0 - t0 * r1 ) / ( t1 - t0 ) rmax = ( r1 - r0 ) / ( t1 - t0 ) + rmin return ( rmin , rmax )
6603	def package_fullpath ( self , package_index ) : ret = os . path . join ( self . path , self . package_relpath ( package_index ) ) return ret
7574	def get_threaded_view ( ipyclient , split = True ) : eids = ipyclient . ids dview = ipyclient . direct_view ( ) hosts = dview . apply_sync ( socket . gethostname ) hostdict = defaultdict ( list ) for host , eid in zip ( hosts , eids ) : hostdict [ host ] . append ( eid ) hostdictkeys = hostdict . keys ( ) for key in hostdictkeys : gids = hostdict [ key ] maxt = len ( gids ) if len ( gids ) >= 4 : maxt = 2 if ( len ( gids ) == 4 ) and ( len ( hosts ) >= 4 ) : maxt = 4 if len ( gids ) >= 6 : maxt = 3 if len ( gids ) >= 8 : maxt = 4 if len ( gids ) >= 16 : maxt = 4 threaded = [ gids [ i : i + maxt ] for i in xrange ( 0 , len ( gids ) , maxt ) ] lth = len ( threaded ) if lth > 1 : hostdict . pop ( key ) for hostid in range ( lth ) : hostdict [ str ( key ) + "_" + str ( hostid ) ] = threaded [ hostid ] LOGGER . info ( "threaded_view: %s" , dict ( hostdict ) ) return hostdict
6841	def supported_locales ( ) : family = distrib_family ( ) if family == 'debian' : return _parse_locales ( '/usr/share/i18n/SUPPORTED' ) elif family == 'arch' : return _parse_locales ( '/etc/locale.gen' ) elif family == 'redhat' : return _supported_locales_redhat ( ) else : raise UnsupportedFamily ( supported = [ 'debian' , 'arch' , 'redhat' ] )
5784	def select_write ( self , timeout = None ) : _ , write_ready , _ = select . select ( [ ] , [ self . _socket ] , [ ] , timeout ) return len ( write_ready ) > 0
6655	def sometimesPruneCache ( p ) : def decorator ( fn ) : @ functools . wraps ( fn ) def wrapped ( * args , ** kwargs ) : r = fn ( * args , ** kwargs ) if random . random ( ) < p : pruneCache ( ) return r return wrapped return decorator
2632	def scale_out ( self , blocks = 1 ) : r = [ ] for i in range ( blocks ) : if self . provider : block = self . provider . submit ( self . launch_cmd , 1 , self . workers_per_node ) logger . debug ( "Launched block {}:{}" . format ( i , block ) ) if not block : raise ( ScalingFailed ( self . provider . label , "Attempts to provision nodes via provider has failed" ) ) self . engines . extend ( [ block ] ) r . extend ( [ block ] ) else : logger . error ( "No execution provider available" ) r = None return r
10384	def remove_inconsistent_edges ( graph : BELGraph ) -> None : for u , v in get_inconsistent_edges ( graph ) : edges = [ ( u , v , k ) for k in graph [ u ] [ v ] ] graph . remove_edges_from ( edges )
3416	def _get_id_compartment ( id ) : bracket_search = _bracket_re . findall ( id ) if len ( bracket_search ) == 1 : return bracket_search [ 0 ] [ 1 ] underscore_search = _underscore_re . findall ( id ) if len ( underscore_search ) == 1 : return underscore_search [ 0 ] [ 1 ] return None
8217	def trigger_fullscreen_action ( self , fullscreen ) : action = self . action_group . get_action ( 'fullscreen' ) action . set_active ( fullscreen )
3596	def bulkDetails ( self , packageNames ) : params = { 'au' : '1' } req = googleplay_pb2 . BulkDetailsRequest ( ) req . docid . extend ( packageNames ) data = req . SerializeToString ( ) message = self . executeRequestApi2 ( BULK_URL , post_data = data . decode ( "utf-8" ) , content_type = CONTENT_TYPE_PROTO , params = params ) response = message . payload . bulkDetailsResponse return [ None if not utils . hasDoc ( entry ) else utils . parseProtobufObj ( entry . doc ) for entry in response . entry ]
13326	def create ( name_or_path , config ) : if not name_or_path : ctx = click . get_current_context ( ) click . echo ( ctx . get_help ( ) ) examples = ( '\nExamples:\n' ' cpenv create my_env\n' ' cpenv create ./relative/path/to/my_env\n' ' cpenv create my_env --config ./relative/path/to/config\n' ' cpenv create my_env --config git@github.com:user/config.git\n' ) click . echo ( examples ) return click . echo ( blue ( 'Creating a new virtual environment ' + name_or_path ) ) try : env = cpenv . create ( name_or_path , config ) except Exception as e : click . echo ( bold_red ( 'FAILED TO CREATE ENVIRONMENT!' ) ) click . echo ( e ) else : click . echo ( bold_green ( 'Successfully created environment!' ) ) click . echo ( blue ( 'Launching subshell' ) ) cpenv . activate ( env ) shell . launch ( env . name )
5557	def _strip_zoom ( input_string , strip_string ) : try : return int ( input_string . strip ( strip_string ) ) except Exception as e : raise MapcheteConfigError ( "zoom level could not be determined: %s" % e )
12703	def _set_params ( target , param , values , dof ) : if not isinstance ( values , ( list , tuple , np . ndarray ) ) : values = [ values ] * dof assert dof == len ( values ) for s , value in zip ( [ '' , '2' , '3' ] [ : dof ] , values ) : target . setParam ( getattr ( ode , 'Param{}{}' . format ( param , s ) ) , value )
2154	def _read ( self , fp , fpname ) : if os . path . isfile ( fpname ) : file_permission = os . stat ( fpname ) if fpname != os . path . join ( tower_dir , 'tower_cli.cfg' ) and ( ( file_permission . st_mode & stat . S_IRGRP ) or ( file_permission . st_mode & stat . S_IROTH ) ) : warnings . warn ( 'File {0} readable by group or others.' . format ( fpname ) , RuntimeWarning ) try : return configparser . ConfigParser . _read ( self , fp , fpname ) except configparser . MissingSectionHeaderError : fp . seek ( 0 ) string = '[general]\n%s' % fp . read ( ) flo = StringIO ( string ) return configparser . ConfigParser . _read ( self , flo , fpname )
4055	def everything ( self , query ) : try : items = [ ] items . extend ( query ) while self . links . get ( "next" ) : items . extend ( self . follow ( ) ) except TypeError : items = copy . deepcopy ( query ) while self . links . get ( "next" ) : items . entries . extend ( self . follow ( ) . entries ) return items
7029	def generalized_lsp_value_notau ( times , mags , errs , omega ) : one_over_errs2 = 1.0 / ( errs * errs ) W = npsum ( one_over_errs2 ) wi = one_over_errs2 / W sin_omegat = npsin ( omega * times ) cos_omegat = npcos ( omega * times ) sin2_omegat = sin_omegat * sin_omegat cos2_omegat = cos_omegat * cos_omegat sincos_omegat = sin_omegat * cos_omegat Y = npsum ( wi * mags ) C = npsum ( wi * cos_omegat ) S = npsum ( wi * sin_omegat ) YpY = npsum ( wi * mags * mags ) YpC = npsum ( wi * mags * cos_omegat ) YpS = npsum ( wi * mags * sin_omegat ) CpC = npsum ( wi * cos2_omegat ) CpS = npsum ( wi * sincos_omegat ) YY = YpY - Y * Y YC = YpC - Y * C YS = YpS - Y * S CC = CpC - C * C SS = 1 - CpC - S * S CS = CpS - C * S Domega = CC * SS - CS * CS lspval = ( SS * YC * YC + CC * YS * YS - 2.0 * CS * YC * YS ) / ( YY * Domega ) return lspval
5192	def send_select_and_operate_command ( self , command , index , callback = asiodnp3 . PrintingCommandCallback . Get ( ) , config = opendnp3 . TaskConfig ( ) . Default ( ) ) : self . master . SelectAndOperate ( command , index , callback , config )
3746	def calculate_P ( self , T , P , method ) : r if method == LUCAS : mu = self . T_dependent_property ( T ) Psat = self . Psat ( T ) if hasattr ( self . Psat , '__call__' ) else self . Psat mu = Lucas ( T , P , self . Tc , self . Pc , self . omega , Psat , mu ) elif method == COOLPROP : mu = PropsSI ( 'V' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : mu = self . interpolate_P ( T , P , method ) return mu
7296	def get_widget ( model_field , disabled = False ) : attrs = get_attrs ( model_field , disabled ) if hasattr ( model_field , "max_length" ) and not model_field . max_length : return forms . Textarea ( attrs = attrs ) elif isinstance ( model_field , DateTimeField ) : return forms . DateTimeInput ( attrs = attrs ) elif isinstance ( model_field , BooleanField ) : return forms . CheckboxInput ( attrs = attrs ) elif isinstance ( model_field , ReferenceField ) or model_field . choices : return forms . Select ( attrs = attrs ) elif ( isinstance ( model_field , ListField ) or isinstance ( model_field , EmbeddedDocumentField ) or isinstance ( model_field , GeoPointField ) ) : return None else : return forms . TextInput ( attrs = attrs )
3604	def _authenticate ( self , params , headers ) : if self . authentication : user = self . authentication . get_user ( ) params . update ( { 'auth' : user . firebase_auth_token } ) headers . update ( self . authentication . authenticator . HEADERS )
12108	def _launch_all ( self , launchers ) : for launcher in launchers : print ( "== Launching %s ==" % launcher . batch_name ) launcher ( ) return True
4782	def is_close_to ( self , other , tolerance ) : self . _validate_close_to_args ( self . val , other , tolerance ) if self . val < ( other - tolerance ) or self . val > ( other + tolerance ) : if type ( self . val ) is datetime . datetime : tolerance_seconds = tolerance . days * 86400 + tolerance . seconds + tolerance . microseconds / 1000000 h , rem = divmod ( tolerance_seconds , 3600 ) m , s = divmod ( rem , 60 ) self . _err ( 'Expected <%s> to be close to <%s> within tolerance <%d:%02d:%02d>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) , h , m , s ) ) else : self . _err ( 'Expected <%s> to be close to <%s> within tolerance <%s>, but was not.' % ( self . val , other , tolerance ) ) return self
8860	def goto_assignments ( request_data ) : code = request_data [ 'code' ] line = request_data [ 'line' ] + 1 column = request_data [ 'column' ] path = request_data [ 'path' ] encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) try : definitions = script . goto_assignments ( ) except jedi . NotFoundError : pass else : ret_val = [ ( d . module_path , d . line - 1 if d . line else None , d . column , d . full_name ) for d in definitions ] return ret_val
10674	def load_data_auxi ( path = '' ) : compounds . clear ( ) if path == '' : path = default_data_path if not os . path . exists ( path ) : warnings . warn ( 'The specified data file path does not exist. (%s)' % path ) return files = glob . glob ( os . path . join ( path , 'Compound_*.json' ) ) for file in files : compound = Compound . read ( file ) compounds [ compound . formula ] = compound
13163	def serialize_text ( out , text ) : padding = len ( out ) add_padding = padding_adder ( padding ) text = add_padding ( text , ignore_first_line = True ) return out + text
1058	def remove_extension ( module , name , code ) : key = ( module , name ) if ( _extension_registry . get ( key ) != code or _inverted_registry . get ( code ) != key ) : raise ValueError ( "key %s is not registered with code %s" % ( key , code ) ) del _extension_registry [ key ] del _inverted_registry [ code ] if code in _extension_cache : del _extension_cache [ code ]
7360	def _check_hla_alleles ( alleles , valid_alleles = None ) : require_iterable_of ( alleles , string_types , "HLA alleles" ) alleles = { normalize_allele_name ( allele . strip ( ) . upper ( ) ) for allele in alleles } if valid_alleles : missing_alleles = [ allele for allele in alleles if allele not in valid_alleles ] if len ( missing_alleles ) > 0 : raise UnsupportedAllele ( "Unsupported HLA alleles: %s" % missing_alleles ) return list ( alleles )
8980	def temporary_path ( self , extension = "" ) : path = NamedTemporaryFile ( delete = False , suffix = extension ) . name self . path ( path ) return path
4907	def _sync_content_metadata ( self , serialized_data , http_method ) : try : status_code , response_body = getattr ( self , '_' + http_method ) ( urljoin ( self . enterprise_configuration . degreed_base_url , self . global_degreed_config . course_api_path ) , serialized_data , self . CONTENT_PROVIDER_SCOPE ) except requests . exceptions . RequestException as exc : raise ClientError ( 'DegreedAPIClient request failed: {error} {message}' . format ( error = exc . __class__ . __name__ , message = str ( exc ) ) ) if status_code >= 400 : raise ClientError ( 'DegreedAPIClient request failed with status {status_code}: {message}' . format ( status_code = status_code , message = response_body ) )
11584	def background_image_finder ( pipeline_index , soup , finder_image_urls = [ ] , * args , ** kwargs ) : now_finder_image_urls = [ ] for tag in soup . find_all ( style = True ) : style_string = tag [ 'style' ] if 'background-image' in style_string . lower ( ) : style = cssutils . parseStyle ( style_string ) background_image = style . getProperty ( 'background-image' ) if background_image : for property_value in background_image . propertyValue : background_image_url = str ( property_value . value ) if background_image_url : if ( background_image_url not in finder_image_urls ) and ( background_image_url not in now_finder_image_urls ) : now_finder_image_urls . append ( background_image_url ) output = { } output [ 'finder_image_urls' ] = finder_image_urls + now_finder_image_urls return output
10311	def prepare_c3 ( data : Union [ List [ Tuple [ str , int ] ] , Mapping [ str , int ] ] , y_axis_label : str = 'y' , x_axis_label : str = 'x' , ) -> str : if not isinstance ( data , list ) : data = sorted ( data . items ( ) , key = itemgetter ( 1 ) , reverse = True ) try : labels , values = zip ( * data ) except ValueError : log . info ( f'no values found for {x_axis_label}, {y_axis_label}' ) labels , values = [ ] , [ ] return json . dumps ( [ [ x_axis_label ] + list ( labels ) , [ y_axis_label ] + list ( values ) , ] )
3682	def logP ( CASRN , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in CRClogPDict . index : methods . append ( CRC ) if CASRN in SyrresDict2 . index : methods . append ( SYRRES ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == CRC : return float ( CRClogPDict . at [ CASRN , 'logP' ] ) elif Method == SYRRES : return float ( SyrresDict2 . at [ CASRN , 'logP' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
10905	def trisect_image ( imshape , edgepts = 'calc' ) : im_x , im_y = np . meshgrid ( np . arange ( imshape [ 0 ] ) , np . arange ( imshape [ 1 ] ) , indexing = 'ij' ) if np . size ( edgepts ) == 1 : f = np . sqrt ( 2. / 3. ) if edgepts == 'calc' else edgepts lower_edge = ( imshape [ 0 ] * ( 1 - f ) , imshape [ 1 ] * f ) upper_edge = ( imshape [ 0 ] * f , imshape [ 1 ] * ( 1 - f ) ) else : upper_edge , lower_edge = edgepts lower_slope = lower_edge [ 1 ] / max ( float ( imshape [ 0 ] - lower_edge [ 0 ] ) , 1e-9 ) upper_slope = ( imshape [ 1 ] - upper_edge [ 1 ] ) / float ( upper_edge [ 0 ] ) lower_intercept = - lower_slope * lower_edge [ 0 ] upper_intercept = upper_edge [ 1 ] lower_mask = im_y < ( im_x * lower_slope + lower_intercept ) upper_mask = im_y > ( im_x * upper_slope + upper_intercept ) center_mask = - ( lower_mask | upper_mask ) return upper_mask , center_mask , lower_mask
226	def get_top_long_short_abs ( positions , top = 10 ) : positions = positions . drop ( 'cash' , axis = 'columns' ) df_max = positions . max ( ) df_min = positions . min ( ) df_abs_max = positions . abs ( ) . max ( ) df_top_long = df_max [ df_max > 0 ] . nlargest ( top ) df_top_short = df_min [ df_min < 0 ] . nsmallest ( top ) df_top_abs = df_abs_max . nlargest ( top ) return df_top_long , df_top_short , df_top_abs
13244	def temp_db ( db , name = None ) : if name is None : name = temp_name ( ) db . create ( name ) if not db . exists ( name ) : raise DatabaseError ( 'failed to create database %s!' ) try : yield name finally : db . drop ( name ) if db . exists ( name ) : raise DatabaseError ( 'failed to drop database %s!' )
12425	def reverse ( self ) : if not self . test_drive and self . bumps : map ( lambda b : b . reverse ( ) , self . bumpers )
13181	def _get_printable_columns ( columns , row ) : if not columns : return row return tuple ( row [ c ] for c in columns )
2257	def allsame ( iterable , eq = operator . eq ) : iter_ = iter ( iterable ) try : first = next ( iter_ ) except StopIteration : return True return all ( eq ( first , item ) for item in iter_ )
5900	def commandline ( self , ** mpiargs ) : cmd = self . MDRUN . commandline ( ) if self . mpiexec : cmd = self . mpicommand ( ** mpiargs ) + cmd return cmd
7543	def chunk_clusters ( data , sample ) : num = 0 optim = int ( ( sample . stats . clusters_total // data . cpus ) + ( sample . stats . clusters_total % data . cpus ) ) chunkslist = [ ] with gzip . open ( sample . files . clusters , 'rb' ) as clusters : pairdealer = itertools . izip ( * [ iter ( clusters ) ] * 2 ) done = 0 while not done : done , chunk = clustdealer ( pairdealer , optim ) chunkhandle = os . path . join ( data . dirs . clusts , "tmp_" + str ( sample . name ) + "." + str ( num * optim ) ) if chunk : chunkslist . append ( ( optim , chunkhandle ) ) with open ( chunkhandle , 'wb' ) as outchunk : outchunk . write ( "//\n//\n" . join ( chunk ) + "//\n//\n" ) num += 1 return chunkslist
1326	def from_keras ( cls , model , bounds , input_shape = None , channel_axis = 3 , preprocessing = ( 0 , 1 ) ) : import tensorflow as tf if input_shape is None : try : input_shape = model . input_shape [ 1 : ] except AttributeError : raise ValueError ( 'Please specify input_shape manually or ' 'provide a model with an input_shape attribute' ) with tf . keras . backend . get_session ( ) . as_default ( ) : inputs = tf . placeholder ( tf . float32 , ( None , ) + input_shape ) logits = model ( inputs ) return cls ( inputs , logits , bounds = bounds , channel_axis = channel_axis , preprocessing = preprocessing )
4354	def _save_ack_callback ( self , msgid , callback ) : if msgid in self . ack_callbacks : return False self . ack_callbacks [ msgid ] = callback
1670	def ProcessFileData ( filename , file_extension , lines , error , extra_check_functions = None ) : lines = ( [ '// marker so line numbers and indices both start at 1' ] + lines + [ '// marker so line numbers end in a known way' ] ) include_state = _IncludeState ( ) function_state = _FunctionState ( ) nesting_state = NestingState ( ) ResetNolintSuppressions ( ) CheckForCopyright ( filename , lines , error ) ProcessGlobalSuppresions ( lines ) RemoveMultiLineComments ( filename , lines , error ) clean_lines = CleansedLines ( lines ) if file_extension in GetHeaderExtensions ( ) : CheckForHeaderGuard ( filename , clean_lines , error ) for line in range ( clean_lines . NumLines ( ) ) : ProcessLine ( filename , file_extension , clean_lines , line , include_state , function_state , nesting_state , error , extra_check_functions ) FlagCxx11Features ( filename , clean_lines , line , error ) nesting_state . CheckCompletedBlocks ( filename , error ) CheckForIncludeWhatYouUse ( filename , clean_lines , include_state , error ) if _IsSourceExtension ( file_extension ) : CheckHeaderFileIncluded ( filename , include_state , error ) CheckForBadCharacters ( filename , lines , error ) CheckForNewlineAtEOF ( filename , lines , error )
9432	def _c_func ( func , restype , argtypes , errcheck = None ) : func . restype = restype func . argtypes = argtypes if errcheck is not None : func . errcheck = errcheck return func
6695	def upgrade ( safe = True ) : manager = MANAGER if safe : cmd = 'upgrade' else : cmd = 'dist-upgrade' run_as_root ( "%(manager)s --assume-yes %(cmd)s" % locals ( ) , pty = False )
8611	def get_volume ( self , datacenter_id , volume_id ) : response = self . _perform_request ( '/datacenters/%s/volumes/%s' % ( datacenter_id , volume_id ) ) return response
8720	def operation_upload ( uploader , sources , verify , do_compile , do_file , do_restart ) : sources , destinations = destination_from_source ( sources ) if len ( destinations ) == len ( sources ) : if uploader . prepare ( ) : for filename , dst in zip ( sources , destinations ) : if do_compile : uploader . file_remove ( os . path . splitext ( dst ) [ 0 ] + '.lc' ) uploader . write_file ( filename , dst , verify ) if do_compile and dst != 'init.lua' : uploader . file_compile ( dst ) uploader . file_remove ( dst ) if do_file : uploader . file_do ( os . path . splitext ( dst ) [ 0 ] + '.lc' ) elif do_file : uploader . file_do ( dst ) else : raise Exception ( 'Error preparing nodemcu for reception' ) else : raise Exception ( 'You must specify a destination filename for each file you want to upload.' ) if do_restart : uploader . node_restart ( ) log . info ( 'All done!' )
1937	def get_abi ( self , hsh : bytes ) -> Dict [ str , Any ] : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) sig = self . _function_signatures_by_selector . get ( hsh ) if sig is not None : return dict ( self . _function_abi_items_by_signature [ sig ] ) item = self . _fallback_function_abi_item if item is not None : return dict ( item ) return { 'payable' : False , 'stateMutability' : 'nonpayable' , 'type' : 'fallback' }
596	def _buildArgs ( f , self = None , kwargs = { } ) : argTuples = getArgumentDescriptions ( f ) argTuples = argTuples [ 1 : ] init = TMRegion . __init__ ourArgNames = [ t [ 0 ] for t in getArgumentDescriptions ( init ) ] ourArgNames += [ 'numberOfCols' , ] for argTuple in argTuples [ : ] : if argTuple [ 0 ] in ourArgNames : argTuples . remove ( argTuple ) if self : for argTuple in argTuples : argName = argTuple [ 0 ] if argName in kwargs : argValue = kwargs . pop ( argName ) else : if len ( argTuple ) == 2 : raise TypeError ( "Must provide '%s'" % argName ) argValue = argTuple [ 2 ] setattr ( self , argName , argValue ) return argTuples
10590	def report ( self , format = ReportFormat . printout , output_path = None ) : rpt = GlsRpt ( self , output_path ) return rpt . render ( format )
5683	def tripI_takes_place_on_dsut ( self , trip_I , day_start_ut ) : query = "SELECT * FROM days WHERE trip_I=? AND day_start_ut=?" params = ( trip_I , day_start_ut ) cur = self . conn . cursor ( ) rows = list ( cur . execute ( query , params ) ) if len ( rows ) == 0 : return False else : assert len ( rows ) == 1 , 'On a day, a trip_I should be present at most once' return True
12376	def assert_operations ( self , * args ) : if not set ( args ) . issubset ( self . allowed_operations ) : raise http . exceptions . Forbidden ( )
4296	def parse_config_file ( parser , stdin_args ) : config_args = [ ] required_args = [ ] for action in parser . _actions : if action . required : required_args . append ( action ) action . required = False parsed_args = parser . parse_args ( stdin_args ) for action in required_args : action . required = True if not parsed_args . config_file : return config_args config = ConfigParser ( ) if not config . read ( parsed_args . config_file ) : sys . stderr . write ( 'Config file "{0}" doesn\'t exists\n' . format ( parsed_args . config_file ) ) sys . exit ( 7 ) config_args = _convert_config_to_stdin ( config , parser ) return config_args
13455	def _parse_args ( args ) : parser = argparse . ArgumentParser ( description = "Remove and/or rearrange " + "sections from each line of a file(s)." , usage = _usage ( ) [ len ( 'usage: ' ) : ] ) parser . add_argument ( '-b' , "--bytes" , action = 'store' , type = lst , default = [ ] , help = "Bytes to select" ) parser . add_argument ( '-c' , "--chars" , action = 'store' , type = lst , default = [ ] , help = "Character to select" ) parser . add_argument ( '-f' , "--fields" , action = 'store' , type = lst , default = [ ] , help = "Fields to select" ) parser . add_argument ( '-d' , "--delimiter" , action = 'store' , default = "\t" , help = "Sets field delimiter(default is TAB)" ) parser . add_argument ( '-e' , "--regex" , action = 'store_true' , help = 'Enable regular expressions to be used as input ' + 'delimiter' ) parser . add_argument ( '-s' , '--skip' , action = 'store_true' , help = "Skip lines that do not contain input delimiter." ) parser . add_argument ( '-S' , "--separator" , action = 'store' , default = "\t" , help = "Sets field separator for output." ) parser . add_argument ( 'file' , nargs = '*' , default = "-" , help = "File(s) to cut" ) return parser . parse_args ( args )
12505	def signed_session ( self , session = None ) : if session : session = super ( ClientCertAuthentication , self ) . signed_session ( session ) else : session = super ( ClientCertAuthentication , self ) . signed_session ( ) if self . cert is not None : session . cert = self . cert if self . ca_cert is not None : session . verify = self . ca_cert if self . no_verify : session . verify = False return session
1045	def float_pack ( x , size ) : if size == 8 : MIN_EXP = - 1021 MAX_EXP = 1024 MANT_DIG = 53 BITS = 64 elif size == 4 : MIN_EXP = - 125 MAX_EXP = 128 MANT_DIG = 24 BITS = 32 else : raise ValueError ( "invalid size value" ) sign = math . copysign ( 1.0 , x ) < 0.0 if math . isinf ( x ) : mant = 0 exp = MAX_EXP - MIN_EXP + 2 elif math . isnan ( x ) : mant = 1 << ( MANT_DIG - 2 ) exp = MAX_EXP - MIN_EXP + 2 elif x == 0.0 : mant = 0 exp = 0 else : m , e = math . frexp ( abs ( x ) ) exp = e - ( MIN_EXP - 1 ) if exp > 0 : mant = round_to_nearest ( m * ( 1 << MANT_DIG ) ) mant -= 1 << MANT_DIG - 1 else : if exp + MANT_DIG - 1 >= 0 : mant = round_to_nearest ( m * ( 1 << exp + MANT_DIG - 1 ) ) else : mant = 0 exp = 0 assert 0 <= mant <= 1 << MANT_DIG - 1 if mant == 1 << MANT_DIG - 1 : mant = 0 exp += 1 if exp >= MAX_EXP - MIN_EXP + 2 : raise OverflowError ( "float too large to pack in this format" ) assert 0 <= mant < 1 << MANT_DIG - 1 assert 0 <= exp <= MAX_EXP - MIN_EXP + 2 assert 0 <= sign <= 1 return ( ( sign << BITS - 1 ) | ( exp << MANT_DIG - 1 ) ) | mant
7685	def clicks ( annotation , sr = 22050 , length = None , ** kwargs ) : interval , _ = annotation . to_interval_values ( ) return filter_kwargs ( mir_eval . sonify . clicks , interval [ : , 0 ] , fs = sr , length = length , ** kwargs )
5987	def compute_deflections_at_next_plane ( plane_index , total_planes ) : if plane_index < total_planes - 1 : return True elif plane_index == total_planes - 1 : return False else : raise exc . RayTracingException ( 'A galaxy was not correctly allocated its previous / next redshifts' )
9691	def stop ( self ) : if self . receiver != None : self . receiver . join ( ) for s in self . senders . values ( ) : s . join ( )
1220	def reset ( self ) : fetches = [ ] for processor in self . preprocessors : fetches . extend ( processor . reset ( ) or [ ] ) return fetches
638	def getString ( cls , prop ) : if cls . _properties is None : cls . _readStdConfigFiles ( ) envValue = os . environ . get ( "%s%s" % ( cls . envPropPrefix , prop . replace ( '.' , '_' ) ) , None ) if envValue is not None : return envValue return cls . _properties [ prop ]
13711	def invalidate_ip ( self , ip ) : if self . _use_cache : key = self . _make_cache_key ( ip ) self . _cache . delete ( key , version = self . _cache_version )
8495	def _get_module_filename ( module ) : module = module . split ( '.' ) package = '.' . join ( module [ : - 1 ] ) module = module [ - 1 ] try : if not package : module = __import__ ( module ) else : package = __import__ ( package , fromlist = [ module ] ) module = getattr ( package , module , None ) filename = getattr ( module , '__file__' , None ) if not filename : return Unparseable ( ) if filename . endswith ( '.pyc' ) : filename = filename [ : - 1 ] if not os . path . exists ( filename ) and os . path . isfile ( filename ) : return Unparseable ( ) if filename . endswith ( '__init__.py' ) : filename = filename [ : - 11 ] return filename except ImportError : return
7243	def geotiff ( self , ** kwargs ) : if 'proj' not in kwargs : kwargs [ 'proj' ] = self . proj return to_geotiff ( self , ** kwargs )
5840	def check_predict_status ( self , view_id , predict_request_id ) : failure_message = "Get status on predict failed" bare_response = self . _get_success_json ( self . _get ( 'v1/data_views/' + str ( view_id ) + '/predict/' + str ( predict_request_id ) + '/status' , None , failure_message = failure_message ) ) result = bare_response [ "data" ] return result
12159	def abfGroupFiles ( groups , folder ) : assert os . path . exists ( folder ) files = os . listdir ( folder ) group2 = { } for parent in groups . keys ( ) : if not parent in group2 . keys ( ) : group2 [ parent ] = [ ] for ID in groups [ parent ] : for fname in [ x . lower ( ) for x in files if ID in x . lower ( ) ] : group2 [ parent ] . extend ( [ fname ] ) return group2
6376	def dist_abs ( self , src , tar , max_offset = 5 ) : if not src : return len ( tar ) if not tar : return len ( src ) src_len = len ( src ) tar_len = len ( tar ) src_cur = 0 tar_cur = 0 lcss = 0 local_cs = 0 while ( src_cur < src_len ) and ( tar_cur < tar_len ) : if src [ src_cur ] == tar [ tar_cur ] : local_cs += 1 else : lcss += local_cs local_cs = 0 if src_cur != tar_cur : src_cur = tar_cur = max ( src_cur , tar_cur ) for i in range ( max_offset ) : if not ( ( src_cur + i < src_len ) or ( tar_cur + i < tar_len ) ) : break if ( src_cur + i < src_len ) and ( src [ src_cur + i ] == tar [ tar_cur ] ) : src_cur += i local_cs += 1 break if ( tar_cur + i < tar_len ) and ( src [ src_cur ] == tar [ tar_cur + i ] ) : tar_cur += i local_cs += 1 break src_cur += 1 tar_cur += 1 lcss += local_cs return round ( max ( src_len , tar_len ) - lcss )
8525	def log_callback ( wrapped_function ) : def debug_log ( message ) : logger . debug ( message . encode ( 'unicode_escape' ) . decode ( ) ) @ functools . wraps ( wrapped_function ) def _wrapper ( parser , match , ** kwargs ) : func_name = wrapped_function . __name__ debug_log ( u'{func_name} <- {matched_string}' . format ( func_name = func_name , matched_string = match . group ( ) , ) ) try : result = wrapped_function ( parser , match , ** kwargs ) except IgnoredMatchException : debug_log ( u'{func_name} -> IGNORED' . format ( func_name = func_name ) ) raise debug_log ( u'{func_name} -> {result}' . format ( func_name = func_name , result = result , ) ) return result return _wrapper
9044	def listen ( self , func ) : self . _C0 . listen ( func ) self . _C1 . listen ( func )
5439	def _interval_to_seconds ( interval , valid_units = 'smhdw' ) : if not interval : return None try : last_char = interval [ - 1 ] if last_char == 's' and 's' in valid_units : return str ( float ( interval [ : - 1 ] ) ) + 's' elif last_char == 'm' and 'm' in valid_units : return str ( float ( interval [ : - 1 ] ) * 60 ) + 's' elif last_char == 'h' and 'h' in valid_units : return str ( float ( interval [ : - 1 ] ) * 60 * 60 ) + 's' elif last_char == 'd' and 'd' in valid_units : return str ( float ( interval [ : - 1 ] ) * 60 * 60 * 24 ) + 's' elif last_char == 'w' and 'w' in valid_units : return str ( float ( interval [ : - 1 ] ) * 60 * 60 * 24 * 7 ) + 's' else : raise ValueError ( 'Unsupported units in interval string %s: %s' % ( interval , last_char ) ) except ( ValueError , OverflowError ) as e : raise ValueError ( 'Unable to parse interval string %s: %s' % ( interval , e ) )
5226	def _to_gen_ ( iterable ) : from collections import Iterable for elm in iterable : if isinstance ( elm , Iterable ) and not isinstance ( elm , ( str , bytes ) ) : yield from flatten ( elm ) else : yield elm
2982	def cmd_add ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) b . add_container ( opts . containers )
152	def get_intersections ( self ) : if Real is float : return list ( self . intersections . keys ( ) ) else : return [ ( float ( p [ 0 ] ) , float ( p [ 1 ] ) ) for p in self . intersections . keys ( ) ]
4094	def AIC ( N , rho , k ) : r from numpy import log , array res = N * log ( array ( rho ) ) + 2. * ( array ( k ) + 1 ) return res
2153	def config_from_environment ( ) : kwargs = { } for k in CONFIG_OPTIONS : env = 'TOWER_' + k . upper ( ) v = os . getenv ( env , None ) if v is not None : kwargs [ k ] = v return kwargs
2256	def boolmask ( indices , maxval = None ) : if maxval is None : indices = list ( indices ) maxval = max ( indices ) + 1 mask = [ False ] * maxval for index in indices : mask [ index ] = True return mask
6522	def add_issues ( self , issues ) : if not isinstance ( issues , ( list , tuple ) ) : issues = [ issues ] with self . _lock : self . _all_issues . extend ( issues ) self . _cleaned_issues = None
7624	def pattern_to_mireval ( ann ) : patterns = defaultdict ( lambda : defaultdict ( list ) ) for time , observation in zip ( * ann . to_event_values ( ) ) : pattern_id = observation [ 'pattern_id' ] occurrence_id = observation [ 'occurrence_id' ] obs = ( time , observation [ 'midi_pitch' ] ) patterns [ pattern_id ] [ occurrence_id ] . append ( obs ) return [ list ( _ . values ( ) ) for _ in six . itervalues ( patterns ) ]
4947	def send_course_completion_statement ( lrs_configuration , user , course_overview , course_grade ) : user_details = LearnerInfoSerializer ( user ) course_details = CourseInfoSerializer ( course_overview ) statement = LearnerCourseCompletionStatement ( user , course_overview , user_details . data , course_details . data , course_grade , ) EnterpriseXAPIClient ( lrs_configuration ) . save_statement ( statement )
8531	def of_messages ( cls , msg_a , msg_b ) : ok_to_diff , reason = cls . can_diff ( msg_a , msg_b ) if not ok_to_diff : raise ValueError ( reason ) return [ cls . of_structs ( x . value , y . value ) for x , y in zip ( msg_a . args , msg_b . args ) if x . field_type == 'struct' ]
11961	def is_wildcard_nm ( nm ) : try : dec = 0xFFFFFFFF - _dot_to_dec ( nm , check = True ) except ValueError : return False if dec in _NETMASKS_VALUES : return True return False
13580	def dmap ( fn , record ) : values = ( fn ( v ) for k , v in record . items ( ) ) return dict ( itertools . izip ( record , values ) )
10527	def cast_to_list ( position ) : @ wrapt . decorator def wrapper ( function , instance , args , kwargs ) : if not isinstance ( args [ position ] , list ) : args = list ( args ) args [ position ] = [ args [ position ] ] args = tuple ( args ) return function ( * args , ** kwargs ) return wrapper
10813	def search ( cls , query , q ) : return query . filter ( Group . name . like ( '%{0}%' . format ( q ) ) )
6119	def circular_anti_annular ( cls , shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , outer_radius_2_arcsec , centre = ( 0. , 0. ) , invert = False ) : mask = mask_util . mask_circular_anti_annular_from_shape_pixel_scale_and_radii ( shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , outer_radius_2_arcsec , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
11381	def do_autodiscover ( parser , token ) : args = token . split_contents ( ) if len ( args ) != 2 : raise template . TemplateSyntaxError ( '%s takes an object as its parameter.' % args [ 0 ] ) else : obj = args [ 1 ] return OEmbedAutodiscoverNode ( obj )
223	def build_environ ( scope : Scope , body : bytes ) -> dict : environ = { "REQUEST_METHOD" : scope [ "method" ] , "SCRIPT_NAME" : scope . get ( "root_path" , "" ) , "PATH_INFO" : scope [ "path" ] , "QUERY_STRING" : scope [ "query_string" ] . decode ( "ascii" ) , "SERVER_PROTOCOL" : f"HTTP/{scope['http_version']}" , "wsgi.version" : ( 1 , 0 ) , "wsgi.url_scheme" : scope . get ( "scheme" , "http" ) , "wsgi.input" : io . BytesIO ( body ) , "wsgi.errors" : sys . stdout , "wsgi.multithread" : True , "wsgi.multiprocess" : True , "wsgi.run_once" : False , } server = scope . get ( "server" ) or ( "localhost" , 80 ) environ [ "SERVER_NAME" ] = server [ 0 ] environ [ "SERVER_PORT" ] = server [ 1 ] if scope . get ( "client" ) : environ [ "REMOTE_ADDR" ] = scope [ "client" ] [ 0 ] for name , value in scope . get ( "headers" , [ ] ) : name = name . decode ( "latin1" ) if name == "content-length" : corrected_name = "CONTENT_LENGTH" elif name == "content-type" : corrected_name = "CONTENT_TYPE" else : corrected_name = f"HTTP_{name}" . upper ( ) . replace ( "-" , "_" ) value = value . decode ( "latin1" ) if corrected_name in environ : value = environ [ corrected_name ] + "," + value environ [ corrected_name ] = value return environ
4069	def _validate ( self , conditions ) : allowed_keys = set ( self . searchkeys ) operators_set = set ( self . operators . keys ( ) ) for condition in conditions : if set ( condition . keys ( ) ) != allowed_keys : raise ze . ParamNotPassed ( "Keys must be all of: %s" % ", " . join ( self . searchkeys ) ) if condition . get ( "operator" ) not in operators_set : raise ze . ParamNotPassed ( "You have specified an unknown operator: %s" % condition . get ( "operator" ) ) permitted_operators = self . conditions_operators . get ( condition . get ( "condition" ) ) permitted_operators_list = set ( [ self . operators . get ( op ) for op in permitted_operators ] ) if condition . get ( "operator" ) not in permitted_operators_list : raise ze . ParamNotPassed ( "You may not use the '%s' operator when selecting the '%s' condition. \nAllowed operators: %s" % ( condition . get ( "operator" ) , condition . get ( "condition" ) , ", " . join ( list ( permitted_operators_list ) ) , ) )
13062	def get_siblings ( self , objectId , subreference , passage ) : reffs = [ reff for reff , _ in self . get_reffs ( objectId ) ] if subreference in reffs : index = reffs . index ( subreference ) if 0 < index < len ( reffs ) - 1 : return reffs [ index - 1 ] , reffs [ index + 1 ] elif index == 0 and index < len ( reffs ) - 1 : return None , reffs [ 1 ] elif index > 0 and index == len ( reffs ) - 1 : return reffs [ index - 1 ] , None else : return None , None else : return passage . siblingsId
8988	def last_consumed_mesh ( self ) : for instruction in reversed ( self . instructions ) : if instruction . consumes_meshes ( ) : return instruction . last_consumed_mesh raise IndexError ( "{} consumes no meshes" . format ( self ) )
4369	def send ( self , message , json = False , callback = None ) : pkt = dict ( type = "message" , data = message , endpoint = self . ns_name ) if json : pkt [ 'type' ] = "json" if callback : pkt [ 'ack' ] = True pkt [ 'id' ] = msgid = self . socket . _get_next_msgid ( ) self . socket . _save_ack_callback ( msgid , callback ) self . socket . send_packet ( pkt )
2484	def create_checksum_node ( self , chksum ) : chksum_node = BNode ( ) type_triple = ( chksum_node , RDF . type , self . spdx_namespace . Checksum ) self . graph . add ( type_triple ) algorithm_triple = ( chksum_node , self . spdx_namespace . algorithm , Literal ( chksum . identifier ) ) self . graph . add ( algorithm_triple ) value_triple = ( chksum_node , self . spdx_namespace . checksumValue , Literal ( chksum . value ) ) self . graph . add ( value_triple ) return chksum_node
5665	def evaluate_earliest_arrival_time_at_target ( self , dep_time , transfer_margin ) : minimum = dep_time + self . _walk_to_target_duration dep_time_plus_transfer_margin = dep_time + transfer_margin for label in self . _labels : if label . departure_time >= dep_time_plus_transfer_margin and label . arrival_time_target < minimum : minimum = label . arrival_time_target return float ( minimum )
2822	def convert_relu ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting relu ...' ) if names == 'short' : tf_name = 'RELU' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) relu = keras . layers . Activation ( 'relu' , name = tf_name ) layers [ scope_name ] = relu ( layers [ inputs [ 0 ] ] )
4223	def init_backend ( limit = None ) : backend . _limit = limit keyrings = filter ( limit , backend . get_all_keyring ( ) ) set_keyring ( load_env ( ) or load_config ( ) or max ( keyrings , default = fail . Keyring ( ) , key = backend . by_priority ) )
4115	def lar2rc ( g ) : assert numpy . isrealobj ( g ) , 'Log area ratios not defined for complex reflection coefficients.' return - numpy . tanh ( - numpy . array ( g ) / 2 )
12929	def as_dict ( self ) : self_as_dict = { 'chrom' : self . chrom , 'start' : self . start , 'ref_allele' : self . ref_allele , 'alt_alleles' : self . alt_alleles , 'alleles' : [ x . as_dict ( ) for x in self . alleles ] } try : self_as_dict [ 'info' ] = self . info except AttributeError : pass return self_as_dict
4603	def copy ( self ) : return self . __class__ ( amount = self [ "amount" ] , asset = self [ "asset" ] . copy ( ) , blockchain_instance = self . blockchain , )
9816	def check ( self ) : if not self . is_valid : raise PolyaxonDeploymentConfigError ( 'Deployment type `{}` not supported' . format ( self . deployment_type ) ) check = False if self . is_kubernetes : check = self . check_for_kubernetes ( ) elif self . is_docker_compose : check = self . check_for_docker_compose ( ) elif self . is_docker : check = self . check_for_docker ( ) elif self . is_heroku : check = self . check_for_heroku ( ) if not check : raise PolyaxonDeploymentConfigError ( 'Deployment `{}` is not valid' . format ( self . deployment_type ) )
1627	def CheckForCopyright ( filename , lines , error ) : for line in range ( 1 , min ( len ( lines ) , 11 ) ) : if re . search ( r'Copyright' , lines [ line ] , re . I ) : break else : error ( filename , 0 , 'legal/copyright' , 5 , 'No copyright message found. ' 'You should have a line: "Copyright [year] <Copyright Owner>"' )
2267	def invert_dict ( dict_ , unique_vals = True ) : r if unique_vals : if isinstance ( dict_ , OrderedDict ) : inverted = OrderedDict ( ( val , key ) for key , val in dict_ . items ( ) ) else : inverted = { val : key for key , val in dict_ . items ( ) } else : inverted = defaultdict ( set ) for key , value in dict_ . items ( ) : inverted [ value ] . add ( key ) inverted = dict ( inverted ) return inverted
5360	def execute_batch_tasks ( self , tasks_cls , big_delay = 0 , small_delay = 0 , wait_for_threads = True ) : def _split_tasks ( tasks_cls ) : backend_t = [ ] global_t = [ ] for t in tasks_cls : if t . is_backend_task ( t ) : backend_t . append ( t ) else : global_t . append ( t ) return backend_t , global_t backend_tasks , global_tasks = _split_tasks ( tasks_cls ) logger . debug ( 'backend_tasks = %s' % ( backend_tasks ) ) logger . debug ( 'global_tasks = %s' % ( global_tasks ) ) threads = [ ] stopper = threading . Event ( ) if len ( backend_tasks ) > 0 : repos_backend = self . _get_repos_by_backend ( ) for backend in repos_backend : t = TasksManager ( backend_tasks , backend , stopper , self . config , small_delay ) threads . append ( t ) t . start ( ) if len ( global_tasks ) > 0 : gt = TasksManager ( global_tasks , "Global tasks" , stopper , self . config , big_delay ) threads . append ( gt ) gt . start ( ) if big_delay > 0 : when = datetime . now ( ) + timedelta ( seconds = big_delay ) when_str = when . strftime ( '%a, %d %b %Y %H:%M:%S %Z' ) logger . info ( "%s will be executed on %s" % ( global_tasks , when_str ) ) if wait_for_threads : time . sleep ( 1 ) stopper . set ( ) for t in threads : t . join ( ) self . __check_queue_for_errors ( ) logger . debug ( "[thread:main] All threads (and their tasks) are finished" )
2780	def get_object ( cls , api_token , domain , record_id ) : record = cls ( token = api_token , domain = domain , id = record_id ) record . load ( ) return record
12057	def version_upload ( fname , username = "nibjb" ) : print ( "popping up pasword window..." ) password = TK_askPassword ( "FTP LOGIN" , "enter password for %s" % username ) if not password : return print ( "username:" , username ) print ( "password:" , "*" * ( len ( password ) ) ) print ( "connecting..." ) ftp = ftplib . FTP ( "swharden.com" ) ftp . login ( username , password ) print ( "successful login!" ) ftp . cwd ( "/software/swhlab/versions" ) print ( "uploading" , os . path . basename ( fname ) ) ftp . storbinary ( "STOR " + os . path . basename ( fname ) , open ( fname , "rb" ) , 1024 ) print ( "disconnecting..." ) ftp . quit ( )
10545	def delete_task ( task_id ) : try : res = _pybossa_req ( 'delete' , 'task' , task_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : raise
5846	def load_file_as_yaml ( path ) : with open ( path , "r" ) as f : raw_yaml = f . read ( ) parsed_dict = yaml . load ( raw_yaml ) return parsed_dict
3589	def set_color ( self , r , g , b ) : command = '\x58\x01\x03\x01\xFF\x00{0}{1}{2}' . format ( chr ( r & 0xFF ) , chr ( g & 0xFF ) , chr ( b & 0xFF ) ) self . _color . write_value ( command )
3394	def find_gene_knockout_reactions ( cobra_model , gene_list , compiled_gene_reaction_rules = None ) : potential_reactions = set ( ) for gene in gene_list : if isinstance ( gene , string_types ) : gene = cobra_model . genes . get_by_id ( gene ) potential_reactions . update ( gene . _reaction ) gene_set = { str ( i ) for i in gene_list } if compiled_gene_reaction_rules is None : compiled_gene_reaction_rules = { r : parse_gpr ( r . gene_reaction_rule ) [ 0 ] for r in potential_reactions } return [ r for r in potential_reactions if not eval_gpr ( compiled_gene_reaction_rules [ r ] , gene_set ) ]
2906	def _assign_new_thread_id ( self , recursive = True ) : self . __class__ . thread_id_pool += 1 self . thread_id = self . __class__ . thread_id_pool if not recursive : return self . thread_id for child in self : child . thread_id = self . thread_id return self . thread_id
2297	def fit ( self , x , y ) : train = np . vstack ( ( np . array ( [ self . featurize_row ( row . iloc [ 0 ] , row . iloc [ 1 ] ) for idx , row in x . iterrows ( ) ] ) , np . array ( [ self . featurize_row ( row . iloc [ 1 ] , row . iloc [ 0 ] ) for idx , row in x . iterrows ( ) ] ) ) ) labels = np . vstack ( ( y , - y ) ) . ravel ( ) verbose = 1 if self . verbose else 0 self . clf = CLF ( verbose = verbose , min_samples_leaf = self . L , n_estimators = self . E , max_depth = self . max_depth , n_jobs = self . n_jobs ) . fit ( train , labels )
6527	def get_tools ( ) : if not hasattr ( get_tools , '_CACHE' ) : get_tools . _CACHE = dict ( ) for entry in pkg_resources . iter_entry_points ( 'tidypy.tools' ) : try : get_tools . _CACHE [ entry . name ] = entry . load ( ) except ImportError as exc : output_error ( 'Could not load tool "%s" defined by "%s": %s' % ( entry , entry . dist , exc , ) , ) return get_tools . _CACHE
5986	def bulge_disk_tag_from_align_bulge_disks ( align_bulge_disk_centre , align_bulge_disk_axis_ratio , align_bulge_disk_phi ) : align_bulge_disk_centre_tag = align_bulge_disk_centre_tag_from_align_bulge_disk_centre ( align_bulge_disk_centre = align_bulge_disk_centre ) align_bulge_disk_axis_ratio_tag = align_bulge_disk_axis_ratio_tag_from_align_bulge_disk_axis_ratio ( align_bulge_disk_axis_ratio = align_bulge_disk_axis_ratio ) align_bulge_disk_phi_tag = align_bulge_disk_phi_tag_from_align_bulge_disk_phi ( align_bulge_disk_phi = align_bulge_disk_phi ) return align_bulge_disk_centre_tag + align_bulge_disk_axis_ratio_tag + align_bulge_disk_phi_tag
6386	def _sb_r1 ( self , term , r1_prefixes = None ) : vowel_found = False if hasattr ( r1_prefixes , '__iter__' ) : for prefix in r1_prefixes : if term [ : len ( prefix ) ] == prefix : return len ( prefix ) for i in range ( len ( term ) ) : if not vowel_found and term [ i ] in self . _vowels : vowel_found = True elif vowel_found and term [ i ] not in self . _vowels : return i + 1 return len ( term )
3664	def calculate ( self , T , method ) : r if method == PERRY151 : Cp = ( self . PERRY151_const + self . PERRY151_lin * T + self . PERRY151_quadinv / T ** 2 + self . PERRY151_quad * T ** 2 ) * calorie elif method == CRCSTD : Cp = self . CRCSTD_Cp elif method == LASTOVKA_S : Cp = Lastovka_solid ( T , self . similarity_variable ) Cp = property_mass_to_molar ( Cp , self . MW ) elif method in self . tabular_data : Cp = self . interpolate ( T , method ) return Cp
6456	def sim ( src , tar , method = sim_levenshtein ) : if callable ( method ) : return method ( src , tar ) else : raise AttributeError ( 'Unknown similarity function: ' + str ( method ) )
1136	def isdir ( s ) : try : st = os . stat ( s ) except os . error : return False return stat . S_ISDIR ( st . st_mode )
7023	def _base64_to_file ( b64str , outfpath , writetostrio = False ) : try : filebytes = base64 . b64decode ( b64str ) if writetostrio : outobj = StrIO ( filebytes ) return outobj else : with open ( outfpath , 'wb' ) as outfd : outfd . write ( filebytes ) if os . path . exists ( outfpath ) : return outfpath else : LOGERROR ( 'could not write output file: %s' % outfpath ) return None except Exception as e : LOGEXCEPTION ( 'failed while trying to convert ' 'b64 string to file %s' % outfpath ) return None
3956	def update_nginx_from_config ( nginx_config ) : logging . info ( 'Updating nginx with new Dusty config' ) temp_dir = tempfile . mkdtemp ( ) os . mkdir ( os . path . join ( temp_dir , 'html' ) ) _write_nginx_config ( constants . NGINX_BASE_CONFIG , os . path . join ( temp_dir , constants . NGINX_PRIMARY_CONFIG_NAME ) ) _write_nginx_config ( nginx_config [ 'http' ] , os . path . join ( temp_dir , constants . NGINX_HTTP_CONFIG_NAME ) ) _write_nginx_config ( nginx_config [ 'stream' ] , os . path . join ( temp_dir , constants . NGINX_STREAM_CONFIG_NAME ) ) _write_nginx_config ( constants . NGINX_502_PAGE_HTML , os . path . join ( temp_dir , 'html' , constants . NGINX_502_PAGE_NAME ) ) sync_local_path_to_vm ( temp_dir , constants . NGINX_CONFIG_DIR_IN_VM )
11844	def run ( self , steps = 1000 ) : "Run the Environment for given number of time steps." for step in range ( steps ) : if self . is_done ( ) : return self . step ( )
5703	def get_vehicle_hours_by_type ( gtfs , route_type ) : day = gtfs . get_suitable_date_for_daily_extract ( ) query = ( " SELECT * , SUM(end_time_ds - start_time_ds)/3600 as vehicle_hours_type" " FROM" " (SELECT * FROM day_trips as q1" " INNER JOIN" " (SELECT route_I, type FROM routes) as q2" " ON q1.route_I = q2.route_I" " WHERE type = {route_type}" " AND date = '{day}')" . format ( day = day , route_type = route_type ) ) df = gtfs . execute_custom_query_pandas ( query ) return df [ 'vehicle_hours_type' ] . item ( )
11292	def consume_json ( request ) : client = OEmbedConsumer ( ) urls = request . GET . getlist ( 'urls' ) width = request . GET . get ( 'width' ) height = request . GET . get ( 'height' ) template_dir = request . GET . get ( 'template_dir' ) output = { } ctx = RequestContext ( request ) for url in urls : try : provider = oembed . site . provider_for_url ( url ) except OEmbedMissingEndpoint : oembeds = None rendered = None else : oembeds = url rendered = client . parse_text ( url , width , height , context = ctx , template_dir = template_dir ) output [ url ] = { 'oembeds' : oembeds , 'rendered' : rendered , } return HttpResponse ( simplejson . dumps ( output ) , mimetype = 'application/json' )
9960	def get_object ( self , name ) : parts = name . split ( "." ) model_name = parts . pop ( 0 ) return self . models [ model_name ] . get_object ( "." . join ( parts ) )
6652	def findProgram ( self , builddir , program ) : if os . path . isfile ( os . path . join ( builddir , program ) ) : logging . info ( 'found %s' % program ) return program exact_matches = [ ] insensitive_matches = [ ] approx_matches = [ ] for path , dirs , files in os . walk ( builddir ) : if program in files : exact_matches . append ( os . path . relpath ( os . path . join ( path , program ) , builddir ) ) continue files_lower = [ f . lower ( ) for f in files ] if program . lower ( ) in files_lower : insensitive_matches . append ( os . path . relpath ( os . path . join ( path , files [ files_lower . index ( program . lower ( ) ) ] ) , builddir ) ) continue pg_basen_lower_noext = os . path . splitext ( os . path . basename ( program ) . lower ( ) ) [ 0 ] for f in files_lower : if pg_basen_lower_noext in f : approx_matches . append ( os . path . relpath ( os . path . join ( path , files [ files_lower . index ( f ) ] ) , builddir ) ) if len ( exact_matches ) == 1 : logging . info ( 'found %s at %s' , program , exact_matches [ 0 ] ) return exact_matches [ 0 ] elif len ( exact_matches ) > 1 : logging . error ( '%s matches multiple executables, please use a full path (one of %s)' % ( program , ', or ' . join ( [ '"' + os . path . join ( m , program ) + '"' for m in exact_matches ] ) ) ) return None reduced_approx_matches = [ ] for m in approx_matches : root = os . path . splitext ( m ) [ 0 ] if ( m == root ) or ( root not in approx_matches ) : reduced_approx_matches . append ( m ) approx_matches = reduced_approx_matches for matches in ( insensitive_matches , approx_matches ) : if len ( matches ) == 1 : logging . info ( 'found %s at %s' % ( program , matches [ 0 ] ) ) return matches [ 0 ] elif len ( matches ) > 1 : logging . error ( '%s is similar to several executables found. Please use an exact name:\n%s' % ( program , '\n' . join ( matches ) ) ) return None logging . error ( 'could not find program "%s" to debug' % program ) return None
7717	def _roster_set ( self , item , callback , error_callback ) : stanza = Iq ( to_jid = self . server , stanza_type = "set" ) payload = RosterPayload ( [ item ] ) stanza . set_payload ( payload ) def success_cb ( result_stanza ) : if callback : callback ( item ) def error_cb ( error_stanza ) : if error_callback : error_callback ( error_stanza ) else : logger . error ( "Roster change of '{0}' failed" . format ( item . jid ) ) processor = self . stanza_processor processor . set_response_handlers ( stanza , success_cb , error_cb ) processor . send ( stanza )
11360	def fix_dashes ( string ) : string = string . replace ( u'\u05BE' , '-' ) string = string . replace ( u'\u1806' , '-' ) string = string . replace ( u'\u2E3A' , '-' ) string = string . replace ( u'\u2E3B' , '-' ) string = unidecode ( string ) return re . sub ( r'--+' , '-' , string )
1404	def load_configs ( self ) : self . statemgr_config . set_state_locations ( self . configs [ STATEMGRS_KEY ] ) if EXTRA_LINKS_KEY in self . configs : for extra_link in self . configs [ EXTRA_LINKS_KEY ] : self . extra_links . append ( self . validate_extra_link ( extra_link ) )
1231	def tf_import_experience ( self , states , internals , actions , terminal , reward ) : return self . memory . store ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
8710	def __write_chunk ( self , chunk ) : log . debug ( 'writing %d bytes chunk' , len ( chunk ) ) data = BLOCK_START + chr ( len ( chunk ) ) + chunk if len ( chunk ) < 128 : padding = 128 - len ( chunk ) log . debug ( 'pad with %d characters' , padding ) data = data + ( ' ' * padding ) log . debug ( "packet size %d" , len ( data ) ) self . __write ( data ) self . _port . flush ( ) return self . __got_ack ( )
5067	def get_cache_key ( ** kwargs ) : key = '__' . join ( [ '{}:{}' . format ( item , value ) for item , value in iteritems ( kwargs ) ] ) return hashlib . md5 ( key . encode ( 'utf-8' ) ) . hexdigest ( )
196	def Clouds ( name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return meta . SomeOf ( ( 1 , 2 ) , children = [ CloudLayer ( intensity_mean = ( 196 , 255 ) , intensity_freq_exponent = ( - 2.5 , - 2.0 ) , intensity_coarse_scale = 10 , alpha_min = 0 , alpha_multiplier = ( 0.25 , 0.75 ) , alpha_size_px_max = ( 2 , 8 ) , alpha_freq_exponent = ( - 2.5 , - 2.0 ) , sparsity = ( 0.8 , 1.0 ) , density_multiplier = ( 0.5 , 1.0 ) ) , CloudLayer ( intensity_mean = ( 196 , 255 ) , intensity_freq_exponent = ( - 2.0 , - 1.0 ) , intensity_coarse_scale = 10 , alpha_min = 0 , alpha_multiplier = ( 0.5 , 1.0 ) , alpha_size_px_max = ( 64 , 128 ) , alpha_freq_exponent = ( - 2.0 , - 1.0 ) , sparsity = ( 1.0 , 1.4 ) , density_multiplier = ( 0.8 , 1.5 ) ) ] , random_order = False , name = name , deterministic = deterministic , random_state = random_state )
5608	def bounds_to_ranges ( out_bounds = None , in_affine = None , in_shape = None ) : return itertools . chain ( * from_bounds ( * out_bounds , transform = in_affine , height = in_shape [ - 2 ] , width = in_shape [ - 1 ] ) . round_lengths ( pixel_precision = 0 ) . round_offsets ( pixel_precision = 0 ) . toranges ( ) )
10664	def molar_mass ( compound = '' ) : result = 0.0 if compound is None or len ( compound ) == 0 : return result compound = compound . strip ( ) parsed = parse_compound ( compound ) return parsed . molar_mass ( )
2935	def parse_node ( self , node ) : if node . get ( 'id' ) in self . parsed_nodes : return self . parsed_nodes [ node . get ( 'id' ) ] ( node_parser , spec_class ) = self . parser . _get_parser_class ( node . tag ) if not node_parser or not spec_class : raise ValidationException ( "There is no support implemented for this task type." , node = node , filename = self . filename ) np = node_parser ( self , spec_class , node ) task_spec = np . parse_node ( ) return task_spec
1591	def _setup_custom_grouping ( self , topology ) : for i in range ( len ( topology . bolts ) ) : for in_stream in topology . bolts [ i ] . inputs : if in_stream . stream . component_name == self . my_component_name and in_stream . gtype == topology_pb2 . Grouping . Value ( "CUSTOM" ) : if in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "PYTHON_OBJECT" ) : custom_grouping_obj = default_serializer . deserialize ( in_stream . custom_grouping_object ) if isinstance ( custom_grouping_obj , str ) : pex_loader . load_pex ( self . topology_pex_abs_path ) grouping_cls = pex_loader . import_and_get_class ( self . topology_pex_abs_path , custom_grouping_obj ) custom_grouping_obj = grouping_cls ( ) assert isinstance ( custom_grouping_obj , ICustomGrouping ) self . custom_grouper . add ( in_stream . stream . id , self . _get_taskids_for_component ( topology . bolts [ i ] . comp . name ) , custom_grouping_obj , self . my_component_name ) elif in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "JAVA_OBJECT" ) : raise NotImplementedError ( "Java-serialized custom grouping is not yet supported " "for python topology" ) else : raise ValueError ( "Unrecognized custom grouping type found: %s" % str ( in_stream . type ) )
1876	def MOVQ ( cpu , dest , src ) : if dest . size == src . size and dest . size == 64 : dest . write ( src . read ( ) ) elif dest . size == src . size and dest . size == 128 : src_lo = Operators . EXTRACT ( src . read ( ) , 0 , 64 ) dest . write ( Operators . ZEXTEND ( src_lo , 128 ) ) elif dest . size == 128 and src . size == 64 : dest . write ( Operators . ZEXTEND ( src . read ( ) , dest . size ) ) elif dest . size == 64 and src . size == 128 : dest . write ( Operators . EXTRACT ( src . read ( ) , 0 , dest . size ) ) else : msg = 'Invalid size in MOVQ' logger . error ( msg ) raise Exception ( msg )
11582	def retrieve_url ( self , url ) : try : r = requests . get ( url ) except requests . ConnectionError : raise exceptions . RetrieveError ( 'Connection fail' ) if r . status_code >= 400 : raise exceptions . RetrieveError ( 'Connected, but status code is %s' % ( r . status_code ) ) real_url = r . url content = r . content try : content_type = r . headers [ 'Content-Type' ] except KeyError : content_type , encoding = mimetypes . guess_type ( real_url , strict = False ) self . response = r return content_type . lower ( ) , content
13038	def main ( ) : cred_search = CredentialSearch ( ) arg = argparse . ArgumentParser ( parents = [ cred_search . argparser ] , conflict_handler = 'resolve' ) arg . add_argument ( '-c' , '--count' , help = "Only show the number of results" , action = "store_true" ) arguments = arg . parse_args ( ) if arguments . count : print_line ( "Number of credentials: {}" . format ( cred_search . argument_count ( ) ) ) else : response = cred_search . get_credentials ( ) for hit in response : print_json ( hit . to_dict ( include_meta = True ) )
1840	def JNG ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . OR ( cpu . ZF , cpu . SF != cpu . OF ) , target . read ( ) , cpu . PC )
12694	def is_disjoint ( set1 , set2 , warn ) : for elem in set2 : if elem in set1 : raise ValueError ( warn ) return True
3633	def cardInfo ( self , resource_id ) : base_id = baseId ( resource_id ) if base_id in self . players : return self . players [ base_id ] else : url = '{0}{1}.json' . format ( card_info_url , base_id ) return requests . get ( url , timeout = self . timeout ) . json ( )
4801	def is_named ( self , filename ) : self . is_file ( ) if not isinstance ( filename , str_types ) : raise TypeError ( 'given filename arg must be a path' ) val_filename = os . path . basename ( os . path . abspath ( self . val ) ) if val_filename != filename : self . _err ( 'Expected filename <%s> to be equal to <%s>, but was not.' % ( val_filename , filename ) ) return self
1187	def unexpo ( intpart , fraction , expo ) : if expo > 0 : f = len ( fraction ) intpart , fraction = intpart + fraction [ : expo ] , fraction [ expo : ] if expo > f : intpart = intpart + '0' * ( expo - f ) elif expo < 0 : i = len ( intpart ) intpart , fraction = intpart [ : expo ] , intpart [ expo : ] + fraction if expo < - i : fraction = '0' * ( - expo - i ) + fraction return intpart , fraction
8838	def get_var ( data , var_name , not_found = None ) : try : for key in str ( var_name ) . split ( '.' ) : try : data = data [ key ] except TypeError : data = data [ int ( key ) ] except ( KeyError , TypeError , ValueError ) : return not_found else : return data
1356	def get_argument_environ ( self ) : try : return self . get_argument ( constants . PARAM_ENVIRON ) except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
215	def deepcopy ( self ) : return HeatmapsOnImage ( self . get_arr ( ) , shape = self . shape , min_value = self . min_value , max_value = self . max_value )
7799	def check_password ( self , username , password , properties ) : logger . debug ( "check_password{0!r}" . format ( ( username , password , properties ) ) ) pwd , pwd_format = self . get_password ( username , ( u"plain" , u"md5:user:realm:password" ) , properties ) if pwd_format == u"plain" : logger . debug ( "got plain password: {0!r}" . format ( pwd ) ) return pwd is not None and password == pwd elif pwd_format in ( u"md5:user:realm:password" ) : logger . debug ( "got md5:user:realm:password password: {0!r}" . format ( pwd ) ) realm = properties . get ( "realm" ) if realm is None : realm = "" else : realm = realm . encode ( "utf-8" ) username = username . encode ( "utf-8" ) password = password . encode ( "utf-8" ) urp_hash = hashlib . md5 ( b"%s:%s:%s" ) . hexdigest ( ) return urp_hash == pwd logger . debug ( "got password in unknown format: {0!r}" . format ( pwd_format ) ) return False
3292	def set_property_value ( self , name , value , dry_run = False ) : assert value is None or xml_tools . is_etree_element ( value ) if name in _lockPropertyNames : raise DAVError ( HTTP_FORBIDDEN , err_condition = PRECONDITION_CODE_ProtectedProperty ) config = self . environ [ "wsgidav.config" ] mutableLiveProps = config . get ( "mutable_live_props" , [ ] ) if ( name . startswith ( "{DAV:}" ) and name in _standardLivePropNames and name in mutableLiveProps ) : if name in ( "{DAV:}getlastmodified" , "{DAV:}last_modified" ) : try : return self . set_last_modified ( self . path , value . text , dry_run ) except Exception : _logger . warning ( "Provider does not support set_last_modified on {}." . format ( self . path ) ) raise DAVError ( HTTP_FORBIDDEN ) if name . startswith ( "{urn:schemas-microsoft-com:}" ) : agent = self . environ . get ( "HTTP_USER_AGENT" , "None" ) win32_emu = config . get ( "hotfixes" , { } ) . get ( "emulate_win32_lastmod" , False ) if win32_emu and "MiniRedir/6.1" not in agent : if "Win32LastModifiedTime" in name : return self . set_last_modified ( self . path , value . text , dry_run ) elif "Win32FileAttributes" in name : return True elif "Win32CreationTime" in name : return True elif "Win32LastAccessTime" in name : return True pm = self . provider . prop_manager if pm and not name . startswith ( "{DAV:}" ) : refUrl = self . get_ref_url ( ) if value is None : return pm . remove_property ( refUrl , name , dry_run , self . environ ) else : value = etree . tostring ( value ) return pm . write_property ( refUrl , name , value , dry_run , self . environ ) raise DAVError ( HTTP_FORBIDDEN )
1796	def XADD ( cpu , dest , src ) : MASK = ( 1 << dest . size ) - 1 SIGN_MASK = 1 << ( dest . size - 1 ) arg0 = dest . read ( ) arg1 = src . read ( ) temp = ( arg1 + arg0 ) & MASK src . write ( arg0 ) dest . write ( temp ) tempCF = Operators . OR ( Operators . ULT ( temp , arg0 ) , Operators . ULT ( temp , arg1 ) ) cpu . CF = tempCF cpu . AF = ( ( arg0 ^ arg1 ) ^ temp ) & 0x10 != 0 cpu . ZF = temp == 0 cpu . SF = ( temp & SIGN_MASK ) != 0 cpu . OF = ( ( ( arg0 ^ arg1 ^ SIGN_MASK ) & ( temp ^ arg1 ) ) & SIGN_MASK ) != 0 cpu . PF = cpu . _calculate_parity_flag ( temp )
7354	def predict_peptides ( self , peptides ) : from mhcflurry . encodable_sequences import EncodableSequences binding_predictions = [ ] encodable_sequences = EncodableSequences . create ( peptides ) for allele in self . alleles : predictions_df = self . predictor . predict_to_dataframe ( encodable_sequences , allele = allele ) for ( _ , row ) in predictions_df . iterrows ( ) : binding_prediction = BindingPrediction ( allele = allele , peptide = row . peptide , affinity = row . prediction , percentile_rank = ( row . prediction_percentile if 'prediction_percentile' in row else nan ) , prediction_method_name = "mhcflurry" ) binding_predictions . append ( binding_prediction ) return BindingPredictionCollection ( binding_predictions )
767	def getMetrics ( self ) : result = { } for metricObj , label in zip ( self . __metrics , self . __metricLabels ) : value = metricObj . getMetric ( ) result [ label ] = value [ 'value' ] return result
321	def get_max_drawdown ( returns ) : returns = returns . copy ( ) df_cum = cum_returns ( returns , 1.0 ) running_max = np . maximum . accumulate ( df_cum ) underwater = df_cum / running_max - 1 return get_max_drawdown_underwater ( underwater )
12895	def get_modes ( self ) : if not self . __modes : self . __modes = yield from self . handle_list ( self . API . get ( 'valid_modes' ) ) return self . __modes
242	def create_perf_attrib_tear_sheet ( returns , positions , factor_returns , factor_loadings , transactions = None , pos_in_dollars = True , return_fig = False , factor_partitions = FACTOR_PARTITIONS ) : portfolio_exposures , perf_attrib_data = perf_attrib . perf_attrib ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars = pos_in_dollars ) display ( Markdown ( "## Performance Relative to Common Risk Factors" ) ) perf_attrib . show_perf_attrib_stats ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars ) vertical_sections = 1 + 2 * max ( len ( factor_partitions ) , 1 ) current_section = 0 fig = plt . figure ( figsize = [ 14 , vertical_sections * 6 ] ) gs = gridspec . GridSpec ( vertical_sections , 1 , wspace = 0.5 , hspace = 0.5 ) perf_attrib . plot_returns ( perf_attrib_data , ax = plt . subplot ( gs [ current_section ] ) ) current_section += 1 if factor_partitions is not None : for factor_type , partitions in factor_partitions . iteritems ( ) : columns_to_select = perf_attrib_data . columns . intersection ( partitions ) perf_attrib . plot_factor_contribution_to_perf ( perf_attrib_data [ columns_to_select ] , ax = plt . subplot ( gs [ current_section ] ) , title = ( 'Cumulative common {} returns attribution' ) . format ( factor_type ) ) current_section += 1 for factor_type , partitions in factor_partitions . iteritems ( ) : perf_attrib . plot_risk_exposures ( portfolio_exposures [ portfolio_exposures . columns . intersection ( partitions ) ] , ax = plt . subplot ( gs [ current_section ] ) , title = 'Daily {} factor exposures' . format ( factor_type ) ) current_section += 1 else : perf_attrib . plot_factor_contribution_to_perf ( perf_attrib_data , ax = plt . subplot ( gs [ current_section ] ) ) current_section += 1 perf_attrib . plot_risk_exposures ( portfolio_exposures , ax = plt . subplot ( gs [ current_section ] ) ) gs . tight_layout ( fig ) if return_fig : return fig
11457	def match ( self , query = None , ** kwargs ) : from invenio . search_engine import perform_request_search if not query : recid = self . record [ "001" ] [ 0 ] [ 3 ] return perform_request_search ( p = "035:%s" % ( recid , ) , of = "id" ) else : if "recid" not in kwargs : kwargs [ "recid" ] = self . record [ "001" ] [ 0 ] [ 3 ] return perform_request_search ( p = query % kwargs , of = "id" )
3426	def medium ( self , medium ) : def set_active_bound ( reaction , bound ) : if reaction . reactants : reaction . lower_bound = - bound elif reaction . products : reaction . upper_bound = bound media_rxns = list ( ) exchange_rxns = frozenset ( self . exchanges ) for rxn_id , bound in iteritems ( medium ) : rxn = self . reactions . get_by_id ( rxn_id ) if rxn not in exchange_rxns : LOGGER . warn ( "%s does not seem to be an" " an exchange reaction. Applying bounds anyway." , rxn . id ) media_rxns . append ( rxn ) set_active_bound ( rxn , bound ) media_rxns = frozenset ( media_rxns ) for rxn in ( exchange_rxns - media_rxns ) : set_active_bound ( rxn , 0 )
1089	def indexOf ( a , b ) : "Return the first index of b in a." for i , j in enumerate ( a ) : if j == b : return i else : raise ValueError ( 'sequence.index(x): x not in sequence' )
9507	def union ( self , i ) : if self . intersects ( i ) or self . end + 1 == i . start or i . end + 1 == self . start : return Interval ( min ( self . start , i . start ) , max ( self . end , i . end ) ) else : return None
13809	def get_version ( relpath ) : from os . path import dirname , join if '__file__' not in globals ( ) : root = '.' else : root = dirname ( __file__ ) for line in open ( join ( root , relpath ) , 'rb' ) : line = line . decode ( 'cp437' ) if '__version__' in line : if '"' in line : return line . split ( '"' ) [ 1 ] elif "'" in line : return line . split ( "'" ) [ 1 ]
2876	def add_bpmn_xml ( self , bpmn , svg = None , filename = None ) : xpath = xpath_eval ( bpmn ) processes = xpath ( './/bpmn:process' ) for process in processes : process_parser = self . PROCESS_PARSER_CLASS ( self , process , svg , filename = filename , doc_xpath = xpath ) if process_parser . get_id ( ) in self . process_parsers : raise ValidationException ( 'Duplicate process ID' , node = process , filename = filename ) if process_parser . get_name ( ) in self . process_parsers_by_name : raise ValidationException ( 'Duplicate process name' , node = process , filename = filename ) self . process_parsers [ process_parser . get_id ( ) ] = process_parser self . process_parsers_by_name [ process_parser . get_name ( ) ] = process_parser
7000	def _runpf_worker ( task ) : ( lcfile , outdir , timecols , magcols , errcols , lcformat , lcformatdir , pfmethods , pfkwargs , getblssnr , sigclip , nworkers , minobservations , excludeprocessed ) = task if os . path . exists ( lcfile ) : pfresult = runpf ( lcfile , outdir , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = lcformatdir , pfmethods = pfmethods , pfkwargs = pfkwargs , getblssnr = getblssnr , sigclip = sigclip , nworkers = nworkers , minobservations = minobservations , excludeprocessed = excludeprocessed ) return pfresult else : LOGERROR ( 'LC does not exist for requested file %s' % lcfile ) return None
12327	def update ( globalvars ) : global config profileini = getprofileini ( ) config = configparser . ConfigParser ( ) config . read ( profileini ) defaults = { } if globalvars is not None : defaults = { a [ 0 ] : a [ 1 ] for a in globalvars } generic_configs = [ { 'name' : 'User' , 'nature' : 'generic' , 'description' : "General information" , 'variables' : [ 'user.email' , 'user.name' , 'user.fullname' ] , 'defaults' : { 'user.email' : { 'value' : defaults . get ( 'user.email' , '' ) , 'description' : "Email address" , 'validator' : EmailValidator ( ) } , 'user.fullname' : { 'value' : defaults . get ( 'user.fullname' , '' ) , 'description' : "Full Name" , 'validator' : NonEmptyValidator ( ) } , 'user.name' : { 'value' : defaults . get ( 'user.name' , getpass . getuser ( ) ) , 'description' : "Name" , 'validator' : NonEmptyValidator ( ) } , } } ] mgr = plugins_get_mgr ( ) extra_configs = mgr . gather_configs ( ) allconfigs = generic_configs + extra_configs for c in allconfigs : name = c [ 'name' ] for v in c [ 'variables' ] : try : c [ 'defaults' ] [ v ] [ 'value' ] = config [ name ] [ v ] except : continue for c in allconfigs : print ( "" ) print ( c [ 'description' ] ) print ( "==================" ) if len ( c [ 'variables' ] ) == 0 : print ( "Nothing to do. Enabled by default" ) continue name = c [ 'name' ] config [ name ] = { } config [ name ] [ 'nature' ] = c [ 'nature' ] for v in c [ 'variables' ] : value = '' description = v + " " helptext = "" validator = None if v in c [ 'defaults' ] : value = c [ 'defaults' ] [ v ] . get ( 'value' , '' ) helptext = c [ 'defaults' ] [ v ] . get ( "description" , "" ) validator = c [ 'defaults' ] [ v ] . get ( 'validator' , None ) if helptext != "" : description += "(" + helptext + ")" while True : choice = input_with_default ( description , value ) if validator is not None : if validator . is_valid ( choice ) : break else : print ( "Invalid input. Expected input is {}" . format ( validator . message ) ) else : break config [ name ] [ v ] = choice if v == 'enable' and choice == 'n' : break with open ( profileini , 'w' ) as fd : config . write ( fd ) print ( "Updated profile file:" , config )
6442	def _cond_k ( self , word , suffix_len ) : return ( len ( word ) - suffix_len >= 3 ) and ( word [ - suffix_len - 1 ] in { 'i' , 'l' } or ( word [ - suffix_len - 3 ] == 'u' and word [ - suffix_len - 1 ] == 'e' ) )
13872	def StandardizePath ( path , strip = False ) : path = path . replace ( SEPARATOR_WINDOWS , SEPARATOR_UNIX ) if strip : path = path . rstrip ( SEPARATOR_UNIX ) return path
5448	def make_param ( self , name , raw_uri , disk_size ) : if raw_uri . startswith ( 'https://www.googleapis.com/compute' ) : docker_path = self . _parse_image_uri ( raw_uri ) return job_model . PersistentDiskMountParam ( name , raw_uri , docker_path , disk_size , disk_type = None ) elif raw_uri . startswith ( 'file://' ) : local_path , docker_path = self . _parse_local_mount_uri ( raw_uri ) return job_model . LocalMountParam ( name , raw_uri , docker_path , local_path ) elif raw_uri . startswith ( 'gs://' ) : docker_path = self . _parse_gcs_uri ( raw_uri ) return job_model . GCSMountParam ( name , raw_uri , docker_path ) else : raise ValueError ( 'Mount parameter {} must begin with valid prefix.' . format ( raw_uri ) )
13666	def command_handle ( self ) : self . __results = self . execute ( self . args . command ) self . close ( ) self . logger . debug ( "results: {}" . format ( self . __results ) ) if not self . __results : self . unknown ( "{} return nothing." . format ( self . args . command ) ) if len ( self . __results ) != 1 : self . unknown ( "{} return more than one number." . format ( self . args . command ) ) self . __result = int ( self . __results [ 0 ] ) self . logger . debug ( "result: {}" . format ( self . __result ) ) if not isinstance ( self . __result , ( int , long ) ) : self . unknown ( "{} didn't return single number." . format ( self . args . command ) ) status = self . ok if self . __result > self . args . warning : status = self . warning if self . __result > self . args . critical : status = self . critical self . shortoutput = "{0} return {1}." . format ( self . args . command , self . __result ) [ self . longoutput . append ( line ) for line in self . __results if self . __results ] self . perfdata . append ( "{command}={result};{warn};{crit};0;" . format ( crit = self . args . critical , warn = self . args . warning , result = self . __result , command = self . args . command ) ) status ( self . output ( long_output_limit = None ) ) self . logger . debug ( "Return status and exit to Nagios." )
8861	def defined_names ( request_data ) : global _old_definitions ret_val = [ ] path = request_data [ 'path' ] toplvl_definitions = jedi . names ( request_data [ 'code' ] , path , 'utf-8' ) for d in toplvl_definitions : definition = _extract_def ( d , path ) if d . type != 'import' : ret_val . append ( definition ) ret_val = [ d . to_dict ( ) for d in ret_val ] return ret_val
2439	def reset_annotations ( self ) : self . annotation_date_set = False self . annotation_comment_set = False self . annotation_type_set = False self . annotation_spdx_id_set = False
3990	def _nginx_http_spec ( port_spec , bridge_ip ) : server_string_spec = "\t server {\n" server_string_spec += "\t \t {}\n" . format ( _nginx_max_file_size_string ( ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_listen_string ( port_spec ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_server_name_string ( port_spec ) ) server_string_spec += _nginx_location_spec ( port_spec , bridge_ip ) server_string_spec += _custom_502_page ( ) server_string_spec += "\t }\n" return server_string_spec
1589	def set_topology_context ( self , metrics_collector ) : Log . debug ( "Setting topology context" ) cluster_config = self . get_topology_config ( ) cluster_config . update ( self . _get_dict_from_config ( self . my_component . config ) ) task_to_component_map = self . _get_task_to_comp_map ( ) self . context = TopologyContextImpl ( cluster_config , self . pplan . topology , task_to_component_map , self . my_task_id , metrics_collector , self . topology_pex_abs_path )
1943	def protect_memory_callback ( self , start , size , perms ) : logger . info ( f"Changing permissions on {hex(start)}:{hex(start + size)} to {perms}" ) self . _emu . mem_protect ( start , size , convert_permissions ( perms ) )
2588	def get_data_manager ( cls ) : from parsl . dataflow . dflow import DataFlowKernelLoader dfk = DataFlowKernelLoader . dfk ( ) return dfk . executors [ 'data_manager' ]
8273	def color ( self , d = 0.035 ) : s = sum ( [ w for clr , rng , w in self . ranges ] ) r = random ( ) for clr , rng , weight in self . ranges : if weight / s >= r : break r -= weight / s return rng ( clr , d )
11317	def update_reportnumbers ( self ) : report_037_fields = record_get_field_instances ( self . record , '037' ) for field in report_037_fields : subs = field_get_subfields ( field ) for val in subs . get ( "a" , [ ] ) : if "arXiv" not in val : record_delete_field ( self . record , tag = "037" , field_position_global = field [ 4 ] ) new_subs = [ ( code , val [ 0 ] ) for code , val in subs . items ( ) ] record_add_field ( self . record , "088" , subfields = new_subs ) break
10369	def summarize_edge_filter ( graph : BELGraph , edge_predicates : EdgePredicates ) -> None : passed = count_passed_edge_filter ( graph , edge_predicates ) print ( '{}/{} edges passed {}' . format ( passed , graph . number_of_edges ( ) , ( ', ' . join ( edge_filter . __name__ for edge_filter in edge_predicates ) if isinstance ( edge_predicates , Iterable ) else edge_predicates . __name__ ) ) )
2938	def deserialize_assign_list ( self , workflow , start_node ) : assignments = [ ] for node in start_node . childNodes : if node . nodeType != minidom . Node . ELEMENT_NODE : continue if node . nodeName . lower ( ) == 'assign' : assignments . append ( self . deserialize_assign ( workflow , node ) ) else : _exc ( 'Unknown node: %s' % node . nodeName ) return assignments
9210	def capture ( target_url , user_agent = "archiveis (https://github.com/pastpages/archiveis)" , proxies = { } ) : domain = "http://archive.vn" save_url = urljoin ( domain , "/submit/" ) headers = { 'User-Agent' : user_agent , "host" : "archive.vn" , } logger . debug ( "Requesting {}" . format ( domain + "/" ) ) get_kwargs = dict ( timeout = 120 , allow_redirects = True , headers = headers , ) if proxies : get_kwargs [ 'proxies' ] = proxies response = requests . get ( domain + "/" , ** get_kwargs ) response . raise_for_status ( ) html = str ( response . content ) try : unique_id = html . split ( 'name="submitid' , 1 ) [ 1 ] . split ( 'value="' , 1 ) [ 1 ] . split ( '"' , 1 ) [ 0 ] logger . debug ( "Unique identifier: {}" . format ( unique_id ) ) except IndexError : logger . warn ( "Unable to extract unique identifier from archive.is. Submitting without it." ) unique_id = None data = { "url" : target_url , "anyway" : 1 , } if unique_id : data . update ( { "submitid" : unique_id } ) post_kwargs = dict ( timeout = 120 , allow_redirects = True , headers = headers , data = data ) if proxies : post_kwargs [ 'proxies' ] = proxies logger . debug ( "Requesting {}" . format ( save_url ) ) response = requests . post ( save_url , ** post_kwargs ) response . raise_for_status ( ) if 'Refresh' in response . headers : memento = str ( response . headers [ 'Refresh' ] ) . split ( ';url=' ) [ 1 ] logger . debug ( "Memento from Refresh header: {}" . format ( memento ) ) return memento if 'Location' in response . headers : memento = response . headers [ 'Location' ] logger . debug ( "Memento from Location header: {}" . format ( memento ) ) return memento logger . debug ( "Memento not found in response headers. Inspecting history." ) for i , r in enumerate ( response . history ) : logger . debug ( "Inspecting history request #{}" . format ( i ) ) logger . debug ( r . headers ) if 'Location' in r . headers : memento = r . headers [ 'Location' ] logger . debug ( "Memento from the Location header of {} history response: {}" . format ( i + 1 , memento ) ) return memento logger . error ( "No memento returned by archive.is" ) logger . error ( "Status code: {}" . format ( response . status_code ) ) logger . error ( response . headers ) logger . error ( response . text ) raise Exception ( "No memento returned by archive.is" )
13074	def register_assets ( self ) : self . blueprint . add_url_rule ( "{0}.secondary/<filetype>/<asset>" . format ( self . static_url_path ) , view_func = self . r_assets , endpoint = "secondary_assets" , methods = [ "GET" ] )
11121	def get_file_relative_path_by_name ( self , name , skip = 0 ) : if skip is None : paths = [ ] else : paths = None for path , info in self . walk_files_info ( ) : _ , n = os . path . split ( path ) if n == name : if skip is None : paths . append ( path ) elif skip > 0 : skip -= 1 else : paths = path break return paths
3318	def get ( self , token ) : self . _lock . acquire_read ( ) try : lock = self . _dict . get ( token ) if lock is None : _logger . debug ( "Lock purged dangling: {}" . format ( token ) ) self . delete ( token ) return None expire = float ( lock [ "expire" ] ) if expire >= 0 and expire < time . time ( ) : _logger . debug ( "Lock timed-out({}): {}" . format ( expire , lock_string ( lock ) ) ) self . delete ( token ) return None return lock finally : self . _lock . release ( )
1716	def pad ( num , n = 2 , sign = False ) : s = unicode ( abs ( num ) ) if len ( s ) < n : s = '0' * ( n - len ( s ) ) + s if not sign : return s if num >= 0 : return '+' + s else : return '-' + s
11574	def digital_message ( self , data ) : port = data [ 0 ] port_data = ( data [ self . MSB ] << 7 ) + data [ self . LSB ] pin = port * 8 for pin in range ( pin , min ( pin + 8 , self . total_pins_discovered ) ) : with self . pymata . data_lock : prev_data = self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] = port_data & 0x01 if prev_data != port_data & 0x01 : callback = self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_CALLBACK ] if callback : callback ( [ self . pymata . DIGITAL , pin , self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] ] ) latching_entry = self . digital_latch_table [ pin ] if latching_entry [ self . LATCH_STATE ] == self . LATCH_ARMED : if latching_entry [ self . LATCHED_THRESHOLD_TYPE ] == self . DIGITAL_LATCH_LOW : if ( port_data & 0x01 ) == 0 : if latching_entry [ self . DIGITAL_LATCH_CALLBACK ] is not None : self . digital_latch_table [ pin ] = [ 0 , 0 , 0 , 0 , None ] latching_entry [ self . DIGITAL_LATCH_CALLBACK ] ( [ self . pymata . OUTPUT | self . pymata . LATCH_MODE , pin , 0 , time . time ( ) ] ) else : updated_latch_entry = latching_entry updated_latch_entry [ self . LATCH_STATE ] = self . LATCH_LATCHED updated_latch_entry [ self . DIGITAL_LATCHED_DATA ] = self . DIGITAL_LATCH_LOW updated_latch_entry [ self . DIGITAL_TIME_STAMP ] = time . time ( ) else : pass elif latching_entry [ self . LATCHED_THRESHOLD_TYPE ] == self . DIGITAL_LATCH_HIGH : if port_data & 0x01 : if latching_entry [ self . DIGITAL_LATCH_CALLBACK ] is not None : self . digital_latch_table [ pin ] = [ 0 , 0 , 0 , 0 , None ] latching_entry [ self . DIGITAL_LATCH_CALLBACK ] ( [ self . pymata . OUTPUT | self . pymata . LATCH_MODE , pin , 1 , time . time ( ) ] ) else : updated_latch_entry = latching_entry updated_latch_entry [ self . LATCH_STATE ] = self . LATCH_LATCHED updated_latch_entry [ self . DIGITAL_LATCHED_DATA ] = self . DIGITAL_LATCH_HIGH updated_latch_entry [ self . DIGITAL_TIME_STAMP ] = time . time ( ) else : pass else : pass port_data >>= 1
9567	def byteswap ( fmt , data , offset = 0 ) : data = BytesIO ( data ) data . seek ( offset ) data_swapped = BytesIO ( ) for f in fmt : swapped = data . read ( int ( f ) ) [ : : - 1 ] data_swapped . write ( swapped ) return data_swapped . getvalue ( )
3386	def _random_point ( self ) : idx = np . random . randint ( self . n_warmup , size = min ( 2 , np . ceil ( np . sqrt ( self . n_warmup ) ) ) ) return self . warmup [ idx , : ] . mean ( axis = 0 )
2764	def get_all_snapshots ( self ) : data = self . get_data ( "snapshots/" ) return [ Snapshot ( token = self . token , ** snapshot ) for snapshot in data [ 'snapshots' ] ]
12836	def render_vars ( self ) : return { 'records' : [ { 'message' : record . getMessage ( ) , 'time' : dt . datetime . fromtimestamp ( record . created ) . strftime ( '%H:%M:%S' ) , } for record in self . handler . records ] }
3636	def club ( self , sort = 'desc' , ctype = 'player' , defId = '' , start = 0 , count = None , page_size = itemsPerPage [ 'club' ] , level = None , category = None , assetId = None , league = None , club = None , position = None , zone = None , nationality = None , rare = False , playStyle = None ) : method = 'GET' url = 'club' if count : page_size = count params = { 'sort' : sort , 'type' : ctype , 'defId' : defId , 'start' : start , 'count' : page_size } if level : params [ 'level' ] = level if category : params [ 'cat' ] = category if assetId : params [ 'maskedDefId' ] = assetId if league : params [ 'leag' ] = league if club : params [ 'team' ] = club if position : params [ 'pos' ] = position if zone : params [ 'zone' ] = zone if nationality : params [ 'nat' ] = nationality if rare : params [ 'rare' ] = 'SP' if playStyle : params [ 'playStyle' ] = playStyle rc = self . __request__ ( method , url , params = params ) if start == 0 : if ctype == 'player' : pgid = 'Club - Players - List View' elif ctype == 'staff' : pgid = 'Club - Staff - List View' elif ctype in ( 'item' , 'kit' , 'ball' , 'badge' , 'stadium' ) : pgid = 'Club - Club Items - List View' events = [ self . pin . event ( 'page_view' , 'Hub - Club' ) , self . pin . event ( 'page_view' , pgid ) ] if rc [ 'itemData' ] : events . append ( self . pin . event ( 'page_view' , 'Item - Detail View' ) ) self . pin . send ( events ) return [ itemParse ( { 'itemData' : i } ) for i in rc [ 'itemData' ] ]
6994	def shutdown_check_handler ( ) : url = 'http://169.254.169.254/latest/meta-data/spot/instance-action' try : resp = requests . get ( url , timeout = 1.0 ) resp . raise_for_status ( ) stopinfo = resp . json ( ) if 'action' in stopinfo and stopinfo [ 'action' ] in ( 'stop' , 'terminate' , 'hibernate' ) : stoptime = stopinfo [ 'time' ] LOGWARNING ( 'instance is going to %s at %s' % ( stopinfo [ 'action' ] , stoptime ) ) resp . close ( ) return True else : resp . close ( ) return False except HTTPError as e : resp . close ( ) return False except Exception as e : resp . close ( ) return False
10196	def aggregate_events ( aggregations , start_date = None , end_date = None , update_bookmark = True ) : start_date = dateutil_parse ( start_date ) if start_date else None end_date = dateutil_parse ( end_date ) if end_date else None results = [ ] for a in aggregations : aggr_cfg = current_stats . aggregations [ a ] aggregator = aggr_cfg . aggregator_class ( name = aggr_cfg . name , ** aggr_cfg . aggregator_config ) results . append ( aggregator . run ( start_date , end_date , update_bookmark ) ) return results
8448	def _has_branch ( branch ) : ret = temple . utils . shell ( 'git rev-parse --verify {}' . format ( branch ) , stderr = subprocess . DEVNULL , stdout = subprocess . DEVNULL , check = False ) return ret . returncode == 0
8348	def convert_charref ( self , name ) : try : n = int ( name ) except ValueError : return if not 0 <= n <= 127 : return return self . convert_codepoint ( n )
1495	def find_closing_braces ( self , query ) : if query [ 0 ] != '(' : raise Exception ( "Trying to find closing braces for no opening braces" ) num_open_braces = 0 for i in range ( len ( query ) ) : c = query [ i ] if c == '(' : num_open_braces += 1 elif c == ')' : num_open_braces -= 1 if num_open_braces == 0 : return i raise Exception ( "No closing braces found" )
7225	def delete ( self , project_id ) : self . logger . debug ( 'Deleting project by id: ' + project_id ) url = '%(base_url)s/%(project_id)s' % { 'base_url' : self . base_url , 'project_id' : project_id } r = self . gbdx_connection . delete ( url ) r . raise_for_status ( )
12482	def find_in_sections ( var_name , app_name ) : sections = get_sections ( app_name ) if not sections : raise ValueError ( 'No sections found in {} rcfiles.' . format ( app_name ) ) for s in sections : try : var_value = get_rcfile_variable_value ( var_name , section_name = s , app_name = app_name ) except : pass else : return s , var_value raise KeyError ( 'No variable {} has been found in {} ' 'rcfiles.' . format ( var_name , app_name ) )
11179	def authorize_url ( self ) : auth_url = OAUTH_ROOT + '/authorize' params = { 'client_id' : self . client_id , 'redirect_uri' : self . redirect_uri , } return "{}?{}" . format ( auth_url , urlencode ( params ) )
2637	def parent_callback ( self , parent_fu ) : if parent_fu . done ( ) is True : e = parent_fu . _exception if e : super ( ) . set_exception ( e ) else : super ( ) . set_result ( self . file_obj ) return
2907	def _is_descendant_of ( self , parent ) : if self . parent is None : return False if self . parent == parent : return True return self . parent . _is_descendant_of ( parent )
7231	def create_from_wkt ( self , wkt , item_type , ingest_source , ** attributes ) : geojson = load_wkt ( wkt ) . __geo_interface__ vector = { 'type' : "Feature" , 'geometry' : geojson , 'properties' : { 'item_type' : item_type , 'ingest_source' : ingest_source , 'attributes' : attributes } } return self . create ( vector ) [ 0 ]
4103	def setup ( app ) : app . add_config_value ( 'plot_gallery' , True , 'html' ) app . add_config_value ( 'abort_on_example_error' , False , 'html' ) app . add_config_value ( 'sphinx_gallery_conf' , gallery_conf , 'html' ) app . add_stylesheet ( 'gallery.css' ) app . connect ( 'builder-inited' , generate_gallery_rst ) app . connect ( 'build-finished' , embed_code_links )
9338	def map ( self , func , sequence , reduce = None , star = False , minlength = 0 ) : def realreduce ( r ) : if reduce : if isinstance ( r , tuple ) : return reduce ( * r ) else : return reduce ( r ) return r def realfunc ( i ) : if star : return func ( * i ) else : return func ( i ) if len ( sequence ) <= 0 or self . np == 0 or get_debug ( ) : self . local = lambda : None self . local . rank = 0 rt = [ realreduce ( realfunc ( i ) ) for i in sequence ] self . local = None return rt np = min ( [ self . np , len ( sequence ) ] ) Q = self . backend . QueueFactory ( 64 ) R = self . backend . QueueFactory ( 64 ) self . ordered . reset ( ) pg = ProcessGroup ( main = self . _main , np = np , backend = self . backend , args = ( Q , R , sequence , realfunc ) ) pg . start ( ) L = [ ] N = [ ] def feeder ( pg , Q , N ) : j = 0 try : for i , work in enumerate ( sequence ) : if not hasattr ( sequence , '__getitem__' ) : pg . put ( Q , ( i , work ) ) else : pg . put ( Q , ( i , ) ) j = j + 1 N . append ( j ) for i in range ( np ) : pg . put ( Q , None ) except StopProcessGroup : return finally : pass feeder = threading . Thread ( None , feeder , args = ( pg , Q , N ) ) feeder . start ( ) count = 0 try : while True : try : capsule = pg . get ( R ) except queue . Empty : continue except StopProcessGroup : raise pg . get_exception ( ) capsule = capsule [ 0 ] , realreduce ( capsule [ 1 ] ) heapq . heappush ( L , capsule ) count = count + 1 if len ( N ) > 0 and count == N [ 0 ] : break rt = [ ] while len ( L ) > 0 : rt . append ( heapq . heappop ( L ) [ 1 ] ) pg . join ( ) feeder . join ( ) assert N [ 0 ] == len ( rt ) return rt except BaseException as e : pg . killall ( ) pg . join ( ) feeder . join ( ) raise
13684	def post ( self , url , params = { } , files = None ) : params . update ( { 'api_key' : self . api_key } ) try : response = requests . post ( self . host + url , data = params , files = files ) return self . json_parse ( response . content ) except RequestException as e : return self . json_parse ( e . args )
4516	def bresenham_line ( self , x0 , y0 , x1 , y1 , color = None , colorFunc = None ) : md . bresenham_line ( self . set , x0 , y0 , x1 , y1 , color , colorFunc )
6398	def dist_monge_elkan ( src , tar , sim_func = sim_levenshtein , symmetric = False ) : return MongeElkan ( ) . dist ( src , tar , sim_func , symmetric )
11593	def _rc_msetnx ( self , mapping ) : for k in iterkeys ( mapping ) : if self . exists ( k ) : return False return self . _rc_mset ( mapping )
10935	def check_update_J ( self ) : self . _J_update_counter += 1 update = self . _J_update_counter >= self . update_J_frequency return update & ( not self . _fresh_JTJ )
6853	def partitions ( device = "" ) : partitions_list = { } with settings ( hide ( 'running' , 'stdout' ) ) : res = run_as_root ( 'sfdisk -d %(device)s' % locals ( ) ) spart = re . compile ( r'(?P<pname>^/.*) : .* Id=(?P<ptypeid>[0-9a-z]+)' ) for line in res . splitlines ( ) : m = spart . search ( line ) if m : partitions_list [ m . group ( 'pname' ) ] = int ( m . group ( 'ptypeid' ) , 16 ) return partitions_list
5481	def retry_api_check ( exception ) : if isinstance ( exception , apiclient . errors . HttpError ) : if exception . resp . status in TRANSIENT_HTTP_ERROR_CODES : _print_error ( 'Retrying...' ) return True if isinstance ( exception , socket . error ) : if exception . errno in TRANSIENT_SOCKET_ERROR_CODES : _print_error ( 'Retrying...' ) return True if isinstance ( exception , oauth2client . client . AccessTokenRefreshError ) : _print_error ( 'Retrying...' ) return True if isinstance ( exception , SSLError ) : _print_error ( 'Retrying...' ) return True if isinstance ( exception , ServerNotFoundError ) : _print_error ( 'Retrying...' ) return True return False
11214	def compare_signature ( expected : Union [ str , bytes ] , actual : Union [ str , bytes ] ) -> bool : expected = util . to_bytes ( expected ) actual = util . to_bytes ( actual ) return hmac . compare_digest ( expected , actual )
2798	def rename ( self , new_name ) : return self . get_data ( "images/%s" % self . id , type = PUT , params = { "name" : new_name } )
8804	def build_payload ( ipaddress , event_type , event_time = None , start_time = None , end_time = None ) : payload = { 'event_type' : unicode ( event_type ) , 'tenant_id' : unicode ( ipaddress . used_by_tenant_id ) , 'ip_address' : unicode ( ipaddress . address_readable ) , 'ip_version' : int ( ipaddress . version ) , 'ip_type' : unicode ( ipaddress . address_type ) , 'id' : unicode ( ipaddress . id ) } if event_type == IP_EXISTS : if start_time is None or end_time is None : raise ValueError ( 'IP_BILL: {} start_time/end_time cannot be empty' . format ( event_type ) ) payload . update ( { 'startTime' : unicode ( convert_timestamp ( start_time ) ) , 'endTime' : unicode ( convert_timestamp ( end_time ) ) } ) elif event_type in [ IP_ADD , IP_DEL , IP_ASSOC , IP_DISASSOC ] : if event_time is None : raise ValueError ( 'IP_BILL: {}: event_time cannot be NULL' . format ( event_type ) ) payload . update ( { 'eventTime' : unicode ( convert_timestamp ( event_time ) ) , 'subnet_id' : unicode ( ipaddress . subnet_id ) , 'network_id' : unicode ( ipaddress . network_id ) , 'public' : True if ipaddress . network_id == PUBLIC_NETWORK_ID else False , } ) else : raise ValueError ( 'IP_BILL: bad event_type: {}' . format ( event_type ) ) return payload
10988	def _translate_particles ( s , max_mem = 1e9 , desc = '' , min_rad = 'calc' , max_rad = 'calc' , invert = 'guess' , rz_order = 0 , do_polish = True ) : if desc is not None : desc_trans = desc + 'translate-particles' desc_burn = desc + 'addsub_burn' desc_polish = desc + 'addsub_polish' else : desc_trans , desc_burn , desc_polish = [ None ] * 3 RLOG . info ( 'Translate Particles:' ) opt . burn ( s , mode = 'do-particles' , n_loop = 4 , fractol = 0.1 , desc = desc_trans , max_mem = max_mem , include_rad = False , dowarn = False ) opt . burn ( s , mode = 'do-particles' , n_loop = 4 , fractol = 0.05 , desc = desc_trans , max_mem = max_mem , include_rad = True , dowarn = False ) RLOG . info ( 'Start add-subtract' ) addsub . add_subtract ( s , tries = 30 , min_rad = min_rad , max_rad = max_rad , invert = invert ) if desc is not None : states . save ( s , desc = desc + 'translate-addsub' ) if do_polish : RLOG . info ( 'Final Burn:' ) opt . burn ( s , mode = 'burn' , n_loop = 3 , fractol = 3e-4 , desc = desc_burn , max_mem = max_mem , rz_order = rz_order , dowarn = False ) RLOG . info ( 'Final Polish:' ) d = opt . burn ( s , mode = 'polish' , n_loop = 4 , fractol = 3e-4 , desc = desc_polish , max_mem = max_mem , rz_order = rz_order , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' )
3185	def get ( self , store_id , product_id , image_id , ** queryparams ) : self . store_id = store_id self . product_id = product_id self . image_id = image_id return self . _mc_client . _post ( url = self . _build_path ( store_id , 'products' , product_id , 'images' , image_id ) , ** queryparams )
6391	def encode ( self , word , max_length = 8 ) : word = '' . join ( char for char in word . lower ( ) if char in self . _initial_phones ) if not word : word = '' values = [ self . _initial_phones [ word [ 0 ] ] ] values += [ self . _trailing_phones [ char ] for char in word [ 1 : ] ] shifted_values = [ _ >> 1 for _ in values ] condensed_values = [ values [ 0 ] ] for n in range ( 1 , len ( shifted_values ) ) : if shifted_values [ n ] != shifted_values [ n - 1 ] : condensed_values . append ( values [ n ] ) values = ( [ condensed_values [ 0 ] ] + [ 0 ] * max ( 0 , max_length - len ( condensed_values ) ) + condensed_values [ 1 : max_length ] ) hash_value = 0 for val in values : hash_value = ( hash_value << 8 ) | val return hash_value
8309	def pangocairo_create_context ( cr ) : try : return PangoCairo . create_context ( cr ) except KeyError as e : if e . args == ( 'could not find foreign type Context' , ) : raise ShoebotInstallError ( "Error creating PangoCairo missing dependency: python-gi-cairo" ) else : raise
6675	def umask ( self , use_sudo = False ) : func = use_sudo and run_as_root or self . run return func ( 'umask' )
5180	def base_url ( self ) : return '{proto}://{host}:{port}{url_path}' . format ( proto = self . protocol , host = self . host , port = self . port , url_path = self . url_path , )
5457	def _from_yaml_v0 ( cls , job ) : job_metadata = { } for key in [ 'job-id' , 'job-name' , 'create-time' ] : job_metadata [ key ] = job . get ( key ) job_metadata [ 'create-time' ] = dsub_util . replace_timezone ( datetime . datetime . strptime ( job [ 'create-time' ] , '%Y-%m-%d %H:%M:%S.%f' ) , tzlocal ( ) ) job_resources = Resources ( ) params = { } labels = job . get ( 'labels' , { } ) if 'dsub-version' in labels : job_metadata [ 'dsub-version' ] = labels [ 'dsub-version' ] del labels [ 'dsub-version' ] params [ 'labels' ] = cls . _label_params_from_dict ( labels ) params [ 'envs' ] = cls . _env_params_from_dict ( job . get ( 'envs' , { } ) ) params [ 'inputs' ] = cls . _input_file_params_from_dict ( job . get ( 'inputs' , { } ) , False ) params [ 'outputs' ] = cls . _output_file_params_from_dict ( job . get ( 'outputs' , { } ) , False ) if job . get ( 'task-id' ) is None : job_params = params task_metadata = { 'task-id' : None } task_params = { } else : job_params = { } task_metadata = { 'task-id' : str ( job . get ( 'task-id' ) ) } task_params = params task_resources = Resources ( logging_path = job . get ( 'logging' ) ) task_descriptors = [ TaskDescriptor . get_complete_descriptor ( task_metadata , task_params , task_resources ) ] return JobDescriptor . get_complete_descriptor ( job_metadata , job_params , job_resources , task_descriptors )
2769	def get_all_firewalls ( self ) : data = self . get_data ( "firewalls" ) firewalls = list ( ) for jsoned in data [ 'firewalls' ] : firewall = Firewall ( ** jsoned ) firewall . token = self . token in_rules = list ( ) for rule in jsoned [ 'inbound_rules' ] : in_rules . append ( InboundRule ( ** rule ) ) firewall . inbound_rules = in_rules out_rules = list ( ) for rule in jsoned [ 'outbound_rules' ] : out_rules . append ( OutboundRule ( ** rule ) ) firewall . outbound_rules = out_rules firewalls . append ( firewall ) return firewalls
1431	def getInstancePid ( topology_info , instance_id ) : try : http_client = tornado . httpclient . AsyncHTTPClient ( ) endpoint = utils . make_shell_endpoint ( topology_info , instance_id ) url = "%s/pid/%s" % ( endpoint , instance_id ) Log . debug ( "HTTP call for url: %s" , url ) response = yield http_client . fetch ( url ) raise tornado . gen . Return ( response . body ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) )
12750	def set_pid_params ( self , * args , ** kwargs ) : for joint in self . joints : joint . target_angles = [ None ] * joint . ADOF joint . controllers = [ pid ( * args , ** kwargs ) for i in range ( joint . ADOF ) ]
12286	def lookup ( username , reponame ) : mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'repomanager' , name = 'git' ) repo = repomgr . lookup ( username = username , reponame = reponame ) return repo
1370	def get_subparser ( parser , command ) : subparsers_actions = [ action for action in parser . _actions if isinstance ( action , argparse . _SubParsersAction ) ] for subparsers_action in subparsers_actions : for choice , subparser in subparsers_action . choices . items ( ) : if choice == command : return subparser return None
5614	def segmentize_geometry ( geometry , segmentize_value ) : if geometry . geom_type != "Polygon" : raise TypeError ( "segmentize geometry type must be Polygon" ) return Polygon ( LinearRing ( [ p for l in map ( lambda x : LineString ( [ x [ 0 ] , x [ 1 ] ] ) , zip ( geometry . exterior . coords [ : - 1 ] , geometry . exterior . coords [ 1 : ] ) ) for p in [ l . interpolate ( segmentize_value * i ) . coords [ 0 ] for i in range ( int ( l . length / segmentize_value ) ) ] + [ l . coords [ 1 ] ] ] ) )
12681	def copy_attributes ( source , destination , ignore_patterns = [ ] ) : for attr in _wildcard_filter ( dir ( source ) , * ignore_patterns ) : setattr ( destination , attr , getattr ( source , attr ) )
7465	def _parse_01 ( ofiles , individual = False ) : cols = [ ] dats = [ ] for ofile in ofiles : with open ( ofile ) as infile : dat = infile . read ( ) lastbits = dat . split ( ".mcmc.txt\n\n" ) [ 1 : ] results = lastbits [ 0 ] . split ( "\n\n" ) [ 0 ] . split ( ) shape = ( ( ( len ( results ) - 3 ) / 4 ) , 4 ) dat = np . array ( results [ 3 : ] ) . reshape ( shape ) cols . append ( dat [ : , 3 ] . astype ( float ) ) if not individual : cols = np . array ( cols ) cols = cols . sum ( axis = 0 ) / len ( ofiles ) dat [ : , 3 ] = cols . astype ( str ) df = pd . DataFrame ( dat [ : , 1 : ] ) df . columns = [ "delim" , "prior" , "posterior" ] nspecies = 1 + np . array ( [ list ( i ) for i in dat [ : , 1 ] ] , dtype = int ) . sum ( axis = 1 ) df [ "nspecies" ] = nspecies return df else : res = [ ] for i in xrange ( len ( cols ) ) : x = dat x [ : , 3 ] = cols [ i ] . astype ( str ) x = pd . DataFrame ( x [ : , 1 : ] ) x . columns = [ 'delim' , 'prior' , 'posterior' ] nspecies = 1 + np . array ( [ list ( i ) for i in dat [ : , 1 ] ] , dtype = int ) . sum ( axis = 1 ) x [ "nspecies" ] = nspecies res . append ( x ) return res
2237	def import_module_from_path ( modpath , index = - 1 ) : import os if not os . path . exists ( modpath ) : import re import zipimport pat = '(.zip[' + re . escape ( os . path . sep ) + '/:])' parts = re . split ( pat , modpath , flags = re . IGNORECASE ) if len ( parts ) > 2 : archivepath = '' . join ( parts [ : - 1 ] ) [ : - 1 ] internal = parts [ - 1 ] modname = os . path . splitext ( internal ) [ 0 ] modname = os . path . normpath ( modname ) if os . path . exists ( archivepath ) : zimp_file = zipimport . zipimporter ( archivepath ) module = zimp_file . load_module ( modname ) return module raise IOError ( 'modpath={} does not exist' . format ( modpath ) ) else : module = _custom_import_modpath ( modpath ) return module
11296	def process_response ( self , resp , multiple_rates ) : self . _check_for_exceptions ( resp , multiple_rates ) rates = { } for result in resp [ 'results' ] : rate = ZipTaxClient . _cast_tax_rate ( result [ 'taxSales' ] ) rates [ result [ 'geoCity' ] ] = rate if not multiple_rates : return rates [ list ( rates . keys ( ) ) [ 0 ] ] return rates
5766	def _setup_evp_encrypt_decrypt ( cipher , data ) : evp_cipher = { 'aes128' : libcrypto . EVP_aes_128_cbc , 'aes192' : libcrypto . EVP_aes_192_cbc , 'aes256' : libcrypto . EVP_aes_256_cbc , 'rc2' : libcrypto . EVP_rc2_cbc , 'rc4' : libcrypto . EVP_rc4 , 'des' : libcrypto . EVP_des_cbc , 'tripledes_2key' : libcrypto . EVP_des_ede_cbc , 'tripledes_3key' : libcrypto . EVP_des_ede3_cbc , } [ cipher ] ( ) if cipher == 'rc4' : buffer_size = len ( data ) else : block_size = { 'aes128' : 16 , 'aes192' : 16 , 'aes256' : 16 , 'rc2' : 8 , 'des' : 8 , 'tripledes_2key' : 8 , 'tripledes_3key' : 8 , } [ cipher ] buffer_size = block_size * int ( math . ceil ( len ( data ) / block_size ) ) return ( evp_cipher , buffer_size )
80	def CoarseDropout ( p = 0 , size_px = None , size_percent = None , per_channel = False , min_size = 4 , name = None , deterministic = False , random_state = None ) : if ia . is_single_number ( p ) : p2 = iap . Binomial ( 1 - p ) elif ia . is_iterable ( p ) : ia . do_assert ( len ( p ) == 2 ) ia . do_assert ( p [ 0 ] < p [ 1 ] ) ia . do_assert ( 0 <= p [ 0 ] <= 1.0 ) ia . do_assert ( 0 <= p [ 1 ] <= 1.0 ) p2 = iap . Binomial ( iap . Uniform ( 1 - p [ 1 ] , 1 - p [ 0 ] ) ) elif isinstance ( p , iap . StochasticParameter ) : p2 = p else : raise Exception ( "Expected p to be float or int or StochasticParameter, got %s." % ( type ( p ) , ) ) if size_px is not None : p3 = iap . FromLowerResolution ( other_param = p2 , size_px = size_px , min_size = min_size ) elif size_percent is not None : p3 = iap . FromLowerResolution ( other_param = p2 , size_percent = size_percent , min_size = min_size ) else : raise Exception ( "Either size_px or size_percent must be set." ) if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return MultiplyElementwise ( p3 , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
1199	def get_variables ( self , include_nontrainable = False ) : if include_nontrainable : return [ self . all_variables [ key ] for key in sorted ( self . all_variables ) ] else : return [ self . variables [ key ] for key in sorted ( self . variables ) ]
5338	def __upload_title ( self , kibiter_major ) : if kibiter_major == "6" : resource = ".kibana/doc/projectname" data = { "projectname" : { "name" : self . project_name } } mapping_resource = ".kibana/_mapping/doc" mapping = { "dynamic" : "true" } url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , resource ) mapping_url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , mapping_resource ) logger . debug ( "Adding mapping for dashboard title" ) res = self . grimoire_con . put ( mapping_url , data = json . dumps ( mapping ) , headers = ES6_HEADER ) try : res . raise_for_status ( ) except requests . exceptions . HTTPError : logger . error ( "Couldn't create mapping for dashboard title." ) logger . error ( res . json ( ) ) logger . debug ( "Uploading dashboard title" ) res = self . grimoire_con . post ( url , data = json . dumps ( data ) , headers = ES6_HEADER ) try : res . raise_for_status ( ) except requests . exceptions . HTTPError : logger . error ( "Couldn't create dashboard title." ) logger . error ( res . json ( ) )
5214	def earning ( ticker , by = 'Geo' , typ = 'Revenue' , ccy = None , level = None , ** kwargs ) -> pd . DataFrame : ovrd = 'G' if by [ 0 ] . upper ( ) == 'G' else 'P' new_kw = dict ( raw = True , Product_Geo_Override = ovrd ) header = bds ( tickers = ticker , flds = 'PG_Bulk_Header' , ** new_kw , ** kwargs ) if ccy : kwargs [ 'Eqy_Fund_Crncy' ] = ccy if level : kwargs [ 'PG_Hierarchy_Level' ] = level data = bds ( tickers = ticker , flds = f'PG_{typ}' , ** new_kw , ** kwargs ) return assist . format_earning ( data = data , header = header )
445	def prefetch_input_data ( reader , file_pattern , is_training , batch_size , values_per_shard , input_queue_capacity_factor = 16 , num_reader_threads = 1 , shard_queue_name = "filename_queue" , value_queue_name = "input_queue" ) : data_files = [ ] for pattern in file_pattern . split ( "," ) : data_files . extend ( tf . gfile . Glob ( pattern ) ) if not data_files : tl . logging . fatal ( "Found no input files matching %s" , file_pattern ) else : tl . logging . info ( "Prefetching values from %d files matching %s" , len ( data_files ) , file_pattern ) if is_training : print ( " is_training == True : RandomShuffleQueue" ) filename_queue = tf . train . string_input_producer ( data_files , shuffle = True , capacity = 16 , name = shard_queue_name ) min_queue_examples = values_per_shard * input_queue_capacity_factor capacity = min_queue_examples + 100 * batch_size values_queue = tf . RandomShuffleQueue ( capacity = capacity , min_after_dequeue = min_queue_examples , dtypes = [ tf . string ] , name = "random_" + value_queue_name ) else : print ( " is_training == False : FIFOQueue" ) filename_queue = tf . train . string_input_producer ( data_files , shuffle = False , capacity = 1 , name = shard_queue_name ) capacity = values_per_shard + 3 * batch_size values_queue = tf . FIFOQueue ( capacity = capacity , dtypes = [ tf . string ] , name = "fifo_" + value_queue_name ) enqueue_ops = [ ] for _ in range ( num_reader_threads ) : _ , value = reader . read ( filename_queue ) enqueue_ops . append ( values_queue . enqueue ( [ value ] ) ) tf . train . queue_runner . add_queue_runner ( tf . train . queue_runner . QueueRunner ( values_queue , enqueue_ops ) ) tf . summary . scalar ( "queue/%s/fraction_of_%d_full" % ( values_queue . name , capacity ) , tf . cast ( values_queue . size ( ) , tf . float32 ) * ( 1. / capacity ) ) return values_queue
11061	def send_message ( self , channel , text , thread = None , reply_broadcast = None ) : if isinstance ( channel , SlackRoomIMBase ) : channel = channel . id self . log . debug ( "Trying to send to %s: %s" , channel , text ) self . sc . rtm_send_message ( channel , text , thread = thread , reply_broadcast = reply_broadcast )
13583	def admin_obj_link ( obj , display = '' ) : url = reverse ( 'admin:%s_%s_changelist' % ( obj . _meta . app_label , obj . _meta . model_name ) ) url += '?id__exact=%s' % obj . id text = str ( obj ) if display : text = display return format_html ( '<a href="{}">{}</a>' , url , text )
1008	def _learnPhase2 ( self , readOnly = False ) : self . lrnPredictedState [ 't' ] . fill ( 0 ) for c in xrange ( self . numberOfCols ) : i , s , numActive = self . _getBestMatchingCell ( c , self . lrnActiveState [ 't' ] , minThreshold = self . activationThreshold ) if i is None : continue self . lrnPredictedState [ 't' ] [ c , i ] = 1 if readOnly : continue segUpdate = self . _getSegmentActiveSynapses ( c , i , s , activeState = self . lrnActiveState [ 't' ] , newSynapses = ( numActive < self . newSynapseCount ) ) s . totalActivations += 1 self . _addToSegmentUpdates ( c , i , segUpdate ) if self . doPooling : predSegment = self . _getBestMatchingSegment ( c , i , self . lrnActiveState [ 't-1' ] ) segUpdate = self . _getSegmentActiveSynapses ( c , i , predSegment , self . lrnActiveState [ 't-1' ] , newSynapses = True ) self . _addToSegmentUpdates ( c , i , segUpdate )
9237	def open ( self ) : if self . is_open : return try : os . chdir ( self . working_directory ) if self . chroot_directory : os . chroot ( self . chroot_directory ) os . setgid ( self . gid ) os . setuid ( self . uid ) os . umask ( self . umask ) except OSError as err : raise DaemonError ( 'Setting up Environment failed: {0}' . format ( err ) ) if self . prevent_core : try : resource . setrlimit ( resource . RLIMIT_CORE , ( 0 , 0 ) ) except Exception as err : raise DaemonError ( 'Could not disable core files: {0}' . format ( err ) ) if self . detach_process : try : if os . fork ( ) > 0 : os . _exit ( 0 ) except OSError as err : raise DaemonError ( 'First fork failed: {0}' . format ( err ) ) os . setsid ( ) try : if os . fork ( ) > 0 : os . _exit ( 0 ) except OSError as err : raise DaemonError ( 'Second fork failed: {0}' . format ( err ) ) for ( signal_number , handler ) in self . _signal_handler_map . items ( ) : signal . signal ( signal_number , handler ) close_filenos ( self . _files_preserve ) redirect_stream ( sys . stdin , self . stdin ) redirect_stream ( sys . stdout , self . stdout ) redirect_stream ( sys . stderr , self . stderr ) if self . pidfile : self . pidfile . acquire ( ) self . _is_open = True
6938	def parallel_update_objectinfo_cpdir ( cpdir , cpglob = 'checkplot-*.pkl*' , liststartindex = None , maxobjects = None , nworkers = NCPUS , fast_mode = False , findercmap = 'gray_r' , finderconvolve = None , deredden_object = True , custom_bandpasses = None , gaia_submit_timeout = 10.0 , gaia_submit_tries = 3 , gaia_max_timeout = 180.0 , gaia_mirror = None , complete_query_later = True , lclistpkl = None , nbrradiusarcsec = 60.0 , maxnumneighbors = 5 , plotdpi = 100 , findercachedir = '~/.astrobase/stamp-cache' , verbose = True ) : cplist = sorted ( glob . glob ( os . path . join ( cpdir , cpglob ) ) ) return parallel_update_objectinfo_cplist ( cplist , liststartindex = liststartindex , maxobjects = maxobjects , nworkers = nworkers , fast_mode = fast_mode , findercmap = findercmap , finderconvolve = finderconvolve , deredden_object = deredden_object , custom_bandpasses = custom_bandpasses , gaia_submit_timeout = gaia_submit_timeout , gaia_submit_tries = gaia_submit_tries , gaia_max_timeout = gaia_max_timeout , gaia_mirror = gaia_mirror , complete_query_later = complete_query_later , lclistpkl = lclistpkl , nbrradiusarcsec = nbrradiusarcsec , maxnumneighbors = maxnumneighbors , plotdpi = plotdpi , findercachedir = findercachedir , verbose = verbose )
4047	def _totals ( self , query ) : self . add_parameters ( limit = 1 ) query = self . _build_query ( query ) self . _retrieve_data ( query ) self . url_params = None return int ( self . request . headers [ "Total-Results" ] )
7301	def post ( self , request , * args , ** kwargs ) : form_class = self . get_form_class ( ) form = self . get_form ( form_class ) mongo_ids = self . get_initial ( ) [ 'mongo_id' ] for form_mongo_id in form . data . getlist ( 'mongo_id' ) : for mongo_id in mongo_ids : if form_mongo_id == mongo_id : self . document . objects . get ( pk = mongo_id ) . delete ( ) return self . form_invalid ( form )
5359	def execute_nonstop_tasks ( self , tasks_cls ) : self . execute_batch_tasks ( tasks_cls , self . conf [ 'sortinghat' ] [ 'sleep_for' ] , self . conf [ 'general' ] [ 'min_update_delay' ] , False )
3257	def publish_featuretype ( self , name , store , native_crs , srs = None , jdbc_virtual_table = None , native_name = None ) : if native_crs is None : raise ValueError ( "must specify native_crs" ) srs = srs or native_crs feature_type = FeatureType ( self , store . workspace , store , name ) feature_type . dirty [ 'name' ] = name feature_type . dirty [ 'srs' ] = srs feature_type . dirty [ 'nativeCRS' ] = native_crs feature_type . enabled = True feature_type . advertised = True feature_type . title = name if native_name is not None : feature_type . native_name = native_name headers = { "Content-type" : "application/xml" , "Accept" : "application/xml" } resource_url = store . resource_url if jdbc_virtual_table is not None : feature_type . metadata = ( { 'JDBC_VIRTUAL_TABLE' : jdbc_virtual_table } ) params = dict ( ) resource_url = build_url ( self . service_url , [ "workspaces" , store . workspace . name , "datastores" , store . name , "featuretypes.xml" ] , params ) resp = self . http_request ( resource_url , method = 'post' , data = feature_type . message ( ) , headers = headers ) if resp . status_code not in ( 200 , 201 , 202 ) : FailedRequestError ( 'Failed to publish feature type {} : {}, {}' . format ( name , resp . status_code , resp . text ) ) self . _cache . clear ( ) feature_type . fetch ( ) return feature_type
2419	def write_extracted_licenses ( lics , out ) : write_value ( 'LicenseID' , lics . identifier , out ) if lics . full_name is not None : write_value ( 'LicenseName' , lics . full_name , out ) if lics . comment is not None : write_text_value ( 'LicenseComment' , lics . comment , out ) for xref in sorted ( lics . cross_ref ) : write_value ( 'LicenseCrossReference' , xref , out ) write_text_value ( 'ExtractedText' , lics . text , out )
7022	def pklc_fovcatalog_objectinfo ( pklcdir , fovcatalog , fovcatalog_columns = [ 0 , 1 , 2 , 6 , 7 , 8 , 9 , 10 , 11 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 ] , fovcatalog_colnames = [ 'objectid' , 'ra' , 'decl' , 'jmag' , 'jmag_err' , 'hmag' , 'hmag_err' , 'kmag' , 'kmag_err' , 'bmag' , 'vmag' , 'rmag' , 'imag' , 'sdssu' , 'sdssg' , 'sdssr' , 'sdssi' , 'sdssz' ] , fovcatalog_colformats = ( 'U20,f8,f8,' 'f8,f8,' 'f8,f8,' 'f8,f8,' 'f8,f8,f8,f8,' 'f8,f8,f8,' 'f8,f8' ) ) : if fovcatalog . endswith ( '.gz' ) : catfd = gzip . open ( fovcatalog ) else : catfd = open ( fovcatalog ) fovcat = np . genfromtxt ( catfd , usecols = fovcatalog_columns , names = fovcatalog_colnames , dtype = fovcatalog_colformats ) catfd . close ( ) pklclist = sorted ( glob . glob ( os . path . join ( pklcdir , '*HAT*-pklc.pkl' ) ) ) updatedpklcs , failedpklcs = [ ] , [ ] for pklc in pklclist : lcdict = read_hatpi_pklc ( pklc ) objectid = lcdict [ 'objectid' ] catind = np . where ( fovcat [ 'objectid' ] == objectid ) if len ( catind ) > 0 and catind [ 0 ] : lcdict [ 'objectinfo' ] . update ( { x : y for x , y in zip ( fovcatalog_colnames , [ np . asscalar ( fovcat [ z ] [ catind ] ) for z in fovcatalog_colnames ] ) } ) with open ( pklc + '-tmp' , 'wb' ) as outfd : pickle . dump ( lcdict , outfd , pickle . HIGHEST_PROTOCOL ) if os . path . exists ( pklc + '-tmp' ) : shutil . move ( pklc + '-tmp' , pklc ) LOGINFO ( 'updated %s with catalog info for %s at %.3f, %.3f OK' % ( pklc , objectid , lcdict [ 'objectinfo' ] [ 'ra' ] , lcdict [ 'objectinfo' ] [ 'decl' ] ) ) updatedpklcs . append ( pklc ) else : failedpklcs . append ( pklc ) return updatedpklcs , failedpklcs
11941	def mark_all_read ( user ) : BackendClass = stored_messages_settings . STORAGE_BACKEND backend = BackendClass ( ) backend . inbox_purge ( user )
11923	def parse ( self ) : if exists ( self . filepath ) : content = open ( self . filepath ) . read ( ) . decode ( charset ) else : content = "" try : config = toml . loads ( content ) except toml . TomlSyntaxError : raise ConfigSyntaxError return config
9551	def _init_unique_sets ( self ) : ks = dict ( ) for t in self . _unique_checks : key = t [ 0 ] ks [ key ] = set ( ) return ks
8064	def precmd ( self , line ) : args = shlex . split ( line or "" ) if args and 'cookie=' in args [ - 1 ] : cookie_index = line . index ( 'cookie=' ) cookie = line [ cookie_index + 7 : ] line = line [ : cookie_index ] . strip ( ) self . cookie = cookie if line . startswith ( '#' ) : return '' elif '=' in line : cmdname = line . partition ( " " ) [ 0 ] if hasattr ( self , "do_%s" % cmdname ) : return line if not line . startswith ( "set " ) : return "set " + line else : return line if len ( args ) and args [ 0 ] in self . shortcuts : return "%s %s" % ( self . shortcuts [ args [ 0 ] ] , " " . join ( args [ 1 : ] ) ) else : return line
8308	def ensure_pycairo_context ( self , ctx ) : if self . cairocffi and isinstance ( ctx , self . cairocffi . Context ) : from shoebot . util . cairocffi . cairocffi_to_pycairo import _UNSAFE_cairocffi_context_to_pycairo return _UNSAFE_cairocffi_context_to_pycairo ( ctx ) else : return ctx
1707	def run ( command , data = None , timeout = None , kill_timeout = None , env = None , cwd = None ) : command = expand_args ( command ) history = [ ] for c in command : if len ( history ) : data = history [ - 1 ] . std_out [ 0 : 10 * 1024 ] cmd = Command ( c ) try : out , err = cmd . run ( data , timeout , kill_timeout , env , cwd ) status_code = cmd . returncode except OSError as e : out , err = '' , u"\n" . join ( [ e . strerror , traceback . format_exc ( ) ] ) status_code = 127 r = Response ( process = cmd ) r . command = c r . std_out = out r . std_err = err r . status_code = status_code history . append ( r ) r = history . pop ( ) r . history = history return r
8068	def refresh ( self ) : self . reset ( ) self . parse ( self . source ) return self . output ( )
66	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , alpha = 1.0 , size = 1 , copy = True , raise_if_out_of_image = False , thickness = None ) : if thickness is not None : ia . warn_deprecated ( "Usage of argument 'thickness' in BoundingBox.draw_on_image() " "is deprecated. The argument was renamed to 'size'." ) size = thickness if raise_if_out_of_image and self . is_out_of_image ( image ) : raise Exception ( "Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s." % ( self . x1 , self . y1 , self . x2 , self . y2 , image . shape ) ) result = np . copy ( image ) if copy else image if isinstance ( color , ( tuple , list ) ) : color = np . uint8 ( color ) for i in range ( size ) : y1 , y2 , x1 , x2 = self . y1_int , self . y2_int , self . x1_int , self . x2_int if self . is_fully_within_image ( image ) : y1 = np . clip ( y1 , 0 , image . shape [ 0 ] - 1 ) y2 = np . clip ( y2 , 0 , image . shape [ 0 ] - 1 ) x1 = np . clip ( x1 , 0 , image . shape [ 1 ] - 1 ) x2 = np . clip ( x2 , 0 , image . shape [ 1 ] - 1 ) y = [ y1 - i , y1 - i , y2 + i , y2 + i ] x = [ x1 - i , x2 + i , x2 + i , x1 - i ] rr , cc = skimage . draw . polygon_perimeter ( y , x , shape = result . shape ) if alpha >= 0.99 : result [ rr , cc , : ] = color else : if ia . is_float_array ( result ) : result [ rr , cc , : ] = ( 1 - alpha ) * result [ rr , cc , : ] + alpha * color result = np . clip ( result , 0 , 255 ) else : input_dtype = result . dtype result = result . astype ( np . float32 ) result [ rr , cc , : ] = ( 1 - alpha ) * result [ rr , cc , : ] + alpha * color result = np . clip ( result , 0 , 255 ) . astype ( input_dtype ) return result
10749	def validate_sceneInfo ( self ) : if self . sceneInfo . prefix not in self . __satellitesMap : logger . error ( 'Google Downloader: Prefix of %s (%s) is invalid' % ( self . sceneInfo . name , self . sceneInfo . prefix ) ) raise WrongSceneNameError ( 'Google Downloader: Prefix of %s (%s) is invalid' % ( self . sceneInfo . name , self . sceneInfo . prefix ) )
6244	def draw ( self , projection_matrix = None , view_matrix = None , camera_matrix = None , time = 0 ) : if self . mesh_program : self . mesh_program . draw ( self , projection_matrix = projection_matrix , view_matrix = view_matrix , camera_matrix = camera_matrix , time = time )
9932	def get_refkey ( self , obj , referent ) : if isinstance ( obj , dict ) : for k , v in obj . items ( ) : if v is referent : return " (via its %r key)" % k for k in dir ( obj ) + [ '__dict__' ] : if getattr ( obj , k , None ) is referent : return " (via its %r attribute)" % k return ""
400	def normalized_mean_square_error ( output , target , name = "normalized_mean_squared_error_loss" ) : if output . get_shape ( ) . ndims == 2 : nmse_a = tf . sqrt ( tf . reduce_sum ( tf . squared_difference ( output , target ) , axis = 1 ) ) nmse_b = tf . sqrt ( tf . reduce_sum ( tf . square ( target ) , axis = 1 ) ) elif output . get_shape ( ) . ndims == 3 : nmse_a = tf . sqrt ( tf . reduce_sum ( tf . squared_difference ( output , target ) , axis = [ 1 , 2 ] ) ) nmse_b = tf . sqrt ( tf . reduce_sum ( tf . square ( target ) , axis = [ 1 , 2 ] ) ) elif output . get_shape ( ) . ndims == 4 : nmse_a = tf . sqrt ( tf . reduce_sum ( tf . squared_difference ( output , target ) , axis = [ 1 , 2 , 3 ] ) ) nmse_b = tf . sqrt ( tf . reduce_sum ( tf . square ( target ) , axis = [ 1 , 2 , 3 ] ) ) nmse = tf . reduce_mean ( nmse_a / nmse_b , name = name ) return nmse
8675	def load_keys ( key_file , origin_passphrase , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) click . echo ( 'Importing all keys from {0}...' . format ( key_file ) ) stash . load ( origin_passphrase , key_file = key_file ) click . echo ( 'Import complete!' )
12865	def setup ( self , app ) : super ( ) . setup ( app ) self . database . initialize ( connect ( self . cfg . connection , ** self . cfg . connection_params ) ) if self . database . database == ':memory:' : self . cfg . connection_manual = True if not self . cfg . migrations_enabled : return self . router = Router ( self . database , migrate_dir = self . cfg . migrations_path ) def pw_migrate ( name : str = None , fake : bool = False ) : self . router . run ( name , fake = fake ) self . app . manage . command ( pw_migrate ) def pw_rollback ( name : str = None ) : if not name : name = self . router . done [ - 1 ] self . router . rollback ( name ) self . app . manage . command ( pw_rollback ) def pw_create ( name : str = 'auto' , auto : bool = False ) : if auto : auto = list ( self . models . values ( ) ) self . router . create ( name , auto ) self . app . manage . command ( pw_create ) def pw_list ( ) : self . router . logger . info ( 'Migrations are done:' ) self . router . logger . info ( '\n' . join ( self . router . done ) ) self . router . logger . info ( '' ) self . router . logger . info ( 'Migrations are undone:' ) self . router . logger . info ( '\n' . join ( self . router . diff ) ) self . app . manage . command ( pw_list ) @ self . app . manage . command def pw_merge ( ) : self . router . merge ( ) self . app . manage . command ( pw_merge )
10410	def finalize_canonical_averages ( number_of_nodes , ps , canonical_averages , alpha , ) : spanning_cluster = ( ( 'percolation_probability_mean' in canonical_averages . dtype . names ) and 'percolation_probability_m2' in canonical_averages . dtype . names ) ret = np . empty_like ( canonical_averages , dtype = finalized_canonical_averages_dtype ( spanning_cluster = spanning_cluster ) , ) n = canonical_averages [ 'number_of_runs' ] sqrt_n = np . sqrt ( canonical_averages [ 'number_of_runs' ] ) ret [ 'number_of_runs' ] = n ret [ 'p' ] = ps ret [ 'alpha' ] = alpha def _transform ( original_key , final_key = None , normalize = False , transpose = False , ) : if final_key is None : final_key = original_key keys_mean = [ '{}_mean' . format ( key ) for key in [ original_key , final_key ] ] keys_std = [ '{}_m2' . format ( original_key ) , '{}_std' . format ( final_key ) , ] key_ci = '{}_ci' . format ( final_key ) ret [ keys_mean [ 1 ] ] = canonical_averages [ keys_mean [ 0 ] ] if normalize : ret [ keys_mean [ 1 ] ] /= number_of_nodes array = canonical_averages [ keys_std [ 0 ] ] result = np . sqrt ( ( array . T if transpose else array ) / ( n - 1 ) ) ret [ keys_std [ 1 ] ] = ( result . T if transpose else result ) if normalize : ret [ keys_std [ 1 ] ] /= number_of_nodes array = ret [ keys_std [ 1 ] ] scale = ( array . T if transpose else array ) / sqrt_n array = ret [ keys_mean [ 1 ] ] mean = ( array . T if transpose else array ) result = scipy . stats . t . interval ( 1 - alpha , df = n - 1 , loc = mean , scale = scale , ) ( ret [ key_ci ] [ ... , 0 ] , ret [ key_ci ] [ ... , 1 ] ) = ( [ my_array . T for my_array in result ] if transpose else result ) if spanning_cluster : _transform ( 'percolation_probability' ) _transform ( 'max_cluster_size' , 'percolation_strength' , normalize = True ) _transform ( 'moments' , normalize = True , transpose = True ) return ret
3495	def reaction_elements ( reaction ) : c_elements = [ coeff * met . elements . get ( 'C' , 0 ) for met , coeff in iteritems ( reaction . metabolites ) ] return [ elem for elem in c_elements if elem != 0 ]
6996	def spline_fit_magseries ( times , mags , errs , period , knotfraction = 0.01 , maxknots = 30 , sigclip = 30.0 , plotfit = False , ignoreinitfail = False , magsarefluxes = False , verbose = True ) : if errs is None : errs = npfull_like ( mags , 0.005 ) stimes , smags , serrs = sigclip_magseries ( times , mags , errs , sigclip = sigclip , magsarefluxes = magsarefluxes ) nzind = npnonzero ( serrs ) stimes , smags , serrs = stimes [ nzind ] , smags [ nzind ] , serrs [ nzind ] phase , pmags , perrs , ptimes , mintime = ( get_phased_quantities ( stimes , smags , serrs , period ) ) nobs = len ( phase ) nknots = int ( npfloor ( knotfraction * nobs ) ) nknots = maxknots if nknots > maxknots else nknots splineknots = nplinspace ( phase [ 0 ] + 0.01 , phase [ - 1 ] - 0.01 , num = nknots ) phase_diffs_ind = npdiff ( phase ) > 0.0 incphase_ind = npconcatenate ( ( nparray ( [ True ] ) , phase_diffs_ind ) ) phase , pmags , perrs = ( phase [ incphase_ind ] , pmags [ incphase_ind ] , perrs [ incphase_ind ] ) spl = LSQUnivariateSpline ( phase , pmags , t = splineknots , w = 1.0 / perrs ) fitmags = spl ( phase ) fitchisq = npsum ( ( ( fitmags - pmags ) * ( fitmags - pmags ) ) / ( perrs * perrs ) ) fitredchisq = fitchisq / ( len ( pmags ) - nknots - 1 ) if verbose : LOGINFO ( 'spline fit done. nknots = %s, ' 'chisq = %.5f, reduced chisq = %.5f' % ( nknots , fitchisq , fitredchisq ) ) if not magsarefluxes : fitmagminind = npwhere ( fitmags == npmax ( fitmags ) ) else : fitmagminind = npwhere ( fitmags == npmin ( fitmags ) ) if len ( fitmagminind [ 0 ] ) > 1 : fitmagminind = ( fitmagminind [ 0 ] [ 0 ] , ) magseriesepoch = ptimes [ fitmagminind ] returndict = { 'fittype' : 'spline' , 'fitinfo' : { 'nknots' : nknots , 'fitmags' : fitmags , 'fitepoch' : magseriesepoch } , 'fitchisq' : fitchisq , 'fitredchisq' : fitredchisq , 'fitplotfile' : None , 'magseries' : { 'times' : ptimes , 'phase' : phase , 'mags' : pmags , 'errs' : perrs , 'magsarefluxes' : magsarefluxes } , } if plotfit and isinstance ( plotfit , str ) : make_fit_plot ( phase , pmags , perrs , fitmags , period , mintime , magseriesepoch , plotfit , magsarefluxes = magsarefluxes ) returndict [ 'fitplotfile' ] = plotfit return returndict
6468	def csi_wrap ( self , value , capname , * args ) : if isinstance ( value , str ) : value = value . encode ( 'utf-8' ) return b'' . join ( [ self . csi ( capname , * args ) , value , self . csi ( 'sgr0' ) , ] )
5556	def _filter_by_zoom ( element = None , conf_string = None , zoom = None ) : for op_str , op_func in [ ( "=" , operator . eq ) , ( "<=" , operator . le ) , ( ">=" , operator . ge ) , ( "<" , operator . lt ) , ( ">" , operator . gt ) , ] : if conf_string . startswith ( op_str ) : return element if op_func ( zoom , _strip_zoom ( conf_string , op_str ) ) else None
257	def print_round_trip_stats ( round_trips , hide_pos = False ) : stats = gen_round_trip_stats ( round_trips ) print_table ( stats [ 'summary' ] , float_format = '{:.2f}' . format , name = 'Summary stats' ) print_table ( stats [ 'pnl' ] , float_format = '${:.2f}' . format , name = 'PnL stats' ) print_table ( stats [ 'duration' ] , float_format = '{:.2f}' . format , name = 'Duration stats' ) print_table ( stats [ 'returns' ] * 100 , float_format = '{:.2f}%' . format , name = 'Return stats' ) if not hide_pos : stats [ 'symbols' ] . columns = stats [ 'symbols' ] . columns . map ( format_asset ) print_table ( stats [ 'symbols' ] * 100 , float_format = '{:.2f}%' . format , name = 'Symbol stats' )
8510	def _predict ( self , X , method = 'fprop' ) : import theano X_sym = self . trainer . model . get_input_space ( ) . make_theano_batch ( ) y_sym = getattr ( self . trainer . model , method ) ( X_sym ) f = theano . function ( [ X_sym ] , y_sym , allow_input_downcast = True ) return f ( X )
1932	def get_description ( self , name : str ) -> str : if name not in self . _vars : raise ConfigError ( f"{self.name}.{name} not defined." ) return self . _vars [ name ] . description
975	def _createBucket ( self , index ) : if index < self . minIndex : if index == self . minIndex - 1 : self . bucketMap [ index ] = self . _newRepresentation ( self . minIndex , index ) self . minIndex = index else : self . _createBucket ( index + 1 ) self . _createBucket ( index ) else : if index == self . maxIndex + 1 : self . bucketMap [ index ] = self . _newRepresentation ( self . maxIndex , index ) self . maxIndex = index else : self . _createBucket ( index - 1 ) self . _createBucket ( index )
5070	def format_price ( price , currency = '$' ) : if int ( price ) == price : return '{}{}' . format ( currency , int ( price ) ) return '{}{:0.2f}' . format ( currency , price )
25	def store_args ( method ) : argspec = inspect . getfullargspec ( method ) defaults = { } if argspec . defaults is not None : defaults = dict ( zip ( argspec . args [ - len ( argspec . defaults ) : ] , argspec . defaults ) ) if argspec . kwonlydefaults is not None : defaults . update ( argspec . kwonlydefaults ) arg_names = argspec . args [ 1 : ] @ functools . wraps ( method ) def wrapper ( * positional_args , ** keyword_args ) : self = positional_args [ 0 ] args = defaults . copy ( ) for name , value in zip ( arg_names , positional_args [ 1 : ] ) : args [ name ] = value args . update ( keyword_args ) self . __dict__ . update ( args ) return method ( * positional_args , ** keyword_args ) return wrapper
12609	def _query_data ( data , field_names = None , operators = '__eq__' ) : if field_names is None : field_names = list ( data . keys ( ) ) if isinstance ( field_names , str ) : field_names = [ field_names ] sample = OrderedDict ( [ ( fn , data [ fn ] ) for fn in field_names ] ) return _query_sample ( sample , operators = operators )
7290	def make_key ( * args , ** kwargs ) : sep = kwargs . get ( 'sep' , u"_" ) exclude_last_string = kwargs . get ( 'exclude_last_string' , False ) string_array = [ ] for arg in args : if isinstance ( arg , list ) : string_array . append ( six . text_type ( sep . join ( arg ) ) ) else : if exclude_last_string : new_key_array = arg . split ( sep ) [ : - 1 ] if len ( new_key_array ) > 0 : string_array . append ( make_key ( new_key_array ) ) else : string_array . append ( six . text_type ( arg ) ) return sep . join ( string_array )
1346	def gradient ( self , image , label ) : _ , gradient = self . predictions_and_gradient ( image , label ) return gradient
1433	def custom ( cls , customgrouper ) : if customgrouper is None : raise TypeError ( "Argument to custom() must be ICustomGrouping instance or classpath" ) if not isinstance ( customgrouper , ICustomGrouping ) and not isinstance ( customgrouper , str ) : raise TypeError ( "Argument to custom() must be ICustomGrouping instance or classpath" ) serialized = default_serializer . serialize ( customgrouper ) return cls . custom_serialized ( serialized , is_java = False )
10075	def merge_with_published ( self ) : pid , first = self . fetch_published ( ) lca = first . revisions [ self [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] ] args = [ lca . dumps ( ) , first . dumps ( ) , self . dumps ( ) ] for arg in args : del arg [ '$schema' ] , arg [ '_deposit' ] args . append ( { } ) m = Merger ( * args ) try : m . run ( ) except UnresolvedConflictsException : raise MergeConflict ( ) return patch ( m . unified_patches , lca )
10561	def _mutagen_fields_to_single_value ( metadata ) : return dict ( ( k , v [ 0 ] ) for k , v in metadata . items ( ) if v )
1961	def sys_fsync ( self , fd ) : ret = 0 try : self . files [ fd ] . sync ( ) except IndexError : ret = - errno . EBADF except FdError : ret = - errno . EINVAL return ret
9133	def get_data_dir ( module_name : str ) -> str : module_name = module_name . lower ( ) data_dir = os . path . join ( BIO2BEL_DIR , module_name ) os . makedirs ( data_dir , exist_ok = True ) return data_dir
12391	def indexesOptional ( f ) : stack = inspect . stack ( ) _NO_INDEX_CHECK_NEEDED . add ( '%s.%s.%s' % ( f . __module__ , stack [ 1 ] [ 3 ] , f . __name__ ) ) del stack return f
1300	def WindowFromPoint ( x : int , y : int ) -> int : return ctypes . windll . user32 . WindowFromPoint ( ctypes . wintypes . POINT ( x , y ) )
10243	def count_citation_years ( graph : BELGraph ) -> typing . Counter [ int ] : result = defaultdict ( set ) for _ , _ , data in graph . edges ( data = True ) : if CITATION not in data or CITATION_DATE not in data [ CITATION ] : continue try : dt = _ensure_datetime ( data [ CITATION ] [ CITATION_DATE ] ) result [ dt . year ] . add ( ( data [ CITATION ] [ CITATION_TYPE ] , data [ CITATION ] [ CITATION_REFERENCE ] ) ) except Exception : continue return count_dict_values ( result )
9086	def _sort ( self , concepts , sort = None , language = 'any' , reverse = False ) : sorted = copy . copy ( concepts ) if sort : sorted . sort ( key = methodcaller ( '_sortkey' , sort , language ) , reverse = reverse ) return sorted
11965	def _oct_to_dec ( ip , check = True ) : if check and not is_oct ( ip ) : raise ValueError ( '_oct_to_dec: invalid IP: "%s"' % ip ) if isinstance ( ip , int ) : ip = oct ( ip ) return int ( str ( ip ) , 8 )
2291	def orient_directed_graph ( self , data , dag , alg = 'HC' ) : alg_dic = { 'HC' : hill_climbing , 'HCr' : hill_climbing_with_removal , 'tabu' : tabu_search , 'EHC' : exploratory_hill_climbing } return alg_dic [ alg ] ( data , dag , nh = self . nh , nb_runs = self . nb_runs , gpu = self . gpu , nb_jobs = self . nb_jobs , lr = self . lr , train_epochs = self . train_epochs , test_epochs = self . test_epochs , verbose = self . verbose )
3331	def release ( self ) : me = currentThread ( ) self . __condition . acquire ( ) try : if self . __writer is me : self . __writercount -= 1 if not self . __writercount : self . __writer = None self . __condition . notifyAll ( ) elif me in self . __readers : self . __readers [ me ] -= 1 if not self . __readers [ me ] : del self . __readers [ me ] if not self . __readers : self . __condition . notifyAll ( ) else : raise ValueError ( "Trying to release unheld lock" ) finally : self . __condition . release ( )
9066	def delta ( self ) : v = float ( self . _logistic . value ) if v > 0.0 : v = 1 / ( 1 + exp ( - v ) ) else : v = exp ( v ) v = v / ( v + 1.0 ) return min ( max ( v , epsilon . tiny ) , 1 - epsilon . tiny )
13132	def parse_user ( entry , domain_groups ) : result = { } distinguished_name = get_field ( entry , 'distinguishedName' ) result [ 'domain' ] = "." . join ( distinguished_name . split ( ',DC=' ) [ 1 : ] ) result [ 'name' ] = get_field ( entry , 'name' ) result [ 'username' ] = get_field ( entry , 'sAMAccountName' ) result [ 'description' ] = get_field ( entry , 'description' ) result [ 'sid' ] = get_field ( entry , 'objectSid' ) . split ( '-' ) [ - 1 ] primary_group = get_field ( entry , 'primaryGroupID' ) member_of = entry [ 'attributes' ] . get ( 'memberOf' , [ ] ) groups = [ ] for member in member_of : for e in member . split ( ',' ) : if e . startswith ( 'CN=' ) : groups . append ( e [ 3 : ] ) groups . append ( domain_groups . get ( primary_group , '' ) ) result [ 'groups' ] = groups flags = [ ] try : uac = int ( get_field ( entry , 'userAccountControl' ) ) for flag , value in uac_flags . items ( ) : if uac & value : flags . append ( flag ) except ValueError : pass result [ 'flags' ] = flags return result
2313	def predict_proba ( self , a , b , ** kwargs ) : a = scale ( a ) . reshape ( ( - 1 , 1 ) ) b = scale ( b ) . reshape ( ( - 1 , 1 ) ) return self . anm_score ( b , a ) - self . anm_score ( a , b )
5701	def _feed_calendar_span ( gtfs , stats ) : n_feeds = _n_gtfs_sources ( gtfs ) [ 0 ] max_start = None min_end = None if n_feeds > 1 : for i in range ( n_feeds ) : feed_key = "feed_" + str ( i ) + "_" start_key = feed_key + "calendar_start" end_key = feed_key + "calendar_end" calendar_span = gtfs . conn . cursor ( ) . execute ( 'SELECT min(date), max(date) FROM trips, days ' 'WHERE trips.trip_I = days.trip_I AND trip_id LIKE ?;' , ( feed_key + '%' , ) ) . fetchone ( ) stats [ start_key ] = calendar_span [ 0 ] stats [ end_key ] = calendar_span [ 1 ] if calendar_span [ 0 ] is not None and calendar_span [ 1 ] is not None : if not max_start and not min_end : max_start = calendar_span [ 0 ] min_end = calendar_span [ 1 ] else : if gtfs . get_day_start_ut ( calendar_span [ 0 ] ) > gtfs . get_day_start_ut ( max_start ) : max_start = calendar_span [ 0 ] if gtfs . get_day_start_ut ( calendar_span [ 1 ] ) < gtfs . get_day_start_ut ( min_end ) : min_end = calendar_span [ 1 ] stats [ "latest_feed_start_date" ] = max_start stats [ "earliest_feed_end_date" ] = min_end else : stats [ "latest_feed_start_date" ] = stats [ "start_date" ] stats [ "earliest_feed_end_date" ] = stats [ "end_date" ] return stats
4939	def get_link_by_email ( self , user_email ) : try : user = User . objects . get ( email = user_email ) try : return self . get ( user_id = user . id ) except EnterpriseCustomerUser . DoesNotExist : pass except User . DoesNotExist : pass try : return PendingEnterpriseCustomerUser . objects . get ( user_email = user_email ) except PendingEnterpriseCustomerUser . DoesNotExist : pass return None
13644	def append_arguments ( klass , sub_parsers , default_epilog , general_arguments ) : entry_name = hump_to_underscore ( klass . __name__ ) . replace ( '_component' , '' ) epilog = default_epilog if default_epilog else 'This tool generate by `cliez` ' 'https://www.github.com/wangwenpei/cliez' sub_parser = sub_parsers . add_parser ( entry_name , help = klass . __doc__ , epilog = epilog ) sub_parser . description = klass . add_arguments . __doc__ if hasattr ( klass , 'add_slot_args' ) : slot_args = klass . add_slot_args ( ) or [ ] for v in slot_args : sub_parser . add_argument ( * v [ 0 ] , ** v [ 1 ] ) sub_parser . description = klass . add_slot_args . __doc__ pass user_arguments = klass . add_arguments ( ) or [ ] for v in user_arguments : sub_parser . add_argument ( * v [ 0 ] , ** v [ 1 ] ) if not klass . exclude_global_option : for v in general_arguments : sub_parser . add_argument ( * v [ 0 ] , ** v [ 1 ] ) return sub_parser
11083	def help ( self , msg , args ) : output = [ ] if len ( args ) == 0 : commands = sorted ( self . _bot . dispatcher . commands . items ( ) , key = itemgetter ( 0 ) ) commands = filter ( lambda x : x [ 1 ] . is_subcmd is False , commands ) if self . _should_filter_help_commands ( msg . user ) : commands = filter ( lambda x : x [ 1 ] . admin_only is False , commands ) for name , cmd in commands : output . append ( self . _get_short_help_for_command ( name ) ) else : name = '!' + args [ 0 ] output = [ self . _get_help_for_command ( name ) ] return '\n' . join ( output )
9190	def admin_content_status_single ( request ) : uuid = request . matchdict [ 'uuid' ] try : UUID ( uuid ) except ValueError : raise httpexceptions . HTTPBadRequest ( '{} is not a valid uuid' . format ( uuid ) ) statement , sql_args = get_baking_statuses_sql ( { 'uuid' : uuid } ) with db_connect ( cursor_factory = DictCursor ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( statement , sql_args ) modules = cursor . fetchall ( ) if len ( modules ) == 0 : raise httpexceptions . HTTPBadRequest ( '{} is not a book' . format ( uuid ) ) states = [ ] collection_info = modules [ 0 ] for row in modules : message = '' state = row [ 'state' ] or 'PENDING' if state == 'FAILURE' : if row [ 'traceback' ] is not None : message = row [ 'traceback' ] latest_recipe = row [ 'latest_recipe_id' ] current_recipe = row [ 'recipe_id' ] if ( latest_recipe is not None and current_recipe != latest_recipe ) : state += ' stale_recipe' states . append ( { 'version' : row [ 'current_version' ] , 'recipe' : row [ 'recipe' ] , 'created' : str ( row [ 'created' ] ) , 'state' : state , 'state_message' : message , } ) return { 'uuid' : str ( collection_info [ 'uuid' ] ) , 'title' : collection_info [ 'name' ] . decode ( 'utf-8' ) , 'authors' : format_authors ( collection_info [ 'authors' ] ) , 'print_style' : collection_info [ 'print_style' ] , 'current_recipe' : collection_info [ 'recipe_id' ] , 'current_ident' : collection_info [ 'module_ident' ] , 'current_state' : states [ 0 ] [ 'state' ] , 'states' : states }
11524	def mfa_otp_login ( self , temp_token , one_time_pass ) : parameters = dict ( ) parameters [ 'mfaTokenId' ] = temp_token parameters [ 'otp' ] = one_time_pass response = self . request ( 'midas.mfa.otp.login' , parameters ) return response [ 'token' ]
4399	def adsSyncSetTimeoutEx ( port , nMs ) : adsSyncSetTimeoutFct = _adsDLL . AdsSyncSetTimeoutEx cms = ctypes . c_long ( nMs ) err_code = adsSyncSetTimeoutFct ( port , cms ) if err_code : raise ADSError ( err_code )
11225	def dump_OrderedDict ( self , obj , class_name = "collections.OrderedDict" ) : return { "$" + class_name : [ ( key , self . _json_convert ( value ) ) for key , value in iteritems ( obj ) ] }
9918	def validate_key ( self , key ) : try : confirmation = models . EmailConfirmation . objects . select_related ( "email__user" ) . get ( key = key ) except models . EmailConfirmation . DoesNotExist : raise serializers . ValidationError ( _ ( "The provided verification key is invalid." ) ) if confirmation . is_expired : raise serializers . ValidationError ( _ ( "That verification code has expired." ) ) self . _confirmation = confirmation return key
12448	def from_cookie_string ( self , cookie_string ) : for key_value in cookie_string . split ( ';' ) : if '=' in key_value : key , value = key_value . split ( '=' , 1 ) else : key = key_value strip_key = key . strip ( ) if strip_key and strip_key . lower ( ) not in COOKIE_ATTRIBUTE_NAMES : self [ strip_key ] = value . strip ( )
2328	def orient_directed_graph ( self , data , graph ) : warnings . warn ( "The algorithm is ran on the skeleton of the given graph." ) return self . orient_undirected_graph ( data , nx . Graph ( graph ) )
10609	def create_stream ( self , assay = None , mfr = 0.0 , P = 1.0 , T = 25.0 , normalise = True ) : if assay is None : return MaterialStream ( self , self . create_empty_assay ( ) , P , T ) if normalise : assay_total = self . get_assay_total ( assay ) else : assay_total = 1.0 return MaterialStream ( self , mfr * self . converted_assays [ assay ] / assay_total , P , T , self . _isCoal ( assay ) , self . _get_HHV ( assay ) )
509	def stripUnlearnedColumns ( self , activeArray ) : neverLearned = numpy . where ( self . _activeDutyCycles == 0 ) [ 0 ] activeArray [ neverLearned ] = 0
1610	def make_root_tuple_info ( stream_id , tuple_id ) : key = random . getrandbits ( TupleHelper . MAX_SFIXED64_RAND_BITS ) return RootTupleInfo ( stream_id = stream_id , tuple_id = tuple_id , insertion_time = time . time ( ) , key = key )
9398	def exit ( self ) : if self . _engine : self . _engine . repl . terminate ( ) self . _engine = None
2296	def featurize_row ( self , x , y ) : x = x . ravel ( ) y = y . ravel ( ) b = np . ones ( x . shape ) dx = np . cos ( np . dot ( self . W2 , np . vstack ( ( x , b ) ) ) ) . mean ( 1 ) dy = np . cos ( np . dot ( self . W2 , np . vstack ( ( y , b ) ) ) ) . mean ( 1 ) if ( sum ( dx ) > sum ( dy ) ) : return np . hstack ( ( dx , dy , np . cos ( np . dot ( self . W , np . vstack ( ( x , y , b ) ) ) ) . mean ( 1 ) ) ) else : return np . hstack ( ( dx , dy , np . cos ( np . dot ( self . W , np . vstack ( ( y , x , b ) ) ) ) . mean ( 1 ) ) )
13678	def filenumber_handle ( self ) : self . __results = [ ] self . __dirs = [ ] self . __files = [ ] self . __ftp = self . connect ( ) self . __ftp . dir ( self . args . path , self . __results . append ) self . logger . debug ( "dir results: {}" . format ( self . __results ) ) self . quit ( ) status = self . ok for data in self . __results : if "<DIR>" in data : self . __dirs . append ( str ( data . split ( ) [ 3 ] ) ) else : self . __files . append ( str ( data . split ( ) [ 2 ] ) ) self . __result = len ( self . __files ) self . logger . debug ( "result: {}" . format ( self . __result ) ) if self . __result > self . args . warning : status = self . warning if self . __result > self . args . critical : status = self . critical self . shortoutput = "Found {0} files in {1}." . format ( self . __result , self . args . path ) [ self . longoutput . append ( line ) for line in self . __results if self . __results ] self . perfdata . append ( "{path}={result};{warn};{crit};0;" . format ( crit = self . args . critical , warn = self . args . warning , result = self . __result , path = self . args . path ) ) self . logger . debug ( "Return status and output." ) status ( self . output ( ) )
12426	def _expand_targets ( self , targets , base_dir = None ) : all_targets = [ ] for target in targets : target_dirs = [ p for p in [ base_dir , os . path . dirname ( target ) ] if p ] target_dir = target_dirs and os . path . join ( * target_dirs ) or '' target = os . path . basename ( target ) target_path = os . path . join ( target_dir , target ) if os . path . exists ( target_path ) : all_targets . append ( target_path ) with open ( target_path ) as fp : for line in fp : if line . startswith ( '-r ' ) : _ , new_target = line . split ( ' ' , 1 ) all_targets . extend ( self . _expand_targets ( [ new_target . strip ( ) ] , base_dir = target_dir ) ) return all_targets
5980	def bin_up_mask_2d ( mask_2d , bin_up_factor ) : padded_array_2d = array_util . pad_2d_array_for_binning_up_with_bin_up_factor ( array_2d = mask_2d , bin_up_factor = bin_up_factor , pad_value = True ) binned_array_2d = np . zeros ( shape = ( padded_array_2d . shape [ 0 ] // bin_up_factor , padded_array_2d . shape [ 1 ] // bin_up_factor ) ) for y in range ( binned_array_2d . shape [ 0 ] ) : for x in range ( binned_array_2d . shape [ 1 ] ) : value = True for y1 in range ( bin_up_factor ) : for x1 in range ( bin_up_factor ) : padded_y = y * bin_up_factor + y1 padded_x = x * bin_up_factor + x1 if padded_array_2d [ padded_y , padded_x ] == False : value = False binned_array_2d [ y , x ] = value return binned_array_2d
4150	def plot ( self , filename = None , norm = False , ylim = None , sides = None , ** kargs ) : import pylab from pylab import ylim as plt_ylim _ = self . psd if sides is not None : if sides not in self . _sides_choices : raise errors . SpectrumChoiceError ( sides , self . _sides_choices ) if sides is None or sides == self . sides : frequencies = self . frequencies ( ) psd = self . psd sides = self . sides elif sides is not None : if self . datatype == 'complex' : if sides == 'onesided' : raise ValueError ( "sides cannot be one-sided with complex data" ) logging . debug ( "sides is different from the one provided. Converting PSD" ) frequencies = self . frequencies ( sides = sides ) psd = self . get_converted_psd ( sides ) if len ( psd ) != len ( frequencies ) : raise ValueError ( "PSD length is %s and freq length is %s" % ( len ( psd ) , len ( frequencies ) ) ) if 'ax' in list ( kargs . keys ( ) ) : save_ax = pylab . gca ( ) pylab . sca ( kargs [ 'ax' ] ) rollback = True del kargs [ 'ax' ] else : rollback = False if norm : pylab . plot ( frequencies , 10 * stools . log10 ( psd / max ( psd ) ) , ** kargs ) else : pylab . plot ( frequencies , 10 * stools . log10 ( psd ) , ** kargs ) pylab . xlabel ( 'Frequency' ) pylab . ylabel ( 'Power (dB)' ) pylab . grid ( True ) if ylim : plt_ylim ( ylim ) if sides == 'onesided' : pylab . xlim ( 0 , self . sampling / 2. ) elif sides == 'twosided' : pylab . xlim ( 0 , self . sampling ) elif sides == 'centerdc' : pylab . xlim ( - self . sampling / 2. , self . sampling / 2. ) if filename : pylab . savefig ( filename ) if rollback : pylab . sca ( save_ax ) del psd , frequencies
9773	def statuses ( ctx , page ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) page = page or 1 try : response = PolyaxonClient ( ) . job . get_statuses ( user , project_name , _job , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get status for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Statuses for Job `{}`.' . format ( _job ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No statuses found for job `{}`.' . format ( _job ) ) objects = list_dicts_to_tabulate ( [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) , status_key = 'status' ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Statuses:" ) objects . pop ( 'job' , None ) dict_tabulate ( objects , is_list_dict = True )
12320	def add_files ( self , repo , files ) : rootdir = repo . rootdir for f in files : relativepath = f [ 'relativepath' ] sourcepath = f [ 'localfullpath' ] if sourcepath is None : continue targetpath = os . path . join ( rootdir , relativepath ) try : os . makedirs ( os . path . dirname ( targetpath ) ) except : pass print ( "Updating: {}" . format ( relativepath ) ) shutil . copyfile ( sourcepath , targetpath ) with cd ( repo . rootdir ) : self . _run ( [ 'add' , relativepath ] )
2534	def validate_str_fields ( self , fields , optional , messages ) : for field_str in fields : field = getattr ( self , field_str ) if field is not None : attr = getattr ( field , '__str__' , None ) if not callable ( attr ) : messages = messages + [ '{0} must provide __str__ method.' . format ( field ) ] elif not optional : messages = messages + [ 'Package {0} can not be None.' . format ( field_str ) ] return messages
12219	def _make_all_matchers ( cls , parameters ) : for name , param in parameters : annotation = param . annotation if annotation is not Parameter . empty : yield name , cls . _make_param_matcher ( annotation , param . kind )
12267	def check_grad ( f_df , xref , stepsize = 1e-6 , tol = 1e-6 , width = 15 , style = 'round' , out = sys . stdout ) : CORRECT = u'\x1b[32m\N{CHECK MARK}\x1b[0m' INCORRECT = u'\x1b[31m\N{BALLOT X}\x1b[0m' obj , grad = wrap ( f_df , xref , size = 0 ) x0 = destruct ( xref ) df = grad ( x0 ) out . write ( tp . header ( [ "Numerical" , "Analytic" , "Error" ] , width = width , style = style ) + "\n" ) out . flush ( ) def parse_error ( number ) : failure = "\033[91m" passing = "\033[92m" warning = "\033[93m" end = "\033[0m" base = "{}{:0.3e}{}" if error < 0.1 * tol : return base . format ( passing , error , end ) elif error < tol : return base . format ( warning , error , end ) else : return base . format ( failure , error , end ) num_errors = 0 for j in range ( x0 . size ) : dx = np . zeros ( x0 . size ) dx [ j ] = stepsize df_approx = ( obj ( x0 + dx ) - obj ( x0 - dx ) ) / ( 2 * stepsize ) df_analytic = df [ j ] abs_error = np . linalg . norm ( df_approx - df_analytic ) error = abs_error if np . allclose ( abs_error , 0 ) else abs_error / ( np . linalg . norm ( df_analytic ) + np . linalg . norm ( df_approx ) ) num_errors += error >= tol errstr = CORRECT if error < tol else INCORRECT out . write ( tp . row ( [ df_approx , df_analytic , parse_error ( error ) + ' ' + errstr ] , width = width , style = style ) + "\n" ) out . flush ( ) out . write ( tp . bottom ( 3 , width = width , style = style ) + "\n" ) return num_errors
9659	def get_levels ( G ) : levels = [ ] ends = get_sinks ( G ) levels . append ( ends ) while get_direct_ancestors ( G , ends ) : ends = get_direct_ancestors ( G , ends ) levels . append ( ends ) levels . reverse ( ) return levels
12401	def satisfied_by_checked ( self , req ) : req_man = RequirementsManager ( [ req ] ) return any ( req_man . check ( * checked ) for checked in self . checked )
9639	def emit ( self , record ) : try : if self . max_messages : p = self . redis_client . pipeline ( ) p . rpush ( self . key , self . format ( record ) ) p . ltrim ( self . key , - self . max_messages , - 1 ) p . execute ( ) else : self . redis_client . rpush ( self . key , self . format ( record ) ) except redis . RedisError : pass
2080	def callback ( self , pk = None , host_config_key = '' , extra_vars = None ) : url = self . endpoint + '%s/callback/' % pk if not host_config_key : host_config_key = client . get ( url ) . json ( ) [ 'host_config_key' ] post_data = { 'host_config_key' : host_config_key } if extra_vars : post_data [ 'extra_vars' ] = parser . process_extra_vars ( list ( extra_vars ) , force_json = True ) r = client . post ( url , data = post_data , auth = None ) if r . status_code == 201 : return { 'changed' : True }
860	def getTemporalDelay ( inferenceElement , key = None ) : if inferenceElement in ( InferenceElement . prediction , InferenceElement . encodings ) : return 1 if inferenceElement in ( InferenceElement . anomalyScore , InferenceElement . anomalyLabel , InferenceElement . classification , InferenceElement . classConfidences ) : return 0 if inferenceElement in ( InferenceElement . multiStepPredictions , InferenceElement . multiStepBestPredictions ) : return int ( key ) return 0
8992	def rows_after ( self ) : rows_after = [ ] for mesh in self . produced_meshes : if mesh . is_consumed ( ) : row = mesh . consuming_row if rows_after not in rows_after : rows_after . append ( row ) return rows_after
6263	def check_glfw_version ( self ) : print ( "glfw version: {} (python wrapper version {})" . format ( glfw . get_version ( ) , glfw . __version__ ) ) if glfw . get_version ( ) < self . min_glfw_version : raise ValueError ( "Please update glfw binaries to version {} or later" . format ( self . min_glfw_version ) )
1000	def printParameters ( self ) : print "numberOfCols=" , self . numberOfCols print "cellsPerColumn=" , self . cellsPerColumn print "minThreshold=" , self . minThreshold print "newSynapseCount=" , self . newSynapseCount print "activationThreshold=" , self . activationThreshold print print "initialPerm=" , self . initialPerm print "connectedPerm=" , self . connectedPerm print "permanenceInc=" , self . permanenceInc print "permanenceDec=" , self . permanenceDec print "permanenceMax=" , self . permanenceMax print "globalDecay=" , self . globalDecay print print "doPooling=" , self . doPooling print "segUpdateValidDuration=" , self . segUpdateValidDuration print "pamLength=" , self . pamLength
12141	def load_dframe ( self , dframe ) : filename_series = dframe [ self . key ] loaded_data = filename_series . map ( self . filetype . data ) keys = [ list ( el . keys ( ) ) for el in loaded_data . values ] for key in set ( ) . union ( * keys ) : key_exists = key in dframe . columns if key_exists : self . warning ( "Appending '_data' suffix to data key %r to avoid" "overwriting existing metadata with the same name." % key ) suffix = '_data' if key_exists else '' dframe [ key + suffix ] = loaded_data . map ( lambda x : x . get ( key , np . nan ) ) return dframe
7026	def objectlist_conesearch ( racenter , declcenter , searchradiusarcsec , gaia_mirror = None , columns = ( 'source_id' , 'ra' , 'dec' , 'phot_g_mean_mag' , 'l' , 'b' , 'parallax' , 'parallax_error' , 'pmra' , 'pmra_error' , 'pmdec' , 'pmdec_error' ) , extra_filter = None , returnformat = 'csv' , forcefetch = False , cachedir = '~/.astrobase/gaia-cache' , verbose = True , timeout = 15.0 , refresh = 2.0 , maxtimeout = 300.0 , maxtries = 3 , complete_query_later = True ) : query = ( "select {columns}, " "(DISTANCE(POINT('ICRS', " "{{table}}.ra, {{table}}.dec), " "POINT('ICRS', {ra_center:.5f}, {decl_center:.5f})))*3600.0 " "AS dist_arcsec " "from {{table}} where " "CONTAINS(POINT('ICRS',{{table}}.ra, {{table}}.dec)," "CIRCLE('ICRS',{ra_center:.5f},{decl_center:.5f}," "{search_radius:.6f}))=1 " "{extra_filter_str}" "ORDER by dist_arcsec asc " ) if extra_filter is not None : extra_filter_str = ' and %s ' % extra_filter else : extra_filter_str = '' formatted_query = query . format ( ra_center = racenter , decl_center = declcenter , search_radius = searchradiusarcsec / 3600.0 , extra_filter_str = extra_filter_str , columns = ', ' . join ( columns ) ) return tap_query ( formatted_query , gaia_mirror = gaia_mirror , returnformat = returnformat , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , complete_query_later = complete_query_later )
1821	def SETPE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . PF , 1 , 0 ) )
5121	def reset_colors ( self ) : for k , e in enumerate ( self . g . edges ( ) ) : self . g . set_ep ( e , 'edge_color' , self . edge2queue [ k ] . colors [ 'edge_color' ] ) for v in self . g . nodes ( ) : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_fill_color' ] )
1506	def template_heron_tools_hcl ( cl_args , masters , zookeepers ) : heron_tools_hcl_template = "%s/standalone/templates/heron_tools.template.hcl" % cl_args [ "config_path" ] heron_tools_hcl_actual = "%s/standalone/resources/heron_tools.hcl" % cl_args [ "config_path" ] single_master = masters [ 0 ] template_file ( heron_tools_hcl_template , heron_tools_hcl_actual , { "<zookeeper_host:zookeeper_port>" : "," . join ( [ '%s' % zk if ":" in zk else '%s:2181' % zk for zk in zookeepers ] ) , "<heron_tracker_executable>" : '"%s/heron-tracker"' % config . get_heron_bin_dir ( ) , "<heron_tools_hostname>" : '"%s"' % get_hostname ( single_master , cl_args ) , "<heron_ui_executable>" : '"%s/heron-ui"' % config . get_heron_bin_dir ( ) } )
3922	def keypress ( self , size , key ) : self . _coroutine_queue . put ( self . _client . set_active ( ) ) self . _coroutine_queue . put ( self . _conversation . update_read_timestamp ( ) ) return super ( ) . keypress ( size , key )
4253	def org_by_name ( self , hostname ) : addr = self . _gethostbyname ( hostname ) return self . org_by_addr ( addr )
5641	def remove_dangling_shapes ( db_conn ) : db_conn . execute ( DELETE_SHAPES_NOT_REFERENCED_IN_TRIPS_SQL ) SELECT_MIN_MAX_SHAPE_BREAKS_BY_TRIP_I_SQL = "SELECT trips.trip_I, shape_id, min(shape_break) as min_shape_break, max(shape_break) as max_shape_break FROM trips, stop_times WHERE trips.trip_I=stop_times.trip_I GROUP BY trips.trip_I" trip_min_max_shape_seqs = pandas . read_sql ( SELECT_MIN_MAX_SHAPE_BREAKS_BY_TRIP_I_SQL , db_conn ) rows = [ ] for row in trip_min_max_shape_seqs . itertuples ( ) : shape_id , min_shape_break , max_shape_break = row . shape_id , row . min_shape_break , row . max_shape_break if min_shape_break is None or max_shape_break is None : min_shape_break = float ( '-inf' ) max_shape_break = float ( '-inf' ) rows . append ( ( shape_id , min_shape_break , max_shape_break ) ) DELETE_SQL_BASE = "DELETE FROM shapes WHERE shape_id=? AND (seq<? OR seq>?)" db_conn . executemany ( DELETE_SQL_BASE , rows ) remove_dangling_shapes_references ( db_conn )
11076	def load_user_rights ( self , user ) : if user . username in self . admins : user . is_admin = True elif not hasattr ( user , 'is_admin' ) : user . is_admin = False
5078	def get_current_course_run ( course , users_active_course_runs ) : current_course_run = None filtered_course_runs = [ ] all_course_runs = course [ 'course_runs' ] if users_active_course_runs : current_course_run = get_closest_course_run ( users_active_course_runs ) else : for course_run in all_course_runs : if is_course_run_enrollable ( course_run ) and is_course_run_upgradeable ( course_run ) : filtered_course_runs . append ( course_run ) if not filtered_course_runs : filtered_course_runs = all_course_runs if filtered_course_runs : current_course_run = get_closest_course_run ( filtered_course_runs ) return current_course_run
1655	def IsDerivedFunction ( clean_lines , linenum ) : for i in xrange ( linenum , max ( - 1 , linenum - 10 ) , - 1 ) : match = Match ( r'^([^()]*\w+)\(' , clean_lines . elided [ i ] ) if match : line , _ , closing_paren = CloseExpression ( clean_lines , i , len ( match . group ( 1 ) ) ) return ( closing_paren >= 0 and Search ( r'\boverride\b' , line [ closing_paren : ] ) ) return False
12776	def forward_dynamics ( self , torques , start = 0 , states = None ) : if states is not None : self . skeleton . set_body_states ( states ) for frame_no , torque in enumerate ( torques ) : if frame_no < start : continue if frame_no >= end : break self . ode_space . collide ( None , self . on_collision ) self . skeleton . add_torques ( torque ) self . ode_world . step ( self . dt ) yield self . ode_contactgroup . empty ( )
13477	def NonUniformImage ( x , y , z , ax = None , fig = None , cmap = None , alpha = None , scalex = True , scaley = True , add_cbar = True , ** kwargs ) : if ax is None and fig is None : fig , ax = _setup_axes ( ) elif ax is None : ax = fig . gca ( ) elif fig is None : fig = ax . get_figure ( ) norm = kwargs . get ( 'norm' , None ) im = _mplim . NonUniformImage ( ax , ** kwargs ) vmin = kwargs . pop ( 'vmin' , _np . min ( z ) ) vmax = kwargs . pop ( 'vmax' , _np . max ( z ) ) if cmap is not None : im . set_cmap ( cmap ) m = _cm . ScalarMappable ( cmap = im . get_cmap ( ) , norm = norm ) m . set_array ( z ) if add_cbar : cax , cb = _cb ( ax = ax , im = m , fig = fig ) if alpha is not None : im . set_alpha ( alpha ) im . set_data ( x , y , z ) ax . images . append ( im ) if scalex : xmin = min ( x ) xmax = max ( x ) ax . set_xlim ( xmin , xmax ) if scaley : ymin = min ( y ) ymax = max ( y ) ax . set_ylim ( ymin , ymax ) return _SI ( im = im , cb = cb , cax = cax )
1730	def match ( self , string , pos ) : return self . pat . match ( string , int ( pos ) )
10192	def get_geoip ( ip ) : reader = geolite2 . reader ( ) ip_data = reader . get ( ip ) or { } return ip_data . get ( 'country' , { } ) . get ( 'iso_code' )
4493	def list_ ( args ) : osf = _setup_osf ( args ) project = osf . project ( args . project ) for store in project . storages : prefix = store . name for file_ in store . files : path = file_ . path if path . startswith ( '/' ) : path = path [ 1 : ] print ( os . path . join ( prefix , path ) )
4249	def netspeed_by_name ( self , hostname ) : addr = self . _gethostbyname ( hostname ) return self . netspeed_by_addr ( addr )
3898	def compile_protofile ( proto_file_path ) : out_file = tempfile . mkstemp ( ) [ 1 ] try : subprocess . check_output ( [ 'protoc' , '--include_source_info' , '--descriptor_set_out' , out_file , proto_file_path ] ) except subprocess . CalledProcessError as e : sys . exit ( 'protoc returned status {}' . format ( e . returncode ) ) return out_file
8034	def write ( self , text , newline = False ) : if not self . isatty : self . fobj . write ( '%s\n' % text ) return msg_len = len ( text ) self . max_len = max ( self . max_len , msg_len ) self . fobj . write ( "\r%-*s" % ( self . max_len , text ) ) if newline or not self . isatty : self . fobj . write ( '\n' ) self . max_len = 0
13137	def _unicode ( string ) : for encoding in [ 'utf-8' , 'latin1' ] : try : result = unicode ( string , encoding ) return result except UnicodeDecodeError : pass result = unicode ( string , 'utf-8' , 'replace' ) return result
833	def createInput ( self ) : print "-" * 70 + "Creating a random input vector" + "-" * 70 self . inputArray [ 0 : ] = 0 for i in range ( self . inputSize ) : self . inputArray [ i ] = random . randrange ( 2 )
8927	def dist ( ctx , devpi = False , egg = False , wheel = False , auto = True ) : config . load ( ) cmd = [ "python" , "setup.py" , "sdist" ] if auto : egg = sys . version_info . major == 2 try : import wheel as _ wheel = True except ImportError : wheel = False if egg : cmd . append ( "bdist_egg" ) if wheel : cmd . append ( "bdist_wheel" ) ctx . run ( "invoke clean --all build --docs test check" ) ctx . run ( ' ' . join ( cmd ) ) if devpi : ctx . run ( "devpi upload dist/*" )
662	def checkMatch ( input , prediction , sparse = True , verbosity = 0 ) : if sparse : activeElementsInInput = set ( input ) activeElementsInPrediction = set ( prediction ) else : activeElementsInInput = set ( input . nonzero ( ) [ 0 ] ) activeElementsInPrediction = set ( prediction . nonzero ( ) [ 0 ] ) totalActiveInPrediction = len ( activeElementsInPrediction ) totalActiveInInput = len ( activeElementsInInput ) foundInInput = len ( activeElementsInPrediction . intersection ( activeElementsInInput ) ) missingFromInput = len ( activeElementsInPrediction . difference ( activeElementsInInput ) ) missingFromPrediction = len ( activeElementsInInput . difference ( activeElementsInPrediction ) ) if verbosity >= 1 : print "preds. found in input:" , foundInInput , "out of" , totalActiveInPrediction , print "; preds. missing from input:" , missingFromInput , "out of" , totalActiveInPrediction , print "; unexpected active in input:" , missingFromPrediction , "out of" , totalActiveInInput return ( foundInInput , totalActiveInInput , missingFromInput , totalActiveInPrediction )
11171	def optionhelp ( self , indent = 0 , maxindent = 25 , width = 79 ) : def makelabels ( option ) : labels = '%*s--%s' % ( indent , ' ' , option . name ) if option . abbreviation : labels += ', -' + option . abbreviation return labels + ': ' docs = [ ] helpindent = _autoindent ( [ makelabels ( o ) for o in self . options . values ( ) ] , indent , maxindent ) for name in self . option_order : option = self . options [ name ] labels = makelabels ( option ) helpstring = "%s(%s). %s" % ( option . formatname , option . strvalue , option . docs ) wrapped = self . _wrap_labelled ( labels , helpstring , helpindent , width ) docs . extend ( wrapped ) return '\n' . join ( docs )
9087	async def update ( self ) -> None : _LOGGER . debug ( "Requesting state update from server (S00, S14)" ) await asyncio . gather ( self . send_command ( 'S00' ) , self . send_command ( 'S14' ) , )
8894	def add_to_deleted_models ( sender , instance = None , * args , ** kwargs ) : if issubclass ( sender , SyncableModel ) : instance . _update_deleted_models ( )
8101	def copy ( self , graph ) : g = styleguide ( graph ) g . order = self . order dict . __init__ ( g , [ ( k , v ) for k , v in self . iteritems ( ) ] ) return g
6925	def autocommit ( self ) : if len ( self . cursors . keys ( ) ) == 0 : self . connection . autocommit = True else : raise AttributeError ( 'database cursors are already active, ' 'cannot switch to autocommit now' )
9169	def parse_archive_uri ( uri ) : parsed = urlparse ( uri ) path = parsed . path . rstrip ( '/' ) . split ( '/' ) ident_hash = path [ - 1 ] ident_hash = unquote ( ident_hash ) return ident_hash
13269	def deparagraph ( element , doc ) : if isinstance ( element , Para ) : if element . next is not None : return element elif element . prev is not None : return element return Plain ( * element . content )
7966	def end ( self , tag ) : self . _level -= 1 if self . _level < 0 : self . _handler . stream_parse_error ( u"Unexpected end tag for: {0!r}" . format ( tag ) ) return if self . _level == 0 : if tag != self . _root . tag : self . _handler . stream_parse_error ( u"Unexpected end tag for:" " {0!r} (stream end tag expected)" . format ( tag ) ) return self . _handler . stream_end ( ) return element = self . _builder . end ( tag ) if self . _level == 1 : self . _handler . stream_element ( element )
5215	def active_futures ( ticker : str , dt ) -> str : t_info = ticker . split ( ) prefix , asset = ' ' . join ( t_info [ : - 1 ] ) , t_info [ - 1 ] info = const . market_info ( f'{prefix[:-1]}1 {asset}' ) f1 , f2 = f'{prefix[:-1]}1 {asset}' , f'{prefix[:-1]}2 {asset}' fut_2 = fut_ticker ( gen_ticker = f2 , dt = dt , freq = info [ 'freq' ] ) fut_1 = fut_ticker ( gen_ticker = f1 , dt = dt , freq = info [ 'freq' ] ) fut_tk = bdp ( tickers = [ fut_1 , fut_2 ] , flds = 'Last_Tradeable_Dt' , cache = True ) if pd . Timestamp ( dt ) . month < pd . Timestamp ( fut_tk . last_tradeable_dt [ 0 ] ) . month : return fut_1 d1 = bdib ( ticker = f1 , dt = dt ) d2 = bdib ( ticker = f2 , dt = dt ) return fut_1 if d1 [ f1 ] . volume . sum ( ) > d2 [ f2 ] . volume . sum ( ) else fut_2
9161	def delete_acl_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] posted = request . json permissions = [ ( x [ 'uid' ] , x [ 'permission' ] , ) for x in posted ] with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : remove_acl ( cursor , uuid_ , permissions ) resp = request . response resp . status_int = 200 return resp
9148	def actions ( connection ) : session = _make_session ( connection = connection ) for action in Action . ls ( session = session ) : click . echo ( f'{action.created} {action.action} {action.resource}' )
10926	def do_run_1 ( self ) : while not self . check_terminate ( ) : self . _has_run = True self . _run1 ( ) self . _num_iter += 1 self . _inner_run_counter += 1
2584	def get_tasks ( self , count ) : tasks = [ ] for i in range ( 0 , count ) : try : x = self . pending_task_queue . get ( block = False ) except queue . Empty : break else : tasks . append ( x ) return tasks
11716	def edit ( self , config , etag ) : data = self . _json_encode ( config ) headers = self . _default_headers ( ) if etag is not None : headers [ "If-Match" ] = etag return self . _request ( self . name , ok_status = None , data = data , headers = headers , method = "PUT" )
6910	def generate_transit_lightcurve ( times , mags = None , errs = None , paramdists = { 'transitperiod' : sps . uniform ( loc = 0.1 , scale = 49.9 ) , 'transitdepth' : sps . uniform ( loc = 1.0e-4 , scale = 2.0e-2 ) , 'transitduration' : sps . uniform ( loc = 0.01 , scale = 0.29 ) } , magsarefluxes = False , ) : if mags is None : mags = np . full_like ( times , 0.0 ) if errs is None : errs = np . full_like ( times , 0.0 ) epoch = npr . random ( ) * ( times . max ( ) - times . min ( ) ) + times . min ( ) period = paramdists [ 'transitperiod' ] . rvs ( size = 1 ) depth = paramdists [ 'transitdepth' ] . rvs ( size = 1 ) duration = paramdists [ 'transitduration' ] . rvs ( size = 1 ) ingduration = npr . random ( ) * ( 0.5 * duration - 0.05 * duration ) + 0.05 * duration if magsarefluxes and depth < 0.0 : depth = - depth elif not magsarefluxes and depth > 0.0 : depth = - depth modelmags , phase , ptimes , pmags , perrs = ( transits . trapezoid_transit_func ( [ period , epoch , depth , duration , ingduration ] , times , mags , errs ) ) timeind = np . argsort ( ptimes ) mtimes = ptimes [ timeind ] mmags = modelmags [ timeind ] merrs = perrs [ timeind ] modeldict = { 'vartype' : 'planet' , 'params' : { x : np . asscalar ( y ) for x , y in zip ( [ 'transitperiod' , 'transitepoch' , 'transitdepth' , 'transitduration' , 'ingressduration' ] , [ period , epoch , depth , duration , ingduration ] ) } , 'times' : mtimes , 'mags' : mmags , 'errs' : merrs , 'varperiod' : period , 'varamplitude' : depth } return modeldict
10955	def get_update_io_tiles ( self , params , values ) : otile = self . get_update_tile ( params , values ) if otile is None : return [ None ] * 3 ptile = self . get_padding_size ( otile ) or util . Tile ( 0 , dim = otile . dim ) otile = util . Tile . intersection ( otile , self . oshape ) if ( otile . shape <= 0 ) . any ( ) : raise UpdateError ( "update triggered invalid tile size" ) if ( ptile . shape < 0 ) . any ( ) or ( ptile . shape > self . oshape . shape ) . any ( ) : raise UpdateError ( "update triggered invalid padding tile size" ) outer = otile . pad ( ( ptile . shape + 1 ) // 2 ) inner , outer = outer . reflect_overhang ( self . oshape ) iotile = inner . translate ( - outer . l ) outer = util . Tile . intersection ( outer , self . oshape ) inner = util . Tile . intersection ( inner , self . oshape ) return outer , inner , iotile
7059	def s3_delete_file ( bucket , filename , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 's3' ) try : resp = client . delete_object ( Bucket = bucket , Key = filename ) if not resp : LOGERROR ( 'could not delete file %s from bucket %s' % ( filename , bucket ) ) else : return resp [ 'DeleteMarker' ] except Exception as e : LOGEXCEPTION ( 'could not delete file %s from bucket %s' % ( filename , bucket ) ) if raiseonfail : raise return None
8580	def create_server ( self , datacenter_id , server ) : data = json . dumps ( self . _create_server_dict ( server ) ) response = self . _perform_request ( url = '/datacenters/%s/servers' % ( datacenter_id ) , method = 'POST' , data = data ) return response
13047	def f_i18n_citation_type ( string , lang = "eng" ) : s = " " . join ( string . strip ( "%" ) . split ( "|" ) ) return s . capitalize ( )
1521	def get_hostname ( ip_addr , cl_args ) : if is_self ( ip_addr ) : return get_self_hostname ( ) cmd = "hostname" ssh_cmd = ssh_remote_execute ( cmd , ip_addr , cl_args ) pid = subprocess . Popen ( ssh_cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) if return_code != 0 : Log . error ( "Failed to get hostname for remote host %s with output:\n%s" % ( ip_addr , output ) ) sys . exit ( - 1 ) return output [ 0 ] . strip ( "\n" )
5176	def fact ( self , name ) : facts = self . facts ( name = name ) return next ( fact for fact in facts )
5355	def convert_from_eclipse ( self , eclipse_projects ) : projects = { } projects [ 'unknown' ] = { "gerrit" : [ "git.eclipse.org" ] , "bugzilla" : [ "https://bugs.eclipse.org/bugs/" ] } projects = compose_title ( projects , eclipse_projects ) projects = compose_projects_json ( projects , eclipse_projects ) return projects
10481	def _generateChildren ( self ) : try : children = self . AXChildren except _a11y . Error : return if children : for child in children : yield child
8578	def list_servers ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
541	def __deleteModelCheckpoint ( self , modelID ) : checkpointID = self . _jobsDAO . modelsGetFields ( modelID , [ 'modelCheckpointId' ] ) [ 0 ] if checkpointID is None : return try : shutil . rmtree ( os . path . join ( self . _experimentDir , str ( self . _modelCheckpointGUID ) ) ) except : self . _logger . warn ( "Failed to delete model checkpoint %s. " "Assuming that another worker has already deleted it" , checkpointID ) return self . _jobsDAO . modelSetFields ( modelID , { 'modelCheckpointId' : None } , ignoreUnchanged = True ) return
13392	def paginate_update ( update ) : from happenings . models import Update time = update . pub_time event = update . event try : next = Update . objects . filter ( event = event , pub_time__gt = time ) . order_by ( 'pub_time' ) . only ( 'title' ) [ 0 ] except : next = None try : previous = Update . objects . filter ( event = event , pub_time__lt = time ) . order_by ( '-pub_time' ) . only ( 'title' ) [ 0 ] except : previous = None return { 'next' : next , 'previous' : previous , 'event' : event }
1389	def get_machines ( self ) : if self . physical_plan : stmgrs = list ( self . physical_plan . stmgrs ) return map ( lambda s : s . host_name , stmgrs ) return [ ]
466	def generate_skip_gram_batch ( data , batch_size , num_skips , skip_window , data_index = 0 ) : if batch_size % num_skips != 0 : raise Exception ( "batch_size should be able to be divided by num_skips." ) if num_skips > 2 * skip_window : raise Exception ( "num_skips <= 2 * skip_window" ) batch = np . ndarray ( shape = ( batch_size ) , dtype = np . int32 ) labels = np . ndarray ( shape = ( batch_size , 1 ) , dtype = np . int32 ) span = 2 * skip_window + 1 buffer = collections . deque ( maxlen = span ) for _ in range ( span ) : buffer . append ( data [ data_index ] ) data_index = ( data_index + 1 ) % len ( data ) for i in range ( batch_size // num_skips ) : target = skip_window targets_to_avoid = [ skip_window ] for j in range ( num_skips ) : while target in targets_to_avoid : target = random . randint ( 0 , span - 1 ) targets_to_avoid . append ( target ) batch [ i * num_skips + j ] = buffer [ skip_window ] labels [ i * num_skips + j , 0 ] = buffer [ target ] buffer . append ( data [ data_index ] ) data_index = ( data_index + 1 ) % len ( data ) return batch , labels , data_index
5907	def edit_txt ( filename , substitutions , newname = None ) : if newname is None : newname = filename _substitutions = [ { 'lRE' : re . compile ( str ( lRE ) ) , 'sRE' : re . compile ( str ( sRE ) ) , 'repl' : repl } for lRE , sRE , repl in substitutions if repl is not None ] with tempfile . TemporaryFile ( ) as target : with open ( filename , 'rb' ) as src : logger . info ( "editing txt = {0!r} ({1:d} substitutions)" . format ( filename , len ( substitutions ) ) ) for line in src : line = line . decode ( "utf-8" ) keep_line = True for subst in _substitutions : m = subst [ 'lRE' ] . match ( line ) if m : logger . debug ( 'match: ' + line . rstrip ( ) ) if subst [ 'repl' ] is False : keep_line = False else : line = subst [ 'sRE' ] . sub ( str ( subst [ 'repl' ] ) , line ) logger . debug ( 'replaced: ' + line . rstrip ( ) ) if keep_line : target . write ( line . encode ( 'utf-8' ) ) else : logger . debug ( "Deleting line %r" , line ) target . seek ( 0 ) with open ( newname , 'wb' ) as final : shutil . copyfileobj ( target , final ) logger . info ( "edited txt = {newname!r}" . format ( ** vars ( ) ) )
4880	def create_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . update_or_create ( name = ENTERPRISE_ROLE_BASED_ACCESS_CONTROL_SWITCH , defaults = { 'active' : False } )
10380	def calculate_concordance_probability_by_annotation ( graph , annotation , key , cutoff = None , permutations = None , percentage = None , use_ambiguous = False ) : result = [ ( value , calculate_concordance_probability ( subgraph , key , cutoff = cutoff , permutations = permutations , percentage = percentage , use_ambiguous = use_ambiguous , ) ) for value , subgraph in get_subgraphs_by_annotation ( graph , annotation ) . items ( ) ] return dict ( result )
9597	def save_screenshot ( self , filename , quietly = False ) : imgData = self . take_screenshot ( ) try : with open ( filename , "wb" ) as f : f . write ( b64decode ( imgData . encode ( 'ascii' ) ) ) except IOError as err : if not quietly : raise err
4208	def lpc ( x , N = None ) : m = len ( x ) if N is None : N = m - 1 elif N > m - 1 : x . resize ( N + 1 ) X = fft ( x , 2 ** nextpow2 ( 2. * len ( x ) - 1 ) ) R = real ( ifft ( abs ( X ) ** 2 ) ) R = R / ( m - 1. ) a , e , ref = LEVINSON ( R , N ) return a , e
7336	def _parse_iedb_response ( response ) : if len ( response ) == 0 : raise ValueError ( "Empty response from IEDB!" ) df = pd . read_csv ( io . BytesIO ( response ) , delim_whitespace = True , header = 0 ) assert type ( df ) == pd . DataFrame df = pd . DataFrame ( df ) if len ( df ) == 0 : raise ValueError ( "No binding predictions in response from IEDB: %s" % ( response , ) ) required_columns = [ "allele" , "peptide" , "ic50" , "start" , "end" , ] for column in required_columns : if column not in df . columns : raise ValueError ( "Response from IEDB is missing '%s' column: %s. Full " "response:\n%s" % ( column , df . ix [ 0 ] , response ) ) df = df . rename ( columns = { "percentile_rank" : "rank" , "percentile rank" : "rank" } ) return df
8545	def get_datacenter_by_name ( self , name , depth = 1 ) : all_data_centers = self . list_datacenters ( depth = depth ) [ 'items' ] data_center = find_item_by_name ( all_data_centers , lambda i : i [ 'properties' ] [ 'name' ] , name ) if not data_center : raise NameError ( "No data center found with name " "containing '{name}'." . format ( name = name ) ) if len ( data_center ) > 1 : raise NameError ( "Found {n} data centers with the name '{name}': {names}" . format ( n = len ( data_center ) , name = name , names = ", " . join ( d [ 'properties' ] [ 'name' ] for d in data_center ) ) ) return data_center [ 0 ]
9379	def detect_timestamp_format ( timestamp ) : time_formats = { 'epoch' : re . compile ( r'^[0-9]{10}$' ) , 'epoch_ms' : re . compile ( r'^[0-9]{13}$' ) , 'epoch_fraction' : re . compile ( r'^[0-9]{10}\.[0-9]{3,9}$' ) , '%Y-%m-%d %H:%M:%S' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y-%m-%dT%H:%M:%S' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y-%m-%d_%H:%M:%S' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y-%m-%d %H:%M:%S.%f' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y-%m-%dT%H:%M:%S.%f' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y-%m-%d_%H:%M:%S.%f' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y%m%d %H:%M:%S' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y%m%dT%H:%M:%S' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y%m%d_%H:%M:%S' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y%m%d %H:%M:%S.%f' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y%m%dT%H:%M:%S.%f' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y%m%d_%H:%M:%S.%f' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%H:%M:%S' : re . compile ( r'^[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%H:%M:%S.%f' : re . compile ( r'^[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y-%m-%dT%H:%M:%S.%f%z' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+[+-][0-9]{4}$' ) } for time_format in time_formats : if re . match ( time_formats [ time_format ] , timestamp ) : return time_format return 'unknown'
3761	def draw_2d ( self , Hs = False ) : r try : from rdkit . Chem import Draw from rdkit . Chem . Draw import IPythonConsole if Hs : mols = [ i . rdkitmol_Hs for i in self . Chemicals ] else : mols = [ i . rdkitmol for i in self . Chemicals ] return Draw . MolsToImage ( mols ) except : return 'Rdkit is required for this feature.'
9125	def _store_helper ( model : Action , session : Optional [ Session ] = None ) -> None : if session is None : session = _make_session ( ) session . add ( model ) session . commit ( ) session . close ( )
8182	def remove_edge ( self , id1 , id2 ) : for e in list ( self . edges ) : if id1 in ( e . node1 . id , e . node2 . id ) and id2 in ( e . node1 . id , e . node2 . id ) : e . node1 . links . remove ( e . node2 ) e . node2 . links . remove ( e . node1 ) self . edges . remove ( e )
88	def is_float_array ( val ) : return is_np_array ( val ) and issubclass ( val . dtype . type , np . floating )
11983	async def upload_file ( self , bucket , file , uploadpath = None , key = None , ContentType = None , ** kw ) : is_filename = False if hasattr ( file , 'read' ) : if hasattr ( file , 'seek' ) : file . seek ( 0 ) file = file . read ( ) size = len ( file ) elif key : size = len ( file ) else : is_filename = True size = os . stat ( file ) . st_size key = os . path . basename ( file ) assert key , 'key not available' if not ContentType : ContentType , _ = mimetypes . guess_type ( key ) if uploadpath : if not uploadpath . endswith ( '/' ) : uploadpath = '%s/' % uploadpath key = '%s%s' % ( uploadpath , key ) params = dict ( Bucket = bucket , Key = key ) if not ContentType : ContentType = 'application/octet-stream' params [ 'ContentType' ] = ContentType if size > MULTI_PART_SIZE and is_filename : resp = await _multipart ( self , file , params ) elif is_filename : with open ( file , 'rb' ) as fp : params [ 'Body' ] = fp . read ( ) resp = await self . put_object ( ** params ) else : params [ 'Body' ] = file resp = await self . put_object ( ** params ) if 'Key' not in resp : resp [ 'Key' ] = key if 'Bucket' not in resp : resp [ 'Bucket' ] = bucket return resp
7124	def validate_date ( date_text ) : try : if int ( date_text ) < 0 : return True except ValueError : pass try : datetime . strptime ( date_text , '%Y-%m-%d' ) return True except ValueError : pass raise ValueError ( 'Dates must be negative integers or YYYY-MM-DD in the past.' )
11746	def init_app ( self , app ) : if len ( self . _attached_bundles ) == 0 : raise NoBundlesAttached ( "At least one bundle must be attached before initializing Journey" ) for bundle in self . _attached_bundles : processed_bundle = { 'path' : bundle . path , 'description' : bundle . description , 'blueprints' : [ ] } for ( bp , description ) in bundle . blueprints : blueprint = self . _register_blueprint ( app , bp , bundle . path , self . get_bp_path ( bp ) , description ) processed_bundle [ 'blueprints' ] . append ( blueprint ) self . _registered_bundles . append ( processed_bundle )
1845	def JP ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . PF , target . read ( ) , cpu . PC )
8383	def update ( self ) : if self . delay > 0 : self . delay -= 1 return if self . fi == 0 : if len ( self . q ) == 1 : self . fn = float ( "inf" ) else : self . fn = len ( self . q [ self . i ] ) / self . speed self . fn = max ( self . fn , self . mf ) self . fi += 1 if self . fi > self . fn : self . fi = 0 self . i = ( self . i + 1 ) % len ( self . q )
12476	def ux_file_len ( filepath ) : p = subprocess . Popen ( [ 'wc' , '-l' , filepath ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) result , err = p . communicate ( ) if p . returncode != 0 : raise IOError ( err ) l = result . strip ( ) l = int ( l . split ( ) [ 0 ] ) return l
615	def _generateInferenceArgs ( options , tokenReplacements ) : inferenceType = options [ 'inferenceType' ] optionInferenceArgs = options . get ( 'inferenceArgs' , None ) resultInferenceArgs = { } predictedField = _getPredictedField ( options ) [ 0 ] if inferenceType in ( InferenceType . TemporalNextStep , InferenceType . TemporalAnomaly ) : assert predictedField , "Inference Type '%s' needs a predictedField " "specified in the inferenceArgs dictionary" % inferenceType if optionInferenceArgs : if options [ 'dynamicPredictionSteps' ] : altOptionInferenceArgs = copy . deepcopy ( optionInferenceArgs ) altOptionInferenceArgs [ 'predictionSteps' ] = '$REPLACE_ME' resultInferenceArgs = pprint . pformat ( altOptionInferenceArgs ) resultInferenceArgs = resultInferenceArgs . replace ( "'$REPLACE_ME'" , '[predictionSteps]' ) else : resultInferenceArgs = pprint . pformat ( optionInferenceArgs ) tokenReplacements [ '\$INFERENCE_ARGS' ] = resultInferenceArgs tokenReplacements [ '\$PREDICTION_FIELD' ] = predictedField
4339	def pitch ( self , n_semitones , quick = False ) : if not is_number ( n_semitones ) : raise ValueError ( "n_semitones must be a positive number" ) if n_semitones < - 12 or n_semitones > 12 : logger . warning ( "Using an extreme pitch shift. " "Quality of results will be poor" ) if not isinstance ( quick , bool ) : raise ValueError ( "quick must be a boolean." ) effect_args = [ 'pitch' ] if quick : effect_args . append ( '-q' ) effect_args . append ( '{:f}' . format ( n_semitones * 100. ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'pitch' ) return self
4669	def decrypt ( encrypted_privkey , passphrase ) : d = unhexlify ( base58decode ( encrypted_privkey ) ) d = d [ 2 : ] flagbyte = d [ 0 : 1 ] d = d [ 1 : ] assert flagbyte == b"\xc0" , "Flagbyte has to be 0xc0" salt = d [ 0 : 4 ] d = d [ 4 : - 4 ] if SCRYPT_MODULE == "scrypt" : key = scrypt . hash ( passphrase , salt , 16384 , 8 , 8 ) elif SCRYPT_MODULE == "pylibscrypt" : key = scrypt . scrypt ( bytes ( passphrase , "utf-8" ) , salt , 16384 , 8 , 8 ) else : raise ValueError ( "No scrypt module loaded" ) derivedhalf1 = key [ 0 : 32 ] derivedhalf2 = key [ 32 : 64 ] encryptedhalf1 = d [ 0 : 16 ] encryptedhalf2 = d [ 16 : 32 ] aes = AES . new ( derivedhalf2 , AES . MODE_ECB ) decryptedhalf2 = aes . decrypt ( encryptedhalf2 ) decryptedhalf1 = aes . decrypt ( encryptedhalf1 ) privraw = decryptedhalf1 + decryptedhalf2 privraw = "%064x" % ( int ( hexlify ( privraw ) , 16 ) ^ int ( hexlify ( derivedhalf1 ) , 16 ) ) wif = Base58 ( privraw ) privkey = PrivateKey ( format ( wif , "wif" ) ) addr = format ( privkey . bitcoin . address , "BTC" ) a = _bytes ( addr ) saltverify = hashlib . sha256 ( hashlib . sha256 ( a ) . digest ( ) ) . digest ( ) [ 0 : 4 ] if saltverify != salt : raise SaltException ( "checksum verification failed! Password may be incorrect." ) return wif
2862	def _i2c_write_bytes ( self , data ) : for byte in data : self . _command . append ( str ( bytearray ( ( 0x11 , 0x00 , 0x00 , byte ) ) ) ) self . _ft232h . output_pins ( { 0 : GPIO . LOW , 1 : GPIO . HIGH } , write = False ) self . _command . append ( self . _ft232h . mpsse_gpio ( ) * _REPEAT_DELAY ) self . _command . append ( '\x22\x00' ) self . _expected += len ( data )
11674	def copy ( self , stack = False , copy_meta = False , memo = None ) : if self . stacked : fs = deepcopy ( self . stacked_features , memo ) n_pts = self . n_pts . copy ( ) elif stack : fs = np . vstack ( self . features ) n_pts = self . n_pts . copy ( ) else : fs = deepcopy ( self . features , memo ) n_pts = None meta = deepcopy ( self . meta , memo ) if copy_meta else self . meta return Features ( fs , n_pts , copy = False , ** meta )
1489	def save_file ( self , obj ) : try : import StringIO as pystringIO except ImportError : import io as pystringIO if not hasattr ( obj , 'name' ) or not hasattr ( obj , 'mode' ) : raise pickle . PicklingError ( "Cannot pickle files that do not map to an actual file" ) if obj is sys . stdout : return self . save_reduce ( getattr , ( sys , 'stdout' ) , obj = obj ) if obj is sys . stderr : return self . save_reduce ( getattr , ( sys , 'stderr' ) , obj = obj ) if obj is sys . stdin : raise pickle . PicklingError ( "Cannot pickle standard input" ) if hasattr ( obj , 'isatty' ) and obj . isatty ( ) : raise pickle . PicklingError ( "Cannot pickle files that map to tty objects" ) if 'r' not in obj . mode : raise pickle . PicklingError ( "Cannot pickle files that are not opened for reading" ) name = obj . name try : fsize = os . stat ( name ) . st_size except OSError : raise pickle . PicklingError ( "Cannot pickle file %s as it cannot be stat" % name ) if obj . closed : retval = pystringIO . StringIO ( "" ) retval . close ( ) elif not fsize : retval = pystringIO . StringIO ( "" ) try : tmpfile = file ( name ) tst = tmpfile . read ( 1 ) except IOError : raise pickle . PicklingError ( "Cannot pickle file %s as it cannot be read" % name ) tmpfile . close ( ) if tst != '' : raise pickle . PicklingError ( "Cannot pickle file %s as it does not appear to map to a physical, real file" % name ) else : try : tmpfile = file ( name ) contents = tmpfile . read ( ) tmpfile . close ( ) except IOError : raise pickle . PicklingError ( "Cannot pickle file %s as it cannot be read" % name ) retval = pystringIO . StringIO ( contents ) curloc = obj . tell ( ) retval . seek ( curloc ) retval . name = name self . save ( retval ) self . memoize ( obj )
1093	def split ( pattern , string , maxsplit = 0 , flags = 0 ) : return _compile ( pattern , flags ) . split ( string , maxsplit )
10747	def fetch ( self , url , path , filename ) : logger . debug ( 'initializing download in ' , url ) remote_file_size = self . get_remote_file_size ( url ) if exists ( join ( path , filename ) ) : size = getsize ( join ( path , filename ) ) if size == remote_file_size : logger . error ( '%s already exists on your system' % filename ) print ( '%s already exists on your system' % filename ) return [ join ( path , filename ) , size ] logger . debug ( 'Downloading: %s' % filename ) print ( 'Downloading: %s' % filename ) fetch ( url , path ) print ( 'stored at %s' % path ) logger . debug ( 'stored at %s' % path ) return [ join ( path , filename ) , remote_file_size ]
13676	def add_prepare_handler ( self , prepare_handlers ) : if not isinstance ( prepare_handlers , static_bundle . BUNDLE_ITERABLE_TYPES ) : prepare_handlers = [ prepare_handlers ] if self . prepare_handlers_chain is None : self . prepare_handlers_chain = [ ] for handler in prepare_handlers : self . prepare_handlers_chain . append ( handler )
6370	def specificity ( self ) : r if self . _tn + self . _fp == 0 : return float ( 'NaN' ) return self . _tn / ( self . _tn + self . _fp )
1949	def write_back_register ( self , reg , val ) : if self . write_backs_disabled : return if issymbolic ( val ) : logger . warning ( "Skipping Symbolic write-back" ) return if reg in self . flag_registers : self . _emu . reg_write ( self . _to_unicorn_id ( 'EFLAGS' ) , self . _cpu . read_register ( 'EFLAGS' ) ) return self . _emu . reg_write ( self . _to_unicorn_id ( reg ) , val )
10224	def get_chaotic_pairs ( graph : BELGraph ) -> SetOfNodePairs : cg = get_causal_subgraph ( graph ) results = set ( ) for u , v , d in cg . edges ( data = True ) : if d [ RELATION ] not in CAUSAL_INCREASE_RELATIONS : continue if cg . has_edge ( v , u ) and any ( dd [ RELATION ] in CAUSAL_INCREASE_RELATIONS for dd in cg [ v ] [ u ] . values ( ) ) : results . add ( tuple ( sorted ( [ u , v ] , key = str ) ) ) return results
5630	def _postreceive ( self ) : digest = self . _get_digest ( ) if digest is not None : sig_parts = _get_header ( 'X-Hub-Signature' ) . split ( '=' , 1 ) if not isinstance ( digest , six . text_type ) : digest = six . text_type ( digest ) if ( len ( sig_parts ) < 2 or sig_parts [ 0 ] != 'sha1' or not hmac . compare_digest ( sig_parts [ 1 ] , digest ) ) : abort ( 400 , 'Invalid signature' ) event_type = _get_header ( 'X-Github-Event' ) data = request . get_json ( ) if data is None : abort ( 400 , 'Request body must contain json' ) self . _logger . info ( '%s (%s)' , _format_event ( event_type , data ) , _get_header ( 'X-Github-Delivery' ) ) for hook in self . _hooks . get ( event_type , [ ] ) : hook ( data ) return '' , 204
4444	def get_suggestions ( self , prefix , fuzzy = False , num = 10 , with_scores = False , with_payloads = False ) : args = [ AutoCompleter . SUGGET_COMMAND , self . key , prefix , 'MAX' , num ] if fuzzy : args . append ( AutoCompleter . FUZZY ) if with_scores : args . append ( AutoCompleter . WITHSCORES ) if with_payloads : args . append ( AutoCompleter . WITHPAYLOADS ) ret = self . redis . execute_command ( * args ) results = [ ] if not ret : return results parser = SuggestionParser ( with_scores , with_payloads , ret ) return [ s for s in parser ]
12024	def adopt ( self , old_parent , new_parent ) : try : old_id = old_parent [ 'attributes' ] [ 'ID' ] except TypeError : try : old_id = self . lines [ old_parent ] [ 'attributes' ] [ 'ID' ] except TypeError : old_id = old_parent old_feature = self . features [ old_id ] old_indexes = [ ld [ 'line_index' ] for ld in old_feature ] try : new_id = new_parent [ 'attributes' ] [ 'ID' ] except TypeError : try : new_id = self . lines [ new_parent ] [ 'attributes' ] [ 'ID' ] except TypeError : new_id = new_parent new_feature = self . features [ new_id ] new_indexes = [ ld [ 'line_index' ] for ld in new_feature ] children = old_feature [ 0 ] [ 'children' ] new_parent_children_set = set ( [ ld [ 'line_index' ] for ld in new_feature [ 0 ] [ 'children' ] ] ) for child in children : if child [ 'line_index' ] not in new_parent_children_set : new_parent_children_set . add ( child [ 'line_index' ] ) for new_ld in new_feature : new_ld [ 'children' ] . append ( child ) child [ 'parents' ] . append ( new_feature ) child [ 'attributes' ] [ 'Parent' ] . append ( new_id ) child [ 'parents' ] = [ f for f in child [ 'parents' ] if f [ 0 ] [ 'attributes' ] [ 'ID' ] != old_id ] child [ 'attributes' ] [ 'Parent' ] = [ d for d in child [ 'attributes' ] [ 'Parent' ] if d != old_id ] for old_ld in old_feature : old_ld [ 'children' ] = [ ] return children
10343	def overlay_type_data ( graph : BELGraph , data : Mapping [ str , float ] , func : str , namespace : str , label : Optional [ str ] = None , overwrite : bool = False , impute : Optional [ float ] = None , ) -> None : new_data = { node : data . get ( node [ NAME ] , impute ) for node in filter_nodes ( graph , function_namespace_inclusion_builder ( func , namespace ) ) } overlay_data ( graph , new_data , label = label , overwrite = overwrite )
8845	def _at_block_start ( tc , line ) : if tc . atBlockStart ( ) : return True column = tc . columnNumber ( ) indentation = len ( line ) - len ( line . lstrip ( ) ) return column <= indentation
6352	def _remove_dupes ( self , phonetic ) : alt_string = phonetic alt_array = alt_string . split ( '|' ) result = '|' for i in range ( len ( alt_array ) ) : alt = alt_array [ i ] if alt and '|' + alt + '|' not in result : result += alt + '|' return result [ 1 : - 1 ]
4040	def _cleanup ( self , to_clean , allow = ( ) ) : if to_clean . keys ( ) == [ "links" , "library" , "version" , "meta" , "key" , "data" ] : to_clean = to_clean [ "data" ] return dict ( [ [ k , v ] for k , v in list ( to_clean . items ( ) ) if ( k in allow or k not in self . temp_keys ) ] )
3731	def checkCAS ( CASRN ) : try : check = CASRN [ - 1 ] CASRN = CASRN [ : : - 1 ] [ 1 : ] productsum = 0 i = 1 for num in CASRN : if num == '-' : pass else : productsum += i * int ( num ) i += 1 return ( productsum % 10 == int ( check ) ) except : return False
10566	def _check_field_value ( field_value , pattern ) : if isinstance ( field_value , list ) : return any ( re . search ( pattern , str ( value ) , re . I ) for value in field_value ) else : return re . search ( pattern , str ( field_value ) , re . I )
9506	def contains ( self , i ) : return self . start <= i . start and i . end <= self . end
1699	def log ( self ) : from heronpy . streamlet . impl . logbolt import LogStreamlet log_streamlet = LogStreamlet ( self ) self . _add_child ( log_streamlet ) return
13568	def selected_course ( func ) : @ wraps ( func ) def inner ( * args , ** kwargs ) : course = Course . get_selected ( ) return func ( course , * args , ** kwargs ) return inner
3289	def get_resource_inst ( self , path , environ ) : self . _count_get_resource_inst += 1 localHgPath = path . strip ( "/" ) rev = None cmd , rest = util . pop_path ( path ) if cmd == "" : return VirtualCollection ( path , environ , "root" , [ "edit" , "released" , "archive" ] ) elif cmd == "edit" : localHgPath = rest . strip ( "/" ) rev = None elif cmd == "released" : localHgPath = rest . strip ( "/" ) rev = "tip" elif cmd == "archive" : if rest == "/" : loglist = self . _get_log ( limit = 10 ) members = [ compat . to_native ( l [ "local_id" ] ) for l in loglist ] return VirtualCollection ( path , environ , "Revisions" , members ) revid , rest = util . pop_path ( rest ) try : int ( revid ) except Exception : return None rev = revid localHgPath = rest . strip ( "/" ) else : return None cache = self . _get_repo_info ( environ , rev ) if localHgPath in cache [ "filedict" ] : return HgResource ( path , False , environ , rev , localHgPath ) if localHgPath in cache [ "dirinfos" ] or localHgPath == "" : return HgResource ( path , True , environ , rev , localHgPath ) return None
7015	def concatenate_textlcs_for_objectid ( lcbasedir , objectid , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , recursive = True ) : LOGINFO ( 'looking for light curves for %s, aperture %s in directory: %s' % ( objectid , aperture , lcbasedir ) ) if recursive is False : matching = glob . glob ( os . path . join ( lcbasedir , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) ) else : if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( lcbasedir , '**' , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) , recursive = True ) LOGINFO ( 'found %s files: %s' % ( len ( matching ) , repr ( matching ) ) ) else : walker = os . walk ( lcbasedir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) LOGINFO ( 'found %s in dir: %s' % ( repr ( foundfiles ) , os . path . join ( root , sdir ) ) ) if matching and len ( matching ) > 0 : clcdict = concatenate_textlcs ( matching , sortby = sortby , normalize = normalize ) return clcdict else : LOGERROR ( 'did not find any light curves for %s and aperture %s' % ( objectid , aperture ) ) return None
7422	def ref_build_and_muscle_chunk ( data , sample ) : regions = bedtools_merge ( data , sample ) . strip ( ) . split ( "\n" ) nregions = len ( regions ) chunksize = ( nregions / 10 ) + ( nregions % 10 ) LOGGER . debug ( "nregions {} chunksize {}" . format ( nregions , chunksize ) ) idx = 0 tmpfile = os . path . join ( data . tmpdir , sample . name + "_chunk_{}.ali" ) for i in range ( 11 ) : if os . path . exists ( tmpfile . format ( i ) ) : os . remove ( tmpfile . format ( i ) ) fopen = open if data . paramsdict [ "assembly_method" ] == "denovo+reference" : tmpfile = os . path . join ( data . dirs . clusts , sample . name + ".clust.gz" ) fopen = gzip . open samfile = pysam . AlignmentFile ( sample . files . mapped_reads , 'rb' ) clusts = [ ] nclusts = 0 for region in regions : chrom , pos1 , pos2 = region . split ( ) try : if "pair" in data . paramsdict [ "datatype" ] : clust = fetch_cluster_pairs ( data , samfile , chrom , int ( pos1 ) , int ( pos2 ) ) else : clust = fetch_cluster_se ( data , samfile , chrom , int ( pos1 ) , int ( pos2 ) ) except IndexError as inst : LOGGER . error ( "Bad region chrom:start-end {}:{}-{}" . format ( chrom , pos1 , pos2 ) ) continue if clust : clusts . append ( "\n" . join ( clust ) ) nclusts += 1 if nclusts == chunksize : tmphandle = tmpfile . format ( idx ) with fopen ( tmphandle , 'a' ) as tmp : tmp . write ( "\n//\n//\n" . join ( clusts ) + "\n//\n//\n" ) idx += 1 nclusts = 0 clusts = [ ] if clusts : with fopen ( tmpfile . format ( idx ) , 'a' ) as tmp : tmp . write ( "\n//\n//\n" . join ( clusts ) + "\n//\n//\n" ) clusts = [ ] if not data . paramsdict [ "assembly_method" ] == "denovo+reference" : chunkfiles = glob . glob ( os . path . join ( data . tmpdir , sample . name + "_chunk_*.ali" ) ) LOGGER . info ( "created chunks %s" , chunkfiles ) samfile . close ( )
7768	def _stream_authorized ( self , event ) : with self . lock : if event . stream != self . stream : return self . me = event . stream . me self . peer = event . stream . peer presence = self . settings [ u"initial_presence" ] if presence : self . send ( presence )
10182	def _aggregations_process ( aggregation_types = None , start_date = None , end_date = None , update_bookmark = False , eager = False ) : aggregation_types = ( aggregation_types or list ( current_stats . enabled_aggregations ) ) if eager : aggregate_events . apply ( ( aggregation_types , ) , dict ( start_date = start_date , end_date = end_date , update_bookmark = update_bookmark ) , throw = True ) click . secho ( 'Aggregations processed successfully.' , fg = 'green' ) else : aggregate_events . delay ( aggregation_types , start_date = start_date , end_date = end_date ) click . secho ( 'Aggregations processing task sent...' , fg = 'yellow' )
6828	def fetch ( self , path , use_sudo = False , user = None , remote = None ) : if path is None : raise ValueError ( "Path to the working copy is needed to fetch from a remote repository." ) if remote is not None : cmd = 'git fetch %s' % remote else : cmd = 'git fetch' with cd ( path ) : if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
2190	def _product_file_hash ( self , product = None ) : if self . hasher is None : return None else : products = self . _rectify_products ( product ) product_file_hash = [ util_hash . hash_file ( p , hasher = self . hasher , base = 'hex' ) for p in products ] return product_file_hash
8054	def handler ( self , conn , * args ) : self . shell . stdout . write ( self . shell . prompt ) line = self . shell . stdin . readline ( ) if not len ( line ) : line = 'EOF' return False else : line = line . rstrip ( '\r\n' ) line = self . shell . precmd ( line ) stop = self . shell . onecmd ( line ) stop = self . shell . postcmd ( stop , line ) self . shell . stdout . flush ( ) self . shell . postloop ( ) if stop : self . shell = None conn . close ( ) return not stop
6563	def load_cnf ( fp ) : fp = iter ( fp ) csp = ConstraintSatisfactionProblem ( dimod . BINARY ) num_clauses = num_variables = 0 problem_pattern = re . compile ( _PROBLEM_REGEX ) for line in fp : matches = problem_pattern . findall ( line ) if matches : if len ( matches ) > 1 : raise ValueError nv , nc = matches [ 0 ] num_variables , num_clauses = int ( nv ) , int ( nc ) break clause_pattern = re . compile ( _CLAUSE_REGEX ) for line in fp : if clause_pattern . match ( line ) is not None : clause = [ int ( v ) for v in line . split ( ' ' ) [ : - 1 ] ] variables = [ abs ( v ) for v in clause ] f = _cnf_or ( clause ) csp . add_constraint ( f , variables ) for v in range ( 1 , num_variables + 1 ) : csp . add_variable ( v ) for v in csp . variables : if v > num_variables : msg = ( "given .cnf file's header defines variables [1, {}] and {} clauses " "but constraints a reference to variable {}" ) . format ( num_variables , num_clauses , v ) raise ValueError ( msg ) if len ( csp ) != num_clauses : msg = ( "given .cnf file's header defines {} " "clauses but the file contains {}" ) . format ( num_clauses , len ( csp ) ) raise ValueError ( msg ) return csp
591	def compute ( self , inputs , outputs ) : if False and self . learningMode and self . _iterations > 0 and self . _iterations <= 10 : import hotshot if self . _iterations == 10 : print "\n Collecting and sorting internal node profiling stats generated by hotshot..." stats = hotshot . stats . load ( "hotshot.stats" ) stats . strip_dirs ( ) stats . sort_stats ( 'time' , 'calls' ) stats . print_stats ( ) if self . _profileObj is None : print "\n Preparing to capture profile using hotshot..." if os . path . exists ( 'hotshot.stats' ) : os . remove ( 'hotshot.stats' ) self . _profileObj = hotshot . Profile ( "hotshot.stats" , 1 , 1 ) self . _profileObj . runcall ( self . _compute , * [ inputs , outputs ] ) else : self . _compute ( inputs , outputs )
8488	def load ( self , prefix = None , depth = None ) : prefix = prefix or self . prefix prefix = '/' + prefix . strip ( '/' ) + '/' if depth is None : depth = self . inherit_depth if not self . configured : log . debug ( "etcd not available" ) return if self . watching : log . info ( "Starting watcher for %r" , prefix ) self . start_watching ( ) log . info ( "Loading from etcd %r" , prefix ) try : result = self . client . get ( prefix ) except self . module . EtcdKeyNotFound : result = None if not result : log . info ( "No configuration found" ) return { } update = { } for item in result . children : key = item . key value = item . value try : value = pytool . json . from_json ( value ) except : pass if not self . case_sensitive : key = key . lower ( ) if key . startswith ( prefix ) : key = key [ len ( prefix ) : ] update [ key ] = value inherited = Config ( ) . settings . get ( self . inherit_key , update . get ( self . inherit_key , None ) ) if depth > 0 and inherited : log . info ( " ... inheriting ..." ) inherited = self . load ( inherited , depth - 1 ) or { } inherited . update ( update ) update = inherited return update
10306	def calculate_tanimoto_set_distances ( dict_of_sets : Mapping [ X , Set ] ) -> Mapping [ X , Mapping [ X , float ] ] : result : Dict [ X , Dict [ X , float ] ] = defaultdict ( dict ) for x , y in itt . combinations ( dict_of_sets , 2 ) : result [ x ] [ y ] = result [ y ] [ x ] = tanimoto_set_similarity ( dict_of_sets [ x ] , dict_of_sets [ y ] ) for x in dict_of_sets : result [ x ] [ x ] = 1.0 return dict ( result )
3051	def FromResponse ( cls , response ) : kwargs = { 'device_code' : response [ 'device_code' ] , 'user_code' : response [ 'user_code' ] , } verification_url = response . get ( 'verification_url' , response . get ( 'verification_uri' ) ) if verification_url is None : raise OAuth2DeviceCodeError ( 'No verification_url provided in server response' ) kwargs [ 'verification_url' ] = verification_url kwargs . update ( { 'interval' : response . get ( 'interval' ) , 'user_code_expiry' : None , } ) if 'expires_in' in response : kwargs [ 'user_code_expiry' ] = ( _UTCNOW ( ) + datetime . timedelta ( seconds = int ( response [ 'expires_in' ] ) ) ) return cls ( ** kwargs )
10090	def rst2node ( doc_name , data ) : if not data : return parser = docutils . parsers . rst . Parser ( ) document = docutils . utils . new_document ( '<%s>' % doc_name ) document . settings = docutils . frontend . OptionParser ( ) . get_default_values ( ) document . settings . tab_width = 4 document . settings . pep_references = False document . settings . rfc_references = False document . settings . env = Env ( ) parser . parse ( data , document ) if len ( document . children ) == 1 : return document . children [ 0 ] else : par = docutils . nodes . paragraph ( ) for child in document . children : par += child return par
13288	def read_git_commit_timestamp_for_file ( filepath , repo_path = None , repo = None ) : logger = logging . getLogger ( __name__ ) if repo is None : repo = git . repo . base . Repo ( path = repo_path , search_parent_directories = True ) repo_path = repo . working_tree_dir head_commit = repo . head . commit logger . debug ( 'Using Git repo at %r' , repo_path ) filepath = os . path . relpath ( os . path . abspath ( filepath ) , start = repo_path ) logger . debug ( 'Repo-relative filepath is %r' , filepath ) for commit in head_commit . iter_items ( repo , head_commit , [ filepath ] , skip = 0 ) : return commit . committed_datetime raise IOError ( 'File {} not found' . format ( filepath ) )
12657	def merge_dict_of_lists ( adict , indices , pop_later = True , copy = True ) : def check_indices ( idxs , x ) : for i in chain ( * idxs ) : if i < 0 or i >= x : raise IndexError ( "Given indices are out of dict range." ) check_indices ( indices , len ( adict ) ) rdict = adict . copy ( ) if copy else adict dict_keys = list ( rdict . keys ( ) ) for i , j in zip ( * indices ) : rdict [ dict_keys [ i ] ] . extend ( rdict [ dict_keys [ j ] ] ) if pop_later : for i , j in zip ( * indices ) : rdict . pop ( dict_keys [ j ] , '' ) return rdict
13329	def remove ( path ) : r = cpenv . resolve ( path ) if isinstance ( r . resolved [ 0 ] , cpenv . VirtualEnvironment ) : EnvironmentCache . discard ( r . resolved [ 0 ] ) EnvironmentCache . save ( )
13634	def _parseAccept ( headers ) : def sort ( value ) : return float ( value [ 1 ] . get ( 'q' , 1 ) ) return OrderedDict ( sorted ( _splitHeaders ( headers ) , key = sort , reverse = True ) )
5879	def store_image ( cls , http_client , link_hash , src , config ) : image = cls . read_localfile ( link_hash , src , config ) if image : return image if src . startswith ( 'data:image' ) : image = cls . write_localfile_base64 ( link_hash , src , config ) return image data = http_client . fetch ( src ) if data : image = cls . write_localfile ( data , link_hash , src , config ) if image : return image return None
2140	def disassociate ( self , group , parent , ** kwargs ) : parent_id = self . lookup_with_inventory ( parent , kwargs . get ( 'inventory' , None ) ) [ 'id' ] group_id = self . lookup_with_inventory ( group , kwargs . get ( 'inventory' , None ) ) [ 'id' ] return self . _disassoc ( 'children' , parent_id , group_id )
1641	def _IsType ( clean_lines , nesting_state , expr ) : last_word = Match ( r'^.*(\b\S+)$' , expr ) if last_word : token = last_word . group ( 1 ) else : token = expr if _TYPES . match ( token ) : return True typename_pattern = ( r'\b(?:typename|class|struct)\s+' + re . escape ( token ) + r'\b' ) block_index = len ( nesting_state . stack ) - 1 while block_index >= 0 : if isinstance ( nesting_state . stack [ block_index ] , _NamespaceInfo ) : return False last_line = nesting_state . stack [ block_index ] . starting_linenum next_block_start = 0 if block_index > 0 : next_block_start = nesting_state . stack [ block_index - 1 ] . starting_linenum first_line = last_line while first_line >= next_block_start : if clean_lines . elided [ first_line ] . find ( 'template' ) >= 0 : break first_line -= 1 if first_line < next_block_start : block_index -= 1 continue for i in xrange ( first_line , last_line + 1 , 1 ) : if Search ( typename_pattern , clean_lines . elided [ i ] ) : return True block_index -= 1 return False
9452	def conference_mute ( self , call_params ) : path = '/' + self . api_version + '/ConferenceMute/' method = 'POST' return self . request ( path , method , call_params )
4138	def scale_image ( in_fname , out_fname , max_width , max_height ) : try : from PIL import Image except ImportError : import Image img = Image . open ( in_fname ) width_in , height_in = img . size scale_w = max_width / float ( width_in ) scale_h = max_height / float ( height_in ) if height_in * scale_w <= max_height : scale = scale_w else : scale = scale_h if scale >= 1.0 and in_fname == out_fname : return width_sc = int ( round ( scale * width_in ) ) height_sc = int ( round ( scale * height_in ) ) img . thumbnail ( ( width_sc , height_sc ) , Image . ANTIALIAS ) thumb = Image . new ( 'RGB' , ( max_width , max_height ) , ( 255 , 255 , 255 ) ) pos_insert = ( ( max_width - width_sc ) // 2 , ( max_height - height_sc ) // 2 ) thumb . paste ( img , pos_insert ) thumb . save ( out_fname ) if os . environ . get ( 'SKLEARN_DOC_OPTIPNG' , False ) : try : subprocess . call ( [ "optipng" , "-quiet" , "-o" , "9" , out_fname ] ) except Exception : warnings . warn ( 'Install optipng to reduce the size of the \ generated images' )
11348	def is_instance ( self ) : ret = False val = self . callback if self . is_class ( ) : return False ret = not inspect . isfunction ( val ) and not inspect . ismethod ( val ) return ret
4340	def remix ( self , remix_dictionary = None , num_output_channels = None ) : if not ( isinstance ( remix_dictionary , dict ) or remix_dictionary is None ) : raise ValueError ( "remix_dictionary must be a dictionary or None." ) if remix_dictionary is not None : if not all ( [ isinstance ( i , int ) and i > 0 for i in remix_dictionary . keys ( ) ] ) : raise ValueError ( "remix dictionary must have positive integer keys." ) if not all ( [ isinstance ( v , list ) for v in remix_dictionary . values ( ) ] ) : raise ValueError ( "remix dictionary values must be lists." ) for v_list in remix_dictionary . values ( ) : if not all ( [ isinstance ( v , int ) and v > 0 for v in v_list ] ) : raise ValueError ( "elements of remix dictionary values must " "be positive integers" ) if not ( ( isinstance ( num_output_channels , int ) and num_output_channels > 0 ) or num_output_channels is None ) : raise ValueError ( "num_output_channels must be a positive integer or None." ) effect_args = [ 'remix' ] if remix_dictionary is None : effect_args . append ( '-' ) else : if num_output_channels is None : num_output_channels = max ( remix_dictionary . keys ( ) ) for channel in range ( 1 , num_output_channels + 1 ) : if channel in remix_dictionary . keys ( ) : out_channel = ',' . join ( [ str ( i ) for i in remix_dictionary [ channel ] ] ) else : out_channel = '0' effect_args . append ( out_channel ) self . effects . extend ( effect_args ) self . effects_log . append ( 'remix' ) return self
11540	def pin_type ( self , pin ) : if type ( pin ) is list : return [ self . pin_type ( p ) for p in pin ] pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : return self . _pin_type ( pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
8565	def update_loadbalancer ( self , datacenter_id , loadbalancer_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
7267	def run ( self , * args , ** kw ) : log . debug ( '[operator] run "{}" with arguments: {}' . format ( self . __class__ . __name__ , args ) ) if self . kind == OperatorTypes . ATTRIBUTE : return self . match ( self . ctx ) else : return self . run_matcher ( * args , ** kw )
4910	def _create_session ( self , scope ) : now = datetime . datetime . utcnow ( ) if self . session is None or self . expires_at is None or now >= self . expires_at : if self . session : self . session . close ( ) oauth_access_token , expires_at = self . _get_oauth_access_token ( self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . degreed_user_id , self . enterprise_configuration . degreed_user_password , scope ) session = requests . Session ( ) session . timeout = self . SESSION_TIMEOUT session . headers [ 'Authorization' ] = 'Bearer {}' . format ( oauth_access_token ) session . headers [ 'content-type' ] = 'application/json' self . session = session self . expires_at = expires_at
3595	def details ( self , packageName ) : path = DETAILS_URL + "?doc={}" . format ( requests . utils . quote ( packageName ) ) data = self . executeRequestApi2 ( path ) return utils . parseProtobufObj ( data . payload . detailsResponse . docV2 )
1583	def generate ( ) : data_bytes = bytearray ( random . getrandbits ( 8 ) for i in range ( REQID . REQID_SIZE ) ) return REQID ( data_bytes )
4128	def readwav ( filename ) : from scipy . io . wavfile import read as readwav samplerate , signal = readwav ( filename ) return signal , samplerate
1674	def ParseArguments ( args ) : try : ( opts , filenames ) = getopt . getopt ( args , '' , [ 'help' , 'output=' , 'verbose=' , 'counting=' , 'filter=' , 'root=' , 'repository=' , 'linelength=' , 'extensions=' , 'exclude=' , 'headers=' , 'quiet' , 'recursive' ] ) except getopt . GetoptError : PrintUsage ( 'Invalid arguments.' ) verbosity = _VerboseLevel ( ) output_format = _OutputFormat ( ) filters = '' counting_style = '' recursive = False for ( opt , val ) in opts : if opt == '--help' : PrintUsage ( None ) elif opt == '--output' : if val not in ( 'emacs' , 'vs7' , 'eclipse' , 'junit' ) : PrintUsage ( 'The only allowed output formats are emacs, vs7, eclipse ' 'and junit.' ) output_format = val elif opt == '--verbose' : verbosity = int ( val ) elif opt == '--filter' : filters = val if not filters : PrintCategories ( ) elif opt == '--counting' : if val not in ( 'total' , 'toplevel' , 'detailed' ) : PrintUsage ( 'Valid counting options are total, toplevel, and detailed' ) counting_style = val elif opt == '--root' : global _root _root = val elif opt == '--repository' : global _repository _repository = val elif opt == '--linelength' : global _line_length try : _line_length = int ( val ) except ValueError : PrintUsage ( 'Line length must be digits.' ) elif opt == '--exclude' : global _excludes if not _excludes : _excludes = set ( ) _excludes . update ( glob . glob ( val ) ) elif opt == '--extensions' : global _valid_extensions try : _valid_extensions = set ( val . split ( ',' ) ) except ValueError : PrintUsage ( 'Extensions must be comma seperated list.' ) elif opt == '--headers' : global _header_extensions try : _header_extensions = set ( val . split ( ',' ) ) except ValueError : PrintUsage ( 'Extensions must be comma seperated list.' ) elif opt == '--recursive' : recursive = True elif opt == '--quiet' : global _quiet _quiet = True if not filenames : PrintUsage ( 'No files were specified.' ) if recursive : filenames = _ExpandDirectories ( filenames ) if _excludes : filenames = _FilterExcludedFiles ( filenames ) _SetOutputFormat ( output_format ) _SetVerboseLevel ( verbosity ) _SetFilters ( filters ) _SetCountingStyle ( counting_style ) return filenames
9694	def replace ( self , ** k ) : if self . date != 'infinity' : return Date ( self . date . replace ( ** k ) ) else : return Date ( 'infinity' )
1559	def component_id ( self ) : if isinstance ( self . _component_id , HeronComponentSpec ) : if self . _component_id . name is None : return "<No name available for HeronComponentSpec yet, uuid: %s>" % self . _component_id . uuid return self . _component_id . name elif isinstance ( self . _component_id , str ) : return self . _component_id else : raise ValueError ( "Component Id for this GlobalStreamId is not properly set: <%s:%s>" % ( str ( type ( self . _component_id ) ) , str ( self . _component_id ) ) )
123	def terminate ( self ) : for worker in self . workers : if worker . is_alive ( ) : worker . terminate ( ) self . nb_workers_finished = len ( self . workers ) if not self . queue_result . _closed : self . queue_result . close ( ) time . sleep ( 0.01 )
13237	def next_interval ( self , after = None ) : if after is None : after = timezone . now ( ) after = self . to_timezone ( after ) return next ( self . intervals ( range_start = after ) , None )
955	def title ( s = None , additional = '' , stream = sys . stdout ) : if s is None : callable_name , file_name , class_name = getCallerInfo ( 2 ) s = callable_name if class_name is not None : s = class_name + '.' + callable_name lines = ( s + additional ) . split ( '\n' ) length = max ( len ( line ) for line in lines ) print >> stream , '-' * length print >> stream , s + additional print >> stream , '-' * length
10742	def print_profile ( function ) : import memory_profiler def wrapper ( * args , ** kwargs ) : m = StringIO ( ) pr = cProfile . Profile ( ) pr . enable ( ) temp_func = memory_profiler . profile ( func = function , stream = m , precision = 4 ) output = temp_func ( * args , ** kwargs ) print ( m . getvalue ( ) ) pr . disable ( ) ps = pstats . Stats ( pr ) ps . sort_stats ( 'cumulative' ) . print_stats ( '(?!.*memory_profiler.*)(^.*$)' , 20 ) m . close ( ) return output return wrapper
2467	def set_file_license_in_file ( self , doc , lic ) : if self . has_package ( doc ) and self . has_file ( doc ) : if validations . validate_file_lics_in_file ( lic ) : self . file ( doc ) . add_lics ( lic ) return True else : raise SPDXValueError ( 'File::LicenseInFile' ) else : raise OrderError ( 'File::LicenseInFile' )
6771	def list_required ( self , type = None , service = None ) : from burlap . common import ( required_system_packages , required_python_packages , required_ruby_packages , ) service = ( service or '' ) . strip ( ) . upper ( ) type = ( type or '' ) . lower ( ) . strip ( ) assert not type or type in PACKAGE_TYPES , 'Unknown package type: %s' % ( type , ) packages_set = set ( ) packages = [ ] version = self . os_version for _service , satchel in self . all_other_enabled_satchels . items ( ) : _service = _service . strip ( ) . upper ( ) if service and service != _service : continue _new = [ ] if not type or type == SYSTEM : _new . extend ( required_system_packages . get ( _service , { } ) . get ( ( version . distro , version . release ) , [ ] ) ) try : _pkgs = satchel . packager_system_packages if self . verbose : print ( 'pkgs:' ) pprint ( _pkgs , indent = 4 ) for _key in [ ( version . distro , version . release ) , version . distro ] : if self . verbose : print ( 'checking key:' , _key ) if _key in _pkgs : if self . verbose : print ( 'satchel %s requires:' % satchel , _pkgs [ _key ] ) _new . extend ( _pkgs [ _key ] ) break except AttributeError : pass if not type or type == PYTHON : _new . extend ( required_python_packages . get ( _service , { } ) . get ( ( version . distro , version . release ) , [ ] ) ) try : _pkgs = satchel . packager_python_packages for _key in [ ( version . distro , version . release ) , version . distro ] : if _key in _pkgs : _new . extend ( _pkgs [ _key ] ) except AttributeError : pass print ( '_new:' , _new ) if not type or type == RUBY : _new . extend ( required_ruby_packages . get ( _service , { } ) . get ( ( version . distro , version . release ) , [ ] ) ) for _ in _new : if _ in packages_set : continue packages_set . add ( _ ) packages . append ( _ ) if self . verbose : for package in sorted ( packages ) : print ( 'package:' , package ) return packages
11861	def consistent_with ( event , evidence ) : "Is event consistent with the given evidence?" return every ( lambda ( k , v ) : evidence . get ( k , v ) == v , event . items ( ) )
13515	def residual_resistance_coef ( slenderness , prismatic_coef , froude_number ) : Cr = cr ( slenderness , prismatic_coef , froude_number ) if math . isnan ( Cr ) : Cr = cr_nearest ( slenderness , prismatic_coef , froude_number ) return Cr
2945	def refresh_waiting_tasks ( self ) : assert not self . read_only for my_task in self . get_tasks ( Task . WAITING ) : my_task . task_spec . _update ( my_task )
2814	def convert_shape ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting shape ...' ) def target_layer ( x ) : import tensorflow as tf return tf . shape ( x ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
7476	def inserted_indels ( indels , ocatg ) : newcatg = np . zeros ( ocatg . shape , dtype = np . uint32 ) for iloc in xrange ( ocatg . shape [ 0 ] ) : indidx = np . where ( indels [ iloc , : ] ) [ 0 ] if np . any ( indidx ) : allrows = np . arange ( ocatg . shape [ 1 ] ) mask = np . ones ( allrows . shape [ 0 ] , dtype = np . bool_ ) for idx in indidx : mask [ idx ] = False not_idx = allrows [ mask == 1 ] newcatg [ iloc ] [ not_idx ] = ocatg [ iloc , : not_idx . shape [ 0 ] ] else : newcatg [ iloc ] = ocatg [ iloc ] return newcatg
853	def appendRecord ( self , record ) : assert self . _file is not None assert self . _mode == self . _FILE_WRITE_MODE assert isinstance ( record , ( list , tuple ) ) , "unexpected record type: " + repr ( type ( record ) ) assert len ( record ) == self . _fieldCount , "len(record): %s, fieldCount: %s" % ( len ( record ) , self . _fieldCount ) if self . _recordCount == 0 : names , types , specials = zip ( * self . getFields ( ) ) for line in names , types , specials : self . _writer . writerow ( line ) self . _updateSequenceInfo ( record ) line = [ self . _adapters [ i ] ( f ) for i , f in enumerate ( record ) ] self . _writer . writerow ( line ) self . _recordCount += 1
5017	def handle_transmission_error ( self , learner_data , request_exception ) : try : sys_msg = request_exception . response . content except AttributeError : sys_msg = 'Not available' LOGGER . error ( ( 'Failed to send completion status call for enterprise enrollment %s' 'with payload %s' '\nError message: %s' '\nSystem message: %s' ) , learner_data . enterprise_course_enrollment_id , learner_data , str ( request_exception ) , sys_msg )
10797	def users ( ) : from invenio_groups . models import Group , Membership , PrivacyPolicy , SubscriptionPolicy admin = accounts . datastore . create_user ( email = 'admin@inveniosoftware.org' , password = encrypt_password ( '123456' ) , active = True , ) reader = accounts . datastore . create_user ( email = 'reader@inveniosoftware.org' , password = encrypt_password ( '123456' ) , active = True , ) admins = Group . create ( name = 'admins' , admins = [ admin ] ) for i in range ( 10 ) : Group . create ( name = 'group-{0}' . format ( i ) , admins = [ admin ] ) Membership . create ( admins , reader ) db . session . commit ( )
6705	def create ( self , username , groups = None , uid = None , create_home = None , system = False , password = None , home_dir = None ) : r = self . local_renderer r . env . username = username args = [ ] if uid : args . append ( '-u %s' % uid ) if create_home is None : create_home = not system if create_home is True : if home_dir : args . append ( '--home %s' % home_dir ) elif create_home is False : args . append ( '--no-create-home' ) if password is None : pass elif password : crypted_password = _crypt_password ( password ) args . append ( '-p %s' % quote ( crypted_password ) ) else : args . append ( '--disabled-password' ) args . append ( '--gecos ""' ) if system : args . append ( '--system' ) r . env . args = ' ' . join ( args ) r . env . groups = ( groups or '' ) . strip ( ) r . sudo ( 'adduser {args} {username} || true' ) if groups : for group in groups . split ( ' ' ) : group = group . strip ( ) if not group : continue r . sudo ( 'adduser %s %s || true' % ( username , group ) )
2124	def associate_always_node ( self , parent , child = None , ** kwargs ) : return self . _assoc_or_create ( 'always' , parent , child , ** kwargs )
98	def quokka_polygons ( size = None , extract = None ) : from imgaug . augmentables . polys import Polygon , PolygonsOnImage left , top = 0 , 0 if extract is not None : bb_extract = _quokka_normalize_extract ( extract ) left = bb_extract . x1 top = bb_extract . y1 with open ( QUOKKA_ANNOTATIONS_FP , "r" ) as f : json_dict = json . load ( f ) polygons = [ ] for poly_json in json_dict [ "polygons" ] : polygons . append ( Polygon ( [ ( point [ "x" ] - left , point [ "y" ] - top ) for point in poly_json [ "keypoints" ] ] ) ) if extract is not None : shape = ( bb_extract . height , bb_extract . width , 3 ) else : shape = ( 643 , 960 , 3 ) psoi = PolygonsOnImage ( polygons , shape = shape ) if size is not None : shape_resized = _compute_resized_shape ( shape , size ) psoi = psoi . on ( shape_resized ) return psoi
5597	def is_on_edge ( self ) : return ( self . left <= self . tile_pyramid . left or self . bottom <= self . tile_pyramid . bottom or self . right >= self . tile_pyramid . right or self . top >= self . tile_pyramid . top )
965	def bitsToString ( arr ) : s = array ( 'c' , '.' * len ( arr ) ) for i in xrange ( len ( arr ) ) : if arr [ i ] == 1 : s [ i ] = '*' return s
9564	def create_validator ( ) : field_names = ( 'study_id' , 'patient_id' , 'gender' , 'age_years' , 'age_months' , 'date_inclusion' ) validator = CSVValidator ( field_names ) validator . add_header_check ( 'EX1' , 'bad header' ) validator . add_record_length_check ( 'EX2' , 'unexpected record length' ) validator . add_value_check ( 'study_id' , int , 'EX3' , 'study id must be an integer' ) validator . add_value_check ( 'patient_id' , int , 'EX4' , 'patient id must be an integer' ) validator . add_value_check ( 'gender' , enumeration ( 'M' , 'F' ) , 'EX5' , 'invalid gender' ) validator . add_value_check ( 'age_years' , number_range_inclusive ( 0 , 120 , int ) , 'EX6' , 'invalid age in years' ) validator . add_value_check ( 'date_inclusion' , datetime_string ( '%Y-%m-%d' ) , 'EX7' , 'invalid date' ) def check_age_variables ( r ) : age_years = int ( r [ 'age_years' ] ) age_months = int ( r [ 'age_months' ] ) valid = ( age_months >= age_years * 12 and age_months % age_years < 12 ) if not valid : raise RecordError ( 'EX8' , 'invalid age variables' ) validator . add_record_check ( check_age_variables ) return validator
12114	def save ( self , filename , imdata , ** data ) : if isinstance ( imdata , numpy . ndarray ) : imdata = Image . fromarray ( numpy . uint8 ( imdata ) ) elif isinstance ( imdata , Image . Image ) : imdata . save ( self . _savepath ( filename ) )
7601	def get_popular_decks ( self , ** params : keys ) : url = self . api . POPULAR + '/decks' return self . _get_model ( url , ** params )
1766	def decode_instruction ( self , pc ) : if pc in self . _instruction_cache : return self . _instruction_cache [ pc ] text = b'' for address in range ( pc , pc + self . max_instr_width ) : if not self . memory . access_ok ( address , 'x' ) : break c = self . memory [ address ] if issymbolic ( c ) : if isinstance ( self . memory , LazySMemory ) : try : vals = visitors . simplify_array_select ( c ) c = bytes ( [ vals [ 0 ] ] ) except visitors . ArraySelectSimplifier . ExpressionNotSimple : c = struct . pack ( 'B' , solver . get_value ( self . memory . constraints , c ) ) elif isinstance ( c , Constant ) : c = bytes ( [ c . value ] ) else : logger . error ( 'Concretize executable memory %r %r' , c , text ) raise ConcretizeMemory ( self . memory , address = pc , size = 8 * self . max_instr_width , policy = 'INSTRUCTION' ) text += c code = text . ljust ( self . max_instr_width , b'\x00' ) try : insn = self . disasm . disassemble_instruction ( code , pc ) except StopIteration as e : raise DecodeException ( pc , code ) if not self . memory . access_ok ( slice ( pc , pc + insn . size ) , 'x' ) : logger . info ( "Trying to execute instructions from non-executable memory" ) raise InvalidMemoryAccess ( pc , 'x' ) insn . operands = self . _wrap_operands ( insn . operands ) self . _instruction_cache [ pc ] = insn return insn
9890	def _boottime_linux ( ) : global __boottime try : f = open ( '/proc/stat' , 'r' ) for line in f : if line . startswith ( 'btime' ) : __boottime = int ( line . split ( ) [ 1 ] ) if datetime is None : raise NotImplementedError ( 'datetime module required.' ) return datetime . fromtimestamp ( __boottime ) except ( IOError , IndexError ) : return None
7341	async def get_size ( media ) : if hasattr ( media , 'seek' ) : await execute ( media . seek ( 0 , os . SEEK_END ) ) size = await execute ( media . tell ( ) ) await execute ( media . seek ( 0 ) ) elif hasattr ( media , 'headers' ) : size = int ( media . headers [ 'Content-Length' ] ) elif isinstance ( media , bytes ) : size = len ( media ) else : raise TypeError ( "Can't get size of media of type:" , type ( media ) . __name__ ) _logger . info ( "media size: %dB" % size ) return size
7866	def set_item ( self , key , value , timeout = None , timeout_callback = None ) : with self . _lock : logger . debug ( "expdict.__setitem__({0!r}, {1!r}, {2!r}, {3!r})" . format ( key , value , timeout , timeout_callback ) ) if not timeout : timeout = self . _default_timeout self . _timeouts [ key ] = ( time . time ( ) + timeout , timeout_callback ) return dict . __setitem__ ( self , key , value )
9930	def post ( self , request ) : serializer = self . get_serializer ( data = request . data ) if serializer . is_valid ( ) : serializer . save ( ) return Response ( serializer . data ) return Response ( serializer . errors , status = status . HTTP_400_BAD_REQUEST )
7824	def finish ( self , data ) : if not self . _server_first_message : logger . debug ( "Got success too early" ) return Failure ( "bad-success" ) if self . _finished : return Success ( { "username" : self . username , "authzid" : self . authzid } ) else : ret = self . _final_challenge ( data ) if isinstance ( ret , Failure ) : return ret if self . _finished : return Success ( { "username" : self . username , "authzid" : self . authzid } ) else : logger . debug ( "Something went wrong when processing additional" " data with success?" ) return Failure ( "bad-success" )
8106	def copytree ( src , dst , symlinks = False , ignore = None ) : if not os . path . exists ( dst ) : os . makedirs ( dst ) shutil . copystat ( src , dst ) lst = os . listdir ( src ) if ignore : excl = ignore ( src , lst ) lst = [ x for x in lst if x not in excl ] for item in lst : s = os . path . join ( src , item ) d = os . path . join ( dst , item ) if symlinks and os . path . islink ( s ) : if os . path . lexists ( d ) : os . remove ( d ) os . symlink ( os . readlink ( s ) , d ) try : st = os . lstat ( s ) mode = stat . S_IMODE ( st . st_mode ) os . lchmod ( d , mode ) except : pass elif os . path . isdir ( s ) : copytree ( s , d , symlinks , ignore ) else : shutil . copy2 ( s , d )
12354	def change_kernel ( self , kernel_id , wait = True ) : return self . _action ( 'change_kernel' , kernel = kernel_id , wait = wait )
8065	def drawdaisy ( x , y , color = '#fefefe' ) : _ctx . push ( ) _fill = _ctx . fill ( ) _stroke = _ctx . stroke ( ) sc = ( 1.0 / _ctx . HEIGHT ) * float ( y * 0.5 ) * 4.0 _ctx . strokewidth ( sc * 2.0 ) _ctx . stroke ( '#3B240B' ) _ctx . line ( x + ( sin ( x * 0.1 ) * 10.0 ) , y + 80 , x + sin ( _ctx . FRAME * 0.1 ) , y ) _ctx . translate ( - 20 , 0 ) _ctx . scale ( sc ) _ctx . fill ( color ) _ctx . nostroke ( ) for angle in xrange ( 0 , 360 , 45 ) : _ctx . rotate ( degrees = 45 ) _ctx . rect ( x , y , 40 , 8 , 1 ) _ctx . fill ( '#F7FE2E' ) _ctx . ellipse ( x + 15 , y , 10 , 10 ) _ctx . fill ( _fill ) _ctx . stroke ( _stroke ) _ctx . pop ( )
11066	def add_user_to_allow ( self , name , user ) : if not self . remove_user_from_acl ( name , user ) : return False if name not in self . _acl : return False self . _acl [ name ] [ 'allow' ] . append ( user ) return True
2426	def set_doc_spdx_id ( self , doc , doc_spdx_id_line ) : if not self . doc_spdx_id_set : if doc_spdx_id_line == 'SPDXRef-DOCUMENT' : doc . spdx_id = doc_spdx_id_line self . doc_spdx_id_set = True return True else : raise SPDXValueError ( 'Document::SPDXID' ) else : raise CardinalityError ( 'Document::SPDXID' )
10350	def lint_directory ( source , target ) : for path in os . listdir ( source ) : if not path . endswith ( '.bel' ) : continue log . info ( 'linting: %s' , path ) with open ( os . path . join ( source , path ) ) as i , open ( os . path . join ( target , path ) , 'w' ) as o : lint_file ( i , o )
7252	def order ( self , image_catalog_ids , batch_size = 100 , callback = None ) : def _order_single_batch ( url_ , ids , results_list ) : data = json . dumps ( ids ) if callback is None else json . dumps ( { "acquisitionIds" : ids , "callback" : callback } ) r = self . gbdx_connection . post ( url_ , data = data ) r . raise_for_status ( ) order_id = r . json ( ) . get ( "order_id" ) if order_id : results_list . append ( order_id ) self . logger . debug ( 'Place order' ) url = ( '%s/order' if callback is None else '%s/ordercb' ) % self . base_url batch_size = min ( 100 , batch_size ) if not isinstance ( image_catalog_ids , list ) : image_catalog_ids = [ image_catalog_ids ] sanitized_ids = list ( set ( ( id for id in ( _id . strip ( ) for _id in image_catalog_ids ) if id ) ) ) res = [ ] acq_ids_by_batch = zip ( * ( [ iter ( sanitized_ids ) ] * batch_size ) ) for ids_batch in acq_ids_by_batch : _order_single_batch ( url , ids_batch , res ) remain_count = len ( sanitized_ids ) % batch_size if remain_count > 0 : _order_single_batch ( url , sanitized_ids [ - remain_count : ] , res ) if len ( res ) == 1 : return res [ 0 ] elif len ( res ) > 1 : return res
5402	def _get_prepare_env ( self , script , job_descriptor , inputs , outputs , mounts ) : docker_paths = sorted ( [ var . docker_path if var . recursive else os . path . dirname ( var . docker_path ) for var in inputs | outputs | mounts if var . value ] ) env = { _SCRIPT_VARNAME : repr ( script . value ) , _META_YAML_VARNAME : repr ( job_descriptor . to_yaml ( ) ) , 'DIR_COUNT' : str ( len ( docker_paths ) ) } for idx , path in enumerate ( docker_paths ) : env [ 'DIR_{}' . format ( idx ) ] = os . path . join ( providers_util . DATA_MOUNT_POINT , path ) return env
4197	def HERMTOEP ( T0 , T , Z ) : assert len ( T ) > 0 M = len ( T ) X = numpy . zeros ( M + 1 , dtype = complex ) A = numpy . zeros ( M , dtype = complex ) P = T0 if P == 0 : raise ValueError ( "P must be different from zero" ) X [ 0 ] = Z [ 0 ] / T0 for k in range ( 0 , M ) : save = T [ k ] beta = X [ 0 ] * T [ k ] if k == 0 : temp = - save / P else : for j in range ( 0 , k ) : save = save + A [ j ] * T [ k - j - 1 ] beta = beta + X [ j + 1 ] * T [ k - j - 1 ] temp = - save / P P = P * ( 1. - ( temp . real ** 2 + temp . imag ** 2 ) ) if P <= 0 : raise ValueError ( "singular matrix" ) A [ k ] = temp alpha = ( Z [ k + 1 ] - beta ) / P if k == 0 : X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * A [ k - j ] . conjugate ( ) continue khalf = ( k + 1 ) // 2 for j in range ( 0 , khalf ) : kj = k - j - 1 save = A [ j ] A [ j ] = save + temp * A [ kj ] . conjugate ( ) if j != kj : A [ kj ] = A [ kj ] + temp * save . conjugate ( ) X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * A [ k - j ] . conjugate ( ) return X
7722	def set_password ( self , password ) : for child in xml_element_iter ( self . xmlnode . children ) : if get_node_ns_uri ( child ) == MUC_NS and child . name == "password" : child . unlinkNode ( ) child . freeNode ( ) break if password is not None : self . xmlnode . newTextChild ( self . xmlnode . ns ( ) , "password" , to_utf8 ( password ) )
7062	def sqs_put_item ( queue_url , item , delay_seconds = 0 , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 'sqs' ) try : json_msg = json . dumps ( item ) resp = client . send_message ( QueueUrl = queue_url , MessageBody = json_msg , DelaySeconds = delay_seconds , ) if not resp : LOGERROR ( 'could not send item to queue: %s' % queue_url ) return None else : return resp except Exception as e : LOGEXCEPTION ( 'could not send item to queue: %s' % queue_url ) if raiseonfail : raise return None
4753	def tcase_parse_descr ( tcase ) : descr_short = "SHORT" descr_long = "LONG" try : comment = tcase_comment ( tcase ) except ( IOError , OSError , ValueError ) as exc : comment = [ ] cij . err ( "tcase_parse_descr: failed: %r, tcase: %r" % ( exc , tcase ) ) comment = [ l for l in comment if l . strip ( ) ] for line_number , line in enumerate ( comment ) : if line . startswith ( "#" ) : comment [ line_number ] = line [ 1 : ] if comment : descr_short = comment [ 0 ] if len ( comment ) > 1 : descr_long = "\n" . join ( comment [ 1 : ] ) return descr_short , descr_long
642	def readConfigFile ( cls , filename , path = None ) : properties = cls . _readConfigFile ( filename , path ) if cls . _properties is None : cls . _properties = dict ( ) for name in properties : if 'value' in properties [ name ] : cls . _properties [ name ] = properties [ name ] [ 'value' ]
8352	def handle_charref ( self , ref ) : "Handle character references as data." if self . convertEntities : data = unichr ( int ( ref ) ) else : data = '&#%s;' % ref self . handle_data ( data )
13690	def remove_peer ( self , peer ) : if type ( peer ) == list : for x in peer : check_url ( x ) for i in self . PEERS : if x in i : self . PEERS . remove ( i ) elif type ( peer ) == str : check_url ( peer ) for i in self . PEERS : if peer == i : self . PEERS . remove ( i ) else : raise ValueError ( 'peer paramater did not pass url validation' )
11831	def path ( self ) : "Return a list of nodes forming the path from the root to this node." node , path_back = self , [ ] while node : path_back . append ( node ) node = node . parent return list ( reversed ( path_back ) )
10290	def enrich_variants ( graph : BELGraph , func : Union [ None , str , Iterable [ str ] ] = None ) : if func is None : func = { PROTEIN , RNA , MIRNA , GENE } nodes = list ( get_nodes_by_function ( graph , func ) ) for u in nodes : parent = u . get_parent ( ) if parent is None : continue if parent not in graph : graph . add_has_variant ( parent , u )
9393	def check_important_sub_metrics ( self , sub_metric ) : if not self . important_sub_metrics : return False if sub_metric in self . important_sub_metrics : return True items = sub_metric . split ( '.' ) if items [ - 1 ] in self . important_sub_metrics : return True return False
9868	def rt_subscription_running ( self ) : return ( self . _tibber_control . sub_manager is not None and self . _tibber_control . sub_manager . is_running and self . _subscription_id is not None )
9139	def label ( labels = [ ] , language = 'any' , sortLabel = False ) : if not labels : return None if not language : language = 'und' labels = [ dict_to_label ( l ) for l in labels ] l = False if sortLabel : l = find_best_label_for_type ( labels , language , 'sortLabel' ) if not l : l = find_best_label_for_type ( labels , language , 'prefLabel' ) if not l : l = find_best_label_for_type ( labels , language , 'altLabel' ) if l : return l else : return label ( labels , 'any' , sortLabel ) if language != 'any' else None
9119	def dropbox_form ( request ) : from briefkasten import generate_post_token token = generate_post_token ( secret = request . registry . settings [ 'post_secret' ] ) return dict ( action = request . route_url ( 'dropbox_form_submit' , token = token ) , fileupload_url = request . route_url ( 'dropbox_fileupload' , token = token ) , ** defaults ( request ) )
13631	def _adaptToResource ( self , result ) : if result is None : return NotFound ( ) spinneretResource = ISpinneretResource ( result , None ) if spinneretResource is not None : return SpinneretResource ( spinneretResource ) renderable = IRenderable ( result , None ) if renderable is not None : return _RenderableResource ( renderable ) resource = IResource ( result , None ) if resource is not None : return resource if isinstance ( result , URLPath ) : return Redirect ( str ( result ) ) return result
9958	def restore_ipython ( self ) : if not self . is_ipysetup : return shell_class = type ( self . shell ) shell_class . showtraceback = shell_class . default_showtraceback del shell_class . default_showtraceback self . is_ipysetup = False
2886	def is_connected ( self , callback ) : index = self . _weakly_connected_index ( callback ) if index is not None : return True if self . hard_subscribers is None : return False return callback in self . _hard_callbacks ( )
6298	def draw ( self , mesh , projection_matrix = None , view_matrix = None , camera_matrix = None , time = 0 ) : self . program [ "m_proj" ] . write ( projection_matrix ) self . program [ "m_mv" ] . write ( view_matrix ) mesh . vao . render ( self . program )
4272	def create_output_directories ( self ) : check_or_create_dir ( self . dst_path ) if self . medias : check_or_create_dir ( join ( self . dst_path , self . settings [ 'thumb_dir' ] ) ) if self . medias and self . settings [ 'keep_orig' ] : self . orig_path = join ( self . dst_path , self . settings [ 'orig_dir' ] ) check_or_create_dir ( self . orig_path )
6256	def get_finder ( import_path ) : Finder = import_string ( import_path ) if not issubclass ( Finder , BaseFileSystemFinder ) : raise ImproperlyConfigured ( 'Finder {} is not a subclass of core.finders.FileSystemFinder' . format ( import_path ) ) return Finder ( )
8279	def _append_element ( self , render_func , pe ) : self . _render_funcs . append ( render_func ) self . _elements . append ( pe )
6341	def sim ( self , src , tar ) : if src == tar : return 1.0 if not src or not tar : return 0.0 min_word , max_word = ( src , tar ) if len ( src ) < len ( tar ) else ( tar , src ) min_len = len ( min_word ) for i in range ( min_len , 0 , - 1 ) : if min_word [ : i ] == max_word [ : i ] : return i / min_len return 0.0
12879	def sep1 ( parser , separator ) : first = [ parser ( ) ] def inner ( ) : separator ( ) return parser ( ) return first + many ( tri ( inner ) )
13788	def open ( name = None , fileobj = None , closefd = True ) : return Guesser ( ) . open ( name = name , fileobj = fileobj , closefd = closefd )
2592	def get_all_checkpoints ( rundir = "runinfo" ) : if ( not os . path . isdir ( rundir ) ) : return [ ] dirs = sorted ( os . listdir ( rundir ) ) checkpoints = [ ] for runid in dirs : checkpoint = os . path . abspath ( '{}/{}/checkpoint' . format ( rundir , runid ) ) if os . path . isdir ( checkpoint ) : checkpoints . append ( checkpoint ) return checkpoints
9359	def paragraphs ( quantity = 2 , separator = '\n\n' , wrap_start = '' , wrap_end = '' , html = False , sentences_quantity = 3 , as_list = False ) : if html : wrap_start = '<p>' wrap_end = '</p>' separator = '\n\n' result = [ ] try : for _ in xrange ( 0 , quantity ) : result . append ( wrap_start + sentences ( sentences_quantity ) + wrap_end ) except NameError : for _ in range ( 0 , quantity ) : result . append ( wrap_start + sentences ( sentences_quantity ) + wrap_end ) if as_list : return result else : return separator . join ( result )
11298	def get_all_text ( node ) : if node . nodeType == node . TEXT_NODE : return node . data else : text_string = "" for child_node in node . childNodes : text_string += get_all_text ( child_node ) return text_string
9826	def experiments ( ctx , metrics , declarations , independent , group , query , sort , page ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) page = page or 1 try : response = PolyaxonClient ( ) . project . list_experiments ( username = user , project_name = project_name , independent = independent , group = group , metrics = metrics , declarations = declarations , query = query , sort = sort , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get experiments for project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Experiments for project `{}/{}`.' . format ( user , project_name ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No experiments found for project `{}/{}`.' . format ( user , project_name ) ) if metrics : objects = get_experiments_with_metrics ( response ) elif declarations : objects = get_experiments_with_declarations ( response ) else : objects = [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) ) for o in response [ 'results' ] ] objects = list_dicts_to_tabulate ( objects ) if objects : Printer . print_header ( "Experiments:" ) objects . pop ( 'project_name' , None ) dict_tabulate ( objects , is_list_dict = True )
5354	def get_repos_by_backend_section ( cls , backend_section , raw = True ) : repos = [ ] projects = TaskProjects . get_projects ( ) for pro in projects : if backend_section in projects [ pro ] : if cls . GLOBAL_PROJECT not in projects : repos += projects [ pro ] [ backend_section ] else : if raw : if pro != cls . GLOBAL_PROJECT : if backend_section not in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ pro ] [ backend_section ] elif backend_section in projects [ pro ] and backend_section in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ cls . GLOBAL_PROJECT ] [ backend_section ] else : not_in_unknown = [ projects [ pro ] for pro in projects if pro != cls . GLOBAL_PROJECT ] [ 0 ] if backend_section not in not_in_unknown : repos += projects [ cls . GLOBAL_PROJECT ] [ backend_section ] else : if pro != cls . GLOBAL_PROJECT : if backend_section not in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ pro ] [ backend_section ] elif backend_section in projects [ pro ] and backend_section in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ pro ] [ backend_section ] else : not_in_unknown_prj = [ projects [ prj ] for prj in projects if prj != cls . GLOBAL_PROJECT ] not_in_unknown_sections = list ( set ( [ section for prj in not_in_unknown_prj for section in list ( prj . keys ( ) ) ] ) ) if backend_section not in not_in_unknown_sections : repos += projects [ pro ] [ backend_section ] logger . debug ( "List of repos for %s: %s (raw=%s)" , backend_section , repos , raw ) repos = list ( set ( repos ) ) return repos
6768	def update ( self ) : packager = self . packager if packager == APT : self . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq update' ) elif packager == YUM : self . sudo ( 'yum update' ) else : raise Exception ( 'Unknown packager: %s' % ( packager , ) )
9776	def outputs ( ctx ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) try : PolyaxonClient ( ) . job . download_outputs ( user , project_name , _job ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not download outputs for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( 'Files downloaded.' )
3347	def add_members ( self , new_members ) : if isinstance ( new_members , string_types ) or hasattr ( new_members , "id" ) : warn ( "need to pass in a list" ) new_members = [ new_members ] self . _members . update ( new_members )
13036	def read_openke_translation ( filename , delimiter = '\t' , entity_first = True ) : result = { } with open ( filename , "r" ) as f : _ = next ( f ) for line in f : line_slice = line . rstrip ( ) . split ( delimiter ) if not entity_first : line_slice = list ( reversed ( line_slice ) ) result [ line_slice [ 0 ] ] = line_slice [ 1 ] return result
977	def _newRepresentationOK ( self , newRep , newIndex ) : if newRep . size != self . w : return False if ( newIndex < self . minIndex - 1 ) or ( newIndex > self . maxIndex + 1 ) : raise ValueError ( "newIndex must be within one of existing indices" ) newRepBinary = numpy . array ( [ False ] * self . n ) newRepBinary [ newRep ] = True midIdx = self . _maxBuckets / 2 runningOverlap = self . _countOverlap ( self . bucketMap [ self . minIndex ] , newRep ) if not self . _overlapOK ( self . minIndex , newIndex , overlap = runningOverlap ) : return False for i in range ( self . minIndex + 1 , midIdx + 1 ) : newBit = ( i - 1 ) % self . w if newRepBinary [ self . bucketMap [ i - 1 ] [ newBit ] ] : runningOverlap -= 1 if newRepBinary [ self . bucketMap [ i ] [ newBit ] ] : runningOverlap += 1 if not self . _overlapOK ( i , newIndex , overlap = runningOverlap ) : return False for i in range ( midIdx + 1 , self . maxIndex + 1 ) : newBit = i % self . w if newRepBinary [ self . bucketMap [ i - 1 ] [ newBit ] ] : runningOverlap -= 1 if newRepBinary [ self . bucketMap [ i ] [ newBit ] ] : runningOverlap += 1 if not self . _overlapOK ( i , newIndex , overlap = runningOverlap ) : return False return True
4736	def info ( txt ) : print ( "%s# %s%s%s" % ( PR_EMPH_CC , get_time_stamp ( ) , txt , PR_NC ) ) sys . stdout . flush ( )
5563	def output ( self ) : output_params = dict ( self . _raw [ "output" ] , grid = self . output_pyramid . grid , pixelbuffer = self . output_pyramid . pixelbuffer , metatiling = self . output_pyramid . metatiling ) if "path" in output_params : output_params . update ( path = absolute_path ( path = output_params [ "path" ] , base_dir = self . config_dir ) ) if "format" not in output_params : raise MapcheteConfigError ( "output format not specified" ) if output_params [ "format" ] not in available_output_formats ( ) : raise MapcheteConfigError ( "format %s not available in %s" % ( output_params [ "format" ] , str ( available_output_formats ( ) ) ) ) writer = load_output_writer ( output_params ) try : writer . is_valid_with_config ( output_params ) except Exception as e : logger . exception ( e ) raise MapcheteConfigError ( "driver %s not compatible with configuration: %s" % ( writer . METADATA [ "driver_name" ] , e ) ) return writer
5833	def create_ml_configuration_from_datasets ( self , dataset_ids ) : available_columns = self . search_template_client . get_available_columns ( dataset_ids ) search_template = self . search_template_client . create ( dataset_ids , available_columns ) return self . create_ml_configuration ( search_template , available_columns , dataset_ids )
133	def clip_out_of_image ( self , image ) : import shapely . geometry if self . is_out_of_image ( image , fully = True , partly = False ) : return [ ] h , w = image . shape [ 0 : 2 ] if ia . is_np_array ( image ) else image [ 0 : 2 ] poly_shapely = self . to_shapely_polygon ( ) poly_image = shapely . geometry . Polygon ( [ ( 0 , 0 ) , ( w , 0 ) , ( w , h ) , ( 0 , h ) ] ) multipoly_inter_shapely = poly_shapely . intersection ( poly_image ) if not isinstance ( multipoly_inter_shapely , shapely . geometry . MultiPolygon ) : ia . do_assert ( isinstance ( multipoly_inter_shapely , shapely . geometry . Polygon ) ) multipoly_inter_shapely = shapely . geometry . MultiPolygon ( [ multipoly_inter_shapely ] ) polygons = [ ] for poly_inter_shapely in multipoly_inter_shapely . geoms : polygons . append ( Polygon . from_shapely ( poly_inter_shapely , label = self . label ) ) polygons_reordered = [ ] for polygon in polygons : found = False for x , y in self . exterior : closest_idx , dist = polygon . find_closest_point_index ( x = x , y = y , return_distance = True ) if dist < 1e-6 : polygon_reordered = polygon . change_first_point_by_index ( closest_idx ) polygons_reordered . append ( polygon_reordered ) found = True break ia . do_assert ( found ) return polygons_reordered
6536	def output_error ( msg ) : click . echo ( click . style ( msg , fg = 'red' ) , err = True )
288	def plot_returns ( returns , live_start_date = None , ax = None ) : if ax is None : ax = plt . gca ( ) ax . set_label ( '' ) ax . set_ylabel ( 'Returns' ) if live_start_date is not None : live_start_date = ep . utils . get_utc_timestamp ( live_start_date ) is_returns = returns . loc [ returns . index < live_start_date ] oos_returns = returns . loc [ returns . index >= live_start_date ] is_returns . plot ( ax = ax , color = 'g' ) oos_returns . plot ( ax = ax , color = 'r' ) else : returns . plot ( ax = ax , color = 'g' ) return ax
11571	def set_bit_map ( self , shape , color ) : for row in range ( 0 , 8 ) : data = shape [ row ] bit_mask = 0x80 for column in range ( 0 , 8 ) : if data & bit_mask : self . set_pixel ( row , column , color , True ) bit_mask >>= 1 self . output_entire_buffer ( )
11068	def delete_acl ( self , name ) : if name not in self . _acl : return False del self . _acl [ name ] return True
9165	def expandvars_dict ( settings ) : return dict ( ( key , os . path . expandvars ( value ) ) for key , value in settings . iteritems ( ) )
11824	def genetic_search ( problem , fitness_fn , ngen = 1000 , pmut = 0.1 , n = 20 ) : s = problem . initial_state states = [ problem . result ( s , a ) for a in problem . actions ( s ) ] random . shuffle ( states ) return genetic_algorithm ( states [ : n ] , problem . value , ngen , pmut )
383	def pt2map ( list_points = None , size = ( 100 , 100 ) , val = 1 ) : if list_points is None : raise Exception ( "list_points : list of 2 int" ) i_m = np . zeros ( size ) if len ( list_points ) == 0 : return i_m for xx in list_points : for x in xx : i_m [ int ( np . round ( x [ 0 ] ) ) ] [ int ( np . round ( x [ 1 ] ) ) ] = val return i_m
12676	def _escape_char ( c , escape_char = ESCAPE_CHAR ) : buf = [ ] for byte in c . encode ( 'utf8' ) : buf . append ( escape_char ) buf . append ( '%X' % _ord ( byte ) ) return '' . join ( buf )
4888	def update_enterprise_courses ( self , enterprise_customer , course_container_key = 'results' , ** kwargs ) : enterprise_context = { 'tpa_hint' : enterprise_customer and enterprise_customer . identity_provider , 'enterprise_id' : enterprise_customer and str ( enterprise_customer . uuid ) , } enterprise_context . update ( ** kwargs ) courses = [ ] for course in self . data [ course_container_key ] : courses . append ( self . update_course ( course , enterprise_customer , enterprise_context ) ) self . data [ course_container_key ] = courses
5437	def args_to_job_params ( envs , labels , inputs , inputs_recursive , outputs , outputs_recursive , mounts , input_file_param_util , output_file_param_util , mount_param_util ) : env_data = parse_pair_args ( envs , job_model . EnvParam ) label_data = parse_pair_args ( labels , job_model . LabelParam ) input_data = set ( ) for ( recursive , args ) in ( ( False , inputs ) , ( True , inputs_recursive ) ) : for arg in args : name , value = split_pair ( arg , '=' , nullable_idx = 0 ) name = input_file_param_util . get_variable_name ( name ) input_data . add ( input_file_param_util . make_param ( name , value , recursive ) ) output_data = set ( ) for ( recursive , args ) in ( ( False , outputs ) , ( True , outputs_recursive ) ) : for arg in args : name , value = split_pair ( arg , '=' , 0 ) name = output_file_param_util . get_variable_name ( name ) output_data . add ( output_file_param_util . make_param ( name , value , recursive ) ) mount_data = set ( ) for arg in mounts : if ' ' in arg : key_value_pair , disk_size = arg . split ( ' ' ) name , value = split_pair ( key_value_pair , '=' , 1 ) mount_data . add ( mount_param_util . make_param ( name , value , disk_size ) ) else : name , value = split_pair ( arg , '=' , 1 ) mount_data . add ( mount_param_util . make_param ( name , value , disk_size = None ) ) return { 'envs' : env_data , 'inputs' : input_data , 'outputs' : output_data , 'labels' : label_data , 'mounts' : mount_data , }
13843	def close ( self ) : try : self . conn . close ( ) self . logger . debug ( "Close connect succeed." ) except pymssql . Error as e : self . unknown ( "Close connect error: %s" % e )
7541	def nfilter1 ( data , reps ) : if sum ( reps ) >= data . paramsdict [ "mindepth_majrule" ] and sum ( reps ) <= data . paramsdict [ "maxdepth" ] : return 1 else : return 0
6100	def intensities_from_grid_radii ( self , grid_radii ) : return np . multiply ( np . divide ( self . intensity , self . sigma * np . sqrt ( 2.0 * np . pi ) ) , np . exp ( - 0.5 * np . square ( np . divide ( grid_radii , self . sigma ) ) ) )
13651	def get_reference_data ( self , modified_since : Optional [ datetime . datetime ] = None ) -> GetReferenceDataResponse : if modified_since is None : modified_since = datetime . datetime ( year = 2010 , month = 1 , day = 1 ) response = requests . get ( '{}/lovs' . format ( API_URL_BASE ) , headers = { 'if-modified-since' : self . _format_dt ( modified_since ) , ** self . _get_headers ( ) , } , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) return GetReferenceDataResponse . deserialize ( response . json ( ) )
9007	def transfer_to_row ( self , new_row ) : if new_row != self . _row : index = self . get_index_in_row ( ) if index is not None : self . _row . instructions . pop ( index ) self . _row = new_row
2498	def handle_package_has_file ( self , package , package_node ) : file_nodes = map ( self . handle_package_has_file_helper , package . files ) triples = [ ( package_node , self . spdx_namespace . hasFile , node ) for node in file_nodes ] for triple in triples : self . graph . add ( triple )
425	def delete_tasks ( self , ** kwargs ) : self . _fill_project_info ( kwargs ) self . db . Task . delete_many ( kwargs ) logging . info ( "[Database] Delete Task SUCCESS" )
3698	def Hsub ( T = 298.15 , P = 101325 , MW = None , AvailableMethods = False , Method = None , CASRN = '' ) : def list_methods ( ) : methods = [ ] if CASRN in GharagheiziHsub_data . index : methods . append ( 'Ghazerati Appendix, at 298K' ) methods . append ( 'None' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'Ghazerati Appendix, at 298K' : _Hsub = float ( GharagheiziHsub_data . at [ CASRN , 'Hsub' ] ) elif Method == 'None' or not _Hsub or not MW : return None else : raise Exception ( 'Failure in in function' ) _Hsub = property_molar_to_mass ( _Hsub , MW ) return _Hsub
11516	def search_item_by_name_and_folder_name ( self , name , folder_name , token = None ) : parameters = dict ( ) parameters [ 'name' ] = name parameters [ 'folderName' ] = folder_name if token : parameters [ 'token' ] = token response = self . request ( 'midas.item.searchbynameandfoldername' , parameters ) return response [ 'items' ]
8234	def complementary ( clr ) : clr = color ( clr ) colors = colorlist ( clr ) c = clr . copy ( ) if clr . brightness > 0.4 : c . brightness = 0.1 + c . brightness * 0.25 else : c . brightness = 1.0 - c . brightness * 0.25 colors . append ( c ) c = clr . copy ( ) c . brightness = 0.3 + c . brightness c . saturation = 0.1 + c . saturation * 0.3 colors . append ( c ) clr = clr . complement c = clr . copy ( ) if clr . brightness > 0.3 : c . brightness = 0.1 + clr . brightness * 0.25 else : c . brightness = 1.0 - c . brightness * 0.25 colors . append ( c ) colors . append ( clr ) c = clr . copy ( ) c . brightness = 0.3 + c . brightness c . saturation = 0.1 + c . saturation * 0.25 colors . append ( c ) return colors
10841	def delete ( self ) : url = PATHS [ 'DELETE' ] % self . id return self . api . post ( url = url )
11688	def get_changeset ( changeset ) : url = 'https://www.openstreetmap.org/api/0.6/changeset/{}/download' . format ( changeset ) return ET . fromstring ( requests . get ( url ) . content )
11290	def load_dict ( self , source , namespace = '' ) : for key , value in source . items ( ) : if isinstance ( key , str ) : nskey = ( namespace + '.' + key ) . strip ( '.' ) if isinstance ( value , dict ) : self . load_dict ( value , namespace = nskey ) else : self [ nskey ] = value else : raise TypeError ( 'Key has type %r (not a string)' % type ( key ) ) return self
5610	def _shift_required ( tiles ) : if tiles [ 0 ] [ 0 ] . tile_pyramid . is_global : tile_cols = sorted ( list ( set ( [ t [ 0 ] . col for t in tiles ] ) ) ) if tile_cols == list ( range ( min ( tile_cols ) , max ( tile_cols ) + 1 ) ) : return False else : def gen_groups ( items ) : j = items [ 0 ] group = [ j ] for i in items [ 1 : ] : if i == j + 1 : group . append ( i ) else : yield group group = [ i ] j = i yield group groups = list ( gen_groups ( tile_cols ) ) if len ( groups ) == 1 : return False normal_distance = groups [ - 1 ] [ - 1 ] - groups [ 0 ] [ 0 ] antimeridian_distance = ( groups [ 0 ] [ - 1 ] + tiles [ 0 ] [ 0 ] . tile_pyramid . matrix_width ( tiles [ 0 ] [ 0 ] . zoom ) ) - groups [ - 1 ] [ 0 ] return antimeridian_distance < normal_distance else : return False
7611	def get_top_clans ( self , location_id = 'global' , ** params : keys ) : url = self . api . LOCATIONS + '/' + str ( location_id ) + '/rankings/clans' return self . _get_model ( url , PartialClan , ** params )
11829	def expand ( self , problem ) : "List the nodes reachable in one step from this node." return [ self . child_node ( problem , action ) for action in problem . actions ( self . state ) ]
12364	def transfer ( self , region ) : action = self . post ( type = 'transfer' , region = region ) [ 'action' ] return self . parent . get ( action [ 'resource_id' ] )
5205	def proc_elms ( ** kwargs ) -> list : return [ ( ELEM_KEYS . get ( k , k ) , ELEM_VALS . get ( ELEM_KEYS . get ( k , k ) , dict ( ) ) . get ( v , v ) ) for k , v in kwargs . items ( ) if ( k in list ( ELEM_KEYS . keys ( ) ) + list ( ELEM_KEYS . values ( ) ) ) and ( k not in PRSV_COLS ) ]
7451	def get_quart_iter ( tups ) : if tups [ 0 ] . endswith ( ".gz" ) : ofunc = gzip . open else : ofunc = open ofile1 = ofunc ( tups [ 0 ] , 'r' ) fr1 = iter ( ofile1 ) quart1 = itertools . izip ( fr1 , fr1 , fr1 , fr1 ) if tups [ 1 ] : ofile2 = ofunc ( tups [ 1 ] , 'r' ) fr2 = iter ( ofile2 ) quart2 = itertools . izip ( fr2 , fr2 , fr2 , fr2 ) quarts = itertools . izip ( quart1 , quart2 ) else : ofile2 = 0 quarts = itertools . izip ( quart1 , iter ( int , 1 ) ) def feedme ( quarts ) : for quart in quarts : yield quart genquarts = feedme ( quarts ) return genquarts , ofile1 , ofile2
12115	def fileModifiedTimestamp ( fname ) : modifiedTime = os . path . getmtime ( fname ) stamp = time . strftime ( '%Y-%m-%d' , time . localtime ( modifiedTime ) ) return stamp
10344	def load_differential_gene_expression ( path : str , gene_symbol_column : str = 'Gene.symbol' , logfc_column : str = 'logFC' , aggregator : Optional [ Callable [ [ List [ float ] ] , float ] ] = None , ) -> Mapping [ str , float ] : if aggregator is None : aggregator = np . median df = pd . read_csv ( path ) assert gene_symbol_column in df . columns assert logfc_column in df . columns df = df . loc [ df [ gene_symbol_column ] . notnull ( ) , [ gene_symbol_column , logfc_column ] ] values = defaultdict ( list ) for _ , gene_symbol , log_fold_change in df . itertuples ( ) : values [ gene_symbol ] . append ( log_fold_change ) return { gene_symbol : aggregator ( log_fold_changes ) for gene_symbol , log_fold_changes in values . items ( ) }
3414	def model_to_dict ( model , sort = False ) : obj = OrderedDict ( ) obj [ "metabolites" ] = list ( map ( metabolite_to_dict , model . metabolites ) ) obj [ "reactions" ] = list ( map ( reaction_to_dict , model . reactions ) ) obj [ "genes" ] = list ( map ( gene_to_dict , model . genes ) ) obj [ "id" ] = model . id _update_optional ( model , obj , _OPTIONAL_MODEL_ATTRIBUTES , _ORDERED_OPTIONAL_MODEL_KEYS ) if sort : get_id = itemgetter ( "id" ) obj [ "metabolites" ] . sort ( key = get_id ) obj [ "reactions" ] . sort ( key = get_id ) obj [ "genes" ] . sort ( key = get_id ) return obj
9502	def intersection ( l1 , l2 ) : if len ( l1 ) == 0 or len ( l2 ) == 0 : return [ ] out = [ ] l2_pos = 0 for l in l1 : while l2_pos < len ( l2 ) and l2 [ l2_pos ] . end < l . start : l2_pos += 1 if l2_pos == len ( l2 ) : break while l2_pos < len ( l2 ) and l . intersects ( l2 [ l2_pos ] ) : out . append ( l . intersection ( l2 [ l2_pos ] ) ) l2_pos += 1 l2_pos = max ( 0 , l2_pos - 1 ) return out
7913	def validate_string_list ( value ) : try : if sys . version_info . major < 3 : from locale import getpreferredencoding encoding = getpreferredencoding ( ) value = value . decode ( encoding ) return [ x . strip ( ) for x in value . split ( u"," ) ] except ( AttributeError , TypeError , UnicodeError ) : raise ValueError ( "Bad string list" )
8562	def list_loadbalancers ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
771	def __constructMetricsModules ( self , metricSpecs ) : if not metricSpecs : return self . __metricSpecs = metricSpecs for spec in metricSpecs : if not InferenceElement . validate ( spec . inferenceElement ) : raise ValueError ( "Invalid inference element for metric spec: %r" % spec ) self . __metrics . append ( metrics . getModule ( spec ) ) self . __metricLabels . append ( spec . getLabel ( ) )
9140	def find_best_label_for_type ( labels , language , labeltype ) : typelabels = [ l for l in labels if l . type == labeltype ] if not typelabels : return False if language == 'any' : return typelabels [ 0 ] exact = filter_labels_by_language ( typelabels , language ) if exact : return exact [ 0 ] inexact = filter_labels_by_language ( typelabels , language , True ) if inexact : return inexact [ 0 ] return False
2717	def add_droplets ( self , droplet ) : droplets = droplet if not isinstance ( droplets , list ) : droplets = [ droplet ] resources = self . __extract_resources_from_droplets ( droplets ) if len ( resources ) > 0 : return self . __add_resources ( resources ) return False
13829	def remove ( self , collection , ** kwargs ) : callback = kwargs . pop ( 'callback' ) yield Op ( self . db [ collection ] . remove , kwargs ) callback ( )
12506	def signed_session ( self , session = None ) : from sfctl . config import ( aad_metadata , aad_cache ) if session : session = super ( AdalAuthentication , self ) . signed_session ( session ) else : session = super ( AdalAuthentication , self ) . signed_session ( ) if self . no_verify : session . verify = False authority_uri , cluster_id , client_id = aad_metadata ( ) existing_token , existing_cache = aad_cache ( ) context = adal . AuthenticationContext ( authority_uri , cache = existing_cache ) new_token = context . acquire_token ( cluster_id , existing_token [ 'userId' ] , client_id ) header = "{} {}" . format ( "Bearer" , new_token [ 'accessToken' ] ) session . headers [ 'Authorization' ] = header return session
5604	def _get_warped_array ( input_file = None , indexes = None , dst_bounds = None , dst_shape = None , dst_crs = None , resampling = None , src_nodata = None , dst_nodata = None ) : try : return _rasterio_read ( input_file = input_file , indexes = indexes , dst_bounds = dst_bounds , dst_shape = dst_shape , dst_crs = dst_crs , resampling = resampling , src_nodata = src_nodata , dst_nodata = dst_nodata ) except Exception as e : logger . exception ( "error while reading file %s: %s" , input_file , e ) raise
11281	def clone ( self ) : new_object = copy . copy ( self ) if new_object . next : new_object . next = new_object . next . clone ( ) return new_object
13139	def to_json ( self ) : if self . subreference is not None : return { "source" : self . objectId , "selector" : { "type" : "FragmentSelector" , "conformsTo" : "http://ontology-dts.org/terms/subreference" , "value" : self . subreference } } else : return { "source" : self . objectId }
9516	def subseq ( self , start , end ) : return Fastq ( self . id , self . seq [ start : end ] , self . qual [ start : end ] )
7566	def ambigcutters ( seq ) : resos = [ ] if any ( [ i in list ( "RKSYWM" ) for i in seq ] ) : for base in list ( "RKSYWM" ) : if base in seq : resos . append ( seq . replace ( base , AMBIGS [ base ] [ 0 ] ) ) resos . append ( seq . replace ( base , AMBIGS [ base ] [ 1 ] ) ) return resos else : return [ seq , "" ]
10824	def query_by_group ( cls , group_or_id , with_invitations = False , ** kwargs ) : if isinstance ( group_or_id , Group ) : id_group = group_or_id . id else : id_group = group_or_id if not with_invitations : return cls . _filter ( cls . query . filter_by ( id_group = id_group ) , ** kwargs ) else : return cls . query . filter ( Membership . id_group == id_group , db . or_ ( Membership . state == MembershipState . PENDING_USER , Membership . state == MembershipState . ACTIVE ) )
11633	def get_orthographies ( self , _library = library ) : results = [ ] for charset in _library . charsets : if self . _charsets : cn = getattr ( charset , 'common_name' , False ) abbr = getattr ( charset , 'abbreviation' , False ) nn = getattr ( charset , 'short_name' , False ) naive = getattr ( charset , 'native_name' , False ) if cn and cn . lower ( ) in self . _charsets : results . append ( charset ) elif nn and nn . lower ( ) in self . _charsets : results . append ( charset ) elif naive and naive . lower ( ) in self . _charsets : results . append ( charset ) elif abbr and abbr . lower ( ) in self . _charsets : results . append ( charset ) else : results . append ( charset ) for result in results : yield CharsetInfo ( self , result )
606	def nupicBindingsPrereleaseInstalled ( ) : try : nupicDistribution = pkg_resources . get_distribution ( "nupic.bindings" ) if pkg_resources . parse_version ( nupicDistribution . version ) . is_prerelease : return True except pkg_resources . DistributionNotFound : pass return False
8642	def create_milestone_payment ( session , project_id , bidder_id , amount , reason , description ) : milestone_data = { 'project_id' : project_id , 'bidder_id' : bidder_id , 'amount' : amount , 'reason' : reason , 'description' : description } response = make_post_request ( session , 'milestones' , json_data = milestone_data ) json_data = response . json ( ) if response . status_code == 200 : milestone_data = json_data [ 'result' ] return Milestone ( milestone_data ) else : raise MilestoneNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
6366	def population ( self ) : return self . _tp + self . _tn + self . _fp + self . _fn
8684	def load ( self , origin_passphrase , keys = None , key_file = None ) : self . _assert_valid_stash ( ) if not ( bool ( keys ) ^ bool ( key_file ) ) : raise GhostError ( 'You must either provide a path to an exported stash file ' 'or a list of key dicts to import' ) if key_file : with open ( key_file ) as stash_file : keys = json . loads ( stash_file . read ( ) ) decrypt = origin_passphrase != self . passphrase if decrypt : stub = Stash ( TinyDBStorage ( 'stub' ) , origin_passphrase ) for key in keys : self . put ( name = key [ 'name' ] , value = stub . _decrypt ( key [ 'value' ] ) if decrypt else key [ 'value' ] , metadata = key [ 'metadata' ] , description = key [ 'description' ] , lock = key . get ( 'lock' ) , key_type = key . get ( 'type' ) , encrypt = decrypt )
12332	def get_diffs ( history ) : mgr = plugins_get_mgr ( ) keys = mgr . search ( 'representation' ) [ 'representation' ] representations = [ mgr . get_by_key ( 'representation' , k ) for k in keys ] for i in range ( len ( history ) ) : if i + 1 > len ( history ) - 1 : continue prev = history [ i ] curr = history [ i + 1 ] for c in curr [ 'changes' ] : path = c [ 'path' ] if c [ 'path' ] . endswith ( 'datapackage.json' ) : continue handler = None for r in representations : if r . can_process ( path ) : handler = r break if handler is None : continue v1_hex = prev [ 'commit' ] v2_hex = curr [ 'commit' ] temp1 = tempfile . mkdtemp ( prefix = "dgit-diff-" ) try : for h in [ v1_hex , v2_hex ] : filename = '{}/{}/checkout.tar' . format ( temp1 , h ) try : os . makedirs ( os . path . dirname ( filename ) ) except : pass extractcmd = [ 'git' , 'archive' , '-o' , filename , h , path ] output = run ( extractcmd ) if 'fatal' in output : raise Exception ( "File not present in commit" ) with cd ( os . path . dirname ( filename ) ) : cmd = [ 'tar' , 'xvf' , 'checkout.tar' ] output = run ( cmd ) if 'fatal' in output : print ( "Cleaning up - fatal 1" , temp1 ) shutil . rmtree ( temp1 ) continue path1 = os . path . join ( temp1 , v1_hex , path ) path2 = os . path . join ( temp1 , v2_hex , path ) if not os . path . exists ( path1 ) or not os . path . exists ( path2 ) : shutil . rmtree ( temp1 ) continue diff = handler . get_diff ( path1 , path2 ) c [ 'diff' ] = diff except Exception as e : shutil . rmtree ( temp1 )
8880	def predict_proba ( self , X ) : check_is_fitted ( self , [ 'tree' ] ) X = check_array ( X ) return self . tree . query ( X ) [ 0 ] . flatten ( )
3744	def _round_whole_even ( i ) : r if i % .5 == 0 : if ( i + 0.5 ) % 2 == 0 : i = i + 0.5 else : i = i - 0.5 else : i = round ( i , 0 ) return int ( i )
6152	def fir_remez_bpf ( f_stop1 , f_pass1 , f_pass2 , f_stop2 , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : n , ff , aa , wts = bandpass_order ( f_stop1 , f_pass1 , f_pass2 , f_stop2 , d_pass , d_stop , fsamp = fs ) N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 ) print ( 'Remez filter taps = %d.' % N_taps ) return b
11067	def create_acl ( self , name ) : if name in self . _acl : return False self . _acl [ name ] = { 'allow' : [ ] , 'deny' : [ ] } return True
6353	def _normalize_lang_attrs ( self , text , strip ) : uninitialized = - 1 attrib = uninitialized while '[' in text : bracket_start = text . find ( '[' ) bracket_end = text . find ( ']' , bracket_start ) if bracket_end == - 1 : raise ValueError ( 'No closing square bracket: text=(' + text + ') strip=(' + text_type ( strip ) + ')' ) attrib &= int ( text [ bracket_start + 1 : bracket_end ] ) text = text [ : bracket_start ] + text [ bracket_end + 1 : ] if attrib == uninitialized or strip : return text elif attrib == 0 : return '[0]' return text + '[' + str ( attrib ) + ']'
3835	async def set_active_client ( self , set_active_client_request ) : response = hangouts_pb2 . SetActiveClientResponse ( ) await self . _pb_request ( 'clients/setactiveclient' , set_active_client_request , response ) return response
12678	def unescape ( escaped , escape_char = ESCAPE_CHAR ) : if isinstance ( escaped , bytes ) : escaped = escaped . decode ( 'utf8' ) escape_pat = re . compile ( re . escape ( escape_char ) . encode ( 'utf8' ) + b'([a-z0-9]{2})' , re . IGNORECASE ) buf = escape_pat . subn ( _unescape_char , escaped . encode ( 'utf8' ) ) [ 0 ] return buf . decode ( 'utf8' )
9286	def sendall ( self , line ) : if isinstance ( line , APRSPacket ) : line = str ( line ) elif not isinstance ( line , string_type ) : raise TypeError ( "Expected line to be str or APRSPacket, got %s" , type ( line ) ) if not self . _connected : raise ConnectionError ( "not connected" ) if line == "" : return line = line . rstrip ( "\r\n" ) + "\r\n" try : self . sock . setblocking ( 1 ) self . sock . settimeout ( 5 ) self . _sendall ( line ) except socket . error as exp : self . close ( ) raise ConnectionError ( str ( exp ) )
9562	def _apply_skips ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for skip in self . _skips : try : result = skip ( r ) if result is True : yield True except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( skip . __name__ , skip . __doc__ ) if context is not None : p [ 'context' ] = context yield p
962	def matchPatterns ( patterns , keys ) : results = [ ] if patterns : for pattern in patterns : prog = re . compile ( pattern ) for key in keys : if prog . match ( key ) : results . append ( key ) else : return None return results
5150	def parse ( self , native ) : if not hasattr ( self , 'parser' ) or not self . parser : raise NotImplementedError ( 'Parser class not specified' ) parser = self . parser ( native ) self . intermediate_data = parser . intermediate_data del parser self . to_netjson ( )
1483	def launch ( self ) : with self . process_lock : current_commands = dict ( map ( ( lambda process : ( process . name , process . command ) ) , self . processes_to_monitor . values ( ) ) ) updated_commands = self . get_commands_to_run ( ) commands_to_kill , commands_to_keep , commands_to_start = self . get_command_changes ( current_commands , updated_commands ) Log . info ( "current commands: %s" % sorted ( current_commands . keys ( ) ) ) Log . info ( "new commands : %s" % sorted ( updated_commands . keys ( ) ) ) Log . info ( "commands_to_kill: %s" % sorted ( commands_to_kill . keys ( ) ) ) Log . info ( "commands_to_keep: %s" % sorted ( commands_to_keep . keys ( ) ) ) Log . info ( "commands_to_start: %s" % sorted ( commands_to_start . keys ( ) ) ) self . _kill_processes ( commands_to_kill ) self . _start_processes ( commands_to_start ) Log . info ( "Launch complete - processes killed=%s kept=%s started=%s monitored=%s" % ( len ( commands_to_kill ) , len ( commands_to_keep ) , len ( commands_to_start ) , len ( self . processes_to_monitor ) ) )
8498	def _format_call ( call , args ) : out = '' if args . source : out += call . annotation ( ) + '\n' if args . only_keys : out += call . get_key ( ) return out if args . view_call : out += call . as_call ( ) elif args . load_configs : out += call . as_live ( ) else : out += call . as_namespace ( ) return out
9145	def clear ( skip ) : for name in sorted ( MODULES ) : if name in skip : continue click . secho ( f'clearing cache for {name}' , fg = 'cyan' , bold = True ) clear_cache ( name )
2149	def modify ( self , pk = None , create_on_missing = False , ** kwargs ) : if pk is None and create_on_missing : try : self . get ( ** copy . deepcopy ( kwargs ) ) except exc . NotFound : return self . create ( ** kwargs ) config_item = self . _separate ( kwargs ) notification_type = kwargs . pop ( 'notification_type' , None ) debug . log ( 'Modify everything except notification type and' ' configuration' , header = 'details' ) part_result = super ( Resource , self ) . modify ( pk = pk , create_on_missing = create_on_missing , ** kwargs ) if notification_type is None or notification_type == part_result [ 'notification_type' ] : for item in part_result [ 'notification_configuration' ] : if item not in config_item or not config_item [ item ] : to_add = part_result [ 'notification_configuration' ] [ item ] if not ( to_add == '$encrypted$' and item in Resource . encrypted_fields ) : config_item [ item ] = to_add if notification_type is None : kwargs [ 'notification_type' ] = part_result [ 'notification_type' ] else : kwargs [ 'notification_type' ] = notification_type self . _configuration ( kwargs , config_item ) debug . log ( 'Modify notification type and configuration' , header = 'details' ) result = super ( Resource , self ) . modify ( pk = pk , create_on_missing = create_on_missing , ** kwargs ) if 'changed' in result and 'changed' in part_result : result [ 'changed' ] = result [ 'changed' ] or part_result [ 'changed' ] return result
7038	def get_dataset ( lcc_server , dataset_id , strformat = False , page = 1 ) : urlparams = { 'strformat' : 1 if strformat else 0 , 'page' : page , 'json' : 1 } urlqs = urlencode ( urlparams ) dataset_url = '%s/set/%s?%s' % ( lcc_server , dataset_id , urlqs ) LOGINFO ( 'retrieving dataset %s from %s, using URL: %s ...' % ( lcc_server , dataset_id , dataset_url ) ) try : have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } req = Request ( dataset_url , data = None , headers = headers ) resp = urlopen ( req ) dataset = json . loads ( resp . read ( ) ) return dataset except Exception as e : LOGEXCEPTION ( 'could not retrieve the dataset JSON!' ) return None
13012	def pprint ( arr , columns = ( 'temperature' , 'luminosity' ) , names = ( 'Temperature (Kelvin)' , 'Luminosity (solar units)' ) , max_rows = 32 , precision = 2 ) : if max_rows is True : pd . set_option ( 'display.max_rows' , 1000 ) elif type ( max_rows ) is int : pd . set_option ( 'display.max_rows' , max_rows ) pd . set_option ( 'precision' , precision ) df = pd . DataFrame ( arr . flatten ( ) , index = arr [ 'id' ] . flatten ( ) , columns = columns ) df . columns = names return df . style . format ( { names [ 0 ] : '{:.0f}' , names [ 1 ] : '{:.2f}' } )
3885	def get_user ( self , user_id ) : try : return self . _user_dict [ user_id ] except KeyError : logger . warning ( 'UserList returning unknown User for UserID %s' , user_id ) return User ( user_id , None , None , None , [ ] , False )
11164	def atime ( self ) : try : return self . _stat . st_atime except : self . _stat = self . stat ( ) return self . atime
13516	def dimension ( self , length , draught , beam , speed , slenderness_coefficient , prismatic_coefficient ) : self . length = length self . draught = draught self . beam = beam self . speed = speed self . slenderness_coefficient = slenderness_coefficient self . prismatic_coefficient = prismatic_coefficient self . displacement = ( self . length / self . slenderness_coefficient ) ** 3 self . surface_area = 1.025 * ( 1.7 * self . length * self . draught + self . displacement / self . draught )
83	def Pepper ( p = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : replacement01 = iap . ForceSign ( iap . Beta ( 0.5 , 0.5 ) - 0.5 , positive = False , mode = "invert" ) + 0.5 replacement = replacement01 * 255 if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return ReplaceElementwise ( mask = p , replacement = replacement , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
7619	def chord ( ref , est , ** kwargs ) : r namespace = 'chord' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_interval , ref_value = ref . to_interval_values ( ) est_interval , est_value = est . to_interval_values ( ) return mir_eval . chord . evaluate ( ref_interval , ref_value , est_interval , est_value , ** kwargs )
13837	def ConsumeIdentifier ( self ) : result = self . token if not self . _IDENTIFIER . match ( result ) : raise self . _ParseError ( 'Expected identifier.' ) self . NextToken ( ) return result
7057	def s3_get_file ( bucket , filename , local_file , altexts = None , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 's3' ) try : client . download_file ( bucket , filename , local_file ) return local_file except Exception as e : if altexts is not None : for alt_extension in altexts : split_ext = os . path . splitext ( filename ) check_file = split_ext [ 0 ] + alt_extension try : client . download_file ( bucket , check_file , local_file . replace ( split_ext [ - 1 ] , alt_extension ) ) return local_file . replace ( split_ext [ - 1 ] , alt_extension ) except Exception as e : pass else : LOGEXCEPTION ( 'could not download s3://%s/%s' % ( bucket , filename ) ) if raiseonfail : raise return None
8843	def unindent ( self ) : if self . tab_always_indent : cursor = self . editor . textCursor ( ) if not cursor . hasSelection ( ) : cursor . select ( cursor . LineUnderCursor ) self . unindent_selection ( cursor ) else : super ( PyIndenterMode , self ) . unindent ( )
8013	async def _create_upstream_applications ( self ) : loop = asyncio . get_event_loop ( ) for steam_name , ApplicationsCls in self . applications . items ( ) : application = ApplicationsCls ( self . scope ) upstream_queue = asyncio . Queue ( ) self . application_streams [ steam_name ] = upstream_queue self . application_futures [ steam_name ] = loop . create_task ( application ( upstream_queue . get , partial ( self . dispatch_downstream , steam_name = steam_name ) ) )
10375	def calculate_concordance_helper ( graph : BELGraph , key : str , cutoff : Optional [ float ] = None , ) -> Tuple [ int , int , int , int ] : scores = defaultdict ( int ) for u , v , k , d in graph . edges ( keys = True , data = True ) : c = edge_concords ( graph , u , v , k , d , key , cutoff = cutoff ) scores [ c ] += 1 return ( scores [ Concordance . correct ] , scores [ Concordance . incorrect ] , scores [ Concordance . ambiguous ] , scores [ Concordance . unassigned ] , )
2446	def create_package ( self , doc , name ) : if not self . package_set : self . package_set = True doc . package = package . Package ( name = name ) return True else : raise CardinalityError ( 'Package::Name' )
9617	def PlugIn ( self ) : ids = self . available_ids ( ) if len ( ids ) == 0 : raise MaxInputsReachedError ( 'Max Inputs Reached' ) self . id = ids [ 0 ] _xinput . PlugIn ( self . id ) while self . id in self . available_ids ( ) : pass
2755	def get_ssh_key ( self , ssh_key_id ) : return SSHKey . get_object ( api_token = self . token , ssh_key_id = ssh_key_id )
1847	def JZ ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . ZF , target . read ( ) , cpu . PC )
1380	def get_version_number ( zipped_pex = False ) : if zipped_pex : release_file = get_zipped_heron_release_file ( ) else : release_file = get_heron_release_file ( ) with open ( release_file ) as release_info : for line in release_info : trunks = line [ : - 1 ] . split ( ' ' ) if trunks [ 0 ] == 'heron.build.version' : return trunks [ - 1 ] . replace ( "'" , "" ) return 'unknown'
5657	def main_make_views ( gtfs_fname ) : print ( "creating views" ) conn = GTFS ( fname_or_conn = gtfs_fname ) . conn for L in Loaders : L ( None ) . make_views ( conn ) conn . commit ( )
6723	def get_or_create_ec2_key_pair ( name = None , verbose = 1 ) : verbose = int ( verbose ) name = name or env . vm_ec2_keypair_name pem_path = 'roles/%s/%s.pem' % ( env . ROLE , name ) conn = get_ec2_connection ( ) kp = conn . get_key_pair ( name ) if kp : print ( 'Key pair %s already exists.' % name ) else : kp = conn . create_key_pair ( name ) open ( pem_path , 'wb' ) . write ( kp . material ) os . system ( 'chmod 600 %s' % pem_path ) print ( 'Key pair %s created.' % name ) return pem_path
3973	def _get_ports_list ( app_name , port_specs ) : if app_name not in port_specs [ 'docker_compose' ] : return [ ] return [ "{}:{}" . format ( port_spec [ 'mapped_host_port' ] , port_spec [ 'in_container_port' ] ) for port_spec in port_specs [ 'docker_compose' ] [ app_name ] ]
7615	def get_datetime ( self , timestamp : str , unix = True ) : time = datetime . strptime ( timestamp , '%Y%m%dT%H%M%S.%fZ' ) if unix : return int ( time . timestamp ( ) ) else : return time
3757	def LFL ( Hc = None , atoms = { } , CASRN = '' , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in IEC_2010 . index and not np . isnan ( IEC_2010 . at [ CASRN , 'LFL' ] ) : methods . append ( IEC ) if CASRN in NFPA_2008 . index and not np . isnan ( NFPA_2008 . at [ CASRN , 'LFL' ] ) : methods . append ( NFPA ) if Hc : methods . append ( SUZUKI ) if atoms : methods . append ( CROWLLOUVAR ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IEC : return float ( IEC_2010 . at [ CASRN , 'LFL' ] ) elif Method == NFPA : return float ( NFPA_2008 . at [ CASRN , 'LFL' ] ) elif Method == SUZUKI : return Suzuki_LFL ( Hc = Hc ) elif Method == CROWLLOUVAR : return Crowl_Louvar_LFL ( atoms = atoms ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
7929	def _start_thread ( self ) : with self . lock : if self . threads and self . queue . empty ( ) : return if len ( self . threads ) >= self . max_threads : return thread_n = self . last_thread_n + 1 self . last_thread_n = thread_n thread = threading . Thread ( target = self . _run , name = "{0!r} #{1}" . format ( self , thread_n ) , args = ( thread_n , ) ) self . threads . append ( thread ) thread . daemon = True thread . start ( )
9423	def _load_metadata ( self , handle ) : rarinfo = self . _read_header ( handle ) while rarinfo : self . filelist . append ( rarinfo ) self . NameToInfo [ rarinfo . filename ] = rarinfo self . _process_current ( handle , constants . RAR_SKIP ) rarinfo = self . _read_header ( handle )
6168	def from_bin ( bin_array ) : width = len ( bin_array ) bin_wgts = 2 ** np . arange ( width - 1 , - 1 , - 1 ) return int ( np . dot ( bin_array , bin_wgts ) )
4022	def docker_vm_is_running ( ) : running_vms = check_output_demoted ( [ 'VBoxManage' , 'list' , 'runningvms' ] ) for line in running_vms . splitlines ( ) : if '"{}"' . format ( constants . VM_MACHINE_NAME ) in line : return True return False
12817	def _length ( self ) : self . _build_chunk_headers ( ) length = 0 if self . _data : for field in self . _data : length += len ( self . _chunk_headers [ field ] ) length += len ( self . _data [ field ] ) length += 2 if self . _files : for field in self . _files : length += len ( self . _chunk_headers [ field ] ) length += self . _file_size ( field ) length += 2 length += len ( self . boundary ) length += 6 return length
2379	def _get_rules ( self , cls ) : result = [ ] for rule_class in cls . __subclasses__ ( ) : rule_name = rule_class . __name__ . lower ( ) if rule_name not in self . _rules : rule = rule_class ( self ) self . _rules [ rule_name ] = rule result . append ( self . _rules [ rule_name ] ) return result
6576	def populate_fields ( api_client , instance , data ) : for key , value in instance . __class__ . _fields . items ( ) : default = getattr ( value , "default" , None ) newval = data . get ( value . field , default ) if isinstance ( value , SyntheticField ) : newval = value . formatter ( api_client , data , newval ) setattr ( instance , key , newval ) continue model_class = getattr ( value , "model" , None ) if newval and model_class : if isinstance ( newval , list ) : newval = model_class . from_json_list ( api_client , newval ) else : newval = model_class . from_json ( api_client , newval ) if newval and value . formatter : newval = value . formatter ( api_client , newval ) setattr ( instance , key , newval )
8849	def setup_actions ( self ) : self . actionOpen . triggered . connect ( self . on_open ) self . actionNew . triggered . connect ( self . on_new ) self . actionSave . triggered . connect ( self . on_save ) self . actionSave_as . triggered . connect ( self . on_save_as ) self . actionQuit . triggered . connect ( QtWidgets . QApplication . instance ( ) . quit ) self . tabWidget . current_changed . connect ( self . on_current_tab_changed ) self . tabWidget . last_tab_closed . connect ( self . on_last_tab_closed ) self . actionAbout . triggered . connect ( self . on_about ) self . actionRun . triggered . connect ( self . on_run ) self . interactiveConsole . process_finished . connect ( self . on_process_finished ) self . actionConfigure_run . triggered . connect ( self . on_configure_run )
403	def ramp ( x , v_min = 0 , v_max = 1 , name = None ) : return tf . clip_by_value ( x , clip_value_min = v_min , clip_value_max = v_max , name = name )
3692	def a_alpha_and_derivatives ( self , T , full = True , quick = True ) : r return TWU_a_alpha_common ( T , self . Tc , self . omega , self . a , full = full , quick = quick , method = 'SRK' )
4380	def deny ( self , role , method , resource , with_children = False ) : if with_children : for r in role . get_children ( ) : permission = ( r . get_name ( ) , method , resource ) if permission not in self . _denied : self . _denied . append ( permission ) permission = ( role . get_name ( ) , method , resource ) if permission not in self . _denied : self . _denied . append ( permission )
2427	def set_doc_comment ( self , doc , comment ) : if not self . doc_comment_set : self . doc_comment_set = True if validations . validate_doc_comment ( comment ) : doc . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'Document::Comment' ) else : raise CardinalityError ( 'Document::Comment' )
11200	def fromutc ( self , dt ) : if not isinstance ( dt , datetime ) : raise TypeError ( "fromutc() requires a datetime argument" ) if dt . tzinfo is not self : raise ValueError ( "dt.tzinfo is not self" ) transitions = self . transitions ( dt . year ) if transitions is None : return dt + self . utcoffset ( dt ) dston , dstoff = transitions dston -= self . _std_offset dstoff -= self . _std_offset utc_transitions = ( dston , dstoff ) dt_utc = dt . replace ( tzinfo = None ) isdst = self . _naive_isdst ( dt_utc , utc_transitions ) if isdst : dt_wall = dt + self . _dst_offset else : dt_wall = dt + self . _std_offset _fold = int ( not isdst and self . is_ambiguous ( dt_wall ) ) return enfold ( dt_wall , fold = _fold )
6137	def add_model_file ( self , model_fpath , position = 1 , file_id = None ) : if file_id is None : file_id = self . make_unique_id ( 'file_input' ) ret_data = self . file_create ( File . from_file ( model_fpath , position , file_id ) ) return ret_data
2568	def construct_start_message ( self ) : uname = getpass . getuser ( ) . encode ( 'latin1' ) hashed_username = hashlib . sha256 ( uname ) . hexdigest ( ) [ 0 : 10 ] hname = socket . gethostname ( ) . encode ( 'latin1' ) hashed_hostname = hashlib . sha256 ( hname ) . hexdigest ( ) [ 0 : 10 ] message = { 'uuid' : self . uuid , 'uname' : hashed_username , 'hname' : hashed_hostname , 'test' : self . test_mode , 'parsl_v' : self . parsl_version , 'python_v' : self . python_version , 'os' : platform . system ( ) , 'os_v' : platform . release ( ) , 'start' : time . time ( ) } return json . dumps ( message )
10820	def _filter ( cls , query , state = MembershipState . ACTIVE , eager = None ) : query = query . filter_by ( state = state ) eager = eager or [ ] for field in eager : query = query . options ( joinedload ( field ) ) return query
7098	def destroy ( self ) : marker = self . marker parent = self . parent ( ) if marker : if parent : del parent . markers [ marker . __id__ ] marker . remove ( ) super ( AndroidMapItemBase , self ) . destroy ( )
12026	def abfIDfromFname ( fname ) : fname = os . path . abspath ( fname ) basename = os . path . basename ( fname ) return os . path . splitext ( basename ) [ 0 ]
5272	def _find_lcs ( self , node , stringIdxs ) : nodes = [ self . _find_lcs ( n , stringIdxs ) for ( n , _ ) in node . transition_links if n . generalized_idxs . issuperset ( stringIdxs ) ] if nodes == [ ] : return node deepestNode = max ( nodes , key = lambda n : n . depth ) return deepestNode
10104	def execute ( self , timeout = None ) : logger . debug ( ' > Batch API request (length %s)' % len ( self . _commands ) ) auth = self . _build_http_auth ( ) headers = self . _build_request_headers ( ) logger . debug ( '\tbatch headers: %s' % headers ) logger . debug ( '\tbatch command length: %s' % len ( self . _commands ) ) path = self . _build_request_path ( self . BATCH_ENDPOINT ) data = json . dumps ( self . _commands , cls = self . _json_encoder ) r = requests . post ( path , auth = auth , headers = headers , data = data , timeout = ( self . DEFAULT_TIMEOUT if timeout is None else timeout ) ) self . _commands = [ ] logger . debug ( '\tresponse code:%s' % r . status_code ) try : logger . debug ( '\tresponse: %s' % r . json ( ) ) except : logger . debug ( '\tresponse: %s' % r . content ) return r
3338	def is_child_uri ( parentUri , childUri ) : return ( parentUri and childUri and childUri . rstrip ( "/" ) . startswith ( parentUri . rstrip ( "/" ) + "/" ) )
6382	def sim_hamming ( src , tar , diff_lens = True ) : return Hamming ( ) . sim ( src , tar , diff_lens )
10484	def _matchOther ( self , obj , ** kwargs ) : if obj is not None : if self . _findFirstR ( ** kwargs ) : return obj . _match ( ** kwargs ) return False
4946	def send_course_enrollment_statement ( lrs_configuration , course_enrollment ) : user_details = LearnerInfoSerializer ( course_enrollment . user ) course_details = CourseInfoSerializer ( course_enrollment . course ) statement = LearnerCourseEnrollmentStatement ( course_enrollment . user , course_enrollment . course , user_details . data , course_details . data , ) EnterpriseXAPIClient ( lrs_configuration ) . save_statement ( statement )
3206	def get ( self , batch_id , ** queryparams ) : self . batch_id = batch_id self . operation_status = None return self . _mc_client . _get ( url = self . _build_path ( batch_id ) , ** queryparams )
10951	def update ( self , params , values ) : return super ( State , self ) . update ( params , values )
6164	def rc_imp ( Ns , alpha , M = 6 ) : n = np . arange ( - M * Ns , M * Ns + 1 ) b = np . zeros ( len ( n ) ) a = alpha Ns *= 1.0 for i in range ( len ( n ) ) : if ( 1 - 4 * ( a * n [ i ] / Ns ) ** 2 ) == 0 : b [ i ] = np . pi / 4 * np . sinc ( 1 / ( 2. * a ) ) else : b [ i ] = np . sinc ( n [ i ] / Ns ) * np . cos ( np . pi * a * n [ i ] / Ns ) / ( 1 - 4 * ( a * n [ i ] / Ns ) ** 2 ) return b
6120	def elliptical ( cls , shape , pixel_scale , major_axis_radius_arcsec , axis_ratio , phi , centre = ( 0. , 0. ) , invert = False ) : mask = mask_util . mask_elliptical_from_shape_pixel_scale_and_radius ( shape , pixel_scale , major_axis_radius_arcsec , axis_ratio , phi , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
1639	def CheckParenthesisSpacing ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] match = Search ( r' (if\(|for\(|while\(|switch\()' , line ) if match : error ( filename , linenum , 'whitespace/parens' , 5 , 'Missing space before ( in %s' % match . group ( 1 ) ) match = Search ( r'\b(if|for|while|switch)\s*' r'\(([ ]*)(.).*[^ ]+([ ]*)\)\s*{\s*$' , line ) if match : if len ( match . group ( 2 ) ) != len ( match . group ( 4 ) ) : if not ( match . group ( 3 ) == ';' and len ( match . group ( 2 ) ) == 1 + len ( match . group ( 4 ) ) or not match . group ( 2 ) and Search ( r'\bfor\s*\(.*; \)' , line ) ) : error ( filename , linenum , 'whitespace/parens' , 5 , 'Mismatching spaces inside () in %s' % match . group ( 1 ) ) if len ( match . group ( 2 ) ) not in [ 0 , 1 ] : error ( filename , linenum , 'whitespace/parens' , 5 , 'Should have zero or one spaces inside ( and ) in %s' % match . group ( 1 ) )
7061	def sqs_delete_queue ( queue_url , client = None ) : if not client : client = boto3 . client ( 'sqs' ) try : client . delete_queue ( QueueUrl = queue_url ) return True except Exception as e : LOGEXCEPTION ( 'could not delete the specified queue: %s' % ( queue_url , ) ) return False
2360	def t_stringdollar_rbrace ( self , t ) : r'\}' t . lexer . braces -= 1 if t . lexer . braces == 0 : t . lexer . begin ( 'string' )
5689	def get_day_start_ut_span ( self ) : cur = self . conn . cursor ( ) first_day_start_ut , last_day_start_ut = cur . execute ( "SELECT min(day_start_ut), max(day_start_ut) FROM days;" ) . fetchone ( ) return first_day_start_ut , last_day_start_ut
16	def value ( self , t ) : for ( l_t , l ) , ( r_t , r ) in zip ( self . _endpoints [ : - 1 ] , self . _endpoints [ 1 : ] ) : if l_t <= t and t < r_t : alpha = float ( t - l_t ) / ( r_t - l_t ) return self . _interpolation ( l , r , alpha ) assert self . _outside_value is not None return self . _outside_value
3527	def piwik ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return PiwikNode ( )
7542	def storealleles ( consens , hidx , alleles ) : bigbase = PRIORITY [ consens [ hidx [ 0 ] ] ] bigallele = [ i for i in alleles if i [ 0 ] == bigbase ] [ 0 ] for hsite , pbase in zip ( hidx [ 1 : ] , bigallele [ 1 : ] ) : if PRIORITY [ consens [ hsite ] ] != pbase : consens [ hsite ] = consens [ hsite ] . lower ( ) return consens
9244	def set_date_from_event ( self , event , issue ) : if not event . get ( 'commit_id' , None ) : issue [ 'actual_date' ] = timestring_to_datetime ( issue [ 'closed_at' ] ) return try : commit = self . fetcher . fetch_commit ( event ) issue [ 'actual_date' ] = timestring_to_datetime ( commit [ 'author' ] [ 'date' ] ) except ValueError : print ( "WARNING: Can't fetch commit {0}. " "It is probably referenced from another repo." . format ( event [ 'commit_id' ] ) ) issue [ 'actual_date' ] = timestring_to_datetime ( issue [ 'closed_at' ] )
9513	def orfs ( self , frame = 0 , revcomp = False ) : assert frame in [ 0 , 1 , 2 ] if revcomp : self . revcomp ( ) aa_seq = self . translate ( frame = frame ) . seq . rstrip ( 'X' ) if revcomp : self . revcomp ( ) orfs = _orfs_from_aa_seq ( aa_seq ) for i in range ( len ( orfs ) ) : if revcomp : start = len ( self ) - ( orfs [ i ] . end * 3 + 3 ) - frame end = len ( self ) - ( orfs [ i ] . start * 3 ) - 1 - frame else : start = orfs [ i ] . start * 3 + frame end = orfs [ i ] . end * 3 + 2 + frame orfs [ i ] = intervals . Interval ( start , end ) return orfs
13253	async def download_metadata_yaml ( session , github_url ) : metadata_yaml_url = _build_metadata_yaml_url ( github_url ) async with session . get ( metadata_yaml_url ) as response : response . raise_for_status ( ) yaml_data = await response . text ( ) return yaml . safe_load ( yaml_data )
79	def Dropout ( p = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : if ia . is_single_number ( p ) : p2 = iap . Binomial ( 1 - p ) elif ia . is_iterable ( p ) : ia . do_assert ( len ( p ) == 2 ) ia . do_assert ( p [ 0 ] < p [ 1 ] ) ia . do_assert ( 0 <= p [ 0 ] <= 1.0 ) ia . do_assert ( 0 <= p [ 1 ] <= 1.0 ) p2 = iap . Binomial ( iap . Uniform ( 1 - p [ 1 ] , 1 - p [ 0 ] ) ) elif isinstance ( p , iap . StochasticParameter ) : p2 = p else : raise Exception ( "Expected p to be float or int or StochasticParameter, got %s." % ( type ( p ) , ) ) if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return MultiplyElementwise ( p2 , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
10881	def delistify ( a , b = None ) : if isinstance ( b , ( tuple , list , np . ndarray ) ) : if isinstance ( a , ( tuple , list , np . ndarray ) ) : return type ( b ) ( a ) return type ( b ) ( [ a ] ) else : if isinstance ( a , ( tuple , list , np . ndarray ) ) and len ( a ) == 1 : return a [ 0 ] return a return a
8454	def _cookiecutter_configs_have_changed ( template , old_version , new_version ) : temple . check . is_git_ssh_path ( template ) repo_path = temple . utils . get_repo_path ( template ) github_client = temple . utils . GithubClient ( ) api = '/repos/{}/contents/cookiecutter.json' . format ( repo_path ) old_config_resp = github_client . get ( api , params = { 'ref' : old_version } ) old_config_resp . raise_for_status ( ) new_config_resp = github_client . get ( api , params = { 'ref' : new_version } ) new_config_resp . raise_for_status ( ) return old_config_resp . json ( ) [ 'content' ] != new_config_resp . json ( ) [ 'content' ]
7407	def populate ( publications ) : customlinks = CustomLink . objects . filter ( publication__in = publications ) customfiles = CustomFile . objects . filter ( publication__in = publications ) publications_ = { } for publication in publications : publication . links = [ ] publication . files = [ ] publications_ [ publication . id ] = publication for link in customlinks : publications_ [ link . publication_id ] . links . append ( link ) for file in customfiles : publications_ [ file . publication_id ] . files . append ( file )
700	def getOrphanParticleInfos ( self , swarmId , genIdx ) : entryIdxs = range ( len ( self . _allResults ) ) if len ( entryIdxs ) == 0 : return ( [ ] , [ ] , [ ] , [ ] , [ ] ) particleStates = [ ] modelIds = [ ] errScores = [ ] completedFlags = [ ] maturedFlags = [ ] for idx in entryIdxs : entry = self . _allResults [ idx ] if not entry [ 'hidden' ] : continue modelParams = entry [ 'modelParams' ] if modelParams [ 'particleState' ] [ 'swarmId' ] != swarmId : continue isCompleted = entry [ 'completed' ] isMatured = entry [ 'matured' ] particleState = modelParams [ 'particleState' ] particleGenIdx = particleState [ 'genIdx' ] particleId = particleState [ 'id' ] if genIdx is not None and particleGenIdx != genIdx : continue particleStates . append ( particleState ) modelIds . append ( entry [ 'modelID' ] ) errScores . append ( entry [ 'errScore' ] ) completedFlags . append ( isCompleted ) maturedFlags . append ( isMatured ) return ( particleStates , modelIds , errScores , completedFlags , maturedFlags )
6958	def list_trilegal_filtersystems ( ) : print ( '%-40s %s' % ( 'FILTER SYSTEM NAME' , 'DESCRIPTION' ) ) print ( '%-40s %s' % ( '------------------' , '-----------' ) ) for key in sorted ( TRILEGAL_FILTER_SYSTEMS . keys ( ) ) : print ( '%-40s %s' % ( key , TRILEGAL_FILTER_SYSTEMS [ key ] [ 'desc' ] ) )
8740	def _create_flip ( context , flip , port_fixed_ips ) : if port_fixed_ips : context . session . begin ( ) try : ports = [ val [ 'port' ] for val in port_fixed_ips . values ( ) ] flip = db_api . port_associate_ip ( context , ports , flip , port_fixed_ips . keys ( ) ) for port_id in port_fixed_ips : fixed_ip = port_fixed_ips [ port_id ] [ 'fixed_ip' ] flip = db_api . floating_ip_associate_fixed_ip ( context , flip , fixed_ip ) flip_driver = registry . DRIVER_REGISTRY . get_driver ( ) flip_driver . register_floating_ip ( flip , port_fixed_ips ) context . session . commit ( ) except Exception : context . session . rollback ( ) raise billing . notify ( context , billing . IP_ASSOC , flip )
7486	def run_cutadapt ( data , subsamples , lbview ) : start = time . time ( ) printstr = " processing reads | {} | s2 |" finished = 0 rawedits = { } subsamples . sort ( key = lambda x : x . stats . reads_raw , reverse = True ) LOGGER . info ( [ i . stats . reads_raw for i in subsamples ] ) if "pair" in data . paramsdict [ "datatype" ] : for sample in subsamples : rawedits [ sample . name ] = lbview . apply ( cutadaptit_pairs , * ( data , sample ) ) else : for sample in subsamples : rawedits [ sample . name ] = lbview . apply ( cutadaptit_single , * ( data , sample ) ) while 1 : finished = sum ( [ i . ready ( ) for i in rawedits . values ( ) ] ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( rawedits ) , finished , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if finished == len ( rawedits ) : print ( "" ) break for async in rawedits : if rawedits [ async ] . successful ( ) : res = rawedits [ async ] . result ( ) if "pair" not in data . paramsdict [ "datatype" ] : parse_single_results ( data , data . samples [ async ] , res ) else : parse_pair_results ( data , data . samples [ async ] , res ) else : print ( " found an error in step2; see ipyrad_log.txt" ) LOGGER . error ( "error in run_cutadapt(): %s" , rawedits [ async ] . exception ( ) )
9276	def apply_exclude_tags ( self , all_tags ) : filtered = copy . deepcopy ( all_tags ) for tag in all_tags : if tag [ "name" ] not in self . options . exclude_tags : self . warn_if_tag_not_found ( tag , "exclude-tags" ) else : filtered . remove ( tag ) return filtered
12722	def velocities ( self , velocities ) : _set_params ( self . ode_obj , 'Vel' , velocities , self . ADOF + self . LDOF )
2061	def _declare ( self , var ) : if var . name in self . _declarations : raise ValueError ( 'Variable already declared' ) self . _declarations [ var . name ] = var return var
11696	def verify_editor ( self ) : powerful_editors = [ 'josm' , 'level0' , 'merkaartor' , 'qgis' , 'arcgis' , 'upload.py' , 'osmapi' , 'Services_OpenStreetMap' ] if self . editor is not None : for editor in powerful_editors : if editor in self . editor . lower ( ) : self . powerfull_editor = True break if 'iD' in self . editor : trusted_hosts = [ 'www.openstreetmap.org/id' , 'www.openstreetmap.org/edit' , 'improveosm.org' , 'strava.github.io/iD' , 'preview.ideditor.com/release' , 'preview.ideditor.com/master' , 'hey.mapbox.com/iD-internal' , 'projets.pavie.info/id-indoor' , 'maps.mapcat.com/edit' , 'id.softek.ir' ] if self . host . split ( '://' ) [ - 1 ] . strip ( '/' ) not in trusted_hosts : self . label_suspicious ( 'Unknown iD instance' ) else : self . powerfull_editor = True self . label_suspicious ( 'Software editor was not declared' )
11672	def fit ( self , X , y = None , get_rhos = False ) : self . features_ = X = as_features ( X , stack = True , bare = True ) Ks = self . _get_Ks ( ) _ , _ , _ , max_K , save_all_Ks , _ = _choose_funcs ( self . div_funcs , Ks , X . dim , X . n_pts , None , self . version ) if max_K >= X . n_pts . min ( ) : msg = "asked for K = {}, but there's a bag with only {} points" raise ValueError ( msg . format ( max_K , X . n_pts . min ( ) ) ) memory = self . memory if isinstance ( memory , string_types ) : memory = Memory ( cachedir = memory , verbose = 0 ) self . indices_ = id = memory . cache ( _build_indices ) ( X , self . _flann_args ( ) ) if get_rhos : self . rhos_ = _get_rhos ( X , id , Ks , max_K , save_all_Ks , self . min_dist ) elif hasattr ( self , 'rhos_' ) : del self . rhos_ return self
6669	def task_or_dryrun ( * args , ** kwargs ) : invoked = bool ( not args or kwargs ) task_class = kwargs . pop ( "task_class" , WrappedCallableTask ) func , args = args [ 0 ] , ( ) def wrapper ( func ) : return task_class ( func , * args , ** kwargs ) wrapper . is_task_or_dryrun = True wrapper . wrapped = func return wrapper if invoked else wrapper ( func )
9644	def _flatten ( iterable ) : for i in iterable : if isinstance ( i , Iterable ) and not isinstance ( i , string_types ) : for sub_i in _flatten ( i ) : yield sub_i else : yield i
12791	def delete ( self , url = None , post_data = { } , parse_data = False , key = None , parameters = None ) : return self . _fetch ( "DELETE" , url , post_data = post_data , parse_data = parse_data , key = key , parameters = parameters , full_return = True )
1161	def acquire ( self , blocking = 1 ) : rc = False with self . __cond : while self . __value == 0 : if not blocking : break if __debug__ : self . _note ( "%s.acquire(%s): blocked waiting, value=%s" , self , blocking , self . __value ) self . __cond . wait ( ) else : self . __value = self . __value - 1 if __debug__ : self . _note ( "%s.acquire: success, value=%s" , self , self . __value ) rc = True return rc
511	def _updateMinDutyCyclesGlobal ( self ) : self . _minOverlapDutyCycles . fill ( self . _minPctOverlapDutyCycles * self . _overlapDutyCycles . max ( ) )
459	def predict ( sess , network , X , x , y_op , batch_size = None ) : if batch_size is None : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X , } feed_dict . update ( dp_dict ) return sess . run ( y_op , feed_dict = feed_dict ) else : result = None for X_a , _ in tl . iterate . minibatches ( X , X , batch_size , shuffle = False ) : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X_a , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) if result is None : result = result_a else : result = np . concatenate ( ( result , result_a ) ) if result is None : if len ( X ) % batch_size != 0 : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X [ - ( len ( X ) % batch_size ) : , : ] , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) result = result_a else : if len ( X ) != len ( result ) and len ( X ) % batch_size != 0 : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X [ - ( len ( X ) % batch_size ) : , : ] , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) result = np . concatenate ( ( result , result_a ) ) return result
9345	def call ( self , args , axis = 0 , out = None , chunksize = 1024 * 1024 , ** kwargs ) : if self . altreduce is not None : ret = [ None ] else : if out is None : if self . outdtype is not None : dtype = self . outdtype else : try : dtype = numpy . result_type ( * [ args [ i ] for i in self . ins ] * 2 ) except : dtype = None out = sharedmem . empty ( numpy . broadcast ( * [ args [ i ] for i in self . ins ] * 2 ) . shape , dtype = dtype ) if axis != 0 : for i in self . ins : args [ i ] = numpy . rollaxis ( args [ i ] , axis ) out = numpy . rollaxis ( out , axis ) size = numpy . max ( [ len ( args [ i ] ) for i in self . ins ] ) with sharedmem . MapReduce ( ) as pool : def work ( i ) : sl = slice ( i , i + chunksize ) myargs = args [ : ] for j in self . ins : try : tmp = myargs [ j ] [ sl ] a , b , c = sl . indices ( len ( args [ j ] ) ) myargs [ j ] = tmp except Exception as e : print tmp print j , e pass if b == a : return None rt = self . ufunc ( * myargs , ** kwargs ) if self . altreduce is not None : return rt else : out [ sl ] = rt def reduce ( rt ) : if self . altreduce is None : return if ret [ 0 ] is None : ret [ 0 ] = rt elif rt is not None : ret [ 0 ] = self . altreduce ( ret [ 0 ] , rt ) pool . map ( work , range ( 0 , size , chunksize ) , reduce = reduce ) if self . altreduce is None : if axis != 0 : out = numpy . rollaxis ( out , 0 , axis + 1 ) return out else : return ret [ 0 ]
11184	def wrap_state_dict ( self , typename : str , state ) -> Dict [ str , Any ] : return { self . type_key : typename , self . state_key : state }
6726	def delete ( name = None , group = None , release = None , except_release = None , dryrun = 1 , verbose = 1 ) : verbose = int ( verbose ) if env . vm_type == EC2 : conn = get_ec2_connection ( ) instances = list_instances ( name = name , group = group , release = release , except_release = except_release , ) for instance_name , instance_data in instances . items ( ) : public_dns_name = instance_data [ 'public_dns_name' ] print ( '\nDeleting %s (%s)...' % ( instance_name , instance_data [ 'id' ] ) ) if not get_dryrun ( ) : conn . terminate_instances ( instance_ids = [ instance_data [ 'id' ] ] ) known_hosts = os . path . expanduser ( '~/.ssh/known_hosts' ) cmd = 'ssh-keygen -f "%s" -R %s' % ( known_hosts , public_dns_name ) local_or_dryrun ( cmd ) else : raise NotImplementedError
5550	def get_zoom_levels ( process_zoom_levels = None , init_zoom_levels = None ) : process_zoom_levels = _validate_zooms ( process_zoom_levels ) if init_zoom_levels is None : return process_zoom_levels else : init_zoom_levels = _validate_zooms ( init_zoom_levels ) if not set ( init_zoom_levels ) . issubset ( set ( process_zoom_levels ) ) : raise MapcheteConfigError ( "init zooms must be a subset of process zoom" ) return init_zoom_levels
7647	def scaper_to_tag ( annotation ) : annotation . namespace = 'tag_open' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = obs . value [ 'label' ] ) return annotation
8569	def remove_loadbalanced_nic ( self , datacenter_id , loadbalancer_id , nic_id ) : response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s/balancednics/%s' % ( datacenter_id , loadbalancer_id , nic_id ) , method = 'DELETE' ) return response
6045	def padded_blurred_image_2d_from_padded_image_1d_and_psf ( self , padded_image_1d , psf ) : padded_model_image_1d = self . convolve_array_1d_with_psf ( padded_array_1d = padded_image_1d , psf = psf ) return self . scaled_array_2d_from_array_1d ( array_1d = padded_model_image_1d )
13769	def get_minifier ( self ) : if self . minifier is None : if not self . has_bundles ( ) : raise Exception ( "Unable to get default minifier, no bundles in build group" ) minifier = self . get_first_bundle ( ) . get_default_minifier ( ) else : minifier = self . minifier if minifier : minifier . init_asset ( self ) return minifier
12892	def handle_long ( self , item ) : doc = yield from self . handle_get ( item ) if doc is None : return None return int ( doc . value . u32 . text ) or None
13439	def lock_file ( filename ) : lockfile = "%s.lock" % filename if isfile ( lockfile ) : return False else : with open ( lockfile , "w" ) : pass return True
12808	def fetch ( self ) : try : if not self . _last_message_id : messages = self . _connection . get ( "room/%s/recent" % self . _room_id , key = "messages" , parameters = { "limit" : 1 } ) self . _last_message_id = messages [ - 1 ] [ "id" ] messages = self . _connection . get ( "room/%s/recent" % self . _room_id , key = "messages" , parameters = { "since_message_id" : self . _last_message_id } ) except : messages = [ ] if messages : self . _last_message_id = messages [ - 1 ] [ "id" ] self . received ( messages )
5778	def _bcrypt_encrypt ( certificate_or_public_key , data , rsa_oaep_padding = False ) : flags = BcryptConst . BCRYPT_PAD_PKCS1 if rsa_oaep_padding is True : flags = BcryptConst . BCRYPT_PAD_OAEP padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_OAEP_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) hash_buffer = buffer_from_unicode ( BcryptConst . BCRYPT_SHA1_ALGORITHM ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info_struct . pbLabel = null ( ) padding_info_struct . cbLabel = 0 padding_info = cast ( bcrypt , 'void *' , padding_info_struct_pointer ) else : padding_info = null ( ) out_len = new ( bcrypt , 'ULONG *' ) res = bcrypt . BCryptEncrypt ( certificate_or_public_key . key_handle , data , len ( data ) , padding_info , null ( ) , 0 , null ( ) , 0 , out_len , flags ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) res = bcrypt . BCryptEncrypt ( certificate_or_public_key . key_handle , data , len ( data ) , padding_info , null ( ) , 0 , buffer , buffer_len , out_len , flags ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) )
9296	def get_database ( self , model ) : for router in self . routers : r = router . get_database ( model ) if r is not None : return r return self . get ( 'default' )
783	def jobGetDemand ( self , ) : rows = self . _getMatchingRowsWithRetries ( self . _jobs , dict ( status = self . STATUS_RUNNING ) , [ self . _jobs . pubToDBNameDict [ f ] for f in self . _jobs . jobDemandNamedTuple . _fields ] ) return [ self . _jobs . jobDemandNamedTuple . _make ( r ) for r in rows ]
7483	def parse_single_results ( data , sample , res1 ) : sample . stats_dfs . s2 [ "trim_adapter_bp_read1" ] = 0 sample . stats_dfs . s2 [ "trim_quality_bp_read1" ] = 0 sample . stats_dfs . s2 [ "reads_filtered_by_Ns" ] = 0 sample . stats_dfs . s2 [ "reads_filtered_by_minlen" ] = 0 sample . stats_dfs . s2 [ "reads_passed_filter" ] = 0 lines = res1 . strip ( ) . split ( "\n" ) for line in lines : if "Total reads processed:" in line : value = int ( line . split ( ) [ 3 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_raw" ] = value if "Reads with adapters:" in line : value = int ( line . split ( ) [ 3 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "trim_adapter_bp_read1" ] = value if "Quality-trimmed" in line : value = int ( line . split ( ) [ 1 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "trim_quality_bp_read1" ] = value if "Reads that were too short" in line : value = int ( line . split ( ) [ 5 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_filtered_by_minlen" ] = value if "Reads with too many N" in line : value = int ( line . split ( ) [ 5 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_filtered_by_Ns" ] = value if "Reads written (passing filters):" in line : value = int ( line . split ( ) [ 4 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_passed_filter" ] = value if sample . stats_dfs . s2 . reads_passed_filter : sample . stats . state = 2 sample . stats . reads_passed_filter = sample . stats_dfs . s2 . reads_passed_filter sample . files . edits = [ ( OPJ ( data . dirs . edits , sample . name + ".trimmed_R1_.fastq.gz" ) , 0 ) ] LOGGER . info ( res1 ) else : print ( "{}No reads passed filtering in Sample: {}" . format ( data . _spacer , sample . name ) )
10843	def pending ( self ) : pending_updates = [ ] url = PATHS [ 'GET_PENDING' ] % self . profile_id response = self . api . get ( url = url ) for update in response [ 'updates' ] : pending_updates . append ( Update ( api = self . api , raw_response = update ) ) self . __pending = pending_updates return self . __pending
11866	def sum_out ( self , var , bn ) : "Make a factor eliminating var by summing over its values." vars = [ X for X in self . vars if X != var ] cpt = dict ( ( event_values ( e , vars ) , sum ( self . p ( extend ( e , var , val ) ) for val in bn . variable_values ( var ) ) ) for e in all_events ( vars , bn , { } ) ) return Factor ( vars , cpt )
7958	def handle_read ( self ) : with self . lock : logger . debug ( "handle_read()" ) if self . _eof or self . _socket is None : return if self . _state == "tls-handshake" : while True : logger . debug ( "tls handshake read..." ) self . _continue_tls_handshake ( ) logger . debug ( " state: {0}" . format ( self . _tls_state ) ) if self . _tls_state != "want_read" : break elif self . _tls_state == "connected" : while self . _socket and not self . _eof : logger . debug ( "tls socket read..." ) try : data = self . _socket . read ( 4096 ) except ssl . SSLError , err : if err . args [ 0 ] == ssl . SSL_ERROR_WANT_READ : break elif err . args [ 0 ] == ssl . SSL_ERROR_WANT_WRITE : break else : raise except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue elif err . args [ 0 ] in BLOCKING_ERRORS : break elif err . args [ 0 ] == errno . ECONNRESET : logger . warning ( "Connection reset by peer" ) data = None else : raise self . _feed_reader ( data ) else : while self . _socket and not self . _eof : logger . debug ( "raw socket read..." ) try : data = self . _socket . recv ( 4096 ) except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue elif err . args [ 0 ] in BLOCKING_ERRORS : break elif err . args [ 0 ] == errno . ECONNRESET : logger . warning ( "Connection reset by peer" ) data = None else : raise self . _feed_reader ( data )
11876	def getch ( ) : try : termios . tcsetattr ( _fd , termios . TCSANOW , _new_settings ) ch = sys . stdin . read ( 1 ) finally : termios . tcsetattr ( _fd , termios . TCSADRAIN , _old_settings ) return ch
6186	def get_last_commit_line ( git_path = None ) : if git_path is None : git_path = GIT_PATH output = check_output ( [ git_path , "log" , "--pretty=format:'%ad %h %s'" , "--date=short" , "-n1" ] ) return output . strip ( ) [ 1 : - 1 ]
4384	def allow ( self , roles , methods , with_children = True ) : def decorator ( view_func ) : _methods = [ m . upper ( ) for m in methods ] for r , m , v in itertools . product ( roles , _methods , [ view_func . __name__ ] ) : self . before_acl [ 'allow' ] . append ( ( r , m , v , with_children ) ) return view_func return decorator
2125	def disassociate_always_node ( self , parent , child ) : return self . _disassoc ( self . _forward_rel_name ( 'always' ) , parent , child )
13005	def bruteforce ( users , domain , password , host ) : cs = CredentialSearch ( use_pipe = False ) print_notification ( "Connecting to {}" . format ( host ) ) s = Server ( host ) c = Connection ( s ) for user in users : if c . rebind ( user = "{}\\{}" . format ( domain , user . username ) , password = password , authentication = NTLM ) : print_success ( 'Success for: {}:{}' . format ( user . username , password ) ) credential = cs . find_object ( user . username , password , domain = domain , host_ip = host ) if not credential : credential = Credential ( username = user . username , secret = password , domain = domain , host_ip = host , type = "plaintext" , port = 389 ) credential . add_tag ( tag ) credential . save ( ) user . add_tag ( tag ) user . save ( ) else : print_error ( "Fail for: {}:{}" . format ( user . username , password ) )
3908	def put ( self , coro ) : assert asyncio . iscoroutine ( coro ) self . _queue . put_nowait ( coro )
11541	def write ( self , pin , value , pwm = False ) : if type ( pin ) is list : for p in pin : self . write ( p , value , pwm ) return if pwm and type ( value ) is not int and type ( value ) is not float : raise TypeError ( 'pwm is set, but value is not a float or int' ) pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : lpin = self . _pin_lin . get ( pin , None ) if lpin and type ( lpin [ 'write' ] ) is tuple : write_range = lpin [ 'write' ] value = self . _linear_interpolation ( value , * write_range ) self . _write ( pin_id , value , pwm ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
13471	def apply_changesets ( args , changesets , catalog ) : tmpdir = tempfile . mkdtemp ( ) tmp_patch = join ( tmpdir , "tmp.patch" ) tmp_lcat = join ( tmpdir , "tmp.lcat" ) for node in changesets : remove ( tmp_patch ) copy ( node . mfile [ 'changeset' ] [ 'filename' ] , tmp_patch ) logging . info ( "mv %s %s" % ( catalog , tmp_lcat ) ) shutil . move ( catalog , tmp_lcat ) cmd = args . patch_cmd . replace ( "$in1" , tmp_lcat ) . replace ( "$patch" , tmp_patch ) . replace ( "$out" , catalog ) logging . info ( "Patch: %s" % cmd ) subprocess . check_call ( cmd , shell = True ) shutil . rmtree ( tmpdir , ignore_errors = True )
343	def validation_metrics ( self ) : if ( self . _validation_iterator is None ) or ( self . _validation_metrics is None ) : raise AttributeError ( 'Validation is not setup.' ) n = 0.0 metric_sums = [ 0.0 ] * len ( self . _validation_metrics ) self . _sess . run ( self . _validation_iterator . initializer ) while True : try : metrics = self . _sess . run ( self . _validation_metrics ) for i , m in enumerate ( metrics ) : metric_sums [ i ] += m n += 1.0 except tf . errors . OutOfRangeError : break for i , m in enumerate ( metric_sums ) : metric_sums [ i ] = metric_sums [ i ] / n return zip ( self . _validation_metrics , metric_sums )
11146	def get_file_info ( self , relativePath ) : relativePath = self . to_repo_relative_path ( path = relativePath , split = False ) fileName = os . path . basename ( relativePath ) isRepoFile , fileOnDisk , infoOnDisk , classOnDisk = self . is_repository_file ( relativePath ) if not isRepoFile : return None , "file is not a registered repository file." if not infoOnDisk : return None , "file is a registered repository file but info file missing" fileInfoPath = os . path . join ( self . __path , os . path . dirname ( relativePath ) , self . __fileInfo % fileName ) try : with open ( fileInfoPath , 'rb' ) as fd : info = pickle . load ( fd ) except Exception as err : return None , "Unable to read file info from disk (%s)" % str ( err ) return info , ''
12801	def get_rooms ( self , sort = True ) : rooms = self . _connection . get ( "rooms" ) if sort : rooms . sort ( key = operator . itemgetter ( "name" ) ) return rooms
947	def _printAvailableCheckpoints ( experimentDir ) : checkpointParentDir = getCheckpointParentDir ( experimentDir ) if not os . path . exists ( checkpointParentDir ) : print "No available checkpoints." return checkpointDirs = [ x for x in os . listdir ( checkpointParentDir ) if _isCheckpointDir ( os . path . join ( checkpointParentDir , x ) ) ] if not checkpointDirs : print "No available checkpoints." return print "Available checkpoints:" checkpointList = [ _checkpointLabelFromCheckpointDir ( x ) for x in checkpointDirs ] for checkpoint in sorted ( checkpointList ) : print "\t" , checkpoint print print "To start from a checkpoint:" print " python run_opf_experiment.py experiment --load <CHECKPOINT>" print "For example, to start from the checkpoint \"MyCheckpoint\":" print " python run_opf_experiment.py experiment --load MyCheckpoint"
6923	def aovhm_theta ( times , mags , errs , frequency , nharmonics , magvariance ) : period = 1.0 / frequency ndet = times . size two_nharmonics = nharmonics + nharmonics phasedseries = phase_magseries_with_errs ( times , mags , errs , period , times [ 0 ] , sort = True , wrap = False ) phase = phasedseries [ 'phase' ] pmags = phasedseries [ 'mags' ] perrs = phasedseries [ 'errs' ] pweights = 1.0 / perrs phase = phase * 2.0 * pi_value z = npcos ( phase ) + 1.0j * npsin ( phase ) phase = nharmonics * phase psi = pmags * pweights * ( npcos ( phase ) + 1j * npsin ( phase ) ) zn = 1.0 + 0.0j phi = pweights + 0.0j theta_aov = 0.0 for _ in range ( two_nharmonics ) : phi_dot_phi = npsum ( phi * phi . conjugate ( ) ) alpha = npsum ( pweights * z * phi ) phi_dot_psi = npvdot ( phi , psi ) phi_dot_phi = npmax ( [ phi_dot_phi , 10.0e-9 ] ) alpha = alpha / phi_dot_phi theta_aov = ( theta_aov + npabs ( phi_dot_psi ) * npabs ( phi_dot_psi ) / phi_dot_phi ) phi = phi * z - alpha * zn * phi . conjugate ( ) zn = zn * z theta_aov = ( ( ndet - two_nharmonics - 1.0 ) * theta_aov / ( two_nharmonics * npmax ( [ magvariance - theta_aov , 1.0e-9 ] ) ) ) return theta_aov
12010	def getTableOfContents ( self ) : self . directory_size = self . getDirectorySize ( ) if self . directory_size > 65536 : self . directory_size += 2 self . requestContentDirectory ( ) directory_start = unpack ( "i" , self . raw_bytes [ self . directory_end + 16 : self . directory_end + 20 ] ) [ 0 ] self . raw_bytes = self . raw_bytes current_start = directory_start - self . start filestart = 0 compressedsize = 0 tableOfContents = [ ] try : while True : zip_n = unpack ( "H" , self . raw_bytes [ current_start + 28 : current_start + 28 + 2 ] ) [ 0 ] zip_m = unpack ( "H" , self . raw_bytes [ current_start + 30 : current_start + 30 + 2 ] ) [ 0 ] zip_k = unpack ( "H" , self . raw_bytes [ current_start + 32 : current_start + 32 + 2 ] ) [ 0 ] filename = self . raw_bytes [ current_start + 46 : current_start + 46 + zip_n ] filestart = unpack ( "I" , self . raw_bytes [ current_start + 42 : current_start + 42 + 4 ] ) [ 0 ] compressedsize = unpack ( "I" , self . raw_bytes [ current_start + 20 : current_start + 20 + 4 ] ) [ 0 ] uncompressedsize = unpack ( "I" , self . raw_bytes [ current_start + 24 : current_start + 24 + 4 ] ) [ 0 ] tableItem = { 'filename' : filename , 'compressedsize' : compressedsize , 'uncompressedsize' : uncompressedsize , 'filestart' : filestart } tableOfContents . append ( tableItem ) current_start = current_start + 46 + zip_n + zip_m + zip_k except : pass self . tableOfContents = tableOfContents return tableOfContents
6585	def input ( self , input , song ) : try : cmd = getattr ( self , self . CMD_MAP [ input ] [ 1 ] ) except ( IndexError , KeyError ) : return self . screen . print_error ( "Invalid command {!r}!" . format ( input ) ) cmd ( song )
7436	def _tuplecheck ( newvalue , dtype = str ) : if isinstance ( newvalue , list ) : newvalue = tuple ( newvalue ) if isinstance ( newvalue , str ) : newvalue = newvalue . rstrip ( ")" ) . strip ( "(" ) try : newvalue = tuple ( [ dtype ( i . strip ( ) ) for i in newvalue . split ( "," ) ] ) except TypeError : newvalue = tuple ( dtype ( newvalue ) ) except ValueError : LOGGER . info ( "Assembly.tuplecheck() failed to cast to {} - {}" . format ( dtype , newvalue ) ) raise except Exception as inst : LOGGER . info ( inst ) raise SystemExit ( "\nError: Param`{}` is not formatted correctly.\n({})\n" . format ( newvalue , inst ) ) return newvalue
5298	def get_queryset ( self ) : qs = super ( BaseCalendarMonthView , self ) . get_queryset ( ) year = self . get_year ( ) month = self . get_month ( ) date_field = self . get_date_field ( ) end_date_field = self . get_end_date_field ( ) date = _date_from_string ( year , self . get_year_format ( ) , month , self . get_month_format ( ) ) since = date until = self . get_next_month ( date ) if since . weekday ( ) != self . get_first_of_week ( ) : diff = math . fabs ( since . weekday ( ) - self . get_first_of_week ( ) ) since = since - datetime . timedelta ( days = diff ) if until . weekday ( ) != ( ( self . get_first_of_week ( ) + 6 ) % 7 ) : diff = math . fabs ( ( ( self . get_first_of_week ( ) + 6 ) % 7 ) - until . weekday ( ) ) until = until + datetime . timedelta ( days = diff ) if end_date_field : predicate1 = Q ( ** { '%s__gte' % date_field : since , end_date_field : None } ) predicate2 = Q ( ** { '%s__gte' % date_field : since , '%s__lt' % end_date_field : until } ) predicate3 = Q ( ** { '%s__lt' % date_field : since , '%s__gte' % end_date_field : since , '%s__lt' % end_date_field : until } ) predicate4 = Q ( ** { '%s__gte' % date_field : since , '%s__lt' % date_field : until , '%s__gte' % end_date_field : until } ) predicate5 = Q ( ** { '%s__lt' % date_field : since , '%s__gte' % end_date_field : until } ) return qs . filter ( predicate1 | predicate2 | predicate3 | predicate4 | predicate5 ) return qs . filter ( ** { '%s__gte' % date_field : since } )
13102	def get_template_uuid ( self ) : response = requests . get ( self . url + 'editor/scan/templates' , headers = self . headers , verify = False ) templates = json . loads ( response . text ) for template in templates [ 'templates' ] : if template [ 'name' ] == self . template_name : return template [ 'uuid' ]
10363	def has_protein_modification_increases_activity ( graph : BELGraph , source : BaseEntity , target : BaseEntity , key : str , ) -> bool : edge_data = graph [ source ] [ target ] [ key ] return has_protein_modification ( graph , source ) and part_has_modifier ( edge_data , OBJECT , ACTIVITY )
11570	def set_brightness ( self , brightness ) : if brightness > 15 : brightness = 15 brightness |= 0xE0 self . brightness = brightness self . firmata . i2c_write ( 0x70 , brightness )
11145	def get_repository_state ( self , relaPath = None ) : state = [ ] def _walk_dir ( relaPath , dirList ) : dirDict = { 'type' : 'dir' , 'exists' : os . path . isdir ( os . path . join ( self . __path , relaPath ) ) , 'pyrepdirinfo' : os . path . isfile ( os . path . join ( self . __path , relaPath , self . __dirInfo ) ) , } state . append ( { relaPath : dirDict } ) for fname in sorted ( [ f for f in dirList if isinstance ( f , basestring ) ] ) : relaFilePath = os . path . join ( relaPath , fname ) realFilePath = os . path . join ( self . __path , relaFilePath ) fileDict = { 'type' : 'file' , 'exists' : os . path . isfile ( realFilePath ) , 'pyrepfileinfo' : os . path . isfile ( os . path . join ( self . __path , relaPath , self . __fileInfo % fname ) ) , } state . append ( { relaFilePath : fileDict } ) for ddict in sorted ( [ d for d in dirList if isinstance ( d , dict ) ] , key = lambda k : list ( k ) [ 0 ] ) : dirname = list ( ddict ) [ 0 ] _walk_dir ( relaPath = os . path . join ( relaPath , dirname ) , dirList = ddict [ dirname ] ) if relaPath is None : _walk_dir ( relaPath = '' , dirList = self . __repo [ 'walk_repo' ] ) else : assert isinstance ( relaPath , basestring ) , "relaPath must be None or a str" relaPath = self . to_repo_relative_path ( path = relaPath , split = False ) spath = relaPath . split ( os . sep ) dirList = self . __repo [ 'walk_repo' ] while len ( spath ) : dirname = spath . pop ( 0 ) dList = [ d for d in dirList if isinstance ( d , dict ) ] if not len ( dList ) : dirList = None break cDict = [ d for d in dList if dirname in d ] if not len ( cDict ) : dirList = None break dirList = cDict [ 0 ] [ dirname ] if dirList is not None : _walk_dir ( relaPath = relaPath , dirList = dirList ) return state
7793	def set_fetcher ( self , fetcher_class ) : self . _lock . acquire ( ) try : self . _fetcher = fetcher_class finally : self . _lock . release ( )
3964	def stop_apps_or_services ( app_or_service_names = None , rm_containers = False ) : if app_or_service_names : log_to_client ( "Stopping the following apps or services: {}" . format ( ', ' . join ( app_or_service_names ) ) ) else : log_to_client ( "Stopping all running containers associated with Dusty" ) compose . stop_running_services ( app_or_service_names ) if rm_containers : compose . rm_containers ( app_or_service_names )
10496	def leftMouseDragged ( self , stopCoord , strCoord = ( 0 , 0 ) , speed = 1 ) : self . _leftMouseDragged ( stopCoord , strCoord , speed )
2424	def set_doc_data_lics ( self , doc , lics ) : if not self . doc_data_lics_set : self . doc_data_lics_set = True if validations . validate_data_lics ( lics ) : doc . data_license = document . License . from_identifier ( lics ) return True else : raise SPDXValueError ( 'Document::DataLicense' ) else : raise CardinalityError ( 'Document::DataLicense' )
5652	def create_file ( fname = None , fname_tmp = None , tmpdir = None , save_tmpfile = False , keepext = False ) : if fname == ':memory:' : yield fname return if fname_tmp is None : basename = os . path . basename ( fname ) root , ext = os . path . splitext ( basename ) dir_ = this_dir = os . path . dirname ( fname ) if not keepext : root = root + ext ext = '' if tmpdir : if tmpdir is True : for dir__ in possible_tmpdirs : if os . access ( dir__ , os . F_OK ) : dir_ = dir__ break tmpfile = tempfile . NamedTemporaryFile ( prefix = 'tmp-' + root + '-' , suffix = ext , dir = dir_ , delete = False ) fname_tmp = tmpfile . name try : yield fname_tmp except Exception as e : if save_tmpfile : print ( "Temporary file is '%s'" % fname_tmp ) else : os . unlink ( fname_tmp ) raise try : os . rename ( fname_tmp , fname ) os . chmod ( fname , 0o777 & ~ current_umask ) except OSError as e : tmpfile2 = tempfile . NamedTemporaryFile ( prefix = 'tmp-' + root + '-' , suffix = ext , dir = this_dir , delete = False ) shutil . copy ( fname_tmp , tmpfile2 . name ) os . rename ( tmpfile2 . name , fname ) os . chmod ( fname , 0o666 & ~ current_umask ) os . unlink ( fname_tmp )
2625	def submit ( self , command = 'sleep 1' , blocksize = 1 , tasks_per_node = 1 , job_name = "parsl.auto" ) : job_name = "parsl.auto.{0}" . format ( time . time ( ) ) wrapped_cmd = self . launcher ( command , tasks_per_node , self . nodes_per_block ) [ instance , * rest ] = self . spin_up_instance ( command = wrapped_cmd , job_name = job_name ) if not instance : logger . error ( "Failed to submit request to EC2" ) return None logger . debug ( "Started instance_id: {0}" . format ( instance . instance_id ) ) state = translate_table . get ( instance . state [ 'Name' ] , "PENDING" ) self . resources [ instance . instance_id ] = { "job_id" : instance . instance_id , "instance" : instance , "status" : state } return instance . instance_id
8550	def update_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value if attr == 'source_mac' : data [ 'sourceMac' ] = value elif attr == 'source_ip' : data [ 'sourceIp' ] = value elif attr == 'target_ip' : data [ 'targetIp' ] = value elif attr == 'port_range_start' : data [ 'portRangeStart' ] = value elif attr == 'port_range_end' : data [ 'portRangeEnd' ] = value elif attr == 'icmp_type' : data [ 'icmpType' ] = value elif attr == 'icmp_code' : data [ 'icmpCode' ] = value else : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s/firewallrules/%s' % ( datacenter_id , server_id , nic_id , firewall_rule_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
10286	def get_subgraph_peripheral_nodes ( graph : BELGraph , subgraph : Iterable [ BaseEntity ] , node_predicates : NodePredicates = None , edge_predicates : EdgePredicates = None , ) : node_filter = concatenate_node_predicates ( node_predicates = node_predicates ) edge_filter = and_edge_predicates ( edge_predicates = edge_predicates ) result = defaultdict ( lambda : defaultdict ( lambda : defaultdict ( list ) ) ) for u , v , k , d in get_peripheral_successor_edges ( graph , subgraph ) : if not node_filter ( graph , v ) or not node_filter ( graph , u ) or not edge_filter ( graph , u , v , k ) : continue result [ v ] [ 'predecessor' ] [ u ] . append ( ( k , d ) ) for u , v , k , d in get_peripheral_predecessor_edges ( graph , subgraph ) : if not node_filter ( graph , v ) or not node_filter ( graph , u ) or not edge_filter ( graph , u , v , k ) : continue result [ u ] [ 'successor' ] [ v ] . append ( ( k , d ) ) return result
4332	def noiseprof ( self , input_filepath , profile_path ) : if os . path . isdir ( profile_path ) : raise ValueError ( "profile_path {} is a directory." . format ( profile_path ) ) if os . path . dirname ( profile_path ) == '' and profile_path != '' : _abs_profile_path = os . path . join ( os . getcwd ( ) , profile_path ) else : _abs_profile_path = profile_path if not os . access ( os . path . dirname ( _abs_profile_path ) , os . W_OK ) : raise IOError ( "profile_path {} is not writeable." . format ( _abs_profile_path ) ) effect_args = [ 'noiseprof' , profile_path ] self . build ( input_filepath , None , extra_args = effect_args ) return None
8005	def from_xml ( self , xmlnode ) : if xmlnode . type != "element" : raise ValueError ( "XML node is not a jabber:x:delay element (not an element)" ) ns = get_node_ns_uri ( xmlnode ) if ns and ns != DELAY_NS or xmlnode . name != "x" : raise ValueError ( "XML node is not a jabber:x:delay element" ) stamp = xmlnode . prop ( "stamp" ) if stamp . endswith ( "Z" ) : stamp = stamp [ : - 1 ] if "-" in stamp : stamp = stamp . split ( "-" , 1 ) [ 0 ] try : tm = time . strptime ( stamp , "%Y%m%dT%H:%M:%S" ) except ValueError : raise BadRequestProtocolError ( "Bad timestamp" ) tm = tm [ 0 : 8 ] + ( 0 , ) self . timestamp = datetime . datetime . fromtimestamp ( time . mktime ( tm ) ) delay_from = from_utf8 ( xmlnode . prop ( "from" ) ) if delay_from : try : self . delay_from = JID ( delay_from ) except JIDError : raise JIDMalformedProtocolError ( "Bad JID in the jabber:x:delay 'from' attribute" ) else : self . delay_from = None self . reason = from_utf8 ( xmlnode . getContent ( ) )
5385	def _get_operation_input_field_values ( self , metadata , file_input ) : input_args = metadata [ 'request' ] [ 'ephemeralPipeline' ] [ 'inputParameters' ] vals_dict = metadata [ 'request' ] [ 'pipelineArgs' ] [ 'inputs' ] names = [ arg [ 'name' ] for arg in input_args if ( 'localCopy' in arg ) == file_input ] return { name : vals_dict [ name ] for name in names if name in vals_dict }
7999	def auth_properties ( self ) : props = dict ( self . settings [ "extra_auth_properties" ] ) if self . transport : props . update ( self . transport . auth_properties ) props [ "local-jid" ] = self . me props [ "service-type" ] = "xmpp" return props
86	def is_single_float ( val ) : return isinstance ( val , numbers . Real ) and not is_single_integer ( val ) and not isinstance ( val , bool )
2585	def migrate_tasks_to_internal ( self , kill_event ) : logger . info ( "[TASK_PULL_THREAD] Starting" ) task_counter = 0 poller = zmq . Poller ( ) poller . register ( self . task_incoming , zmq . POLLIN ) while not kill_event . is_set ( ) : try : msg = self . task_incoming . recv_pyobj ( ) except zmq . Again : logger . debug ( "[TASK_PULL_THREAD] {} tasks in internal queue" . format ( self . pending_task_queue . qsize ( ) ) ) continue if msg == 'STOP' : kill_event . set ( ) break else : self . pending_task_queue . put ( msg ) task_counter += 1 logger . debug ( "[TASK_PULL_THREAD] Fetched task:{}" . format ( task_counter ) )
2913	def get_state_name ( self ) : state_name = [ ] for state , name in list ( self . state_names . items ( ) ) : if self . _has_state ( state ) : state_name . append ( name ) return '|' . join ( state_name )
8659	def filter_by ( zips = _zips , ** kwargs ) : return [ z for z in zips if all ( [ k in z and z [ k ] == v for k , v in kwargs . items ( ) ] ) ]
12416	def end ( self , * args , ** kwargs ) : self . send ( * args , ** kwargs ) self . close ( )
6673	def is_link ( self , path , use_sudo = False ) : func = use_sudo and _sudo or _run with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : return func ( '[ -L "%(path)s" ]' % locals ( ) ) . succeeded
9627	def url ( self ) : return reverse ( '%s:detail' % URL_NAMESPACE , kwargs = { 'module' : self . module , 'preview' : type ( self ) . __name__ , } )
4623	def _derive_checksum ( self , s ) : checksum = hashlib . sha256 ( bytes ( s , "ascii" ) ) . hexdigest ( ) return checksum [ : 4 ]
13271	def unique_justseen ( iterable , key = None ) : "List unique elements, preserving order. Remember only the element just seen." try : from itertools import imap as map except ImportError : from builtins import map return map ( next , map ( operator . itemgetter ( 1 ) , itertools . groupby ( iterable , key ) ) )
6249	def get_effect_class ( self , effect_name : str , package_name : str = None ) -> Type [ 'Effect' ] : return self . _project . get_effect_class ( effect_name , package_name = package_name )
687	def getAllEncodings ( self ) : numEncodings = self . fields [ 0 ] . numEncodings assert ( all ( field . numEncodings == numEncodings for field in self . fields ) ) encodings = [ self . getEncoding ( index ) for index in range ( numEncodings ) ] return encodings
4532	def clone ( self ) : args = { k : getattr ( self , k ) for k in self . CLONE_ATTRS } args [ 'color_list' ] = copy . copy ( self . color_list ) return self . __class__ ( [ ] , ** args )
9136	def get_modules ( ) -> Mapping : modules = { } for entry_point in iter_entry_points ( group = 'bio2bel' , name = None ) : entry = entry_point . name try : modules [ entry ] = entry_point . load ( ) except VersionConflict as exc : log . warning ( 'Version conflict in %s: %s' , entry , exc ) continue except UnknownExtra as exc : log . warning ( 'Unknown extra in %s: %s' , entry , exc ) continue except ImportError as exc : log . exception ( 'Issue with importing module %s: %s' , entry , exc ) continue return modules
3899	def main ( ) : parser = argparse . ArgumentParser ( ) parser . add_argument ( 'protofilepath' ) args = parser . parse_args ( ) out_file = compile_protofile ( args . protofilepath ) with open ( out_file , 'rb' ) as proto_file : file_descriptor_set = descriptor_pb2 . FileDescriptorSet . FromString ( proto_file . read ( ) ) for file_descriptor in file_descriptor_set . file : locations = { } for location in file_descriptor . source_code_info . location : locations [ tuple ( location . path ) ] = location print ( make_comment ( 'This file was automatically generated from {} and ' 'should not be edited directly.' . format ( args . protofilepath ) ) ) for index , message_desc in enumerate ( file_descriptor . message_type ) : generate_message_doc ( message_desc , locations , ( 4 , index ) ) for index , enum_desc in enumerate ( file_descriptor . enum_type ) : generate_enum_doc ( enum_desc , locations , ( 5 , index ) )
1626	def ReverseCloseExpression ( clean_lines , linenum , pos ) : line = clean_lines . elided [ linenum ] if line [ pos ] not in ')}]>' : return ( line , 0 , - 1 ) ( start_pos , stack ) = FindStartOfExpressionInLine ( line , pos , [ ] ) if start_pos > - 1 : return ( line , linenum , start_pos ) while stack and linenum > 0 : linenum -= 1 line = clean_lines . elided [ linenum ] ( start_pos , stack ) = FindStartOfExpressionInLine ( line , len ( line ) - 1 , stack ) if start_pos > - 1 : return ( line , linenum , start_pos ) return ( line , 0 , - 1 )
8195	def load ( self , id ) : self . clear ( ) self . add_node ( id , root = True ) for w , id2 in self . get_links ( id ) : self . add_edge ( id , id2 , weight = w ) if len ( self ) > self . max : break for w , id2 , links in self . get_cluster ( id ) : for id3 in links : self . add_edge ( id3 , id2 , weight = w ) self . add_edge ( id , id3 , weight = w ) if len ( self ) > self . max : break if self . event . clicked : g . add_node ( self . event . clicked )
1500	def fail ( self , tup ) : if not isinstance ( tup , HeronTuple ) : Log . error ( "Only HeronTuple type is supported in fail()" ) return if self . acking_enabled : fail_tuple = tuple_pb2 . AckTuple ( ) fail_tuple . ackedtuple = int ( tup . id ) tuple_size_in_bytes = 0 for rt in tup . roots : to_add = fail_tuple . roots . add ( ) to_add . CopyFrom ( rt ) tuple_size_in_bytes += rt . ByteSize ( ) super ( BoltInstance , self ) . admit_control_tuple ( fail_tuple , tuple_size_in_bytes , False ) fail_latency_ns = ( time . time ( ) - tup . creation_time ) * system_constants . SEC_TO_NS self . pplan_helper . context . invoke_hook_bolt_fail ( tup , fail_latency_ns ) self . bolt_metrics . failed_tuple ( tup . stream , tup . component , fail_latency_ns )
3931	def _auth_with_refresh_token ( session , refresh_token ) : token_request_data = { 'client_id' : OAUTH2_CLIENT_ID , 'client_secret' : OAUTH2_CLIENT_SECRET , 'grant_type' : 'refresh_token' , 'refresh_token' : refresh_token , } res = _make_token_request ( session , token_request_data ) return res [ 'access_token' ]
11920	def get_object ( self ) : dataframe = self . filter_dataframe ( self . get_dataframe ( ) ) assert self . lookup_url_kwarg in self . kwargs , ( 'Expected view %s to be called with a URL keyword argument ' 'named "%s". Fix your URL conf, or set the `.lookup_field` ' 'attribute on the view correctly.' % ( self . __class__ . __name__ , self . lookup_url_kwarg ) ) try : obj = self . index_row ( dataframe ) except ( IndexError , KeyError , ValueError ) : raise Http404 self . check_object_permissions ( self . request , obj ) return obj
4171	def enbw ( data ) : r N = len ( data ) return N * np . sum ( data ** 2 ) / np . sum ( data ) ** 2
6533	def get_project_config ( project_path , use_cache = True ) : return get_local_config ( project_path , use_cache = use_cache ) or get_user_config ( project_path , use_cache = use_cache ) or get_default_config ( )
117	def imap_batches ( self , batches , chunksize = 1 ) : assert ia . is_generator ( batches ) , ( "Expected to get a generator as 'batches', got type %s. " + "Call map_batches() if you use lists." ) % ( type ( batches ) , ) gen = self . pool . imap ( _Pool_starworker , self . _handle_batch_ids_gen ( batches ) , chunksize = chunksize ) for batch in gen : yield batch
7938	def _connect ( self , addr , port , service ) : self . _dst_name = addr self . _dst_port = port family = None try : res = socket . getaddrinfo ( addr , port , socket . AF_UNSPEC , socket . SOCK_STREAM , 0 , socket . AI_NUMERICHOST ) family = res [ 0 ] [ 0 ] sockaddr = res [ 0 ] [ 4 ] except socket . gaierror : family = None sockaddr = None if family is not None : if not port : raise ValueError ( "No port number given with literal IP address" ) self . _dst_service = None self . _family = family self . _dst_addrs = [ ( family , sockaddr ) ] self . _set_state ( "connect" ) elif service is not None : self . _dst_service = service self . _set_state ( "resolve-srv" ) self . _dst_name = addr elif port : self . _dst_nameports = [ ( self . _dst_name , self . _dst_port ) ] self . _dst_service = None self . _set_state ( "resolve-hostname" ) else : raise ValueError ( "No port number and no SRV service name given" )
4497	def project ( self , project_id ) : type_ = self . guid ( project_id ) url = self . _build_url ( type_ , project_id ) if type_ in Project . _types : return Project ( self . _json ( self . _get ( url ) , 200 ) , self . session ) raise OSFException ( '{} is unrecognized type {}. Clone supports projects and registrations' . format ( project_id , type_ ) )
8870	def create_metafile ( bgen_filepath , metafile_filepath , verbose = True ) : r if verbose : verbose = 1 else : verbose = 0 bgen_filepath = make_sure_bytes ( bgen_filepath ) metafile_filepath = make_sure_bytes ( metafile_filepath ) assert_file_exist ( bgen_filepath ) assert_file_readable ( bgen_filepath ) if exists ( metafile_filepath ) : raise ValueError ( f"The file {metafile_filepath} already exists." ) with bgen_file ( bgen_filepath ) as bgen : nparts = _estimate_best_npartitions ( lib . bgen_nvariants ( bgen ) ) metafile = lib . bgen_create_metafile ( bgen , metafile_filepath , nparts , verbose ) if metafile == ffi . NULL : raise RuntimeError ( f"Error while creating metafile: {metafile_filepath}." ) if lib . bgen_close_metafile ( metafile ) != 0 : raise RuntimeError ( f"Error while closing metafile: {metafile_filepath}." )
7518	def maxind_numba ( block ) : inds = 0 for row in xrange ( block . shape [ 0 ] ) : where = np . where ( block [ row ] != 45 ) [ 0 ] if len ( where ) == 0 : obs = 100 else : left = np . min ( where ) right = np . max ( where ) obs = np . sum ( block [ row , left : right ] == 45 ) if obs > inds : inds = obs return inds
1286	def build_metagraph_list ( self ) : ops = [ ] self . ignore_unknown_dtypes = True for key in sorted ( self . meta_params ) : value = self . convert_data_to_string ( self . meta_params [ key ] ) if len ( value ) == 0 : continue if isinstance ( value , str ) : ops . append ( tf . contrib . summary . generic ( name = key , tensor = tf . convert_to_tensor ( str ( value ) ) ) ) else : ops . append ( tf . contrib . summary . generic ( name = key , tensor = tf . as_string ( tf . convert_to_tensor ( value ) ) ) ) return ops
10208	def record_view_event_builder ( event , sender_app , pid = None , record = None , ** kwargs ) : event . update ( dict ( timestamp = datetime . datetime . utcnow ( ) . isoformat ( ) , record_id = str ( record . id ) , pid_type = pid . pid_type , pid_value = str ( pid . pid_value ) , referrer = request . referrer , ** get_user ( ) ) ) return event
2454	def set_pkg_source_info ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_source_info_set : self . package_source_info_set = True if validations . validate_pkg_src_info ( text ) : doc . package . source_info = str_from_text ( text ) return True else : raise SPDXValueError ( 'Pacckage::SourceInfo' ) else : raise CardinalityError ( 'Package::SourceInfo' )
10422	def count_annotation_values_filtered ( graph : BELGraph , annotation : str , source_predicate : Optional [ NodePredicate ] = None , target_predicate : Optional [ NodePredicate ] = None , ) -> Counter : if source_predicate and target_predicate : return Counter ( data [ ANNOTATIONS ] [ annotation ] for u , v , data in graph . edges ( data = True ) if edge_has_annotation ( data , annotation ) and source_predicate ( graph , u ) and target_predicate ( graph , v ) ) elif source_predicate : return Counter ( data [ ANNOTATIONS ] [ annotation ] for u , v , data in graph . edges ( data = True ) if edge_has_annotation ( data , annotation ) and source_predicate ( graph , u ) ) elif target_predicate : return Counter ( data [ ANNOTATIONS ] [ annotation ] for u , v , data in graph . edges ( data = True ) if edge_has_annotation ( data , annotation ) and target_predicate ( graph , u ) ) else : return Counter ( data [ ANNOTATIONS ] [ annotation ] for u , v , data in graph . edges ( data = True ) if edge_has_annotation ( data , annotation ) )
3627	def normalize_cols ( table ) : longest_row_len = max ( [ len ( row ) for row in table ] ) for row in table : while len ( row ) < longest_row_len : row . append ( '' ) return table
5700	def _distribution ( gtfs , table , column ) : cur = gtfs . conn . cursor ( ) cur . execute ( 'SELECT {column}, count(*) ' 'FROM {table} GROUP BY {column} ' 'ORDER BY {column}' . format ( column = column , table = table ) ) return ' ' . join ( '%s:%s' % ( t , c ) for t , c in cur )
6773	def uninstall_blacklisted ( self ) : from burlap . system import distrib_family blacklisted_packages = self . env . blacklisted_packages if not blacklisted_packages : print ( 'No blacklisted packages.' ) return else : family = distrib_family ( ) if family == DEBIAN : self . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq purge %s' % ' ' . join ( blacklisted_packages ) ) else : raise NotImplementedError ( 'Unknown family: %s' % family )
7040	def list_recent_datasets ( lcc_server , nrecent = 25 ) : urlparams = { 'nsets' : nrecent } urlqs = urlencode ( urlparams ) url = '%s/api/datasets?%s' % ( lcc_server , urlqs ) try : LOGINFO ( 'getting list of recent publicly ' 'visible and owned datasets from %s' % ( lcc_server , ) ) have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } req = Request ( url , data = None , headers = headers ) resp = urlopen ( req ) recent_datasets = json . loads ( resp . read ( ) ) [ 'result' ] return recent_datasets except HTTPError as e : LOGERROR ( 'could not retrieve recent datasets list, ' 'URL used: %s, error code: %s, reason: %s' % ( url , e . code , e . reason ) ) return None
10985	def optimize_from_initial ( s , max_mem = 1e9 , invert = 'guess' , desc = '' , rz_order = 3 , min_rad = None , max_rad = None ) : RLOG . info ( 'Initial burn:' ) if desc is not None : desc_burn = desc + 'initial-burn' desc_polish = desc + 'addsub-polish' else : desc_burn , desc_polish = [ None ] * 2 opt . burn ( s , mode = 'burn' , n_loop = 3 , fractol = 0.1 , desc = desc_burn , max_mem = max_mem , include_rad = False , dowarn = False ) opt . burn ( s , mode = 'burn' , n_loop = 3 , fractol = 0.1 , desc = desc_burn , max_mem = max_mem , include_rad = True , dowarn = False ) RLOG . info ( 'Start add-subtract' ) rad = s . obj_get_radii ( ) if min_rad is None : min_rad = 0.5 * np . median ( rad ) if max_rad is None : max_rad = 1.5 * np . median ( rad ) addsub . add_subtract ( s , tries = 30 , min_rad = min_rad , max_rad = max_rad , invert = invert ) if desc is not None : states . save ( s , desc = desc + 'initial-addsub' ) RLOG . info ( 'Final polish:' ) d = opt . burn ( s , mode = 'polish' , n_loop = 8 , fractol = 3e-4 , desc = desc_polish , max_mem = max_mem , rz_order = rz_order , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' ) return s
4587	def stop ( self ) : if self . is_running : log . info ( 'Stopping' ) self . is_running = False self . __class__ . _INSTANCE = None try : self . thread and self . thread . stop ( ) except : log . error ( 'Error stopping thread' ) traceback . print_exc ( ) self . thread = None return True
4457	def get_args ( self ) : args = [ self . _query_string ] if self . _no_content : args . append ( 'NOCONTENT' ) if self . _fields : args . append ( 'INFIELDS' ) args . append ( len ( self . _fields ) ) args += self . _fields if self . _verbatim : args . append ( 'VERBATIM' ) if self . _no_stopwords : args . append ( 'NOSTOPWORDS' ) if self . _filters : for flt in self . _filters : assert isinstance ( flt , Filter ) args += flt . args if self . _with_payloads : args . append ( 'WITHPAYLOADS' ) if self . _ids : args . append ( 'INKEYS' ) args . append ( len ( self . _ids ) ) args += self . _ids if self . _slop >= 0 : args += [ 'SLOP' , self . _slop ] if self . _in_order : args . append ( 'INORDER' ) if self . _return_fields : args . append ( 'RETURN' ) args . append ( len ( self . _return_fields ) ) args += self . _return_fields if self . _sortby : assert isinstance ( self . _sortby , SortbyField ) args . append ( 'SORTBY' ) args += self . _sortby . args if self . _language : args += [ 'LANGUAGE' , self . _language ] args += self . _summarize_fields + self . _highlight_fields args += [ "LIMIT" , self . _offset , self . _num ] return args
3408	def parse_gpr ( str_expr ) : str_expr = str_expr . strip ( ) if len ( str_expr ) == 0 : return None , set ( ) for char , escaped in replacements : if char in str_expr : str_expr = str_expr . replace ( char , escaped ) escaped_str = keyword_re . sub ( "__cobra_escape__" , str_expr ) escaped_str = number_start_re . sub ( "__cobra_escape__" , escaped_str ) tree = ast_parse ( escaped_str , "<string>" , "eval" ) cleaner = GPRCleaner ( ) cleaner . visit ( tree ) eval_gpr ( tree , set ( ) ) return tree , cleaner . gene_set
5111	def _get_queues ( g , queues , edge , edge_type ) : INT = numbers . Integral if isinstance ( queues , INT ) : queues = [ queues ] elif queues is None : if edge is not None : if isinstance ( edge , tuple ) : if isinstance ( edge [ 0 ] , INT ) and isinstance ( edge [ 1 ] , INT ) : queues = [ g . edge_index [ edge ] ] elif isinstance ( edge [ 0 ] , collections . Iterable ) : if np . array ( [ len ( e ) == 2 for e in edge ] ) . all ( ) : queues = [ g . edge_index [ e ] for e in edge ] else : queues = [ g . edge_index [ edge ] ] elif edge_type is not None : if isinstance ( edge_type , collections . Iterable ) : edge_type = set ( edge_type ) else : edge_type = set ( [ edge_type ] ) tmp = [ ] for e in g . edges ( ) : if g . ep ( e , 'edge_type' ) in edge_type : tmp . append ( g . edge_index [ e ] ) queues = np . array ( tmp , int ) if queues is None : queues = range ( g . number_of_edges ( ) ) return queues
4821	def redirect_if_blocked ( course_run_ids , user = None , ip_address = None , url = None ) : for course_run_id in course_run_ids : redirect_url = embargo_api . redirect_if_blocked ( CourseKey . from_string ( course_run_id ) , user = user , ip_address = ip_address , url = url ) if redirect_url : return redirect_url
13833	def ParseInteger ( text , is_signed = False , is_long = False ) : try : if is_long : result = long ( text , 0 ) else : result = int ( text , 0 ) except ValueError : raise ValueError ( 'Couldn\'t parse integer: %s' % text ) checker = _INTEGER_CHECKERS [ 2 * int ( is_long ) + int ( is_signed ) ] checker . CheckValue ( result ) return result
9515	def to_Fastq ( self , qual_scores ) : if len ( self ) != len ( qual_scores ) : raise Error ( 'Error making Fastq from Fasta, lengths differ.' , self . id ) return Fastq ( self . id , self . seq , '' . join ( [ chr ( max ( 0 , min ( x , 93 ) ) + 33 ) for x in qual_scores ] ) )
5674	def get_main_database_path ( self ) : cur = self . conn . cursor ( ) cur . execute ( "PRAGMA database_list" ) rows = cur . fetchall ( ) for row in rows : if row [ 1 ] == str ( "main" ) : return row [ 2 ]
11798	def suppose ( self , var , value ) : "Start accumulating inferences from assuming var=value." self . support_pruning ( ) removals = [ ( var , a ) for a in self . curr_domains [ var ] if a != value ] self . curr_domains [ var ] = [ value ] return removals
5869	def _inactivate_organization_course_relationship ( relationship ) : relationship = internal . OrganizationCourse . objects . get ( id = relationship . id , active = True ) _inactivate_record ( relationship )
9434	def load_savefile ( input_file , layers = 0 , verbose = False , lazy = False ) : global VERBOSE old_verbose = VERBOSE VERBOSE = verbose __TRACE__ ( '[+] attempting to load {:s}' , ( input_file . name , ) ) header = _load_savefile_header ( input_file ) if __validate_header__ ( header ) : __TRACE__ ( '[+] found valid header' ) if lazy : packets = _generate_packets ( input_file , header , layers ) __TRACE__ ( '[+] created packet generator' ) else : packets = _load_packets ( input_file , header , layers ) __TRACE__ ( '[+] loaded {:d} packets' , ( len ( packets ) , ) ) sfile = pcap_savefile ( header , packets ) __TRACE__ ( '[+] finished loading savefile.' ) else : __TRACE__ ( '[!] invalid savefile' ) sfile = None VERBOSE = old_verbose return sfile
11450	def get_date ( self , filename ) : try : self . document = parse ( filename ) return self . _get_date ( ) except DateNotFoundException : print ( "Date problem found in {0}" . format ( filename ) ) return datetime . datetime . strftime ( datetime . datetime . now ( ) , "%Y-%m-%d" )
4313	def silent ( input_filepath , threshold = 0.001 ) : validate_input_file ( input_filepath ) stat_dictionary = stat ( input_filepath ) mean_norm = stat_dictionary [ 'Mean norm' ] if mean_norm is not float ( 'nan' ) : if mean_norm >= threshold : return False else : return True else : return True
5681	def get_spreading_trips ( self , start_time_ut , lat , lon , max_duration_ut = 4 * 3600 , min_transfer_time = 30 , use_shapes = False ) : from gtfspy . spreading . spreader import Spreader spreader = Spreader ( self , start_time_ut , lat , lon , max_duration_ut , min_transfer_time , use_shapes ) return spreader . spread ( )
12306	def find_executable_files ( ) : files = glob . glob ( "*" ) + glob . glob ( "*/*" ) + glob . glob ( '*/*/*' ) files = filter ( lambda f : os . path . isfile ( f ) , files ) executable = stat . S_IEXEC | stat . S_IXGRP | stat . S_IXOTH final = [ ] for filename in files : if os . path . isfile ( filename ) : st = os . stat ( filename ) mode = st . st_mode if mode & executable : final . append ( filename ) if len ( final ) > 5 : break return final
1019	def getSimplePatterns ( numOnes , numPatterns , patternOverlap = 0 ) : assert ( patternOverlap < numOnes ) numNewBitsInEachPattern = numOnes - patternOverlap numCols = numNewBitsInEachPattern * numPatterns + patternOverlap p = [ ] for i in xrange ( numPatterns ) : x = numpy . zeros ( numCols , dtype = 'float32' ) startBit = i * numNewBitsInEachPattern nextStartBit = startBit + numOnes x [ startBit : nextStartBit ] = 1 p . append ( x ) return p
4882	def create_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . get_or_create ( name = 'SAP_USE_ENTERPRISE_ENROLLMENT_PAGE' , defaults = { 'active' : False } )
5677	def get_trip_trajectories_within_timespan ( self , start , end , use_shapes = True , filter_name = None ) : trips = [ ] trip_df = self . get_tripIs_active_in_range ( start , end ) print ( "gtfs_viz.py: fetched " + str ( len ( trip_df ) ) + " trip ids" ) shape_cache = { } for row in trip_df . itertuples ( ) : trip_I = row . trip_I day_start_ut = row . day_start_ut shape_id = row . shape_id trip = { } name , route_type = self . get_route_name_and_type_of_tripI ( trip_I ) trip [ 'route_type' ] = int ( route_type ) trip [ 'name' ] = str ( name ) if filter_name and ( name != filter_name ) : continue stop_lats = [ ] stop_lons = [ ] stop_dep_times = [ ] shape_breaks = [ ] stop_seqs = [ ] stop_time_df = self . get_trip_stop_time_data ( trip_I , day_start_ut ) for stop_row in stop_time_df . itertuples ( ) : stop_lats . append ( float ( stop_row . lat ) ) stop_lons . append ( float ( stop_row . lon ) ) stop_dep_times . append ( float ( stop_row . dep_time_ut ) ) try : stop_seqs . append ( int ( stop_row . seq ) ) except TypeError : stop_seqs . append ( None ) if use_shapes : try : shape_breaks . append ( int ( stop_row . shape_break ) ) except ( TypeError , ValueError ) : shape_breaks . append ( None ) if use_shapes : if shape_id not in shape_cache : shape_cache [ shape_id ] = shapes . get_shape_points2 ( self . conn . cursor ( ) , shape_id ) shape_data = shape_cache [ shape_id ] try : trip [ 'times' ] = shapes . interpolate_shape_times ( shape_data [ 'd' ] , shape_breaks , stop_dep_times ) trip [ 'lats' ] = shape_data [ 'lats' ] trip [ 'lons' ] = shape_data [ 'lons' ] start_break = shape_breaks [ 0 ] end_break = shape_breaks [ - 1 ] trip [ 'times' ] = trip [ 'times' ] [ start_break : end_break + 1 ] trip [ 'lats' ] = trip [ 'lats' ] [ start_break : end_break + 1 ] trip [ 'lons' ] = trip [ 'lons' ] [ start_break : end_break + 1 ] except : trip [ 'times' ] = stop_dep_times trip [ 'lats' ] = stop_lats trip [ 'lons' ] = stop_lons else : trip [ 'times' ] = stop_dep_times trip [ 'lats' ] = stop_lats trip [ 'lons' ] = stop_lons trips . append ( trip ) return { "trips" : trips }
1823	def SETS ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . SF , 1 , 0 ) )
2948	def evaluate ( self , task , expression ) : if isinstance ( expression , Operator ) : return expression . _matches ( task ) else : return self . _eval ( task , expression , ** task . data )
669	def createDataOutLink ( network , sensorRegionName , regionName ) : network . link ( sensorRegionName , regionName , "UniformLink" , "" , srcOutput = "dataOut" , destInput = "bottomUpIn" )
10119	def circle ( cls , center , radius , n_vertices = 50 , ** kwargs ) : return cls . regular_polygon ( center , radius , n_vertices , ** kwargs )
10016	def swap_environment_cnames ( self , from_env_name , to_env_name ) : self . ebs . swap_environment_cnames ( source_environment_name = from_env_name , destination_environment_name = to_env_name )
2782	def destroy ( self ) : return self . get_data ( "domains/%s/records/%s" % ( self . domain , self . id ) , type = DELETE , )
9467	def conference_play ( self , call_params ) : path = '/' + self . api_version + '/ConferencePlay/' method = 'POST' return self . request ( path , method , call_params )
34	def gpu_count ( ) : if shutil . which ( 'nvidia-smi' ) is None : return 0 output = subprocess . check_output ( [ 'nvidia-smi' , '--query-gpu=gpu_name' , '--format=csv' ] ) return max ( 0 , len ( output . split ( b'\n' ) ) - 2 )
11455	def get_config_item ( cls , key , kb_name , allow_substring = True ) : config_dict = cls . kbs . get ( kb_name , None ) if config_dict : if key in config_dict : return config_dict [ key ] elif allow_substring : res = [ v for k , v in config_dict . items ( ) if key in k ] if res : return res [ 0 ] return key
8535	def pop ( self , nbytes ) : size = 0 popped = [ ] with self . _lock_packets : while size < nbytes : try : packet = self . _packets . pop ( 0 ) size += len ( packet . data . data ) self . _remaining -= len ( packet . data . data ) popped . append ( packet ) except IndexError : break return popped
2510	def handle_extracted_license ( self , extr_lic ) : lic = self . parse_only_extr_license ( extr_lic ) if lic is not None : self . doc . add_extr_lic ( lic ) return lic
9771	def stop ( ctx , yes ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) if not yes and not click . confirm ( "Are sure you want to stop " "job `{}`" . format ( _job ) ) : click . echo ( 'Existing without stopping job.' ) sys . exit ( 0 ) try : PolyaxonClient ( ) . job . stop ( user , project_name , _job ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not stop job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Job is being stopped." )
6184	def get_git_version ( git_path = None ) : if git_path is None : git_path = GIT_PATH git_version = check_output ( [ git_path , "--version" ] ) . split ( ) [ 2 ] return git_version
8269	def contains ( self , clr ) : if not isinstance ( clr , Color ) : return False if not isinstance ( clr , _list ) : clr = [ clr ] for clr in clr : if clr . is_grey and not self . grayscale : return ( self . black . contains ( clr ) or self . white . contains ( clr ) ) for r , v in [ ( self . h , clr . h ) , ( self . s , clr . s ) , ( self . b , clr . brightness ) , ( self . a , clr . a ) ] : if isinstance ( r , _list ) : pass elif isinstance ( r , tuple ) : r = [ r ] else : r = [ ( r , r ) ] for min , max in r : if not ( min <= v <= max ) : return False return True
6303	def get_package ( self , name ) -> 'EffectPackage' : name , cls_name = parse_package_string ( name ) try : return self . package_map [ name ] except KeyError : raise EffectError ( "No package '{}' registered" . format ( name ) )
8228	def show ( self , format = 'png' , as_data = False ) : from io import BytesIO b = BytesIO ( ) if format == 'png' : from IPython . display import Image surface = cairo . ImageSurface ( cairo . FORMAT_ARGB32 , self . WIDTH , self . HEIGHT ) self . snapshot ( surface ) surface . write_to_png ( b ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return Image ( data ) elif format == 'svg' : from IPython . display import SVG surface = cairo . SVGSurface ( b , self . WIDTH , self . HEIGHT ) surface . finish ( ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return SVG ( data )
8144	def flip ( self , axis = HORIZONTAL ) : if axis == HORIZONTAL : self . img = self . img . transpose ( Image . FLIP_LEFT_RIGHT ) if axis == VERTICAL : self . img = self . img . transpose ( Image . FLIP_TOP_BOTTOM )
1900	def get_all_values ( self , constraints , expression , maxcnt = None , silent = False ) : if not isinstance ( expression , Expression ) : return [ expression ] assert isinstance ( constraints , ConstraintSet ) assert isinstance ( expression , Expression ) expression = simplify ( expression ) if maxcnt is None : maxcnt = consts . maxsolutions with constraints as temp_cs : if isinstance ( expression , Bool ) : var = temp_cs . new_bool ( ) elif isinstance ( expression , BitVec ) : var = temp_cs . new_bitvec ( expression . size ) elif isinstance ( expression , Array ) : var = temp_cs . new_array ( index_max = expression . index_max , value_bits = expression . value_bits , taint = expression . taint ) . array else : raise NotImplementedError ( f"get_all_values only implemented for {type(expression)} expression type." ) temp_cs . add ( var == expression ) self . _reset ( temp_cs . to_string ( related_to = var ) ) result = [ ] while self . _is_sat ( ) : value = self . _getvalue ( var ) result . append ( value ) self . _assert ( var != value ) if len ( result ) >= maxcnt : if silent : break else : raise TooManySolutions ( result ) return result
2887	def _try_disconnect ( self , ref ) : with self . lock : weak = [ s [ 0 ] for s in self . weak_subscribers ] try : index = weak . index ( ref ) except ValueError : pass else : self . weak_subscribers . pop ( index )
13299	def install ( self , package ) : logger . debug ( 'Installing ' + package ) shell . run ( self . pip_path , 'install' , package )
10431	def selectrowpartialmatch ( self , window_name , object_name , row_text ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) for cell in object_handle . AXRows : if re . search ( row_text , cell . AXChildren [ 0 ] . AXValue ) : if not cell . AXSelected : object_handle . activate ( ) cell . AXSelected = True else : pass return 1 raise LdtpServerException ( u"Unable to select row: %s" % row_text )
10614	def T ( self , T ) : self . _T = T self . _H = self . _calculate_H ( T )
6599	def key_vals_dict_to_tuple_list ( key_vals_dict , fill = float ( 'nan' ) ) : tuple_list = [ ] if not key_vals_dict : return tuple_list vlen = max ( [ len ( vs ) for vs in itertools . chain ( * key_vals_dict . values ( ) ) ] ) for k , vs in key_vals_dict . items ( ) : try : tuple_list . extend ( [ k + tuple ( v ) + ( fill , ) * ( vlen - len ( v ) ) for v in vs ] ) except TypeError : tuple_list . extend ( [ ( k , ) + tuple ( v ) + ( fill , ) * ( vlen - len ( v ) ) for v in vs ] ) return tuple_list
7455	def _cleanup_and_die ( data ) : tmpfiles = glob . glob ( os . path . join ( data . dirs . fastqs , "tmp_*_R*.fastq" ) ) tmpfiles += glob . glob ( os . path . join ( data . dirs . fastqs , "tmp_*.p" ) ) for tmpf in tmpfiles : os . remove ( tmpf )
13059	def get_inventory ( self ) : if self . _inventory is not None : return self . _inventory self . _inventory = self . resolver . getMetadata ( ) return self . _inventory
10409	def finalized_canonical_averages_dtype ( spanning_cluster = True ) : fields = list ( ) fields . extend ( [ ( 'number_of_runs' , 'uint32' ) , ( 'p' , 'float64' ) , ( 'alpha' , 'float64' ) , ] ) if spanning_cluster : fields . extend ( [ ( 'percolation_probability_mean' , 'float64' ) , ( 'percolation_probability_std' , 'float64' ) , ( 'percolation_probability_ci' , '(2,)float64' ) , ] ) fields . extend ( [ ( 'percolation_strength_mean' , 'float64' ) , ( 'percolation_strength_std' , 'float64' ) , ( 'percolation_strength_ci' , '(2,)float64' ) , ( 'moments_mean' , '(5,)float64' ) , ( 'moments_std' , '(5,)float64' ) , ( 'moments_ci' , '(5,2)float64' ) , ] ) return _ndarray_dtype ( fields )
11646	def transform ( self , X ) : n = self . flip_ . shape [ 0 ] if X . ndim != 2 or X . shape [ 1 ] != n : msg = "X should have {} columns, the number of samples at fit time" raise TypeError ( msg . format ( self . flip_ . shape [ 0 ] ) ) return np . dot ( X , self . flip_ )
3475	def compartments ( self ) : if self . _compartments is None : self . _compartments = { met . compartment for met in self . _metabolites if met . compartment is not None } return self . _compartments
7176	def retype_path ( src , pyi_dir , targets , * , src_explicitly_given = False , quiet = False , hg = False ) : if src . is_dir ( ) : for child in src . iterdir ( ) : if child == pyi_dir or child == targets : continue yield from retype_path ( child , pyi_dir / src . name , targets / src . name , quiet = quiet , hg = hg , ) elif src . suffix == '.py' or src_explicitly_given : try : retype_file ( src , pyi_dir , targets , quiet = quiet , hg = hg ) except Exception as e : yield ( src , str ( e ) , type ( e ) , traceback . format_tb ( e . __traceback__ ) , )
10321	def microcanonical_averages ( graph , runs = 40 , spanning_cluster = True , model = 'bond' , alpha = alpha_1sigma , copy_result = True ) : r try : runs = int ( runs ) except : raise ValueError ( "runs needs to be a positive integer" ) if runs <= 0 : raise ValueError ( "runs needs to be a positive integer" ) try : alpha = float ( alpha ) except : raise ValueError ( "alpha needs to be a float in the interval (0, 1)" ) if alpha <= 0.0 or alpha >= 1.0 : raise ValueError ( "alpha needs to be a float in the interval (0, 1)" ) run_iterators = [ sample_states ( graph , spanning_cluster = spanning_cluster , model = model , copy_result = False ) for _ in range ( runs ) ] ret = dict ( ) for microcanonical_ensemble in zip ( * run_iterators ) : ret [ 'n' ] = microcanonical_ensemble [ 0 ] [ 'n' ] ret [ 'N' ] = microcanonical_ensemble [ 0 ] [ 'N' ] ret [ 'M' ] = microcanonical_ensemble [ 0 ] [ 'M' ] max_cluster_size = np . empty ( runs ) moments = np . empty ( ( runs , 5 ) ) if spanning_cluster : has_spanning_cluster = np . empty ( runs ) for r , state in enumerate ( microcanonical_ensemble ) : assert state [ 'n' ] == ret [ 'n' ] assert state [ 'N' ] == ret [ 'N' ] assert state [ 'M' ] == ret [ 'M' ] max_cluster_size [ r ] = state [ 'max_cluster_size' ] moments [ r ] = state [ 'moments' ] if spanning_cluster : has_spanning_cluster [ r ] = state [ 'has_spanning_cluster' ] ret . update ( _microcanonical_average_max_cluster_size ( max_cluster_size , alpha ) ) ret . update ( _microcanonical_average_moments ( moments , alpha ) ) if spanning_cluster : ret . update ( _microcanonical_average_spanning_cluster ( has_spanning_cluster , alpha ) ) if copy_result : yield copy . deepcopy ( ret ) else : yield ret
3496	def reaction_weight ( reaction ) : if len ( reaction . metabolites ) != 1 : raise ValueError ( 'Reaction weight is only defined for single ' 'metabolite products or educts.' ) met , coeff = next ( iteritems ( reaction . metabolites ) ) return [ coeff * met . formula_weight ]
444	def roi_pooling ( input , rois , pool_height , pool_width ) : out = roi_pooling_module . roi_pooling ( input , rois , pool_height = pool_height , pool_width = pool_width ) output , argmax_output = out [ 0 ] , out [ 1 ] return output
719	def queryModelIDs ( self ) : jobID = self . getJobID ( ) modelCounterPairs = _clientJobsDB ( ) . modelsGetUpdateCounters ( jobID ) modelIDs = tuple ( x [ 0 ] for x in modelCounterPairs ) return modelIDs
11089	def _sort_by ( key ) : @ staticmethod def sort_by ( p_list , reverse = False ) : return sorted ( p_list , key = lambda p : getattr ( p , key ) , reverse = reverse , ) return sort_by
11498	def get_community_by_name ( self , name , token = None ) : parameters = dict ( ) parameters [ 'name' ] = name if token : parameters [ 'token' ] = token response = self . request ( 'midas.community.get' , parameters ) return response
11356	def escape_for_xml ( data , tags_to_keep = None ) : data = re . sub ( "&" , "&amp;" , data ) if tags_to_keep : data = re . sub ( r"(<)(?![\/]?({0})\b)" . format ( "|" . join ( tags_to_keep ) ) , '&lt;' , data ) else : data = re . sub ( "<" , "&lt;" , data ) return data
12964	def all ( self , cascadeFetch = False ) : matchedKeys = self . getPrimaryKeys ( ) if matchedKeys : return self . getMultiple ( matchedKeys , cascadeFetch = cascadeFetch ) return IRQueryableList ( [ ] , mdl = self . mdl )
11970	def _wildcard_to_dec ( nm , check = False ) : if check and not is_wildcard_nm ( nm ) : raise ValueError ( '_wildcard_to_dec: invalid netmask: "%s"' % nm ) return 0xFFFFFFFF - _dot_to_dec ( nm , check = False )
6106	def masses_of_galaxies_within_ellipses_in_units ( self , major_axis : dim . Length , unit_mass = 'angular' , critical_surface_density = None ) : return list ( map ( lambda galaxy : galaxy . mass_within_ellipse_in_units ( major_axis = major_axis , unit_mass = unit_mass , kpc_per_arcsec = self . kpc_per_arcsec , critical_surface_density = critical_surface_density ) , self . galaxies ) )
6378	def manhattan ( src , tar , qval = 2 , normalized = False , alphabet = None ) : return Manhattan ( ) . dist_abs ( src , tar , qval , normalized , alphabet )
2164	def list_facts ( self , pk = None , ** kwargs ) : res = self . get ( pk = pk , ** kwargs ) url = self . endpoint + '%d/%s/' % ( res [ 'id' ] , 'ansible_facts' ) return client . get ( url , params = { } ) . json ( )
12649	def is_valid_regex ( string ) : try : re . compile ( string ) is_valid = True except re . error : is_valid = False return is_valid
11984	async def copy_storage_object ( self , source_bucket , source_key , bucket , key ) : info = await self . head_object ( Bucket = source_bucket , Key = source_key ) size = info [ 'ContentLength' ] if size > MULTI_PART_SIZE : result = await _multipart_copy ( self , source_bucket , source_key , bucket , key , size ) else : result = await self . copy_object ( Bucket = bucket , Key = key , CopySource = _source_string ( source_bucket , source_key ) ) return result
12121	def get_data_around ( self , timePoints , thisSweep = False , padding = 0.02 , msDeriv = 0 ) : if not np . array ( timePoints ) . shape : timePoints = [ float ( timePoints ) ] data = None for timePoint in timePoints : if thisSweep : sweep = self . currentSweep else : sweep = int ( timePoint / self . sweepInterval ) timePoint = timePoint - sweep * self . sweepInterval self . setSweep ( sweep ) if msDeriv : dx = int ( msDeriv * self . rate / 1000 ) newData = ( self . dataY [ dx : ] - self . dataY [ : - dx ] ) * self . rate / 1000 / dx else : newData = self . dataY padPoints = int ( padding * self . rate ) pad = np . empty ( padPoints ) * np . nan Ic = timePoint * self . rate newData = np . concatenate ( ( pad , pad , newData , pad , pad ) ) Ic += padPoints * 2 newData = newData [ Ic - padPoints : Ic + padPoints ] newData = newData [ : int ( padPoints * 2 ) ] if data is None : data = [ newData ] else : data = np . vstack ( ( data , newData ) ) return data
9688	def read_bin_boundaries ( self ) : config = [ ] data = { } self . cnxn . xfer ( [ 0x33 ] ) sleep ( 10e-3 ) for i in range ( 30 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) for i in range ( 0 , 14 ) : data [ "Bin Boundary {0}" . format ( i ) ] = self . _16bit_unsigned ( config [ 2 * i ] , config [ 2 * i + 1 ] ) return data
7924	def is_ipv6_available ( ) : try : socket . socket ( socket . AF_INET6 ) . close ( ) except ( socket . error , AttributeError ) : return False return True
11906	def to_permutation_matrix ( matches ) : n = len ( matches ) P = np . zeros ( ( n , n ) ) P [ list ( zip ( * ( matches . items ( ) ) ) ) ] = 1 return P
10486	def _generateFindR ( self , ** kwargs ) : for needle in self . _generateChildrenR ( ) : if needle . _match ( ** kwargs ) : yield needle
9400	def _feval ( self , func_name , func_args = ( ) , dname = '' , nout = 0 , timeout = None , stream_handler = None , store_as = '' , plot_dir = None ) : engine = self . _engine if engine is None : raise Oct2PyError ( 'Session is closed' ) out_file = osp . join ( self . temp_dir , 'writer.mat' ) out_file = out_file . replace ( osp . sep , '/' ) in_file = osp . join ( self . temp_dir , 'reader.mat' ) in_file = in_file . replace ( osp . sep , '/' ) func_args = list ( func_args ) ref_indices = [ ] for ( i , value ) in enumerate ( func_args ) : if isinstance ( value , OctavePtr ) : ref_indices . append ( i + 1 ) func_args [ i ] = value . address ref_indices = np . array ( ref_indices ) req = dict ( func_name = func_name , func_args = tuple ( func_args ) , dname = dname or '' , nout = nout , store_as = store_as or '' , ref_indices = ref_indices ) write_file ( req , out_file , oned_as = self . _oned_as , convert_to_float = self . convert_to_float ) engine . stream_handler = stream_handler or self . logger . info if timeout is None : timeout = self . timeout try : engine . eval ( '_pyeval("%s", "%s");' % ( out_file , in_file ) , timeout = timeout ) except KeyboardInterrupt as e : stream_handler ( engine . repl . interrupt ( ) ) raise except TIMEOUT : stream_handler ( engine . repl . interrupt ( ) ) raise Oct2PyError ( 'Timed out, interrupting' ) except EOF : stream_handler ( engine . repl . child . before ) self . restart ( ) raise Oct2PyError ( 'Session died, restarting' ) resp = read_file ( in_file , self ) if resp [ 'err' ] : msg = self . _parse_error ( resp [ 'err' ] ) raise Oct2PyError ( msg ) result = resp [ 'result' ] . ravel ( ) . tolist ( ) if isinstance ( result , list ) and len ( result ) == 1 : result = result [ 0 ] if ( isinstance ( result , Cell ) and result . size == 1 and isinstance ( result [ 0 ] , string_types ) and result [ 0 ] == '__no_value__' ) : result = None if plot_dir : self . _engine . make_figures ( plot_dir ) return result
481	def main_restore_embedding_layer ( ) : vocabulary_size = 50000 embedding_size = 128 model_file_name = "model_word2vec_50k_128" batch_size = None print ( "Load existing embedding matrix and dictionaries" ) all_var = tl . files . load_npy_to_any ( name = model_file_name + '.npy' ) data = all_var [ 'data' ] count = all_var [ 'count' ] dictionary = all_var [ 'dictionary' ] reverse_dictionary = all_var [ 'reverse_dictionary' ] tl . nlp . save_vocab ( count , name = 'vocab_' + model_file_name + '.txt' ) del all_var , data , count load_params = tl . files . load_npz ( name = model_file_name + '.npz' ) x = tf . placeholder ( tf . int32 , shape = [ batch_size ] ) emb_net = tl . layers . EmbeddingInputlayer ( x , vocabulary_size , embedding_size , name = 'emb' ) sess . run ( tf . global_variables_initializer ( ) ) tl . files . assign_params ( sess , [ load_params [ 0 ] ] , emb_net ) emb_net . print_params ( ) emb_net . print_layers ( ) word = b'hello' word_id = dictionary [ word ] print ( 'word_id:' , word_id ) words = [ b'i' , b'am' , b'tensor' , b'layer' ] word_ids = tl . nlp . words_to_word_ids ( words , dictionary , _UNK ) context = tl . nlp . word_ids_to_words ( word_ids , reverse_dictionary ) print ( 'word_ids:' , word_ids ) print ( 'context:' , context ) vector = sess . run ( emb_net . outputs , feed_dict = { x : [ word_id ] } ) print ( 'vector:' , vector . shape ) vectors = sess . run ( emb_net . outputs , feed_dict = { x : word_ids } ) print ( 'vectors:' , vectors . shape )
7185	def copy_type_comments_to_annotations ( args ) : for arg in args . args : copy_type_comment_to_annotation ( arg ) if args . vararg : copy_type_comment_to_annotation ( args . vararg ) for arg in args . kwonlyargs : copy_type_comment_to_annotation ( arg ) if args . kwarg : copy_type_comment_to_annotation ( args . kwarg )
9299	def apply_filters ( self , query , filters ) : assert isinstance ( query , peewee . Query ) assert isinstance ( filters , dict )
10885	def oslicer ( self , tile ) : mask = None vecs = tile . coords ( form = 'meshed' ) for v in vecs : v [ self . slicer ] = - 1 mask = mask & ( v > 0 ) if mask is not None else ( v > 0 ) return tuple ( np . array ( i ) . astype ( 'int' ) for i in zip ( * [ v [ mask ] for v in vecs ] ) )
9927	def get_user ( self , user_id ) : try : return get_user_model ( ) . objects . get ( id = user_id ) except get_user_model ( ) . DoesNotExist : return None
4943	def get_data_sharing_consent ( username , enterprise_customer_uuid , course_id = None , program_uuid = None ) : EnterpriseCustomer = apps . get_model ( 'enterprise' , 'EnterpriseCustomer' ) try : if course_id : return get_course_data_sharing_consent ( username , course_id , enterprise_customer_uuid ) return get_program_data_sharing_consent ( username , program_uuid , enterprise_customer_uuid ) except EnterpriseCustomer . DoesNotExist : return None
3537	def crazy_egg ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return CrazyEggNode ( )
6891	def _starfeatures_worker ( task ) : try : ( lcfile , outdir , kdtree , objlist , lcflist , neighbor_radius_arcsec , deredden , custom_bandpasses , lcformat , lcformatdir ) = task return get_starfeatures ( lcfile , outdir , kdtree , objlist , lcflist , neighbor_radius_arcsec , deredden = deredden , custom_bandpasses = custom_bandpasses , lcformat = lcformat , lcformatdir = lcformatdir ) except Exception as e : return None
7220	def to_geotiff ( arr , path = './output.tif' , proj = None , spec = None , bands = None , ** kwargs ) : assert has_rasterio , "To create geotiff images please install rasterio" try : img_md = arr . rda . metadata [ "image" ] x_size = img_md [ "tileXSize" ] y_size = img_md [ "tileYSize" ] except ( AttributeError , KeyError ) : x_size = kwargs . get ( "chunk_size" , 256 ) y_size = kwargs . get ( "chunk_size" , 256 ) try : tfm = kwargs [ 'transform' ] if 'transform' in kwargs else arr . affine except : tfm = None dtype = arr . dtype . name if arr . dtype . name != 'int8' else 'uint8' if spec is not None and spec . lower ( ) == 'rgb' : if bands is None : bands = arr . _rgb_bands if not arr . options . get ( 'dra' ) : from gbdxtools . rda . interface import RDA rda = RDA ( ) dra = rda . HistogramDRA ( arr ) arr = dra . aoi ( bbox = arr . bounds ) arr = arr [ bands , ... ] . astype ( np . uint8 ) dtype = 'uint8' else : if bands is not None : arr = arr [ bands , ... ] meta = { 'width' : arr . shape [ 2 ] , 'height' : arr . shape [ 1 ] , 'count' : arr . shape [ 0 ] , 'dtype' : dtype , 'driver' : 'GTiff' , 'transform' : tfm } if proj is not None : meta [ "crs" ] = { 'init' : proj } if "tiled" in kwargs and kwargs [ "tiled" ] : meta . update ( blockxsize = x_size , blockysize = y_size , tiled = "yes" ) with rasterio . open ( path , "w" , ** meta ) as dst : writer = rio_writer ( dst ) result = store ( arr , writer , compute = False ) result . compute ( scheduler = threaded_get ) return path
9429	def extract ( self , member , path = None , pwd = None ) : if isinstance ( member , RarInfo ) : member = member . filename if path is None : path = os . getcwd ( ) self . _extract_members ( [ member ] , path , pwd ) return os . path . join ( path , member )
9779	def whoami ( ) : try : user = PolyaxonClient ( ) . auth . get_user ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load user info.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) click . echo ( "\nUsername: {username}, Email: {email}\n" . format ( ** user . to_dict ( ) ) )
3020	def create_with_claims ( self , claims ) : new_kwargs = dict ( self . _kwargs ) new_kwargs . update ( claims ) result = self . __class__ ( self . _service_account_email , self . _signer , scopes = self . _scopes , private_key_id = self . _private_key_id , client_id = self . client_id , user_agent = self . _user_agent , ** new_kwargs ) result . token_uri = self . token_uri result . revoke_uri = self . revoke_uri result . _private_key_pkcs8_pem = self . _private_key_pkcs8_pem result . _private_key_pkcs12 = self . _private_key_pkcs12 result . _private_key_password = self . _private_key_password return result
11703	def set_gender ( self , gender = None ) : if gender and gender in genders : self . gender = gender else : if not self . chromosomes : self . set_chromosomes ( ) self . gender = npchoice ( genders , 1 , p = p_gender [ self . chromosomes ] ) [ 0 ]
6595	def poll ( self ) : ret = self . communicationChannel . receive_finished ( ) self . nruns -= len ( ret ) return ret
8661	def from_config ( cls , cfg , default_fg = DEFAULT_FG_16 , default_bg = DEFAULT_BG_16 , default_fg_hi = DEFAULT_FG_256 , default_bg_hi = DEFAULT_BG_256 , max_colors = 2 ** 24 ) : e = PaletteEntry ( mono = default_fg , foreground = default_fg , background = default_bg , foreground_high = default_fg_hi , background_high = default_bg_hi ) if isinstance ( cfg , str ) : e . foreground_high = cfg if e . allowed ( cfg , 16 ) : e . foreground = cfg else : rgb = AttrSpec ( fg = cfg , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . foreground = nearest_basic_color ( rgb ) elif isinstance ( cfg , dict ) : bg = cfg . get ( "bg" , None ) if isinstance ( bg , str ) : e . background_high = bg if e . allowed ( bg , 16 ) : e . background = bg else : rgb = AttrSpec ( fg = bg , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . background = nearest_basic_color ( rgb ) elif isinstance ( bg , dict ) : e . background_high = bg . get ( "hi" , default_bg_hi ) if "lo" in bg : if e . allowed ( bg [ "lo" ] , 16 ) : e . background = bg [ "lo" ] else : rgb = AttrSpec ( fg = bg [ "lo" ] , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . background = nearest_basic_color ( rgb ) fg = cfg . get ( "fg" , cfg ) if isinstance ( fg , str ) : e . foreground_high = fg if e . allowed ( fg , 16 ) : e . foreground = fg else : rgb = AttrSpec ( fg = fg , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . foreground = nearest_basic_color ( rgb ) elif isinstance ( fg , dict ) : e . foreground_high = fg . get ( "hi" , default_fg_hi ) if "lo" in fg : if e . allowed ( fg [ "lo" ] , 16 ) : e . foreground = fg [ "lo" ] else : rgb = AttrSpec ( fg = fg [ "lo" ] , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . foreground = nearest_basic_color ( rgb ) return e
660	def percentOutputsStableOverNTimeSteps ( vectors , numSamples = None ) : totalSamples = len ( vectors ) windowSize = numSamples numWindows = 0 pctStable = 0 for wStart in range ( 0 , totalSamples - windowSize + 1 ) : data = vectors [ wStart : wStart + windowSize ] outputSums = data . sum ( axis = 0 ) stableOutputs = ( outputSums == windowSize ) . sum ( ) samplePctStable = float ( stableOutputs ) / data [ 0 ] . sum ( ) print samplePctStable pctStable += samplePctStable numWindows += 1 return float ( pctStable ) / numWindows
7171	def train_subprocess ( self , * args , ** kwargs ) : ret = call ( [ sys . executable , '-m' , 'padatious' , 'train' , self . cache_dir , '-d' , json . dumps ( self . serialized_args ) , '-a' , json . dumps ( args ) , '-k' , json . dumps ( kwargs ) , ] ) if ret == 2 : raise TypeError ( 'Invalid train arguments: {} {}' . format ( args , kwargs ) ) data = self . serialized_args self . clear ( ) self . apply_training_args ( data ) self . padaos . compile ( ) if ret == 0 : self . must_train = False return True elif ret == 10 : return False else : raise ValueError ( 'Training failed and returned code: {}' . format ( ret ) )
5182	def nodes ( self , unreported = 2 , with_status = False , ** kwargs ) : nodes = self . _query ( 'nodes' , ** kwargs ) now = datetime . datetime . utcnow ( ) if type ( nodes ) == dict : nodes = [ nodes , ] if with_status : latest_events = self . event_counts ( query = EqualsOperator ( "latest_report?" , True ) , summarize_by = 'certname' ) for node in nodes : node [ 'status_report' ] = None node [ 'events' ] = None if with_status : status = [ s for s in latest_events if s [ 'subject' ] [ 'title' ] == node [ 'certname' ] ] try : node [ 'status_report' ] = node [ 'latest_report_status' ] if status : node [ 'events' ] = status [ 0 ] except KeyError : if status : node [ 'events' ] = status = status [ 0 ] if status [ 'successes' ] > 0 : node [ 'status_report' ] = 'changed' if status [ 'noops' ] > 0 : node [ 'status_report' ] = 'noop' if status [ 'failures' ] > 0 : node [ 'status_report' ] = 'failed' else : node [ 'status_report' ] = 'unchanged' if node [ 'report_timestamp' ] is not None : try : last_report = json_to_datetime ( node [ 'report_timestamp' ] ) last_report = last_report . replace ( tzinfo = None ) unreported_border = now - timedelta ( hours = unreported ) if last_report < unreported_border : delta = ( now - last_report ) node [ 'unreported' ] = True node [ 'unreported_time' ] = '{0}d {1}h {2}m' . format ( delta . days , int ( delta . seconds / 3600 ) , int ( ( delta . seconds % 3600 ) / 60 ) ) except AttributeError : node [ 'unreported' ] = True if not node [ 'report_timestamp' ] : node [ 'unreported' ] = True yield Node ( self , name = node [ 'certname' ] , deactivated = node [ 'deactivated' ] , expired = node [ 'expired' ] , report_timestamp = node [ 'report_timestamp' ] , catalog_timestamp = node [ 'catalog_timestamp' ] , facts_timestamp = node [ 'facts_timestamp' ] , status_report = node [ 'status_report' ] , noop = node . get ( 'latest_report_noop' ) , noop_pending = node . get ( 'latest_report_noop_pending' ) , events = node [ 'events' ] , unreported = node . get ( 'unreported' ) , unreported_time = node . get ( 'unreported_time' ) , report_environment = node [ 'report_environment' ] , catalog_environment = node [ 'catalog_environment' ] , facts_environment = node [ 'facts_environment' ] , latest_report_hash = node . get ( 'latest_report_hash' ) , cached_catalog_status = node . get ( 'cached_catalog_status' ) )
4918	def course_detail ( self , request , pk , course_key ) : enterprise_customer_catalog = self . get_object ( ) course = enterprise_customer_catalog . get_course ( course_key ) if not course : raise Http404 context = self . get_serializer_context ( ) context [ 'enterprise_customer_catalog' ] = enterprise_customer_catalog serializer = serializers . CourseDetailSerializer ( course , context = context ) return Response ( serializer . data )
6506	def process_result ( cls , dictionary , match_phrase , user ) : result_processor = _load_class ( getattr ( settings , "SEARCH_RESULT_PROCESSOR" , None ) , cls ) srp = result_processor ( dictionary , match_phrase ) if srp . should_remove ( user ) : return None try : srp . add_properties ( ) except Exception as ex : log . exception ( "error processing properties for %s - %s: will remove from results" , json . dumps ( dictionary , cls = DjangoJSONEncoder ) , str ( ex ) ) return None return dictionary
2420	def write_document ( document , out , validate = True ) : messages = [ ] messages = document . validate ( messages ) if validate and messages : raise InvalidDocumentError ( messages ) out . write ( '# Document Information\n\n' ) write_value ( 'SPDXVersion' , str ( document . version ) , out ) write_value ( 'DataLicense' , document . data_license . identifier , out ) write_value ( 'DocumentName' , document . name , out ) write_value ( 'SPDXID' , 'SPDXRef-DOCUMENT' , out ) write_value ( 'DocumentNamespace' , document . namespace , out ) if document . has_comment : write_text_value ( 'DocumentComment' , document . comment , out ) for doc_ref in document . ext_document_references : doc_ref_str = ' ' . join ( [ doc_ref . external_document_id , doc_ref . spdx_document_uri , doc_ref . check_sum . identifier + ':' + doc_ref . check_sum . value ] ) write_value ( 'ExternalDocumentRef' , doc_ref_str , out ) write_separators ( out ) write_creation_info ( document . creation_info , out ) write_separators ( out ) for review in sorted ( document . reviews ) : write_review ( review , out ) write_separators ( out ) for annotation in sorted ( document . annotations ) : write_annotation ( annotation , out ) write_separators ( out ) write_package ( document . package , out ) write_separators ( out ) out . write ( '# Extracted Licenses\n\n' ) for lic in sorted ( document . extracted_licenses ) : write_extracted_licenses ( lic , out ) write_separators ( out )
4523	def color_scale ( color , level ) : return tuple ( [ int ( i * level ) >> 8 for i in list ( color ) ] )
10581	def calculate ( self , ** state ) : super ( ) . calculate ( ** state ) mm_average = 0.0 for compound , molefraction in state [ "x" ] . items ( ) : mm_average += molefraction * mm ( compound ) mm_average /= 1000.0 return mm_average * state [ "P" ] / R / state [ "T" ]
4207	def arcovar ( x , order ) : r from spectrum import corrmtx import scipy . linalg X = corrmtx ( x , order , 'covariance' ) Xc = np . matrix ( X [ : , 1 : ] ) X1 = np . array ( X [ : , 0 ] ) a , _residues , _rank , _singular_values = scipy . linalg . lstsq ( - Xc , X1 ) Cz = np . dot ( X1 . conj ( ) . transpose ( ) , Xc ) e = np . dot ( X1 . conj ( ) . transpose ( ) , X1 ) + np . dot ( Cz , a ) assert e . imag < 1e-4 , 'wierd behaviour' e = float ( e . real ) return a , e
2452	def set_pkg_home ( self , doc , location ) : self . assert_package_exists ( ) if not self . package_home_set : self . package_home_set = True if validations . validate_pkg_homepage ( location ) : doc . package . homepage = location return True else : raise SPDXValueError ( 'Package::HomePage' ) else : raise CardinalityError ( 'Package::HomePage' )
8777	def _chunks ( self , iterable , chunk_size ) : iterator = iter ( iterable ) chunk = list ( itertools . islice ( iterator , 0 , chunk_size ) ) while chunk : yield chunk chunk = list ( itertools . islice ( iterator , 0 , chunk_size ) )
1439	def update_sent_packet ( self , sent_pkt_size_bytes ) : self . update_count ( self . SENT_PKT_COUNT ) self . update_count ( self . SENT_PKT_SIZE , incr_by = sent_pkt_size_bytes )
1880	def PSRLQ ( cpu , dest , src ) : count = src . read ( ) count = Operators . ITEBV ( src . size , Operators . UGT ( count , 63 ) , 64 , count ) count = Operators . EXTRACT ( count , 0 , 64 ) if dest . size == 64 : dest . write ( dest . read ( ) >> count ) else : hi = Operators . EXTRACT ( dest . read ( ) , 64 , 64 ) >> count low = Operators . EXTRACT ( dest . read ( ) , 0 , 64 ) >> count dest . write ( Operators . CONCAT ( 128 , hi , low ) )
1269	def _fly ( self , board , layers , things , the_plot ) : if self . character in the_plot [ 'bunker_hitters' ] : return self . _teleport ( ( - 1 , - 1 ) ) if self . position == things [ 'P' ] . position : the_plot . terminate_episode ( ) self . _south ( board , the_plot )
7328	def _get_base_url ( base_url , api , version ) : format_args = { } if "{api}" in base_url : if api == "" : base_url = base_url . replace ( '{api}.' , '' ) else : format_args [ 'api' ] = api if "{version}" in base_url : if version == "" : base_url = base_url . replace ( '/{version}' , '' ) else : format_args [ 'version' ] = version return base_url . format ( api = api , version = version )
597	def _compute ( self , inputs , outputs ) : if self . _tfdr is None : raise RuntimeError ( "TM has not been initialized" ) self . _conditionalBreak ( ) self . _iterations += 1 buInputVector = inputs [ 'bottomUpIn' ] resetSignal = False if 'resetIn' in inputs : assert len ( inputs [ 'resetIn' ] ) == 1 if inputs [ 'resetIn' ] [ 0 ] != 0 : self . _tfdr . reset ( ) self . _sequencePos = 0 if self . computePredictedActiveCellIndices : prevPredictedState = self . _tfdr . getPredictedState ( ) . reshape ( - 1 ) . astype ( 'float32' ) if self . anomalyMode : prevPredictedColumns = self . _tfdr . topDownCompute ( ) . copy ( ) . nonzero ( ) [ 0 ] tpOutput = self . _tfdr . compute ( buInputVector , self . learningMode , self . inferenceMode ) self . _sequencePos += 1 if self . orColumnOutputs : tpOutput = tpOutput . reshape ( self . columnCount , self . cellsPerColumn ) . max ( axis = 1 ) if self . _fpLogTPOutput : output = tpOutput . reshape ( - 1 ) outputNZ = tpOutput . nonzero ( ) [ 0 ] outStr = " " . join ( [ "%d" % int ( token ) for token in outputNZ ] ) print >> self . _fpLogTPOutput , output . size , outStr outputs [ 'bottomUpOut' ] [ : ] = tpOutput . flat if self . topDownMode : outputs [ 'topDownOut' ] [ : ] = self . _tfdr . topDownCompute ( ) . copy ( ) if self . anomalyMode : activeLearnCells = self . _tfdr . getLearnActiveStateT ( ) size = activeLearnCells . shape [ 0 ] * activeLearnCells . shape [ 1 ] outputs [ 'lrnActiveStateT' ] [ : ] = activeLearnCells . reshape ( size ) activeColumns = buInputVector . nonzero ( ) [ 0 ] outputs [ 'anomalyScore' ] [ : ] = anomaly . computeRawAnomalyScore ( activeColumns , prevPredictedColumns ) if self . computePredictedActiveCellIndices : activeState = self . _tfdr . _getActiveState ( ) . reshape ( - 1 ) . astype ( 'float32' ) activeIndices = numpy . where ( activeState != 0 ) [ 0 ] predictedIndices = numpy . where ( prevPredictedState != 0 ) [ 0 ] predictedActiveIndices = numpy . intersect1d ( activeIndices , predictedIndices ) outputs [ "activeCells" ] . fill ( 0 ) outputs [ "activeCells" ] [ activeIndices ] = 1 outputs [ "predictedActiveCells" ] . fill ( 0 ) outputs [ "predictedActiveCells" ] [ predictedActiveIndices ] = 1
6664	def list_expiration_dates ( self , base = 'roles/all/ssl' ) : max_fn_len = 0 max_date_len = 0 data = [ ] for fn in os . listdir ( base ) : fqfn = os . path . join ( base , fn ) if not os . path . isfile ( fqfn ) : continue if not fn . endswith ( '.crt' ) : continue expiration_date = self . get_expiration_date ( fqfn ) max_fn_len = max ( max_fn_len , len ( fn ) ) max_date_len = max ( max_date_len , len ( str ( expiration_date ) ) ) data . append ( ( fn , expiration_date ) ) print ( '%s %s %s' % ( 'Filename' . ljust ( max_fn_len ) , 'Expiration Date' . ljust ( max_date_len ) , 'Expired' ) ) now = datetime . now ( ) . replace ( tzinfo = pytz . UTC ) for fn , dt in sorted ( data ) : if dt is None : expired = '?' elif dt < now : expired = 'YES' else : expired = 'NO' print ( '%s %s %s' % ( fn . ljust ( max_fn_len ) , str ( dt ) . ljust ( max_date_len ) , expired ) )
3099	def _validate_clientsecrets ( clientsecrets_dict ) : _INVALID_FILE_FORMAT_MSG = ( 'Invalid file format. See ' 'https://developers.google.com/api-client-library/' 'python/guide/aaa_client_secrets' ) if clientsecrets_dict is None : raise InvalidClientSecretsError ( _INVALID_FILE_FORMAT_MSG ) try : ( client_type , client_info ) , = clientsecrets_dict . items ( ) except ( ValueError , AttributeError ) : raise InvalidClientSecretsError ( _INVALID_FILE_FORMAT_MSG + ' ' 'Expected a JSON object with a single property for a "web" or ' '"installed" application' ) if client_type not in VALID_CLIENT : raise InvalidClientSecretsError ( 'Unknown client type: {0}.' . format ( client_type ) ) for prop_name in VALID_CLIENT [ client_type ] [ 'required' ] : if prop_name not in client_info : raise InvalidClientSecretsError ( 'Missing property "{0}" in a client type of "{1}".' . format ( prop_name , client_type ) ) for prop_name in VALID_CLIENT [ client_type ] [ 'string' ] : if client_info [ prop_name ] . startswith ( '[[' ) : raise InvalidClientSecretsError ( 'Property "{0}" is not configured.' . format ( prop_name ) ) return client_type , client_info
7068	def read_fakelc ( fakelcfile ) : try : with open ( fakelcfile , 'rb' ) as infd : lcdict = pickle . load ( infd ) except UnicodeDecodeError : with open ( fakelcfile , 'rb' ) as infd : lcdict = pickle . load ( infd , encoding = 'latin1' ) return lcdict
2836	def read ( self , length , assert_ss = True , deassert_ss = True ) : if self . _miso is None : raise RuntimeError ( 'Read attempted with no MISO pin specified.' ) if assert_ss and self . _ss is not None : self . _gpio . set_low ( self . _ss ) result = bytearray ( length ) for i in range ( length ) : for j in range ( 8 ) : self . _gpio . output ( self . _sclk , not self . _clock_base ) if self . _read_leading : if self . _gpio . is_high ( self . _miso ) : result [ i ] |= self . _read_shift ( self . _mask , j ) else : result [ i ] &= ~ self . _read_shift ( self . _mask , j ) self . _gpio . output ( self . _sclk , self . _clock_base ) if not self . _read_leading : if self . _gpio . is_high ( self . _miso ) : result [ i ] |= self . _read_shift ( self . _mask , j ) else : result [ i ] &= ~ self . _read_shift ( self . _mask , j ) if deassert_ss and self . _ss is not None : self . _gpio . set_high ( self . _ss ) return result
5122	def set_transitions ( self , mat ) : if isinstance ( mat , dict ) : for key , value in mat . items ( ) : probs = list ( value . values ( ) ) if key not in self . g . node : msg = "One of the keys don't correspond to a vertex." raise ValueError ( msg ) elif len ( self . out_edges [ key ] ) > 0 and not np . isclose ( sum ( probs ) , 1 ) : msg = "Sum of transition probabilities at a vertex was not 1." raise ValueError ( msg ) elif ( np . array ( probs ) < 0 ) . any ( ) : msg = "Some transition probabilities were negative." raise ValueError ( msg ) for k , e in enumerate ( sorted ( self . g . out_edges ( key ) ) ) : self . _route_probs [ key ] [ k ] = value . get ( e [ 1 ] , 0 ) elif isinstance ( mat , np . ndarray ) : non_terminal = np . array ( [ self . g . out_degree ( v ) > 0 for v in self . g . nodes ( ) ] ) if mat . shape != ( self . nV , self . nV ) : msg = ( "Matrix is the wrong shape, should " "be {0} x {1}." ) . format ( self . nV , self . nV ) raise ValueError ( msg ) elif not np . allclose ( np . sum ( mat [ non_terminal , : ] , axis = 1 ) , 1 ) : msg = "Sum of transition probabilities at a vertex was not 1." raise ValueError ( msg ) elif ( mat < 0 ) . any ( ) : raise ValueError ( "Some transition probabilities were negative." ) for k in range ( self . nV ) : for j , e in enumerate ( sorted ( self . g . out_edges ( k ) ) ) : self . _route_probs [ k ] [ j ] = mat [ k , e [ 1 ] ] else : raise TypeError ( "mat must be a numpy array or a dict." )
7689	def piano_roll ( annotation , sr = 22050 , length = None , ** kwargs ) : intervals , pitches = annotation . to_interval_values ( ) pitch_map = { f : idx for idx , f in enumerate ( np . unique ( pitches ) ) } gram = np . zeros ( ( len ( pitch_map ) , len ( intervals ) ) ) for col , f in enumerate ( pitches ) : gram [ pitch_map [ f ] , col ] = 1 return filter_kwargs ( mir_eval . sonify . time_frequency , gram , pitches , intervals , sr , length = length , ** kwargs )
10206	def run ( self , start_date = None , end_date = None , ** kwargs ) : start_date = self . extract_date ( start_date ) if start_date else None end_date = self . extract_date ( end_date ) if end_date else None self . validate_arguments ( start_date , end_date , ** kwargs ) agg_query = self . build_query ( start_date , end_date , ** kwargs ) query_result = agg_query . execute ( ) . to_dict ( ) res = self . process_query_result ( query_result , start_date , end_date ) return res
8	def observation_input ( ob_space , batch_size = None , name = 'Ob' ) : placeholder = observation_placeholder ( ob_space , batch_size , name ) return placeholder , encode_observation ( ob_space , placeholder )
9231	def fetch_events_async ( self , issues , tag_name ) : if not issues : return issues max_simultaneous_requests = self . options . max_simultaneous_requests verbose = self . options . verbose gh = self . github user = self . options . user repo = self . options . project self . events_cnt = 0 if verbose : print ( "fetching events for {} {}... " . format ( len ( issues ) , tag_name ) ) def worker ( issue ) : page = 1 issue [ 'events' ] = [ ] while page > 0 : rc , data = gh . repos [ user ] [ repo ] . issues [ issue [ 'number' ] ] . events . get ( page = page , per_page = PER_PAGE_NUMBER ) if rc == 200 : issue [ 'events' ] . extend ( data ) self . events_cnt += len ( data ) else : self . raise_GitHubError ( rc , data , gh . getheaders ( ) ) page = NextPage ( gh ) threads = [ ] cnt = len ( issues ) for i in range ( 0 , ( cnt // max_simultaneous_requests ) + 1 ) : for j in range ( max_simultaneous_requests ) : idx = i * max_simultaneous_requests + j if idx == cnt : break t = threading . Thread ( target = worker , args = ( issues [ idx ] , ) ) threads . append ( t ) t . start ( ) if verbose > 2 : print ( "." , end = "" ) if not idx % PER_PAGE_NUMBER : print ( "" ) for t in threads : t . join ( ) if verbose > 2 : print ( "." )
7240	def window_cover ( self , window_shape , pad = True ) : size_y , size_x = window_shape [ 0 ] , window_shape [ 1 ] _ndepth , _nheight , _nwidth = self . shape nheight , _m = divmod ( _nheight , size_y ) nwidth , _n = divmod ( _nwidth , size_x ) img = self if pad is True : new_height , new_width = _nheight , _nwidth if _m != 0 : new_height = ( nheight + 1 ) * size_y if _n != 0 : new_width = ( nwidth + 1 ) * size_x if ( new_height , new_width ) != ( _nheight , _nwidth ) : bounds = box ( 0 , 0 , new_width , new_height ) geom = ops . transform ( self . __geo_transform__ . fwd , bounds ) img = self [ geom ] row_lims = range ( 0 , img . shape [ 1 ] , size_y ) col_lims = range ( 0 , img . shape [ 2 ] , size_x ) for maxy , maxx in product ( row_lims , col_lims ) : reg = img [ : , maxy : ( maxy + size_y ) , maxx : ( maxx + size_x ) ] if pad is False : if reg . shape [ 1 : ] == window_shape : yield reg else : yield reg
11114	def remove_repository ( self , path = None , relatedFiles = False , relatedFolders = False , verbose = True ) : if path is not None : realPath = os . path . realpath ( os . path . expanduser ( path ) ) else : realPath = self . __path if realPath is None : if verbose : warnings . warn ( 'path is None and current Repository is not initialized!' ) return if not self . is_repository ( realPath ) : if verbose : warnings . warn ( "No repository found in '%s'!" % realPath ) return if realPath == os . path . realpath ( '/..' ) : if verbose : warnings . warn ( 'You are about to wipe out your system !!! action aboarded' ) return if path is not None : repo = Repository ( ) repo . load_repository ( realPath ) else : repo = self if relatedFiles : for relativePath in repo . walk_files_relative_path ( ) : realPath = os . path . join ( repo . path , relativePath ) if not os . path . isfile ( realPath ) : continue if not os . path . exists ( realPath ) : continue os . remove ( realPath ) if relatedFolders : for relativePath in reversed ( list ( repo . walk_directories_relative_path ( ) ) ) : realPath = os . path . join ( repo . path , relativePath ) if not os . path . isdir ( realPath ) : continue if not os . path . exists ( realPath ) : continue if not len ( os . listdir ( realPath ) ) : os . rmdir ( realPath ) os . remove ( os . path . join ( repo . path , ".pyrepinfo" ) ) for fname in ( ".pyrepstate" , ".pyreplock" ) : p = os . path . join ( repo . path , fname ) if os . path . exists ( p ) : os . remove ( p ) if os . path . isdir ( repo . path ) : if not len ( os . listdir ( repo . path ) ) : os . rmdir ( repo . path ) repo . __reset_repository ( )
6580	def _ensure_started ( self ) : if self . _process and self . _process . poll ( ) is None : return if not getattr ( self , "_cmd" ) : raise RuntimeError ( "Player command is not configured" ) log . debug ( "Starting playback command: %r" , self . _cmd ) self . _process = SilentPopen ( self . _cmd ) self . _post_start ( )
2115	def status ( self , pk = None , detail = False , ** kwargs ) : job = self . last_job_data ( pk , ** kwargs ) if detail : return job return { 'elapsed' : job [ 'elapsed' ] , 'failed' : job [ 'failed' ] , 'status' : job [ 'status' ] , }
12169	def _dispatch ( self , event , listener , * args , ** kwargs ) : if ( asyncio . iscoroutinefunction ( listener ) or isinstance ( listener , functools . partial ) and asyncio . iscoroutinefunction ( listener . func ) ) : return self . _dispatch_coroutine ( event , listener , * args , ** kwargs ) return self . _dispatch_function ( event , listener , * args , ** kwargs )
1330	def predictions ( self , image , strict = True , return_details = False ) : in_bounds = self . in_bounds ( image ) assert not strict or in_bounds self . _total_prediction_calls += 1 predictions = self . __model . predictions ( image ) is_adversarial , is_best , distance = self . __is_adversarial ( image , predictions , in_bounds ) assert predictions . ndim == 1 if return_details : return predictions , is_adversarial , is_best , distance else : return predictions , is_adversarial
8316	def parse_images ( self , markup , treshold = 6 ) : images = [ ] m = re . findall ( self . re [ "image" ] , markup ) for p in m : p = self . parse_balanced_image ( p ) img = p . split ( "|" ) path = img [ 0 ] . replace ( "[[Image:" , "" ) . strip ( ) description = u"" links = { } properties = [ ] if len ( img ) > 1 : img = "|" . join ( img [ 1 : ] ) links = self . parse_links ( img ) properties = self . plain ( img ) . split ( "|" ) description = u"" if len ( properties [ - 1 ] ) > treshold : description = properties [ - 1 ] properties = properties [ : - 1 ] img = WikipediaImage ( path , description , links , properties ) images . append ( img ) markup = markup . replace ( p , "" ) return images , markup . strip ( )
5815	def _read_callback ( connection_id , data_buffer , data_length_pointer ) : self = None try : self = _connection_refs . get ( connection_id ) if not self : socket = _socket_refs . get ( connection_id ) else : socket = self . _socket if not self and not socket : return 0 bytes_requested = deref ( data_length_pointer ) timeout = socket . gettimeout ( ) error = None data = b'' try : while len ( data ) < bytes_requested : if timeout is not None and timeout > 0.0 : read_ready , _ , _ = select . select ( [ socket ] , [ ] , [ ] , timeout ) if len ( read_ready ) == 0 : raise socket_ . error ( errno . EAGAIN , 'timed out' ) chunk = socket . recv ( bytes_requested - len ( data ) ) data += chunk if chunk == b'' : if len ( data ) == 0 : if timeout is None : return SecurityConst . errSSLClosedNoNotify return SecurityConst . errSSLClosedAbort break except ( socket_ . error ) as e : error = e . errno if error is not None and error != errno . EAGAIN : if error == errno . ECONNRESET or error == errno . EPIPE : return SecurityConst . errSSLClosedNoNotify return SecurityConst . errSSLClosedAbort if self and not self . _done_handshake : if len ( data ) >= 3 and len ( self . _server_hello ) == 0 : valid_record_type = data [ 0 : 1 ] in set ( [ b'\x15' , b'\x16' ] ) valid_protocol_version = data [ 1 : 3 ] in set ( [ b'\x03\x00' , b'\x03\x01' , b'\x03\x02' , b'\x03\x03' , b'\x03\x04' ] ) if not valid_record_type or not valid_protocol_version : self . _server_hello += data + _read_remaining ( socket ) return SecurityConst . errSSLProtocol self . _server_hello += data write_to_buffer ( data_buffer , data ) pointer_set ( data_length_pointer , len ( data ) ) if len ( data ) != bytes_requested : return SecurityConst . errSSLWouldBlock return 0 except ( KeyboardInterrupt ) as e : if self : self . _exception = e return SecurityConst . errSSLClosedAbort
1695	def filter ( self , filter_function ) : from heronpy . streamlet . impl . filterbolt import FilterStreamlet filter_streamlet = FilterStreamlet ( filter_function , self ) self . _add_child ( filter_streamlet ) return filter_streamlet
13754	def read_from_file ( file_path , encoding = "utf-8" ) : with codecs . open ( file_path , "r" , encoding ) as f : return f . read ( )
7809	def from_ssl_socket ( cls , ssl_socket ) : try : data = ssl_socket . getpeercert ( True ) except AttributeError : data = None if not data : logger . debug ( "No certificate infromation" ) return cls ( ) result = cls . from_der_data ( data ) result . validated = bool ( ssl_socket . getpeercert ( ) ) return result
11917	def render ( template , ** data ) : try : return renderer . render ( template , ** data ) except JinjaTemplateNotFound as e : logger . error ( e . __doc__ + ', Template: %r' % template ) sys . exit ( e . exit_code )
13597	def get_state ( self ) : return [ os . path . join ( dp , f ) for dp , _ , fn in os . walk ( self . dir ) for f in fn ]
5924	def setup ( filename = CONFIGNAME ) : get_configuration ( ) if not os . path . exists ( filename ) : with open ( filename , 'w' ) as configfile : cfg . write ( configfile ) msg = "NOTE: GromacsWrapper created the configuration file \n\t%r\n" " for you. Edit the file to customize the package." % filename print ( msg ) for d in config_directories : utilities . mkdir_p ( d )
8143	def rotate ( self , angle ) : from math import sqrt , pow , sin , cos , degrees , radians , asin w0 , h0 = self . img . size d = sqrt ( pow ( w0 , 2 ) + pow ( h0 , 2 ) ) d_angle = degrees ( asin ( ( w0 * 0.5 ) / ( d * 0.5 ) ) ) angle = angle % 360 if angle > 90 and angle <= 270 : d_angle += 180 w = sin ( radians ( d_angle + angle ) ) * d w = max ( w , sin ( radians ( d_angle - angle ) ) * d ) w = int ( abs ( w ) ) h = cos ( radians ( d_angle + angle ) ) * d h = max ( h , cos ( radians ( d_angle - angle ) ) * d ) h = int ( abs ( h ) ) dx = int ( ( w - w0 ) / 2 ) dy = int ( ( h - h0 ) / 2 ) d = int ( d ) bg = ImageStat . Stat ( self . img ) . mean bg = ( int ( bg [ 0 ] ) , int ( bg [ 1 ] ) , int ( bg [ 2 ] ) , 0 ) box = Image . new ( "RGBA" , ( d , d ) , bg ) box . paste ( self . img , ( ( d - w0 ) / 2 , ( d - h0 ) / 2 ) ) box = box . rotate ( angle , INTERPOLATION ) box = box . crop ( ( ( d - w ) / 2 + 2 , ( d - h ) / 2 , d - ( d - w ) / 2 , d - ( d - h ) / 2 ) ) self . img = box self . x += ( self . w - w ) / 2 self . y += ( self . h - h ) / 2 self . w = w self . h = h
230	def plot_style_factor_exposures ( tot_style_factor_exposure , factor_name = None , ax = None ) : if ax is None : ax = plt . gca ( ) if factor_name is None : factor_name = tot_style_factor_exposure . name ax . plot ( tot_style_factor_exposure . index , tot_style_factor_exposure , label = factor_name ) avg = tot_style_factor_exposure . mean ( ) ax . axhline ( avg , linestyle = '-.' , label = 'Mean = {:.3}' . format ( avg ) ) ax . axhline ( 0 , color = 'k' , linestyle = '-' ) _ , _ , y1 , y2 = plt . axis ( ) lim = max ( abs ( y1 ) , abs ( y2 ) ) ax . set ( title = 'Exposure to {}' . format ( factor_name ) , ylabel = '{} \n weighted exposure' . format ( factor_name ) , ylim = ( - lim , lim ) ) ax . legend ( frameon = True , framealpha = 0.5 ) return ax
9740	def get_2d_markers_linearized ( self , component_info = None , data = None , component_position = None , index = None ) : return self . _get_2d_markers ( data , component_info , component_position , index = index )
378	def samplewise_norm ( x , rescale = None , samplewise_center = False , samplewise_std_normalization = False , channel_index = 2 , epsilon = 1e-7 ) : if rescale : x *= rescale if x . shape [ channel_index ] == 1 : if samplewise_center : x = x - np . mean ( x ) if samplewise_std_normalization : x = x / np . std ( x ) return x elif x . shape [ channel_index ] == 3 : if samplewise_center : x = x - np . mean ( x , axis = channel_index , keepdims = True ) if samplewise_std_normalization : x = x / ( np . std ( x , axis = channel_index , keepdims = True ) + epsilon ) return x else : raise Exception ( "Unsupported channels %d" % x . shape [ channel_index ] )
11051	def listen_events ( self , reconnects = 0 ) : self . log . info ( 'Listening for events from Marathon...' ) self . _attached = False def on_finished ( result , reconnects ) : self . log . warn ( 'Connection lost listening for events, ' 'reconnecting... ({reconnects} so far)' , reconnects = reconnects ) reconnects += 1 return self . listen_events ( reconnects ) def log_failure ( failure ) : self . log . failure ( 'Failed to listen for events' , failure ) return failure return self . marathon_client . get_events ( { 'event_stream_attached' : self . _sync_on_event_stream_attached , 'api_post_event' : self . _sync_on_api_post_event } ) . addCallbacks ( on_finished , log_failure , callbackArgs = [ reconnects ] )
1698	def union ( self , other_streamlet ) : from heronpy . streamlet . impl . unionbolt import UnionStreamlet union_streamlet = UnionStreamlet ( self , other_streamlet ) self . _add_child ( union_streamlet ) other_streamlet . _add_child ( union_streamlet ) return union_streamlet
8943	def search_file_upwards ( name , base = None ) : base = base or os . getcwd ( ) while base != os . path . dirname ( base ) : if os . path . exists ( os . path . join ( base , name ) ) : return base base = os . path . dirname ( base ) return None
3725	def dipole_moment ( CASRN , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in _dipole_CCDB . index and not np . isnan ( _dipole_CCDB . at [ CASRN , 'Dipole' ] ) : methods . append ( CCCBDB ) if CASRN in _dipole_Muller . index and not np . isnan ( _dipole_Muller . at [ CASRN , 'Dipole' ] ) : methods . append ( MULLER ) if CASRN in _dipole_Poling . index and not np . isnan ( _dipole_Poling . at [ CASRN , 'Dipole' ] ) : methods . append ( POLING ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == CCCBDB : _dipole = float ( _dipole_CCDB . at [ CASRN , 'Dipole' ] ) elif Method == MULLER : _dipole = float ( _dipole_Muller . at [ CASRN , 'Dipole' ] ) elif Method == POLING : _dipole = float ( _dipole_Poling . at [ CASRN , 'Dipole' ] ) elif Method == NONE : _dipole = None else : raise Exception ( 'Failure in in function' ) return _dipole
12630	def compose_err_msg ( msg , ** kwargs ) : updated_msg = msg for k , v in sorted ( kwargs . items ( ) ) : if isinstance ( v , _basestring ) : updated_msg += "\n" + k + ": " + v return updated_msg
10425	def infer_missing_backwards_edge ( graph , u , v , k ) : if u in graph [ v ] : for attr_dict in graph [ v ] [ u ] . values ( ) : if attr_dict == graph [ u ] [ v ] [ k ] : return graph . add_edge ( v , u , key = k , ** graph [ u ] [ v ] [ k ] )
12077	def figure ( self , forceNew = False ) : if plt . _pylab_helpers . Gcf . get_num_fig_managers ( ) > 0 and forceNew is False : self . log . debug ( "figure already seen, not creating one." ) return if self . subplot : self . log . debug ( "subplot mode enabled, not creating new figure" ) else : self . log . debug ( "creating new figure" ) plt . figure ( figsize = ( self . figure_width , self . figure_height ) )
10762	def _wait_for_connection ( self , port ) : connected = False max_tries = 10 num_tries = 0 wait_time = 0.5 while not connected or num_tries >= max_tries : time . sleep ( wait_time ) try : af = socket . AF_INET addr = ( '127.0.0.1' , port ) sock = socket . socket ( af , socket . SOCK_STREAM ) sock . connect ( addr ) except socket . error : if sock : sock . close ( ) num_tries += 1 continue connected = True if not connected : print ( "Error connecting to sphinx searchd" , file = sys . stderr )
4668	def encrypt ( privkey , passphrase ) : if isinstance ( privkey , str ) : privkey = PrivateKey ( privkey ) else : privkey = PrivateKey ( repr ( privkey ) ) privkeyhex = repr ( privkey ) addr = format ( privkey . bitcoin . address , "BTC" ) a = _bytes ( addr ) salt = hashlib . sha256 ( hashlib . sha256 ( a ) . digest ( ) ) . digest ( ) [ 0 : 4 ] if SCRYPT_MODULE == "scrypt" : key = scrypt . hash ( passphrase , salt , 16384 , 8 , 8 ) elif SCRYPT_MODULE == "pylibscrypt" : key = scrypt . scrypt ( bytes ( passphrase , "utf-8" ) , salt , 16384 , 8 , 8 ) else : raise ValueError ( "No scrypt module loaded" ) ( derived_half1 , derived_half2 ) = ( key [ : 32 ] , key [ 32 : ] ) aes = AES . new ( derived_half2 , AES . MODE_ECB ) encrypted_half1 = _encrypt_xor ( privkeyhex [ : 32 ] , derived_half1 [ : 16 ] , aes ) encrypted_half2 = _encrypt_xor ( privkeyhex [ 32 : ] , derived_half1 [ 16 : ] , aes ) " flag byte is forced 0xc0 because Graphene only uses compressed keys " payload = b"\x01" + b"\x42" + b"\xc0" + salt + encrypted_half1 + encrypted_half2 " Checksum " checksum = hashlib . sha256 ( hashlib . sha256 ( payload ) . digest ( ) ) . digest ( ) [ : 4 ] privatkey = hexlify ( payload + checksum ) . decode ( "ascii" ) return Base58 ( privatkey )
5520	def check_codes ( self , expected_codes , received_code , info ) : if not any ( map ( received_code . matches , expected_codes ) ) : raise errors . StatusCodeError ( expected_codes , received_code , info )
4199	def identify_names ( code ) : finder = NameFinder ( ) finder . visit ( ast . parse ( code ) ) example_code_obj = { } for name , full_name in finder . get_mapping ( ) : module , attribute = full_name . rsplit ( '.' , 1 ) module_short = get_short_module_name ( module , attribute ) cobj = { 'name' : attribute , 'module' : module , 'module_short' : module_short } example_code_obj [ name ] = cobj return example_code_obj
3021	def get_access_token ( self , http = None , additional_claims = None ) : if additional_claims is None : if self . access_token is None or self . access_token_expired : self . refresh ( None ) return client . AccessTokenInfo ( access_token = self . access_token , expires_in = self . _expires_in ( ) ) else : token , unused_expiry = self . _create_token ( additional_claims ) return client . AccessTokenInfo ( access_token = token , expires_in = self . _MAX_TOKEN_LIFETIME_SECS )
1819	def SETO ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . OF , 1 , 0 ) )
3378	def assert_optimal ( model , message = 'optimization failed' ) : status = model . solver . status if status != OPTIMAL : exception_cls = OPTLANG_TO_EXCEPTIONS_DICT . get ( status , OptimizationError ) raise exception_cls ( "{} ({})" . format ( message , status ) )
5799	def walk_ast ( node , code_lines , sections , md_chunks ) : if isinstance ( node , _ast . FunctionDef ) : key = ( 'function' , node . name ) if key not in sections : return docstring = ast . get_docstring ( node ) def_lineno = node . lineno + len ( node . decorator_list ) definition , description_md = _get_func_info ( docstring , def_lineno , code_lines , '> ' ) md_chunk = textwrap . dedent ( ) . strip ( ) % ( node . name , definition , description_md ) + "\n" md_chunks [ key ] = md_chunk . replace ( '>\n\n' , '' ) elif isinstance ( node , _ast . ClassDef ) : if ( 'class' , node . name ) not in sections : return for subnode in node . body : if isinstance ( subnode , _ast . FunctionDef ) : node_id = node . name + '.' + subnode . name method_key = ( 'method' , node_id ) is_method = method_key in sections attribute_key = ( 'attribute' , node_id ) is_attribute = attribute_key in sections is_constructor = subnode . name == '__init__' if not is_constructor and not is_attribute and not is_method : continue docstring = ast . get_docstring ( subnode ) def_lineno = subnode . lineno + len ( subnode . decorator_list ) if not docstring : continue if is_method or is_constructor : definition , description_md = _get_func_info ( docstring , def_lineno , code_lines , '> > ' ) if is_constructor : key = ( 'class' , node . name ) class_docstring = ast . get_docstring ( node ) or '' class_description = textwrap . dedent ( class_docstring ) . strip ( ) if class_description : class_description_md = "> %s\n>" % ( class_description . replace ( "\n" , "\n> " ) ) else : class_description_md = '' md_chunk = textwrap . dedent ( ) . strip ( ) % ( node . name , class_description_md , definition , description_md ) md_chunk = md_chunk . replace ( '\n\n\n' , '\n\n' ) else : key = method_key md_chunk = textwrap . dedent ( ) . strip ( ) % ( subnode . name , definition , description_md ) if md_chunk [ - 5 : ] == '\n> >\n' : md_chunk = md_chunk [ 0 : - 5 ] else : key = attribute_key description = textwrap . dedent ( docstring ) . strip ( ) description_md = "> > %s" % ( description . replace ( "\n" , "\n> > " ) ) md_chunk = textwrap . dedent ( ) . strip ( ) % ( subnode . name , description_md ) md_chunks [ key ] = re . sub ( '[ \\t]+\n' , '\n' , md_chunk . rstrip ( ) ) elif isinstance ( node , _ast . If ) : for subast in node . body : walk_ast ( subast , code_lines , sections , md_chunks ) for subast in node . orelse : walk_ast ( subast , code_lines , sections , md_chunks )
10448	def click ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) size = self . _getobjectsize ( object_handle ) self . _grabfocus ( object_handle ) self . wait ( 0.5 ) self . generatemouseevent ( size [ 0 ] + size [ 2 ] / 2 , size [ 1 ] + size [ 3 ] / 2 , "b1c" ) return 1
1504	def template_apiserver_hcl ( cl_args , masters , zookeepers ) : single_master = masters [ 0 ] apiserver_config_template = "%s/standalone/templates/apiserver.template.hcl" % cl_args [ "config_path" ] apiserver_config_actual = "%s/standalone/resources/apiserver.hcl" % cl_args [ "config_path" ] replacements = { "<heron_apiserver_hostname>" : '"%s"' % get_hostname ( single_master , cl_args ) , "<heron_apiserver_executable>" : '"%s/heron-apiserver"' % config . get_heron_bin_dir ( ) if is_self ( single_master ) else '"%s/.heron/bin/heron-apiserver"' % get_remote_home ( single_master , cl_args ) , "<zookeeper_host:zookeeper_port>" : "," . join ( [ '%s' % zk if ":" in zk else '%s:2181' % zk for zk in zookeepers ] ) , "<scheduler_uri>" : "http://%s:4646" % single_master } template_file ( apiserver_config_template , apiserver_config_actual , replacements )
9872	def start_response ( self , status , response_headers , exc_info = None ) : if exc_info : try : if self . headers_sent : raise finally : exc_info = None elif self . header_set : raise AssertionError ( "Headers already set!" ) if PY3K and not isinstance ( status , str ) : self . status = str ( status , 'ISO-8859-1' ) else : self . status = status try : self . header_set = Headers ( response_headers ) except UnicodeDecodeError : self . error = ( '500 Internal Server Error' , 'HTTP Headers should be bytes' ) self . err_log . error ( 'Received HTTP Headers from client that contain' ' invalid characters for Latin-1 encoding.' ) return self . write_warning
68	def copy ( self , x1 = None , y1 = None , x2 = None , y2 = None , label = None ) : return BoundingBox ( x1 = self . x1 if x1 is None else x1 , x2 = self . x2 if x2 is None else x2 , y1 = self . y1 if y1 is None else y1 , y2 = self . y2 if y2 is None else y2 , label = self . label if label is None else label )
13864	def tsms ( when , tz = None ) : if not when : return None when = totz ( when , tz ) return calendar . timegm ( when . timetuple ( ) ) * 1000 + int ( round ( when . microsecond / 1000.0 ) )
164	def compute_pointwise_distances ( self , other , default = None ) : import shapely . geometry from . kps import Keypoint if isinstance ( other , Keypoint ) : other = shapely . geometry . Point ( ( other . x , other . y ) ) elif isinstance ( other , LineString ) : if len ( other . coords ) == 0 : return default elif len ( other . coords ) == 1 : other = shapely . geometry . Point ( other . coords [ 0 , : ] ) else : other = shapely . geometry . LineString ( other . coords ) elif isinstance ( other , tuple ) : assert len ( other ) == 2 other = shapely . geometry . Point ( other ) else : raise ValueError ( ( "Expected Keypoint or LineString or tuple (x,y), " + "got type %s." ) % ( type ( other ) , ) ) return [ shapely . geometry . Point ( point ) . distance ( other ) for point in self . coords ]
7959	def handle_hup ( self ) : with self . lock : if self . _state == 'connecting' and self . _dst_addrs : self . _hup = False self . _set_state ( "connect" ) return self . _hup = True
5743	def update_running_pids ( old_procs ) : new_procs = [ ] for proc in old_procs : if proc . poll ( ) is None and check_pid ( proc . pid ) : publisher . debug ( str ( proc . pid ) + ' is alive' ) new_procs . append ( proc ) else : try : publisher . debug ( str ( proc . pid ) + ' is gone' ) os . kill ( proc . pid , signal . SIGKILL ) except : pass return new_procs
8801	def run_migrations_online ( ) : engine = create_engine ( neutron_config . database . connection , poolclass = pool . NullPool ) connection = engine . connect ( ) context . configure ( connection = connection , target_metadata = target_metadata ) try : with context . begin_transaction ( ) : context . run_migrations ( ) finally : connection . close ( )
2908	def _find_child_of ( self , parent_task_spec ) : if self . parent is None : return self if self . parent . task_spec == parent_task_spec : return self return self . parent . _find_child_of ( parent_task_spec )
11444	def parse ( self , path_to_xml = None ) : if not path_to_xml : if not self . path : self . logger . error ( "No path defined!" ) return path_to_xml = self . path root = self . _clean_xml ( path_to_xml ) if root . tag . lower ( ) == 'collection' : tree = ET . ElementTree ( root ) self . records = element_tree_collection_to_records ( tree ) elif root . tag . lower ( ) == 'record' : new_root = ET . Element ( 'collection' ) new_root . append ( root ) tree = ET . ElementTree ( new_root ) self . records = element_tree_collection_to_records ( tree ) else : header_subs = get_request_subfields ( root ) records = root . find ( 'ListRecords' ) if records is None : records = root . find ( 'GetRecord' ) if records is None : raise ValueError ( "Cannot find ListRecords or GetRecord!" ) tree = ET . ElementTree ( records ) for record , is_deleted in element_tree_oai_records ( tree , header_subs ) : if is_deleted : self . deleted_records . append ( self . create_deleted_record ( record ) ) else : self . records . append ( record )
13665	def update_item ( filename , item , uuid ) : with atomic_write ( os . fsencode ( str ( filename ) ) ) as temp_file : with open ( os . fsencode ( str ( filename ) ) ) as products_file : products_data = json . load ( products_file ) if 'products' in products_data [ - 1 ] : [ products_data [ i ] [ "products" ] [ 0 ] . update ( item ) for ( i , j ) in enumerate ( products_data ) if j [ "uuid" ] == str ( uuid ) ] else : [ products_data [ i ] . update ( item ) for ( i , j ) in enumerate ( products_data ) if j [ "uuid" ] == str ( uuid ) ] json . dump ( products_data , temp_file ) return True
13405	def prepareImages ( self , fileName , logType ) : import subprocess if self . imageType == "png" : self . imagePixmap . save ( fileName + ".png" , "PNG" , - 1 ) if logType == "Physics" : makePostScript = "convert " + fileName + ".png " + fileName + ".ps" process = subprocess . Popen ( makePostScript , shell = True ) process . wait ( ) thumbnailPixmap = self . imagePixmap . scaled ( 500 , 450 , Qt . KeepAspectRatio ) thumbnailPixmap . save ( fileName + ".png" , "PNG" , - 1 ) else : renameImage = "cp " + self . image + " " + fileName + ".gif" process = subprocess . Popen ( renameImage , shell = True ) process . wait ( ) if logType == "Physics" : thumbnailPixmap = self . imagePixmap . scaled ( 500 , 450 , Qt . KeepAspectRatio ) thumbnailPixmap . save ( fileName + ".png" , "PNG" , - 1 )
6952	def aov_theta ( times , mags , errs , frequency , binsize = 0.05 , minbin = 9 ) : period = 1.0 / frequency fold_time = times [ 0 ] phased = phase_magseries ( times , mags , period , fold_time , wrap = False , sort = True ) phases = phased [ 'phase' ] pmags = phased [ 'mags' ] bins = nparange ( 0.0 , 1.0 , binsize ) ndets = phases . size binnedphaseinds = npdigitize ( phases , bins ) bin_s1_tops = [ ] bin_s2_tops = [ ] binndets = [ ] goodbins = 0 all_xbar = npmedian ( pmags ) for x in npunique ( binnedphaseinds ) : thisbin_inds = binnedphaseinds == x thisbin_mags = pmags [ thisbin_inds ] if thisbin_mags . size > minbin : thisbin_ndet = thisbin_mags . size thisbin_xbar = npmedian ( thisbin_mags ) thisbin_s1_top = ( thisbin_ndet * ( thisbin_xbar - all_xbar ) * ( thisbin_xbar - all_xbar ) ) thisbin_s2_top = npsum ( ( thisbin_mags - all_xbar ) * ( thisbin_mags - all_xbar ) ) bin_s1_tops . append ( thisbin_s1_top ) bin_s2_tops . append ( thisbin_s2_top ) binndets . append ( thisbin_ndet ) goodbins = goodbins + 1 bin_s1_tops = nparray ( bin_s1_tops ) bin_s2_tops = nparray ( bin_s2_tops ) binndets = nparray ( binndets ) s1 = npsum ( bin_s1_tops ) / ( goodbins - 1.0 ) s2 = npsum ( bin_s2_tops ) / ( ndets - goodbins ) theta_aov = s1 / s2 return theta_aov
3629	def horiz_div ( col_widths , horiz , vert , padding ) : horizs = [ horiz * w for w in col_widths ] div = '' . join ( [ padding * horiz , vert , padding * horiz ] ) return div . join ( horizs )
8434	def apply ( cls , x , palette , na_value = None , trans = None ) : if trans is not None : x = trans . transform ( x ) limits = cls . train ( x ) return cls . map ( x , palette , limits , na_value )
13419	def schema ( args ) : try : import south cmd = args and 'schemamigration %s' % ' ' . join ( options . args ) or 'schemamigration' call_manage ( cmd ) except ImportError : error ( 'Could not import south.' )
9386	def parse ( self ) : for infile in self . infile_list : logger . info ( 'Processing : %s' , infile ) status = True file_status = naarad . utils . is_valid_file ( infile ) if not file_status : return False with open ( infile ) as fh : for line in fh : words = line . split ( ) if not words : continue if re . match ( '^\d\d\d\d-\d\d-\d\d$' , line ) : self . ts_date = words [ 0 ] continue prefix_word = words [ 0 ] . strip ( ) if prefix_word == 'top' : self . process_top_line ( words ) self . saw_pid = False elif self . ts_valid_lines : if prefix_word == 'Tasks:' : self . process_tasks_line ( words ) elif prefix_word == 'Cpu(s):' : self . process_cpu_line ( words ) elif prefix_word == 'Mem:' : self . process_mem_line ( words ) elif prefix_word == 'Swap:' : self . process_swap_line ( words ) elif prefix_word == 'PID' : self . saw_pid = True self . process_headers = words else : if self . saw_pid and len ( words ) >= len ( self . process_headers ) : self . process_individual_command ( words ) for out_csv in self . data . keys ( ) : self . csv_files . append ( out_csv ) with open ( out_csv , 'w' ) as fh : fh . write ( '\n' . join ( self . data [ out_csv ] ) ) gc . collect ( ) return status
1784	def CMP ( cpu , src1 , src2 ) : arg0 = src1 . read ( ) arg1 = Operators . SEXTEND ( src2 . read ( ) , src2 . size , src1 . size ) cpu . _calculate_CMP_flags ( src1 . size , arg0 - arg1 , arg0 , arg1 )
11332	def err ( format_msg , * args , ** kwargs ) : exc_info = kwargs . pop ( "exc_info" , False ) stderr . warning ( str ( format_msg ) . format ( * args , ** kwargs ) , exc_info = exc_info )
2272	def _win32_is_junction ( path ) : if not exists ( path ) : if os . path . isdir ( path ) : if not os . path . islink ( path ) : return True return False return jwfs . is_reparse_point ( path ) and not os . path . islink ( path )
6640	def hasDependency ( self , name , target = None , test_dependencies = False ) : if name in self . description . get ( 'dependencies' , { } ) . keys ( ) : return True target_deps = self . description . get ( 'targetDependencies' , { } ) if target is not None : for conf_key , target_conf_deps in target_deps . items ( ) : if _truthyConfValue ( target . getConfigValue ( conf_key ) ) or conf_key in target . getSimilarTo_Deprecated ( ) : if name in target_conf_deps : return True if test_dependencies : if name in self . description . get ( 'testDependencies' , { } ) . keys ( ) : return True if target is not None : test_target_deps = self . description . get ( 'testTargetDependencies' , { } ) for conf_key , target_conf_deps in test_target_deps . items ( ) : if _truthyConfValue ( target . getConfigValue ( conf_key ) ) or conf_key in target . getSimilarTo_Deprecated ( ) : if name in target_conf_deps : return True return False
1485	def run ( self , name , config , builder ) : if not isinstance ( name , str ) : raise RuntimeError ( "Name has to be a string type" ) if not isinstance ( config , Config ) : raise RuntimeError ( "config has to be a Config type" ) if not isinstance ( builder , Builder ) : raise RuntimeError ( "builder has to be a Builder type" ) bldr = TopologyBuilder ( name = name ) builder . build ( bldr ) bldr . set_config ( config . _api_config ) bldr . build_and_submit ( )
2466	def set_concluded_license ( self , doc , lic ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_conc_lics_set : self . file_conc_lics_set = True if validations . validate_lics_conc ( lic ) : self . file ( doc ) . conc_lics = lic return True else : raise SPDXValueError ( 'File::ConcludedLicense' ) else : raise CardinalityError ( 'File::ConcludedLicense' ) else : raise OrderError ( 'File::ConcludedLicense' )
11000	def _tz ( self , z ) : return ( z - self . param_dict [ 'psf-zslab' ] ) * self . param_dict [ self . zscale ]
11506	def create_item ( self , token , name , parent_id , ** kwargs ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'name' ] = name parameters [ 'parentid' ] = parent_id optional_keys = [ 'description' , 'uuid' , 'privacy' ] for key in optional_keys : if key in kwargs : parameters [ key ] = kwargs [ key ] response = self . request ( 'midas.item.create' , parameters ) return response
13428	def get_site ( self , site_id ) : url = "/2/sites/%s" % site_id return self . site_from_json ( self . _get_resource ( url ) [ "site" ] )
6206	def _calc_hash_da ( self , rs ) : self . hash_d = hash_ ( rs . get_state ( ) ) [ : 6 ] self . hash_a = self . hash_d
7784	def got_it ( self , value , state = "new" ) : if not self . active : return item = CacheItem ( self . address , value , self . _item_freshness_period , self . _item_expiration_period , self . _item_purge_period , state ) self . _object_handler ( item . address , item . value , item . state ) self . cache . add_item ( item ) self . _deactivate ( )
13659	def route ( self , * components ) : def _factory ( f ) : self . _addRoute ( f , route ( * components ) ) return f return _factory
9188	def admin_print_styles ( request ) : styles = [ ] with db_connect ( cursor_factory = DictCursor ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( ) for row in cursor . fetchall ( ) : styles . append ( { 'print_style' : row [ 'print_style' ] , 'title' : row [ 'title' ] , 'type' : row [ 'type' ] , 'revised' : row [ 'revised' ] , 'tag' : row [ 'tag' ] , 'commit_id' : row [ 'commit_id' ] , 'number' : row [ 'count' ] , 'bad' : row [ 'bad' ] , 'link' : request . route_path ( 'admin-print-style-single' , style = row [ 'print_style' ] ) } ) return { 'styles' : styles }
2725	def __get_ssh_keys_id_or_fingerprint ( ssh_keys , token , name ) : ssh_keys_id = list ( ) for ssh_key in ssh_keys : if type ( ssh_key ) in [ int , type ( 2 ** 64 ) ] : ssh_keys_id . append ( int ( ssh_key ) ) elif type ( ssh_key ) == SSHKey : ssh_keys_id . append ( ssh_key . id ) elif type ( ssh_key ) in [ type ( u'' ) , type ( '' ) ] : regexp_of_fingerprint = '([0-9a-fA-F]{2}:){15}[0-9a-fA-F]' match = re . match ( regexp_of_fingerprint , ssh_key ) if match is not None and match . end ( ) == len ( ssh_key ) - 1 : ssh_keys_id . append ( ssh_key ) else : key = SSHKey ( ) key . token = token results = key . load_by_pub_key ( ssh_key ) if results is None : key . public_key = ssh_key key . name = "SSH Key %s" % name key . create ( ) else : key = results ssh_keys_id . append ( key . id ) else : raise BadSSHKeyFormat ( "Droplet.ssh_keys should be a list of IDs, public keys" + " or fingerprints." ) return ssh_keys_id
8509	def fit ( self , X , y = None ) : from pylearn2 . config import yaml_parse from pylearn2 . train import Train params = self . get_params ( ) yaml_string = Template ( self . yaml_string ) . substitute ( params ) self . trainer = yaml_parse . load ( yaml_string ) assert isinstance ( self . trainer , Train ) if self . trainer . dataset is not None : raise ValueError ( 'Train YAML database must evaluate to None.' ) self . trainer . dataset = self . _get_dataset ( X , y ) if ( hasattr ( self . trainer . algorithm , 'monitoring_dataset' ) and self . trainer . algorithm . monitoring_dataset is not None ) : monitoring_dataset = self . trainer . algorithm . monitoring_dataset if len ( monitoring_dataset ) == 1 and '' in monitoring_dataset : monitoring_dataset [ '' ] = self . trainer . dataset else : monitoring_dataset [ 'train' ] = self . trainer . dataset self . trainer . algorithm . _set_monitoring_dataset ( monitoring_dataset ) else : self . trainer . algorithm . _set_monitoring_dataset ( self . trainer . dataset ) self . trainer . main_loop ( )
9972	def read_range ( filepath , range_expr , sheet = None , dict_generator = None ) : def default_generator ( cells ) : for row_ind , row in enumerate ( cells ) : for col_ind , cell in enumerate ( row ) : yield ( row_ind , col_ind ) , cell . value book = opxl . load_workbook ( filepath , data_only = True ) if _is_range_address ( range_expr ) : sheet_names = [ name . upper ( ) for name in book . sheetnames ] index = sheet_names . index ( sheet . upper ( ) ) cells = book . worksheets [ index ] [ range_expr ] else : cells = _get_namedrange ( book , range_expr , sheet ) if isinstance ( cells , opxl . cell . Cell ) : return cells . value if dict_generator is None : dict_generator = default_generator gen = dict_generator ( cells ) return { keyval [ 0 ] : keyval [ 1 ] for keyval in gen }
10009	def check_mro ( self , bases ) : try : self . add_node ( "temp" ) for base in bases : nx . DiGraph . add_edge ( self , base , "temp" ) result = self . get_mro ( "temp" ) [ 1 : ] finally : self . remove_node ( "temp" ) return result
3781	def calculate ( self , T , method ) : r if method == TEST_METHOD_1 : prop = self . TEST_METHOD_1_coeffs [ 0 ] + self . TEST_METHOD_1_coeffs [ 1 ] * T elif method == TEST_METHOD_2 : prop = self . TEST_METHOD_2_coeffs [ 0 ] + self . TEST_METHOD_2_coeffs [ 1 ] * T elif method in self . tabular_data : prop = self . interpolate ( T , method ) return prop
12795	def _get_password_url ( self ) : password_url = None if self . _settings [ "user" ] or self . _settings [ "authorization" ] : if self . _settings [ "url" ] : password_url = self . _settings [ "url" ] elif self . _settings [ "base_url" ] : password_url = self . _settings [ "base_url" ] return password_url
11757	def pl_true ( exp , model = { } ) : op , args = exp . op , exp . args if exp == TRUE : return True elif exp == FALSE : return False elif is_prop_symbol ( op ) : return model . get ( exp ) elif op == '~' : p = pl_true ( args [ 0 ] , model ) if p is None : return None else : return not p elif op == '|' : result = False for arg in args : p = pl_true ( arg , model ) if p is True : return True if p is None : result = None return result elif op == '&' : result = True for arg in args : p = pl_true ( arg , model ) if p is False : return False if p is None : result = None return result p , q = args if op == '>>' : return pl_true ( ~ p | q , model ) elif op == '<<' : return pl_true ( p | ~ q , model ) pt = pl_true ( p , model ) if pt is None : return None qt = pl_true ( q , model ) if qt is None : return None if op == '<=>' : return pt == qt elif op == '^' : return pt != qt else : raise ValueError , "illegal operator in logic expression" + str ( exp )
6361	def sim_matrix ( src , tar , mat = None , mismatch_cost = 0 , match_cost = 1 , symmetric = True , alphabet = None , ) : if alphabet : alphabet = tuple ( alphabet ) for i in src : if i not in alphabet : raise ValueError ( 'src value not in alphabet' ) for i in tar : if i not in alphabet : raise ValueError ( 'tar value not in alphabet' ) if src == tar : if mat and ( src , src ) in mat : return mat [ ( src , src ) ] return match_cost if mat and ( src , tar ) in mat : return mat [ ( src , tar ) ] elif symmetric and mat and ( tar , src ) in mat : return mat [ ( tar , src ) ] return mismatch_cost
7058	def s3_put_file ( local_file , bucket , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 's3' ) try : client . upload_file ( local_file , bucket , os . path . basename ( local_file ) ) return 's3://%s/%s' % ( bucket , os . path . basename ( local_file ) ) except Exception as e : LOGEXCEPTION ( 'could not upload %s to bucket: %s' % ( local_file , bucket ) ) if raiseonfail : raise return None
5451	def convert_to_label_chars ( s ) : accepted_characters = string . ascii_lowercase + string . digits + '-' def label_char_transform ( char ) : if char in accepted_characters : return char if char in string . ascii_uppercase : return char . lower ( ) return '-' return '' . join ( label_char_transform ( c ) for c in s )
7371	def main ( args_list = None ) : args = parse_args ( args_list ) binding_predictions = run_predictor ( args ) df = binding_predictions . to_dataframe ( ) logger . info ( '\n%s' , df ) if args . output_csv : df . to_csv ( args . output_csv , index = False ) print ( "Wrote: %s" % args . output_csv )
545	def __checkIfBestCompletedModel ( self ) : jobResultsStr = self . _jobsDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] if jobResultsStr is None : jobResults = { } else : jobResults = json . loads ( jobResultsStr ) isSaved = jobResults . get ( 'saved' , False ) bestMetric = jobResults . get ( 'bestValue' , None ) currentMetric = self . _getMetrics ( ) [ self . _optimizedMetricLabel ] self . _isBestModel = ( not isSaved ) or ( currentMetric < bestMetric ) return self . _isBestModel , jobResults , jobResultsStr
8857	def on_run ( self ) : filename = self . tabWidget . current_widget ( ) . file . path wd = os . path . dirname ( filename ) args = Settings ( ) . get_run_config_for_file ( filename ) self . interactiveConsole . start_process ( Settings ( ) . interpreter , args = [ filename ] + args , cwd = wd ) self . dockWidget . show ( ) self . actionRun . setEnabled ( False ) self . actionConfigure_run . setEnabled ( False )
13033	def write_index_translation ( translation_filename , entity_ids , relation_ids ) : translation = triple_pb . Translation ( ) entities = [ ] for name , index in entity_ids . items ( ) : translation . entities . add ( element = name , index = index ) relations = [ ] for name , index in relation_ids . items ( ) : translation . relations . add ( element = name , index = index ) with open ( translation_filename , "wb" ) as f : f . write ( translation . SerializeToString ( ) )
12197	def get_task_options ( ) : options = ( ) task_classes = get_tasks ( ) for cls in task_classes : options += cls . option_list return options
7573	def progressbar ( njobs , finished , msg = "" , spacer = " " ) : if njobs : progress = 100 * ( finished / float ( njobs ) ) else : progress = 100 hashes = '#' * int ( progress / 5. ) nohash = ' ' * int ( 20 - len ( hashes ) ) if not ipyrad . __interactive__ : msg = msg . rsplit ( "|" , 2 ) [ 0 ] args = [ spacer , hashes + nohash , int ( progress ) , msg ] print ( "\r{}[{}] {:>3}% {} " . format ( * args ) , end = "" ) sys . stdout . flush ( )
1673	def PrintCategories ( ) : sys . stderr . write ( '' . join ( ' %s\n' % cat for cat in _ERROR_CATEGORIES ) ) sys . exit ( 0 )
1383	def unregister_watch ( self , uid ) : Log . info ( "Unregister a watch with uid: " + str ( uid ) ) self . watches . pop ( uid , None )
6103	def luminosities_of_galaxies_within_circles_in_units ( self , radius : dim . Length , unit_luminosity = 'eps' , exposure_time = None ) : return list ( map ( lambda galaxy : galaxy . luminosity_within_circle_in_units ( radius = radius , unit_luminosity = unit_luminosity , kpc_per_arcsec = self . kpc_per_arcsec , exposure_time = exposure_time ) , self . galaxies ) )
632	def destroySegment ( self , segment ) : for synapse in segment . _synapses : self . _removeSynapseFromPresynapticMap ( synapse ) self . _numSynapses -= len ( segment . _synapses ) segments = self . _cells [ segment . cell ] . _segments i = segments . index ( segment ) del segments [ i ] self . _freeFlatIdxs . append ( segment . flatIdx ) self . _segmentForFlatIdx [ segment . flatIdx ] = None
10077	def create ( cls , data , id_ = None ) : data . setdefault ( '$schema' , current_jsonschemas . path_to_url ( current_app . config [ 'DEPOSIT_DEFAULT_JSONSCHEMA' ] ) ) if '_deposit' not in data : id_ = id_ or uuid . uuid4 ( ) cls . deposit_minter ( id_ , data ) data [ '_deposit' ] . setdefault ( 'owners' , list ( ) ) if current_user and current_user . is_authenticated : creator_id = int ( current_user . get_id ( ) ) if creator_id not in data [ '_deposit' ] [ 'owners' ] : data [ '_deposit' ] [ 'owners' ] . append ( creator_id ) data [ '_deposit' ] [ 'created_by' ] = creator_id return super ( Deposit , cls ) . create ( data , id_ = id_ )
2628	def teardown ( self ) : self . shut_down_instance ( self . instances ) self . instances = [ ] try : self . client . delete_internet_gateway ( InternetGatewayId = self . internet_gateway ) self . internet_gateway = None self . client . delete_route_table ( RouteTableId = self . route_table ) self . route_table = None for subnet in list ( self . sn_ids ) : self . client . delete_subnet ( SubnetId = subnet ) self . sn_ids . remove ( subnet ) self . client . delete_security_group ( GroupId = self . sg_id ) self . sg_id = None self . client . delete_vpc ( VpcId = self . vpc_id ) self . vpc_id = None except Exception as e : logger . error ( "{}" . format ( e ) ) raise e self . show_summary ( ) os . remove ( self . config [ 'state_file_path' ] )
4793	def is_subset_of ( self , * supersets ) : if not isinstance ( self . val , Iterable ) : raise TypeError ( 'val is not iterable' ) if len ( supersets ) == 0 : raise ValueError ( 'one or more superset args must be given' ) missing = [ ] if hasattr ( self . val , 'keys' ) and callable ( getattr ( self . val , 'keys' ) ) and hasattr ( self . val , '__getitem__' ) : superdict = { } for l , j in enumerate ( supersets ) : self . _check_dict_like ( j , check_values = False , name = 'arg #%d' % ( l + 1 ) ) for k in j . keys ( ) : superdict . update ( { k : j [ k ] } ) for i in self . val . keys ( ) : if i not in superdict : missing . append ( { i : self . val [ i ] } ) elif self . val [ i ] != superdict [ i ] : missing . append ( { i : self . val [ i ] } ) if missing : self . _err ( 'Expected <%s> to be subset of %s, but %s %s missing.' % ( self . val , self . _fmt_items ( superdict ) , self . _fmt_items ( missing ) , 'was' if len ( missing ) == 1 else 'were' ) ) else : superset = set ( ) for j in supersets : try : for k in j : superset . add ( k ) except Exception : superset . add ( j ) for i in self . val : if i not in superset : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to be subset of %s, but %s %s missing.' % ( self . val , self . _fmt_items ( superset ) , self . _fmt_items ( missing ) , 'was' if len ( missing ) == 1 else 'were' ) ) return self
4010	def get_docker_client ( ) : env = get_docker_env ( ) host , cert_path , tls_verify = env [ 'DOCKER_HOST' ] , env [ 'DOCKER_CERT_PATH' ] , env [ 'DOCKER_TLS_VERIFY' ] params = { 'base_url' : host . replace ( 'tcp://' , 'https://' ) , 'timeout' : None , 'version' : 'auto' } if tls_verify and cert_path : params [ 'tls' ] = docker . tls . TLSConfig ( client_cert = ( os . path . join ( cert_path , 'cert.pem' ) , os . path . join ( cert_path , 'key.pem' ) ) , ca_cert = os . path . join ( cert_path , 'ca.pem' ) , verify = True , ssl_version = None , assert_hostname = False ) return docker . Client ( ** params )
7937	def connect ( self , addr , port = None , service = None ) : with self . lock : self . _connect ( addr , port , service )
8080	def relmoveto ( self , x , y ) : if self . _path is None : raise ShoebotError ( _ ( "No current path. Use beginpath() first." ) ) self . _path . relmoveto ( x , y )
5468	def get_event_of_type ( op , event_type ) : events = get_events ( op ) if not events : return None return [ e for e in events if e . get ( 'details' , { } ) . get ( '@type' ) == event_type ]
12196	def get_tasks ( ) : task_classes = [ ] for task_path in TASKS : try : module , classname = task_path . rsplit ( '.' , 1 ) except ValueError : raise ImproperlyConfigured ( '%s isn\'t a task module' % task_path ) try : mod = import_module ( module ) except ImportError as e : raise ImproperlyConfigured ( 'Error importing task %s: "%s"' % ( module , e ) ) try : task_class = getattr ( mod , classname ) except AttributeError : raise ImproperlyConfigured ( 'Task module "%s" does not define a ' '"%s" class' % ( module , classname ) ) task_classes . append ( task_class ) return task_classes
5173	def get_install_requires ( ) : requirements = [ ] for line in open ( 'requirements.txt' ) . readlines ( ) : if line . startswith ( '#' ) or line == '' or line . startswith ( 'http' ) or line . startswith ( 'git' ) : continue requirements . append ( line . replace ( '\n' , '' ) ) if sys . version_info . major < 3 : requirements . append ( 'py2-ipaddress' ) return requirements
9425	def open ( self , member , pwd = None ) : if isinstance ( member , RarInfo ) : member = member . filename archive = unrarlib . RAROpenArchiveDataEx ( self . filename , mode = constants . RAR_OM_EXTRACT ) handle = self . _open ( archive ) password = pwd or self . pwd if password is not None : unrarlib . RARSetPassword ( handle , b ( password ) ) data = _ReadIntoMemory ( ) c_callback = unrarlib . UNRARCALLBACK ( data . _callback ) unrarlib . RARSetCallback ( handle , c_callback , 0 ) try : rarinfo = self . _read_header ( handle ) while rarinfo is not None : if rarinfo . filename == member : self . _process_current ( handle , constants . RAR_TEST ) break else : self . _process_current ( handle , constants . RAR_SKIP ) rarinfo = self . _read_header ( handle ) if rarinfo is None : data = None except unrarlib . MissingPassword : raise RuntimeError ( "File is encrypted, password required" ) except unrarlib . BadPassword : raise RuntimeError ( "Bad password for File" ) except unrarlib . BadDataError : if password is not None : raise RuntimeError ( "File CRC error or incorrect password" ) else : raise RuntimeError ( "File CRC error" ) except unrarlib . UnrarException as e : raise BadRarFile ( "Bad RAR archive data: %s" % str ( e ) ) finally : self . _close ( handle ) if data is None : raise KeyError ( 'There is no item named %r in the archive' % member ) return data . get_bytes ( )
6541	def parse_python_file ( filepath ) : with _AST_CACHE_LOCK : if filepath not in _AST_CACHE : source = read_file ( filepath ) _AST_CACHE [ filepath ] = ast . parse ( source , filename = filepath ) return _AST_CACHE [ filepath ]
9273	def filter_between_tags ( self , all_tags ) : tag_names = [ t [ "name" ] for t in all_tags ] between_tags = [ ] for tag in self . options . between_tags : try : idx = tag_names . index ( tag ) except ValueError : raise ChangelogGeneratorError ( "ERROR: can't find tag {0}, specified with " "--between-tags option." . format ( tag ) ) between_tags . append ( all_tags [ idx ] ) between_tags = self . sort_tags_by_date ( between_tags ) if len ( between_tags ) == 1 : between_tags . append ( between_tags [ 0 ] ) older = self . get_time_of_tag ( between_tags [ 1 ] ) newer = self . get_time_of_tag ( between_tags [ 0 ] ) for tag in all_tags : if older < self . get_time_of_tag ( tag ) < newer : between_tags . append ( tag ) if older == newer : between_tags . pop ( 0 ) return between_tags
7229	def paint ( self ) : snippet = { 'heatmap-radius' : VectorStyle . get_style_value ( self . radius ) , 'heatmap-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'heatmap-color' : VectorStyle . get_style_value ( self . color ) , 'heatmap-intensity' : VectorStyle . get_style_value ( self . intensity ) , 'heatmap-weight' : VectorStyle . get_style_value ( self . weight ) } return snippet
13729	def balance_over_time ( address ) : forged_blocks = None txhistory = Address . transactions ( address ) delegates = Delegate . delegates ( ) for i in delegates : if address == i . address : forged_blocks = Delegate . blocks ( i . pubkey ) balance_over_time = [ ] balance = 0 block = 0 Balance = namedtuple ( 'balance' , 'timestamp amount' ) for tx in txhistory : if forged_blocks : while forged_blocks [ block ] . timestamp <= tx . timestamp : balance += ( forged_blocks [ block ] . reward + forged_blocks [ block ] . totalFee ) balance_over_time . append ( Balance ( timestamp = forged_blocks [ block ] . timestamp , amount = balance ) ) block += 1 if tx . senderId == address : balance -= ( tx . amount + tx . fee ) res = Balance ( timestamp = tx . timestamp , amount = balance ) balance_over_time . append ( res ) if tx . recipientId == address : balance += tx . amount res = Balance ( timestamp = tx . timestamp , amount = balance ) balance_over_time . append ( res ) if forged_blocks and block <= len ( forged_blocks ) - 1 : if forged_blocks [ block ] . timestamp > txhistory [ - 1 ] . timestamp : for i in forged_blocks [ block : ] : balance += ( i . reward + i . totalFee ) res = Balance ( timestamp = i . timestamp , amount = balance ) balance_over_time . append ( res ) return balance_over_time
48	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , alpha = 1.0 , size = 3 , copy = True , raise_if_out_of_image = False ) : if copy : image = np . copy ( image ) if image . ndim == 2 : assert ia . is_single_number ( color ) , ( "Got a 2D image. Expected then 'color' to be a single number, " "but got %s." % ( str ( color ) , ) ) elif image . ndim == 3 and ia . is_single_number ( color ) : color = [ color ] * image . shape [ - 1 ] input_dtype = image . dtype alpha_color = color if alpha < 0.01 : return image elif alpha > 0.99 : alpha = 1 else : image = image . astype ( np . float32 , copy = False ) alpha_color = alpha * np . array ( color ) height , width = image . shape [ 0 : 2 ] y , x = self . y_int , self . x_int x1 = max ( x - size // 2 , 0 ) x2 = min ( x + 1 + size // 2 , width ) y1 = max ( y - size // 2 , 0 ) y2 = min ( y + 1 + size // 2 , height ) x1_clipped , x2_clipped = np . clip ( [ x1 , x2 ] , 0 , width ) y1_clipped , y2_clipped = np . clip ( [ y1 , y2 ] , 0 , height ) x1_clipped_ooi = ( x1_clipped < 0 or x1_clipped >= width ) x2_clipped_ooi = ( x2_clipped < 0 or x2_clipped >= width + 1 ) y1_clipped_ooi = ( y1_clipped < 0 or y1_clipped >= height ) y2_clipped_ooi = ( y2_clipped < 0 or y2_clipped >= height + 1 ) x_ooi = ( x1_clipped_ooi and x2_clipped_ooi ) y_ooi = ( y1_clipped_ooi and y2_clipped_ooi ) x_zero_size = ( x2_clipped - x1_clipped ) < 1 y_zero_size = ( y2_clipped - y1_clipped ) < 1 if not x_ooi and not y_ooi and not x_zero_size and not y_zero_size : if alpha == 1 : image [ y1_clipped : y2_clipped , x1_clipped : x2_clipped ] = color else : image [ y1_clipped : y2_clipped , x1_clipped : x2_clipped ] = ( ( 1 - alpha ) * image [ y1_clipped : y2_clipped , x1_clipped : x2_clipped ] + alpha_color ) else : if raise_if_out_of_image : raise Exception ( "Cannot draw keypoint x=%.8f, y=%.8f on image with " "shape %s." % ( y , x , image . shape ) ) if image . dtype . name != input_dtype . name : if input_dtype . name == "uint8" : image = np . clip ( image , 0 , 255 , out = image ) image = image . astype ( input_dtype , copy = False ) return image
11147	def is_repository_file ( self , relativePath ) : relativePath = self . to_repo_relative_path ( path = relativePath , split = False ) if relativePath == '' : return False , False , False , False relaDir , name = os . path . split ( relativePath ) fileOnDisk = os . path . isfile ( os . path . join ( self . __path , relativePath ) ) infoOnDisk = os . path . isfile ( os . path . join ( self . __path , os . path . dirname ( relativePath ) , self . __fileInfo % name ) ) classOnDisk = os . path . isfile ( os . path . join ( self . __path , os . path . dirname ( relativePath ) , self . __fileClass % name ) ) cDir = self . __repo [ 'walk_repo' ] if len ( relaDir ) : for dirname in relaDir . split ( os . sep ) : dList = [ d for d in cDir if isinstance ( d , dict ) ] if not len ( dList ) : cDir = None break cDict = [ d for d in dList if dirname in d ] if not len ( cDict ) : cDir = None break cDir = cDict [ 0 ] [ dirname ] if cDir is None : return False , fileOnDisk , infoOnDisk , classOnDisk if str ( name ) not in [ str ( i ) for i in cDir ] : return False , fileOnDisk , infoOnDisk , classOnDisk return True , fileOnDisk , infoOnDisk , classOnDisk
12258	def sdcone ( x , rho ) : U , V = np . linalg . eigh ( x ) return V . dot ( np . diag ( np . maximum ( U , 0 ) ) . dot ( V . T ) )
3227	def rewrite_kwargs ( conn_type , kwargs , module_name = None ) : if conn_type != 'cloud' and module_name != 'compute' : if 'project' in kwargs : kwargs [ 'name' ] = 'projects/%s' % kwargs . pop ( 'project' ) if conn_type == 'cloud' and module_name == 'storage' : if 'project' in kwargs : del kwargs [ 'project' ] return kwargs
5508	def worker ( f ) : @ functools . wraps ( f ) async def wrapper ( cls , connection , rest ) : try : await f ( cls , connection , rest ) except asyncio . CancelledError : connection . response ( "426" , "transfer aborted" ) connection . response ( "226" , "abort successful" ) return wrapper
4073	def eval_environ ( value ) : def eval_environ_str ( value ) : parts = value . split ( ';' ) if len ( parts ) < 2 : return value expr = parts [ 1 ] . lstrip ( ) if not re . match ( "^((\\w+(\\.\\w+)?|'.*?'|\".*?\")\\s+" '(in|==|!=|not in)\\s+' "(\\w+(\\.\\w+)?|'.*?'|\".*?\")" '(\\s+(or|and)\\s+)?)+$' , expr ) : raise ValueError ( 'bad environment marker: %r' % expr ) expr = re . sub ( r"(platform\.\w+)" , r"\1()" , expr ) return parts [ 0 ] if eval ( expr ) else '' if isinstance ( value , list ) : new_value = [ ] for element in value : element = eval_environ_str ( element ) if element : new_value . append ( element ) elif isinstance ( value , str ) : new_value = eval_environ_str ( value ) else : new_value = value return new_value
5019	def validate_image_extension ( value ) : config = get_app_config ( ) ext = os . path . splitext ( value . name ) [ 1 ] if config and not ext . lower ( ) in config . valid_image_extensions : raise ValidationError ( _ ( "Unsupported file extension." ) )
1128	def SeqN ( n , * inner_rules , ** kwargs ) : @ action ( Seq ( * inner_rules ) , loc = kwargs . get ( "loc" , None ) ) def rule ( parser , * values ) : return values [ n ] return rule
11699	def serve ( self , sock , request_handler , error_handler , debug = False , request_timeout = 60 , ssl = None , request_max_size = None , reuse_port = False , loop = None , protocol = HttpProtocol , backlog = 100 , ** kwargs ) : if debug : loop . set_debug ( debug ) server = partial ( protocol , loop = loop , connections = self . connections , signal = self . signal , request_handler = request_handler , error_handler = error_handler , request_timeout = request_timeout , request_max_size = request_max_size , ) server_coroutine = loop . create_server ( server , host = None , port = None , ssl = ssl , reuse_port = reuse_port , sock = sock , backlog = backlog ) loop . call_soon ( partial ( update_current_time , loop ) ) return server_coroutine
2960	def __base_state ( self , containers ) : return dict ( blockade_id = self . _blockade_id , containers = containers , version = self . _state_version )
8575	def update_nic ( self , datacenter_id , server_id , nic_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s' % ( datacenter_id , server_id , nic_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
1942	def unmap_memory_callback ( self , start , size ) : logger . info ( f"Unmapping memory from {hex(start)} to {hex(start + size)}" ) mask = ( 1 << 12 ) - 1 if ( start & mask ) != 0 : logger . error ( "Memory to be unmapped is not aligned to a page" ) if ( size & mask ) != 0 : size = ( ( size >> 12 ) + 1 ) << 12 logger . warning ( "Forcing unmap size to align to a page" ) self . _emu . mem_unmap ( start , size )
4706	def read ( self , path ) : with open ( path , "rb" ) as fout : memmove ( self . m_buf , fout . read ( self . m_size ) , self . m_size )
4062	def show_condition_operators ( self , condition ) : permitted_operators = self . savedsearch . conditions_operators . get ( condition ) permitted_operators_list = set ( [ self . savedsearch . operators . get ( op ) for op in permitted_operators ] ) return permitted_operators_list
1929	def process_config_values ( parser : argparse . ArgumentParser , args : argparse . Namespace ) : load_overrides ( args . config ) defined_vars = list ( get_config_keys ( ) ) command_line_args = vars ( args ) config_cli_args = get_group ( 'cli' ) for k in command_line_args : default = parser . get_default ( k ) set_val = getattr ( args , k ) if default is not set_val : if k not in defined_vars : config_cli_args . update ( k , value = set_val ) else : group_name , key = k . split ( '.' ) group = get_group ( group_name ) setattr ( group , key , set_val ) else : if k in config_cli_args : setattr ( args , k , getattr ( config_cli_args , k ) )
12002	def _remove_magic ( self , data ) : if not self . magic : return data magic_size = len ( self . magic ) magic = data [ : magic_size ] if magic != self . magic : raise Exception ( 'Invalid magic' ) data = data [ magic_size : ] return data
3598	def reviews ( self , packageName , filterByDevice = False , sort = 2 , nb_results = None , offset = None ) : path = REVIEWS_URL + "?doc={}&sort={}" . format ( requests . utils . quote ( packageName ) , sort ) if nb_results is not None : path += "&n={}" . format ( nb_results ) if offset is not None : path += "&o={}" . format ( offset ) if filterByDevice : path += "&dfil=1" data = self . executeRequestApi2 ( path ) output = [ ] for review in data . payload . reviewResponse . getResponse . review : output . append ( utils . parseProtobufObj ( review ) ) return output
13150	def log_state ( entity , state ) : p = { 'on' : entity , 'state' : state } _log ( TYPE_CODES . STATE , p )
7697	def from_xml ( cls , element ) : if element . tag != ITEM_TAG : raise ValueError ( "{0!r} is not a roster item" . format ( element ) ) try : jid = JID ( element . get ( "jid" ) ) except ValueError : raise BadRequestProtocolError ( u"Bad item JID" ) subscription = element . get ( "subscription" ) ask = element . get ( "ask" ) name = element . get ( "name" ) duplicate_group = False groups = set ( ) for child in element : if child . tag != GROUP_TAG : continue group = child . text if group is None : group = u"" if group in groups : duplicate_group = True else : groups . add ( group ) approved = element . get ( "approved" ) if approved == "true" : approved = True elif approved in ( "false" , None ) : approved = False else : logger . debug ( "RosterItem.from_xml: got unknown 'approved':" " {0!r}, changing to False" . format ( approved ) ) approved = False result = cls ( jid , name , groups , subscription , ask , approved ) result . _duplicate_group = duplicate_group return result
13451	def imgmax ( self ) : if not hasattr ( self , '_imgmax' ) : imgmax = _np . max ( self . images [ 0 ] ) for img in self . images : imax = _np . max ( img ) if imax > imgmax : imgmax = imax self . _imgmax = imgmax return self . _imgmax
13341	def expand_dims ( a , axis ) : if hasattr ( a , 'expand_dims' ) and hasattr ( type ( a ) , '__array_interface__' ) : return a . expand_dims ( axis ) else : return np . expand_dims ( a , axis )
3522	def _hashable_bytes ( data ) : if isinstance ( data , bytes ) : return data elif isinstance ( data , str ) : return data . encode ( 'ascii' ) else : raise TypeError ( data )
465	def set_gpu_fraction ( gpu_fraction = 0.3 ) : tl . logging . info ( "[TL]: GPU MEM Fraction %f" % gpu_fraction ) gpu_options = tf . GPUOptions ( per_process_gpu_memory_fraction = gpu_fraction ) sess = tf . Session ( config = tf . ConfigProto ( gpu_options = gpu_options ) ) return sess
1810	def SETGE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . SF == cpu . OF , 1 , 0 ) )
11376	def _normalize_issue_dir_with_dtd ( self , path ) : if exists ( join ( path , 'resolved_issue.xml' ) ) : return issue_xml_content = open ( join ( path , 'issue.xml' ) ) . read ( ) sis = [ 'si510.dtd' , 'si520.dtd' , 'si540.dtd' ] tmp_extracted = 0 for si in sis : if si in issue_xml_content : self . _extract_correct_dtd_package ( si . split ( '.' ) [ 0 ] , path ) tmp_extracted = 1 if not tmp_extracted : message = "It looks like the path " + path message += " does not contain an si510, si520 or si540 in issue.xml file" self . logger . error ( message ) raise ValueError ( message ) command = [ "xmllint" , "--format" , "--loaddtd" , join ( path , 'issue.xml' ) , "--output" , join ( path , 'resolved_issue.xml' ) ] dummy , dummy , cmd_err = run_shell_command ( command ) if cmd_err : message = "Error in cleaning %s: %s" % ( join ( path , 'issue.xml' ) , cmd_err ) self . logger . error ( message ) raise ValueError ( message )
7794	def register_fetcher ( self , object_class , fetcher_class ) : self . _lock . acquire ( ) try : cache = self . _caches . get ( object_class ) if not cache : cache = Cache ( self . max_items , self . default_freshness_period , self . default_expiration_period , self . default_purge_period ) self . _caches [ object_class ] = cache cache . set_fetcher ( fetcher_class ) finally : self . _lock . release ( )
6818	def install_auth_basic_user_file ( self , site = None ) : r = self . local_renderer hostname = self . current_hostname target_sites = self . genv . available_sites_by_host . get ( hostname , None ) for _site , site_data in self . iter_sites ( site = site , setter = self . set_site_specifics ) : if self . verbose : print ( '~' * 80 , file = sys . stderr ) print ( 'Site:' , _site , file = sys . stderr ) print ( 'env.apache_auth_basic:' , r . env . auth_basic , file = sys . stderr ) if target_sites is not None : assert isinstance ( target_sites , ( tuple , list ) ) if _site not in target_sites : continue if not r . env . auth_basic : continue assert r . env . auth_basic_users , 'No apache auth users specified.' for username , password in r . env . auth_basic_users : r . env . auth_basic_username = username r . env . auth_basic_password = password r . env . apache_site = _site r . env . fn = r . format ( r . env . auth_basic_authuserfile ) if self . files . exists ( r . env . fn ) : r . sudo ( 'htpasswd -b {fn} {auth_basic_username} {auth_basic_password}' ) else : r . sudo ( 'htpasswd -b -c {fn} {auth_basic_username} {auth_basic_password}' )
2963	def get_source_chains ( self , blockade_id ) : result = { } if not blockade_id : raise ValueError ( "invalid blockade_id" ) lines = self . get_chain_rules ( "FORWARD" ) for line in lines : parts = line . split ( ) if len ( parts ) < 4 : continue try : partition_index = parse_partition_index ( blockade_id , parts [ 0 ] ) except ValueError : continue source = parts [ 3 ] if source : result [ source ] = partition_index return result
4737	def good ( txt ) : print ( "%s# %s%s%s" % ( PR_GOOD_CC , get_time_stamp ( ) , txt , PR_NC ) ) sys . stdout . flush ( )
6698	def preseed_package ( pkg_name , preseed ) : for q_name , _ in preseed . items ( ) : q_type , q_answer = _ run_as_root ( 'echo "%(pkg_name)s %(q_name)s %(q_type)s %(q_answer)s" | debconf-set-selections' % locals ( ) )
11460	def add_systemnumber ( self , source , recid = None ) : if not recid : recid = self . get_recid ( ) if not self . hidden and recid : record_add_field ( self . record , tag = '035' , subfields = [ ( '9' , source ) , ( 'a' , recid ) ] )
13051	def import_nmap ( result , tag , check_function = all_hosts , import_services = False ) : host_search = HostSearch ( arguments = False ) service_search = ServiceSearch ( ) parser = NmapParser ( ) report = parser . parse_fromstring ( result ) imported_hosts = 0 imported_services = 0 for nmap_host in report . hosts : if check_function ( nmap_host ) : imported_hosts += 1 host = host_search . id_to_object ( nmap_host . address ) host . status = nmap_host . status host . add_tag ( tag ) if nmap_host . os_fingerprinted : host . os = nmap_host . os_fingerprint if nmap_host . hostnames : host . hostname . extend ( nmap_host . hostnames ) if import_services : for service in nmap_host . services : imported_services += 1 serv = Service ( ** service . get_dict ( ) ) serv . address = nmap_host . address service_id = service_search . object_to_id ( serv ) if service_id : serv_old = Service . get ( service_id ) if service . banner : serv_old . banner = service . banner serv_old . save ( ) else : serv . address = nmap_host . address serv . save ( ) if service . state == 'open' : host . open_ports . append ( service . port ) if service . state == 'closed' : host . closed_ports . append ( service . port ) if service . state == 'filtered' : host . filtered_ports . append ( service . port ) host . save ( ) if imported_hosts : print_success ( "Imported {} hosts, with tag {}" . format ( imported_hosts , tag ) ) else : print_error ( "No hosts found" ) return { 'hosts' : imported_hosts , 'services' : imported_services }
5816	def _read_remaining ( socket ) : output = b'' old_timeout = socket . gettimeout ( ) try : socket . settimeout ( 0.0 ) output += socket . recv ( 8192 ) except ( socket_ . error ) : pass finally : socket . settimeout ( old_timeout ) return output
1382	def register_watch ( self , callback ) : RETRY_COUNT = 5 for _ in range ( RETRY_COUNT ) : uid = uuid . uuid4 ( ) if uid not in self . watches : Log . info ( "Registering a watch with uid: " + str ( uid ) ) try : callback ( self ) except Exception as e : Log . error ( "Caught exception while triggering callback: " + str ( e ) ) Log . debug ( traceback . format_exc ( ) ) return None self . watches [ uid ] = callback return uid return None
9287	def consumer ( self , callback , blocking = True , immortal = False , raw = False ) : if not self . _connected : raise ConnectionError ( "not connected to a server" ) line = b'' while True : try : for line in self . _socket_readlines ( blocking ) : if line [ 0 : 1 ] != b'#' : if raw : callback ( line ) else : callback ( self . _parse ( line ) ) else : self . logger . debug ( "Server: %s" , line . decode ( 'utf8' ) ) except ParseError as exp : self . logger . log ( 11 , "%s\n Packet: %s" , exp . message , exp . packet ) except UnknownFormat as exp : self . logger . log ( 9 , "%s\n Packet: %s" , exp . message , exp . packet ) except LoginError as exp : self . logger . error ( "%s: %s" , exp . __class__ . __name__ , exp . message ) except ( KeyboardInterrupt , SystemExit ) : raise except ( ConnectionDrop , ConnectionError ) : self . close ( ) if not immortal : raise else : self . connect ( blocking = blocking ) continue except GenericError : pass except StopIteration : break except : self . logger . error ( "APRS Packet: %s" , line ) raise if not blocking : break
7149	def encode ( cls , hex ) : out = [ ] for i in range ( len ( hex ) // 8 ) : word = endian_swap ( hex [ 8 * i : 8 * i + 8 ] ) x = int ( word , 16 ) w1 = x % cls . n w2 = ( x // cls . n + w1 ) % cls . n w3 = ( x // cls . n // cls . n + w2 ) % cls . n out += [ cls . word_list [ w1 ] , cls . word_list [ w2 ] , cls . word_list [ w3 ] ] checksum = cls . get_checksum ( " " . join ( out ) ) out . append ( checksum ) return " " . join ( out )
13828	def read ( readme ) : extend = os . path . splitext ( readme ) [ 1 ] if ( extend == '.rst' ) : import codecs return codecs . open ( readme , 'r' , 'utf-8' ) . read ( ) elif ( extend == '.md' ) : import pypandoc return pypandoc . convert ( readme , 'rst' )
7553	def nworker ( data , chunk ) : oldlimit = set_mkl_thread_limit ( 1 ) with h5py . File ( data . database . input , 'r' ) as io5 : seqview = io5 [ "bootsarr" ] [ : ] maparr = io5 [ "bootsmap" ] [ : , 0 ] smps = io5 [ "quartets" ] [ chunk : chunk + data . _chunksize ] nall_mask = seqview [ : ] == 78 rquartets = np . zeros ( ( smps . shape [ 0 ] , 4 ) , dtype = np . uint16 ) rinvariants = np . zeros ( ( smps . shape [ 0 ] , 16 , 16 ) , dtype = np . uint16 ) for idx in xrange ( smps . shape [ 0 ] ) : sidx = smps [ idx ] seqs = seqview [ sidx ] nmask = np . any ( nall_mask [ sidx ] , axis = 0 ) nmask += np . all ( seqs == seqs [ 0 ] , axis = 0 ) bidx , invar = calculate ( seqs , maparr , nmask , TESTS ) rquartets [ idx ] = smps [ idx ] [ bidx ] rinvariants [ idx ] = invar set_mkl_thread_limit ( oldlimit ) return rquartets , rinvariants
4814	def _word_ngrams ( self , tokens ) : if self . stop_words is not None : tokens = [ w for w in tokens if w not in self . stop_words ] min_n , max_n = self . ngram_range if max_n != 1 : original_tokens = tokens if min_n == 1 : tokens = list ( original_tokens ) min_n += 1 else : tokens = [ ] n_original_tokens = len ( original_tokens ) tokens_append = tokens . append space_join = " " . join for n in range ( min_n , min ( max_n + 1 , n_original_tokens + 1 ) ) : for i in range ( n_original_tokens - n + 1 ) : tokens_append ( space_join ( original_tokens [ i : i + n ] ) ) return tokens
3887	def add_observer ( self , callback ) : if callback in self . _observers : raise ValueError ( '{} is already an observer of {}' . format ( callback , self ) ) self . _observers . append ( callback )
5580	def extract_contours ( array , tile , interval = 100 , field = 'elev' , base = 0 ) : import matplotlib . pyplot as plt levels = _get_contour_values ( array . min ( ) , array . max ( ) , interval = interval , base = base ) if not levels : return [ ] contours = plt . contour ( array , levels ) index = 0 out_contours = [ ] for level in range ( len ( contours . collections ) ) : elevation = levels [ index ] index += 1 paths = contours . collections [ level ] . get_paths ( ) for path in paths : out_coords = [ ( tile . left + ( y * tile . pixel_x_size ) , tile . top - ( x * tile . pixel_y_size ) , ) for x , y in zip ( path . vertices [ : , 1 ] , path . vertices [ : , 0 ] ) ] if len ( out_coords ) >= 2 : out_contours . append ( dict ( properties = { field : elevation } , geometry = mapping ( LineString ( out_coords ) ) ) ) return out_contours
10266	def collapse_consistent_edges ( graph : BELGraph ) : for u , v in graph . edges ( ) : relation = pair_is_consistent ( graph , u , v ) if not relation : continue edges = [ ( u , v , k ) for k in graph [ u ] [ v ] ] graph . remove_edges_from ( edges ) graph . add_edge ( u , v , attr_dict = { RELATION : relation } )
6377	def sim_typo ( src , tar , metric = 'euclidean' , cost = ( 1 , 1 , 0.5 , 0.5 ) , layout = 'QWERTY' ) : return Typo ( ) . sim ( src , tar , metric , cost , layout )
5386	def _format_task_name ( job_id , task_id , task_attempt ) : docker_name = '%s.%s' % ( job_id , 'task' if task_id is None else task_id ) if task_attempt is not None : docker_name += '.' + str ( task_attempt ) return 'dsub-{}' . format ( _convert_suffix_to_docker_chars ( docker_name ) )
10331	def group_nodes_by_annotation ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Mapping [ str , Set [ BaseEntity ] ] : result = defaultdict ( set ) for u , v , d in graph . edges ( data = True ) : if not edge_has_annotation ( d , annotation ) : continue result [ d [ ANNOTATIONS ] [ annotation ] ] . add ( u ) result [ d [ ANNOTATIONS ] [ annotation ] ] . add ( v ) return dict ( result )
12758	def labels ( self ) : return sorted ( self . channels , key = lambda c : self . channels [ c ] )
8240	def tetrad ( clr , angle = 90 ) : clr = color ( clr ) colors = colorlist ( clr ) c = clr . rotate_ryb ( angle ) if clr . brightness < 0.5 : c . brightness += 0.2 else : c . brightness -= - 0.2 colors . append ( c ) c = clr . rotate_ryb ( angle * 2 ) if clr . brightness < 0.5 : c . brightness += 0.1 else : c . brightness -= - 0.1 colors . append ( c ) colors . append ( clr . rotate_ryb ( angle * 3 ) . lighten ( 0.1 ) ) return colors
12183	def method_exists ( cls , method ) : methods = cls . API_METHODS for key in method . split ( '.' ) : methods = methods . get ( key ) if methods is None : break if isinstance ( methods , str ) : logger . debug ( '%r: %r' , method , methods ) return True return False
3232	def get_user_agent_default ( pkg_name = 'cloudaux' ) : version = '0.0.1' try : import pkg_resources version = pkg_resources . get_distribution ( pkg_name ) . version except pkg_resources . DistributionNotFound : pass except ImportError : pass return 'cloudaux/%s' % ( version )
13893	def _HandleContentsEol ( contents , eol_style ) : if eol_style == EOL_STYLE_NONE : return contents if eol_style == EOL_STYLE_UNIX : return contents . replace ( '\r\n' , eol_style ) . replace ( '\r' , eol_style ) if eol_style == EOL_STYLE_MAC : return contents . replace ( '\r\n' , eol_style ) . replace ( '\n' , eol_style ) if eol_style == EOL_STYLE_WINDOWS : return contents . replace ( '\r\n' , '\n' ) . replace ( '\r' , '\n' ) . replace ( '\n' , EOL_STYLE_WINDOWS ) raise ValueError ( 'Unexpected eol style: %r' % ( eol_style , ) )
1455	def add_ckpt_state ( self , ckpt_id , ckpt_state ) : self . _flush_remaining ( ) msg = ckptmgr_pb2 . StoreInstanceStateCheckpoint ( ) istate = ckptmgr_pb2 . InstanceStateCheckpoint ( ) istate . checkpoint_id = ckpt_id istate . state = ckpt_state msg . state . CopyFrom ( istate ) self . _push_tuple_to_stream ( msg )
10549	def get_results ( project_id , limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) params [ 'project_id' ] = project_id try : res = _pybossa_req ( 'get' , 'result' , params = params ) if type ( res ) . __name__ == 'list' : return [ Result ( result ) for result in res ] else : return res except : raise
4151	def power ( self ) : r if self . scale_by_freq == False : return sum ( self . psd ) * len ( self . psd ) else : return sum ( self . psd ) * self . df / ( 2. * numpy . pi )
9661	def find_standard_sakefile ( settings ) : error = settings [ "error" ] if settings [ "customsake" ] : custom = settings [ "customsake" ] if not os . path . isfile ( custom ) : error ( "Specified sakefile '{}' doesn't exist" , custom ) sys . exit ( 1 ) return custom for name in [ "Sakefile" , "Sakefile.yaml" , "Sakefile.yml" ] : if os . path . isfile ( name ) : return name error ( "Error: there is no Sakefile to read" ) sys . exit ( 1 )
3109	def locked_put ( self , credentials ) : entity , _ = self . model_class . objects . get_or_create ( ** { self . key_name : self . key_value } ) setattr ( entity , self . property_name , credentials ) entity . save ( )
4287	def get_thumb ( settings , filename ) : path , filen = os . path . split ( filename ) name , ext = os . path . splitext ( filen ) if ext . lower ( ) in settings [ 'video_extensions' ] : ext = '.jpg' return join ( path , settings [ 'thumb_dir' ] , settings [ 'thumb_prefix' ] + name + settings [ 'thumb_suffix' ] + ext )
11899	def _get_src_from_image ( img , fallback_image_file ) : if img is None : return fallback_image_file target_format = img . format if target_format . lower ( ) in [ 'tif' , 'tiff' ] : target_format = 'JPEG' try : bytesio = io . BytesIO ( ) img . save ( bytesio , target_format ) byte_value = bytesio . getvalue ( ) b64 = base64 . b64encode ( byte_value ) return 'data:image/%s;base64,%s' % ( target_format . lower ( ) , b64 ) except IOError as exptn : print ( 'IOError while saving image bytes: %s' % exptn ) return fallback_image_file
10632	def get_compound_mfr ( self , compound ) : if compound in self . material . compounds : return self . _compound_mfrs [ self . material . get_compound_index ( compound ) ] else : return 0.0
7216	def register ( self , task_json = None , json_filename = None ) : if not task_json and not json_filename : raise Exception ( "Both task json and filename can't be none." ) if task_json and json_filename : raise Exception ( "Both task json and filename can't be provided." ) if json_filename : task_json = json . load ( open ( json_filename , 'r' ) ) r = self . gbdx_connection . post ( self . _base_url , json = task_json ) raise_for_status ( r ) return r . text
5377	def _build_pipeline_input_file_param ( cls , var_name , docker_path ) : path , filename = os . path . split ( docker_path ) if '*' in filename : return cls . _build_pipeline_file_param ( var_name , path + '/' ) else : return cls . _build_pipeline_file_param ( var_name , docker_path )
5524	def jenks_breaks ( values , nb_class ) : if not isinstance ( values , Iterable ) or isinstance ( values , ( str , bytes ) ) : raise TypeError ( "A sequence of numbers is expected" ) if isinstance ( nb_class , float ) and int ( nb_class ) == nb_class : nb_class = int ( nb_class ) if not isinstance ( nb_class , int ) : raise TypeError ( "Number of class have to be a positive integer: " "expected an instance of 'int' but found {}" . format ( type ( nb_class ) ) ) nb_values = len ( values ) if np and isinstance ( values , np . ndarray ) : values = values [ np . argwhere ( np . isfinite ( values ) ) . reshape ( - 1 ) ] else : values = [ i for i in values if isfinite ( i ) ] if len ( values ) != nb_values : warnings . warn ( 'Invalid values encountered (NaN or Inf) were ignored' ) nb_values = len ( values ) if nb_class >= nb_values or nb_class < 2 : raise ValueError ( "Number of class have to be an integer " "greater than 2 and " "smaller than the number of values to use" ) return jenks . _jenks_breaks ( values , nb_class )
7813	def _decode_alt_names ( self , alt_names ) : for alt_name in alt_names : tname = alt_name . getName ( ) comp = alt_name . getComponent ( ) if tname == "dNSName" : key = "DNS" value = _decode_asn1_string ( comp ) elif tname == "uniformResourceIdentifier" : key = "URI" value = _decode_asn1_string ( comp ) elif tname == "otherName" : oid = comp . getComponentByName ( "type-id" ) value = comp . getComponentByName ( "value" ) if oid == XMPPADDR_OID : key = "XmppAddr" value = der_decoder . decode ( value , asn1Spec = UTF8String ( ) ) [ 0 ] value = _decode_asn1_string ( value ) elif oid == SRVNAME_OID : key = "SRVName" value = der_decoder . decode ( value , asn1Spec = IA5String ( ) ) [ 0 ] value = _decode_asn1_string ( value ) else : logger . debug ( "Unknown other name: {0}" . format ( oid ) ) continue else : logger . debug ( "Unsupported general name: {0}" . format ( tname ) ) continue self . alt_names [ key ] . append ( value )
5582	def create ( mapchete_file , process_file , out_format , out_path = None , pyramid_type = None , force = False ) : if os . path . isfile ( process_file ) or os . path . isfile ( mapchete_file ) : if not force : raise IOError ( "file(s) already exists" ) out_path = out_path if out_path else os . path . join ( os . getcwd ( ) , "output" ) process_template = pkg_resources . resource_filename ( "mapchete.static" , "process_template.py" ) process_file = os . path . join ( os . getcwd ( ) , process_file ) copyfile ( process_template , process_file ) mapchete_template = pkg_resources . resource_filename ( "mapchete.static" , "mapchete_template.mapchete" ) output_options = dict ( format = out_format , path = out_path , ** FORMAT_MANDATORY [ out_format ] ) pyramid_options = { 'grid' : pyramid_type } substitute_elements = { 'process_file' : process_file , 'output' : dump ( { 'output' : output_options } , default_flow_style = False ) , 'pyramid' : dump ( { 'pyramid' : pyramid_options } , default_flow_style = False ) } with open ( mapchete_template , 'r' ) as config_template : config = Template ( config_template . read ( ) ) customized_config = config . substitute ( substitute_elements ) with open ( mapchete_file , 'w' ) as target_config : target_config . write ( customized_config )
6550	def from_func ( cls , func , variables , vartype , name = None ) : variables = tuple ( variables ) configurations = frozenset ( config for config in itertools . product ( vartype . value , repeat = len ( variables ) ) if func ( * config ) ) return cls ( func , configurations , variables , vartype , name )
1948	def write_back_memory ( self , where , expr , size ) : if self . write_backs_disabled : return if type ( expr ) is bytes : self . _emu . mem_write ( where , expr ) else : if issymbolic ( expr ) : data = [ Operators . CHR ( Operators . EXTRACT ( expr , offset , 8 ) ) for offset in range ( 0 , size , 8 ) ] concrete_data = [ ] for c in data : if issymbolic ( c ) : c = chr ( solver . get_value ( self . _cpu . memory . constraints , c ) ) concrete_data . append ( c ) data = concrete_data else : data = [ Operators . CHR ( Operators . EXTRACT ( expr , offset , 8 ) ) for offset in range ( 0 , size , 8 ) ] logger . debug ( f"Writing back {hr_size(size // 8)} to {hex(where)}: {data}" ) self . _emu . mem_write ( where , b'' . join ( b . encode ( 'utf-8' ) if type ( b ) is str else b for b in data ) )
7641	def parse_arguments ( args ) : parser = argparse . ArgumentParser ( description = 'Convert JAMS to .lab files' ) parser . add_argument ( '-c' , '--comma-separated' , dest = 'csv' , action = 'store_true' , default = False , help = 'Output in .csv instead of .lab' ) parser . add_argument ( '--comment' , dest = 'comment_char' , type = str , default = '#' , help = 'Comment character' ) parser . add_argument ( '-n' , '--namespace' , dest = 'namespaces' , nargs = '+' , default = [ '.*' ] , help = 'One or more namespaces to output. Default is all.' ) parser . add_argument ( 'jams_file' , help = 'Path to the input jams file' ) parser . add_argument ( 'output_prefix' , help = 'Prefix for output files' ) return vars ( parser . parse_args ( args ) )
3740	def omega ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ 'LK' , 'DEFINITION' ] ) : r def list_methods ( ) : methods = [ ] if CASRN in _crit_PSRKR4 . index and not np . isnan ( _crit_PSRKR4 . at [ CASRN , 'omega' ] ) : methods . append ( 'PSRK' ) if CASRN in _crit_PassutDanner . index and not np . isnan ( _crit_PassutDanner . at [ CASRN , 'omega' ] ) : methods . append ( 'PD' ) if CASRN in _crit_Yaws . index and not np . isnan ( _crit_Yaws . at [ CASRN , 'omega' ] ) : methods . append ( 'YAWS' ) Tcrit , Pcrit = Tc ( CASRN ) , Pc ( CASRN ) if Tcrit and Pcrit : if Tb ( CASRN ) : methods . append ( 'LK' ) if VaporPressure ( CASRN = CASRN ) . T_dependent_property ( Tcrit * 0.7 ) : methods . append ( 'DEFINITION' ) if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( 'NONE' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'PSRK' : _omega = float ( _crit_PSRKR4 . at [ CASRN , 'omega' ] ) elif Method == 'PD' : _omega = float ( _crit_PassutDanner . at [ CASRN , 'omega' ] ) elif Method == 'YAWS' : _omega = float ( _crit_Yaws . at [ CASRN , 'omega' ] ) elif Method == 'LK' : _omega = LK_omega ( Tb ( CASRN ) , Tc ( CASRN ) , Pc ( CASRN ) ) elif Method == 'DEFINITION' : P = VaporPressure ( CASRN = CASRN ) . T_dependent_property ( Tc ( CASRN ) * 0.7 ) _omega = - log10 ( P / Pc ( CASRN ) ) - 1.0 elif Method == 'NONE' : _omega = None else : raise Exception ( 'Failure in in function' ) return _omega
283	def plot_long_short_holdings ( returns , positions , legend_loc = 'upper left' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) positions = positions . drop ( 'cash' , axis = 'columns' ) positions = positions . replace ( 0 , np . nan ) df_longs = positions [ positions > 0 ] . count ( axis = 1 ) df_shorts = positions [ positions < 0 ] . count ( axis = 1 ) lf = ax . fill_between ( df_longs . index , 0 , df_longs . values , color = 'g' , alpha = 0.5 , lw = 2.0 ) sf = ax . fill_between ( df_shorts . index , 0 , df_shorts . values , color = 'r' , alpha = 0.5 , lw = 2.0 ) bf = patches . Rectangle ( [ 0 , 0 ] , 1 , 1 , color = 'darkgoldenrod' ) leg = ax . legend ( [ lf , sf , bf ] , [ 'Long (max: %s, min: %s)' % ( df_longs . max ( ) , df_longs . min ( ) ) , 'Short (max: %s, min: %s)' % ( df_shorts . max ( ) , df_shorts . min ( ) ) , 'Overlap' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) leg . get_frame ( ) . set_edgecolor ( 'black' ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) ax . set_title ( 'Long and short holdings' ) ax . set_ylabel ( 'Holdings' ) ax . set_xlabel ( '' ) return ax
8497	def _output ( calls , args ) : if args . natural_sort or args . source : calls = sorted ( calls , key = lambda c : ( c . filename , c . lineno ) ) else : calls = sorted ( calls , key = lambda c : c . key ) out = [ ] if args . only_keys : keys = set ( ) for call in calls : if call . key in keys : continue out . append ( _format_call ( call , args ) ) keys . add ( call . key ) out = '\n' . join ( out ) if args . color : out = _colorize ( out ) print ( out , end = ' ' ) return keys = set ( ) for call in calls : if call . default : keys . add ( call . key ) for call in calls : if not args . all and not call . default and call . key in keys : continue out . append ( _format_call ( call , args ) ) out = '\n' . join ( out ) if args . color : out = _colorize ( out ) print ( out , end = ' ' )
4789	def is_digit ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if not self . val . isdigit ( ) : self . _err ( 'Expected <%s> to contain only digits, but did not.' % self . val ) return self
7130	def create_log_config ( verbose , quiet ) : if verbose and quiet : raise ValueError ( "Supplying both --quiet and --verbose makes no sense." ) elif verbose : level = logging . DEBUG elif quiet : level = logging . ERROR else : level = logging . INFO logger_cfg = { "handlers" : [ "click_handler" ] , "level" : level } return { "version" : 1 , "formatters" : { "click_formatter" : { "format" : "%(message)s" } } , "handlers" : { "click_handler" : { "level" : level , "class" : "doc2dash.__main__.ClickEchoHandler" , "formatter" : "click_formatter" , } } , "loggers" : { "doc2dash" : logger_cfg , "__main__" : logger_cfg } , }
3820	async def create_conversation ( self , create_conversation_request ) : response = hangouts_pb2 . CreateConversationResponse ( ) await self . _pb_request ( 'conversations/createconversation' , create_conversation_request , response ) return response
8264	def swarm ( self , x , y , r = 100 ) : sc = _ctx . stroke ( 0 , 0 , 0 , 0 ) sw = _ctx . strokewidth ( 0 ) _ctx . push ( ) _ctx . transform ( _ctx . CORNER ) _ctx . translate ( x , y ) for i in _range ( r * 3 ) : clr = choice ( self ) . copy ( ) clr . alpha -= 0.5 * random ( ) _ctx . fill ( clr ) clr = choice ( self ) _ctx . stroke ( clr ) _ctx . strokewidth ( 10 * random ( ) ) _ctx . rotate ( 360 * random ( ) ) r2 = r * 0.5 * random ( ) _ctx . oval ( r * random ( ) , 0 , r2 , r2 ) _ctx . pop ( ) _ctx . strokewidth ( sw ) if sc is None : _ctx . nostroke ( ) else : _ctx . stroke ( sc )
11325	def extract_oembeds ( self , text , maxwidth = None , maxheight = None , resource_type = None ) : parser = text_parser ( ) urls = parser . extract_urls ( text ) return self . handle_extracted_urls ( urls , maxwidth , maxheight , resource_type )
10737	def path_from_keywords ( keywords , into = 'path' ) : subdirs = [ ] def prepare_string ( s ) : s = str ( s ) s = re . sub ( '[][{},*"' + f"'{os.sep}]" , '_' , s ) if into == 'file' : s = s . replace ( '_' , ' ' ) if ' ' in s : s = s . title ( ) s = s . replace ( ' ' , '' ) return s if isinstance ( keywords , set ) : keywords_list = sorted ( keywords ) for property in keywords_list : subdirs . append ( prepare_string ( property ) ) else : keywords_list = sorted ( keywords . items ( ) ) for property , value in keywords_list : if Bool . valid ( value ) : subdirs . append ( ( '' if value else ( 'not_' if into == 'path' else 'not' ) ) + prepare_string ( property ) ) elif ( Float | Integer ) . valid ( value ) : subdirs . append ( '{}{}' . format ( prepare_string ( property ) , prepare_string ( value ) ) ) else : subdirs . append ( '{}{}{}' . format ( prepare_string ( property ) , '_' if into == 'path' else '' , prepare_string ( value ) ) ) if into == 'path' : out = os . path . join ( * subdirs ) else : out = '_' . join ( subdirs ) return out
7202	def deprecate_module_attr ( mod , deprecated ) : deprecated = set ( deprecated ) class Wrapper ( object ) : def __getattr__ ( self , attr ) : if attr in deprecated : warnings . warn ( "Property {} is deprecated" . format ( attr ) , GBDXDeprecation ) return getattr ( mod , attr ) def __setattr__ ( self , attr , value ) : if attr in deprecated : warnings . warn ( "Property {} is deprecated" . format ( attr ) , GBDXDeprecation ) return setattr ( mod , attr , value ) return Wrapper ( )
9195	def get_publication ( request ) : publication_id = request . matchdict [ 'id' ] state , messages = check_publication_state ( publication_id ) response_data = { 'publication' : publication_id , 'state' : state , 'messages' : messages , } return response_data
5458	def from_yaml ( cls , yaml_string ) : try : job = yaml . full_load ( yaml_string ) except AttributeError : job = yaml . load ( yaml_string ) dsub_version = job . get ( 'dsub-version' ) if not dsub_version : return cls . _from_yaml_v0 ( job ) job_metadata = { } for key in [ 'job-id' , 'job-name' , 'task-ids' , 'user-id' , 'dsub-version' , 'user-project' , 'script-name' ] : if job . get ( key ) is not None : job_metadata [ key ] = job . get ( key ) job_metadata [ 'create-time' ] = dsub_util . replace_timezone ( job . get ( 'create-time' ) , pytz . utc ) job_resources = Resources ( logging = job . get ( 'logging' ) ) job_params = { } job_params [ 'labels' ] = cls . _label_params_from_dict ( job . get ( 'labels' , { } ) ) job_params [ 'envs' ] = cls . _env_params_from_dict ( job . get ( 'envs' , { } ) ) job_params [ 'inputs' ] = cls . _input_file_params_from_dict ( job . get ( 'inputs' , { } ) , False ) job_params [ 'input-recursives' ] = cls . _input_file_params_from_dict ( job . get ( 'input-recursives' , { } ) , True ) job_params [ 'outputs' ] = cls . _output_file_params_from_dict ( job . get ( 'outputs' , { } ) , False ) job_params [ 'output-recursives' ] = cls . _output_file_params_from_dict ( job . get ( 'output-recursives' , { } ) , True ) job_params [ 'mounts' ] = cls . _mount_params_from_dict ( job . get ( 'mounts' , { } ) ) task_descriptors = [ ] for task in job . get ( 'tasks' , [ ] ) : task_metadata = { 'task-id' : task . get ( 'task-id' ) } create_time = task . get ( 'create-time' ) if create_time : task_metadata [ 'create-time' ] = dsub_util . replace_timezone ( create_time , pytz . utc ) if task . get ( 'task-attempt' ) is not None : task_metadata [ 'task-attempt' ] = task . get ( 'task-attempt' ) task_params = { } task_params [ 'labels' ] = cls . _label_params_from_dict ( task . get ( 'labels' , { } ) ) task_params [ 'envs' ] = cls . _env_params_from_dict ( task . get ( 'envs' , { } ) ) task_params [ 'inputs' ] = cls . _input_file_params_from_dict ( task . get ( 'inputs' , { } ) , False ) task_params [ 'input-recursives' ] = cls . _input_file_params_from_dict ( task . get ( 'input-recursives' , { } ) , True ) task_params [ 'outputs' ] = cls . _output_file_params_from_dict ( task . get ( 'outputs' , { } ) , False ) task_params [ 'output-recursives' ] = cls . _output_file_params_from_dict ( task . get ( 'output-recursives' , { } ) , True ) task_resources = Resources ( logging_path = task . get ( 'logging-path' ) ) task_descriptors . append ( TaskDescriptor ( task_metadata , task_params , task_resources ) ) return JobDescriptor ( job_metadata , job_params , job_resources , task_descriptors )
9215	def t_istringapostrophe_css_string ( self , t ) : r'[^\'@]+' t . lexer . lineno += t . value . count ( '\n' ) return t
991	def copy ( reader , writer , start , stop , insertLocation = None , tsCol = None ) : assert stop >= start startRows = [ ] copyRows = [ ] ts = None inc = None if tsCol is None : tsCol = reader . getTimestampFieldIdx ( ) for i , row in enumerate ( reader ) : if ts is None : ts = row [ tsCol ] elif inc is None : inc = row [ tsCol ] - ts if i >= start and i <= stop : copyRows . append ( row ) startRows . append ( row ) if insertLocation is None : insertLocation = stop + 1 startRows [ insertLocation : insertLocation ] = copyRows for row in startRows : row [ tsCol ] = ts writer . appendRecord ( row ) ts += inc
2809	def convert_constant ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting constant ...' ) params_list = params [ 'value' ] . numpy ( ) def target_layer ( x , value = params_list ) : return tf . constant ( value . tolist ( ) , shape = value . shape ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name + '_np' ] = params_list layers [ scope_name ] = lambda_layer ( layers [ list ( layers . keys ( ) ) [ 0 ] ] )
11069	def mongo ( daemon = False , port = 20771 ) : cmd = "mongod --port {0}" . format ( port ) if daemon : cmd += " --fork" run ( cmd )
10825	def search ( cls , query , q ) : query = query . join ( User ) . filter ( User . email . like ( '%{0}%' . format ( q ) ) , ) return query
3241	def _get_base ( server_certificate , ** conn ) : server_certificate [ '_version' ] = 1 cert_details = get_server_certificate_api ( server_certificate [ 'ServerCertificateName' ] , ** conn ) if cert_details : server_certificate . update ( cert_details [ 'ServerCertificateMetadata' ] ) server_certificate [ 'CertificateBody' ] = cert_details [ 'CertificateBody' ] server_certificate [ 'CertificateChain' ] = cert_details . get ( 'CertificateChain' , None ) server_certificate [ 'UploadDate' ] = get_iso_string ( server_certificate [ 'UploadDate' ] ) server_certificate [ 'Expiration' ] = get_iso_string ( server_certificate [ 'Expiration' ] ) return server_certificate
67	def extract_from_image ( self , image , pad = True , pad_max = None , prevent_zero_size = True ) : pad_top = 0 pad_right = 0 pad_bottom = 0 pad_left = 0 height , width = image . shape [ 0 ] , image . shape [ 1 ] x1 , x2 , y1 , y2 = self . x1_int , self . x2_int , self . y1_int , self . y2_int fully_within = self . is_fully_within_image ( image ) if fully_within : y1 , y2 = np . clip ( [ y1 , y2 ] , 0 , height - 1 ) x1 , x2 = np . clip ( [ x1 , x2 ] , 0 , width - 1 ) if prevent_zero_size : if abs ( x2 - x1 ) < 1 : x2 = x1 + 1 if abs ( y2 - y1 ) < 1 : y2 = y1 + 1 if pad : if x1 < 0 : pad_left = abs ( x1 ) x2 = x2 + pad_left width = width + pad_left x1 = 0 if y1 < 0 : pad_top = abs ( y1 ) y2 = y2 + pad_top height = height + pad_top y1 = 0 if x2 >= width : pad_right = x2 - width if y2 >= height : pad_bottom = y2 - height paddings = [ pad_top , pad_right , pad_bottom , pad_left ] any_padded = any ( [ val > 0 for val in paddings ] ) if any_padded : if pad_max is None : pad_max = max ( paddings ) image = ia . pad ( image , top = min ( pad_top , pad_max ) , right = min ( pad_right , pad_max ) , bottom = min ( pad_bottom , pad_max ) , left = min ( pad_left , pad_max ) ) return image [ y1 : y2 , x1 : x2 ] else : within_image = ( ( 0 , 0 , 0 , 0 ) <= ( x1 , y1 , x2 , y2 ) < ( width , height , width , height ) ) out_height , out_width = ( y2 - y1 ) , ( x2 - x1 ) nonzero_height = ( out_height > 0 ) nonzero_width = ( out_width > 0 ) if within_image and nonzero_height and nonzero_width : return image [ y1 : y2 , x1 : x2 ] if prevent_zero_size : out_height = 1 out_width = 1 else : out_height = 0 out_width = 0 if image . ndim == 2 : return np . zeros ( ( out_height , out_width ) , dtype = image . dtype ) return np . zeros ( ( out_height , out_width , image . shape [ - 1 ] ) , dtype = image . dtype )
11834	def connect ( self , A , B , distance = 1 ) : self . connect1 ( A , B , distance ) if not self . directed : self . connect1 ( B , A , distance )
13594	def print_information ( handler , label ) : click . echo ( '=> Latest stable: {tag}' . format ( tag = click . style ( str ( handler . latest_stable or 'N/A' ) , fg = 'yellow' if handler . latest_stable else 'magenta' ) ) ) if label is not None : latest_revision = handler . latest_revision ( label ) click . echo ( '=> Latest relative revision ({label}): {tag}' . format ( label = click . style ( label , fg = 'blue' ) , tag = click . style ( str ( latest_revision or 'N/A' ) , fg = 'yellow' if latest_revision else 'magenta' ) ) )
3327	def refresh ( self , token , timeout = None ) : if timeout is None : timeout = LockManager . LOCK_TIME_OUT_DEFAULT return self . storage . refresh ( token , timeout )
11592	def _rc_sunionstore ( self , dst , src , * args ) : args = list_or_args ( src , args ) result = self . sunion ( * args ) if result is not set ( [ ] ) : return self . sadd ( dst , * list ( result ) ) return 0
234	def plot_sector_exposures_net ( net_exposures , sector_dict = None , ax = None ) : if ax is None : ax = plt . gca ( ) if sector_dict is None : sector_names = SECTORS . values ( ) else : sector_names = sector_dict . values ( ) color_list = plt . cm . gist_rainbow ( np . linspace ( 0 , 1 , 11 ) ) for i in range ( len ( net_exposures ) ) : ax . plot ( net_exposures [ i ] , color = color_list [ i ] , alpha = 0.8 , label = sector_names [ i ] ) ax . set ( title = 'Net exposures to sectors' , ylabel = 'Proportion of net exposure \n in sectors' ) return ax
4135	def check_md5sum_change ( src_file ) : src_md5 = get_md5sum ( src_file ) src_md5_file = src_file + '.md5' src_file_changed = True if os . path . exists ( src_md5_file ) : with open ( src_md5_file , 'r' ) as file_checksum : ref_md5 = file_checksum . read ( ) if src_md5 == ref_md5 : src_file_changed = False if src_file_changed : with open ( src_md5_file , 'w' ) as file_checksum : file_checksum . write ( src_md5 ) return src_file_changed
6592	def poll ( self ) : self . runid_to_return . extend ( self . dispatcher . poll ( ) ) ret = self . _collect_all_finished_pkgidx_result_pairs ( ) return ret
3242	def boto3_cached_conn ( service , service_type = 'client' , future_expiration_minutes = 15 , account_number = None , assume_role = None , session_name = 'cloudaux' , region = 'us-east-1' , return_credentials = False , external_id = None , arn_partition = 'aws' ) : key = ( account_number , assume_role , session_name , external_id , region , service_type , service , arn_partition ) if key in CACHE : retval = _get_cached_creds ( key , service , service_type , region , future_expiration_minutes , return_credentials ) if retval : return retval role = None if assume_role : sts = boto3 . session . Session ( ) . client ( 'sts' ) if not all ( [ account_number , assume_role ] ) : raise ValueError ( "Account number and role to assume are both required" ) arn = 'arn:{partition}:iam::{0}:role/{1}' . format ( account_number , assume_role , partition = arn_partition ) assume_role_kwargs = { 'RoleArn' : arn , 'RoleSessionName' : session_name } if external_id : assume_role_kwargs [ 'ExternalId' ] = external_id role = sts . assume_role ( ** assume_role_kwargs ) if service_type == 'client' : conn = _client ( service , region , role ) elif service_type == 'resource' : conn = _resource ( service , region , role ) if role : CACHE [ key ] = role if return_credentials : return conn , role [ 'Credentials' ] return conn
9512	def gaps ( self , min_length = 1 ) : gaps = [ ] regex = re . compile ( 'N+' , re . IGNORECASE ) for m in regex . finditer ( self . seq ) : if m . span ( ) [ 1 ] - m . span ( ) [ 0 ] + 1 >= min_length : gaps . append ( intervals . Interval ( m . span ( ) [ 0 ] , m . span ( ) [ 1 ] - 1 ) ) return gaps
9946	def cur_space ( self , name = None ) : if name is None : return self . _impl . model . currentspace . interface else : self . _impl . model . currentspace = self . _impl . spaces [ name ] return self . cur_space ( )
1023	def assertNoTMDiffs ( tms ) : if len ( tms ) == 1 : return if len ( tms ) > 2 : raise "Not implemented for more than 2 TMs" same = fdrutils . tmDiff2 ( tms . values ( ) , verbosity = VERBOSITY ) assert ( same ) return
13520	def configure ( self , url = None , token = None , test = False ) : if url is None : url = Config . get_value ( "url" ) if token is None : token = Config . get_value ( "token" ) self . server_url = url self . auth_header = { "Authorization" : "Basic {0}" . format ( token ) } self . configured = True if test : self . test_connection ( ) Config . set ( "url" , url ) Config . set ( "token" , token )
12129	def _build_specs ( self , specs , kwargs , fp_precision ) : if specs is None : overrides = param . ParamOverrides ( self , kwargs , allow_extra_keywords = True ) extra_kwargs = overrides . extra_keywords ( ) kwargs = dict ( [ ( k , v ) for ( k , v ) in kwargs . items ( ) if k not in extra_kwargs ] ) rounded_specs = list ( self . round_floats ( [ extra_kwargs ] , fp_precision ) ) if extra_kwargs == { } : return [ ] , kwargs , True else : return rounded_specs , kwargs , False return list ( self . round_floats ( specs , fp_precision ) ) , kwargs , True
4872	def to_representation ( self , data ) : return [ self . child . to_representation ( item ) if 'detail' in item else item for item in data ]
668	def sample ( self , rgen ) : x = rgen . poisson ( self . lambdaParameter ) return x , self . logDensity ( x )
6046	def array_2d_from_array_1d ( self , padded_array_1d ) : padded_array_2d = self . map_to_2d_keep_padded ( padded_array_1d ) pad_size_0 = self . mask . shape [ 0 ] - self . image_shape [ 0 ] pad_size_1 = self . mask . shape [ 1 ] - self . image_shape [ 1 ] return ( padded_array_2d [ pad_size_0 // 2 : self . mask . shape [ 0 ] - pad_size_0 // 2 , pad_size_1 // 2 : self . mask . shape [ 1 ] - pad_size_1 // 2 ] )
1366	def validateInterval ( self , startTime , endTime ) : start = int ( startTime ) end = int ( endTime ) if start > end : raise Exception ( "starttime is greater than endtime." )
9121	def dropbox_submission ( dropbox , request ) : try : data = dropbox_schema . deserialize ( request . POST ) except Exception : return HTTPFound ( location = request . route_url ( 'dropbox_form' ) ) dropbox . message = data . get ( 'message' ) if 'testing_secret' in dropbox . settings : dropbox . from_watchdog = is_equal ( unicode ( dropbox . settings [ 'test_submission_secret' ] ) , data . pop ( 'testing_secret' , u'' ) ) if data . get ( 'upload' ) is not None : dropbox . add_attachment ( data [ 'upload' ] ) dropbox . submit ( ) drop_url = request . route_url ( 'dropbox_view' , drop_id = dropbox . drop_id ) print ( "Created dropbox %s" % drop_url ) return HTTPFound ( location = drop_url )
1603	def to_table ( metrics ) : all_queries = tracker_access . metric_queries ( ) m = tracker_access . queries_map ( ) names = metrics . values ( ) [ 0 ] . keys ( ) stats = [ ] for n in names : info = [ n ] for field in all_queries : try : info . append ( str ( metrics [ field ] [ n ] ) ) except KeyError : pass stats . append ( info ) header = [ 'container id' ] + [ m [ k ] for k in all_queries if k in metrics . keys ( ) ] return stats , header
1861	def MOVS ( cpu , dest , src ) : base , size , ty = cpu . get_descriptor ( cpu . DS ) src_addr = src . address ( ) + base dest_addr = dest . address ( ) + base src_reg = src . mem . base dest_reg = dest . mem . base size = dest . size dest . write ( src . read ( ) ) increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( src_reg , cpu . read_register ( src_reg ) + increment ) cpu . write_register ( dest_reg , cpu . read_register ( dest_reg ) + increment )
13737	def get_context ( request , model = None ) : param_values = get_param_values ( request , model = model ) context = param_values . pop ( 'orb_context' , { } ) if isinstance ( context , ( unicode , str ) ) : context = projex . rest . unjsonify ( context ) has_limit = 'limit' in context or 'limit' in param_values orb_context = orb . Context ( ** context ) used = set ( ) query_context = { } for key in orb . Context . Defaults : if key in param_values : used . add ( key ) query_context [ key ] = param_values . get ( key ) schema_values = { } if model : for key , value in request . matchdict . items ( ) : if model . schema ( ) . column ( key , raise_ = False ) : schema_values [ key ] = value for key , value in param_values . items ( ) : root_key = key . split ( '.' ) [ 0 ] schema_object = model . schema ( ) . column ( root_key , raise_ = False ) or model . schema ( ) . collector ( root_key ) if schema_object : value = param_values . pop ( key ) if isinstance ( schema_object , orb . Collector ) and type ( value ) not in ( tuple , list ) : value = [ value ] schema_values [ key ] = value query_context [ 'scope' ] = { 'request' : request } try : default_context = request . orb_default_context except AttributeError : try : query_context [ 'scope' ] . update ( request . orb_scope ) except AttributeError : pass else : if 'scope' in default_context : query_context [ 'scope' ] . update ( default_context . pop ( 'scope' ) ) for k , v in default_context . items ( ) : query_context . setdefault ( k , v ) orb_context . update ( query_context ) return schema_values , orb_context
11693	def label_suspicious ( self , reason ) : self . suspicion_reasons . append ( reason ) self . is_suspect = True
12590	def get_reliabledictionary_list ( client , application_name , service_name ) : cluster = Cluster . from_sfclient ( client ) service = cluster . get_application ( application_name ) . get_service ( service_name ) for dictionary in service . get_dictionaries ( ) : print ( dictionary . name )
8125	def draw_cornu_bezier ( x0 , y0 , t0 , t1 , s0 , c0 , flip , cs , ss , cmd , scale , rot ) : s = None for j in range ( 0 , 5 ) : t = j * .2 t2 = t + .2 curvetime = t0 + t * ( t1 - t0 ) curvetime2 = t0 + t2 * ( t1 - t0 ) Dt = ( curvetime2 - curvetime ) * scale if not s : s , c = eval_cornu ( curvetime ) s *= flip s -= s0 c -= c0 dx1 = cos ( pow ( curvetime , 2 ) + ( flip * rot ) ) dy1 = flip * sin ( pow ( curvetime , 2 ) + ( flip * rot ) ) x = ( ( c * cs - s * ss ) + x0 ) y = ( ( s * cs + c * ss ) + y0 ) s2 , c2 = eval_cornu ( curvetime2 ) s2 *= flip s2 -= s0 c2 -= c0 dx2 = cos ( pow ( curvetime2 , 2 ) + ( flip * rot ) ) dy2 = flip * sin ( pow ( curvetime2 , 2 ) + ( flip * rot ) ) x3 = ( ( c2 * cs - s2 * ss ) + x0 ) y3 = ( ( s2 * cs + c2 * ss ) + y0 ) x1 = ( x + ( ( Dt / 3.0 ) * dx1 ) ) y1 = ( y + ( ( Dt / 3.0 ) * dy1 ) ) x2 = ( x3 - ( ( Dt / 3.0 ) * dx2 ) ) y2 = ( y3 - ( ( Dt / 3.0 ) * dy2 ) ) if cmd == 'moveto' : print_pt ( x , y , cmd ) cmd = 'curveto' print_crv ( x1 , y1 , x2 , y2 , x3 , y3 ) dx1 , dy1 = dx2 , dy2 x , y = x3 , y3 return cmd
1046	def context ( self , * notes ) : self . _appended_notes += notes yield del self . _appended_notes [ - len ( notes ) : ]
4505	def get_and_run_edits ( self ) : if self . empty ( ) : return edits = [ ] while True : try : edits . append ( self . get_nowait ( ) ) except queue . Empty : break for e in edits : try : e ( ) except : log . error ( 'Error on edit %s' , e ) traceback . print_exc ( )
7752	def process_presence ( self , stanza ) : stanza_type = stanza . stanza_type return self . __try_handlers ( self . _presence_handlers , stanza , stanza_type )
12869	async def manage ( self ) : cm = _ContextManager ( self . database ) if isinstance ( self . database . obj , AIODatabase ) : cm . connection = await self . database . async_connect ( ) else : cm . connection = self . database . connect ( ) return cm
6904	def hms_to_decimal ( hours , minutes , seconds , returndeg = True ) : if hours > 24 : return None else : dec_hours = fabs ( hours ) + fabs ( minutes ) / 60.0 + fabs ( seconds ) / 3600.0 if returndeg : dec_deg = dec_hours * 15.0 if dec_deg < 0 : dec_deg = dec_deg + 360.0 dec_deg = dec_deg % 360.0 return dec_deg else : return dec_hours
12949	def copyModel ( mdl ) : copyNum = _modelCopyMap [ mdl ] _modelCopyMap [ mdl ] += 1 mdlCopy = type ( mdl . __name__ + '_Copy' + str ( copyNum ) , mdl . __bases__ , copy . deepcopy ( dict ( mdl . __dict__ ) ) ) mdlCopy . FIELDS = [ field . copy ( ) for field in mdl . FIELDS ] mdlCopy . INDEXED_FIELDS = [ str ( idxField ) for idxField in mdl . INDEXED_FIELDS ] mdlCopy . validateModel ( ) return mdlCopy
1309	def _CreateInput ( structure ) -> INPUT : if isinstance ( structure , MOUSEINPUT ) : return INPUT ( InputType . Mouse , _INPUTUnion ( mi = structure ) ) if isinstance ( structure , KEYBDINPUT ) : return INPUT ( InputType . Keyboard , _INPUTUnion ( ki = structure ) ) if isinstance ( structure , HARDWAREINPUT ) : return INPUT ( InputType . Hardware , _INPUTUnion ( hi = structure ) ) raise TypeError ( 'Cannot create INPUT structure!' )
2661	def hold_worker ( self , worker_id ) : c = self . command_client . run ( "HOLD_WORKER;{}" . format ( worker_id ) ) logger . debug ( "Sent hold request to worker: {}" . format ( worker_id ) ) return c
3129	def get_subscriber_hash ( member_email ) : check_email ( member_email ) member_email = member_email . lower ( ) . encode ( ) m = hashlib . md5 ( member_email ) return m . hexdigest ( )
6041	def unmasked_sparse_to_sparse ( self ) : return mapping_util . unmasked_sparse_to_sparse_from_mask_and_pixel_centres ( mask = self . regular_grid . mask , unmasked_sparse_grid_pixel_centres = self . unmasked_sparse_grid_pixel_centres , total_sparse_pixels = self . total_sparse_pixels ) . astype ( 'int' )
8705	def write_file ( self , path , destination = '' , verify = 'none' ) : filename = os . path . basename ( path ) if not destination : destination = filename log . info ( 'Transferring %s as %s' , path , destination ) self . __writeln ( "recv()" ) res = self . __expect ( 'C> ' ) if not res . endswith ( 'C> ' ) : log . error ( 'Error waiting for esp "%s"' , res ) raise CommunicationTimeout ( 'Error waiting for device to start receiving' , res ) log . debug ( 'sending destination filename "%s"' , destination ) self . __write ( destination + '\x00' , True ) if not self . __got_ack ( ) : log . error ( 'did not ack destination filename' ) raise NoAckException ( 'Device did not ACK destination filename' ) content = from_file ( path ) log . debug ( 'sending %d bytes in %s' , len ( content ) , filename ) pos = 0 chunk_size = 128 while pos < len ( content ) : rest = len ( content ) - pos if rest > chunk_size : rest = chunk_size data = content [ pos : pos + rest ] if not self . __write_chunk ( data ) : resp = self . __expect ( ) log . error ( 'Bad chunk response "%s" %s' , resp , hexify ( resp ) ) raise BadResponseException ( 'Bad chunk response' , ACK , resp ) pos += chunk_size log . debug ( 'sending zero block' ) self . __write_chunk ( '' ) if verify != 'none' : self . verify_file ( path , destination , verify )
11675	def bare ( self ) : "Make a Features object with no metadata; points to the same features." if not self . meta : return self elif self . stacked : return Features ( self . stacked_features , self . n_pts , copy = False ) else : return Features ( self . features , copy = False )
13323	def format_objects ( objects , children = False , columns = None , header = True ) : columns = columns or ( 'NAME' , 'TYPE' , 'PATH' ) objects = sorted ( objects , key = _type_and_name ) data = [ ] for obj in objects : if isinstance ( obj , cpenv . VirtualEnvironment ) : data . append ( get_info ( obj ) ) modules = obj . get_modules ( ) if children and modules : for mod in modules : data . append ( get_info ( mod , indent = 2 , root = obj . path ) ) else : data . append ( get_info ( obj ) ) maxes = [ len ( max ( col , key = len ) ) for col in zip ( * data ) ] tmpl = '{:%d} {:%d} {:%d}' % tuple ( maxes ) lines = [ ] if header : lines . append ( '\n' + bold_blue ( tmpl . format ( * columns ) ) ) for obj_data in data : lines . append ( tmpl . format ( * obj_data ) ) return '\n' . join ( lines )
2371	def settings ( self ) : for table in self . tables : if isinstance ( table , SettingTable ) : for statement in table . statements : yield statement
5566	def params_at_zoom ( self , zoom ) : if zoom not in self . init_zoom_levels : raise ValueError ( "zoom level not available with current configuration" ) out = dict ( self . _params_at_zoom [ zoom ] , input = { } , output = self . output ) if "input" in self . _params_at_zoom [ zoom ] : flat_inputs = { } for k , v in _flatten_tree ( self . _params_at_zoom [ zoom ] [ "input" ] ) : if v is None : flat_inputs [ k ] = None else : flat_inputs [ k ] = self . input [ get_hash ( v ) ] out [ "input" ] = _unflatten_tree ( flat_inputs ) else : out [ "input" ] = { } return out
2579	def cleanup ( self ) : logger . info ( "DFK cleanup initiated" ) if self . cleanup_called : raise Exception ( "attempt to clean up DFK when it has already been cleaned-up" ) self . cleanup_called = True self . log_task_states ( ) if self . checkpoint_mode is not None : self . checkpoint ( ) if self . _checkpoint_timer : logger . info ( "Stopping checkpoint timer" ) self . _checkpoint_timer . close ( ) self . usage_tracker . send_message ( ) self . usage_tracker . close ( ) logger . info ( "Terminating flow_control and strategy threads" ) self . flowcontrol . close ( ) for executor in self . executors . values ( ) : if executor . managed : if executor . scaling_enabled : job_ids = executor . provider . resources . keys ( ) executor . scale_in ( len ( job_ids ) ) executor . shutdown ( ) self . time_completed = datetime . datetime . now ( ) if self . monitoring : self . monitoring . send ( MessageType . WORKFLOW_INFO , { 'tasks_failed_count' : self . tasks_failed_count , 'tasks_completed_count' : self . tasks_completed_count , "time_began" : self . time_began , 'time_completed' : self . time_completed , 'workflow_duration' : ( self . time_completed - self . time_began ) . total_seconds ( ) , 'run_id' : self . run_id , 'rundir' : self . run_dir } ) self . monitoring . close ( ) logger . info ( "DFK cleanup complete" )
9841	def __field ( self ) : try : tok = self . __consume ( ) except DXParserNoTokens : return if tok . equals ( 'component' ) : component = self . __consume ( ) . value ( ) if not self . __consume ( ) . equals ( 'value' ) : raise DXParseError ( 'field: "value" expected' ) classid = self . __consume ( ) . value ( ) try : self . currentobject [ 'components' ] [ component ] = classid except KeyError : self . currentobject [ 'components' ] = { component : classid } else : raise DXParseError ( 'field: ' + str ( tok ) + ' not recognized.' )
9208	def remove_prefix ( bytes_ ) : prefix_int = extract_prefix ( bytes_ ) prefix = varint . encode ( prefix_int ) return bytes_ [ len ( prefix ) : ]
11810	def index_document ( self , text , url ) : "Index the text of a document." title = text [ : text . index ( '\n' ) ] . strip ( ) docwords = words ( text ) docid = len ( self . documents ) self . documents . append ( Document ( title , url , len ( docwords ) ) ) for word in docwords : if word not in self . stopwords : self . index [ word ] [ docid ] += 1
12434	def redirect ( cls , request , response ) : if cls . meta . legacy_redirect : if request . method in ( 'GET' , 'HEAD' , ) : response . status = http . client . MOVED_PERMANENTLY else : response . status = http . client . TEMPORARY_REDIRECT else : response . status = http . client . PERMANENT_REDIRECT response . close ( )
8619	def getServerInfo ( pbclient = None , dc_id = None ) : if pbclient is None : raise ValueError ( "argument 'pbclient' must not be None" ) if dc_id is None : raise ValueError ( "argument 'dc_id' must not be None" ) server_info = [ ] servers = pbclient . list_servers ( dc_id , 1 ) for server in servers [ 'items' ] : props = server [ 'properties' ] info = dict ( id = server [ 'id' ] , name = props [ 'name' ] , state = server [ 'metadata' ] [ 'state' ] , vmstate = props [ 'vmState' ] ) server_info . append ( info ) return server_info
7079	def tic_xmatch ( ra , decl , radius_arcsec = 5.0 , apiversion = 'v0' , forcefetch = False , cachedir = '~/.astrobase/mast-cache' , verbose = True , timeout = 90.0 , refresh = 5.0 , maxtimeout = 180.0 , maxtries = 3 , jitter = 5.0 , raiseonfail = False ) : service = 'Mast.Tic.Crossmatch' xmatch_input = { 'fields' : [ { 'name' : 'ra' , 'type' : 'float' } , { 'name' : 'dec' , 'type' : 'float' } ] } xmatch_input [ 'data' ] = [ { 'ra' : x , 'dec' : y } for ( x , y ) in zip ( ra , decl ) ] params = { 'raColumn' : 'ra' , 'decColumn' : 'dec' , 'radius' : radius_arcsec / 3600.0 } return mast_query ( service , params , data = xmatch_input , jitter = jitter , apiversion = apiversion , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , raiseonfail = raiseonfail )
3513	def clicky ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ClickyNode ( )
2747	def get_all_regions ( self ) : data = self . get_data ( "regions/" ) regions = list ( ) for jsoned in data [ 'regions' ] : region = Region ( ** jsoned ) region . token = self . token regions . append ( region ) return regions
791	def jobSetCompleted ( self , jobID , completionReason , completionMsg , useConnectionID = True ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET status=%%s, ' ' completion_reason=%%s, ' ' completion_msg=%%s, ' ' end_time=UTC_TIMESTAMP(), ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE job_id=%%s' % ( self . jobsTableName , ) sqlParams = [ self . STATUS_COMPLETED , completionReason , completionMsg , jobID ] if useConnectionID : query += ' AND _eng_cjm_conn_id=%s' sqlParams . append ( self . _connectionID ) result = conn . cursor . execute ( query , sqlParams ) if result != 1 : raise RuntimeError ( "Tried to change the status of jobID=%s to " "completed, but this job could not be found or " "belongs to some other CJM" % ( jobID ) )
9819	def teardown ( self , hooks = True ) : if not self . is_valid : raise PolyaxonDeploymentConfigError ( 'Deployment type `{}` not supported' . format ( self . deployment_type ) ) if self . is_kubernetes : self . teardown_on_kubernetes ( hooks = hooks ) elif self . is_docker_compose : self . teardown_on_docker_compose ( ) elif self . is_docker : self . teardown_on_docker ( hooks = hooks ) elif self . is_heroku : self . teardown_on_heroku ( hooks = hooks )
9590	def switch_to_window ( self , window_name ) : data = { 'name' : window_name } self . _execute ( Command . SWITCH_TO_WINDOW , data )
11383	def parser ( self ) : module = self . module subcommands = self . subcommands if subcommands : module_desc = inspect . getdoc ( module ) parser = Parser ( description = module_desc , module = module ) subparsers = parser . add_subparsers ( ) for sc_name , callback in subcommands . items ( ) : sc_name = sc_name . replace ( "_" , "-" ) cb_desc = inspect . getdoc ( callback ) sc_parser = subparsers . add_parser ( sc_name , callback = callback , help = cb_desc ) else : parser = Parser ( callback = self . callbacks [ self . function_name ] , module = module ) return parser
6161	def scatter ( x , Ns , start ) : xI = np . real ( x [ start : : Ns ] ) xQ = np . imag ( x [ start : : Ns ] ) return xI , xQ
625	def _neighbors ( coordinate , radius ) : ranges = ( xrange ( n - radius , n + radius + 1 ) for n in coordinate . tolist ( ) ) return numpy . array ( list ( itertools . product ( * ranges ) ) )
3895	def print_table ( col_tuple , row_tuples ) : col_widths = [ max ( len ( str ( row [ col ] ) ) for row in [ col_tuple ] + row_tuples ) for col in range ( len ( col_tuple ) ) ] format_str = ' ' . join ( '{{:<{}}}' . format ( col_width ) for col_width in col_widths ) header_border = ' ' . join ( '=' * col_width for col_width in col_widths ) print ( header_border ) print ( format_str . format ( * col_tuple ) ) print ( header_border ) for row_tuple in row_tuples : print ( format_str . format ( * row_tuple ) ) print ( header_border ) print ( )
11788	def smooth_for ( self , o ) : if o not in self . dictionary : self . dictionary [ o ] = self . default self . n_obs += self . default self . sampler = None
13498	def with_revision ( self , label , number ) : t = self . clone ( ) t . revision = Revision ( label , number ) return t
5155	def type_cast ( self , item , schema = None ) : if schema is None : schema = self . _schema properties = schema [ 'properties' ] for key , value in item . items ( ) : if key not in properties : continue try : json_type = properties [ key ] [ 'type' ] except KeyError : json_type = None if json_type == 'integer' and not isinstance ( value , int ) : value = int ( value ) elif json_type == 'boolean' and not isinstance ( value , bool ) : value = value == '1' item [ key ] = value return item
13727	def set_delegate ( address = None , pubkey = None , secret = None ) : c . DELEGATE [ 'ADDRESS' ] = address c . DELEGATE [ 'PUBKEY' ] = pubkey c . DELEGATE [ 'PASSPHRASE' ] = secret
6308	def load_resource_module ( self ) : try : name = '{}.{}' . format ( self . name , 'dependencies' ) self . dependencies_module = importlib . import_module ( name ) except ModuleNotFoundError as err : raise EffectError ( ( "Effect package '{}' has no 'dependencies' module or the module has errors. " "Forwarded error from importlib: {}" ) . format ( self . name , err ) ) try : self . resources = getattr ( self . dependencies_module , 'resources' ) except AttributeError : raise EffectError ( "Effect dependencies module '{}' has no 'resources' attribute" . format ( name ) ) if not isinstance ( self . resources , list ) : raise EffectError ( "Effect dependencies module '{}': 'resources' is of type {} instead of a list" . format ( name , type ( self . resources ) ) ) try : self . effect_packages = getattr ( self . dependencies_module , 'effect_packages' ) except AttributeError : raise EffectError ( "Effect dependencies module '{}' has 'effect_packages' attribute" . format ( name ) ) if not isinstance ( self . effect_packages , list ) : raise EffectError ( "Effect dependencies module '{}': 'effect_packages' is of type {} instead of a list" . format ( name , type ( self . effects ) ) )
13543	def from_server ( cls , server , slug , identifier ) : task = server . get ( 'task' , replacements = { 'slug' : slug , 'identifier' : identifier } ) return cls ( ** task )
7661	def trim ( self , start_time , end_time , strict = False ) : if end_time <= start_time : raise ParameterError ( 'end_time must be greater than start_time.' ) if self . duration is None : orig_time = start_time orig_duration = end_time - start_time warnings . warn ( "Annotation.duration is not defined, cannot check " "for temporal intersection, assuming the annotation " "is valid between start_time and end_time." ) else : orig_time = self . time orig_duration = self . duration if start_time > ( orig_time + orig_duration ) or ( end_time < orig_time ) : warnings . warn ( 'Time range defined by [start_time,end_time] does not ' 'intersect with the time range spanned by this annotation, ' 'the trimmed annotation will be empty.' ) trim_start = self . time trim_end = trim_start else : trim_start = max ( orig_time , start_time ) trim_end = min ( orig_time + orig_duration , end_time ) ann_trimmed = Annotation ( self . namespace , data = None , annotation_metadata = self . annotation_metadata , sandbox = self . sandbox , time = trim_start , duration = trim_end - trim_start ) for obs in self . data : obs_start = obs . time obs_end = obs_start + obs . duration if obs_start < trim_end and obs_end > trim_start : new_start = max ( obs_start , trim_start ) new_end = min ( obs_end , trim_end ) new_duration = new_end - new_start if ( ( not strict ) or ( new_start == obs_start and new_end == obs_end ) ) : ann_trimmed . append ( time = new_start , duration = new_duration , value = obs . value , confidence = obs . confidence ) if 'trim' not in ann_trimmed . sandbox . keys ( ) : ann_trimmed . sandbox . update ( trim = [ { 'start_time' : start_time , 'end_time' : end_time , 'trim_start' : trim_start , 'trim_end' : trim_end } ] ) else : ann_trimmed . sandbox . trim . append ( { 'start_time' : start_time , 'end_time' : end_time , 'trim_start' : trim_start , 'trim_end' : trim_end } ) return ann_trimmed
2673	def invoke ( src , event_file = 'event.json' , config_file = 'config.yaml' , profile_name = None , verbose = False , ) : path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) if profile_name : os . environ [ 'AWS_PROFILE' ] = profile_name env_vars = cfg . get ( 'environment_variables' ) if env_vars : for key , value in env_vars . items ( ) : os . environ [ key ] = get_environment_variable_value ( value ) path_to_event_file = os . path . join ( src , event_file ) event = read ( path_to_event_file , loader = json . loads ) try : sys . path . index ( src ) except ValueError : sys . path . append ( src ) handler = cfg . get ( 'handler' ) fn = get_callable_handler_function ( src , handler ) timeout = cfg . get ( 'timeout' ) if timeout : context = LambdaContext ( cfg . get ( 'function_name' ) , timeout ) else : context = LambdaContext ( cfg . get ( 'function_name' ) ) start = time . time ( ) results = fn ( event , context ) end = time . time ( ) print ( '{0}' . format ( results ) ) if verbose : print ( '\nexecution time: {:.8f}s\nfunction execution ' 'timeout: {:2}s' . format ( end - start , cfg . get ( 'timeout' , 15 ) ) )
3223	def _googleauth ( key_file = None , scopes = [ ] , user_agent = None ) : if key_file : if not scopes : scopes = DEFAULT_SCOPES creds = ServiceAccountCredentials . from_json_keyfile_name ( key_file , scopes = scopes ) else : creds = GoogleCredentials . get_application_default ( ) http = Http ( ) if user_agent : http = set_user_agent ( http , user_agent ) http_auth = creds . authorize ( http ) return http_auth
8296	def render ( self , size , frame , drawqueue ) : r_context = self . create_rcontext ( size , frame ) drawqueue . render ( r_context ) self . rendering_finished ( size , frame , r_context ) return r_context
7667	def search ( self , ** kwargs ) : results = AnnotationArray ( ) for annotation in self : if annotation . search ( ** kwargs ) : results . append ( annotation ) return results
6749	def capture_bash ( self ) : class Capture ( object ) : def __init__ ( self , satchel ) : self . satchel = satchel self . _dryrun = self . satchel . dryrun self . satchel . dryrun = 1 begincap ( ) self . _stdout = sys . stdout self . _stderr = sys . stderr self . stdout = sys . stdout = StringIO ( ) self . stderr = sys . stderr = StringIO ( ) def __enter__ ( self ) : return self def __exit__ ( self , type , value , traceback ) : endcap ( ) self . satchel . dryrun = self . _dryrun sys . stdout = self . _stdout sys . stderr = self . _stderr return Capture ( self )
6187	def get_last_commit ( git_path = None ) : if git_path is None : git_path = GIT_PATH line = get_last_commit_line ( git_path ) revision_id = line . split ( ) [ 1 ] return revision_id
3947	def decode ( message , pblite , ignore_first_item = False ) : if not isinstance ( pblite , list ) : logger . warning ( 'Ignoring invalid message: expected list, got %r' , type ( pblite ) ) return if ignore_first_item : pblite = pblite [ 1 : ] if pblite and isinstance ( pblite [ - 1 ] , dict ) : extra_fields = { int ( field_number ) : value for field_number , value in pblite [ - 1 ] . items ( ) } pblite = pblite [ : - 1 ] else : extra_fields = { } fields_values = itertools . chain ( enumerate ( pblite , start = 1 ) , extra_fields . items ( ) ) for field_number , value in fields_values : if value is None : continue try : field = message . DESCRIPTOR . fields_by_number [ field_number ] except KeyError : if value not in [ [ ] , '' , 0 ] : logger . debug ( 'Message %r contains unknown field %s with value ' '%r' , message . __class__ . __name__ , field_number , value ) continue if field . label == FieldDescriptor . LABEL_REPEATED : _decode_repeated_field ( message , field , value ) else : _decode_field ( message , field , value )
3652	def run ( self ) : while self . _base . is_running : if self . _worker : self . _worker ( ) time . sleep ( self . _sleep_duration )
958	def aggregationToMonthsSeconds ( interval ) : seconds = interval . get ( 'microseconds' , 0 ) * 0.000001 seconds += interval . get ( 'milliseconds' , 0 ) * 0.001 seconds += interval . get ( 'seconds' , 0 ) seconds += interval . get ( 'minutes' , 0 ) * 60 seconds += interval . get ( 'hours' , 0 ) * 60 * 60 seconds += interval . get ( 'days' , 0 ) * 24 * 60 * 60 seconds += interval . get ( 'weeks' , 0 ) * 7 * 24 * 60 * 60 months = interval . get ( 'months' , 0 ) months += 12 * interval . get ( 'years' , 0 ) return { 'months' : months , 'seconds' : seconds }
12871	def chain ( * args ) : def chain_block ( * args , ** kwargs ) : v = args [ 0 ] ( * args , ** kwargs ) for p in args [ 1 : ] : v = p ( v ) return v return chain_block
115	def map_batches ( self , batches , chunksize = None ) : assert isinstance ( batches , list ) , ( "Expected to get a list as 'batches', got type %s. " + "Call imap_batches() if you use generators." ) % ( type ( batches ) , ) return self . pool . map ( _Pool_starworker , self . _handle_batch_ids ( batches ) , chunksize = chunksize )
10591	def create_transaction ( self , name , description = None , tx_date = datetime . min . date ( ) , dt_account = None , cr_account = None , source = None , amount = 0.00 ) : new_tx = Transaction ( name , description , tx_date , dt_account , cr_account , source , amount ) self . transactions . append ( new_tx ) return new_tx
8300	def handle ( self , data , source = None ) : decoded = decodeOSC ( data ) self . dispatch ( decoded , source )
6326	def corpus_importer ( self , corpus , n_val = 1 , bos = '_START_' , eos = '_END_' ) : r if not corpus or not isinstance ( corpus , Corpus ) : raise TypeError ( 'Corpus argument of the Corpus class required.' ) sentences = corpus . sents ( ) for sent in sentences : ngs = Counter ( sent ) for key in ngs . keys ( ) : self . _add_to_ngcorpus ( self . ngcorpus , [ key ] , ngs [ key ] ) if n_val > 1 : if bos and bos != '' : sent = [ bos ] + sent if eos and eos != '' : sent += [ eos ] for i in range ( 2 , n_val + 1 ) : for j in range ( len ( sent ) - i + 1 ) : self . _add_to_ngcorpus ( self . ngcorpus , sent [ j : j + i ] , 1 )
13166	def parse_query ( query ) : parts = query . split ( '/' ) norm = [ ] for p in parts : p = p . strip ( ) if p : norm . append ( p ) elif '' not in norm : norm . append ( '' ) return norm
841	def getPattern ( self , idx , sparseBinaryForm = False , cat = None ) : if cat is not None : assert idx is None idx = self . _categoryList . index ( cat ) if not self . useSparseMemory : pattern = self . _Memory [ idx ] if sparseBinaryForm : pattern = pattern . nonzero ( ) [ 0 ] else : ( nz , values ) = self . _Memory . rowNonZeros ( idx ) if not sparseBinaryForm : pattern = numpy . zeros ( self . _Memory . nCols ( ) ) numpy . put ( pattern , nz , 1 ) else : pattern = nz return pattern
4220	def backends ( cls ) : allowed = ( keyring for keyring in filter ( backend . _limit , backend . get_all_keyring ( ) ) if not isinstance ( keyring , ChainerBackend ) and keyring . priority > 0 ) return sorted ( allowed , key = backend . by_priority , reverse = True )
10225	def get_correlation_graph ( graph : BELGraph ) -> Graph : result = Graph ( ) for u , v , d in graph . edges ( data = True ) : if d [ RELATION ] not in CORRELATIVE_RELATIONS : continue if not result . has_edge ( u , v ) : result . add_edge ( u , v , ** { d [ RELATION ] : True } ) elif d [ RELATION ] not in result [ u ] [ v ] : log . log ( 5 , 'broken correlation relation for %s, %s' , u , v ) result [ u ] [ v ] [ d [ RELATION ] ] = True result [ v ] [ u ] [ d [ RELATION ] ] = True return result
8778	def _check_collisions ( self , new_range , existing_ranges ) : def _contains ( num , r1 ) : return ( num >= r1 [ 0 ] and num <= r1 [ 1 ] ) def _is_overlap ( r1 , r2 ) : return ( _contains ( r1 [ 0 ] , r2 ) or _contains ( r1 [ 1 ] , r2 ) or _contains ( r2 [ 0 ] , r1 ) or _contains ( r2 [ 1 ] , r1 ) ) for existing_range in existing_ranges : if _is_overlap ( new_range , existing_range ) : return True return False
12672	def group ( * args ) : if args and isinstance ( args [ 0 ] , dataframe . DataFrame ) : return args [ 0 ] . group ( * args [ 1 : ] ) elif not args : raise ValueError ( "No arguments provided" ) else : return pipeable . Pipeable ( pipeable . PipingMethod . GROUP , * args )
6525	def get_grouped_issues ( self , keyfunc = None , sortby = None ) : if not keyfunc : keyfunc = default_group if not sortby : sortby = self . DEFAULT_SORT self . _ensure_cleaned_issues ( ) return self . _group_issues ( self . _cleaned_issues , keyfunc , sortby )
3925	def _update_tabs ( self ) : text = [ ] for num , widget in enumerate ( self . _widgets ) : palette = ( 'active_tab' if num == self . _tab_index else 'inactive_tab' ) text += [ ( palette , ' {} ' . format ( self . _widget_title [ widget ] ) ) , ( 'tab_background' , ' ' ) , ] self . _tabs . set_text ( text ) self . _frame . contents [ 'body' ] = ( self . _widgets [ self . _tab_index ] , None )
12003	def _add_header ( self , data , options ) : version_info = self . _get_version_info ( options [ 'version' ] ) flags = options [ 'flags' ] header_flags = dict ( ( i , str ( int ( j ) ) ) for i , j in options [ 'flags' ] . iteritems ( ) ) header_flags = '' . join ( version_info [ 'flags' ] ( ** header_flags ) ) header_flags = int ( header_flags , 2 ) options [ 'flags' ] = header_flags header = version_info [ 'header' ] header = header ( ** options ) header = pack ( version_info [ 'header_format' ] , * header ) if 'timestamp' in flags and flags [ 'timestamp' ] : timestamp = long ( time ( ) ) timestamp = pack ( version_info [ 'timestamp_format' ] , timestamp ) header = header + timestamp return header + data
12132	def linspace ( self , start , stop , n ) : if n == 1 : return [ start ] L = [ 0.0 ] * n nm1 = n - 1 nm1inv = 1.0 / nm1 for i in range ( n ) : L [ i ] = nm1inv * ( start * ( nm1 - i ) + stop * i ) return L
3313	def _stream_data_chunked ( self , environ , block_size ) : if "Darwin" in environ . get ( "HTTP_USER_AGENT" , "" ) and environ . get ( "HTTP_X_EXPECTED_ENTITY_LENGTH" ) : WORKAROUND_CHUNK_LENGTH = True buf = environ . get ( "HTTP_X_EXPECTED_ENTITY_LENGTH" , "0" ) length = int ( buf ) else : WORKAROUND_CHUNK_LENGTH = False buf = environ [ "wsgi.input" ] . readline ( ) environ [ "wsgidav.some_input_read" ] = 1 if buf == compat . b_empty : length = 0 else : length = int ( buf , 16 ) while length > 0 : buf = environ [ "wsgi.input" ] . read ( block_size ) yield buf if WORKAROUND_CHUNK_LENGTH : environ [ "wsgidav.some_input_read" ] = 1 if buf == compat . b_empty : length = 0 else : length -= len ( buf ) else : environ [ "wsgi.input" ] . readline ( ) buf = environ [ "wsgi.input" ] . readline ( ) if buf == compat . b_empty : length = 0 else : length = int ( buf , 16 ) environ [ "wsgidav.all_input_read" ] = 1
11485	def upload ( file_pattern , destination = 'Private' , leaf_folders_as_items = False , reuse_existing = False ) : session . token = verify_credentials ( ) parent_folder_id = None user_folders = session . communicator . list_user_folders ( session . token ) if destination . startswith ( '/' ) : parent_folder_id = _find_resource_id_from_path ( destination ) else : for cur_folder in user_folders : if cur_folder [ 'name' ] == destination : parent_folder_id = cur_folder [ 'folder_id' ] if parent_folder_id is None : print ( 'Unable to locate specified destination. Defaulting to {0}.' . format ( user_folders [ 0 ] [ 'name' ] ) ) parent_folder_id = user_folders [ 0 ] [ 'folder_id' ] for current_file in glob . iglob ( file_pattern ) : current_file = os . path . normpath ( current_file ) if os . path . isfile ( current_file ) : print ( 'Uploading item from {0}' . format ( current_file ) ) _upload_as_item ( os . path . basename ( current_file ) , parent_folder_id , current_file , reuse_existing ) else : _upload_folder_recursive ( current_file , parent_folder_id , leaf_folders_as_items , reuse_existing )
7865	def handle_authorized ( self , event ) : request_software_version ( self . client , self . target_jid , self . success , self . failure )
7744	def _timeout_cb ( self , method ) : self . _anything_done = True logger . debug ( "_timeout_cb() called for: {0!r}" . format ( method ) ) result = method ( ) rec = method . _pyxmpp_recurring if rec : self . _prepare_pending ( ) return True if rec is None and result is not None : logger . debug ( " auto-recurring, restarting in {0} s" . format ( result ) ) tag = glib . timeout_add ( int ( result * 1000 ) , self . _timeout_cb , method ) self . _timer_sources [ method ] = tag else : self . _timer_sources . pop ( method , None ) self . _prepare_pending ( ) return False
7713	def handle_roster_push ( self , stanza ) : if self . server is None and stanza . from_jid : logger . debug ( u"Server address not known, cannot verify roster push" " from {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) if self . server and stanza . from_jid and stanza . from_jid != self . server : logger . debug ( u"Roster push from invalid source: {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) payload = stanza . get_payload ( RosterPayload ) if len ( payload ) != 1 : logger . warning ( "Bad roster push received ({0} items)" . format ( len ( payload ) ) ) return stanza . make_error_response ( u"bad-request" ) if self . roster is None : logger . debug ( "Dropping roster push - no roster here" ) return True item = payload [ 0 ] item . verify_roster_push ( True ) old_item = self . roster . get ( item . jid ) if item . subscription == "remove" : if old_item : self . roster . remove_item ( item . jid ) else : self . roster . add_item ( item , replace = True ) self . _event_queue . put ( RosterUpdatedEvent ( self , old_item , item ) ) return stanza . make_result_response ( )
8881	def fit ( self , X , y = None ) : X = check_array ( X ) self . inverse_influence_matrix = self . __make_inverse_matrix ( X ) if self . threshold == 'auto' : self . threshold_value = 3 * ( 1 + X . shape [ 1 ] ) / X . shape [ 0 ] elif self . threshold == 'cv' : if y is None : raise ValueError ( "Y must be specified to find the optimal threshold." ) y = check_array ( y , accept_sparse = 'csc' , ensure_2d = False , dtype = None ) self . threshold_value = 0 score = 0 Y_pred , Y_true , AD = [ ] , [ ] , [ ] cv = KFold ( n_splits = 5 , random_state = 1 , shuffle = True ) for train_index , test_index in cv . split ( X ) : x_train = safe_indexing ( X , train_index ) x_test = safe_indexing ( X , test_index ) y_train = safe_indexing ( y , train_index ) y_test = safe_indexing ( y , test_index ) if self . reg_model is None : reg_model = RandomForestRegressor ( n_estimators = 500 , random_state = 1 ) . fit ( x_train , y_train ) else : reg_model = clone ( self . reg_model ) . fit ( x_train , y_train ) Y_pred . append ( reg_model . predict ( x_test ) ) Y_true . append ( y_test ) ad_model = self . __make_inverse_matrix ( x_train ) AD . append ( self . __find_leverages ( x_test , ad_model ) ) AD_ = unique ( hstack ( AD ) ) for z in AD_ : AD_new = hstack ( AD ) <= z if self . score == 'ba_ad' : val = balanced_accuracy_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) elif self . score == 'rmse_ad' : val = rmse_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) if val >= score : score = val self . threshold_value = z else : self . threshold_value = self . threshold return self
4388	def adsPortCloseEx ( port ) : port_close_ex = _adsDLL . AdsPortCloseEx port_close_ex . restype = ctypes . c_long error_code = port_close_ex ( port ) if error_code : raise ADSError ( error_code )
12240	def booth ( theta ) : x , y = theta A = x + 2 * y - 7 B = 2 * x + y - 5 obj = A ** 2 + B ** 2 grad = np . array ( [ 2 * A + 4 * B , 4 * A + 2 * B ] ) return obj , grad
9499	def convert_completezip ( path ) : for filepath in path . glob ( '**/index_auto_generated.cnxml' ) : filepath . rename ( filepath . parent / 'index.cnxml' ) logger . debug ( 'removed {}' . format ( filepath ) ) for filepath in path . glob ( '**/index.cnxml.html' ) : filepath . unlink ( ) return parse_litezip ( path )
12488	def _import_config ( filepath ) : if not op . isfile ( filepath ) : raise IOError ( 'Data config file not found. ' 'Got: {0}' . format ( filepath ) ) cfg = import_pyfile ( filepath ) if not hasattr ( cfg , 'root_path' ) : raise KeyError ( 'Config file root_path key not found.' ) if not hasattr ( cfg , 'filetree' ) : raise KeyError ( 'Config file filetree key not found.' ) return cfg . root_path , cfg . filetree
12439	def serialize ( self , data , response = None , request = None , format = None ) : if isinstance ( self , Resource ) : if not request : request = self . _request Serializer = None if format : Serializer = self . meta . serializers [ format ] if not Serializer : media_ranges = ( request . get ( 'Accept' ) or '*/*' ) . strip ( ) if not media_ranges : media_ranges = '*/*' if media_ranges != '*/*' : media_types = six . iterkeys ( self . _serializer_map ) media_type = mimeparse . best_match ( media_types , media_ranges ) if media_type : format = self . _serializer_map [ media_type ] Serializer = self . meta . serializers [ format ] else : default = self . meta . default_serializer Serializer = self . meta . serializers [ default ] if Serializer : try : serializer = Serializer ( request , response ) return serializer . serialize ( data ) , serializer except ValueError : pass available = { } for name in self . meta . allowed_serializers : Serializer = self . meta . serializers [ name ] instance = Serializer ( request , None ) if instance . can_serialize ( data ) : available [ name ] = Serializer . media_types [ 0 ] raise http . exceptions . NotAcceptable ( available )
13600	def increment ( cls , name ) : with transaction . atomic ( ) : counter = Counter . objects . select_for_update ( ) . get ( name = name ) counter . value += 1 counter . save ( ) return counter . value
8669	def unlock_key ( key_name , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Unlocking key...' ) stash . unlock ( key_name = key_name ) click . echo ( 'Key unlocked successfully' ) except GhostError as ex : sys . exit ( ex )
13796	def handle_rereduce ( self , reduce_function_names , values ) : reduce_functions = [ ] for reduce_function_name in reduce_function_names : try : reduce_function = get_function ( reduce_function_name ) if getattr ( reduce_function , 'view_decorated' , None ) : reduce_function = reduce_function ( self . log ) reduce_functions . append ( reduce_function ) except Exception , exc : self . log ( repr ( exc ) ) reduce_functions . append ( lambda * args , ** kwargs : None ) results = [ ] for reduce_function in reduce_functions : try : results . append ( reduce_function ( None , values , rereduce = True ) ) except Exception , exc : self . log ( repr ( exc ) ) results . append ( None ) return [ True , results ]
10157	def get_transition_viewset_method ( transition_name , ** kwargs ) : @ detail_route ( methods = [ 'post' ] , ** kwargs ) def inner_func ( self , request , pk = None , ** kwargs ) : object = self . get_object ( ) transition_method = getattr ( object , transition_name ) transition_method ( by = self . request . user ) if self . save_after_transition : object . save ( ) serializer = self . get_serializer ( object ) return Response ( serializer . data ) return inner_func
12715	def positions ( self ) : return [ self . ode_obj . getPosition ( i ) for i in range ( self . LDOF ) ]
13548	def update ( dst , src ) : stack = [ ( dst , src ) ] def isdict ( o ) : return hasattr ( o , 'keys' ) while stack : current_dst , current_src = stack . pop ( ) for key in current_src : if key not in current_dst : current_dst [ key ] = current_src [ key ] else : if isdict ( current_src [ key ] ) and isdict ( current_dst [ key ] ) : stack . append ( ( current_dst [ key ] , current_src [ key ] ) ) else : current_dst [ key ] = current_src [ key ] return dst
7779	def __from_rfc2426 ( self , data ) : data = from_utf8 ( data ) lines = data . split ( "\n" ) started = 0 current = None for l in lines : if not l : continue if l [ - 1 ] == "\r" : l = l [ : - 1 ] if not l : continue if l [ 0 ] in " \t" : if current is None : continue current += l [ 1 : ] continue if not started and current and current . upper ( ) . strip ( ) == "BEGIN:VCARD" : started = 1 elif started and current . upper ( ) . strip ( ) == "END:VCARD" : current = None break elif current and started : self . _process_rfc2425_record ( current ) current = l if started and current : self . _process_rfc2425_record ( current )
1041	def source_lines ( self ) : return [ self . source_buffer . source_line ( line ) for line in range ( self . line ( ) , self . end ( ) . line ( ) + 1 ) ]
5370	def _load_file_from_gcs ( gcs_file_path , credentials = None ) : gcs_service = _get_storage_service ( credentials ) bucket_name , object_name = gcs_file_path [ len ( 'gs://' ) : ] . split ( '/' , 1 ) request = gcs_service . objects ( ) . get_media ( bucket = bucket_name , object = object_name ) file_handle = io . BytesIO ( ) downloader = MediaIoBaseDownload ( file_handle , request , chunksize = 1024 * 1024 ) done = False while not done : _ , done = _downloader_next_chunk ( downloader ) filevalue = file_handle . getvalue ( ) if not isinstance ( filevalue , six . string_types ) : filevalue = filevalue . decode ( ) return six . StringIO ( filevalue )
130	def is_fully_within_image ( self , image ) : return not self . is_out_of_image ( image , fully = True , partly = True )
13421	def map ( self , ID_s , FROM = None , TO = None , target_as_set = False , no_match_sub = None ) : def io_mode ( ID_s ) : unlist_return = False list_of_lists = False if isinstance ( ID_s , str ) : ID_s = [ ID_s ] unlist_return = True elif isinstance ( ID_s , list ) : if len ( ID_s ) > 0 and isinstance ( ID_s [ 0 ] , list ) : list_of_lists = True return ID_s , unlist_return , list_of_lists if FROM == TO : return ID_s ID_s , unlist_return , list_of_lists = io_mode ( ID_s ) if list_of_lists : mapped_ids = [ self . map ( ID , FROM , TO , target_as_set , no_match_sub ) for ID in ID_s ] else : mapped_ids = self . _map ( ID_s , FROM , TO , target_as_set , no_match_sub ) if unlist_return : return mapped_ids [ 0 ] return Mapping ( ID_s , mapped_ids )
8965	def whichgen ( command , path = None , verbose = 0 , exts = None ) : matches = [ ] if path is None : using_given_path = 0 path = os . environ . get ( "PATH" , "" ) . split ( os . pathsep ) if sys . platform . startswith ( "win" ) : path . insert ( 0 , os . curdir ) else : using_given_path = 1 if sys . platform . startswith ( "win" ) : if exts is None : exts = os . environ . get ( "PATHEXT" , "" ) . split ( os . pathsep ) for ext in exts : if ext . lower ( ) == ".exe" : break else : exts = [ '.COM' , '.EXE' , '.BAT' ] elif not isinstance ( exts , list ) : raise TypeError ( "'exts' argument must be a list or None" ) else : if exts is not None : raise WhichError ( "'exts' argument is not supported on platform '%s'" % sys . platform ) exts = [ ] if os . sep in command or os . altsep and os . altsep in command : pass else : for i , dir_name in enumerate ( path ) : if sys . platform . startswith ( "win" ) and len ( dir_name ) >= 2 and dir_name [ 0 ] == '"' and dir_name [ - 1 ] == '"' : dir_name = dir_name [ 1 : - 1 ] for ext in [ '' ] + exts : abs_name = os . path . abspath ( os . path . normpath ( os . path . join ( dir_name , command + ext ) ) ) if os . path . isfile ( abs_name ) : if using_given_path : from_where = "from given path element %d" % i elif not sys . platform . startswith ( "win" ) : from_where = "from PATH element %d" % i elif i == 0 : from_where = "from current directory" else : from_where = "from PATH element %d" % ( i - 1 ) match = _cull ( ( abs_name , from_where ) , matches , verbose ) if match : if verbose : yield match else : yield match [ 0 ] match = _get_registered_executable ( command ) if match is not None : match = _cull ( match , matches , verbose ) if match : if verbose : yield match else : yield match [ 0 ]
7145	def transfer_multiple ( self , destinations , priority = prio . NORMAL , payment_id = None , unlock_time = 0 , relay = True ) : return self . _backend . transfer ( destinations , priority , payment_id , unlock_time , account = self . index , relay = relay )
10160	def ci ( ctx ) : opts = [ '' ] if os . environ . get ( 'TRAVIS' , '' ) . lower ( ) == 'true' : opts += [ 'test.pytest' ] else : opts += [ 'test.tox' ] ctx . run ( "invoke --echo --pty clean --all build --docs check --reports{}" . format ( ' ' . join ( opts ) ) )
9068	def _lml_arbitrary_scale ( self ) : s = self . scale D = self . _D n = len ( self . _y ) lml = - self . _df * log2pi - n * log ( s ) lml -= sum ( npsum ( log ( d ) ) for d in D ) d = ( mTQ - yTQ for ( mTQ , yTQ ) in zip ( self . _mTQ , self . _yTQ ) ) lml -= sum ( ( i / j ) @ i for ( i , j ) in zip ( d , D ) ) / s return lml / 2
10532	def create_project ( name , short_name , description ) : try : project = dict ( name = name , short_name = short_name , description = description ) res = _pybossa_req ( 'post' , 'project' , payload = project ) if res . get ( 'id' ) : return Project ( res ) else : return res except : raise
11590	def _rc_smove ( self , src , dst , value ) : if self . type ( src ) != b ( "set" ) : return self . smove ( src + "{" + src + "}" , dst , value ) if self . type ( dst ) != b ( "set" ) : return self . smove ( dst + "{" + dst + "}" , src , value ) if self . srem ( src , value ) : return 1 if self . sadd ( dst , value ) else 0 return 0
3128	def delete ( self , template_id ) : self . template_id = template_id return self . _mc_client . _delete ( url = self . _build_path ( template_id ) )
4796	def contains_entry ( self , * args , ** kwargs ) : self . _check_dict_like ( self . val , check_values = False ) entries = list ( args ) + [ { k : v } for k , v in kwargs . items ( ) ] if len ( entries ) == 0 : raise ValueError ( 'one or more entry args must be given' ) missing = [ ] for e in entries : if type ( e ) is not dict : raise TypeError ( 'given entry arg must be a dict' ) if len ( e ) != 1 : raise ValueError ( 'given entry args must contain exactly one key-value pair' ) k = next ( iter ( e ) ) if k not in self . val : missing . append ( e ) elif self . val [ k ] != e [ k ] : missing . append ( e ) if missing : self . _err ( 'Expected <%s> to contain entries %s, but did not contain %s.' % ( self . val , self . _fmt_items ( entries ) , self . _fmt_items ( missing ) ) ) return self
8199	def transform_from_local ( xp , yp , cphi , sphi , mx , my ) : x = xp * cphi - yp * sphi + mx y = xp * sphi + yp * cphi + my return ( x , y )
12974	def compat_convertHashedIndexes ( self , fetchAll = True ) : saver = IndexedRedisSave ( self . mdl ) if fetchAll is True : objs = self . all ( ) saver . compat_convertHashedIndexes ( objs ) else : didWarnOnce = False pks = self . getPrimaryKeys ( ) for pk in pks : obj = self . get ( pk ) if not obj : if didWarnOnce is False : sys . stderr . write ( 'WARNING(once)! An object (type=%s , pk=%d) disappered while ' 'running compat_convertHashedIndexes! This probably means an application ' 'is using the model while converting indexes. This is a very BAD IDEA (tm).' ) didWarnOnce = True continue saver . compat_convertHashedIndexes ( [ obj ] )
11939	def broadcast_message ( level , message_text , extra_tags = '' , date = None , url = None , fail_silently = False ) : from django . contrib . auth import get_user_model users = get_user_model ( ) . objects . all ( ) add_message_for ( users , level , message_text , extra_tags = extra_tags , date = date , url = url , fail_silently = fail_silently )
5484	def execute ( api ) : try : return api . execute ( ) except Exception as exception : now = datetime . now ( ) . strftime ( '%Y-%m-%d %H:%M:%S.%f' ) _print_error ( '%s: Exception %s: %s' % ( now , type ( exception ) . __name__ , str ( exception ) ) ) raise exception
5590	def tile ( self , zoom , row , col ) : tile = self . tile_pyramid . tile ( zoom , row , col ) return BufferedTile ( tile , pixelbuffer = self . pixelbuffer )
9631	def send ( self , extra_context = None , ** kwargs ) : message = self . render_to_message ( extra_context = extra_context , ** kwargs ) return message . send ( )
10154	def _extract_transform_colander_schema ( self , args ) : schema = args . get ( 'schema' , colander . MappingSchema ( ) ) if not isinstance ( schema , colander . Schema ) : schema = schema ( ) schema = schema . clone ( ) for transformer in self . schema_transformers : schema = transformer ( schema , args ) return schema
9760	def resources ( ctx , job , gpu ) : def get_experiment_resources ( ) : try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . experiment . resources ( user , project_name , _experiment , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) def get_experiment_job_resources ( ) : try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . experiment_job . resources ( user , project_name , _experiment , _job , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if job : _job = get_experiment_job_or_local ( job ) get_experiment_job_resources ( ) else : get_experiment_resources ( )
487	def release ( self ) : self . _logger . debug ( "Releasing: %r" , self ) if self . _addedToInstanceSet : try : self . _clsOutstandingInstances . remove ( self ) except : self . _logger . exception ( "Failed to remove self from _clsOutstandingInstances: %r;" , self ) raise self . _releaser ( dbConn = self . dbConn , cursor = self . cursor ) self . __class__ . _clsNumOutstanding -= 1 assert self . _clsNumOutstanding >= 0 , "_clsNumOutstanding=%r" % ( self . _clsNumOutstanding , ) self . _releaser = None self . cursor = None self . dbConn = None self . _creationTracebackString = None self . _addedToInstanceSet = False self . _logger = None return
12909	def from_json ( cls , fh ) : if isinstance ( fh , str ) : return cls ( json . loads ( fh ) ) else : return cls ( json . load ( fh ) )
11029	def sse_content ( response , handler , ** sse_kwargs ) : raise_for_not_ok_status ( response ) raise_for_header ( response , 'Content-Type' , 'text/event-stream' ) finished , _ = _sse_content_with_protocol ( response , handler , ** sse_kwargs ) return finished
7681	def piano_roll ( annotation , ** kwargs ) : times , midi = annotation . to_interval_values ( ) return mir_eval . display . piano_roll ( times , midi = midi , ** kwargs )
5521	def parse_directory_response ( s ) : seq_quotes = 0 start = False directory = "" for ch in s : if not start : if ch == "\"" : start = True else : if ch == "\"" : seq_quotes += 1 else : if seq_quotes == 1 : break elif seq_quotes == 2 : seq_quotes = 0 directory += '"' directory += ch return pathlib . PurePosixPath ( directory )
12382	def link ( self , request , response ) : from armet . resources . managed . request import read if self . slug is None : raise http . exceptions . NotImplemented ( ) target = self . read ( ) links = self . _parse_link_headers ( request [ 'Link' ] ) for link in links : self . relate ( target , read ( self , link [ 'uri' ] ) ) self . response . status = http . client . NO_CONTENT self . make_response ( )
11868	def strip_minidom_whitespace ( node ) : for child in node . childNodes : if child . nodeType == Node . TEXT_NODE : if child . nodeValue : child . nodeValue = child . nodeValue . strip ( ) elif child . nodeType == Node . ELEMENT_NODE : strip_minidom_whitespace ( child )
4042	def _extract_links ( self ) : extracted = dict ( ) try : for key , value in self . request . links . items ( ) : parsed = urlparse ( value [ "url" ] ) fragment = "{path}?{query}" . format ( path = parsed [ 2 ] , query = parsed [ 4 ] ) extracted [ key ] = fragment parsed = list ( urlparse ( self . self_link ) ) stripped = "&" . join ( [ "%s=%s" % ( p [ 0 ] , p [ 1 ] ) for p in parse_qsl ( parsed [ 4 ] ) if p [ 0 ] != "format" ] ) extracted [ "self" ] = urlunparse ( [ parsed [ 0 ] , parsed [ 1 ] , parsed [ 2 ] , parsed [ 3 ] , stripped , parsed [ 5 ] ] ) return extracted except KeyError : return None
7539	def get_binom ( base1 , base2 , estE , estH ) : prior_homo = ( 1. - estH ) / 2. prior_hete = estH bsum = base1 + base2 hetprob = scipy . misc . comb ( bsum , base1 ) / ( 2. ** ( bsum ) ) homoa = scipy . stats . binom . pmf ( base2 , bsum , estE ) homob = scipy . stats . binom . pmf ( base1 , bsum , estE ) hetprob *= prior_hete homoa *= prior_homo homob *= prior_homo probabilities = [ homoa , homob , hetprob ] bestprob = max ( probabilities ) / float ( sum ( probabilities ) ) if hetprob > homoa : return True , bestprob else : return False , bestprob
8640	def retract_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'retract' } endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotRetractedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
5311	def translate_colorname_to_ansi_code ( colorname , offset , colormode , colorpalette ) : try : red , green , blue = colorpalette [ colorname ] except KeyError : raise ColorfulError ( 'the color "{0}" is unknown. Use a color in your color palette (by default: X11 rgb.txt)' . format ( colorname ) ) else : return translate_rgb_to_ansi_code ( red , green , blue , offset , colormode )
4764	def is_equal_to ( self , other , ** kwargs ) : if self . _check_dict_like ( self . val , check_values = False , return_as_bool = True ) and self . _check_dict_like ( other , check_values = False , return_as_bool = True ) : if self . _dict_not_equal ( self . val , other , ignore = kwargs . get ( 'ignore' ) , include = kwargs . get ( 'include' ) ) : self . _dict_err ( self . val , other , ignore = kwargs . get ( 'ignore' ) , include = kwargs . get ( 'include' ) ) else : if self . val != other : self . _err ( 'Expected <%s> to be equal to <%s>, but was not.' % ( self . val , other ) ) return self
8522	def add_int ( self , name , min , max , warp = None ) : min , max = map ( int , ( min , max ) ) if max < min : raise ValueError ( 'variable %s: max < min error' % name ) if warp not in ( None , 'log' ) : raise ValueError ( 'variable %s: warp=%s is not supported. use ' 'None or "log",' % ( name , warp ) ) if min <= 0 and warp == 'log' : raise ValueError ( 'variable %s: log-warping requires min > 0' ) self . variables [ name ] = IntVariable ( name , min , max , warp )
10610	def _calculate_H ( self , T ) : if self . isCoal : return self . _calculate_Hfr_coal ( T ) H = 0.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) dH = thermo . H ( compound , T , self . _compound_masses [ index ] ) H = H + dH return H
12658	def append_dict_values ( list_of_dicts , keys = None ) : if keys is None : keys = list ( list_of_dicts [ 0 ] . keys ( ) ) dict_of_lists = DefaultOrderedDict ( list ) for d in list_of_dicts : for k in keys : dict_of_lists [ k ] . append ( d [ k ] ) return dict_of_lists
12667	def vector_to_volume ( arr , mask , order = 'C' ) : if mask . dtype != np . bool : raise ValueError ( "mask must be a boolean array" ) if arr . ndim != 1 : raise ValueError ( "vector must be a 1-dimensional array" ) if arr . ndim == 2 and any ( v == 1 for v in arr . shape ) : log . debug ( 'Got an array of shape {}, flattening for my purposes.' . format ( arr . shape ) ) arr = arr . flatten ( ) volume = np . zeros ( mask . shape [ : 3 ] , dtype = arr . dtype , order = order ) volume [ mask ] = arr return volume
3796	def setup_a_alpha_and_derivatives ( self , i , T = None ) : r if not hasattr ( self , 'kappas' ) : self . kappas = [ ] for Tc , kappa0 , kappa1 , kappa2 , kappa3 in zip ( self . Tcs , self . kappa0s , self . kappa1s , self . kappa2s , self . kappa3s ) : Tr = T / Tc kappa = kappa0 + ( ( kappa1 + kappa2 * ( kappa3 - Tr ) * ( 1. - Tr ** 0.5 ) ) * ( 1. + Tr ** 0.5 ) * ( 0.7 - Tr ) ) self . kappas . append ( kappa ) ( self . a , self . kappa , self . kappa0 , self . kappa1 , self . kappa2 , self . kappa3 , self . Tc ) = ( self . ais [ i ] , self . kappas [ i ] , self . kappa0s [ i ] , self . kappa1s [ i ] , self . kappa2s [ i ] , self . kappa3s [ i ] , self . Tcs [ i ] )
3061	def positional ( max_positional_args ) : def positional_decorator ( wrapped ) : @ functools . wraps ( wrapped ) def positional_wrapper ( * args , ** kwargs ) : if len ( args ) > max_positional_args : plural_s = '' if max_positional_args != 1 : plural_s = 's' message = ( '{function}() takes at most {args_max} positional ' 'argument{plural} ({args_given} given)' . format ( function = wrapped . __name__ , args_max = max_positional_args , args_given = len ( args ) , plural = plural_s ) ) if positional_parameters_enforcement == POSITIONAL_EXCEPTION : raise TypeError ( message ) elif positional_parameters_enforcement == POSITIONAL_WARNING : logger . warning ( message ) return wrapped ( * args , ** kwargs ) return positional_wrapper if isinstance ( max_positional_args , six . integer_types ) : return positional_decorator else : args , _ , _ , defaults = inspect . getargspec ( max_positional_args ) return positional ( len ( args ) - len ( defaults ) ) ( max_positional_args )
8437	def parse ( type : Type ) : def decorator ( parser ) : EnvVar . parsers [ type ] = parser return parser return decorator
7203	def get_matching_multiplex_port ( self , name ) : matching_multiplex_ports = [ self . __getattribute__ ( p ) for p in self . _portnames if name . startswith ( p ) and name != p and hasattr ( self , p ) and self . __getattribute__ ( p ) . is_multiplex ] for port in matching_multiplex_ports : return port return None
2788	def resize ( self , size_gigabytes , region ) : return self . get_data ( "volumes/%s/actions/" % self . id , type = POST , params = { "type" : "resize" , "size_gigabytes" : size_gigabytes , "region" : region } )
6809	def configure_camera ( self ) : r = self . local_renderer if self . env . camera_enabled : r . pc ( 'Enabling camera.' ) r . enable_attr ( filename = '/boot/config.txt' , key = 'start_x' , value = 1 , use_sudo = True , ) r . enable_attr ( filename = '/boot/config.txt' , key = 'gpu_mem' , value = r . env . gpu_mem , use_sudo = True , ) r . run ( 'cd ~; git clone https://github.com/raspberrypi/userland.git; cd userland; ./buildme' ) r . run ( 'touch ~/.bash_aliases' ) r . append ( r'PATH=$PATH:/opt/vc/bin\nexport PATH' , '~/.bash_aliases' ) r . append ( r'LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/vc/lib\nexport LD_LIBRARY_PATH' , '~/.bash_aliases' ) r . run ( 'source ~/.bashrc' ) r . sudo ( 'ldconfig' ) r . sudo ( "echo 'SUBSYSTEM==\"vchiq\",GROUP=\"video\",MODE=\"0660\"' > /etc/udev/rules.d/10-vchiq-permissions.rules" ) r . sudo ( "usermod -a -G video {user}" ) r . reboot ( wait = 300 , timeout = 60 ) self . test_camera ( ) else : r . disable_attr ( filename = '/boot/config.txt' , key = 'start_x' , use_sudo = True , ) r . disable_attr ( filename = '/boot/config.txt' , key = 'gpu_mem' , use_sudo = True , ) r . reboot ( wait = 300 , timeout = 60 )
12557	def cli ( ) : return VersionedCLI ( cli_name = SF_CLI_NAME , config_dir = SF_CLI_CONFIG_DIR , config_env_var_prefix = SF_CLI_ENV_VAR_PREFIX , commands_loader_cls = SFCommandLoader , help_cls = SFCommandHelp )
6102	def intensities_from_grid_radii ( self , grid_radii ) : return np . multiply ( np . multiply ( self . intensity_prime , np . power ( np . add ( 1 , np . power ( np . divide ( self . radius_break , grid_radii ) , self . alpha ) ) , ( self . gamma / self . alpha ) ) ) , np . exp ( np . multiply ( - self . sersic_constant , ( np . power ( np . divide ( np . add ( np . power ( grid_radii , self . alpha ) , ( self . radius_break ** self . alpha ) ) , ( self . effective_radius ** self . alpha ) ) , ( 1.0 / ( self . alpha * self . sersic_index ) ) ) ) ) ) )
12695	def contains_all ( set1 , set2 , warn ) : for elem in set2 : if elem not in set1 : raise ValueError ( warn ) return True
7826	def payload_element_name ( element_name ) : def decorator ( klass ) : from . stanzapayload import STANZA_PAYLOAD_CLASSES from . stanzapayload import STANZA_PAYLOAD_ELEMENTS if hasattr ( klass , "_pyxmpp_payload_element_name" ) : klass . _pyxmpp_payload_element_name . append ( element_name ) else : klass . _pyxmpp_payload_element_name = [ element_name ] if element_name in STANZA_PAYLOAD_CLASSES : logger = logging . getLogger ( 'pyxmpp.payload_element_name' ) logger . warning ( "Overriding payload class for {0!r}" . format ( element_name ) ) STANZA_PAYLOAD_CLASSES [ element_name ] = klass STANZA_PAYLOAD_ELEMENTS [ klass ] . append ( element_name ) return klass return decorator
2463	def set_file_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_comment_set : self . file_comment_set = True if validations . validate_file_comment ( text ) : self . file ( doc ) . comment = str_from_text ( text ) return True else : raise SPDXValueError ( 'File::Comment' ) else : raise CardinalityError ( 'File::Comment' ) else : raise OrderError ( 'File::Comment' )
10904	def compare_data_model_residuals ( s , tile , data_vmin = 'calc' , data_vmax = 'calc' , res_vmin = - 0.1 , res_vmax = 0.1 , edgepts = 'calc' , do_imshow = True , data_cmap = plt . cm . bone , res_cmap = plt . cm . RdBu ) : residuals = s . residuals [ tile . slicer ] . squeeze ( ) data = s . data [ tile . slicer ] . squeeze ( ) model = s . model [ tile . slicer ] . squeeze ( ) if data . ndim != 2 : raise ValueError ( 'tile does not give a 2D slice' ) im = np . zeros ( [ data . shape [ 0 ] , data . shape [ 1 ] , 4 ] ) if data_vmin == 'calc' : data_vmin = 0.5 * ( data . min ( ) + model . min ( ) ) if data_vmax == 'calc' : data_vmax = 0.5 * ( data . max ( ) + model . max ( ) ) upper_mask , center_mask , lower_mask = trisect_image ( im . shape , edgepts ) gm = data_cmap ( center_data ( model , data_vmin , data_vmax ) ) dt = data_cmap ( center_data ( data , data_vmin , data_vmax ) ) rs = res_cmap ( center_data ( residuals , res_vmin , res_vmax ) ) for a in range ( 4 ) : im [ : , : , a ] [ upper_mask ] = rs [ : , : , a ] [ upper_mask ] im [ : , : , a ] [ center_mask ] = gm [ : , : , a ] [ center_mask ] im [ : , : , a ] [ lower_mask ] = dt [ : , : , a ] [ lower_mask ] if do_imshow : return plt . imshow ( im ) else : return im
600	def compute ( self , activeColumns , predictedColumns , inputValue = None , timestamp = None ) : anomalyScore = computeRawAnomalyScore ( activeColumns , predictedColumns ) if self . _mode == Anomaly . MODE_PURE : score = anomalyScore elif self . _mode == Anomaly . MODE_LIKELIHOOD : if inputValue is None : raise ValueError ( "Selected anomaly mode 'Anomaly.MODE_LIKELIHOOD' " "requires 'inputValue' as parameter to compute() method. " ) probability = self . _likelihood . anomalyProbability ( inputValue , anomalyScore , timestamp ) score = 1 - probability elif self . _mode == Anomaly . MODE_WEIGHTED : probability = self . _likelihood . anomalyProbability ( inputValue , anomalyScore , timestamp ) score = anomalyScore * ( 1 - probability ) if self . _movingAverage is not None : score = self . _movingAverage . next ( score ) if self . _binaryThreshold is not None : if score >= self . _binaryThreshold : score = 1.0 else : score = 0.0 return score
1501	def template_slave_hcl ( cl_args , masters ) : slave_config_template = "%s/standalone/templates/slave.template.hcl" % cl_args [ "config_path" ] slave_config_actual = "%s/standalone/resources/slave.hcl" % cl_args [ "config_path" ] masters_in_quotes = [ '"%s"' % master for master in masters ] template_file ( slave_config_template , slave_config_actual , { "<nomad_masters:master_port>" : ", " . join ( masters_in_quotes ) } )
103	def compute_paddings_for_aspect_ratio ( arr , aspect_ratio ) : do_assert ( arr . ndim in [ 2 , 3 ] ) do_assert ( aspect_ratio > 0 ) height , width = arr . shape [ 0 : 2 ] do_assert ( height > 0 ) aspect_ratio_current = width / height pad_top = 0 pad_right = 0 pad_bottom = 0 pad_left = 0 if aspect_ratio_current < aspect_ratio : diff = ( aspect_ratio * height ) - width pad_right = int ( np . ceil ( diff / 2 ) ) pad_left = int ( np . floor ( diff / 2 ) ) elif aspect_ratio_current > aspect_ratio : diff = ( ( 1 / aspect_ratio ) * width ) - height pad_top = int ( np . floor ( diff / 2 ) ) pad_bottom = int ( np . ceil ( diff / 2 ) ) return pad_top , pad_right , pad_bottom , pad_left
5631	def long_description ( ) : import argparse parser = argparse . ArgumentParser ( ) parser . add_argument ( '--doc' , dest = "doc" , action = "store_true" , default = False ) args , sys . argv = parser . parse_known_args ( sys . argv ) if args . doc : import doc2md , pypandoc md = doc2md . doc2md ( doc2md . __doc__ , "doc2md" , toc = False ) long_description = pypandoc . convert ( md , 'rst' , format = 'md' ) else : return None
5209	def info_qry ( tickers , flds ) -> str : full_list = '\n' . join ( [ f'tickers: {tickers[:8]}' ] + [ f' {tickers[n:(n + 8)]}' for n in range ( 8 , len ( tickers ) , 8 ) ] ) return f'{full_list}\nfields: {flds}'
10529	def get_projects ( limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : print ( OFFSET_WARNING ) params = dict ( limit = limit , offset = offset ) try : res = _pybossa_req ( 'get' , 'project' , params = params ) if type ( res ) . __name__ == 'list' : return [ Project ( project ) for project in res ] else : raise TypeError except : raise
12513	def _crop_img_to ( image , slices , copy = True ) : img = check_img ( image ) data = img . get_data ( ) affine = img . get_affine ( ) cropped_data = data [ slices ] if copy : cropped_data = cropped_data . copy ( ) linear_part = affine [ : 3 , : 3 ] old_origin = affine [ : 3 , 3 ] new_origin_voxel = np . array ( [ s . start for s in slices ] ) new_origin = old_origin + linear_part . dot ( new_origin_voxel ) new_affine = np . eye ( 4 ) new_affine [ : 3 , : 3 ] = linear_part new_affine [ : 3 , 3 ] = new_origin new_img = nib . Nifti1Image ( cropped_data , new_affine ) return new_img
7981	def auth_error ( self , stanza ) : self . lock . acquire ( ) try : err = stanza . get_error ( ) ae = err . xpath_eval ( "e:*" , { "e" : "jabber:iq:auth:error" } ) if ae : ae = ae [ 0 ] . name else : ae = err . get_condition ( ) . name raise LegacyAuthenticationError ( "Authentication error condition: %s" % ( ae , ) ) finally : self . lock . release ( )
6579	def _send_cmd ( self , cmd ) : self . _process . stdin . write ( "{}\n" . format ( cmd ) . encode ( "utf-8" ) ) self . _process . stdin . flush ( )
10700	def paginate_link_tag ( item ) : a_tag = Page . default_link_tag ( item ) if item [ 'type' ] == 'current_page' : return make_html_tag ( 'li' , a_tag , ** { 'class' : 'blue white-text' } ) return make_html_tag ( 'li' , a_tag )
12940	def getRedisPool ( params ) : global RedisPools global _defaultRedisConnectionParams global _redisManagedConnectionParams if not params : params = _defaultRedisConnectionParams isDefaultParams = True else : isDefaultParams = bool ( params is _defaultRedisConnectionParams ) if 'connection_pool' in params : return params [ 'connection_pool' ] hashValue = hashDictOneLevel ( params ) if hashValue in RedisPools : params [ 'connection_pool' ] = RedisPools [ hashValue ] return RedisPools [ hashValue ] if not isDefaultParams : origParams = params params = copy . copy ( params ) else : origParams = params checkAgain = False if 'host' not in params : if not isDefaultParams and 'host' in _defaultRedisConnectionParams : params [ 'host' ] = _defaultRedisConnectionParams [ 'host' ] else : params [ 'host' ] = '127.0.0.1' checkAgain = True if 'port' not in params : if not isDefaultParams and 'port' in _defaultRedisConnectionParams : params [ 'port' ] = _defaultRedisConnectionParams [ 'port' ] else : params [ 'port' ] = 6379 checkAgain = True if 'db' not in params : if not isDefaultParams and 'db' in _defaultRedisConnectionParams : params [ 'db' ] = _defaultRedisConnectionParams [ 'db' ] else : params [ 'db' ] = 0 checkAgain = True if not isDefaultParams : otherGlobalKeys = set ( _defaultRedisConnectionParams . keys ( ) ) - set ( params . keys ( ) ) for otherKey in otherGlobalKeys : if otherKey == 'connection_pool' : continue params [ otherKey ] = _defaultRedisConnectionParams [ otherKey ] checkAgain = True if checkAgain : hashValue = hashDictOneLevel ( params ) if hashValue in RedisPools : params [ 'connection_pool' ] = RedisPools [ hashValue ] return RedisPools [ hashValue ] connectionPool = redis . ConnectionPool ( ** params ) origParams [ 'connection_pool' ] = params [ 'connection_pool' ] = connectionPool RedisPools [ hashValue ] = connectionPool origParamsHash = hashDictOneLevel ( origParams ) if origParamsHash not in _redisManagedConnectionParams : _redisManagedConnectionParams [ origParamsHash ] = [ origParams ] elif origParams not in _redisManagedConnectionParams [ origParamsHash ] : _redisManagedConnectionParams [ origParamsHash ] . append ( origParams ) return connectionPool
2334	def predict_proba ( self , a , b , idx = 0 , ** kwargs ) : return self . predict_dataset ( DataFrame ( [ [ a , b ] ] , columns = [ 'A' , 'B' ] ) )
9847	def _load_cpp4 ( self , filename ) : ccp4 = CCP4 . CCP4 ( ) ccp4 . read ( filename ) grid , edges = ccp4 . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
5841	def submit_design_run ( self , data_view_id , num_candidates , effort , target = None , constraints = [ ] , sampler = "Default" ) : if effort > 30 : raise CitrinationClientError ( "Parameter effort must be less than 30 to trigger a design run" ) if target is not None : target = target . to_dict ( ) constraint_dicts = [ c . to_dict ( ) for c in constraints ] body = { "num_candidates" : num_candidates , "target" : target , "effort" : effort , "constraints" : constraint_dicts , "sampler" : sampler } url = routes . submit_data_view_design ( data_view_id ) response = self . _post_json ( url , body ) . json ( ) return DesignRun ( response [ "data" ] [ "design_run" ] [ "uid" ] )
8977	def _file ( self , file ) : if not self . __text_is_expected : file = BytesWrapper ( file , self . __encoding ) self . __dump_to_file ( file )
10489	def _getBundleId ( self ) : ra = AppKit . NSRunningApplication app = ra . runningApplicationWithProcessIdentifier_ ( self . _getPid ( ) ) return app . bundleIdentifier ( )
11494	def get_default_api_key ( self , email , password ) : parameters = dict ( ) parameters [ 'email' ] = email parameters [ 'password' ] = password response = self . request ( 'midas.user.apikey.default' , parameters ) return response [ 'apikey' ]
367	def rotation ( x , rg = 20 , is_random = False , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , order = 1 ) : if is_random : theta = np . pi / 180 * np . random . uniform ( - rg , rg ) else : theta = np . pi / 180 * rg rotation_matrix = np . array ( [ [ np . cos ( theta ) , - np . sin ( theta ) , 0 ] , [ np . sin ( theta ) , np . cos ( theta ) , 0 ] , [ 0 , 0 , 1 ] ] ) h , w = x . shape [ row_index ] , x . shape [ col_index ] transform_matrix = transform_matrix_offset_center ( rotation_matrix , h , w ) x = affine_transform ( x , transform_matrix , channel_index , fill_mode , cval , order ) return x
12737	def parse_amc ( source ) : lines = 0 frames = 1 frame = { } degrees = False for line in source : lines += 1 line = line . split ( '#' ) [ 0 ] . strip ( ) if not line : continue if line . startswith ( ':' ) : if line . lower ( ) . startswith ( ':deg' ) : degrees = True continue if line . isdigit ( ) : if int ( line ) != frames : raise RuntimeError ( 'frame mismatch on line {}: ' 'produced {} but file claims {}' . format ( lines , frames , line ) ) yield frame frames += 1 frame = { } continue fields = line . split ( ) frame [ fields [ 0 ] ] = list ( map ( float , fields [ 1 : ] ) )
4108	def chirp ( t , f0 = 0. , t1 = 1. , f1 = 100. , form = 'linear' , phase = 0 ) : r valid_forms = [ 'linear' , 'quadratic' , 'logarithmic' ] if form not in valid_forms : raise ValueError ( "Invalid form. Valid form are %s" % valid_forms ) t = numpy . array ( t ) phase = 2. * pi * phase / 360. if form == "linear" : a = pi * ( f1 - f0 ) / t1 b = 2. * pi * f0 y = numpy . cos ( a * t ** 2 + b * t + phase ) elif form == "quadratic" : a = ( 2 / 3. * pi * ( f1 - f0 ) / t1 / t1 ) b = 2. * pi * f0 y = numpy . cos ( a * t ** 3 + b * t + phase ) elif form == "logarithmic" : a = 2. * pi * t1 / numpy . log ( f1 - f0 ) b = 2. * pi * f0 x = ( f1 - f0 ) ** ( 1. / t1 ) y = numpy . cos ( a * x ** t + b * t + phase ) return y
5576	def load_output_writer ( output_params , readonly = False ) : if not isinstance ( output_params , dict ) : raise TypeError ( "output_params must be a dictionary" ) driver_name = output_params [ "format" ] for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : _driver = v . load ( ) if all ( [ hasattr ( _driver , attr ) for attr in [ "OutputData" , "METADATA" ] ] ) and ( _driver . METADATA [ "driver_name" ] == driver_name ) : return _driver . OutputData ( output_params , readonly = readonly ) raise MapcheteDriverError ( "no loader for driver '%s' could be found." % driver_name )
5275	def _edgeLabel ( self , node , parent ) : return self . word [ node . idx + parent . depth : node . idx + node . depth ]
8330	def findAllNext ( self , name = None , attrs = { } , text = None , limit = None , ** kwargs ) : return self . _findAll ( name , attrs , text , limit , self . nextGenerator , ** kwargs )
5031	def get ( self , request , template_id , view_type ) : template = get_object_or_404 ( EnrollmentNotificationEmailTemplate , pk = template_id ) if view_type not in self . view_type_contexts : return HttpResponse ( status = 404 ) base_context = self . view_type_contexts [ view_type ] . copy ( ) base_context . update ( { 'user_name' : self . get_user_name ( request ) } ) return HttpResponse ( template . render_html_template ( base_context ) , content_type = 'text/html' )
3999	def copy_to_local ( local_path , remote_name , remote_path , demote = True ) : if not container_path_exists ( remote_name , remote_path ) : raise RuntimeError ( 'ERROR: Path {} does not exist inside container {}.' . format ( remote_path , remote_name ) ) temp_identifier = str ( uuid . uuid1 ( ) ) copy_path_inside_container ( remote_name , remote_path , os . path . join ( constants . CONTAINER_CP_DIR , temp_identifier ) ) vm_path = os . path . join ( vm_cp_path ( remote_name ) , temp_identifier ) is_dir = vm_path_is_directory ( vm_path ) sync_local_path_from_vm ( local_path , vm_path , demote = demote , is_dir = is_dir )
6513	def _most_popular_gender ( self , name , counter ) : if name not in self . names : return self . unknown_value max_count , max_tie = ( 0 , 0 ) best = self . names [ name ] . keys ( ) [ 0 ] for gender , country_values in self . names [ name ] . items ( ) : count , tie = counter ( country_values ) if count > max_count or ( count == max_count and tie > max_tie ) : max_count , max_tie , best = count , tie , gender return best if max_count > 0 else self . unknown_value
13260	def main ( argv = None , white_list = None , load_yaz_extension = True ) : assert argv is None or isinstance ( argv , list ) , type ( argv ) assert white_list is None or isinstance ( white_list , list ) , type ( white_list ) assert isinstance ( load_yaz_extension , bool ) , type ( load_yaz_extension ) argv = sys . argv if argv is None else argv assert len ( argv ) > 0 , len ( argv ) if load_yaz_extension : load ( "~/.yaz" , "yaz_extension" ) parser = Parser ( prog = argv [ 0 ] ) parser . add_task_tree ( get_task_tree ( white_list ) ) task , kwargs = parser . parse_arguments ( argv ) if task : try : result = task ( ** kwargs ) if isinstance ( result , bool ) : code = 0 if result else 1 output = None elif isinstance ( result , int ) : code = result % 256 output = None else : code = 0 output = result except Error as error : code = error . return_code output = error else : code = 1 output = parser . format_help ( ) . rstrip ( ) if output is not None : print ( output ) sys . exit ( code )
13679	def register_json ( self , data ) : j = json . loads ( data ) self . last_data_timestamp = datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) try : for v in j : self . data [ v [ self . id_key ] ] = { } self . data [ v [ self . id_key ] ] [ self . id_key ] = v [ self . id_key ] self . data [ v [ self . id_key ] ] [ self . value_key ] = v [ self . value_key ] if self . unit_key in v : self . data [ v [ self . id_key ] ] [ self . unit_key ] = v [ self . unit_key ] if self . threshold_key in v : self . data [ v [ self . id_key ] ] [ self . threshold_key ] = v [ self . threshold_key ] for k in self . other_keys : if k in v : self . data [ v [ self . id_key ] ] [ k ] = v [ k ] if self . sensor_time_key in v : self . data [ v [ self . sensor_time_key ] ] [ self . sensor_time_key ] = v [ self . sensor_time_key ] self . data [ v [ self . id_key ] ] [ self . time_key ] = self . last_data_timestamp except KeyError as e : print ( "The main key was not found on the serial input line: " + str ( e ) ) except ValueError as e : print ( "No valid JSON string received. Waiting for the next turn." ) print ( "The error was: " + str ( e ) )
9004	def to_svg ( self , zoom ) : def on_dump ( ) : knitting_pattern = self . patterns . at ( 0 ) layout = GridLayout ( knitting_pattern ) instruction_to_svg = default_instruction_svg_cache ( ) builder = SVGBuilder ( ) kp_to_svg = KnittingPatternToSVG ( knitting_pattern , layout , instruction_to_svg , builder , zoom ) return kp_to_svg . build_SVG_dict ( ) return XMLDumper ( on_dump )
6757	def reboot_or_dryrun ( self , * args , ** kwargs ) : warnings . warn ( 'Use self.run() instead.' , DeprecationWarning , stacklevel = 2 ) self . reboot ( * args , ** kwargs )
105	def pool ( arr , block_size , func , cval = 0 , preserve_dtype = True ) : from . import dtypes as iadt iadt . gate_dtypes ( arr , allowed = [ "bool" , "uint8" , "uint16" , "uint32" , "int8" , "int16" , "int32" , "float16" , "float32" , "float64" , "float128" ] , disallowed = [ "uint64" , "uint128" , "uint256" , "int64" , "int128" , "int256" , "float256" ] , augmenter = None ) do_assert ( arr . ndim in [ 2 , 3 ] ) is_valid_int = is_single_integer ( block_size ) and block_size >= 1 is_valid_tuple = is_iterable ( block_size ) and len ( block_size ) in [ 2 , 3 ] and [ is_single_integer ( val ) and val >= 1 for val in block_size ] do_assert ( is_valid_int or is_valid_tuple ) if is_single_integer ( block_size ) : block_size = [ block_size , block_size ] if len ( block_size ) < arr . ndim : block_size = list ( block_size ) + [ 1 ] input_dtype = arr . dtype arr_reduced = skimage . measure . block_reduce ( arr , tuple ( block_size ) , func , cval = cval ) if preserve_dtype and arr_reduced . dtype . type != input_dtype : arr_reduced = arr_reduced . astype ( input_dtype ) return arr_reduced
4599	def _clean_animation ( desc , parent ) : desc = load . load_if_filename ( desc ) or desc if isinstance ( desc , str ) : animation = { 'typename' : desc } elif not isinstance ( desc , dict ) : raise TypeError ( 'Unexpected type %s in collection' % type ( desc ) ) elif 'typename' in desc or 'animation' not in desc : animation = desc else : animation = desc . pop ( 'animation' , { } ) if isinstance ( animation , str ) : animation = { 'typename' : animation } animation [ 'run' ] = desc . pop ( 'run' , { } ) if desc : raise ValueError ( 'Extra animation fields: ' + ', ' . join ( desc ) ) animation . setdefault ( 'typename' , DEFAULT_ANIMATION ) animation = construct . to_type_constructor ( animation , ANIMATION_PATH ) datatype = animation . setdefault ( 'datatype' , failed . Failed ) animation . setdefault ( 'name' , datatype . __name__ ) run = animation . setdefault ( 'run' , { } ) run_parent = parent . setdefault ( 'run' , { } ) if not ( 'fps' in run or 'sleep_time' in run ) : if 'fps' in run_parent : run . update ( fps = run_parent [ 'fps' ] ) elif 'sleep_time' in run_parent : run . update ( sleep_time = run_parent [ 'sleep_time' ] ) return animation
8991	def rows_before ( self ) : rows_before = [ ] for mesh in self . consumed_meshes : if mesh . is_produced ( ) : row = mesh . producing_row if rows_before not in rows_before : rows_before . append ( row ) return rows_before
13092	def load_targets ( self ) : ldap_services = [ ] if self . ldap : ldap_services = self . search . get_services ( ports = [ 389 ] , up = True ) self . ldap_strings = [ "ldap://{}" . format ( service . address ) for service in ldap_services ] self . services = self . search . get_services ( tags = [ 'smb_signing_disabled' ] ) self . ips = [ str ( service . address ) for service in self . services ]
6825	def deploy_services ( self , site = None ) : verbose = self . verbose r = self . local_renderer if not r . env . manage_configs : return self . render_paths ( ) supervisor_services = [ ] if r . env . purge_all_confs : r . sudo ( 'rm -Rf /etc/supervisor/conf.d/*' ) self . write_configs ( site = site ) for _site , site_data in self . iter_sites ( site = site , renderer = self . render_paths ) : if verbose : print ( 'deploy_services.site:' , _site ) for cb in self . genv . _supervisor_create_service_callbacks : if self . verbose : print ( 'cb:' , cb ) ret = cb ( site = _site ) if self . verbose : print ( 'ret:' , ret ) if isinstance ( ret , six . string_types ) : supervisor_services . append ( ret ) elif isinstance ( ret , tuple ) : assert len ( ret ) == 2 conf_name , conf_content = ret if self . dryrun : print ( 'supervisor conf filename:' , conf_name ) print ( conf_content ) self . write_to_file ( conf_content ) self . env . services_rendered = '\n' . join ( supervisor_services ) fn = self . render_to_file ( self . env . config_template ) r . put ( local_path = fn , remote_path = self . env . config_path , use_sudo = True ) if not self . is_running ( ) : self . start ( ) r . sudo ( 'supervisorctl update' )
13527	def clean ( options , info ) : info ( "Cleaning patterns %s" , options . paved . clean . patterns ) for wd in options . paved . clean . dirs : info ( "Cleaning in %s" , wd ) for p in options . paved . clean . patterns : for f in wd . walkfiles ( p ) : f . remove ( )
9652	def get_sha ( a_file , settings = None ) : if settings : error = settings [ "error" ] else : error = ERROR_FN try : BLOCKSIZE = 65536 hasher = hashlib . sha1 ( ) with io . open ( a_file , "rb" ) as fh : buf = fh . read ( BLOCKSIZE ) while len ( buf ) > 0 : hasher . update ( buf ) buf = fh . read ( BLOCKSIZE ) the_hash = hasher . hexdigest ( ) except IOError : errmes = "File '{}' could not be read! Exiting!" . format ( a_file ) error ( errmes ) sys . exit ( 1 ) except : errmes = "Unspecified error returning sha1 hash. Exiting!" error ( errmes ) sys . exit ( 1 ) return the_hash
12301	def instantiate ( repo , validator_name = None , filename = None , rulesfiles = None ) : default_validators = repo . options . get ( 'validator' , { } ) validators = { } if validator_name is not None : if validator_name in default_validators : validators = { validator_name : default_validators [ validator_name ] } else : validators = { validator_name : { 'files' : [ ] , 'rules' : { } , 'rules-files' : [ ] } } else : validators = default_validators if filename is not None : matching_files = repo . find_matching_files ( [ filename ] ) if len ( matching_files ) == 0 : print ( "Filename could not be found" , filename ) raise Exception ( "Invalid filename pattern" ) for v in validators : validators [ v ] [ 'files' ] = matching_files else : for v in validators : if 'files' not in validators [ v ] : validators [ v ] [ 'files' ] = [ ] elif len ( validators [ v ] [ 'files' ] ) > 0 : matching_files = repo . find_matching_files ( validators [ v ] [ 'files' ] ) validators [ v ] [ 'files' ] = matching_files if rulesfiles is not None : matching_files = repo . find_matching_files ( [ rulesfiles ] ) if len ( matching_files ) == 0 : print ( "Could not find matching rules files ({}) for {}" . format ( rulesfiles , v ) ) raise Exception ( "Invalid rules" ) for v in validators : validators [ v ] [ 'rules-files' ] = matching_files else : for v in validators : if 'rules-files' not in validators [ v ] : validators [ v ] [ 'rules-files' ] = [ ] else : rulesfiles = validators [ v ] [ 'rules-files' ] matching_files = repo . find_matching_files ( rulesfiles ) validators [ v ] [ 'rules-files' ] = matching_files return validators
9843	def __tokenize ( self , string ) : for m in self . dx_regex . finditer ( string . strip ( ) ) : code = m . lastgroup text = m . group ( m . lastgroup ) tok = Token ( code , text ) if not tok . iscode ( 'WHITESPACE' ) : self . tokens . append ( tok )
11640	def json_get_data ( filename ) : with open ( filename ) as fp : json_data = json . load ( fp ) return json_data return False
2733	def get_object ( cls , api_token ) : acct = cls ( token = api_token ) acct . load ( ) return acct
1912	def SInt ( value , width ) : return Operators . ITEBV ( width , Bit ( value , width - 1 ) == 1 , GetNBits ( value , width ) - 2 ** width , GetNBits ( value , width ) )
5779	def _advapi32_decrypt ( private_key , ciphertext , rsa_oaep_padding = False ) : flags = 0 if rsa_oaep_padding : flags = Advapi32Const . CRYPT_OAEP ciphertext = ciphertext [ : : - 1 ] buffer = buffer_from_bytes ( ciphertext ) out_len = new ( advapi32 , 'DWORD *' , len ( ciphertext ) ) res = advapi32 . CryptDecrypt ( private_key . ex_key_handle , null ( ) , True , flags , buffer , out_len ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) )
3486	def _check_required ( sbase , value , attribute ) : if ( value is None ) or ( value == "" ) : msg = "Required attribute '%s' cannot be found or parsed in '%s'" % ( attribute , sbase ) if hasattr ( sbase , "getId" ) and sbase . getId ( ) : msg += " with id '%s'" % sbase . getId ( ) elif hasattr ( sbase , "getName" ) and sbase . getName ( ) : msg += " with name '%s'" % sbase . getName ( ) elif hasattr ( sbase , "getMetaId" ) and sbase . getMetaId ( ) : msg += " with metaId '%s'" % sbase . getName ( ) raise CobraSBMLError ( msg ) return value
4429	async def _queue ( self , ctx , page : int = 1 ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue : return await ctx . send ( 'There\'s nothing in the queue! Why not queue something?' ) items_per_page = 10 pages = math . ceil ( len ( player . queue ) / items_per_page ) start = ( page - 1 ) * items_per_page end = start + items_per_page queue_list = '' for index , track in enumerate ( player . queue [ start : end ] , start = start ) : queue_list += f'`{index + 1}.` [**{track.title}**]({track.uri})\n' embed = discord . Embed ( colour = discord . Color . blurple ( ) , description = f'**{len(player.queue)} tracks**\n\n{queue_list}' ) embed . set_footer ( text = f'Viewing page {page}/{pages}' ) await ctx . send ( embed = embed )
1133	def getlines ( filename , module_globals = None ) : if filename in cache : return cache [ filename ] [ 2 ] try : return updatecache ( filename , module_globals ) except MemoryError : clearcache ( ) return [ ]
1828	def RET ( cpu , * operands ) : N = 0 if len ( operands ) > 0 : N = operands [ 0 ] . read ( ) cpu . PC = cpu . pop ( cpu . address_bit_size ) cpu . STACK += N
822	def next ( self , newValue ) : newAverage , self . slidingWindow , self . total = self . compute ( self . slidingWindow , self . total , newValue , self . windowSize ) return newAverage
8050	def load_source ( self ) : if self . filename in self . STDIN_NAMES : self . filename = "stdin" if sys . version_info [ 0 ] < 3 : self . source = sys . stdin . read ( ) else : self . source = TextIOWrapper ( sys . stdin . buffer , errors = "ignore" ) . read ( ) else : handle = tokenize_open ( self . filename ) self . source = handle . read ( ) handle . close ( )
9690	def start ( self ) : self . receiver = self . Receiver ( self . read , self . write , self . send_lock , self . senders , self . frames_received , callback = self . receive_callback , fcs_nack = self . fcs_nack , ) self . receiver . start ( )
13008	def path ( self ) : path = super ( WindowsPath2 , self ) . path if path . startswith ( "\\\\?\\" ) : return path [ 4 : ] return path
3601	def create_token ( self , data , options = None ) : if not options : options = { } options . update ( { 'admin' : self . admin , 'debug' : self . debug } ) claims = self . _create_options_claims ( options ) claims [ 'v' ] = self . TOKEN_VERSION claims [ 'iat' ] = int ( time . mktime ( time . gmtime ( ) ) ) claims [ 'd' ] = data return self . _encode_token ( self . secret , claims )
7783	def _deactivate ( self ) : self . cache . remove_fetcher ( self ) if self . active : self . _deactivated ( )
758	def generateRandomInput ( numRecords , elemSize = 400 , numSet = 42 ) : inputs = [ ] for _ in xrange ( numRecords ) : input = np . zeros ( elemSize , dtype = realDType ) for _ in range ( 0 , numSet ) : ind = np . random . random_integers ( 0 , elemSize - 1 , 1 ) [ 0 ] input [ ind ] = 1 while abs ( input . sum ( ) - numSet ) > 0.1 : ind = np . random . random_integers ( 0 , elemSize - 1 , 1 ) [ 0 ] input [ ind ] = 1 inputs . append ( input ) return inputs
281	def plot_monthly_returns_dist ( returns , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) x_axis_formatter = FuncFormatter ( utils . percentage ) ax . xaxis . set_major_formatter ( FuncFormatter ( x_axis_formatter ) ) ax . tick_params ( axis = 'x' , which = 'major' ) monthly_ret_table = ep . aggregate_returns ( returns , 'monthly' ) ax . hist ( 100 * monthly_ret_table , color = 'orangered' , alpha = 0.80 , bins = 20 , ** kwargs ) ax . axvline ( 100 * monthly_ret_table . mean ( ) , color = 'gold' , linestyle = '--' , lw = 4 , alpha = 1.0 ) ax . axvline ( 0.0 , color = 'black' , linestyle = '-' , lw = 3 , alpha = 0.75 ) ax . legend ( [ 'Mean' ] , frameon = True , framealpha = 0.5 ) ax . set_ylabel ( 'Number of months' ) ax . set_xlabel ( 'Returns' ) ax . set_title ( "Distribution of monthly returns" ) return ax
11811	def score ( self , word , docid ) : "Compute a score for this word on this docid." return ( math . log ( 1 + self . index [ word ] [ docid ] ) / math . log ( 1 + self . documents [ docid ] . nwords ) )
4710	def get_chunk_information ( self , chk , lun , chunk_name ) : cmd = [ "nvm_cmd rprt_lun" , self . envs , "%d %d > %s" % ( chk , lun , chunk_name ) ] status , _ , _ = cij . ssh . command ( cmd , shell = True ) return status
6549	def fill_field ( self , ypos , xpos , tosend , length ) : if length < len ( tosend ) : raise FieldTruncateError ( 'length limit %d, but got "%s"' % ( length , tosend ) ) if xpos is not None and ypos is not None : self . move_to ( ypos , xpos ) self . delete_field ( ) self . send_string ( tosend )
8356	def _toUnicode ( self , data , encoding ) : if ( len ( data ) >= 4 ) and ( data [ : 2 ] == '\xfe\xff' ) and ( data [ 2 : 4 ] != '\x00\x00' ) : encoding = 'utf-16be' data = data [ 2 : ] elif ( len ( data ) >= 4 ) and ( data [ : 2 ] == '\xff\xfe' ) and ( data [ 2 : 4 ] != '\x00\x00' ) : encoding = 'utf-16le' data = data [ 2 : ] elif data [ : 3 ] == '\xef\xbb\xbf' : encoding = 'utf-8' data = data [ 3 : ] elif data [ : 4 ] == '\x00\x00\xfe\xff' : encoding = 'utf-32be' data = data [ 4 : ] elif data [ : 4 ] == '\xff\xfe\x00\x00' : encoding = 'utf-32le' data = data [ 4 : ] newdata = unicode ( data , encoding ) return newdata
11340	def set_target_celsius ( self , celsius , mode = config . SCHEDULE_HOLD ) : temperature = celsius_to_nuheat ( celsius ) self . set_target_temperature ( temperature , mode )
7457	def putstats ( pfile , handle , statdicts ) : with open ( pfile , 'r' ) as infile : filestats , samplestats = pickle . load ( infile ) perfile , fsamplehits , fbarhits , fmisses , fdbars = statdicts perfile [ handle ] += filestats samplehits , barhits , misses , dbars = samplestats fsamplehits . update ( samplehits ) fbarhits . update ( barhits ) fmisses . update ( misses ) fdbars . update ( dbars ) statdicts = perfile , fsamplehits , fbarhits , fmisses , fdbars return statdicts
5126	def start_collecting_data ( self , queues = None , edge = None , edge_type = None ) : queues = _get_queues ( self . g , queues , edge , edge_type ) for k in queues : self . edge2queue [ k ] . collect_data = True
1304	def SendMessage ( handle : int , msg : int , wParam : int , lParam : int ) -> int : return ctypes . windll . user32 . SendMessageW ( ctypes . c_void_p ( handle ) , msg , wParam , lParam )
11378	def get_publication_date ( self , xml_doc ) : start_date = get_value_in_tag ( xml_doc , "prism:coverDate" ) if not start_date : start_date = get_value_in_tag ( xml_doc , "prism:coverDisplayDate" ) if not start_date : start_date = get_value_in_tag ( xml_doc , 'oa:openAccessEffective' ) if start_date : start_date = datetime . datetime . strptime ( start_date , "%Y-%m-%dT%H:%M:%SZ" ) return start_date . strftime ( "%Y-%m-%d" ) import dateutil . parser start_date = re . sub ( '([A-Z][a-z]+)[\s\-][A-Z][a-z]+ (\d{4})' , r'\1 \2' , start_date ) try : date = dateutil . parser . parse ( start_date ) except ValueError : return '' if len ( start_date . split ( " " ) ) == 3 : return date . strftime ( "%Y-%m-%d" ) else : return date . strftime ( "%Y-%m" ) else : if len ( start_date ) is 8 : start_date = time . strftime ( '%Y-%m-%d' , time . strptime ( start_date , '%Y%m%d' ) ) elif len ( start_date ) is 6 : start_date = time . strftime ( '%Y-%m' , time . strptime ( start_date , '%Y%m' ) ) return start_date
13146	def remove_direct_link_triples ( train , valid , test ) : pairs = set ( ) merged = valid + test for t in merged : pairs . add ( ( t . head , t . tail ) ) filtered = filterfalse ( lambda t : ( t . head , t . tail ) in pairs or ( t . tail , t . head ) in pairs , train ) return list ( filtered )
5595	def to_dict ( self ) : return dict ( grid = self . grid . to_dict ( ) , metatiling = self . metatiling , tile_size = self . tile_size , pixelbuffer = self . pixelbuffer )
7654	def summary ( obj , indent = 0 ) : if hasattr ( obj , '__summary__' ) : rep = obj . __summary__ ( ) elif isinstance ( obj , SortedKeyList ) : rep = '<{:d} observations>' . format ( len ( obj ) ) else : rep = repr ( obj ) return rep . replace ( '\n' , '\n' + ' ' * indent )
1632	def CheckForBadCharacters ( filename , lines , error ) : for linenum , line in enumerate ( lines ) : if unicode_escape_decode ( '\ufffd' ) in line : error ( filename , linenum , 'readability/utf8' , 5 , 'Line contains invalid UTF-8 (or Unicode replacement character).' ) if '\0' in line : error ( filename , linenum , 'readability/nul' , 5 , 'Line contains NUL byte.' )
9454	def play_stop ( self , call_params ) : path = '/' + self . api_version + '/PlayStop/' method = 'POST' return self . request ( path , method , call_params )
3642	def sell ( self , item_id , bid , buy_now , duration = 3600 , fast = False ) : method = 'POST' url = 'auctionhouse' data = { 'buyNowPrice' : buy_now , 'startingBid' : bid , 'duration' : duration , 'itemData' : { 'id' : item_id } } rc = self . __request__ ( method , url , data = json . dumps ( data ) , params = { 'sku_b' : self . sku_b } ) if not fast : self . tradeStatus ( rc [ 'id' ] ) return rc [ 'id' ]
7988	def _setup_stream_element_handlers ( self ) : if self . initiator : mode = "initiator" else : mode = "receiver" self . _element_handlers = { } for handler in self . handlers : if not isinstance ( handler , StreamFeatureHandler ) : continue for _unused , meth in inspect . getmembers ( handler , callable ) : if not hasattr ( meth , "_pyxmpp_stream_element_handled" ) : continue element_handled = meth . _pyxmpp_stream_element_handled if element_handled in self . _element_handlers : continue if meth . _pyxmpp_usage_restriction in ( None , mode ) : self . _element_handlers [ element_handled ] = meth
12531	def rename_file_group_to_serial_nums ( file_lst ) : file_lst . sort ( ) c = 1 for f in file_lst : dirname = get_abspath ( f . dirname ( ) ) fdest = f . joinpath ( dirname , "{0:04d}" . format ( c ) + OUTPUT_DICOM_EXTENSION ) log . info ( 'Renaming {0} to {1}' . format ( f , fdest ) ) f . rename ( fdest ) c += 1
13751	def handle_data ( self , data ) : if data . strip ( ) : data = djeffify_string ( data ) self . djhtml += data
2839	def pullup ( self , pin , enabled ) : self . _validate_pin ( pin ) if enabled : self . gppu [ int ( pin / 8 ) ] |= 1 << ( int ( pin % 8 ) ) else : self . gppu [ int ( pin / 8 ) ] &= ~ ( 1 << ( int ( pin % 8 ) ) ) self . write_gppu ( )
2721	def get_object ( cls , api_token , droplet_id ) : droplet = cls ( token = api_token , id = droplet_id ) droplet . load ( ) return droplet
13655	def _matchRoute ( components , request , segments , partialMatching ) : if len ( components ) == 1 and isinstance ( components [ 0 ] , bytes ) : components = components [ 0 ] if components [ : 1 ] == '/' : components = components [ 1 : ] components = components . split ( '/' ) results = OrderedDict ( ) NO_MATCH = None , segments remaining = list ( segments ) if len ( segments ) == len ( components ) == 0 : return results , remaining for us , them in izip_longest ( components , segments ) : if us is None : if partialMatching : break else : return NO_MATCH elif them is None : return NO_MATCH if callable ( us ) : name , match = us ( request , them ) if match is None : return NO_MATCH results [ name ] = match elif us != them : return NO_MATCH remaining . pop ( 0 ) return results , remaining
10502	def waitForValueToChange ( self , timeout = 10 ) : callback = AXCallbacks . returnElemCallback retelem = None return self . waitFor ( timeout , 'AXValueChanged' , callback = callback , args = ( retelem , ) )
13021	def process_columns ( self , columns ) : if type ( columns ) == list : self . columns = columns elif type ( columns ) == str : self . columns = [ c . strip ( ) for c in columns . split ( ) ] elif type ( columns ) == IntEnum : self . columns = [ str ( c ) for c in columns ] else : raise RawlException ( "Unknown format for columns" )
13056	def _plugin_endpoint_rename ( fn_name , instance ) : if instance and instance . namespaced : fn_name = "r_{0}_{1}" . format ( instance . name , fn_name [ 2 : ] ) return fn_name
935	def readFromCheckpoint ( cls , checkpointDir ) : checkpointPath = cls . _getModelCheckpointFilePath ( checkpointDir ) with open ( checkpointPath , 'r' ) as f : proto = cls . getSchema ( ) . read ( f , traversal_limit_in_words = _TRAVERSAL_LIMIT_IN_WORDS ) model = cls . read ( proto ) return model
4356	def get_multiple_client_msgs ( self , ** kwargs ) : client_queue = self . client_queue msgs = [ client_queue . get ( ** kwargs ) ] while client_queue . qsize ( ) : msgs . append ( client_queue . get ( ) ) return msgs
8200	def set_bot ( self , bot ) : self . bot = bot self . sink . set_bot ( bot )
4320	def set_globals ( self , dither = False , guard = False , multithread = False , replay_gain = False , verbosity = 2 ) : if not isinstance ( dither , bool ) : raise ValueError ( 'dither must be a boolean.' ) if not isinstance ( guard , bool ) : raise ValueError ( 'guard must be a boolean.' ) if not isinstance ( multithread , bool ) : raise ValueError ( 'multithread must be a boolean.' ) if not isinstance ( replay_gain , bool ) : raise ValueError ( 'replay_gain must be a boolean.' ) if verbosity not in VERBOSITY_VALS : raise ValueError ( 'Invalid value for VERBOSITY. Must be one {}' . format ( VERBOSITY_VALS ) ) global_args = [ ] if not dither : global_args . append ( '-D' ) if guard : global_args . append ( '-G' ) if multithread : global_args . append ( '--multi-threaded' ) if replay_gain : global_args . append ( '--replay-gain' ) global_args . append ( 'track' ) global_args . append ( '-V{}' . format ( verbosity ) ) self . globals = global_args return self
6497	def index ( self , doc_type , sources , ** kwargs ) : try : actions = [ ] for source in sources : self . _check_mappings ( doc_type , source ) id_ = source [ 'id' ] if 'id' in source else None log . debug ( "indexing %s object with id %s" , doc_type , id_ ) action = { "_index" : self . index_name , "_type" : doc_type , "_id" : id_ , "_source" : source } actions . append ( action ) _ , indexing_errors = bulk ( self . _es , actions , ** kwargs ) if indexing_errors : ElasticSearchEngine . log_indexing_error ( indexing_errors ) except Exception as ex : log . exception ( "error while indexing - %s" , str ( ex ) ) raise
12498	def xfm_atlas_to_functional ( atlas_filepath , anatbrain_filepath , meanfunc_filepath , atlas2anat_nonlin_xfm_filepath , is_atlas2anat_inverted , anat2func_lin_xfm_filepath , atlasinanat_out_filepath , atlasinfunc_out_filepath , interp = 'nn' , rewrite = True , parallel = False ) : if is_atlas2anat_inverted : anat_to_mni_nl_inv = atlas2anat_nonlin_xfm_filepath else : output_dir = op . abspath ( op . dirname ( atlasinanat_out_filepath ) ) ext = get_extension ( atlas2anat_nonlin_xfm_filepath ) anat_to_mni_nl_inv = op . join ( output_dir , remove_ext ( op . basename ( atlas2anat_nonlin_xfm_filepath ) ) + '_inv' + ext ) invwarp_cmd = op . join ( '${FSLDIR}' , 'bin' , 'invwarp' ) applywarp_cmd = op . join ( '${FSLDIR}' , 'bin' , 'applywarp' ) fslsub_cmd = op . join ( '${FSLDIR}' , 'bin' , 'fsl_sub' ) if parallel : invwarp_cmd = fslsub_cmd + ' ' + invwarp_cmd applywarp_cmd = fslsub_cmd + ' ' + applywarp_cmd if rewrite or ( not is_atlas2anat_inverted and not op . exists ( anat_to_mni_nl_inv ) ) : log . debug ( 'Creating {}.\n' . format ( anat_to_mni_nl_inv ) ) cmd = invwarp_cmd + ' ' cmd += '-w {} ' . format ( atlas2anat_nonlin_xfm_filepath ) cmd += '-o {} ' . format ( anat_to_mni_nl_inv ) cmd += '-r {} ' . format ( anatbrain_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd ) if rewrite or not op . exists ( atlasinanat_out_filepath ) : log . debug ( 'Creating {}.\n' . format ( atlasinanat_out_filepath ) ) cmd = applywarp_cmd + ' ' cmd += '--in={} ' . format ( atlas_filepath ) cmd += '--ref={} ' . format ( anatbrain_filepath ) cmd += '--warp={} ' . format ( anat_to_mni_nl_inv ) cmd += '--interp={} ' . format ( interp ) cmd += '--out={} ' . format ( atlasinanat_out_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd ) if rewrite or not op . exists ( atlasinfunc_out_filepath ) : log . debug ( 'Creating {}.\n' . format ( atlasinfunc_out_filepath ) ) cmd = applywarp_cmd + ' ' cmd += '--in={} ' . format ( atlasinanat_out_filepath ) cmd += '--ref={} ' . format ( meanfunc_filepath ) cmd += '--premat={} ' . format ( anat2func_lin_xfm_filepath ) cmd += '--interp={} ' . format ( interp ) cmd += '--out={} ' . format ( atlasinfunc_out_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd )
138	def to_shapely_line_string ( self , closed = False , interpolate = 0 ) : return _convert_points_to_shapely_line_string ( self . exterior , closed = closed , interpolate = interpolate )
1268	def _fire ( self , layers , things , the_plot ) : if the_plot . get ( 'last_player_shot' ) == the_plot . frame : return the_plot [ 'last_player_shot' ] = the_plot . frame row , col = things [ 'P' ] . position self . _teleport ( ( row - 1 , col ) )
3582	def _get_objects_by_path ( self , paths ) : return map ( lambda x : self . _bus . get_object ( 'org.bluez' , x ) , paths )
4893	def _collect_certificate_data ( self , enterprise_enrollment ) : if self . certificates_api is None : self . certificates_api = CertificatesApiClient ( self . user ) course_id = enterprise_enrollment . course_id username = enterprise_enrollment . enterprise_customer_user . user . username try : certificate = self . certificates_api . get_course_certificate ( course_id , username ) completed_date = certificate . get ( 'created_date' ) if completed_date : completed_date = parse_datetime ( completed_date ) else : completed_date = timezone . now ( ) is_passing = certificate . get ( 'is_passing' ) grade = self . grade_passing if is_passing else self . grade_failing except HttpNotFoundError : completed_date = None grade = self . grade_incomplete is_passing = False return completed_date , grade , is_passing
6209	def print_attrs ( data_file , node_name = '/' , which = 'user' , compress = False ) : node = data_file . get_node ( node_name ) print ( 'List of attributes for:\n %s\n' % node ) for attr in node . _v_attrs . _f_list ( ) : print ( '\t%s' % attr ) attr_content = repr ( node . _v_attrs [ attr ] ) if compress : attr_content = attr_content . split ( '\n' ) [ 0 ] print ( "\t %s" % attr_content )
13674	def add_directory ( self , * args , ** kwargs ) : exc = kwargs . get ( 'exclusions' , None ) for path in args : self . files . append ( DirectoryPath ( path , self , exclusions = exc ) )
4937	def transform_description ( self , content_metadata_item ) : full_description = content_metadata_item . get ( 'full_description' ) or '' if 0 < len ( full_description ) <= self . LONG_STRING_LIMIT : return full_description return content_metadata_item . get ( 'short_description' ) or content_metadata_item . get ( 'title' ) or ''
9916	def validate_is_primary ( self , is_primary ) : if is_primary and not ( self . instance and self . instance . is_verified ) : raise serializers . ValidationError ( _ ( "Unverified email addresses may not be used as the " "primary address." ) ) return is_primary
9367	def bik ( ) : return '04' + '' . join ( [ str ( random . randint ( 1 , 9 ) ) for _ in range ( 5 ) ] ) + str ( random . randint ( 0 , 49 ) + 50 )
8895	def _with_error_handling ( resp , error , mode , response_format ) : def safe_parse ( r ) : try : return APIWrapper . _parse_resp ( r , response_format ) except ( ValueError , SyntaxError ) as ex : log . error ( ex ) r . parsed = None return r if isinstance ( error , requests . HTTPError ) : if resp . status_code == 400 : resp = safe_parse ( resp ) if resp . parsed is not None : parsed_resp = resp . parsed messages = [ ] if response_format == 'xml' and parsed_resp . find ( './ValidationErrors' ) is not None : messages = [ e . find ( './Message' ) . text for e in parsed_resp . findall ( './ValidationErrors/ValidationErrorDto' ) ] elif response_format == 'json' and 'ValidationErrors' in parsed_resp : messages = [ e [ 'Message' ] for e in parsed_resp [ 'ValidationErrors' ] ] error = requests . HTTPError ( '%s: %s' % ( error , '\n\t' . join ( messages ) ) , response = resp ) elif resp . status_code == 429 : error = requests . HTTPError ( '%sToo many requests in the last minute.' % error , response = resp ) if STRICT == mode : raise error elif GRACEFUL == mode : if isinstance ( error , EmptyResponse ) : log . warning ( error ) resp . parsed = None return resp elif isinstance ( error , requests . HTTPError ) : if resp . status_code == 429 : log . warning ( error ) return safe_parse ( resp ) else : raise error else : raise error else : log . error ( error ) return safe_parse ( resp )
3944	def serialize ( self ) : segment = hangouts_pb2 . Segment ( type = self . type_ , text = self . text , formatting = hangouts_pb2 . Formatting ( bold = self . is_bold , italic = self . is_italic , strikethrough = self . is_strikethrough , underline = self . is_underline , ) , ) if self . link_target is not None : segment . link_data . link_target = self . link_target return segment
6921	def _autocorr_func3 ( mags , lag , maglen , magmed , magstd ) : result = npcorrelate ( mags , mags , mode = 'full' ) result = result / npmax ( result ) return result [ int ( result . size / 2 ) : ]
1620	def FindNextMultiLineCommentStart ( lines , lineix ) : while lineix < len ( lines ) : if lines [ lineix ] . strip ( ) . startswith ( '/*' ) : if lines [ lineix ] . strip ( ) . find ( '*/' , 2 ) < 0 : return lineix lineix += 1 return len ( lines )
5619	def execute ( mp , td_resampling = "nearest" , td_matching_method = "gdal" , td_matching_max_zoom = None , td_matching_precision = 8 , td_fallback_to_higher_zoom = False , clip_pixelbuffer = 0 , ** kwargs ) : if "clip" in mp . params [ "input" ] : clip_geom = mp . open ( "clip" ) . read ( ) if not clip_geom : logger . debug ( "no clip data over tile" ) return "empty" else : clip_geom = [ ] with mp . open ( "raster" , matching_method = td_matching_method , matching_max_zoom = td_matching_max_zoom , matching_precision = td_matching_precision , fallback_to_higher_zoom = td_fallback_to_higher_zoom , resampling = td_resampling ) as raster : raster_data = raster . read ( ) if raster . is_empty ( ) or raster_data [ 0 ] . mask . all ( ) : logger . debug ( "raster empty" ) return "empty" if clip_geom : clipped = mp . clip ( np . where ( raster_data [ 0 ] . mask , mp . params [ "output" ] . nodata , raster_data ) , clip_geom , clip_buffer = clip_pixelbuffer , inverted = True ) return np . where ( clipped . mask , clipped , mp . params [ "output" ] . nodata ) else : return np . where ( raster_data [ 0 ] . mask , mp . params [ "output" ] . nodata , raster_data )
2830	def convert_upsample ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting upsample...' ) if params [ 'mode' ] != 'nearest' : raise AssertionError ( 'Cannot convert non-nearest upsampling' ) if names == 'short' : tf_name = 'UPSL' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'height_scale' in params : scale = ( params [ 'height_scale' ] , params [ 'width_scale' ] ) elif len ( inputs ) == 2 : scale = layers [ inputs [ - 1 ] + '_np' ] [ - 2 : ] upsampling = keras . layers . UpSampling2D ( size = scale , name = tf_name ) layers [ scope_name ] = upsampling ( layers [ inputs [ 0 ] ] )
11751	def get_blueprint_routes ( app , base_path ) : routes = [ ] for child in app . url_map . iter_rules ( ) : if child . rule . startswith ( base_path ) : relative_path = child . rule [ len ( base_path ) : ] routes . append ( { 'path' : relative_path , 'endpoint' : child . endpoint , 'methods' : list ( child . methods ) } ) return routes
10020	def environment_exists ( self , env_name ) : response = self . ebs . describe_environments ( application_name = self . app_name , environment_names = [ env_name ] , include_deleted = False ) return len ( response [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ] ) > 0 and response [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ] [ 0 ] [ 'Status' ] != 'Terminated'
5690	def read_data_as_dataframe ( self , travel_impedance_measure , from_stop_I = None , to_stop_I = None , statistic = None ) : to_select = [ ] where_clauses = [ ] to_select . append ( "from_stop_I" ) to_select . append ( "to_stop_I" ) if from_stop_I is not None : where_clauses . append ( "from_stop_I=" + str ( int ( from_stop_I ) ) ) if to_stop_I is not None : where_clauses . append ( "to_stop_I=" + str ( int ( to_stop_I ) ) ) where_clause = "" if len ( where_clauses ) > 0 : where_clause = " WHERE " + " AND " . join ( where_clauses ) if not statistic : to_select . extend ( [ "min" , "mean" , "median" , "max" ] ) else : to_select . append ( statistic ) to_select_clause = "," . join ( to_select ) if not to_select_clause : to_select_clause = "*" sql = "SELECT " + to_select_clause + " FROM " + travel_impedance_measure + where_clause + ";" df = pd . read_sql ( sql , self . conn ) return df
10475	def _sendKeyWithModifiers ( self , keychr , modifiers , globally = False ) : if not self . _isSingleCharacter ( keychr ) : raise ValueError ( 'Please provide only one character to send' ) if not hasattr ( self , 'keyboard' ) : self . keyboard = AXKeyboard . loadKeyboard ( ) modFlags = self . _pressModifiers ( modifiers , globally = globally ) self . _sendKey ( keychr , modFlags , globally = globally ) self . _releaseModifiers ( modifiers , globally = globally ) self . _postQueuedEvents ( )
3622	def __post_save_receiver ( self , instance , ** kwargs ) : logger . debug ( 'RECEIVE post_save FOR %s' , instance . __class__ ) self . save_record ( instance , ** kwargs )
6653	def start ( self , builddir , program , forward_args ) : child = None try : prog_path = self . findProgram ( builddir , program ) if prog_path is None : return start_env , start_vars = self . buildProgEnvAndVars ( prog_path , builddir ) if self . getScript ( 'start' ) : cmd = [ os . path . expandvars ( string . Template ( x ) . safe_substitute ( ** start_vars ) ) for x in self . getScript ( 'start' ) ] + forward_args else : cmd = shlex . split ( './' + prog_path ) + forward_args logger . debug ( 'starting program: %s' , cmd ) child = subprocess . Popen ( cmd , cwd = builddir , env = start_env ) child . wait ( ) if child . returncode : return "process exited with status %s" % child . returncode child = None except OSError as e : import errno if e . errno == errno . ENOEXEC : return ( "the program %s cannot be run (perhaps your target " + "needs to define a 'start' script to start it on its " "intended execution target?)" ) % prog_path finally : if child is not None : _tryTerminate ( child )
969	def _extractCallingMethodArgs ( ) : import inspect import copy callingFrame = inspect . stack ( ) [ 1 ] [ 0 ] argNames , _ , _ , frameLocalVarDict = inspect . getargvalues ( callingFrame ) argNames . remove ( "self" ) args = copy . copy ( frameLocalVarDict ) for varName in frameLocalVarDict : if varName not in argNames : args . pop ( varName ) return args
1742	def make_grid ( tensor , nrow = 8 , padding = 2 , pad_value = 0 ) : if not ( isinstance ( tensor , np . ndarray ) or ( isinstance ( tensor , list ) and all ( isinstance ( t , np . ndarray ) for t in tensor ) ) ) : raise TypeError ( 'tensor or list of tensors expected, got {}' . format ( type ( tensor ) ) ) if isinstance ( tensor , list ) : tensor = np . stack ( tensor , 0 ) if tensor . ndim == 2 : tensor = tensor . reshape ( ( 1 , tensor . shape [ 0 ] , tensor . shape [ 1 ] ) ) if tensor . ndim == 3 : if tensor . shape [ 0 ] == 1 : tensor = np . concatenate ( ( tensor , tensor , tensor ) , 0 ) tensor = tensor . reshape ( ( 1 , tensor . shape [ 0 ] , tensor . shape [ 1 ] , tensor . shape [ 2 ] ) ) if tensor . ndim == 4 and tensor . shape [ 1 ] == 1 : tensor = np . concatenate ( ( tensor , tensor , tensor ) , 1 ) if tensor . shape [ 0 ] == 1 : return np . squeeze ( tensor ) nmaps = tensor . shape [ 0 ] xmaps = min ( nrow , nmaps ) ymaps = int ( math . ceil ( float ( nmaps ) / xmaps ) ) height , width = int ( tensor . shape [ 2 ] + padding ) , int ( tensor . shape [ 3 ] + padding ) grid = np . ones ( ( 3 , height * ymaps + padding , width * xmaps + padding ) ) * pad_value k = 0 for y in range ( ymaps ) : for x in range ( xmaps ) : if k >= nmaps : break grid [ : , y * height + padding : ( y + 1 ) * height , x * width + padding : ( x + 1 ) * width ] = tensor [ k ] k = k + 1 return grid
13049	def check_service ( service ) : service . add_tag ( 'header_scan' ) http = False try : result = requests . head ( 'http://{}:{}' . format ( service . address , service . port ) , timeout = 1 ) print_success ( "Found http service on {}:{}" . format ( service . address , service . port ) ) service . add_tag ( 'http' ) http = True try : service . banner = result . headers [ 'Server' ] except KeyError : pass except ( ConnectionError , ConnectTimeout , ReadTimeout , Error ) : pass if not http : try : result = requests . head ( 'https://{}:{}' . format ( service . address , service . port ) , verify = False , timeout = 3 ) service . add_tag ( 'https' ) print_success ( "Found https service on {}:{}" . format ( service . address , service . port ) ) try : service . banner = result . headers [ 'Server' ] except KeyError : pass except ( ConnectionError , ConnectTimeout , ReadTimeout , Error ) : pass service . save ( )
9637	def format ( self , record ) : data = record . _raw . copy ( ) data [ 'time' ] = data [ 'time' ] . isoformat ( ) if data . get ( 'traceback' ) : data [ 'traceback' ] = self . formatException ( data [ 'traceback' ] ) return json . dumps ( data )
9152	def _convert_vpathlist ( input_obj ) : vpl = pgmagick . VPathList ( ) for obj in input_obj : obj = pgmagick . PathMovetoAbs ( pgmagick . Coordinate ( obj [ 0 ] , obj [ 1 ] ) ) vpl . append ( obj ) return vpl
5471	def lookup_job_tasks ( provider , statuses , user_ids = None , job_ids = None , job_names = None , task_ids = None , task_attempts = None , labels = None , create_time_min = None , create_time_max = None , max_tasks = 0 , page_size = 0 , summary_output = False ) : tasks_generator = provider . lookup_job_tasks ( statuses , user_ids = user_ids , job_ids = job_ids , job_names = job_names , task_ids = task_ids , task_attempts = task_attempts , labels = labels , create_time_min = create_time_min , create_time_max = create_time_max , max_tasks = max_tasks , page_size = page_size ) for task in tasks_generator : yield _prepare_row ( task , True , summary_output )
10645	def create ( dataset , symbol , degree ) : x_vals = dataset . data [ 'T' ] . tolist ( ) y_vals = dataset . data [ symbol ] . tolist ( ) coeffs = np . polyfit ( x_vals , y_vals , degree ) result = PolynomialModelT ( dataset . material , dataset . names_dict [ symbol ] , symbol , dataset . display_symbols_dict [ symbol ] , dataset . units_dict [ symbol ] , None , [ dataset . name ] , coeffs ) result . state_schema [ 'T' ] [ 'min' ] = float ( min ( x_vals ) ) result . state_schema [ 'T' ] [ 'max' ] = float ( max ( x_vals ) ) return result
2586	def _command_server ( self , kill_event ) : logger . debug ( "[COMMAND] Command Server Starting" ) while not kill_event . is_set ( ) : try : command_req = self . command_channel . recv_pyobj ( ) logger . debug ( "[COMMAND] Received command request: {}" . format ( command_req ) ) if command_req == "OUTSTANDING_C" : outstanding = self . pending_task_queue . qsize ( ) for manager in self . _ready_manager_queue : outstanding += len ( self . _ready_manager_queue [ manager ] [ 'tasks' ] ) reply = outstanding elif command_req == "WORKERS" : num_workers = 0 for manager in self . _ready_manager_queue : num_workers += self . _ready_manager_queue [ manager ] [ 'worker_count' ] reply = num_workers elif command_req == "MANAGERS" : reply = [ ] for manager in self . _ready_manager_queue : resp = { 'manager' : manager . decode ( 'utf-8' ) , 'block_id' : self . _ready_manager_queue [ manager ] [ 'block_id' ] , 'worker_count' : self . _ready_manager_queue [ manager ] [ 'worker_count' ] , 'tasks' : len ( self . _ready_manager_queue [ manager ] [ 'tasks' ] ) , 'active' : self . _ready_manager_queue [ manager ] [ 'active' ] } reply . append ( resp ) elif command_req . startswith ( "HOLD_WORKER" ) : cmd , s_manager = command_req . split ( ';' ) manager = s_manager . encode ( 'utf-8' ) logger . info ( "[CMD] Received HOLD_WORKER for {}" . format ( manager ) ) if manager in self . _ready_manager_queue : self . _ready_manager_queue [ manager ] [ 'active' ] = False reply = True else : reply = False elif command_req == "SHUTDOWN" : logger . info ( "[CMD] Received SHUTDOWN command" ) kill_event . set ( ) reply = True else : reply = None logger . debug ( "[COMMAND] Reply: {}" . format ( reply ) ) self . command_channel . send_pyobj ( reply ) except zmq . Again : logger . debug ( "[COMMAND] is alive" ) continue
4891	def export ( self ) : enrollment_queryset = EnterpriseCourseEnrollment . objects . select_related ( 'enterprise_customer_user' ) . filter ( enterprise_customer_user__enterprise_customer = self . enterprise_customer , enterprise_customer_user__active = True , ) . order_by ( 'course_id' ) course_details = None for enterprise_enrollment in enrollment_queryset : course_id = enterprise_enrollment . course_id if course_details is None or course_details [ 'course_id' ] != course_id : if self . course_api is None : self . course_api = CourseApiClient ( ) course_details = self . course_api . get_course_details ( course_id ) if course_details is None : LOGGER . error ( "No course run details found for enrollment [%d]: [%s]" , enterprise_enrollment . pk , course_id ) continue consent = DataSharingConsent . objects . proxied_get ( username = enterprise_enrollment . enterprise_customer_user . username , course_id = enterprise_enrollment . course_id , enterprise_customer = enterprise_enrollment . enterprise_customer_user . enterprise_customer ) if not consent . granted or enterprise_enrollment . audit_reporting_disabled : continue if course_details . get ( 'pacing' ) == 'instructor' : completed_date , grade , is_passing = self . _collect_certificate_data ( enterprise_enrollment ) else : completed_date , grade , is_passing = self . _collect_grades_data ( enterprise_enrollment , course_details ) records = self . get_learner_data_records ( enterprise_enrollment = enterprise_enrollment , completed_date = completed_date , grade = grade , is_passing = is_passing , ) if records : for record in records : yield record
6623	def _getTarball ( url , into_directory , cache_key , origin_info = None ) : try : access_common . unpackFromCache ( cache_key , into_directory ) except KeyError as e : tok = settings . getProperty ( 'github' , 'authtoken' ) headers = { } if tok is not None : headers [ 'Authorization' ] = 'token ' + str ( tok ) logger . debug ( 'GET %s' , url ) response = requests . get ( url , allow_redirects = True , stream = True , headers = headers ) response . raise_for_status ( ) logger . debug ( 'getting file: %s' , url ) logger . debug ( 'headers: %s' , response . headers ) response . raise_for_status ( ) access_common . unpackTarballStream ( stream = response , into_directory = into_directory , hash = { } , cache_key = cache_key , origin_info = origin_info )
3726	def Pc ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ SURF ] ) : r def list_methods ( ) : methods = [ ] if CASRN in _crit_IUPAC . index and not np . isnan ( _crit_IUPAC . at [ CASRN , 'Pc' ] ) : methods . append ( IUPAC ) if CASRN in _crit_Matthews . index and not np . isnan ( _crit_Matthews . at [ CASRN , 'Pc' ] ) : methods . append ( MATTHEWS ) if CASRN in _crit_CRC . index and not np . isnan ( _crit_CRC . at [ CASRN , 'Pc' ] ) : methods . append ( CRC ) if CASRN in _crit_PSRKR4 . index and not np . isnan ( _crit_PSRKR4 . at [ CASRN , 'Pc' ] ) : methods . append ( PSRK ) if CASRN in _crit_PassutDanner . index and not np . isnan ( _crit_PassutDanner . at [ CASRN , 'Pc' ] ) : methods . append ( PD ) if CASRN in _crit_Yaws . index and not np . isnan ( _crit_Yaws . at [ CASRN , 'Pc' ] ) : methods . append ( YAWS ) if CASRN : methods . append ( SURF ) if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IUPAC : _Pc = float ( _crit_IUPAC . at [ CASRN , 'Pc' ] ) elif Method == MATTHEWS : _Pc = float ( _crit_Matthews . at [ CASRN , 'Pc' ] ) elif Method == CRC : _Pc = float ( _crit_CRC . at [ CASRN , 'Pc' ] ) elif Method == PSRK : _Pc = float ( _crit_PSRKR4 . at [ CASRN , 'Pc' ] ) elif Method == PD : _Pc = float ( _crit_PassutDanner . at [ CASRN , 'Pc' ] ) elif Method == YAWS : _Pc = float ( _crit_Yaws . at [ CASRN , 'Pc' ] ) elif Method == SURF : _Pc = third_property ( CASRN = CASRN , P = True ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' ) return _Pc
1467	def process ( self , tup ) : curtime = int ( time . time ( ) ) self . current_tuples . append ( ( tup , curtime ) ) self . _expire ( curtime )
8128	def search_news ( q , start = 1 , count = 10 , wait = 10 , asynchronous = False , cached = False ) : service = YAHOO_NEWS return YahooSearch ( q , start , count , service , None , wait , asynchronous , cached )
4542	def apply ( self , function ) : for cut in self . cuts : value = self . read ( cut ) function ( value ) self . write ( cut , value )
12568	def create_dataset ( self , ds_name , data , attrs = None , dtype = None ) : if ds_name in self . _datasets : ds = self . _datasets [ ds_name ] if ds . dtype != data . dtype : warnings . warn ( 'Dataset and data dtype are different!' ) else : if dtype is None : dtype = data . dtype ds = self . _group . create_dataset ( ds_name , data . shape , dtype = dtype ) if attrs is not None : for key in attrs : setattr ( ds . attrs , key , attrs [ key ] ) ds . read_direct ( data ) self . _datasets [ ds_name ] = ds return ds
12615	def count ( self , table_name , sample ) : return len ( list ( search_sample ( table = self . table ( table_name ) , sample = sample ) ) )
2421	def checksum_from_sha1 ( value ) : CHECKSUM_RE = re . compile ( 'SHA1:\s*([\S]+)' , re . UNICODE ) match = CHECKSUM_RE . match ( value ) if match : return checksum . Algorithm ( identifier = 'SHA1' , value = match . group ( 1 ) ) else : return None
9069	def _df ( self ) : if not self . _restricted : return self . nsamples return self . nsamples - self . _X [ "tX" ] . shape [ 1 ]
5867	def _inactivate_organization ( organization ) : [ _inactivate_organization_course_relationship ( record ) for record in internal . OrganizationCourse . objects . filter ( organization_id = organization . id , active = True ) ] [ _inactivate_record ( record ) for record in internal . Organization . objects . filter ( id = organization . id , active = True ) ]
2369	def keywords ( self ) : for table in self . tables : if isinstance ( table , KeywordTable ) : for keyword in table . keywords : yield keyword
753	def _translateMetricsToJSON ( self , metrics , label ) : metricsDict = metrics def _mapNumpyValues ( obj ) : import numpy if isinstance ( obj , numpy . float32 ) : return float ( obj ) elif isinstance ( obj , numpy . bool_ ) : return bool ( obj ) elif isinstance ( obj , numpy . ndarray ) : return obj . tolist ( ) else : raise TypeError ( "UNEXPECTED OBJ: %s; class=%s" % ( obj , obj . __class__ ) ) jsonString = json . dumps ( metricsDict , indent = 4 , default = _mapNumpyValues ) return jsonString
8429	def cmap_pal ( name = None , lut = None ) : colormap = get_cmap ( name , lut ) def _cmap_pal ( vals ) : return ratios_to_colors ( vals , colormap ) return _cmap_pal
11077	def send_message ( self , channel , text ) : if isinstance ( channel , SlackIM ) or isinstance ( channel , SlackUser ) : self . _bot . send_im ( channel , text ) elif isinstance ( channel , SlackRoom ) : self . _bot . send_message ( channel , text ) elif isinstance ( channel , basestring ) : if channel [ 0 ] == '@' : self . _bot . send_im ( channel [ 1 : ] , text ) elif channel [ 0 ] == '#' : self . _bot . send_message ( channel [ 1 : ] , text ) else : self . _bot . send_message ( channel , text ) else : self . _bot . send_message ( channel , text )
10333	def group_nodes_by_annotation_filtered ( graph : BELGraph , node_predicates : NodePredicates = None , annotation : str = 'Subgraph' , ) -> Mapping [ str , Set [ BaseEntity ] ] : node_filter = concatenate_node_predicates ( node_predicates ) return { key : { node for node in nodes if node_filter ( graph , node ) } for key , nodes in group_nodes_by_annotation ( graph , annotation ) . items ( ) }
8360	def draw ( self , widget , cr ) : if self . bot_size is None : self . draw_default_image ( cr ) return cr = driver . ensure_pycairo_context ( cr ) surface = self . backing_store . surface cr . set_source_surface ( surface ) cr . paint ( )
8410	def best_units ( self , sequence ) : ts_range = self . value ( max ( sequence ) ) - self . value ( min ( sequence ) ) package = self . determine_package ( sequence [ 0 ] ) if package == 'pandas' : cuts = [ ( 0.9 , 'us' ) , ( 0.9 , 'ms' ) , ( 0.9 , 's' ) , ( 9 , 'm' ) , ( 6 , 'h' ) , ( 4 , 'd' ) , ( 4 , 'w' ) , ( 4 , 'M' ) , ( 3 , 'y' ) ] denomination = NANOSECONDS base_units = 'ns' else : cuts = [ ( 0.9 , 's' ) , ( 9 , 'm' ) , ( 6 , 'h' ) , ( 4 , 'd' ) , ( 4 , 'w' ) , ( 4 , 'M' ) , ( 3 , 'y' ) ] denomination = SECONDS base_units = 'ms' for size , units in reversed ( cuts ) : if ts_range >= size * denomination [ units ] : return units return base_units
12885	def python_value ( self , value ) : if self . field_type == 'TEXT' and isinstance ( value , str ) : return self . loads ( value ) return value
7760	def _call_timeout_handlers ( self ) : sources_handled = 0 now = time . time ( ) schedule = None while self . _timeout_handlers : schedule , handler = self . _timeout_handlers [ 0 ] if schedule <= now : logger . debug ( "About to call a timeout handler: {0!r}" . format ( handler ) ) self . _timeout_handlers = self . _timeout_handlers [ 1 : ] result = handler ( ) logger . debug ( " handler result: {0!r}" . format ( result ) ) rec = handler . _pyxmpp_recurring if rec : logger . debug ( " recurring, restarting in {0} s" . format ( handler . _pyxmpp_timeout ) ) self . _timeout_handlers . append ( ( now + handler . _pyxmpp_timeout , handler ) ) self . _timeout_handlers . sort ( key = lambda x : x [ 0 ] ) elif rec is None and result is not None : logger . debug ( " auto-recurring, restarting in {0} s" . format ( result ) ) self . _timeout_handlers . append ( ( now + result , handler ) ) self . _timeout_handlers . sort ( key = lambda x : x [ 0 ] ) sources_handled += 1 else : break if self . check_events ( ) : return 0 , sources_handled if self . _timeout_handlers and schedule : timeout = schedule - now else : timeout = None return timeout , sources_handled
9382	def aggregate_count_over_time ( self , metric_store , line_data , transaction_list , aggregate_timestamp ) : for transaction in transaction_list : if line_data . get ( 's' ) == 'true' : all_qps = metric_store [ 'qps' ] else : all_qps = metric_store [ 'eqps' ] qps = all_qps [ transaction ] if aggregate_timestamp in qps : qps [ aggregate_timestamp ] += 1 else : qps [ aggregate_timestamp ] = 1 return None
5961	def set_correlparameters ( self , ** kwargs ) : self . ncorrel = kwargs . pop ( 'ncorrel' , self . ncorrel ) or 25000 nstep = kwargs . pop ( 'nstep' , None ) if nstep is None : nstep = len ( self . array [ 0 ] ) / float ( self . ncorrel ) nstep = int ( numpy . ceil ( nstep ) ) kwargs [ 'nstep' ] = nstep self . __correlkwargs . update ( kwargs ) return self . __correlkwargs
927	def getFilename ( aggregationInfo , inputFile ) : inputFile = resource_filename ( "nupic.datafiles" , inputFile ) a = defaultdict ( lambda : 0 , aggregationInfo ) outputDir = os . path . dirname ( inputFile ) outputFile = 'agg_%s' % os . path . splitext ( os . path . basename ( inputFile ) ) [ 0 ] noAggregation = True timePeriods = 'years months weeks days ' 'hours minutes seconds milliseconds microseconds' for k in timePeriods . split ( ) : if a [ k ] > 0 : noAggregation = False outputFile += '_%s_%d' % ( k , a [ k ] ) if noAggregation : return inputFile outputFile += '.csv' outputFile = os . path . join ( outputDir , outputFile ) return outputFile
5837	def tsne ( self , data_view_id ) : analysis = self . _data_analysis ( data_view_id ) projections = analysis [ 'projections' ] tsne = Tsne ( ) for k , v in projections . items ( ) : projection = Projection ( xs = v [ 'x' ] , ys = v [ 'y' ] , responses = v [ 'label' ] , tags = v [ 'inputs' ] , uids = v [ 'uid' ] ) tsne . add_projection ( k , projection ) return tsne
7342	def set_debug ( ) : logging . basicConfig ( level = logging . WARNING ) peony . logger . setLevel ( logging . DEBUG )
13578	def update ( course = False ) : if course : with Spinner . context ( msg = "Updated course metadata." , waitmsg = "Updating course metadata." ) : for course in api . get_courses ( ) : old = None try : old = Course . get ( Course . tid == course [ "id" ] ) except peewee . DoesNotExist : old = None if old : old . details_url = course [ "details_url" ] old . save ( ) continue Course . create ( tid = course [ "id" ] , name = course [ "name" ] , details_url = course [ "details_url" ] ) else : selected = Course . get_selected ( ) print ( "Updating exercise data." ) for exercise in api . get_exercises ( selected ) : old = None try : old = Exercise . byid ( exercise [ "id" ] ) except peewee . DoesNotExist : old = None if old is not None : old . name = exercise [ "name" ] old . course = selected . id old . is_attempted = exercise [ "attempted" ] old . is_completed = exercise [ "completed" ] old . deadline = exercise . get ( "deadline" ) old . is_downloaded = os . path . isdir ( old . path ( ) ) old . return_url = exercise [ "return_url" ] old . zip_url = exercise [ "zip_url" ] old . submissions_url = exercise [ "exercise_submissions_url" ] old . save ( ) download_exercise ( old , update = True ) else : ex = Exercise . create ( tid = exercise [ "id" ] , name = exercise [ "name" ] , course = selected . id , is_attempted = exercise [ "attempted" ] , is_completed = exercise [ "completed" ] , deadline = exercise . get ( "deadline" ) , return_url = exercise [ "return_url" ] , zip_url = exercise [ "zip_url" ] , submissions_url = exercise [ ( "exercise_" "submissions_" "url" ) ] ) ex . is_downloaded = os . path . isdir ( ex . path ( ) ) ex . save ( )
6926	def cursor ( self , handle , dictcursor = False ) : if handle in self . cursors : return self . cursors [ handle ] else : if dictcursor : self . cursors [ handle ] = self . connection . cursor ( cursor_factory = psycopg2 . extras . DictCursor ) else : self . cursors [ handle ] = self . connection . cursor ( ) return self . cursors [ handle ]
2348	def seed_url ( self ) : url = self . base_url if self . URL_TEMPLATE is not None : url = urlparse . urljoin ( self . base_url , self . URL_TEMPLATE . format ( ** self . url_kwargs ) ) if not url : return None url_parts = list ( urlparse . urlparse ( url ) ) query = urlparse . parse_qsl ( url_parts [ 4 ] ) for k , v in self . url_kwargs . items ( ) : if v is None : continue if "{{{}}}" . format ( k ) not in str ( self . URL_TEMPLATE ) : for i in iterable ( v ) : query . append ( ( k , i ) ) url_parts [ 4 ] = urlencode ( query ) return urlparse . urlunparse ( url_parts )
12859	def to_date ( self ) : y , m , d = self . to_ymd ( ) return date ( y , m , d )
12173	def htmlABF ( ID , group , d , folder , overwrite = False ) : fname = folder + "/swhlab4/%s_index.html" % ID if overwrite is False and os . path . exists ( fname ) : return html = TEMPLATES [ 'abf' ] html = html . replace ( "~ID~" , ID ) html = html . replace ( "~CONTENT~" , htmlABFcontent ( ID , group , d ) ) print ( " <- writing [%s]" % os . path . basename ( fname ) ) with open ( fname , 'w' ) as f : f . write ( html ) return
9720	async def release_control ( self ) : cmd = "releasecontrol" return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
10482	def _generateChildrenR ( self , target = None ) : if target is None : target = self try : children = target . AXChildren except _a11y . Error : return if children : for child in children : yield child for c in self . _generateChildrenR ( child ) : yield c
6411	def heronian_mean ( nums ) : r mag = len ( nums ) rolling_sum = 0 for i in range ( mag ) : for j in range ( i , mag ) : if nums [ i ] == nums [ j ] : rolling_sum += nums [ i ] else : rolling_sum += ( nums [ i ] * nums [ j ] ) ** 0.5 return rolling_sum * 2 / ( mag * ( mag + 1 ) )
7482	def assembly_cleanup ( data ) : data . stats_dfs . s2 = data . _build_stat ( "s2" ) data . stats_files . s2 = os . path . join ( data . dirs . edits , 's2_rawedit_stats.txt' ) with io . open ( data . stats_files . s2 , 'w' , encoding = 'utf-8' ) as outfile : data . stats_dfs . s2 . fillna ( value = 0 ) . astype ( np . int ) . to_string ( outfile )
637	def read ( cls , proto ) : protoCells = proto . cells connections = cls ( len ( protoCells ) ) for cellIdx , protoCell in enumerate ( protoCells ) : protoCell = protoCells [ cellIdx ] protoSegments = protoCell . segments connections . _cells [ cellIdx ] = CellData ( ) segments = connections . _cells [ cellIdx ] . _segments for segmentIdx , protoSegment in enumerate ( protoSegments ) : segment = Segment ( cellIdx , connections . _nextFlatIdx , connections . _nextSegmentOrdinal ) segments . append ( segment ) connections . _segmentForFlatIdx . append ( segment ) connections . _nextFlatIdx += 1 connections . _nextSegmentOrdinal += 1 synapses = segment . _synapses protoSynapses = protoSegment . synapses for synapseIdx , protoSynapse in enumerate ( protoSynapses ) : presynapticCell = protoSynapse . presynapticCell synapse = Synapse ( segment , presynapticCell , protoSynapse . permanence , ordinal = connections . _nextSynapseOrdinal ) connections . _nextSynapseOrdinal += 1 synapses . add ( synapse ) connections . _synapsesForPresynapticCell [ presynapticCell ] . add ( synapse ) connections . _numSynapses += 1 return connections
3675	def charge ( self ) : r try : if not self . rdkitmol : return charge_from_formula ( self . formula ) else : return Chem . GetFormalCharge ( self . rdkitmol ) except : return charge_from_formula ( self . formula )
7118	def merge_dicts ( d1 , d2 , _path = None ) : if _path is None : _path = ( ) if isinstance ( d1 , dict ) and isinstance ( d2 , dict ) : for k , v in d2 . items ( ) : if isinstance ( v , MissingValue ) and v . name is None : v . name = '.' . join ( _path + ( k , ) ) if isinstance ( v , DeletedValue ) : d1 . pop ( k , None ) elif k not in d1 : if isinstance ( v , dict ) : d1 [ k ] = merge_dicts ( { } , v , _path + ( k , ) ) else : d1 [ k ] = v else : if isinstance ( d1 [ k ] , dict ) and isinstance ( v , dict ) : d1 [ k ] = merge_dicts ( d1 [ k ] , v , _path + ( k , ) ) elif isinstance ( d1 [ k ] , list ) and isinstance ( v , list ) : d1 [ k ] += v elif isinstance ( d1 [ k ] , MissingValue ) : d1 [ k ] = v elif d1 [ k ] is None : d1 [ k ] = v elif type ( d1 [ k ] ) == type ( v ) : d1 [ k ] = v else : raise TypeError ( 'Refusing to replace a %s with a %s' % ( type ( d1 [ k ] ) , type ( v ) ) ) else : raise TypeError ( 'Cannot merge a %s with a %s' % ( type ( d1 ) , type ( d2 ) ) ) return d1
13388	def ttl ( self , response ) : if response . code != 200 : return 0 if not self . request . method in [ 'GET' , 'HEAD' , 'OPTIONS' ] : return 0 try : pragma = self . request . headers [ 'pragma' ] if pragma == 'no-cache' : return 0 except KeyError : pass try : cache_control = self . request . headers [ 'cache-control' ] for option in [ 'private' , 'no-cache' , 'no-store' , 'must-revalidate' , 'proxy-revalidate' ] : if cache_control . find ( option ) : return 0 options = parse_cache_control ( cache_control ) try : return int ( options [ 's-maxage' ] ) except KeyError : pass try : return int ( options [ 'max-age' ] ) except KeyError : pass if 's-maxage' in options : max_age = options [ 's-maxage' ] if max_age < ttl : ttl = max_age if 'max-age' in options : max_age = options [ 'max-age' ] if max_age < ttl : ttl = max_age return ttl except KeyError : pass try : expires = self . request . headers [ 'expires' ] return time . mktime ( time . strptime ( expires , '%a, %d %b %Y %H:%M:%S' ) ) - time . time ( ) except KeyError : pass
7643	def convert ( annotation , target_namespace ) : annotation . validate ( strict = True ) if annotation . namespace == target_namespace : return annotation if target_namespace in __CONVERSION__ : annotation = deepcopy ( annotation ) for source in __CONVERSION__ [ target_namespace ] : if annotation . search ( namespace = source ) : return __CONVERSION__ [ target_namespace ] [ source ] ( annotation ) raise NamespaceError ( 'Unable to convert annotation from namespace=' '"{0}" to "{1}"' . format ( annotation . namespace , target_namespace ) )
9884	def _read_all_z_variable_info ( self ) : self . z_variable_info = { } self . z_variable_names_by_num = { } info = fortran_cdf . z_var_all_inquire ( self . fname , self . _num_z_vars , len ( self . fname ) ) status = info [ 0 ] data_types = info [ 1 ] num_elems = info [ 2 ] rec_varys = info [ 3 ] dim_varys = info [ 4 ] num_dims = info [ 5 ] dim_sizes = info [ 6 ] rec_nums = info [ 7 ] var_nums = info [ 8 ] var_names = info [ 9 ] if status == 0 : for i in np . arange ( len ( data_types ) ) : out = { } out [ 'data_type' ] = data_types [ i ] out [ 'num_elems' ] = num_elems [ i ] out [ 'rec_vary' ] = rec_varys [ i ] out [ 'dim_varys' ] = dim_varys [ i ] out [ 'num_dims' ] = num_dims [ i ] out [ 'dim_sizes' ] = dim_sizes [ i , : 1 ] if out [ 'dim_sizes' ] [ 0 ] == 0 : out [ 'dim_sizes' ] [ 0 ] += 1 out [ 'rec_num' ] = rec_nums [ i ] out [ 'var_num' ] = var_nums [ i ] var_name = '' . join ( var_names [ i ] . astype ( 'U' ) ) out [ 'var_name' ] = var_name . rstrip ( ) self . z_variable_info [ out [ 'var_name' ] ] = out self . z_variable_names_by_num [ out [ 'var_num' ] ] = var_name else : raise IOError ( fortran_cdf . statusreporter ( status ) )
9911	def is_expired ( self ) : expiration_time = self . created_at + datetime . timedelta ( days = 1 ) return timezone . now ( ) > expiration_time
1867	def PMOVMSKB ( cpu , op0 , op1 ) : arg0 = op0 . read ( ) arg1 = op1 . read ( ) res = 0 for i in reversed ( range ( 7 , op1 . size , 8 ) ) : res = ( res << 1 ) | ( ( arg1 >> i ) & 1 ) op0 . write ( Operators . EXTRACT ( res , 0 , op0 . size ) )
206	def offer ( self , p , e : Event ) : existing = self . events_scan . setdefault ( p , ( [ ] , [ ] , [ ] , [ ] ) if USE_VERTICAL else ( [ ] , [ ] , [ ] ) ) existing [ e . type ] . append ( e )
979	def _countOverlap ( rep1 , rep2 ) : overlap = 0 for e in rep1 : if e in rep2 : overlap += 1 return overlap
9791	def find_matching ( cls , path , patterns ) : for pattern in patterns : if pattern . match ( path ) : yield pattern
537	def run ( self ) : descriptionPyModule = helpers . loadExperimentDescriptionScriptFromDir ( self . _experimentDir ) expIface = helpers . getExperimentDescriptionInterfaceFromModule ( descriptionPyModule ) expIface . normalizeStreamSources ( ) modelDescription = expIface . getModelDescription ( ) self . _modelControl = expIface . getModelControl ( ) streamDef = self . _modelControl [ 'dataset' ] from nupic . data . stream_reader import StreamReader readTimeout = 0 self . _inputSource = StreamReader ( streamDef , isBlocking = False , maxTimeout = readTimeout ) fieldStats = self . _getFieldStats ( ) self . _model = ModelFactory . create ( modelDescription ) self . _model . setFieldStatistics ( fieldStats ) self . _model . enableLearning ( ) self . _model . enableInference ( self . _modelControl . get ( "inferenceArgs" , None ) ) self . __metricMgr = MetricsManager ( self . _modelControl . get ( 'metrics' , None ) , self . _model . getFieldInfo ( ) , self . _model . getInferenceType ( ) ) self . __loggedMetricPatterns = self . _modelControl . get ( "loggedMetrics" , [ ] ) self . _optimizedMetricLabel = self . __getOptimizedMetricLabel ( ) self . _reportMetricLabels = matchPatterns ( self . _reportKeyPatterns , self . _getMetricLabels ( ) ) self . _periodic = self . _initPeriodicActivities ( ) numIters = self . _modelControl . get ( 'iterationCount' , - 1 ) learningOffAt = None iterationCountInferOnly = self . _modelControl . get ( 'iterationCountInferOnly' , 0 ) if iterationCountInferOnly == - 1 : self . _model . disableLearning ( ) elif iterationCountInferOnly > 0 : assert numIters > iterationCountInferOnly , "when iterationCountInferOnly " "is specified, iterationCount must be greater than " "iterationCountInferOnly." learningOffAt = numIters - iterationCountInferOnly self . __runTaskMainLoop ( numIters , learningOffAt = learningOffAt ) self . _finalize ( ) return ( self . _cmpReason , None )
12611	def search_by_eid ( self , table_name , eid ) : elem = self . table ( table_name ) . get ( eid = eid ) if elem is None : raise KeyError ( 'Could not find {} with eid {}.' . format ( table_name , eid ) ) return elem
10829	def accept ( self ) : with db . session . begin_nested ( ) : self . state = MembershipState . ACTIVE db . session . merge ( self )
7930	def resolve_address ( self , hostname , callback , allow_cname = True ) : if self . settings [ "ipv6" ] : if self . settings [ "ipv4" ] : family = socket . AF_UNSPEC else : family = socket . AF_INET6 elif self . settings [ "ipv4" ] : family = socket . AF_INET else : logger . warning ( "Neither IPv6 or IPv4 allowed." ) callback ( [ ] ) return try : ret = socket . getaddrinfo ( hostname , 0 , family , socket . SOCK_STREAM , 0 ) except socket . gaierror , err : logger . warning ( "Couldn't resolve {0!r}: {1}" . format ( hostname , err ) ) callback ( [ ] ) return except IOError as err : logger . warning ( "Couldn't resolve {0!r}, unexpected error: {1}" . format ( hostname , err ) ) callback ( [ ] ) return if family == socket . AF_UNSPEC : tmp = ret if self . settings [ "prefer_ipv6" ] : ret = [ addr for addr in tmp if addr [ 0 ] == socket . AF_INET6 ] ret += [ addr for addr in tmp if addr [ 0 ] == socket . AF_INET ] else : ret = [ addr for addr in tmp if addr [ 0 ] == socket . AF_INET ] ret += [ addr for addr in tmp if addr [ 0 ] == socket . AF_INET6 ] callback ( [ ( addr [ 0 ] , addr [ 4 ] [ 0 ] ) for addr in ret ] )
6838	def distrib_id ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : kernel = ( run ( 'uname -s' ) or '' ) . strip ( ) . lower ( ) if kernel == LINUX : if is_file ( '/usr/bin/lsb_release' ) : id_ = run ( 'lsb_release --id --short' ) . strip ( ) . lower ( ) if id in [ 'arch' , 'archlinux' ] : id_ = ARCH return id_ else : if is_file ( '/etc/debian_version' ) : return DEBIAN elif is_file ( '/etc/fedora-release' ) : return FEDORA elif is_file ( '/etc/arch-release' ) : return ARCH elif is_file ( '/etc/redhat-release' ) : release = run ( 'cat /etc/redhat-release' ) if release . startswith ( 'Red Hat Enterprise Linux' ) : return REDHAT elif release . startswith ( 'CentOS' ) : return CENTOS elif release . startswith ( 'Scientific Linux' ) : return SLES elif is_file ( '/etc/gentoo-release' ) : return GENTOO elif kernel == SUNOS : return SUNOS
10913	def find_particles_in_tile ( positions , tile ) : bools = tile . contains ( positions ) return np . arange ( bools . size ) [ bools ]
8855	def setup_mnu_style ( self , editor ) : menu = QtWidgets . QMenu ( 'Styles' , self . menuEdit ) group = QtWidgets . QActionGroup ( self ) self . styles_group = group current_style = editor . syntax_highlighter . color_scheme . name group . triggered . connect ( self . on_style_changed ) for s in sorted ( PYGMENTS_STYLES ) : a = QtWidgets . QAction ( menu ) a . setText ( s ) a . setCheckable ( True ) if s == current_style : a . setChecked ( True ) group . addAction ( a ) menu . addAction ( a ) self . menuEdit . addMenu ( menu )
2712	def pretty_print ( obj , indent = False ) : if indent : return json . dumps ( obj , sort_keys = True , indent = 2 , separators = ( ',' , ': ' ) ) else : return json . dumps ( obj , sort_keys = True )
6935	def add_cmds_cpdir ( cpdir , cmdpkl , cpfileglob = 'checkplot*.pkl*' , require_cmd_magcolor = True , save_cmd_pngs = False ) : cplist = glob . glob ( os . path . join ( cpdir , cpfileglob ) ) return add_cmds_cplist ( cplist , cmdpkl , require_cmd_magcolor = require_cmd_magcolor , save_cmd_pngs = save_cmd_pngs )
13320	def get_modules ( ) : modules = set ( ) cwd = os . getcwd ( ) for d in os . listdir ( cwd ) : if d == 'module.yml' : modules . add ( Module ( cwd ) ) path = unipath ( cwd , d ) if utils . is_module ( path ) : modules . add ( Module ( cwd ) ) module_paths = get_module_paths ( ) for module_path in module_paths : for d in os . listdir ( module_path ) : path = unipath ( module_path , d ) if utils . is_module ( path ) : modules . add ( Module ( path ) ) return sorted ( list ( modules ) , key = lambda x : x . name )
3178	def get ( self , list_id , merge_id ) : self . list_id = list_id self . merge_id = merge_id return self . _mc_client . _get ( url = self . _build_path ( list_id , 'merge-fields' , merge_id ) )
6586	def retries ( max_tries , exceptions = ( Exception , ) ) : def decorator ( func ) : def function ( * args , ** kwargs ) : retries_left = max_tries while retries_left > 0 : try : retries_left -= 1 return func ( * args , ** kwargs ) except exceptions as exc : if isinstance ( exc , PandoraException ) : raise if retries_left > 0 : time . sleep ( delay_exponential ( 0.5 , 2 , max_tries - retries_left ) ) else : raise return function return decorator
394	def cross_entropy_reward_loss ( logits , actions , rewards , name = None ) : cross_entropy = tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = actions , logits = logits , name = name ) return tf . reduce_sum ( tf . multiply ( cross_entropy , rewards ) )
13606	def main ( ) : args = get_arguments ( ) setup_logging ( args ) version_path = os . path . abspath ( os . path . join ( os . path . dirname ( __file__ ) , '..' , '..' , '.VERSION' ) ) try : version_text = open ( version_path ) . read ( ) . strip ( ) except Exception : print ( 'Could not open or read the .VERSION file' ) sys . exit ( 1 ) try : semver . parse ( version_text ) except ValueError : print ( ( 'The .VERSION file contains an invalid ' 'version: "{}"' ) . format ( version_text ) ) sys . exit ( 1 ) new_version = version_text if args . version : try : if semver . parse ( args . version ) : new_version = args . version except Exception : print ( 'Could not parse "{}" as a version' . format ( args . version ) ) sys . exit ( 1 ) elif args . bump_major : new_version = semver . bump_major ( version_text ) elif args . bump_minor : new_version = semver . bump_minor ( version_text ) elif args . bump_patch : new_version = semver . bump_patch ( version_text ) try : with open ( version_path , 'w' ) as version_file : version_file . write ( new_version ) except Exception : print ( 'Could not write the .VERSION file' ) sys . exit ( 1 ) print ( new_version )
10723	def xformers ( sig ) : return [ ( _wrapper ( f ) , l ) for ( f , l ) in _XFORMER . PARSER . parseString ( sig , parseAll = True ) ]
5469	def get_last_update ( op ) : last_update = get_end_time ( op ) if not last_update : last_event = get_last_event ( op ) if last_event : last_update = last_event [ 'timestamp' ] if not last_update : last_update = get_create_time ( op ) return last_update
8161	def next_event ( block = False , timeout = None ) : try : return channel . listen ( block = block , timeout = timeout ) . next ( ) [ 'data' ] except StopIteration : return None
10117	def extend ( self , items , replace = True ) : if isinstance ( items , dict ) or isinstance ( items , SortableDict ) : items = list ( items . items ( ) ) for ( key , value ) in items : self . append ( key , value , replace = replace )
11270	def safe_substitute ( prev , * args , ** kw ) : template_obj = string . Template ( * args , ** kw ) for data in prev : yield template_obj . safe_substitute ( data )
1307	def IsDesktopLocked ( ) -> bool : isLocked = False desk = ctypes . windll . user32 . OpenDesktopW ( ctypes . c_wchar_p ( 'Default' ) , 0 , 0 , 0x0100 ) if desk : isLocked = not ctypes . windll . user32 . SwitchDesktop ( desk ) ctypes . windll . user32 . CloseDesktop ( desk ) return isLocked
4190	def window_poisson_hanning ( N , alpha = 2 ) : r w1 = window_hann ( N ) w2 = window_poisson ( N , alpha = alpha ) return w1 * w2
3389	def batch ( self , batch_size , batch_num , fluxes = True ) : for i in range ( batch_num ) : yield self . sample ( batch_size , fluxes = fluxes )
6021	def from_fits_renormalized ( cls , file_path , hdu , pixel_scale ) : psf = PSF . from_fits_with_scale ( file_path , hdu , pixel_scale ) psf [ : , : ] = np . divide ( psf , np . sum ( psf ) ) return psf
7020	def generate_hatpi_binnedlc_pkl ( binnedpklf , textlcf , timebinsec , outfile = None ) : binlcdict = read_hatpi_binnedlc ( binnedpklf , textlcf , timebinsec ) if binlcdict : if outfile is None : outfile = os . path . join ( os . path . dirname ( binnedpklf ) , '%s-hplc.pkl' % ( os . path . basename ( binnedpklf ) . replace ( 'sec-lc.pkl.gz' , '' ) ) ) return lcdict_to_pickle ( binlcdict , outfile = outfile ) else : LOGERROR ( 'could not read binned HATPI LC: %s' % binnedpklf ) return None
801	def modelsGetFieldsForCheckpointed ( self , jobID , fields ) : assert len ( fields ) >= 1 , "fields is empty" with ConnectionFactory . get ( ) as conn : dbFields = [ self . _models . pubToDBNameDict [ f ] for f in fields ] dbFieldStr = ", " . join ( dbFields ) query = 'SELECT model_id, {fields} from {models}' ' WHERE job_id=%s AND model_checkpoint_id IS NOT NULL' . format ( fields = dbFieldStr , models = self . modelsTableName ) conn . cursor . execute ( query , [ jobID ] ) rows = conn . cursor . fetchall ( ) return [ ( r [ 0 ] , list ( r [ 1 : ] ) ) for r in rows ]
5138	def process_file ( self , file ) : if sys . version_info [ 0 ] >= 3 : nxt = file . __next__ else : nxt = file . next for token in tokenize . generate_tokens ( nxt ) : self . process_token ( * token ) self . make_index ( )
10969	def read_environment ( ) : out = { } for k , v in iteritems ( os . environ ) : if transform ( k ) in default_conf : out [ transform ( k ) ] = v return out
12489	def remove_nodes ( self , pattern , adict ) : mydict = self . _filetree if adict is None else adict if isinstance ( mydict , dict ) : for nom in mydict . keys ( ) : if isinstance ( mydict [ nom ] , dict ) : matchs = filter_list ( mydict [ nom ] , pattern ) for nom in matchs : mydict = self . remove_nodes ( pattern , mydict [ nom ] ) mydict . pop ( nom ) else : mydict [ nom ] = filter_list ( mydict [ nom ] , pattern ) else : matchs = set ( filter_list ( mydict , pattern ) ) mydict = set ( mydict ) - matchs return mydict
8993	def folder ( self , folder ) : result = [ ] for root , _ , files in os . walk ( folder ) : for file in files : path = os . path . join ( root , file ) if self . _chooses_path ( path ) : result . append ( self . path ( path ) ) return result
3046	def _do_refresh_request ( self , http ) : body = self . _generate_refresh_request_body ( ) headers = self . _generate_refresh_request_headers ( ) logger . info ( 'Refreshing access_token' ) resp , content = transport . request ( http , self . token_uri , method = 'POST' , body = body , headers = headers ) content = _helpers . _from_bytes ( content ) if resp . status == http_client . OK : d = json . loads ( content ) self . token_response = d self . access_token = d [ 'access_token' ] self . refresh_token = d . get ( 'refresh_token' , self . refresh_token ) if 'expires_in' in d : delta = datetime . timedelta ( seconds = int ( d [ 'expires_in' ] ) ) self . token_expiry = delta + _UTCNOW ( ) else : self . token_expiry = None if 'id_token' in d : self . id_token = _extract_id_token ( d [ 'id_token' ] ) self . id_token_jwt = d [ 'id_token' ] else : self . id_token = None self . id_token_jwt = None self . invalid = False if self . store : self . store . locked_put ( self ) else : logger . info ( 'Failed to retrieve access token: %s' , content ) error_msg = 'Invalid response {0}.' . format ( resp . status ) try : d = json . loads ( content ) if 'error' in d : error_msg = d [ 'error' ] if 'error_description' in d : error_msg += ': ' + d [ 'error_description' ] self . invalid = True if self . store is not None : self . store . locked_put ( self ) except ( TypeError , ValueError ) : pass raise HttpAccessTokenRefreshError ( error_msg , status = resp . status )
12218	def _bind_args ( sig , param_matchers , args , kwargs ) : bound = sig . bind ( * args , ** kwargs ) if not all ( param_matcher ( bound . arguments [ param_name ] ) for param_name , param_matcher in param_matchers ) : raise TypeError return bound
13555	def delete_shifts ( self , shifts ) : url = "/2/shifts/?%s" % urlencode ( { 'ids' : "," . join ( str ( s ) for s in shifts ) } ) data = self . _delete_resource ( url ) return data
268	def print_table ( table , name = None , float_format = None , formatters = None , header_rows = None ) : if isinstance ( table , pd . Series ) : table = pd . DataFrame ( table ) if name is not None : table . columns . name = name html = table . to_html ( float_format = float_format , formatters = formatters ) if header_rows is not None : n_cols = html . split ( '<thead>' ) [ 1 ] . split ( '</thead>' ) [ 0 ] . count ( '<th>' ) rows = '' for name , value in header_rows . items ( ) : rows += ( '\n <tr style="text-align: right;"><th>%s</th>' + '<td colspan=%d>%s</td></tr>' ) % ( name , n_cols , value ) html = html . replace ( '<thead>' , '<thead>' + rows ) display ( HTML ( html ) )
961	def initLogger ( obj ) : if inspect . isclass ( obj ) : myClass = obj else : myClass = obj . __class__ logger = logging . getLogger ( "." . join ( [ 'com.numenta' , myClass . __module__ , myClass . __name__ ] ) ) return logger
6849	def needs_initrole ( self , stop_on_error = False ) : ret = False target_host_present = self . is_present ( ) if not target_host_present : default_host_present = self . is_present ( self . env . default_hostname ) if default_host_present : if self . verbose : print ( 'Target host missing and default host present so host init required.' ) ret = True else : if self . verbose : print ( 'Target host missing but default host also missing, ' 'so no host init required.' ) else : if self . verbose : print ( 'Target host is present so no host init required.' ) return ret
11826	def print_boggle ( board ) : "Print the board in a 2-d array." n2 = len ( board ) n = exact_sqrt ( n2 ) for i in range ( n2 ) : if i % n == 0 and i > 0 : print if board [ i ] == 'Q' : print 'Qu' , else : print str ( board [ i ] ) + ' ' , print
2698	def write_dot ( graph , ranks , path = "graph.dot" ) : dot = Digraph ( ) for node in graph . nodes ( ) : dot . node ( node , "%s %0.3f" % ( node , ranks [ node ] ) ) for edge in graph . edges ( ) : dot . edge ( edge [ 0 ] , edge [ 1 ] , constraint = "false" ) with open ( path , 'w' ) as f : f . write ( dot . source )
980	def _overlapOK ( self , i , j , overlap = None ) : if overlap is None : overlap = self . _countOverlapIndices ( i , j ) if abs ( i - j ) < self . w : if overlap == ( self . w - abs ( i - j ) ) : return True else : return False else : if overlap <= self . _maxOverlap : return True else : return False
8605	def delete_user ( self , user_id ) : response = self . _perform_request ( url = '/um/users/%s' % user_id , method = 'DELETE' ) return response
1999	def _method ( self , expression , * args ) : assert expression . __class__ . __mro__ [ - 1 ] is object for cls in expression . __class__ . __mro__ : sort = cls . __name__ methodname = 'visit_%s' % sort method = getattr ( self , methodname , None ) if method is not None : method ( expression , * args ) return return
1194	def put ( self , item , block = True , timeout = None ) : self . not_full . acquire ( ) try : if self . maxsize > 0 : if not block : if self . _qsize ( ) == self . maxsize : raise Full elif timeout is None : while self . _qsize ( ) == self . maxsize : self . not_full . wait ( ) elif timeout < 0 : raise ValueError ( "'timeout' must be a non-negative number" ) else : endtime = _time ( ) + timeout while self . _qsize ( ) == self . maxsize : remaining = endtime - _time ( ) if remaining <= 0.0 : raise Full self . not_full . wait ( remaining ) self . _put ( item ) self . unfinished_tasks += 1 self . not_empty . notify ( ) finally : self . not_full . release ( )
11838	def result ( self , state , row ) : "Place the next queen at the given row." col = state . index ( None ) new = state [ : ] new [ col ] = row return new
11371	def convert_images ( image_list ) : png_output_contains = 'PNG image' ret_list = [ ] for image_file in image_list : if os . path . isdir ( image_file ) : continue dummy1 , cmd_out , dummy2 = run_shell_command ( 'file %s' , ( image_file , ) ) if cmd_out . find ( png_output_contains ) > - 1 : ret_list . append ( image_file ) else : converted_image_file = get_converted_image_name ( image_file ) cmd_list = [ 'convert' , image_file , converted_image_file ] dummy1 , cmd_out , cmd_err = run_shell_command ( cmd_list ) if cmd_err == '' : ret_list . append ( converted_image_file ) else : raise Exception ( cmd_err ) return ret_list
2956	def load ( self ) : try : with open ( self . _state_file ) as f : state = yaml . safe_load ( f ) self . _containers = state [ 'containers' ] except ( IOError , OSError ) as err : if err . errno == errno . ENOENT : raise NotInitializedError ( "No blockade exists in this context" ) raise InconsistentStateError ( "Failed to load Blockade state: " + str ( err ) ) except Exception as err : raise InconsistentStateError ( "Failed to load Blockade state: " + str ( err ) )
7345	async def call_on_response ( self , data ) : since_id = self . kwargs . get ( self . param , 0 ) + 1 if self . fill_gaps : if data [ - 1 ] [ 'id' ] != since_id : max_id = data [ - 1 ] [ 'id' ] - 1 responses = with_max_id ( self . request ( ** self . kwargs , max_id = max_id ) ) async for tweets in responses : data . extend ( tweets ) if data [ - 1 ] [ 'id' ] == self . last_id : data = data [ : - 1 ] if not data and not self . force : raise StopAsyncIteration await self . set_param ( data )
7295	def create_document_dictionary ( self , document , document_key = None , owner_document = None ) : doc_dict = self . create_doc_dict ( document , document_key , owner_document ) for doc_key , doc_field in doc_dict . items ( ) : if doc_key . startswith ( "_" ) : continue if isinstance ( doc_field , ListField ) : doc_dict [ doc_key ] = self . create_list_dict ( document , doc_field , doc_key ) elif isinstance ( doc_field , EmbeddedDocumentField ) : doc_dict [ doc_key ] = self . create_document_dictionary ( doc_dict [ doc_key ] . document_type_obj , doc_key ) else : doc_dict [ doc_key ] = { "_document" : document , "_key" : doc_key , "_document_field" : doc_field , "_widget" : get_widget ( doc_dict [ doc_key ] , getattr ( doc_field , 'disabled' , False ) ) } return doc_dict
6584	def station_selection_menu ( self , error = None ) : self . screen . clear ( ) if error : self . screen . print_error ( "{}\n" . format ( error ) ) for i , station in enumerate ( self . stations ) : i = "{:>3}" . format ( i ) print ( "{}: {}" . format ( Colors . yellow ( i ) , station . name ) ) return self . stations [ self . screen . get_integer ( "Station: " ) ]
6854	def getdevice_by_uuid ( uuid ) : with settings ( hide ( 'running' , 'warnings' , 'stdout' ) , warn_only = True ) : res = run_as_root ( 'blkid -U %s' % uuid ) if not res . succeeded : return None return res
9389	def check_sla ( self , sla , diff_metric ) : try : if sla . display is '%' : diff_val = float ( diff_metric [ 'percent_diff' ] ) else : diff_val = float ( diff_metric [ 'absolute_diff' ] ) except ValueError : return False if not ( sla . check_sla_passed ( diff_val ) ) : self . sla_failures += 1 self . sla_failure_list . append ( DiffSLAFailure ( sla , diff_metric ) ) return True
7693	def _check_authorization ( self , properties , stream ) : authzid = properties . get ( "authzid" ) if not authzid : return True try : jid = JID ( authzid ) except ValueError : return False if "username" not in properties : result = False elif jid . local != properties [ "username" ] : result = False elif jid . domain != stream . me . domain : result = False elif jid . resource : result = False else : result = True return result
4342	def reverse ( self ) : effect_args = [ 'reverse' ] self . effects . extend ( effect_args ) self . effects_log . append ( 'reverse' ) return self
12134	def write_log ( log_path , data , allow_append = True ) : append = os . path . isfile ( log_path ) islist = isinstance ( data , list ) if append and not allow_append : raise Exception ( 'Appending has been disabled' ' and file %s exists' % log_path ) if not ( islist or isinstance ( data , Args ) ) : raise Exception ( 'Can only write Args objects or dictionary' ' lists to log file.' ) specs = data if islist else data . specs if not all ( isinstance ( el , dict ) for el in specs ) : raise Exception ( 'List elements must be dictionaries.' ) log_file = open ( log_path , 'r+' ) if append else open ( log_path , 'w' ) start = int ( log_file . readlines ( ) [ - 1 ] . split ( ) [ 0 ] ) + 1 if append else 0 ascending_indices = range ( start , start + len ( data ) ) log_str = '\n' . join ( [ '%d %s' % ( tid , json . dumps ( el ) ) for ( tid , el ) in zip ( ascending_indices , specs ) ] ) log_file . write ( "\n" + log_str if append else log_str ) log_file . close ( )
5036	def _handle_singular ( cls , enterprise_customer , manage_learners_form ) : form_field_value = manage_learners_form . cleaned_data [ ManageLearnersForm . Fields . EMAIL_OR_USERNAME ] email = email_or_username__to__email ( form_field_value ) try : validate_email_to_link ( email , form_field_value , ValidationMessages . INVALID_EMAIL_OR_USERNAME , True ) except ValidationError as exc : manage_learners_form . add_error ( ManageLearnersForm . Fields . EMAIL_OR_USERNAME , exc ) else : EnterpriseCustomerUser . objects . link_user ( enterprise_customer , email ) return [ email ]
13657	def _forObject ( self , obj ) : router = type ( self ) ( ) router . _routes = list ( self . _routes ) router . _self = obj return router
1762	def push_bytes ( self , data , force = False ) : self . STACK -= len ( data ) self . write_bytes ( self . STACK , data , force ) return self . STACK
13689	def add_peer ( self , peer ) : if type ( peer ) == list : for i in peer : check_url ( i ) self . PEERS . extend ( peer ) elif type ( peer ) == str : check_url ( peer ) self . PEERS . append ( peer )
1789	def DIV ( cpu , src ) : size = src . size reg_name_h = { 8 : 'DL' , 16 : 'DX' , 32 : 'EDX' , 64 : 'RDX' } [ size ] reg_name_l = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ size ] dividend = Operators . CONCAT ( size * 2 , cpu . read_register ( reg_name_h ) , cpu . read_register ( reg_name_l ) ) divisor = Operators . ZEXTEND ( src . read ( ) , size * 2 ) if isinstance ( divisor , int ) and divisor == 0 : raise DivideByZeroError ( ) quotient = Operators . UDIV ( dividend , divisor ) MASK = ( 1 << size ) - 1 if isinstance ( quotient , int ) and quotient > MASK : raise DivideByZeroError ( ) remainder = Operators . UREM ( dividend , divisor ) cpu . write_register ( reg_name_l , Operators . EXTRACT ( quotient , 0 , size ) ) cpu . write_register ( reg_name_h , Operators . EXTRACT ( remainder , 0 , size ) )
10977	def members ( group_id ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) q = request . args . get ( 'q' , '' ) s = request . args . get ( 's' , '' ) group = Group . query . get_or_404 ( group_id ) if group . can_see_members ( current_user ) : members = Membership . query_by_group ( group_id , with_invitations = True ) if q : members = Membership . search ( members , q ) if s : members = Membership . order ( members , Membership . state , s ) members = members . paginate ( page , per_page = per_page ) return render_template ( "invenio_groups/members.html" , group = group , members = members , page = page , per_page = per_page , q = q , s = s , ) flash ( _ ( 'You are not allowed to see members of this group %(group_name)s.' , group_name = group . name ) , 'error' ) return redirect ( url_for ( '.index' ) )
5198	def GetApplicationIIN ( self ) : application_iin = opendnp3 . ApplicationIIN ( ) application_iin . configCorrupt = False application_iin . deviceTrouble = False application_iin . localControl = False application_iin . needTime = False iin_field = application_iin . ToIIN ( ) _log . debug ( 'OutstationApplication.GetApplicationIIN: IINField LSB={}, MSB={}' . format ( iin_field . LSB , iin_field . MSB ) ) return application_iin
6170	def filter ( self , x ) : y = signal . sosfilt ( self . sos , x ) return y
3759	def atom_fractions ( self ) : r things = dict ( ) for zi , atoms in zip ( self . zs , self . atomss ) : for atom , count in atoms . iteritems ( ) : if atom in things : things [ atom ] += zi * count else : things [ atom ] = zi * count tot = sum ( things . values ( ) ) return { atom : value / tot for atom , value in things . iteritems ( ) }
9526	def to_fastg ( infile , outfile , circular = None ) : if circular is None : to_circularise = set ( ) elif type ( circular ) is not set : f = utils . open_file_read ( circular ) to_circularise = set ( [ x . rstrip ( ) for x in f . readlines ( ) ] ) utils . close ( f ) else : to_circularise = circular seq_reader = sequences . file_reader ( infile ) fout = utils . open_file_write ( outfile ) nodes = 1 for seq in seq_reader : new_id = '_' . join ( [ 'NODE' , str ( nodes ) , 'length' , str ( len ( seq ) ) , 'cov' , '1' , 'ID' , seq . id ] ) if seq . id in to_circularise : seq . id = new_id + ':' + new_id + ';' print ( seq , file = fout ) seq . revcomp ( ) seq . id = new_id + "':" + new_id + "';" print ( seq , file = fout ) else : seq . id = new_id + ';' print ( seq , file = fout ) seq . revcomp ( ) seq . id = new_id + "';" print ( seq , file = fout ) nodes += 1 utils . close ( fout )
153	def min_item ( self ) : if self . is_empty ( ) : raise ValueError ( "Tree is empty" ) node = self . _root while node . left is not None : node = node . left return node . key , node . value
11001	def _kpad ( self , field , finalshape , zpad = False , norm = True ) : currshape = np . array ( field . shape ) if any ( finalshape < currshape ) : raise IndexError ( "PSF tile size is less than minimum support size" ) d = finalshape - currshape o = d % 2 d = np . floor_divide ( d , 2 ) if not zpad : o [ 0 ] = 0 axes = None pad = tuple ( ( d [ i ] + o [ i ] , d [ i ] ) for i in [ 0 , 1 , 2 ] ) rpsf = np . pad ( field , pad , mode = 'constant' , constant_values = 0 ) rpsf = np . fft . ifftshift ( rpsf , axes = axes ) kpsf = fft . rfftn ( rpsf , ** fftkwargs ) if norm : kpsf /= kpsf [ 0 , 0 , 0 ] return kpsf
12037	def matrixValues ( matrix , key ) : assert key in matrix . dtype . names col = matrix . dtype . names . index ( key ) values = np . empty ( len ( matrix ) ) * np . nan for i in range ( len ( matrix ) ) : values [ i ] = matrix [ i ] [ col ] return values
12691	def write_table_pair_potential ( func , dfunc = None , bounds = ( 1.0 , 10.0 ) , samples = 1000 , tollerance = 1e-6 , keyword = 'PAIR' ) : r_min , r_max = bounds if dfunc is None : dfunc = lambda r : ( func ( r + tollerance ) - func ( r - tollerance ) ) / ( 2 * tollerance ) i = np . arange ( 1 , samples + 1 ) r = np . linspace ( r_min , r_max , samples ) forces = func ( r ) energies = dfunc ( r ) lines = [ '%d %f %f %f\n' % ( index , radius , force , energy ) for index , radius , force , energy in zip ( i , r , forces , energies ) ] return "%s\nN %d\n\n" % ( keyword , samples ) + '' . join ( lines )
4948	def export ( self ) : content_metadata_export = { } content_metadata_items = self . enterprise_api . get_content_metadata ( self . enterprise_customer ) LOGGER . info ( 'Retrieved content metadata for enterprise [%s]' , self . enterprise_customer . name ) for item in content_metadata_items : transformed = self . _transform_item ( item ) LOGGER . info ( 'Exporting content metadata item with plugin configuration [%s]: [%s]' , self . enterprise_configuration , json . dumps ( transformed , indent = 4 ) , ) content_metadata_item_export = ContentMetadataItemExport ( item , transformed ) content_metadata_export [ content_metadata_item_export . content_id ] = content_metadata_item_export return OrderedDict ( sorted ( content_metadata_export . items ( ) ) )
1782	def AAS ( cpu ) : if ( cpu . AL & 0x0F > 9 ) or cpu . AF == 1 : cpu . AX = cpu . AX - 6 cpu . AH = cpu . AH - 1 cpu . AF = True cpu . CF = True else : cpu . AF = False cpu . CF = False cpu . AL = cpu . AL & 0x0f
6039	def xticks ( self ) : return np . linspace ( np . min ( self [ : , 1 ] ) , np . max ( self [ : , 1 ] ) , 4 )
7568	def comp ( seq ) : return seq . replace ( "A" , 't' ) . replace ( 'T' , 'a' ) . replace ( 'C' , 'g' ) . replace ( 'G' , 'c' ) . replace ( 'n' , 'Z' ) . upper ( ) . replace ( "Z" , "n" )
12569	def save ( self , ds_name , data , dtype = None ) : return self . create_dataset ( ds_name , data , dtype )
211	def to_uint8 ( self ) : arr_0to255 = np . clip ( np . round ( self . arr_0to1 * 255 ) , 0 , 255 ) arr_uint8 = arr_0to255 . astype ( np . uint8 ) return arr_uint8
11249	def average ( numbers , numtype = 'float' ) : if type == 'decimal' : return Decimal ( sum ( numbers ) ) / len ( numbers ) else : return float ( sum ( numbers ) ) / len ( numbers )
5914	def _process_command ( self , command , name = None ) : self . _command_counter += 1 if name is None : name = "CMD{0:03d}" . format ( self . _command_counter ) try : fd , tmp_ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = 'tmp_' + name + '__' ) cmd = [ command , '' , 'q' ] rc , out , err = self . make_ndx ( o = tmp_ndx , input = cmd ) self . check_output ( out , "No atoms found for selection {command!r}." . format ( ** vars ( ) ) , err = err ) groups = parse_ndxlist ( out ) last = groups [ - 1 ] fd , ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = name + '__' ) name_cmd = [ "keep {0:d}" . format ( last [ 'nr' ] ) , "name 0 {0!s}" . format ( name ) , 'q' ] rc , out , err = self . make_ndx ( n = tmp_ndx , o = ndx , input = name_cmd ) finally : utilities . unlink_gmx ( tmp_ndx ) return name , ndx
10891	def intersection ( tiles , * args ) : tiles = listify ( tiles ) + listify ( args ) if len ( tiles ) < 2 : return tiles [ 0 ] tile = tiles [ 0 ] l , r = tile . l . copy ( ) , tile . r . copy ( ) for tile in tiles [ 1 : ] : l = amax ( l , tile . l ) r = amin ( r , tile . r ) return Tile ( l , r , dtype = l . dtype )
3771	def mixing_logarithmic ( fracs , props ) : r if not none_and_length_check ( [ fracs , props ] ) : return None return exp ( sum ( frac * log ( prop ) for frac , prop in zip ( fracs , props ) ) )
10423	def pair_is_consistent ( graph : BELGraph , u : BaseEntity , v : BaseEntity ) -> Optional [ str ] : relations = { data [ RELATION ] for data in graph [ u ] [ v ] . values ( ) } if 1 != len ( relations ) : return return list ( relations ) [ 0 ]
10341	def main ( graph : BELGraph , xlsx : str , tsvs : str ) : if not xlsx and not tsvs : click . secho ( 'Specify at least one option --xlsx or --tsvs' , fg = 'red' ) sys . exit ( 1 ) spia_matrices = bel_to_spia_matrices ( graph ) if xlsx : spia_matrices_to_excel ( spia_matrices , xlsx ) if tsvs : spia_matrices_to_tsvs ( spia_matrices , tsvs )
7135	def filter_dict ( d , exclude ) : ret = { } for key , value in d . items ( ) : if key not in exclude : ret . update ( { key : value } ) return ret
4622	def _new_masterpassword ( self , password ) : if self . config_key in self . config and self . config [ self . config_key ] : raise Exception ( "Storage already has a masterpassword!" ) self . decrypted_master = hexlify ( os . urandom ( 32 ) ) . decode ( "ascii" ) self . password = password self . _save_encrypted_masterpassword ( ) return self . masterkey
13276	def update_desc_rsib_path ( desc , sibs_len ) : if ( desc [ 'sib_seq' ] < ( sibs_len - 1 ) ) : rsib_path = copy . deepcopy ( desc [ 'path' ] ) rsib_path [ - 1 ] = desc [ 'sib_seq' ] + 1 desc [ 'rsib_path' ] = rsib_path else : pass return ( desc )
2958	def _assure_dir ( self ) : try : os . makedirs ( self . _state_dir ) except OSError as err : if err . errno != errno . EEXIST : raise
3120	def value_to_string ( self , obj ) : value = self . _get_val_from_obj ( obj ) return self . get_prep_value ( value )
4123	def _twosided_zerolag ( data , zerolag ) : res = twosided ( np . insert ( data , 0 , zerolag ) ) return res
1967	def sched ( self ) : if len ( self . procs ) > 1 : logger . debug ( "SCHED:" ) logger . debug ( f"\tProcess: {self.procs!r}" ) logger . debug ( f"\tRunning: {self.running!r}" ) logger . debug ( f"\tRWait: {self.rwait!r}" ) logger . debug ( f"\tTWait: {self.twait!r}" ) logger . debug ( f"\tTimers: {self.timers!r}" ) logger . debug ( f"\tCurrent clock: {self.clocks}" ) logger . debug ( f"\tCurrent cpu: {self._current}" ) if len ( self . running ) == 0 : logger . debug ( "None running checking if there is some process waiting for a timeout" ) if all ( [ x is None for x in self . timers ] ) : raise Deadlock ( ) self . clocks = min ( x for x in self . timers if x is not None ) + 1 self . check_timers ( ) assert len ( self . running ) != 0 , "DEADLOCK!" self . _current = self . running [ 0 ] return next_index = ( self . running . index ( self . _current ) + 1 ) % len ( self . running ) next_running_idx = self . running [ next_index ] if len ( self . procs ) > 1 : logger . debug ( f"\tTransfer control from process {self._current} to {next_running_idx}" ) self . _current = next_running_idx
10288	def enrich_composites ( graph : BELGraph ) : nodes = list ( get_nodes_by_function ( graph , COMPOSITE ) ) for u in nodes : for v in u . members : graph . add_has_component ( u , v )
9196	def includeme ( config ) : global cache_manager settings = config . registry . settings cache_manager = CacheManager ( ** parse_cache_config_options ( settings ) )
8319	def parse_tables ( self , markup ) : tables = [ ] m = re . findall ( self . re [ "table" ] , markup ) for chunk in m : table = WikipediaTable ( ) table . properties = chunk . split ( "\n" ) [ 0 ] . strip ( "{|" ) . strip ( ) self . connect_table ( table , chunk , markup ) row = None for chunk in chunk . split ( "\n" ) : chunk = chunk . strip ( ) if chunk . startswith ( "|+" ) : title = self . plain ( chunk . strip ( "|+" ) ) table . title = title elif chunk . startswith ( "|-" ) : if row : row . properties = chunk . strip ( "|-" ) . strip ( ) table . append ( row ) row = None elif chunk . startswith ( "|}" ) : pass elif chunk . startswith ( "|" ) or chunk . startswith ( "!" ) : row = self . parse_table_row ( chunk , row ) if row : table . append ( row ) if len ( table ) > 0 : tables . append ( table ) return tables
8189	def eigenvector_centrality ( self , normalized = True , reversed = True , rating = { } , start = None , iterations = 100 , tolerance = 0.0001 ) : ec = proximity . eigenvector_centrality ( self , normalized , reversed , rating , start , iterations , tolerance ) for id , w in ec . iteritems ( ) : self [ id ] . _eigenvalue = w return ec
4945	def get_program_data_sharing_consent ( username , program_uuid , enterprise_customer_uuid ) : enterprise_customer = get_enterprise_customer ( enterprise_customer_uuid ) discovery_client = CourseCatalogApiServiceClient ( enterprise_customer . site ) course_ids = discovery_client . get_program_course_keys ( program_uuid ) child_consents = ( get_data_sharing_consent ( username , enterprise_customer_uuid , course_id = individual_course_id ) for individual_course_id in course_ids ) return ProxyDataSharingConsent . from_children ( program_uuid , * child_consents )
4556	def all_named_colors ( ) : yield from _TO_COLOR_USER . items ( ) for name , color in _TO_COLOR . items ( ) : if name not in _TO_COLOR_USER : yield name , color
2040	def RETURN ( self , offset , size ) : data = self . read_buffer ( offset , size ) raise EndTx ( 'RETURN' , data )
13017	def addHook ( self , name , callable ) : if name not in self . _hooks : self . _hooks [ name ] = [ ] self . _hooks [ name ] . append ( callable )
8138	def contrast ( self , value = 1.0 ) : c = ImageEnhance . Contrast ( self . img ) self . img = c . enhance ( value )
13245	async def _download_text ( url , session ) : logger = logging . getLogger ( __name__ ) async with session . get ( url ) as response : logger . info ( 'Downloading %r' , url ) return await response . text ( )
6714	def _install_from_scratch ( python_cmd , use_sudo ) : with cd ( "/tmp" ) : download ( EZ_SETUP_URL ) command = '%(python_cmd)s ez_setup.py' % locals ( ) if use_sudo : run_as_root ( command ) else : run ( command ) run ( 'rm -f ez_setup.py' )
464	def clear_all_placeholder_variables ( printable = True ) : tl . logging . info ( 'clear all .....................................' ) gl = globals ( ) . copy ( ) for var in gl : if var [ 0 ] == '_' : continue if 'func' in str ( globals ( ) [ var ] ) : continue if 'module' in str ( globals ( ) [ var ] ) : continue if 'class' in str ( globals ( ) [ var ] ) : continue if printable : tl . logging . info ( " clear_all ------- %s" % str ( globals ( ) [ var ] ) ) del globals ( ) [ var ]
5915	def _process_range ( self , selection , name = None ) : try : first , last , gmx_atomname = selection except ValueError : try : first , last = selection gmx_atomname = '*' except : logger . error ( "%r is not a valid range selection" , selection ) raise if name is None : name = "{first!s}-{last!s}_{gmx_atomname!s}" . format ( ** vars ( ) ) _first = self . _translate_residue ( first , default_atomname = gmx_atomname ) _last = self . _translate_residue ( last , default_atomname = gmx_atomname ) _selection = 'r {0:d} - {1:d} & & a {2!s}' . format ( _first [ 'resid' ] , _last [ 'resid' ] , gmx_atomname ) cmd = [ 'keep 0' , 'del 0' , _selection , 'name 0 {name!s}' . format ( ** vars ( ) ) , 'q' ] fd , ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = name + '__' ) rc , out , err = self . make_ndx ( n = self . ndx , o = ndx , input = cmd ) self . check_output ( out , "No atoms found for " "%(selection)r % vars ( ) ) return name , ndx
1201	def reset ( self ) : self . level . reset ( ) return self . level . observations ( ) [ self . state_attribute ]
8634	def get_bids ( session , project_ids = [ ] , bid_ids = [ ] , limit = 10 , offset = 0 ) : get_bids_data = { } if bid_ids : get_bids_data [ 'bids[]' ] = bid_ids if project_ids : get_bids_data [ 'projects[]' ] = project_ids get_bids_data [ 'limit' ] = limit get_bids_data [ 'offset' ] = offset response = make_get_request ( session , 'bids' , params_data = get_bids_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise BidsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
2789	def snapshot ( self , name ) : return self . get_data ( "volumes/%s/snapshots/" % self . id , type = POST , params = { "name" : name } )
8311	def draw_math ( str , x , y , alpha = 1.0 ) : try : from web import _ctx except : pass str = re . sub ( "</{0,1}math>" , "" , str . strip ( ) ) img = mimetex . gif ( str ) w , h = _ctx . imagesize ( img ) _ctx . image ( img , x , y , alpha = alpha ) return w , h
13633	def _negotiateHandler ( self , request ) : accept = _parseAccept ( request . requestHeaders . getRawHeaders ( 'Accept' ) ) for contentType in accept . keys ( ) : handler = self . _acceptHandlers . get ( contentType . lower ( ) ) if handler is not None : return handler , handler . contentType if self . _fallback : handler = self . _handlers [ 0 ] return handler , handler . contentType return NotAcceptable ( ) , None
10827	def create ( cls , group , user , state = MembershipState . ACTIVE ) : with db . session . begin_nested ( ) : membership = cls ( user_id = user . get_id ( ) , id_group = group . id , state = state , ) db . session . add ( membership ) return membership
9849	def _load_plt ( self , filename ) : g = gOpenMol . Plt ( ) g . read ( filename ) grid , edges = g . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
13500	def tile ( ) : figs = plt . get_fignums ( ) x = 0 y = 0 toppad = 21 size = np . array ( [ 0 , 0 ] ) if ( len ( figs ) != 0 ) : fig = plt . figure ( figs [ 0 ] ) screen = fig . canvas . window . get_screen ( ) screenx = screen . get_monitor_geometry ( screen . get_primary_monitor ( ) ) screenx = screenx [ 2 ] fig = plt . figure ( figs [ 0 ] ) fig . canvas . manager . window . move ( x , y ) maxy = np . array ( fig . canvas . manager . window . get_position ( ) ) [ 1 ] size = np . array ( fig . canvas . manager . window . get_size ( ) ) y = maxy x += size [ 0 ] + 1 for fig in figs [ 1 : ] : fig = plt . figure ( fig ) size = np . array ( fig . canvas . manager . window . get_size ( ) ) if ( x + size [ 0 ] > screenx ) : x = 0 y = maxy maxy = y + size [ 1 ] + toppad else : maxy = max ( maxy , y + size [ 1 ] + toppad ) fig . canvas . manager . window . move ( x , y ) x += size [ 0 ] + 1
13357	def moyennes_glissantes ( df , sur = 8 , rep = 0.75 ) : return pd . rolling_mean ( df , window = sur , min_periods = rep * sur )
9060	def beta ( self ) : from numpy_sugar . linalg import rsolve return rsolve ( self . _X [ "VT" ] , rsolve ( self . _X [ "tX" ] , self . mean ( ) ) )
1535	def init_topology ( mcs , classname , class_dict ) : if classname == 'Topology' : return heron_options = TopologyType . get_heron_options_from_env ( ) initial_state = heron_options . get ( "cmdline.topology.initial.state" , "RUNNING" ) tmp_directory = heron_options . get ( "cmdline.topologydefn.tmpdirectory" ) if tmp_directory is None : raise RuntimeError ( "Topology definition temp directory not specified" ) topology_name = heron_options . get ( "cmdline.topology.name" , classname ) topology_id = topology_name + str ( uuid . uuid4 ( ) ) topology = topology_pb2 . Topology ( ) topology . id = topology_id topology . name = topology_name topology . state = topology_pb2 . TopologyState . Value ( initial_state ) topology . topology_config . CopyFrom ( TopologyType . get_topology_config_protobuf ( class_dict ) ) TopologyType . add_bolts_and_spouts ( topology , class_dict ) class_dict [ 'topology_name' ] = topology_name class_dict [ 'topology_id' ] = topology_id class_dict [ 'protobuf_topology' ] = topology class_dict [ 'topologydefn_tmpdir' ] = tmp_directory class_dict [ 'heron_runtime_options' ] = heron_options
7703	def get_items_by_group ( self , group , case_sensitive = True ) : result = [ ] if not group : for item in self . _items : if not item . groups : result . append ( item ) return result if not case_sensitive : group = group . lower ( ) for item in self . _items : if group in item . groups : result . append ( item ) elif not case_sensitive and group in [ g . lower ( ) for g in item . groups ] : result . append ( item ) return result
11447	def _login ( self , session , get_request = False ) : req = session . post ( self . _login_url , data = self . _logindata ) if _LOGIN_ERROR_STRING in req . text or req . status_code == 403 or req . url == _LOGIN_URL : err_mess = "YesssSMS: login failed, username or password wrong" if _LOGIN_LOCKED_MESS in req . text : err_mess += ", page says: " + _LOGIN_LOCKED_MESS_ENG self . _suspended = True raise self . AccountSuspendedError ( err_mess ) raise self . LoginError ( err_mess ) self . _suspended = False return ( session , req ) if get_request else session
5634	def make_toc ( sections , maxdepth = 0 ) : if not sections : return [ ] outer = min ( n for n , t in sections ) refs = [ ] for ind , sec in sections : if maxdepth and ind - outer + 1 > maxdepth : continue ref = sec . lower ( ) ref = ref . replace ( '`' , '' ) ref = ref . replace ( ' ' , '-' ) ref = ref . replace ( '?' , '' ) refs . append ( " " * ( ind - outer ) + "- [%s](#%s)" % ( sec , ref ) ) return refs
4862	def save ( self ) : course_id = self . validated_data [ 'course_id' ] __ , created = models . EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = self . enterprise_customer_user , course_id = course_id , ) if created : track_enrollment ( 'rest-api-enrollment' , self . enterprise_customer_user . user_id , course_id )
2754	def get_all_sshkeys ( self ) : data = self . get_data ( "account/keys/" ) ssh_keys = list ( ) for jsoned in data [ 'ssh_keys' ] : ssh_key = SSHKey ( ** jsoned ) ssh_key . token = self . token ssh_keys . append ( ssh_key ) return ssh_keys
10849	def set_verbosity ( self , verbosity = 'vvv' , handlers = None ) : self . verbosity = sanitize ( verbosity ) self . set_level ( v2l [ verbosity ] , handlers = handlers ) self . set_formatter ( v2f [ verbosity ] , handlers = handlers )
3962	def prep_for_start_local_env ( pull_repos ) : if pull_repos : update_managed_repos ( force = True ) assembled_spec = spec_assembler . get_assembled_specs ( ) if not assembled_spec [ constants . CONFIG_BUNDLES_KEY ] : raise RuntimeError ( 'No bundles are activated. Use `dusty bundles` to activate bundles before running `dusty up`.' ) virtualbox . initialize_docker_vm ( )
560	def anyGoodSprintsActive ( self ) : if self . _state [ 'lastGoodSprint' ] is not None : goodSprints = self . _state [ 'sprints' ] [ 0 : self . _state [ 'lastGoodSprint' ] + 1 ] else : goodSprints = self . _state [ 'sprints' ] for sprint in goodSprints : if sprint [ 'status' ] == 'active' : anyActiveSprints = True break else : anyActiveSprints = False return anyActiveSprints
3937	def submit_form ( self , form_selector , input_dict ) : logger . info ( 'Submitting form on page %r' , self . _page . url . split ( '?' ) [ 0 ] ) logger . info ( 'Page contains forms: %s' , [ elem . get ( 'id' ) for elem in self . _page . soup . select ( 'form' ) ] ) try : form = self . _page . soup . select ( form_selector ) [ 0 ] except IndexError : raise GoogleAuthError ( 'Failed to find form {!r} in page' . format ( form_selector ) ) logger . info ( 'Page contains inputs: %s' , [ elem . get ( 'id' ) for elem in form . select ( 'input' ) ] ) for selector , value in input_dict . items ( ) : try : form . select ( selector ) [ 0 ] [ 'value' ] = value except IndexError : raise GoogleAuthError ( 'Failed to find input {!r} in form' . format ( selector ) ) try : self . _page = self . _browser . submit ( form , self . _page . url ) self . _page . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'Failed to submit form: {}' . format ( e ) )
9842	def use_parser ( self , parsername ) : self . __parser = self . parsers [ parsername ] self . __parser ( )
5541	def hillshade ( self , elevation , azimuth = 315.0 , altitude = 45.0 , z = 1.0 , scale = 1.0 ) : return commons_hillshade . hillshade ( elevation , self , azimuth , altitude , z , scale )
334	def compute_bayes_cone ( preds , starting_value = 1. ) : def scoreatpercentile ( cum_preds , p ) : return [ stats . scoreatpercentile ( c , p ) for c in cum_preds . T ] cum_preds = np . cumprod ( preds + 1 , 1 ) * starting_value perc = { p : scoreatpercentile ( cum_preds , p ) for p in ( 5 , 25 , 75 , 95 ) } return perc
7313	def process_request ( self , request ) : if not request : return if not db_loaded : load_db ( ) tz = request . session . get ( 'django_timezone' ) if not tz : tz = timezone . get_default_timezone ( ) client_ip = get_ip_address_from_request ( request ) ip_addrs = client_ip . split ( ',' ) for ip in ip_addrs : if is_valid_ip ( ip ) and not is_local_ip ( ip ) : if ':' in ip : tz = db_v6 . time_zone_by_addr ( ip ) break else : tz = db . time_zone_by_addr ( ip ) break if tz : timezone . activate ( tz ) request . session [ 'django_timezone' ] = str ( tz ) if getattr ( settings , 'AUTH_USER_MODEL' , None ) and getattr ( request , 'user' , None ) : detected_timezone . send ( sender = get_user_model ( ) , instance = request . user , timezone = tz ) else : timezone . deactivate ( )
9807	def teardown ( file ) : config = read_deployment_config ( file ) manager = DeployManager ( config = config , filepath = file ) exception = None try : if click . confirm ( 'Would you like to execute pre-delete hooks?' , default = True ) : manager . teardown ( hooks = True ) else : manager . teardown ( hooks = False ) except Exception as e : Printer . print_error ( 'Polyaxon could not teardown the deployment.' ) exception = e if exception : Printer . print_error ( 'Error message `{}`.' . format ( exception ) )
8249	def nearest_hue ( self , primary = False ) : if self . is_black : return "black" elif self . is_white : return "white" elif self . is_grey : return "grey" if primary : hues = primary_hues else : hues = named_hues . keys ( ) nearest , d = "" , 1.0 for hue in hues : if abs ( self . hue - named_hues [ hue ] ) % 1 < d : nearest , d = hue , abs ( self . hue - named_hues [ hue ] ) % 1 return nearest
2122	def associate_failure_node ( self , parent , child = None , ** kwargs ) : return self . _assoc_or_create ( 'failure' , parent , child , ** kwargs )
3137	def create ( self , data ) : self . app_id = None if 'client_id' not in data : raise KeyError ( 'The authorized app must have a client_id' ) if 'client_secret' not in data : raise KeyError ( 'The authorized app must have a client_secret' ) return self . _mc_client . _post ( url = self . _build_path ( ) , data = data )
12793	def get ( self , url = None , parse_data = True , key = None , parameters = None ) : return self . _fetch ( "GET" , url , post_data = None , parse_data = parse_data , key = key , parameters = parameters )
10912	def vectorize_damping ( params , damping = 1.0 , increase_list = [ [ 'psf-' , 1e4 ] ] ) : damp_vec = np . ones ( len ( params ) ) * damping for nm , fctr in increase_list : for a in range ( damp_vec . size ) : if nm in params [ a ] : damp_vec [ a ] *= fctr return damp_vec
9900	def data ( self , data ) : if self . is_caching : self . cache = data else : fcontents = self . file_contents with open ( self . path , "w" ) as f : try : indent = self . indent if self . pretty else None json . dump ( data , f , sort_keys = self . sort_keys , indent = indent ) except Exception as e : f . seek ( 0 ) f . truncate ( ) f . write ( fcontents ) raise e self . _updateType ( )
3998	def copy_from_local ( local_path , remote_name , remote_path , demote = True ) : if not os . path . exists ( local_path ) : raise RuntimeError ( 'ERROR: Path {} does not exist' . format ( local_path ) ) temp_identifier = str ( uuid . uuid1 ( ) ) if os . path . isdir ( local_path ) : sync_local_path_to_vm ( local_path , os . path . join ( vm_cp_path ( remote_name ) , temp_identifier ) , demote = demote ) move_dir_inside_container ( remote_name , os . path . join ( constants . CONTAINER_CP_DIR , temp_identifier ) , remote_path ) else : sync_local_path_to_vm ( local_path , os . path . join ( vm_cp_path ( remote_name ) , temp_identifier ) , demote = demote ) move_file_inside_container ( remote_name , os . path . join ( constants . CONTAINER_CP_DIR , temp_identifier ) , remote_path )
9865	def currency ( self ) : try : current_subscription = self . info [ "viewer" ] [ "home" ] [ "currentSubscription" ] return current_subscription [ "priceInfo" ] [ "current" ] [ "currency" ] except ( KeyError , TypeError , IndexError ) : _LOGGER . error ( "Could not find currency." ) return ""
12039	def html_temp_launch ( html ) : fname = tempfile . gettempdir ( ) + "/swhlab/temp.html" with open ( fname , 'w' ) as f : f . write ( html ) webbrowser . open ( fname )
3492	def _error_string ( error , k = None ) : package = error . getPackage ( ) if package == '' : package = 'core' template = 'E{} ({}): {} ({}, L{}); {}; {}' error_str = template . format ( k , error . getSeverityAsString ( ) , error . getCategoryAsString ( ) , package , error . getLine ( ) , error . getShortMessage ( ) , error . getMessage ( ) ) return error_str
6907	def equatorial_to_galactic ( ra , decl , equinox = 'J2000' ) : radecl = SkyCoord ( ra = ra * u . degree , dec = decl * u . degree , equinox = equinox ) gl = radecl . galactic . l . degree gb = radecl . galactic . b . degree return gl , gb
12214	def get_pref_model_class ( app , prefs , get_prefs_func ) : module = '%s.%s' % ( app , PREFS_MODULE_NAME ) model_dict = { '_prefs_app' : app , '_get_prefs' : staticmethod ( get_prefs_func ) , '__module__' : module , 'Meta' : type ( 'Meta' , ( models . options . Options , ) , { 'verbose_name' : _ ( 'Preference' ) , 'verbose_name_plural' : _ ( 'Preferences' ) , 'app_label' : app , 'managed' : False , } ) } for field_name , val_proxy in prefs . items ( ) : model_dict [ field_name ] = val_proxy . field model = type ( 'Preferences' , ( models . Model , ) , model_dict ) def fake_save_base ( self , * args , ** kwargs ) : updated_prefs = { f . name : getattr ( self , f . name ) for f in self . _meta . fields if not isinstance ( f , models . fields . AutoField ) } app_prefs = self . _get_prefs ( self . _prefs_app ) for pref in app_prefs . keys ( ) : if pref in updated_prefs : app_prefs [ pref ] . db_value = updated_prefs [ pref ] self . pk = self . _prefs_app prefs_save . send ( sender = self , app = self . _prefs_app , updated_prefs = updated_prefs ) return True model . save_base = fake_save_base return model
11022	def get_node ( self , string_key ) : pos = self . get_node_pos ( string_key ) if pos is None : return None return self . ring [ self . _sorted_keys [ pos ] ]
7257	def get_address_coords ( self , address ) : url = "https://maps.googleapis.com/maps/api/geocode/json?&address=" + address r = requests . get ( url ) r . raise_for_status ( ) results = r . json ( ) [ 'results' ] lat = results [ 0 ] [ 'geometry' ] [ 'location' ] [ 'lat' ] lng = results [ 0 ] [ 'geometry' ] [ 'location' ] [ 'lng' ] return lat , lng
11427	def record_make_all_subfields_volatile ( rec ) : for tag in rec . keys ( ) : for field_position , field in enumerate ( rec [ tag ] ) : for subfield_position , subfield in enumerate ( field [ 0 ] ) : if subfield [ 1 ] [ : 9 ] != "VOLATILE:" : record_modify_subfield ( rec , tag , subfield [ 0 ] , "VOLATILE:" + subfield [ 1 ] , subfield_position , field_position_local = field_position )
8313	def draw_table ( table , x , y , w , padding = 5 ) : try : from web import _ctx except : pass f = _ctx . fill ( ) _ctx . stroke ( f ) h = _ctx . textheight ( " " ) + padding * 2 row_y = y if table . title != "" : _ctx . fill ( f ) _ctx . rect ( x , row_y , w , h ) _ctx . fill ( 1 ) _ctx . text ( table . title , x + padding , row_y + _ctx . fontsize ( ) + padding ) row_y += h rowspans = [ 1 for i in range ( 10 ) ] previous_cell_w = 0 for row in table : cell_x = x cell_w = 1.0 * w cell_w -= previous_cell_w * len ( [ n for n in rowspans if n > 1 ] ) cell_w /= len ( row ) cell_h = 0 for cell in row : this_h = _ctx . textheight ( cell , width = cell_w - padding * 2 ) + padding * 2 cell_h = max ( cell_h , this_h ) i = 0 for cell in row : if rowspans [ i ] > 1 : rowspans [ i ] -= 1 cell_x += previous_cell_w i += 1 m = re . search ( "rowspan=\"(.*?)\"" , cell . properties ) if m : rowspan = int ( m . group ( 1 ) ) rowspans [ i ] = rowspan else : rowspan = 1 _ctx . fill ( f ) _ctx . text ( cell , cell_x + padding , row_y + _ctx . fontsize ( ) + padding , cell_w - padding * 2 ) _ctx . line ( cell_x , row_y , cell_x + cell_w , row_y ) if cell_x > x : _ctx . nofill ( ) _ctx . line ( cell_x , row_y , cell_x , row_y + cell_h ) cell_x += cell_w i += 1 row_y += cell_h previous_cell_w = cell_w _ctx . nofill ( ) _ctx . rect ( x , y , w , row_y - y )
8627	def get_users ( session , query ) : response = make_get_request ( session , 'users' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise UsersNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
11770	def name ( object ) : "Try to find some reasonable name for the object." return ( getattr ( object , 'name' , 0 ) or getattr ( object , '__name__' , 0 ) or getattr ( getattr ( object , '__class__' , 0 ) , '__name__' , 0 ) or str ( object ) )
74	def EdgeDetect ( alpha = 0 , name = None , deterministic = False , random_state = None ) : alpha_param = iap . handle_continuous_param ( alpha , "alpha" , value_range = ( 0 , 1.0 ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( _image , nb_channels , random_state_func ) : alpha_sample = alpha_param . draw_sample ( random_state = random_state_func ) ia . do_assert ( 0 <= alpha_sample <= 1.0 ) matrix_nochange = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) matrix_effect = np . array ( [ [ 0 , 1 , 0 ] , [ 1 , - 4 , 1 ] , [ 0 , 1 , 0 ] ] , dtype = np . float32 ) matrix = ( 1 - alpha_sample ) * matrix_nochange + alpha_sample * matrix_effect return [ matrix ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
12804	def get_user ( self , id = None ) : if not id : id = self . _user . id if id not in self . _users : self . _users [ id ] = self . _user if id == self . _user . id else User ( self , id ) return self . _users [ id ]
11919	def index_row ( self , dataframe ) : return dataframe . loc [ self . kwargs [ self . lookup_url_kwarg ] ] . to_frame ( ) . T
13044	def main ( ) : config = Config ( ) pipes_dir = config . get ( 'pipes' , 'directory' ) pipes_config = config . get ( 'pipes' , 'config_file' ) pipes_config_path = os . path . join ( config . config_dir , pipes_config ) if not os . path . exists ( pipes_config_path ) : print_error ( "Please configure the named pipes first" ) return workers = create_pipe_workers ( pipes_config_path , pipes_dir ) if workers : for worker in workers : worker . start ( ) try : for worker in workers : worker . join ( ) except KeyboardInterrupt : print_notification ( "Shutting down" ) for worker in workers : worker . terminate ( ) worker . join ( )
4466	def __reconstruct ( params ) : if isinstance ( params , dict ) : if '__class__' in params : cls = params [ '__class__' ] data = __reconstruct ( params [ 'params' ] ) return cls ( ** data ) else : data = dict ( ) for key , value in six . iteritems ( params ) : data [ key ] = __reconstruct ( value ) return data elif isinstance ( params , ( list , tuple ) ) : return [ __reconstruct ( v ) for v in params ] else : return params
12033	def averageSweep ( self , sweepFirst = 0 , sweepLast = None ) : if sweepLast is None : sweepLast = self . sweeps - 1 nSweeps = sweepLast - sweepFirst + 1 runningSum = np . zeros ( len ( self . sweepY ) ) self . log . debug ( "averaging sweep %d to %d" , sweepFirst , sweepLast ) for sweep in np . arange ( nSweeps ) + sweepFirst : self . setsweep ( sweep ) runningSum += self . sweepY . flatten ( ) average = runningSum / nSweeps return average
4879	def get_paginated_response ( data , request ) : url = urlparse ( request . build_absolute_uri ( ) ) . _replace ( query = None ) . geturl ( ) next_page = None previous_page = None if data [ 'next' ] : next_page = "{base_url}?{query_parameters}" . format ( base_url = url , query_parameters = urlparse ( data [ 'next' ] ) . query , ) next_page = next_page . rstrip ( '?' ) if data [ 'previous' ] : previous_page = "{base_url}?{query_parameters}" . format ( base_url = url , query_parameters = urlparse ( data [ 'previous' ] or "" ) . query , ) previous_page = previous_page . rstrip ( '?' ) return Response ( OrderedDict ( [ ( 'count' , data [ 'count' ] ) , ( 'next' , next_page ) , ( 'previous' , previous_page ) , ( 'results' , data [ 'results' ] ) ] ) )
4806	def _fmt_args_kwargs ( self , * some_args , ** some_kwargs ) : if some_args : out_args = str ( some_args ) . lstrip ( '(' ) . rstrip ( ',)' ) if some_kwargs : out_kwargs = ', ' . join ( [ str ( i ) . lstrip ( '(' ) . rstrip ( ')' ) . replace ( ', ' , ': ' ) for i in [ ( k , some_kwargs [ k ] ) for k in sorted ( some_kwargs . keys ( ) ) ] ] ) if some_args and some_kwargs : return out_args + ', ' + out_kwargs elif some_args : return out_args elif some_kwargs : return out_kwargs else : return ''
5974	def isMine ( self , scriptname ) : suffix = os . path . splitext ( scriptname ) [ 1 ] . lower ( ) if suffix . startswith ( '.' ) : suffix = suffix [ 1 : ] return self . suffix == suffix
6802	def loadable ( self , src , dst ) : from fabric import state from fabric . task_utils import crawl src_task = crawl ( src , state . commands ) assert src_task , 'Unknown source role: %s' % src dst_task = crawl ( dst , state . commands ) assert dst_task , 'Unknown destination role: %s' % src src_task ( ) env . host_string = env . hosts [ 0 ] src_size_bytes = self . get_size ( ) dst_task ( ) env . host_string = env . hosts [ 0 ] try : dst_size_bytes = self . get_size ( ) except ( ValueError , TypeError ) : dst_size_bytes = 0 free_space_bytes = self . get_free_space ( ) balance_bytes = free_space_bytes + dst_size_bytes - src_size_bytes balance_bytes_scaled , units = pretty_bytes ( balance_bytes ) viable = balance_bytes >= 0 if self . verbose : print ( 'src_db_size:' , pretty_bytes ( src_size_bytes ) ) print ( 'dst_db_size:' , pretty_bytes ( dst_size_bytes ) ) print ( 'dst_free_space:' , pretty_bytes ( free_space_bytes ) ) print if viable : print ( 'Viable! There will be %.02f %s of disk space left.' % ( balance_bytes_scaled , units ) ) else : print ( 'Not viable! We would be %.02f %s short.' % ( balance_bytes_scaled , units ) ) return viable
11241	def get_line_count ( fname ) : i = 0 with open ( fname ) as f : for i , l in enumerate ( f ) : pass return i + 1
7359	def predict_subsequences ( self , sequence_dict , peptide_lengths = None ) : if isinstance ( sequence_dict , string_types ) : sequence_dict = { "seq" : sequence_dict } elif isinstance ( sequence_dict , ( list , tuple ) ) : sequence_dict = { seq : seq for seq in sequence_dict } peptide_lengths = self . _check_peptide_lengths ( peptide_lengths ) peptide_set = set ( [ ] ) peptide_to_name_offset_pairs = defaultdict ( list ) for name , sequence in sequence_dict . items ( ) : for peptide_length in peptide_lengths : for i in range ( len ( sequence ) - peptide_length + 1 ) : peptide = sequence [ i : i + peptide_length ] peptide_set . add ( peptide ) peptide_to_name_offset_pairs [ peptide ] . append ( ( name , i ) ) peptide_list = sorted ( peptide_set ) binding_predictions = self . predict_peptides ( peptide_list ) results = [ ] for binding_prediction in binding_predictions : for name , offset in peptide_to_name_offset_pairs [ binding_prediction . peptide ] : results . append ( binding_prediction . clone_with_updates ( source_sequence_name = name , offset = offset ) ) self . _check_results ( results , peptides = peptide_set , alleles = self . alleles ) return BindingPredictionCollection ( results )
2189	def _rectify_products ( self , product = None ) : products = self . product if product is None else product if products is None : return None if not isinstance ( products , ( list , tuple ) ) : products = [ products ] return products
7361	async def _connect ( self ) : logger . debug ( "connecting to the stream" ) await self . client . setup if self . session is None : self . session = self . client . _session kwargs = await self . client . headers . prepare_request ( ** self . kwargs ) request = self . client . error_handler ( self . session . request ) return await request ( timeout = 0 , ** kwargs )
10916	def calc_particle_group_region_size ( s , region_size = 40 , max_mem = 1e9 , ** kwargs ) : region_size = np . array ( region_size ) . astype ( 'int' ) def calc_mem_usage ( region_size ) : rs = np . array ( region_size ) particle_groups = separate_particles_into_groups ( s , region_size = rs . tolist ( ) , ** kwargs ) numpart = [ np . size ( g ) for g in particle_groups ] biggroups = [ particle_groups [ i ] for i in np . argsort ( numpart ) [ - 5 : ] ] def get_tile_jsize ( group ) : nms = s . param_particle ( group ) tile = s . get_update_io_tiles ( nms , s . get_values ( nms ) ) [ 2 ] return tile . shape . prod ( ) * len ( nms ) mems = [ 8 * get_tile_jsize ( g ) for g in biggroups ] return np . max ( mems ) im_shape = s . oshape . shape if calc_mem_usage ( region_size ) > max_mem : while ( ( calc_mem_usage ( region_size ) > max_mem ) and np . any ( region_size > 2 ) ) : region_size = np . clip ( region_size - 1 , 2 , im_shape ) else : while ( ( calc_mem_usage ( region_size ) < max_mem ) and np . any ( region_size < im_shape ) ) : region_size = np . clip ( region_size + 1 , 2 , im_shape ) region_size -= 1 return region_size
6218	def get_bbox ( self , primitive ) : accessor = primitive . attributes . get ( 'POSITION' ) return accessor . min , accessor . max
4886	def update_throttle_scope ( self ) : self . scope = SERVICE_USER_SCOPE self . rate = self . get_rate ( ) self . num_requests , self . duration = self . parse_rate ( self . rate )
12823	def fspaths ( draw , allow_pathlike = None ) : has_pathlike = hasattr ( os , 'PathLike' ) if allow_pathlike is None : allow_pathlike = has_pathlike if allow_pathlike and not has_pathlike : raise InvalidArgument ( 'allow_pathlike: os.PathLike not supported, use None instead ' 'to enable it only when available' ) result_type = draw ( sampled_from ( [ bytes , text_type ] ) ) def tp ( s = '' ) : return _str_to_path ( s , result_type ) special_component = sampled_from ( [ tp ( os . curdir ) , tp ( os . pardir ) ] ) normal_component = _filename ( result_type ) path_component = one_of ( normal_component , special_component ) extension = normal_component . map ( lambda f : tp ( os . extsep ) + f ) root = _path_root ( result_type ) def optional ( st ) : return one_of ( st , just ( result_type ( ) ) ) sep = sampled_from ( [ os . sep , os . altsep or os . sep ] ) . map ( tp ) path_part = builds ( lambda s , l : s . join ( l ) , sep , lists ( path_component ) ) main_strategy = builds ( lambda * x : tp ( ) . join ( x ) , optional ( root ) , path_part , optional ( extension ) ) if allow_pathlike and hasattr ( os , 'fspath' ) : pathlike_strategy = main_strategy . map ( lambda p : _PathLike ( p ) ) main_strategy = one_of ( main_strategy , pathlike_strategy ) return draw ( main_strategy )
3699	def Tliquidus ( Tms = None , ws = None , xs = None , CASRNs = None , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if none_and_length_check ( [ Tms ] ) : methods . append ( 'Maximum' ) methods . append ( 'Simple' ) methods . append ( 'None' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'Maximum' : _Tliq = max ( Tms ) elif Method == 'Simple' : _Tliq = mixing_simple ( xs , Tms ) elif Method == 'None' : return None else : raise Exception ( 'Failure in in function' ) return _Tliq
448	def _bias_add ( x , b , data_format ) : if data_format == 'NHWC' : return tf . add ( x , b ) elif data_format == 'NCHW' : return tf . add ( x , _to_channel_first_bias ( b ) ) else : raise ValueError ( 'invalid data_format: %s' % data_format )
9625	def register ( self , cls ) : preview = cls ( site = self ) logger . debug ( 'Registering %r with %r' , preview , self ) index = self . __previews . setdefault ( preview . module , { } ) index [ cls . __name__ ] = preview
4660	def finalizeOp ( self , ops , account , permission , ** kwargs ) : if "append_to" in kwargs and kwargs [ "append_to" ] : if self . proposer : log . warning ( "You may not use append_to and self.proposer at " "the same time. Append new_proposal(..) instead" ) append_to = kwargs [ "append_to" ] parent = append_to . get_parent ( ) assert isinstance ( append_to , ( self . transactionbuilder_class , self . proposalbuilder_class ) ) append_to . appendOps ( ops ) if isinstance ( append_to , self . proposalbuilder_class ) : parent . appendSigner ( append_to . proposer , permission ) else : parent . appendSigner ( account , permission ) return append_to . get_parent ( ) elif self . proposer : proposal = self . proposal ( ) proposal . set_proposer ( self . proposer ) proposal . set_expiration ( self . proposal_expiration ) proposal . set_review ( self . proposal_review ) proposal . appendOps ( ops ) else : self . txbuffer . appendOps ( ops ) if "fee_asset" in kwargs and kwargs [ "fee_asset" ] : self . txbuffer . set_fee_asset ( kwargs [ "fee_asset" ] ) if self . unsigned : self . txbuffer . addSigningInformation ( account , permission ) return self . txbuffer elif self . bundle : self . txbuffer . appendSigner ( account , permission ) return self . txbuffer . json ( ) else : self . txbuffer . appendSigner ( account , permission ) self . txbuffer . sign ( ) return self . txbuffer . broadcast ( )
3502	def assess_products ( model , reaction , flux_coefficient_cutoff = 0.001 , solver = None ) : warn ( 'use assess_component instead' , DeprecationWarning ) return assess_component ( model , reaction , 'products' , flux_coefficient_cutoff , solver )
8459	def read_temple_config ( ) : with open ( temple . constants . TEMPLE_CONFIG_FILE ) as temple_config_file : return yaml . load ( temple_config_file , Loader = yaml . SafeLoader )
12984	def keywords ( func ) : @ wraps ( func ) def decorator ( * args , ** kwargs ) : idx = 0 if inspect . ismethod ( func ) else 1 if len ( args ) > idx : if isinstance ( args [ idx ] , ( dict , composite ) ) : for key in args [ idx ] : kwargs [ key ] = args [ idx ] [ key ] args = args [ : idx ] return func ( * args , ** kwargs ) return decorator
10929	def do_internal_run ( self , initial_count = 0 , subblock = None , update_derr = True ) : self . _inner_run_counter = initial_count good_step = True n_good_steps = 0 CLOG . debug ( 'Running...' ) _last_residuals = self . calc_residuals ( ) . copy ( ) while ( ( self . _inner_run_counter < self . run_length ) & good_step & ( not self . check_terminate ( ) ) ) : if self . check_Broyden_J ( ) and self . _inner_run_counter != 0 : self . update_Broyden_J ( ) if self . check_update_eig_J ( ) and self . _inner_run_counter != 0 : self . update_eig_J ( ) er0 = 1 * self . error delta_vals = self . find_LM_updates ( self . calc_grad ( ) , do_correct_damping = False , subblock = subblock ) er1 = self . update_function ( self . param_vals + delta_vals ) good_step = er1 < er0 if good_step : n_good_steps += 1 CLOG . debug ( '%f\t%f' % ( er0 , er1 ) ) self . update_param_vals ( delta_vals , incremental = True ) self . _last_residuals = _last_residuals . copy ( ) if update_derr : self . _last_error = er0 self . error = er1 _last_residuals = self . calc_residuals ( ) . copy ( ) else : er0_0 = self . update_function ( self . param_vals ) CLOG . debug ( 'Bad step!' ) if np . abs ( er0 - er0_0 ) > 1e-6 : raise RuntimeError ( 'Function updates are not exact.' ) self . _inner_run_counter += 1 return n_good_steps
1082	def replace ( self , hour = None , minute = None , second = None , microsecond = None , tzinfo = True ) : if hour is None : hour = self . hour if minute is None : minute = self . minute if second is None : second = self . second if microsecond is None : microsecond = self . microsecond if tzinfo is True : tzinfo = self . tzinfo return time . __new__ ( type ( self ) , hour , minute , second , microsecond , tzinfo )
12718	def angle_rates ( self ) : return [ self . ode_obj . getAngleRate ( i ) for i in range ( self . ADOF ) ]
4139	def save_thumbnail ( image_path , base_image_name , gallery_conf ) : first_image_file = image_path . format ( 1 ) thumb_dir = os . path . join ( os . path . dirname ( first_image_file ) , 'thumb' ) if not os . path . exists ( thumb_dir ) : os . makedirs ( thumb_dir ) thumb_file = os . path . join ( thumb_dir , 'sphx_glr_%s_thumb.png' % base_image_name ) if os . path . exists ( first_image_file ) : scale_image ( first_image_file , thumb_file , 400 , 280 ) elif not os . path . exists ( thumb_file ) : default_thumb_file = os . path . join ( glr_path_static ( ) , 'no_image.png' ) default_thumb_file = gallery_conf . get ( "default_thumb_file" , default_thumb_file ) scale_image ( default_thumb_file , thumb_file , 200 , 140 )
3437	def repair ( self , rebuild_index = True , rebuild_relationships = True ) : if rebuild_index : self . reactions . _generate_index ( ) self . metabolites . _generate_index ( ) self . genes . _generate_index ( ) self . groups . _generate_index ( ) if rebuild_relationships : for met in self . metabolites : met . _reaction . clear ( ) for gene in self . genes : gene . _reaction . clear ( ) for rxn in self . reactions : for met in rxn . _metabolites : met . _reaction . add ( rxn ) for gene in rxn . _genes : gene . _reaction . add ( rxn ) for l in ( self . reactions , self . genes , self . metabolites , self . groups ) : for e in l : e . _model = self
10116	def append ( self , key , value = MARKER , replace = True ) : return self . add_item ( key , value , replace = replace )
857	def _updateSequenceInfo ( self , r ) : newSequence = False sequenceId = ( r [ self . _sequenceIdIdx ] if self . _sequenceIdIdx is not None else None ) if sequenceId != self . _currSequence : if sequenceId in self . _sequences : raise Exception ( 'Broken sequence: %s, record: %s' % ( sequenceId , r ) ) self . _sequences . add ( self . _currSequence ) self . _currSequence = sequenceId if self . _resetIdx : assert r [ self . _resetIdx ] == 1 newSequence = True else : reset = False if self . _resetIdx : reset = r [ self . _resetIdx ] if reset == 1 : newSequence = True if not newSequence : if self . _timeStampIdx and self . _currTime is not None : t = r [ self . _timeStampIdx ] if t < self . _currTime : raise Exception ( 'No time travel. Early timestamp for record: %s' % r ) if self . _timeStampIdx : self . _currTime = r [ self . _timeStampIdx ]
3205	def all ( self , get_all = False , ** queryparams ) : self . batch_id = None self . operation_status = None if get_all : return self . _iterate ( url = self . _build_path ( ) , ** queryparams ) else : return self . _mc_client . _get ( url = self . _build_path ( ) , ** queryparams )
6865	def get_time_flux_errs_from_Ames_lightcurve ( infile , lctype , cadence_min = 2 ) : warnings . warn ( "Use the astrotess.read_tess_fitslc and " "astrotess.consolidate_tess_fitslc functions instead of this function. " "This function will be removed in astrobase v0.4.2." , FutureWarning ) if lctype not in ( 'PDCSAP' , 'SAP' ) : raise ValueError ( 'unknown light curve type requested: %s' % lctype ) hdulist = pyfits . open ( infile ) main_hdr = hdulist [ 0 ] . header lc_hdr = hdulist [ 1 ] . header lc = hdulist [ 1 ] . data if ( ( 'Ames' not in main_hdr [ 'ORIGIN' ] ) or ( 'LIGHTCURVE' not in lc_hdr [ 'EXTNAME' ] ) ) : raise ValueError ( 'could not understand input LC format. ' 'Is it a TESS TOI LC file?' ) time = lc [ 'TIME' ] flux = lc [ '{:s}_FLUX' . format ( lctype ) ] err_flux = lc [ '{:s}_FLUX_ERR' . format ( lctype ) ] sel = ( lc [ 'QUALITY' ] == 0 ) sel &= np . isfinite ( time ) sel &= np . isfinite ( flux ) sel &= np . isfinite ( err_flux ) sel &= ~ np . isnan ( time ) sel &= ~ np . isnan ( flux ) sel &= ~ np . isnan ( err_flux ) sel &= ( time != 0 ) sel &= ( flux != 0 ) sel &= ( err_flux != 0 ) time = time [ sel ] flux = flux [ sel ] err_flux = err_flux [ sel ] lc_cadence_diff = np . abs ( np . nanmedian ( np . diff ( time ) ) * 24 * 60 - cadence_min ) if lc_cadence_diff > 1.0e-2 : raise ValueError ( 'the light curve is not at the required cadence specified: %.2f' % cadence_min ) fluxmedian = np . nanmedian ( flux ) flux /= fluxmedian err_flux /= fluxmedian return time , flux , err_flux
4996	def default_content_filter ( sender , instance , ** kwargs ) : if kwargs [ 'created' ] and not instance . content_filter : instance . content_filter = get_default_catalog_content_filter ( ) instance . save ( )
7071	def recall ( ntp , nfn ) : if ( ntp + nfn ) > 0 : return ntp / ( ntp + nfn ) else : return np . nan
3450	def flux_variability_analysis ( model , reaction_list = None , loopless = False , fraction_of_optimum = 1.0 , pfba_factor = None , processes = None ) : if reaction_list is None : reaction_ids = [ r . id for r in model . reactions ] else : reaction_ids = [ r . id for r in model . reactions . get_by_any ( reaction_list ) ] if processes is None : processes = CONFIGURATION . processes num_reactions = len ( reaction_ids ) processes = min ( processes , num_reactions ) fva_result = DataFrame ( { "minimum" : zeros ( num_reactions , dtype = float ) , "maximum" : zeros ( num_reactions , dtype = float ) } , index = reaction_ids ) prob = model . problem with model : model . slim_optimize ( error_value = None , message = "There is no optimal solution for the " "chosen objective!" ) if model . solver . objective . direction == "max" : fva_old_objective = prob . Variable ( "fva_old_objective" , lb = fraction_of_optimum * model . solver . objective . value ) else : fva_old_objective = prob . Variable ( "fva_old_objective" , ub = fraction_of_optimum * model . solver . objective . value ) fva_old_obj_constraint = prob . Constraint ( model . solver . objective . expression - fva_old_objective , lb = 0 , ub = 0 , name = "fva_old_objective_constraint" ) model . add_cons_vars ( [ fva_old_objective , fva_old_obj_constraint ] ) if pfba_factor is not None : if pfba_factor < 1. : warn ( "The 'pfba_factor' should be larger or equal to 1." , UserWarning ) with model : add_pfba ( model , fraction_of_optimum = 0 ) ub = model . slim_optimize ( error_value = None ) flux_sum = prob . Variable ( "flux_sum" , ub = pfba_factor * ub ) flux_sum_constraint = prob . Constraint ( model . solver . objective . expression - flux_sum , lb = 0 , ub = 0 , name = "flux_sum_constraint" ) model . add_cons_vars ( [ flux_sum , flux_sum_constraint ] ) model . objective = Zero for what in ( "minimum" , "maximum" ) : if processes > 1 : chunk_size = len ( reaction_ids ) // processes pool = multiprocessing . Pool ( processes , initializer = _init_worker , initargs = ( model , loopless , what [ : 3 ] ) ) for rxn_id , value in pool . imap_unordered ( _fva_step , reaction_ids , chunksize = chunk_size ) : fva_result . at [ rxn_id , what ] = value pool . close ( ) pool . join ( ) else : _init_worker ( model , loopless , what [ : 3 ] ) for rxn_id , value in map ( _fva_step , reaction_ids ) : fva_result . at [ rxn_id , what ] = value return fva_result [ [ "minimum" , "maximum" ] ]
75	def DirectedEdgeDetect ( alpha = 0 , direction = ( 0.0 , 1.0 ) , name = None , deterministic = False , random_state = None ) : alpha_param = iap . handle_continuous_param ( alpha , "alpha" , value_range = ( 0 , 1.0 ) , tuple_to_uniform = True , list_to_choice = True ) direction_param = iap . handle_continuous_param ( direction , "direction" , value_range = None , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( _image , nb_channels , random_state_func ) : alpha_sample = alpha_param . draw_sample ( random_state = random_state_func ) ia . do_assert ( 0 <= alpha_sample <= 1.0 ) direction_sample = direction_param . draw_sample ( random_state = random_state_func ) deg = int ( direction_sample * 360 ) % 360 rad = np . deg2rad ( deg ) x = np . cos ( rad - 0.5 * np . pi ) y = np . sin ( rad - 0.5 * np . pi ) direction_vector = np . array ( [ x , y ] ) matrix_effect = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 0 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) for x in [ - 1 , 0 , 1 ] : for y in [ - 1 , 0 , 1 ] : if ( x , y ) != ( 0 , 0 ) : cell_vector = np . array ( [ x , y ] ) distance_deg = np . rad2deg ( ia . angle_between_vectors ( cell_vector , direction_vector ) ) distance = distance_deg / 180 similarity = ( 1 - distance ) ** 4 matrix_effect [ y + 1 , x + 1 ] = similarity matrix_effect = matrix_effect / np . sum ( matrix_effect ) matrix_effect = matrix_effect * ( - 1 ) matrix_effect [ 1 , 1 ] = 1 matrix_nochange = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) matrix = ( 1 - alpha_sample ) * matrix_nochange + alpha_sample * matrix_effect return [ matrix ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
10454	def stateenabled ( self , window_name , object_name ) : try : object_handle = self . _get_object_handle ( window_name , object_name ) if object_handle . AXEnabled : return 1 except LdtpServerException : pass return 0
4387	def adsAddRoute ( net_id , ip_address ) : add_route = _adsDLL . AdsAddRoute add_route . restype = ctypes . c_long ip_address_p = ctypes . c_char_p ( ip_address . encode ( "utf-8" ) ) error_code = add_route ( net_id , ip_address_p ) if error_code : raise ADSError ( error_code )
9303	def get_request_date ( cls , req ) : date = None for header in [ 'x-amz-date' , 'date' ] : if header not in req . headers : continue try : date_str = cls . parse_date ( req . headers [ header ] ) except DateFormatError : continue try : date = datetime . datetime . strptime ( date_str , '%Y-%m-%d' ) . date ( ) except ValueError : continue else : break return date
1392	def synch_topologies ( self ) : self . state_managers = statemanagerfactory . get_all_state_managers ( self . config . statemgr_config ) try : for state_manager in self . state_managers : state_manager . start ( ) except Exception as ex : Log . error ( "Found exception while initializing state managers: %s. Bailing out..." % ex ) traceback . print_exc ( ) sys . exit ( 1 ) def on_topologies_watch ( state_manager , topologies ) : Log . info ( "State watch triggered for topologies." ) Log . debug ( "Topologies: " + str ( topologies ) ) existingTopologies = self . getTopologiesForStateLocation ( state_manager . name ) existingTopNames = map ( lambda t : t . name , existingTopologies ) Log . debug ( "Existing topologies: " + str ( existingTopNames ) ) for name in existingTopNames : if name not in topologies : Log . info ( "Removing topology: %s in rootpath: %s" , name , state_manager . rootpath ) self . removeTopology ( name , state_manager . name ) for name in topologies : if name not in existingTopNames : self . addNewTopology ( state_manager , name ) for state_manager in self . state_managers : onTopologiesWatch = partial ( on_topologies_watch , state_manager ) state_manager . get_topologies ( onTopologiesWatch )
13736	def get_param_values ( request , model = None ) : if type ( request ) == dict : return request params = get_payload ( request ) try : del params [ 'pk' ] params [ params . pop ( 'name' ) ] = params . pop ( 'value' ) except KeyError : pass return { k . rstrip ( '[]' ) : safe_eval ( v ) if not type ( v ) == list else [ safe_eval ( sv ) for sv in v ] for k , v in params . items ( ) }
12901	def set_sleep ( self , value = False ) : return ( yield from self . handle_set ( self . API . get ( 'sleep' ) , int ( value ) ) )
5358	def es_version ( self , url ) : try : res = self . grimoire_con . get ( url ) res . raise_for_status ( ) major = res . json ( ) [ 'version' ] [ 'number' ] . split ( "." ) [ 0 ] except Exception : logger . error ( "Error retrieving Elasticsearch version: " + url ) raise return major
3974	def _get_compose_volumes ( app_name , assembled_specs ) : volumes = [ ] volumes . append ( _get_cp_volume_mount ( app_name ) ) volumes += get_app_volume_mounts ( app_name , assembled_specs ) return volumes
12610	def _concat_queries ( queries , operators = '__and__' ) : if not queries : raise ValueError ( 'Expected some `queries`, got {}.' . format ( queries ) ) if len ( queries ) == 1 : return queries [ 0 ] if isinstance ( operators , str ) : operators = [ operators ] * ( len ( queries ) - 1 ) if len ( queries ) - 1 != len ( operators ) : raise ValueError ( 'Expected `operators` to be a string or a list with the same' ' length as `field_names` ({}), got {}.' . format ( len ( queries ) , operators ) ) first , rest , end = queries [ 0 ] , queries [ 1 : - 1 ] , queries [ - 1 : ] [ 0 ] bigop = getattr ( first , operators [ 0 ] ) for i , q in enumerate ( rest ) : bigop = getattr ( bigop ( q ) , operators [ i ] ) return bigop ( end )
13057	def get_locale ( self ) : best_match = request . accept_languages . best_match ( [ 'de' , 'fr' , 'en' , 'la' ] ) if best_match is None : if len ( request . accept_languages ) > 0 : best_match = request . accept_languages [ 0 ] [ 0 ] [ : 2 ] else : return self . __default_lang__ lang = self . __default_lang__ if best_match == "de" : lang = "ger" elif best_match == "fr" : lang = "fre" elif best_match == "en" : lang = "eng" elif best_match == "la" : lang = "lat" return lang
3278	def get_resource_inst ( self , path , environ ) : _logger . info ( "get_resource_inst('%s')" % path ) self . _count_get_resource_inst += 1 root = RootCollection ( environ ) return root . resolve ( "" , path )
13470	def copy ( src , dst ) : ( szip , dzip ) = ( src . endswith ( ".zip" ) , dst . endswith ( ".zip" ) ) logging . info ( "Copy: %s => %s" % ( src , dst ) ) if szip and dzip : shutil . copy2 ( src , dst ) elif szip : with zipfile . ZipFile ( src , mode = 'r' ) as z : tmpdir = tempfile . mkdtemp ( ) try : z . extractall ( tmpdir ) if len ( z . namelist ( ) ) != 1 : raise RuntimeError ( "The zip file '%s' should only have one " "compressed file" % src ) tmpfile = join ( tmpdir , z . namelist ( ) [ 0 ] ) try : os . remove ( dst ) except OSError : pass shutil . move ( tmpfile , dst ) finally : shutil . rmtree ( tmpdir , ignore_errors = True ) elif dzip : with zipfile . ZipFile ( dst , mode = 'w' , compression = ZIP_DEFLATED ) as z : z . write ( src , arcname = basename ( src ) ) else : shutil . copy2 ( src , dst )
219	def get_directories ( self , directory : str = None , packages : typing . List [ str ] = None ) -> typing . List [ str ] : directories = [ ] if directory is not None : directories . append ( directory ) for package in packages or [ ] : spec = importlib . util . find_spec ( package ) assert spec is not None , f"Package {package!r} could not be found." assert ( spec . origin is not None ) , "Directory 'statics' in package {package!r} could not be found." directory = os . path . normpath ( os . path . join ( spec . origin , ".." , "statics" ) ) assert os . path . isdir ( directory ) , "Directory 'statics' in package {package!r} could not be found." directories . append ( directory ) return directories
10578	def get_element_masses ( self ) : result = [ 0 ] * len ( self . material . elements ) for compound in self . material . compounds : c = self . get_compound_mass ( compound ) f = [ c * x for x in emf ( compound , self . material . elements ) ] result = [ v + f [ ix ] for ix , v in enumerate ( result ) ] return result
12704	def make_quaternion ( theta , * axis ) : x , y , z = axis r = np . sqrt ( x * x + y * y + z * z ) st = np . sin ( theta / 2. ) ct = np . cos ( theta / 2. ) return [ x * st / r , y * st / r , z * st / r , ct ]
6141	def in_out_check ( self ) : devices = available_devices ( ) if not self . in_idx in devices : raise OSError ( "Input device is unavailable" ) in_check = devices [ self . in_idx ] if not self . out_idx in devices : raise OSError ( "Output device is unavailable" ) out_check = devices [ self . out_idx ] if ( ( in_check [ 'inputs' ] == 0 ) and ( out_check [ 'outputs' ] == 0 ) ) : raise StandardError ( 'Invalid input and output devices' ) elif ( in_check [ 'inputs' ] == 0 ) : raise ValueError ( 'Selected input device has no inputs' ) elif ( out_check [ 'outputs' ] == 0 ) : raise ValueError ( 'Selected output device has no outputs' ) return True
4742	def env_export ( prefix , exported , env ) : for exp in exported : ENV [ "_" . join ( [ prefix , exp ] ) ] = env [ exp ]
746	def anomalyAddLabel ( self , start , end , labelName ) : self . _getAnomalyClassifier ( ) . getSelf ( ) . addLabel ( start , end , labelName )
3140	def get ( self , folder_id , ** queryparams ) : self . folder_id = folder_id return self . _mc_client . _get ( url = self . _build_path ( folder_id ) , ** queryparams )
7142	def balance ( self , unlocked = False ) : return self . _backend . balances ( account = self . index ) [ 1 if unlocked else 0 ]
8481	def env ( key , default ) : value = os . environ . get ( key , None ) if value is not None : log . info ( ' %s = %r' , key . lower ( ) . replace ( '_' , '.' ) , value ) return value key = key . lower ( ) . replace ( '_' , '.' ) value = get ( key ) if value is not None : return value return default
6568	def random_xorsat ( num_variables , num_clauses , vartype = dimod . BINARY , satisfiable = True ) : if num_variables < 3 : raise ValueError ( "a xor problem needs at least 3 variables" ) if num_clauses > 8 * _nchoosek ( num_variables , 3 ) : raise ValueError ( "too many clauses" ) csp = ConstraintSatisfactionProblem ( vartype ) variables = list ( range ( num_variables ) ) constraints = set ( ) if satisfiable : values = tuple ( vartype . value ) planted_solution = { v : choice ( values ) for v in variables } configurations = [ ( 0 , 0 , 0 ) , ( 0 , 1 , 1 ) , ( 1 , 0 , 1 ) , ( 1 , 1 , 0 ) ] while len ( constraints ) < num_clauses : x , y , z = sample ( variables , 3 ) if y > x : x , y = y , x const = xor_gate ( [ x , y , z ] , vartype = vartype ) config = choice ( configurations ) for idx , v in enumerate ( const . variables ) : if config [ idx ] != ( planted_solution [ v ] > 0 ) : const . flip_variable ( v ) assert const . check ( planted_solution ) constraints . add ( const ) else : while len ( constraints ) < num_clauses : x , y , z = sample ( variables , 3 ) if y > x : x , y = y , x const = xor_gate ( [ x , y , z ] , vartype = vartype ) for idx , v in enumerate ( const . variables ) : if random ( ) > .5 : const . flip_variable ( v ) assert const . check ( planted_solution ) constraints . add ( const ) for const in constraints : csp . add_constraint ( const ) for v in variables : csp . add_variable ( v ) return csp
10054	def put ( self , pid , record ) : try : ids = [ data [ 'id' ] for data in json . loads ( request . data . decode ( 'utf-8' ) ) ] except KeyError : raise WrongFile ( ) record . files . sort_by ( * ids ) record . commit ( ) db . session . commit ( ) return self . make_response ( obj = record . files , pid = pid , record = record )
2186	def load ( self , cfgstr = None ) : from six . moves import cPickle as pickle cfgstr = self . _rectify_cfgstr ( cfgstr ) dpath = self . dpath fname = self . fname verbose = self . verbose if not self . enabled : if verbose > 1 : self . log ( '[cacher] ... cache disabled: fname={}' . format ( self . fname ) ) raise IOError ( 3 , 'Cache Loading Is Disabled' ) fpath = self . get_fpath ( cfgstr = cfgstr ) if not exists ( fpath ) : if verbose > 2 : self . log ( '[cacher] ... cache does not exist: ' 'dpath={} fname={} cfgstr={}' . format ( basename ( dpath ) , fname , cfgstr ) ) raise IOError ( 2 , 'No such file or directory: %r' % ( fpath , ) ) else : if verbose > 3 : self . log ( '[cacher] ... cache exists: ' 'dpath={} fname={} cfgstr={}' . format ( basename ( dpath ) , fname , cfgstr ) ) try : with open ( fpath , 'rb' ) as file_ : data = pickle . load ( file_ ) except Exception as ex : if verbose > 0 : self . log ( 'CORRUPTED? fpath = %s' % ( fpath , ) ) if verbose > 1 : self . log ( '[cacher] ... CORRUPTED? dpath={} cfgstr={}' . format ( basename ( dpath ) , cfgstr ) ) if isinstance ( ex , ( EOFError , IOError , ImportError ) ) : raise IOError ( str ( ex ) ) else : if verbose > 1 : self . log ( '[cacher] ... unknown reason for exception' ) raise else : if self . verbose > 2 : self . log ( '[cacher] ... {} cache hit' . format ( self . fname ) ) elif verbose > 1 : self . log ( '[cacher] ... cache hit' ) return data
13434	def _setup_index ( index ) : index = int ( index ) if index > 0 : index -= 1 elif index == 0 : raise ValueError return index
4419	async def play_previous ( self ) : if not self . previous : raise NoPreviousTrack self . queue . insert ( 0 , self . previous ) await self . play ( ignore_shuffle = True )
4913	def courses ( self , request , pk = None ) : enterprise_customer = self . get_object ( ) self . check_object_permissions ( request , enterprise_customer ) self . ensure_data_exists ( request , enterprise_customer . catalog , error_message = "No catalog is associated with Enterprise {enterprise_name} from endpoint '{path}'." . format ( enterprise_name = enterprise_customer . name , path = request . get_full_path ( ) ) ) catalog_api = CourseCatalogApiClient ( request . user , enterprise_customer . site ) courses = catalog_api . get_paginated_catalog_courses ( enterprise_customer . catalog , request . GET ) self . ensure_data_exists ( request , courses , error_message = ( "Unable to fetch API response for catalog courses for " "Enterprise {enterprise_name} from endpoint '{path}'." . format ( enterprise_name = enterprise_customer . name , path = request . get_full_path ( ) ) ) ) serializer = serializers . EnterpriseCatalogCoursesReadOnlySerializer ( courses ) serializer . update_enterprise_courses ( enterprise_customer , catalog_id = enterprise_customer . catalog ) return get_paginated_response ( serializer . data , request )
10312	def prepare_c3_time_series ( data : List [ Tuple [ int , int ] ] , y_axis_label : str = 'y' , x_axis_label : str = 'x' ) -> str : years , counter = zip ( * data ) years = [ datetime . date ( year , 1 , 1 ) . isoformat ( ) for year in years ] return json . dumps ( [ [ x_axis_label ] + list ( years ) , [ y_axis_label ] + list ( counter ) ] )
7332	async def run_tasks ( self ) : tasks = self . get_tasks ( ) self . _gathered_tasks = asyncio . gather ( * tasks , loop = self . loop ) try : await self . _gathered_tasks except CancelledError : pass
13424	def get_message ( self , message_id ) : url = "/2/messages/%s" % message_id return self . message_from_json ( self . _get_resource ( url ) [ "message" ] )
3971	def _composed_app_dict ( app_name , assembled_specs , port_specs ) : logging . info ( "Compose Compiler: Compiling dict for app {}" . format ( app_name ) ) app_spec = assembled_specs [ 'apps' ] [ app_name ] compose_dict = app_spec [ "compose" ] _apply_env_overrides ( env_overrides_for_app_or_service ( app_name ) , compose_dict ) if 'image' in app_spec and 'build' in app_spec : raise RuntimeError ( "image and build are both specified in the spec for {}" . format ( app_name ) ) elif 'image' in app_spec : logging . info compose_dict [ 'image' ] = app_spec [ 'image' ] elif 'build' in app_spec : compose_dict [ 'build' ] = _get_build_path ( app_spec ) else : raise RuntimeError ( "Neither image nor build was specified in the spec for {}" . format ( app_name ) ) compose_dict [ 'entrypoint' ] = [ ] compose_dict [ 'command' ] = _compile_docker_command ( app_spec ) compose_dict [ 'container_name' ] = "dusty_{}_1" . format ( app_name ) logging . info ( "Compose Compiler: compiled command {}" . format ( compose_dict [ 'command' ] ) ) compose_dict [ 'links' ] = _links_for_app ( app_spec , assembled_specs ) logging . info ( "Compose Compiler: links {}" . format ( compose_dict [ 'links' ] ) ) compose_dict [ 'volumes' ] = compose_dict [ 'volumes' ] + _get_compose_volumes ( app_name , assembled_specs ) logging . info ( "Compose Compiler: volumes {}" . format ( compose_dict [ 'volumes' ] ) ) port_list = _get_ports_list ( app_name , port_specs ) if port_list : compose_dict [ 'ports' ] = port_list logging . info ( "Compose Compiler: ports {}" . format ( port_list ) ) compose_dict [ 'user' ] = 'root' return compose_dict
3624	def decode ( geohash ) : lat , lon , lat_err , lon_err = decode_exactly ( geohash ) lats = "%.*f" % ( max ( 1 , int ( round ( - log10 ( lat_err ) ) ) ) - 1 , lat ) lons = "%.*f" % ( max ( 1 , int ( round ( - log10 ( lon_err ) ) ) ) - 1 , lon ) if '.' in lats : lats = lats . rstrip ( '0' ) if '.' in lons : lons = lons . rstrip ( '0' ) return lats , lons
674	def _loadDummyModelParameters ( self , params ) : for key , value in params . iteritems ( ) : if type ( value ) == list : index = self . modelIndex % len ( params [ key ] ) self . _params [ key ] = params [ key ] [ index ] else : self . _params [ key ] = params [ key ]
3827	async def get_suggested_entities ( self , get_suggested_entities_request ) : response = hangouts_pb2 . GetSuggestedEntitiesResponse ( ) await self . _pb_request ( 'contacts/getsuggestedentities' , get_suggested_entities_request , response ) return response
4201	def modcovar ( x , order ) : from spectrum import corrmtx import scipy . linalg X = corrmtx ( x , order , 'modified' ) Xc = np . matrix ( X [ : , 1 : ] ) X1 = np . array ( X [ : , 0 ] ) a , residues , rank , singular_values = scipy . linalg . lstsq ( - Xc , X1 ) Cz = np . dot ( X1 . conj ( ) . transpose ( ) , Xc ) e = np . dot ( X1 . conj ( ) . transpose ( ) , X1 ) + np . dot ( Cz , a ) assert e . imag < 1e-4 , 'wierd behaviour' e = float ( e . real ) return a , e
12890	def handle_text ( self , item ) : doc = yield from self . handle_get ( item ) if doc is None : return None return doc . value . c8_array . text or None
271	def estimate_intraday ( returns , positions , transactions , EOD_hour = 23 ) : txn_val = transactions . copy ( ) txn_val . index . names = [ 'date' ] txn_val [ 'value' ] = txn_val . amount * txn_val . price txn_val = txn_val . reset_index ( ) . pivot_table ( index = 'date' , values = 'value' , columns = 'symbol' ) . replace ( np . nan , 0 ) txn_val [ 'date' ] = txn_val . index . date txn_val = txn_val . groupby ( 'date' ) . cumsum ( ) txn_val [ 'exposure' ] = txn_val . abs ( ) . sum ( axis = 1 ) condition = ( txn_val [ 'exposure' ] == txn_val . groupby ( pd . TimeGrouper ( '24H' ) ) [ 'exposure' ] . transform ( max ) ) txn_val = txn_val [ condition ] . drop ( 'exposure' , axis = 1 ) txn_val [ 'cash' ] = - txn_val . sum ( axis = 1 ) positions_shifted = positions . copy ( ) . shift ( 1 ) . fillna ( 0 ) starting_capital = positions . iloc [ 0 ] . sum ( ) / ( 1 + returns [ 0 ] ) positions_shifted . cash [ 0 ] = starting_capital txn_val . index = txn_val . index . normalize ( ) corrected_positions = positions_shifted . add ( txn_val , fill_value = 0 ) corrected_positions . index . name = 'period_close' corrected_positions . columns . name = 'sid' return corrected_positions
1375	def parse_override_config ( namespace ) : overrides = dict ( ) for config in namespace : kv = config . split ( "=" ) if len ( kv ) != 2 : raise Exception ( "Invalid config property format (%s) expected key=value" % config ) if kv [ 1 ] in [ 'true' , 'True' , 'TRUE' ] : overrides [ kv [ 0 ] ] = True elif kv [ 1 ] in [ 'false' , 'False' , 'FALSE' ] : overrides [ kv [ 0 ] ] = False else : overrides [ kv [ 0 ] ] = kv [ 1 ] return overrides
10783	def check_add_particles ( st , guess , rad = 'calc' , do_opt = True , im_change_frac = 0.2 , min_derr = '3sig' , ** kwargs ) : if min_derr == '3sig' : min_derr = 3 * st . sigma accepts = 0 new_poses = [ ] if rad == 'calc' : rad = guess_add_radii ( st ) message = ( '-' * 30 + 'ADDING' + '-' * 30 + '\n Z\t Y\t X\t R\t|\t ERR0\t\t ERR1' ) with log . noformat ( ) : CLOG . info ( message ) for a in range ( guess . shape [ 0 ] ) : p0 = guess [ a ] absent_err = st . error absent_d = st . residuals . copy ( ) ind = st . obj_add_particle ( p0 , rad ) if do_opt : opt . do_levmarq_particles ( st , ind , damping = 1.0 , max_iter = 1 , run_length = 3 , eig_update = False , include_rad = False ) present_err = st . error present_d = st . residuals . copy ( ) dont_kill = should_particle_exist ( absent_err , present_err , absent_d , present_d , im_change_frac = im_change_frac , min_derr = min_derr ) if dont_kill : accepts += 1 p = tuple ( st . obj_get_positions ( ) [ ind ] . ravel ( ) ) r = tuple ( st . obj_get_radii ( ) [ ind ] . ravel ( ) ) new_poses . append ( p ) part_msg = '%2.2f\t%3.2f\t%3.2f\t%3.2f\t|\t%4.3f \t%4.3f' % ( p + r + ( absent_err , st . error ) ) with log . noformat ( ) : CLOG . info ( part_msg ) else : st . obj_remove_particle ( ind ) if np . abs ( absent_err - st . error ) > 1e-4 : raise RuntimeError ( 'updates not exact?' ) return accepts , new_poses
12861	def add_months ( self , month_int ) : month_int += self . month while month_int > 12 : self = BusinessDate . add_years ( self , 1 ) month_int -= 12 while month_int < 1 : self = BusinessDate . add_years ( self , - 1 ) month_int += 12 l = monthrange ( self . year , month_int ) [ 1 ] return BusinessDate . from_ymd ( self . year , month_int , min ( l , self . day ) )
11246	def future_value ( present_value , annual_rate , periods_per_year , years ) : rate_per_period = annual_rate / float ( periods_per_year ) periods = periods_per_year * years return present_value * ( 1 + rate_per_period ) ** periods
9536	def enumeration ( * args ) : assert len ( args ) > 0 , 'at least one argument is required' if len ( args ) == 1 : members = args [ 0 ] else : members = args def checker ( value ) : if value not in members : raise ValueError ( value ) return checker
7545	def calculate_depths ( data , samples , lbview ) : start = time . time ( ) printstr = " calculating depths | {} | s5 |" recaljobs = { } maxlens = [ ] for sample in samples : recaljobs [ sample . name ] = lbview . apply ( recal_hidepth , * ( data , sample ) ) while 1 : ready = [ i . ready ( ) for i in recaljobs . values ( ) ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( "" ) break modsamples = [ ] for sample in samples : if not recaljobs [ sample . name ] . successful ( ) : LOGGER . error ( " sample %s failed: %s" , sample . name , recaljobs [ sample . name ] . exception ( ) ) else : modsample , _ , maxlen , _ , _ = recaljobs [ sample . name ] . result ( ) modsamples . append ( modsample ) maxlens . append ( maxlen ) data . _hackersonly [ "max_fragment_length" ] = int ( max ( maxlens ) ) + 4 return samples
10832	def delete ( cls , group , admin ) : with db . session . begin_nested ( ) : obj = cls . query . filter ( cls . admin == admin , cls . group == group ) . one ( ) db . session . delete ( obj )
7392	def adjust_angles ( self , start_node , start_angle , end_node , end_angle ) : start_group = self . find_node_group_membership ( start_node ) end_group = self . find_node_group_membership ( end_node ) if start_group == 0 and end_group == len ( self . nodes . keys ( ) ) - 1 : if self . has_edge_within_group ( start_group ) : start_angle = correct_negative_angle ( start_angle - self . minor_angle ) if self . has_edge_within_group ( end_group ) : end_angle = correct_negative_angle ( end_angle + self . minor_angle ) elif start_group == len ( self . nodes . keys ( ) ) - 1 and end_group == 0 : if self . has_edge_within_group ( start_group ) : start_angle = correct_negative_angle ( start_angle + self . minor_angle ) if self . has_edge_within_group ( end_group ) : end_angle = correct_negative_angle ( end_angle - self . minor_angle ) elif start_group < end_group : if self . has_edge_within_group ( end_group ) : end_angle = correct_negative_angle ( end_angle - self . minor_angle ) if self . has_edge_within_group ( start_group ) : start_angle = correct_negative_angle ( start_angle + self . minor_angle ) elif end_group < start_group : if self . has_edge_within_group ( start_group ) : start_angle = correct_negative_angle ( start_angle - self . minor_angle ) if self . has_edge_within_group ( end_group ) : end_angle = correct_negative_angle ( end_angle + self . minor_angle ) return start_angle , end_angle
9370	def person_inn ( ) : mask11 = [ 7 , 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] mask12 = [ 3 , 7 , 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] inn = [ random . randint ( 1 , 9 ) for _ in range ( 12 ) ] weighted11 = [ v * mask11 [ i ] for i , v in enumerate ( inn [ : - 2 ] ) ] inn [ 10 ] = sum ( weighted11 ) % 11 % 10 weighted12 = [ v * mask12 [ i ] for i , v in enumerate ( inn [ : - 1 ] ) ] inn [ 11 ] = sum ( weighted12 ) % 11 % 10 return "" . join ( map ( str , inn ) )
9793	def _matches_patterns ( path , patterns ) : for glob in patterns : try : if PurePath ( path ) . match ( glob ) : return True except TypeError : pass return False
2878	def serialize_value ( self , parent_elem , value ) : if isinstance ( value , ( str , int ) ) or type ( value ) . __name__ == 'str' : parent_elem . text = str ( value ) elif value is None : parent_elem . text = None else : parent_elem . append ( value . serialize ( self ) )
2442	def add_annotation_comment ( self , doc , comment ) : if len ( doc . annotations ) != 0 : if not self . annotation_comment_set : self . annotation_comment_set = True if validations . validate_annotation_comment ( comment ) : doc . annotations [ - 1 ] . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'AnnotationComment::Comment' ) else : raise CardinalityError ( 'AnnotationComment::Comment' ) else : raise OrderError ( 'AnnotationComment::Comment' )
10612	def _calculate_T ( self , H ) : x = list ( ) x . append ( self . _T ) x . append ( self . _T + 10.0 ) y = list ( ) y . append ( self . _calculate_H ( x [ 0 ] ) - H ) y . append ( self . _calculate_H ( x [ 1 ] ) - H ) for i in range ( 2 , 50 ) : x . append ( x [ i - 1 ] - y [ i - 1 ] * ( ( x [ i - 1 ] - x [ i - 2 ] ) / ( y [ i - 1 ] - y [ i - 2 ] ) ) ) y . append ( self . _calculate_H ( x [ i ] ) - H ) if abs ( y [ i - 1 ] ) < 1.0e-5 : break return x [ len ( x ) - 1 ]
9310	def amz_cano_path ( self , path ) : safe_chars = '/~' qs = '' fixed_path = path if '?' in fixed_path : fixed_path , qs = fixed_path . split ( '?' , 1 ) fixed_path = posixpath . normpath ( fixed_path ) fixed_path = re . sub ( '/+' , '/' , fixed_path ) if path . endswith ( '/' ) and not fixed_path . endswith ( '/' ) : fixed_path += '/' full_path = fixed_path if PY2 : full_path = full_path . encode ( 'utf-8' ) safe_chars = safe_chars . encode ( 'utf-8' ) qs = qs . encode ( 'utf-8' ) if self . service in [ 's3' , 'host' ] : full_path = unquote ( full_path ) full_path = quote ( full_path , safe = safe_chars ) if qs : qm = b'?' if PY2 else '?' full_path = qm . join ( ( full_path , qs ) ) if PY2 : full_path = unicode ( full_path ) return full_path
6340	def sim ( self , src , tar ) : r if src == tar : return 1.0 elif not src or not tar : return 0.0 return len ( self . lcsseq ( src , tar ) ) / max ( len ( src ) , len ( tar ) )
11718	def delete ( self ) : headers = self . _default_headers ( ) return self . _request ( self . name , ok_status = None , data = None , headers = headers , method = "DELETE" )
1454	def add_data_tuple ( self , stream_id , new_data_tuple , tuple_size_in_bytes ) : if ( self . current_data_tuple_set is None ) or ( self . current_data_tuple_set . stream . id != stream_id ) or ( len ( self . current_data_tuple_set . tuples ) >= self . data_tuple_set_capacity ) or ( self . current_data_tuple_size_in_bytes >= self . max_data_tuple_size_in_bytes ) : self . _init_new_data_tuple ( stream_id ) added_tuple = self . current_data_tuple_set . tuples . add ( ) added_tuple . CopyFrom ( new_data_tuple ) self . current_data_tuple_size_in_bytes += tuple_size_in_bytes self . total_data_emitted_in_bytes += tuple_size_in_bytes
1044	def float_unpack ( Q , size , le ) : if size == 8 : MIN_EXP = - 1021 MAX_EXP = 1024 MANT_DIG = 53 BITS = 64 elif size == 4 : MIN_EXP = - 125 MAX_EXP = 128 MANT_DIG = 24 BITS = 32 else : raise ValueError ( "invalid size value" ) if Q >> BITS : raise ValueError ( "input out of range" ) sign = Q >> BITS - 1 exp = ( Q & ( ( 1 << BITS - 1 ) - ( 1 << MANT_DIG - 1 ) ) ) >> MANT_DIG - 1 mant = Q & ( ( 1 << MANT_DIG - 1 ) - 1 ) if exp == MAX_EXP - MIN_EXP + 2 : result = float ( 'nan' ) if mant else float ( 'inf' ) elif exp == 0 : result = math . ldexp ( float ( mant ) , MIN_EXP - MANT_DIG ) else : mant += 1 << MANT_DIG - 1 result = math . ldexp ( float ( mant ) , exp + MIN_EXP - MANT_DIG - 1 ) return - result if sign else result
2612	def deserialize_object ( buffers , g = None ) : bufs = list ( buffers ) pobj = buffer_to_bytes_py2 ( bufs . pop ( 0 ) ) canned = pickle . loads ( pobj ) if istype ( canned , sequence_types ) and len ( canned ) < MAX_ITEMS : for c in canned : _restore_buffers ( c , bufs ) newobj = uncan_sequence ( canned , g ) elif istype ( canned , dict ) and len ( canned ) < MAX_ITEMS : newobj = { } for k in sorted ( canned ) : c = canned [ k ] _restore_buffers ( c , bufs ) newobj [ k ] = uncan ( c , g ) else : _restore_buffers ( canned , bufs ) newobj = uncan ( canned , g ) return newobj , bufs
1923	def binary_arch ( binary ) : with open ( binary , 'rb' ) as f : elffile = ELFFile ( f ) if elffile [ 'e_machine' ] == 'EM_X86_64' : return True else : return False
11641	def yaml_get_data ( filename ) : with open ( filename , 'rb' ) as fd : yaml_data = yaml . load ( fd ) return yaml_data return False
13100	def render ( self , ** kwargs ) : breadcrumbs = [ ] breadcrumbs = [ ] if "collections" in kwargs : breadcrumbs = [ { "title" : "Text Collections" , "link" : ".r_collections" , "args" : { } } ] if "parents" in kwargs [ "collections" ] : breadcrumbs += [ { "title" : parent [ "label" ] , "link" : ".r_collection_semantic" , "args" : { "objectId" : parent [ "id" ] , "semantic" : f_slugify ( parent [ "label" ] ) , } , } for parent in kwargs [ "collections" ] [ "parents" ] ] [ : : - 1 ] if "current" in kwargs [ "collections" ] : breadcrumbs . append ( { "title" : kwargs [ "collections" ] [ "current" ] [ "label" ] , "link" : None , "args" : { } } ) if len ( breadcrumbs ) > 0 : breadcrumbs [ - 1 ] [ "link" ] = None return { "breadcrumbs" : breadcrumbs }
9836	def __comment ( self ) : tok = self . __consume ( ) self . DXfield . add_comment ( tok . value ( ) ) self . set_parser ( 'general' )
11520	def search ( self , search , token = None ) : parameters = dict ( ) parameters [ 'search' ] = search if token : parameters [ 'token' ] = token response = self . request ( 'midas.resource.search' , parameters ) return response
10180	def get ( self , timeout = None ) : result = None try : result = self . _result . get ( True , timeout = timeout ) except Empty : raise Timeout ( ) if isinstance ( result , Failure ) : six . reraise ( * result . exc_info ) else : return result
7190	def new ( n , prefix = None ) : if isinstance ( n , Leaf ) : return Leaf ( n . type , n . value , prefix = n . prefix if prefix is None else prefix ) n . parent = None if prefix is not None : n . prefix = prefix return n
12651	def where_is ( strings , pattern , n = 1 , lookup_func = re . match ) : count = 0 for idx , item in enumerate ( strings ) : if lookup_func ( pattern , item ) : count += 1 if count == n : return idx return - 1
7087	def get_epochs_given_midtimes_and_period ( t_mid , period , err_t_mid = None , t0_fixed = None , t0_percentile = None , verbose = False ) : kwargarr = np . array ( [ isinstance ( err_t_mid , np . ndarray ) , t0_fixed , t0_percentile ] ) if not _single_true ( kwargarr ) and not np . all ( ~ kwargarr . astype ( bool ) ) : raise AssertionError ( 'can have at most one of err_t_mid, t0_fixed, t0_percentile' ) t_mid = t_mid [ np . isfinite ( t_mid ) ] N_midtimes = len ( t_mid ) if t0_fixed : t0 = t0_fixed elif isinstance ( err_t_mid , np . ndarray ) : t0_avg = np . average ( t_mid , weights = 1 / err_t_mid ** 2 ) t0_options = np . arange ( min ( t_mid ) , max ( t_mid ) + period , period ) t0 = t0_options [ np . argmin ( np . abs ( t0_options - t0_avg ) ) ] else : if not t0_percentile : if N_midtimes % 2 == 1 : t0 = np . median ( t_mid ) else : t0 = t_mid [ int ( N_midtimes / 2 ) ] else : t0 = np . sort ( t_mid ) [ int ( N_midtimes * t0_percentile / 100 ) ] epoch = ( t_mid - t0 ) / period int_epoch = np . round ( epoch , 0 ) if verbose : LOGINFO ( 'epochs before rounding' ) LOGINFO ( '\n{:s}' . format ( repr ( epoch ) ) ) LOGINFO ( 'epochs after rounding' ) LOGINFO ( '\n{:s}' . format ( repr ( int_epoch ) ) ) return int_epoch , t0
5041	def enroll_users_in_program ( cls , enterprise_customer , program_details , course_mode , emails , cohort = None ) : existing_users , unregistered_emails = cls . get_users_by_email ( emails ) course_ids = get_course_runs_from_program ( program_details ) successes = [ ] pending = [ ] failures = [ ] for user in existing_users : succeeded = cls . enroll_user ( enterprise_customer , user , course_mode , * course_ids ) if succeeded : successes . append ( user ) else : failures . append ( user ) for email in unregistered_emails : pending_user = enterprise_customer . enroll_user_pending_registration ( email , course_mode , * course_ids , cohort = cohort ) pending . append ( pending_user ) return successes , pending , failures
2091	def copy ( self , pk = None , new_name = None , ** kwargs ) : orig = self . read ( pk , fail_on_no_results = True , fail_on_multiple_results = True ) orig = orig [ 'results' ] [ 0 ] self . _pop_none ( kwargs ) newresource = copy ( orig ) newresource . pop ( 'id' ) basename = newresource [ 'name' ] . split ( '@' , 1 ) [ 0 ] . strip ( ) for field in self . fields : if field . multiple and field . name in newresource : newresource [ field . name ] = ( newresource . get ( field . name ) , ) if new_name is None : newresource [ 'name' ] = "%s @ %s" % ( basename , time . strftime ( '%X' ) ) newresource . update ( kwargs ) return self . write ( create_on_missing = True , fail_on_found = True , ** newresource ) else : if kwargs : raise exc . TowerCLIError ( 'Cannot override {} and also use --new-name.' . format ( kwargs . keys ( ) ) ) copy_endpoint = '{}/{}/copy/' . format ( self . endpoint . strip ( '/' ) , pk ) return client . post ( copy_endpoint , data = { 'name' : new_name } ) . json ( )
9064	def fit ( self , verbose = True ) : if not self . _isfixed ( "logistic" ) : self . _maximize_scalar ( desc = "LMM" , rtol = 1e-6 , atol = 1e-6 , verbose = verbose ) if not self . _fix [ "beta" ] : self . _update_beta ( ) if not self . _fix [ "scale" ] : self . _update_scale ( )
5942	def _get_gmx_docs ( self ) : if self . _doc_cache is not None : return self . _doc_cache try : logging . disable ( logging . CRITICAL ) rc , header , docs = self . run ( 'h' , stdout = PIPE , stderr = PIPE , use_input = False ) except : logging . critical ( "Invoking command {0} failed when determining its doc string. Proceed with caution" . format ( self . command_name ) ) self . _doc_cache = "(No Gromacs documentation available)" return self . _doc_cache finally : logging . disable ( logging . NOTSET ) m = re . match ( self . doc_pattern , docs , re . DOTALL ) if m is None : m = re . match ( self . doc_pattern , header , re . DOTALL ) if m is None : self . _doc_cache = "(No Gromacs documentation available)" return self . _doc_cache self . _doc_cache = m . group ( 'DOCS' ) return self . _doc_cache
13821	def _ConvertStructMessage ( value , message ) : if not isinstance ( value , dict ) : raise ParseError ( 'Struct must be in a dict which is {0}.' . format ( value ) ) for key in value : _ConvertValueMessage ( value [ key ] , message . fields [ key ] ) return
11421	def field_xml_output ( field , tag ) : marcxml = [ ] if field [ 3 ] : marcxml . append ( ' <controlfield tag="%s">%s</controlfield>' % ( tag , MathMLParser . html_to_text ( field [ 3 ] ) ) ) else : marcxml . append ( ' <datafield tag="%s" ind1="%s" ind2="%s">' % ( tag , field [ 1 ] , field [ 2 ] ) ) marcxml += [ _subfield_xml_output ( subfield ) for subfield in field [ 0 ] ] marcxml . append ( ' </datafield>' ) return '\n' . join ( marcxml )
1889	def min ( self , constraints , X : BitVec , M = 10000 ) : assert isinstance ( X , BitVec ) return self . optimize ( constraints , X , 'minimize' , M )
1795	def SBB ( cpu , dest , src ) : cpu . _SUB ( dest , src , carry = True )
9565	def main ( ) : description = 'Validate a CSV data file.' parser = argparse . ArgumentParser ( description = description ) parser . add_argument ( 'file' , metavar = 'FILE' , help = 'a file to be validated' ) parser . add_argument ( '-l' , '--limit' , dest = 'limit' , type = int , action = 'store' , default = 0 , help = 'limit the number of problems reported' ) parser . add_argument ( '-s' , '--summarize' , dest = 'summarize' , action = 'store_true' , default = False , help = 'output only a summary of the different types of problem found' ) parser . add_argument ( '-e' , '--report-unexpected-exceptions' , dest = 'report_unexpected_exceptions' , action = 'store_true' , default = False , help = 'report any unexpected exceptions as problems' ) args = parser . parse_args ( ) if not os . path . isfile ( args . file ) : print '%s is not a file' % args . file sys . exit ( 1 ) with open ( args . file , 'r' ) as f : data = csv . reader ( f , delimiter = '\t' ) validator = create_validator ( ) problems = validator . validate ( data , summarize = args . summarize , report_unexpected_exceptions = args . report_unexpected_exceptions , context = { 'file' : args . file } ) write_problems ( problems , sys . stdout , summarize = args . summarize , limit = args . limit ) if problems : sys . exit ( 1 ) else : sys . exit ( 0 )
8078	def arrow ( self , x , y , width , type = NORMAL , draw = True , ** kwargs ) : path = self . BezierPath ( ** kwargs ) if type == self . NORMAL : head = width * .4 tail = width * .2 path . moveto ( x , y ) path . lineto ( x - head , y + head ) path . lineto ( x - head , y + tail ) path . lineto ( x - width , y + tail ) path . lineto ( x - width , y - tail ) path . lineto ( x - head , y - tail ) path . lineto ( x - head , y - head ) path . lineto ( x , y ) elif type == self . FORTYFIVE : head = .3 tail = 1 + head path . moveto ( x , y ) path . lineto ( x , y + width * ( 1 - head ) ) path . lineto ( x - width * head , y + width ) path . lineto ( x - width * head , y + width * tail * .4 ) path . lineto ( x - width * tail * .6 , y + width ) path . lineto ( x - width , y + width * tail * .6 ) path . lineto ( x - width * tail * .4 , y + width * head ) path . lineto ( x - width , y + width * head ) path . lineto ( x - width * ( 1 - head ) , y ) path . lineto ( x , y ) else : raise NameError ( _ ( "arrow: available types for arrow() are NORMAL and FORTYFIVE\n" ) ) if draw : path . draw ( ) return path
10449	def getallstates ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) _obj_states = [ ] if object_handle . AXEnabled : _obj_states . append ( "enabled" ) if object_handle . AXFocused : _obj_states . append ( "focused" ) else : try : if object_handle . AXFocused : _obj_states . append ( "focusable" ) except : pass if re . match ( "AXCheckBox" , object_handle . AXRole , re . M | re . U | re . L ) or re . match ( "AXRadioButton" , object_handle . AXRole , re . M | re . U | re . L ) : if object_handle . AXValue : _obj_states . append ( "checked" ) return _obj_states
4980	def get ( self , request , enterprise_uuid , course_id ) : enrollment_course_mode = request . GET . get ( 'course_mode' ) enterprise_catalog_uuid = request . GET . get ( 'catalog' ) if not enrollment_course_mode : return redirect ( LMS_DASHBOARD_URL ) enrollment_api_client = EnrollmentApiClient ( ) course_modes = enrollment_api_client . get_course_modes ( course_id ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) enterprise_customer_user = get_enterprise_customer_user ( request . user . id , enterprise_customer . uuid ) if not course_modes : context_data = get_global_context ( request , enterprise_customer ) error_code = 'ENTHCE000' log_message = ( 'No course_modes for course_id {course_id} for enterprise_catalog_uuid ' '{enterprise_catalog_uuid}.' 'The following error was presented to ' 'user {userid}: {error_code}' . format ( userid = request . user . id , enterprise_catalog_uuid = enterprise_catalog_uuid , course_id = course_id , error_code = error_code ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) selected_course_mode = None for course_mode in course_modes : if course_mode [ 'slug' ] == enrollment_course_mode : selected_course_mode = course_mode break if not selected_course_mode : return redirect ( LMS_DASHBOARD_URL ) __ , created = EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = enterprise_customer_user , course_id = course_id , ) if created : track_enrollment ( 'course-landing-page-enrollment' , request . user . id , course_id , request . get_full_path ( ) ) DataSharingConsent . objects . update_or_create ( username = enterprise_customer_user . username , course_id = course_id , enterprise_customer = enterprise_customer_user . enterprise_customer , defaults = { 'granted' : True } , ) audit_modes = getattr ( settings , 'ENTERPRISE_COURSE_ENROLLMENT_AUDIT_MODES' , [ 'audit' , 'honor' ] ) if selected_course_mode [ 'slug' ] in audit_modes : enrollment_api_client . enroll_user_in_course ( request . user . username , course_id , selected_course_mode [ 'slug' ] ) return redirect ( LMS_COURSEWARE_URL . format ( course_id = course_id ) ) premium_flow = LMS_START_PREMIUM_COURSE_FLOW_URL . format ( course_id = course_id ) if enterprise_catalog_uuid : premium_flow += '?catalog={catalog_uuid}' . format ( catalog_uuid = enterprise_catalog_uuid ) return redirect ( premium_flow )
811	def generateStats ( filename , statsInfo , maxSamples = None , filters = [ ] , cache = True ) : if not isinstance ( statsInfo , dict ) : raise RuntimeError ( "statsInfo must be a dict -- " "found '%s' instead" % type ( statsInfo ) ) filename = resource_filename ( "nupic.datafiles" , filename ) if cache : statsFilename = getStatsFilename ( filename , statsInfo , filters ) if os . path . exists ( statsFilename ) : try : r = pickle . load ( open ( statsFilename , "rb" ) ) except : print "Warning: unable to load stats for %s -- " "will regenerate" % filename r = dict ( ) requestedKeys = set ( [ s for s in statsInfo ] ) availableKeys = set ( r . keys ( ) ) unavailableKeys = requestedKeys . difference ( availableKeys ) if len ( unavailableKeys ) == 0 : return r else : print "generateStats: re-generating stats file %s because " "keys %s are not available" % ( filename , str ( unavailableKeys ) ) os . remove ( filename ) print "Generating statistics for file '%s' with filters '%s'" % ( filename , filters ) sensor = RecordSensor ( ) sensor . dataSource = FileRecordStream ( filename ) sensor . preEncodingFilters = filters stats = [ ] for field in statsInfo : if statsInfo [ field ] == "number" : statsInfo [ field ] = NumberStatsCollector ( ) elif statsInfo [ field ] == "category" : statsInfo [ field ] = CategoryStatsCollector ( ) else : raise RuntimeError ( "Unknown stats type '%s' for field '%s'" % ( statsInfo [ field ] , field ) ) if maxSamples is None : maxSamples = 500000 for i in xrange ( maxSamples ) : try : record = sensor . getNextRecord ( ) except StopIteration : break for ( name , collector ) in statsInfo . items ( ) : collector . add ( record [ name ] ) del sensor r = dict ( ) for ( field , collector ) in statsInfo . items ( ) : stats = collector . getStats ( ) if field not in r : r [ field ] = stats else : r [ field ] . update ( stats ) if cache : f = open ( statsFilename , "wb" ) pickle . dump ( r , f ) f . close ( ) r [ "_filename" ] = statsFilename return r
4402	def remove_node ( self , node ) : self . nodes . remove ( node ) for x in xrange ( self . replicas ) : ring_key = self . hash_method ( b ( "%s:%d" % ( node , x ) ) ) self . ring . pop ( ring_key ) self . sorted_keys . remove ( ring_key )
11243	def add_newlines ( f , output , char ) : line_count = get_line_count ( f ) f = open ( f , 'r+' ) output = open ( output , 'r+' ) for line in range ( line_count ) : string = f . readline ( ) string = re . sub ( char , char + '\n' , string ) output . write ( string )
2718	def remove_droplets ( self , droplet ) : droplets = droplet if not isinstance ( droplets , list ) : droplets = [ droplet ] resources = self . __extract_resources_from_droplets ( droplets ) if len ( resources ) > 0 : return self . __remove_resources ( resources ) return False
4995	def handle_user_post_save ( sender , ** kwargs ) : created = kwargs . get ( "created" , False ) user_instance = kwargs . get ( "instance" , None ) if user_instance is None : return try : pending_ecu = PendingEnterpriseCustomerUser . objects . get ( user_email = user_instance . email ) except PendingEnterpriseCustomerUser . DoesNotExist : return if not created : try : existing_record = EnterpriseCustomerUser . objects . get ( user_id = user_instance . id ) message_template = "User {user} have changed email to match pending Enterprise Customer link, " "but was already linked to Enterprise Customer {enterprise_customer} - " "deleting pending link record" logger . info ( message_template . format ( user = user_instance , enterprise_customer = existing_record . enterprise_customer ) ) pending_ecu . delete ( ) return except EnterpriseCustomerUser . DoesNotExist : pass enterprise_customer_user = EnterpriseCustomerUser . objects . create ( enterprise_customer = pending_ecu . enterprise_customer , user_id = user_instance . id ) pending_enrollments = list ( pending_ecu . pendingenrollment_set . all ( ) ) if pending_enrollments : def _complete_user_enrollment ( ) : for enrollment in pending_enrollments : enterprise_customer_user . enroll ( enrollment . course_id , enrollment . course_mode , cohort = enrollment . cohort_name ) track_enrollment ( 'pending-admin-enrollment' , user_instance . id , enrollment . course_id ) pending_ecu . delete ( ) transaction . on_commit ( _complete_user_enrollment ) else : pending_ecu . delete ( )
12157	def list_order_by ( l , firstItems ) : l = list ( l ) for item in firstItems [ : : - 1 ] : if item in l : l . remove ( item ) l . insert ( 0 , item ) return l
12761	def process_data ( self ) : self . visibility = self . data [ : , : , 3 ] self . positions = self . data [ : , : , : 3 ] self . velocities = np . zeros_like ( self . positions ) + 1000 for frame_no in range ( 1 , len ( self . data ) - 1 ) : prev = self . data [ frame_no - 1 ] next = self . data [ frame_no + 1 ] for c in range ( self . num_markers ) : if - 1 < prev [ c , 3 ] < 100 and - 1 < next [ c , 3 ] < 100 : self . velocities [ frame_no , c ] = ( next [ c , : 3 ] - prev [ c , : 3 ] ) / ( 2 * self . world . dt ) self . cfms = np . zeros_like ( self . visibility ) + self . DEFAULT_CFM
11624	def from_devanagari ( self , data ) : from indic_transliteration import sanscript return sanscript . transliterate ( data = data , _from = sanscript . DEVANAGARI , _to = self . name )
5	def clear_mpi_env_vars ( ) : removed_environment = { } for k , v in list ( os . environ . items ( ) ) : for prefix in [ 'OMPI_' , 'PMI_' ] : if k . startswith ( prefix ) : removed_environment [ k ] = v del os . environ [ k ] try : yield finally : os . environ . update ( removed_environment )
678	def generateRecords ( self , records ) : if self . verbosity > 0 : print 'Generating' , len ( records ) , 'records...' for record in records : self . generateRecord ( record )
6017	def absolute_signal_to_noise_map ( self ) : return np . divide ( np . abs ( self . image ) , self . noise_map )
9197	def get ( self , key , default = _sentinel ) : tup = self . _data . get ( key . lower ( ) ) if tup is not None : return tup [ 1 ] elif default is not _sentinel : return default else : return None
1903	def colored_level_name ( self , levelname ) : if self . colors_disabled : return self . plain_levelname_format . format ( levelname ) else : return self . colored_levelname_format . format ( self . color_map [ levelname ] , levelname )
2017	def DIV ( self , a , b ) : try : result = Operators . UDIV ( a , b ) except ZeroDivisionError : result = 0 return Operators . ITEBV ( 256 , b == 0 , 0 , result )
178	def subdivide ( self , points_per_edge ) : if len ( self . coords ) <= 1 or points_per_edge < 1 : return self . deepcopy ( ) coords = interpolate_points ( self . coords , nb_steps = points_per_edge , closed = False ) return self . deepcopy ( coords = coords )
11405	def record_drop_duplicate_fields ( record ) : out = { } position = 0 tags = sorted ( record . keys ( ) ) for tag in tags : fields = record [ tag ] out [ tag ] = [ ] current_fields = set ( ) for full_field in fields : field = ( tuple ( full_field [ 0 ] ) , ) + full_field [ 1 : 4 ] if field not in current_fields : current_fields . add ( field ) position += 1 out [ tag ] . append ( full_field [ : 4 ] + ( position , ) ) return out
3276	def handle_copy ( self , dest_path , depth_infinity ) : if "/by_tag/" not in dest_path : raise DAVError ( HTTP_FORBIDDEN ) catType , tag , _rest = util . save_split ( dest_path . strip ( "/" ) , "/" , 2 ) assert catType == "by_tag" if tag not in self . data [ "tags" ] : self . data [ "tags" ] . append ( tag ) return True
13322	def rem_active_module ( module ) : modules = set ( get_active_modules ( ) ) modules . discard ( module ) new_modules_path = os . pathsep . join ( [ m . path for m in modules ] ) os . environ [ 'CPENV_ACTIVE_MODULES' ] = str ( new_modules_path )
5664	def interpolate_shape_times ( shape_distances , shape_breaks , stop_times ) : shape_times = np . zeros ( len ( shape_distances ) ) shape_times [ : shape_breaks [ 0 ] ] = stop_times [ 0 ] for i in range ( len ( shape_breaks ) - 1 ) : cur_break = shape_breaks [ i ] cur_time = stop_times [ i ] next_break = shape_breaks [ i + 1 ] next_time = stop_times [ i + 1 ] if cur_break == next_break : shape_times [ cur_break ] = stop_times [ i ] else : cur_distances = shape_distances [ cur_break : next_break + 1 ] norm_distances = ( ( np . array ( cur_distances ) - float ( cur_distances [ 0 ] ) ) / float ( cur_distances [ - 1 ] - cur_distances [ 0 ] ) ) times = ( 1. - norm_distances ) * cur_time + norm_distances * next_time shape_times [ cur_break : next_break ] = times [ : - 1 ] shape_times [ shape_breaks [ - 1 ] : ] = stop_times [ - 1 ] return list ( shape_times )
5852	def get_dataset_file ( self , dataset_id , file_path , version = None ) : return self . get_dataset_files ( dataset_id , "^{}$" . format ( file_path ) , version_number = version ) [ 0 ]
446	def batch_with_dynamic_pad ( images_and_captions , batch_size , queue_capacity , add_summaries = True ) : enqueue_list = [ ] for image , caption in images_and_captions : caption_length = tf . shape ( caption ) [ 0 ] input_length = tf . expand_dims ( tf . subtract ( caption_length , 1 ) , 0 ) input_seq = tf . slice ( caption , [ 0 ] , input_length ) target_seq = tf . slice ( caption , [ 1 ] , input_length ) indicator = tf . ones ( input_length , dtype = tf . int32 ) enqueue_list . append ( [ image , input_seq , target_seq , indicator ] ) images , input_seqs , target_seqs , mask = tf . train . batch_join ( enqueue_list , batch_size = batch_size , capacity = queue_capacity , dynamic_pad = True , name = "batch_and_pad" ) if add_summaries : lengths = tf . add ( tf . reduce_sum ( mask , 1 ) , 1 ) tf . summary . scalar ( "caption_length/batch_min" , tf . reduce_min ( lengths ) ) tf . summary . scalar ( "caption_length/batch_max" , tf . reduce_max ( lengths ) ) tf . summary . scalar ( "caption_length/batch_mean" , tf . reduce_mean ( lengths ) ) return images , input_seqs , target_seqs , mask
8888	def fit ( self , X ) : X = iter2array ( X , dtype = ReactionContainer ) self . _train_signatures = { self . __get_signature ( x ) for x in X } return self
2141	def parse_kv ( var_string ) : return_dict = { } if var_string is None : return { } fix_encoding_26 = False if sys . version_info < ( 2 , 7 ) and '\x00' in shlex . split ( u'a' ) [ 0 ] : fix_encoding_26 = True is_unicode = False if fix_encoding_26 or not isinstance ( var_string , str ) : if isinstance ( var_string , six . text_type ) : var_string = var_string . encode ( 'UTF-8' ) is_unicode = True else : var_string = str ( var_string ) for token in shlex . split ( var_string ) : if ( is_unicode ) : token = token . decode ( 'UTF-8' ) if fix_encoding_26 : token = six . text_type ( token ) if '=' in token : ( k , v ) = token . split ( '=' , 1 ) if len ( k ) == 0 or len ( v ) == 0 : raise Exception try : return_dict [ k ] = ast . literal_eval ( v ) except Exception : return_dict [ k ] = v else : raise Exception return return_dict
9653	def write_shas_to_shastore ( sha_dict ) : if sys . version_info [ 0 ] < 3 : fn_open = open else : fn_open = io . open with fn_open ( ".shastore" , "w" ) as fh : fh . write ( "---\n" ) fh . write ( 'sake version: {}\n' . format ( constants . VERSION ) ) if sha_dict : fh . write ( yaml . dump ( sha_dict ) ) fh . write ( "..." )
4194	def plot_window ( self ) : from pylab import plot , xlim , grid , title , ylabel , axis x = linspace ( 0 , 1 , self . N ) xlim ( 0 , 1 ) plot ( x , self . data ) grid ( True ) title ( '%s Window (%s points)' % ( self . name . capitalize ( ) , self . N ) ) ylabel ( 'Amplitude' ) axis ( [ 0 , 1 , 0 , 1.1 ] )
2279	def retrieve_adjacency_matrix ( graph , order_nodes = None , weight = False ) : if isinstance ( graph , np . ndarray ) : return graph elif isinstance ( graph , nx . DiGraph ) : if order_nodes is None : order_nodes = graph . nodes ( ) if not weight : return np . array ( nx . adjacency_matrix ( graph , order_nodes , weight = None ) . todense ( ) ) else : return np . array ( nx . adjacency_matrix ( graph , order_nodes ) . todense ( ) ) else : raise TypeError ( "Only networkx.DiGraph and np.ndarray (adjacency matrixes) are supported." )
13868	def weekday ( when , weekday , start = mon ) : if isinstance ( when , datetime ) : when = when . date ( ) today = when . weekday ( ) delta = weekday - today if weekday < start and today >= start : delta += 7 elif weekday >= start and today < start : delta -= 7 return when + timedelta ( days = delta )
4214	def get_all_keyring ( ) : _load_plugins ( ) viable_classes = KeyringBackend . get_viable_backends ( ) rings = util . suppress_exceptions ( viable_classes , exceptions = TypeError ) return list ( rings )
12126	def spec_formatter ( cls , spec ) : " Formats the elements of an argument set appropriately" return type ( spec ) ( ( k , str ( v ) ) for ( k , v ) in spec . items ( ) )
7814	def from_file ( cls , filename ) : with open ( filename , "r" ) as pem_file : data = pem . readPemFromFile ( pem_file ) return cls . from_der_data ( data )
6430	def sim ( self , src , tar ) : def _lcsstr_stl ( src , tar ) : lengths = np_zeros ( ( len ( src ) + 1 , len ( tar ) + 1 ) , dtype = np_int ) longest , src_longest , tar_longest = 0 , 0 , 0 for i in range ( 1 , len ( src ) + 1 ) : for j in range ( 1 , len ( tar ) + 1 ) : if src [ i - 1 ] == tar [ j - 1 ] : lengths [ i , j ] = lengths [ i - 1 , j - 1 ] + 1 if lengths [ i , j ] > longest : longest = lengths [ i , j ] src_longest = i tar_longest = j else : lengths [ i , j ] = 0 return src_longest - longest , tar_longest - longest , longest def _sstr_matches ( src , tar ) : src_start , tar_start , length = _lcsstr_stl ( src , tar ) if length == 0 : return 0 return ( _sstr_matches ( src [ : src_start ] , tar [ : tar_start ] ) + length + _sstr_matches ( src [ src_start + length : ] , tar [ tar_start + length : ] ) ) if src == tar : return 1.0 elif not src or not tar : return 0.0 return 2 * _sstr_matches ( src , tar ) / ( len ( src ) + len ( tar ) )
8879	def fit ( self , X , y = None ) : X = check_array ( X ) self . tree = BallTree ( X , leaf_size = self . leaf_size , metric = self . metric ) dist_train = self . tree . query ( X , k = 2 ) [ 0 ] if self . threshold == 'auto' : self . threshold_value = 0.5 * sqrt ( var ( dist_train [ : , 1 ] ) ) + mean ( dist_train [ : , 1 ] ) elif self . threshold == 'cv' : if y is None : raise ValueError ( "Y must be specified to find the optimal threshold." ) y = check_array ( y , accept_sparse = 'csc' , ensure_2d = False , dtype = None ) self . threshold_value = 0 score = 0 Y_pred , Y_true , AD = [ ] , [ ] , [ ] cv = KFold ( n_splits = 5 , random_state = 1 , shuffle = True ) for train_index , test_index in cv . split ( X ) : x_train = safe_indexing ( X , train_index ) x_test = safe_indexing ( X , test_index ) y_train = safe_indexing ( y , train_index ) y_test = safe_indexing ( y , test_index ) data_test = safe_indexing ( dist_train [ : , 1 ] , test_index ) if self . reg_model is None : reg_model = RandomForestRegressor ( n_estimators = 500 , random_state = 1 ) . fit ( x_train , y_train ) else : reg_model = clone ( self . reg_model ) . fit ( x_train , y_train ) Y_pred . append ( reg_model . predict ( x_test ) ) Y_true . append ( y_test ) AD . append ( data_test ) AD_ = unique ( hstack ( AD ) ) for z in AD_ : AD_new = hstack ( AD ) <= z if self . score == 'ba_ad' : val = balanced_accuracy_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) elif self . score == 'rmse_ad' : val = rmse_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) if val >= score : score = val self . threshold_value = z else : self . threshold_value = self . threshold return self
12246	def get_bucket ( self , bucket_name , validate = True , headers = None , force = None ) : if force : bucket = super ( S3Connection , self ) . get_bucket ( bucket_name , validate , headers ) mimicdb . backend . sadd ( tpl . connection , bucket . name ) return bucket if mimicdb . backend . sismember ( tpl . connection , bucket_name ) : return Bucket ( self , bucket_name ) else : if validate : raise S3ResponseError ( 404 , 'NoSuchBucket' ) else : return Bucket ( self , bucket_name )
4245	def _gethostbyname ( self , hostname ) : if self . _databaseType in const . IPV6_EDITIONS : response = socket . getaddrinfo ( hostname , 0 , socket . AF_INET6 ) family , socktype , proto , canonname , sockaddr = response [ 0 ] address , port , flow , scope = sockaddr return address else : return socket . gethostbyname ( hostname )
10778	def update_field ( self , poses = None ) : m = np . clip ( self . particle_field , 0 , 1 ) part_color = np . zeros ( self . _image . shape ) for a in range ( 4 ) : part_color [ : , : , : , a ] = self . part_col [ a ] self . field = np . zeros ( self . _image . shape ) for a in range ( 4 ) : self . field [ : , : , : , a ] = m * part_color [ : , : , : , a ] + ( 1 - m ) * self . _image [ : , : , : , a ]
333	def plot_stoch_vol ( data , trace = None , ax = None ) : if trace is None : trace = model_stoch_vol ( data ) if ax is None : fig , ax = plt . subplots ( figsize = ( 15 , 8 ) ) data . abs ( ) . plot ( ax = ax ) ax . plot ( data . index , np . exp ( trace [ 's' , : : 30 ] . T ) , 'r' , alpha = .03 ) ax . set ( title = 'Stochastic volatility' , xlabel = 'Time' , ylabel = 'Volatility' ) ax . legend ( [ 'Abs returns' , 'Stochastic volatility process' ] , frameon = True , framealpha = 0.5 ) return ax
3542	def popen_streaming_output ( cmd , callback , timeout = None ) : if os . name == 'nt' : process = subprocess . Popen ( shlex . split ( cmd ) , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) stdout = process . stdout else : master , slave = os . openpty ( ) process = subprocess . Popen ( shlex . split ( cmd , posix = True ) , stdout = slave , stderr = slave ) stdout = os . fdopen ( master ) os . close ( slave ) def kill ( process_ ) : try : process_ . kill ( ) except OSError : pass timer = Timer ( timeout , kill , [ process ] ) timer . setDaemon ( True ) timer . start ( ) while process . returncode is None : try : if os . name == 'nt' : line = stdout . readline ( ) line = line . decode ( "utf-8" ) if line : callback ( line . rstrip ( ) ) else : while True : line = stdout . readline ( ) if not line : break callback ( line . rstrip ( ) ) except ( IOError , OSError ) : pass if not timer . is_alive ( ) : raise TimeoutError ( "subprocess running command '{}' timed out after {} seconds" . format ( cmd , timeout ) ) process . poll ( ) timer . cancel ( ) return process . returncode
2219	def register ( self , type ) : def _decorator ( func ) : if isinstance ( type , tuple ) : for t in type : self . func_registry [ t ] = func else : self . func_registry [ type ] = func return func return _decorator
356	def save_ckpt ( sess = None , mode_name = 'model.ckpt' , save_dir = 'checkpoint' , var_list = None , global_step = None , printable = False ) : if sess is None : raise ValueError ( "session is None." ) if var_list is None : var_list = [ ] ckpt_file = os . path . join ( save_dir , mode_name ) if var_list == [ ] : var_list = tf . global_variables ( ) logging . info ( "[*] save %s n_params: %d" % ( ckpt_file , len ( var_list ) ) ) if printable : for idx , v in enumerate ( var_list ) : logging . info ( " param {:3}: {:15} {}" . format ( idx , v . name , str ( v . get_shape ( ) ) ) ) saver = tf . train . Saver ( var_list ) saver . save ( sess , ckpt_file , global_step = global_step )
9411	def _encode ( data , convert_to_float ) : ctf = convert_to_float if isinstance ( data , ( OctaveVariablePtr ) ) : return _encode ( data . value , ctf ) if isinstance ( data , OctaveUserClass ) : return _encode ( OctaveUserClass . to_value ( data ) , ctf ) if isinstance ( data , ( OctaveFunctionPtr , MatlabFunction ) ) : raise Oct2PyError ( 'Cannot write Octave functions' ) if isinstance ( data , MatlabObject ) : view = data . view ( np . ndarray ) out = MatlabObject ( data , data . classname ) for name in out . dtype . names : out [ name ] = _encode ( view [ name ] , ctf ) return out if isinstance ( data , ( DataFrame , Series ) ) : return _encode ( data . values , ctf ) if isinstance ( data , dict ) : out = dict ( ) for ( key , value ) in data . items ( ) : out [ key ] = _encode ( value , ctf ) return out if data is None : return np . NaN if isinstance ( data , set ) : return _encode ( list ( data ) , ctf ) if isinstance ( data , list ) : if _is_simple_numeric ( data ) : return _encode ( np . array ( data ) , ctf ) return _encode ( tuple ( data ) , ctf ) if isinstance ( data , tuple ) : obj = np . empty ( len ( data ) , dtype = object ) for ( i , item ) in enumerate ( data ) : obj [ i ] = _encode ( item , ctf ) return obj if isinstance ( data , spmatrix ) : return data . astype ( np . float64 ) if not isinstance ( data , np . ndarray ) : return data if data . dtype . kind in 'OV' : out = np . empty ( data . size , dtype = data . dtype ) for ( i , item ) in enumerate ( data . ravel ( ) ) : if data . dtype . names : for name in data . dtype . names : out [ i ] [ name ] = _encode ( item [ name ] , ctf ) else : out [ i ] = _encode ( item , ctf ) return out . reshape ( data . shape ) if data . dtype . name == 'complex256' : return data . astype ( np . complex128 ) if ctf and data . dtype . kind in 'ui' : return data . astype ( np . float64 ) return data
3568	def stop_scan ( self , timeout_sec = TIMEOUT_SEC ) : self . _scan_stopped . clear ( ) self . _adapter . StopDiscovery ( ) if not self . _scan_stopped . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to stop scanning!' )
12477	def merge ( dict_1 , dict_2 ) : return dict ( ( str ( key ) , dict_1 . get ( key ) or dict_2 . get ( key ) ) for key in set ( dict_2 ) | set ( dict_1 ) )
9281	def parse_header ( head ) : try : ( fromcall , path ) = head . split ( '>' , 1 ) except : raise ParseError ( "invalid packet header" ) if ( not 1 <= len ( fromcall ) <= 9 or not re . findall ( r"^[a-z0-9]{0,9}(\-[a-z0-9]{1,8})?$" , fromcall , re . I ) ) : raise ParseError ( "fromcallsign is invalid" ) path = path . split ( ',' ) if len ( path [ 0 ] ) == 0 : raise ParseError ( "no tocallsign in header" ) tocall = path [ 0 ] path = path [ 1 : ] validate_callsign ( tocall , "tocallsign" ) for digi in path : if not re . findall ( r"^[A-Z0-9\-]{1,9}\*?$" , digi , re . I ) : raise ParseError ( "invalid callsign in path" ) parsed = { 'from' : fromcall , 'to' : tocall , 'path' : path , } viacall = "" if len ( path ) >= 2 and re . match ( r"^q..$" , path [ - 2 ] ) : viacall = path [ - 1 ] parsed . update ( { 'via' : viacall } ) return parsed
9797	def delete ( ctx ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) if not click . confirm ( "Are sure you want to delete experiment group `{}`" . format ( _group ) ) : click . echo ( 'Existing without deleting experiment group.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . experiment_group . delete_experiment_group ( user , project_name , _group ) GroupManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete experiment group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Experiment group `{}` was delete successfully" . format ( _group ) )
9781	def get ( ctx ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) try : response = PolyaxonClient ( ) . build_job . get_build ( user , project_name , _build ) cache . cache ( config_manager = BuildJobManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get build job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_build_details ( response )
474	def save_vocab ( count = None , name = 'vocab.txt' ) : if count is None : count = [ ] pwd = os . getcwd ( ) vocabulary_size = len ( count ) with open ( os . path . join ( pwd , name ) , "w" ) as f : for i in xrange ( vocabulary_size ) : f . write ( "%s %d\n" % ( tf . compat . as_text ( count [ i ] [ 0 ] ) , count [ i ] [ 1 ] ) ) tl . logging . info ( "%d vocab saved to %s in %s" % ( vocabulary_size , name , pwd ) )
4714	def trun_to_file ( trun , fpath = None ) : if fpath is None : fpath = yml_fpath ( trun [ "conf" ] [ "OUTPUT" ] ) with open ( fpath , 'w' ) as yml_file : data = yaml . dump ( trun , explicit_start = True , default_flow_style = False ) yml_file . write ( data )
11452	def _attach_fulltext ( self , rec , doi ) : url = os . path . join ( self . url_prefix , doi ) record_add_field ( rec , 'FFT' , subfields = [ ( 'a' , url ) , ( 't' , 'INSPIRE-PUBLIC' ) , ( 'd' , 'Fulltext' ) ] )
7993	def _restart_stream ( self ) : self . _input_state = "restart" self . _output_state = "restart" self . features = None self . transport . restart ( ) if self . initiator : self . _send_stream_start ( self . stream_id )
3723	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : sigmas = [ i ( T ) for i in self . SurfaceTensions ] return mixing_simple ( zs , sigmas ) elif method == DIGUILIOTEJA : return Diguilio_Teja ( T = T , xs = zs , sigmas_Tb = self . sigmas_Tb , Tbs = self . Tbs , Tcs = self . Tcs ) elif method == WINTERFELDSCRIVENDAVIS : sigmas = [ i ( T ) for i in self . SurfaceTensions ] rhoms = [ 1. / i ( T , P ) for i in self . VolumeLiquids ] return Winterfeld_Scriven_Davis ( zs , sigmas , rhoms ) else : raise Exception ( 'Method not valid' )
5409	def _validate_ram ( ram_in_mb ) : return int ( GoogleV2CustomMachine . _MEMORY_MULTIPLE * math . ceil ( ram_in_mb / GoogleV2CustomMachine . _MEMORY_MULTIPLE ) )
676	def __shouldSysExit ( self , iteration ) : if self . _exitAfter is None or iteration < self . _exitAfter : return False results = self . _jobsDAO . modelsGetFieldsForJob ( self . _jobID , [ 'params' ] ) modelIDs = [ e [ 0 ] for e in results ] modelNums = [ json . loads ( e [ 1 ] [ 0 ] ) [ 'structuredParams' ] [ '__model_num' ] for e in results ] sameModelNumbers = filter ( lambda x : x [ 1 ] == self . modelIndex , zip ( modelIDs , modelNums ) ) firstModelID = min ( zip ( * sameModelNumbers ) [ 0 ] ) return firstModelID == self . _modelID
208	def draw_on_image ( self , image , alpha = 0.75 , cmap = "jet" , resize = "heatmaps" ) : ia . do_assert ( image . ndim == 3 ) ia . do_assert ( image . shape [ 2 ] == 3 ) ia . do_assert ( image . dtype . type == np . uint8 ) ia . do_assert ( 0 - 1e-8 <= alpha <= 1.0 + 1e-8 ) ia . do_assert ( resize in [ "heatmaps" , "image" ] ) if resize == "image" : image = ia . imresize_single_image ( image , self . arr_0to1 . shape [ 0 : 2 ] , interpolation = "cubic" ) heatmaps_drawn = self . draw ( size = image . shape [ 0 : 2 ] if resize == "heatmaps" else None , cmap = cmap ) mix = [ np . clip ( ( 1 - alpha ) * image + alpha * heatmap_i , 0 , 255 ) . astype ( np . uint8 ) for heatmap_i in heatmaps_drawn ] return mix
3504	def _add_cycle_free ( model , fluxes ) : model . objective = model . solver . interface . Objective ( Zero , direction = "min" , sloppy = True ) objective_vars = [ ] for rxn in model . reactions : flux = fluxes [ rxn . id ] if rxn . boundary : rxn . bounds = ( flux , flux ) continue if flux >= 0 : rxn . bounds = max ( 0 , rxn . lower_bound ) , max ( flux , rxn . upper_bound ) objective_vars . append ( rxn . forward_variable ) else : rxn . bounds = min ( flux , rxn . lower_bound ) , min ( 0 , rxn . upper_bound ) objective_vars . append ( rxn . reverse_variable ) model . objective . set_linear_coefficients ( { v : 1.0 for v in objective_vars } )
11723	def init_app ( self , app , ** kwargs ) : self . init_config ( app ) self . limiter = Limiter ( app , key_func = get_ipaddr ) if app . config [ 'APP_ENABLE_SECURE_HEADERS' ] : self . talisman = Talisman ( app , ** app . config . get ( 'APP_DEFAULT_SECURE_HEADERS' , { } ) ) if app . config [ 'APP_HEALTH_BLUEPRINT_ENABLED' ] : blueprint = Blueprint ( 'invenio_app_ping' , __name__ ) @ blueprint . route ( '/ping' ) def ping ( ) : return 'OK' ping . talisman_view_options = { 'force_https' : False } app . register_blueprint ( blueprint ) requestid_header = app . config . get ( 'APP_REQUESTID_HEADER' ) if requestid_header : @ app . before_request def set_request_id ( ) : request_id = request . headers . get ( requestid_header ) if request_id : g . request_id = request_id [ : 200 ] try : from flask_debugtoolbar import DebugToolbarExtension app . extensions [ 'flask-debugtoolbar' ] = DebugToolbarExtension ( app ) except ImportError : app . logger . debug ( 'Flask-DebugToolbar extension not installed.' ) app . extensions [ 'invenio-app' ] = self
11889	def get_lights ( self ) : now = datetime . datetime . now ( ) if ( now - self . _last_updated ) < datetime . timedelta ( seconds = UPDATE_INTERVAL_SECONDS ) : return self . _bulbs else : self . _last_updated = now light_data = self . get_data ( ) _LOGGER . debug ( "got: %s" , light_data ) if not light_data : return [ ] if self . _bulbs : for bulb in self . _bulbs : try : values = light_data [ bulb . zid ] bulb . _online , bulb . _red , bulb . _green , bulb . _blue , bulb . _level = values except KeyError : pass else : for light_id in light_data : self . _bulbs . append ( Bulb ( self , light_id , * light_data [ light_id ] ) ) return self . _bulbs
875	def copyVarStatesFrom ( self , particleState , varNames ) : allowedToMove = True for varName in particleState [ 'varStates' ] : if varName in varNames : if varName not in self . permuteVars : continue state = copy . deepcopy ( particleState [ 'varStates' ] [ varName ] ) state [ '_position' ] = state [ 'position' ] state [ 'bestPosition' ] = state [ 'position' ] if not allowedToMove : state [ 'velocity' ] = 0 self . permuteVars [ varName ] . setState ( state ) if allowedToMove : self . permuteVars [ varName ] . resetVelocity ( self . _rng )
10017	def upload_archive ( self , filename , key , auto_create_bucket = True ) : try : bucket = self . s3 . get_bucket ( self . aws . bucket ) if ( ( self . aws . region != 'us-east-1' and self . aws . region != 'eu-west-1' ) and bucket . get_location ( ) != self . aws . region ) or ( self . aws . region == 'us-east-1' and bucket . get_location ( ) != '' ) or ( self . aws . region == 'eu-west-1' and bucket . get_location ( ) != 'eu-west-1' ) : raise Exception ( "Existing bucket doesn't match region" ) except S3ResponseError : bucket = self . s3 . create_bucket ( self . aws . bucket , location = self . aws . region ) def __report_upload_progress ( sent , total ) : if not sent : sent = 0 if not total : total = 0 out ( "Uploaded " + str ( sent ) + " bytes of " + str ( total ) + " (" + str ( int ( float ( max ( 1 , sent ) ) / float ( total ) * 100 ) ) + "%)" ) k = Key ( bucket ) k . key = self . aws . bucket_path + key k . set_metadata ( 'time' , str ( time ( ) ) ) k . set_contents_from_filename ( filename , cb = __report_upload_progress , num_cb = 10 )
10007	def get_object ( self , name ) : parts = name . split ( "." ) space = self . spaces [ parts . pop ( 0 ) ] if parts : return space . get_object ( "." . join ( parts ) ) else : return space
2662	def _hold_block ( self , block_id ) : managers = self . connected_managers for manager in managers : if manager [ 'block_id' ] == block_id : logger . debug ( "[HOLD_BLOCK]: Sending hold to manager:{}" . format ( manager [ 'manager' ] ) ) self . hold_worker ( manager [ 'manager' ] )
7868	def _expire_item ( self , key ) : ( timeout , callback ) = self . _timeouts [ key ] now = time . time ( ) if timeout <= now : item = dict . pop ( self , key ) del self . _timeouts [ key ] if callback : try : callback ( key , item ) except TypeError : try : callback ( key ) except TypeError : callback ( ) return None else : return timeout - now
3209	def get_load_balancer ( load_balancer , flags = FLAGS . ALL ^ FLAGS . POLICY_TYPES , ** conn ) : try : basestring except NameError as _ : basestring = str if isinstance ( load_balancer , basestring ) : load_balancer = dict ( LoadBalancerName = load_balancer ) return registry . build_out ( flags , start_with = load_balancer , pass_datastructure = True , ** conn )
7683	def display_multi ( annotations , fig_kw = None , meta = True , ** kwargs ) : if fig_kw is None : fig_kw = dict ( ) fig_kw . setdefault ( 'sharex' , True ) fig_kw . setdefault ( 'squeeze' , True ) display_annotations = [ ] for ann in annotations : for namespace in VIZ_MAPPING : if can_convert ( ann , namespace ) : display_annotations . append ( ann ) break if not len ( display_annotations ) : raise ParameterError ( 'No displayable annotations found' ) fig , axs = plt . subplots ( nrows = len ( display_annotations ) , ncols = 1 , ** fig_kw ) if len ( display_annotations ) == 1 : axs = [ axs ] for ann , ax in zip ( display_annotations , axs ) : kwargs [ 'ax' ] = ax display ( ann , meta = meta , ** kwargs ) return fig , axs
8912	def includeme ( config ) : settings = config . registry . settings if asbool ( settings . get ( 'twitcher.rpcinterface' , True ) ) : LOGGER . debug ( 'Twitcher XML-RPC Interface enabled.' ) config . include ( 'twitcher.config' ) config . include ( 'twitcher.basicauth' ) config . include ( 'pyramid_rpc.xmlrpc' ) config . include ( 'twitcher.db' ) config . add_xmlrpc_endpoint ( 'api' , '/RPC2' ) config . add_xmlrpc_method ( RPCInterface , attr = 'generate_token' , endpoint = 'api' , method = 'generate_token' ) config . add_xmlrpc_method ( RPCInterface , attr = 'revoke_token' , endpoint = 'api' , method = 'revoke_token' ) config . add_xmlrpc_method ( RPCInterface , attr = 'revoke_all_tokens' , endpoint = 'api' , method = 'revoke_all_tokens' ) config . add_xmlrpc_method ( RPCInterface , attr = 'register_service' , endpoint = 'api' , method = 'register_service' ) config . add_xmlrpc_method ( RPCInterface , attr = 'unregister_service' , endpoint = 'api' , method = 'unregister_service' ) config . add_xmlrpc_method ( RPCInterface , attr = 'get_service_by_name' , endpoint = 'api' , method = 'get_service_by_name' ) config . add_xmlrpc_method ( RPCInterface , attr = 'get_service_by_url' , endpoint = 'api' , method = 'get_service_by_url' ) config . add_xmlrpc_method ( RPCInterface , attr = 'clear_services' , endpoint = 'api' , method = 'clear_services' ) config . add_xmlrpc_method ( RPCInterface , attr = 'list_services' , endpoint = 'api' , method = 'list_services' )
12262	def add ( self , operator , * args ) : if isinstance ( operator , str ) : op = getattr ( proxops , operator ) ( * args ) elif isinstance ( operator , proxops . ProximalOperatorBaseClass ) : op = operator else : raise ValueError ( "operator must be a string or a subclass of ProximalOperator" ) self . operators . append ( op ) return self
1994	def save_state ( self , state , state_id = None ) : assert isinstance ( state , StateBase ) if state_id is None : state_id = self . _get_id ( ) else : self . rm_state ( state_id ) self . _store . save_state ( state , f'{self._prefix}{state_id:08x}{self._suffix}' ) return state_id
1757	def write_int ( self , where , expression , size = None , force = False ) : if size is None : size = self . address_bit_size assert size in SANE_SIZES self . _publish ( 'will_write_memory' , where , expression , size ) data = [ Operators . CHR ( Operators . EXTRACT ( expression , offset , 8 ) ) for offset in range ( 0 , size , 8 ) ] self . _memory . write ( where , data , force ) self . _publish ( 'did_write_memory' , where , expression , size )
8455	def _apply_template ( template , target , * , checkout , extra_context ) : with tempfile . TemporaryDirectory ( ) as tempdir : repo_dir = cc_main . cookiecutter ( template , checkout = checkout , no_input = True , output_dir = tempdir , extra_context = extra_context ) for item in os . listdir ( repo_dir ) : src = os . path . join ( repo_dir , item ) dst = os . path . join ( target , item ) if os . path . isdir ( src ) : if os . path . exists ( dst ) : shutil . rmtree ( dst ) shutil . copytree ( src , dst ) else : if os . path . exists ( dst ) : os . remove ( dst ) shutil . copy2 ( src , dst )
4181	def window_nuttall ( N ) : r a0 = 0.355768 a1 = 0.487396 a2 = 0.144232 a3 = 0.012604 return _coeff4 ( N , a0 , a1 , a2 , a3 )
8214	def gtk_mouse_button_down ( self , widget , event ) : if self . menu_enabled and event . button == 3 : menu = self . uimanager . get_widget ( '/Save as' ) menu . popup ( None , None , None , None , event . button , event . time ) else : super ( ShoebotWindow , self ) . gtk_mouse_button_down ( widget , event )
12005	def _remove_header ( self , data , options ) : version_info = self . _get_version_info ( options [ 'version' ] ) header_size = version_info [ 'header_size' ] if options [ 'flags' ] [ 'timestamp' ] : header_size += version_info [ 'timestamp_size' ] data = data [ header_size : ] return data
4489	def might_need_auth ( f ) : @ wraps ( f ) def wrapper ( cli_args ) : try : return_value = f ( cli_args ) except UnauthorizedException as e : config = config_from_env ( config_from_file ( ) ) username = _get_username ( cli_args , config ) if username is None : sys . exit ( "Please set a username (run `osf -h` for details)." ) else : sys . exit ( "You are not authorized to access this project." ) return return_value return wrapper
9828	def write ( self , file , optstring = "" , quote = False ) : classid = str ( self . id ) if quote : classid = '"' + classid + '"' file . write ( 'object ' + classid + ' class ' + str ( self . name ) + ' ' + optstring + '\n' )
1487	def _load_class ( cls , d ) : for k , v in d . items ( ) : if isinstance ( k , tuple ) : typ , k = k if typ == 'property' : v = property ( * v ) elif typ == 'staticmethod' : v = staticmethod ( v ) elif typ == 'classmethod' : v = classmethod ( v ) setattr ( cls , k , v ) return cls
10229	def summarize_stability ( graph : BELGraph ) -> Mapping [ str , int ] : regulatory_pairs = get_regulatory_pairs ( graph ) chaotic_pairs = get_chaotic_pairs ( graph ) dampened_pairs = get_dampened_pairs ( graph ) contraditory_pairs = get_contradiction_summary ( graph ) separately_unstable_triples = get_separate_unstable_correlation_triples ( graph ) mutually_unstable_triples = get_mutually_unstable_correlation_triples ( graph ) jens_unstable_triples = get_jens_unstable ( graph ) increase_mismatch_triples = get_increase_mismatch_triplets ( graph ) decrease_mismatch_triples = get_decrease_mismatch_triplets ( graph ) chaotic_triples = get_chaotic_triplets ( graph ) dampened_triples = get_dampened_triplets ( graph ) return { 'Regulatory Pairs' : _count_or_len ( regulatory_pairs ) , 'Chaotic Pairs' : _count_or_len ( chaotic_pairs ) , 'Dampened Pairs' : _count_or_len ( dampened_pairs ) , 'Contradictory Pairs' : _count_or_len ( contraditory_pairs ) , 'Separately Unstable Triples' : _count_or_len ( separately_unstable_triples ) , 'Mutually Unstable Triples' : _count_or_len ( mutually_unstable_triples ) , 'Jens Unstable Triples' : _count_or_len ( jens_unstable_triples ) , 'Increase Mismatch Triples' : _count_or_len ( increase_mismatch_triples ) , 'Decrease Mismatch Triples' : _count_or_len ( decrease_mismatch_triples ) , 'Chaotic Triples' : _count_or_len ( chaotic_triples ) , 'Dampened Triples' : _count_or_len ( dampened_triples ) }
8100	def apply ( self ) : sorted = self . order + self . keys ( ) unique = [ ] [ unique . append ( x ) for x in sorted if x not in unique ] for node in self . graph . nodes : for s in unique : if self . has_key ( s ) and self [ s ] ( self . graph , node ) : node . style = s
13190	def json_doc_to_xml ( json_obj , lang = 'en' , custom_namespace = None ) : if 'meta' not in json_obj : raise Exception ( "This function requires a conforming Open511 JSON document with a 'meta' section." ) json_obj = dict ( json_obj ) meta = json_obj . pop ( 'meta' ) elem = get_base_open511_element ( lang = lang , version = meta . pop ( 'version' ) ) pagination = json_obj . pop ( 'pagination' , None ) json_struct_to_xml ( json_obj , elem , custom_namespace = custom_namespace ) if pagination : elem . append ( json_struct_to_xml ( pagination , 'pagination' , custom_namespace = custom_namespace ) ) json_struct_to_xml ( meta , elem ) return elem
2677	def _install_packages ( path , packages ) : def _filter_blacklist ( package ) : blacklist = [ '-i' , '#' , 'Python==' , 'python-lambda==' ] return all ( package . startswith ( entry ) is False for entry in blacklist ) filtered_packages = filter ( _filter_blacklist , packages ) for package in filtered_packages : if package . startswith ( '-e ' ) : package = package . replace ( '-e ' , '' ) print ( 'Installing {package}' . format ( package = package ) ) subprocess . check_call ( [ sys . executable , '-m' , 'pip' , 'install' , package , '-t' , path , '--ignore-installed' ] ) print ( 'Install directory contents are now: {directory}' . format ( directory = os . listdir ( path ) ) )
10773	def add_node ( self , node , offset ) : width = self . end [ 0 ] - self . start [ 0 ] height = self . end [ 1 ] - self . start [ 1 ] node . x = self . start [ 0 ] + ( width * offset ) node . y = self . start [ 1 ] + ( height * offset ) self . nodes [ node . ID ] = node
3845	def to_participantid ( user_id ) : return hangouts_pb2 . ParticipantId ( chat_id = user_id . chat_id , gaia_id = user_id . gaia_id )
2499	def create_doc ( self ) : doc_node = URIRef ( 'http://www.spdx.org/tools#SPDXRef-DOCUMENT' ) self . graph . add ( ( doc_node , RDF . type , self . spdx_namespace . SpdxDocument ) ) vers_literal = Literal ( str ( self . document . version ) ) self . graph . add ( ( doc_node , self . spdx_namespace . specVersion , vers_literal ) ) data_lics = URIRef ( self . document . data_license . url ) self . graph . add ( ( doc_node , self . spdx_namespace . dataLicense , data_lics ) ) doc_name = URIRef ( self . document . name ) self . graph . add ( ( doc_node , self . spdx_namespace . name , doc_name ) ) return doc_node
10542	def find_tasks ( project_id , ** kwargs ) : try : kwargs [ 'project_id' ] = project_id res = _pybossa_req ( 'get' , 'task' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ Task ( task ) for task in res ] else : return res except : raise
9059	def gradient ( self ) : L = self . L n = self . L . shape [ 0 ] grad = { "Lu" : zeros ( ( n , n , n * self . _L . shape [ 1 ] ) ) } for ii in range ( self . _L . shape [ 0 ] * self . _L . shape [ 1 ] ) : row = ii // self . _L . shape [ 1 ] col = ii % self . _L . shape [ 1 ] grad [ "Lu" ] [ row , : , ii ] = L [ : , col ] grad [ "Lu" ] [ : , row , ii ] += L [ : , col ] return grad
1373	def defaults_cluster_role_env ( cluster_role_env ) : if len ( cluster_role_env [ 1 ] ) == 0 and len ( cluster_role_env [ 2 ] ) == 0 : return ( cluster_role_env [ 0 ] , getpass . getuser ( ) , ENVIRON ) return ( cluster_role_env [ 0 ] , cluster_role_env [ 1 ] , cluster_role_env [ 2 ] )
3007	def _redirect_with_params ( url_name , * args , ** kwargs ) : url = urlresolvers . reverse ( url_name , args = args ) params = parse . urlencode ( kwargs , True ) return "{0}?{1}" . format ( url , params )
11605	def convert_ranges ( cls , ranges , length ) : result = [ ] for start , end in ranges : if end is None : result . append ( ( start , length - 1 ) ) elif start is None : s = length - end result . append ( ( 0 if s < 0 else s , length - 1 ) ) else : result . append ( ( start , end if end < length else length - 1 ) ) return result
8445	def ls ( github_user , template , long_format ) : github_urls = temple . ls . ls ( github_user , template = template ) for ssh_path , info in github_urls . items ( ) : if long_format : print ( ssh_path , '-' , info [ 'description' ] or '(no project description found)' ) else : print ( ssh_path )
8048	def _parse_from_import_names ( self , is_future_import ) : if self . current . value == "(" : self . consume ( tk . OP ) expected_end_kinds = ( tk . OP , ) else : expected_end_kinds = ( tk . NEWLINE , tk . ENDMARKER ) while self . current . kind not in expected_end_kinds and not ( self . current . kind == tk . OP and self . current . value == ";" ) : if self . current . kind != tk . NAME : self . stream . move ( ) continue self . log . debug ( "parsing import, token is %r (%s)" , self . current . kind , self . current . value , ) if is_future_import : self . log . debug ( "found future import: %s" , self . current . value ) self . future_imports . add ( self . current . value ) self . consume ( tk . NAME ) self . log . debug ( "parsing import, token is %r (%s)" , self . current . kind , self . current . value , ) if self . current . kind == tk . NAME and self . current . value == "as" : self . consume ( tk . NAME ) if self . current . kind == tk . NAME : self . consume ( tk . NAME ) if self . current . value == "," : self . consume ( tk . OP ) self . log . debug ( "parsing import, token is %r (%s)" , self . current . kind , self . current . value , )
13565	def register ( app ) : error_handler = json . http_exception_error_handler @ app . errorhandler ( 400 ) def handle_bad_request ( exception ) : return error_handler ( exception ) @ app . errorhandler ( 404 ) def handle_not_found ( exception ) : return error_handler ( exception ) @ app . errorhandler ( 405 ) def handle_method_not_allowed ( exception ) : return error_handler ( exception ) @ app . errorhandler ( 422 ) def handle_unprocessable_entity ( exception ) : return error_handler ( exception ) @ app . errorhandler ( 500 ) def handle_internal_server_error ( exception ) : return error_handler ( exception )
5702	def route_frequencies ( gtfs , results_by_mode = False ) : day = gtfs . get_suitable_date_for_daily_extract ( ) query = ( " SELECT f.route_I, type, frequency FROM routes as r" " JOIN" " (SELECT route_I, COUNT(route_I) as frequency" " FROM" " (SELECT date, route_I, trip_I" " FROM day_stop_times" " WHERE date = '{day}'" " GROUP by route_I, trip_I)" " GROUP BY route_I) as f" " ON f.route_I = r.route_I" " ORDER BY frequency DESC" . format ( day = day ) ) return pd . DataFrame ( gtfs . execute_custom_query_pandas ( query ) )
5991	def weighted_regularization_matrix_from_pixel_neighbors ( regularization_weights , pixel_neighbors , pixel_neighbors_size ) : pixels = len ( regularization_weights ) regularization_matrix = np . zeros ( shape = ( pixels , pixels ) ) regularization_weight = regularization_weights ** 2.0 for i in range ( pixels ) : for j in range ( pixel_neighbors_size [ i ] ) : neighbor_index = pixel_neighbors [ i , j ] regularization_matrix [ i , i ] += regularization_weight [ neighbor_index ] regularization_matrix [ neighbor_index , neighbor_index ] += regularization_weight [ neighbor_index ] regularization_matrix [ i , neighbor_index ] -= regularization_weight [ neighbor_index ] regularization_matrix [ neighbor_index , i ] -= regularization_weight [ neighbor_index ] return regularization_matrix
10122	def _kwargs ( self ) : return dict ( color = self . color , velocity = self . velocity , colors = self . colors )
9712	def heappushpop_max ( heap , item ) : if heap and heap [ 0 ] > item : item , heap [ 0 ] = heap [ 0 ] , item _siftup_max ( heap , 0 ) return item
8518	def fromdict ( cls , config , check_fields = True ) : m = super ( Config , cls ) . __new__ ( cls ) m . path = '.' m . verbose = False m . config = m . _merge_defaults ( config ) if check_fields : m . _check_fields ( ) return m
3271	def resolution_millis ( self ) : if self . resolution is None or not isinstance ( self . resolution , basestring ) : return self . resolution val , mult = self . resolution . split ( ' ' ) return int ( float ( val ) * self . _multipier ( mult ) * 1000 )
3545	def _update_advertised ( self , advertised ) : if 'kCBAdvDataServiceUUIDs' in advertised : self . _advertised = self . _advertised + map ( cbuuid_to_uuid , advertised [ 'kCBAdvDataServiceUUIDs' ] )
11724	def init_config ( self , app ) : config_apps = [ 'APP_' , 'RATELIMIT_' ] flask_talisman_debug_mode = [ "'unsafe-inline'" ] for k in dir ( config ) : if any ( [ k . startswith ( prefix ) for prefix in config_apps ] ) : app . config . setdefault ( k , getattr ( config , k ) ) if app . config [ 'DEBUG' ] : app . config . setdefault ( 'APP_DEFAULT_SECURE_HEADERS' , { } ) headers = app . config [ 'APP_DEFAULT_SECURE_HEADERS' ] if headers . get ( 'content_security_policy' ) != { } : headers . setdefault ( 'content_security_policy' , { } ) csp = headers [ 'content_security_policy' ] if csp . get ( 'default-src' ) != [ ] : csp . setdefault ( 'default-src' , [ ] ) csp [ 'default-src' ] += flask_talisman_debug_mode
2595	def use_pickle ( ) : from . import serialize serialize . pickle = serialize . _stdlib_pickle can_map [ FunctionType ] = _original_can_map [ FunctionType ]
7433	def _read_sample_names ( fname ) : try : with open ( fname , 'r' ) as infile : subsamples = [ x . split ( ) [ 0 ] for x in infile . readlines ( ) if x . strip ( ) ] except Exception as inst : print ( "Failed to read input file with sample names.\n{}" . format ( inst ) ) raise inst return subsamples
9318	def wait_until_final ( self , poll_interval = 1 , timeout = 60 ) : start_time = time . time ( ) elapsed = 0 while ( self . status != "complete" and ( timeout <= 0 or elapsed < timeout ) ) : time . sleep ( poll_interval ) self . refresh ( ) elapsed = time . time ( ) - start_time
10731	def lower ( option , value ) : if type ( option ) is str : option = option . lower ( ) if type ( value ) is str : value = value . lower ( ) return ( option , value )
2184	def clear ( self , cfgstr = None ) : data_fpath = self . get_fpath ( cfgstr ) if self . verbose > 0 : self . log ( '[cacher] clear cache' ) if exists ( data_fpath ) : if self . verbose > 0 : self . log ( '[cacher] removing {}' . format ( data_fpath ) ) os . remove ( data_fpath ) meta_fpath = data_fpath + '.meta' if exists ( meta_fpath ) : os . remove ( meta_fpath ) else : if self . verbose > 0 : self . log ( '[cacher] ... nothing to clear' )
4868	def to_representation ( self , instance ) : updated_course_run = copy . deepcopy ( instance ) enterprise_customer_catalog = self . context [ 'enterprise_customer_catalog' ] updated_course_run [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_run_enrollment_url ( updated_course_run [ 'key' ] ) return updated_course_run
7609	def get_all_locations ( self , timeout : int = None ) : url = self . api . LOCATIONS return self . _get_model ( url , timeout = timeout )
12156	def list_move_to_back ( l , value = 'other' ) : l = list ( l ) if value in l : l . remove ( value ) l . append ( value ) return l
10000	def clear_obj ( self , obj ) : obj_nodes = self . get_nodes_with ( obj ) removed = set ( ) for node in obj_nodes : if self . has_node ( node ) : removed . update ( self . clear_descendants ( node ) ) return removed
13569	def selected_exercise ( func ) : @ wraps ( func ) def inner ( * args , ** kwargs ) : exercise = Exercise . get_selected ( ) return func ( exercise , * args , ** kwargs ) return inner
7205	def savedata ( self , output , location = None ) : output . persist = True if location : output . persist_location = location
8524	def add_enum ( self , name , choices ) : if not isinstance ( choices , Iterable ) : raise ValueError ( 'variable %s: choices must be iterable' % name ) self . variables [ name ] = EnumVariable ( name , choices )
997	def printState ( self , aState ) : def formatRow ( var , i ) : s = '' for c in range ( self . numberOfCols ) : if c > 0 and c % 10 == 0 : s += ' ' s += str ( var [ c , i ] ) s += ' ' return s for i in xrange ( self . cellsPerColumn ) : print formatRow ( aState , i )
7664	def to_samples ( self , times , confidence = False ) : times = np . asarray ( times ) if times . ndim != 1 or np . any ( times < 0 ) : raise ParameterError ( 'times must be 1-dimensional and non-negative' ) idx = np . argsort ( times ) samples = times [ idx ] values = [ list ( ) for _ in samples ] confidences = [ list ( ) for _ in samples ] for obs in self . data : start = np . searchsorted ( samples , obs . time ) end = np . searchsorted ( samples , obs . time + obs . duration , side = 'right' ) for i in range ( start , end ) : values [ idx [ i ] ] . append ( obs . value ) confidences [ idx [ i ] ] . append ( obs . confidence ) if confidence : return values , confidences else : return values
10678	def H ( self , T ) : result = 0.0 if T < self . Tmax : lT = T else : lT = self . Tmax Tref = self . Tmin for c , e in zip ( self . _coefficients , self . _exponents ) : if e == - 1.0 : result += c * math . log ( lT / Tref ) else : result += c * ( lT ** ( e + 1.0 ) - Tref ** ( e + 1.0 ) ) / ( e + 1.0 ) return result
12032	def average ( self , t1 = 0 , t2 = None , setsweep = False ) : if setsweep : self . setsweep ( setsweep ) if t2 is None or t2 > self . sweepLength : t2 = self . sweepLength self . log . debug ( "resetting t2 to [%f]" , t2 ) t1 = max ( t1 , 0 ) if t1 > t2 : self . log . error ( "t1 cannot be larger than t2" ) return False I1 , I2 = int ( t1 * self . pointsPerSec ) , int ( t2 * self . pointsPerSec ) if I1 == I2 : return np . nan return np . average ( self . sweepY [ I1 : I2 ] )
8015	async def dispatch_downstream ( self , message , steam_name ) : handler = getattr ( self , get_handler_name ( message ) , None ) if handler : await handler ( message , stream_name = steam_name ) else : await self . base_send ( message )
6485	def do_search ( request , course_id = None ) : SearchInitializer . set_search_enviroment ( request = request , course_id = course_id ) results = { "error" : _ ( "Nothing to search" ) } status_code = 500 search_term = request . POST . get ( "search_string" , None ) try : if not search_term : raise ValueError ( _ ( 'No search term provided for search' ) ) size , from_ , page = _process_pagination_values ( request ) track . emit ( 'edx.course.search.initiated' , { "search_term" : search_term , "page_size" : size , "page_number" : page , } ) results = perform_search ( search_term , user = request . user , size = size , from_ = from_ , course_id = course_id ) status_code = 200 track . emit ( 'edx.course.search.results_displayed' , { "search_term" : search_term , "page_size" : size , "page_number" : page , "results_count" : results [ "total" ] , } ) except ValueError as invalid_err : results = { "error" : six . text_type ( invalid_err ) } log . debug ( six . text_type ( invalid_err ) ) except QueryParseError : results = { "error" : _ ( 'Your query seems malformed. Check for unmatched quotes.' ) } except Exception as err : results = { "error" : _ ( 'An error occurred when searching for "{search_string}"' ) . format ( search_string = search_term ) } log . exception ( 'Search view exception when searching for %s for user %s: %r' , search_term , request . user . id , err ) return JsonResponse ( results , status = status_code )
10714	def _setRTSDTR ( port , RTS , DTR ) : port . setRTS ( RTS ) port . setDTR ( DTR )
13826	def FromJsonString ( self , value ) : self . Clear ( ) for path in value . split ( ',' ) : self . paths . append ( path )
4718	def tsuite_enter ( trun , tsuite ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:enter { name: %r }" % tsuite [ "name" ] ) rcode = 0 for hook in tsuite [ "hooks" ] [ "enter" ] : rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:enter { rcode: %r } " % rcode , rcode ) return rcode
12648	def filter_objlist ( olist , fieldname , fieldval ) : return [ x for x in olist if getattr ( x , fieldname ) == fieldval ]
8290	def _description ( self ) : meta = self . find ( "meta" , { "name" : "description" } ) if isinstance ( meta , dict ) and meta . has_key ( "content" ) : return meta [ "content" ] else : return u""
6118	def circular_annular ( cls , shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , centre = ( 0. , 0. ) , invert = False ) : mask = mask_util . mask_circular_annular_from_shape_pixel_scale_and_radii ( shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
12558	def drain_rois ( img ) : img_data = get_img_data ( img ) out = np . zeros ( img_data . shape , dtype = img_data . dtype ) krn_dim = [ 3 ] * img_data . ndim kernel = np . ones ( krn_dim , dtype = int ) vals = np . unique ( img_data ) vals = vals [ vals != 0 ] for i in vals : roi = img_data == i hits = scn . binary_hit_or_miss ( roi , kernel ) roi [ hits ] = 0 out [ roi > 0 ] = i return out
3087	def _get_entity ( self ) : if self . _is_ndb ( ) : return self . _model . get_by_id ( self . _key_name ) else : return self . _model . get_by_key_name ( self . _key_name )
11807	def encode ( plaintext , code ) : "Encodes text, using a code which is a permutation of the alphabet." from string import maketrans trans = maketrans ( alphabet + alphabet . upper ( ) , code + code . upper ( ) ) return plaintext . translate ( trans )
539	def _finalize ( self ) : self . _logger . info ( "Finished: modelID=%r; %r records processed. Performing final activities" , self . _modelID , self . _currentRecordIndex + 1 ) self . _updateModelDBResults ( ) if not self . _isKilled : self . __updateJobResults ( ) else : self . __deleteOutputCache ( self . _modelID ) if self . _predictionLogger : self . _predictionLogger . close ( ) if self . _inputSource : self . _inputSource . close ( )
12423	def load ( fp , separator = DEFAULT , index_separator = DEFAULT , cls = dict , list_cls = list ) : converter = None output = cls ( ) arraykeys = set ( ) for line in fp : if converter is None : if isinstance ( line , six . text_type ) : converter = six . u else : converter = six . b default_separator = converter ( '|' ) default_index_separator = converter ( '_' ) newline = converter ( '\n' ) if separator is DEFAULT : separator = default_separator if index_separator is DEFAULT : index_separator = default_index_separator key , value = line . strip ( ) . split ( separator , 1 ) keyparts = key . split ( index_separator ) try : index = int ( keyparts [ - 1 ] ) endwithint = True except ValueError : endwithint = False if len ( keyparts ) > 1 and endwithint : basekey = key . rsplit ( index_separator , 1 ) [ 0 ] if basekey not in arraykeys : arraykeys . add ( basekey ) if basekey in output : if not isinstance ( output [ basekey ] , dict ) : output [ basekey ] = { - 1 : output [ basekey ] } else : output [ basekey ] = { } output [ basekey ] [ index ] = value else : if key in output and isinstance ( output [ key ] , dict ) : output [ key ] [ - 1 ] = value else : output [ key ] = value for key in arraykeys : output [ key ] = list_cls ( pair [ 1 ] for pair in sorted ( six . iteritems ( output [ key ] ) ) ) return output
9108	def cleanup ( self ) : try : remove ( join ( self . fs_path , u'message' ) ) remove ( join ( self . fs_path , 'dirty.zip.pgp' ) ) except OSError : pass shutil . rmtree ( join ( self . fs_path , u'clean' ) , ignore_errors = True ) shutil . rmtree ( join ( self . fs_path , u'attach' ) , ignore_errors = True )
350	def load_flickr25k_dataset ( tag = 'sky' , path = "data" , n_threads = 50 , printable = False ) : path = os . path . join ( path , 'flickr25k' ) filename = 'mirflickr25k.zip' url = 'http://press.liacs.nl/mirflickr/mirflickr25k/' if folder_exists ( os . path . join ( path , "mirflickr" ) ) is False : logging . info ( "[*] Flickr25k is nonexistent in {}" . format ( path ) ) maybe_download_and_extract ( filename , path , url , extract = True ) del_file ( os . path . join ( path , filename ) ) folder_imgs = os . path . join ( path , "mirflickr" ) path_imgs = load_file_list ( path = folder_imgs , regx = '\\.jpg' , printable = False ) path_imgs . sort ( key = natural_keys ) folder_tags = os . path . join ( path , "mirflickr" , "meta" , "tags" ) path_tags = load_file_list ( path = folder_tags , regx = '\\.txt' , printable = False ) path_tags . sort ( key = natural_keys ) if tag is None : logging . info ( "[Flickr25k] reading all images" ) else : logging . info ( "[Flickr25k] reading images with tag: {}" . format ( tag ) ) images_list = [ ] for idx , _v in enumerate ( path_tags ) : tags = read_file ( os . path . join ( folder_tags , path_tags [ idx ] ) ) . split ( '\n' ) if tag is None or tag in tags : images_list . append ( path_imgs [ idx ] ) images = visualize . read_images ( images_list , folder_imgs , n_threads = n_threads , printable = printable ) return images
10139	def encrypt_files ( selected_host , only_link , file_name ) : if ENCRYPTION_DISABLED : print ( 'For encryption please install gpg' ) exit ( ) passphrase = '%030x' % random . randrange ( 16 ** 30 ) source_filename = file_name cmd = 'gpg --batch --symmetric --cipher-algo AES256 --passphrase-fd 0 ' '--output - {}' . format ( source_filename ) encrypted_output = Popen ( shlex . split ( cmd ) , stdout = PIPE , stdin = PIPE , stderr = PIPE ) encrypted_data = encrypted_output . communicate ( passphrase . encode ( ) ) [ 0 ] return upload_files ( encrypted_data , selected_host , only_link , file_name ) + '#' + passphrase
8098	def create ( self , stylename , ** kwargs ) : if stylename == "default" : self [ stylename ] = style ( stylename , self . _ctx , ** kwargs ) return self [ stylename ] k = kwargs . get ( "template" , "default" ) s = self [ stylename ] = self [ k ] . copy ( stylename ) for attr in kwargs : if s . __dict__ . has_key ( attr ) : s . __dict__ [ attr ] = kwargs [ attr ] return s
10456	def verifycheck ( self , window_name , object_name ) : try : object_handle = self . _get_object_handle ( window_name , object_name , wait_for_object = False ) if object_handle . AXValue == 1 : return 1 except LdtpServerException : pass return 0
550	def __checkCancelation ( self ) : print >> sys . stderr , "reporter:counter:HypersearchWorker,numRecords,50" jobCancel = self . _jobsDAO . jobGetFields ( self . _jobID , [ 'cancel' ] ) [ 0 ] if jobCancel : self . _cmpReason = ClientJobsDAO . CMPL_REASON_KILLED self . _isCanceled = True self . _logger . info ( "Model %s canceled because Job %s was stopped." , self . _modelID , self . _jobID ) else : stopReason = self . _jobsDAO . modelsGetFields ( self . _modelID , [ 'engStop' ] ) [ 0 ] if stopReason is None : pass elif stopReason == ClientJobsDAO . STOP_REASON_KILLED : self . _cmpReason = ClientJobsDAO . CMPL_REASON_KILLED self . _isKilled = True self . _logger . info ( "Model %s canceled because it was killed by hypersearch" , self . _modelID ) elif stopReason == ClientJobsDAO . STOP_REASON_STOPPED : self . _cmpReason = ClientJobsDAO . CMPL_REASON_STOPPED self . _isCanceled = True self . _logger . info ( "Model %s stopped because hypersearch ended" , self . _modelID ) else : raise RuntimeError ( "Unexpected stop reason encountered: %s" % ( stopReason ) )
12063	def getAvgBySweep ( abf , feature , T0 = None , T1 = None ) : if T1 is None : T1 = abf . sweepLength if T0 is None : T0 = 0 data = [ np . empty ( ( 0 ) ) ] * abf . sweeps for AP in cm . dictFlat ( cm . matrixToDicts ( abf . APs ) ) : if T0 < AP [ 'sweepT' ] < T1 : val = AP [ feature ] data [ int ( AP [ 'sweep' ] ) ] = np . concatenate ( ( data [ int ( AP [ 'sweep' ] ) ] , [ val ] ) ) for sweep in range ( abf . sweeps ) : if len ( data [ sweep ] ) > 1 and np . any ( data [ sweep ] ) : data [ sweep ] = np . nanmean ( data [ sweep ] ) elif len ( data [ sweep ] ) == 1 : data [ sweep ] = data [ sweep ] [ 0 ] else : data [ sweep ] = np . nan return data
8995	def relative_file ( self , module , file ) : path = self . _relative_to_absolute ( module , file ) return self . path ( path )
4562	def recurse ( desc , pre = 'pre_recursion' , post = None , python_path = None ) : def call ( f , desc ) : if isinstance ( f , str ) : f = getattr ( datatype , f , None ) return f and f ( desc ) desc = load . load_if_filename ( desc ) or desc desc = construct . to_type_constructor ( desc , python_path ) datatype = desc . get ( 'datatype' ) desc = call ( pre , desc ) or desc for child_name in getattr ( datatype , 'CHILDREN' , [ ] ) : child = desc . get ( child_name ) if child : is_plural = child_name . endswith ( 's' ) remove_s = is_plural and child_name != 'drivers' cname = child_name [ : - 1 ] if remove_s else child_name new_path = python_path or ( 'bibliopixel.' + cname ) if is_plural : if isinstance ( child , ( dict , str ) ) : child = [ child ] for i , c in enumerate ( child ) : child [ i ] = recurse ( c , pre , post , new_path ) desc [ child_name ] = child else : desc [ child_name ] = recurse ( child , pre , post , new_path ) d = call ( post , desc ) return desc if d is None else d
3669	def Rachford_Rice_flash_error ( V_over_F , zs , Ks ) : r return sum ( [ zi * ( Ki - 1. ) / ( 1. + V_over_F * ( Ki - 1. ) ) for Ki , zi in zip ( Ks , zs ) ] )
2326	def orient_graph ( self , df_data , graph , nb_runs = 6 , printout = None , ** kwargs ) : if type ( graph ) == nx . DiGraph : edges = [ a for a in list ( graph . edges ( ) ) if ( a [ 1 ] , a [ 0 ] ) in list ( graph . edges ( ) ) ] oriented_edges = [ a for a in list ( graph . edges ( ) ) if ( a [ 1 ] , a [ 0 ] ) not in list ( graph . edges ( ) ) ] for a in edges : if ( a [ 1 ] , a [ 0 ] ) in list ( graph . edges ( ) ) : edges . remove ( a ) output = nx . DiGraph ( ) for i in oriented_edges : output . add_edge ( * i ) elif type ( graph ) == nx . Graph : edges = list ( graph . edges ( ) ) output = nx . DiGraph ( ) else : raise TypeError ( "Data type not understood." ) res = [ ] for idx , ( a , b ) in enumerate ( edges ) : weight = self . predict_proba ( df_data [ a ] . values . reshape ( ( - 1 , 1 ) ) , df_data [ b ] . values . reshape ( ( - 1 , 1 ) ) , idx = idx , nb_runs = nb_runs , ** kwargs ) if weight > 0 : output . add_edge ( a , b , weight = weight ) else : output . add_edge ( b , a , weight = abs ( weight ) ) if printout is not None : res . append ( [ str ( a ) + '-' + str ( b ) , weight ] ) DataFrame ( res , columns = [ 'SampleID' , 'Predictions' ] ) . to_csv ( printout , index = False ) for node in list ( df_data . columns . values ) : if node not in output . nodes ( ) : output . add_node ( node ) return output
4580	def toggle ( s ) : is_numeric = ',' in s or s . startswith ( '0x' ) or s . startswith ( '#' ) c = name_to_color ( s ) return color_to_name ( c ) if is_numeric else str ( c )
4335	def oops ( self ) : effect_args = [ 'oops' ] self . effects . extend ( effect_args ) self . effects_log . append ( 'oops' ) return self
11630	def __readNamelist ( cache , filename , unique_glyphs ) : if filename in cache : item = cache [ filename ] else : cps , header , noncodes = parseNamelist ( filename ) item = { "fileName" : filename , "ownCharset" : cps , "header" : header , "ownNoCharcode" : noncodes , "includes" : None , "charset" : None , "noCharcode" : None } cache [ filename ] = item if unique_glyphs or item [ "charset" ] is not None : return item _loadNamelistIncludes ( item , unique_glyphs , cache ) return item
1976	def sys_receive ( self , cpu , fd , buf , count , rx_bytes ) : data = '' if count != 0 : if not self . _is_open ( fd ) : logger . info ( "RECEIVE: Not valid file descriptor on receive. Returning EBADF" ) return Decree . CGC_EBADF if buf not in cpu . memory : logger . info ( "RECEIVE: buf points to invalid address. Returning EFAULT" ) return Decree . CGC_EFAULT if fd > 2 and self . files [ fd ] . is_empty ( ) : cpu . PC -= cpu . instruction . size self . wait ( [ fd ] , [ ] , None ) raise RestartSyscall ( ) data = self . files [ fd ] . receive ( count ) self . syscall_trace . append ( ( "_receive" , fd , data ) ) cpu . write_bytes ( buf , data ) self . signal_receive ( fd ) if rx_bytes : if rx_bytes not in cpu . memory : logger . info ( "RECEIVE: Not valid file descriptor on receive. Returning EFAULT" ) return Decree . CGC_EFAULT cpu . write_int ( rx_bytes , len ( data ) , 32 ) logger . info ( "RECEIVE(%d, 0x%08x, %d, 0x%08x) -> <%s> (size:%d)" % ( fd , buf , count , rx_bytes , repr ( data ) [ : min ( count , 10 ) ] , len ( data ) ) ) return 0
2094	def stdout ( self , pk , start_line = None , end_line = None , outfile = sys . stdout , ** kwargs ) : if self . unified_job_type != self . endpoint : unified_job = self . last_job_data ( pk , ** kwargs ) pk = unified_job [ 'id' ] elif not pk : unified_job = self . get ( ** kwargs ) pk = unified_job [ 'id' ] content = self . lookup_stdout ( pk , start_line , end_line ) opened = False if isinstance ( outfile , six . string_types ) : outfile = open ( outfile , 'w' ) opened = True if len ( content ) > 0 : click . echo ( content , nl = 1 , file = outfile ) if opened : outfile . close ( ) return { "changed" : False }
2179	def fetch_request_token ( self , url , realm = None , ** request_kwargs ) : r self . _client . client . realm = " " . join ( realm ) if realm else None token = self . _fetch_token ( url , ** request_kwargs ) log . debug ( "Resetting callback_uri and realm (not needed in next phase)." ) self . _client . client . callback_uri = None self . _client . client . realm = None return token
8126	def search ( q , start = 1 , count = 10 , context = None , wait = 10 , asynchronous = False , cached = False ) : service = YAHOO_SEARCH return YahooSearch ( q , start , count , service , context , wait , asynchronous , cached )
4655	def verify_authority ( self ) : try : if not self . blockchain . rpc . verify_authority ( self . json ( ) ) : raise InsufficientAuthorityError except Exception as e : raise e
3707	def Amgat ( xs , Vms ) : r if not none_and_length_check ( [ xs , Vms ] ) : raise Exception ( 'Function inputs are incorrect format' ) return mixing_simple ( xs , Vms )
3266	def prepare_upload_bundle ( name , data ) : fd , path = mkstemp ( ) zip_file = ZipFile ( path , 'w' ) for ext , stream in data . items ( ) : fname = "%s.%s" % ( name , ext ) if ( isinstance ( stream , basestring ) ) : zip_file . write ( stream , fname ) else : zip_file . writestr ( fname , stream . read ( ) ) zip_file . close ( ) os . close ( fd ) return path
2337	def remove_indirect_links ( g , alg = "aracne" , ** kwargs ) : alg = { "aracne" : aracne , "nd" : network_deconvolution , "clr" : clr } [ alg ] mat = np . array ( nx . adjacency_matrix ( g ) . todense ( ) ) return nx . relabel_nodes ( nx . DiGraph ( alg ( mat , ** kwargs ) ) , { idx : i for idx , i in enumerate ( list ( g . nodes ( ) ) ) } )
5015	def filter_queryset ( self , request , queryset , view ) : if request . user . is_staff : email = request . query_params . get ( 'email' , None ) username = request . query_params . get ( 'username' , None ) query_parameters = { } if email : query_parameters . update ( email = email ) if username : query_parameters . update ( username = username ) if query_parameters : users = User . objects . filter ( ** query_parameters ) . values_list ( 'id' , flat = True ) queryset = queryset . filter ( user_id__in = users ) else : queryset = queryset . filter ( user_id = request . user . id ) return queryset
5320	def find_ports ( device ) : bus_id = device . bus dev_id = device . address for dirent in os . listdir ( USB_SYS_PREFIX ) : matches = re . match ( USB_PORTS_STR + '$' , dirent ) if matches : bus_str = readattr ( dirent , 'busnum' ) if bus_str : busnum = float ( bus_str ) else : busnum = None dev_str = readattr ( dirent , 'devnum' ) if dev_str : devnum = float ( dev_str ) else : devnum = None if busnum == bus_id and devnum == dev_id : return str ( matches . groups ( ) [ 1 ] )
6531	def get_user_config ( project_path , use_cache = True ) : if sys . platform == 'win32' : user_config = os . path . expanduser ( r'~\\tidypy' ) else : user_config = os . path . join ( os . getenv ( 'XDG_CONFIG_HOME' ) or os . path . expanduser ( '~/.config' ) , 'tidypy' ) if os . path . exists ( user_config ) : with open ( user_config , 'r' ) as config_file : config = pytoml . load ( config_file ) . get ( 'tidypy' , { } ) config = merge_dict ( get_default_config ( ) , config ) config = process_extensions ( config , project_path , use_cache = use_cache ) return config return None
9608	def format_map ( self , format_string , mapping ) : return self . vformat ( format_string , args = None , kwargs = mapping )
2986	def get_cors_options ( appInstance , * dicts ) : options = DEFAULT_OPTIONS . copy ( ) options . update ( get_app_kwarg_dict ( appInstance ) ) if dicts : for d in dicts : options . update ( d ) return serialize_options ( options )
9577	def read_var_header ( fd , endian ) : mtpn , num_bytes = unpack ( endian , 'II' , fd . read ( 8 ) ) next_pos = fd . tell ( ) + num_bytes if mtpn == etypes [ 'miCOMPRESSED' ] [ 'n' ] : data = fd . read ( num_bytes ) dcor = zlib . decompressobj ( ) fd_var = BytesIO ( dcor . decompress ( data ) ) del data fd = fd_var if dcor . flush ( ) != b'' : raise ParseError ( 'Error in compressed data.' ) mtpn , num_bytes = unpack ( endian , 'II' , fd . read ( 8 ) ) if mtpn != etypes [ 'miMATRIX' ] [ 'n' ] : raise ParseError ( 'Expecting miMATRIX type number {}, ' 'got {}' . format ( etypes [ 'miMATRIX' ] [ 'n' ] , mtpn ) ) header = read_header ( fd , endian ) return header , next_pos , fd
9274	def filter_excluded_tags ( self , all_tags ) : filtered_tags = copy . deepcopy ( all_tags ) if self . options . exclude_tags : filtered_tags = self . apply_exclude_tags ( filtered_tags ) if self . options . exclude_tags_regex : filtered_tags = self . apply_exclude_tags_regex ( filtered_tags ) return filtered_tags
5428	def _validate_job_and_task_arguments ( job_params , task_descriptors ) : if not task_descriptors : return task_params = task_descriptors [ 0 ] . task_params from_jobs = { label . name for label in job_params [ 'labels' ] } from_tasks = { label . name for label in task_params [ 'labels' ] } intersect = from_jobs & from_tasks if intersect : raise ValueError ( 'Names for labels on the command-line and in the --tasks file must not ' 'be repeated: {}' . format ( ',' . join ( intersect ) ) ) from_jobs = { item . name for item in job_params [ 'envs' ] | job_params [ 'inputs' ] | job_params [ 'outputs' ] } from_tasks = { item . name for item in task_params [ 'envs' ] | task_params [ 'inputs' ] | task_params [ 'outputs' ] } intersect = from_jobs & from_tasks if intersect : raise ValueError ( 'Names for envs, inputs, and outputs on the command-line and in the ' '--tasks file must not be repeated: {}' . format ( ',' . join ( intersect ) ) )
8139	def desaturate ( self ) : alpha = self . img . split ( ) [ 3 ] self . img = self . img . convert ( "L" ) self . img = self . img . convert ( "RGBA" ) self . img . putalpha ( alpha )
7192	def histogram_equalize ( self , use_bands , ** kwargs ) : data = self . _read ( self [ use_bands , ... ] , ** kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) flattened = data . flatten ( ) if 0 in data : masked = np . ma . masked_values ( data , 0 ) . compressed ( ) image_histogram , bin_edges = np . histogram ( masked , 256 ) else : image_histogram , bin_edges = np . histogram ( flattened , 256 ) bins = ( bin_edges [ : - 1 ] + bin_edges [ 1 : ] ) / 2.0 cdf = image_histogram . cumsum ( ) cdf = cdf / float ( cdf [ - 1 ] ) image_equalized = np . interp ( flattened , bins , cdf ) . reshape ( data . shape ) if 'stretch' in kwargs or 'gamma' in kwargs : return self . _histogram_stretch ( image_equalized , ** kwargs ) else : return image_equalized
12928	def _parse_info ( self , info_field ) : info = dict ( ) for item in info_field . split ( ';' ) : info_item_data = item . split ( '=' ) if len ( info_item_data ) == 1 : info [ info_item_data [ 0 ] ] = True elif len ( info_item_data ) == 2 : info [ info_item_data [ 0 ] ] = info_item_data [ 1 ] return info
4112	def rc2ac ( k , R0 ) : [ a , efinal ] = rc2poly ( k , R0 ) R , u , kr , e = rlevinson ( a , efinal ) return R
10706	def get_vacations ( ) : arequest = requests . get ( VACATIONS_URL , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
3208	def _reformat_policy ( policy ) : policy_name = policy [ 'PolicyName' ] ret = { } ret [ 'type' ] = policy [ 'PolicyTypeName' ] attrs = policy [ 'PolicyAttributeDescriptions' ] if ret [ 'type' ] != 'SSLNegotiationPolicyType' : return policy_name , ret attributes = dict ( ) for attr in attrs : attributes [ attr [ 'AttributeName' ] ] = attr [ 'AttributeValue' ] ret [ 'protocols' ] = dict ( ) ret [ 'protocols' ] [ 'sslv2' ] = bool ( attributes . get ( 'Protocol-SSLv2' ) ) ret [ 'protocols' ] [ 'sslv3' ] = bool ( attributes . get ( 'Protocol-SSLv3' ) ) ret [ 'protocols' ] [ 'tlsv1' ] = bool ( attributes . get ( 'Protocol-TLSv1' ) ) ret [ 'protocols' ] [ 'tlsv1_1' ] = bool ( attributes . get ( 'Protocol-TLSv1.1' ) ) ret [ 'protocols' ] [ 'tlsv1_2' ] = bool ( attributes . get ( 'Protocol-TLSv1.2' ) ) ret [ 'server_defined_cipher_order' ] = bool ( attributes . get ( 'Server-Defined-Cipher-Order' ) ) ret [ 'reference_security_policy' ] = attributes . get ( 'Reference-Security-Policy' , None ) non_ciphers = [ 'Server-Defined-Cipher-Order' , 'Protocol-SSLv2' , 'Protocol-SSLv3' , 'Protocol-TLSv1' , 'Protocol-TLSv1.1' , 'Protocol-TLSv1.2' , 'Reference-Security-Policy' ] ciphers = [ ] for cipher in attributes : if attributes [ cipher ] == 'true' and cipher not in non_ciphers : ciphers . append ( cipher ) ciphers . sort ( ) ret [ 'supported_ciphers' ] = ciphers return policy_name , ret
9016	def instruction_in_row ( self , row , specification ) : whole_instruction_ = self . _as_instruction ( specification ) return self . _spec . new_instruction_in_row ( row , whole_instruction_ )
5464	def get_action_by_id ( op , action_id ) : actions = get_actions ( op ) if actions and 1 <= action_id < len ( actions ) : return actions [ action_id - 1 ]
6306	def load_package ( self ) : try : self . package = importlib . import_module ( self . name ) except ModuleNotFoundError : raise ModuleNotFoundError ( "Effect package '{}' not found." . format ( self . name ) )
9193	def _insert_file ( cursor , file , media_type ) : resource_hash = _get_file_sha1 ( file ) cursor . execute ( "SELECT fileid FROM files WHERE sha1 = %s" , ( resource_hash , ) ) try : fileid = cursor . fetchone ( ) [ 0 ] except ( IndexError , TypeError ) : cursor . execute ( "INSERT INTO files (file, media_type) " "VALUES (%s, %s)" "RETURNING fileid" , ( psycopg2 . Binary ( file . read ( ) ) , media_type , ) ) fileid = cursor . fetchone ( ) [ 0 ] return fileid , resource_hash
9473	def DFS_prefix ( self , root = None ) : if not root : root = self . _root return self . _DFS_prefix ( root )
12311	def record ( self , localStreamName , pathToFile , ** kwargs ) : return self . protocol . execute ( 'record' , localStreamName = localStreamName , pathToFile = pathToFile , ** kwargs )
2738	def assign ( self , droplet_id ) : return self . get_data ( "floating_ips/%s/actions/" % self . ip , type = POST , params = { "type" : "assign" , "droplet_id" : droplet_id } )
6057	def resized_array_2d_from_array_2d_and_resized_shape ( array_2d , resized_shape , origin = ( - 1 , - 1 ) , pad_value = 0.0 ) : y_is_even = int ( array_2d . shape [ 0 ] ) % 2 == 0 x_is_even = int ( array_2d . shape [ 1 ] ) % 2 == 0 if origin is ( - 1 , - 1 ) : if y_is_even : y_centre = int ( array_2d . shape [ 0 ] / 2 ) elif not y_is_even : y_centre = int ( array_2d . shape [ 0 ] / 2 ) if x_is_even : x_centre = int ( array_2d . shape [ 1 ] / 2 ) elif not x_is_even : x_centre = int ( array_2d . shape [ 1 ] / 2 ) origin = ( y_centre , x_centre ) resized_array = np . zeros ( shape = resized_shape ) if y_is_even : y_min = origin [ 0 ] - int ( resized_shape [ 0 ] / 2 ) y_max = origin [ 0 ] + int ( ( resized_shape [ 0 ] / 2 ) ) + 1 elif not y_is_even : y_min = origin [ 0 ] - int ( resized_shape [ 0 ] / 2 ) y_max = origin [ 0 ] + int ( ( resized_shape [ 0 ] / 2 ) ) + 1 if x_is_even : x_min = origin [ 1 ] - int ( resized_shape [ 1 ] / 2 ) x_max = origin [ 1 ] + int ( ( resized_shape [ 1 ] / 2 ) ) + 1 elif not x_is_even : x_min = origin [ 1 ] - int ( resized_shape [ 1 ] / 2 ) x_max = origin [ 1 ] + int ( ( resized_shape [ 1 ] / 2 ) ) + 1 for y_resized , y in enumerate ( range ( y_min , y_max ) ) : for x_resized , x in enumerate ( range ( x_min , x_max ) ) : if y >= 0 and y < array_2d . shape [ 0 ] and x >= 0 and x < array_2d . shape [ 1 ] : if y_resized >= 0 and y_resized < resized_shape [ 0 ] and x_resized >= 0 and x_resized < resized_shape [ 1 ] : resized_array [ y_resized , x_resized ] = array_2d [ y , x ] else : if y_resized >= 0 and y_resized < resized_shape [ 0 ] and x_resized >= 0 and x_resized < resized_shape [ 1 ] : resized_array [ y_resized , x_resized ] = pad_value return resized_array
374	def illumination ( x , gamma = 1. , contrast = 1. , saturation = 1. , is_random = False ) : if is_random : if not ( len ( gamma ) == len ( contrast ) == len ( saturation ) == 2 ) : raise AssertionError ( "if is_random = True, the arguments are (min, max)" ) illum_settings = np . random . randint ( 0 , 3 ) if illum_settings == 0 : gamma = np . random . uniform ( gamma [ 0 ] , 1.0 ) elif illum_settings == 1 : gamma = np . random . uniform ( 1.0 , gamma [ 1 ] ) else : gamma = 1 im_ = brightness ( x , gamma = gamma , gain = 1 , is_random = False ) image = PIL . Image . fromarray ( im_ ) contrast_adjust = PIL . ImageEnhance . Contrast ( image ) image = contrast_adjust . enhance ( np . random . uniform ( contrast [ 0 ] , contrast [ 1 ] ) ) saturation_adjust = PIL . ImageEnhance . Color ( image ) image = saturation_adjust . enhance ( np . random . uniform ( saturation [ 0 ] , saturation [ 1 ] ) ) im_ = np . array ( image ) else : im_ = brightness ( x , gamma = gamma , gain = 1 , is_random = False ) image = PIL . Image . fromarray ( im_ ) contrast_adjust = PIL . ImageEnhance . Contrast ( image ) image = contrast_adjust . enhance ( contrast ) saturation_adjust = PIL . ImageEnhance . Color ( image ) image = saturation_adjust . enhance ( saturation ) im_ = np . array ( image ) return np . asarray ( im_ )
11875	def getpassword ( prompt = "Password: " ) : fd = sys . stdin . fileno ( ) old = termios . tcgetattr ( fd ) new = termios . tcgetattr ( fd ) new [ 3 ] &= ~ termios . ECHO try : termios . tcsetattr ( fd , termios . TCSADRAIN , new ) passwd = raw_input ( prompt ) finally : termios . tcsetattr ( fd , termios . TCSADRAIN , old ) return passwd
10948	def reset ( self , ** kwargs ) : self . aug_state . reset ( ) super ( LMAugmentedState , self ) . reset ( ** kwargs )
11128	def ensure_str ( value ) : if isinstance ( value , six . string_types ) : return value else : return six . text_type ( value )
12351	def restore ( self , image , wait = True ) : return self . _action ( 'restore' , image = image , wait = wait )
12472	def get_extension ( filepath , check_if_exists = False , allowed_exts = ALLOWED_EXTS ) : if check_if_exists : if not op . exists ( filepath ) : raise IOError ( 'File not found: ' + filepath ) rest , ext = op . splitext ( filepath ) if ext in allowed_exts : alloweds = allowed_exts [ ext ] _ , ext2 = op . splitext ( rest ) if ext2 in alloweds : ext = ext2 + ext return ext
6870	def get_snr_of_dip ( times , mags , modeltimes , modelmags , atol_normalization = 1e-8 , indsforrms = None , magsarefluxes = False , verbose = True , transitdepth = None , npoints_in_transit = None ) : if magsarefluxes : if not np . isclose ( np . nanmedian ( modelmags ) , 1 , atol = atol_normalization ) : raise AssertionError ( 'snr calculation assumes modelmags are ' 'median-normalized' ) else : raise NotImplementedError ( 'need to implement a method for identifying in-transit points when' 'mags are mags, and not fluxes' ) if not transitdepth : transitdepth = np . abs ( np . max ( modelmags ) - np . min ( modelmags ) ) if not len ( mags ) == len ( modelmags ) : from scipy . interpolate import interp1d fn = interp1d ( modeltimes , modelmags , kind = 'cubic' , bounds_error = True , fill_value = np . nan ) modelmags = fn ( times ) if verbose : LOGINFO ( 'interpolated model timeseries onto the data timeseries' ) subtractedmags = mags - modelmags if isinstance ( indsforrms , np . ndarray ) : subtractedrms = np . std ( subtractedmags [ indsforrms ] ) if verbose : LOGINFO ( 'using selected points to measure RMS' ) else : subtractedrms = np . std ( subtractedmags ) if verbose : LOGINFO ( 'using all points to measure RMS' ) def _get_npoints_in_transit ( modelmags ) : if np . nanmedian ( modelmags ) == 1 : return len ( modelmags [ ( modelmags != 1 ) ] ) else : raise NotImplementedError if not npoints_in_transit : npoints_in_transit = _get_npoints_in_transit ( modelmags ) snr = np . sqrt ( npoints_in_transit ) * transitdepth / subtractedrms if verbose : LOGINFO ( '\npoints in transit: {:d}' . format ( npoints_in_transit ) + '\ndepth: {:.2e}' . format ( transitdepth ) + '\nrms in residual: {:.2e}' . format ( subtractedrms ) + '\n\t SNR: {:.2e}' . format ( snr ) ) return snr , transitdepth , subtractedrms
4576	def color_cmp ( a , b ) : if a == b : return 0 a , b = rgb_to_hsv ( a ) , rgb_to_hsv ( b ) return - 1 if a < b else 1
5927	def configuration ( self ) : configuration = { 'configfilename' : self . filename , 'logfilename' : self . getpath ( 'Logging' , 'logfilename' ) , 'loglevel_console' : self . getLogLevel ( 'Logging' , 'loglevel_console' ) , 'loglevel_file' : self . getLogLevel ( 'Logging' , 'loglevel_file' ) , 'configdir' : self . getpath ( 'DEFAULT' , 'configdir' ) , 'qscriptdir' : self . getpath ( 'DEFAULT' , 'qscriptdir' ) , 'templatesdir' : self . getpath ( 'DEFAULT' , 'templatesdir' ) , } configuration [ 'path' ] = [ os . path . curdir , configuration [ 'qscriptdir' ] , configuration [ 'templatesdir' ] ] return configuration
13910	def check_path_action ( self ) : class CheckPathAction ( argparse . Action ) : def __call__ ( self , parser , args , value , option_string = None ) : if type ( value ) is list : value = value [ 0 ] user_value = value if option_string == 'None' : if not os . path . isdir ( value ) : _current_user = os . path . expanduser ( "~" ) if not value . startswith ( _current_user ) and not value . startswith ( os . getcwd ( ) ) : if os . path . isdir ( os . path . join ( _current_user , value ) ) : value = os . path . join ( _current_user , value ) elif os . path . isdir ( os . path . join ( os . getcwd ( ) , value ) ) : value = os . path . join ( os . getcwd ( ) , value ) else : value = None else : value = None elif option_string == '--template-name' : if not os . path . isdir ( value ) : if not os . path . isdir ( os . path . join ( args . target , value ) ) : value = None if not value : logger . error ( "Could not to find path %s. Please provide " "correct path to %s option" , user_value , option_string ) exit ( 1 ) setattr ( args , self . dest , value ) return CheckPathAction
1520	def get_remote_home ( host , cl_args ) : cmd = "echo ~" if not is_self ( host ) : cmd = ssh_remote_execute ( cmd , host , cl_args ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) if return_code != 0 : Log . error ( "Failed to get home path for remote host %s with output:\n%s" % ( host , output ) ) sys . exit ( - 1 ) return output [ 0 ] . strip ( "\n" )
10722	def _wrapper ( func ) : @ functools . wraps ( func ) def the_func ( expr ) : try : return func ( expr ) except ( TypeError , ValueError ) as err : raise IntoDPValueError ( expr , "expr" , "could not be transformed" ) from err return the_func
4936	def strfdelta ( tdelta , fmt = '{D:02}d {H:02}h {M:02}m {S:02}s' , input_type = 'timedelta' ) : if input_type == 'timedelta' : remainder = int ( tdelta . total_seconds ( ) ) elif input_type in [ 's' , 'seconds' ] : remainder = int ( tdelta ) elif input_type in [ 'm' , 'minutes' ] : remainder = int ( tdelta ) * 60 elif input_type in [ 'h' , 'hours' ] : remainder = int ( tdelta ) * 3600 elif input_type in [ 'd' , 'days' ] : remainder = int ( tdelta ) * 86400 elif input_type in [ 'w' , 'weeks' ] : remainder = int ( tdelta ) * 604800 else : raise ValueError ( 'input_type is not valid. Valid input_type strings are: "timedelta", "s", "m", "h", "d", "w"' ) f = Formatter ( ) desired_fields = [ field_tuple [ 1 ] for field_tuple in f . parse ( fmt ) ] possible_fields = ( 'W' , 'D' , 'H' , 'M' , 'S' ) constants = { 'W' : 604800 , 'D' : 86400 , 'H' : 3600 , 'M' : 60 , 'S' : 1 } values = { } for field in possible_fields : if field in desired_fields and field in constants : values [ field ] , remainder = divmod ( remainder , constants [ field ] ) return f . format ( fmt , ** values )
6850	def initrole ( self , check = True ) : if self . env . original_user is None : self . env . original_user = self . genv . user if self . env . original_key_filename is None : self . env . original_key_filename = self . genv . key_filename host_string = None user = None password = None if self . env . login_check : host_string , user , password = self . find_working_password ( usernames = [ self . genv . user , self . env . default_user ] , host_strings = [ self . genv . host_string , self . env . default_hostname ] , ) if self . verbose : print ( 'host.initrole.host_string:' , host_string ) print ( 'host.initrole.user:' , user ) print ( 'host.initrole.password:' , password ) needs = False if host_string is not None : self . genv . host_string = host_string if user is not None : self . genv . user = user if password is not None : self . genv . password = password if not needs : return assert self . env . default_hostname , 'No default hostname set.' assert self . env . default_user , 'No default user set.' self . genv . host_string = self . env . default_hostname if self . env . default_hosts : self . genv . hosts = self . env . default_hosts else : self . genv . hosts = [ self . env . default_hostname ] self . genv . user = self . env . default_user self . genv . password = self . env . default_password self . genv . key_filename = self . env . default_key_filename self . purge_keys ( ) for task_name in self . env . post_initrole_tasks : if self . verbose : print ( 'Calling post initrole task %s' % task_name ) satchel_name , method_name = task_name . split ( '.' ) satchel = self . get_satchel ( name = satchel_name ) getattr ( satchel , method_name ) ( ) print ( '^' * 80 ) print ( 'host.initrole.host_string:' , self . genv . host_string ) print ( 'host.initrole.user:' , self . genv . user ) print ( 'host.initrole.password:' , self . genv . password )
1683	def PrintErrorCounts ( self ) : for category , count in sorted ( iteritems ( self . errors_by_category ) ) : self . PrintInfo ( 'Category \'%s\' errors found: %d\n' % ( category , count ) ) if self . error_count > 0 : self . PrintInfo ( 'Total errors found: %d\n' % self . error_count )
7876	def bind ( self , stream , resource ) : self . stream = stream stanza = Iq ( stanza_type = "set" ) payload = ResourceBindingPayload ( resource = resource ) stanza . set_payload ( payload ) self . stanza_processor . set_response_handlers ( stanza , self . _bind_success , self . _bind_error ) stream . send ( stanza ) stream . event ( BindingResourceEvent ( resource ) )
8441	def _parse_link_header ( headers ) : links = { } if 'link' in headers : link_headers = headers [ 'link' ] . split ( ', ' ) for link_header in link_headers : ( url , rel ) = link_header . split ( '; ' ) url = url [ 1 : - 1 ] rel = rel [ 5 : - 1 ] links [ rel ] = url return links
13802	def _auth ( self , client_id , key , method , callback ) : available = auth_methods . keys ( ) if method not in available : raise Proauth2Error ( 'invalid_request' , 'unsupported authentication method: %s' 'available methods: %s' % ( method , '\n' . join ( available ) ) ) client = yield Task ( self . data_store . fetch , 'applications' , client_id = client_id ) if not client : raise Proauth2Error ( 'access_denied' ) if not auth_methods [ method ] ( key , client [ 'client_secret' ] ) : raise Proauth2Error ( 'access_denied' ) callback ( )
7556	def random_combination ( nsets , n , k ) : sets = set ( ) while len ( sets ) < nsets : newset = tuple ( sorted ( np . random . choice ( n , k , replace = False ) ) ) sets . add ( newset ) return tuple ( sets )
5189	def main ( ) : app = MyMaster ( log_handler = MyLogger ( ) , listener = AppChannelListener ( ) , soe_handler = SOEHandler ( ) , master_application = MasterApplication ( ) ) _log . debug ( 'Initialization complete. In command loop.' ) app . shutdown ( ) _log . debug ( 'Exiting.' ) exit ( )
866	def setCustomProperties ( cls , properties ) : _getLogger ( ) . info ( "Setting custom configuration properties=%r; caller=%r" , properties , traceback . format_stack ( ) ) _CustomConfigurationFileWrapper . edit ( properties ) for propertyName , value in properties . iteritems ( ) : cls . set ( propertyName , value )
8676	def migrate_stash ( source_stash_path , source_passphrase , source_backend , destination_stash_path , destination_passphrase , destination_backend ) : click . echo ( 'Migrating all keys from {0} to {1}...' . format ( source_stash_path , destination_stash_path ) ) try : migrate ( src_path = source_stash_path , src_passphrase = source_passphrase , src_backend = source_backend , dst_path = destination_stash_path , dst_passphrase = destination_passphrase , dst_backend = destination_backend ) except GhostError as ex : sys . exit ( ex ) click . echo ( 'Migration complete!' )
13161	def select ( cls , cur , table : str , order_by : str , columns : list = None , where_keys : list = None , limit = 100 , offset = 0 ) : if columns : columns_string = cls . _COMMA . join ( columns ) if where_keys : where_clause , values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _select_selective_column_with_condition . format ( columns_string , table , where_clause , order_by , limit , offset ) q , t = query , values else : query = cls . _select_selective_column . format ( columns_string , table , order_by , limit , offset ) q , t = query , ( ) else : if where_keys : where_clause , values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _select_all_string_with_condition . format ( table , where_clause , order_by , limit , offset ) q , t = query , values else : query = cls . _select_all_string . format ( table , order_by , limit , offset ) q , t = query , ( ) yield from cur . execute ( q , t ) return ( yield from cur . fetchall ( ) )
3550	def list_characteristics ( self ) : paths = self . _props . Get ( _SERVICE_INTERFACE , 'Characteristics' ) return map ( BluezGattCharacteristic , get_provider ( ) . _get_objects_by_path ( paths ) )
7112	def serve ( self , port = 62000 ) : from http . server import HTTPServer , CGIHTTPRequestHandler os . chdir ( self . log_folder ) httpd = HTTPServer ( ( '' , port ) , CGIHTTPRequestHandler ) print ( "Starting LanguageBoard on port: " + str ( httpd . server_port ) ) webbrowser . open ( 'http://0.0.0.0:{}' . format ( port ) ) httpd . serve_forever ( )
4111	def rc2poly ( kr , r0 = None ) : from . levinson import levup p = len ( kr ) a = numpy . array ( [ 1 , kr [ 0 ] ] ) e = numpy . zeros ( len ( kr ) ) if r0 is None : e0 = 0 else : e0 = r0 e [ 0 ] = e0 * ( 1. - numpy . conj ( numpy . conjugate ( kr [ 0 ] ) * kr [ 0 ] ) ) for k in range ( 1 , p ) : [ a , e [ k ] ] = levup ( a , kr [ k ] , e [ k - 1 ] ) efinal = e [ - 1 ] return a , efinal
5420	def _get_job_resources ( args ) : logging = param_util . build_logging_param ( args . logging ) if args . logging else None timeout = param_util . timeout_in_seconds ( args . timeout ) log_interval = param_util . log_interval_in_seconds ( args . log_interval ) return job_model . Resources ( min_cores = args . min_cores , min_ram = args . min_ram , machine_type = args . machine_type , disk_size = args . disk_size , disk_type = args . disk_type , boot_disk_size = args . boot_disk_size , preemptible = args . preemptible , image = args . image , regions = args . regions , zones = args . zones , logging = logging , logging_path = None , service_account = args . service_account , scopes = args . scopes , keep_alive = args . keep_alive , cpu_platform = args . cpu_platform , network = args . network , subnetwork = args . subnetwork , use_private_address = args . use_private_address , accelerator_type = args . accelerator_type , accelerator_count = args . accelerator_count , nvidia_driver_version = args . nvidia_driver_version , timeout = timeout , log_interval = log_interval , ssh = args . ssh )
7905	def forget ( self , rs ) : try : del self . rooms [ rs . room_jid . bare ( ) . as_unicode ( ) ] except KeyError : pass
2245	def hzcat ( args , sep = '' ) : import unicodedata if '\n' in sep or '\r' in sep : raise ValueError ( '`sep` cannot contain newline characters' ) args = [ unicodedata . normalize ( 'NFC' , ensure_unicode ( val ) ) for val in args ] arglines = [ a . split ( '\n' ) for a in args ] height = max ( map ( len , arglines ) ) arglines = [ lines + [ '' ] * ( height - len ( lines ) ) for lines in arglines ] all_lines = [ '' for _ in range ( height ) ] width = 0 n_args = len ( args ) for sx , lines in enumerate ( arglines ) : for lx , line in enumerate ( lines ) : all_lines [ lx ] += line width = max ( width , max ( map ( len , all_lines ) ) ) if sx < n_args - 1 : for lx , line in list ( enumerate ( all_lines ) ) : residual = width - len ( line ) all_lines [ lx ] = line + ( ' ' * residual ) + sep width += len ( sep ) all_lines = [ line . rstrip ( ' ' ) for line in all_lines ] ret = '\n' . join ( all_lines ) return ret
13385	def store_env ( path = None ) : path = path or get_store_env_tmp ( ) env_dict = yaml . safe_dump ( os . environ . data , default_flow_style = False ) with open ( path , 'w' ) as f : f . write ( env_dict ) return path
3807	def nested_formula_parser ( formula , check = True ) : r formula = formula . replace ( '[' , '' ) . replace ( ']' , '' ) charge_splits = bracketed_charge_re . split ( formula ) if len ( charge_splits ) > 1 : formula = charge_splits [ 0 ] else : formula = formula . split ( '+' ) [ 0 ] . split ( '-' ) [ 0 ] stack = [ [ ] ] last = stack [ 0 ] tokens = formula_token_matcher_rational . findall ( formula ) if check : token_letters = set ( [ j for i in tokens for j in i if j in letter_set ] ) formula_letters = set ( i for i in formula if i in letter_set ) if formula_letters != token_letters : raise Exception ( 'Input may not be a formula; extra letters were detected' ) for token in tokens : if token == "(" : stack . append ( [ ] ) last = stack [ - 1 ] elif token == ")" : temp_dict = { } for d in last : for ele , count in d . items ( ) : if ele in temp_dict : temp_dict [ ele ] = temp_dict [ ele ] + count else : temp_dict [ ele ] = count stack . pop ( ) last = stack [ - 1 ] last . append ( temp_dict ) elif token . isalpha ( ) : last . append ( { token : 1 } ) else : v = float ( token ) v_int = int ( v ) if v_int == v : v = v_int last [ - 1 ] = { ele : count * v for ele , count in last [ - 1 ] . items ( ) } ans = { } for d in last : for ele , count in d . items ( ) : if ele in ans : ans [ ele ] = ans [ ele ] + count else : ans [ ele ] = count return ans
4154	def add_markdown_cell ( self , text ) : markdown_cell = { "cell_type" : "markdown" , "metadata" : { } , "source" : [ rst2md ( text ) ] } self . work_notebook [ "cells" ] . append ( markdown_cell )
10668	def get_datetime_at_period_ix ( self , ix ) : if self . timestep_period_duration == TimePeriod . millisecond : return self . start_datetime + timedelta ( milliseconds = ix ) elif self . timestep_period_duration == TimePeriod . second : return self . start_datetime + timedelta ( seconds = ix ) elif self . timestep_period_duration == TimePeriod . minute : return self . start_datetime + timedelta ( minutes = ix ) elif self . timestep_period_duration == TimePeriod . hour : return self . start_datetime + timedelta ( hours = ix ) elif self . timestep_period_duration == TimePeriod . day : return self . start_datetime + relativedelta ( days = ix ) elif self . timestep_period_duration == TimePeriod . week : return self . start_datetime + relativedelta ( days = ix * 7 ) elif self . timestep_period_duration == TimePeriod . month : return self . start_datetime + relativedelta ( months = ix ) elif self . timestep_period_duration == TimePeriod . year : return self . start_datetime + relativedelta ( years = ix )
1543	def get_clusters ( ) : instance = tornado . ioloop . IOLoop . instance ( ) try : return instance . run_sync ( lambda : API . get_clusters ( ) ) except Exception : Log . debug ( traceback . format_exc ( ) ) raise
8919	def _get_service ( self ) : if "service" in self . document . attrib : value = self . document . attrib [ "service" ] . lower ( ) if value in allowed_service_types : self . params [ "service" ] = value else : raise OWSInvalidParameterValue ( "Service %s is not supported" % value , value = "service" ) else : raise OWSMissingParameterValue ( 'Parameter "service" is missing' , value = "service" ) return self . params [ "service" ]
4082	def set_directory ( path = None ) : old_path = get_directory ( ) terminate_server ( ) cache . clear ( ) if path : cache [ 'language_check_dir' ] = path try : get_jar_info ( ) except Error : cache [ 'language_check_dir' ] = old_path raise
8945	def url_as_file ( url , ext = None ) : if ext : ext = '.' + ext . strip ( '.' ) url_hint = 'www-{}-' . format ( urlparse ( url ) . hostname or 'any' ) if url . startswith ( 'file://' ) : url = os . path . abspath ( url [ len ( 'file://' ) : ] ) if os . path . isabs ( url ) : with open ( url , 'rb' ) as handle : content = handle . read ( ) else : content = requests . get ( url ) . content with tempfile . NamedTemporaryFile ( suffix = ext or '' , prefix = url_hint , delete = False ) as handle : handle . write ( content ) try : yield handle . name finally : if os . path . exists ( handle . name ) : os . remove ( handle . name )
5061	def get_enterprise_customer_for_user ( auth_user ) : EnterpriseCustomerUser = apps . get_model ( 'enterprise' , 'EnterpriseCustomerUser' ) try : return EnterpriseCustomerUser . objects . get ( user_id = auth_user . id ) . enterprise_customer except EnterpriseCustomerUser . DoesNotExist : return None
7506	def _dump_qmc ( self ) : io5 = h5py . File ( self . database . output , 'r' ) self . files . qdump = os . path . join ( self . dirs , self . name + ".quartets.txt" ) LOGGER . info ( "qdump file %s" , self . files . qdump ) outfile = open ( self . files . qdump , 'w' ) for idx in xrange ( 0 , self . params . nquartets , self . _chunksize ) : masked_quartets = io5 [ "quartets" ] [ idx : idx + self . _chunksize , : ] quarts = [ list ( j ) for j in masked_quartets ] chunk = [ "{},{}|{},{}" . format ( * i ) for i in quarts ] outfile . write ( "\n" . join ( chunk ) + "\n" ) outfile . close ( ) io5 . close ( )
108	def draw_grid ( images , rows = None , cols = None ) : nb_images = len ( images ) do_assert ( nb_images > 0 ) if is_np_array ( images ) : do_assert ( images . ndim == 4 ) else : do_assert ( is_iterable ( images ) and is_np_array ( images [ 0 ] ) and images [ 0 ] . ndim == 3 ) dts = [ image . dtype . name for image in images ] nb_dtypes = len ( set ( dts ) ) do_assert ( nb_dtypes == 1 , ( "All images provided to draw_grid() must have the same dtype, " + "found %d dtypes (%s)" ) % ( nb_dtypes , ", " . join ( dts ) ) ) cell_height = max ( [ image . shape [ 0 ] for image in images ] ) cell_width = max ( [ image . shape [ 1 ] for image in images ] ) channels = set ( [ image . shape [ 2 ] for image in images ] ) do_assert ( len ( channels ) == 1 , "All images are expected to have the same number of channels, " + "but got channel set %s with length %d instead." % ( str ( channels ) , len ( channels ) ) ) nb_channels = list ( channels ) [ 0 ] if rows is None and cols is None : rows = cols = int ( math . ceil ( math . sqrt ( nb_images ) ) ) elif rows is not None : cols = int ( math . ceil ( nb_images / rows ) ) elif cols is not None : rows = int ( math . ceil ( nb_images / cols ) ) do_assert ( rows * cols >= nb_images ) width = cell_width * cols height = cell_height * rows dt = images . dtype if is_np_array ( images ) else images [ 0 ] . dtype grid = np . zeros ( ( height , width , nb_channels ) , dtype = dt ) cell_idx = 0 for row_idx in sm . xrange ( rows ) : for col_idx in sm . xrange ( cols ) : if cell_idx < nb_images : image = images [ cell_idx ] cell_y1 = cell_height * row_idx cell_y2 = cell_y1 + image . shape [ 0 ] cell_x1 = cell_width * col_idx cell_x2 = cell_x1 + image . shape [ 1 ] grid [ cell_y1 : cell_y2 , cell_x1 : cell_x2 , : ] = image cell_idx += 1 return grid
292	def plot_rolling_sharpe ( returns , factor_returns = None , rolling_window = APPROX_BDAYS_PER_MONTH * 6 , legend_loc = 'best' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) rolling_sharpe_ts = timeseries . rolling_sharpe ( returns , rolling_window ) rolling_sharpe_ts . plot ( alpha = .7 , lw = 3 , color = 'orangered' , ax = ax , ** kwargs ) if factor_returns is not None : rolling_sharpe_ts_factor = timeseries . rolling_sharpe ( factor_returns , rolling_window ) rolling_sharpe_ts_factor . plot ( alpha = .7 , lw = 3 , color = 'grey' , ax = ax , ** kwargs ) ax . set_title ( 'Rolling Sharpe ratio (6-month)' ) ax . axhline ( rolling_sharpe_ts . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 ) ax . axhline ( 0.0 , color = 'black' , linestyle = '-' , lw = 3 ) ax . set_ylabel ( 'Sharpe ratio' ) ax . set_xlabel ( '' ) if factor_returns is None : ax . legend ( [ 'Sharpe' , 'Average' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) else : ax . legend ( [ 'Sharpe' , 'Benchmark Sharpe' , 'Average' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) return ax
13105	def cmpToDataStore_uri ( base , ds1 , ds2 ) : ret = difflib . get_close_matches ( base . uri , [ ds1 . uri , ds2 . uri ] , 1 , cutoff = 0.5 ) if len ( ret ) <= 0 : return 0 if ret [ 0 ] == ds1 . uri : return - 1 return 1
12838	async def async_connect ( self ) : if self . _async_lock is None : raise Exception ( 'Error, database not properly initialized before async connection' ) async with self . _async_lock : self . connect ( True ) return self . _state . conn
8154	def create ( self , name , overwrite = True ) : self . _name = name . rstrip ( ".db" ) from os import unlink if overwrite : try : unlink ( self . _name + ".db" ) except : pass self . _con = sqlite . connect ( self . _name + ".db" ) self . _cur = self . _con . cursor ( )
2910	def _find_ancestor ( self , task_spec ) : if self . parent is None : return self if self . parent . task_spec == task_spec : return self . parent return self . parent . _find_ancestor ( task_spec )
784	def jobCancelAllRunningJobs ( self ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET cancel=TRUE WHERE status<>%%s ' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . STATUS_COMPLETED ] ) return
10262	def _collapse_edge_passing_predicates ( graph : BELGraph , edge_predicates : EdgePredicates = None ) -> None : for u , v , _ in filter_edges ( graph , edge_predicates = edge_predicates ) : collapse_pair ( graph , survivor = u , victim = v )
8406	def zero_range ( x , tol = np . finfo ( float ) . eps * 100 ) : try : if len ( x ) == 1 : return True except TypeError : return True if len ( x ) != 2 : raise ValueError ( 'x must be length 1 or 2' ) x = tuple ( x ) if isinstance ( x [ 0 ] , ( pd . Timestamp , datetime . datetime ) ) : x = date2num ( x ) elif isinstance ( x [ 0 ] , np . datetime64 ) : return x [ 0 ] == x [ 1 ] elif isinstance ( x [ 0 ] , ( pd . Timedelta , datetime . timedelta ) ) : x = x [ 0 ] . total_seconds ( ) , x [ 1 ] . total_seconds ( ) elif isinstance ( x [ 0 ] , np . timedelta64 ) : return x [ 0 ] == x [ 1 ] elif not isinstance ( x [ 0 ] , ( float , int , np . number ) ) : raise TypeError ( "zero_range objects cannot work with objects " "of type '{}'" . format ( type ( x [ 0 ] ) ) ) if any ( np . isnan ( x ) ) : return np . nan if x [ 0 ] == x [ 1 ] : return True if all ( np . isinf ( x ) ) : return False m = np . abs ( x ) . min ( ) if m == 0 : return False return np . abs ( ( x [ 0 ] - x [ 1 ] ) / m ) < tol
6252	def create_normal_matrix ( self , modelview ) : normal_m = Matrix33 . from_matrix44 ( modelview ) normal_m = normal_m . inverse normal_m = normal_m . transpose ( ) return normal_m
7972	def _remove_timeout_handler ( self , handler ) : if handler not in self . timeout_handlers : return self . timeout_handlers . remove ( handler ) for thread in self . timeout_threads : if thread . method . im_self is handler : thread . stop ( )
2324	def predict ( self , x , * args , ** kwargs ) : if len ( args ) > 0 : if type ( args [ 0 ] ) == nx . Graph or type ( args [ 0 ] ) == nx . DiGraph : return self . orient_graph ( x , * args , ** kwargs ) else : return self . predict_proba ( x , * args , ** kwargs ) elif type ( x ) == DataFrame : return self . predict_dataset ( x , * args , ** kwargs ) elif type ( x ) == Series : return self . predict_proba ( x . iloc [ 0 ] , x . iloc [ 1 ] , * args , ** kwargs )
577	def rUpdate ( original , updates ) : dictPairs = [ ( original , updates ) ] while len ( dictPairs ) > 0 : original , updates = dictPairs . pop ( ) for k , v in updates . iteritems ( ) : if k in original and isinstance ( original [ k ] , dict ) and isinstance ( v , dict ) : dictPairs . append ( ( original [ k ] , v ) ) else : original [ k ] = v
7989	def event ( self , event ) : event . stream = self logger . debug ( u"Stream event: {0}" . format ( event ) ) self . settings [ "event_queue" ] . put ( event ) return False
9610	def execute ( self , command , data = { } ) : method , uri = command try : path = self . _formatter . format_map ( uri , data ) body = self . _formatter . get_unused_kwargs ( ) url = "{0}{1}" . format ( self . _url , path ) return self . _request ( method , url , body ) except KeyError as err : LOGGER . debug ( 'Endpoint {0} is missing argument {1}' . format ( uri , err ) ) raise
4917	def contains_content_items ( self , request , pk , course_run_ids , program_uuids ) : enterprise_customer_catalog = self . get_object ( ) course_run_ids = [ unquote ( quote_plus ( course_run_id ) ) for course_run_id in course_run_ids ] contains_content_items = True if course_run_ids : contains_content_items = enterprise_customer_catalog . contains_courses ( course_run_ids ) if program_uuids : contains_content_items = ( contains_content_items and enterprise_customer_catalog . contains_programs ( program_uuids ) ) return Response ( { 'contains_content_items' : contains_content_items } )
3834	async def send_offnetwork_invitation ( self , send_offnetwork_invitation_request ) : response = hangouts_pb2 . SendOffnetworkInvitationResponse ( ) await self . _pb_request ( 'devices/sendoffnetworkinvitation' , send_offnetwork_invitation_request , response ) return response
566	def createEncoder ( ) : consumption_encoder = ScalarEncoder ( 21 , 0.0 , 100.0 , n = 50 , name = "consumption" , clipInput = True ) time_encoder = DateEncoder ( timeOfDay = ( 21 , 9.5 ) , name = "timestamp_timeOfDay" ) encoder = MultiEncoder ( ) encoder . addEncoder ( "consumption" , consumption_encoder ) encoder . addEncoder ( "timestamp" , time_encoder ) return encoder
12669	def niftilist_mask_to_array ( img_filelist , mask_file = None , outdtype = None ) : img = check_img ( img_filelist [ 0 ] ) if not outdtype : outdtype = img . dtype mask_data , _ = load_mask_data ( mask_file ) indices = np . where ( mask_data ) mask = check_img ( mask_file ) outmat = np . zeros ( ( len ( img_filelist ) , np . count_nonzero ( mask_data ) ) , dtype = outdtype ) for i , img_item in enumerate ( img_filelist ) : img = check_img ( img_item ) if not are_compatible_imgs ( img , mask ) : raise NiftiFilesNotCompatible ( repr_imgs ( img ) , repr_imgs ( mask_file ) ) vol = get_img_data ( img ) outmat [ i , : ] = vol [ indices ] return outmat , mask_data
4712	def script_run ( trun , script ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:script:run { script: %s }" % script ) cij . emph ( "rnr:script:run:evars: %s" % script [ "evars" ] ) launchers = { ".py" : "python" , ".sh" : "source" } ext = os . path . splitext ( script [ "fpath" ] ) [ - 1 ] if not ext in launchers . keys ( ) : cij . err ( "rnr:script:run { invalid script[\"fpath\"]: %r }" % script [ "fpath" ] ) return 1 launch = launchers [ ext ] with open ( script [ "log_fpath" ] , "a" ) as log_fd : log_fd . write ( "# script_fpath: %r\n" % script [ "fpath" ] ) log_fd . flush ( ) bgn = time . time ( ) cmd = [ 'bash' , '-c' , 'CIJ_ROOT=$(cij_root) && ' 'source $CIJ_ROOT/modules/cijoe.sh && ' 'source %s && ' 'CIJ_TEST_RES_ROOT="%s" %s %s ' % ( trun [ "conf" ] [ "ENV_FPATH" ] , script [ "res_root" ] , launch , script [ "fpath" ] ) ] if trun [ "conf" ] [ "VERBOSE" ] > 1 : cij . emph ( "rnr:script:run { cmd: %r }" % " " . join ( cmd ) ) evars = os . environ . copy ( ) evars . update ( { k : str ( script [ "evars" ] [ k ] ) for k in script [ "evars" ] } ) process = Popen ( cmd , stdout = log_fd , stderr = STDOUT , cwd = script [ "res_root" ] , env = evars ) process . wait ( ) script [ "rcode" ] = process . returncode script [ "wallc" ] = time . time ( ) - bgn if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:script:run { wallc: %02f }" % script [ "wallc" ] ) cij . emph ( "rnr:script:run { rcode: %r } " % script [ "rcode" ] , script [ "rcode" ] ) return script [ "rcode" ]
4730	def start ( self ) : self . __thread = Thread ( target = self . __run , args = ( True , False ) ) self . __thread . setDaemon ( True ) self . __thread . start ( )
8584	def attach_volume ( self , datacenter_id , server_id , volume_id ) : data = '{ "id": "' + volume_id + '" }' response = self . _perform_request ( url = '/datacenters/%s/servers/%s/volumes' % ( datacenter_id , server_id ) , method = 'POST' , data = data ) return response
9480	def _arg_parser ( ) : description = "Converts a completezip to a litezip" parser = argparse . ArgumentParser ( description = description ) verbose_group = parser . add_mutually_exclusive_group ( ) verbose_group . add_argument ( '-v' , '--verbose' , action = 'store_true' , dest = 'verbose' , default = None , help = "increase verbosity" ) verbose_group . add_argument ( '-q' , '--quiet' , action = 'store_false' , dest = 'verbose' , default = None , help = "print nothing to stdout or stderr" ) parser . add_argument ( 'location' , help = "Location of the unpacked litezip" ) return parser
3879	async def _handle_set_typing_notification ( self , set_typing_notification ) : conv_id = set_typing_notification . conversation_id . id res = parsers . parse_typing_status_message ( set_typing_notification ) await self . on_typing . fire ( res ) try : conv = await self . _get_or_fetch_conversation ( conv_id ) except exceptions . NetworkError : logger . warning ( 'Failed to fetch conversation for typing notification: %s' , conv_id ) else : await conv . on_typing . fire ( res )
2851	def _mpsse_sync ( self , max_retries = 10 ) : self . _write ( '\xAB' ) tries = 0 sync = False while not sync : data = self . _poll_read ( 2 ) if data == '\xFA\xAB' : sync = True tries += 1 if tries >= max_retries : raise RuntimeError ( 'Could not synchronize with FT232H!' )
4414	def add_next ( self , requester : int , track : dict ) : self . queue . insert ( 0 , AudioTrack ( ) . build ( track , requester ) )
10966	def set_shape ( self , shape , inner ) : for c in self . comps : c . set_shape ( shape , inner )
5391	def _delocalize_logging_command ( self , logging_path , user_project ) : logging_prefix = os . path . splitext ( logging_path . uri ) [ 0 ] if logging_path . file_provider == job_model . P_LOCAL : mkdir_cmd = 'mkdir -p "%s"\n' % os . path . dirname ( logging_prefix ) cp_cmd = 'cp' elif logging_path . file_provider == job_model . P_GCS : mkdir_cmd = '' if user_project : cp_cmd = 'gsutil -u {} -mq cp' . format ( user_project ) else : cp_cmd = 'gsutil -mq cp' else : assert False copy_logs_cmd = textwrap . dedent ( ) . format ( cp_cmd = cp_cmd , prefix = logging_prefix ) body = textwrap . dedent ( ) . format ( mkdir_cmd = mkdir_cmd , copy_logs_cmd = copy_logs_cmd ) return body
9695	def findall ( text ) : results = TIMESTRING_RE . findall ( text ) dates = [ ] for date in results : if re . compile ( '((next|last)\s(\d+|couple(\sof))\s(weeks|months|quarters|years))|(between|from)' , re . I ) . match ( date [ 0 ] ) : dates . append ( ( date [ 0 ] . strip ( ) , Range ( date [ 0 ] ) ) ) else : dates . append ( ( date [ 0 ] . strip ( ) , Date ( date [ 0 ] ) ) ) return dates
3972	def _composed_service_dict ( service_spec ) : compose_dict = service_spec . plain_dict ( ) _apply_env_overrides ( env_overrides_for_app_or_service ( service_spec . name ) , compose_dict ) compose_dict . setdefault ( 'volumes' , [ ] ) . append ( _get_cp_volume_mount ( service_spec . name ) ) compose_dict [ 'container_name' ] = "dusty_{}_1" . format ( service_spec . name ) return compose_dict
5955	def find_executables ( path ) : execs = [ ] for exe in os . listdir ( path ) : fullexe = os . path . join ( path , exe ) if ( os . access ( fullexe , os . X_OK ) and not os . path . isdir ( fullexe ) and exe not in [ 'GMXRC' , 'GMXRC.bash' , 'GMXRC.csh' , 'GMXRC.zsh' , 'demux.pl' , 'xplor2gmx.pl' ] ) : execs . append ( exe ) return execs
6999	def parallel_cp_pfdir ( pfpickledir , outdir , lcbasedir , pfpickleglob = 'periodfinding-*.pkl*' , lclistpkl = None , cprenorm = False , nbrradiusarcsec = 60.0 , maxnumneighbors = 5 , makeneighborlcs = True , fast_mode = False , gaia_max_timeout = 60.0 , gaia_mirror = None , xmatchinfo = None , xmatchradiusarcsec = 3.0 , minobservations = 99 , sigclip = 10.0 , lcformat = 'hat-sql' , lcformatdir = None , timecols = None , magcols = None , errcols = None , skipdone = False , done_callback = None , done_callback_args = None , done_callback_kwargs = None , maxobjects = None , nworkers = 32 ) : pfpicklelist = sorted ( glob . glob ( os . path . join ( pfpickledir , pfpickleglob ) ) ) LOGINFO ( 'found %s period-finding pickles, running cp...' % len ( pfpicklelist ) ) return parallel_cp ( pfpicklelist , outdir , lcbasedir , fast_mode = fast_mode , lclistpkl = lclistpkl , nbrradiusarcsec = nbrradiusarcsec , gaia_max_timeout = gaia_max_timeout , gaia_mirror = gaia_mirror , maxnumneighbors = maxnumneighbors , makeneighborlcs = makeneighborlcs , xmatchinfo = xmatchinfo , xmatchradiusarcsec = xmatchradiusarcsec , sigclip = sigclip , minobservations = minobservations , cprenorm = cprenorm , maxobjects = maxobjects , lcformat = lcformat , lcformatdir = lcformatdir , timecols = timecols , magcols = magcols , errcols = errcols , skipdone = skipdone , nworkers = nworkers , done_callback = done_callback , done_callback_args = done_callback_args , done_callback_kwargs = done_callback_kwargs )
5315	def colorpalette ( self , colorpalette ) : if isinstance ( colorpalette , str ) : colorpalette = colors . parse_colors ( colorpalette ) self . _colorpalette = colors . sanitize_color_palette ( colorpalette )
255	def apply_sector_mappings_to_round_trips ( round_trips , sector_mappings ) : sector_round_trips = round_trips . copy ( ) sector_round_trips . symbol = sector_round_trips . symbol . apply ( lambda x : sector_mappings . get ( x , 'No Sector Mapping' ) ) sector_round_trips = sector_round_trips . dropna ( axis = 0 ) return sector_round_trips
6523	def issue_count ( self , include_unclean = False ) : if include_unclean : return len ( self . _all_issues ) self . _ensure_cleaned_issues ( ) return len ( self . _cleaned_issues )
887	def _createSegment ( cls , connections , lastUsedIterationForSegment , cell , iteration , maxSegmentsPerCell ) : while connections . numSegments ( cell ) >= maxSegmentsPerCell : leastRecentlyUsedSegment = min ( connections . segmentsForCell ( cell ) , key = lambda segment : lastUsedIterationForSegment [ segment . flatIdx ] ) connections . destroySegment ( leastRecentlyUsedSegment ) segment = connections . createSegment ( cell ) if segment . flatIdx == len ( lastUsedIterationForSegment ) : lastUsedIterationForSegment . append ( iteration ) elif segment . flatIdx < len ( lastUsedIterationForSegment ) : lastUsedIterationForSegment [ segment . flatIdx ] = iteration else : raise AssertionError ( "All segments should be created with the TM createSegment method." ) return segment
7912	def add_setting ( cls , name , type = unicode , default = None , factory = None , cache = False , default_d = None , doc = None , cmdline_help = None , validator = None , basic = False ) : setting_def = _SettingDefinition ( name , type , default , factory , cache , default_d , doc , cmdline_help , validator , basic ) if name not in cls . _defs : cls . _defs [ name ] = setting_def return duplicate = cls . _defs [ name ] if duplicate . type != setting_def . type : raise ValueError ( "Setting duplicate, with a different type" ) if duplicate . default != setting_def . default : raise ValueError ( "Setting duplicate, with a different default" ) if duplicate . factory != setting_def . factory : raise ValueError ( "Setting duplicate, with a different factory" )
12786	def _effective_filename ( self ) : config_filename = '' if self . filename : config_filename = self . filename env_filename = getenv ( self . env_filename_name ) if env_filename : self . _log . info ( 'Configuration filename was overridden with %r ' 'by the environment variable %s.' , env_filename , self . env_filename_name ) config_filename = env_filename return config_filename
12847	def require_active_token ( object ) : require_token ( object ) token = object if not token . has_id : raise ApiUsageError ( ) if not token . has_world : raise ApiUsageError ( )
11654	def fit ( self , X , y = None , ** params ) : X = as_features ( X , stack = True ) self . transformer . fit ( X . stacked_features , y , ** params ) return self
2912	def _ready ( self ) : if self . _has_state ( self . COMPLETED ) or self . _has_state ( self . CANCELLED ) : return self . _set_state ( self . READY ) self . task_spec . _on_ready ( self )
3198	def start ( self , workflow_id , email_id ) : self . workflow_id = workflow_id self . email_id = email_id return self . _mc_client . _post ( url = self . _build_path ( workflow_id , 'emails' , email_id , 'actions/start' ) )
7867	def expire ( self ) : with self . _lock : logger . debug ( "expdict.expire. timeouts: {0!r}" . format ( self . _timeouts ) ) next_timeout = None for k in self . _timeouts . keys ( ) : ret = self . _expire_item ( k ) if ret is not None : if next_timeout is None : next_timeout = ret else : next_timeout = min ( next_timeout , ret ) return next_timeout
2211	def inject_method ( self , func , name = None ) : new_method = func . __get__ ( self , self . __class__ ) if name is None : name = func . __name__ setattr ( self , name , new_method )
10631	def clear ( self ) : self . _compound_mfrs = self . _compound_mfrs * 0.0 self . _P = 1.0 self . _T = 25.0 self . _H = 0.0
721	def getAllMetrics ( self ) : result = self . getReportMetrics ( ) result . update ( self . getOptimizationMetrics ( ) ) return result
4846	def get_content_metadata ( self , enterprise_customer ) : content_metadata = OrderedDict ( ) if enterprise_customer . catalog : response = self . _load_data ( self . ENTERPRISE_CUSTOMER_ENDPOINT , detail_resource = 'courses' , resource_id = str ( enterprise_customer . uuid ) , traverse_pagination = True , ) for course in response [ 'results' ] : for course_run in course [ 'course_runs' ] : course_run [ 'content_type' ] = 'courserun' content_metadata [ course_run [ 'key' ] ] = course_run for enterprise_customer_catalog in enterprise_customer . enterprise_customer_catalogs . all ( ) : response = self . _load_data ( self . ENTERPRISE_CUSTOMER_CATALOGS_ENDPOINT , resource_id = str ( enterprise_customer_catalog . uuid ) , traverse_pagination = True , querystring = { 'page_size' : 1000 } , ) for item in response [ 'results' ] : content_id = utils . get_content_metadata_item_id ( item ) content_metadata [ content_id ] = item return content_metadata . values ( )
10924	def fit_comp ( new_comp , old_comp , ** kwargs ) : new_cat = new_comp . category new_comp . category = 'ilm' fake_s = states . ImageState ( Image ( old_comp . get ( ) . copy ( ) ) , [ new_comp ] , pad = 0 , mdl = mdl . SmoothFieldModel ( ) ) do_levmarq ( fake_s , new_comp . params , ** kwargs ) new_comp . category = new_cat
5277	def query ( self , i , j ) : "Query the oracle to find out whether i and j should be must-linked" if self . queries_cnt < self . max_queries_cnt : self . queries_cnt += 1 return self . labels [ i ] == self . labels [ j ] else : raise MaximumQueriesExceeded
6087	def contribution_maps_1d_from_hyper_images_and_galaxies ( hyper_model_image_1d , hyper_galaxy_images_1d , hyper_galaxies , hyper_minimum_values ) : return list ( map ( lambda hyper_galaxy , hyper_galaxy_image_1d , hyper_minimum_value : hyper_galaxy . contributions_from_model_image_and_galaxy_image ( model_image = hyper_model_image_1d , galaxy_image = hyper_galaxy_image_1d , minimum_value = hyper_minimum_value ) , hyper_galaxies , hyper_galaxy_images_1d , hyper_minimum_values ) )
2645	def App ( apptype , data_flow_kernel = None , walltime = 60 , cache = False , executors = 'all' ) : from parsl . app . python import PythonApp from parsl . app . bash import BashApp logger . warning ( "The 'App' decorator will be deprecated in Parsl 0.8. Please use 'python_app' or 'bash_app' instead." ) if apptype == 'python' : app_class = PythonApp elif apptype == 'bash' : app_class = BashApp else : raise InvalidAppTypeError ( "Invalid apptype requested {}; must be 'python' or 'bash'" . format ( apptype ) ) def wrapper ( f ) : return app_class ( f , data_flow_kernel = data_flow_kernel , walltime = walltime , cache = cache , executors = executors ) return wrapper
9670	def normalize ( self , string ) : return '' . join ( [ self . _normalize . get ( x , x ) for x in nfd ( string ) ] )
11921	def paginator ( self ) : if not hasattr ( self , '_paginator' ) : if self . pagination_class is None : self . _paginator = None else : self . _paginator = self . pagination_class ( ) return self . _paginator
7475	def dask_chroms ( data , samples ) : h5s = [ os . path . join ( data . dirs . across , s . name + ".tmp.h5" ) for s in samples ] handles = [ h5py . File ( i ) for i in h5s ] dsets = [ i [ '/ichrom' ] for i in handles ] arrays = [ da . from_array ( dset , chunks = ( 10000 , 3 ) ) for dset in dsets ] stack = da . stack ( arrays , axis = 2 ) maxchrom = da . max ( stack , axis = 2 ) [ : , 0 ] maxpos = da . max ( stack , axis = 2 ) [ : , 2 ] mask = stack == 0 stack [ mask ] = 9223372036854775807 minpos = da . min ( stack , axis = 2 ) [ : , 1 ] final = da . stack ( [ maxchrom , minpos , maxpos ] , axis = 1 ) final . to_hdf5 ( data . clust_database , "/chroms" ) _ = [ i . close ( ) for i in handles ]
3106	def code_challenge ( verifier ) : digest = hashlib . sha256 ( verifier ) . digest ( ) return base64 . urlsafe_b64encode ( digest ) . rstrip ( b'=' )
13366	def apply ( f , obj , * args , ** kwargs ) : return vectorize ( f ) ( obj , * args , ** kwargs )
9738	def get_3d_markers_no_label_residual ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPositionNoLabelResidual , component_info , data , component_position )
10564	def get_supported_filepaths ( filepaths , supported_extensions , max_depth = float ( 'inf' ) ) : supported_filepaths = [ ] for path in filepaths : if os . name == 'nt' and CYGPATH_RE . match ( path ) : path = convert_cygwin_path ( path ) if os . path . isdir ( path ) : for root , __ , files in walk_depth ( path , max_depth ) : for f in files : if f . lower ( ) . endswith ( supported_extensions ) : supported_filepaths . append ( os . path . join ( root , f ) ) elif os . path . isfile ( path ) and path . lower ( ) . endswith ( supported_extensions ) : supported_filepaths . append ( path ) return supported_filepaths
6634	def displayOutdated ( modules , dependency_specs , use_colours ) : if use_colours : DIM = colorama . Style . DIM NORMAL = colorama . Style . NORMAL BRIGHT = colorama . Style . BRIGHT YELLOW = colorama . Fore . YELLOW RED = colorama . Fore . RED GREEN = colorama . Fore . GREEN RESET = colorama . Style . RESET_ALL else : DIM = BRIGHT = YELLOW = RED = GREEN = RESET = u'' status = 0 from yotta . lib import access from yotta . lib import access_common from yotta . lib import sourceparse for name , m in modules . items ( ) : if m . isTestDependency ( ) : continue try : latest_v = access . latestSuitableVersion ( name , '*' , registry = 'modules' , quiet = True ) except access_common . Unavailable as e : latest_v = None if not m : m_version = u' ' + RESET + BRIGHT + RED + u"missing" + RESET else : m_version = DIM + u'@%s' % ( m . version ) if not latest_v : print ( u'%s%s%s%s not available from the registry%s' % ( RED , name , m_version , NORMAL , RESET ) ) status = 2 continue elif not m or m . version < latest_v : update_prevented_by = '' if m : specs_preventing_update = [ x for x in dependency_specs if x . name == name and not sourceparse . parseSourceURL ( x . nonShrinkwrappedVersionReq ( ) ) . semanticSpecMatches ( latest_v ) ] shrinkwrap_prevents_update = [ x for x in dependency_specs if x . name == name and x . isShrinkwrapped ( ) and not sourceparse . parseSourceURL ( x . versionReq ( ) ) . semanticSpecMatches ( latest_v ) ] if len ( specs_preventing_update ) : update_prevented_by = ' (update prevented by specifications: %s)' % ( ', ' . join ( [ '%s from %s' % ( x . version_req , x . specifying_module ) for x in specs_preventing_update ] ) ) if len ( shrinkwrap_prevents_update ) : update_prevented_by += ' yotta-shrinkwrap.json prevents update' if m . version . major ( ) < latest_v . major ( ) : colour = GREEN elif m . version . minor ( ) < latest_v . minor ( ) : colour = YELLOW else : colour = RED else : colour = RED print ( u'%s%s%s latest: %s%s%s%s' % ( name , m_version , RESET , colour , latest_v . version , update_prevented_by , RESET ) ) if not status : status = 1 return status
6029	def set_xy_labels ( units , kpc_per_arcsec , xlabelsize , ylabelsize , xyticksize ) : if units in 'arcsec' or kpc_per_arcsec is None : plt . xlabel ( 'x (arcsec)' , fontsize = xlabelsize ) plt . ylabel ( 'y (arcsec)' , fontsize = ylabelsize ) elif units in 'kpc' : plt . xlabel ( 'x (kpc)' , fontsize = xlabelsize ) plt . ylabel ( 'y (kpc)' , fontsize = ylabelsize ) else : raise exc . PlottingException ( 'The units supplied to the plotted are not a valid string (must be pixels | ' 'arcsec | kpc)' ) plt . tick_params ( labelsize = xyticksize )
7233	def aggregate_query ( self , searchAreaWkt , agg_def , query = None , start_date = None , end_date = None , count = 10 , index = default_index ) : geojson = load_wkt ( searchAreaWkt ) . __geo_interface__ aggs_str = str ( agg_def ) params = { "count" : count , "aggs" : aggs_str } if query : params [ 'query' ] = query if start_date : params [ 'start_date' ] = start_date if end_date : params [ 'end_date' ] = end_date url = self . aggregations_by_index_url % index if index else self . aggregations_url r = self . gbdx_connection . post ( url , params = params , json = geojson ) r . raise_for_status ( ) return r . json ( object_pairs_hook = OrderedDict ) [ 'aggregations' ]
10436	def verifypartialtablecell ( self , window_name , object_name , row_index , column_index , row_text ) : try : value = getcellvalue ( window_name , object_name , row_index , column_index ) if re . searchmatch ( row_text , value ) : return 1 except LdtpServerException : pass return 0
13153	def dict_cursor ( func ) : @ wraps ( func ) def wrapper ( cls , * args , ** kwargs ) : with ( yield from cls . get_cursor ( _CursorType . DICT ) ) as c : return ( yield from func ( cls , c , * args , ** kwargs ) ) return wrapper
4729	def __run ( self , shell = True , echo = True ) : if env ( ) : return 1 cij . emph ( "cij.dmesg.start: shell: %r, cmd: %r" % ( shell , self . __prefix + self . __suffix ) ) return cij . ssh . command ( self . __prefix , shell , echo , self . __suffix )
8761	def delete_subnet ( context , id ) : LOG . info ( "delete_subnet %s for tenant %s" % ( id , context . tenant_id ) ) with context . session . begin ( ) : subnet = db_api . subnet_find ( context , id = id , scope = db_api . ONE ) if not subnet : raise n_exc . SubnetNotFound ( subnet_id = id ) if not context . is_admin : if STRATEGY . is_provider_network ( subnet . network_id ) : if subnet . tenant_id == context . tenant_id : raise n_exc . NotAuthorized ( subnet_id = id ) else : raise n_exc . SubnetNotFound ( subnet_id = id ) _delete_subnet ( context , subnet )
11235	def get ( ) : config = { } try : config = _load_config ( ) except IOError : try : _create_default_config ( ) config = _load_config ( ) except IOError as e : raise ConfigError ( _FILE_CREATION_ERROR . format ( e . args [ 0 ] ) ) except SyntaxError as e : raise ConfigError ( _JSON_SYNTAX_ERROR . format ( e . args [ 0 ] ) ) except Exception : raise ConfigError ( _JSON_SYNTAX_ERROR . format ( 'Yaml syntax error..' ) ) try : _validate ( config ) except KeyError as e : raise ConfigError ( _MANDATORY_KEY_ERROR . format ( e . args [ 0 ] ) ) except SyntaxError as e : raise ConfigError ( _INVALID_KEY_ERROR . format ( e . args [ 0 ] ) ) except ValueError as e : raise ConfigError ( _INVALID_VALUE_ERROR . format ( e . args [ 0 ] ) ) config [ 'projects-path' ] = os . path . expanduser ( config [ 'projects-path' ] ) _complete_config ( config ) return config
2781	def create ( self ) : input_params = { "type" : self . type , "data" : self . data , "name" : self . name , "priority" : self . priority , "port" : self . port , "ttl" : self . ttl , "weight" : self . weight , "flags" : self . flags , "tags" : self . tags } data = self . get_data ( "domains/%s/records" % ( self . domain ) , type = POST , params = input_params , ) if data : self . id = data [ 'domain_record' ] [ 'id' ]
11643	def transform ( self , X ) : X = check_array ( X ) X_rbf = np . empty_like ( X ) if self . copy else X X_in = X if not self . squared : np . power ( X_in , 2 , out = X_rbf ) X_in = X_rbf if self . scale_by_median : scale = self . median_ if self . squared else self . median_ ** 2 gamma = self . gamma * scale else : gamma = self . gamma np . multiply ( X_in , - gamma , out = X_rbf ) np . exp ( X_rbf , out = X_rbf ) return X_rbf
8347	def _getAttrMap ( self ) : if not getattr ( self , 'attrMap' ) : self . attrMap = { } for ( key , value ) in self . attrs : self . attrMap [ key ] = value return self . attrMap
24	def update ( self , new_val ) : if self . _value is None : self . _value = new_val else : self . _value = self . _gamma * self . _value + ( 1.0 - self . _gamma ) * new_val
520	def _initPermNonConnected ( self ) : p = self . _synPermConnected * self . _random . getReal64 ( ) p = int ( p * 100000 ) / 100000.0 return p
11435	def _validate_record_field_positions_global ( record ) : all_fields = [ ] for tag , fields in record . items ( ) : previous_field_position_global = - 1 for field in fields : if field [ 4 ] < previous_field_position_global : return ( "Non ascending global field positions in tag '%s'." % tag ) previous_field_position_global = field [ 4 ] if field [ 4 ] in all_fields : return ( "Duplicate global field position '%d' in tag '%s'" % ( field [ 4 ] , tag ) )
1313	def ControlFromPoint ( x : int , y : int ) -> Control : element = _AutomationClient . instance ( ) . IUIAutomation . ElementFromPoint ( ctypes . wintypes . POINT ( x , y ) ) return Control . CreateControlFromElement ( element )
6484	def _process_field_values ( request ) : return { field_key : request . POST [ field_key ] for field_key in request . POST if field_key in course_discovery_filter_fields ( ) }
13255	def time ( self , t ) : _time = arrow . get ( t ) . format ( 'YYYY-MM-DDTHH:mm:ss' ) self . _time = datetime . datetime . strptime ( _time , '%Y-%m-%dT%H:%M:%S' )
9308	def get_canonical_headers ( cls , req , include = None ) : if include is None : include = cls . default_include_headers include = [ x . lower ( ) for x in include ] headers = req . headers . copy ( ) if 'host' not in headers : headers [ 'host' ] = urlparse ( req . url ) . netloc . split ( ':' ) [ 0 ] cano_headers_dict = { } for hdr , val in headers . items ( ) : hdr = hdr . strip ( ) . lower ( ) val = cls . amz_norm_whitespace ( val ) . strip ( ) if ( hdr in include or '*' in include or ( 'x-amz-*' in include and hdr . startswith ( 'x-amz-' ) and not hdr == 'x-amz-client-context' ) ) : vals = cano_headers_dict . setdefault ( hdr , [ ] ) vals . append ( val ) cano_headers = '' signed_headers_list = [ ] for hdr in sorted ( cano_headers_dict ) : vals = cano_headers_dict [ hdr ] val = ',' . join ( sorted ( vals ) ) cano_headers += '{}:{}\n' . format ( hdr , val ) signed_headers_list . append ( hdr ) signed_headers = ';' . join ( signed_headers_list ) return ( cano_headers , signed_headers )
6185	def check_clean_status ( git_path = None ) : output = get_status ( git_path ) is_unmodified = ( len ( output . strip ( ) ) == 0 ) return is_unmodified
3892	def run_example ( example_coroutine , * extra_args ) : args = _get_parser ( extra_args ) . parse_args ( ) logging . basicConfig ( level = logging . DEBUG if args . debug else logging . WARNING ) cookies = hangups . auth . get_auth_stdin ( args . token_path ) client = hangups . Client ( cookies ) loop = asyncio . get_event_loop ( ) task = asyncio . ensure_future ( _async_main ( example_coroutine , client , args ) , loop = loop ) try : loop . run_until_complete ( task ) except KeyboardInterrupt : task . cancel ( ) loop . run_until_complete ( task ) finally : loop . close ( )
4043	def _build_query ( self , query_string , no_params = False ) : try : query = quote ( query_string . format ( u = self . library_id , t = self . library_type ) ) except KeyError as err : raise ze . ParamNotPassed ( "There's a request parameter missing: %s" % err ) if no_params is False : if not self . url_params : self . add_parameters ( ) query = "%s?%s" % ( query , self . url_params ) return query
1803	def MOVBE ( cpu , dest , src ) : size = dest . size arg0 = dest . read ( ) temp = 0 for pos in range ( 0 , size , 8 ) : temp = ( temp << 8 ) | ( arg0 & 0xff ) arg0 = arg0 >> 8 dest . write ( arg0 )
10348	def export_namespace ( graph , namespace , directory = None , cacheable = False ) : directory = os . getcwd ( ) if directory is None else directory path = os . path . join ( directory , '{}.belns' . format ( namespace ) ) with open ( path , 'w' ) as file : log . info ( 'Outputting to %s' , path ) right_names = get_names_by_namespace ( graph , namespace ) log . info ( 'Graph has %d correct names in %s' , len ( right_names ) , namespace ) wrong_names = get_incorrect_names_by_namespace ( graph , namespace ) log . info ( 'Graph has %d incorrect names in %s' , len ( right_names ) , namespace ) undefined_ns_names = get_undefined_namespace_names ( graph , namespace ) log . info ( 'Graph has %d names in missing namespace %s' , len ( right_names ) , namespace ) names = ( right_names | wrong_names | undefined_ns_names ) if 0 == len ( names ) : log . warning ( '%s is empty' , namespace ) write_namespace ( namespace_name = namespace , namespace_keyword = namespace , namespace_domain = 'Other' , author_name = graph . authors , author_contact = graph . contact , citation_name = graph . name , values = names , cacheable = cacheable , file = file )
725	def get ( self , number ) : if not number in self . _patterns : raise IndexError ( "Invalid number" ) return self . _patterns [ number ]
11736	def _validate_schema ( obj ) : if obj is not None and not isinstance ( obj , Schema ) : raise IncompatibleSchema ( 'Schema must be of type {0}' . format ( Schema ) ) return obj
13370	def is_redirecting ( path ) : candidate = unipath ( path , '.cpenv' ) return os . path . exists ( candidate ) and os . path . isfile ( candidate )
3977	def _expand_libs_in_libs ( specs ) : for lib_name , lib_spec in specs [ 'libs' ] . iteritems ( ) : if 'depends' in lib_spec and 'libs' in lib_spec [ 'depends' ] : lib_spec [ 'depends' ] [ 'libs' ] = _get_dependent ( 'libs' , lib_name , specs , 'libs' )
5480	def cancel ( batch_fn , cancel_fn , ops ) : canceled_ops = [ ] error_messages = [ ] max_batch = 256 total_ops = len ( ops ) for first_op in range ( 0 , total_ops , max_batch ) : batch_canceled , batch_messages = _cancel_batch ( batch_fn , cancel_fn , ops [ first_op : first_op + max_batch ] ) canceled_ops . extend ( batch_canceled ) error_messages . extend ( batch_messages ) return canceled_ops , error_messages
12597	def _check_xl_path ( xl_path : str ) : xl_path = op . abspath ( op . expanduser ( xl_path ) ) if not op . isfile ( xl_path ) : raise IOError ( "Could not find file in {}." . format ( xl_path ) ) return xl_path , _use_openpyxl_or_xlrf ( xl_path )
7936	def _set_state ( self , state ) : logger . debug ( " _set_state({0!r})" . format ( state ) ) self . _state = state self . _state_cond . notify ( )
4049	def fulltext_item ( self , itemkey , ** kwargs ) : query_string = "/{t}/{u}/items/{itemkey}/fulltext" . format ( t = self . library_type , u = self . library_id , itemkey = itemkey ) return self . _build_query ( query_string )
87	def is_integer_array ( val ) : return is_np_array ( val ) and issubclass ( val . dtype . type , np . integer )
7571	def revcomp ( sequence ) : "returns reverse complement of a string" sequence = sequence [ : : - 1 ] . strip ( ) . replace ( "A" , "t" ) . replace ( "T" , "a" ) . replace ( "C" , "g" ) . replace ( "G" , "c" ) . upper ( ) return sequence
10763	def get_unique_token ( self ) : if self . _unique_token is None : self . _unique_token = self . _random_token ( ) return self . _unique_token
2410	def create_essay_set_and_dump_model ( text , score , prompt , model_path , additional_array = None ) : essay_set = create_essay_set ( text , score , prompt ) feature_ext , clf = extract_features_and_generate_model ( essay_set , additional_array ) dump_model_to_file ( prompt , feature_ext , clf , model_path )
2767	def get_all_volumes ( self , region = None ) : if region : url = "volumes?region={}" . format ( region ) else : url = "volumes" data = self . get_data ( url ) volumes = list ( ) for jsoned in data [ 'volumes' ] : volume = Volume ( ** jsoned ) volume . token = self . token volumes . append ( volume ) return volumes
6144	def DSP_callback_tic ( self ) : if self . Tcapture > 0 : self . DSP_tic . append ( time . time ( ) - self . start_time )
8146	def levels ( self ) : h = self . img . histogram ( ) r = h [ 0 : 255 ] g = h [ 256 : 511 ] b = h [ 512 : 767 ] a = h [ 768 : 1024 ] return r , g , b , a
4867	def to_representation ( self , instance ) : updated_course = copy . deepcopy ( instance ) enterprise_customer_catalog = self . context [ 'enterprise_customer_catalog' ] updated_course [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_enrollment_url ( updated_course [ 'key' ] ) for course_run in updated_course [ 'course_runs' ] : course_run [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_run_enrollment_url ( course_run [ 'key' ] ) return updated_course
7857	def __error ( self , stanza ) : try : self . error ( stanza . get_error ( ) ) except ProtocolError : from . . error import StanzaErrorNode self . error ( StanzaErrorNode ( "undefined-condition" ) )
6624	def availableVersions ( self ) : r = [ ] for t in self . _getTags ( ) : logger . debug ( "available version tag: %s" , t ) if not len ( t [ 0 ] . strip ( ) ) : continue try : r . append ( GithubComponentVersion ( t [ 0 ] , t [ 0 ] , url = t [ 1 ] , name = self . name , cache_key = None ) ) except ValueError : logger . debug ( 'invalid version tag: %s' , t ) return r
9245	def encapsulate_string ( raw_string ) : raw_string . replace ( '\\' , '\\\\' ) enc_string = re . sub ( "([<>*_()\[\]#])" , r"\\\1" , raw_string ) return enc_string
6542	def on_tool_finish ( self , tool ) : with self . _lock : if tool in self . current_tools : self . current_tools . remove ( tool ) self . completed_tools . append ( tool )
5786	def _raw_write ( self ) : data_available = libssl . BIO_ctrl_pending ( self . _wbio ) if data_available == 0 : return b'' to_read = min ( self . _buffer_size , data_available ) read = libssl . BIO_read ( self . _wbio , self . _bio_write_buffer , to_read ) to_write = bytes_from_buffer ( self . _bio_write_buffer , read ) output = to_write while len ( to_write ) : raise_disconnect = False try : sent = self . _socket . send ( to_write ) except ( socket_ . error ) as e : if e . errno == 104 or e . errno == 32 : raise_disconnect = True else : raise if raise_disconnect : raise_disconnection ( ) to_write = to_write [ sent : ] if len ( to_write ) : self . select_write ( ) return output
12411	def close ( self ) : self . require_not_closed ( ) if not self . streaming or self . asynchronous : if 'Content-Length' not in self . headers : self . headers [ 'Content-Length' ] = self . tell ( ) self . flush ( ) self . _closed = True
5803	def extract_chain ( server_handshake_bytes ) : output = [ ] chain_bytes = None for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type == b'\x0b' : chain_bytes = message_data break if chain_bytes : break if chain_bytes : pointer = 3 while pointer < len ( chain_bytes ) : cert_length = int_from_bytes ( chain_bytes [ pointer : pointer + 3 ] ) cert_start = pointer + 3 cert_end = cert_start + cert_length pointer = cert_end cert_bytes = chain_bytes [ cert_start : cert_end ] output . append ( Certificate . load ( cert_bytes ) ) return output
6799	def database_renderer ( self , name = None , site = None , role = None ) : name = name or self . env . default_db_name site = site or self . genv . SITE role = role or self . genv . ROLE key = ( name , site , role ) self . vprint ( 'checking key:' , key ) if key not in self . _database_renderers : self . vprint ( 'No cached db renderer, generating...' ) if self . verbose : print ( 'db.name:' , name ) print ( 'db.databases:' , self . env . databases ) print ( 'db.databases[%s]:' % name , self . env . databases . get ( name ) ) d = type ( self . genv ) ( self . lenv ) d . update ( self . get_database_defaults ( ) ) d . update ( self . env . databases . get ( name , { } ) ) d [ 'db_name' ] = name if self . verbose : print ( 'db.d:' ) pprint ( d , indent = 4 ) print ( 'db.connection_handler:' , d . connection_handler ) if d . connection_handler == CONNECTION_HANDLER_DJANGO : self . vprint ( 'Using django handler...' ) dj = self . get_satchel ( 'dj' ) if self . verbose : print ( 'Loading Django DB settings for site {} and role {}.' . format ( site , role ) , file = sys . stderr ) dj . set_db ( name = name , site = site , role = role ) _d = dj . local_renderer . collect_genv ( include_local = True , include_global = False ) for k , v in _d . items ( ) : if k . startswith ( 'dj_db_' ) : _d [ k [ 3 : ] ] = v del _d [ k ] if self . verbose : print ( 'Loaded:' ) pprint ( _d ) d . update ( _d ) elif d . connection_handler and d . connection_handler . startswith ( CONNECTION_HANDLER_CUSTOM + ':' ) : _callable_str = d . connection_handler [ len ( CONNECTION_HANDLER_CUSTOM + ':' ) : ] self . vprint ( 'Using custom handler %s...' % _callable_str ) _d = str_to_callable ( _callable_str ) ( role = self . genv . ROLE ) if self . verbose : print ( 'Loaded:' ) pprint ( _d ) d . update ( _d ) r = LocalRenderer ( self , lenv = d ) self . set_root_login ( r ) self . _database_renderers [ key ] = r else : self . vprint ( 'Cached db renderer found.' ) return self . _database_renderers [ key ]
8325	def buildTagMap ( default , * args ) : built = { } for portion in args : if hasattr ( portion , 'items' ) : for k , v in portion . items ( ) : built [ k ] = v elif isList ( portion ) : for k in portion : built [ k ] = default else : built [ portion ] = default return built
13427	def delete_messages ( self , messages ) : url = "/2/messages/?%s" % urlencode ( [ ( 'ids' , "," . join ( messages ) ) ] ) data = self . _delete_resource ( url ) return data
12784	def get_xdg_dirs ( self ) : config_dirs = getenv ( 'XDG_CONFIG_DIRS' , '' ) if config_dirs : self . _log . debug ( 'XDG_CONFIG_DIRS is set to %r' , config_dirs ) output = [ ] for path in reversed ( config_dirs . split ( ':' ) ) : output . append ( join ( path , self . group_name , self . app_name ) ) return output return [ '/etc/xdg/%s/%s' % ( self . group_name , self . app_name ) ]
1813	def SETNBE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . AND ( cpu . CF == False , cpu . ZF == False ) , 1 , 0 ) )
12755	def enable_motors ( self , max_force ) : for joint in self . joints : amotor = getattr ( joint , 'amotor' , joint ) amotor . max_forces = max_force if max_force > 0 : amotor . enable_feedback ( ) else : amotor . disable_feedback ( )
679	def getRecord ( self , n = None ) : if n is None : assert len ( self . fields ) > 0 n = self . fields [ 0 ] . numRecords - 1 assert ( all ( field . numRecords > n for field in self . fields ) ) record = [ field . values [ n ] for field in self . fields ] return record
11084	def save ( self , msg , args ) : self . send_message ( msg . channel , "Saving current state..." ) self . _bot . plugins . save_state ( ) self . send_message ( msg . channel , "Done." )
7373	def get_error ( data ) : if isinstance ( data , dict ) : if 'errors' in data : error = data [ 'errors' ] [ 0 ] else : error = data . get ( 'error' , None ) if isinstance ( error , dict ) : if error . get ( 'code' ) in errors : return error
4465	def save ( filename_audio , filename_jam , jam , strict = True , fmt = 'auto' , ** kwargs ) : y = jam . sandbox . muda . _audio [ 'y' ] sr = jam . sandbox . muda . _audio [ 'sr' ] psf . write ( filename_audio , y , sr , ** kwargs ) jam . save ( filename_jam , strict = strict , fmt = fmt )
8112	def hash ( self , id ) : h = md5 ( id ) . hexdigest ( ) return os . path . join ( self . path , h + self . type )
4395	def adsSyncReadByNameEx ( port , address , data_name , data_type , return_ctypes = False ) : handle = adsSyncReadWriteReqEx2 ( port , address , ADSIGRP_SYM_HNDBYNAME , 0x0 , PLCTYPE_UDINT , data_name , PLCTYPE_STRING , ) value = adsSyncReadReqEx2 ( port , address , ADSIGRP_SYM_VALBYHND , handle , data_type , return_ctypes ) adsSyncWriteReqEx ( port , address , ADSIGRP_SYM_RELEASEHND , 0 , handle , PLCTYPE_UDINT ) return value
3614	def _get_available_choices ( self , queryset , value ) : item = queryset . filter ( pk = value ) . first ( ) if item : try : pk = getattr ( item , self . chained_model_field + "_id" ) filter = { self . chained_model_field : pk } except AttributeError : try : pks = getattr ( item , self . chained_model_field ) . all ( ) . values_list ( 'pk' , flat = True ) filter = { self . chained_model_field + "__in" : pks } except AttributeError : try : pks = getattr ( item , self . chained_model_field + "_set" ) . all ( ) . values_list ( 'pk' , flat = True ) filter = { self . chained_model_field + "__in" : pks } except AttributeError : filter = { } filtered = list ( get_model ( self . to_app_name , self . to_model_name ) . objects . filter ( ** filter ) . distinct ( ) ) if self . sort : sort_results ( filtered ) else : filtered = [ ] return filtered
12993	def level_chunker ( text , getreffs , level = 1 ) : references = getreffs ( level = level ) return [ ( ref . split ( ":" ) [ - 1 ] , ref . split ( ":" ) [ - 1 ] ) for ref in references ]
9105	def dropbox_factory ( request ) : try : return request . registry . settings [ 'dropbox_container' ] . get_dropbox ( request . matchdict [ 'drop_id' ] ) except KeyError : raise HTTPNotFound ( 'no such dropbox' )
13048	def f_annotation_filter ( annotations , type_uri , number ) : filtered = [ annotation for annotation in annotations if annotation . type_uri == type_uri ] number = min ( [ len ( filtered ) , number ] ) if number == 0 : return None else : return filtered [ number - 1 ]
11217	def _pop_claims_from_payload ( self ) : claims_in_payload = [ k for k in self . payload . keys ( ) if k in registered_claims . values ( ) ] for name in claims_in_payload : self . registered_claims [ name ] = self . payload . pop ( name )
1252	def add_random_tile ( self ) : x_pos , y_pos = np . where ( self . _state == 0 ) assert len ( x_pos ) != 0 empty_index = np . random . choice ( len ( x_pos ) ) value = np . random . choice ( [ 1 , 2 ] , p = [ 0.9 , 0.1 ] ) self . _state [ x_pos [ empty_index ] , y_pos [ empty_index ] ] = value
7745	def _loop_timeout_cb ( self , main_loop ) : self . _anything_done = True logger . debug ( "_loop_timeout_cb() called" ) main_loop . quit ( )
2411	def initialize_dictionaries ( self , p_set ) : success = False if not ( hasattr ( p_set , '_type' ) ) : error_message = "needs to be an essay set of the train type." log . exception ( error_message ) raise util_functions . InputError ( p_set , error_message ) if not ( p_set . _type == "train" ) : error_message = "needs to be an essay set of the train type." log . exception ( error_message ) raise util_functions . InputError ( p_set , error_message ) div_length = len ( p_set . _essay_sets ) if div_length == 0 : div_length = 1 max_feats2 = int ( math . floor ( 200 / div_length ) ) for i in xrange ( 0 , len ( p_set . _essay_sets ) ) : self . _extractors . append ( FeatureExtractor ( ) ) self . _extractors [ i ] . initialize_dictionaries ( p_set . _essay_sets [ i ] , max_feats2 = max_feats2 ) self . _initialized = True success = True return success
2943	def validate ( self ) : results = [ ] from . . specs import Join def recursive_find_loop ( task , history ) : current = history [ : ] current . append ( task ) if isinstance ( task , Join ) : if task in history : msg = "Found loop with '%s': %s then '%s' again" % ( task . name , '->' . join ( [ p . name for p in history ] ) , task . name ) raise Exception ( msg ) for predecessor in task . inputs : recursive_find_loop ( predecessor , current ) for parent in task . inputs : recursive_find_loop ( parent , current ) for task_id , task in list ( self . task_specs . items ( ) ) : try : recursive_find_loop ( task , [ ] ) except Exception as exc : results . append ( exc . __str__ ( ) ) if not task . inputs and task . name not in [ 'Start' , 'Root' ] : if task . outputs : results . append ( "Task '%s' is disconnected (no inputs)" % task . name ) else : LOG . debug ( "Task '%s' is not being used" % task . name ) return results
3503	def add_loopless ( model , zero_cutoff = None ) : zero_cutoff = normalize_cutoff ( model , zero_cutoff ) internal = [ i for i , r in enumerate ( model . reactions ) if not r . boundary ] s_int = create_stoichiometric_matrix ( model ) [ : , numpy . array ( internal ) ] n_int = nullspace ( s_int ) . T max_bound = max ( max ( abs ( b ) for b in r . bounds ) for r in model . reactions ) prob = model . problem to_add = [ ] for i in internal : rxn = model . reactions [ i ] indicator = prob . Variable ( "indicator_" + rxn . id , type = "binary" ) on_off_constraint = prob . Constraint ( rxn . flux_expression - max_bound * indicator , lb = - max_bound , ub = 0 , name = "on_off_" + rxn . id ) delta_g = prob . Variable ( "delta_g_" + rxn . id ) delta_g_range = prob . Constraint ( delta_g + ( max_bound + 1 ) * indicator , lb = 1 , ub = max_bound , name = "delta_g_range_" + rxn . id ) to_add . extend ( [ indicator , on_off_constraint , delta_g , delta_g_range ] ) model . add_cons_vars ( to_add ) for i , row in enumerate ( n_int ) : name = "nullspace_constraint_" + str ( i ) nullspace_constraint = prob . Constraint ( Zero , lb = 0 , ub = 0 , name = name ) model . add_cons_vars ( [ nullspace_constraint ] ) coefs = { model . variables [ "delta_g_" + model . reactions [ ridx ] . id ] : row [ i ] for i , ridx in enumerate ( internal ) if abs ( row [ i ] ) > zero_cutoff } model . constraints [ name ] . set_linear_coefficients ( coefs )
11589	def _rc_rpoplpush ( self , src , dst ) : rpop = self . rpop ( src ) if rpop is not None : self . lpush ( dst , rpop ) return rpop return None
7663	def pop_data ( self ) : data = self . data self . data = SortedKeyList ( key = self . _key ) return data
4174	def window_kaiser ( N , beta = 8.6 , method = 'numpy' ) : r if N == 1 : return ones ( 1 ) if method == 'numpy' : from numpy import kaiser return kaiser ( N , beta ) else : return _kaiser ( N , beta )
13263	def get_parameters ( self ) : if self . plugin_class is None : sig = inspect . signature ( self . func ) for index , parameter in enumerate ( sig . parameters . values ( ) ) : if not parameter . kind in [ parameter . POSITIONAL_ONLY , parameter . KEYWORD_ONLY , parameter . POSITIONAL_OR_KEYWORD ] : raise RuntimeError ( "Task {} contains an unsupported {} parameter" . format ( parameter , parameter . kind ) ) yield parameter else : var_keyword_seen = set ( ) for cls in inspect . getmro ( self . plugin_class ) : if issubclass ( cls , BasePlugin ) and hasattr ( cls , self . func . __name__ ) : func = getattr ( cls , self . func . __name__ ) logger . debug ( "Found method %s from class %s" , func , cls ) var_keyword_found = False sig = inspect . signature ( func ) for index , parameter in enumerate ( sig . parameters . values ( ) ) : if index == 0 : continue if parameter . kind == inspect . Parameter . VAR_KEYWORD : var_keyword_found = True continue if parameter . kind in [ parameter . POSITIONAL_ONLY , parameter . VAR_POSITIONAL ] : raise RuntimeError ( "Task {} contains an unsupported parameter \"{}\"" . format ( func , parameter ) ) if not parameter . name in var_keyword_seen : var_keyword_seen . add ( parameter . name ) logger . debug ( "Found parameter %s (%s)" , parameter , parameter . kind ) yield parameter if not var_keyword_found : break
8453	def clean ( ) : temple . check . in_git_repo ( ) current_branch = _get_current_branch ( ) update_branch = temple . constants . UPDATE_BRANCH_NAME temp_update_branch = temple . constants . TEMP_UPDATE_BRANCH_NAME if current_branch in ( update_branch , temp_update_branch ) : err_msg = ( 'You must change from the "{}" branch since it will be deleted during cleanup' ) . format ( current_branch ) raise temple . exceptions . InvalidCurrentBranchError ( err_msg ) if temple . check . _has_branch ( update_branch ) : temple . utils . shell ( 'git branch -D {}' . format ( update_branch ) ) if temple . check . _has_branch ( temp_update_branch ) : temple . utils . shell ( 'git branch -D {}' . format ( temp_update_branch ) )
8563	def delete_loadbalancer ( self , datacenter_id , loadbalancer_id ) : response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) , method = 'DELETE' ) return response
12769	def load_markers ( self , filename , attachments , max_frames = 1e100 ) : self . markers = Markers ( self ) fn = filename . lower ( ) if fn . endswith ( '.c3d' ) : self . markers . load_c3d ( filename , max_frames = max_frames ) elif fn . endswith ( '.csv' ) or fn . endswith ( '.csv.gz' ) : self . markers . load_csv ( filename , max_frames = max_frames ) else : logging . fatal ( '%s: not sure how to load markers!' , filename ) self . markers . load_attachments ( attachments , self . skeleton )
13621	def one ( func , n = 0 ) : def _one ( result ) : if _isSequenceTypeNotText ( result ) and len ( result ) > n : return func ( result [ n ] ) return None return maybe ( _one )
5530	def _process_worker ( process , process_tile ) : logger . debug ( ( process_tile . id , "running on %s" % current_process ( ) . name ) ) if ( process . config . mode == "continue" and process . config . output . tiles_exist ( process_tile ) ) : logger . debug ( ( process_tile . id , "tile exists, skipping" ) ) return ProcessInfo ( tile = process_tile , processed = False , process_msg = "output already exists" , written = False , write_msg = "nothing written" ) else : with Timer ( ) as t : try : output = process . execute ( process_tile , raise_nodata = True ) except MapcheteNodataTile : output = None processor_message = "processed in %s" % t logger . debug ( ( process_tile . id , processor_message ) ) writer_info = process . write ( process_tile , output ) return ProcessInfo ( tile = process_tile , processed = True , process_msg = processor_message , written = writer_info . written , write_msg = writer_info . write_msg )
6163	def QPSK_rx ( fc , N_symb , Rs , EsN0 = 100 , fs = 125 , lfsr_len = 10 , phase = 0 , pulse = 'src' ) : Ns = int ( np . round ( fs / Rs ) ) print ( 'Ns = ' , Ns ) print ( 'Rs = ' , fs / float ( Ns ) ) print ( 'EsN0 = ' , EsN0 , 'dB' ) print ( 'phase = ' , phase , 'degrees' ) print ( 'pulse = ' , pulse ) x , b , data = QPSK_bb ( N_symb , Ns , lfsr_len , pulse ) x = cpx_AWGN ( x , EsN0 , Ns ) n = np . arange ( len ( x ) ) xc = x * np . exp ( 1j * 2 * np . pi * fc / float ( fs ) * n ) * np . exp ( 1j * phase ) return xc , b , data
13430	def create_site ( self , params = { } ) : url = "/2/sites/" body = params data = self . _post_resource ( url , body ) return self . site_from_json ( data [ "site" ] )
8463	def _call_api ( self , verb , url , ** request_kwargs ) : api = 'https://api.github.com{}' . format ( url ) auth_headers = { 'Authorization' : 'token {}' . format ( self . api_token ) } headers = { ** auth_headers , ** request_kwargs . pop ( 'headers' , { } ) } return getattr ( requests , verb ) ( api , headers = headers , ** request_kwargs )
5153	def evaluate_vars ( data , context = None ) : context = context or { } if isinstance ( data , ( dict , list ) ) : if isinstance ( data , dict ) : loop_items = data . items ( ) elif isinstance ( data , list ) : loop_items = enumerate ( data ) for key , value in loop_items : data [ key ] = evaluate_vars ( value , context ) elif isinstance ( data , six . string_types ) : vars_found = var_pattern . findall ( data ) for var in vars_found : var = var . strip ( ) if len ( vars_found ) > 1 : pattern = r'\{\{(\s*%s\s*)\}\}' % var else : pattern = var_pattern if var in context : data = re . sub ( pattern , context [ var ] , data ) return data
7991	def _send_stream_start ( self , stream_id = None , stream_to = None ) : if self . _output_state in ( "open" , "closed" ) : raise StreamError ( "Stream start already sent" ) if not self . language : self . language = self . settings [ "language" ] if stream_to : stream_to = unicode ( stream_to ) elif self . peer and self . initiator : stream_to = unicode ( self . peer ) stream_from = None if self . me and ( self . tls_established or not self . initiator ) : stream_from = unicode ( self . me ) if stream_id : self . stream_id = stream_id else : self . stream_id = None self . transport . send_stream_head ( self . stanza_namespace , stream_from , stream_to , self . stream_id , language = self . language ) self . _output_state = "open"
8601	def delete_share ( self , group_id , resource_id ) : response = self . _perform_request ( url = '/um/groups/%s/shares/%s' % ( group_id , resource_id ) , method = 'DELETE' ) return response
4966	def clean ( self ) : cleaned_data = super ( ManageLearnersForm , self ) . clean ( ) email_or_username = self . data . get ( self . Fields . EMAIL_OR_USERNAME , None ) bulk_upload_csv = self . files . get ( self . Fields . BULK_UPLOAD , None ) if not email_or_username and not bulk_upload_csv : raise ValidationError ( ValidationMessages . NO_FIELDS_SPECIFIED ) if email_or_username and bulk_upload_csv : raise ValidationError ( ValidationMessages . BOTH_FIELDS_SPECIFIED ) if email_or_username : mode = self . Modes . MODE_SINGULAR else : mode = self . Modes . MODE_BULK cleaned_data [ self . Fields . MODE ] = mode cleaned_data [ self . Fields . NOTIFY ] = self . clean_notify ( ) self . _validate_course ( ) self . _validate_program ( ) if self . data . get ( self . Fields . PROGRAM , None ) and self . data . get ( self . Fields . COURSE , None ) : raise ValidationError ( ValidationMessages . COURSE_AND_PROGRAM_ERROR ) return cleaned_data
13639	def bind ( mod_path , with_path = None ) : if with_path : if os . path . isdir ( with_path ) : sys . path . insert ( 0 , with_path ) else : sys . path . insert ( 0 , with_path . rsplit ( '/' , 2 ) [ 0 ] ) pass mod = importlib . import_module ( mod_path ) settings = Settings ( ) for v in dir ( mod ) : if v [ 0 ] == '_' or type ( getattr ( mod , v ) ) . __name__ == 'module' : continue setattr ( settings , v , getattr ( mod , v ) ) pass Settings . _path = mod_path Settings . _wrapped = settings return settings
4510	def get ( name = None ) : if name is None or name == 'default' : return _DEFAULT_PALETTE if isinstance ( name , str ) : return PROJECT_PALETTES . get ( name ) or BUILT_IN_PALETTES . get ( name )
3404	def normalize_cutoff ( model , zero_cutoff = None ) : if zero_cutoff is None : return model . tolerance else : if zero_cutoff < model . tolerance : raise ValueError ( "The chosen zero cutoff cannot be less than the model's " "tolerance value." ) else : return zero_cutoff
6226	def rot_state ( self , x , y ) : if self . last_x is None : self . last_x = x if self . last_y is None : self . last_y = y x_offset = self . last_x - x y_offset = self . last_y - y self . last_x = x self . last_y = y x_offset *= self . mouse_sensitivity y_offset *= self . mouse_sensitivity self . yaw -= x_offset self . pitch += y_offset if self . pitch > 85.0 : self . pitch = 85.0 if self . pitch < - 85.0 : self . pitch = - 85.0 self . _update_yaw_and_pitch ( )
2532	def parse_ext_doc_ref ( self , ext_doc_ref_term ) : for _s , _p , o in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'externalDocumentId' ] , None ) ) : try : self . builder . set_ext_doc_id ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'External Document ID' ) break for _s , _p , o in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'spdxDocument' ] , None ) ) : try : self . builder . set_spdx_doc_uri ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'SPDX Document URI' ) break for _s , _p , checksum in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'checksum' ] , None ) ) : for _ , _ , value in self . graph . triples ( ( checksum , self . spdx_namespace [ 'checksumValue' ] , None ) ) : try : self . builder . set_chksum ( self . doc , six . text_type ( value ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'Checksum' ) break
443	def _get_init_args ( self , skip = 4 ) : stack = inspect . stack ( ) if len ( stack ) < skip + 1 : raise ValueError ( "The length of the inspection stack is shorter than the requested start position." ) args , _ , _ , values = inspect . getargvalues ( stack [ skip ] [ 0 ] ) params = { } for arg in args : if values [ arg ] is not None and arg not in [ 'self' , 'prev_layer' , 'inputs' ] : val = values [ arg ] if inspect . isfunction ( val ) : params [ arg ] = { "module_path" : val . __module__ , "func_name" : val . __name__ } elif arg . endswith ( 'init' ) : continue else : params [ arg ] = val return params
743	def writeToFile ( self , f , packed = True ) : schema = self . getSchema ( ) proto = schema . new_message ( ) self . write ( proto ) if packed : proto . write_packed ( f ) else : proto . write ( f )
1665	def CheckRedundantVirtual ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] virtual = Match ( r'^(.*)(\bvirtual\b)(.*)$' , line ) if not virtual : return if ( Search ( r'\b(public|protected|private)\s+$' , virtual . group ( 1 ) ) or Match ( r'^\s+(public|protected|private)\b' , virtual . group ( 3 ) ) ) : return if Match ( r'^.*[^:]:[^:].*$' , line ) : return end_col = - 1 end_line = - 1 start_col = len ( virtual . group ( 2 ) ) for start_line in xrange ( linenum , min ( linenum + 3 , clean_lines . NumLines ( ) ) ) : line = clean_lines . elided [ start_line ] [ start_col : ] parameter_list = Match ( r'^([^(]*)\(' , line ) if parameter_list : ( _ , end_line , end_col ) = CloseExpression ( clean_lines , start_line , start_col + len ( parameter_list . group ( 1 ) ) ) break start_col = 0 if end_col < 0 : return for i in xrange ( end_line , min ( end_line + 3 , clean_lines . NumLines ( ) ) ) : line = clean_lines . elided [ i ] [ end_col : ] match = Search ( r'\b(override|final)\b' , line ) if match : error ( filename , linenum , 'readability/inheritance' , 4 , ( '"virtual" is redundant since function is ' 'already declared as "%s"' % match . group ( 1 ) ) ) end_col = 0 if Search ( r'[^\w]\s*$' , line ) : break
5233	def all_folders ( path_name , keyword = '' , has_date = False , date_fmt = DATE_FMT ) -> list : if not os . path . exists ( path = path_name ) : return [ ] path_name = path_name . replace ( '\\' , '/' ) if keyword : folders = sort_by_modified ( [ f . replace ( '\\' , '/' ) for f in glob . iglob ( f'{path_name}/*{keyword}*' ) if os . path . isdir ( f ) and ( f . replace ( '\\' , '/' ) . split ( '/' ) [ - 1 ] [ 0 ] != '~' ) ] ) else : folders = sort_by_modified ( [ f'{path_name}/{f}' for f in os . listdir ( path = path_name ) if os . path . isdir ( f'{path_name}/{f}' ) and ( f [ 0 ] != '~' ) ] ) if has_date : folders = filter_by_dates ( folders , date_fmt = date_fmt ) return folders
7706	def load_roster ( self , source ) : try : tree = ElementTree . parse ( source ) except ElementTree . ParseError , err : raise ValueError ( "Invalid roster format: {0}" . format ( err ) ) roster = Roster . from_xml ( tree . getroot ( ) ) for item in roster : item . verify_roster_result ( True ) self . roster = roster
9801	def config ( list ) : if list : _config = GlobalConfigManager . get_config_or_default ( ) Printer . print_header ( 'Current config:' ) dict_tabulate ( _config . to_dict ( ) )
4591	def to_triplets ( colors ) : try : colors [ 0 ] [ 0 ] return colors except : pass extra = len ( colors ) % 3 if extra : colors = colors [ : - extra ] return list ( zip ( * [ iter ( colors ) ] * 3 ) )
11856	def extender ( self , edge ) : "See what edges can be extended by this edge." ( j , k , B , _ , _ ) = edge for ( i , j , A , alpha , B1b ) in self . chart [ j ] : if B1b and B == B1b [ 0 ] : self . add_edge ( [ i , k , A , alpha + [ edge ] , B1b [ 1 : ] ] )
2385	def from_spec_resolver ( cls , spec_resolver ) : deref = DerefValidatorDecorator ( spec_resolver ) for key , validator_callable in iteritems ( cls . validators ) : yield key , deref ( validator_callable )
4101	def mdl_eigen ( s , N ) : r import numpy as np kmdl = [ ] n = len ( s ) for k in range ( 0 , n - 1 ) : ak = 1. / ( n - k ) * np . sum ( s [ k + 1 : ] ) gk = np . prod ( s [ k + 1 : ] ** ( 1. / ( n - k ) ) ) kmdl . append ( - ( n - k ) * N * np . log ( gk / ak ) + 0.5 * k * ( 2. * n - k ) * np . log ( N ) ) return kmdl
4239	def config_finish ( self ) : _LOGGER . info ( "Config finish" ) if not self . config_started : return True success , _ = self . _make_request ( SERVICE_DEVICE_CONFIG , "ConfigurationFinished" , { "NewStatus" : "ChangesApplied" } ) self . config_started = not success return success
13856	def getbalance ( self , url = 'http://services.ambientmobile.co.za/credits' ) : postXMLList = [ ] postXMLList . append ( "<api-key>%s</api-key>" % self . api_key ) postXMLList . append ( "<password>%s</password>" % self . password ) postXML = '<sms>%s</sms>' % "" . join ( postXMLList ) result = self . curl ( url , postXML ) if result . get ( "credits" , None ) : return result [ "credits" ] else : raise AmbientSMSError ( result [ "status" ] )
2576	def _add_input_deps ( self , executor , args , kwargs ) : if executor == 'data_manager' : return args , kwargs inputs = kwargs . get ( 'inputs' , [ ] ) for idx , f in enumerate ( inputs ) : if isinstance ( f , File ) and f . is_remote ( ) : inputs [ idx ] = self . data_manager . stage_in ( f , executor ) for kwarg , f in kwargs . items ( ) : if isinstance ( f , File ) and f . is_remote ( ) : kwargs [ kwarg ] = self . data_manager . stage_in ( f , executor ) newargs = list ( args ) for idx , f in enumerate ( newargs ) : if isinstance ( f , File ) and f . is_remote ( ) : newargs [ idx ] = self . data_manager . stage_in ( f , executor ) return tuple ( newargs ) , kwargs
9227	def NextPage ( gh ) : header = dict ( gh . getheaders ( ) ) if 'Link' in header : parts = header [ 'Link' ] . split ( ',' ) for part in parts : subparts = part . split ( ';' ) sub = subparts [ 1 ] . split ( '=' ) if sub [ 0 ] . strip ( ) == 'rel' : if sub [ 1 ] == '"next"' : page = int ( re . match ( r'.*page=(\d+).*' , subparts [ 0 ] , re . IGNORECASE | re . DOTALL | re . UNICODE ) . groups ( ) [ 0 ] ) return page return 0
5600	def for_web ( self , data ) : rgba = self . _prepare_array_for_png ( data ) data = ma . masked_where ( rgba == self . nodata , rgba ) return memory_file ( data , self . profile ( ) ) , 'image/png'
13436	def _setup_positions ( self , positions ) : updated_positions = [ ] for i , position in enumerate ( positions ) : ranger = re . search ( r'(?P<start>-?\d*):(?P<end>\d*)' , position ) if ranger : if i > 0 : updated_positions . append ( self . separator ) start = group_val ( ranger . group ( 'start' ) ) end = group_val ( ranger . group ( 'end' ) ) if start and end : updated_positions . extend ( self . _extendrange ( start , end + 1 ) ) elif ranger . group ( 'start' ) : updated_positions . append ( [ start ] ) else : updated_positions . extend ( self . _extendrange ( 1 , end + 1 ) ) else : updated_positions . append ( positions [ i ] ) try : if int ( position ) and int ( positions [ i + 1 ] ) : updated_positions . append ( self . separator ) except ( ValueError , IndexError ) : pass return updated_positions
9988	def new_cells ( self , name = None , formula = None ) : return self . _impl . new_cells ( name , formula ) . interface
4527	def report ( function , * args , ** kwds ) : try : function ( * args , ** kwds ) except Exception : traceback . print_exc ( )
7497	def nworker ( data , smpchunk , tests ) : with h5py . File ( data . database . input , 'r' ) as io5 : seqview = io5 [ "bootsarr" ] [ : ] maparr = io5 [ "bootsmap" ] [ : ] nall_mask = seqview [ : ] == 78 rquartets = np . zeros ( ( smpchunk . shape [ 0 ] , 4 ) , dtype = np . uint16 ) rweights = None rdstats = np . zeros ( ( smpchunk . shape [ 0 ] , 4 ) , dtype = np . uint32 ) for idx in xrange ( smpchunk . shape [ 0 ] ) : sidx = smpchunk [ idx ] seqchunk = seqview [ sidx ] nmask = np . any ( nall_mask [ sidx ] , axis = 0 ) nmask += np . all ( seqchunk == seqchunk [ 0 ] , axis = 0 ) bidx , qstats = calculate ( seqchunk , maparr [ : , 0 ] , nmask , tests ) rdstats [ idx ] = qstats rquartets [ idx ] = smpchunk [ idx ] [ bidx ] return rquartets , rweights , rdstats
12588	def insert_volumes_in_one_dataset ( file_path , h5path , file_list , newshape = None , concat_axis = 0 , dtype = None , append = True ) : def isalambda ( v ) : return isinstance ( v , type ( lambda : None ) ) and v . __name__ == '<lambda>' mode = 'w' if os . path . exists ( file_path ) : if append : mode = 'a' imgs = [ nib . load ( vol ) for vol in file_list ] shapes = [ np . array ( img . get_shape ( ) ) for img in imgs ] if newshape is not None : if isalambda ( newshape ) : nushapes = np . array ( [ newshape ( shape ) for shape in shapes ] ) else : nushapes = np . array ( [ shape for shape in shapes ] ) for nushape in nushapes : assert ( len ( nushape ) - 1 < concat_axis ) n_dims = nushapes . shape [ 1 ] ds_shape = np . zeros ( n_dims , dtype = np . int ) for a in list ( range ( n_dims ) ) : if a == concat_axis : ds_shape [ a ] = np . sum ( nushapes [ : , concat_axis ] ) else : ds_shape [ a ] = np . max ( nushapes [ : , a ] ) if dtype is None : dtype = imgs [ 0 ] . get_data_dtype ( ) with h5py . File ( file_path , mode ) as f : try : ic = 0 h5grp = f . create_group ( os . path . dirname ( h5path ) ) h5ds = h5grp . create_dataset ( os . path . basename ( h5path ) , ds_shape , dtype ) for img in imgs : nushape = nushapes [ ic , : ] def append_to_dataset ( h5ds , idx , data , concat_axis ) : shape = data . shape ndims = len ( shape ) if ndims == 1 : if concat_axis == 0 : h5ds [ idx ] = data elif ndims == 2 : if concat_axis == 0 : h5ds [ idx ] = data elif concat_axis == 1 : h5ds [ idx ] = data elif ndims == 3 : if concat_axis == 0 : h5ds [ idx ] = data elif concat_axis == 1 : h5ds [ idx ] = data elif concat_axis == 2 : h5ds [ idx ] = data append_to_dataset ( h5ds , ic , np . reshape ( img . get_data ( ) , tuple ( nushape ) ) , concat_axis ) ic += 1 except ValueError as ve : raise Exception ( 'Error creating group {} in hdf file {}' . format ( h5path , file_path ) ) from ve
184	def coords_almost_equals ( self , other , max_distance = 1e-6 , points_per_edge = 8 ) : if isinstance ( other , LineString ) : pass elif isinstance ( other , tuple ) : other = LineString ( [ other ] ) else : other = LineString ( other ) if len ( self . coords ) == 0 and len ( other . coords ) == 0 : return True elif 0 in [ len ( self . coords ) , len ( other . coords ) ] : return False self_subd = self . subdivide ( points_per_edge ) other_subd = other . subdivide ( points_per_edge ) dist_self2other = self_subd . compute_pointwise_distances ( other_subd ) dist_other2self = other_subd . compute_pointwise_distances ( self_subd ) dist = max ( np . max ( dist_self2other ) , np . max ( dist_other2self ) ) return dist < max_distance
12186	async def join_rtm ( self , filters = None ) : if filters is None : filters = [ cls ( self ) for cls in self . MESSAGE_FILTERS ] url = await self . _get_socket_url ( ) logger . debug ( 'Connecting to %r' , url ) async with ws_connect ( url ) as socket : first_msg = await socket . receive ( ) self . _validate_first_message ( first_msg ) self . socket = socket async for message in socket : if message . tp == MsgType . text : await self . handle_message ( message , filters ) elif message . tp in ( MsgType . closed , MsgType . error ) : if not socket . closed : await socket . close ( ) self . socket = None break logger . info ( 'Left real-time messaging.' )
7496	def calculate ( seqnon , mapcol , nmask , tests ) : mats = chunk_to_matrices ( seqnon , mapcol , nmask ) svds = np . zeros ( ( 3 , 16 ) , dtype = np . float64 ) qscores = np . zeros ( 3 , dtype = np . float64 ) ranks = np . zeros ( 3 , dtype = np . float64 ) for test in range ( 3 ) : svds [ test ] = np . linalg . svd ( mats [ test ] . astype ( np . float64 ) ) [ 1 ] ranks [ test ] = np . linalg . matrix_rank ( mats [ test ] . astype ( np . float64 ) ) minrank = int ( min ( 11 , ranks . min ( ) ) ) for test in range ( 3 ) : qscores [ test ] = np . sqrt ( np . sum ( svds [ test , minrank : ] ** 2 ) ) best = np . where ( qscores == qscores . min ( ) ) [ 0 ] bidx = tests [ best ] [ 0 ] qsnps = count_snps ( mats [ best ] [ 0 ] ) return bidx , qsnps
7491	def compute_tree_stats ( self , ipyclient ) : names = self . samples if self . params . nboots : fulltre = ete3 . Tree ( self . trees . tree , format = 0 ) fulltre . unroot ( ) with open ( self . trees . boots , 'r' ) as inboots : bb = [ ete3 . Tree ( i . strip ( ) , format = 0 ) for i in inboots . readlines ( ) ] wboots = [ fulltre ] + bb [ - self . params . nboots : ] wctre , wcounts = consensus_tree ( wboots , names = names ) self . trees . cons = os . path . join ( self . dirs , self . name + ".cons" ) with open ( self . trees . cons , 'w' ) as ocons : ocons . write ( wctre . write ( format = 0 ) ) else : wctre = ete3 . Tree ( self . trees . tree , format = 0 ) wctre . unroot ( ) self . trees . nhx = os . path . join ( self . dirs , self . name + ".nhx" ) with open ( self . files . stats , 'w' ) as ostats : if self . params . nboots : ostats . write ( "## splits observed in {} trees\n" . format ( len ( wboots ) ) ) for i , j in enumerate ( self . samples ) : ostats . write ( "{:<3} {}\n" . format ( i , j ) ) ostats . write ( "\n" ) for split , freq in wcounts : if split . count ( '1' ) > 1 : ostats . write ( "{} {:.2f}\n" . format ( split , round ( freq , 2 ) ) ) ostats . write ( "\n" ) lbview = ipyclient . load_balanced_view ( ) qtots = { } qsamp = { } tots = sum ( 1 for i in wctre . iter_leaves ( ) ) totn = set ( wctre . get_leaf_names ( ) ) for node in wctre . traverse ( ) : qtots [ node ] = lbview . apply ( _get_total , * ( tots , node ) ) qsamp [ node ] = lbview . apply ( _get_sampled , * ( self , totn , node ) ) ipyclient . wait ( ) for node in wctre . traverse ( ) : total = qtots [ node ] . result ( ) sampled = qsamp [ node ] . result ( ) node . add_feature ( "quartets_total" , total ) node . add_feature ( "quartets_sampled" , sampled ) features = [ "quartets_total" , "quartets_sampled" ] with open ( self . trees . nhx , 'w' ) as outtre : outtre . write ( wctre . write ( format = 0 , features = features ) )
11122	def add_directory ( self , relativePath , info = None ) : path = os . path . normpath ( relativePath ) currentDir = self . path currentDict = self if path in ( "" , "." ) : return currentDict save = False for dir in path . split ( os . sep ) : dirPath = os . path . join ( currentDir , dir ) if not os . path . exists ( dirPath ) : os . mkdir ( dirPath ) currentDict = dict . __getitem__ ( currentDict , "directories" ) if currentDict . get ( dir , None ) is None : save = True currentDict [ dir ] = { "directories" : { } , "files" : { } , "timestamp" : datetime . utcnow ( ) , "id" : str ( uuid . uuid1 ( ) ) , "info" : info } currentDict = currentDict [ dir ] currentDir = dirPath if save : self . save ( ) return currentDict
3521	def performable ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return PerformableNode ( )
9923	def save ( self ) : try : email = models . EmailAddress . objects . get ( email = self . validated_data [ "email" ] , is_verified = False ) logger . debug ( "Resending verification email to %s" , self . validated_data [ "email" ] , ) email . send_confirmation ( ) except models . EmailAddress . DoesNotExist : logger . debug ( "Not resending verification email to %s because the address " "doesn't exist in the database." , self . validated_data [ "email" ] , )
1470	def getStmgrsRegSummary ( self , tmaster , callback = None ) : if not tmaster or not tmaster . host or not tmaster . stats_port : return reg_request = tmaster_pb2 . StmgrsRegistrationSummaryRequest ( ) request_str = reg_request . SerializeToString ( ) port = str ( tmaster . stats_port ) host = tmaster . host url = "http://{0}:{1}/stmgrsregistrationsummary" . format ( host , port ) request = tornado . httpclient . HTTPRequest ( url , body = request_str , method = 'POST' , request_timeout = 5 ) Log . debug ( 'Making HTTP call to fetch stmgrsregistrationsummary url: %s' , url ) try : client = tornado . httpclient . AsyncHTTPClient ( ) result = yield client . fetch ( request ) Log . debug ( "HTTP call complete." ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) ) responseCode = result . code if responseCode >= 400 : message = "Error in getting exceptions from Tmaster, code: " + responseCode Log . error ( message ) raise tornado . gen . Return ( { "message" : message } ) reg_response = tmaster_pb2 . StmgrsRegistrationSummaryResponse ( ) reg_response . ParseFromString ( result . body ) ret = { } for stmgr in reg_response . registered_stmgrs : ret [ stmgr ] = True for stmgr in reg_response . absent_stmgrs : ret [ stmgr ] = False raise tornado . gen . Return ( ret )
12713	def join_to ( self , joint , other_body = None , ** kwargs ) : self . world . join ( joint , self , other_body , ** kwargs )
7800	def encode ( self ) : if self . data is None : return "" elif not self . data : return "=" else : ret = standard_b64encode ( self . data ) return ret . decode ( "us-ascii" )
9850	def export ( self , filename , file_format = None , type = None , typequote = '"' ) : exporter = self . _get_exporter ( filename , file_format = file_format ) exporter ( filename , type = type , typequote = typequote )
13458	def download_s3 ( bucket_name , file_key , file_path , force = False ) : file_path = path ( file_path ) bucket = open_s3 ( bucket_name ) file_dir = file_path . dirname ( ) file_dir . makedirs ( ) s3_key = bucket . get_key ( file_key ) if file_path . exists ( ) : file_data = file_path . bytes ( ) file_md5 , file_md5_64 = s3_key . get_md5_from_hexdigest ( hashlib . md5 ( file_data ) . hexdigest ( ) ) try : s3_md5 = s3_key . etag . replace ( '"' , '' ) except KeyError : pass else : if s3_md5 == file_md5 : info ( 'Hash is the same. Skipping %s' % file_path ) return elif not force : s3_datetime = datetime . datetime ( * time . strptime ( s3_key . last_modified , '%a, %d %b %Y %H:%M:%S %Z' ) [ 0 : 6 ] ) local_datetime = datetime . datetime . utcfromtimestamp ( file_path . stat ( ) . st_mtime ) if s3_datetime < local_datetime : info ( "File at %s is less recent than the local version." % ( file_key ) ) return info ( "Downloading %s..." % ( file_key ) ) try : with open ( file_path , 'w' ) as fo : s3_key . get_contents_to_file ( fo ) except Exception as e : error ( "Failed: %s" % e ) raise
10809	def update ( self , name = None , description = None , privacy_policy = None , subscription_policy = None , is_managed = None ) : with db . session . begin_nested ( ) : if name is not None : self . name = name if description is not None : self . description = description if ( privacy_policy is not None and PrivacyPolicy . validate ( privacy_policy ) ) : self . privacy_policy = privacy_policy if ( subscription_policy is not None and SubscriptionPolicy . validate ( subscription_policy ) ) : self . subscription_policy = subscription_policy if is_managed is not None : self . is_managed = is_managed db . session . merge ( self ) return self
1723	def translate_file ( input_path , output_path ) : js = get_file_contents ( input_path ) py_code = translate_js ( js ) lib_name = os . path . basename ( output_path ) . split ( '.' ) [ 0 ] head = '__all__ = [%s]\n\n# Don\'t look below, you will not understand this Python code :) I don\'t.\n\n' % repr ( lib_name ) tail = '\n\n# Add lib to the module scope\n%s = var.to_python()' % lib_name out = head + py_code + tail write_file_contents ( output_path , out )
5593	def tiles_from_geom ( self , geometry , zoom ) : for tile in self . tile_pyramid . tiles_from_geom ( geometry , zoom ) : yield self . tile ( * tile . id )
1558	def _get_stream_schema ( fields ) : stream_schema = topology_pb2 . StreamSchema ( ) for field in fields : key = stream_schema . keys . add ( ) key . key = field key . type = topology_pb2 . Type . Value ( "OBJECT" ) return stream_schema
8747	def create_scalingip ( context , content ) : LOG . info ( 'create_scalingip for tenant %s and body %s' , context . tenant_id , content ) network_id = content . get ( 'scaling_network_id' ) ip_address = content . get ( 'scaling_ip_address' ) requested_ports = content . get ( 'ports' , [ ] ) network = _get_network ( context , network_id ) port_fixed_ips = { } for req_port in requested_ports : port = _get_port ( context , req_port [ 'port_id' ] ) fixed_ip = _get_fixed_ip ( context , req_port . get ( 'fixed_ip_address' ) , port ) port_fixed_ips [ port . id ] = { "port" : port , "fixed_ip" : fixed_ip } scip = _allocate_ip ( context , network , None , ip_address , ip_types . SCALING ) _create_flip ( context , scip , port_fixed_ips ) return v . _make_scaling_ip_dict ( scip )
8722	def operation_list ( uploader ) : files = uploader . file_list ( ) for f in files : log . info ( "{file:30s} {size}" . format ( file = f [ 0 ] , size = f [ 1 ] ) )
13305	def foex ( a , b ) : return ( np . sum ( a > b , dtype = float ) / len ( a ) - 0.5 ) * 100
7237	def randwindow ( self , window_shape ) : row = random . randrange ( window_shape [ 0 ] , self . shape [ 1 ] ) col = random . randrange ( window_shape [ 1 ] , self . shape [ 2 ] ) return self [ : , row - window_shape [ 0 ] : row , col - window_shape [ 1 ] : col ]
12324	def save ( self ) : if self . code : raise HolviError ( "Orders cannot be updated" ) send_json = self . to_holvi_dict ( ) send_json . update ( { 'pool' : self . api . connection . pool } ) url = six . u ( self . api . base_url + "order/" ) stat = self . api . connection . make_post ( url , send_json ) code = stat [ "details_uri" ] . split ( "/" ) [ - 2 ] return ( stat [ "checkout_uri" ] , self . api . get_order ( code ) )
9746	def datagram_received ( self , datagram , address ) : size , _ = RTheader . unpack_from ( datagram , 0 ) info , = struct . unpack_from ( "{0}s" . format ( size - 3 - 8 ) , datagram , RTheader . size ) base_port , = QRTDiscoveryBasePort . unpack_from ( datagram , size - 2 ) if self . receiver is not None : self . receiver ( QRTDiscoveryResponse ( info , address [ 0 ] , base_port ) )
10412	def summarize_node_filter ( graph : BELGraph , node_filters : NodePredicates ) -> None : passed = count_passed_node_filter ( graph , node_filters ) print ( '{}/{} nodes passed' . format ( passed , graph . number_of_nodes ( ) ) )
9851	def _export_python ( self , filename , ** kwargs ) : data = dict ( grid = self . grid , edges = self . edges , metadata = self . metadata ) with open ( filename , 'wb' ) as f : cPickle . dump ( data , f , cPickle . HIGHEST_PROTOCOL )
6578	def _base_repr ( self , and_also = None ) : items = [ "=" . join ( ( key , repr ( getattr ( self , key ) ) ) ) for key in sorted ( self . _fields . keys ( ) ) ] if items : output = ", " . join ( items ) else : output = None if and_also : return "{}({}, {})" . format ( self . __class__ . __name__ , output , and_also ) else : return "{}({})" . format ( self . __class__ . __name__ , output )
1341	def imagenet_example ( shape = ( 224 , 224 ) , data_format = 'channels_last' ) : assert len ( shape ) == 2 assert data_format in [ 'channels_first' , 'channels_last' ] from PIL import Image path = os . path . join ( os . path . dirname ( __file__ ) , 'example.png' ) image = Image . open ( path ) image = image . resize ( shape ) image = np . asarray ( image , dtype = np . float32 ) image = image [ : , : , : 3 ] assert image . shape == shape + ( 3 , ) if data_format == 'channels_first' : image = np . transpose ( image , ( 2 , 0 , 1 ) ) return image , 282
2470	def set_file_notice ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_notice_set : self . file_notice_set = True if validations . validate_file_notice ( text ) : self . file ( doc ) . notice = str_from_text ( text ) else : raise SPDXValueError ( 'File::Notice' ) else : raise CardinalityError ( 'File::Notice' ) else : raise OrderError ( 'File::Notice' )
8871	def match ( self , subsetLines , offsetOfSubset , fileName ) : for ( offset , l ) in enumerate ( subsetLines ) : column = l . find ( self . literal ) if column != - 1 : truePosition = offset + offsetOfSubset _logger . debug ( 'Found match on line {}, col {}' . format ( str ( truePosition + 1 ) , column ) ) _logger . debug ( 'Line is {}' . format ( l ) ) self . matchLocation = CheckFileParser . FileLocation ( fileName , truePosition + 1 ) return truePosition self . failed = True raise DirectiveException ( self )
4131	def get_docstring_and_rest ( filename ) : with open ( filename ) as f : content = f . read ( ) node = ast . parse ( content ) if not isinstance ( node , ast . Module ) : raise TypeError ( "This function only supports modules. " "You provided {0}" . format ( node . __class__ . __name__ ) ) if node . body and isinstance ( node . body [ 0 ] , ast . Expr ) and isinstance ( node . body [ 0 ] . value , ast . Str ) : docstring_node = node . body [ 0 ] docstring = docstring_node . value . s rest = content . split ( '\n' , docstring_node . lineno ) [ - 1 ] return docstring , rest else : raise ValueError ( ( 'Could not find docstring in file "{0}". ' 'A docstring is required by sphinx-gallery' ) . format ( filename ) )
7838	def set_node ( self , node ) : if node is None : if self . xmlnode . hasProp ( "node" ) : self . xmlnode . unsetProp ( "node" ) return node = unicode ( node ) self . xmlnode . setProp ( "node" , node . encode ( "utf-8" ) )
2320	def check_cuda_devices ( ) : import ctypes CUDA_SUCCESS = 0 libnames = ( 'libcuda.so' , 'libcuda.dylib' , 'cuda.dll' ) for libname in libnames : try : cuda = ctypes . CDLL ( libname ) except OSError : continue else : break else : return 0 nGpus = ctypes . c_int ( ) error_str = ctypes . c_char_p ( ) result = cuda . cuInit ( 0 ) if result != CUDA_SUCCESS : cuda . cuGetErrorString ( result , ctypes . byref ( error_str ) ) return 0 result = cuda . cuDeviceGetCount ( ctypes . byref ( nGpus ) ) if result != CUDA_SUCCESS : cuda . cuGetErrorString ( result , ctypes . byref ( error_str ) ) return 0 return nGpus . value
891	def _adaptSegment ( cls , connections , segment , prevActiveCells , permanenceIncrement , permanenceDecrement ) : synapsesToDestroy = [ ] for synapse in connections . synapsesForSegment ( segment ) : permanence = synapse . permanence if binSearch ( prevActiveCells , synapse . presynapticCell ) != - 1 : permanence += permanenceIncrement else : permanence -= permanenceDecrement permanence = max ( 0.0 , min ( 1.0 , permanence ) ) if permanence < EPSILON : synapsesToDestroy . append ( synapse ) else : connections . updateSynapsePermanence ( synapse , permanence ) for synapse in synapsesToDestroy : connections . destroySynapse ( synapse ) if connections . numSynapses ( segment ) == 0 : connections . destroySegment ( segment )
8798	def get_security_group_states ( self , interfaces ) : LOG . debug ( "Getting security groups from Redis for {0}" . format ( interfaces ) ) interfaces = tuple ( interfaces ) vif_keys = [ self . vif_key ( vif . device_id , vif . mac_address ) for vif in interfaces ] sec_grp_all = self . get_fields_all ( vif_keys ) ret = { } for vif , group in zip ( interfaces , sec_grp_all ) : if group : ret [ vif ] = { SECURITY_GROUP_ACK : None , SECURITY_GROUP_HASH_ATTR : [ ] } temp_ack = group [ SECURITY_GROUP_ACK ] . lower ( ) temp_rules = group [ SECURITY_GROUP_HASH_ATTR ] if temp_rules : temp_rules = json . loads ( temp_rules ) ret [ vif ] [ SECURITY_GROUP_HASH_ATTR ] = temp_rules [ "rules" ] if "true" in temp_ack : ret [ vif ] [ SECURITY_GROUP_ACK ] = True elif "false" in temp_ack : ret [ vif ] [ SECURITY_GROUP_ACK ] = False else : ret . pop ( vif , None ) LOG . debug ( "Skipping bad ack value %s" % temp_ack ) return ret
3149	def create ( self , list_id , data ) : self . list_id = list_id if 'url' not in data : raise KeyError ( 'The list webhook must have a url' ) check_url ( data [ 'url' ] ) response = self . _mc_client . _post ( url = self . _build_path ( list_id , 'webhooks' ) , data = data ) if response is not None : self . webhook_id = response [ 'id' ] else : self . webhook_id = None return response
7419	def sample_cleanup ( data , sample ) : umap1file = os . path . join ( data . dirs . edits , sample . name + "-tmp-umap1.fastq" ) umap2file = os . path . join ( data . dirs . edits , sample . name + "-tmp-umap2.fastq" ) unmapped = os . path . join ( data . dirs . refmapping , sample . name + "-unmapped.bam" ) samplesam = os . path . join ( data . dirs . refmapping , sample . name + ".sam" ) split1 = os . path . join ( data . dirs . edits , sample . name + "-split1.fastq" ) split2 = os . path . join ( data . dirs . edits , sample . name + "-split2.fastq" ) refmap_derep = os . path . join ( data . dirs . edits , sample . name + "-refmap_derep.fastq" ) for f in [ umap1file , umap2file , unmapped , samplesam , split1 , split2 , refmap_derep ] : try : os . remove ( f ) except : pass
7409	def get_order ( tre ) : anode = tre . tree & ">A" sister = anode . get_sisters ( ) [ 0 ] sisters = ( anode . name [ 1 : ] , sister . name [ 1 : ] ) others = [ i for i in list ( "ABCD" ) if i not in sisters ] return sorted ( sisters ) + sorted ( others )
4653	def add_required_fees ( self , ops , asset_id = "1.3.0" ) : ws = self . blockchain . rpc fees = ws . get_required_fees ( [ i . json ( ) for i in ops ] , asset_id ) for i , d in enumerate ( ops ) : if isinstance ( fees [ i ] , list ) : ops [ i ] . op . data [ "fee" ] = Asset ( amount = fees [ i ] [ 0 ] [ "amount" ] , asset_id = fees [ i ] [ 0 ] [ "asset_id" ] ) for j , _ in enumerate ( ops [ i ] . op . data [ "proposed_ops" ] . data ) : ops [ i ] . op . data [ "proposed_ops" ] . data [ j ] . data [ "op" ] . op . data [ "fee" ] = Asset ( amount = fees [ i ] [ 1 ] [ j ] [ "amount" ] , asset_id = fees [ i ] [ 1 ] [ j ] [ "asset_id" ] , ) else : ops [ i ] . op . data [ "fee" ] = Asset ( amount = fees [ i ] [ "amount" ] , asset_id = fees [ i ] [ "asset_id" ] ) return ops
4105	def xcorr ( x , y = None , maxlags = None , norm = 'biased' ) : N = len ( x ) if y is None : y = x assert len ( x ) == len ( y ) , 'x and y must have the same length. Add zeros if needed' if maxlags is None : maxlags = N - 1 lags = np . arange ( 0 , 2 * N - 1 ) else : assert maxlags <= N , 'maxlags must be less than data length' lags = np . arange ( N - maxlags - 1 , N + maxlags ) res = np . correlate ( x , y , mode = 'full' ) if norm == 'biased' : Nf = float ( N ) res = res [ lags ] / float ( N ) elif norm == 'unbiased' : res = res [ lags ] / ( float ( N ) - abs ( np . arange ( - N + 1 , N ) ) ) [ lags ] elif norm == 'coeff' : Nf = float ( N ) rms = pylab_rms_flat ( x ) * pylab_rms_flat ( y ) res = res [ lags ] / rms / Nf else : res = res [ lags ] lags = np . arange ( - maxlags , maxlags + 1 ) return res , lags
3876	async def _get_or_fetch_conversation ( self , conv_id ) : conv = self . _conv_dict . get ( conv_id , None ) if conv is None : logger . info ( 'Fetching unknown conversation %s' , conv_id ) res = await self . _client . get_conversation ( hangouts_pb2 . GetConversationRequest ( request_header = self . _client . get_request_header ( ) , conversation_spec = hangouts_pb2 . ConversationSpec ( conversation_id = hangouts_pb2 . ConversationId ( id = conv_id ) ) , include_event = False ) ) conv_state = res . conversation_state event_cont_token = None if conv_state . HasField ( 'event_continuation_token' ) : event_cont_token = conv_state . event_continuation_token return self . _add_conversation ( conv_state . conversation , event_cont_token = event_cont_token ) else : return conv
1907	def all_events ( cls ) : all_evts = set ( ) for cls , evts in cls . __all_events__ . items ( ) : all_evts . update ( evts ) return all_evts
4757	def rehome ( old , new , struct ) : if old == new : return if isinstance ( struct , list ) : for item in struct : rehome ( old , new , item ) elif isinstance ( struct , dict ) : for key , val in struct . iteritems ( ) : if isinstance ( val , ( dict , list ) ) : rehome ( old , new , val ) elif "conf" in key : continue elif "orig" in key : continue elif "root" in key or "path" in key : struct [ key ] = struct [ key ] . replace ( old , new )
10298	def get_incorrect_names ( graph : BELGraph ) -> Mapping [ str , Set [ str ] ] : return { namespace : get_incorrect_names_by_namespace ( graph , namespace ) for namespace in get_namespaces ( graph ) }
7143	def new_address ( self , label = None ) : return self . _backend . new_address ( account = self . index , label = label )
1223	def from_spec ( spec , kwargs = None ) : if isinstance ( spec , dict ) : spec = [ spec ] stack = PreprocessorStack ( ) for preprocessor_spec in spec : preprocessor_kwargs = copy . deepcopy ( kwargs ) preprocessor = util . get_object ( obj = preprocessor_spec , predefined_objects = tensorforce . core . preprocessors . preprocessors , kwargs = preprocessor_kwargs ) assert isinstance ( preprocessor , Preprocessor ) stack . preprocessors . append ( preprocessor ) return stack
7687	def multi_segment ( annotation , sr = 22050 , length = None , ** kwargs ) : PENT = [ 1 , 32. / 27 , 4. / 3 , 3. / 2 , 16. / 9 ] DURATION = 0.1 h_int , _ = hierarchy_flatten ( annotation ) if length is None : length = int ( sr * ( max ( np . max ( _ ) for _ in h_int ) + 1. / DURATION ) + 1 ) y = 0.0 for ints , ( oc , scale ) in zip ( h_int , product ( range ( 3 , 3 + len ( h_int ) ) , PENT ) ) : click = mkclick ( 440.0 * scale * oc , sr = sr , duration = DURATION ) y = y + filter_kwargs ( mir_eval . sonify . clicks , np . unique ( ints ) , fs = sr , length = length , click = click ) return y
9461	def conference_kick ( self , call_params ) : path = '/' + self . api_version + '/ConferenceKick/' method = 'POST' return self . request ( path , method , call_params )
2412	def gen_feats ( self , p_set ) : if self . _initialized != True : error_message = "Dictionaries have not been initialized." log . exception ( error_message ) raise util_functions . InputError ( p_set , error_message ) textual_features = [ ] for i in xrange ( 0 , len ( p_set . _essay_sets ) ) : textual_features . append ( self . _extractors [ i ] . gen_feats ( p_set . _essay_sets [ i ] ) ) textual_matrix = numpy . concatenate ( textual_features , axis = 1 ) predictor_matrix = numpy . array ( p_set . _numeric_features ) print textual_matrix . shape print predictor_matrix . shape overall_matrix = numpy . concatenate ( ( textual_matrix , predictor_matrix ) , axis = 1 ) return overall_matrix . copy ( )
9716	async def await_event ( self , event = None , timeout = 30 ) : return await self . _protocol . await_event ( event , timeout = timeout )
8129	def suggest_spelling ( q , wait = 10 , asynchronous = False , cached = False ) : return YahooSpelling ( q , wait , asynchronous , cached )
5004	def transmit ( self , payload , ** kwargs ) : kwargs [ 'app_label' ] = 'degreed' kwargs [ 'model_name' ] = 'DegreedLearnerDataTransmissionAudit' kwargs [ 'remote_user_id' ] = 'degreed_user_email' super ( DegreedLearnerTransmitter , self ) . transmit ( payload , ** kwargs )
621	def parseStringList ( s ) : assert isinstance ( s , basestring ) return [ int ( i ) for i in s . split ( ) ]
8537	def push ( self , ip_packet ) : data_len = len ( ip_packet . data . data ) seq_id = ip_packet . data . seq if data_len == 0 : self . _next_seq_id = seq_id return False if self . _next_seq_id != - 1 and seq_id != self . _next_seq_id : return False self . _next_seq_id = seq_id + data_len with self . _lock_packets : self . _length += len ( ip_packet . data . data ) self . _remaining += len ( ip_packet . data . data ) self . _packets . append ( ip_packet ) return True
12451	def deref ( self , data ) : deref = copy . deepcopy ( jsonref . JsonRef . replace_refs ( data ) ) self . write_template ( deref , filename = 'swagger.json' ) return deref
11369	def convert_date_to_iso ( value ) : date_formats = [ "%d %b %Y" , "%Y/%m/%d" ] for dformat in date_formats : try : date = datetime . strptime ( value , dformat ) return date . strftime ( "%Y-%m-%d" ) except ValueError : pass return value
12896	def get_volume_steps ( self ) : if not self . __volume_steps : self . __volume_steps = yield from self . handle_int ( self . API . get ( 'volume_steps' ) ) return self . __volume_steps
7587	def taxon_table ( self ) : if self . tests : keys = sorted ( self . tests [ 0 ] . keys ( ) ) if isinstance ( self . tests , list ) : ld = [ [ ( key , i [ key ] ) for key in keys ] for i in self . tests ] dd = [ dict ( i ) for i in ld ] df = pd . DataFrame ( dd ) return df else : return pd . DataFrame ( pd . Series ( self . tests ) ) . T else : return None
5295	def get_start_date ( self , obj ) : obj_date = getattr ( obj , self . get_date_field ( ) ) try : obj_date = obj_date . date ( ) except AttributeError : pass return obj_date
8431	def desaturate_pal ( color , prop , reverse = False ) : if not 0 <= prop <= 1 : raise ValueError ( "prop must be between 0 and 1" ) rgb = mcolors . colorConverter . to_rgb ( color ) h , l , s = colorsys . rgb_to_hls ( * rgb ) s *= prop desaturated_color = colorsys . hls_to_rgb ( h , l , s ) colors = [ color , desaturated_color ] if reverse : colors = colors [ : : - 1 ] return gradient_n_pal ( colors , name = 'desaturated' )
1323	def SetActive ( self , waitTime : float = OPERATION_WAIT_TIME ) -> bool : if self . IsTopLevel ( ) : handle = self . NativeWindowHandle if IsIconic ( handle ) : ret = ShowWindow ( handle , SW . Restore ) elif not IsWindowVisible ( handle ) : ret = ShowWindow ( handle , SW . Show ) ret = SetForegroundWindow ( handle ) time . sleep ( waitTime ) return ret return False
1293	def tf_demo_loss ( self , states , actions , terminal , reward , internals , update , reference = None ) : embedding = self . network . apply ( x = states , internals = internals , update = update ) deltas = list ( ) for name in sorted ( actions ) : action = actions [ name ] distr_params = self . distributions [ name ] . parameterize ( x = embedding ) state_action_value = self . distributions [ name ] . state_action_value ( distr_params = distr_params , action = action ) if self . actions_spec [ name ] [ 'type' ] == 'bool' : num_actions = 2 action = tf . cast ( x = action , dtype = util . tf_dtype ( 'int' ) ) else : num_actions = self . actions_spec [ name ] [ 'num_actions' ] one_hot = tf . one_hot ( indices = action , depth = num_actions ) ones = tf . ones_like ( tensor = one_hot , dtype = tf . float32 ) inverted_one_hot = ones - one_hot state_action_values = self . distributions [ name ] . state_action_value ( distr_params = distr_params ) state_action_values = state_action_values + inverted_one_hot * self . expert_margin supervised_selector = tf . reduce_max ( input_tensor = state_action_values , axis = - 1 ) delta = supervised_selector - state_action_value action_size = util . prod ( self . actions_spec [ name ] [ 'shape' ] ) delta = tf . reshape ( tensor = delta , shape = ( - 1 , action_size ) ) deltas . append ( delta ) loss_per_instance = tf . reduce_mean ( input_tensor = tf . concat ( values = deltas , axis = 1 ) , axis = 1 ) loss_per_instance = tf . square ( x = loss_per_instance ) return tf . reduce_mean ( input_tensor = loss_per_instance , axis = 0 )
11107	def walk_files_relative_path ( self , relativePath = "" ) : def walk_files ( directory , relativePath ) : directories = dict . __getitem__ ( directory , 'directories' ) files = dict . __getitem__ ( directory , 'files' ) for f in sorted ( files ) : yield os . path . join ( relativePath , f ) for k in sorted ( dict . keys ( directories ) ) : path = os . path . join ( relativePath , k ) dir = directories . __getitem__ ( k ) for e in walk_files ( dir , path ) : yield e dir , errorMessage = self . get_directory_info ( relativePath ) assert dir is not None , errorMessage return walk_files ( dir , relativePath = '' )
12357	def connect ( self , interactive = False ) : from poseidon . ssh import SSHClient rs = SSHClient ( self . ip_address , interactive = interactive ) return rs
1519	def read_and_parse_roles ( cl_args ) : roles = dict ( ) with open ( get_inventory_file ( cl_args ) , 'r' ) as stream : try : roles = yaml . load ( stream ) except yaml . YAMLError as exc : Log . error ( "Error parsing inventory file: %s" % exc ) sys . exit ( - 1 ) if Role . ZOOKEEPERS not in roles or not roles [ Role . ZOOKEEPERS ] : Log . error ( "Zookeeper servers node defined!" ) sys . exit ( - 1 ) if Role . CLUSTER not in roles or not roles [ Role . CLUSTER ] : Log . error ( "Heron cluster nodes defined!" ) sys . exit ( - 1 ) roles [ Role . MASTERS ] = set ( [ roles [ Role . CLUSTER ] [ 0 ] ] ) roles [ Role . SLAVES ] = set ( roles [ Role . CLUSTER ] ) roles [ Role . ZOOKEEPERS ] = set ( roles [ Role . ZOOKEEPERS ] ) roles [ Role . CLUSTER ] = set ( roles [ Role . CLUSTER ] ) return roles
13566	def plot ( * args , ax = None , ** kwargs ) : if ax is None : fig , ax = _setup_axes ( ) pl = ax . plot ( * args , ** kwargs ) if _np . shape ( args ) [ 0 ] > 1 : if type ( args [ 1 ] ) is not str : min_x = min ( args [ 0 ] ) max_x = max ( args [ 0 ] ) ax . set_xlim ( ( min_x , max_x ) ) return pl
8815	def update_interfaces ( self , added_sg , updated_sg , removed_sg ) : if not ( added_sg or updated_sg or removed_sg ) : return with self . sessioned ( ) as session : self . _set_security_groups ( session , added_sg ) self . _unset_security_groups ( session , removed_sg ) combined = added_sg + updated_sg + removed_sg self . _refresh_interfaces ( session , combined )
9672	def iteration ( self ) : i = 0 conv = np . inf old_conv = - np . inf conv_list = [ ] m = self . original if isinstance ( self . original , pd . DataFrame ) : ipfn_method = self . ipfn_df elif isinstance ( self . original , np . ndarray ) : ipfn_method = self . ipfn_np self . original = self . original . astype ( 'float64' ) else : print ( 'Data input instance not recognized' ) sys . exit ( 0 ) while ( ( i <= self . max_itr and conv > self . conv_rate ) and ( i <= self . max_itr and abs ( conv - old_conv ) > self . rate_tolerance ) ) : old_conv = conv m , conv = ipfn_method ( m , self . aggregates , self . dimensions , self . weight_col ) conv_list . append ( conv ) i += 1 converged = 1 if i <= self . max_itr : if not conv > self . conv_rate : print ( 'ipfn converged: convergence_rate below threshold' ) elif not abs ( conv - old_conv ) > self . rate_tolerance : print ( 'ipfn converged: convergence_rate not updating or below rate_tolerance' ) else : print ( 'Maximum iterations reached' ) converged = 0 if self . verbose == 0 : return m elif self . verbose == 1 : return m , converged elif self . verbose == 2 : return m , converged , pd . DataFrame ( { 'iteration' : range ( i ) , 'conv' : conv_list } ) . set_index ( 'iteration' ) else : print ( 'wrong verbose input, return None' ) sys . exit ( 0 )
1164	def start ( self ) : if not self . __initialized : raise RuntimeError ( "thread.__init__() not called" ) if self . __started . is_set ( ) : raise RuntimeError ( "threads can only be started once" ) if __debug__ : self . _note ( "%s.start(): starting thread" , self ) with _active_limbo_lock : _limbo [ self ] = self try : _start_new_thread ( self . __bootstrap , ( ) ) except Exception : with _active_limbo_lock : del _limbo [ self ] raise self . __started . wait ( )
8864	def icon_from_typename ( name , icon_type ) : ICONS = { 'CLASS' : ICON_CLASS , 'IMPORT' : ICON_NAMESPACE , 'STATEMENT' : ICON_VAR , 'FORFLOW' : ICON_VAR , 'FORSTMT' : ICON_VAR , 'WITHSTMT' : ICON_VAR , 'GLOBALSTMT' : ICON_VAR , 'MODULE' : ICON_NAMESPACE , 'KEYWORD' : ICON_KEYWORD , 'PARAM' : ICON_VAR , 'ARRAY' : ICON_VAR , 'INSTANCEELEMENT' : ICON_VAR , 'INSTANCE' : ICON_VAR , 'PARAM-PRIV' : ICON_VAR , 'PARAM-PROT' : ICON_VAR , 'FUNCTION' : ICON_FUNC , 'DEF' : ICON_FUNC , 'FUNCTION-PRIV' : ICON_FUNC_PRIVATE , 'FUNCTION-PROT' : ICON_FUNC_PROTECTED } ret_val = None icon_type = icon_type . upper ( ) if hasattr ( name , "string" ) : name = name . string if icon_type == "FORFLOW" or icon_type == "STATEMENT" : icon_type = "PARAM" if icon_type == "PARAM" or icon_type == "FUNCTION" : if name . startswith ( "__" ) : icon_type += "-PRIV" elif name . startswith ( "_" ) : icon_type += "-PROT" if icon_type in ICONS : ret_val = ICONS [ icon_type ] elif icon_type : _logger ( ) . warning ( "Unimplemented completion icon_type: %s" , icon_type ) return ret_val
11177	def parsestr ( self , argstr ) : argv = shlex . split ( argstr , comments = True ) if len ( argv ) != 1 : raise BadNumberOfArguments ( 1 , len ( argv ) ) arg = argv [ 0 ] lower = arg . lower ( ) if lower in self . true : return True if lower in self . false : return False raise BadArgument ( arg , "Allowed values are " + self . allowed + '.' )
12539	def find_all_dicom_files ( root_path ) : dicoms = set ( ) try : for fpath in get_all_files ( root_path ) : if is_dicom_file ( fpath ) : dicoms . add ( fpath ) except IOError as ioe : raise IOError ( 'Error reading file {0}.' . format ( fpath ) ) from ioe return dicoms
6973	def epd_magseries ( times , mags , errs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd , magsarefluxes = False , epdsmooth_sigclip = 3.0 , epdsmooth_windowsize = 21 , epdsmooth_func = smooth_magseries_savgol , epdsmooth_extraparams = None ) : finind = np . isfinite ( times ) & np . isfinite ( mags ) & np . isfinite ( errs ) ftimes , fmags , ferrs = times [ : : ] [ finind ] , mags [ : : ] [ finind ] , errs [ : : ] [ finind ] ffsv , ffdv , ffkv , fxcc , fycc , fbgv , fbge , fiha , fizd = ( fsv [ : : ] [ finind ] , fdv [ : : ] [ finind ] , fkv [ : : ] [ finind ] , xcc [ : : ] [ finind ] , ycc [ : : ] [ finind ] , bgv [ : : ] [ finind ] , bge [ : : ] [ finind ] , iha [ : : ] [ finind ] , izd [ : : ] [ finind ] , ) stimes , smags , serrs , separams = sigclip_magseries_with_extparams ( times , mags , errs , [ fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ] , sigclip = epdsmooth_sigclip , magsarefluxes = magsarefluxes ) sfsv , sfdv , sfkv , sxcc , sycc , sbgv , sbge , siha , sizd = separams if isinstance ( epdsmooth_extraparams , dict ) : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize , ** epdsmooth_extraparams ) else : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize ) initcoeffs = np . zeros ( 22 ) leastsqfit = leastsq ( _epd_residual , initcoeffs , args = ( smoothedmags , sfsv , sfdv , sfkv , sxcc , sycc , sbgv , sbge , siha , sizd ) , full_output = True ) if leastsqfit [ - 1 ] in ( 1 , 2 , 3 , 4 ) : fitcoeffs = leastsqfit [ 0 ] epdfit = _epd_function ( fitcoeffs , ffsv , ffdv , ffkv , fxcc , fycc , fbgv , fbge , fiha , fizd ) epdmags = npmedian ( fmags ) + fmags - epdfit retdict = { 'times' : ftimes , 'mags' : epdmags , 'errs' : ferrs , 'fitcoeffs' : fitcoeffs , 'fitinfo' : leastsqfit , 'fitmags' : epdfit , 'mags_median' : npmedian ( epdmags ) , 'mags_mad' : npmedian ( npabs ( epdmags - npmedian ( epdmags ) ) ) } return retdict else : LOGERROR ( 'EPD fit did not converge' ) return None
3982	def get_repo_of_app_or_library ( app_or_library_name ) : specs = get_specs ( ) repo_name = specs . get_app_or_lib ( app_or_library_name ) [ 'repo' ] if not repo_name : return None return Repo ( repo_name )
786	def jobGetCancellingJobs ( self , ) : with ConnectionFactory . get ( ) as conn : query = 'SELECT job_id ' 'FROM %s ' 'WHERE (status<>%%s AND cancel is TRUE)' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . STATUS_COMPLETED ] ) rows = conn . cursor . fetchall ( ) return tuple ( r [ 0 ] for r in rows )
1566	def invoke_hook_spout_fail ( self , message_id , fail_latency_ns ) : if len ( self . task_hooks ) > 0 : spout_fail_info = SpoutFailInfo ( message_id = message_id , spout_task_id = self . get_task_id ( ) , fail_latency_ms = fail_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . spout_fail ( spout_fail_info )
3878	async def _handle_conversation_delta ( self , conversation ) : conv_id = conversation . conversation_id . id conv = self . _conv_dict . get ( conv_id , None ) if conv is None : await self . _get_or_fetch_conversation ( conv_id ) else : conv . update_conversation ( conversation )
3258	def get_resources ( self , names = None , stores = None , workspaces = None ) : stores = self . get_stores ( names = stores , workspaces = workspaces ) resources = [ ] for s in stores : try : resources . extend ( s . get_resources ( ) ) except FailedRequestError : continue if names is None : names = [ ] elif isinstance ( names , basestring ) : names = [ s . strip ( ) for s in names . split ( ',' ) if s . strip ( ) ] if resources and names : return ( [ resource for resource in resources if resource . name in names ] ) return resources
355	def save_npz_dict ( save_list = None , name = 'model.npz' , sess = None ) : if sess is None : raise ValueError ( "session is None." ) if save_list is None : save_list = [ ] save_list_names = [ tensor . name for tensor in save_list ] save_list_var = sess . run ( save_list ) save_var_dict = { save_list_names [ idx ] : val for idx , val in enumerate ( save_list_var ) } np . savez ( name , ** save_var_dict ) save_list_var = None save_var_dict = None del save_list_var del save_var_dict logging . info ( "[*] Model saved in npz_dict %s" % name )
13019	def _assemble_select ( self , sql_str , columns , * args , ** kwargs ) : warnings . warn ( "_assemble_select has been depreciated for _assemble_with_columns. It will be removed in a future version." , DeprecationWarning ) return self . _assemble_with_columns ( sql_str , columns , * args , ** kwargs )
1185	def dispatch ( self , opcode , context ) : if id ( context ) in self . executing_contexts : generator = self . executing_contexts [ id ( context ) ] del self . executing_contexts [ id ( context ) ] has_finished = generator . next ( ) else : method = self . DISPATCH_TABLE . get ( opcode , _OpcodeDispatcher . unknown ) has_finished = method ( self , context ) if hasattr ( has_finished , "next" ) : generator = has_finished has_finished = generator . next ( ) if not has_finished : self . executing_contexts [ id ( context ) ] = generator return has_finished
1854	def SHLD ( cpu , dest , src , count ) : OperandSize = dest . size tempCount = Operators . ZEXTEND ( count . read ( ) , OperandSize ) & ( OperandSize - 1 ) arg0 = dest . read ( ) arg1 = src . read ( ) MASK = ( ( 1 << OperandSize ) - 1 ) t0 = ( arg0 << tempCount ) t1 = arg1 >> ( OperandSize - tempCount ) res = Operators . ITEBV ( OperandSize , tempCount == 0 , arg0 , t0 | t1 ) res = res & MASK dest . write ( res ) if isinstance ( tempCount , int ) and tempCount == 0 : pass else : SIGN_MASK = 1 << ( OperandSize - 1 ) lastbit = 0 != ( ( arg0 << ( tempCount - 1 ) ) & SIGN_MASK ) cpu . _set_shiftd_flags ( OperandSize , arg0 , res , lastbit , tempCount )
823	def addInstance ( self , groundTruth , prediction , record = None , result = None ) : self . value = self . avg ( prediction )
11504	def delete_folder ( self , token , folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = folder_id response = self . request ( 'midas.folder.delete' , parameters ) return response
7831	def add_field ( self , name = None , values = None , field_type = None , label = None , options = None , required = False , desc = None , value = None ) : field = Field ( name , values , field_type , label , options , required , desc , value ) self . fields . append ( field ) return field
13511	def http_exception_error_handler ( exception ) : assert issubclass ( type ( exception ) , HTTPException ) , type ( exception ) assert hasattr ( exception , "code" ) assert hasattr ( exception , "description" ) return response ( exception . code , exception . description )
11274	def check_pidfile ( pidfile , debug ) : if os . path . isfile ( pidfile ) : pidfile_handle = open ( pidfile , 'r' ) try : pid = int ( pidfile_handle . read ( ) ) pidfile_handle . close ( ) if check_pid ( pid , debug ) : return True except : pass os . unlink ( pidfile ) pid = str ( os . getpid ( ) ) open ( pidfile , 'w' ) . write ( pid ) return False
10347	def run_rcr ( graph , tag = 'dgxp' ) : hypotheses = defaultdict ( set ) increases = defaultdict ( set ) decreases = defaultdict ( set ) for u , v , d in graph . edges ( data = True ) : hypotheses [ u ] . add ( v ) if d [ RELATION ] in CAUSAL_INCREASE_RELATIONS : increases [ u ] . add ( v ) elif d [ RELATION ] in CAUSAL_DECREASE_RELATIONS : decreases [ u ] . add ( v ) correct = defaultdict ( int ) contra = defaultdict ( int ) ambiguous = defaultdict ( int ) missing = defaultdict ( int ) for controller , downstream_nodes in hypotheses . items ( ) : if len ( downstream_nodes ) < 4 : continue for node in downstream_nodes : if node in increases [ controller ] and node in decreases [ controller ] : ambiguous [ controller ] += 1 elif node in increases [ controller ] : if graph . node [ node ] [ tag ] == 1 : correct [ controller ] += 1 elif graph . node [ node ] [ tag ] == - 1 : contra [ controller ] += 1 elif node in decreases [ controller ] : if graph . node [ node ] [ tag ] == 1 : contra [ controller ] += 1 elif graph . node [ node ] [ tag ] == - 1 : correct [ controller ] += 1 else : missing [ controller ] += 1 controllers = { controller for controller , downstream_nodes in hypotheses . items ( ) if 4 <= len ( downstream_nodes ) } concordance_scores = { controller : scipy . stats . beta ( 0.5 , correct [ controller ] , contra [ controller ] ) for controller in controllers } population = { node for controller in controllers for node in hypotheses [ controller ] } population_size = len ( population ) return pandas . DataFrame ( { 'contra' : contra , 'correct' : correct , 'concordance' : concordance_scores } )
8432	def manual_pal ( values ) : max_n = len ( values ) def _manual_pal ( n ) : if n > max_n : msg = ( "Palette can return a maximum of {} values. " "{} were requested from it." ) warnings . warn ( msg . format ( max_n , n ) ) return values [ : n ] return _manual_pal
11994	def set_algorithms ( self , signature = None , encryption = None , serialization = None , compression = None ) : self . signature_algorithms = self . _update_dict ( signature , self . DEFAULT_SIGNATURE ) self . encryption_algorithms = self . _update_dict ( encryption , self . DEFAULT_ENCRYPTION ) self . serialization_algorithms = self . _update_dict ( serialization , self . DEFAULT_SERIALIZATION ) self . compression_algorithms = self . _update_dict ( compression , self . DEFAULT_COMPRESSION )
10922	def do_levmarq_n_directions ( s , directions , max_iter = 2 , run_length = 2 , damping = 1e-3 , collect_stats = False , marquardt_damping = True , ** kwargs ) : normals = np . array ( [ d / np . sqrt ( np . dot ( d , d ) ) for d in directions ] ) if np . isnan ( normals ) . any ( ) : raise ValueError ( '`directions` must not be 0s or contain nan' ) obj = OptState ( s , normals ) lo = LMOptObj ( obj , max_iter = max_iter , run_length = run_length , damping = damping , marquardt_damping = marquardt_damping , ** kwargs ) lo . do_run_1 ( ) if collect_stats : return lo . get_termination_stats ( )
5011	def _call_post_with_session ( self , url , payload ) : now = datetime . datetime . utcnow ( ) if now >= self . expires_at : self . session . close ( ) self . _create_session ( ) response = self . session . post ( url , data = payload ) return response . status_code , response . text
6708	def get_file_hash ( fin , block_size = 2 ** 20 ) : if isinstance ( fin , six . string_types ) : fin = open ( fin ) h = hashlib . sha512 ( ) while True : data = fin . read ( block_size ) if not data : break try : h . update ( data ) except TypeError : h . update ( data . encode ( 'utf-8' ) ) return h . hexdigest ( )
10635	def afr ( self ) : result = 0.0 for compound in self . material . compounds : result += self . get_compound_afr ( compound ) return result
644	def addNoise ( input , noise = 0.1 , doForeground = True , doBackground = True ) : if doForeground and doBackground : return numpy . abs ( input - ( numpy . random . random ( input . shape ) < noise ) ) else : if doForeground : return numpy . logical_and ( input , numpy . random . random ( input . shape ) > noise ) if doBackground : return numpy . logical_or ( input , numpy . random . random ( input . shape ) < noise ) return input
237	def compute_volume_exposures ( shares_held , volumes , percentile ) : shares_held = shares_held . replace ( 0 , np . nan ) shares_longed = shares_held [ shares_held > 0 ] shares_shorted = - 1 * shares_held [ shares_held < 0 ] shares_grossed = shares_held . abs ( ) longed_frac = shares_longed . divide ( volumes ) shorted_frac = shares_shorted . divide ( volumes ) grossed_frac = shares_grossed . divide ( volumes ) longed_threshold = 100 * longed_frac . apply ( partial ( np . nanpercentile , q = 100 * percentile ) , axis = 'columns' , ) shorted_threshold = 100 * shorted_frac . apply ( partial ( np . nanpercentile , q = 100 * percentile ) , axis = 'columns' , ) grossed_threshold = 100 * grossed_frac . apply ( partial ( np . nanpercentile , q = 100 * percentile ) , axis = 'columns' , ) return longed_threshold , shorted_threshold , grossed_threshold
8966	def which ( command , path = None , verbose = 0 , exts = None ) : matched = whichgen ( command , path , verbose , exts ) try : match = next ( matched ) except StopIteration : raise WhichError ( "Could not find '%s' on the path." % command ) else : return match
5713	def is_safe_path ( path ) : contains_windows_var = lambda val : re . match ( r'%.+%' , val ) contains_posix_var = lambda val : re . match ( r'\$.+' , val ) unsafeness_conditions = [ os . path . isabs ( path ) , ( '..%s' % os . path . sep ) in path , path . startswith ( '~' ) , os . path . expandvars ( path ) != path , contains_windows_var ( path ) , contains_posix_var ( path ) , ] return not any ( unsafeness_conditions )
4675	def removeAccount ( self , account ) : accounts = self . getAccounts ( ) for a in accounts : if a [ "name" ] == account : self . store . delete ( a [ "pubkey" ] )
10032	def execute ( helper , config , args ) : if not helper . application_exists ( ) : helper . create_application ( get ( config , 'app.description' ) ) else : out ( "Application " + get ( config , 'app.app_name' ) + " exists" ) environment_names = [ ] environments_to_wait_for_green = [ ] for env_name , env_config in list ( get ( config , 'app.environments' ) . items ( ) ) : environment_names . append ( env_name ) env_config = parse_env_config ( config , env_name ) if not helper . environment_exists ( env_name ) : option_settings = parse_option_settings ( env_config . get ( 'option_settings' , { } ) ) helper . create_environment ( env_name , solution_stack_name = env_config . get ( 'solution_stack_name' ) , cname_prefix = env_config . get ( 'cname_prefix' , None ) , description = env_config . get ( 'description' , None ) , option_settings = option_settings , tier_name = env_config . get ( 'tier_name' ) , tier_type = env_config . get ( 'tier_type' ) , tier_version = env_config . get ( 'tier_version' ) , version_label = args . version_label ) environments_to_wait_for_green . append ( env_name ) else : out ( "Environment " + env_name ) environments_to_wait_for_term = [ ] if args . delete : environments = helper . get_environments ( ) for env in environments : if env [ 'EnvironmentName' ] not in environment_names : if env [ 'Status' ] != 'Ready' : out ( "Unable to delete " + env [ 'EnvironmentName' ] + " because it's not in status Ready (" + env [ 'Status' ] + ")" ) else : out ( "Deleting environment: " + env [ 'EnvironmentName' ] ) helper . delete_environment ( env [ 'EnvironmentName' ] ) environments_to_wait_for_term . append ( env [ 'EnvironmentName' ] ) if not args . dont_wait and len ( environments_to_wait_for_green ) > 0 : helper . wait_for_environments ( environments_to_wait_for_green , status = 'Ready' , include_deleted = False ) if not args . dont_wait and len ( environments_to_wait_for_term ) > 0 : helper . wait_for_environments ( environments_to_wait_for_term , status = 'Terminated' , include_deleted = False ) out ( "Application initialized" ) return 0
11149	def rename ( self , key : Any , new_key : Any ) : if new_key == key : return required_locks = [ self . _key_locks [ key ] , self . _key_locks [ new_key ] ] ordered_required_locks = sorted ( required_locks , key = lambda x : id ( x ) ) for lock in ordered_required_locks : lock . acquire ( ) try : if key not in self . _data : raise KeyError ( "Attribute to rename \"%s\" does not exist" % key ) self . _data [ new_key ] = self [ key ] del self . _data [ key ] finally : for lock in required_locks : lock . release ( )
4423	async def handle_event ( self , event ) : if isinstance ( event , ( TrackStuckEvent , TrackExceptionEvent ) ) or isinstance ( event , TrackEndEvent ) and event . reason == 'FINISHED' : await self . play ( )
4107	def morlet ( lb , ub , n ) : r if n <= 0 : raise ValueError ( "n must be strictly positive" ) x = numpy . linspace ( lb , ub , n ) psi = numpy . cos ( 5 * x ) * numpy . exp ( - x ** 2 / 2. ) return psi
10145	def from_schema ( self , schema_node ) : params = [ ] for param_schema in schema_node . children : location = param_schema . name if location is 'body' : name = param_schema . __class__ . __name__ if name == 'body' : name = schema_node . __class__ . __name__ + 'Body' param = self . parameter_converter ( location , param_schema ) param [ 'name' ] = name if self . ref : param = self . _ref ( param ) params . append ( param ) elif location in ( ( 'path' , 'header' , 'headers' , 'querystring' , 'GET' ) ) : for node_schema in param_schema . children : param = self . parameter_converter ( location , node_schema ) if self . ref : param = self . _ref ( param ) params . append ( param ) return params
13324	def info ( ) : env = cpenv . get_active_env ( ) modules = [ ] if env : modules = env . get_modules ( ) active_modules = cpenv . get_active_modules ( ) if not env and not modules and not active_modules : click . echo ( '\nNo active modules...' ) return click . echo ( bold ( '\nActive modules' ) ) if env : click . echo ( format_objects ( [ env ] + active_modules ) ) available_modules = set ( modules ) - set ( active_modules ) if available_modules : click . echo ( bold ( '\nInactive modules in {}\n' ) . format ( cyan ( env . name ) ) ) click . echo ( format_objects ( available_modules , header = False ) ) else : click . echo ( format_objects ( active_modules ) ) available_shared_modules = set ( cpenv . get_modules ( ) ) - set ( active_modules ) if not available_shared_modules : return click . echo ( bold ( '\nInactive shared modules \n' ) ) click . echo ( format_objects ( available_shared_modules , header = False ) )
10029	def add_arguments ( parser ) : parser . add_argument ( '-o' , '--old-environment' , help = 'Old environment name' , required = True ) parser . add_argument ( '-n' , '--new-environment' , help = 'New environment name' , required = True )
796	def getActiveJobsForClientInfo ( self , clientInfo , fields = [ ] ) : dbFields = [ self . _jobs . pubToDBNameDict [ x ] for x in fields ] dbFieldsStr = ',' . join ( [ 'job_id' ] + dbFields ) with ConnectionFactory . get ( ) as conn : query = 'SELECT %s FROM %s ' 'WHERE client_info = %%s ' ' AND status != %%s' % ( dbFieldsStr , self . jobsTableName ) conn . cursor . execute ( query , [ clientInfo , self . STATUS_COMPLETED ] ) rows = conn . cursor . fetchall ( ) return rows
13377	def walk_up ( start_dir , depth = 20 ) : root = start_dir for i in xrange ( depth ) : contents = os . listdir ( root ) subdirs , files = [ ] , [ ] for f in contents : if os . path . isdir ( os . path . join ( root , f ) ) : subdirs . append ( f ) else : files . append ( f ) yield root , subdirs , files parent = os . path . dirname ( root ) if parent and not parent == root : root = parent else : break
6071	def luminosity_within_circle_in_units ( self , radius : dim . Length , unit_luminosity = 'eps' , kpc_per_arcsec = None , exposure_time = None ) : if self . has_light_profile : return sum ( map ( lambda p : p . luminosity_within_circle_in_units ( radius = radius , unit_luminosity = unit_luminosity , kpc_per_arcsec = kpc_per_arcsec , exposure_time = exposure_time ) , self . light_profiles ) ) else : return None
5847	def get_credentials_from_file ( filepath ) : try : creds = load_file_as_yaml ( filepath ) except Exception : creds = { } profile_name = os . environ . get ( citr_env_vars . CITRINATION_PROFILE ) if profile_name is None or len ( profile_name ) == 0 : profile_name = DEFAULT_CITRINATION_PROFILE api_key = None site = None try : profile = creds [ profile_name ] api_key = profile [ CREDENTIALS_API_KEY_KEY ] site = profile [ CREDENTIALS_SITE_KEY ] except KeyError : pass return ( api_key , site )
9517	def trim_Ns ( self ) : i = 0 while i < len ( self ) and self . seq [ i ] in 'nN' : i += 1 self . seq = self . seq [ i : ] self . qual = self . qual [ i : ] self . seq = self . seq . rstrip ( 'Nn' ) self . qual = self . qual [ : len ( self . seq ) ]
3467	def functional ( self ) : if self . _model : tree , _ = parse_gpr ( self . gene_reaction_rule ) return eval_gpr ( tree , { gene . id for gene in self . genes if not gene . functional } ) return True
1067	def getheaders ( self , name ) : result = [ ] current = '' have_header = 0 for s in self . getallmatchingheaders ( name ) : if s [ 0 ] . isspace ( ) : if current : current = "%s\n %s" % ( current , s . strip ( ) ) else : current = s . strip ( ) else : if have_header : result . append ( current ) current = s [ s . find ( ":" ) + 1 : ] . strip ( ) have_header = 1 if have_header : result . append ( current ) return result
10608	def _create_element_list ( self ) : element_set = stoich . elements ( self . compounds ) return sorted ( list ( element_set ) )
11931	def using ( context , alias ) : if alias == '' : yield context else : try : widgets = context . render_context [ WIDGET_CONTEXT_KEY ] except KeyError : raise template . TemplateSyntaxError ( 'No widget libraries loaded!' ) try : block_set = widgets [ alias ] except KeyError : raise template . TemplateSyntaxError ( 'No widget library loaded for alias: %r' % alias ) context . render_context . push ( ) context . render_context [ BLOCK_CONTEXT_KEY ] = block_set context . render_context [ WIDGET_CONTEXT_KEY ] = widgets yield context context . render_context . pop ( )
11735	def sanitize_path ( path ) : if path == '/' : return path if path [ : 1 ] != '/' : raise InvalidPath ( 'The path must start with a slash' ) path = re . sub ( r'/+' , '/' , path ) return path . rstrip ( '/' )
7890	def get_user ( self , nick_or_jid , create = False ) : if isinstance ( nick_or_jid , JID ) : if not nick_or_jid . resource : return None for u in self . users . values ( ) : if nick_or_jid in ( u . room_jid , u . real_jid ) : return u if create : return MucRoomUser ( nick_or_jid ) else : return None return self . users . get ( nick_or_jid )
13099	def getAnnotations ( self , targets , wildcard = "." , include = None , exclude = None , limit = None , start = 1 , expand = False , ** kwargs ) : return 0 , [ ]
10199	def hash_id ( iso_timestamp , msg ) : return '{0}-{1}' . format ( iso_timestamp , hashlib . sha1 ( msg . get ( 'unique_id' ) . encode ( 'utf-8' ) + str ( msg . get ( 'visitor_id' ) ) . encode ( 'utf-8' ) ) . hexdigest ( ) )
4295	def less_than_version ( value ) : items = list ( map ( int , str ( value ) . split ( '.' ) ) ) if len ( items ) == 1 : items . append ( 0 ) items [ 1 ] += 1 if value == '1.11' : return '2.0' else : return '.' . join ( map ( str , items ) )
1166	def join ( self , timeout = None ) : if not self . __initialized : raise RuntimeError ( "Thread.__init__() not called" ) if not self . __started . is_set ( ) : raise RuntimeError ( "cannot join thread before it is started" ) if self is current_thread ( ) : raise RuntimeError ( "cannot join current thread" ) if __debug__ : if not self . __stopped : self . _note ( "%s.join(): waiting until thread stops" , self ) self . __block . acquire ( ) try : if timeout is None : while not self . __stopped : self . __block . wait ( ) if __debug__ : self . _note ( "%s.join(): thread stopped" , self ) else : deadline = _time ( ) + timeout while not self . __stopped : delay = deadline - _time ( ) if delay <= 0 : if __debug__ : self . _note ( "%s.join(): timed out" , self ) break self . __block . wait ( delay ) else : if __debug__ : self . _note ( "%s.join(): thread stopped" , self ) finally : self . __block . release ( )
2978	def cmd_partition ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) if opts . random : if opts . partitions : raise BlockadeError ( "Either specify individual partitions " "or --random, but not both" ) b . random_partition ( ) else : partitions = [ ] for partition in opts . partitions : names = [ ] for name in partition . split ( "," ) : name = name . strip ( ) if name : names . append ( name ) partitions . append ( names ) if not partitions : raise BlockadeError ( "Either specify individual partitions " "or random" ) b . partition ( partitions )
11160	def trail_space ( self , filters = lambda p : p . ext == ".py" ) : self . assert_is_dir_and_exists ( ) for p in self . select_file ( filters ) : try : with open ( p . abspath , "rb" ) as f : lines = list ( ) for line in f : lines . append ( line . decode ( "utf-8" ) . rstrip ( ) ) with open ( p . abspath , "wb" ) as f : f . write ( "\n" . join ( lines ) . encode ( "utf-8" ) ) except Exception as e : raise e
6671	def is_file ( self , path , use_sudo = False ) : if self . is_local and not use_sudo : return os . path . isfile ( path ) else : func = use_sudo and _sudo or _run with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : return func ( '[ -f "%(path)s" ]' % locals ( ) ) . succeeded
10181	def _events_process ( event_types = None , eager = False ) : event_types = event_types or list ( current_stats . enabled_events ) if eager : process_events . apply ( ( event_types , ) , throw = True ) click . secho ( 'Events processed successfully.' , fg = 'green' ) else : process_events . delay ( event_types ) click . secho ( 'Events processing task sent...' , fg = 'yellow' )
2042	def human_transactions ( self ) : txs = [ ] for tx in self . transactions : if tx . depth == 0 : txs . append ( tx ) return tuple ( txs )
5219	def ref_file ( ticker : str , fld : str , has_date = False , cache = False , ext = 'parq' , ** kwargs ) -> str : data_path = os . environ . get ( assist . BBG_ROOT , '' ) . replace ( '\\' , '/' ) if ( not data_path ) or ( not cache ) : return '' proper_ticker = ticker . replace ( '/' , '_' ) cache_days = kwargs . pop ( 'cache_days' , 10 ) root = f'{data_path}/{ticker.split()[-1]}/{proper_ticker}/{fld}' if len ( kwargs ) > 0 : info = utils . to_str ( kwargs ) [ 1 : - 1 ] . replace ( '|' , '_' ) else : info = 'ovrd=None' if has_date : cur_dt = utils . cur_time ( ) missing = f'{root}/asof={cur_dt}, {info}.{ext}' to_find = re . compile ( rf'{root}/asof=(.*), {info}\.pkl' ) cur_files = list ( filter ( to_find . match , sorted ( files . all_files ( path_name = root , keyword = info , ext = ext ) ) ) ) if len ( cur_files ) > 0 : upd_dt = to_find . match ( cur_files [ - 1 ] ) . group ( 1 ) diff = pd . Timestamp ( 'today' ) - pd . Timestamp ( upd_dt ) if diff >= pd . Timedelta ( days = cache_days ) : return missing return sorted ( cur_files ) [ - 1 ] else : return missing else : return f'{root}/{info}.{ext}'
13540	def chisq_red ( self ) : if self . _chisq_red is None : self . _chisq_red = chisquare ( self . y_unweighted . transpose ( ) , _np . dot ( self . X_unweighted , self . beta ) , self . y_error , ddof = 3 , verbose = False ) return self . _chisq_red
3967	def _compose_dict_for_nginx ( port_specs ) : spec = { 'image' : constants . NGINX_IMAGE , 'volumes' : [ '{}:{}' . format ( constants . NGINX_CONFIG_DIR_IN_VM , constants . NGINX_CONFIG_DIR_IN_CONTAINER ) ] , 'command' : 'nginx -g "daemon off;" -c /etc/nginx/conf.d/nginx.primary' , 'container_name' : 'dusty_{}_1' . format ( constants . DUSTY_NGINX_NAME ) } all_host_ports = set ( [ nginx_spec [ 'host_port' ] for nginx_spec in port_specs [ 'nginx' ] ] ) if all_host_ports : spec [ 'ports' ] = [ ] for port in all_host_ports : spec [ 'ports' ] . append ( '{0}:{0}' . format ( port ) ) return { constants . DUSTY_NGINX_NAME : spec }
7454	def estimate_optim ( data , testfile , ipyclient ) : insize = os . path . getsize ( testfile ) tmp_file_name = os . path . join ( data . paramsdict [ "project_dir" ] , "tmp-step1-count.fq" ) if testfile . endswith ( ".gz" ) : infile = gzip . open ( testfile ) outfile = gzip . open ( tmp_file_name , 'wb' , compresslevel = 5 ) else : infile = open ( testfile ) outfile = open ( tmp_file_name , 'w' ) outfile . write ( "" . join ( itertools . islice ( infile , 40000 ) ) ) outfile . close ( ) infile . close ( ) tmp_size = os . path . getsize ( tmp_file_name ) inputreads = int ( insize / tmp_size ) * 10000 os . remove ( tmp_file_name ) return inputreads
11075	def set ( self , user ) : self . log . info ( "Loading user information for %s/%s" , user . id , user . username ) self . load_user_info ( user ) self . log . info ( "Loading user rights for %s/%s" , user . id , user . username ) self . load_user_rights ( user ) self . log . info ( "Added user: %s/%s" , user . id , user . username ) self . _add_user_to_cache ( user ) return user
3546	def _characteristics_discovered ( self , service ) : self . _discovered_services . add ( service ) if self . _discovered_services >= set ( self . _peripheral . services ( ) ) : self . _discovered . set ( )
13831	def _api_call ( self , method_name , * args , ** kwargs ) : params = kwargs . setdefault ( 'params' , { } ) params . update ( { 'key' : self . _apikey } ) if self . _token is not None : params . update ( { 'token' : self . _token } ) http_method = getattr ( requests , method_name ) return http_method ( TRELLO_URL + self . _url , * args , ** kwargs )
10270	def get_unweighted_upstream_leaves ( graph : BELGraph , key : Optional [ str ] = None ) -> Iterable [ BaseEntity ] : if key is None : key = WEIGHT return filter_nodes ( graph , [ node_is_upstream_leaf , data_missing_key_builder ( key ) ] )
6397	def sim ( self , src , tar , qval = 2 ) : r if src == tar : return 1.0 if not src or not tar : return 0.0 q_src , q_tar = self . _get_qgrams ( src , tar , qval ) q_src_mag = sum ( q_src . values ( ) ) q_tar_mag = sum ( q_tar . values ( ) ) q_intersection_mag = sum ( ( q_src & q_tar ) . values ( ) ) return q_intersection_mag / sqrt ( q_src_mag * q_tar_mag )
2295	def predict_proba ( self , a , b , ** kwargs ) : estimators = { 'entropy' : lambda x , y : eval_entropy ( y ) - eval_entropy ( x ) , 'integral' : integral_approx_estimator } ref_measures = { 'gaussian' : lambda x : standard_scale . fit_transform ( x . reshape ( ( - 1 , 1 ) ) ) , 'uniform' : lambda x : min_max_scale . fit_transform ( x . reshape ( ( - 1 , 1 ) ) ) , 'None' : lambda x : x } ref_measure = ref_measures [ kwargs . get ( 'refMeasure' , 'gaussian' ) ] estimator = estimators [ kwargs . get ( 'estimator' , 'entropy' ) ] a = ref_measure ( a ) b = ref_measure ( b ) return estimator ( a , b )
4096	def KIC ( N , rho , k ) : r from numpy import log , array res = log ( rho ) + 3. * ( k + 1. ) / float ( N ) return res
2376	def run ( self , args ) : self . args = self . parse_and_process_args ( args ) if self . args . version : print ( __version__ ) return 0 if self . args . rulefile : for filename in self . args . rulefile : self . _load_rule_file ( filename ) if self . args . list : self . list_rules ( ) return 0 if self . args . describe : self . _describe_rules ( self . args . args ) return 0 self . counts = { ERROR : 0 , WARNING : 0 , "other" : 0 } for filename in self . args . args : if not ( os . path . exists ( filename ) ) : sys . stderr . write ( "rflint: %s: No such file or directory\n" % filename ) continue if os . path . isdir ( filename ) : self . _process_folder ( filename ) else : self . _process_file ( filename ) if self . counts [ ERROR ] > 0 : return self . counts [ ERROR ] if self . counts [ ERROR ] < 254 else 255 return 0
5814	def _try_decode ( byte_string ) : try : return str_cls ( byte_string , _encoding ) except ( UnicodeDecodeError ) : for encoding in _fallback_encodings : try : return str_cls ( byte_string , encoding , errors = 'strict' ) except ( UnicodeDecodeError ) : pass return str_cls ( byte_string , errors = 'replace' )
4813	def _document_frequency ( X ) : if sp . isspmatrix_csr ( X ) : return np . bincount ( X . indices , minlength = X . shape [ 1 ] ) return np . diff ( sp . csc_matrix ( X , copy = False ) . indptr )
11189	def edit ( dataset_uri ) : try : dataset = dtoolcore . ProtoDataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) except dtoolcore . DtoolCoreTypeError : dataset = dtoolcore . DataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) readme_content = dataset . get_readme_content ( ) try : readme_content = unicode ( readme_content , "utf-8" ) except NameError : pass edited_content = click . edit ( readme_content ) if edited_content is not None : _validate_and_put_readme ( dataset , edited_content ) click . secho ( "Updated readme " , nl = False , fg = "green" ) else : click . secho ( "Did not update readme " , nl = False , fg = "red" ) click . secho ( dataset_uri )
2170	def command ( method = None , ** kwargs ) : def actual_decorator ( method ) : method . _cli_command = True method . _cli_command_attrs = kwargs return method if method and isinstance ( method , types . FunctionType ) : return actual_decorator ( method ) else : return actual_decorator
10923	def finish ( s , desc = 'finish' , n_loop = 4 , max_mem = 1e9 , separate_psf = True , fractol = 1e-7 , errtol = 1e-3 , dowarn = True ) : values = [ np . copy ( s . state [ s . params ] ) ] remove_params = s . get ( 'psf' ) . params if separate_psf else None global_params = name_globals ( s , remove_params = remove_params ) gs = np . floor ( max_mem / s . residuals . nbytes ) . astype ( 'int' ) groups = [ global_params [ a : a + gs ] for a in range ( 0 , len ( global_params ) , gs ) ] CLOG . info ( 'Start ``finish``:\t{}' . format ( s . error ) ) for a in range ( n_loop ) : start_err = s . error for g in groups : do_levmarq ( s , g , damping = 0.1 , decrease_damp_factor = 20. , max_iter = 1 , max_mem = max_mem , eig_update = False ) if separate_psf : do_levmarq ( s , remove_params , max_mem = max_mem , max_iter = 4 , eig_update = False ) CLOG . info ( 'Globals, loop {}:\t{}' . format ( a , s . error ) ) if desc is not None : states . save ( s , desc = desc ) do_levmarq_all_particle_groups ( s , max_iter = 1 , max_mem = max_mem ) CLOG . info ( 'Particles, loop {}:\t{}' . format ( a , s . error ) ) if desc is not None : states . save ( s , desc = desc ) values . append ( np . copy ( s . state [ s . params ] ) ) new_err = s . error derr = start_err - new_err dobreak = ( derr / new_err < fractol ) or ( derr < errtol ) if dobreak : break if dowarn and ( not dobreak ) : CLOG . warn ( 'finish() did not converge; consider re-running' ) return { 'converged' : dobreak , 'loop_values' : np . array ( values ) }
842	def getPartitionId ( self , i ) : if ( i < 0 ) or ( i >= self . _numPatterns ) : raise RuntimeError ( "index out of bounds" ) partitionId = self . _partitionIdList [ i ] if partitionId == numpy . inf : return None else : return partitionId
2752	def get_all_domains ( self ) : data = self . get_data ( "domains/" ) domains = list ( ) for jsoned in data [ 'domains' ] : domain = Domain ( ** jsoned ) domain . token = self . token domains . append ( domain ) return domains
11213	def decode ( secret : Union [ str , bytes ] , token : Union [ str , bytes ] , alg : str = default_alg ) -> Tuple [ dict , dict ] : secret = util . to_bytes ( secret ) token = util . to_bytes ( token ) pre_signature , signature_segment = token . rsplit ( b'.' , 1 ) header_b64 , payload_b64 = pre_signature . split ( b'.' ) try : header_json = util . b64_decode ( header_b64 ) header = json . loads ( util . from_bytes ( header_json ) ) except ( json . decoder . JSONDecodeError , UnicodeDecodeError , ValueError ) : raise InvalidHeaderError ( 'Invalid header' ) try : payload_json = util . b64_decode ( payload_b64 ) payload = json . loads ( util . from_bytes ( payload_json ) ) except ( json . decoder . JSONDecodeError , UnicodeDecodeError , ValueError ) : raise InvalidPayloadError ( 'Invalid payload' ) if not isinstance ( header , dict ) : raise InvalidHeaderError ( 'Invalid header: {}' . format ( header ) ) if not isinstance ( payload , dict ) : raise InvalidPayloadError ( 'Invalid payload: {}' . format ( payload ) ) signature = util . b64_decode ( signature_segment ) calculated_signature = _hash ( secret , pre_signature , alg ) if not compare_signature ( signature , calculated_signature ) : raise InvalidSignatureError ( 'Invalid signature' ) return header , payload
5475	def string_presenter ( self , dumper , data ) : if '\n' in data : return dumper . represent_scalar ( 'tag:yaml.org,2002:str' , data , style = '|' ) else : return dumper . represent_scalar ( 'tag:yaml.org,2002:str' , data )
5668	def stop_to_stop_network_for_route_type ( gtfs , route_type , link_attributes = None , start_time_ut = None , end_time_ut = None ) : if link_attributes is None : link_attributes = DEFAULT_STOP_TO_STOP_LINK_ATTRIBUTES assert ( route_type in route_types . TRANSIT_ROUTE_TYPES ) stops_dataframe = gtfs . get_stops_for_route_type ( route_type ) net = networkx . DiGraph ( ) _add_stops_to_net ( net , stops_dataframe ) events_df = gtfs . get_transit_events ( start_time_ut = start_time_ut , end_time_ut = end_time_ut , route_type = route_type ) if len ( net . nodes ( ) ) < 2 : assert events_df . shape [ 0 ] == 0 link_event_groups = events_df . groupby ( [ 'from_stop_I' , 'to_stop_I' ] , sort = False ) for key , link_events in link_event_groups : from_stop_I , to_stop_I = key assert isinstance ( link_events , pd . DataFrame ) if link_attributes is None : net . add_edge ( from_stop_I , to_stop_I ) else : link_data = { } if "duration_min" in link_attributes : link_data [ 'duration_min' ] = float ( link_events [ 'duration' ] . min ( ) ) if "duration_max" in link_attributes : link_data [ 'duration_max' ] = float ( link_events [ 'duration' ] . max ( ) ) if "duration_median" in link_attributes : link_data [ 'duration_median' ] = float ( link_events [ 'duration' ] . median ( ) ) if "duration_avg" in link_attributes : link_data [ 'duration_avg' ] = float ( link_events [ 'duration' ] . mean ( ) ) if "n_vehicles" in link_attributes : link_data [ 'n_vehicles' ] = int ( link_events . shape [ 0 ] ) if "capacity_estimate" in link_attributes : link_data [ 'capacity_estimate' ] = route_types . ROUTE_TYPE_TO_APPROXIMATE_CAPACITY [ route_type ] * int ( link_events . shape [ 0 ] ) if "d" in link_attributes : from_lat = net . node [ from_stop_I ] [ 'lat' ] from_lon = net . node [ from_stop_I ] [ 'lon' ] to_lat = net . node [ to_stop_I ] [ 'lat' ] to_lon = net . node [ to_stop_I ] [ 'lon' ] distance = wgs84_distance ( from_lat , from_lon , to_lat , to_lon ) link_data [ 'd' ] = int ( distance ) if "distance_shape" in link_attributes : assert "shape_id" in link_events . columns . values found = None for i , shape_id in enumerate ( link_events [ "shape_id" ] . values ) : if shape_id is not None : found = i break if found is None : link_data [ "distance_shape" ] = None else : link_event = link_events . iloc [ found ] distance = gtfs . get_shape_distance_between_stops ( link_event [ "trip_I" ] , int ( link_event [ "from_seq" ] ) , int ( link_event [ "to_seq" ] ) ) link_data [ 'distance_shape' ] = distance if "route_I_counts" in link_attributes : link_data [ "route_I_counts" ] = link_events . groupby ( "route_I" ) . size ( ) . to_dict ( ) net . add_edge ( from_stop_I , to_stop_I , attr_dict = link_data ) return net
11544	def analog_reference ( self , pin = None ) : if pin is None : return self . _analog_reference ( None ) else : pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : return self . _analog_reference ( pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
6317	def image_data ( image ) : data = image . tobytes ( ) components = len ( data ) // ( image . size [ 0 ] * image . size [ 1 ] ) return components , data
6663	def get_expiration_date ( self , fn ) : r = self . local_renderer r . env . crt_fn = fn with hide ( 'running' ) : ret = r . local ( 'openssl x509 -noout -in {ssl_crt_fn} -dates' , capture = True ) matches = re . findall ( 'notAfter=(.*?)$' , ret , flags = re . IGNORECASE ) if matches : return dateutil . parser . parse ( matches [ 0 ] )
7994	def _send ( self , stanza ) : self . fix_out_stanza ( stanza ) element = stanza . as_xml ( ) self . _write_element ( element )
12283	def lookup ( self , username = None , reponame = None , key = None ) : if key is None : key = self . key ( username , reponame ) if key not in self . repos : raise UnknownRepository ( ) return self . repos [ key ]
8630	def get_projects ( session , query ) : response = make_get_request ( session , 'projects' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
8140	def invert ( self ) : alpha = self . img . split ( ) [ 3 ] self . img = self . img . convert ( "RGB" ) self . img = ImageOps . invert ( self . img ) self . img = self . img . convert ( "RGBA" ) self . img . putalpha ( alpha )
1502	def template_scheduler_yaml ( cl_args , masters ) : single_master = masters [ 0 ] scheduler_config_actual = "%s/standalone/scheduler.yaml" % cl_args [ "config_path" ] scheduler_config_template = "%s/standalone/templates/scheduler.template.yaml" % cl_args [ "config_path" ] template_file ( scheduler_config_template , scheduler_config_actual , { "<scheduler_uri>" : "http://%s:4646" % single_master } )
3085	def service_account_email ( self ) : if self . _service_account_email is None : self . _service_account_email = ( app_identity . get_service_account_name ( ) ) return self . _service_account_email
1471	def setup ( executor ) : def signal_handler ( signal_to_handle , frame ) : Log . info ( 'signal_handler invoked with signal %s' , signal_to_handle ) executor . stop_state_manager_watches ( ) sys . exit ( signal_to_handle ) def cleanup ( ) : Log . info ( 'Executor terminated; exiting all process in executor.' ) for pid in executor . processes_to_monitor . keys ( ) : os . kill ( pid , signal . SIGTERM ) time . sleep ( 5 ) os . killpg ( 0 , signal . SIGTERM ) shardid = executor . shard log . configure ( logfile = 'heron-executor-%s.stdout' % shardid ) pid = os . getpid ( ) sid = os . getsid ( pid ) if pid <> sid : Log . info ( 'Set up process group; executor becomes leader' ) os . setpgrp ( ) Log . info ( 'Register the SIGTERM signal handler' ) signal . signal ( signal . SIGTERM , signal_handler ) Log . info ( 'Register the atexit clean up' ) atexit . register ( cleanup )
10242	def get_evidences_by_pmid ( graph : BELGraph , pmids : Union [ str , Iterable [ str ] ] ) : result = defaultdict ( set ) for _ , _ , _ , data in filter_edges ( graph , build_pmid_inclusion_filter ( pmids ) ) : result [ data [ CITATION ] [ CITATION_REFERENCE ] ] . add ( data [ EVIDENCE ] ) return dict ( result )
5951	def strftime ( self , fmt = "%d:%H:%M:%S" ) : substitutions = { "%d" : str ( self . days ) , "%H" : "{0:02d}" . format ( self . dhours ) , "%h" : str ( 24 * self . days + self . dhours ) , "%M" : "{0:02d}" . format ( self . dminutes ) , "%S" : "{0:02d}" . format ( self . dseconds ) , } s = fmt for search , replacement in substitutions . items ( ) : s = s . replace ( search , replacement ) return s
94	def quokka ( size = None , extract = None ) : img = imageio . imread ( QUOKKA_FP , pilmode = "RGB" ) if extract is not None : bb = _quokka_normalize_extract ( extract ) img = bb . extract_from_image ( img ) if size is not None : shape_resized = _compute_resized_shape ( img . shape , size ) img = imresize_single_image ( img , shape_resized [ 0 : 2 ] ) return img
11236	def reusable ( func ) : sig = signature ( func ) origin = func while hasattr ( origin , '__wrapped__' ) : origin = origin . __wrapped__ return type ( origin . __name__ , ( ReusableGenerator , ) , dict ( [ ( '__doc__' , origin . __doc__ ) , ( '__module__' , origin . __module__ ) , ( '__signature__' , sig ) , ( '__wrapped__' , staticmethod ( func ) ) , ] + [ ( name , property ( compose ( itemgetter ( name ) , attrgetter ( '_bound_args.arguments' ) ) ) ) for name in sig . parameters ] + ( [ ( '__qualname__' , origin . __qualname__ ) , ] if sys . version_info > ( 3 , ) else [ ] ) ) )
7204	def set ( self , ** kwargs ) : for port_name , port_value in kwargs . items ( ) : if hasattr ( port_value , 'value' ) : port_value = port_value . value self . inputs . __setattr__ ( port_name , port_value )
4878	def validate ( self , data ) : lms_user_id = data . get ( 'lms_user_id' ) tpa_user_id = data . get ( 'tpa_user_id' ) user_email = data . get ( 'user_email' ) if not lms_user_id and not tpa_user_id and not user_email : raise serializers . ValidationError ( 'At least one of the following fields must be specified and map to an EnterpriseCustomerUser: ' 'lms_user_id, tpa_user_id, user_email' ) return data
10728	def _handle_struct ( toks ) : subtrees = toks [ 1 : - 1 ] signature = '' . join ( s for ( _ , s ) in subtrees ) funcs = [ f for ( f , _ ) in subtrees ] def the_func ( a_list , variant = 0 ) : if isinstance ( a_list , dict ) : raise IntoDPValueError ( a_list , "a_list" , "must be a simple sequence, is a dict" ) if len ( a_list ) != len ( funcs ) : raise IntoDPValueError ( a_list , "a_list" , "must have exactly %u items, has %u" % ( len ( funcs ) , len ( a_list ) ) ) elements = [ f ( x ) for ( f , x ) in zip ( funcs , a_list ) ] level = 0 if elements == [ ] else max ( x for ( _ , x ) in elements ) ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( level , variant ) return ( dbus . types . Struct ( ( x for ( x , _ ) in elements ) , signature = signature , variant_level = obj_level ) , func_level ) return ( the_func , '(' + signature + ')' )
3494	def total_yield ( input_fluxes , input_elements , output_flux , output_elements ) : carbon_input_flux = sum ( total_components_flux ( flux , components , consumption = True ) for flux , components in zip ( input_fluxes , input_elements ) ) carbon_output_flux = total_components_flux ( output_flux , output_elements , consumption = False ) try : return carbon_output_flux / carbon_input_flux except ZeroDivisionError : return nan
3310	def _run_ext_wsgiutils ( app , config , mode ) : from wsgidav . server import ext_wsgiutils_server _logger . info ( "Running WsgiDAV {} on wsgidav.ext_wsgiutils_server..." . format ( __version__ ) ) _logger . warning ( "WARNING: This single threaded server (ext-wsgiutils) is not meant for production." ) try : ext_wsgiutils_server . serve ( config , app ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
7108	def print_cm ( cm , labels , hide_zeroes = False , hide_diagonal = False , hide_threshold = None ) : columnwidth = max ( [ len ( x ) for x in labels ] + [ 5 ] ) empty_cell = " " * columnwidth print ( " " + empty_cell , end = " " ) for label in labels : print ( "%{0}s" . format ( columnwidth ) % label , end = " " ) print ( ) for i , label1 in enumerate ( labels ) : print ( " %{0}s" . format ( columnwidth ) % label1 , end = " " ) for j in range ( len ( labels ) ) : cell = "%{0}.1f" . format ( columnwidth ) % cm [ i , j ] if hide_zeroes : cell = cell if float ( cm [ i , j ] ) != 0 else empty_cell if hide_diagonal : cell = cell if i != j else empty_cell if hide_threshold : cell = cell if cm [ i , j ] > hide_threshold else empty_cell print ( cell , end = " " ) print ( )
1794	def NEG ( cpu , dest ) : source = dest . read ( ) res = dest . write ( - source ) cpu . _calculate_logic_flags ( dest . size , res ) cpu . CF = source != 0 cpu . AF = ( res & 0x0f ) != 0x00
1642	def CheckBracesSpacing ( filename , clean_lines , linenum , nesting_state , error ) : line = clean_lines . elided [ linenum ] match = Match ( r'^(.*[^ ({>]){' , line ) if match : leading_text = match . group ( 1 ) ( endline , endlinenum , endpos ) = CloseExpression ( clean_lines , linenum , len ( match . group ( 1 ) ) ) trailing_text = '' if endpos > - 1 : trailing_text = endline [ endpos : ] for offset in xrange ( endlinenum + 1 , min ( endlinenum + 3 , clean_lines . NumLines ( ) - 1 ) ) : trailing_text += clean_lines . elided [ offset ] if ( not Match ( r'^[\s}]*[{.;,)<>\]:]' , trailing_text ) and not _IsType ( clean_lines , nesting_state , leading_text ) ) : error ( filename , linenum , 'whitespace/braces' , 5 , 'Missing space before {' ) if Search ( r'}else' , line ) : error ( filename , linenum , 'whitespace/braces' , 5 , 'Missing space before else' ) if Search ( r':\s*;\s*$' , line ) : error ( filename , linenum , 'whitespace/semicolon' , 5 , 'Semicolon defining empty statement. Use {} instead.' ) elif Search ( r'^\s*;\s*$' , line ) : error ( filename , linenum , 'whitespace/semicolon' , 5 , 'Line contains only semicolon. If this should be an empty statement, ' 'use {} instead.' ) elif ( Search ( r'\s+;\s*$' , line ) and not Search ( r'\bfor\b' , line ) ) : error ( filename , linenum , 'whitespace/semicolon' , 5 , 'Extra space before last semicolon. If this should be an empty ' 'statement, use {} instead.' )
3671	def identify_phase ( T , P , Tm = None , Tb = None , Tc = None , Psat = None ) : r if Tm and T <= Tm : return 's' elif Tc and T >= Tc : return 'g' elif Psat : if P <= Psat : return 'g' elif P > Psat : return 'l' elif Tb : if 9E4 < P < 1.1E5 : if T < Tb : return 'l' else : return 'g' elif P > 1.1E5 and T <= Tb : return 'l' else : return None else : return None
12897	def get_mute ( self ) : mute = ( yield from self . handle_int ( self . API . get ( 'mute' ) ) ) return bool ( mute )
11669	def topological_sort ( deps ) : order = [ ] available = set ( ) def _move_available ( ) : to_delete = [ ] for n , parents in iteritems ( deps ) : if not parents : available . add ( n ) to_delete . append ( n ) for n in to_delete : del deps [ n ] _move_available ( ) while available : n = available . pop ( ) order . append ( n ) for parents in itervalues ( deps ) : parents . discard ( n ) _move_available ( ) if available : raise ValueError ( "dependency cycle found" ) return order
2198	def platform_data_dir ( ) : if LINUX : dpath_ = os . environ . get ( 'XDG_DATA_HOME' , '~/.local/share' ) elif DARWIN : dpath_ = '~/Library/Application Support' elif WIN32 : dpath_ = os . environ . get ( 'APPDATA' , '~/AppData/Roaming' ) else : raise '~/AppData/Local' dpath = normpath ( expanduser ( dpath_ ) ) return dpath
5486	def jsonify_status_code ( status_code , * args , ** kw ) : is_batch = kw . pop ( 'is_batch' , False ) if is_batch : response = flask_make_response ( json . dumps ( * args , ** kw ) ) response . mimetype = 'application/json' response . status_code = status_code return response response = jsonify ( * args , ** kw ) response . status_code = status_code return response
6600	def open ( self ) : self . path = self . _prepare_dir ( self . topdir ) self . _copy_executable ( area_path = self . path ) self . _save_logging_levels ( area_path = self . path ) self . _put_python_modules ( modules = self . python_modules , area_path = self . path )
7397	def tex_parse ( string ) : string = string . replace ( '{' , '' ) . replace ( '}' , '' ) def tex_replace ( match ) : return sub ( r'\^(\w)' , r'<sup>\1</sup>' , sub ( r'\^\{(.*?)\}' , r'<sup>\1</sup>' , sub ( r'\_(\w)' , r'<sub>\1</sub>' , sub ( r'\_\{(.*?)\}' , r'<sub>\1</sub>' , sub ( r'\\(' + GREEK_LETTERS + ')' , r'&\1;' , match . group ( 1 ) ) ) ) ) ) return mark_safe ( sub ( r'\$([^\$]*)\$' , tex_replace , escape ( string ) ) )
10959	def set_mem_level ( self , mem_level = 'hi' ) : key = '' . join ( [ c if c in 'mlh' else '' for c in mem_level ] ) if key not in [ 'h' , 'mh' , 'm' , 'ml' , 'm' , 'l' ] : raise ValueError ( 'mem_level must be one of hi, med-hi, med, med-lo, lo.' ) mem_levels = { 'h' : [ np . float64 , np . float64 ] , 'mh' : [ np . float64 , np . float32 ] , 'm' : [ np . float32 , np . float32 ] , 'ml' : [ np . float32 , np . float16 ] , 'l' : [ np . float16 , np . float16 ] } hi_lvl , lo_lvl = mem_levels [ key ] cat_lvls = { 'obj' : lo_lvl , 'ilm' : hi_lvl , 'bkg' : lo_lvl } self . image . float_precision = hi_lvl self . image . image = self . image . image . astype ( lo_lvl ) self . set_image ( self . image ) for cat in cat_lvls . keys ( ) : obj = self . get ( cat ) if hasattr ( obj , 'comps' ) : for c in obj . comps : c . float_precision = lo_lvl else : obj . float_precision = lo_lvl self . _model = self . _model . astype ( hi_lvl ) self . _residuals = self . _model . astype ( hi_lvl ) self . reset ( )
2175	def refresh_token ( self , token_url , refresh_token = None , body = "" , auth = None , timeout = None , headers = None , verify = True , proxies = None , ** kwargs ) : if not token_url : raise ValueError ( "No token endpoint set for auto_refresh." ) if not is_secure_transport ( token_url ) : raise InsecureTransportError ( ) refresh_token = refresh_token or self . token . get ( "refresh_token" ) log . debug ( "Adding auto refresh key word arguments %s." , self . auto_refresh_kwargs ) kwargs . update ( self . auto_refresh_kwargs ) body = self . _client . prepare_refresh_body ( body = body , refresh_token = refresh_token , scope = self . scope , ** kwargs ) log . debug ( "Prepared refresh token request body %s" , body ) if headers is None : headers = { "Accept" : "application/json" , "Content-Type" : ( "application/x-www-form-urlencoded;charset=UTF-8" ) , } r = self . post ( token_url , data = dict ( urldecode ( body ) ) , auth = auth , timeout = timeout , headers = headers , verify = verify , withhold_token = True , proxies = proxies , ) log . debug ( "Request to refresh token completed with status %s." , r . status_code ) log . debug ( "Response headers were %s and content %s." , r . headers , r . text ) log . debug ( "Invoking %d token response hooks." , len ( self . compliance_hook [ "refresh_token_response" ] ) , ) for hook in self . compliance_hook [ "refresh_token_response" ] : log . debug ( "Invoking hook %s." , hook ) r = hook ( r ) self . token = self . _client . parse_request_body_response ( r . text , scope = self . scope ) if not "refresh_token" in self . token : log . debug ( "No new refresh token given. Re-using old." ) self . token [ "refresh_token" ] = refresh_token return self . token
1003	def _inferPhase1 ( self , activeColumns , useStartCells ) : self . infActiveState [ 't' ] . fill ( 0 ) numPredictedColumns = 0 if useStartCells : for c in activeColumns : self . infActiveState [ 't' ] [ c , 0 ] = 1 else : for c in activeColumns : predictingCells = numpy . where ( self . infPredictedState [ 't-1' ] [ c ] == 1 ) [ 0 ] numPredictingCells = len ( predictingCells ) if numPredictingCells > 0 : self . infActiveState [ 't' ] [ c , predictingCells ] = 1 numPredictedColumns += 1 else : self . infActiveState [ 't' ] [ c , : ] = 1 if useStartCells or numPredictedColumns >= 0.50 * len ( activeColumns ) : return True else : return False
6026	def geometry_from_grid ( self , grid , pixel_centres , pixel_neighbors , pixel_neighbors_size , buffer = 1e-8 ) : y_min = np . min ( grid [ : , 0 ] ) - buffer y_max = np . max ( grid [ : , 0 ] ) + buffer x_min = np . min ( grid [ : , 1 ] ) - buffer x_max = np . max ( grid [ : , 1 ] ) + buffer shape_arcsec = ( y_max - y_min , x_max - x_min ) origin = ( ( y_max + y_min ) / 2.0 , ( x_max + x_min ) / 2.0 ) return self . Geometry ( shape_arcsec = shape_arcsec , pixel_centres = pixel_centres , origin = origin , pixel_neighbors = pixel_neighbors , pixel_neighbors_size = pixel_neighbors_size )
1925	def get_group ( name : str ) -> _Group : global _groups if name in _groups : return _groups [ name ] group = _Group ( name ) _groups [ name ] = group return group
12636	def dist_percentile_threshold ( dist_matrix , perc_thr = 0.05 , k = 1 ) : triu_idx = np . triu_indices ( dist_matrix . shape [ 0 ] , k = k ) upper = np . zeros_like ( dist_matrix ) upper [ triu_idx ] = dist_matrix [ triu_idx ] < np . percentile ( dist_matrix [ triu_idx ] , perc_thr ) return upper
5390	def _get_task_from_task_dir ( self , job_id , user_id , task_id , task_attempt ) : task_dir = self . _task_directory ( job_id , task_id , task_attempt ) job_descriptor = self . _read_task_metadata ( task_dir ) if not job_descriptor : return None if not job_descriptor . job_metadata . get ( 'user-id' ) : job_descriptor . job_metadata [ 'user-id' ] = user_id pid = - 1 try : with open ( os . path . join ( task_dir , 'task.pid' ) , 'r' ) as f : pid = int ( f . readline ( ) . strip ( ) ) except ( IOError , OSError ) : pass script = None script_name = job_descriptor . job_metadata . get ( 'script-name' ) if script_name : script = self . _read_script ( task_dir , script_name ) end_time = self . _get_end_time_from_task_dir ( task_dir ) last_update = self . _get_last_update_time_from_task_dir ( task_dir ) events = self . _get_events_from_task_dir ( task_dir ) status = self . _get_status_from_task_dir ( task_dir ) log_detail = self . _get_log_detail_from_task_dir ( task_dir ) if not status : status = 'RUNNING' log_detail = [ 'Pending' ] return LocalTask ( task_status = status , events = events , log_detail = log_detail , job_descriptor = job_descriptor , end_time = end_time , last_update = last_update , pid = pid , script = script )
131	def is_partly_within_image ( self , image ) : return not self . is_out_of_image ( image , fully = True , partly = False )
12607	def find_unique ( table , sample , unique_fields = None ) : res = search_unique ( table , sample , unique_fields ) if res is not None : return res . eid else : return res
1421	def load ( file_object ) : marshaller = JavaObjectUnmarshaller ( file_object ) marshaller . add_transformer ( DefaultObjectTransformer ( ) ) return marshaller . readObject ( )
10573	def get_local_songs ( filepaths , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False , exclude_patterns = None , max_depth = float ( 'inf' ) ) : logger . info ( "Loading local songs..." ) supported_filepaths = get_supported_filepaths ( filepaths , SUPPORTED_SONG_FORMATS , max_depth = max_depth ) included_songs , excluded_songs = exclude_filepaths ( supported_filepaths , exclude_patterns = exclude_patterns ) matched_songs , filtered_songs = filter_local_songs ( included_songs , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) logger . info ( "Excluded {0} local songs" . format ( len ( excluded_songs ) ) ) logger . info ( "Filtered {0} local songs" . format ( len ( filtered_songs ) ) ) logger . info ( "Loaded {0} local songs" . format ( len ( matched_songs ) ) ) return matched_songs , filtered_songs , excluded_songs
6731	def create_module ( name , code = None ) : if name not in sys . modules : sys . modules [ name ] = imp . new_module ( name ) module = sys . modules [ name ] if code : print ( 'executing code for %s: %s' % ( name , code ) ) exec ( code in module . __dict__ ) exec ( "from %s import %s" % ( name , '*' ) ) return module
2489	def create_extracted_license ( self , lic ) : licenses = list ( self . graph . triples ( ( None , self . spdx_namespace . licenseId , lic . identifier ) ) ) if len ( licenses ) != 0 : return licenses [ 0 ] [ 0 ] else : license_node = BNode ( ) type_triple = ( license_node , RDF . type , self . spdx_namespace . ExtractedLicensingInfo ) self . graph . add ( type_triple ) ident_triple = ( license_node , self . spdx_namespace . licenseId , Literal ( lic . identifier ) ) self . graph . add ( ident_triple ) text_triple = ( license_node , self . spdx_namespace . extractedText , Literal ( lic . text ) ) self . graph . add ( text_triple ) if lic . full_name is not None : name_triple = ( license_node , self . spdx_namespace . licenseName , self . to_special_value ( lic . full_name ) ) self . graph . add ( name_triple ) for ref in lic . cross_ref : triple = ( license_node , RDFS . seeAlso , URIRef ( ref ) ) self . graph . add ( triple ) if lic . comment is not None : comment_triple = ( license_node , RDFS . comment , Literal ( lic . comment ) ) self . graph . add ( comment_triple ) return license_node
12744	def get_internal_urls ( self ) : internal_urls = self . get_subfields ( "856" , "u" , i1 = "4" , i2 = "0" ) internal_urls . extend ( self . get_subfields ( "998" , "a" ) ) internal_urls . extend ( self . get_subfields ( "URL" , "u" ) ) return map ( lambda x : x . replace ( "&amp;" , "&" ) , internal_urls )
11180	def exchange_token ( self , code ) : access_token_url = OAUTH_ROOT + '/access_token' params = { 'client_id' : self . client_id , 'client_secret' : self . client_secret , 'redirect_uri' : self . redirect_uri , 'code' : code , } resp = requests . get ( access_token_url , params = params ) if not resp . ok : raise MixcloudOauthError ( "Could not get access token." ) return resp . json ( ) [ 'access_token' ]
11886	def send_command ( self , command ) : with self . _lock : try : self . _socket . send ( command . encode ( "utf8" ) ) result = self . receive ( ) while result . startswith ( "S" ) or result . startswith ( "NEW" ) : _LOGGER . debug ( "!Got response: %s" , result ) result = self . receive ( ) _LOGGER . debug ( "Received: %s" , result ) return result except socket . error as error : _LOGGER . error ( "Error sending command: %s" , error ) self . connect ( ) return ""
3515	def chartbeat_bottom ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ChartbeatBottomNode ( )
7854	def add_identity ( self , item_name , item_category = None , item_type = None ) : return DiscoIdentity ( self , item_name , item_category , item_type )
5104	def lines_scatter_args ( self , line_kwargs = None , scatter_kwargs = None , pos = None ) : if pos is not None : self . set_pos ( pos ) elif self . pos is None : self . set_pos ( ) edge_pos = [ 0 for e in self . edges ( ) ] for e in self . edges ( ) : ei = self . edge_index [ e ] edge_pos [ ei ] = ( self . pos [ e [ 0 ] ] , self . pos [ e [ 1 ] ] ) line_collecton_kwargs = { 'segments' : edge_pos , 'colors' : self . edge_color , 'linewidths' : ( 1 , ) , 'antialiaseds' : ( 1 , ) , 'linestyle' : 'solid' , 'transOffset' : None , 'cmap' : plt . cm . ocean_r , 'pickradius' : 5 , 'zorder' : 0 , 'facecolors' : None , 'norm' : None , 'offsets' : None , 'offset_position' : 'screen' , 'hatch' : None , } scatter_kwargs_ = { 'x' : self . pos [ : , 0 ] , 'y' : self . pos [ : , 1 ] , 's' : 50 , 'c' : self . vertex_fill_color , 'alpha' : None , 'norm' : None , 'vmin' : None , 'vmax' : None , 'marker' : 'o' , 'zorder' : 2 , 'cmap' : plt . cm . ocean_r , 'linewidths' : 1 , 'edgecolors' : self . vertex_color , 'facecolors' : None , 'antialiaseds' : None , 'offset_position' : 'screen' , 'hatch' : None , } line_kwargs = { } if line_kwargs is None else line_kwargs scatter_kwargs = { } if scatter_kwargs is None else scatter_kwargs for key , value in line_kwargs . items ( ) : if key in line_collecton_kwargs : line_collecton_kwargs [ key ] = value for key , value in scatter_kwargs . items ( ) : if key in scatter_kwargs_ : scatter_kwargs_ [ key ] = value return line_collecton_kwargs , scatter_kwargs_
6508	def generate_field_filters ( cls , ** kwargs ) : generator = _load_class ( getattr ( settings , "SEARCH_FILTER_GENERATOR" , None ) , cls ) ( ) return ( generator . field_dictionary ( ** kwargs ) , generator . filter_dictionary ( ** kwargs ) , generator . exclude_dictionary ( ** kwargs ) , )
2120	def associate_success_node ( self , parent , child = None , ** kwargs ) : return self . _assoc_or_create ( 'success' , parent , child , ** kwargs )
11561	def play_tone ( self , pin , tone_command , frequency , duration ) : if tone_command == self . TONE_TONE : if duration : data = [ tone_command , pin , frequency & 0x7f , ( frequency >> 7 ) & 0x7f , duration & 0x7f , ( duration >> 7 ) & 0x7f ] else : data = [ tone_command , pin , frequency & 0x7f , ( frequency >> 7 ) & 0x7f , 0 , 0 ] self . _command_handler . digital_response_table [ pin ] [ self . _command_handler . RESPONSE_TABLE_MODE ] = self . TONE else : data = [ tone_command , pin ] self . _command_handler . send_sysex ( self . _command_handler . TONE_PLAY , data )
1427	def create_parser ( subparsers ) : parser = subparsers . add_parser ( 'update' , help = 'Update a topology' , usage = "%(prog)s [options] cluster/[role]/[env] <topology-name> " + "[--component-parallelism <name:value>] " + "[--container-number value] " + "[--runtime-config [component:]<name:value>]" , add_help = True ) args . add_titles ( parser ) args . add_cluster_role_env ( parser ) args . add_topology ( parser ) args . add_config ( parser ) args . add_dry_run ( parser ) args . add_service_url ( parser ) args . add_verbose ( parser ) def parallelism_type ( value ) : pattern = re . compile ( r"^[\w\.-]+:[\d]+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for component parallelism (<component_name:value>): %s" % value ) return value parser . add_argument ( '--component-parallelism' , action = 'append' , type = parallelism_type , required = False , help = 'Component name and the new parallelism value ' + 'colon-delimited: <component_name>:<parallelism>' ) def runtime_config_type ( value ) : pattern = re . compile ( r"^([\w\.-]+:){1,2}[\w\.-]+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for runtime config ([component:]<name:value>): %s" % value ) return value parser . add_argument ( '--runtime-config' , action = 'append' , type = runtime_config_type , required = False , help = 'Runtime configurations for topology and components ' + 'colon-delimited: [component:]<name>:<value>' ) def container_number_type ( value ) : pattern = re . compile ( r"^\d+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for container number (value): %s" % value ) return value parser . add_argument ( '--container-number' , action = 'append' , type = container_number_type , required = False , help = 'Number of containers <value>' ) parser . set_defaults ( subcommand = 'update' ) return parser
6960	def read_model_table ( modelfile ) : infd = gzip . open ( modelfile ) model = np . genfromtxt ( infd , names = True ) infd . close ( ) return model
1043	def generic_visit ( self , node ) : for field_name in node . _fields : setattr ( node , field_name , self . visit ( getattr ( node , field_name ) ) ) return node
4232	def main ( ) : args = argparser ( ) . parse_args ( sys . argv [ 1 : ] ) password = os . environ . get ( 'PYNETGEAR_PASSWORD' ) or args . password netgear = Netgear ( password , args . host , args . user , args . port , args . ssl , args . url , args . force_login_v2 ) results = run_subcommand ( netgear , args ) formatter = make_formatter ( args . format ) if results is None : print ( "Error communicating with the Netgear router" ) else : formatter ( results )
7582	def _call_raxml ( command_list ) : proc = subprocess . Popen ( command_list , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) comm = proc . communicate ( ) return comm
9342	def kill_all ( self ) : for pid in self . children : try : os . kill ( pid , signal . SIGTRAP ) except OSError : continue self . join ( )
11940	def mark_read ( user , message ) : BackendClass = stored_messages_settings . STORAGE_BACKEND backend = BackendClass ( ) backend . inbox_delete ( user , message )
5913	def cat ( self , out_ndx = None ) : if out_ndx is None : out_ndx = self . output self . make_ndx ( o = out_ndx , input = [ 'q' ] ) return out_ndx
824	def mostLikely ( self , pred ) : if len ( pred ) == 1 : return pred . keys ( ) [ 0 ] mostLikelyOutcome = None maxProbability = 0 for prediction , probability in pred . items ( ) : if probability > maxProbability : mostLikelyOutcome = prediction maxProbability = probability return mostLikelyOutcome
6333	def dist_abs ( self , src , tar ) : return self . _lev . dist_abs ( src , tar , mode = 'lev' , cost = ( 1 , 1 , 9999 , 9999 ) )
7050	def _reform_templatelc_for_tfa ( task ) : try : ( lcfile , lcformat , lcformatdir , tcol , mcol , ecol , timebase , interpolate_type , sigclip ) = task try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None lcdict = readerfunc ( lcfile ) if ( ( isinstance ( lcdict , ( list , tuple ) ) ) and ( isinstance ( lcdict [ 0 ] , dict ) ) ) : lcdict = lcdict [ 0 ] outdict = { } if '.' in tcol : tcolget = tcol . split ( '.' ) else : tcolget = [ tcol ] times = _dict_get ( lcdict , tcolget ) if '.' in mcol : mcolget = mcol . split ( '.' ) else : mcolget = [ mcol ] mags = _dict_get ( lcdict , mcolget ) if '.' in ecol : ecolget = ecol . split ( '.' ) else : ecolget = [ ecol ] errs = _dict_get ( lcdict , ecolget ) if normfunc is None : ntimes , nmags = normalize_magseries ( times , mags , magsarefluxes = magsarefluxes ) times , mags , errs = ntimes , nmags , errs stimes , smags , serrs = sigclip_magseries ( times , mags , errs , sigclip = sigclip ) mags_interpolator = spi . interp1d ( stimes , smags , kind = interpolate_type , fill_value = 'extrapolate' ) errs_interpolator = spi . interp1d ( stimes , serrs , kind = interpolate_type , fill_value = 'extrapolate' ) interpolated_mags = mags_interpolator ( timebase ) interpolated_errs = errs_interpolator ( timebase ) magmedian = np . median ( interpolated_mags ) renormed_mags = interpolated_mags - magmedian outdict = { 'mags' : renormed_mags , 'errs' : interpolated_errs , 'origmags' : interpolated_mags } return outdict except Exception as e : LOGEXCEPTION ( 'reform LC task failed: %s' % repr ( task ) ) return None
8721	def operation_download ( uploader , sources ) : sources , destinations = destination_from_source ( sources , False ) print ( 'sources' , sources ) print ( 'destinations' , destinations ) if len ( destinations ) == len ( sources ) : if uploader . prepare ( ) : for filename , dst in zip ( sources , destinations ) : uploader . read_file ( filename , dst ) else : raise Exception ( 'You must specify a destination filename for each file you want to download.' ) log . info ( 'All done!' )
9757	def stop ( ctx , yes ) : user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if not yes and not click . confirm ( "Are sure you want to stop " "experiment `{}`" . format ( _experiment ) ) : click . echo ( 'Existing without stopping experiment.' ) sys . exit ( 0 ) try : PolyaxonClient ( ) . experiment . stop ( user , project_name , _experiment ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not stop experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiment is being stopped." )
5347	def compose_github ( projects , data ) : for p in [ project for project in data if len ( data [ project ] [ 'github_repos' ] ) > 0 ] : if 'github' not in projects [ p ] : projects [ p ] [ 'github' ] = [ ] urls = [ url [ 'url' ] for url in data [ p ] [ 'github_repos' ] if url [ 'url' ] not in projects [ p ] [ 'github' ] ] projects [ p ] [ 'github' ] += urls return projects
11187	def create ( quiet , name , base_uri , symlink_path ) : _validate_name ( name ) admin_metadata = dtoolcore . generate_admin_metadata ( name ) parsed_base_uri = dtoolcore . utils . generous_parse_uri ( base_uri ) if parsed_base_uri . scheme == "symlink" : if symlink_path is None : raise click . UsageError ( "Need to specify symlink path using the -s/--symlink-path option" ) if symlink_path : base_uri = dtoolcore . utils . sanitise_uri ( "symlink:" + parsed_base_uri . path ) parsed_base_uri = dtoolcore . utils . generous_parse_uri ( base_uri ) proto_dataset = dtoolcore . generate_proto_dataset ( admin_metadata = admin_metadata , base_uri = dtoolcore . utils . urlunparse ( parsed_base_uri ) , config_path = CONFIG_PATH ) if symlink_path : symlink_abspath = os . path . abspath ( symlink_path ) proto_dataset . _storage_broker . symlink_path = symlink_abspath try : proto_dataset . create ( ) except dtoolcore . storagebroker . StorageBrokerOSError as err : raise click . UsageError ( str ( err ) ) proto_dataset . put_readme ( "" ) if quiet : click . secho ( proto_dataset . uri ) else : click . secho ( "Created proto dataset " , nl = False , fg = "green" ) click . secho ( proto_dataset . uri ) click . secho ( "Next steps: " ) step = 1 if parsed_base_uri . scheme != "symlink" : click . secho ( "{}. Add raw data, eg:" . format ( step ) ) click . secho ( " dtool add item my_file.txt {}" . format ( proto_dataset . uri ) , fg = "cyan" ) if parsed_base_uri . scheme == "file" : data_path = proto_dataset . _storage_broker . _data_abspath click . secho ( " Or use your system commands, e.g: " ) click . secho ( " mv my_data_directory {}/" . format ( data_path ) , fg = "cyan" ) step = step + 1 click . secho ( "{}. Add descriptive metadata, e.g: " . format ( step ) ) click . secho ( " dtool readme interactive {}" . format ( proto_dataset . uri ) , fg = "cyan" ) step = step + 1 click . secho ( "{}. Convert the proto dataset into a dataset: " . format ( step ) ) click . secho ( " dtool freeze {}" . format ( proto_dataset . uri ) , fg = "cyan" )
3373	def add_cons_vars_to_problem ( model , what , ** kwargs ) : context = get_context ( model ) model . solver . add ( what , ** kwargs ) if context : context ( partial ( model . solver . remove , what ) )
4360	def _receiver_loop ( self ) : while True : rawdata = self . get_server_msg ( ) if not rawdata : continue try : pkt = packet . decode ( rawdata , self . json_loads ) except ( ValueError , KeyError , Exception ) as e : self . error ( 'invalid_packet' , "There was a decoding error when dealing with packet " "with event: %s... (%s)" % ( rawdata [ : 20 ] , e ) ) continue if pkt [ 'type' ] == 'heartbeat' : continue if pkt [ 'type' ] == 'disconnect' and pkt [ 'endpoint' ] == '' : self . kill ( detach = True ) continue endpoint = pkt [ 'endpoint' ] if endpoint not in self . namespaces : self . error ( "no_such_namespace" , "The endpoint you tried to connect to " "doesn't exist: %s" % endpoint , endpoint = endpoint ) continue elif endpoint in self . active_ns : pkt_ns = self . active_ns [ endpoint ] else : new_ns_class = self . namespaces [ endpoint ] pkt_ns = new_ns_class ( self . environ , endpoint , request = self . request ) for cls in type ( pkt_ns ) . __mro__ : if hasattr ( cls , 'initialize' ) : cls . initialize ( pkt_ns ) self . active_ns [ endpoint ] = pkt_ns retval = pkt_ns . process_packet ( pkt ) if pkt . get ( 'ack' ) == "data" and pkt . get ( 'id' ) : if type ( retval ) is tuple : args = list ( retval ) else : args = [ retval ] returning_ack = dict ( type = 'ack' , ackId = pkt [ 'id' ] , args = args , endpoint = pkt . get ( 'endpoint' , '' ) ) self . send_packet ( returning_ack ) if not self . connected : self . kill ( detach = True ) return
7101	def on_marker ( self , mid ) : self . marker = Circle ( __id__ = mid ) self . parent ( ) . markers [ mid ] = self self . marker . setTag ( mid ) d = self . declaration if d . clickable : self . set_clickable ( d . clickable ) del self . options
12205	def url_builder ( self , endpoint , * , root = None , params = None , url_params = None ) : if root is None : root = self . ROOT scheme , netloc , path , _ , _ = urlsplit ( root ) return urlunsplit ( ( scheme , netloc , urljoin ( path , endpoint ) , urlencode ( url_params or { } ) , '' , ) ) . format ( ** params or { } )
9721	async def start ( self , rtfromfile = False ) : cmd = "start" + ( " rtfromfile" if rtfromfile else "" ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
4008	def _increase_file_handle_limit ( ) : logging . info ( 'Increasing file handle limit to {}' . format ( constants . FILE_HANDLE_LIMIT ) ) resource . setrlimit ( resource . RLIMIT_NOFILE , ( constants . FILE_HANDLE_LIMIT , resource . RLIM_INFINITY ) )
10442	def getobjectlist ( self , window_name ) : try : window_handle , name , app = self . _get_window_handle ( window_name , True ) object_list = self . _get_appmap ( window_handle , name , True ) except atomac . _a11y . ErrorInvalidUIElement : self . _windows = { } window_handle , name , app = self . _get_window_handle ( window_name , True ) object_list = self . _get_appmap ( window_handle , name , True ) return object_list . keys ( )
7637	def smkdirs ( dpath , mode = 0o777 ) : if not os . path . exists ( dpath ) : os . makedirs ( dpath , mode = mode )
1434	def custom_serialized ( cls , serialized , is_java = True ) : if not isinstance ( serialized , bytes ) : raise TypeError ( "Argument to custom_serialized() must be " "a serialized Python class as bytes, given: %s" % str ( serialized ) ) if not is_java : return cls . CUSTOM ( gtype = topology_pb2 . Grouping . Value ( "CUSTOM" ) , python_serialized = serialized ) else : raise NotImplementedError ( "Custom grouping implemented in Java for Python topology" "is not yet supported." )
5483	def setup_service ( api_name , api_version , credentials = None ) : if not credentials : credentials = oauth2client . client . GoogleCredentials . get_application_default ( ) return apiclient . discovery . build ( api_name , api_version , credentials = credentials )
10342	def overlay_data ( graph : BELGraph , data : Mapping [ BaseEntity , Any ] , label : Optional [ str ] = None , overwrite : bool = False , ) -> None : if label is None : label = WEIGHT for node , value in data . items ( ) : if node not in graph : log . debug ( '%s not in graph' , node ) continue if label in graph . nodes [ node ] and not overwrite : log . debug ( '%s already on %s' , label , node ) continue graph . nodes [ node ] [ label ] = value
13589	def json_post_required ( * decorator_args ) : def decorator ( method ) : @ wraps ( method ) def wrapper ( * args , ** kwargs ) : field = decorator_args [ 0 ] if len ( decorator_args ) == 2 : request_name = decorator_args [ 1 ] else : request_name = field request = args [ 0 ] if request . method != 'POST' : logger . error ( 'POST required for this url' ) raise Http404 ( 'only POST allowed for this url' ) if field not in request . POST : s = 'Expected field named %s in POST' % field logger . error ( s ) raise Http404 ( s ) setattr ( request , request_name , json . loads ( request . POST [ field ] ) ) return method ( * args , ** kwargs ) return wrapper return decorator
8976	def file ( self , file = None ) : if file is None : file = StringIO ( ) self . _file ( file ) return file
10356	def random_by_nodes ( graph : BELGraph , percentage : Optional [ float ] = None ) -> BELGraph : percentage = percentage or 0.9 assert 0 < percentage <= 1 nodes = graph . nodes ( ) n = int ( len ( nodes ) * percentage ) subnodes = random . sample ( nodes , n ) result = graph . subgraph ( subnodes ) update_node_helper ( graph , result ) return result
13304	def mfbe ( a , b ) : return 2 * bias ( a , b ) / ( a . mean ( ) + b . mean ( ) )
5051	def commit ( self ) : if self . _child_consents : consents = [ ] for consent in self . _child_consents : consent . granted = self . granted consents . append ( consent . save ( ) or consent ) return ProxyDataSharingConsent . from_children ( self . program_uuid , * consents ) consent , _ = DataSharingConsent . objects . update_or_create ( enterprise_customer = self . enterprise_customer , username = self . username , course_id = self . course_id , defaults = { 'granted' : self . granted } ) self . _exists = consent . exists return consent
6803	def assume_localhost ( self ) : if not self . genv . host_string : self . genv . host_string = 'localhost' self . genv . hosts = [ 'localhost' ] self . genv . user = getpass . getuser ( )
4709	def power_btn ( self , interval = 200 ) : if self . __power_btn_port is None : cij . err ( "cij.usb.relay: Invalid USB_RELAY_POWER_BTN" ) return 1 return self . __press ( self . __power_btn_port , interval = interval )
4417	async def play_now ( self , requester : int , track : dict ) : self . add_next ( requester , track ) await self . play ( ignore_shuffle = True )
12368	def records ( self , name ) : if self . get ( name ) : return DomainRecords ( self . api , name )
5494	def expand_mentions ( text , embed_names = True ) : if embed_names : mention_format = "@<{name} {url}>" else : mention_format = "@<{url}>" def handle_mention ( match ) : source = get_source_by_name ( match . group ( 1 ) ) if source is None : return "@{0}" . format ( match . group ( 1 ) ) return mention_format . format ( name = source . nick , url = source . url ) return short_mention_re . sub ( handle_mention , text )
8420	def same_log10_order_of_magnitude ( x , delta = 0.1 ) : dmin = np . log10 ( np . min ( x ) * ( 1 - delta ) ) dmax = np . log10 ( np . max ( x ) * ( 1 + delta ) ) return np . floor ( dmin ) == np . floor ( dmax )
227	def get_max_median_position_concentration ( positions ) : expos = get_percent_alloc ( positions ) expos = expos . drop ( 'cash' , axis = 1 ) longs = expos . where ( expos . applymap ( lambda x : x > 0 ) ) shorts = expos . where ( expos . applymap ( lambda x : x < 0 ) ) alloc_summary = pd . DataFrame ( ) alloc_summary [ 'max_long' ] = longs . max ( axis = 1 ) alloc_summary [ 'median_long' ] = longs . median ( axis = 1 ) alloc_summary [ 'median_short' ] = shorts . median ( axis = 1 ) alloc_summary [ 'max_short' ] = shorts . min ( axis = 1 ) return alloc_summary
9711	def heappush_max ( heap , item ) : heap . append ( item ) _siftdown_max ( heap , 0 , len ( heap ) - 1 )
5266	def sentencecase ( string ) : joiner = ' ' string = re . sub ( r"[\-_\.\s]" , joiner , str ( string ) ) if not string : return string return capitalcase ( trimcase ( re . sub ( r"[A-Z]" , lambda matched : joiner + lowercase ( matched . group ( 0 ) ) , string ) ) )
12021	def add_line_error ( self , line_data , error_info , log_level = logging . ERROR ) : if not error_info : return try : line_data [ 'line_errors' ] . append ( error_info ) except KeyError : line_data [ 'line_errors' ] = [ error_info ] except TypeError : pass try : self . logger . log ( log_level , Gff3 . error_format . format ( current_line_num = line_data [ 'line_index' ] + 1 , error_type = error_info [ 'error_type' ] , message = error_info [ 'message' ] , line = line_data [ 'line_raw' ] . rstrip ( ) ) ) except AttributeError : pass
188	def clip_out_of_image ( self ) : lss_cut = [ ls_clipped for ls in self . line_strings for ls_clipped in ls . clip_out_of_image ( self . shape ) ] return LineStringsOnImage ( lss_cut , shape = self . shape )
2217	def _list_itemstrs ( list_ , ** kwargs ) : items = list ( list_ ) kwargs [ '_return_info' ] = True _tups = [ repr2 ( item , ** kwargs ) for item in items ] itemstrs = [ t [ 0 ] for t in _tups ] max_height = max ( [ t [ 1 ] [ 'max_height' ] for t in _tups ] ) if _tups else 0 _leaf_info = { 'max_height' : max_height + 1 , } sort = kwargs . get ( 'sort' , None ) if sort is None : sort = isinstance ( list_ , ( set , frozenset ) ) if sort : itemstrs = _sort_itemstrs ( items , itemstrs ) return itemstrs , _leaf_info
2444	def set_annotation_spdx_id ( self , doc , spdx_id ) : if len ( doc . annotations ) != 0 : if not self . annotation_spdx_id_set : self . annotation_spdx_id_set = True doc . annotations [ - 1 ] . spdx_id = spdx_id return True else : raise CardinalityError ( 'Annotation::SPDXREF' ) else : raise OrderError ( 'Annotation::SPDXREF' )
898	def addSpatialNoise ( self , sequence , amount ) : newSequence = [ ] for pattern in sequence : if pattern is not None : pattern = self . patternMachine . addNoise ( pattern , amount ) newSequence . append ( pattern ) return newSequence
23	def pickle_load ( path , compression = False ) : if compression : with zipfile . ZipFile ( path , "r" , compression = zipfile . ZIP_DEFLATED ) as myzip : with myzip . open ( "data" ) as f : return pickle . load ( f ) else : with open ( path , "rb" ) as f : return pickle . load ( f )
3143	def get ( self , file_id , ** queryparams ) : self . file_id = file_id return self . _mc_client . _get ( url = self . _build_path ( file_id ) , ** queryparams )
6242	def load_shader ( self , shader_type : str , path : str ) : if path : resolved_path = self . find_program ( path ) if not resolved_path : raise ValueError ( "Cannot find {} shader '{}'" . format ( shader_type , path ) ) print ( "Loading:" , path ) with open ( resolved_path , 'r' ) as fd : return fd . read ( )
8581	def update_server ( self , datacenter_id , server_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : if attr == 'boot_volume' : boot_volume_properties = { "id" : value } boot_volume_entities = { "bootVolume" : boot_volume_properties } data . update ( boot_volume_entities ) else : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/servers/%s' % ( datacenter_id , server_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
8045	def parse_docstring ( self ) : self . log . debug ( "parsing docstring, token is %r (%s)" , self . current . kind , self . current . value ) while self . current . kind in ( tk . COMMENT , tk . NEWLINE , tk . NL ) : self . stream . move ( ) self . log . debug ( "parsing docstring, token is %r (%s)" , self . current . kind , self . current . value , ) if self . current . kind == tk . STRING : docstring = self . current . value self . stream . move ( ) return docstring return None
7197	def plot ( self , spec = "rgb" , ** kwargs ) : if self . shape [ 0 ] == 1 or ( "bands" in kwargs and len ( kwargs [ "bands" ] ) == 1 ) : if "cmap" in kwargs : cmap = kwargs [ "cmap" ] del kwargs [ "cmap" ] else : cmap = "Greys_r" self . _plot ( tfm = self . _single_band , cmap = cmap , ** kwargs ) else : if spec == "rgb" and self . _has_token ( ** kwargs ) : self . _plot ( tfm = self . rgb , ** kwargs ) else : self . _plot ( tfm = getattr ( self , spec ) , ** kwargs )
3055	def from_string ( cls , key , password = 'notasecret' ) : key = _helpers . _from_bytes ( key ) marker_id , key_bytes = pem . readPemBlocksFromFile ( six . StringIO ( key ) , _PKCS1_MARKER , _PKCS8_MARKER ) if marker_id == 0 : pkey = rsa . key . PrivateKey . load_pkcs1 ( key_bytes , format = 'DER' ) elif marker_id == 1 : key_info , remaining = decoder . decode ( key_bytes , asn1Spec = _PKCS8_SPEC ) if remaining != b'' : raise ValueError ( 'Unused bytes' , remaining ) pkey_info = key_info . getComponentByName ( 'privateKey' ) pkey = rsa . key . PrivateKey . load_pkcs1 ( pkey_info . asOctets ( ) , format = 'DER' ) else : raise ValueError ( 'No key could be detected.' ) return cls ( pkey )
1406	def validated_formatter ( self , url_format ) : valid_parameters = { "${CLUSTER}" : "cluster" , "${ENVIRON}" : "environ" , "${TOPOLOGY}" : "topology" , "${ROLE}" : "role" , "${USER}" : "user" , } dummy_formatted_url = url_format for key , value in valid_parameters . items ( ) : dummy_formatted_url = dummy_formatted_url . replace ( key , value ) if '$' in dummy_formatted_url : raise Exception ( "Invalid viz.url.format: %s" % ( url_format ) ) return url_format
3183	def update ( self , store_id , data ) : self . store_id = store_id return self . _mc_client . _patch ( url = self . _build_path ( store_id ) , data = data )
5560	def bounds ( self ) : if self . _raw [ "bounds" ] is None : return self . process_pyramid . bounds else : return Bounds ( * _validate_bounds ( self . _raw [ "bounds" ] ) )
6898	def serial_periodicfeatures ( pfpkl_list , lcbasedir , outdir , starfeaturesdir = None , fourierorder = 5 , transitparams = ( - 0.01 , 0.1 , 0.1 ) , ebparams = ( - 0.2 , 0.3 , 0.7 , 0.5 ) , pdiff_threshold = 1.0e-4 , sidereal_threshold = 1.0e-4 , sampling_peak_multiplier = 5.0 , sampling_startp = None , sampling_endp = None , starfeatures = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , sigclip = 10.0 , verbose = False , maxobjects = None ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( fileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if maxobjects : pfpkl_list = pfpkl_list [ : maxobjects ] LOGINFO ( '%s periodfinding pickles to process' % len ( pfpkl_list ) ) if starfeaturesdir and os . path . exists ( starfeaturesdir ) : starfeatures_list = [ ] LOGINFO ( 'collecting starfeatures pickles...' ) for pfpkl in pfpkl_list : sfpkl1 = os . path . basename ( pfpkl ) . replace ( 'periodfinding' , 'starfeatures' ) sfpkl2 = sfpkl1 . replace ( '.gz' , '' ) sfpath1 = os . path . join ( starfeaturesdir , sfpkl1 ) sfpath2 = os . path . join ( starfeaturesdir , sfpkl2 ) if os . path . exists ( sfpath1 ) : starfeatures_list . append ( sfpkl1 ) elif os . path . exists ( sfpath2 ) : starfeatures_list . append ( sfpkl2 ) else : starfeatures_list . append ( None ) else : starfeatures_list = [ None for x in pfpkl_list ] kwargs = { 'fourierorder' : fourierorder , 'transitparams' : transitparams , 'ebparams' : ebparams , 'pdiff_threshold' : pdiff_threshold , 'sidereal_threshold' : sidereal_threshold , 'sampling_peak_multiplier' : sampling_peak_multiplier , 'sampling_startp' : sampling_startp , 'sampling_endp' : sampling_endp , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'lcformat' : lcformat , 'lcformatdir' : lcformatdir , 'sigclip' : sigclip , 'verbose' : verbose } tasks = [ ( x , lcbasedir , outdir , y , kwargs ) for ( x , y ) in zip ( pfpkl_list , starfeatures_list ) ] LOGINFO ( 'processing periodfinding pickles...' ) for task in tqdm ( tasks ) : _periodicfeatures_worker ( task )
2736	def create ( self , * args , ** kwargs ) : data = self . get_data ( 'floating_ips/' , type = POST , params = { 'droplet_id' : self . droplet_id } ) if data : self . ip = data [ 'floating_ip' ] [ 'ip' ] self . region = data [ 'floating_ip' ] [ 'region' ] return self
4041	def _retrieve_data ( self , request = None ) : full_url = "%s%s" % ( self . endpoint , request ) self . self_link = request self . request = requests . get ( url = full_url , headers = self . default_headers ( ) ) self . request . encoding = "utf-8" try : self . request . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( self . request ) return self . request
11764	def compute_utility ( self , board , move , player ) : "If X wins with this move, return 1; if O return -1; else return 0." if ( self . k_in_row ( board , move , player , ( 0 , 1 ) ) or self . k_in_row ( board , move , player , ( 1 , 0 ) ) or self . k_in_row ( board , move , player , ( 1 , - 1 ) ) or self . k_in_row ( board , move , player , ( 1 , 1 ) ) ) : return if_ ( player == 'X' , + 1 , - 1 ) else : return 0
4309	def _validate_num_channels ( input_filepath_list , combine_type ) : channels = [ file_info . channels ( f ) for f in input_filepath_list ] if not core . all_equal ( channels ) : raise IOError ( "Input files do not have the same number of channels. The " "{} combine type requires that all files have the same " "number of channels" . format ( combine_type ) )
8006	def handle_read ( self ) : with self . _lock : logger . debug ( "handle_read()" ) if self . _socket is None : return while True : try : sock , address = self . _socket . accept ( ) except socket . error , err : if err . args [ 0 ] in BLOCKING_ERRORS : break else : raise logger . debug ( "Accepted connection from: {0!r}" . format ( address ) ) self . _target ( sock , address )
1792	def INC ( cpu , dest ) : arg0 = dest . read ( ) res = dest . write ( arg0 + 1 ) res &= ( 1 << dest . size ) - 1 SIGN_MASK = 1 << ( dest . size - 1 ) cpu . AF = ( ( arg0 ^ 1 ) ^ res ) & 0x10 != 0 cpu . ZF = res == 0 cpu . SF = ( res & SIGN_MASK ) != 0 cpu . OF = res == SIGN_MASK cpu . PF = cpu . _calculate_parity_flag ( res )
5158	def _add_uninstall ( self , context ) : contents = self . _render_template ( 'uninstall.sh' , context ) self . config . setdefault ( 'files' , [ ] ) self . _add_unique_file ( { "path" : "/uninstall.sh" , "contents" : contents , "mode" : "755" } )
1114	def _collect_lines ( self , diffs ) : fromlist , tolist , flaglist = [ ] , [ ] , [ ] for fromdata , todata , flag in diffs : try : fromlist . append ( self . _format_line ( 0 , flag , * fromdata ) ) tolist . append ( self . _format_line ( 1 , flag , * todata ) ) except TypeError : fromlist . append ( None ) tolist . append ( None ) flaglist . append ( flag ) return fromlist , tolist , flaglist
1807	def SETBE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . OR ( cpu . CF , cpu . ZF ) , 1 , 0 ) )
296	def plot_sector_allocations ( returns , sector_alloc , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) sector_alloc . plot ( title = 'Sector allocation over time' , alpha = 0.5 , ax = ax , ** kwargs ) box = ax . get_position ( ) ax . set_position ( [ box . x0 , box . y0 + box . height * 0.1 , box . width , box . height * 0.9 ] ) ax . legend ( loc = 'upper center' , frameon = True , framealpha = 0.5 , bbox_to_anchor = ( 0.5 , - 0.14 ) , ncol = 5 ) ax . set_xlim ( ( sector_alloc . index [ 0 ] , sector_alloc . index [ - 1 ] ) ) ax . set_ylabel ( 'Exposure by sector' ) ax . set_xlabel ( '' ) return ax
7052	def parallel_tfa_lcdir ( lcdir , templateinfo , lcfileglob = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , interp = 'nearest' , sigclip = 5.0 , mintemplatedist_arcmin = 10.0 , nworkers = NCPUS , maxworkertasks = 1000 ) : if isinstance ( templateinfo , str ) and os . path . exists ( templateinfo ) : with open ( templateinfo , 'rb' ) as infd : templateinfo = pickle . load ( infd ) try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if lcfileglob is None : lcfileglob = dfileglob lclist = sorted ( glob . glob ( os . path . join ( lcdir , lcfileglob ) ) ) return parallel_tfa_lclist ( lclist , templateinfo , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = None , interp = interp , sigclip = sigclip , mintemplatedist_arcmin = mintemplatedist_arcmin , nworkers = nworkers , maxworkertasks = maxworkertasks )
7946	def _write ( self , data ) : OUT_LOGGER . debug ( "OUT: %r" , data ) if self . _hup or not self . _socket : raise PyXMPPIOError ( u"Connection closed." ) try : while data : try : sent = self . _socket . send ( data ) except ssl . SSLError , err : if err . args [ 0 ] == ssl . SSL_ERROR_WANT_WRITE : continue else : raise except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue if err . args [ 0 ] in BLOCKING_ERRORS : wait_for_write ( self . _socket ) continue raise data = data [ sent : ] except ( IOError , OSError , socket . error ) , err : raise PyXMPPIOError ( u"IO Error: {0}" . format ( err ) )
5754	def bootstrap_paginate ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) < 2 : raise TemplateSyntaxError ( "'%s' takes at least one argument" " (Page object reference)" % bits [ 0 ] ) page = parser . compile_filter ( bits [ 1 ] ) kwargs = { } bits = bits [ 2 : ] kwarg_re = re . compile ( r'(\w+)=(.+)' ) if len ( bits ) : for bit in bits : match = kwarg_re . match ( bit ) if not match : raise TemplateSyntaxError ( "Malformed arguments to bootstrap_pagination paginate tag" ) name , value = match . groups ( ) kwargs [ name ] = parser . compile_filter ( value ) return BootstrapPaginationNode ( page , kwargs )
5202	def create_connection ( port = _PORT_ , timeout = _TIMEOUT_ , restart = False ) : if _CON_SYM_ in globals ( ) : if not isinstance ( globals ( ) [ _CON_SYM_ ] , pdblp . BCon ) : del globals ( ) [ _CON_SYM_ ] if ( _CON_SYM_ in globals ( ) ) and ( not restart ) : con = globals ( ) [ _CON_SYM_ ] if getattr ( con , '_session' ) . start ( ) : con . start ( ) return con , False else : con = pdblp . BCon ( port = port , timeout = timeout ) globals ( ) [ _CON_SYM_ ] = con con . start ( ) return con , True
11711	def add_logged_in_session ( self , response = None ) : if not response : response = self . get ( 'go/api/pipelines.xml' ) self . _set_session_cookie ( response ) if not self . _session_id : raise AuthenticationFailed ( 'No session id extracted from request.' ) response = self . get ( 'go/pipelines' ) match = re . search ( r'name="authenticity_token".+?value="([^"]+)' , response . read ( ) . decode ( 'utf-8' ) ) if match : self . _authenticity_token = match . group ( 1 ) else : raise AuthenticationFailed ( 'Authenticity token not found on page' )
6412	def agmean ( nums ) : m_a = amean ( nums ) m_g = gmean ( nums ) if math . isnan ( m_a ) or math . isnan ( m_g ) : return float ( 'nan' ) while round ( m_a , 12 ) != round ( m_g , 12 ) : m_a , m_g = ( m_a + m_g ) / 2 , ( m_a * m_g ) ** ( 1 / 2 ) return m_a
3779	def calculate_integral_over_T ( self , T1 , T2 , method ) : r return float ( quad ( lambda T : self . calculate ( T , method ) / T , T1 , T2 ) [ 0 ] )
5168	def __intermediate_htmode ( self , radio ) : protocol = radio . pop ( 'protocol' ) channel_width = radio . pop ( 'channel_width' ) if 'htmode' in radio : return radio [ 'htmode' ] if protocol == '802.11n' : return 'HT{0}' . format ( channel_width ) elif protocol == '802.11ac' : return 'VHT{0}' . format ( channel_width ) return 'NONE'
5178	def resource ( self , type_ , title , ** kwargs ) : resources = self . __api . resources ( type_ = type_ , title = title , query = EqualsOperator ( "certname" , self . name ) , ** kwargs ) return next ( resource for resource in resources )
12857	def with_peer ( events ) : stack = [ ] for obj in events : if obj [ 'type' ] == ENTER : stack . append ( obj ) yield obj , None elif obj [ 'type' ] == EXIT : yield obj , stack . pop ( ) else : yield obj , None
1482	def get_commands_to_run ( self ) : if len ( self . packing_plan . container_plans ) == 0 : return { } if self . _get_instance_plans ( self . packing_plan , self . shard ) is None and self . shard != 0 : retval = { } retval [ 'heron-shell' ] = Command ( [ '%s' % self . heron_shell_binary , '--port=%s' % self . shell_port , '--log_file_prefix=%s/heron-shell-%s.log' % ( self . log_dir , self . shard ) , '--secret=%s' % self . topology_id ] , self . shell_env ) return retval if self . shard == 0 : commands = self . _get_tmaster_processes ( ) else : self . _untar_if_needed ( ) commands = self . _get_streaming_processes ( ) commands . update ( self . _get_heron_support_processes ( ) ) return commands
1980	def sys_receive ( self , cpu , fd , buf , count , rx_bytes ) : if issymbolic ( fd ) : logger . info ( "Ask to read from a symbolic file descriptor!!" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 0 ) if issymbolic ( buf ) : logger . info ( "Ask to read to a symbolic buffer" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 1 ) if issymbolic ( count ) : logger . info ( "Ask to read a symbolic number of bytes " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 2 ) if issymbolic ( rx_bytes ) : logger . info ( "Ask to return size to a symbolic address " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 3 ) return super ( ) . sys_receive ( cpu , fd , buf , count , rx_bytes )
2062	def declarations ( self ) : declarations = GetDeclarations ( ) for a in self . constraints : try : declarations . visit ( a ) except RuntimeError : if sys . getrecursionlimit ( ) >= PickleSerializer . MAX_RECURSION : raise Exception ( f'declarations recursion limit surpassed {PickleSerializer.MAX_RECURSION}, aborting' ) new_limit = sys . getrecursionlimit ( ) + PickleSerializer . DEFAULT_RECURSION if new_limit <= PickleSerializer . DEFAULT_RECURSION : sys . setrecursionlimit ( new_limit ) return self . declarations return declarations . result
1131	def urldefrag ( url ) : if '#' in url : s , n , p , a , q , frag = urlparse ( url ) defrag = urlunparse ( ( s , n , p , a , q , '' ) ) return defrag , frag else : return url , ''
12235	def generate_versionwarning_data_json ( app , config = None , ** kwargs ) : config = config or kwargs . pop ( 'config' , None ) if config is None : config = app . config if config . versionwarning_project_version in config . versionwarning_messages : custom = True message = config . versionwarning_messages . get ( config . versionwarning_project_version ) else : custom = False message = config . versionwarning_default_message banner_html = config . versionwarning_banner_html . format ( id_div = config . versionwarning_banner_id_div , banner_title = config . versionwarning_banner_title , message = message . format ( ** { config . versionwarning_message_placeholder : '<a href="#"></a>' } , ) , admonition_type = config . versionwarning_admonition_type , ) data = json . dumps ( { 'meta' : { 'api_url' : config . versionwarning_api_url , } , 'banner' : { 'html' : banner_html , 'id_div' : config . versionwarning_banner_id_div , 'body_selector' : config . versionwarning_body_selector , 'custom' : custom , } , 'project' : { 'slug' : config . versionwarning_project_slug , } , 'version' : { 'slug' : config . versionwarning_project_version , } , } , indent = 4 ) data_path = os . path . join ( STATIC_PATH , 'data' ) if not os . path . exists ( data_path ) : os . mkdir ( data_path ) with open ( os . path . join ( data_path , JSON_DATA_FILENAME ) , 'w' ) as f : f . write ( data ) config . html_static_path . append ( STATIC_PATH )
12178	def ensureDetection ( self ) : if self . APs == False : self . log . debug ( "analysis attempted before event detection..." ) self . detect ( )
3602	def get_user ( self ) : token = self . authenticator . create_token ( self . extra ) user_id = self . extra . get ( 'id' ) return FirebaseUser ( self . email , token , self . provider , user_id )
11793	def forward_checking ( csp , var , value , assignment , removals ) : "Prune neighbor values inconsistent with var=value." for B in csp . neighbors [ var ] : if B not in assignment : for b in csp . curr_domains [ B ] [ : ] : if not csp . constraints ( var , value , B , b ) : csp . prune ( B , b , removals ) if not csp . curr_domains [ B ] : return False return True
8122	def error ( message ) : global parser print ( _ ( "Error: " ) + message ) print ( ) parser . print_help ( ) sys . exit ( )
744	def requireAnomalyModel ( func ) : @ wraps ( func ) def _decorator ( self , * args , ** kwargs ) : if not self . getInferenceType ( ) == InferenceType . TemporalAnomaly : raise RuntimeError ( "Method required a TemporalAnomaly model." ) if self . _getAnomalyClassifier ( ) is None : raise RuntimeError ( "Model does not support this command. Model must" "be an active anomalyDetector model." ) return func ( self , * args , ** kwargs ) return _decorator
11996	def _set_options ( self , options ) : if not options : return self . options . copy ( ) options = options . copy ( ) if 'magic' in options : self . set_magic ( options [ 'magic' ] ) del ( options [ 'magic' ] ) if 'flags' in options : flags = options [ 'flags' ] del ( options [ 'flags' ] ) for key , value in flags . iteritems ( ) : if not isinstance ( value , bool ) : raise TypeError ( 'Invalid flag type for: %s' % key ) else : flags = self . options [ 'flags' ] if 'info' in options : del ( options [ 'info' ] ) for key , value in options . iteritems ( ) : if not isinstance ( value , int ) : raise TypeError ( 'Invalid option type for: %s' % key ) if value < 0 or value > 255 : raise ValueError ( 'Option value out of range for: %s' % key ) new_options = self . options . copy ( ) new_options . update ( options ) new_options [ 'flags' ] . update ( flags ) return new_options
869	def clear ( cls , persistent = False ) : if persistent : try : os . unlink ( cls . getPath ( ) ) except OSError , e : if e . errno != errno . ENOENT : _getLogger ( ) . exception ( "Error %s while trying to remove dynamic " "configuration file: %s" , e . errno , cls . getPath ( ) ) raise cls . _path = None
4621	def _decrypt_masterpassword ( self ) : aes = AESCipher ( self . password ) checksum , encrypted_master = self . config [ self . config_key ] . split ( "$" ) try : decrypted_master = aes . decrypt ( encrypted_master ) except Exception : self . _raise_wrongmasterpassexception ( ) if checksum != self . _derive_checksum ( decrypted_master ) : self . _raise_wrongmasterpassexception ( ) self . decrypted_master = decrypted_master
4811	def evaluate ( best_processed_path , model ) : x_test_char , x_test_type , y_test = prepare_feature ( best_processed_path , option = 'test' ) y_predict = model . predict ( [ x_test_char , x_test_type ] ) y_predict = ( y_predict . ravel ( ) > 0.5 ) . astype ( int ) f1score = f1_score ( y_test , y_predict ) precision = precision_score ( y_test , y_predict ) recall = recall_score ( y_test , y_predict ) return f1score , precision , recall
1411	def filter_spouts ( table , header ) : spouts_info = [ ] for row in table : if row [ 0 ] == 'spout' : spouts_info . append ( row ) return spouts_info , header
3035	def flow_from_clientsecrets ( filename , scope , redirect_uri = None , message = None , cache = None , login_hint = None , device_uri = None , pkce = None , code_verifier = None , prompt = None ) : try : client_type , client_info = clientsecrets . loadfile ( filename , cache = cache ) if client_type in ( clientsecrets . TYPE_WEB , clientsecrets . TYPE_INSTALLED ) : constructor_kwargs = { 'redirect_uri' : redirect_uri , 'auth_uri' : client_info [ 'auth_uri' ] , 'token_uri' : client_info [ 'token_uri' ] , 'login_hint' : login_hint , } revoke_uri = client_info . get ( 'revoke_uri' ) optional = ( 'revoke_uri' , 'device_uri' , 'pkce' , 'code_verifier' , 'prompt' ) for param in optional : if locals ( ) [ param ] is not None : constructor_kwargs [ param ] = locals ( ) [ param ] return OAuth2WebServerFlow ( client_info [ 'client_id' ] , client_info [ 'client_secret' ] , scope , ** constructor_kwargs ) except clientsecrets . InvalidClientSecretsError as e : if message is not None : if e . args : message = ( 'The client secrets were invalid: ' '\n{0}\n{1}' . format ( e , message ) ) sys . exit ( message ) else : raise else : raise UnknownClientSecretsFlowError ( 'This OAuth 2.0 flow is unsupported: {0!r}' . format ( client_type ) )
7403	def above ( self , ref ) : if not self . _valid_ordering_reference ( ref ) : raise ValueError ( "%r can only be moved above instances of %r which %s equals %r." % ( self , self . __class__ , self . order_with_respect_to , self . _get_order_with_respect_to ( ) ) ) if self . order == ref . order : return if self . order > ref . order : o = ref . order else : o = self . get_ordering_queryset ( ) . filter ( order__lt = ref . order ) . aggregate ( Max ( 'order' ) ) . get ( 'order__max' ) or 0 self . to ( o )
5302	def parse_rgb_txt_file ( path ) : color_dict = { } with open ( path , 'r' ) as rgb_txt : for line in rgb_txt : line = line . strip ( ) if not line or line . startswith ( '!' ) : continue parts = line . split ( ) color_dict [ " " . join ( parts [ 3 : ] ) ] = ( int ( parts [ 0 ] ) , int ( parts [ 1 ] ) , int ( parts [ 2 ] ) ) return color_dict
1357	def get_argument_topology ( self ) : try : topology = self . get_argument ( constants . PARAM_TOPOLOGY ) return topology except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
10964	def trigger_update ( self , params , values ) : if self . _parent : self . _parent . trigger_update ( params , values ) else : self . update ( params , values )
7836	def register_disco_cache_fetchers ( cache_suite , stream ) : tmp = stream class DiscoInfoCacheFetcher ( DiscoCacheFetcherBase ) : stream = tmp disco_class = DiscoInfo class DiscoItemsCacheFetcher ( DiscoCacheFetcherBase ) : stream = tmp disco_class = DiscoItems cache_suite . register_fetcher ( DiscoInfo , DiscoInfoCacheFetcher ) cache_suite . register_fetcher ( DiscoItems , DiscoItemsCacheFetcher )
2243	def argval ( key , default = util_const . NoParam , argv = None ) : if argv is None : argv = sys . argv keys = [ key ] if isinstance ( key , six . string_types ) else key n_max = len ( argv ) - 1 for argx , item in enumerate ( argv ) : for key_ in keys : if item == key_ : if argx < n_max : value = argv [ argx + 1 ] return value elif item . startswith ( key_ + '=' ) : value = '=' . join ( item . split ( '=' ) [ 1 : ] ) return value value = default return value
4029	def load ( domain_name = "" ) : cj = http . cookiejar . CookieJar ( ) for cookie_fn in [ chrome , firefox ] : try : for cookie in cookie_fn ( domain_name = domain_name ) : cj . set_cookie ( cookie ) except BrowserCookieError : pass return cj
609	def _generateMetricSpecString ( inferenceElement , metric , params = None , field = None , returnLabel = False ) : metricSpecArgs = dict ( metric = metric , field = field , params = params , inferenceElement = inferenceElement ) metricSpecAsString = "MetricSpec(%s)" % ', ' . join ( [ '%s=%r' % ( item [ 0 ] , item [ 1 ] ) for item in metricSpecArgs . iteritems ( ) ] ) if not returnLabel : return metricSpecAsString spec = MetricSpec ( ** metricSpecArgs ) metricLabel = spec . getLabel ( ) return metricSpecAsString , metricLabel
6235	def get_time ( self ) -> float : if self . paused : return self . pause_time return mixer . music . get_pos ( ) / 1000.0
1790	def IDIV ( cpu , src ) : reg_name_h = { 8 : 'AH' , 16 : 'DX' , 32 : 'EDX' , 64 : 'RDX' } [ src . size ] reg_name_l = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ src . size ] dividend = Operators . CONCAT ( src . size * 2 , cpu . read_register ( reg_name_h ) , cpu . read_register ( reg_name_l ) ) divisor = src . read ( ) if isinstance ( divisor , int ) and divisor == 0 : raise DivideByZeroError ( ) dst_size = src . size * 2 divisor = Operators . SEXTEND ( divisor , src . size , dst_size ) mask = ( 1 << dst_size ) - 1 sign_mask = 1 << ( dst_size - 1 ) dividend_sign = ( dividend & sign_mask ) != 0 divisor_sign = ( divisor & sign_mask ) != 0 if isinstance ( divisor , int ) : if divisor_sign : divisor = ( ( ~ divisor ) + 1 ) & mask divisor = - divisor if isinstance ( dividend , int ) : if dividend_sign : dividend = ( ( ~ dividend ) + 1 ) & mask dividend = - dividend quotient = Operators . SDIV ( dividend , divisor ) if ( isinstance ( dividend , int ) and isinstance ( dividend , int ) ) : remainder = dividend - ( quotient * divisor ) else : remainder = Operators . SREM ( dividend , divisor ) cpu . write_register ( reg_name_l , Operators . EXTRACT ( quotient , 0 , src . size ) ) cpu . write_register ( reg_name_h , Operators . EXTRACT ( remainder , 0 , src . size ) )
4784	def contains_ignoring_case ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) if isinstance ( self . val , str_types ) : if len ( items ) == 1 : if not isinstance ( items [ 0 ] , str_types ) : raise TypeError ( 'given arg must be a string' ) if items [ 0 ] . lower ( ) not in self . val . lower ( ) : self . _err ( 'Expected <%s> to case-insensitive contain item <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : missing = [ ] for i in items : if not isinstance ( i , str_types ) : raise TypeError ( 'given args must all be strings' ) if i . lower ( ) not in self . val . lower ( ) : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to case-insensitive contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) elif isinstance ( self . val , Iterable ) : missing = [ ] for i in items : if not isinstance ( i , str_types ) : raise TypeError ( 'given args must all be strings' ) found = False for v in self . val : if not isinstance ( v , str_types ) : raise TypeError ( 'val items must all be strings' ) if i . lower ( ) == v . lower ( ) : found = True break if not found : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to case-insensitive contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
6395	def fingerprint ( self , word ) : word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = '' . join ( c for c in word if c in self . _letters ) key = '' for char in self . _consonants : if char in word : key += char for char in word : if char not in self . _consonants and char not in key : key += char return key
5063	def get_course_track_selection_url ( course_run , query_parameters ) : try : course_root = reverse ( 'course_modes_choose' , kwargs = { 'course_id' : course_run [ 'key' ] } ) except KeyError : LOGGER . exception ( "KeyError while parsing course run data.\nCourse Run: \n[%s]" , course_run , ) raise url = '{}{}' . format ( settings . LMS_ROOT_URL , course_root ) course_run_url = update_query_parameters ( url , query_parameters ) return course_run_url
11226	def dump_nparray ( self , obj , class_name = numpy_ndarray_class_name ) : return { "$" + class_name : self . _json_convert ( obj . tolist ( ) ) }
6756	def param_changed_to ( self , key , to_value , from_value = None ) : last_value = getattr ( self . last_manifest , key ) current_value = self . current_manifest . get ( key ) if from_value is not None : return last_value == from_value and current_value == to_value return last_value != to_value and current_value == to_value
1462	def new_source ( self , source ) : source_streamlet = None if callable ( source ) : source_streamlet = SupplierStreamlet ( source ) elif isinstance ( source , Generator ) : source_streamlet = GeneratorStreamlet ( source ) else : raise RuntimeError ( "Builder's new source has to be either a Generator or a function" ) self . _sources . append ( source_streamlet ) return source_streamlet
447	def _bias_scale ( x , b , data_format ) : if data_format == 'NHWC' : return x * b elif data_format == 'NCHW' : return x * _to_channel_first_bias ( b ) else : raise ValueError ( 'invalid data_format: %s' % data_format )
11902	def serve_dir ( dir_path ) : print ( 'Performing first pass index file generation' ) created_files = _create_index_files ( dir_path , True ) if ( PIL_ENABLED ) : print ( 'Performing PIL-enchanced optimised index file generation in background' ) background_indexer = BackgroundIndexFileGenerator ( dir_path ) background_indexer . run ( ) _run_server ( ) _clean_up ( created_files )
13261	def get_task_tree ( white_list = None ) : assert white_list is None or isinstance ( white_list , list ) , type ( white_list ) if white_list is not None : white_list = set ( item if isinstance ( item , str ) else item . __qualname__ for item in white_list ) tree = dict ( ( task . qualified_name , task ) for task in _task_list . values ( ) if white_list is None or task . qualified_name in white_list ) plugins = get_plugin_list ( ) for plugin in [ plugin for plugin in plugins . values ( ) if white_list is None or plugin . __qualname__ in white_list ] : tasks = [ func for _ , func in inspect . getmembers ( plugin ) if inspect . isfunction ( func ) and hasattr ( func , "yaz_task_config" ) ] if len ( tasks ) == 0 : continue node = tree for name in plugin . __qualname__ . split ( "." ) : if not name in node : node [ name ] = { } node = node [ name ] for func in tasks : logger . debug ( "Found task %s" , func ) node [ func . __name__ ] = Task ( plugin_class = plugin , func = func , config = func . yaz_task_config ) return tree
12199	def description ( self ) : if self . _description is None : text = '\n' . join ( self . __doc__ . splitlines ( ) [ 1 : ] ) . strip ( ) lines = [ ] for line in map ( str . strip , text . splitlines ( ) ) : if line and lines : lines [ - 1 ] = ' ' . join ( ( lines [ - 1 ] , line ) ) elif line : lines . append ( line ) else : lines . append ( '' ) self . _description = '\n' . join ( lines ) return self . _description
2048	def new_address ( self , sender = None , nonce = None ) : if sender is not None and nonce is None : nonce = self . get_nonce ( sender ) new_address = self . calculate_new_address ( sender , nonce ) if sender is None and new_address in self : return self . new_address ( sender , nonce ) return new_address
7577	def _get_clumpp_table ( self , kpop , max_var_multiple , quiet ) : reps , excluded = _concat_reps ( self , kpop , max_var_multiple , quiet ) if reps : ninds = reps [ 0 ] . inds nreps = len ( reps ) else : ninds = nreps = 0 if not reps : return "no result files found" clumphandle = os . path . join ( self . workdir , "tmp.clumppparams.txt" ) self . clumppparams . kpop = kpop self . clumppparams . c = ninds self . clumppparams . r = nreps with open ( clumphandle , 'w' ) as tmp_c : tmp_c . write ( self . clumppparams . _asfile ( ) ) outfile = os . path . join ( self . workdir , "{}-K-{}.outfile" . format ( self . name , kpop ) ) indfile = os . path . join ( self . workdir , "{}-K-{}.indfile" . format ( self . name , kpop ) ) miscfile = os . path . join ( self . workdir , "{}-K-{}.miscfile" . format ( self . name , kpop ) ) cmd = [ "CLUMPP" , clumphandle , "-i" , indfile , "-o" , outfile , "-j" , miscfile , "-r" , str ( nreps ) , "-c" , str ( ninds ) , "-k" , str ( kpop ) ] proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) _ = proc . communicate ( ) for rfile in [ indfile , miscfile ] : if os . path . exists ( rfile ) : os . remove ( rfile ) ofile = os . path . join ( self . workdir , "{}-K-{}.outfile" . format ( self . name , kpop ) ) if os . path . exists ( ofile ) : csvtable = pd . read_csv ( ofile , delim_whitespace = True , header = None ) table = csvtable . loc [ : , 5 : ] table . columns = range ( table . shape [ 1 ] ) table . index = self . labels if not quiet : sys . stderr . write ( "[K{}] {}/{} results permuted across replicates (max_var={}).\n" . format ( kpop , nreps , nreps + excluded , max_var_multiple ) ) return table else : sys . stderr . write ( "No files ready for {}-K-{} in {}\n" . format ( self . name , kpop , self . workdir ) ) return
11567	def stepper_request_library_version ( self ) : data = [ self . STEPPER_LIBRARY_VERSION ] self . _command_handler . send_sysex ( self . _command_handler . STEPPER_DATA , data )
9864	def get_home ( self , home_id ) : if home_id not in self . _all_home_ids : _LOGGER . error ( "Could not find any Tibber home with id: %s" , home_id ) return None if home_id not in self . _homes . keys ( ) : self . _homes [ home_id ] = TibberHome ( home_id , self ) return self . _homes [ home_id ]
4219	def get_preferred_collection ( self ) : bus = secretstorage . dbus_init ( ) try : if hasattr ( self , 'preferred_collection' ) : collection = secretstorage . Collection ( bus , self . preferred_collection ) else : collection = secretstorage . get_default_collection ( bus ) except exceptions . SecretStorageException as e : raise InitError ( "Failed to create the collection: %s." % e ) if collection . is_locked ( ) : collection . unlock ( ) if collection . is_locked ( ) : raise KeyringLocked ( "Failed to unlock the collection!" ) return collection
7044	def lightcurve_flux_measures ( ftimes , fmags , ferrs , magsarefluxes = False ) : ndet = len ( fmags ) if ndet > 9 : if magsarefluxes : series_fluxes = fmags else : series_fluxes = 10.0 ** ( - 0.4 * fmags ) series_flux_median = npmedian ( series_fluxes ) series_flux_percent_amplitude = ( npmax ( npabs ( series_fluxes ) ) / series_flux_median ) series_flux_percentiles = nppercentile ( series_fluxes , [ 5.0 , 10 , 17.5 , 25 , 32.5 , 40 , 60 , 67.5 , 75 , 82.5 , 90 , 95 ] ) series_frat_595 = ( series_flux_percentiles [ - 1 ] - series_flux_percentiles [ 0 ] ) series_frat_1090 = ( series_flux_percentiles [ - 2 ] - series_flux_percentiles [ 1 ] ) series_frat_175825 = ( series_flux_percentiles [ - 3 ] - series_flux_percentiles [ 2 ] ) series_frat_2575 = ( series_flux_percentiles [ - 4 ] - series_flux_percentiles [ 3 ] ) series_frat_325675 = ( series_flux_percentiles [ - 5 ] - series_flux_percentiles [ 4 ] ) series_frat_4060 = ( series_flux_percentiles [ - 6 ] - series_flux_percentiles [ 5 ] ) series_flux_percentile_ratio_mid20 = series_frat_4060 / series_frat_595 series_flux_percentile_ratio_mid35 = series_frat_325675 / series_frat_595 series_flux_percentile_ratio_mid50 = series_frat_2575 / series_frat_595 series_flux_percentile_ratio_mid65 = series_frat_175825 / series_frat_595 series_flux_percentile_ratio_mid80 = series_frat_1090 / series_frat_595 series_percent_difference_flux_percentile = ( series_frat_595 / series_flux_median ) series_percentile_magdiff = - 2.5 * nplog10 ( series_percent_difference_flux_percentile ) return { 'flux_median' : series_flux_median , 'flux_percent_amplitude' : series_flux_percent_amplitude , 'flux_percentiles' : series_flux_percentiles , 'flux_percentile_ratio_mid20' : series_flux_percentile_ratio_mid20 , 'flux_percentile_ratio_mid35' : series_flux_percentile_ratio_mid35 , 'flux_percentile_ratio_mid50' : series_flux_percentile_ratio_mid50 , 'flux_percentile_ratio_mid65' : series_flux_percentile_ratio_mid65 , 'flux_percentile_ratio_mid80' : series_flux_percentile_ratio_mid80 , 'percent_difference_flux_percentile' : series_percentile_magdiff , } else : LOGERROR ( 'not enough detections in this magseries ' 'to calculate flux measures' ) return None
9893	def _uptime_plan9 ( ) : try : f = open ( '/dev/time' , 'r' ) s , ns , ct , cf = f . read ( ) . split ( ) f . close ( ) return float ( ct ) / float ( cf ) except ( IOError , ValueError ) : return None
4351	def join ( self , room ) : self . socket . rooms . add ( self . _get_room_name ( room ) )
6556	def projection ( self , variables ) : variables = set ( variables ) if not variables . issubset ( self . variables ) : raise ValueError ( "Cannot project to variables not in the constraint." ) idxs = [ i for i , v in enumerate ( self . variables ) if v in variables ] configurations = frozenset ( tuple ( config [ i ] for i in idxs ) for config in self . configurations ) variables = tuple ( self . variables [ i ] for i in idxs ) return self . from_configurations ( configurations , variables , self . vartype )
1014	def _getBestMatchingSegment ( self , c , i , activeState ) : maxActivity , which = self . minThreshold , - 1 for j , s in enumerate ( self . cells [ c ] [ i ] ) : activity = self . _getSegmentActivityLevel ( s , activeState , connectedSynapsesOnly = False ) if activity >= maxActivity : maxActivity , which = activity , j if which == - 1 : return None else : return self . cells [ c ] [ i ] [ which ]
9220	def parse_args ( self , args , scope ) : arguments = list ( zip ( args , [ ' ' ] * len ( args ) ) ) if args and args [ 0 ] else None zl = itertools . zip_longest if sys . version_info [ 0 ] == 3 else itertools . izip_longest if self . args : parsed = [ v if hasattr ( v , 'parse' ) else v for v in copy . copy ( self . args ) ] args = args if isinstance ( args , list ) else [ args ] vars = [ self . _parse_arg ( var , arg , scope ) for arg , var in zl ( [ a for a in args ] , parsed ) ] for var in vars : if var : var . parse ( scope ) if not arguments : arguments = [ v . value for v in vars if v ] if not arguments : arguments = '' Variable ( [ '@arguments' , None , arguments ] ) . parse ( scope )
233	def plot_sector_exposures_gross ( gross_exposures , sector_dict = None , ax = None ) : if ax is None : ax = plt . gca ( ) if sector_dict is None : sector_names = SECTORS . values ( ) else : sector_names = sector_dict . values ( ) color_list = plt . cm . gist_rainbow ( np . linspace ( 0 , 1 , 11 ) ) ax . stackplot ( gross_exposures [ 0 ] . index , gross_exposures , labels = sector_names , colors = color_list , alpha = 0.8 , baseline = 'zero' ) ax . axhline ( 0 , color = 'k' , linestyle = '-' ) ax . set ( title = 'Gross exposure to sectors' , ylabel = 'Proportion of gross exposure \n in sectors' ) return ax
3382	def step ( sampler , x , delta , fraction = None , tries = 0 ) : prob = sampler . problem valid = ( ( np . abs ( delta ) > sampler . feasibility_tol ) & np . logical_not ( prob . variable_fixed ) ) valphas = ( ( 1.0 - sampler . bounds_tol ) * prob . variable_bounds - x ) [ : , valid ] valphas = ( valphas / delta [ valid ] ) . flatten ( ) if prob . bounds . shape [ 0 ] > 0 : ineqs = prob . inequalities . dot ( delta ) valid = np . abs ( ineqs ) > sampler . feasibility_tol balphas = ( ( 1.0 - sampler . bounds_tol ) * prob . bounds - prob . inequalities . dot ( x ) ) [ : , valid ] balphas = ( balphas / ineqs [ valid ] ) . flatten ( ) alphas = np . hstack ( [ valphas , balphas ] ) else : alphas = valphas pos_alphas = alphas [ alphas > 0.0 ] neg_alphas = alphas [ alphas <= 0.0 ] alpha_range = np . array ( [ neg_alphas . max ( ) if len ( neg_alphas ) > 0 else 0 , pos_alphas . min ( ) if len ( pos_alphas ) > 0 else 0 ] ) if fraction : alpha = alpha_range [ 0 ] + fraction * ( alpha_range [ 1 ] - alpha_range [ 0 ] ) else : alpha = np . random . uniform ( alpha_range [ 0 ] , alpha_range [ 1 ] ) p = x + alpha * delta if ( np . any ( sampler . _bounds_dist ( p ) < - sampler . bounds_tol ) or np . abs ( np . abs ( alpha_range ) . max ( ) * delta ) . max ( ) < sampler . bounds_tol ) : if tries > MAX_TRIES : raise RuntimeError ( "Can not escape sampling region, model seems" " numerically unstable :( Reporting the " "model to " "https://github.com/opencobra/cobrapy/issues " "will help us to fix this :)" ) LOGGER . info ( "found bounds infeasibility in sample, " "resetting to center" ) newdir = sampler . warmup [ np . random . randint ( sampler . n_warmup ) ] sampler . retries += 1 return step ( sampler , sampler . center , newdir - sampler . center , None , tries + 1 ) return p
4826	def enroll_user_in_course ( self , username , course_id , mode , cohort = None ) : return self . client . enrollment . post ( { 'user' : username , 'course_details' : { 'course_id' : course_id } , 'mode' : mode , 'cohort' : cohort , } )
13219	def shell ( self , expect = pexpect ) : dsn = self . connection_dsn ( ) log . debug ( 'connection string: %s' % dsn ) child = expect . spawn ( 'psql "%s"' % dsn ) if self . _connect_args [ 'password' ] is not None : child . expect ( 'Password: ' ) child . sendline ( self . _connect_args [ 'password' ] ) child . interact ( )
10299	def group_errors ( graph : BELGraph ) -> Mapping [ str , List [ int ] ] : warning_summary = defaultdict ( list ) for _ , exc , _ in graph . warnings : warning_summary [ str ( exc ) ] . append ( exc . line_number ) return dict ( warning_summary )
8163	def _set_mode ( self , mode ) : if mode == CENTER : self . _call_transform_mode = self . _center_transform elif mode == CORNER : self . _call_transform_mode = self . _corner_transform else : raise ValueError ( 'mode must be CENTER or CORNER' )
3214	def get_stats ( self ) : expired = sum ( [ x [ 'expired' ] for _ , x in self . _CACHE_STATS [ 'access_stats' ] . items ( ) ] ) miss = sum ( [ x [ 'miss' ] for _ , x in self . _CACHE_STATS [ 'access_stats' ] . items ( ) ] ) hit = sum ( [ x [ 'hit' ] for _ , x in self . _CACHE_STATS [ 'access_stats' ] . items ( ) ] ) return { 'totals' : { 'keys' : len ( self . _CACHE_STATS [ 'access_stats' ] ) , 'expired' : expired , 'miss' : miss , 'hit' : hit , } }
1899	def can_be_true ( self , constraints , expression ) : if isinstance ( expression , bool ) : if not expression : return expression else : self . _reset ( constraints ) return self . _is_sat ( ) assert isinstance ( expression , Bool ) with constraints as temp_cs : temp_cs . add ( expression ) self . _reset ( temp_cs . to_string ( related_to = expression ) ) return self . _is_sat ( )
4435	async def get_tracks ( self , query ) : log . debug ( 'Requesting tracks for query {}' . format ( query ) ) async with self . http . get ( self . rest_uri + quote ( query ) , headers = { 'Authorization' : self . password } ) as res : return await res . json ( content_type = None )
3387	def _is_redundant ( self , matrix , cutoff = None ) : cutoff = 1.0 - self . feasibility_tol extra_col = matrix [ : , 0 ] + 1 extra_col [ matrix . sum ( axis = 1 ) == 0 ] = 2 corr = np . corrcoef ( np . c_ [ matrix , extra_col ] ) corr = np . tril ( corr , - 1 ) return ( np . abs ( corr ) > cutoff ) . any ( axis = 1 )
5848	def get_preferred_credentials ( api_key , site , cred_file = DEFAULT_CITRINATION_CREDENTIALS_FILE ) : profile_api_key , profile_site = get_credentials_from_file ( cred_file ) if api_key is None : api_key = os . environ . get ( citr_env_vars . CITRINATION_API_KEY ) if api_key is None or len ( api_key ) == 0 : api_key = profile_api_key if site is None : site = os . environ . get ( citr_env_vars . CITRINATION_SITE ) if site is None or len ( site ) == 0 : site = profile_site if site is None : site = "https://citrination.com" return api_key , site
3377	def check_solver_status ( status , raise_error = False ) : if status == OPTIMAL : return elif ( status in has_primals ) and not raise_error : warn ( "solver status is '{}'" . format ( status ) , UserWarning ) elif status is None : raise OptimizationError ( "model was not optimized yet or solver context switched" ) else : raise OptimizationError ( "solver status is '{}'" . format ( status ) )
1763	def pop_bytes ( self , nbytes , force = False ) : data = self . read_bytes ( self . STACK , nbytes , force = force ) self . STACK += nbytes return data
8848	def mousePressEvent ( self , e ) : super ( PyInteractiveConsole , self ) . mousePressEvent ( e ) cursor = self . cursorForPosition ( e . pos ( ) ) p = cursor . positionInBlock ( ) usd = cursor . block ( ) . userData ( ) if usd and usd . start_pos_in_block <= p <= usd . end_pos_in_block : if e . button ( ) == QtCore . Qt . LeftButton : self . open_file_requested . emit ( usd . filename , usd . line )
3514	def chartbeat_top ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ChartbeatTopNode ( )
5046	def _enroll_users ( cls , request , enterprise_customer , emails , mode , course_id = None , program_details = None , notify = True ) : pending_messages = [ ] if course_id : succeeded , pending , failed = cls . enroll_users_in_course ( enterprise_customer = enterprise_customer , course_id = course_id , course_mode = mode , emails = emails , ) all_successes = succeeded + pending if notify : enterprise_customer . notify_enrolled_learners ( catalog_api_user = request . user , course_id = course_id , users = all_successes , ) if succeeded : pending_messages . append ( cls . get_success_enrollment_message ( succeeded , course_id ) ) if failed : pending_messages . append ( cls . get_failed_enrollment_message ( failed , course_id ) ) if pending : pending_messages . append ( cls . get_pending_enrollment_message ( pending , course_id ) ) if program_details : succeeded , pending , failed = cls . enroll_users_in_program ( enterprise_customer = enterprise_customer , program_details = program_details , course_mode = mode , emails = emails , ) all_successes = succeeded + pending if notify : cls . notify_program_learners ( enterprise_customer = enterprise_customer , program_details = program_details , users = all_successes ) program_identifier = program_details . get ( 'title' , program_details . get ( 'uuid' , _ ( 'the program' ) ) ) if succeeded : pending_messages . append ( cls . get_success_enrollment_message ( succeeded , program_identifier ) ) if failed : pending_messages . append ( cls . get_failed_enrollment_message ( failed , program_identifier ) ) if pending : pending_messages . append ( cls . get_pending_enrollment_message ( pending , program_identifier ) ) cls . send_messages ( request , pending_messages )
13457	def upload_s3 ( file_path , bucket_name , file_key , force = False , acl = 'private' ) : file_path = path ( file_path ) bucket = open_s3 ( bucket_name ) if file_path . isdir ( ) : paths = file_path . listdir ( ) paths_keys = list ( zip ( paths , [ '%s/%s' % ( file_key , p . name ) for p in paths ] ) ) else : paths_keys = [ ( file_path , file_key ) ] for p , k in paths_keys : headers = { } s3_key = bucket . get_key ( k ) if not s3_key : from boto . s3 . key import Key s3_key = Key ( bucket , k ) content_type = mimetypes . guess_type ( p ) [ 0 ] if content_type : headers [ 'Content-Type' ] = content_type file_size = p . stat ( ) . st_size file_data = p . bytes ( ) file_md5 , file_md5_64 = s3_key . get_md5_from_hexdigest ( hashlib . md5 ( file_data ) . hexdigest ( ) ) if s3_key . etag : s3_md5 = s3_key . etag . replace ( '"' , '' ) if s3_md5 == file_md5 : info ( 'Hash is the same. Skipping %s' % file_path ) continue elif not force : s3_datetime = datetime . datetime ( * time . strptime ( s3_key . last_modified , '%a, %d %b %Y %H:%M:%S %Z' ) [ 0 : 6 ] ) local_datetime = datetime . datetime . utcfromtimestamp ( p . stat ( ) . st_mtime ) if local_datetime < s3_datetime : info ( "File %s hasn't been modified since last " "being uploaded" % ( file_key ) ) continue info ( "Uploading %s..." % ( file_key ) ) try : s3_key . set_contents_from_string ( file_data , headers , policy = acl , replace = True , md5 = ( file_md5 , file_md5_64 ) ) except Exception as e : error ( "Failed: %s" % e ) raise
6193	def add ( self , num_particles , D ) : self . _plist += self . _generate ( num_particles , D , box = self . box , rs = self . rs )
4781	def is_between ( self , low , high ) : val_type = type ( self . val ) self . _validate_between_args ( val_type , low , high ) if self . val < low or self . val > high : if val_type is datetime . datetime : self . _err ( 'Expected <%s> to be between <%s> and <%s>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , low . strftime ( '%Y-%m-%d %H:%M:%S' ) , high . strftime ( '%Y-%m-%d %H:%M:%S' ) ) ) else : self . _err ( 'Expected <%s> to be between <%s> and <%s>, but was not.' % ( self . val , low , high ) ) return self
2577	def _gather_all_deps ( self , args , kwargs ) : depends = [ ] count = 0 for dep in args : if isinstance ( dep , Future ) : if self . tasks [ dep . tid ] [ 'status' ] not in FINAL_STATES : count += 1 depends . extend ( [ dep ] ) for key in kwargs : dep = kwargs [ key ] if isinstance ( dep , Future ) : if self . tasks [ dep . tid ] [ 'status' ] not in FINAL_STATES : count += 1 depends . extend ( [ dep ] ) for dep in kwargs . get ( 'inputs' , [ ] ) : if isinstance ( dep , Future ) : if self . tasks [ dep . tid ] [ 'status' ] not in FINAL_STATES : count += 1 depends . extend ( [ dep ] ) return count , depends
1239	def move ( self , external_index , new_priority ) : index = external_index + ( self . _capacity - 1 ) return self . _move ( index , new_priority )
7711	def _get_success ( self , stanza ) : payload = stanza . get_payload ( RosterPayload ) if payload is None : if "versioning" in self . server_features and self . roster : logger . debug ( "Server will send roster delta in pushes" ) else : logger . warning ( "Bad roster response (no payload)" ) self . _event_queue . put ( RosterNotReceivedEvent ( self , stanza ) ) return else : items = list ( payload ) for item in items : item . verify_roster_result ( True ) self . roster = Roster ( items , payload . version ) self . _event_queue . put ( RosterReceivedEvent ( self , self . roster ) )
11821	def create ( self , name , value ) : if value is None : raise ValueError ( 'Setting value cannot be `None`.' ) model = Setting . get_model_for_value ( value ) obj = super ( SettingQuerySet , model . objects . all ( ) ) . create ( name = name , value = value ) return obj
5449	def validate_param_name ( name , param_type ) : if not re . match ( r'^[a-zA-Z_][a-zA-Z0-9_]*$' , name ) : raise ValueError ( 'Invalid %s: %s' % ( param_type , name ) )
4919	def course_run_detail ( self , request , pk , course_id ) : enterprise_customer_catalog = self . get_object ( ) course_run = enterprise_customer_catalog . get_course_run ( course_id ) if not course_run : raise Http404 context = self . get_serializer_context ( ) context [ 'enterprise_customer_catalog' ] = enterprise_customer_catalog serializer = serializers . CourseRunDetailSerializer ( course_run , context = context ) return Response ( serializer . data )
12223	def execute ( self , args , kwargs ) : return self . lookup_explicit ( args , kwargs ) ( * args , ** kwargs )
4083	def check ( self , text : str , srctext = None ) -> [ Match ] : root = self . _get_root ( self . _url , self . _encode ( text , srctext ) ) return [ Match ( e . attrib ) for e in root if e . tag == 'error' ]
11144	def to_repo_relative_path ( self , path , split = False ) : path = os . path . normpath ( path ) if path == '.' : path = '' path = path . split ( self . __path ) [ - 1 ] . strip ( os . sep ) if split : return path . split ( os . sep ) else : return path
5901	def mpicommand ( self , * args , ** kwargs ) : if self . mpiexec is None : raise NotImplementedError ( "Override mpiexec to enable the simple OpenMP launcher" ) ncores = kwargs . pop ( 'ncores' , 8 ) return [ self . mpiexec , '-n' , str ( ncores ) ]
3139	def create ( self , store_id , data ) : self . store_id = store_id if 'id' not in data : raise KeyError ( 'The promo rule must have an id' ) if 'description' not in data : raise KeyError ( 'This promo rule must have a description' ) if 'amount' not in data : raise KeyError ( 'This promo rule must have an amount' ) if 'target' not in data : raise KeyError ( 'This promo rule must apply to a target (example per_item, total, or shipping' ) response = self . _mc_client . _post ( url = self . _build_path ( store_id , 'promo-rules' ) , data = data ) if response is not None : return response
9441	def reload_config ( self , call_params ) : path = '/' + self . api_version + '/ReloadConfig/' method = 'POST' return self . request ( path , method , call_params )
11017	def signed_number ( number , precision = 2 ) : prefix = '' if number <= 0 else '+' number_str = '{}{:.{precision}f}' . format ( prefix , number , precision = precision ) return number_str
1576	def make_shell_endpoint ( topologyInfo , instance_id ) : pplan = topologyInfo [ "physical_plan" ] stmgrId = pplan [ "instances" ] [ instance_id ] [ "stmgrId" ] host = pplan [ "stmgrs" ] [ stmgrId ] [ "host" ] shell_port = pplan [ "stmgrs" ] [ stmgrId ] [ "shell_port" ] return "http://%s:%d" % ( host , shell_port )
10102	def _make_file_dict ( self , f ) : if isinstance ( f , dict ) : file_obj = f [ 'file' ] if 'filename' in f : file_name = f [ 'filename' ] else : file_name = file_obj . name else : file_obj = f file_name = f . name b64_data = base64 . b64encode ( file_obj . read ( ) ) return { 'id' : file_name , 'data' : b64_data . decode ( ) if six . PY3 else b64_data , }
11583	def image_urls ( self ) : all_image_urls = self . finder_image_urls [ : ] for image_url in self . extender_image_urls : if image_url not in all_image_urls : all_image_urls . append ( image_url ) return all_image_urls
12772	def follow_markers ( self , start = 0 , end = 1e100 , states = None ) : if states is not None : self . skeleton . set_body_states ( states ) for frame_no , frame in enumerate ( self . markers ) : if frame_no < start : continue if frame_no >= end : break for states in self . _step_to_marker_frame ( frame_no ) : yield states
6812	def deploy ( self ) : for service in self . genv . services : service = service . strip ( ) . upper ( ) funcs = common . service_deployers . get ( service ) if funcs : print ( 'Deploying service %s...' % ( service , ) ) for func in funcs : if not self . dryrun : func ( )
12082	def clampfit_rename ( path , char ) : assert len ( char ) == 1 and type ( char ) == str , "replacement character must be a single character" assert os . path . exists ( path ) , "path doesn't exist" files = sorted ( os . listdir ( path ) ) files = [ x for x in files if len ( x ) > 18 and x [ 4 ] + x [ 7 ] + x [ 10 ] == ' ' ] for fname in files : fname2 = list ( fname ) fname2 [ 11 ] = char fname2 = "" . join ( fname2 ) if fname == fname2 : print ( fname , "==" , fname2 ) else : print ( fname , "->" , fname2 ) return
4641	def reset_counter ( self ) : self . _cnt_retries = 0 for i in self . _url_counter : self . _url_counter [ i ] = 0
12812	def rawDataReceived ( self , data ) : if self . _len_expected is not None : data , extra = data [ : self . _len_expected ] , data [ self . _len_expected : ] self . _len_expected -= len ( data ) else : extra = "" self . _buffer += data if self . _len_expected == 0 : data = self . _buffer . strip ( ) if data : lines = data . split ( "\r" ) for line in lines : try : message = self . factory . get_stream ( ) . get_connection ( ) . parse ( line ) if message : self . factory . get_stream ( ) . received ( [ message ] ) except ValueError : pass self . _buffer = "" self . _len_expected = None self . setLineMode ( extra )
3391	def prune_unused_metabolites ( cobra_model ) : output_model = cobra_model . copy ( ) inactive_metabolites = [ m for m in output_model . metabolites if len ( m . reactions ) == 0 ] output_model . remove_metabolites ( inactive_metabolites ) return output_model , inactive_metabolites
8062	def do_help ( self , arg ) : print ( self . response_prompt , file = self . stdout ) return cmd . Cmd . do_help ( self , arg )
4577	def get_server ( self , key , ** kwds ) : kwds = dict ( self . kwds , ** kwds ) server = self . servers . get ( key ) if server : server . check_keywords ( self . constructor , kwds ) else : server = _CachedServer ( self . constructor , key , kwds ) self . servers [ key ] = server return server
4086	def _process_events ( self , events ) : for f , callback , transferred , key , ov in events : try : self . _logger . debug ( 'Invoking event callback {}' . format ( callback ) ) value = callback ( transferred , key , ov ) except OSError : self . _logger . warning ( 'Event callback failed' , exc_info = sys . exc_info ( ) ) else : f . set_result ( value )
4132	def split_code_and_text_blocks ( source_file ) : docstring , rest_of_content = get_docstring_and_rest ( source_file ) blocks = [ ( 'text' , docstring ) ] pattern = re . compile ( r'(?P<header_line>^#{20,}.*)\s(?P<text_content>(?:^#.*\s)*)' , flags = re . M ) pos_so_far = 0 for match in re . finditer ( pattern , rest_of_content ) : match_start_pos , match_end_pos = match . span ( ) code_block_content = rest_of_content [ pos_so_far : match_start_pos ] text_content = match . group ( 'text_content' ) sub_pat = re . compile ( '^#' , flags = re . M ) text_block_content = dedent ( re . sub ( sub_pat , '' , text_content ) ) if code_block_content . strip ( ) : blocks . append ( ( 'code' , code_block_content ) ) if text_block_content . strip ( ) : blocks . append ( ( 'text' , text_block_content ) ) pos_so_far = match_end_pos remaining_content = rest_of_content [ pos_so_far : ] if remaining_content . strip ( ) : blocks . append ( ( 'code' , remaining_content ) ) return blocks
12275	def iso_reference_isvalid ( ref ) : ref = str ( ref ) cs_source = ref [ 4 : ] + ref [ : 4 ] return ( iso_reference_str2int ( cs_source ) % 97 ) == 1
6476	def set_text ( self , point , text ) : if not self . option . legend : return if not isinstance ( point , Point ) : point = Point ( point ) for offset , char in enumerate ( str ( text ) ) : self . screen . canvas [ point . y ] [ point . x + offset ] = char
9611	def _request ( self , method , url , body ) : if method != 'POST' and method != 'PUT' : body = None s = Session ( ) LOGGER . debug ( 'Method: {0}, Url: {1}, Body: {2}.' . format ( method , url , body ) ) req = Request ( method , url , json = body ) prepped = s . prepare_request ( req ) res = s . send ( prepped , timeout = self . _timeout or None ) res . raise_for_status ( ) return res . json ( )
7825	def feature_uri ( uri ) : def decorator ( class_ ) : if "_pyxmpp_feature_uris" not in class_ . __dict__ : class_ . _pyxmpp_feature_uris = set ( ) class_ . _pyxmpp_feature_uris . add ( uri ) return class_ return decorator
11743	def initial_closure ( self ) : first_rule = DottedRule ( self . start , 0 , END_OF_INPUT ) return self . closure ( [ first_rule ] )
10235	def reaction_cartesian_expansion ( graph : BELGraph , accept_unqualified_edges : bool = True ) -> None : for u , v , d in list ( graph . edges ( data = True ) ) : if CITATION not in d and accept_unqualified_edges : _reaction_cartesion_expansion_unqualified_helper ( graph , u , v , d ) continue if isinstance ( u , Reaction ) and isinstance ( v , Reaction ) : catalysts = _get_catalysts_in_reaction ( u ) | _get_catalysts_in_reaction ( v ) for reactant , product in chain ( itt . product ( u . reactants , u . products ) , itt . product ( v . reactants , v . products ) ) : if reactant in catalysts or product in catalysts : continue graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for product , reactant in itt . product ( u . products , u . reactants ) : if reactant in catalysts or product in catalysts : continue graph . add_qualified_edge ( product , reactant , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( u , Reaction ) : catalysts = _get_catalysts_in_reaction ( u ) for product in u . products : if product in catalysts : continue if v not in u . products and v not in u . reactants : graph . add_increases ( product , v , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for reactant in u . reactants : graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( v , Reaction ) : for reactant in v . reactants : catalysts = _get_catalysts_in_reaction ( v ) if reactant in catalysts : continue if u not in v . products and u not in v . reactants : graph . add_increases ( u , reactant , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for product in v . products : graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) _remove_reaction_nodes ( graph )
5171	def auto_client ( cls , host , server , ca_path = None , ca_contents = None , cert_path = None , cert_contents = None , key_path = None , key_contents = None ) : client = { "mode" : "p2p" , "nobind" : True , "resolv_retry" : "infinite" , "tls_client" : True } port = server . get ( 'port' ) or 1195 client [ 'remote' ] = [ { 'host' : host , 'port' : port } ] if server . get ( 'proto' ) == 'tcp-server' : client [ 'proto' ] = 'tcp-client' else : client [ 'proto' ] = 'udp' if 'server' in server or 'server_bridge' in server : client [ 'pull' ] = True if 'tls_server' not in server or not server [ 'tls_server' ] : client [ 'tls_client' ] = False ns_cert_type = { None : '' , '' : '' , 'client' : 'server' } client [ 'ns_cert_type' ] = ns_cert_type [ server . get ( 'ns_cert_type' ) ] remote_cert_tls = { None : '' , '' : '' , 'client' : 'server' } client [ 'remote_cert_tls' ] = remote_cert_tls [ server . get ( 'remote_cert_tls' ) ] copy_keys = [ 'name' , 'dev_type' , 'dev' , 'comp_lzo' , 'auth' , 'cipher' , 'ca' , 'cert' , 'key' , 'pkcs12' , 'mtu_disc' , 'mtu_test' , 'fragment' , 'mssfix' , 'keepalive' , 'persist_tun' , 'mute' , 'persist_key' , 'script_security' , 'user' , 'group' , 'log' , 'mute_replay_warnings' , 'secret' , 'reneg_sec' , 'tls_timeout' , 'tls_cipher' , 'float' , 'fast_io' , 'verb' ] for key in copy_keys : if key in server : client [ key ] = server [ key ] files = cls . _auto_client_files ( client , ca_path , ca_contents , cert_path , cert_contents , key_path , key_contents ) return { 'openvpn' : [ client ] , 'files' : files }
9628	def detail_view ( self , request ) : context = { 'preview' : self , } kwargs = { } if self . form_class : if request . GET : form = self . form_class ( data = request . GET ) else : form = self . form_class ( ) context [ 'form' ] = form if not form . is_bound or not form . is_valid ( ) : return render ( request , 'mailviews/previews/detail.html' , context ) kwargs . update ( form . get_message_view_kwargs ( ) ) message_view = self . get_message_view ( request , ** kwargs ) message = message_view . render_to_message ( ) raw = message . message ( ) headers = OrderedDict ( ( header , maybe_decode_header ( raw [ header ] ) ) for header in self . headers ) context . update ( { 'message' : message , 'subject' : message . subject , 'body' : message . body , 'headers' : headers , 'raw' : raw . as_string ( ) , } ) alternatives = getattr ( message , 'alternatives' , [ ] ) try : html = next ( alternative [ 0 ] for alternative in alternatives if alternative [ 1 ] == 'text/html' ) context . update ( { 'html' : html , 'escaped_html' : b64encode ( html . encode ( 'utf-8' ) ) , } ) except StopIteration : pass return render ( request , self . template_name , context )
11908	def to_bipartite_matrix ( A ) : m , n = A . shape return four_blocks ( zeros ( m , m ) , A , A . T , zeros ( n , n ) )
9743	async def reboot ( ip_address ) : _ , protocol = await asyncio . get_event_loop ( ) . create_datagram_endpoint ( QRebootProtocol , local_addr = ( ip_address , 0 ) , allow_broadcast = True , reuse_address = True , ) LOG . info ( "Sending reboot on %s" , ip_address ) protocol . send_reboot ( )
4708	def power_off ( self , interval = 200 ) : if self . __power_off_port is None : cij . err ( "cij.usb.relay: Invalid USB_RELAY_POWER_OFF" ) return 1 return self . __press ( self . __power_off_port , interval = interval )
6792	def manage ( self , cmd , * args , ** kwargs ) : r = self . local_renderer environs = kwargs . pop ( 'environs' , '' ) . strip ( ) if environs : environs = ' ' . join ( 'export %s=%s;' % tuple ( _ . split ( '=' ) ) for _ in environs . split ( ',' ) ) environs = ' ' + environs + ' ' r . env . cmd = cmd r . env . SITE = r . genv . SITE or r . genv . default_site r . env . args = ' ' . join ( map ( str , args ) ) r . env . kwargs = ' ' . join ( ( '--%s' % _k if _v in ( True , 'True' ) else '--%s=%s' % ( _k , _v ) ) for _k , _v in kwargs . items ( ) ) r . env . environs = environs if self . is_local : r . env . project_dir = r . env . local_project_dir r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE};{environs} cd {project_dir}; {manage_cmd} {cmd} {args} {kwargs}' )
1440	def register_metrics ( self , context ) : sys_config = system_config . get_sys_config ( ) interval = float ( sys_config [ constants . HERON_METRICS_EXPORT_INTERVAL_SEC ] ) collector = context . get_metrics_collector ( ) super ( ComponentMetrics , self ) . register_metrics ( collector , interval )
11905	def snoise2d ( size , z = 0.0 , scale = 0.05 , octaves = 1 , persistence = 0.25 , lacunarity = 2.0 ) : import noise data = np . empty ( size , dtype = 'float32' ) for y in range ( size [ 0 ] ) : for x in range ( size [ 1 ] ) : v = noise . snoise3 ( x * scale , y * scale , z , octaves = octaves , persistence = persistence , lacunarity = lacunarity ) data [ x , y ] = v data = data * 0.5 + 0.5 if __debug__ : assert data . min ( ) >= 0. and data . max ( ) <= 1.0 return data
12547	def abs_img ( img ) : bool_img = np . abs ( read_img ( img ) . get_data ( ) ) return bool_img . astype ( int )
10574	def get_local_playlists ( filepaths , exclude_patterns = None , max_depth = float ( 'inf' ) ) : logger . info ( "Loading local playlists..." ) included_playlists = [ ] excluded_playlists = [ ] supported_filepaths = get_supported_filepaths ( filepaths , SUPPORTED_PLAYLIST_FORMATS , max_depth = max_depth ) included_playlists , excluded_playlists = exclude_filepaths ( supported_filepaths , exclude_patterns = exclude_patterns ) logger . info ( "Excluded {0} local playlists" . format ( len ( excluded_playlists ) ) ) logger . info ( "Loaded {0} local playlists" . format ( len ( included_playlists ) ) ) return included_playlists , excluded_playlists
12757	def add_torques ( self , torques ) : j = 0 for joint in self . joints : joint . add_torques ( list ( torques [ j : j + joint . ADOF ] ) + [ 0 ] * ( 3 - joint . ADOF ) ) j += joint . ADOF
8901	def authenticate_credentials ( self , userargs , password , request = None ) : credentials = { 'password' : password } if "=" not in userargs : credentials [ get_user_model ( ) . USERNAME_FIELD ] = userargs else : for arg in userargs . split ( "&" ) : key , val = arg . split ( "=" ) credentials [ key ] = val user = authenticate ( ** credentials ) if user is None : raise exceptions . AuthenticationFailed ( 'Invalid credentials.' ) if not user . is_active : raise exceptions . AuthenticationFailed ( 'User inactive or deleted.' ) return ( user , None )
7621	def hierarchy ( ref , est , ** kwargs ) : r namespace = 'multi_segment' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_hier , ref_hier_lab = hierarchy_flatten ( ref ) est_hier , est_hier_lab = hierarchy_flatten ( est ) return mir_eval . hierarchy . evaluate ( ref_hier , ref_hier_lab , est_hier , est_hier_lab , ** kwargs )
3660	def _coeff_ind_from_T ( self , T ) : if self . n == 1 : return 0 for i in range ( self . n ) : if T <= self . Ts [ i + 1 ] : return i return self . n - 1
8582	def get_attached_volumes ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/volumes?depth=%s' % ( datacenter_id , server_id , str ( depth ) ) ) return response
8594	def get_group ( self , group_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s?depth=%s' % ( group_id , str ( depth ) ) ) return response
4515	def drawLine ( self , x0 , y0 , x1 , y1 , color = None , colorFunc = None , aa = False ) : md . draw_line ( self . set , x0 , y0 , x1 , y1 , color , colorFunc , aa )
6878	def _validate_sqlitecurve_filters ( filterstring , lccolumns ) : stringelems = _squeeze ( filterstring ) . lower ( ) stringelems = filterstring . replace ( '(' , '' ) stringelems = stringelems . replace ( ')' , '' ) stringelems = stringelems . replace ( ',' , '' ) stringelems = stringelems . replace ( "'" , '"' ) stringelems = stringelems . replace ( '\n' , ' ' ) stringelems = stringelems . replace ( '\t' , ' ' ) stringelems = _squeeze ( stringelems ) stringelems = stringelems . split ( ' ' ) stringelems = [ x . strip ( ) for x in stringelems ] stringwords = [ ] for x in stringelems : try : float ( x ) except ValueError as e : stringwords . append ( x ) stringwords2 = [ ] for x in stringwords : if not ( x . startswith ( '"' ) and x . endswith ( '"' ) ) : stringwords2 . append ( x ) stringwords2 = [ x for x in stringwords2 if len ( x ) > 0 ] wordset = set ( stringwords2 ) allowedwords = SQLITE_ALLOWED_WORDS + lccolumns checkset = set ( allowedwords ) validatecheck = list ( wordset - checkset ) if len ( validatecheck ) > 0 : LOGWARNING ( "provided SQL filter string '%s' " "contains non-allowed keywords" % filterstring ) return None else : return filterstring
4928	def transform_image ( self , content_metadata_item ) : image_url = '' if content_metadata_item [ 'content_type' ] in [ 'course' , 'program' ] : image_url = content_metadata_item . get ( 'card_image_url' ) elif content_metadata_item [ 'content_type' ] == 'courserun' : image_url = content_metadata_item . get ( 'image_url' ) return image_url
3326	def acquire ( self , url , lock_type , lock_scope , lock_depth , lock_owner , timeout , principal , token_list , ) : url = normalize_lock_root ( url ) self . _lock . acquire_write ( ) try : self . _check_lock_permission ( url , lock_type , lock_scope , lock_depth , token_list , principal ) return self . _generate_lock ( principal , lock_type , lock_scope , lock_depth , lock_owner , url , timeout ) finally : self . _lock . release ( )
2730	def get_object ( cls , api_token , domain_name ) : domain = cls ( token = api_token , name = domain_name ) domain . load ( ) return domain
4189	def window_poisson ( N , alpha = 2 ) : r n = linspace ( - N / 2. , ( N ) / 2. , N ) w = exp ( - alpha * abs ( n ) / ( N / 2. ) ) return w
470	def read_words ( filename = "nietzsche.txt" , replace = None ) : if replace is None : replace = [ '\n' , '<eos>' ] with tf . gfile . GFile ( filename , "r" ) as f : try : context_list = f . read ( ) . replace ( * replace ) . split ( ) except Exception : f . seek ( 0 ) replace = [ x . encode ( 'utf-8' ) for x in replace ] context_list = f . read ( ) . replace ( * replace ) . split ( ) return context_list
9475	def add_node ( self , label ) : try : n = self . _nodes [ label ] except KeyError : n = Node ( ) n [ 'label' ] = label self . _nodes [ label ] = n return n
8059	def do_vars ( self , line ) : if self . bot . _vars : max_name_len = max ( [ len ( name ) for name in self . bot . _vars ] ) for i , ( name , v ) in enumerate ( self . bot . _vars . items ( ) ) : keep = i < len ( self . bot . _vars ) - 1 self . print_response ( "%s = %s" % ( name . ljust ( max_name_len ) , v . value ) , keep = keep ) else : self . print_response ( "No vars" )
2236	def timestamp ( method = 'iso8601' ) : if method == 'iso8601' : tz_hour = time . timezone // 3600 utc_offset = str ( tz_hour ) if tz_hour < 0 else '+' + str ( tz_hour ) stamp = time . strftime ( '%Y-%m-%dT%H%M%S' ) + utc_offset return stamp else : raise ValueError ( 'only iso8601 is accepted for now' )
10835	def all ( self ) : response = self . api . get ( url = PATHS [ 'GET_PROFILES' ] ) for raw_profile in response : self . append ( Profile ( self . api , raw_profile ) ) return self
6793	def load_django_settings ( self ) : r = self . local_renderer _env = { } save_vars = [ 'ALLOW_CELERY' , 'DJANGO_SETTINGS_MODULE' ] for var_name in save_vars : _env [ var_name ] = os . environ . get ( var_name ) try : if r . env . local_project_dir : sys . path . insert ( 0 , r . env . local_project_dir ) os . environ [ 'ALLOW_CELERY' ] = '0' os . environ [ 'DJANGO_SETTINGS_MODULE' ] = r . format ( r . env . settings_module ) try : import django django . setup ( ) except AttributeError : pass settings = self . get_settings ( ) try : from django . contrib import staticfiles from django . conf import settings as _settings if settings is not None : for k , v in settings . __dict__ . items ( ) : setattr ( _settings , k , v ) else : raise ImportError except ( ImportError , RuntimeError ) : print ( 'Unable to load settings.' ) traceback . print_exc ( ) finally : for var_name , var_value in _env . items ( ) : if var_value is None : del os . environ [ var_name ] else : os . environ [ var_name ] = var_value return settings
13852	def age ( self ) : if self . rounds == 1 : self . do_run = False elif self . rounds > 1 : self . rounds -= 1
9120	def dropbox_fileupload ( dropbox , request ) : attachment = request . POST [ 'attachment' ] attached = dropbox . add_attachment ( attachment ) return dict ( files = [ dict ( name = attached , type = attachment . type , ) ] )
10930	def find_LM_updates ( self , grad , do_correct_damping = True , subblock = None ) : if subblock is not None : if ( subblock . sum ( ) == 0 ) or ( subblock . size == 0 ) : CLOG . fatal ( 'Empty subblock in find_LM_updates' ) raise ValueError ( 'Empty sub-block' ) j = self . J [ subblock ] JTJ = np . dot ( j , j . T ) damped_JTJ = self . _calc_damped_jtj ( JTJ , subblock = subblock ) grad = grad [ subblock ] else : damped_JTJ = self . _calc_damped_jtj ( self . JTJ , subblock = subblock ) delta = self . _calc_lm_step ( damped_JTJ , grad , subblock = subblock ) if self . use_accel : accel_correction = self . calc_accel_correction ( damped_JTJ , delta ) nrm_d0 = np . sqrt ( np . sum ( delta ** 2 ) ) nrm_corr = np . sqrt ( np . sum ( accel_correction ** 2 ) ) CLOG . debug ( '|correction| / |LM step|\t%e' % ( nrm_corr / nrm_d0 ) ) if nrm_corr / nrm_d0 < self . max_accel_correction : delta += accel_correction elif do_correct_damping : CLOG . debug ( 'Untrustworthy step! Increasing damping...' ) self . increase_damping ( ) damped_JTJ = self . _calc_damped_jtj ( self . JTJ , subblock = subblock ) delta = self . _calc_lm_step ( damped_JTJ , grad , subblock = subblock ) if np . any ( np . isnan ( delta ) ) : CLOG . fatal ( 'Calculated steps have nans!?' ) raise FloatingPointError ( 'Calculated steps have nans!?' ) return delta
4798	def exists ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a path' ) if not os . path . exists ( self . val ) : self . _err ( 'Expected <%s> to exist, but was not found.' % self . val ) return self
7658	def append_records ( self , records ) : for obs in records : if isinstance ( obs , Observation ) : self . append ( ** obs . _asdict ( ) ) else : self . append ( ** obs )
13127	def get_pipe ( self ) : lines = [ ] for line in sys . stdin : try : lines . append ( self . line_to_object ( line . strip ( ) ) ) except ValueError : pass except KeyError : pass return lines
7766	def _close_stream ( self ) : self . stream . close ( ) if self . stream . transport in self . _ml_handlers : self . _ml_handlers . remove ( self . stream . transport ) self . main_loop . remove_handler ( self . stream . transport ) self . stream = None self . uplink = None
7530	def build_dag ( data , samples ) : snames = [ i . name for i in samples ] dag = nx . DiGraph ( ) joborder = JOBORDER [ data . paramsdict [ "assembly_method" ] ] for sname in snames : for func in joborder : dag . add_node ( "{}-{}-{}" . format ( func , 0 , sname ) ) for chunk in xrange ( 10 ) : dag . add_node ( "{}-{}-{}" . format ( "muscle_align" , chunk , sname ) ) dag . add_node ( "{}-{}-{}" . format ( "reconcat" , 0 , sname ) ) for sname in snames : for sname2 in snames : dag . add_edge ( "{}-{}-{}" . format ( joborder [ 0 ] , 0 , sname2 ) , "{}-{}-{}" . format ( joborder [ 1 ] , 0 , sname ) ) for idx in xrange ( 2 , len ( joborder ) ) : dag . add_edge ( "{}-{}-{}" . format ( joborder [ idx - 1 ] , 0 , sname ) , "{}-{}-{}" . format ( joborder [ idx ] , 0 , sname ) ) for sname2 in snames : for chunk in range ( 10 ) : dag . add_edge ( "{}-{}-{}" . format ( "muscle_chunker" , 0 , sname2 ) , "{}-{}-{}" . format ( "muscle_align" , chunk , sname ) ) dag . add_edge ( "{}-{}-{}" . format ( "muscle_align" , chunk , sname ) , "{}-{}-{}" . format ( "reconcat" , 0 , sname ) ) return dag , joborder
9769	def delete ( ctx ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) if not click . confirm ( "Are sure you want to delete job `{}`" . format ( _job ) ) : click . echo ( 'Existing without deleting job.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . job . delete_job ( user , project_name , _job ) JobManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Job `{}` was delete successfully" . format ( _job ) )
7797	def _register_server_authenticator ( klass , name ) : SERVER_MECHANISMS_D [ name ] = klass items = sorted ( SERVER_MECHANISMS_D . items ( ) , key = _key_func , reverse = True ) SERVER_MECHANISMS [ : ] = [ k for ( k , v ) in items ] SECURE_SERVER_MECHANISMS [ : ] = [ k for ( k , v ) in items if v . _pyxmpp_sasl_secure ]
7039	def object_info ( lcc_server , objectid , db_collection_id ) : urlparams = { 'objectid' : objectid , 'collection' : db_collection_id } urlqs = urlencode ( urlparams ) url = '%s/api/object?%s' % ( lcc_server , urlqs ) try : LOGINFO ( 'getting info for %s in collection %s from %s' % ( objectid , db_collection_id , lcc_server ) ) have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } req = Request ( url , data = None , headers = headers ) resp = urlopen ( req ) objectinfo = json . loads ( resp . read ( ) ) [ 'result' ] return objectinfo except HTTPError as e : if e . code == 404 : LOGERROR ( 'additional info for object %s not ' 'found in collection: %s' % ( objectid , db_collection_id ) ) else : LOGERROR ( 'could not retrieve object info, ' 'URL used: %s, error code: %s, reason: %s' % ( url , e . code , e . reason ) ) return None
8602	def get_user ( self , user_id , depth = 1 ) : response = self . _perform_request ( '/um/users/%s?depth=%s' % ( user_id , str ( depth ) ) ) return response
12316	def init ( self , username , reponame , force , backend = None ) : key = self . key ( username , reponame ) server_repodir = self . server_rootdir ( username , reponame , create = False ) if os . path . exists ( server_repodir ) and not force : raise RepositoryExists ( ) if os . path . exists ( server_repodir ) : shutil . rmtree ( server_repodir ) os . makedirs ( server_repodir ) with cd ( server_repodir ) : git . init ( "." , "--bare" ) if backend is not None : backend . init_repo ( server_repodir ) repodir = self . rootdir ( username , reponame , create = False ) if os . path . exists ( repodir ) and not force : raise Exception ( "Local repo already exists" ) if os . path . exists ( repodir ) : shutil . rmtree ( repodir ) os . makedirs ( repodir ) with cd ( os . path . dirname ( repodir ) ) : git . clone ( server_repodir , '--no-hardlinks' ) url = server_repodir if backend is not None : url = backend . url ( username , reponame ) repo = Repo ( username , reponame ) repo . manager = self repo . remoteurl = url repo . rootdir = self . rootdir ( username , reponame ) self . add ( repo ) return repo
8460	def write_temple_config ( temple_config , template , version ) : with open ( temple . constants . TEMPLE_CONFIG_FILE , 'w' ) as temple_config_file : versioned_config = { ** temple_config , ** { '_version' : version , '_template' : template } , } yaml . dump ( versioned_config , temple_config_file , Dumper = yaml . SafeDumper )
6063	def mass_within_ellipse_in_units ( self , major_axis , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : self . check_units_of_radius_and_critical_surface_density ( radius = major_axis , critical_surface_density = critical_surface_density ) profile = self . new_profile_with_units_converted ( unit_length = major_axis . unit_length , unit_mass = 'angular' , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) mass_angular = dim . Mass ( value = quad ( profile . mass_integral , a = 0.0 , b = major_axis , args = ( self . axis_ratio , ) ) [ 0 ] , unit_mass = 'angular' ) return mass_angular . convert ( unit_mass = unit_mass , critical_surface_density = critical_surface_density )
6363	def dist ( self , src , tar ) : if src == tar : return 0.0 src_comp = self . _rle . encode ( self . _bwt . encode ( src ) ) tar_comp = self . _rle . encode ( self . _bwt . encode ( tar ) ) concat_comp = self . _rle . encode ( self . _bwt . encode ( src + tar ) ) concat_comp2 = self . _rle . encode ( self . _bwt . encode ( tar + src ) ) return ( min ( len ( concat_comp ) , len ( concat_comp2 ) ) - min ( len ( src_comp ) , len ( tar_comp ) ) ) / max ( len ( src_comp ) , len ( tar_comp ) )
10730	def signature ( dbus_object , unpack = False ) : if dbus_object . variant_level != 0 and not unpack : return 'v' if isinstance ( dbus_object , dbus . Array ) : sigs = frozenset ( signature ( x ) for x in dbus_object ) len_sigs = len ( sigs ) if len_sigs > 1 : raise IntoDPValueError ( dbus_object , "dbus_object" , "has bad signature" ) if len_sigs == 0 : return 'a' + dbus_object . signature return 'a' + [ x for x in sigs ] [ 0 ] if isinstance ( dbus_object , dbus . Struct ) : sigs = ( signature ( x ) for x in dbus_object ) return '(' + "" . join ( x for x in sigs ) + ')' if isinstance ( dbus_object , dbus . Dictionary ) : key_sigs = frozenset ( signature ( x ) for x in dbus_object . keys ( ) ) value_sigs = frozenset ( signature ( x ) for x in dbus_object . values ( ) ) len_key_sigs = len ( key_sigs ) len_value_sigs = len ( value_sigs ) if len_key_sigs != len_value_sigs : raise IntoDPValueError ( dbus_object , "dbus_object" , "has bad signature" ) if len_key_sigs > 1 : raise IntoDPValueError ( dbus_object , "dbus_object" , "has bad signature" ) if len_key_sigs == 0 : return 'a{' + dbus_object . signature + '}' return 'a{' + [ x for x in key_sigs ] [ 0 ] + [ x for x in value_sigs ] [ 0 ] + '}' if isinstance ( dbus_object , dbus . Boolean ) : return 'b' if isinstance ( dbus_object , dbus . Byte ) : return 'y' if isinstance ( dbus_object , dbus . Double ) : return 'd' if isinstance ( dbus_object , dbus . Int16 ) : return 'n' if isinstance ( dbus_object , dbus . Int32 ) : return 'i' if isinstance ( dbus_object , dbus . Int64 ) : return 'x' if isinstance ( dbus_object , dbus . ObjectPath ) : return 'o' if isinstance ( dbus_object , dbus . Signature ) : return 'g' if isinstance ( dbus_object , dbus . String ) : return 's' if isinstance ( dbus_object , dbus . UInt16 ) : return 'q' if isinstance ( dbus_object , dbus . UInt32 ) : return 'u' if isinstance ( dbus_object , dbus . UInt64 ) : return 't' if isinstance ( dbus_object , dbus . types . UnixFd ) : return 'h' raise IntoDPValueError ( dbus_object , "dbus_object" , "has no signature" )
12527	def clean ( ctx ) : ctx . run ( f'python setup.py clean' ) dist = ROOT . joinpath ( 'dist' ) print ( f'removing {dist}' ) shutil . rmtree ( str ( dist ) )
1241	def _next_position_then_increment ( self ) : start = self . _capacity - 1 position = start + self . _position self . _position = ( self . _position + 1 ) % self . _capacity return position
6970	def _old_epd_magseries ( times , mags , errs , fsv , fdv , fkv , xcc , ycc , bgv , bge , epdsmooth_windowsize = 21 , epdsmooth_sigclip = 3.0 , epdsmooth_func = smooth_magseries_signal_medfilt , epdsmooth_extraparams = None ) : finiteind = np . isfinite ( mags ) mags_median = np . median ( mags [ finiteind ] ) mags_stdev = np . nanstd ( mags ) if epdsmooth_sigclip : excludeind = abs ( mags - mags_median ) < epdsmooth_sigclip * mags_stdev finalind = finiteind & excludeind else : finalind = finiteind final_mags = mags [ finalind ] final_len = len ( final_mags ) if isinstance ( epdsmooth_extraparams , dict ) : smoothedmags = epdsmooth_func ( final_mags , epdsmooth_windowsize , ** epdsmooth_extraparams ) else : smoothedmags = epdsmooth_func ( final_mags , epdsmooth_windowsize ) epdmatrix = np . c_ [ fsv [ finalind ] ** 2.0 , fsv [ finalind ] , fdv [ finalind ] ** 2.0 , fdv [ finalind ] , fkv [ finalind ] ** 2.0 , fkv [ finalind ] , np . ones ( final_len ) , fsv [ finalind ] * fdv [ finalind ] , fsv [ finalind ] * fkv [ finalind ] , fdv [ finalind ] * fkv [ finalind ] , np . sin ( 2 * np . pi * xcc [ finalind ] ) , np . cos ( 2 * np . pi * xcc [ finalind ] ) , np . sin ( 2 * np . pi * ycc [ finalind ] ) , np . cos ( 2 * np . pi * ycc [ finalind ] ) , np . sin ( 4 * np . pi * xcc [ finalind ] ) , np . cos ( 4 * np . pi * xcc [ finalind ] ) , np . sin ( 4 * np . pi * ycc [ finalind ] ) , np . cos ( 4 * np . pi * ycc [ finalind ] ) , bgv [ finalind ] , bge [ finalind ] ] try : coeffs , residuals , rank , singulars = lstsq ( epdmatrix , smoothedmags , rcond = None ) if DEBUG : print ( 'coeffs = %s, residuals = %s' % ( coeffs , residuals ) ) retdict = { 'times' : times , 'mags' : ( mags_median + _old_epd_diffmags ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , mags ) ) , 'errs' : errs , 'fitcoeffs' : coeffs , 'residuals' : residuals } return retdict except Exception as e : LOGEXCEPTION ( 'EPD solution did not converge' ) retdict = { 'times' : times , 'mags' : np . full_like ( mags , np . nan ) , 'errs' : errs , 'fitcoeffs' : coeffs , 'residuals' : residuals } return retdict
4817	def _dictfetchall ( self , cursor ) : columns = [ col [ 0 ] for col in cursor . description ] return [ dict ( zip ( columns , row ) ) for row in cursor . fetchall ( ) ]
13381	def env_to_dict ( env , pathsep = os . pathsep ) : out_dict = { } for k , v in env . iteritems ( ) : if pathsep in v : out_dict [ k ] = v . split ( pathsep ) else : out_dict [ k ] = v return out_dict
10092	def _parse_response ( self , response ) : if not self . _raise_errors : return response is_4xx_error = str ( response . status_code ) [ 0 ] == '4' is_5xx_error = str ( response . status_code ) [ 0 ] == '5' content = response . content if response . status_code == 403 : raise AuthenticationError ( content ) elif is_4xx_error : raise APIError ( content ) elif is_5xx_error : raise ServerError ( content ) return response
570	def _handleModelRunnerException ( jobID , modelID , jobsDAO , experimentDir , logger , e ) : msg = StringIO . StringIO ( ) print >> msg , "Exception occurred while running model %s: %r (%s)" % ( modelID , e , type ( e ) ) traceback . print_exc ( None , msg ) completionReason = jobsDAO . CMPL_REASON_ERROR completionMsg = msg . getvalue ( ) logger . error ( completionMsg ) if type ( e ) is not InvalidConnectionException : jobsDAO . modelUpdateResults ( modelID , results = None , numRecords = 0 ) if type ( e ) == JobFailException : workerCmpReason = jobsDAO . jobGetFields ( jobID , [ 'workerCompletionReason' ] ) [ 0 ] if workerCmpReason == ClientJobsDAO . CMPL_REASON_SUCCESS : jobsDAO . jobSetFields ( jobID , fields = dict ( cancel = True , workerCompletionReason = ClientJobsDAO . CMPL_REASON_ERROR , workerCompletionMsg = ": " . join ( str ( i ) for i in e . args ) ) , useConnectionID = False , ignoreUnchanged = True ) return ( completionReason , completionMsg )
2828	def convert_selu ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting selu ...' ) if names == 'short' : tf_name = 'SELU' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) selu = keras . layers . Activation ( 'selu' , name = tf_name ) layers [ scope_name ] = selu ( layers [ inputs [ 0 ] ] )
4723	def trun_setup ( conf ) : declr = None try : with open ( conf [ "TESTPLAN_FPATH" ] ) as declr_fd : declr = yaml . safe_load ( declr_fd ) except AttributeError as exc : cij . err ( "rnr: %r" % exc ) if not declr : return None trun = copy . deepcopy ( TRUN ) trun [ "ver" ] = cij . VERSION trun [ "conf" ] = copy . deepcopy ( conf ) trun [ "res_root" ] = conf [ "OUTPUT" ] trun [ "aux_root" ] = os . sep . join ( [ trun [ "res_root" ] , "_aux" ] ) trun [ "evars" ] . update ( copy . deepcopy ( declr . get ( "evars" , { } ) ) ) os . makedirs ( trun [ "aux_root" ] ) hook_names = declr . get ( "hooks" , [ ] ) if "lock" not in hook_names : hook_names = [ "lock" ] + hook_names if hook_names [ 0 ] != "lock" : return None trun [ "hooks" ] = hooks_setup ( trun , trun , hook_names ) for enum , declr in enumerate ( declr [ "testsuites" ] ) : tsuite = tsuite_setup ( trun , declr , enum ) if tsuite is None : cij . err ( "main::FAILED: setting up tsuite: %r" % tsuite ) return 1 trun [ "testsuites" ] . append ( tsuite ) trun [ "progress" ] [ "UNKN" ] += len ( tsuite [ "testcases" ] ) return trun
1251	def _do_action_left ( self , state ) : reward = 0 for row in range ( 4 ) : merge_candidate = - 1 merged = np . zeros ( ( 4 , ) , dtype = np . bool ) for col in range ( 4 ) : if state [ row , col ] == 0 : continue if ( merge_candidate != - 1 and not merged [ merge_candidate ] and state [ row , merge_candidate ] == state [ row , col ] ) : state [ row , col ] = 0 merged [ merge_candidate ] = True state [ row , merge_candidate ] += 1 reward += 2 ** state [ row , merge_candidate ] else : merge_candidate += 1 if col != merge_candidate : state [ row , merge_candidate ] = state [ row , col ] state [ row , col ] = 0 return reward
1369	def create_tar ( tar_filename , files , config_dir , config_files ) : with contextlib . closing ( tarfile . open ( tar_filename , 'w:gz' , dereference = True ) ) as tar : for filename in files : if os . path . isfile ( filename ) : tar . add ( filename , arcname = os . path . basename ( filename ) ) else : raise Exception ( "%s is not an existing file" % filename ) if os . path . isdir ( config_dir ) : tar . add ( config_dir , arcname = get_heron_sandbox_conf_dir ( ) ) else : raise Exception ( "%s is not an existing directory" % config_dir ) for filename in config_files : if os . path . isfile ( filename ) : arcfile = os . path . join ( get_heron_sandbox_conf_dir ( ) , os . path . basename ( filename ) ) tar . add ( filename , arcname = arcfile ) else : raise Exception ( "%s is not an existing file" % filename )
13728	def balance ( address ) : txhistory = Address . transactions ( address ) balance = 0 for i in txhistory : if i . recipientId == address : balance += i . amount if i . senderId == address : balance -= ( i . amount + i . fee ) delegates = Delegate . delegates ( ) for i in delegates : if address == i . address : forged_blocks = Delegate . blocks ( i . pubkey ) for block in forged_blocks : balance += ( block . reward + block . totalFee ) if balance < 0 : height = Node . height ( ) logger . fatal ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) raise NegativeBalanceError ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) return balance
10682	def H_mag ( self , T ) : tau = T / self . Tc_mag if tau <= 1.0 : h = ( - self . _A_mag / tau + self . _B_mag * ( tau ** 3 / 2 + tau ** 9 / 15 + tau ** 15 / 40 ) ) / self . _D_mag else : h = - ( tau ** - 5 / 2 + tau ** - 15 / 21 + tau ** - 25 / 60 ) / self . _D_mag return R * T * math . log ( self . beta0_mag + 1 ) * h
13170	def path ( self , include_root = False ) : path = '%s[%d]' % ( self . tagname , self . index or 0 ) p = self . parent while p is not None : if p . parent or include_root : path = '%s[%d]/%s' % ( p . tagname , p . index or 0 , path ) p = p . parent return path
1196	def _make_future_features ( node ) : assert isinstance ( node , ast . ImportFrom ) assert node . module == '__future__' features = FutureFeatures ( ) for alias in node . names : name = alias . name if name in _FUTURE_FEATURES : if name not in _IMPLEMENTED_FUTURE_FEATURES : msg = 'future feature {} not yet implemented by grumpy' . format ( name ) raise util . ParseError ( node , msg ) setattr ( features , name , True ) elif name == 'braces' : raise util . ParseError ( node , 'not a chance' ) elif name not in _REDUNDANT_FUTURE_FEATURES : msg = 'future feature {} is not defined' . format ( name ) raise util . ParseError ( node , msg ) return features
7657	def append ( self , time = None , duration = None , value = None , confidence = None ) : self . data . add ( Observation ( time = float ( time ) , duration = float ( duration ) , value = value , confidence = confidence ) )
12222	def dispatch_first ( self , func ) : self . callees . appendleft ( self . _make_dispatch ( func ) ) return self . _make_wrapper ( func )
2817	def convert_adaptive_max_pool2d ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting adaptive_avg_pool2d...' ) if names == 'short' : tf_name = 'APOL' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) global_pool = keras . layers . GlobalMaxPooling2D ( data_format = 'channels_first' , name = tf_name ) layers [ scope_name ] = global_pool ( layers [ inputs [ 0 ] ] ) def target_layer ( x ) : import keras return keras . backend . expand_dims ( x ) lambda_layer = keras . layers . Lambda ( target_layer , name = tf_name + 'E' ) layers [ scope_name ] = lambda_layer ( layers [ scope_name ] ) layers [ scope_name ] = lambda_layer ( layers [ scope_name ] )
1675	def _ExpandDirectories ( filenames ) : expanded = set ( ) for filename in filenames : if not os . path . isdir ( filename ) : expanded . add ( filename ) continue for root , _ , files in os . walk ( filename ) : for loopfile in files : fullname = os . path . join ( root , loopfile ) if fullname . startswith ( '.' + os . path . sep ) : fullname = fullname [ len ( '.' + os . path . sep ) : ] expanded . add ( fullname ) filtered = [ ] for filename in expanded : if os . path . splitext ( filename ) [ 1 ] [ 1 : ] in GetAllExtensions ( ) : filtered . append ( filename ) return filtered
5283	def get_success_url ( self ) : if self . success_url : url = self . success_url else : url = self . request . get_full_path ( ) return url
3398	def update_costs ( self ) : for var in self . indicators : if var not in self . costs : self . costs [ var ] = var . cost else : if var . _get_primal ( ) > self . integer_threshold : self . costs [ var ] += var . cost self . model . objective . set_linear_coefficients ( self . costs )
9418	def document_func_view ( serializer_class = None , response_serializer_class = None , filter_backends = None , permission_classes = None , authentication_classes = None , doc_format_args = list ( ) , doc_format_kwargs = dict ( ) ) : def decorator ( func ) : if serializer_class : func . cls . serializer_class = func . view_class . serializer_class = serializer_class if response_serializer_class : func . cls . response_serializer_class = func . view_class . response_serializer_class = response_serializer_class if filter_backends : func . cls . filter_backends = func . view_class . filter_backends = filter_backends if permission_classes : func . cls . permission_classes = func . view_class . permission_classes = permission_classes if authentication_classes : func . cls . authentication_classes = func . view_class . authentication_classes = authentication_classes if doc_format_args or doc_format_kwargs : func . cls . __doc__ = func . view_class . __doc__ = getdoc ( func ) . format ( * doc_format_args , ** doc_format_kwargs ) return func return decorator
279	def plot_monthly_returns_heatmap ( returns , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) monthly_ret_table = ep . aggregate_returns ( returns , 'monthly' ) monthly_ret_table = monthly_ret_table . unstack ( ) . round ( 3 ) sns . heatmap ( monthly_ret_table . fillna ( 0 ) * 100.0 , annot = True , annot_kws = { "size" : 9 } , alpha = 1.0 , center = 0.0 , cbar = False , cmap = matplotlib . cm . RdYlGn , ax = ax , ** kwargs ) ax . set_ylabel ( 'Year' ) ax . set_xlabel ( 'Month' ) ax . set_title ( "Monthly returns (%)" ) return ax
2622	def spin_up_instance ( self , command , job_name ) : command = Template ( template_string ) . substitute ( jobname = job_name , user_script = command , linger = str ( self . linger ) . lower ( ) , worker_init = self . worker_init ) instance_type = self . instance_type subnet = self . sn_ids [ 0 ] ami_id = self . image_id total_instances = len ( self . instances ) if float ( self . spot_max_bid ) > 0 : spot_options = { 'MarketType' : 'spot' , 'SpotOptions' : { 'MaxPrice' : str ( self . spot_max_bid ) , 'SpotInstanceType' : 'one-time' , 'InstanceInterruptionBehavior' : 'terminate' } } else : spot_options = { } if total_instances > self . max_nodes : logger . warn ( "Exceeded instance limit ({}). Cannot continue\n" . format ( self . max_nodes ) ) return [ None ] try : tag_spec = [ { "ResourceType" : "instance" , "Tags" : [ { 'Key' : 'Name' , 'Value' : job_name } ] } ] instance = self . ec2 . create_instances ( MinCount = 1 , MaxCount = 1 , InstanceType = instance_type , ImageId = ami_id , KeyName = self . key_name , SubnetId = subnet , SecurityGroupIds = [ self . sg_id ] , TagSpecifications = tag_spec , InstanceMarketOptions = spot_options , InstanceInitiatedShutdownBehavior = 'terminate' , IamInstanceProfile = { 'Arn' : self . iam_instance_profile_arn } , UserData = command ) except ClientError as e : print ( e ) logger . error ( e . response ) return [ None ] except Exception as e : logger . error ( "Request for EC2 resources failed : {0}" . format ( e ) ) return [ None ] self . instances . append ( instance [ 0 ] . id ) logger . info ( "Started up 1 instance {} . Instance type:{}" . format ( instance [ 0 ] . id , instance_type ) ) return instance
6034	def padded_grid_stack_from_mask_sub_grid_size_and_psf_shape ( cls , mask , sub_grid_size , psf_shape ) : regular_padded_grid = PaddedRegularGrid . padded_grid_from_shape_psf_shape_and_pixel_scale ( shape = mask . shape , psf_shape = psf_shape , pixel_scale = mask . pixel_scale ) sub_padded_grid = PaddedSubGrid . padded_grid_from_mask_sub_grid_size_and_psf_shape ( mask = mask , sub_grid_size = sub_grid_size , psf_shape = psf_shape ) return GridStack ( regular = regular_padded_grid , sub = sub_padded_grid , blurring = np . array ( [ [ 0.0 , 0.0 ] ] ) )
2582	def load_checkpoints ( self , checkpointDirs ) : self . memo_lookup_table = None if not checkpointDirs : return { } if type ( checkpointDirs ) is not list : raise BadCheckpoint ( "checkpointDirs expects a list of checkpoints" ) return self . _load_checkpoints ( checkpointDirs )
9003	def _compute_scale ( self , instruction_id , svg_dict ) : bbox = list ( map ( float , svg_dict [ "svg" ] [ "@viewBox" ] . split ( ) ) ) scale = self . _zoom / ( bbox [ 3 ] - bbox [ 1 ] ) self . _symbol_id_to_scale [ instruction_id ] = scale
5350	def compose_projects_json ( projects , data ) : projects = compose_git ( projects , data ) projects = compose_mailing_lists ( projects , data ) projects = compose_bugzilla ( projects , data ) projects = compose_github ( projects , data ) projects = compose_gerrit ( projects ) projects = compose_mbox ( projects ) return projects
3867	async def set_notification_level ( self , level ) : await self . _client . set_conversation_notification_level ( hangouts_pb2 . SetConversationNotificationLevelRequest ( request_header = self . _client . get_request_header ( ) , conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , level = level , ) )
12717	def angles ( self ) : return [ self . ode_obj . getAngle ( i ) for i in range ( self . ADOF ) ]
12221	def dispatch ( self , func ) : self . callees . append ( self . _make_dispatch ( func ) ) return self . _make_wrapper ( func )
9419	def format_docstring ( * args , ** kwargs ) : def decorator ( func ) : func . __doc__ = getdoc ( func ) . format ( * args , ** kwargs ) return func return decorator
13547	def _setVirtualEnv ( ) : try : activate = options . virtualenv . activate_cmd except AttributeError : activate = None if activate is None : virtualenv = path ( os . environ . get ( 'VIRTUAL_ENV' , '' ) ) if not virtualenv : virtualenv = options . paved . cwd else : virtualenv = path ( virtualenv ) activate = virtualenv / 'bin' / 'activate' if activate . exists ( ) : info ( 'Using default virtualenv at %s' % activate ) options . setdotted ( 'virtualenv.activate_cmd' , 'source %s' % activate )
2861	def _transaction_end ( self ) : self . _command . append ( '\x87' ) self . _ft232h . _write ( '' . join ( self . _command ) ) return bytearray ( self . _ft232h . _poll_read ( self . _expected ) )
9810	def dashboard ( yes , url ) : dashboard_url = "{}/app" . format ( PolyaxonClient ( ) . api_config . http_host ) if url : click . echo ( dashboard_url ) sys . exit ( 0 ) if not yes : click . confirm ( 'Dashboard page will now open in your browser. Continue?' , abort = True , default = True ) click . launch ( dashboard_url )
3543	def python_source_files ( path , tests_dirs ) : if isdir ( path ) : for root , dirs , files in os . walk ( path ) : dirs [ : ] = [ d for d in dirs if os . path . join ( root , d ) not in tests_dirs ] for filename in files : if filename . endswith ( '.py' ) : yield os . path . join ( root , filename ) else : yield path
7914	def get_int_range_validator ( start , stop ) : def validate_int_range ( value ) : value = int ( value ) if value >= start and value < stop : return value raise ValueError ( "Not in <{0},{1}) range" . format ( start , stop ) ) return validate_int_range
8749	def delete_scalingip ( context , id ) : LOG . info ( 'delete_scalingip %s for tenant %s' % ( id , context . tenant_id ) ) _delete_flip ( context , id , ip_types . SCALING )
12453	def config_to_args ( config ) : result = [ ] for key , value in iteritems ( config ) : if value is False : continue key = '--{0}' . format ( key . replace ( '_' , '-' ) ) if isinstance ( value , ( list , set , tuple ) ) : for item in value : result . extend ( ( key , smart_str ( item ) ) ) elif value is not True : result . extend ( ( key , smart_str ( value ) ) ) else : result . append ( key ) return tuple ( result )
3462	def double_reaction_deletion ( model , reaction_list1 = None , reaction_list2 = None , method = "fba" , solution = None , processes = None , ** kwargs ) : reaction_list1 , reaction_list2 = _element_lists ( model . reactions , reaction_list1 , reaction_list2 ) return _multi_deletion ( model , 'reaction' , element_lists = [ reaction_list1 , reaction_list2 ] , method = method , solution = solution , processes = processes , ** kwargs )
13159	def update ( cls , cur , table : str , values : dict , where_keys : list ) -> tuple : keys = cls . _COMMA . join ( values . keys ( ) ) value_place_holder = cls . _PLACEHOLDER * len ( values ) where_clause , where_values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _update_string . format ( table , keys , value_place_holder [ : - 1 ] , where_clause ) yield from cur . execute ( query , ( tuple ( values . values ( ) ) + where_values ) ) return ( yield from cur . fetchall ( ) )
5417	def _format_task_uri ( fmt , job_metadata , task_metadata ) : values = { 'job-id' : None , 'task-id' : 'task' , 'job-name' : None , 'user-id' : None , 'task-attempt' : None } for key in values : values [ key ] = task_metadata . get ( key ) or job_metadata . get ( key ) or values [ key ] return fmt . format ( ** values )
6330	def tf ( self , term ) : r if ' ' in term : raise ValueError ( 'tf can only calculate the term frequency of individual words' ) tcount = self . get_count ( term ) if tcount == 0 : return 0.0 return 1 + log10 ( tcount )
1193	def task_done ( self ) : self . all_tasks_done . acquire ( ) try : unfinished = self . unfinished_tasks - 1 if unfinished <= 0 : if unfinished < 0 : raise ValueError ( 'task_done() called too many times' ) self . all_tasks_done . notify_all ( ) self . unfinished_tasks = unfinished finally : self . all_tasks_done . release ( )
10047	def check_oauth2_scope ( can_method , * myscopes ) : def check ( record , * args , ** kwargs ) : @ require_api_auth ( ) @ require_oauth_scopes ( * myscopes ) def can ( self ) : return can_method ( record ) return type ( 'CheckOAuth2Scope' , ( ) , { 'can' : can } ) ( ) return check
12089	def proto_01_12_steps025 ( abf = exampleABF ) : swhlab . ap . detect ( abf ) standard_groupingForInj ( abf , 200 ) for feature in [ 'freq' , 'downslope' ] : swhlab . ap . plot_values ( abf , feature , continuous = False ) swhlab . plot . save ( abf , tag = 'A_' + feature ) swhlab . plot . gain ( abf ) swhlab . plot . save ( abf , tag = '05-gain' )
963	def _getScaledValue ( self , inpt ) : if inpt == SENTINEL_VALUE_FOR_MISSING_DATA : return None else : val = inpt if val < self . minval : val = self . minval elif val > self . maxval : val = self . maxval scaledVal = math . log10 ( val ) return scaledVal
33	def reset ( self , ** kwargs ) : if self . was_real_done : obs = self . env . reset ( ** kwargs ) else : obs , _ , _ , _ = self . env . step ( 0 ) self . lives = self . env . unwrapped . ale . lives ( ) return obs
5956	def load_v4_tools ( ) : logger . debug ( "Loading v4 tools..." ) names = config . get_tool_names ( ) if len ( names ) == 0 and 'GMXBIN' in os . environ : names = find_executables ( os . environ [ 'GMXBIN' ] ) if len ( names ) == 0 or len ( names ) > len ( V4TOOLS ) * 4 : names = list ( V4TOOLS ) names . extend ( config . get_extra_tool_names ( ) ) tools = { } for name in names : fancy = make_valid_identifier ( name ) tools [ fancy ] = tool_factory ( fancy , name , None ) if not tools : errmsg = "Failed to load v4 tools" logger . debug ( errmsg ) raise GromacsToolLoadingError ( errmsg ) logger . debug ( "Loaded {0} v4 tools successfully!" . format ( len ( tools ) ) ) return tools
6260	def calc_global_bbox ( self , view_matrix , bbox_min , bbox_max ) : if self . matrix is not None : view_matrix = matrix44 . multiply ( self . matrix , view_matrix ) if self . mesh : bbox_min , bbox_max = self . mesh . calc_global_bbox ( view_matrix , bbox_min , bbox_max ) for child in self . children : bbox_min , bbox_max = child . calc_global_bbox ( view_matrix , bbox_min , bbox_max ) return bbox_min , bbox_max
5293	def get_context_data ( self , ** kwargs ) : context = { } inlines_names = self . get_inlines_names ( ) if inlines_names : context . update ( zip ( inlines_names , kwargs . get ( 'inlines' , [ ] ) ) ) if 'formset' in kwargs : context [ inlines_names [ 0 ] ] = kwargs [ 'formset' ] context . update ( kwargs ) return super ( NamedFormsetsMixin , self ) . get_context_data ( ** context )
12285	def add ( self , repo ) : key = self . key ( repo . username , repo . reponame ) repo . key = key self . repos [ key ] = repo return key
7199	def get_chip ( self , coordinates , catid , chip_type = 'PAN' , chip_format = 'TIF' , filename = 'chip.tif' ) : def t2s1 ( t ) : return str ( t ) . strip ( '(,)' ) . replace ( ',' , '' ) def t2s2 ( t ) : return str ( t ) . strip ( '(,)' ) . replace ( ' ' , '' ) if len ( coordinates ) != 4 : print ( 'Wrong coordinate entry' ) return False W , S , E , N = coordinates box = ( ( W , S ) , ( W , N ) , ( E , N ) , ( E , S ) , ( W , S ) ) box_wkt = 'POLYGON ((' + ',' . join ( [ t2s1 ( corner ) for corner in box ] ) + '))' results = self . get_images_by_catid_and_aoi ( catid = catid , aoi_wkt = box_wkt ) description = self . describe_images ( results ) pan_id , ms_id , num_bands = None , None , 0 for catid , images in description . items ( ) : for partnum , part in images [ 'parts' ] . items ( ) : if 'PAN' in part . keys ( ) : pan_id = part [ 'PAN' ] [ 'id' ] bucket = part [ 'PAN' ] [ 'bucket' ] if 'WORLDVIEW_8_BAND' in part . keys ( ) : ms_id = part [ 'WORLDVIEW_8_BAND' ] [ 'id' ] num_bands = 8 bucket = part [ 'WORLDVIEW_8_BAND' ] [ 'bucket' ] elif 'RGBN' in part . keys ( ) : ms_id = part [ 'RGBN' ] [ 'id' ] num_bands = 4 bucket = part [ 'RGBN' ] [ 'bucket' ] band_str = '' if chip_type == 'PAN' : band_str = pan_id + '?bands=0' elif chip_type == 'MS' : band_str = ms_id + '?' elif chip_type == 'PS' : if num_bands == 8 : band_str = ms_id + '?bands=4,2,1&panId=' + pan_id elif num_bands == 4 : band_str = ms_id + '?bands=0,1,2&panId=' + pan_id location_str = '&upperLeft={}&lowerRight={}' . format ( t2s2 ( ( W , N ) ) , t2s2 ( ( E , S ) ) ) service_url = 'https://idaho.geobigdata.io/v1/chip/bbox/' + bucket + '/' url = service_url + band_str + location_str url += '&format=' + chip_format + '&token=' + self . gbdx_connection . access_token r = requests . get ( url ) if r . status_code == 200 : with open ( filename , 'wb' ) as f : f . write ( r . content ) return True else : print ( 'Cannot download chip' ) return False
5516	def append ( self , data , start ) : if self . _limit is not None and self . _limit > 0 : if self . _start is None : self . _start = start if start - self . _start > self . reset_rate : self . _sum -= round ( ( start - self . _start ) * self . _limit ) self . _start = start self . _sum += len ( data )
11338	def schedule_mode ( self , mode ) : modes = [ config . SCHEDULE_RUN , config . SCHEDULE_TEMPORARY_HOLD , config . SCHEDULE_HOLD ] if mode not in modes : raise Exception ( "Invalid mode. Please use one of: {}" . format ( modes ) ) self . set_data ( { "ScheduleMode" : mode } )
11813	def present_results ( self , query_text , n = 10 ) : "Get results for the query and present them." self . present ( self . query ( query_text , n ) )
10580	def calculate ( self , ** state ) : super ( ) . calculate ( ** state ) return self . mm * self . P / R / state [ "T" ]
8012	def check_paypal_api_key ( app_configs = None , ** kwargs ) : messages = [ ] mode = getattr ( djpaypal_settings , "PAYPAL_MODE" , None ) if mode not in VALID_MODES : msg = "Invalid PAYPAL_MODE specified: {}." . format ( repr ( mode ) ) hint = "PAYPAL_MODE must be one of {}" . format ( ", " . join ( repr ( k ) for k in VALID_MODES ) ) messages . append ( checks . Critical ( msg , hint = hint , id = "djpaypal.C001" ) ) for setting in "PAYPAL_CLIENT_ID" , "PAYPAL_CLIENT_SECRET" : if not getattr ( djpaypal_settings , setting , None ) : msg = "Invalid value specified for {}" . format ( setting ) hint = "Add PAYPAL_CLIENT_ID and PAYPAL_CLIENT_SECRET to your settings." messages . append ( checks . Critical ( msg , hint = hint , id = "djpaypal.C002" ) ) return messages
10571	def template_to_filepath ( template , metadata , template_patterns = None ) : if template_patterns is None : template_patterns = TEMPLATE_PATTERNS metadata = metadata if isinstance ( metadata , dict ) else _mutagen_fields_to_single_value ( metadata ) assert isinstance ( metadata , dict ) suggested_filename = get_suggested_filename ( metadata ) . replace ( '.mp3' , '' ) if template == os . getcwd ( ) or template == '%suggested%' : filepath = suggested_filename else : t = template . replace ( '%suggested%' , suggested_filename ) filepath = _replace_template_patterns ( t , metadata , template_patterns ) return filepath
9242	def detect_actual_closed_dates ( self , issues , kind ) : if self . options . verbose : print ( "Fetching closed dates for {} {}..." . format ( len ( issues ) , kind ) ) all_issues = copy . deepcopy ( issues ) for issue in all_issues : if self . options . verbose > 2 : print ( "." , end = "" ) if not issues . index ( issue ) % 30 : print ( "" ) self . find_closed_date_by_commit ( issue ) if not issue . get ( 'actual_date' , False ) : if issue . get ( 'closed_at' , False ) : print ( "Skipping closed non-merged issue: #{0} {1}" . format ( issue [ "number" ] , issue [ "title" ] ) ) all_issues . remove ( issue ) if self . options . verbose > 2 : print ( "." ) return all_issues
2939	def deserialize_logical ( self , node ) : term1_attrib = node . getAttribute ( 'left-field' ) term1_value = node . getAttribute ( 'left-value' ) op = node . nodeName . lower ( ) term2_attrib = node . getAttribute ( 'right-field' ) term2_value = node . getAttribute ( 'right-value' ) if op not in _op_map : _exc ( 'Invalid operator' ) if term1_attrib != '' and term1_value != '' : _exc ( 'Both, left-field and left-value attributes found' ) elif term1_attrib == '' and term1_value == '' : _exc ( 'left-field or left-value attribute required' ) elif term1_value != '' : left = term1_value else : left = operators . Attrib ( term1_attrib ) if term2_attrib != '' and term2_value != '' : _exc ( 'Both, right-field and right-value attributes found' ) elif term2_attrib == '' and term2_value == '' : _exc ( 'right-field or right-value attribute required' ) elif term2_value != '' : right = term2_value else : right = operators . Attrib ( term2_attrib ) return _op_map [ op ] ( left , right )
6156	def stereo_FM ( x , fs = 2.4e6 , file_name = 'test.wav' ) : N1 = 10 b = signal . firwin ( 64 , 2 * 200e3 / float ( fs ) ) y = signal . lfilter ( b , 1 , x ) z = ss . downsample ( y , N1 ) z_bb = discrim ( z ) b12 = signal . firwin ( 128 , 2 * 12e3 / ( float ( fs ) / N1 ) ) y_lpr = signal . lfilter ( b12 , 1 , z_bb ) b19 = signal . firwin ( 128 , 2 * 1e3 * np . array ( [ 19 - 5 , 19 + 5 ] ) / ( float ( fs ) / N1 ) , pass_zero = False ) z_bb19 = signal . lfilter ( b19 , 1 , z_bb ) theta , phi_error = pilot_PLL ( z_bb19 , 19000 , fs / N1 , 2 , 10 , 0.707 ) b38 = signal . firwin ( 128 , 2 * 1e3 * np . array ( [ 38 - 5 , 38 + 5 ] ) / ( float ( fs ) / N1 ) , pass_zero = False ) x_lmr = signal . lfilter ( b38 , 1 , z_bb ) x_lmr = 2 * np . sqrt ( 2 ) * np . cos ( 2 * theta ) * x_lmr y_lmr = signal . lfilter ( b12 , 1 , x_lmr ) y_left = y_lpr + y_lmr y_right = y_lpr - y_lmr N2 = 5 fs2 = float ( fs ) / ( N1 * N2 ) y_left_DN2 = ss . downsample ( y_left , N2 ) y_right_DN2 = ss . downsample ( y_right , N2 ) a_de = np . exp ( - 2.1 * 1e3 * 2 * np . pi / fs2 ) z_left = signal . lfilter ( [ 1 - a_de ] , [ 1 , - a_de ] , y_left_DN2 ) z_right = signal . lfilter ( [ 1 - a_de ] , [ 1 , - a_de ] , y_right_DN2 ) z_out = np . hstack ( ( np . array ( [ z_left ] ) . T , ( np . array ( [ z_right ] ) . T ) ) ) ss . to_wav ( file_name , 48000 , z_out / 2 ) print ( 'Done!' ) return z_bb , theta , y_lpr , y_lmr , z_out
10834	def query_admins_by_group_ids ( cls , groups_ids = None ) : assert groups_ids is None or isinstance ( groups_ids , list ) query = db . session . query ( Group . id , func . count ( GroupAdmin . id ) ) . join ( GroupAdmin ) . group_by ( Group . id ) if groups_ids : query = query . filter ( Group . id . in_ ( groups_ids ) ) return query
4391	def adsSyncReadDeviceInfoReqEx ( port , address ) : sync_read_device_info_request = _adsDLL . AdsSyncReadDeviceInfoReqEx ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) device_name_buffer = ctypes . create_string_buffer ( 20 ) device_name_pointer = ctypes . pointer ( device_name_buffer ) ads_version = SAdsVersion ( ) ads_version_pointer = ctypes . pointer ( ads_version ) error_code = sync_read_device_info_request ( port , ams_address_pointer , device_name_pointer , ads_version_pointer ) if error_code : raise ADSError ( error_code ) return ( device_name_buffer . value . decode ( ) , AdsVersion ( ads_version ) )
13422	def get_all ( self , key = None ) : key = self . definition . main_key if key is None else key key = self . definition . key_synonyms . get ( key , key ) entries = self . _get_all ( key ) if key in self . definition . scalar_nonunique_keys : return set ( entries ) return entries
2990	def cross_origin ( * args , ** kwargs ) : _options = kwargs def decorator ( f ) : LOG . debug ( "Enabling %s for cross_origin using options:%s" , f , _options ) if _options . get ( 'automatic_options' , True ) : f . required_methods = getattr ( f , 'required_methods' , set ( ) ) f . required_methods . add ( 'OPTIONS' ) f . provide_automatic_options = False def wrapped_function ( * args , ** kwargs ) : options = get_cors_options ( current_app , _options ) if options . get ( 'automatic_options' ) and request . method == 'OPTIONS' : resp = current_app . make_default_options_response ( ) else : resp = make_response ( f ( * args , ** kwargs ) ) set_cors_headers ( resp , options ) setattr ( resp , FLASK_CORS_EVALUATED , True ) return resp return update_wrapper ( wrapped_function , f ) return decorator
12587	def all_childnodes_to_nifti1img ( h5group ) : child_nodes = [ ] def append_parent_if_dataset ( name , obj ) : if isinstance ( obj , h5py . Dataset ) : if name . split ( '/' ) [ - 1 ] == 'data' : child_nodes . append ( obj . parent ) vols = [ ] h5group . visititems ( append_parent_if_dataset ) for c in child_nodes : vols . append ( hdfgroup_to_nifti1image ( c ) ) return vols
11754	def parse_definite_clause ( s ) : "Return the antecedents and the consequent of a definite clause." assert is_definite_clause ( s ) if is_symbol ( s . op ) : return [ ] , s else : antecedent , consequent = s . args return conjuncts ( antecedent ) , consequent
2248	def memoize ( func ) : cache = { } @ functools . wraps ( func ) def memoizer ( * args , ** kwargs ) : key = _make_signature_key ( args , kwargs ) if key not in cache : cache [ key ] = func ( * args , ** kwargs ) return cache [ key ] memoizer . cache = cache return memoizer
6642	def satisfyDependenciesRecursive ( self , available_components = None , search_dirs = None , update_installed = False , traverse_links = False , target = None , test = False ) : def provider ( dspec , available_components , search_dirs , working_directory , update_installed , dep_of = None ) : r = access . satisfyFromAvailable ( dspec . name , available_components ) if r : if r . isTestDependency ( ) and not dspec . is_test_dependency : logger . debug ( 'test dependency subsequently occurred as real dependency: %s' , r . getName ( ) ) r . setTestDependency ( False ) return r update_if_installed = False if update_installed is True : update_if_installed = True elif update_installed : update_if_installed = dspec . name in update_installed r = access . satisfyVersionFromSearchPaths ( dspec . name , dspec . versionReq ( ) , search_dirs , update_if_installed , inherit_shrinkwrap = dep_of . getShrinkwrap ( ) ) if r : r . setTestDependency ( dspec . is_test_dependency ) return r default_path = os . path . join ( self . modulesPath ( ) , dspec . name ) if fsutils . isLink ( default_path ) : r = Component ( default_path , test_dependency = dspec . is_test_dependency , installed_linked = fsutils . isLink ( default_path ) , inherit_shrinkwrap = dep_of . getShrinkwrap ( ) ) if r : assert ( r . installedLinked ( ) ) return r else : logger . error ( 'linked module %s is invalid: %s' , dspec . name , r . getError ( ) ) return r r = access . satisfyVersionByInstalling ( dspec . name , dspec . versionReq ( ) , self . modulesPath ( ) , inherit_shrinkwrap = dep_of . getShrinkwrap ( ) ) if not r : logger . error ( 'could not install %s' % dspec . name ) if r is not None : r . setTestDependency ( dspec . is_test_dependency ) return r return self . __getDependenciesRecursiveWithProvider ( available_components = available_components , search_dirs = search_dirs , target = target , traverse_links = traverse_links , update_installed = update_installed , provider = provider , test = test )
10967	def sync_params ( self ) : def _normalize ( comps , param ) : vals = [ c . get_values ( param ) for c in comps ] diff = any ( [ vals [ i ] != vals [ i + 1 ] for i in range ( len ( vals ) - 1 ) ] ) if diff : for c in comps : c . set_values ( param , vals [ 0 ] ) for param , comps in iteritems ( self . lmap ) : if isinstance ( comps , list ) and len ( comps ) > 1 : _normalize ( comps , param )
3676	def rdkitmol ( self ) : r if self . __rdkitmol : return self . __rdkitmol else : try : self . __rdkitmol = Chem . MolFromSmiles ( self . smiles ) return self . __rdkitmol except : return None
9008	def get_index_in_row ( self ) : expected_index = self . _cached_index_in_row instructions = self . _row . instructions if expected_index is not None and 0 <= expected_index < len ( instructions ) and instructions [ expected_index ] is self : return expected_index for index , instruction_in_row in enumerate ( instructions ) : if instruction_in_row is self : self . _cached_index_in_row = index return index return None
13749	def many_to_one ( clsname , ** kw ) : @ declared_attr def m2o ( cls ) : cls . _references ( ( cls . __name__ , clsname ) ) return relationship ( clsname , ** kw ) return m2o
8769	def create_job ( context , body ) : LOG . info ( "create_job for tenant %s" % context . tenant_id ) if not context . is_admin : raise n_exc . NotAuthorized ( ) job = body . get ( 'job' ) if 'parent_id' in job : parent_id = job [ 'parent_id' ] if not parent_id : raise q_exc . JobNotFound ( job_id = parent_id ) parent_job = db_api . async_transaction_find ( context , id = parent_id , scope = db_api . ONE ) if not parent_job : raise q_exc . JobNotFound ( job_id = parent_id ) tid = parent_id if parent_job . get ( 'transaction_id' ) : tid = parent_job . get ( 'transaction_id' ) job [ 'transaction_id' ] = tid if not job : raise n_exc . BadRequest ( resource = "job" , msg = "Invalid request body." ) with context . session . begin ( subtransactions = True ) : new_job = db_api . async_transaction_create ( context , ** job ) return v . _make_job_dict ( new_job )
11543	def set_analog_reference ( self , reference , pin = None ) : if pin is None : self . _set_analog_reference ( reference , None ) else : pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : self . _set_analog_reference ( reference , pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
3056	def _create_file_if_needed ( filename ) : if os . path . exists ( filename ) : return False else : open ( filename , 'a+b' ) . close ( ) logger . info ( 'Credential file {0} created' . format ( filename ) ) return True
9172	def includeme ( config ) : config . include ( 'pyramid_jinja2' ) config . add_jinja2_renderer ( '.html' ) config . add_jinja2_renderer ( '.rss' ) config . add_static_view ( name = '/a/static' , path = "cnxpublishing:static/" ) config . commit ( ) from cnxdb . ident_hash import join_ident_hash for ext in ( '.html' , '.rss' , ) : jinja2_env = config . get_jinja2_environment ( ext ) jinja2_env . globals . update ( join_ident_hash = join_ident_hash , ) declare_api_routes ( config ) declare_browsable_routes ( config )
12762	def create_bodies ( self ) : self . bodies = { } for label in self . channels : body = self . world . create_body ( 'sphere' , name = 'marker:{}' . format ( label ) , radius = 0.02 ) body . is_kinematic = True body . color = 0.9 , 0.1 , 0.1 , 0.5 self . bodies [ label ] = body
3435	def slim_optimize ( self , error_value = float ( 'nan' ) , message = None ) : self . solver . optimize ( ) if self . solver . status == optlang . interface . OPTIMAL : return self . solver . objective . value elif error_value is not None : return error_value else : assert_optimal ( self , message )
6657	def calc_inbag ( n_samples , forest ) : if not forest . bootstrap : e_s = "Cannot calculate the inbag from a forest that has " e_s = " bootstrap=False" raise ValueError ( e_s ) n_trees = forest . n_estimators inbag = np . zeros ( ( n_samples , n_trees ) ) sample_idx = [ ] for t_idx in range ( n_trees ) : sample_idx . append ( _generate_sample_indices ( forest . estimators_ [ t_idx ] . random_state , n_samples ) ) inbag [ : , t_idx ] = np . bincount ( sample_idx [ - 1 ] , minlength = n_samples ) return inbag
10554	def get_helping_materials ( project_id , limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) params [ 'project_id' ] = project_id try : res = _pybossa_req ( 'get' , 'helpingmaterial' , params = params ) if type ( res ) . __name__ == 'list' : return [ HelpingMaterial ( helping ) for helping in res ] else : return res except : raise
5632	def unindent ( lines ) : try : indent = min ( len ( line ) - len ( line . lstrip ( ) ) for line in lines if line ) except ValueError : return lines else : return [ line [ indent : ] for line in lines ]
9376	def extract_diff_sla_from_config_file ( obj , options_file ) : rule_strings = { } config_obj = ConfigParser . ConfigParser ( ) config_obj . optionxform = str config_obj . read ( options_file ) for section in config_obj . sections ( ) : rule_strings , kwargs = get_rule_strings ( config_obj , section ) for ( key , val ) in rule_strings . iteritems ( ) : set_sla ( obj , section , key , val )
6805	def init_ubuntu_disk ( self , yes = 0 ) : self . assume_localhost ( ) yes = int ( yes ) if not self . dryrun : device_question = 'SD card present at %s? ' % self . env . sd_device inp = raw_input ( device_question ) . strip ( ) print ( 'inp:' , inp ) if not yes and inp and not inp . lower ( ) . startswith ( 'y' ) : return r = self . local_renderer r . local ( 'ls {sd_device}' ) r . env . ubuntu_image_fn = os . path . abspath ( os . path . split ( self . env . ubuntu_download_url ) [ - 1 ] ) r . local ( '[ ! -f {ubuntu_image_fn} ] && wget {ubuntu_download_url} || true' ) with self . settings ( warn_only = True ) : r . sudo ( '[ -d "{sd_media_mount_dir}" ] && umount {sd_media_mount_dir}' ) with self . settings ( warn_only = True ) : r . sudo ( '[ -d "{sd_media_mount_dir2}" ] && umount {sd_media_mount_dir2}' ) r . pc ( 'Writing the image onto the card.' ) r . sudo ( 'xzcat {ubuntu_image_fn} | dd bs=4M of={sd_device}' ) r . run ( 'sync' )
304	def show_worst_drawdown_periods ( returns , top = 5 ) : drawdown_df = timeseries . gen_drawdown_table ( returns , top = top ) utils . print_table ( drawdown_df . sort_values ( 'Net drawdown in %' , ascending = False ) , name = 'Worst drawdown periods' , float_format = '{0:.2f}' . format , )
9681	def config2 ( self ) : config = [ ] data = { } self . cnxn . xfer ( [ 0x3D ] ) sleep ( 10e-3 ) for i in range ( 9 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) data [ "AMSamplingInterval" ] = self . _16bit_unsigned ( config [ 0 ] , config [ 1 ] ) data [ "AMIdleIntervalCount" ] = self . _16bit_unsigned ( config [ 2 ] , config [ 3 ] ) data [ 'AMFanOnIdle' ] = config [ 4 ] data [ 'AMLaserOnIdle' ] = config [ 5 ] data [ 'AMMaxDataArraysInFile' ] = self . _16bit_unsigned ( config [ 6 ] , config [ 7 ] ) data [ 'AMOnlySavePMData' ] = config [ 8 ] sleep ( 0.1 ) return data
7478	def sort_seeds ( uhandle , usort ) : cmd = [ "sort" , "-k" , "2" , uhandle , "-o" , usort ] proc = sps . Popen ( cmd , close_fds = True ) proc . communicate ( )
4790	def is_lower ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if self . val != self . val . lower ( ) : self . _err ( 'Expected <%s> to contain only lowercase chars, but did not.' % self . val ) return self
8609	def list_resources ( self , resource_type = None , depth = 1 ) : if resource_type is not None : response = self . _perform_request ( '/um/resources/%s?depth=%s' % ( resource_type , str ( depth ) ) ) else : response = self . _perform_request ( '/um/resources?depth=' + str ( depth ) ) return response
13862	def totz ( when , tz = None ) : if when is None : return None when = to_datetime ( when ) if when . tzinfo is None : when = when . replace ( tzinfo = localtz ) return when . astimezone ( tz or utc )
6948	def jhk_to_sdssg ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSG_JHK , SDSSG_JH , SDSSG_JK , SDSSG_HK , SDSSG_J , SDSSG_H , SDSSG_K )
515	def _avgConnectedSpanForColumn1D ( self , columnIndex ) : assert ( self . _inputDimensions . size == 1 ) connected = self . _connectedSynapses [ columnIndex ] . nonzero ( ) [ 0 ] if connected . size == 0 : return 0 else : return max ( connected ) - min ( connected ) + 1
8490	def start_watching ( self ) : if self . watcher and self . watcher . is_alive ( ) : return self . watcher = Watcher ( ) self . watcher . start ( )
13239	def intervals ( self , range_start = datetime . datetime . min , range_end = datetime . datetime . max ) : current_period = None max_continuous_days = 60 range_start = self . to_timezone ( range_start ) range_end = self . to_timezone ( range_end ) for period in self . _daily_periods ( range_start . date ( ) , range_end . date ( ) ) : if period . end < range_start or period . start > range_end : continue if current_period is None : current_period = period else : if ( ( ( period . start < current_period . end ) or ( period . start - current_period . end ) <= datetime . timedelta ( minutes = 1 ) ) and ( current_period . end - current_period . start ) < datetime . timedelta ( days = max_continuous_days ) ) : current_period = Period ( current_period . start , period . end ) else : yield current_period current_period = period if current_period : yield current_period
8751	def get_scalingips ( context , filters = None , fields = None , sorts = [ 'id' ] , limit = None , marker = None , page_reverse = False ) : LOG . info ( 'get_scalingips for tenant %s filters %s fields %s' % ( context . tenant_id , filters , fields ) ) scaling_ips = _get_ips_by_type ( context , ip_types . SCALING , filters = filters , fields = fields ) return [ v . _make_scaling_ip_dict ( scip ) for scip in scaling_ips ]
10968	def setup_passthroughs ( self ) : self . _nopickle = [ ] for c in self . comps : funcs = inspect . getmembers ( c , predicate = inspect . ismethod ) for func in funcs : if func [ 0 ] . startswith ( 'param_' ) : setattr ( self , func [ 0 ] , func [ 1 ] ) self . _nopickle . append ( func [ 0 ] ) funcs = c . exports ( ) for func in funcs : newname = c . category + '_' + func . __func__ . __name__ setattr ( self , newname , func ) self . _nopickle . append ( newname )
13340	def rollaxis ( a , axis , start = 0 ) : if isinstance ( a , np . ndarray ) : return np . rollaxis ( a , axis , start ) if axis not in range ( a . ndim ) : raise ValueError ( 'rollaxis: axis (%d) must be >=0 and < %d' % ( axis , a . ndim ) ) if start not in range ( a . ndim + 1 ) : raise ValueError ( 'rollaxis: start (%d) must be >=0 and < %d' % ( axis , a . ndim + 1 ) ) axes = list ( range ( a . ndim ) ) axes . remove ( axis ) axes . insert ( start , axis ) return transpose ( a , axes )
5862	def validate ( self , ml_template ) : data = { "ml_template" : ml_template } failure_message = "ML template validation invoke failed" res = self . _get_success_json ( self . _post_json ( 'ml_templates/validate' , data , failure_message = failure_message ) ) [ 'data' ] if res [ 'valid' ] : return 'OK' return res [ 'reason' ]
12770	def step ( self , substeps = 2 ) : self . frame_no += 1 try : next ( self . follower ) except ( AttributeError , StopIteration ) as err : self . reset ( )
7560	def get_total ( tots , node ) : if ( node . is_leaf ( ) or node . is_root ( ) ) : return 0 else : if len ( node . children ) > 2 : down_r = node . children [ 0 ] down_l = node . children [ 1 ] for child in node . children [ 2 : ] : down_l += child else : down_r , down_l = node . children lendr = sum ( 1 for i in down_r . iter_leaves ( ) ) lendl = sum ( 1 for i in down_l . iter_leaves ( ) ) up_r = node . get_sisters ( ) [ 0 ] lenur = sum ( 1 for i in up_r . iter_leaves ( ) ) lenul = tots - ( lendr + lendl + lenur ) return lendr * lendl * lenur * lenul
1317	def GetChildren ( self ) -> list : children = [ ] child = self . GetFirstChildControl ( ) while child : children . append ( child ) child = child . GetNextSiblingControl ( ) return children
5979	def edge_pixels_from_mask ( mask ) : edge_pixel_total = total_edge_pixels_from_mask ( mask ) edge_pixels = np . zeros ( edge_pixel_total ) edge_index = 0 regular_index = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : if mask [ y + 1 , x ] or mask [ y - 1 , x ] or mask [ y , x + 1 ] or mask [ y , x - 1 ] or mask [ y + 1 , x + 1 ] or mask [ y + 1 , x - 1 ] or mask [ y - 1 , x + 1 ] or mask [ y - 1 , x - 1 ] : edge_pixels [ edge_index ] = regular_index edge_index += 1 regular_index += 1 return edge_pixels
10194	def default_permission_factory ( query_name , params ) : from invenio_stats import current_stats if current_stats . queries [ query_name ] . permission_factory is None : return AllowAllPermission else : return current_stats . queries [ query_name ] . permission_factory ( query_name , params )
7446	def _step5func ( self , samples , force , ipyclient ) : if self . _headers : print ( "\n Step 5: Consensus base calling " ) samples = _get_samples ( self , samples ) if not self . _samples_precheck ( samples , 5 , force ) : raise IPyradError ( FIRST_RUN_4 ) elif not force : if all ( [ i . stats . state >= 5 for i in samples ] ) : print ( CONSENS_EXIST . format ( len ( samples ) ) ) return assemble . consens_se . run ( self , samples , force , ipyclient )
1881	def constrain ( self , constraint ) : constraint = self . migrate_expression ( constraint ) self . _constraints . add ( constraint )
8288	def get_child_by_name ( parent , name ) : def iterate_children ( widget , name ) : if widget . get_name ( ) == name : return widget try : for w in widget . get_children ( ) : result = iterate_children ( w , name ) if result is not None : return result else : continue except AttributeError : pass return iterate_children ( parent , name )
10323	def spanning_2d_grid ( length ) : ret = nx . grid_2d_graph ( length + 2 , length ) for i in range ( length ) : ret . node [ ( 0 , i ) ] [ 'span' ] = 0 ret [ ( 0 , i ) ] [ ( 1 , i ) ] [ 'span' ] = 0 ret . node [ ( length + 1 , i ) ] [ 'span' ] = 1 ret [ ( length + 1 , i ) ] [ ( length , i ) ] [ 'span' ] = 1 return ret
11279	def _encode_ids ( * args ) : ids = [ ] for v in args : if isinstance ( v , basestring ) : qv = v . encode ( 'utf-8' ) if isinstance ( v , unicode ) else v ids . append ( urllib . quote ( qv ) ) else : qv = str ( v ) ids . append ( urllib . quote ( qv ) ) return ';' . join ( ids )
11318	def update_isbn ( self ) : isbns = record_get_field_instances ( self . record , '020' ) for field in isbns : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : field [ 0 ] [ idx ] = ( 'a' , value . replace ( "-" , "" ) . strip ( ) )
8837	def merge ( * args ) : ret = [ ] for arg in args : if isinstance ( arg , list ) or isinstance ( arg , tuple ) : ret += list ( arg ) else : ret . append ( arg ) return ret
13369	def is_home_environment ( path ) : home = unipath ( os . environ . get ( 'CPENV_HOME' , '~/.cpenv' ) ) path = unipath ( path ) return path . startswith ( home )
7054	def _check_extmodule ( module , formatkey ) : try : if os . path . exists ( module ) : sys . path . append ( os . path . dirname ( module ) ) importedok = importlib . import_module ( os . path . basename ( module . replace ( '.py' , '' ) ) ) else : importedok = importlib . import_module ( module ) except Exception as e : LOGEXCEPTION ( 'could not import the module: %s for LC format: %s. ' 'check the file path or fully qualified module name?' % ( module , formatkey ) ) importedok = False return importedok
8333	def findPrevious ( self , name = None , attrs = { } , text = None , ** kwargs ) : return self . _findOne ( self . findAllPrevious , name , attrs , text , ** kwargs )
8926	def bump ( ctx , verbose = False , pypi = False ) : cfg = config . load ( ) scm = scm_provider ( cfg . project_root , commit = False , ctx = ctx ) if not scm . workdir_is_clean ( ) : notify . warning ( "You have uncommitted changes, will create a time-stamped version!" ) pep440 = scm . pep440_dev_version ( verbose = verbose , non_local = pypi ) setup_cfg = cfg . rootjoin ( 'setup.cfg' ) if not pep440 : notify . info ( "Working directory contains a release version!" ) elif os . path . exists ( setup_cfg ) : with io . open ( setup_cfg , encoding = 'utf-8' ) as handle : data = handle . readlines ( ) changed = False for i , line in enumerate ( data ) : if re . match ( r"#? *tag_build *= *.*" , line ) : verb , _ = data [ i ] . split ( '=' , 1 ) data [ i ] = '{}= {}\n' . format ( verb , pep440 ) changed = True if changed : notify . info ( "Rewriting 'setup.cfg'..." ) with io . open ( setup_cfg , 'w' , encoding = 'utf-8' ) as handle : handle . write ( '' . join ( data ) ) else : notify . warning ( "No 'tag_build' setting found in 'setup.cfg'!" ) else : notify . warning ( "Cannot rewrite 'setup.cfg', none found!" ) if os . path . exists ( setup_cfg ) : egg_info = shell . capture ( "python setup.py egg_info" , echo = True if verbose else None ) for line in egg_info . splitlines ( ) : if line . endswith ( 'PKG-INFO' ) : pkg_info_file = line . split ( None , 1 ) [ 1 ] with io . open ( pkg_info_file , encoding = 'utf-8' ) as handle : notify . info ( '\n' . join ( i for i in handle . readlines ( ) if i . startswith ( 'Version:' ) ) . strip ( ) ) ctx . run ( "python setup.py -q develop" , echo = True if verbose else None )
1177	def search ( self , string , pos = 0 , endpos = sys . maxint ) : state = _State ( string , pos , endpos , self . flags ) if state . search ( self . _code ) : return SRE_Match ( self , state ) else : return None
6049	def set_defaults ( key ) : def decorator ( func ) : @ functools . wraps ( func ) def wrapper ( phase , new_value ) : new_value = new_value or [ ] for item in new_value : galaxy = new_value [ item ] if isinstance ( item , str ) else item galaxy . redshift = galaxy . redshift or conf . instance . general . get ( "redshift" , key , float ) return func ( phase , new_value ) return wrapper return decorator
9730	def get_force ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . plate_count ) : component_position , plate = QRTPacket . _get_exact ( RTForcePlate , data , component_position ) force_list = [ ] for _ in range ( plate . force_count ) : component_position , force = QRTPacket . _get_exact ( RTForce , data , component_position ) force_list . append ( force ) append_components ( ( plate , force_list ) ) return components
4933	def get_content_id ( self , content_metadata_item ) : content_id = content_metadata_item . get ( 'key' , '' ) if content_metadata_item [ 'content_type' ] == 'program' : content_id = content_metadata_item . get ( 'uuid' , '' ) return content_id
6868	def find_lc_timegroups ( lctimes , mingap = 4.0 ) : lc_time_diffs = np . diff ( lctimes ) group_start_indices = np . where ( lc_time_diffs > mingap ) [ 0 ] if len ( group_start_indices ) > 0 : group_indices = [ ] for i , gindex in enumerate ( group_start_indices ) : if i == 0 : group_indices . append ( slice ( 0 , gindex + 1 ) ) else : group_indices . append ( slice ( group_start_indices [ i - 1 ] + 1 , gindex + 1 ) ) group_indices . append ( slice ( group_start_indices [ - 1 ] + 1 , len ( lctimes ) ) ) else : group_indices = [ slice ( 0 , len ( lctimes ) ) ] return len ( group_indices ) , group_indices
9619	def set_value ( self , control , value = None ) : func = getattr ( _xinput , 'Set' + control ) if 'Axis' in control : target_type = c_short if self . percent : target_value = int ( 32767 * value ) else : target_value = value elif 'Btn' in control : target_type = c_bool target_value = bool ( value ) elif 'Trigger' in control : target_type = c_byte if self . percent : target_value = int ( 255 * value ) else : target_value = value elif 'Dpad' in control : target_type = c_int target_value = int ( value ) func ( c_uint ( self . id ) , target_type ( target_value ) )
1523	def log ( self , message , level = None ) : if level is None : _log_level = logging . INFO else : if level == "trace" or level == "debug" : _log_level = logging . DEBUG elif level == "info" : _log_level = logging . INFO elif level == "warn" : _log_level = logging . WARNING elif level == "error" : _log_level = logging . ERROR else : raise ValueError ( "%s is not supported as logging level" % str ( level ) ) self . logger . log ( _log_level , message )
3612	def do_filter ( qs , keywords , exclude = False ) : and_q = Q ( ) for keyword , value in iteritems ( keywords ) : try : values = value . split ( "," ) if len ( values ) > 0 : or_q = Q ( ) for value in values : or_q |= Q ( ** { keyword : value } ) and_q &= or_q except AttributeError : and_q &= Q ( ** { keyword : value } ) if exclude : qs = qs . exclude ( and_q ) else : qs = qs . filter ( and_q ) return qs
6379	def dist_manhattan ( src , tar , qval = 2 , alphabet = None ) : return Manhattan ( ) . dist ( src , tar , qval , alphabet )
4021	def _start_docker_vm ( ) : is_running = docker_vm_is_running ( ) if not is_running : log_to_client ( 'Starting docker-machine VM {}' . format ( constants . VM_MACHINE_NAME ) ) _apply_nat_dns_host_resolver ( ) _apply_nat_net_less_greedy_subnet ( ) check_and_log_output_and_error_demoted ( [ 'docker-machine' , 'start' , constants . VM_MACHINE_NAME ] , quiet_on_success = True ) return is_running
1342	def samples ( dataset = 'imagenet' , index = 0 , batchsize = 1 , shape = ( 224 , 224 ) , data_format = 'channels_last' ) : from PIL import Image images , labels = [ ] , [ ] basepath = os . path . dirname ( __file__ ) samplepath = os . path . join ( basepath , 'data' ) files = os . listdir ( samplepath ) for idx in range ( index , index + batchsize ) : i = idx % 20 file = [ n for n in files if '{}_{:02d}_' . format ( dataset , i ) in n ] [ 0 ] label = int ( file . split ( '.' ) [ 0 ] . split ( '_' ) [ - 1 ] ) path = os . path . join ( samplepath , file ) image = Image . open ( path ) if dataset == 'imagenet' : image = image . resize ( shape ) image = np . asarray ( image , dtype = np . float32 ) if dataset != 'mnist' and data_format == 'channels_first' : image = np . transpose ( image , ( 2 , 0 , 1 ) ) images . append ( image ) labels . append ( label ) labels = np . array ( labels ) images = np . stack ( images ) return images , labels
11553	def disable_analog_reporting ( self , pin ) : command = [ self . _command_handler . REPORT_ANALOG + pin , self . REPORTING_DISABLE ] self . _command_handler . send_command ( command )
3741	def omega_mixture ( omegas , zs , CASRNs = None , Method = None , AvailableMethods = False ) : r def list_methods ( ) : methods = [ ] if none_and_length_check ( [ zs , omegas ] ) : methods . append ( 'SIMPLE' ) methods . append ( 'NONE' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'SIMPLE' : _omega = mixing_simple ( zs , omegas ) elif Method == 'NONE' : _omega = None else : raise Exception ( 'Failure in in function' ) return _omega
3163	def create ( self , workflow_id , email_id , data ) : self . workflow_id = workflow_id self . email_id = email_id if 'email_address' not in data : raise KeyError ( 'The automation email queue must have an email_address' ) check_email ( data [ 'email_address' ] ) response = self . _mc_client . _post ( url = self . _build_path ( workflow_id , 'emails' , email_id , 'queue' ) , data = data ) if response is not None : self . subscriber_hash = response [ 'id' ] else : self . subscriber_hash = None return response
1080	def isocalendar ( self ) : year = self . _year week1monday = _isoweek1monday ( year ) today = _ymd2ord ( self . _year , self . _month , self . _day ) week , day = divmod ( today - week1monday , 7 ) if week < 0 : year -= 1 week1monday = _isoweek1monday ( year ) week , day = divmod ( today - week1monday , 7 ) elif week >= 52 : if today >= _isoweek1monday ( year + 1 ) : year += 1 week = 0 return year , week + 1 , day + 1
6644	def availableVersions ( self ) : r = [ ] for t in self . vcs . tags ( ) : logger . debug ( "available version tag: %s" , t ) if not len ( t . strip ( ) ) : continue try : r . append ( GitCloneVersion ( t , t , self ) ) except ValueError : logger . debug ( 'invalid version tag: %s' , t ) return r
780	def jobInsert ( self , client , cmdLine , clientInfo = '' , clientKey = '' , params = '' , alreadyRunning = False , minimumWorkers = 0 , maximumWorkers = 0 , jobType = '' , priority = DEFAULT_JOB_PRIORITY ) : jobHash = self . _normalizeHash ( uuid . uuid1 ( ) . bytes ) @ g_retrySQL def insertWithRetries ( ) : with ConnectionFactory . get ( ) as conn : return self . _insertOrGetUniqueJobNoRetries ( conn , client = client , cmdLine = cmdLine , jobHash = jobHash , clientInfo = clientInfo , clientKey = clientKey , params = params , minimumWorkers = minimumWorkers , maximumWorkers = maximumWorkers , jobType = jobType , priority = priority , alreadyRunning = alreadyRunning ) try : jobID = insertWithRetries ( ) except : self . _logger . exception ( 'jobInsert FAILED: jobType=%r; client=%r; clientInfo=%r; clientKey=%r;' 'jobHash=%r; cmdLine=%r' , jobType , client , _abbreviate ( clientInfo , 48 ) , clientKey , jobHash , cmdLine ) raise else : self . _logger . info ( 'jobInsert: returning jobID=%s. jobType=%r; client=%r; clientInfo=%r; ' 'clientKey=%r; jobHash=%r; cmdLine=%r' , jobID , jobType , client , _abbreviate ( clientInfo , 48 ) , clientKey , jobHash , cmdLine ) return jobID
8873	def isA ( instance , typeList ) : return any ( map ( lambda iType : isinstance ( instance , iType ) , typeList ) )
1405	def validate_extra_link ( self , extra_link ) : if EXTRA_LINK_NAME_KEY not in extra_link or EXTRA_LINK_FORMATTER_KEY not in extra_link : raise Exception ( "Invalid extra.links format. " + "Extra link must include a 'name' and 'formatter' field" ) self . validated_formatter ( extra_link [ EXTRA_LINK_FORMATTER_KEY ] ) return extra_link
877	def agitate ( self ) : for ( varName , var ) in self . permuteVars . iteritems ( ) : var . agitate ( ) self . newPosition ( )
8282	def _linelength ( self , x0 , y0 , x1 , y1 ) : a = pow ( abs ( x0 - x1 ) , 2 ) b = pow ( abs ( y0 - y1 ) , 2 ) return sqrt ( a + b )
10711	def models_preparing ( app ) : def wrapper ( resource , parent ) : if isinstance ( resource , DeclarativeMeta ) : resource = ListResource ( resource ) if not getattr ( resource , '__parent__' , None ) : resource . __parent__ = parent return resource resources_preparing_factory ( app , wrapper )
8153	def simple_traceback ( ex , source ) : exc_type , exc_value , exc_tb = sys . exc_info ( ) exc = traceback . format_exception ( exc_type , exc_value , exc_tb ) source_arr = source . splitlines ( ) exc_location = exc [ - 2 ] for i , err in enumerate ( exc ) : if 'exec source in ns' in err : exc_location = exc [ i + 1 ] break fn = exc_location . split ( ',' ) [ 0 ] [ 8 : - 1 ] line_number = int ( exc_location . split ( ',' ) [ 1 ] . replace ( 'line' , '' ) . strip ( ) ) err_msgs = [ ] err_where = ' ' . join ( exc [ i - 1 ] . split ( ',' ) [ 1 : ] ) . strip ( ) err_msgs . append ( 'Error in the Shoebot script at %s:' % err_where ) for i in xrange ( max ( 0 , line_number - 5 ) , line_number ) : if fn == "<string>" : line = source_arr [ i ] else : line = linecache . getline ( fn , i + 1 ) err_msgs . append ( '%s: %s' % ( i + 1 , line . rstrip ( ) ) ) err_msgs . append ( ' %s^ %s' % ( len ( str ( i ) ) * ' ' , exc [ - 1 ] . rstrip ( ) ) ) err_msgs . append ( '' ) err_msgs . append ( exc [ 0 ] . rstrip ( ) ) for err in exc [ 3 : ] : err_msgs . append ( err . rstrip ( ) ) return '\n' . join ( err_msgs )
11789	def sample ( self ) : "Return a random sample from the distribution." if self . sampler is None : self . sampler = weighted_sampler ( self . dictionary . keys ( ) , self . dictionary . values ( ) ) return self . sampler ( )
13541	def create ( self , server ) : if len ( self . geometries ) == 0 : raise Exception ( 'no geometries' ) return server . post ( 'task_admin' , self . as_payload ( ) , replacements = { 'slug' : self . __challenge__ . slug , 'identifier' : self . identifier } )
3252	def get_stores ( self , names = None , workspaces = None ) : if isinstance ( workspaces , Workspace ) : workspaces = [ workspaces ] elif isinstance ( workspaces , list ) and [ w for w in workspaces if isinstance ( w , Workspace ) ] : pass else : workspaces = self . get_workspaces ( names = workspaces ) stores = [ ] for ws in workspaces : ds_list = self . get_xml ( ws . datastore_url ) cs_list = self . get_xml ( ws . coveragestore_url ) wms_list = self . get_xml ( ws . wmsstore_url ) stores . extend ( [ datastore_from_index ( self , ws , n ) for n in ds_list . findall ( "dataStore" ) ] ) stores . extend ( [ coveragestore_from_index ( self , ws , n ) for n in cs_list . findall ( "coverageStore" ) ] ) stores . extend ( [ wmsstore_from_index ( self , ws , n ) for n in wms_list . findall ( "wmsStore" ) ] ) if names is None : names = [ ] elif isinstance ( names , basestring ) : names = [ s . strip ( ) for s in names . split ( ',' ) if s . strip ( ) ] if stores and names : return ( [ store for store in stores if store . name in names ] ) return stores
4602	def merge ( * projects ) : result = { } for project in projects : for name , section in ( project or { } ) . items ( ) : if name not in PROJECT_SECTIONS : raise ValueError ( UNKNOWN_SECTION_ERROR % name ) if section is None : result [ name ] = type ( result [ name ] ) ( ) continue if name in NOT_MERGEABLE + SPECIAL_CASE : result [ name ] = section continue if section and not isinstance ( section , ( dict , str ) ) : cname = section . __class__ . __name__ raise ValueError ( SECTION_ISNT_DICT_ERROR % ( name , cname ) ) if name == 'animation' : adesc = load . load_if_filename ( section ) if adesc : section = adesc . get ( 'animation' , { } ) section [ 'run' ] = adesc . get ( 'run' , { } ) result_section = result . setdefault ( name , { } ) section = construct . to_type ( section ) for k , v in section . items ( ) : if v is None : result_section . pop ( k , None ) else : result_section [ k ] = v return result
10575	def get_local_playlist_songs ( playlist , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False , exclude_patterns = None ) : logger . info ( "Loading local playlist songs..." ) if os . name == 'nt' and CYGPATH_RE . match ( playlist ) : playlist = convert_cygwin_path ( playlist ) filepaths = [ ] base_filepath = os . path . dirname ( os . path . abspath ( playlist ) ) with open ( playlist ) as local_playlist : for line in local_playlist . readlines ( ) : line = line . strip ( ) if line . lower ( ) . endswith ( SUPPORTED_SONG_FORMATS ) : path = line if not os . path . isabs ( path ) : path = os . path . join ( base_filepath , path ) if os . path . isfile ( path ) : filepaths . append ( path ) supported_filepaths = get_supported_filepaths ( filepaths , SUPPORTED_SONG_FORMATS ) included_songs , excluded_songs = exclude_filepaths ( supported_filepaths , exclude_patterns = exclude_patterns ) matched_songs , filtered_songs = filter_local_songs ( included_songs , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) logger . info ( "Excluded {0} local playlist songs" . format ( len ( excluded_songs ) ) ) logger . info ( "Filtered {0} local playlist songs" . format ( len ( filtered_songs ) ) ) logger . info ( "Loaded {0} local playlist songs" . format ( len ( matched_songs ) ) ) return matched_songs , filtered_songs , excluded_songs
5318	def format ( self , string , * args , ** kwargs ) : return string . format ( c = self , * args , ** kwargs )
1312	def HardwareInput ( uMsg : int , param : int = 0 ) -> INPUT : return _CreateInput ( HARDWAREINPUT ( uMsg , param & 0xFFFF , param >> 16 & 0xFFFF ) )
13705	def iter_char_block ( self , text = None , width = 60 , fmtfunc = str ) : if width < 1 : width = 1 text = ( self . text if text is None else text ) or '' text = ' ' . join ( text . split ( '\n' ) ) escapecodes = get_codes ( text ) if not escapecodes : yield from ( fmtfunc ( text [ i : i + width ] ) for i in range ( 0 , len ( text ) , width ) ) else : blockwidth = 0 block = [ ] for i , s in enumerate ( get_indices_list ( text ) ) : block . append ( s ) if len ( s ) == 1 : blockwidth += 1 if blockwidth == width : yield '' . join ( block ) block = [ ] blockwidth = 0 if block : yield '' . join ( block )
2869	def setup ( self , pin , mode , pull_up_down = PUD_OFF ) : self . rpi_gpio . setup ( pin , self . _dir_mapping [ mode ] , pull_up_down = self . _pud_mapping [ pull_up_down ] )
3427	def add_metabolites ( self , metabolite_list ) : if not hasattr ( metabolite_list , '__iter__' ) : metabolite_list = [ metabolite_list ] if len ( metabolite_list ) == 0 : return None metabolite_list = [ x for x in metabolite_list if x . id not in self . metabolites ] bad_ids = [ m for m in metabolite_list if not isinstance ( m . id , string_types ) or len ( m . id ) < 1 ] if len ( bad_ids ) != 0 : raise ValueError ( 'invalid identifiers in {}' . format ( repr ( bad_ids ) ) ) for x in metabolite_list : x . _model = self self . metabolites += metabolite_list to_add = [ ] for met in metabolite_list : if met . id not in self . constraints : constraint = self . problem . Constraint ( Zero , name = met . id , lb = 0 , ub = 0 ) to_add += [ constraint ] self . add_cons_vars ( to_add ) context = get_context ( self ) if context : context ( partial ( self . metabolites . __isub__ , metabolite_list ) ) for x in metabolite_list : context ( partial ( setattr , x , '_model' , None ) )
11134	def is_not_exist_or_allow_overwrite ( self , overwrite = False ) : if self . exists ( ) and overwrite is False : return False else : return True
6633	def sourceDirValidationError ( dirname , component_name ) : if dirname == component_name : return 'Module %s public include directory %s should not contain source files' % ( component_name , dirname ) elif dirname . lower ( ) in ( 'source' , 'src' ) and dirname != 'source' : return 'Module %s has non-standard source directory name: "%s" should be "source"' % ( component_name , dirname ) elif isPotentialTestDir ( dirname ) and dirname != 'test' : return 'Module %s has non-standard test directory name: "%s" should be "test"' % ( component_name , dirname ) elif not Source_Dir_Regex . match ( dirname ) : corrected = Source_Dir_Invalid_Regex . sub ( '' , dirname . lower ( ) ) if not corrected : corrected = 'source' return 'Module %s has non-standard source directory name: "%s" should be "%s"' % ( component_name , dirname , corrected ) else : return None
10231	def list_abundance_expansion ( graph : BELGraph ) -> None : mapping = { node : flatten_list_abundance ( node ) for node in graph if isinstance ( node , ListAbundance ) } relabel_nodes ( graph , mapping , copy = False )
13095	def callback ( self , event ) : if event . mask == 0x00000008 : if event . name . endswith ( '.json' ) : print_success ( "Ldapdomaindump file found" ) if event . name in [ 'domain_groups.json' , 'domain_users.json' ] : if event . name == 'domain_groups.json' : self . domain_groups_file = event . pathname if event . name == 'domain_users.json' : self . domain_users_file = event . pathname if self . domain_groups_file and self . domain_users_file : print_success ( "Importing users" ) subprocess . Popen ( [ 'jk-import-domaindump' , self . domain_groups_file , self . domain_users_file ] ) elif event . name == 'domain_computers.json' : print_success ( "Importing computers" ) subprocess . Popen ( [ 'jk-import-domaindump' , event . pathname ] ) self . ldap_strings = [ ] self . write_targets ( ) if event . name . endswith ( '_samhashes.sam' ) : host = event . name . replace ( '_samhashes.sam' , '' ) print_success ( "Secretsdump file, host ip: {}" . format ( host ) ) subprocess . Popen ( [ 'jk-import-secretsdump' , event . pathname ] ) self . ips . remove ( host ) self . write_targets ( )
216	def setdefault ( self , key : str , value : str ) -> str : set_key = key . lower ( ) . encode ( "latin-1" ) set_value = value . encode ( "latin-1" ) for idx , ( item_key , item_value ) in enumerate ( self . _list ) : if item_key == set_key : return item_value . decode ( "latin-1" ) self . _list . append ( ( set_key , set_value ) ) return value
7049	def massradius ( age , planetdist , coremass , mass = 'massjupiter' , radius = 'radiusjupiter' ) : MR = { 0.3 : MASSESRADII_0_3GYR , 1.0 : MASSESRADII_1_0GYR , 4.5 : MASSESRADII_4_5GYR } if age not in MR : print ( 'given age not in Fortney 2007, returning...' ) return massradius = MR [ age ] if ( planetdist in massradius ) and ( coremass in massradius [ planetdist ] ) : print ( 'getting % Gyr M-R for planet dist %s AU, ' 'core mass %s Mearth...' % ( age , planetdist , coremass ) ) massradrelation = massradius [ planetdist ] [ coremass ] outdict = { 'mass' : array ( massradrelation [ mass ] ) , 'radius' : array ( massradrelation [ radius ] ) } return outdict
581	def _setRandomEncoderResolution ( minResolution = 0.001 ) : encoder = ( model_params . MODEL_PARAMS [ "modelParams" ] [ "sensorParams" ] [ "encoders" ] [ "value" ] ) if encoder [ "type" ] == "RandomDistributedScalarEncoder" : rangePadding = abs ( _INPUT_MAX - _INPUT_MIN ) * 0.2 minValue = _INPUT_MIN - rangePadding maxValue = _INPUT_MAX + rangePadding resolution = max ( minResolution , ( maxValue - minValue ) / encoder . pop ( "numBuckets" ) ) encoder [ "resolution" ] = resolution
353	def assign_params ( sess , params , network ) : ops = [ ] for idx , param in enumerate ( params ) : ops . append ( network . all_params [ idx ] . assign ( param ) ) if sess is not None : sess . run ( ops ) return ops
7453	def collate_files ( data , sname , tmp1s , tmp2s ) : out1 = os . path . join ( data . dirs . fastqs , "{}_R1_.fastq.gz" . format ( sname ) ) out = io . BufferedWriter ( gzip . open ( out1 , 'w' ) ) cmd1 = [ 'cat' ] for tmpfile in tmp1s : cmd1 += [ tmpfile ] proc = sps . Popen ( [ 'which' , 'pigz' ] , stderr = sps . PIPE , stdout = sps . PIPE ) . communicate ( ) if proc [ 0 ] . strip ( ) : compress = [ "pigz" ] else : compress = [ "gzip" ] proc1 = sps . Popen ( cmd1 , stderr = sps . PIPE , stdout = sps . PIPE ) proc2 = sps . Popen ( compress , stdin = proc1 . stdout , stderr = sps . PIPE , stdout = out ) err = proc2 . communicate ( ) if proc2 . returncode : raise IPyradWarningExit ( "error in collate_files R1 %s" , err ) proc1 . stdout . close ( ) out . close ( ) for tmpfile in tmp1s : os . remove ( tmpfile ) if 'pair' in data . paramsdict [ "datatype" ] : out2 = os . path . join ( data . dirs . fastqs , "{}_R2_.fastq.gz" . format ( sname ) ) out = io . BufferedWriter ( gzip . open ( out2 , 'w' ) ) cmd1 = [ 'cat' ] for tmpfile in tmp2s : cmd1 += [ tmpfile ] proc1 = sps . Popen ( cmd1 , stderr = sps . PIPE , stdout = sps . PIPE ) proc2 = sps . Popen ( compress , stdin = proc1 . stdout , stderr = sps . PIPE , stdout = out ) err = proc2 . communicate ( ) if proc2 . returncode : raise IPyradWarningExit ( "error in collate_files R2 %s" , err ) proc1 . stdout . close ( ) out . close ( ) for tmpfile in tmp2s : os . remove ( tmpfile )
5156	def _get_install_context ( self ) : config = self . config l2vpn = [ ] for vpn in self . config . get ( 'openvpn' , [ ] ) : if vpn . get ( 'dev_type' ) != 'tap' : continue tap = vpn . copy ( ) l2vpn . append ( tap ) bridges = [ ] for interface in self . config . get ( 'interfaces' , [ ] ) : if interface [ 'type' ] != 'bridge' : continue bridge = interface . copy ( ) if bridge . get ( 'addresses' ) : bridge [ 'proto' ] = interface [ 'addresses' ] [ 0 ] . get ( 'proto' ) bridge [ 'ip' ] = interface [ 'addresses' ] [ 0 ] . get ( 'address' ) bridges . append ( bridge ) cron = False for _file in config . get ( 'files' , [ ] ) : path = _file [ 'path' ] if path . startswith ( '/crontabs' ) or path . startswith ( 'crontabs' ) : cron = True break return dict ( hostname = config [ 'general' ] [ 'hostname' ] , l2vpn = l2vpn , bridges = bridges , radios = config . get ( 'radios' , [ ] ) , cron = cron )
10588	def _get_account_and_descendants_ ( self , account , result ) : result . append ( account ) for child in account . accounts : self . _get_account_and_descendants_ ( child , result )
5904	def grompp_qtot ( * args , ** kwargs ) : qtot_pattern = re . compile ( 'System has non-zero total charge: *(?P<qtot>[-+]?\d*\.\d+([eE][-+]\d+)?)' ) kwargs [ 'stdout' ] = False kwargs [ 'stderr' ] = False rc , output , error = grompp_warnonly ( * args , ** kwargs ) gmxoutput = "\n" . join ( [ x for x in [ output , error ] if x is not None ] ) if rc != 0 : msg = "grompp_qtot() failed. See warning and screen output for clues." logger . error ( msg ) import sys sys . stderr . write ( "=========== grompp (stdout/stderr) ============\n" ) sys . stderr . write ( gmxoutput ) sys . stderr . write ( "===============================================\n" ) sys . stderr . flush ( ) raise GromacsError ( rc , msg ) qtot = 0 for line in gmxoutput . split ( '\n' ) : m = qtot_pattern . search ( line ) if m : qtot = float ( m . group ( 'qtot' ) ) break logger . info ( "system total charge qtot = {qtot!r}" . format ( ** vars ( ) ) ) return qtot
12959	def _peekNextID ( self , conn = None ) : if conn is None : conn = self . _get_connection ( ) return to_unicode ( conn . get ( self . _get_next_id_key ( ) ) or 0 )
2768	def get_volume ( self , volume_id ) : return Volume . get_object ( api_token = self . token , volume_id = volume_id )
8762	def _perform_async_update_rule ( context , id , db_sg_group , rule_id , action ) : rpc_reply = None sg_rpc = sg_rpc_api . QuarkSGAsyncProcessClient ( ) ports = db_api . sg_gather_associated_ports ( context , db_sg_group ) if len ( ports ) > 0 : rpc_reply = sg_rpc . start_update ( context , id , rule_id , action ) if rpc_reply : job_id = rpc_reply [ 'job_id' ] job_api . add_job_to_context ( context , job_id ) else : LOG . error ( "Async update failed. Is the worker running?" )
5997	def plot_points ( points_arcsec , array , units , kpc_per_arcsec , pointsize , zoom_offset_arcsec ) : if points_arcsec is not None : points_arcsec = list ( map ( lambda position_set : np . asarray ( position_set ) , points_arcsec ) ) point_colors = itertools . cycle ( [ "m" , "y" , "r" , "w" , "c" , "b" , "g" , "k" ] ) for point_set_arcsec in points_arcsec : if zoom_offset_arcsec is not None : point_set_arcsec -= zoom_offset_arcsec point_set_units = convert_grid_units ( array = array , grid_arcsec = point_set_arcsec , units = units , kpc_per_arcsec = kpc_per_arcsec ) plt . scatter ( y = point_set_units [ : , 0 ] , x = point_set_units [ : , 1 ] , color = next ( point_colors ) , s = pointsize )
10213	def calculate_subgraph_edge_overlap ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Tuple [ Mapping [ str , EdgeSet ] , Mapping [ str , Mapping [ str , EdgeSet ] ] , Mapping [ str , Mapping [ str , EdgeSet ] ] , Mapping [ str , Mapping [ str , float ] ] , ] : sg2edge = defaultdict ( set ) for u , v , d in graph . edges ( data = True ) : if not edge_has_annotation ( d , annotation ) : continue sg2edge [ d [ ANNOTATIONS ] [ annotation ] ] . add ( ( u , v ) ) subgraph_intersection = defaultdict ( dict ) subgraph_union = defaultdict ( dict ) result = defaultdict ( dict ) for sg1 , sg2 in itt . product ( sg2edge , repeat = 2 ) : subgraph_intersection [ sg1 ] [ sg2 ] = sg2edge [ sg1 ] & sg2edge [ sg2 ] subgraph_union [ sg1 ] [ sg2 ] = sg2edge [ sg1 ] | sg2edge [ sg2 ] result [ sg1 ] [ sg2 ] = len ( subgraph_intersection [ sg1 ] [ sg2 ] ) / len ( subgraph_union [ sg1 ] [ sg2 ] ) return sg2edge , subgraph_intersection , subgraph_union , result
275	def sample_colormap ( cmap_name , n_samples ) : colors = [ ] colormap = cm . cmap_d [ cmap_name ] for i in np . linspace ( 0 , 1 , n_samples ) : colors . append ( colormap ( i ) ) return colors
13905	def match_part ( string , part ) : if not string or not re . match ( '^(' + PARTS [ part ] + ')$' , string ) : raise ValueError ( '{} should match {}' . format ( part , PARTS [ part ] ) )
145	def deepcopy ( self , exterior = None , label = None ) : return Polygon ( exterior = np . copy ( self . exterior ) if exterior is None else exterior , label = self . label if label is None else label )
13119	def count ( self , * args , ** kwargs ) : search = self . create_search ( * args , ** kwargs ) try : return search . count ( ) except NotFoundError : print_error ( "The index was not found, have you initialized the index?" ) except ( ConnectionError , TransportError ) : print_error ( "Cannot connect to elasticsearch" )
5349	def compose_title ( projects , data ) : for project in data : projects [ project ] = { 'meta' : { 'title' : data [ project ] [ 'title' ] } } return projects
2478	def reset ( self ) : self . reset_creation_info ( ) self . reset_document ( ) self . reset_package ( ) self . reset_file_stat ( ) self . reset_reviews ( ) self . reset_annotations ( ) self . reset_extr_lics ( )
10567	def _check_filters ( song , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False ) : include = True if include_filters : if all_includes : if not all ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in include_filters ) : include = False else : if not any ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in include_filters ) : include = False if exclude_filters : if all_excludes : if all ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in exclude_filters ) : include = False else : if any ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in exclude_filters ) : include = False return include
5472	def prepare_output ( self , row ) : date_fields = [ 'last-update' , 'create-time' , 'start-time' , 'end-time' ] int_fields = [ 'task-attempt' ] for col in date_fields : if col in row : row [ col ] = self . default_format_date ( row [ col ] ) for col in int_fields : if col in row and row [ col ] is not None : row [ col ] = int ( row [ col ] ) return row
11702	def cosine ( vec1 , vec2 ) : if norm ( vec1 ) > 0 and norm ( vec2 ) > 0 : return dot ( vec1 , vec2 ) / ( norm ( vec1 ) * norm ( vec2 ) ) else : return 0.0
7803	def display_name ( self ) : if self . subject_name : return u", " . join ( [ u", " . join ( [ u"{0}={1}" . format ( k , v ) for k , v in dn_tuple ] ) for dn_tuple in self . subject_name ] ) for name_type in ( "XmppAddr" , "DNS" , "SRV" ) : names = self . alt_names . get ( name_type ) if names : return names [ 0 ] return u"<unknown>"
5827	def dataset_search ( self , dataset_returning_query ) : self . _validate_search_query ( dataset_returning_query ) return self . _execute_search_query ( dataset_returning_query , DatasetSearchResult )
9657	def get_direct_ancestors ( G , list_of_nodes ) : parents = [ ] for item in list_of_nodes : anc = G . predecessors ( item ) for one in anc : parents . append ( one ) return parents
12415	def send ( self , * args , ** kwargs ) : self . write ( * args , ** kwargs ) self . flush ( )
11308	def get_image ( self , obj ) : if self . _meta . image_field : return getattr ( obj , self . _meta . image_field )
7626	def transcription ( ref , est , ** kwargs ) : r namespace = 'pitch_contour' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_intervals , ref_p = ref . to_interval_values ( ) est_intervals , est_p = est . to_interval_values ( ) ref_pitches = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in ref_p ] ) est_pitches = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in est_p ] ) return mir_eval . transcription . evaluate ( ref_intervals , ref_pitches , est_intervals , est_pitches , ** kwargs )
9953	def _get_node ( name : str , args : str ) : obj = get_object ( name ) args = ast . literal_eval ( args ) if not isinstance ( args , tuple ) : args = ( args , ) return obj . node ( * args )
11412	def record_get_field ( rec , tag , field_position_global = None , field_position_local = None ) : if field_position_global is None and field_position_local is None : raise InvenioBibRecordFieldError ( "A field position is required to " "complete this operation." ) elif field_position_global is not None and field_position_local is not None : raise InvenioBibRecordFieldError ( "Only one field position is required " "to complete this operation." ) elif field_position_global : if tag not in rec : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) for field in rec [ tag ] : if field [ 4 ] == field_position_global : return field raise InvenioBibRecordFieldError ( "No field has the tag '%s' and the " "global field position '%d'." % ( tag , field_position_global ) ) else : try : return rec [ tag ] [ field_position_local ] except KeyError : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) except IndexError : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the local field position '%d'." % ( tag , field_position_local ) )
10256	def get_causal_central_nodes ( graph : BELGraph , func : str ) -> Set [ BaseEntity ] : return { node for node in graph if node . function == func and is_causal_central ( graph , node ) }
10990	def finish_state ( st , desc = 'finish-state' , invert = 'guess' ) : for minmass in [ None , 0 ] : for _ in range ( 3 ) : npart , poses = addsub . add_subtract_locally ( st , region_depth = 7 , minmass = minmass , invert = invert ) if npart == 0 : break opt . finish ( st , n_loop = 1 , separate_psf = True , desc = desc , dowarn = False ) opt . burn ( st , mode = 'polish' , desc = desc , n_loop = 2 , dowarn = False ) d = opt . finish ( st , desc = desc , n_loop = 4 , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' )
9304	def parse_date ( date_str ) : months = [ 'jan' , 'feb' , 'mar' , 'apr' , 'may' , 'jun' , 'jul' , 'aug' , 'sep' , 'oct' , 'nov' , 'dec' ] formats = { r'^(?:\w{3}, )?(\d{2}) (\w{3}) (\d{4})\D.*$' : lambda m : '{}-{:02d}-{}' . format ( m . group ( 3 ) , months . index ( m . group ( 2 ) . lower ( ) ) + 1 , m . group ( 1 ) ) , r'^\w+day, (\d{2})-(\w{3})-(\d{2})\D.*$' : lambda m : '{}{}-{:02d}-{}' . format ( str ( datetime . date . today ( ) . year ) [ : 2 ] , m . group ( 3 ) , months . index ( m . group ( 2 ) . lower ( ) ) + 1 , m . group ( 1 ) ) , r'^\w{3} (\w{3}) (\d{1,2}) \d{2}:\d{2}:\d{2} (\d{4})$' : lambda m : '{}-{:02d}-{:02d}' . format ( m . group ( 3 ) , months . index ( m . group ( 1 ) . lower ( ) ) + 1 , int ( m . group ( 2 ) ) ) , r'^(\d{4})(\d{2})(\d{2})T\d{6}Z$' : lambda m : '{}-{}-{}' . format ( * m . groups ( ) ) , r'^(\d{4}-\d{2}-\d{2})(?:[Tt].*)?$' : lambda m : m . group ( 1 ) , } out_date = None for regex , xform in formats . items ( ) : m = re . search ( regex , date_str ) if m : out_date = xform ( m ) break if out_date is None : raise DateFormatError else : return out_date
11415	def record_delete_subfield_from ( rec , tag , subfield_position , field_position_global = None , field_position_local = None ) : subfields = record_get_subfields ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) try : del subfields [ subfield_position ] except IndexError : raise InvenioBibRecordFieldError ( "The record does not contain the subfield " "'%(subfieldIndex)s' inside the field (local: " "'%(fieldIndexLocal)s, global: '%(fieldIndexGlobal)s' ) of tag " "'%(tag)s'." % { "subfieldIndex" : subfield_position , "fieldIndexLocal" : str ( field_position_local ) , "fieldIndexGlobal" : str ( field_position_global ) , "tag" : tag } ) if not subfields : if field_position_global is not None : for position , field in enumerate ( rec [ tag ] ) : if field [ 4 ] == field_position_global : del rec [ tag ] [ position ] else : del rec [ tag ] [ field_position_local ] if not rec [ tag ] : del rec [ tag ]
5093	def refresh_maps ( self ) : for robot in self . robots : resp2 = ( requests . get ( urljoin ( self . ENDPOINT , 'users/me/robots/{}/maps' . format ( robot . serial ) ) , headers = self . _headers ) ) resp2 . raise_for_status ( ) self . _maps . update ( { robot . serial : resp2 . json ( ) } )
5467	def get_action_image ( op , name ) : action = _get_action_by_name ( op , name ) if action : return action . get ( 'imageUri' )
4931	def transform_courserun_description ( self , content_metadata_item ) : description_with_locales = [ ] content_metadata_language_code = transform_language_code ( content_metadata_item . get ( 'content_language' , '' ) ) for locale in self . enterprise_configuration . get_locales ( default_locale = content_metadata_language_code ) : description_with_locales . append ( { 'locale' : locale , 'value' : ( content_metadata_item [ 'full_description' ] or content_metadata_item [ 'short_description' ] or content_metadata_item [ 'title' ] or '' ) } ) return description_with_locales
6647	def baseTargetSpec ( self ) : inherits = self . description . get ( 'inherits' , { } ) if len ( inherits ) == 1 : name , version_req = list ( inherits . items ( ) ) [ 0 ] shrinkwrap_version_req = self . getShrinkwrapMapping ( 'targets' ) . get ( name , None ) if shrinkwrap_version_req is not None : logger . debug ( 'respecting shrinkwrap version %s for %s' , shrinkwrap_version_req , name ) return pack . DependencySpec ( name , version_req , shrinkwrap_version_req = shrinkwrap_version_req ) elif len ( inherits ) > 1 : logger . error ( 'target %s specifies multiple base targets, but only one is allowed' , self . getName ( ) ) return None
439	def print_params ( self , details = True , session = None ) : for i , p in enumerate ( self . all_params ) : if details : try : val = p . eval ( session = session ) logging . info ( " param {:3}: {:20} {:15} {} (mean: {:<18}, median: {:<18}, std: {:<18}) " . format ( i , p . name , str ( val . shape ) , p . dtype . name , val . mean ( ) , np . median ( val ) , val . std ( ) ) ) except Exception as e : logging . info ( str ( e ) ) raise Exception ( "Hint: print params details after tl.layers.initialize_global_variables(sess) " "or use network.print_params(False)." ) else : logging . info ( " param {:3}: {:20} {:15} {}" . format ( i , p . name , str ( p . get_shape ( ) ) , p . dtype . name ) ) logging . info ( " num of params: %d" % self . count_params ( ) )
5194	def Process ( self , info , values ) : visitor_class_types = { opendnp3 . ICollectionIndexedBinary : VisitorIndexedBinary , opendnp3 . ICollectionIndexedDoubleBitBinary : VisitorIndexedDoubleBitBinary , opendnp3 . ICollectionIndexedCounter : VisitorIndexedCounter , opendnp3 . ICollectionIndexedFrozenCounter : VisitorIndexedFrozenCounter , opendnp3 . ICollectionIndexedAnalog : VisitorIndexedAnalog , opendnp3 . ICollectionIndexedBinaryOutputStatus : VisitorIndexedBinaryOutputStatus , opendnp3 . ICollectionIndexedAnalogOutputStatus : VisitorIndexedAnalogOutputStatus , opendnp3 . ICollectionIndexedTimeAndInterval : VisitorIndexedTimeAndInterval } visitor_class = visitor_class_types [ type ( values ) ] visitor = visitor_class ( ) values . Foreach ( visitor ) for index , value in visitor . index_and_value : log_string = 'SOEHandler.Process {0}\theaderIndex={1}\tdata_type={2}\tindex={3}\tvalue={4}' _log . debug ( log_string . format ( info . gv , info . headerIndex , type ( values ) . __name__ , index , value ) )
12369	def rename ( self , id , name ) : return super ( DomainRecords , self ) . update ( id , name = name ) [ self . singular ]
1338	def crossentropy ( label , logits ) : assert logits . ndim == 1 logits = logits - np . max ( logits ) e = np . exp ( logits ) s = np . sum ( e ) ce = np . log ( s ) - logits [ label ] return ce
3803	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : ks = [ i ( T , P ) for i in self . ThermalConductivityLiquids ] return mixing_simple ( zs , ks ) elif method == DIPPR_9H : ks = [ i ( T , P ) for i in self . ThermalConductivityLiquids ] return DIPPR9H ( ws , ks ) elif method == FILIPPOV : ks = [ i ( T , P ) for i in self . ThermalConductivityLiquids ] return Filippov ( ws , ks ) elif method == MAGOMEDOV : k_w = self . ThermalConductivityLiquids [ self . index_w ] ( T , P ) ws = list ( ws ) ws . pop ( self . index_w ) return thermal_conductivity_Magomedov ( T , P , ws , self . wCASs , k_w ) else : raise Exception ( 'Method not valid' )
5064	def update_query_parameters ( url , query_parameters ) : scheme , netloc , path , query_string , fragment = urlsplit ( url ) url_params = parse_qs ( query_string ) url_params . update ( query_parameters ) return urlunsplit ( ( scheme , netloc , path , urlencode ( sorted ( url_params . items ( ) ) , doseq = True ) , fragment ) , )
10385	def get_walks_exhaustive ( graph , node , length ) : if 0 == length : return ( node , ) , return tuple ( ( node , key ) + path for neighbor in graph . edge [ node ] for path in get_walks_exhaustive ( graph , neighbor , length - 1 ) if node not in path for key in graph . edge [ node ] [ neighbor ] )
7951	def wait_for_readability ( self ) : with self . lock : while True : if self . _socket is None or self . _eof : return False if self . _state in ( "connected" , "closing" ) : return True if self . _state == "tls-handshake" and self . _tls_state == "want_read" : return True self . _state_cond . wait ( )
3407	def eval_gpr ( expr , knockouts ) : if isinstance ( expr , Expression ) : return eval_gpr ( expr . body , knockouts ) elif isinstance ( expr , Name ) : return expr . id not in knockouts elif isinstance ( expr , BoolOp ) : op = expr . op if isinstance ( op , Or ) : return any ( eval_gpr ( i , knockouts ) for i in expr . values ) elif isinstance ( op , And ) : return all ( eval_gpr ( i , knockouts ) for i in expr . values ) else : raise TypeError ( "unsupported operation " + op . __class__ . __name__ ) elif expr is None : return True else : raise TypeError ( "unsupported operation " + repr ( expr ) )
2005	def _serialize_uint ( value , size = 32 , padding = 0 ) : if size <= 0 or size > 32 : raise ValueError from . account import EVMAccount if not isinstance ( value , ( int , BitVec , EVMAccount ) ) : raise ValueError if issymbolic ( value ) : bytes = ArrayVariable ( index_bits = 256 , index_max = 32 , value_bits = 8 , name = 'temp{}' . format ( uuid . uuid1 ( ) ) ) if value . size <= size * 8 : value = Operators . ZEXTEND ( value , size * 8 ) else : value = Operators . EXTRACT ( value , 0 , size * 8 ) bytes = ArrayProxy ( bytes . write_BE ( padding , value , size ) ) else : value = int ( value ) bytes = bytearray ( ) for _ in range ( padding ) : bytes . append ( 0 ) for position in reversed ( range ( size ) ) : bytes . append ( Operators . EXTRACT ( value , position * 8 , 8 ) ) assert len ( bytes ) == size + padding return bytes
12302	def validate ( repo , validator_name = None , filename = None , rulesfiles = None , args = [ ] ) : mgr = plugins_get_mgr ( ) validator_specs = instantiate ( repo , validator_name , filename , rulesfiles ) allresults = [ ] for v in validator_specs : keys = mgr . search ( what = 'validator' , name = v ) [ 'validator' ] for k in keys : validator = mgr . get_by_key ( 'validator' , k ) result = validator . evaluate ( repo , validator_specs [ v ] , args ) allresults . extend ( result ) return allresults
1233	def from_spec ( spec , kwargs = None ) : distribution = util . get_object ( obj = spec , predefined_objects = tensorforce . core . distributions . distributions , kwargs = kwargs ) assert isinstance ( distribution , Distribution ) return distribution
412	def save_model ( self , network = None , model_name = 'model' , ** kwargs ) : kwargs . update ( { 'model_name' : model_name } ) self . _fill_project_info ( kwargs ) params = network . get_all_params ( ) s = time . time ( ) kwargs . update ( { 'architecture' : network . all_graphs , 'time' : datetime . utcnow ( ) } ) try : params_id = self . model_fs . put ( self . _serialization ( params ) ) kwargs . update ( { 'params_id' : params_id , 'time' : datetime . utcnow ( ) } ) self . db . Model . insert_one ( kwargs ) print ( "[Database] Save model: SUCCESS, took: {}s" . format ( round ( time . time ( ) - s , 2 ) ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) print ( "[Database] Save model: FAIL" ) return False
11963	def _dec_to_dot ( ip ) : first = int ( ( ip >> 24 ) & 255 ) second = int ( ( ip >> 16 ) & 255 ) third = int ( ( ip >> 8 ) & 255 ) fourth = int ( ip & 255 ) return '%d.%d.%d.%d' % ( first , second , third , fourth )
6498	def remove ( self , doc_type , doc_ids , ** kwargs ) : try : actions = [ ] for doc_id in doc_ids : log . debug ( "Removing document of type %s and index %s" , doc_type , doc_id ) action = { '_op_type' : 'delete' , "_index" : self . index_name , "_type" : doc_type , "_id" : doc_id } actions . append ( action ) bulk ( self . _es , actions , ** kwargs ) except BulkIndexError as ex : valid_errors = [ error for error in ex . errors if error [ 'delete' ] [ 'status' ] != 404 ] if valid_errors : log . exception ( "An error occurred while removing documents from the index." ) raise
7648	def deprecated ( version , version_removed ) : def __wrapper ( func , * args , ** kwargs ) : code = six . get_function_code ( func ) warnings . warn_explicit ( "{:s}.{:s}\n\tDeprecated as of JAMS version {:s}." "\n\tIt will be removed in JAMS version {:s}." . format ( func . __module__ , func . __name__ , version , version_removed ) , category = DeprecationWarning , filename = code . co_filename , lineno = code . co_firstlineno + 1 ) return func ( * args , ** kwargs ) return decorator ( __wrapper )
6562	def iter_complete_graphs ( start , stop , factory = None ) : _ , nodes = start nodes = list ( nodes ) if factory is None : factory = count ( ) while len ( nodes ) < stop : G = nx . complete_graph ( nodes ) yield G v = next ( factory ) while v in G : v = next ( factory ) nodes . append ( v )
6591	def receive ( self ) : ret = [ ] while True : if self . runid_pkgidx_map : self . runid_to_return . extend ( self . dispatcher . poll ( ) ) ret . extend ( self . _collect_all_finished_pkgidx_result_pairs ( ) ) if not self . runid_pkgidx_map : break time . sleep ( self . sleep ) ret = sorted ( ret , key = itemgetter ( 0 ) ) return ret
7665	def to_html ( self , max_rows = None ) : n = len ( self . data ) div_id = _get_divid ( self ) out = r . format ( div_id , self . namespace , n ) out += r . format ( div_id ) out += r . format ( self . annotation_metadata . _repr_html_ ( ) ) out += r . format ( self . sandbox . _repr_html_ ( ) ) out += r . format ( self . namespace , n ) out += r if max_rows is None or n <= max_rows : out += self . _fmt_rows ( 0 , n ) else : out += self . _fmt_rows ( 0 , max_rows // 2 ) out += r out += self . _fmt_rows ( n - max_rows // 2 , n ) out += r out += r out += r return out
5500	def get_tweets ( self , url , limit = None ) : try : tweets = self . cache [ url ] [ "tweets" ] self . mark_updated ( ) return sorted ( tweets , reverse = True ) [ : limit ] except KeyError : return [ ]
5545	def pyramid ( input_raster , output_dir , pyramid_type = None , output_format = None , resampling_method = None , scale_method = None , zoom = None , bounds = None , overwrite = False , debug = False ) : bounds = bounds if bounds else None options = dict ( pyramid_type = pyramid_type , scale_method = scale_method , output_format = output_format , resampling = resampling_method , zoom = zoom , bounds = bounds , overwrite = overwrite ) raster2pyramid ( input_raster , output_dir , options )
12281	def run ( self , cmd , * args ) : if self . manager is None : raise Exception ( "Fatal internal error: Missing repository manager" ) if cmd not in dir ( self . manager ) : raise Exception ( "Fatal internal error: Invalid command {} being run" . format ( cmd ) ) func = getattr ( self . manager , cmd ) repo = self return func ( repo , * args )
10464	def verifymenucheck ( self , window_name , object_name ) : try : menu_handle = self . _get_menu_handle ( window_name , object_name , False ) try : if menu_handle . AXMenuItemMarkChar : return 1 except atomac . _a11y . Error : pass except LdtpServerException : pass return 0
12570	def _fill_missing_values ( df , range_values , fill_value = 0 , fill_method = None ) : idx_colnames = df . index . names idx_colranges = [ range_values [ x ] for x in idx_colnames ] fullindex = pd . Index ( [ p for p in product ( * idx_colranges ) ] , name = tuple ( idx_colnames ) ) fulldf = df . reindex ( index = fullindex , fill_value = fill_value , method = fill_method ) fulldf . index . names = idx_colnames return fulldf , idx_colranges
1778	def OR ( cpu , dest , src ) : res = dest . write ( dest . read ( ) | src . read ( ) ) cpu . _calculate_logic_flags ( dest . size , res )
2760	def get_load_balancer ( self , id ) : return LoadBalancer . get_object ( api_token = self . token , id = id )
6629	def read ( self , filenames ) : for fn in filenames : try : self . configs [ fn ] = ordered_json . load ( fn ) except IOError : self . configs [ fn ] = OrderedDict ( ) except Exception as e : self . configs [ fn ] = OrderedDict ( ) logging . warning ( "Failed to read settings file %s, it will be ignored. The error was: %s" , fn , e )
726	def addNoise ( self , bits , amount ) : newBits = set ( ) for bit in bits : if self . _random . getReal64 ( ) < amount : newBits . add ( self . _random . getUInt32 ( self . _n ) ) else : newBits . add ( bit ) return newBits
2264	def dict_union ( * args ) : if not args : return { } else : dictclass = OrderedDict if isinstance ( args [ 0 ] , OrderedDict ) else dict return dictclass ( it . chain . from_iterable ( d . items ( ) for d in args ) )
813	def read ( cls , proto ) : tm = super ( TemporalMemoryMonitorMixin , cls ) . read ( proto ) tm . mmName = None tm . _mmTraces = None tm . _mmData = None tm . mmClearHistory ( ) tm . _mmResetActive = True return tm
5058	def get_notification_subject_line ( course_name , template_configuration = None ) : stock_subject_template = _ ( 'You\'ve been enrolled in {course_name}!' ) default_subject_template = getattr ( settings , 'ENTERPRISE_ENROLLMENT_EMAIL_DEFAULT_SUBJECT_LINE' , stock_subject_template , ) if template_configuration is not None and template_configuration . subject_line : final_subject_template = template_configuration . subject_line else : final_subject_template = default_subject_template try : return final_subject_template . format ( course_name = course_name ) except KeyError : pass try : return default_subject_template . format ( course_name = course_name ) except KeyError : return stock_subject_template . format ( course_name = course_name )
10534	def delete_project ( project_id ) : try : res = _pybossa_req ( 'delete' , 'project' , project_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : raise
7285	def has_add_permission ( self , request ) : return request . user . is_authenticated and request . user . is_active and request . user . is_staff
689	def removeAllRecords ( self ) : for field in self . fields : field . encodings , field . values = [ ] , [ ] field . numRecords , field . numEncodings = ( 0 , 0 )
11480	def _upload_as_item ( local_file , parent_folder_id , file_path , reuse_existing = False ) : current_item_id = _create_or_reuse_item ( local_file , parent_folder_id , reuse_existing ) _create_bitstream ( file_path , local_file , current_item_id ) for callback in session . item_upload_callbacks : callback ( session . communicator , session . token , current_item_id )
13724	def log_post ( self , url = None , credentials = None , do_verify_certificate = True ) : if url is None : url = self . url if credentials is None : credentials = self . credentials if do_verify_certificate is None : do_verify_certificate = self . do_verify_certificate if credentials and "base64" in credentials : headers = { "Content-Type" : "application/json" , 'Authorization' : 'Basic %s' % credentials [ "base64" ] } else : headers = { "Content-Type" : "application/json" } try : request = requests . post ( url , headers = headers , data = self . store . get_json ( ) , verify = do_verify_certificate ) except httplib . IncompleteRead as e : request = e . partial
9104	def dropbox_post_factory ( request ) : try : max_age = int ( request . registry . settings . get ( 'post_token_max_age_seconds' ) ) except Exception : max_age = 300 try : drop_id = parse_post_token ( token = request . matchdict [ 'token' ] , secret = request . registry . settings [ 'post_secret' ] , max_age = max_age ) except SignatureExpired : raise HTTPGone ( 'dropbox expired' ) except Exception : raise HTTPNotFound ( 'no such dropbox' ) dropbox = request . registry . settings [ 'dropbox_container' ] . get_dropbox ( drop_id ) if dropbox . status_int >= 20 : raise HTTPGone ( 'dropbox already in processing, no longer accepts data' ) return dropbox
3752	def STEL ( CASRN , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in _OntarioExposureLimits and ( _OntarioExposureLimits [ CASRN ] [ "STEL (ppm)" ] or _OntarioExposureLimits [ CASRN ] [ "STEL (mg/m^3)" ] ) : methods . append ( ONTARIO ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ONTARIO : if _OntarioExposureLimits [ CASRN ] [ "STEL (ppm)" ] : _STEL = ( _OntarioExposureLimits [ CASRN ] [ "STEL (ppm)" ] , 'ppm' ) elif _OntarioExposureLimits [ CASRN ] [ "STEL (mg/m^3)" ] : _STEL = ( _OntarioExposureLimits [ CASRN ] [ "STEL (mg/m^3)" ] , 'mg/m^3' ) elif Method == NONE : _STEL = None else : raise Exception ( 'Failure in in function' ) return _STEL
1277	def tf_step ( self , x , iteration , deltas , improvement , last_improvement , estimated_improvement ) : x , next_iteration , deltas , improvement , last_improvement , estimated_improvement = super ( LineSearch , self ) . tf_step ( x , iteration , deltas , improvement , last_improvement , estimated_improvement ) next_x = [ t + delta for t , delta in zip ( x , deltas ) ] if self . mode == 'linear' : next_deltas = deltas next_estimated_improvement = estimated_improvement + self . estimated_incr elif self . mode == 'exponential' : next_deltas = [ delta * self . parameter for delta in deltas ] next_estimated_improvement = estimated_improvement * self . parameter target_value = self . fn_x ( next_deltas ) next_improvement = tf . divide ( x = ( target_value - self . base_value ) , y = tf . maximum ( x = next_estimated_improvement , y = util . epsilon ) ) return next_x , next_iteration , next_deltas , next_improvement , improvement , next_estimated_improvement
11359	def add_nations_field ( authors_subfields ) : from . config import NATIONS_DEFAULT_MAP result = [ ] for field in authors_subfields : if field [ 0 ] == 'v' : values = [ x . replace ( '.' , '' ) for x in field [ 1 ] . split ( ', ' ) ] possible_affs = filter ( lambda x : x is not None , map ( NATIONS_DEFAULT_MAP . get , values ) ) if 'CERN' in possible_affs and 'Switzerland' in possible_affs : possible_affs = [ x for x in possible_affs if x != 'Switzerland' ] result . extend ( possible_affs ) result = sorted ( list ( set ( result ) ) ) if result : authors_subfields . extend ( [ ( 'w' , res ) for res in result ] ) else : authors_subfields . append ( ( 'w' , 'HUMAN CHECK' ) )
8060	def do_fullscreen ( self , line ) : self . bot . canvas . sink . trigger_fullscreen_action ( True ) print ( self . response_prompt , file = self . stdout )
12447	def render_to_string ( self ) : values = '' for key , value in self . items ( ) : values += '{}={};' . format ( key , value ) return values
5812	def raise_expired_not_yet_valid ( certificate ) : validity = certificate [ 'tbs_certificate' ] [ 'validity' ] not_after = validity [ 'not_after' ] . native not_before = validity [ 'not_before' ] . native now = datetime . now ( timezone . utc ) if not_before > now : formatted_before = not_before . strftime ( '%Y-%m-%d %H:%M:%SZ' ) message = 'Server certificate verification failed - certificate not valid until %s' % formatted_before elif not_after < now : formatted_after = not_after . strftime ( '%Y-%m-%d %H:%M:%SZ' ) message = 'Server certificate verification failed - certificate expired %s' % formatted_after raise TLSVerificationError ( message , certificate )
1855	def BSF ( cpu , dest , src ) : value = src . read ( ) flag = Operators . EXTRACT ( value , 0 , 1 ) == 1 res = 0 for pos in range ( 1 , src . size ) : res = Operators . ITEBV ( dest . size , flag , res , pos ) flag = Operators . OR ( flag , Operators . EXTRACT ( value , pos , 1 ) == 1 ) cpu . ZF = value == 0 dest . write ( Operators . ITEBV ( dest . size , cpu . ZF , dest . read ( ) , res ) )
13855	def getTextFromNode ( node ) : t = "" for n in node . childNodes : if n . nodeType == n . TEXT_NODE : t += n . nodeValue else : raise NotTextNodeError return t
2872	def all_info_files ( self ) : 'Returns a generator of "Path"s' try : for info_file in list_files_in_dir ( self . info_dir ) : if not os . path . basename ( info_file ) . endswith ( '.trashinfo' ) : self . on_non_trashinfo_found ( ) else : yield info_file except OSError : pass
6979	def filter_kepler_lcdict ( lcdict , filterflags = True , nanfilter = 'sap,pdc' , timestoignore = None ) : cols = lcdict [ 'columns' ] if filterflags : nbefore = lcdict [ 'time' ] . size filterind = lcdict [ 'sap_quality' ] == 0 for col in cols : if '.' in col : key , subkey = col . split ( '.' ) lcdict [ key ] [ subkey ] = lcdict [ key ] [ subkey ] [ filterind ] else : lcdict [ col ] = lcdict [ col ] [ filterind ] nafter = lcdict [ 'time' ] . size LOGINFO ( 'applied quality flag filter, ndet before = %s, ndet after = %s' % ( nbefore , nafter ) ) if nanfilter and nanfilter == 'sap,pdc' : notnanind = ( npisfinite ( lcdict [ 'sap' ] [ 'sap_flux' ] ) & npisfinite ( lcdict [ 'pdc' ] [ 'pdcsap_flux' ] ) & npisfinite ( lcdict [ 'time' ] ) ) elif nanfilter and nanfilter == 'sap' : notnanind = ( npisfinite ( lcdict [ 'sap' ] [ 'sap_flux' ] ) & npisfinite ( lcdict [ 'time' ] ) ) elif nanfilter and nanfilter == 'pdc' : notnanind = ( npisfinite ( lcdict [ 'pdc' ] [ 'pdcsap_flux' ] ) & npisfinite ( lcdict [ 'time' ] ) ) if nanfilter : nbefore = lcdict [ 'time' ] . size for col in cols : if '.' in col : key , subkey = col . split ( '.' ) lcdict [ key ] [ subkey ] = lcdict [ key ] [ subkey ] [ notnanind ] else : lcdict [ col ] = lcdict [ col ] [ notnanind ] nafter = lcdict [ 'time' ] . size LOGINFO ( 'removed nans, ndet before = %s, ndet after = %s' % ( nbefore , nafter ) ) if ( timestoignore and isinstance ( timestoignore , list ) and len ( timestoignore ) > 0 ) : exclind = npfull_like ( lcdict [ 'time' ] , True , dtype = np . bool_ ) nbefore = exclind . size for ignoretime in timestoignore : time0 , time1 = ignoretime [ 0 ] , ignoretime [ 1 ] thismask = ~ ( ( lcdict [ 'time' ] >= time0 ) & ( lcdict [ 'time' ] <= time1 ) ) exclind = exclind & thismask for col in cols : if '.' in col : key , subkey = col . split ( '.' ) lcdict [ key ] [ subkey ] = lcdict [ key ] [ subkey ] [ exclind ] else : lcdict [ col ] = lcdict [ col ] [ exclind ] nafter = lcdict [ 'time' ] . size LOGINFO ( 'removed timestoignore, ndet before = %s, ndet after = %s' % ( nbefore , nafter ) ) return lcdict
4422	async def seek ( self , pos : int ) : await self . _lavalink . ws . send ( op = 'seek' , guildId = self . guild_id , position = pos )
7908	def __presence_error ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : return False rs . process_error_presence ( stanza ) return True
7004	def train_rf_classifier ( collected_features , test_fraction = 0.25 , n_crossval_iterations = 20 , n_kfolds = 5 , crossval_scoring_metric = 'f1' , classifier_to_pickle = None , nworkers = - 1 , ) : if ( isinstance ( collected_features , str ) and os . path . exists ( collected_features ) ) : with open ( collected_features , 'rb' ) as infd : fdict = pickle . load ( infd ) elif isinstance ( collected_features , dict ) : fdict = collected_features else : LOGERROR ( "can't figure out the input collected_features arg" ) return None tfeatures = fdict [ 'features_array' ] tlabels = fdict [ 'labels_array' ] tfeaturenames = fdict [ 'availablefeatures' ] tmagcol = fdict [ 'magcol' ] tobjectids = fdict [ 'objectids' ] training_features , testing_features , training_labels , testing_labels = ( train_test_split ( tfeatures , tlabels , test_size = test_fraction , random_state = RANDSEED , stratify = tlabels ) ) clf = RandomForestClassifier ( n_jobs = nworkers , random_state = RANDSEED ) rf_hyperparams = { "max_depth" : [ 3 , 4 , 5 , None ] , "n_estimators" : sp_randint ( 100 , 2000 ) , "max_features" : sp_randint ( 1 , 5 ) , "min_samples_split" : sp_randint ( 2 , 11 ) , "min_samples_leaf" : sp_randint ( 2 , 11 ) , } cvsearch = RandomizedSearchCV ( clf , param_distributions = rf_hyperparams , n_iter = n_crossval_iterations , scoring = crossval_scoring_metric , cv = StratifiedKFold ( n_splits = n_kfolds , shuffle = True , random_state = RANDSEED ) , random_state = RANDSEED ) LOGINFO ( 'running grid-search CV to optimize RF hyperparameters...' ) cvsearch_classifiers = cvsearch . fit ( training_features , training_labels ) _gridsearch_report ( cvsearch_classifiers . cv_results_ ) bestclf = cvsearch_classifiers . best_estimator_ bestclf_score = cvsearch_classifiers . best_score_ bestclf_hyperparams = cvsearch_classifiers . best_params_ test_predicted_labels = bestclf . predict ( testing_features ) recscore = recall_score ( testing_labels , test_predicted_labels ) precscore = precision_score ( testing_labels , test_predicted_labels ) f1score = f1_score ( testing_labels , test_predicted_labels ) confmatrix = confusion_matrix ( testing_labels , test_predicted_labels ) outdict = { 'features' : tfeatures , 'labels' : tlabels , 'feature_names' : tfeaturenames , 'magcol' : tmagcol , 'objectids' : tobjectids , 'kwargs' : { 'test_fraction' : test_fraction , 'n_crossval_iterations' : n_crossval_iterations , 'n_kfolds' : n_kfolds , 'crossval_scoring_metric' : crossval_scoring_metric , 'nworkers' : nworkers } , 'collect_kwargs' : fdict [ 'kwargs' ] , 'testing_features' : testing_features , 'testing_labels' : testing_labels , 'training_features' : training_features , 'training_labels' : training_labels , 'best_classifier' : bestclf , 'best_score' : bestclf_score , 'best_hyperparams' : bestclf_hyperparams , 'best_recall' : recscore , 'best_precision' : precscore , 'best_f1' : f1score , 'best_confmatrix' : confmatrix } if classifier_to_pickle : with open ( classifier_to_pickle , 'wb' ) as outfd : pickle . dump ( outdict , outfd , pickle . HIGHEST_PROTOCOL ) return outdict
6463	def usage_function ( parser ) : parser . print_usage ( ) print ( '' ) print ( 'available functions:' ) for function in sorted ( FUNCTION ) : doc = FUNCTION [ function ] . __doc__ . strip ( ) . splitlines ( ) [ 0 ] print ( ' %-12s %s' % ( function + ':' , doc ) ) return 0
8167	def run_tenuous ( self ) : with LiveExecution . lock : ns_snapshot = copy . copy ( self . ns ) try : source = self . edited_source self . edited_source = None self . do_exec ( source , ns_snapshot ) self . known_good = source self . call_good_cb ( ) return True , None except Exception as ex : tb = traceback . format_exc ( ) self . call_bad_cb ( tb ) self . ns . clear ( ) self . ns . update ( ns_snapshot ) return False , ex
8587	def attach_cdrom ( self , datacenter_id , server_id , cdrom_id ) : data = '{ "id": "' + cdrom_id + '" }' response = self . _perform_request ( url = '/datacenters/%s/servers/%s/cdroms' % ( datacenter_id , server_id ) , method = 'POST' , data = data ) return response
10883	def patch_docs ( subclass , superclass ) : funcs0 = inspect . getmembers ( subclass , predicate = inspect . ismethod ) funcs1 = inspect . getmembers ( superclass , predicate = inspect . ismethod ) funcs1 = [ f [ 0 ] for f in funcs1 ] for name , func in funcs0 : if name . startswith ( '_' ) : continue if name not in funcs1 : continue if func . __doc__ is None : func = getattr ( subclass , name ) func . __func__ . __doc__ = getattr ( superclass , name ) . __func__ . __doc__
6928	def close_cursor ( self , handle ) : if handle in self . cursors : self . cursors [ handle ] . close ( ) else : raise KeyError ( 'cursor with handle %s was not found' % handle )
3548	def _descriptor_changed ( self , descriptor ) : desc = descriptor_list ( ) . get ( descriptor ) if desc is not None : desc . _value_read . set ( )
2832	def get_platform_pwm ( ** keywords ) : plat = Platform . platform_detect ( ) if plat == Platform . RASPBERRY_PI : import RPi . GPIO return RPi_PWM_Adapter ( RPi . GPIO , ** keywords ) elif plat == Platform . BEAGLEBONE_BLACK : import Adafruit_BBIO . PWM return BBIO_PWM_Adapter ( Adafruit_BBIO . PWM , ** keywords ) elif plat == Platform . UNKNOWN : raise RuntimeError ( 'Could not determine platform.' )
13085	def set ( self , section , key , value ) : if not section in self . config : self . config . add_section ( section ) self . config . set ( section , key , value )
8196	def click ( self , node ) : if not self . has_node ( node . id ) : return if node == self . root : return self . _dx , self . _dy = self . offset ( node ) self . previous = self . root . id self . load ( node . id )
2952	def connect ( self , task_spec ) : assert self . default_task_spec is None self . outputs . append ( task_spec ) self . default_task_spec = task_spec . name task_spec . _connect_notify ( self )
4549	def draw_round_rect ( setter , x , y , w , h , r , color = None , aa = False ) : _draw_fast_hline ( setter , x + r , y , w - 2 * r , color , aa ) _draw_fast_hline ( setter , x + r , y + h - 1 , w - 2 * r , color , aa ) _draw_fast_vline ( setter , x , y + r , h - 2 * r , color , aa ) _draw_fast_vline ( setter , x + w - 1 , y + r , h - 2 * r , color , aa ) _draw_circle_helper ( setter , x + r , y + r , r , 1 , color , aa ) _draw_circle_helper ( setter , x + w - r - 1 , y + r , r , 2 , color , aa ) _draw_circle_helper ( setter , x + w - r - 1 , y + h - r - 1 , r , 4 , color , aa ) _draw_circle_helper ( setter , x + r , y + h - r - 1 , r , 8 , color , aa )
29	def get_session ( config = None ) : sess = tf . get_default_session ( ) if sess is None : sess = make_session ( config = config , make_default = True ) return sess
11193	def metadata ( proto_dataset_uri , relpath_in_dataset , key , value ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( uri = proto_dataset_uri , config_path = CONFIG_PATH ) proto_dataset . add_item_metadata ( handle = relpath_in_dataset , key = key , value = value )
12370	def get ( self , id , ** kwargs ) : return super ( DomainRecords , self ) . get ( id , ** kwargs )
3914	def _on_typing ( self , typing_message ) : self . _typing_statuses [ typing_message . user_id ] = typing_message . status self . _update ( )
8371	def run ( src , grammar = NODEBOX , format = None , outputfile = None , iterations = 1 , buff = None , window = True , title = None , fullscreen = None , close_window = False , server = False , port = 7777 , show_vars = False , vars = None , namespace = None , run_shell = False , args = [ ] , verbose = False , background_thread = True ) : sys . argv = [ sys . argv [ 0 ] ] + args create_args = [ src , grammar , format , outputfile , iterations , buff , window , title , fullscreen , server , port , show_vars ] create_kwargs = dict ( vars = vars , namespace = namespace ) run_args = [ src ] run_kwargs = dict ( iterations = iterations , frame_limiter = window , verbose = verbose , run_forever = window and not ( close_window or bool ( outputfile ) ) , ) if background_thread : sbot_thread = ShoebotThread ( create_args = create_args , create_kwargs = create_kwargs , run_args = run_args , run_kwargs = run_kwargs , send_sigint = run_shell ) sbot_thread . start ( ) sbot = sbot_thread . sbot else : print ( 'background thread disabled' ) if run_shell : raise ValueError ( 'UI Must run in a separate thread to shell and shell needs main thread' ) sbot_thread = None sbot = create_bot ( * create_args , ** create_kwargs ) sbot . run ( * run_args , ** run_kwargs ) if run_shell : import shoebot . sbio . shell shell = shoebot . sbio . shell . ShoebotCmd ( sbot , trusted = True ) try : shell . cmdloop ( ) except KeyboardInterrupt as e : publish_event ( QUIT_EVENT ) if verbose : raise else : return elif background_thread : try : while sbot_thread . is_alive ( ) : sleep ( 1 ) except KeyboardInterrupt : publish_event ( QUIT_EVENT ) if all ( ( background_thread , sbot_thread ) ) : sbot_thread . join ( ) return sbot
4634	def derive_private_key ( self , sequence ) : encoded = "%s %d" % ( str ( self ) , sequence ) a = bytes ( encoded , "ascii" ) s = hashlib . sha256 ( hashlib . sha512 ( a ) . digest ( ) ) . digest ( ) return PrivateKey ( hexlify ( s ) . decode ( "ascii" ) , prefix = self . pubkey . prefix )
4058	def _bib_processor ( self , retrieved ) : items = [ ] for bib in retrieved . entries : items . append ( bib [ "content" ] [ 0 ] [ "value" ] ) self . url_params = None return items
12455	def error_handler ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : try : return func ( * args , ** kwargs ) except BaseException as err : if BOOTSTRAPPER_TEST_KEY in os . environ : raise if ERROR_HANDLER_DISABLED : return True return save_traceback ( err ) return wrapper
5506	def screenshot ( url , * args , ** kwargs ) : phantomscript = os . path . join ( os . path . dirname ( __file__ ) , 'take_screenshot.js' ) directory = kwargs . get ( 'save_dir' , '/tmp' ) image_name = kwargs . get ( 'image_name' , None ) or _image_name_from_url ( url ) ext = kwargs . get ( 'format' , 'png' ) . lower ( ) save_path = os . path . join ( directory , image_name ) + '.' + ext crop_to_visible = kwargs . get ( 'crop_to_visible' , False ) cmd_args = [ 'phantomjs' , '--ssl-protocol=any' , phantomscript , url , '--width' , str ( kwargs [ 'width' ] ) , '--height' , str ( kwargs [ 'height' ] ) , '--useragent' , str ( kwargs [ 'user_agent' ] ) , '--dir' , directory , '--ext' , ext , '--name' , str ( image_name ) , ] if crop_to_visible : cmd_args . append ( '--croptovisible' ) output = subprocess . Popen ( cmd_args , stdout = subprocess . PIPE ) . communicate ( ) [ 0 ] return Screenshot ( save_path , directory , image_name + '.' + ext , ext )
1635	def CheckSpacingForFunctionCall ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] fncall = line for pattern in ( r'\bif\s*\((.*)\)\s*{' , r'\bfor\s*\((.*)\)\s*{' , r'\bwhile\s*\((.*)\)\s*[{;]' , r'\bswitch\s*\((.*)\)\s*{' ) : match = Search ( pattern , line ) if match : fncall = match . group ( 1 ) break if ( not Search ( r'\b(if|for|while|switch|return|new|delete|catch|sizeof)\b' , fncall ) and not Search ( r' \([^)]+\)\([^)]*(\)|,$)' , fncall ) and not Search ( r' \([^)]+\)\[[^\]]+\]' , fncall ) ) : if Search ( r'\w\s*\(\s(?!\s*\\$)' , fncall ) : error ( filename , linenum , 'whitespace/parens' , 4 , 'Extra space after ( in function call' ) elif Search ( r'\(\s+(?!(\s*\\)|\()' , fncall ) : error ( filename , linenum , 'whitespace/parens' , 2 , 'Extra space after (' ) if ( Search ( r'\w\s+\(' , fncall ) and not Search ( r'_{0,2}asm_{0,2}\s+_{0,2}volatile_{0,2}\s+\(' , fncall ) and not Search ( r'#\s*define|typedef|using\s+\w+\s*=' , fncall ) and not Search ( r'\w\s+\((\w+::)*\*\w+\)\(' , fncall ) and not Search ( r'\bcase\s+\(' , fncall ) ) : if Search ( r'\boperator_*\b' , line ) : error ( filename , linenum , 'whitespace/parens' , 0 , 'Extra space before ( in function call' ) else : error ( filename , linenum , 'whitespace/parens' , 4 , 'Extra space before ( in function call' ) if Search ( r'[^)]\s+\)\s*[^{\s]' , fncall ) : if Search ( r'^\s+\)' , fncall ) : error ( filename , linenum , 'whitespace/parens' , 2 , 'Closing ) should be moved to the previous line' ) else : error ( filename , linenum , 'whitespace/parens' , 2 , 'Extra space before )' )
6134	def from_file ( cls , fpath , position = 1 , file_id = None ) : if file_id is None : file_id = fpath with open ( fpath ) as f : code = f . read ( ) file_content = str ( code ) file_metadata = FileMetadata ( file_id , position ) return cls ( file_metadata , file_content )
462	def exit_tensorflow ( sess = None , port = 6006 ) : text = "[TL] Close tensorboard and nvidia-process if available" text2 = "[TL] Close tensorboard and nvidia-process not yet supported by this function (tl.ops.exit_tf) on " if sess is not None : sess . close ( ) if _platform == "linux" or _platform == "linux2" : tl . logging . info ( 'linux: %s' % text ) os . system ( 'nvidia-smi' ) os . system ( 'fuser ' + port + '/tcp -k' ) os . system ( "nvidia-smi | grep python |awk '{print $3}'|xargs kill" ) _exit ( ) elif _platform == "darwin" : tl . logging . info ( 'OS X: %s' % text ) subprocess . Popen ( "lsof -i tcp:" + str ( port ) + " | grep -v PID | awk '{print $2}' | xargs kill" , shell = True ) elif _platform == "win32" : raise NotImplementedError ( "this function is not supported on the Windows platform" ) else : tl . logging . info ( text2 + _platform )
5767	def _advapi32_interpret_rsa_key_blob ( bit_size , blob_struct , blob ) : len1 = bit_size // 8 len2 = bit_size // 16 prime1_offset = len1 prime2_offset = prime1_offset + len2 exponent1_offset = prime2_offset + len2 exponent2_offset = exponent1_offset + len2 coefficient_offset = exponent2_offset + len2 private_exponent_offset = coefficient_offset + len2 public_exponent = blob_struct . rsapubkey . pubexp modulus = int_from_bytes ( blob [ 0 : prime1_offset ] [ : : - 1 ] ) prime1 = int_from_bytes ( blob [ prime1_offset : prime2_offset ] [ : : - 1 ] ) prime2 = int_from_bytes ( blob [ prime2_offset : exponent1_offset ] [ : : - 1 ] ) exponent1 = int_from_bytes ( blob [ exponent1_offset : exponent2_offset ] [ : : - 1 ] ) exponent2 = int_from_bytes ( blob [ exponent2_offset : coefficient_offset ] [ : : - 1 ] ) coefficient = int_from_bytes ( blob [ coefficient_offset : private_exponent_offset ] [ : : - 1 ] ) private_exponent = int_from_bytes ( blob [ private_exponent_offset : private_exponent_offset + len1 ] [ : : - 1 ] ) public_key_info = keys . PublicKeyInfo ( { 'algorithm' : keys . PublicKeyAlgorithm ( { 'algorithm' : 'rsa' , } ) , 'public_key' : keys . RSAPublicKey ( { 'modulus' : modulus , 'public_exponent' : public_exponent , } ) , } ) rsa_private_key = keys . RSAPrivateKey ( { 'version' : 'two-prime' , 'modulus' : modulus , 'public_exponent' : public_exponent , 'private_exponent' : private_exponent , 'prime1' : prime1 , 'prime2' : prime2 , 'exponent1' : exponent1 , 'exponent2' : exponent2 , 'coefficient' : coefficient , } ) private_key_info = keys . PrivateKeyInfo ( { 'version' : 0 , 'private_key_algorithm' : keys . PrivateKeyAlgorithm ( { 'algorithm' : 'rsa' , } ) , 'private_key' : rsa_private_key , } ) return ( public_key_info , private_key_info )
13193	def geom_to_xml_element ( geom ) : if geom . srs . srid != 4326 : raise NotImplementedError ( "Only WGS 84 lat/long geometries (SRID 4326) are supported." ) return geojson_to_gml ( json . loads ( geom . geojson ) )
2072	def get_obj_cols ( df ) : obj_cols = [ ] for idx , dt in enumerate ( df . dtypes ) : if dt == 'object' or is_category ( dt ) : obj_cols . append ( df . columns . values [ idx ] ) return obj_cols
9011	def index_of_first_consumed_mesh_in_row ( self ) : index = 0 for instruction in self . row_instructions : if instruction is self : break index += instruction . number_of_consumed_meshes else : self . _raise_not_found_error ( ) return index
10785	def add_missing_particles ( st , rad = 'calc' , tries = 50 , ** kwargs ) : if rad == 'calc' : rad = guess_add_radii ( st ) guess , npart = feature_guess ( st , rad , ** kwargs ) tries = np . min ( [ tries , npart ] ) accepts , new_poses = check_add_particles ( st , guess [ : tries ] , rad = rad , ** kwargs ) return accepts , new_poses
6107	def trace_grid_stack_to_next_plane ( self ) : def minus ( grid , deflections ) : return grid - deflections return self . grid_stack . map_function ( minus , self . deflection_stack )
2352	def root ( self ) : if self . _root is None and self . _root_locator is not None : return self . page . find_element ( * self . _root_locator ) return self . _root
7552	def _getbins ( ) : if not _sys . maxsize > 2 ** 32 : _sys . exit ( "ipyrad requires 64bit architecture" ) _platform = _sys . platform if 'VIRTUAL_ENV' in _os . environ : ipyrad_path = _os . environ [ 'VIRTUAL_ENV' ] else : path = _os . path . abspath ( _os . path . dirname ( __file__ ) ) ipyrad_path = _os . path . dirname ( path ) ipyrad_path = _os . path . dirname ( path ) bin_path = _os . path . join ( ipyrad_path , "bin" ) if 'linux' in _platform : vsearch = _os . path . join ( _os . path . abspath ( bin_path ) , "vsearch-linux-x86_64" ) muscle = _os . path . join ( _os . path . abspath ( bin_path ) , "muscle-linux-x86_64" ) smalt = _os . path . join ( _os . path . abspath ( bin_path ) , "smalt-linux-x86_64" ) bwa = _os . path . join ( _os . path . abspath ( bin_path ) , "bwa-linux-x86_64" ) samtools = _os . path . join ( _os . path . abspath ( bin_path ) , "samtools-linux-x86_64" ) bedtools = _os . path . join ( _os . path . abspath ( bin_path ) , "bedtools-linux-x86_64" ) qmc = _os . path . join ( _os . path . abspath ( bin_path ) , "QMC-linux-x86_64" ) else : vsearch = _os . path . join ( _os . path . abspath ( bin_path ) , "vsearch-osx-x86_64" ) muscle = _os . path . join ( _os . path . abspath ( bin_path ) , "muscle-osx-x86_64" ) smalt = _os . path . join ( _os . path . abspath ( bin_path ) , "smalt-osx-x86_64" ) bwa = _os . path . join ( _os . path . abspath ( bin_path ) , "bwa-osx-x86_64" ) samtools = _os . path . join ( _os . path . abspath ( bin_path ) , "samtools-osx-x86_64" ) bedtools = _os . path . join ( _os . path . abspath ( bin_path ) , "bedtools-osx-x86_64" ) qmc = _os . path . join ( _os . path . abspath ( bin_path ) , "QMC-osx-x86_64" ) assert _cmd_exists ( muscle ) , "muscle not found here: " + muscle assert _cmd_exists ( vsearch ) , "vsearch not found here: " + vsearch assert _cmd_exists ( smalt ) , "smalt not found here: " + smalt assert _cmd_exists ( bwa ) , "bwa not found here: " + bwa assert _cmd_exists ( samtools ) , "samtools not found here: " + samtools assert _cmd_exists ( bedtools ) , "bedtools not found here: " + bedtools return vsearch , muscle , smalt , bwa , samtools , bedtools , qmc
441	def count_params ( self ) : n_params = 0 for _i , p in enumerate ( self . all_params ) : n = 1 for s in p . get_shape ( ) : try : s = int ( s ) except Exception : s = 1 if s : n = n * s n_params = n_params + n return n_params
12907	def assert_json_type ( value : JsonValue , expected_type : JsonCheckType ) -> None : def type_name ( t : Union [ JsonCheckType , Type [ None ] ] ) -> str : if t is None : return "None" if isinstance ( t , JList ) : return "list" return t . __name__ if expected_type is None : if value is None : return elif expected_type == float : if isinstance ( value , float ) or isinstance ( value , int ) : return elif expected_type in [ str , int , bool , list , dict ] : if isinstance ( value , expected_type ) : return elif isinstance ( expected_type , JList ) : if isinstance ( value , list ) : for v in value : assert_json_type ( v , expected_type . value_type ) return else : raise TypeError ( "unsupported type" ) raise TypeError ( "wrong JSON type {} != {}" . format ( type_name ( expected_type ) , type_name ( type ( value ) ) ) )
734	def _calculateError ( self , recordNum , bucketIdxList ) : error = dict ( ) targetDist = numpy . zeros ( self . _maxBucketIdx + 1 ) numCategories = len ( bucketIdxList ) for bucketIdx in bucketIdxList : targetDist [ bucketIdx ] = 1.0 / numCategories for ( learnRecordNum , learnPatternNZ ) in self . _patternNZHistory : nSteps = recordNum - learnRecordNum if nSteps in self . steps : predictDist = self . inferSingleStep ( learnPatternNZ , self . _weightMatrix [ nSteps ] ) error [ nSteps ] = targetDist - predictDist return error
7372	def _prepare_drb_allele_name ( self , parsed_beta_allele ) : if "DRB" not in parsed_beta_allele . gene : raise ValueError ( "Unexpected allele %s" % parsed_beta_allele ) return "%s_%s%s" % ( parsed_beta_allele . gene , parsed_beta_allele . allele_family , parsed_beta_allele . allele_code )
7540	def basecaller ( arrayed , mindepth_majrule , mindepth_statistical , estH , estE ) : cons = np . zeros ( arrayed . shape [ 1 ] , dtype = np . uint8 ) cons . fill ( 78 ) arr = arrayed . view ( np . uint8 ) for col in xrange ( arr . shape [ 1 ] ) : carr = arr [ : , col ] mask = carr == 45 mask += carr == 78 marr = carr [ ~ mask ] if not marr . shape [ 0 ] : cons [ col ] = 78 elif np . all ( marr == marr [ 0 ] ) : cons [ col ] = marr [ 0 ] else : counts = np . bincount ( marr ) pbase = np . argmax ( counts ) nump = counts [ pbase ] counts [ pbase ] = 0 qbase = np . argmax ( counts ) numq = counts [ qbase ] counts [ qbase ] = 0 rbase = np . argmax ( counts ) numr = counts [ rbase ] bidepth = nump + numq if bidepth < mindepth_majrule : cons [ col ] = 78 else : if bidepth > 500 : base1 = int ( 500 * ( nump / float ( bidepth ) ) ) base2 = int ( 500 * ( numq / float ( bidepth ) ) ) else : base1 = nump base2 = numq if bidepth >= mindepth_statistical : ishet , prob = get_binom ( base1 , base2 , estE , estH ) if prob < 0.95 : cons [ col ] = 78 else : if ishet : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase else : if nump == numq : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase return cons . view ( "S1" )
12972	def getOnlyFields ( self , pk , fields , cascadeFetch = False ) : conn = self . _get_connection ( ) key = self . _get_key_for_id ( pk ) res = conn . hmget ( key , fields ) if type ( res ) != list or not len ( res ) : return None objDict = { } numFields = len ( fields ) i = 0 anyNotNone = False while i < numFields : objDict [ fields [ i ] ] = res [ i ] if res [ i ] != None : anyNotNone = True i += 1 if anyNotNone is False : return None objDict [ '_id' ] = pk ret = self . _redisResultToObj ( objDict ) if cascadeFetch is True : self . _doCascadeFetch ( ret ) return ret
5491	def create_config ( cls , cfgfile , nick , twtfile , twturl , disclose_identity , add_news ) : cfgfile_dir = os . path . dirname ( cfgfile ) if not os . path . exists ( cfgfile_dir ) : os . makedirs ( cfgfile_dir ) cfg = configparser . ConfigParser ( ) cfg . add_section ( "twtxt" ) cfg . set ( "twtxt" , "nick" , nick ) cfg . set ( "twtxt" , "twtfile" , twtfile ) cfg . set ( "twtxt" , "twturl" , twturl ) cfg . set ( "twtxt" , "disclose_identity" , str ( disclose_identity ) ) cfg . set ( "twtxt" , "character_limit" , "140" ) cfg . set ( "twtxt" , "character_warning" , "140" ) cfg . add_section ( "following" ) if add_news : cfg . set ( "following" , "twtxt" , "https://buckket.org/twtxt_news.txt" ) conf = cls ( cfgfile , cfg ) conf . write_config ( ) return conf
2329	def create_graph_from_data ( self , data ) : self . arguments [ '{SCORE}' ] = self . score self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) self . arguments [ '{BETA}' ] = str ( self . beta ) self . arguments [ '{OPTIM}' ] = str ( self . optim ) . upper ( ) self . arguments [ '{ALPHA}' ] = str ( self . alpha ) results = self . _run_bnlearn ( data , verbose = self . verbose ) graph = nx . DiGraph ( ) graph . add_edges_from ( results ) return graph
983	def mmGetMetricFromTrace ( self , trace ) : return Metric . createFromTrace ( trace . makeCountsTrace ( ) , excludeResets = self . mmGetTraceResets ( ) )
5802	def _convert_filetime_to_timestamp ( filetime ) : hundreds_nano_seconds = struct . unpack ( b'>Q' , struct . pack ( b'>LL' , filetime . dwHighDateTime , filetime . dwLowDateTime ) ) [ 0 ] seconds_since_1601 = hundreds_nano_seconds / 10000000 return seconds_since_1601 - 11644473600
7343	def clone_with_updates ( self , ** kwargs ) : fields_dict = self . to_dict ( ) fields_dict . update ( kwargs ) return BindingPrediction ( ** fields_dict )
6447	def stem ( self , word ) : word = normalize ( 'NFC' , text_type ( word . lower ( ) ) ) for suffix_len in range ( 11 , 0 , - 1 ) : ending = word [ - suffix_len : ] if ( ending in self . _suffix and len ( word ) - suffix_len >= 2 and ( self . _suffix [ ending ] is None or self . _suffix [ ending ] ( word , suffix_len ) ) ) : word = word [ : - suffix_len ] break if word [ - 2 : ] in { 'bb' , 'dd' , 'gg' , 'll' , 'mm' , 'nn' , 'pp' , 'rr' , 'ss' , 'tt' , } : word = word [ : - 1 ] for ending , replacement in self . _recode : if word . endswith ( ending ) : if callable ( replacement ) : word = replacement ( word ) else : word = word [ : - len ( ending ) ] + replacement return word
7886	def emit_stanza ( self , element ) : if not self . _head_emitted : raise RuntimeError ( ".emit_head() must be called first." ) string = self . _emit_element ( element , level = 1 , declared_prefixes = self . _root_prefixes ) return remove_evil_characters ( string )
3449	def _init_worker ( model , loopless , sense ) : global _model global _loopless _model = model _model . solver . objective . direction = sense _loopless = loopless
10001	def get_nodes_with ( self , obj ) : result = set ( ) if nx . __version__ [ 0 ] == "1" : nodes = self . nodes_iter ( ) else : nodes = self . nodes for node in nodes : if node [ OBJ ] == obj : result . add ( node ) return result
10294	def get_namespaces_with_incorrect_names ( graph : BELGraph ) -> Set [ str ] : return { exc . namespace for _ , exc , _ in graph . warnings if isinstance ( exc , ( MissingNamespaceNameWarning , MissingNamespaceRegexWarning ) ) }
6946	def jhk_to_imag ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , IJHK , IJH , IJK , IHK , IJ , IH , IK )
4205	def levdown ( anxt , enxt = None ) : if anxt [ 0 ] != 1 : raise ValueError ( 'At least one of the reflection coefficients is equal to one.' ) anxt = anxt [ 1 : ] knxt = anxt [ - 1 ] if knxt == 1.0 : raise ValueError ( 'At least one of the reflection coefficients is equal to one.' ) acur = ( anxt [ 0 : - 1 ] - knxt * numpy . conj ( anxt [ - 2 : : - 1 ] ) ) / ( 1. - abs ( knxt ) ** 2 ) ecur = None if enxt is not None : ecur = enxt / ( 1. - numpy . dot ( knxt . conj ( ) . transpose ( ) , knxt ) ) acur = numpy . insert ( acur , 0 , 1 ) return acur , ecur
11064	def _ignore_event ( self , message ) : if hasattr ( message , 'subtype' ) and message . subtype in self . ignored_events : return True return False
6061	def convolve_mapping_matrix ( self , mapping_matrix ) : return self . convolve_matrix_jit ( mapping_matrix , self . image_frame_indexes , self . image_frame_psfs , self . image_frame_lengths )
5474	def format_pairs ( self , values ) : return ', ' . join ( '%s=%s' % ( key , value ) for key , value in sorted ( values . items ( ) ) )
5646	def createcolorbar ( cmap , norm ) : cax , kw = matplotlib . colorbar . make_axes ( matplotlib . pyplot . gca ( ) ) c = matplotlib . colorbar . ColorbarBase ( cax , cmap = cmap , norm = norm ) return c
6517	def execute_reports ( config , path , collector , on_report_finish = None , output_file = None ) : reports = get_reports ( ) for report in config . get ( 'requested_reports' , [ ] ) : if report . get ( 'type' ) and report [ 'type' ] in reports : cfg = config . get ( 'report' , { } ) . get ( report [ 'type' ] , { } ) cfg . update ( report ) reporter = reports [ report [ 'type' ] ] ( cfg , path , output_file = output_file , ) reporter . produce ( collector ) if on_report_finish : on_report_finish ( report )
10606	def prepare_to_run ( self ) : self . clock . reset ( ) for e in self . entities : e . prepare_to_run ( self . clock , self . period_count )
7442	def branch ( self , newname , subsamples = None , infile = None ) : remove = 0 if ( newname == self . name or os . path . exists ( os . path . join ( self . paramsdict [ "project_dir" ] , newname + ".assembly" ) ) ) : print ( "{}Assembly object named {} already exists" . format ( self . _spacer , newname ) ) else : self . _check_name ( newname ) if newname . startswith ( "params-" ) : newname = newname . split ( "params-" ) [ 1 ] newobj = copy . deepcopy ( self ) newobj . name = newname newobj . paramsdict [ "assembly_name" ] = newname if subsamples and infile : print ( BRANCH_NAMES_AND_INPUT ) if infile : if infile [ 0 ] == "-" : remove = 1 infile = infile [ 1 : ] if os . path . exists ( infile ) : subsamples = _read_sample_names ( infile ) if remove : subsamples = list ( set ( self . samples . keys ( ) ) - set ( subsamples ) ) if subsamples : for sname in subsamples : if sname in self . samples : newobj . samples [ sname ] = copy . deepcopy ( self . samples [ sname ] ) else : print ( "Sample name not found: {}" . format ( sname ) ) newobj . samples = { name : sample for name , sample in newobj . samples . items ( ) if name in subsamples } else : for sample in self . samples : newobj . samples [ sample ] = copy . deepcopy ( self . samples [ sample ] ) newobj . save ( ) return newobj
6604	def result_relpath ( self , package_index ) : dirname = 'task_{:05d}' . format ( package_index ) ret = os . path . join ( 'results' , dirname , 'result.p.gz' ) return ret
1005	def _learnBacktrackFrom ( self , startOffset , readOnly = True ) : numPrevPatterns = len ( self . _prevLrnPatterns ) currentTimeStepsOffset = numPrevPatterns - 1 if not readOnly : self . segmentUpdates = { } if self . verbosity >= 3 : if readOnly : print ( "Trying to lock-on using startCell state from %d steps ago:" % ( numPrevPatterns - 1 - startOffset ) , self . _prevLrnPatterns [ startOffset ] ) else : print ( "Locking on using startCell state from %d steps ago:" % ( numPrevPatterns - 1 - startOffset ) , self . _prevLrnPatterns [ startOffset ] ) inSequence = True for offset in range ( startOffset , numPrevPatterns ) : self . lrnPredictedState [ 't-1' ] [ : , : ] = self . lrnPredictedState [ 't' ] [ : , : ] self . lrnActiveState [ 't-1' ] [ : , : ] = self . lrnActiveState [ 't' ] [ : , : ] inputColumns = self . _prevLrnPatterns [ offset ] if not readOnly : self . _processSegmentUpdates ( inputColumns ) if offset == startOffset : self . lrnActiveState [ 't' ] . fill ( 0 ) for c in inputColumns : self . lrnActiveState [ 't' ] [ c , 0 ] = 1 inSequence = True else : inSequence = self . _learnPhase1 ( inputColumns , readOnly = readOnly ) if not inSequence or offset == currentTimeStepsOffset : break if self . verbosity >= 3 : print " backtrack: computing predictions from " , inputColumns self . _learnPhase2 ( readOnly = readOnly ) return inSequence
11542	def read ( self , pin ) : if type ( pin ) is list : return [ self . read ( p ) for p in pin ] pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : value = self . _read ( pin_id ) lpin = self . _pin_lin . get ( pin , None ) if lpin and type ( lpin [ 'read' ] ) is tuple : read_range = lpin [ 'read' ] value = self . _linear_interpolation ( value , * read_range ) return value else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
2923	def ancestors ( self ) : results = [ ] def recursive_find_ancestors ( task , stack ) : for input in task . inputs : if input not in stack : stack . append ( input ) recursive_find_ancestors ( input , stack ) recursive_find_ancestors ( self , results ) return results
1324	def threadFunc ( root ) : th = threading . currentThread ( ) auto . Logger . WriteLine ( '\nThis is running in a new thread. {} {}' . format ( th . ident , th . name ) , auto . ConsoleColor . Cyan ) time . sleep ( 2 ) auto . InitializeUIAutomationInCurrentThread ( ) auto . GetConsoleWindow ( ) . CaptureToImage ( 'console_newthread.png' ) newRoot = auto . GetRootControl ( ) auto . EnumAndLogControl ( newRoot , 1 ) auto . UninitializeUIAutomationInCurrentThread ( ) auto . Logger . WriteLine ( '\nThread exits. {} {}' . format ( th . ident , th . name ) , auto . ConsoleColor . Cyan )
10083	def edit ( self , pid = None ) : pid = pid or self . pid with db . session . begin_nested ( ) : before_record_update . send ( current_app . _get_current_object ( ) , record = self ) record_pid , record = self . fetch_published ( ) assert PIDStatus . REGISTERED == record_pid . status assert record [ '_deposit' ] == self [ '_deposit' ] self . model . json = self . _prepare_edit ( record ) flag_modified ( self . model , 'json' ) db . session . merge ( self . model ) after_record_update . send ( current_app . _get_current_object ( ) , record = self ) return self . __class__ ( self . model . json , model = self . model )
8645	def get_track_by_id ( session , track_id , track_point_limit = None , track_point_offset = None ) : tracking_data = { } if track_point_limit : tracking_data [ 'track_point_limit' ] = track_point_limit if track_point_offset : tracking_data [ 'track_point_offset' ] = track_point_offset response = make_get_request ( session , 'tracks/{}' . format ( track_id ) , params_data = tracking_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise TrackNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
631	def createSegment ( self , cell ) : cellData = self . _cells [ cell ] if len ( self . _freeFlatIdxs ) > 0 : flatIdx = self . _freeFlatIdxs . pop ( ) else : flatIdx = self . _nextFlatIdx self . _segmentForFlatIdx . append ( None ) self . _nextFlatIdx += 1 ordinal = self . _nextSegmentOrdinal self . _nextSegmentOrdinal += 1 segment = Segment ( cell , flatIdx , ordinal ) cellData . _segments . append ( segment ) self . _segmentForFlatIdx [ flatIdx ] = segment return segment
5207	def format_output ( data : pd . DataFrame , source , col_maps = None ) -> pd . DataFrame : if data . empty : return pd . DataFrame ( ) if source == 'bdp' : req_cols = [ 'ticker' , 'field' , 'value' ] else : req_cols = [ 'ticker' , 'field' , 'name' , 'value' , 'position' ] if any ( col not in data for col in req_cols ) : return pd . DataFrame ( ) if data . dropna ( subset = [ 'value' ] ) . empty : return pd . DataFrame ( ) if source == 'bdp' : res = pd . DataFrame ( pd . concat ( [ pd . Series ( { ** { 'ticker' : t } , ** grp . set_index ( 'field' ) . value . to_dict ( ) } ) for t , grp in data . groupby ( 'ticker' ) ] , axis = 1 , sort = False ) ) . transpose ( ) . set_index ( 'ticker' ) else : res = pd . DataFrame ( pd . concat ( [ grp . loc [ : , [ 'name' , 'value' ] ] . set_index ( 'name' ) . transpose ( ) . reset_index ( drop = True ) . assign ( ticker = t ) for ( t , _ ) , grp in data . groupby ( [ 'ticker' , 'position' ] ) ] , sort = False ) ) . reset_index ( drop = True ) . set_index ( 'ticker' ) res . columns . name = None if col_maps is None : col_maps = dict ( ) return res . rename ( columns = lambda vv : col_maps . get ( vv , vv . lower ( ) . replace ( ' ' , '_' ) . replace ( '-' , '_' ) ) ) . apply ( pd . to_numeric , errors = 'ignore' , downcast = 'float' )
13753	def prepare_path ( path ) : if type ( path ) == list : return os . path . join ( * path ) return path
13053	def nmap_scan ( ) : hs = HostSearch ( ) config = Config ( ) nmap_types = [ 'top10' , 'top100' , 'custom' , 'top1000' , 'all' ] options = { 'top10' : '--top-ports 10' , 'top100' : '--top-ports 100' , 'custom' : config . get ( 'nmap' , 'options' ) , 'top1000' : '--top-ports 1000' , 'all' : '-p-' } hs_parser = hs . argparser argparser = argparse . ArgumentParser ( parents = [ hs_parser ] , conflict_handler = 'resolve' , description = "Scans hosts from the database using nmap, any arguments that are not in the help are passed to nmap" ) argparser . add_argument ( 'type' , metavar = 'type' , help = 'The number of ports to scan: top10, top100, custom, top1000 (default) or all' , type = str , choices = nmap_types , default = 'top1000' , const = 'top1000' , nargs = '?' ) arguments , extra_nmap_args = argparser . parse_known_args ( ) tags = nmap_types [ nmap_types . index ( arguments . type ) : ] tags = [ "!nmap_" + tag for tag in tags ] hosts = hs . get_hosts ( tags = tags ) hosts = [ host for host in hosts ] nmap_args = [ ] nmap_args . extend ( extra_nmap_args ) nmap_args . extend ( options [ arguments . type ] . split ( ' ' ) ) print_notification ( "Running nmap with args: {} on {} hosts(s)" . format ( nmap_args , len ( hosts ) ) ) if len ( hosts ) : result = nmap ( nmap_args , [ str ( h . address ) for h in hosts ] ) for host in hosts : host . add_tag ( "nmap_{}" . format ( arguments . type ) ) host . save ( ) print_notification ( "Nmap done, importing results" ) stats = import_nmap ( result , "nmap_{}" . format ( arguments . type ) , check_function = all_hosts , import_services = True ) stats [ 'scanned_hosts' ] = len ( hosts ) stats [ 'type' ] = arguments . type Logger ( ) . log ( 'nmap_scan' , "Performed nmap {} scan on {} hosts" . format ( arguments . type , len ( hosts ) ) , stats ) else : print_notification ( "No hosts found" )
9271	def filter_since_tag ( self , all_tags ) : tag = self . detect_since_tag ( ) if not tag or tag == REPO_CREATED_TAG_NAME : return copy . deepcopy ( all_tags ) filtered_tags = [ ] tag_names = [ t [ "name" ] for t in all_tags ] try : idx = tag_names . index ( tag ) except ValueError : self . warn_if_tag_not_found ( tag , "since-tag" ) return copy . deepcopy ( all_tags ) since_tag = all_tags [ idx ] since_date = self . get_time_of_tag ( since_tag ) for t in all_tags : tag_date = self . get_time_of_tag ( t ) if since_date <= tag_date : filtered_tags . append ( t ) return filtered_tags
13076	def view_maker ( self , name , instance = None ) : if instance is None : instance = self sig = "lang" in [ parameter . name for parameter in inspect . signature ( getattr ( instance , name ) ) . parameters . values ( ) ] def route ( ** kwargs ) : if sig and "lang" not in kwargs : kwargs [ "lang" ] = self . get_locale ( ) if "semantic" in kwargs : del kwargs [ "semantic" ] return self . route ( getattr ( instance , name ) , ** kwargs ) return route
4722	def trun_enter ( trun ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun::enter" ) trun [ "stamp" ] [ "begin" ] = int ( time . time ( ) ) rcode = 0 for hook in trun [ "hooks" ] [ "enter" ] : rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun::enter { rcode: %r }" % rcode , rcode ) return rcode
11174	def settingshelp ( self , width = 0 ) : out = [ ] out . append ( self . _wrap ( self . docs [ 'title' ] , width = width ) ) if self . docs [ 'description' ] : out . append ( self . _wrap ( self . docs [ 'description' ] , indent = 2 , width = width ) ) out . append ( '' ) out . append ( 'SETTINGS:' ) out . append ( self . strsettings ( indent = 2 , width = width ) ) out . append ( '' ) return '\n' . join ( out )
1217	def register_saver_ops ( self ) : variables = self . get_savable_variables ( ) if variables is None or len ( variables ) == 0 : self . _saver = None return base_scope = self . _get_base_variable_scope ( ) variables_map = { strip_name_scope ( v . name , base_scope ) : v for v in variables } self . _saver = tf . train . Saver ( var_list = variables_map , reshape = False , sharded = False , max_to_keep = 5 , keep_checkpoint_every_n_hours = 10000.0 , name = None , restore_sequentially = False , saver_def = None , builder = None , defer_build = False , allow_empty = True , write_version = tf . train . SaverDef . V2 , pad_step_number = False , save_relative_paths = True )
12886	def get_fsapi_endpoint ( self ) : endpoint = yield from self . __session . get ( self . fsapi_device_url , timeout = self . timeout ) text = yield from endpoint . text ( encoding = 'utf-8' ) doc = objectify . fromstring ( text ) return doc . webfsapi . text
646	def generateVectors ( numVectors = 100 , length = 500 , activity = 50 ) : vectors = [ ] coinc = numpy . zeros ( length , dtype = 'int32' ) indexList = range ( length ) for i in xrange ( numVectors ) : coinc [ : ] = 0 coinc [ random . sample ( indexList , activity ) ] = 1 vectors . append ( coinc . copy ( ) ) return vectors
12664	def union_mask ( filelist ) : firstimg = check_img ( filelist [ 0 ] ) mask = np . zeros_like ( firstimg . get_data ( ) ) try : for volf in filelist : roiimg = check_img ( volf ) check_img_compatibility ( firstimg , roiimg ) mask += get_img_data ( roiimg ) except Exception as exc : raise ValueError ( 'Error joining mask {} and {}.' . format ( repr_imgs ( firstimg ) , repr_imgs ( volf ) ) ) from exc else : return as_ndarray ( mask > 0 , dtype = bool )
5622	def path_is_remote ( path , s3 = True ) : prefixes = ( "http://" , "https://" , "/vsicurl/" ) if s3 : prefixes += ( "s3://" , "/vsis3/" ) return path . startswith ( prefixes )
6225	def move_state ( self , direction , activate ) : if direction == RIGHT : self . _xdir = POSITIVE if activate else STILL elif direction == LEFT : self . _xdir = NEGATIVE if activate else STILL elif direction == FORWARD : self . _zdir = NEGATIVE if activate else STILL elif direction == BACKWARD : self . _zdir = POSITIVE if activate else STILL elif direction == UP : self . _ydir = POSITIVE if activate else STILL elif direction == DOWN : self . _ydir = NEGATIVE if activate else STILL
5179	def reports ( self , ** kwargs ) : return self . __api . reports ( query = EqualsOperator ( "certname" , self . name ) , ** kwargs )
1	def nature_cnn ( unscaled_images , ** conv_kwargs ) : scaled_images = tf . cast ( unscaled_images , tf . float32 ) / 255. activ = tf . nn . relu h = activ ( conv ( scaled_images , 'c1' , nf = 32 , rf = 8 , stride = 4 , init_scale = np . sqrt ( 2 ) , ** conv_kwargs ) ) h2 = activ ( conv ( h , 'c2' , nf = 64 , rf = 4 , stride = 2 , init_scale = np . sqrt ( 2 ) , ** conv_kwargs ) ) h3 = activ ( conv ( h2 , 'c3' , nf = 64 , rf = 3 , stride = 1 , init_scale = np . sqrt ( 2 ) , ** conv_kwargs ) ) h3 = conv_to_fc ( h3 ) return activ ( fc ( h3 , 'fc1' , nh = 512 , init_scale = np . sqrt ( 2 ) ) )
13818	def _ConvertMessage ( value , message ) : message_descriptor = message . DESCRIPTOR full_name = message_descriptor . full_name if _IsWrapperMessage ( message_descriptor ) : _ConvertWrapperMessage ( value , message ) elif full_name in _WKTJSONMETHODS : _WKTJSONMETHODS [ full_name ] [ 1 ] ( value , message ) else : _ConvertFieldValuePair ( value , message )
4366	def process_event ( self , packet ) : args = packet [ 'args' ] name = packet [ 'name' ] if not allowed_event_name_regex . match ( name ) : self . error ( "unallowed_event_name" , "name must only contains alpha numerical characters" ) return method_name = 'on_' + name . replace ( ' ' , '_' ) return self . call_method_with_acl ( method_name , packet , * args )
2583	def load ( cls , config : Optional [ Config ] = None ) : if cls . _dfk is not None : raise RuntimeError ( 'Config has already been loaded' ) if config is None : cls . _dfk = DataFlowKernel ( Config ( ) ) else : cls . _dfk = DataFlowKernel ( config ) return cls . _dfk
4389	def adsGetLocalAddressEx ( port ) : get_local_address_ex = _adsDLL . AdsGetLocalAddressEx ams_address_struct = SAmsAddr ( ) error_code = get_local_address_ex ( port , ctypes . pointer ( ams_address_struct ) ) if error_code : raise ADSError ( error_code ) local_ams_address = AmsAddr ( ) local_ams_address . _ams_addr = ams_address_struct return local_ams_address
6423	def dist ( self , src , tar , word_approx_min = 0.3 , char_approx_min = 0.73 , tests = 2 ** 12 - 1 , ) : return ( synoname ( src , tar , word_approx_min , char_approx_min , tests , False ) / 14 )
11578	def send_command ( self , command ) : send_message = "" for i in command : send_message += chr ( i ) for data in send_message : self . pymata . transport . write ( data )
5323	def _interrupt_read ( self ) : data = self . _device . read ( ENDPOINT , REQ_INT_LEN , timeout = TIMEOUT ) LOGGER . debug ( 'Read data: %r' , data ) return data
13907	def create_commands ( self , commands , parser ) : self . apply_defaults ( commands ) def create_single_command ( command ) : keys = command [ 'keys' ] del command [ 'keys' ] kwargs = { } for item in command : kwargs [ item ] = command [ item ] parser . add_argument ( * keys , ** kwargs ) if len ( commands ) > 1 : for command in commands : create_single_command ( command ) else : create_single_command ( commands [ 0 ] )
9955	def custom_showtraceback ( self , exc_tuple = None , filename = None , tb_offset = None , exception_only = False , running_compiled_code = False , ) : self . default_showtraceback ( exc_tuple , filename , tb_offset , exception_only = True , running_compiled_code = running_compiled_code , )
4169	def zpk2tf ( z , p , k ) : r import scipy . signal b , a = scipy . signal . zpk2tf ( z , p , k ) return b , a
1781	def AAM ( cpu , imm = None ) : if imm is None : imm = 10 else : imm = imm . read ( ) cpu . AH = Operators . UDIV ( cpu . AL , imm ) cpu . AL = Operators . UREM ( cpu . AL , imm ) cpu . _calculate_logic_flags ( 8 , cpu . AL )
3586	def add ( self , cbobject , metadata ) : with self . _lock : if cbobject not in self . _metadata : self . _metadata [ cbobject ] = metadata return self . _metadata [ cbobject ]
6286	def supports_file ( cls , meta ) : path = Path ( meta . path ) for ext in cls . file_extensions : if path . suffixes [ : len ( ext ) ] == ext : return True return False
12554	def sav_to_pandas_rpy2 ( input_file ) : import pandas . rpy . common as com w = com . robj . r ( 'foreign::read.spss("%s", to.data.frame=TRUE)' % input_file ) return com . convert_robj ( w )
11201	def strip_comment_line_with_symbol ( line , start ) : parts = line . split ( start ) counts = [ len ( findall ( r'(?:^|[^"\\]|(?:\\\\|\\")+)(")' , part ) ) for part in parts ] total = 0 for nr , count in enumerate ( counts ) : total += count if total % 2 == 0 : return start . join ( parts [ : nr + 1 ] ) . rstrip ( ) else : return line . rstrip ( )
13402	def selectedLogs ( self ) : mcclogs = [ ] physlogs = [ ] for i in range ( len ( self . logMenus ) ) : logType = self . logMenus [ i ] . selectedType ( ) log = self . logMenus [ i ] . selectedProgram ( ) if logType == "MCC" : if log not in mcclogs : mcclogs . append ( log ) elif logType == "Physics" : if log not in physlogs : physlogs . append ( log ) return mcclogs , physlogs
12280	def find_matching_files ( self , includes ) : if len ( includes ) == 0 : return [ ] files = [ f [ 'relativepath' ] for f in self . package [ 'resources' ] ] includes = r'|' . join ( [ fnmatch . translate ( x ) for x in includes ] ) files = [ f for f in files if re . match ( includes , os . path . basename ( f ) ) ] + [ f for f in files if re . match ( includes , f ) ] files = list ( set ( files ) ) return files
2835	def write ( self , data , assert_ss = True , deassert_ss = True ) : if self . _mosi is None : raise RuntimeError ( 'Write attempted with no MOSI pin specified.' ) if assert_ss and self . _ss is not None : self . _gpio . set_low ( self . _ss ) for byte in data : for i in range ( 8 ) : if self . _write_shift ( byte , i ) & self . _mask : self . _gpio . set_high ( self . _mosi ) else : self . _gpio . set_low ( self . _mosi ) self . _gpio . output ( self . _sclk , not self . _clock_base ) self . _gpio . output ( self . _sclk , self . _clock_base ) if deassert_ss and self . _ss is not None : self . _gpio . set_high ( self . _ss )
4613	def blocks ( self , start = None , stop = None ) : self . block_interval = self . get_block_interval ( ) if not start : start = self . get_current_block_num ( ) while True : if stop : head_block = stop else : head_block = self . get_current_block_num ( ) for blocknum in range ( start , head_block + 1 ) : block = self . wait_for_and_get_block ( blocknum ) block . update ( { "block_num" : blocknum } ) yield block start = head_block + 1 if stop and start > stop : return time . sleep ( self . block_interval )
12538	def get_unique_field_values ( dcm_file_list , field_name ) : field_values = set ( ) for dcm in dcm_file_list : field_values . add ( str ( DicomFile ( dcm ) . get_attributes ( field_name ) ) ) return field_values
9300	def list ( self , filters , cursor , count ) : assert isinstance ( filters , dict ) , "expected filters type 'dict'" assert isinstance ( cursor , dict ) , "expected cursor type 'dict'" query = self . get_query ( ) assert isinstance ( query , peewee . Query ) paginator = self . get_paginator ( ) assert isinstance ( paginator , Pagination ) count += 1 pquery = paginator . filter_query ( query , cursor , count ) items = [ item for item in pquery ] next_item = items . pop ( 1 ) next_cursor = next_item . to_cursor_ref ( ) return items , next_cursor
8021	async def websocket_close ( self , message , stream_name ) : if stream_name in self . applications_accepting_frames : self . applications_accepting_frames . remove ( stream_name ) if self . closing : return if not self . applications_accepting_frames : await self . close ( message . get ( "code" ) )
3022	def _detect_gce_environment ( ) : http = transport . get_http_object ( timeout = GCE_METADATA_TIMEOUT ) try : response , _ = transport . request ( http , _GCE_METADATA_URI , headers = _GCE_HEADERS ) return ( response . status == http_client . OK and response . get ( _METADATA_FLAVOR_HEADER ) == _DESIRED_METADATA_FLAVOR ) except socket . error : logger . info ( 'Timeout attempting to reach GCE metadata service.' ) return False
9524	def sort_by_size ( infile , outfile , smallest_first = False ) : seqs = { } file_to_dict ( infile , seqs ) seqs = list ( seqs . values ( ) ) seqs . sort ( key = lambda x : len ( x ) , reverse = not smallest_first ) fout = utils . open_file_write ( outfile ) for seq in seqs : print ( seq , file = fout ) utils . close ( fout )
6227	def _translate_string ( self , data , length ) : for index , char in enumerate ( data ) : if index == length : break yield self . _meta . characters - 1 - self . _ct [ char ]
3126	def get ( self , template_id , ** queryparams ) : self . template_id = template_id return self . _mc_client . _get ( url = self . _build_path ( template_id ) , ** queryparams )
5159	def _add_tc_script ( self ) : context = dict ( tc_options = self . config . get ( 'tc_options' , [ ] ) ) contents = self . _render_template ( 'tc_script.sh' , context ) self . config . setdefault ( 'files' , [ ] ) self . _add_unique_file ( { "path" : "/tc_script.sh" , "contents" : contents , "mode" : "755" } )
8238	def analogous ( clr , angle = 10 , contrast = 0.25 ) : contrast = max ( 0 , min ( contrast , 1.0 ) ) clr = color ( clr ) colors = colorlist ( clr ) for i , j in [ ( 1 , 2.2 ) , ( 2 , 1 ) , ( - 1 , - 0.5 ) , ( - 2 , 1 ) ] : c = clr . rotate_ryb ( angle * i ) t = 0.44 - j * 0.1 if clr . brightness - contrast * j < t : c . brightness = t else : c . brightness = clr . brightness - contrast * j c . saturation -= 0.05 colors . append ( c ) return colors
488	def _trackInstanceAndCheckForConcurrencyViolation ( self ) : global g_max_concurrency , g_max_concurrency_raise_exception assert g_max_concurrency is not None assert self not in self . _clsOutstandingInstances , repr ( self ) self . _creationTracebackString = traceback . format_stack ( ) if self . _clsNumOutstanding >= g_max_concurrency : errorMsg = ( "With numOutstanding=%r, exceeded concurrency limit=%r " "when requesting %r. OTHER TRACKED UNRELEASED " "INSTANCES (%s): %r" ) % ( self . _clsNumOutstanding , g_max_concurrency , self , len ( self . _clsOutstandingInstances ) , self . _clsOutstandingInstances , ) self . _logger . error ( errorMsg ) if g_max_concurrency_raise_exception : raise ConcurrencyExceededError ( errorMsg ) self . _clsOutstandingInstances . add ( self ) self . _addedToInstanceSet = True return
9640	def require_template_debug ( f ) : def _ ( * args , ** kwargs ) : TEMPLATE_DEBUG = getattr ( settings , 'TEMPLATE_DEBUG' , False ) return f ( * args , ** kwargs ) if TEMPLATE_DEBUG else '' return _
6831	def get_current_commit ( self ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : s = str ( self . local ( 'git rev-parse HEAD' , capture = True ) ) self . vprint ( 'current commit:' , s ) return s
3409	def knock_out ( self ) : self . functional = False for reaction in self . reactions : if not reaction . functional : reaction . bounds = ( 0 , 0 )
6094	def mapping_matrix_from_sub_to_pix ( sub_to_pix , pixels , regular_pixels , sub_to_regular , sub_grid_fraction ) : mapping_matrix = np . zeros ( ( regular_pixels , pixels ) ) for sub_index in range ( sub_to_regular . shape [ 0 ] ) : mapping_matrix [ sub_to_regular [ sub_index ] , sub_to_pix [ sub_index ] ] += sub_grid_fraction return mapping_matrix
5696	def copy ( cls , conn , ** where ) : cur = conn . cursor ( ) if where and cls . copy_where : copy_where = cls . copy_where . format ( ** where ) else : copy_where = '' cur . execute ( 'INSERT INTO %s ' 'SELECT * FROM source.%s %s' % ( cls . table , cls . table , copy_where ) )
6171	def freq_resp ( self , mode = 'dB' , fs = 8000 , ylim = [ - 100 , 2 ] ) : iir_d . freqz_resp_cas_list ( [ self . sos ] , mode , fs = fs ) pylab . grid ( ) pylab . ylim ( ylim )
3825	async def get_group_conversation_url ( self , get_group_conversation_url_request ) : response = hangouts_pb2 . GetGroupConversationUrlResponse ( ) await self . _pb_request ( 'conversations/getgroupconversationurl' , get_group_conversation_url_request , response ) return response
6346	def _language ( self , name , name_mode ) : name = name . strip ( ) . lower ( ) rules = BMDATA [ name_mode ] [ 'language_rules' ] all_langs = ( sum ( _LANG_DICT [ _ ] for _ in BMDATA [ name_mode ] [ 'languages' ] ) - 1 ) choices_remaining = all_langs for rule in rules : letters , languages , accept = rule if search ( letters , name ) is not None : if accept : choices_remaining &= languages else : choices_remaining &= ( ~ languages ) % ( all_langs + 1 ) if choices_remaining == L_NONE : choices_remaining = L_ANY return choices_remaining
339	def log_every_n ( level , msg , n , * args ) : count = _GetNextLogCountPerToken ( _GetFileAndLine ( ) ) log_if ( level , msg , not ( count % n ) , * args )
11632	def codepointsInNamelist ( namFilename , unique_glyphs = False , cache = None ) : key = 'charset' if not unique_glyphs else 'ownCharset' internals_dir = os . path . dirname ( os . path . abspath ( __file__ ) ) target = os . path . join ( internals_dir , namFilename ) result = readNamelist ( target , unique_glyphs , cache ) return result [ key ]
10613	def H ( self , H ) : self . _H = H self . _T = self . _calculate_T ( H )
11049	def _handle_field_value ( self , field , value ) : if field == 'event' : self . _event = value elif field == 'data' : self . _data_lines . append ( value ) elif field == 'id' : pass elif field == 'retry' : pass
3843	def from_timestamp ( microsecond_timestamp ) : return datetime . datetime . fromtimestamp ( microsecond_timestamp // 1000000 , datetime . timezone . utc ) . replace ( microsecond = ( microsecond_timestamp % 1000000 ) )
1139	def wrap ( text , width = 70 , ** kwargs ) : w = TextWrapper ( width = width , ** kwargs ) return w . wrap ( text )
10223	def get_regulatory_pairs ( graph : BELGraph ) -> Set [ NodePair ] : cg = get_causal_subgraph ( graph ) results = set ( ) for u , v , d in cg . edges ( data = True ) : if d [ RELATION ] not in CAUSAL_INCREASE_RELATIONS : continue if cg . has_edge ( v , u ) and any ( dd [ RELATION ] in CAUSAL_DECREASE_RELATIONS for dd in cg [ v ] [ u ] . values ( ) ) : results . add ( ( u , v ) ) return results
8866	def make_python_patterns ( additional_keywords = [ ] , additional_builtins = [ ] ) : kw = r"\b" + any ( "keyword" , kwlist + additional_keywords ) + r"\b" kw_namespace = r"\b" + any ( "namespace" , kw_namespace_list ) + r"\b" word_operators = r"\b" + any ( "operator_word" , wordop_list ) + r"\b" builtinlist = [ str ( name ) for name in dir ( builtins ) if not name . startswith ( '_' ) ] + additional_builtins for v in [ 'None' , 'True' , 'False' ] : builtinlist . remove ( v ) builtin = r"([^.'\"\\#]\b|^)" + any ( "builtin" , builtinlist ) + r"\b" builtin_fct = any ( "builtin_fct" , [ r'_{2}[a-zA-Z_]*_{2}' ] ) comment = any ( "comment" , [ r"#[^\n]*" ] ) instance = any ( "instance" , [ r"\bself\b" , r"\bcls\b" ] ) decorator = any ( 'decorator' , [ r'@\w*' , r'.setter' ] ) number = any ( "number" , [ r"\b[+-]?[0-9]+[lLjJ]?\b" , r"\b[+-]?0[xX][0-9A-Fa-f]+[lL]?\b" , r"\b[+-]?0[oO][0-7]+[lL]?\b" , r"\b[+-]?0[bB][01]+[lL]?\b" , r"\b[+-]?[0-9]+(?:\.[0-9]+)?(?:[eE][+-]?[0-9]+)?[jJ]?\b" ] ) sqstring = r"(\b[rRuU])?'[^'\\\n]*(\\.[^'\\\n]*)*'?" dqstring = r'(\b[rRuU])?"[^"\\\n]*(\\.[^"\\\n]*)*"?' uf_sqstring = r"(\b[rRuU])?'[^'\\\n]*(\\.[^'\\\n]*)*(\\)$(?!')$" uf_dqstring = r'(\b[rRuU])?"[^"\\\n]*(\\.[^"\\\n]*)*(\\)$(?!")$' sq3string = r"(\b[rRuU])?)?" dq3string = r'(\b[rRuU])?)?' uf_sq3string = r"(\b[rRuU])?)$" uf_dq3string = r'(\b[rRuU])?)$' string = any ( "string" , [ sq3string , dq3string , sqstring , dqstring ] ) ufstring1 = any ( "uf_sqstring" , [ uf_sqstring ] ) ufstring2 = any ( "uf_dqstring" , [ uf_dqstring ] ) ufstring3 = any ( "uf_sq3string" , [ uf_sq3string ] ) ufstring4 = any ( "uf_dq3string" , [ uf_dq3string ] ) return "|" . join ( [ instance , decorator , kw , kw_namespace , builtin , word_operators , builtin_fct , comment , ufstring1 , ufstring2 , ufstring3 , ufstring4 , string , number , any ( "SYNC" , [ r"\n" ] ) ] )
6195	def _get_group_randomstate ( rs , seed , group ) : if rs is None : rs = np . random . RandomState ( seed = seed ) if 'last_random_state' in group . _v_attrs : rs . set_state ( group . _v_attrs [ 'last_random_state' ] ) print ( "INFO: Random state set to last saved state in '%s'." % group . _v_name ) else : print ( "INFO: Random state initialized from seed (%d)." % seed ) return rs
3154	def get ( self , list_id , segment_id ) : return self . _mc_client . _get ( url = self . _build_path ( list_id , 'segments' , segment_id ) )
733	def inferSingleStep ( self , patternNZ , weightMatrix ) : outputActivation = weightMatrix [ patternNZ ] . sum ( axis = 0 ) outputActivation = outputActivation - numpy . max ( outputActivation ) expOutputActivation = numpy . exp ( outputActivation ) predictDist = expOutputActivation / numpy . sum ( expOutputActivation ) return predictDist
2920	def _send_call ( self , my_task ) : args , kwargs = None , None if self . args : args = _eval_args ( self . args , my_task ) if self . kwargs : kwargs = _eval_kwargs ( self . kwargs , my_task ) LOG . debug ( "%s (task id %s) calling %s" % ( self . name , my_task . id , self . call ) , extra = dict ( data = dict ( args = args , kwargs = kwargs ) ) ) async_call = default_app . send_task ( self . call , args = args , kwargs = kwargs ) my_task . _set_internal_data ( task_id = async_call . task_id ) my_task . async_call = async_call LOG . debug ( "'%s' called: %s" % ( self . call , my_task . async_call . task_id ) )
2925	def _on_ready ( self , my_task ) : assert my_task is not None self . test ( ) for lock in self . locks : mutex = my_task . workflow . _get_mutex ( lock ) if not mutex . testandset ( ) : return for assignment in self . pre_assign : assignment . assign ( my_task , my_task ) self . _on_ready_before_hook ( my_task ) self . reached_event . emit ( my_task . workflow , my_task ) self . _on_ready_hook ( my_task ) if self . ready_event . emit ( my_task . workflow , my_task ) : for assignment in self . post_assign : assignment . assign ( my_task , my_task ) for lock in self . locks : mutex = my_task . workflow . _get_mutex ( lock ) mutex . unlock ( ) self . finished_event . emit ( my_task . workflow , my_task )
8111	def search_blogs ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_BLOGS return GoogleSearch ( q , start , service , "" , wait , asynchronous , cached )
1240	def _move ( self , index , new_priority ) : item , old_priority = self . _memory [ index ] old_priority = old_priority or 0 self . _memory [ index ] = _SumRow ( item , new_priority ) self . _update_internal_nodes ( index , new_priority - old_priority )
6437	def dist_abs ( self , src , tar , weights = 'exponential' , max_length = 8 , normalized = False ) : xored = eudex ( src , max_length = max_length ) ^ eudex ( tar , max_length = max_length ) if not weights : binary = bin ( xored ) distance = binary . count ( '1' ) if normalized : return distance / ( len ( binary ) - 2 ) return distance if callable ( weights ) : weights = weights ( ) elif weights == 'exponential' : weights = Eudex . gen_exponential ( ) elif weights == 'fibonacci' : weights = Eudex . gen_fibonacci ( ) if isinstance ( weights , GeneratorType ) : weights = [ next ( weights ) for _ in range ( max_length ) ] [ : : - 1 ] distance = 0 max_distance = 0 while ( xored or normalized ) and weights : max_distance += 8 * weights [ - 1 ] distance += bin ( xored & 0xFF ) . count ( '1' ) * weights . pop ( ) xored >>= 8 if normalized : distance /= max_distance return distance
12394	def try_delegation ( method ) : @ functools . wraps ( method ) def delegator ( self , * args , ** kwargs ) : if self . try_delegation : inst = getattr ( self , 'inst' , None ) if inst is not None : method_name = ( self . delegator_prefix or '' ) + method . __name__ func = getattr ( inst , method_name , None ) if func is not None : return func ( * args , ** kwargs ) return method ( self , * args , ** kwargs ) return delegator
8957	def included ( self , path , is_dir = False ) : inclusive = None for pattern in self . patterns : if pattern . is_dir == is_dir and pattern . matches ( path ) : inclusive = pattern . inclusive return inclusive
682	def getSDRforValue ( self , i , j ) : assert len ( self . fields ) > i assert self . fields [ i ] . numRecords > j encoding = self . fields [ i ] . encodings [ j ] return encoding
2487	def create_conjunction_node ( self , conjunction ) : node = BNode ( ) type_triple = ( node , RDF . type , self . spdx_namespace . ConjunctiveLicenseSet ) self . graph . add ( type_triple ) licenses = self . licenses_from_tree ( conjunction ) for lic in licenses : member_triple = ( node , self . spdx_namespace . member , lic ) self . graph . add ( member_triple ) return node
946	def _isCheckpointDir ( checkpointDir ) : lastSegment = os . path . split ( checkpointDir ) [ 1 ] if lastSegment [ 0 ] == '.' : return False if not checkpointDir . endswith ( g_defaultCheckpointExtension ) : return False if not os . path . isdir ( checkpointDir ) : return False return True
9465	def conference_record_start ( self , call_params ) : path = '/' + self . api_version + '/ConferenceRecordStart/' method = 'POST' return self . request ( path , method , call_params )
5401	def _get_logging_env ( self , logging_uri , user_project ) : if not logging_uri . endswith ( '.log' ) : raise ValueError ( 'Logging URI must end in ".log": {}' . format ( logging_uri ) ) logging_prefix = logging_uri [ : - len ( '.log' ) ] return { 'LOGGING_PATH' : '{}.log' . format ( logging_prefix ) , 'STDOUT_PATH' : '{}-stdout.log' . format ( logging_prefix ) , 'STDERR_PATH' : '{}-stderr.log' . format ( logging_prefix ) , 'USER_PROJECT' : user_project , }
2544	def set_file_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_comment_set : self . file_comment_set = True self . file ( doc ) . comment = text return True else : raise CardinalityError ( 'File::Comment' ) else : raise OrderError ( 'File::Comment' )
2630	def scale_in ( self , blocks = 0 , machines = 0 , strategy = None ) : count = 0 instances = self . client . servers . list ( ) for instance in instances [ 0 : machines ] : print ( "Deleting : " , instance ) instance . delete ( ) count += 1 return count
3719	def conductivity ( CASRN = None , AvailableMethods = False , Method = None , full_info = True ) : r def list_methods ( ) : methods = [ ] if CASRN in Lange_cond_pure . index : methods . append ( LANGE_COND ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == LANGE_COND : kappa = float ( Lange_cond_pure . at [ CASRN , 'Conductivity' ] ) if full_info : T = float ( Lange_cond_pure . at [ CASRN , 'T' ] ) elif Method == NONE : kappa , T = None , None else : raise Exception ( 'Failure in in function' ) if full_info : return kappa , T else : return kappa
2204	def find_exe ( name , multi = False , path = None ) : candidates = find_path ( name , path = path , exact = True ) mode = os . X_OK | os . F_OK results = ( fpath for fpath in candidates if os . access ( fpath , mode ) and not isdir ( fpath ) ) if not multi : for fpath in results : return fpath else : return list ( results )
8494	def _error ( msg , * args ) : print ( msg % args , file = sys . stderr ) sys . exit ( 1 )
7032	def check_existing_apikey ( lcc_server ) : USERHOME = os . path . expanduser ( '~' ) APIKEYFILE = os . path . join ( USERHOME , '.astrobase' , 'lccs' , 'apikey-%s' % lcc_server . replace ( 'https://' , 'https-' ) . replace ( 'http://' , 'http-' ) ) if os . path . exists ( APIKEYFILE ) : fileperm = oct ( os . stat ( APIKEYFILE ) [ stat . ST_MODE ] ) if fileperm == '0100600' or fileperm == '0o100600' : with open ( APIKEYFILE ) as infd : apikey , expires = infd . read ( ) . strip ( '\n' ) . split ( ) now = datetime . now ( utc ) if sys . version_info [ : 2 ] < ( 3 , 7 ) : expdt = datetime . strptime ( expires . replace ( 'Z' , '' ) , '%Y-%m-%dT%H:%M:%S.%f' ) . replace ( tzinfo = utc ) else : expdt = datetime . fromisoformat ( expires . replace ( 'Z' , '+00:00' ) ) if now > expdt : LOGERROR ( 'API key has expired. expiry was on: %s' % expires ) return False , apikey , expires else : return True , apikey , expires else : LOGWARNING ( 'The API key file %s has bad permissions ' 'and is insecure, not reading it.\n' '(you need to chmod 600 this file)' % APIKEYFILE ) return False , None , None else : LOGWARNING ( 'No LCC-Server API key ' 'found in: {apikeyfile}' . format ( apikeyfile = APIKEYFILE ) ) return False , None , None
6254	def root_path ( ) : module_dir = os . path . dirname ( globals ( ) [ '__file__' ] ) return os . path . dirname ( os . path . dirname ( module_dir ) )
7286	def has_delete_permission ( self , request ) : return request . user . is_authenticated and request . user . is_active and request . user . is_superuser
1542	def queries_map ( ) : qs = _all_metric_queries ( ) return dict ( zip ( qs [ 0 ] , qs [ 1 ] ) + zip ( qs [ 2 ] , qs [ 3 ] ) )
5875	def get_images_bytesize_match ( self , images ) : cnt = 0 max_bytes_size = 15728640 good_images = [ ] for image in images : if cnt > 30 : return good_images src = self . parser . getAttribute ( image , attr = 'src' ) src = self . build_image_path ( src ) src = self . add_schema_if_none ( src ) local_image = self . get_local_image ( src ) if local_image : filesize = local_image . bytes if ( filesize == 0 or filesize > self . images_min_bytes ) and filesize < max_bytes_size : good_images . append ( image ) else : images . remove ( image ) cnt += 1 return good_images if len ( good_images ) > 0 else None
3551	def list_descriptors ( self ) : paths = self . _props . Get ( _CHARACTERISTIC_INTERFACE , 'Descriptors' ) return map ( BluezGattDescriptor , get_provider ( ) . _get_objects_by_path ( paths ) )
10149	def _ref ( self , resp , base_name = None ) : name = base_name or resp . get ( 'title' , '' ) or resp . get ( 'name' , '' ) pointer = self . json_pointer + name self . response_registry [ name ] = resp return { '$ref' : pointer }
10616	def clear ( self ) : self . _compound_masses = self . _compound_masses * 0.0 self . _P = 1.0 self . _T = 25.0 self . _H = 0.0
6553	def fix_variable ( self , v , value ) : variables = self . variables try : idx = variables . index ( v ) except ValueError : raise ValueError ( "given variable {} is not part of the constraint" . format ( v ) ) if value not in self . vartype . value : raise ValueError ( "expected value to be in {}, received {} instead" . format ( self . vartype . value , value ) ) configurations = frozenset ( config [ : idx ] + config [ idx + 1 : ] for config in self . configurations if config [ idx ] == value ) if not configurations : raise UnsatError ( "fixing {} to {} makes this constraint unsatisfiable" . format ( v , value ) ) variables = variables [ : idx ] + variables [ idx + 1 : ] self . configurations = configurations self . variables = variables def func ( * args ) : return args in configurations self . func = func self . name = '{} ({} fixed to {})' . format ( self . name , v , value )
7753	def route_stanza ( self , stanza ) : if stanza . stanza_type not in ( "error" , "result" ) : response = stanza . make_error_response ( u"recipient-unavailable" ) self . send ( response ) return True
8049	def run ( self ) : if self . err is not None : assert self . source is None msg = "%s%03i %s" % ( rst_prefix , rst_fail_load , "Failed to load file: %s" % self . err , ) yield 0 , 0 , msg , type ( self ) module = [ ] try : module = parse ( StringIO ( self . source ) , self . filename ) except SyntaxError as err : msg = "%s%03i %s" % ( rst_prefix , rst_fail_parse , "Failed to parse file: %s" % err , ) yield 0 , 0 , msg , type ( self ) module = [ ] except AllError : msg = "%s%03i %s" % ( rst_prefix , rst_fail_all , "Failed to parse __all__ entry." , ) yield 0 , 0 , msg , type ( self ) module = [ ] for definition in module : if not definition . docstring : continue try : unindented = trim ( dequote_docstring ( definition . docstring ) ) rst_errors = list ( rst_lint . lint ( unindented ) ) except Exception as err : msg = "%s%03i %s" % ( rst_prefix , rst_fail_lint , "Failed to lint docstring: %s - %s" % ( definition . name , err ) , ) yield definition . start , 0 , msg , type ( self ) continue for rst_error in rst_errors : if rst_error . level <= 1 : continue msg = rst_error . message . split ( "\n" , 1 ) [ 0 ] code = code_mapping ( rst_error . level , msg ) assert code < 100 , code code += 100 * rst_error . level msg = "%s%03i %s" % ( rst_prefix , code , msg ) yield definition . start + rst_error . line , 0 , msg , type ( self )
5493	def validate_config_key ( ctx , param , value ) : if not value : return value try : section , item = value . split ( "." , 1 ) except ValueError : raise click . BadArgumentUsage ( "Given key does not contain a section name." ) else : return section , item
2679	def get_account_id ( profile_name , aws_access_key_id , aws_secret_access_key , region = None , ) : client = get_client ( 'sts' , profile_name , aws_access_key_id , aws_secret_access_key , region , ) return client . get_caller_identity ( ) . get ( 'Account' )
11573	def clear_display_buffer ( self ) : for row in range ( 0 , 8 ) : self . firmata . i2c_write ( 0x70 , row * 2 , 0 , 0 ) self . firmata . i2c_write ( 0x70 , ( row * 2 ) + 1 , 0 , 0 ) for column in range ( 0 , 8 ) : self . display_buffer [ row ] [ column ] = 0
11154	def auto_complete_choices ( self , case_sensitive = False ) : self_basename = self . basename self_basename_lower = self . basename . lower ( ) if case_sensitive : def match ( basename ) : return basename . startswith ( self_basename ) else : def match ( basename ) : return basename . lower ( ) . startswith ( self_basename_lower ) choices = list ( ) if self . is_dir ( ) : choices . append ( self ) for p in self . sort_by_abspath ( self . select ( recursive = False ) ) : choices . append ( p ) else : p_parent = self . parent if p_parent . is_dir ( ) : for p in self . sort_by_abspath ( p_parent . select ( recursive = False ) ) : if match ( p . basename ) : choices . append ( p ) else : raise ValueError ( "'%s' directory does not exist!" % p_parent ) return choices
2598	def can_sequence ( obj ) : if istype ( obj , sequence_types ) : t = type ( obj ) return t ( [ can ( i ) for i in obj ] ) else : return obj
6987	def _varfeatures_worker ( task ) : try : ( lcfile , outdir , timecols , magcols , errcols , mindet , lcformat , lcformatdir ) = task return get_varfeatures ( lcfile , outdir , timecols = timecols , magcols = magcols , errcols = errcols , mindet = mindet , lcformat = lcformat , lcformatdir = lcformatdir ) except Exception as e : return None
6088	def scaled_noise_map_from_hyper_galaxies_and_contribution_maps ( contribution_maps , hyper_galaxies , noise_map ) : scaled_noise_maps = list ( map ( lambda hyper_galaxy , contribution_map : hyper_galaxy . hyper_noise_from_contributions ( noise_map = noise_map , contributions = contribution_map ) , hyper_galaxies , contribution_maps ) ) return noise_map + sum ( scaled_noise_maps )
2476	def set_lic_comment ( self , doc , comment ) : if self . has_extr_lic ( doc ) : if not self . extr_lic_comment_set : self . extr_lic_comment_set = True if validations . validate_is_free_form_text ( comment ) : self . extr_lic ( doc ) . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'ExtractedLicense::comment' ) else : raise CardinalityError ( 'ExtractedLicense::comment' ) else : raise OrderError ( 'ExtractedLicense::comment' )
2212	def touch ( fpath , mode = 0o666 , dir_fd = None , verbose = 0 , ** kwargs ) : if verbose : print ( 'Touching file {}' . format ( fpath ) ) if six . PY2 : with open ( fpath , 'a' ) : os . utime ( fpath , None ) else : flags = os . O_CREAT | os . O_APPEND with os . fdopen ( os . open ( fpath , flags = flags , mode = mode , dir_fd = dir_fd ) ) as f : os . utime ( f . fileno ( ) if os . utime in os . supports_fd else fpath , dir_fd = None if os . supports_fd else dir_fd , ** kwargs ) return fpath
4819	def connect ( self ) : if JwtBuilder is None : raise NotConnectedToOpenEdX ( "This package must be installed in an OpenEdX environment." ) now = int ( time ( ) ) jwt = JwtBuilder . create_jwt_for_user ( self . user ) self . client = EdxRestApiClient ( self . API_BASE_URL , append_slash = self . APPEND_SLASH , jwt = jwt , ) self . expires_at = now + self . expires_in
3607	def put ( self , url , name , data , params = None , headers = None , connection = None ) : assert name , 'Snapshot name must be specified' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) data = json . dumps ( data , cls = JSONEncoder ) return make_put_request ( endpoint , data , params , headers , connection = connection )
565	def updateResultsForJob ( self , forceUpdate = True ) : updateInterval = time . time ( ) - self . _lastUpdateAttemptTime if updateInterval < self . _MIN_UPDATE_INTERVAL and not forceUpdate : return self . logger . info ( "Attempting model selection for jobID=%d: time=%f" " lastUpdate=%f" % ( self . _jobID , time . time ( ) , self . _lastUpdateAttemptTime ) ) timestampUpdated = self . _cjDB . jobUpdateSelectionSweep ( self . _jobID , self . _MIN_UPDATE_INTERVAL ) if not timestampUpdated : self . logger . info ( "Unable to update selection sweep timestamp: jobID=%d" " updateTime=%f" % ( self . _jobID , self . _lastUpdateAttemptTime ) ) if not forceUpdate : return self . _lastUpdateAttemptTime = time . time ( ) self . logger . info ( "Succesfully updated selection sweep timestamp jobid=%d updateTime=%f" % ( self . _jobID , self . _lastUpdateAttemptTime ) ) minUpdateRecords = self . _MIN_UPDATE_THRESHOLD jobResults = self . _getJobResults ( ) if forceUpdate or jobResults is None : minUpdateRecords = 0 candidateIDs , bestMetric = self . _cjDB . modelsGetCandidates ( self . _jobID , minUpdateRecords ) self . logger . info ( "Candidate models=%s, metric=%s, jobID=%s" % ( candidateIDs , bestMetric , self . _jobID ) ) if len ( candidateIDs ) == 0 : return self . _jobUpdateCandidate ( candidateIDs [ 0 ] , bestMetric , results = jobResults )
4582	def construct ( cls , project , * , run = None , name = None , data = None , ** desc ) : from . failed import Failed exception = desc . pop ( '_exception' , None ) if exception : a = Failed ( project . layout , desc , exception ) else : try : a = cls ( project . layout , ** desc ) a . _set_runner ( run or { } ) except Exception as e : if cls . FAIL_ON_EXCEPTION : raise a = Failed ( project . layout , desc , e ) a . name = name a . data = data return a
6257	def find ( self , path : Path ) : if getattr ( self , 'settings_attr' , None ) : self . paths = getattr ( settings , self . settings_attr ) path_found = None for entry in self . paths : abspath = entry / path if abspath . exists ( ) : path_found = abspath return path_found
920	def critical ( self , msg , * args , ** kwargs ) : self . _baseLogger . critical ( self , self . getExtendedMsg ( msg ) , * args , ** kwargs )
915	def normalize ( lx ) : lx = numpy . asarray ( lx ) base = lx . max ( ) x = numpy . exp ( lx - base ) result = x / x . sum ( ) conventional = ( numpy . exp ( lx ) / numpy . exp ( lx ) . sum ( ) ) assert similar ( result , conventional ) return result
3966	def case_insensitive_rename ( src , dst ) : temp_dir = tempfile . mkdtemp ( ) shutil . rmtree ( temp_dir ) shutil . move ( src , temp_dir ) shutil . move ( temp_dir , dst )
9706	def get_field_settings ( self ) : field_settings = None if self . field_settings : if isinstance ( self . field_settings , six . string_types ) : profiles = settings . CONFIG . get ( self . PROFILE_KEY , { } ) field_settings = profiles . get ( self . field_settings ) else : field_settings = self . field_settings return field_settings
7444	def _step2func ( self , samples , force , ipyclient ) : if self . _headers : print ( "\n Step 2: Filtering reads " ) if not self . samples . keys ( ) : raise IPyradWarningExit ( FIRST_RUN_1 ) samples = _get_samples ( self , samples ) if not force : if all ( [ i . stats . state >= 2 for i in samples ] ) : print ( EDITS_EXIST . format ( len ( samples ) ) ) return assemble . rawedit . run2 ( self , samples , force , ipyclient )
7207	def execute ( self ) : self . generate_workflow_description ( ) if self . batch_values : self . id = self . workflow . launch_batch_workflow ( self . definition ) else : self . id = self . workflow . launch ( self . definition ) return self . id
12730	def axes ( self ) : return [ np . array ( self . ode_obj . getAxis1 ( ) ) , np . array ( self . ode_obj . getAxis2 ( ) ) ]
6405	def cmp_features ( feat1 , feat2 ) : if feat1 < 0 or feat2 < 0 : return - 1.0 if feat1 == feat2 : return 1.0 magnitude = len ( _FEATURE_MASK ) featxor = feat1 ^ feat2 diffbits = 0 while featxor : if featxor & 0b1 : diffbits += 1 featxor >>= 1 return 1 - ( diffbits / ( 2 * magnitude ) )
4453	def alias ( self , alias ) : if alias is FIELDNAME : if not self . _field : raise ValueError ( "Cannot use FIELDNAME alias with no field" ) alias = self . _field [ 1 : ] self . _alias = alias return self
10309	def barv ( d , plt , title = None , rotation = 'vertical' ) : labels = sorted ( d , key = d . get , reverse = True ) index = range ( len ( labels ) ) plt . xticks ( index , labels , rotation = rotation ) plt . bar ( index , [ d [ v ] for v in labels ] ) if title is not None : plt . title ( title )
6020	def simulate_as_gaussian ( cls , shape , pixel_scale , sigma , centre = ( 0.0 , 0.0 ) , axis_ratio = 1.0 , phi = 0.0 ) : from autolens . model . profiles . light_profiles import EllipticalGaussian gaussian = EllipticalGaussian ( centre = centre , axis_ratio = axis_ratio , phi = phi , intensity = 1.0 , sigma = sigma ) grid_1d = grid_util . regular_grid_1d_masked_from_mask_pixel_scales_and_origin ( mask = np . full ( shape , False ) , pixel_scales = ( pixel_scale , pixel_scale ) ) gaussian_1d = gaussian . intensities_from_grid ( grid = grid_1d ) gaussian_2d = mapping_util . map_unmasked_1d_array_to_2d_array_from_array_1d_and_shape ( array_1d = gaussian_1d , shape = shape ) return PSF ( array = gaussian_2d , pixel_scale = pixel_scale , renormalize = True )
8797	def apply_rules ( self , device_id , mac_address , rules ) : LOG . info ( "Applying security group rules for device %s with MAC %s" % ( device_id , mac_address ) ) rule_dict = { SECURITY_GROUP_RULE_KEY : rules } redis_key = self . vif_key ( device_id , mac_address ) self . set_field ( redis_key , SECURITY_GROUP_HASH_ATTR , rule_dict ) self . set_field_raw ( redis_key , SECURITY_GROUP_ACK , False )
10975	def manage ( group_id ) : group = Group . query . get_or_404 ( group_id ) form = GroupForm ( request . form , obj = group ) if form . validate_on_submit ( ) : if group . can_edit ( current_user ) : try : group . update ( ** form . data ) flash ( _ ( 'Group "%(name)s" was updated' , name = group . name ) , 'success' ) except Exception as e : flash ( str ( e ) , 'error' ) return render_template ( "invenio_groups/new.html" , form = form , group = group , ) else : flash ( _ ( 'You cannot edit group %(group_name)s' , group_name = group . name ) , 'error' ) return render_template ( "invenio_groups/new.html" , form = form , group = group , )
13309	def fmt ( a , b ) : return 100 * np . min ( [ a , b ] , axis = 0 ) . sum ( ) / np . max ( [ a , b ] , axis = 0 ) . sum ( )
12224	def convertShpToExtend ( pathToShp ) : driver = ogr . GetDriverByName ( 'ESRI Shapefile' ) dataset = driver . Open ( pathToShp ) if dataset is not None : layer = dataset . GetLayer ( ) spatialRef = layer . GetSpatialRef ( ) feature = layer . GetNextFeature ( ) geom = feature . GetGeometryRef ( ) spatialRef = geom . GetSpatialReference ( ) outSpatialRef = osr . SpatialReference ( ) outSpatialRef . ImportFromEPSG ( 4326 ) coordTrans = osr . CoordinateTransformation ( spatialRef , outSpatialRef ) env = geom . GetEnvelope ( ) pointMAX = ogr . Geometry ( ogr . wkbPoint ) pointMAX . AddPoint ( env [ 1 ] , env [ 3 ] ) pointMAX . Transform ( coordTrans ) pointMIN = ogr . Geometry ( ogr . wkbPoint ) pointMIN . AddPoint ( env [ 0 ] , env [ 2 ] ) pointMIN . Transform ( coordTrans ) return [ pointMAX . GetPoint ( ) [ 1 ] , pointMIN . GetPoint ( ) [ 0 ] , pointMIN . GetPoint ( ) [ 1 ] , pointMAX . GetPoint ( ) [ 0 ] ] else : exit ( " shapefile not found. Please verify your path to the shapefile" )
6451	def flake8_color ( score ) : score_cutoffs = ( 0 , 20 , 50 , 100 , 200 ) for i in range ( len ( score_cutoffs ) ) : if score <= score_cutoffs [ i ] : return BADGE_COLORS [ i ] return BADGE_COLORS [ - 1 ]
3174	def create_or_update ( self , store_id , product_id , variant_id , data ) : self . store_id = store_id self . product_id = product_id self . variant_id = variant_id if 'id' not in data : raise KeyError ( 'The product variant must have an id' ) if 'title' not in data : raise KeyError ( 'The product variant must have a title' ) return self . _mc_client . _put ( url = self . _build_path ( store_id , 'products' , product_id , 'variants' , variant_id ) , data = data )
3321	def delete ( self , token ) : self . _lock . acquire_write ( ) try : lock = self . _dict . get ( token ) _logger . debug ( "delete {}" . format ( lock_string ( lock ) ) ) if lock is None : return False key = "URL2TOKEN:{}" . format ( lock . get ( "root" ) ) if key in self . _dict : tokList = self . _dict [ key ] if len ( tokList ) > 1 : tokList . remove ( token ) self . _dict [ key ] = tokList else : del self . _dict [ key ] del self . _dict [ token ] self . _flush ( ) finally : self . _lock . release ( ) return True
12217	def print_file_info ( ) : tpl = TableLogger ( columns = 'file,created,modified,size' ) for f in os . listdir ( '.' ) : size = os . stat ( f ) . st_size date_created = datetime . fromtimestamp ( os . path . getctime ( f ) ) date_modified = datetime . fromtimestamp ( os . path . getmtime ( f ) ) tpl ( f , date_created , date_modified , size )
4224	def _load_keyring_class ( keyring_name ) : module_name , sep , class_name = keyring_name . rpartition ( '.' ) __import__ ( module_name ) module = sys . modules [ module_name ] return getattr ( module , class_name )
11901	def _run_server ( ) : port = _get_server_port ( ) SocketServer . TCPServer . allow_reuse_address = True server = SocketServer . TCPServer ( ( '' , port ) , SimpleHTTPServer . SimpleHTTPRequestHandler ) print ( 'Your images are at http://127.0.0.1:%d/%s' % ( port , INDEX_FILE_NAME ) ) try : server . serve_forever ( ) except KeyboardInterrupt : print ( 'User interrupted, stopping' ) except Exception as exptn : print ( exptn ) print ( 'Unhandled exception in server, stopping' )
11528	def add_scalar_data ( self , token , community_id , producer_display_name , metric_name , producer_revision , submit_time , value , ** kwargs ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'communityId' ] = community_id parameters [ 'producerDisplayName' ] = producer_display_name parameters [ 'metricName' ] = metric_name parameters [ 'producerRevision' ] = producer_revision parameters [ 'submitTime' ] = submit_time parameters [ 'value' ] = value optional_keys = [ 'config_item_id' , 'test_dataset_id' , 'truth_dataset_id' , 'silent' , 'unofficial' , 'build_results_url' , 'branch' , 'extra_urls' , 'params' , 'submission_id' , 'submission_uuid' , 'unit' , 'reproduction_command' ] for key in optional_keys : if key in kwargs : if key == 'config_item_id' : parameters [ 'configItemId' ] = kwargs [ key ] elif key == 'test_dataset_id' : parameters [ 'testDatasetId' ] = kwargs [ key ] elif key == 'truth_dataset_id' : parameters [ 'truthDatasetId' ] = kwargs [ key ] elif key == 'build_results_url' : parameters [ 'buildResultsUrl' ] = kwargs [ key ] elif key == 'extra_urls' : parameters [ 'extraUrls' ] = json . dumps ( kwargs [ key ] ) elif key == 'params' : parameters [ key ] = json . dumps ( kwargs [ key ] ) elif key == 'silent' : if kwargs [ key ] : parameters [ key ] = kwargs [ key ] elif key == 'unofficial' : if kwargs [ key ] : parameters [ key ] = kwargs [ key ] elif key == 'submission_id' : parameters [ 'submissionId' ] = kwargs [ key ] elif key == 'submission_uuid' : parameters [ 'submissionUuid' ] = kwargs [ key ] elif key == 'unit' : parameters [ 'unit' ] = kwargs [ key ] elif key == 'reproduction_command' : parameters [ 'reproductionCommand' ] = kwargs [ key ] else : parameters [ key ] = kwargs [ key ] response = self . request ( 'midas.tracker.scalar.add' , parameters ) return response
5811	def raise_hostname ( certificate , hostname ) : is_ip = re . match ( '^\\d+\\.\\d+\\.\\d+\\.\\d+$' , hostname ) or hostname . find ( ':' ) != - 1 if is_ip : hostname_type = 'IP address %s' % hostname else : hostname_type = 'domain name %s' % hostname message = 'Server certificate verification failed - %s does not match' % hostname_type valid_ips = ', ' . join ( certificate . valid_ips ) valid_domains = ', ' . join ( certificate . valid_domains ) if valid_domains : message += ' valid domains: %s' % valid_domains if valid_domains and valid_ips : message += ' or' if valid_ips : message += ' valid IP addresses: %s' % valid_ips raise TLSVerificationError ( message , certificate )
2333	def predict_dataset ( self , df ) : if len ( list ( df . columns ) ) == 2 : df . columns = [ "A" , "B" ] if self . model is None : raise AssertionError ( "Model has not been trained before predictions" ) df2 = DataFrame ( ) for idx , row in df . iterrows ( ) : df2 = df2 . append ( row , ignore_index = True ) df2 = df2 . append ( { 'A' : row [ "B" ] , 'B' : row [ "A" ] } , ignore_index = True ) return predict . predict ( deepcopy ( df2 ) , deepcopy ( self . model ) ) [ : : 2 ]
7468	def multi_muscle_align ( data , samples , ipyclient ) : LOGGER . info ( "starting alignments" ) lbview = ipyclient . load_balanced_view ( ) start = time . time ( ) printstr = " aligning clusters | {} | s6 |" elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 20 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) path = os . path . join ( data . tmpdir , data . name + ".chunk_*" ) clustbits = glob . glob ( path ) jobs = { } for idx in xrange ( len ( clustbits ) ) : args = [ data , samples , clustbits [ idx ] ] jobs [ idx ] = lbview . apply ( persistent_popen_align3 , * args ) allwait = len ( jobs ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 20 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) while 1 : finished = [ i . ready ( ) for i in jobs . values ( ) ] fwait = sum ( finished ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( allwait , fwait , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if all ( finished ) : break keys = jobs . keys ( ) for idx in keys : if not jobs [ idx ] . successful ( ) : LOGGER . error ( "error in persistent_popen_align %s" , jobs [ idx ] . exception ( ) ) raise IPyradWarningExit ( "error in step 6 {}" . format ( jobs [ idx ] . exception ( ) ) ) del jobs [ idx ] print ( "" )
8081	def rellineto ( self , x , y ) : if self . _path is None : raise ShoebotError ( _ ( "No current path. Use beginpath() first." ) ) self . _path . rellineto ( x , y )
3856	def unread_events ( self ) : return [ conv_event for conv_event in self . _events if conv_event . timestamp > self . latest_read_timestamp ]
10882	def aN ( a , dim = 3 , dtype = 'int' ) : if not hasattr ( a , '__iter__' ) : return np . array ( [ a ] * dim , dtype = dtype ) return np . array ( a ) . astype ( dtype )
11733	def isValidClass ( self , class_ ) : module = inspect . getmodule ( class_ ) valid = ( module in self . _valid_modules or ( hasattr ( module , '__file__' ) and module . __file__ in self . _valid_named_modules ) ) return valid and not private ( class_ )
11392	def fetch_url ( url , method = 'GET' , user_agent = 'django-oembed' , timeout = SOCKET_TIMEOUT ) : sock = httplib2 . Http ( timeout = timeout ) request_headers = { 'User-Agent' : user_agent , 'Accept-Encoding' : 'gzip' } try : headers , raw = sock . request ( url , headers = request_headers , method = method ) except : raise OEmbedHTTPException ( 'Error fetching %s' % url ) return headers , raw
11486	def _descend_folder_for_id ( parsed_path , folder_id ) : if len ( parsed_path ) == 0 : return folder_id session . token = verify_credentials ( ) base_folder = session . communicator . folder_get ( session . token , folder_id ) cur_folder_id = - 1 for path_part in parsed_path : cur_folder_id = base_folder [ 'folder_id' ] cur_children = session . communicator . folder_children ( session . token , cur_folder_id ) for inner_folder in cur_children [ 'folders' ] : if inner_folder [ 'name' ] == path_part : base_folder = session . communicator . folder_get ( session . token , inner_folder [ 'folder_id' ] ) cur_folder_id = base_folder [ 'folder_id' ] break else : return - 1 return cur_folder_id
714	def loadSavedHyperSearchJob ( cls , permWorkDir , outputLabel ) : jobID = cls . __loadHyperSearchJobID ( permWorkDir = permWorkDir , outputLabel = outputLabel ) searchJob = _HyperSearchJob ( nupicJobID = jobID ) return searchJob
12983	def filename ( file_name , start_on = None , ignore = ( ) , use_short = True , ** queries ) : with open ( file_name ) as template_file : return file ( template_file , start_on = start_on , ignore = ignore , use_short = use_short , ** queries )
4904	def populate_data_sharing_consent ( apps , schema_editor ) : DataSharingConsent = apps . get_model ( 'consent' , 'DataSharingConsent' ) EnterpriseCourseEnrollment = apps . get_model ( 'enterprise' , 'EnterpriseCourseEnrollment' ) User = apps . get_model ( 'auth' , 'User' ) for enrollment in EnterpriseCourseEnrollment . objects . all ( ) : user = User . objects . get ( pk = enrollment . enterprise_customer_user . user_id ) data_sharing_consent , __ = DataSharingConsent . objects . get_or_create ( username = user . username , enterprise_customer = enrollment . enterprise_customer_user . enterprise_customer , course_id = enrollment . course_id , ) if enrollment . consent_granted is not None : data_sharing_consent . granted = enrollment . consent_granted else : consent_state = enrollment . enterprise_customer_user . data_sharing_consent . first ( ) if consent_state is not None : data_sharing_consent . granted = consent_state . state in [ 'enabled' , 'external' ] else : data_sharing_consent . granted = False data_sharing_consent . save ( )
8842	def indent ( self ) : if not self . tab_always_indent : super ( PyIndenterMode , self ) . indent ( ) else : cursor = self . editor . textCursor ( ) assert isinstance ( cursor , QtGui . QTextCursor ) if cursor . hasSelection ( ) : self . indent_selection ( cursor ) else : tab_len = self . editor . tab_length cursor . beginEditBlock ( ) if self . editor . use_spaces_instead_of_tabs : cursor . insertText ( tab_len * " " ) else : cursor . insertText ( '\t' ) cursor . endEditBlock ( ) self . editor . setTextCursor ( cursor )
1597	def format_prefix ( filename , sres ) : try : pwent = pwd . getpwuid ( sres . st_uid ) user = pwent . pw_name except KeyError : user = sres . st_uid try : grent = grp . getgrgid ( sres . st_gid ) group = grent . gr_name except KeyError : group = sres . st_gid return '%s %3d %10s %10s %10d %s' % ( format_mode ( sres ) , sres . st_nlink , user , group , sres . st_size , format_mtime ( sres . st_mtime ) , )
5537	def get_raw_output ( self , tile , _baselevel_readonly = False ) : if not isinstance ( tile , ( BufferedTile , tuple ) ) : raise TypeError ( "'tile' must be a tuple or BufferedTile" ) if isinstance ( tile , tuple ) : tile = self . config . output_pyramid . tile ( * tile ) if _baselevel_readonly : tile = self . config . baselevels [ "tile_pyramid" ] . tile ( * tile . id ) if tile . zoom not in self . config . zoom_levels : return self . config . output . empty ( tile ) if tile . crs != self . config . process_pyramid . crs : raise NotImplementedError ( "reprojection between processes not yet implemented" ) if self . config . mode == "memory" : process_tile = self . config . process_pyramid . intersecting ( tile ) [ 0 ] return self . _extract ( in_tile = process_tile , in_data = self . _execute_using_cache ( process_tile ) , out_tile = tile ) process_tile = self . config . process_pyramid . intersecting ( tile ) [ 0 ] if tile . pixelbuffer > self . config . output . pixelbuffer : output_tiles = list ( self . config . output_pyramid . tiles_from_bounds ( tile . bounds , tile . zoom ) ) else : output_tiles = self . config . output_pyramid . intersecting ( tile ) if self . config . mode == "readonly" or _baselevel_readonly : if self . config . output . tiles_exist ( process_tile ) : return self . _read_existing_output ( tile , output_tiles ) else : return self . config . output . empty ( tile ) elif self . config . mode == "continue" and not _baselevel_readonly : if self . config . output . tiles_exist ( process_tile ) : return self . _read_existing_output ( tile , output_tiles ) else : return self . _process_and_overwrite_output ( tile , process_tile ) elif self . config . mode == "overwrite" and not _baselevel_readonly : return self . _process_and_overwrite_output ( tile , process_tile )
2159	def _format_yaml ( self , payload ) : return parser . ordered_dump ( payload , Dumper = yaml . SafeDumper , default_flow_style = False )
2018	def MOD ( self , a , b ) : try : result = Operators . ITEBV ( 256 , b == 0 , 0 , a % b ) except ZeroDivisionError : result = 0 return result
2304	def _run_gies ( self , data , fixedGaps = None , verbose = True ) : id = str ( uuid . uuid4 ( ) ) os . makedirs ( '/tmp/cdt_gies' + id + '/' ) self . arguments [ '{FOLDER}' ] = '/tmp/cdt_gies' + id + '/' def retrieve_result ( ) : return read_csv ( '/tmp/cdt_gies' + id + '/result.csv' , delimiter = ',' ) . values try : data . to_csv ( '/tmp/cdt_gies' + id + '/data.csv' , header = False , index = False ) if fixedGaps is not None : fixedGaps . to_csv ( '/tmp/cdt_gies' + id + '/fixedgaps.csv' , index = False , header = False ) self . arguments [ '{SKELETON}' ] = 'TRUE' else : self . arguments [ '{SKELETON}' ] = 'FALSE' gies_result = launch_R_script ( "{}/R_templates/gies.R" . format ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) , self . arguments , output_function = retrieve_result , verbose = verbose ) except Exception as e : rmtree ( '/tmp/cdt_gies' + id + '' ) raise e except KeyboardInterrupt : rmtree ( '/tmp/cdt_gies' + id + '/' ) raise KeyboardInterrupt rmtree ( '/tmp/cdt_gies' + id + '' ) return gies_result
6116	def unmasked_for_shape_and_pixel_scale ( cls , shape , pixel_scale , invert = False ) : mask = np . full ( tuple ( map ( lambda d : int ( d ) , shape ) ) , False ) if invert : mask = np . invert ( mask ) return cls ( array = mask , pixel_scale = pixel_scale )
12475	def remove_all ( filelist , folder = '' ) : if not folder : for f in filelist : os . remove ( f ) else : for f in filelist : os . remove ( op . join ( folder , f ) )
6198	def print_sizes ( self ) : float_size = 4 MB = 1024 * 1024 size_ = self . n_samples * float_size em_size = size_ * self . num_particles / MB pos_size = 3 * size_ * self . num_particles / MB print ( " Number of particles:" , self . num_particles ) print ( " Number of time steps:" , self . n_samples ) print ( " Emission array - 1 particle (float32): %.1f MB" % ( size_ / MB ) ) print ( " Emission array (float32): %.1f MB" % em_size ) print ( " Position array (float32): %.1f MB " % pos_size )
13758	def split_path ( path ) : result_parts = [ ] while path != "/" : parts = os . path . split ( path ) if parts [ 1 ] == path : result_parts . insert ( 0 , parts [ 1 ] ) break elif parts [ 0 ] == path : result_parts . insert ( 0 , parts [ 0 ] ) break else : path = parts [ 0 ] result_parts . insert ( 0 , parts [ 1 ] ) return result_parts
12008	def _generate_key ( pass_id , passphrases , salt , algorithm ) : if pass_id not in passphrases : raise Exception ( 'Passphrase not defined for id: %d' % pass_id ) passphrase = passphrases [ pass_id ] if len ( passphrase ) < 32 : raise Exception ( 'Passphrase less than 32 characters long' ) digestmod = EncryptedPickle . _get_hashlib ( algorithm [ 'pbkdf2_algorithm' ] ) encoder = PBKDF2 ( passphrase , salt , iterations = algorithm [ 'pbkdf2_iterations' ] , digestmodule = digestmod ) return encoder . read ( algorithm [ 'key_size' ] )
2455	def set_pkg_licenses_concluded ( self , doc , licenses ) : self . assert_package_exists ( ) if not self . package_conc_lics_set : self . package_conc_lics_set = True if validations . validate_lics_conc ( licenses ) : doc . package . conc_lics = licenses return True else : raise SPDXValueError ( 'Package::ConcludedLicenses' ) else : raise CardinalityError ( 'Package::ConcludedLicenses' )
428	def load_and_preprocess_imdb_data ( n_gram = None ) : X_train , y_train , X_test , y_test = tl . files . load_imdb_dataset ( nb_words = VOCAB_SIZE ) if n_gram is not None : X_train = np . array ( [ augment_with_ngrams ( x , VOCAB_SIZE , N_BUCKETS , n = n_gram ) for x in X_train ] ) X_test = np . array ( [ augment_with_ngrams ( x , VOCAB_SIZE , N_BUCKETS , n = n_gram ) for x in X_test ] ) return X_train , y_train , X_test , y_test
11137	def path_required ( func ) : @ wraps ( func ) def wrapper ( self , * args , ** kwargs ) : if self . path is None : warnings . warn ( 'Must load (Repository.load_repository) or initialize (Repository.create_repository) the repository first !' ) return return func ( self , * args , ** kwargs ) return wrapper
3841	async def sync_all_new_events ( self , sync_all_new_events_request ) : response = hangouts_pb2 . SyncAllNewEventsResponse ( ) await self . _pb_request ( 'conversations/syncallnewevents' , sync_all_new_events_request , response ) return response
10340	def spia_matrices_to_tsvs ( spia_matrices : Mapping [ str , pd . DataFrame ] , directory : str ) -> None : os . makedirs ( directory , exist_ok = True ) for relation , df in spia_matrices . items ( ) : df . to_csv ( os . path . join ( directory , f'{relation}.tsv' ) , index = True )
2548	def validate ( self , messages ) : messages = self . validate_version ( messages ) messages = self . validate_data_lics ( messages ) messages = self . validate_name ( messages ) messages = self . validate_spdx_id ( messages ) messages = self . validate_namespace ( messages ) messages = self . validate_ext_document_references ( messages ) messages = self . validate_creation_info ( messages ) messages = self . validate_package ( messages ) messages = self . validate_extracted_licenses ( messages ) messages = self . validate_reviews ( messages ) return messages
949	def _createPeriodicActivities ( self ) : periodicActivities = [ ] class MetricsReportCb ( object ) : def __init__ ( self , taskRunner ) : self . __taskRunner = taskRunner return def __call__ ( self ) : self . __taskRunner . _getAndEmitExperimentMetrics ( ) reportMetrics = PeriodicActivityRequest ( repeating = True , period = 1000 , cb = MetricsReportCb ( self ) ) periodicActivities . append ( reportMetrics ) class IterationProgressCb ( object ) : PROGRESS_UPDATE_PERIOD_TICKS = 1000 def __init__ ( self , taskLabel , requestedIterationCount , logger ) : self . __taskLabel = taskLabel self . __requestedIterationCount = requestedIterationCount self . __logger = logger self . __numIterationsSoFar = 0 def __call__ ( self ) : self . __numIterationsSoFar += self . PROGRESS_UPDATE_PERIOD_TICKS self . __logger . debug ( "%s: ITERATION PROGRESS: %s of %s" % ( self . __taskLabel , self . __numIterationsSoFar , self . __requestedIterationCount ) ) iterationProgressCb = IterationProgressCb ( taskLabel = self . __task [ 'taskLabel' ] , requestedIterationCount = self . __task [ 'iterationCount' ] , logger = self . __logger ) iterationProgressReporter = PeriodicActivityRequest ( repeating = True , period = IterationProgressCb . PROGRESS_UPDATE_PERIOD_TICKS , cb = iterationProgressCb ) periodicActivities . append ( iterationProgressReporter ) return periodicActivities
613	def _generateExtraMetricSpecs ( options ) : _metricSpecSchema = { 'properties' : { } } results = [ ] for metric in options [ 'metrics' ] : for propertyName in _metricSpecSchema [ 'properties' ] . keys ( ) : _getPropertyValue ( _metricSpecSchema , propertyName , metric ) specString , label = _generateMetricSpecString ( field = metric [ 'field' ] , metric = metric [ 'metric' ] , params = metric [ 'params' ] , inferenceElement = metric [ 'inferenceElement' ] , returnLabel = True ) if metric [ 'logged' ] : options [ 'loggedMetrics' ] . append ( label ) results . append ( specString ) return results
4072	def split_elements ( value ) : items = [ v . strip ( ) for v in value . split ( ',' ) ] if len ( items ) == 1 : items = value . split ( ) return items
10500	def waitForCreation ( self , timeout = 10 , notification = 'AXCreated' ) : callback = AXCallbacks . returnElemCallback retelem = None args = ( retelem , ) return self . waitFor ( timeout , notification , callback = callback , args = args )
13090	def main ( branch ) : try : output = subprocess . check_output ( [ 'git' , 'rev-parse' ] ) . decode ( 'utf-8' ) sys . stdout . write ( output ) except subprocess . CalledProcessError : return ensure_remote_branch_is_tracked ( branch ) subprocess . check_call ( [ 'git' , 'checkout' , '--quiet' , branch ] ) subprocess . check_call ( [ 'git' , 'pull' , '--quiet' ] ) subprocess . check_call ( [ 'git' , 'checkout' , '--quiet' , '%s~0' % branch ] ) subprocess . check_call ( [ 'find' , '.' , '-name' , '"*.pyc"' , '-delete' ] ) print ( 'Your branch is up to date with branch \'origin/%s\'.' % branch )
1835	def JRCXZ ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . RCX == 0 , target . read ( ) , cpu . PC )
5135	def minimal_random_graph ( num_vertices , seed = None , ** kwargs ) : if isinstance ( seed , numbers . Integral ) : np . random . seed ( seed ) points = np . random . random ( ( num_vertices , 2 ) ) * 10 edges = [ ] for k in range ( num_vertices - 1 ) : for j in range ( k + 1 , num_vertices ) : v = points [ k ] - points [ j ] edges . append ( ( k , j , v [ 0 ] ** 2 + v [ 1 ] ** 2 ) ) mytype = [ ( 'n1' , int ) , ( 'n2' , int ) , ( 'distance' , np . float ) ] edges = np . array ( edges , dtype = mytype ) edges = np . sort ( edges , order = 'distance' ) unionF = UnionFind ( [ k for k in range ( num_vertices ) ] ) g = nx . Graph ( ) for n1 , n2 , dummy in edges : unionF . union ( n1 , n2 ) g . add_edge ( n1 , n2 ) if unionF . nClusters == 1 : break pos = { j : p for j , p in enumerate ( points ) } g = QueueNetworkDiGraph ( g . to_directed ( ) ) g . set_pos ( pos ) return g
11038	def maybe_key_vault ( client , mount_path ) : d = client . read_kv2 ( 'client_key' , mount_path = mount_path ) def get_or_create_key ( client_key ) : if client_key is not None : key_data = client_key [ 'data' ] [ 'data' ] key = _load_pem_private_key_bytes ( key_data [ 'key' ] . encode ( 'utf-8' ) ) return JWKRSA ( key = key ) else : key = generate_private_key ( u'rsa' ) key_data = { 'key' : _dump_pem_private_key_bytes ( key ) . decode ( 'utf-8' ) } d = client . create_or_update_kv2 ( 'client_key' , key_data , mount_path = mount_path ) return d . addCallback ( lambda _result : JWKRSA ( key = key ) ) return d . addCallback ( get_or_create_key )
11035	def _request ( self , endpoint , * args , ** kwargs ) : kwargs [ 'url' ] = endpoint return ( super ( MarathonLbClient , self ) . request ( * args , ** kwargs ) . addCallback ( raise_for_status ) )
8700	def __exchange ( self , output , timeout = None ) : self . __writeln ( output ) self . _port . flush ( ) return self . __expect ( timeout = timeout or self . _timeout )
5515	def setlocale ( name ) : with LOCALE_LOCK : old_locale = locale . setlocale ( locale . LC_ALL ) try : yield locale . setlocale ( locale . LC_ALL , name ) finally : locale . setlocale ( locale . LC_ALL , old_locale )
4519	def set_project ( self , project ) : def visit ( x ) : set_project = getattr ( x , 'set_project' , None ) if set_project : set_project ( project ) values = getattr ( x , 'values' , lambda : ( ) ) for v in values ( ) : visit ( v ) visit ( self . routing )
4841	def get_program_by_uuid ( self , program_uuid ) : return self . _load_data ( self . PROGRAMS_ENDPOINT , resource_id = program_uuid , default = None )
6774	def deploy ( self , site = None ) : r = self . local_renderer self . deploy_logrotate ( ) cron_crontabs = [ ] for _site , site_data in self . iter_sites ( site = site ) : r . env . cron_stdout_log = r . format ( r . env . stdout_log_template ) r . env . cron_stderr_log = r . format ( r . env . stderr_log_template ) r . sudo ( 'touch {cron_stdout_log}' ) r . sudo ( 'touch {cron_stderr_log}' ) r . sudo ( 'sudo chown {user}:{user} {cron_stdout_log}' ) r . sudo ( 'sudo chown {user}:{user} {cron_stderr_log}' ) if self . verbose : print ( 'site:' , site , file = sys . stderr ) print ( 'env.crontabs_selected:' , self . env . crontabs_selected , file = sys . stderr ) for selected_crontab in self . env . crontabs_selected : lines = self . env . crontabs_available . get ( selected_crontab , [ ] ) if self . verbose : print ( 'lines:' , lines , file = sys . stderr ) for line in lines : cron_crontabs . append ( r . format ( line ) ) if not cron_crontabs : return cron_crontabs = self . env . crontab_headers + cron_crontabs cron_crontabs . append ( '\n' ) r . env . crontabs_rendered = '\n' . join ( cron_crontabs ) fn = self . write_to_file ( content = r . env . crontabs_rendered ) print ( 'fn:' , fn ) r . env . put_remote_path = r . put ( local_path = fn ) if isinstance ( r . env . put_remote_path , ( tuple , list ) ) : r . env . put_remote_path = r . env . put_remote_path [ 0 ] r . sudo ( 'crontab -u {cron_user} {put_remote_path}' )
9341	def MetaOrdered ( parallel , done , turnstile ) : class Ordered : def __init__ ( self , iterref ) : if parallel . master : done [ ... ] = 0 self . iterref = iterref parallel . barrier ( ) @ classmethod def abort ( self ) : turnstile . release ( ) def __enter__ ( self ) : while self . iterref != done : pass turnstile . acquire ( ) return self def __exit__ ( self , * args ) : done [ ... ] += 1 turnstile . release ( ) return Ordered
12255	def sync ( self ) : for key in mimicdb . backend . smembers ( tpl . bucket % self . name ) : mimicdb . backend . delete ( tpl . key % ( self . name , key ) ) mimicdb . backend . delete ( tpl . bucket % self . name ) mimicdb . backend . sadd ( tpl . connection , self . name ) for key in self . list ( force = True ) : mimicdb . backend . sadd ( tpl . bucket % self . name , key . name ) mimicdb . backend . hmset ( tpl . key % ( self . name , key . name ) , dict ( size = key . size , md5 = key . etag . strip ( '"' ) ) )
10503	def waitForFocusedWindowToChange ( self , nextWinName , timeout = 10 ) : callback = AXCallbacks . returnElemCallback retelem = None return self . waitFor ( timeout , 'AXFocusedWindowChanged' , AXTitle = nextWinName )
9620	def main ( ) : import time print ( 'Testing controller in position 1:' ) print ( 'Running 3 x 3 seconds tests' ) con = rController ( 1 ) for i in range ( 3 ) : print ( 'Waiting...' ) time . sleep ( 2.5 ) print ( 'State: ' , con . gamepad ) print ( 'Buttons: ' , con . buttons ) time . sleep ( 0.5 ) print ( 'Done!' )
5353	def retain_identities ( self , retention_time ) : enrich_es = self . conf [ 'es_enrichment' ] [ 'url' ] sortinghat_db = self . db current_data_source = self . get_backend ( self . backend_section ) active_data_sources = self . config . get_active_data_sources ( ) if retention_time is None : logger . debug ( "[identities retention] Retention policy disabled, no identities will be deleted." ) return if retention_time <= 0 : logger . debug ( "[identities retention] Retention time must be greater than 0." ) return retain_identities ( retention_time , enrich_es , sortinghat_db , current_data_source , active_data_sources )
13909	def show_version ( self ) : class ShowVersionAction ( argparse . Action ) : def __init__ ( inner_self , nargs = 0 , ** kw ) : super ( ShowVersionAction , inner_self ) . __init__ ( nargs = nargs , ** kw ) def __call__ ( inner_self , parser , args , value , option_string = None ) : print ( "{parser_name} version: {version}" . format ( parser_name = self . config . get ( "parser" , { } ) . get ( "prog" ) , version = self . prog_version ) ) return ShowVersionAction
13472	def clean ( self ) : cleaned = super ( EventForm , self ) . clean ( ) if Event . objects . filter ( name = cleaned [ 'name' ] , start_date = cleaned [ 'start_date' ] ) . count ( ) : raise forms . ValidationError ( u'This event appears to be in the database already.' ) return cleaned
11505	def move_folder ( self , token , folder_id , dest_folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = folder_id parameters [ 'dstfolderid' ] = dest_folder_id response = self . request ( 'midas.folder.move' , parameters ) return response
7883	def _make_prefixed ( self , name , is_element , declared_prefixes , declarations ) : namespace , name = self . _split_qname ( name , is_element ) if namespace is None : prefix = None elif namespace in declared_prefixes : prefix = declared_prefixes [ namespace ] elif namespace in self . _prefixes : prefix = self . _prefixes [ namespace ] declarations [ namespace ] = prefix declared_prefixes [ namespace ] = prefix else : if is_element : prefix = None else : prefix = self . _make_prefix ( declared_prefixes ) declarations [ namespace ] = prefix declared_prefixes [ namespace ] = prefix if prefix : return prefix + u":" + name else : return name
12175	def plotAllSweeps ( abfFile ) : r = io . AxonIO ( filename = abfFile ) bl = r . read_block ( lazy = False , cascade = True ) print ( abfFile + "\nplotting %d sweeps..." % len ( bl . segments ) ) plt . figure ( figsize = ( 12 , 10 ) ) plt . title ( abfFile ) for sweep in range ( len ( bl . segments ) ) : trace = bl . segments [ sweep ] . analogsignals [ 0 ] plt . plot ( trace . times - trace . times [ 0 ] , trace . magnitude , alpha = .5 ) plt . ylabel ( trace . dimensionality ) plt . xlabel ( "seconds" ) plt . show ( ) plt . close ( )
5574	def available_output_formats ( ) : output_formats = [ ] for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : driver_ = v . load ( ) if hasattr ( driver_ , "METADATA" ) and ( driver_ . METADATA [ "mode" ] in [ "w" , "rw" ] ) : output_formats . append ( driver_ . METADATA [ "driver_name" ] ) return output_formats
3985	def _dusty_hosts_config ( hosts_specs ) : rules = '' . join ( [ '{} {}\n' . format ( spec [ 'forwarded_ip' ] , spec [ 'host_address' ] ) for spec in hosts_specs ] ) return config_file . create_config_section ( rules )
11176	def parse ( self , argv ) : if len ( argv ) < self . nargs : raise BadNumberOfArguments ( self . nargs , len ( argv ) ) if self . nargs == 1 : return self . parse_argument ( argv . pop ( 0 ) ) return [ self . parse_argument ( argv . pop ( 0 ) ) for tmp in range ( self . nargs ) ]
8644	def update_track ( session , track_id , latitude , longitude , stop_tracking = False ) : tracking_data = { 'track_point' : { 'latitude' : latitude , 'longitude' : longitude , } , 'stop_tracking' : stop_tracking } response = make_put_request ( session , 'tracks/{}' . format ( track_id ) , json_data = tracking_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise TrackNotUpdatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
12071	def update ( self , tids , info ) : outputs_dir = os . path . join ( info [ 'root_directory' ] , 'streams' ) pattern = '%s_*_tid_*{tid}.o.{tid}*' % info [ 'batch_name' ] flist = os . listdir ( outputs_dir ) try : outputs = [ ] for tid in tids : matches = fnmatch . filter ( flist , pattern . format ( tid = tid ) ) if len ( matches ) != 1 : self . warning ( "No unique output file for tid %d" % tid ) contents = open ( os . path . join ( outputs_dir , matches [ 0 ] ) , 'r' ) . read ( ) outputs . append ( self . output_extractor ( contents ) ) self . _next_val = self . _update_state ( outputs ) self . trace . append ( ( outputs , self . _next_val ) ) except : self . warning ( "Cannot load required output files. Cannot continue." ) self . _next_val = StopIteration
13346	def run ( * args , ** kwargs ) : kwargs . setdefault ( 'env' , os . environ ) kwargs . setdefault ( 'shell' , True ) try : subprocess . check_call ( ' ' . join ( args ) , ** kwargs ) return True except subprocess . CalledProcessError : logger . debug ( 'Error running: {}' . format ( args ) ) return False
1028	def b64encode ( s , altchars = None ) : encoded = binascii . b2a_base64 ( s ) [ : - 1 ] if altchars is not None : return encoded . translate ( string . maketrans ( b'+/' , altchars [ : 2 ] ) ) return encoded
12338	def stitch_macro ( path , output_folder = None ) : output_folder = output_folder or path debug ( 'stitching ' + path + ' to ' + output_folder ) fields = glob ( _pattern ( path , _field ) ) xs = [ attribute ( field , 'X' ) for field in fields ] ys = [ attribute ( field , 'Y' ) for field in fields ] x_min , x_max = min ( xs ) , max ( xs ) y_min , y_max = min ( ys ) , max ( ys ) fields_column = len ( set ( xs ) ) fields_row = len ( set ( ys ) ) images = glob ( _pattern ( fields [ 0 ] , _image ) ) attr = attributes ( images [ 0 ] ) channels = [ ] z_stacks = [ ] for image in images : channel = attribute_as_str ( image , 'C' ) if channel not in channels : channels . append ( channel ) z = attribute_as_str ( image , 'Z' ) if z not in z_stacks : z_stacks . append ( z ) debug ( 'channels ' + str ( channels ) ) debug ( 'z-stacks ' + str ( z_stacks ) ) _ , extension = os . path . splitext ( images [ - 1 ] ) if extension == '.tif' : extension = '.ome.tif' macros = [ ] output_files = [ ] for Z in z_stacks : for C in channels : filenames = os . path . join ( _field + '--X{xx}--Y{yy}' , _image + '--L' + attr . L + '--S' + attr . S + '--U' + attr . U + '--V' + attr . V + '--J' + attr . J + '--E' + attr . E + '--O' + attr . O + '--X{xx}--Y{yy}' + '--T' + attr . T + '--Z' + Z + '--C' + C + extension ) debug ( 'filenames ' + filenames ) cur_attr = attributes ( filenames ) . _asdict ( ) f = 'stitched--U{U}--V{V}--C{C}--Z{Z}.png' . format ( ** cur_attr ) output = os . path . join ( output_folder , f ) debug ( 'output ' + output ) output_files . append ( output ) if os . path . isfile ( output ) : print ( 'leicaexperiment stitched file already' ' exists {}' . format ( output ) ) continue macros . append ( fijibin . macro . stitch ( path , filenames , fields_column , fields_row , output_filename = output , x_start = x_min , y_start = y_min ) ) return ( output_files , macros )
13126	def get_domains ( self ) : search = User . search ( ) search . aggs . bucket ( 'domains' , 'terms' , field = 'domain' , order = { '_count' : 'desc' } , size = 100 ) response = search . execute ( ) return [ entry . key for entry in response . aggregations . domains . buckets ]
9493	def _simulate_stack ( code : list ) -> int : max_stack = 0 curr_stack = 0 def _check_stack ( ins ) : if curr_stack < 0 : raise CompileError ( "Stack turned negative on instruction: {}" . format ( ins ) ) if curr_stack > max_stack : return curr_stack for instruction in code : assert isinstance ( instruction , dis . Instruction ) if instruction . arg is not None : try : effect = dis . stack_effect ( instruction . opcode , instruction . arg ) except ValueError as e : raise CompileError ( "Invalid opcode `{}` when compiling" . format ( instruction . opcode ) ) from e else : try : effect = dis . stack_effect ( instruction . opcode ) except ValueError as e : raise CompileError ( "Invalid opcode `{}` when compiling" . format ( instruction . opcode ) ) from e curr_stack += effect _should_new_stack = _check_stack ( instruction ) if _should_new_stack : max_stack = _should_new_stack return max_stack
8486	def get ( self , name , default , allow_default = True ) : if not self . settings . get ( 'pyconfig.case_sensitive' , False ) : name = name . lower ( ) if name not in self . settings : if not allow_default : raise LookupError ( 'No setting "{name}"' . format ( name = name ) ) self . settings [ name ] = default return self . settings [ name ]
13273	def default ( self , obj ) : if isinstance ( obj , np . ndarray ) : return obj . tolist ( ) elif isinstance ( obj , np . generic ) : return np . asscalar ( obj ) return json . JSONEncoder ( self , obj )
1115	def _make_prefix ( self ) : fromprefix = "from%d_" % HtmlDiff . _default_prefix toprefix = "to%d_" % HtmlDiff . _default_prefix HtmlDiff . _default_prefix += 1 self . _prefix = [ fromprefix , toprefix ]
8970	def step ( self , other_pub ) : if self . triggersStep ( other_pub ) : self . __wrapOtherPub ( other_pub ) self . __newRootKey ( "receiving" ) self . __newRatchetKey ( ) self . __newRootKey ( "sending" )
1658	def CheckForNonConstReference ( filename , clean_lines , linenum , nesting_state , error ) : line = clean_lines . elided [ linenum ] if '&' not in line : return if IsDerivedFunction ( clean_lines , linenum ) : return if IsOutOfLineMethodDefinition ( clean_lines , linenum ) : return if linenum > 1 : previous = None if Match ( r'\s*::(?:[\w<>]|::)+\s*&\s*\S' , line ) : previous = Search ( r'\b((?:const\s*)?(?:[\w<>]|::)+[\w<>])\s*$' , clean_lines . elided [ linenum - 1 ] ) elif Match ( r'\s*[a-zA-Z_]([\w<>]|::)+\s*&\s*\S' , line ) : previous = Search ( r'\b((?:const\s*)?(?:[\w<>]|::)+::)\s*$' , clean_lines . elided [ linenum - 1 ] ) if previous : line = previous . group ( 1 ) + line . lstrip ( ) else : endpos = line . rfind ( '>' ) if endpos > - 1 : ( _ , startline , startpos ) = ReverseCloseExpression ( clean_lines , linenum , endpos ) if startpos > - 1 and startline < linenum : line = '' for i in xrange ( startline , linenum + 1 ) : line += clean_lines . elided [ i ] . strip ( ) if ( nesting_state . previous_stack_top and not ( isinstance ( nesting_state . previous_stack_top , _ClassInfo ) or isinstance ( nesting_state . previous_stack_top , _NamespaceInfo ) ) ) : return if linenum > 0 : for i in xrange ( linenum - 1 , max ( 0 , linenum - 10 ) , - 1 ) : previous_line = clean_lines . elided [ i ] if not Search ( r'[),]\s*$' , previous_line ) : break if Match ( r'^\s*:\s+\S' , previous_line ) : return if Search ( r'\\\s*$' , line ) : return if IsInitializerList ( clean_lines , linenum ) : return whitelisted_functions = ( r'(?:[sS]wap(?:<\w:+>)?|' r'operator\s*[<>][<>]|' r'static_assert|COMPILE_ASSERT' r')\s*\(' ) if Search ( whitelisted_functions , line ) : return elif not Search ( r'\S+\([^)]*$' , line ) : for i in xrange ( 2 ) : if ( linenum > i and Search ( whitelisted_functions , clean_lines . elided [ linenum - i - 1 ] ) ) : return decls = ReplaceAll ( r'{[^}]*}' , ' ' , line ) for parameter in re . findall ( _RE_PATTERN_REF_PARAM , decls ) : if ( not Match ( _RE_PATTERN_CONST_REF_PARAM , parameter ) and not Match ( _RE_PATTERN_REF_STREAM_PARAM , parameter ) ) : error ( filename , linenum , 'runtime/references' , 2 , 'Is this a non-const reference? ' 'If so, make const or use a pointer: ' + ReplaceAll ( ' *<' , '<' , parameter ) )
9686	def pm ( self ) : resp = [ ] data = { } self . cnxn . xfer ( [ 0x32 ] ) sleep ( 10e-3 ) for i in range ( 12 ) : r = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] resp . append ( r ) data [ 'PM1' ] = self . _calculate_float ( resp [ 0 : 4 ] ) data [ 'PM2.5' ] = self . _calculate_float ( resp [ 4 : 8 ] ) data [ 'PM10' ] = self . _calculate_float ( resp [ 8 : ] ) sleep ( 0.1 ) return data
2274	def _win32_rmtree ( path , verbose = 0 ) : def _rmjunctions ( root ) : subdirs = [ ] for name in os . listdir ( root ) : current = join ( root , name ) if os . path . isdir ( current ) : if _win32_is_junction ( current ) : os . rmdir ( current ) elif not os . path . islink ( current ) : subdirs . append ( current ) for subdir in subdirs : _rmjunctions ( subdir ) if _win32_is_junction ( path ) : if verbose : print ( 'Deleting <JUNCTION> directory="{}"' . format ( path ) ) os . rmdir ( path ) else : if verbose : print ( 'Deleting directory="{}"' . format ( path ) ) _rmjunctions ( path ) import shutil shutil . rmtree ( path )
3096	def callback_handler ( self ) : decorator = self class OAuth2Handler ( webapp . RequestHandler ) : @ login_required def get ( self ) : error = self . request . get ( 'error' ) if error : errormsg = self . request . get ( 'error_description' , error ) self . response . out . write ( 'The authorization request failed: {0}' . format ( _safe_html ( errormsg ) ) ) else : user = users . get_current_user ( ) decorator . _create_flow ( self ) credentials = decorator . flow . step2_exchange ( self . request . params ) decorator . _storage_class ( decorator . _credentials_class , None , decorator . _credentials_property_name , user = user ) . put ( credentials ) redirect_uri = _parse_state_value ( str ( self . request . get ( 'state' ) ) , user ) if redirect_uri is None : self . response . out . write ( 'The authorization request failed' ) return if ( decorator . _token_response_param and credentials . token_response ) : resp_json = json . dumps ( credentials . token_response ) redirect_uri = _helpers . _add_query_parameter ( redirect_uri , decorator . _token_response_param , resp_json ) self . redirect ( redirect_uri ) return OAuth2Handler
6782	def get_previous_thumbprint ( self , components = None ) : components = str_to_component_list ( components ) tp_fn = self . manifest_filename tp_text = None if self . file_exists ( tp_fn ) : fd = six . BytesIO ( ) get ( tp_fn , fd ) tp_text = fd . getvalue ( ) manifest_data = { } raw_data = yaml . load ( tp_text ) for k , v in raw_data . items ( ) : manifest_key = assert_valid_satchel ( k ) service_name = clean_service_name ( k ) if components and service_name not in components : continue manifest_data [ manifest_key ] = v return manifest_data
3374	def remove_cons_vars_from_problem ( model , what ) : context = get_context ( model ) model . solver . remove ( what ) if context : context ( partial ( model . solver . add , what ) )
3863	def _get_event_request_header ( self ) : otr_status = ( hangouts_pb2 . OFF_THE_RECORD_STATUS_OFF_THE_RECORD if self . is_off_the_record else hangouts_pb2 . OFF_THE_RECORD_STATUS_ON_THE_RECORD ) return hangouts_pb2 . EventRequestHeader ( conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , client_generated_id = self . _client . get_client_generated_id ( ) , expected_otr = otr_status , delivery_medium = self . _get_default_delivery_medium ( ) , )
5281	def make_tpot_pmml_config ( config , user_classpath = [ ] ) : tpot_keys = set ( config . keys ( ) ) classes = _supported_classes ( user_classpath ) pmml_keys = ( set ( classes ) ) . union ( set ( [ _strip_module ( class_ ) for class_ in classes ] ) ) return { key : config [ key ] for key in ( tpot_keys ) . intersection ( pmml_keys ) }
2065	def inverse_transform ( self , X_in ) : X = X_in . copy ( deep = True ) X = util . convert_input ( X ) if self . _dim is None : raise ValueError ( 'Must train encoder before it can be used to inverse_transform data' ) if X . shape [ 1 ] != self . _dim : if self . drop_invariant : raise ValueError ( "Unexpected input dimension %d, the attribute drop_invariant should " "set as False when transform data" % ( X . shape [ 1 ] , ) ) else : raise ValueError ( 'Unexpected input dimension %d, expected %d' % ( X . shape [ 1 ] , self . _dim , ) ) if not self . cols : return X if self . return_df else X . values if self . handle_unknown == 'value' : for col in self . cols : if any ( X [ col ] == - 1 ) : warnings . warn ( "inverse_transform is not supported because transform impute " "the unknown category -1 when encode %s" % ( col , ) ) if self . handle_unknown == 'return_nan' and self . handle_missing == 'return_nan' : for col in self . cols : if X [ col ] . isnull ( ) . any ( ) : warnings . warn ( "inverse_transform is not supported because transform impute " "the unknown category nan when encode %s" % ( col , ) ) for switch in self . mapping : column_mapping = switch . get ( 'mapping' ) inverse = pd . Series ( data = column_mapping . index , index = column_mapping . get_values ( ) ) X [ switch . get ( 'col' ) ] = X [ switch . get ( 'col' ) ] . map ( inverse ) . astype ( switch . get ( 'data_type' ) ) return X if self . return_df else X . values
5707	def get_lockdown_form ( form_path ) : if not form_path : raise ImproperlyConfigured ( 'No LOCKDOWN_FORM specified.' ) form_path_list = form_path . split ( "." ) new_module = "." . join ( form_path_list [ : - 1 ] ) attr = form_path_list [ - 1 ] try : mod = import_module ( new_module ) except ( ImportError , ValueError ) : raise ImproperlyConfigured ( 'Module configured in LOCKDOWN_FORM (%s) to' ' contain the form class couldn\'t be ' 'found.' % new_module ) try : form = getattr ( mod , attr ) except AttributeError : raise ImproperlyConfigured ( 'The module configured in LOCKDOWN_FORM ' ' (%s) doesn\'t define a "%s" form.' % ( new_module , attr ) ) return form
9522	def merge_to_one_seq ( infile , outfile , seqname = 'union' ) : seq_reader = sequences . file_reader ( infile ) seqs = [ ] for seq in seq_reader : seqs . append ( copy . copy ( seq ) ) new_seq = '' . join ( [ seq . seq for seq in seqs ] ) if type ( seqs [ 0 ] ) == sequences . Fastq : new_qual = '' . join ( [ seq . qual for seq in seqs ] ) seqs [ : ] = [ ] merged = sequences . Fastq ( seqname , new_seq , new_qual ) else : merged = sequences . Fasta ( seqname , new_seq ) seqs [ : ] = [ ] f = utils . open_file_write ( outfile ) print ( merged , file = f ) utils . close ( f )
11707	def generate_gamete ( self , egg_or_sperm_word ) : p_rate_of_mutation = [ 0.9 , 0.1 ] should_use_mutant_pool = ( npchoice ( [ 0 , 1 ] , 1 , p = p_rate_of_mutation ) [ 0 ] == 1 ) if should_use_mutant_pool : pool = tokens . secondary_tokens else : pool = tokens . primary_tokens return get_matches ( egg_or_sperm_word , pool , 23 )
11133	def tear_down ( self ) : while len ( self . _temp_directories ) > 0 : directory = self . _temp_directories . pop ( ) shutil . rmtree ( directory , ignore_errors = True ) while len ( self . _temp_files ) > 0 : file = self . _temp_files . pop ( ) try : os . remove ( file ) except OSError : pass
13602	def warn_message ( self , message , fh = None , prefix = "[warn]:" , suffix = "..." ) : msg = prefix + message + suffix fh = fh or sys . stdout if fh is sys . stdout : termcolor . cprint ( msg , color = "yellow" ) else : fh . write ( msg ) pass
7934	def _compute_handshake ( self ) : return hashlib . sha1 ( to_utf8 ( self . stream_id ) + to_utf8 ( self . secret ) ) . hexdigest ( )
6044	def padded_grid_from_shape_psf_shape_and_pixel_scale ( cls , shape , psf_shape , pixel_scale ) : padded_shape = ( shape [ 0 ] + psf_shape [ 0 ] - 1 , shape [ 1 ] + psf_shape [ 1 ] - 1 ) padded_regular_grid = grid_util . regular_grid_1d_masked_from_mask_pixel_scales_and_origin ( mask = np . full ( padded_shape , False ) , pixel_scales = ( pixel_scale , pixel_scale ) ) padded_mask = msk . Mask . unmasked_for_shape_and_pixel_scale ( shape = padded_shape , pixel_scale = pixel_scale ) return PaddedRegularGrid ( arr = padded_regular_grid , mask = padded_mask , image_shape = shape )
6035	def map_function ( self , func , * arg_lists ) : return GridStack ( * [ func ( * args ) for args in zip ( self , * arg_lists ) ] )
10477	def _leftMouseDragged ( self , stopCoord , strCoord , speed ) : appPid = self . _getPid ( ) if strCoord == ( 0 , 0 ) : loc = AppKit . NSEvent . mouseLocation ( ) strCoord = ( loc . x , Quartz . CGDisplayPixelsHigh ( 0 ) - loc . y ) appPid = self . _getPid ( ) pressLeftButton = Quartz . CGEventCreateMouseEvent ( None , Quartz . kCGEventLeftMouseDown , strCoord , Quartz . kCGMouseButtonLeft ) Quartz . CGEventPost ( Quartz . CoreGraphics . kCGHIDEventTap , pressLeftButton ) time . sleep ( 5 ) speed = round ( 1 / float ( speed ) , 2 ) xmoved = stopCoord [ 0 ] - strCoord [ 0 ] ymoved = stopCoord [ 1 ] - strCoord [ 1 ] if ymoved == 0 : raise ValueError ( 'Not support horizontal moving' ) else : k = abs ( ymoved / xmoved ) if xmoved != 0 : for xpos in range ( int ( abs ( xmoved ) ) ) : if xmoved > 0 and ymoved > 0 : currcoord = ( strCoord [ 0 ] + xpos , strCoord [ 1 ] + xpos * k ) elif xmoved > 0 and ymoved < 0 : currcoord = ( strCoord [ 0 ] + xpos , strCoord [ 1 ] - xpos * k ) elif xmoved < 0 and ymoved < 0 : currcoord = ( strCoord [ 0 ] - xpos , strCoord [ 1 ] - xpos * k ) elif xmoved < 0 and ymoved > 0 : currcoord = ( strCoord [ 0 ] - xpos , strCoord [ 1 ] + xpos * k ) dragLeftButton = Quartz . CGEventCreateMouseEvent ( None , Quartz . kCGEventLeftMouseDragged , currcoord , Quartz . kCGMouseButtonLeft ) Quartz . CGEventPost ( Quartz . CoreGraphics . kCGHIDEventTap , dragLeftButton ) time . sleep ( speed ) else : raise ValueError ( 'Not support vertical moving' ) upLeftButton = Quartz . CGEventCreateMouseEvent ( None , Quartz . kCGEventLeftMouseUp , stopCoord , Quartz . kCGMouseButtonLeft ) time . sleep ( 5 ) Quartz . CGEventPost ( Quartz . CoreGraphics . kCGHIDEventTap , upLeftButton )
9976	def alter_freevars ( func , globals_ = None , ** vars ) : if globals_ is None : globals_ = func . __globals__ frees = tuple ( vars . keys ( ) ) oldlocs = func . __code__ . co_names newlocs = tuple ( name for name in oldlocs if name not in frees ) code = _alter_code ( func . __code__ , co_freevars = frees , co_names = newlocs , co_flags = func . __code__ . co_flags | inspect . CO_NESTED ) closure = _create_closure ( * vars . values ( ) ) return FunctionType ( code , globals_ , closure = closure )
7334	async def _chunked_upload ( self , media , media_size , path = None , media_type = None , media_category = None , chunk_size = 2 ** 20 , ** params ) : if isinstance ( media , bytes ) : media = io . BytesIO ( media ) chunk = media . read ( chunk_size ) is_coro = asyncio . iscoroutine ( chunk ) if is_coro : chunk = await chunk if media_type is None : media_metadata = await utils . get_media_metadata ( chunk , path ) media_type , media_category = media_metadata elif media_category is None : media_category = utils . get_category ( media_type ) response = await self . upload . media . upload . post ( command = "INIT" , total_bytes = media_size , media_type = media_type , media_category = media_category , ** params ) media_id = response [ 'media_id' ] i = 0 while chunk : if is_coro : req = self . upload . media . upload . post ( command = "APPEND" , media_id = media_id , media = chunk , segment_index = i ) chunk , _ = await asyncio . gather ( media . read ( chunk_size ) , req ) else : await self . upload . media . upload . post ( command = "APPEND" , media_id = media_id , media = chunk , segment_index = i ) chunk = media . read ( chunk_size ) i += 1 status = await self . upload . media . upload . post ( command = "FINALIZE" , media_id = media_id ) if 'processing_info' in status : while status [ 'processing_info' ] . get ( 'state' ) != "succeeded" : processing_info = status [ 'processing_info' ] if processing_info . get ( 'state' ) == "failed" : error = processing_info . get ( 'error' , { } ) message = error . get ( 'message' , str ( status ) ) raise exceptions . MediaProcessingError ( data = status , message = message , ** params ) delay = processing_info [ 'check_after_secs' ] await asyncio . sleep ( delay ) status = await self . upload . media . upload . get ( command = "STATUS" , media_id = media_id , ** params ) return response
11930	def deploy_blog ( ) : logger . info ( deploy_blog . __doc__ ) call ( 'rsync -aqu ' + join ( dirname ( __file__ ) , 'res' , '*' ) + ' .' , shell = True ) logger . success ( 'Done' ) logger . info ( 'Please edit config.toml to meet your needs' )
7919	def __from_unicode ( cls , data , check = True ) : parts1 = data . split ( u"/" , 1 ) parts2 = parts1 [ 0 ] . split ( u"@" , 1 ) if len ( parts2 ) == 2 : local = parts2 [ 0 ] domain = parts2 [ 1 ] if check : local = cls . __prepare_local ( local ) domain = cls . __prepare_domain ( domain ) else : local = None domain = parts2 [ 0 ] if check : domain = cls . __prepare_domain ( domain ) if len ( parts1 ) == 2 : resource = parts1 [ 1 ] if check : resource = cls . __prepare_resource ( parts1 [ 1 ] ) else : resource = None if not domain : raise JIDError ( "Domain is required in JID." ) return ( local , domain , resource )
12749	def load_asf ( self , source , ** kwargs ) : if hasattr ( source , 'read' ) : p = parser . parse_asf ( source , self . world , self . jointgroup , ** kwargs ) else : with open ( source ) as handle : p = parser . parse_asf ( handle , self . world , self . jointgroup , ** kwargs ) self . bodies = p . bodies self . joints = p . joints self . set_pid_params ( kp = 0.999 / self . world . dt )
497	def _constructClassificationRecord ( self , inputs ) : allSPColumns = inputs [ "spBottomUpOut" ] activeSPColumns = allSPColumns . nonzero ( ) [ 0 ] score = anomaly . computeRawAnomalyScore ( activeSPColumns , self . _prevPredictedColumns ) spSize = len ( allSPColumns ) allTPCells = inputs [ 'tpTopDownOut' ] tpSize = len ( inputs [ 'tpLrnActiveStateT' ] ) classificationVector = numpy . array ( [ ] ) if self . classificationVectorType == 1 : classificationVector = numpy . zeros ( tpSize ) activeCellMatrix = inputs [ "tpLrnActiveStateT" ] . reshape ( tpSize , 1 ) activeCellIdx = numpy . where ( activeCellMatrix > 0 ) [ 0 ] if activeCellIdx . shape [ 0 ] > 0 : classificationVector [ numpy . array ( activeCellIdx , dtype = numpy . uint16 ) ] = 1 elif self . classificationVectorType == 2 : classificationVector = numpy . zeros ( spSize + spSize ) if activeSPColumns . shape [ 0 ] > 0 : classificationVector [ activeSPColumns ] = 1.0 errorColumns = numpy . setdiff1d ( self . _prevPredictedColumns , activeSPColumns ) if errorColumns . shape [ 0 ] > 0 : errorColumnIndexes = ( numpy . array ( errorColumns , dtype = numpy . uint16 ) + spSize ) classificationVector [ errorColumnIndexes ] = 1.0 else : raise TypeError ( "Classification vector type must be either 'tpc' or" " 'sp_tpe', current value is %s" % ( self . classificationVectorType ) ) numPredictedCols = len ( self . _prevPredictedColumns ) predictedColumns = allTPCells . nonzero ( ) [ 0 ] self . _prevPredictedColumns = copy . deepcopy ( predictedColumns ) if self . _anomalyVectorLength is None : self . _anomalyVectorLength = len ( classificationVector ) result = _CLAClassificationRecord ( ROWID = self . _iteration , anomalyScore = score , anomalyVector = classificationVector . nonzero ( ) [ 0 ] . tolist ( ) , anomalyLabel = [ ] ) return result
6228	def init ( window = None , project = None , timeline = None ) : from demosys . effects . registry import Effect from demosys . scene import camera window . timeline = timeline setattr ( Effect , '_window' , window ) setattr ( Effect , '_ctx' , window . ctx ) setattr ( Effect , '_project' , project ) window . sys_camera = camera . SystemCamera ( aspect = window . aspect_ratio , fov = 60.0 , near = 1 , far = 1000 ) setattr ( Effect , '_sys_camera' , window . sys_camera ) print ( "Loading started at" , time . time ( ) ) project . load ( ) timer_cls = import_string ( settings . TIMER ) window . timer = timer_cls ( ) window . timer . start ( )
3339	def is_equal_or_child_uri ( parentUri , childUri ) : return ( parentUri and childUri and ( childUri . rstrip ( "/" ) + "/" ) . startswith ( parentUri . rstrip ( "/" ) + "/" ) )
5609	def tiles_to_affine_shape ( tiles ) : if not tiles : raise TypeError ( "no tiles provided" ) pixel_size = tiles [ 0 ] . pixel_x_size left , bottom , right , top = ( min ( [ t . left for t in tiles ] ) , min ( [ t . bottom for t in tiles ] ) , max ( [ t . right for t in tiles ] ) , max ( [ t . top for t in tiles ] ) , ) return ( Affine ( pixel_size , 0 , left , 0 , - pixel_size , top ) , Shape ( width = int ( round ( ( right - left ) / pixel_size , 0 ) ) , height = int ( round ( ( top - bottom ) / pixel_size , 0 ) ) , ) )
8409	def _extend_breaks ( self , major ) : trans = self . trans trans = trans if isinstance ( trans , type ) else trans . __class__ is_log = trans . __name__ . startswith ( 'log' ) diff = np . diff ( major ) step = diff [ 0 ] if is_log and all ( diff == step ) : major = np . hstack ( [ major [ 0 ] - step , major , major [ - 1 ] + step ] ) return major
2359	def t_intnumber ( self , t ) : r'-?\d+' t . value = int ( t . value ) t . type = 'NUMBER' return t
8219	def do_unfullscreen ( self , widget ) : self . unfullscreen ( ) self . is_fullscreen = False self . bot . _screen_ratio = None
13176	def get_observations ( self ) : if self . empty : return [ ] rows = list ( self . tbody ) observations = [ ] for row_observation , row_details in zip ( rows [ : : 2 ] , rows [ 1 : : 2 ] ) : data = { } cells = OBSERVATION_XPATH ( row_observation ) data [ 'name' ] = _clean_cell ( cells [ 0 ] ) data [ 'date' ] = _clean_cell ( cells [ 1 ] ) data [ 'magnitude' ] = _clean_cell ( cells [ 3 ] ) data [ 'obscode' ] = _clean_cell ( cells [ 6 ] ) cells = DETAILS_XPATH ( row_details ) data [ 'comp1' ] = _clean_cell ( cells [ 0 ] ) data [ 'chart' ] = _clean_cell ( cells [ 3 ] ) . replace ( 'None' , '' ) data [ 'comment_code' ] = _clean_cell ( cells [ 4 ] ) data [ 'notes' ] = _clean_cell ( cells [ 5 ] ) observations . append ( data ) return observations
3134	def update ( self , list_id , data ) : self . list_id = list_id if 'name' not in data : raise KeyError ( 'The list must have a name' ) if 'contact' not in data : raise KeyError ( 'The list must have a contact' ) if 'company' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a company' ) if 'address1' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a address1' ) if 'city' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a city' ) if 'state' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a state' ) if 'zip' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a zip' ) if 'country' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a country' ) if 'permission_reminder' not in data : raise KeyError ( 'The list must have a permission_reminder' ) if 'campaign_defaults' not in data : raise KeyError ( 'The list must have a campaign_defaults' ) if 'from_name' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a from_name' ) if 'from_email' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a from_email' ) check_email ( data [ 'campaign_defaults' ] [ 'from_email' ] ) if 'subject' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a subject' ) if 'language' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a language' ) if 'email_type_option' not in data : raise KeyError ( 'The list must have an email_type_option' ) if data [ 'email_type_option' ] not in [ True , False ] : raise TypeError ( 'The list email_type_option must be True or False' ) return self . _mc_client . _patch ( url = self . _build_path ( list_id ) , data = data )
3343	def calc_base64 ( s ) : s = compat . to_bytes ( s ) s = compat . base64_encodebytes ( s ) . strip ( ) return compat . to_native ( s )
9529	def get_encrypted_field ( base_class ) : assert not isinstance ( base_class , models . Field ) field_name = 'Encrypted' + base_class . __name__ if base_class not in FIELD_CACHE : FIELD_CACHE [ base_class ] = type ( field_name , ( EncryptedMixin , base_class ) , { 'base_class' : base_class , } ) return FIELD_CACHE [ base_class ]
882	def activateCells ( self , activeColumns , learn = True ) : prevActiveCells = self . activeCells prevWinnerCells = self . winnerCells self . activeCells = [ ] self . winnerCells = [ ] segToCol = lambda segment : int ( segment . cell / self . cellsPerColumn ) identity = lambda x : x for columnData in groupby2 ( activeColumns , identity , self . activeSegments , segToCol , self . matchingSegments , segToCol ) : ( column , activeColumns , columnActiveSegments , columnMatchingSegments ) = columnData if activeColumns is not None : if columnActiveSegments is not None : cellsToAdd = self . activatePredictedColumn ( column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells , learn ) self . activeCells += cellsToAdd self . winnerCells += cellsToAdd else : ( cellsToAdd , winnerCell ) = self . burstColumn ( column , columnMatchingSegments , prevActiveCells , prevWinnerCells , learn ) self . activeCells += cellsToAdd self . winnerCells . append ( winnerCell ) else : if learn : self . punishPredictedColumn ( column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells )
2057	def TBH ( cpu , dest ) : base_addr = dest . get_mem_base_addr ( ) if dest . mem . base in ( 'PC' , 'R15' ) : base_addr = cpu . PC offset = cpu . read_int ( base_addr + dest . get_mem_offset ( ) , 16 ) offset = Operators . ZEXTEND ( offset , cpu . address_bit_size ) cpu . PC += ( offset << 1 )
12782	def set_topic ( self , topic ) : if not topic : topic = '' result = self . _connection . put ( "room/%s" % self . id , { "room" : { "topic" : topic } } ) if result [ "success" ] : self . _load ( ) return result [ "success" ]
10639	def extract ( self , other ) : if type ( other ) is float or type ( other ) is numpy . float64 or type ( other ) is numpy . float32 : return self . _extract_mfr ( other ) elif self . _is_compound_mfr_tuple ( other ) : return self . _extract_compound_mfr ( other [ 0 ] , other [ 1 ] ) elif type ( other ) is str : return self . _extract_compound ( other ) elif type ( other ) is Material : return self . _extract_material ( other ) else : raise TypeError ( "Invalid extraction argument." )
6167	def to_bin ( data , width ) : data_str = bin ( data & ( 2 ** width - 1 ) ) [ 2 : ] . zfill ( width ) return [ int ( x ) for x in tuple ( data_str ) ]
10688	def _create_polynomial_model ( name : str , symbol : str , degree : int , ds : DataSet , dss : dict ) : ds_name = ds . name . split ( "." ) [ 0 ] . lower ( ) file_name = f"{name.lower()}-{symbol.lower()}-polynomialmodelt-{ds_name}" newmod = PolynomialModelT . create ( ds , symbol , degree ) newmod . plot ( dss , _path ( f"data/{file_name}.pdf" ) , False ) newmod . write ( _path ( f"data/{file_name}.json" ) )
6810	def fix_lsmod_for_pi3 ( self ) : r = self . local_renderer r . env . rpi2_conf = '/etc/modules-load.d/rpi2.conf' r . sudo ( "sed '/bcm2808_rng/d' {rpi2_conf}" ) r . sudo ( "echo bcm2835_rng >> {rpi2_conf}" )
11183	def default_decoder ( self , obj ) : typename , marshalled_state = self . unwrap_callback ( obj ) if typename is None : return obj try : cls , unmarshaller = self . serializer . unmarshallers [ typename ] except KeyError : raise LookupError ( 'no unmarshaller found for type "{}"' . format ( typename ) ) from None if cls is not None : instance = cls . __new__ ( cls ) unmarshaller ( instance , marshalled_state ) return instance else : return unmarshaller ( marshalled_state )
2152	def get ( self , pk = None , ** kwargs ) : self . _separate ( kwargs ) return super ( Resource , self ) . get ( pk = pk , ** kwargs )
2488	def create_disjunction_node ( self , disjunction ) : node = BNode ( ) type_triple = ( node , RDF . type , self . spdx_namespace . DisjunctiveLicenseSet ) self . graph . add ( type_triple ) licenses = self . licenses_from_tree ( disjunction ) for lic in licenses : member_triple = ( node , self . spdx_namespace . member , lic ) self . graph . add ( member_triple ) return node
6236	def set_time ( self , value : float ) : if value < 0 : value = 0 mixer . music . set_pos ( value )
1274	def from_spec ( spec , kwargs = None ) : memory = util . get_object ( obj = spec , predefined_objects = tensorforce . core . memories . memories , kwargs = kwargs ) assert isinstance ( memory , Memory ) return memory
5075	def is_course_run_enrollable ( course_run ) : now = datetime . datetime . now ( pytz . UTC ) end = parse_datetime_handle_invalid ( course_run . get ( 'end' ) ) enrollment_start = parse_datetime_handle_invalid ( course_run . get ( 'enrollment_start' ) ) enrollment_end = parse_datetime_handle_invalid ( course_run . get ( 'enrollment_end' ) ) return ( not end or end > now ) and ( not enrollment_start or enrollment_start < now ) and ( not enrollment_end or enrollment_end > now )
7294	def create_list_dict ( self , document , list_field , doc_key ) : list_dict = { "_document" : document } if isinstance ( list_field . field , EmbeddedDocumentField ) : list_dict . update ( self . create_document_dictionary ( document = list_field . field . document_type_obj , owner_document = document ) ) list_dict . update ( { "_document_field" : list_field . field , "_key" : doc_key , "_field_type" : ListField , "_widget" : get_widget ( list_field . field ) , "_value" : getattr ( document , doc_key , None ) } ) return list_dict
6449	def pylint_color ( score ) : score_cutoffs = ( 10 , 9.5 , 8.5 , 7.5 , 5 ) for i in range ( len ( score_cutoffs ) ) : if score >= score_cutoffs [ i ] : return BADGE_COLORS [ i ] return BADGE_COLORS [ - 1 ]
2036	def SSTORE ( self , offset , value ) : storage_address = self . address self . _publish ( 'will_evm_write_storage' , storage_address , offset , value ) if istainted ( self . pc ) : for taint in get_taints ( self . pc ) : value = taint_with ( value , taint ) self . world . set_storage_data ( storage_address , offset , value ) self . _publish ( 'did_evm_write_storage' , storage_address , offset , value )
1085	def timetz ( self ) : "Return the time part, with same tzinfo." return time ( self . hour , self . minute , self . second , self . microsecond , self . _tzinfo )
8653	def create_project_thread ( session , member_ids , project_id , message ) : return create_thread ( session , member_ids , 'project' , project_id , message )
2126	def data_endpoint ( cls , in_data , ignore = [ ] ) : obj , obj_type , res , res_type = cls . obj_res ( in_data , fail_on = [ ] ) data = { } if 'obj' in ignore : obj = None if 'res' in ignore : res = None if obj and obj_type == 'user' : data [ 'members__in' ] = obj if obj and obj_type == 'team' : endpoint = '%s/%s/roles/' % ( grammar . pluralize ( obj_type ) , obj ) if res is not None : data [ 'object_id' ] = res elif res : endpoint = '%s/%s/object_roles/' % ( grammar . pluralize ( res_type ) , res ) else : endpoint = '/roles/' if in_data . get ( 'type' , False ) : data [ 'role_field' ] = '%s_role' % in_data [ 'type' ] . lower ( ) for key , value in in_data . items ( ) : if key not in RESOURCE_FIELDS and key not in [ 'type' , 'user' , 'team' ] : data [ key ] = value return data , endpoint
4009	def _start_http_server ( ) : logging . info ( 'Starting HTTP server at {}:{}' . format ( constants . DAEMON_HTTP_BIND_IP , constants . DAEMON_HTTP_BIND_PORT ) ) thread = threading . Thread ( target = http_server . app . run , args = ( constants . DAEMON_HTTP_BIND_IP , constants . DAEMON_HTTP_BIND_PORT ) ) thread . daemon = True thread . start ( )
8859	def calltips ( request_data ) : code = request_data [ 'code' ] line = request_data [ 'line' ] + 1 column = request_data [ 'column' ] path = request_data [ 'path' ] encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) signatures = script . call_signatures ( ) for sig in signatures : results = ( str ( sig . module_name ) , str ( sig . name ) , [ p . description for p in sig . params ] , sig . index , sig . bracket_start , column ) return results return [ ]
9632	def render_subject ( self , context ) : rendered = self . subject_template . render ( unescape ( context ) ) return rendered . strip ( )
205	def deepcopy ( self ) : segmap = SegmentationMapOnImage ( self . arr , shape = self . shape , nb_classes = self . nb_classes ) segmap . input_was = self . input_was return segmap
6158	def FIR_fix_header ( fname_out , h ) : M = len ( h ) hq = int16 ( rint ( h * 2 ** 15 ) ) N = 8 f = open ( fname_out , 'wt' ) f . write ( '//define a FIR coefficient Array\n\n' ) f . write ( '#include <stdint.h>\n\n' ) f . write ( '#ifndef M_FIR\n' ) f . write ( '#define M_FIR %d\n' % M ) f . write ( '#endif\n' ) f . write ( '/************************************************************************/\n' ) f . write ( '/* FIR Filter Coefficients */\n' ) f . write ( 'int16_t h_FIR[M_FIR] = {' ) kk = 0 for k in range ( M ) : if ( kk < N - 1 ) and ( k < M - 1 ) : f . write ( '%5d,' % hq [ k ] ) kk += 1 elif ( kk == N - 1 ) & ( k < M - 1 ) : f . write ( '%5d,\n' % hq [ k ] ) if k < M : f . write ( ' ' ) kk = 0 else : f . write ( '%5d' % hq [ k ] ) f . write ( '};\n' ) f . write ( '/************************************************************************/\n' ) f . close ( )
6261	def swap_buffers ( self ) : self . frames += 1 glfw . swap_buffers ( self . window ) self . poll_events ( )
5286	def post ( self , request , * args , ** kwargs ) : formset = self . construct_formset ( ) if formset . is_valid ( ) : return self . formset_valid ( formset ) else : return self . formset_invalid ( formset )
3703	def Rackett ( T , Tc , Pc , Zc ) : r return R * Tc / Pc * Zc ** ( 1 + ( 1 - T / Tc ) ** ( 2 / 7. ) )
5892	def get_urls ( self ) : urls = patterns ( '' , url ( r'^upload/$' , self . admin_site . admin_view ( self . handle_upload ) , name = 'quill-file-upload' ) , ) return urls + super ( QuillAdmin , self ) . get_urls ( )
4331	def loudness ( self , gain_db = - 10.0 , reference_level = 65.0 ) : if not is_number ( gain_db ) : raise ValueError ( 'gain_db must be a number.' ) if not is_number ( reference_level ) : raise ValueError ( 'reference_level must be a number' ) if reference_level > 75 or reference_level < 50 : raise ValueError ( 'reference_level must be between 50 and 75' ) effect_args = [ 'loudness' , '{:f}' . format ( gain_db ) , '{:f}' . format ( reference_level ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'loudness' ) return self
8844	def _handle_indent_between_paren ( self , column , line , parent_impl , tc ) : pre , post = parent_impl next_char = self . _get_next_char ( tc ) prev_char = self . _get_prev_char ( tc ) prev_open = prev_char in [ '[' , '(' , '{' ] next_close = next_char in [ ']' , ')' , '}' ] ( open_line , open_symbol_col ) , ( close_line , close_col ) = self . _get_paren_pos ( tc , column ) open_line_txt = self . _helper . line_text ( open_line ) open_line_indent = len ( open_line_txt ) - len ( open_line_txt . lstrip ( ) ) if prev_open : post = ( open_line_indent + self . editor . tab_length ) * ' ' elif next_close and prev_char != ',' : post = open_line_indent * ' ' elif tc . block ( ) . blockNumber ( ) == open_line : post = open_symbol_col * ' ' if close_line and close_col : txt = self . _helper . line_text ( close_line ) bn = tc . block ( ) . blockNumber ( ) flg = bn == close_line next_indent = self . _helper . line_indent ( bn + 1 ) * ' ' if flg and txt . strip ( ) . endswith ( ':' ) and next_indent == post : post += self . editor . tab_length * ' ' if next_char in [ '"' , "'" ] : tc . movePosition ( tc . Left ) is_string = self . _helper . is_comment_or_string ( tc , formats = [ 'string' ] ) if next_char in [ '"' , "'" ] : tc . movePosition ( tc . Right ) if is_string : trav = QTextCursor ( tc ) while self . _helper . is_comment_or_string ( trav , formats = [ 'string' ] ) : trav . movePosition ( trav . Left ) trav . movePosition ( trav . Right ) symbol = '%s' % self . _get_next_char ( trav ) pre += symbol post += symbol return pre , post
1057	def add_extension ( module , name , code ) : code = int ( code ) if not 1 <= code <= 0x7fffffff : raise ValueError , "code out of range" key = ( module , name ) if ( _extension_registry . get ( key ) == code and _inverted_registry . get ( code ) == key ) : return if key in _extension_registry : raise ValueError ( "key %s is already registered with code %s" % ( key , _extension_registry [ key ] ) ) if code in _inverted_registry : raise ValueError ( "code %s is already in use for key %s" % ( code , _inverted_registry [ code ] ) ) _extension_registry [ key ] = code _inverted_registry [ code ] = key
8341	def _invert ( h ) : "Cheap function to invert a hash." i = { } for k , v in h . items ( ) : i [ v ] = k return i
10021	def get_environments ( self ) : response = self . ebs . describe_environments ( application_name = self . app_name , include_deleted = False ) return response [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ]
5981	def setup_figure ( figsize , as_subplot ) : if not as_subplot : fig = plt . figure ( figsize = figsize ) return fig
217	def append ( self , key : str , value : str ) -> None : append_key = key . lower ( ) . encode ( "latin-1" ) append_value = value . encode ( "latin-1" ) self . _list . append ( ( append_key , append_value ) )
11230	def xafter ( self , dt , count = None , inc = False ) : if self . _cache_complete : gen = self . _cache else : gen = self if inc : def comp ( dc , dtc ) : return dc >= dtc else : def comp ( dc , dtc ) : return dc > dtc n = 0 for d in gen : if comp ( d , dt ) : if count is not None : n += 1 if n > count : break yield d
6145	def DSP_callback_toc ( self ) : if self . Tcapture > 0 : self . DSP_toc . append ( time . time ( ) - self . start_time )
11661	def transform ( self , X ) : self . _check_fitted ( ) X = as_features ( X , stack = True ) assignments = self . kmeans_fit_ . predict ( X . stacked_features ) return self . _group_assignments ( X , assignments )
12962	def exists ( self , pk ) : conn = self . _get_connection ( ) key = self . _get_key_for_id ( pk ) return conn . exists ( key )
2587	def start ( self ) : start = time . time ( ) self . _kill_event = threading . Event ( ) self . procs = { } for worker_id in range ( self . worker_count ) : p = multiprocessing . Process ( target = worker , args = ( worker_id , self . uid , self . pending_task_queue , self . pending_result_queue , self . ready_worker_queue , ) ) p . start ( ) self . procs [ worker_id ] = p logger . debug ( "Manager synced with workers" ) self . _task_puller_thread = threading . Thread ( target = self . pull_tasks , args = ( self . _kill_event , ) ) self . _result_pusher_thread = threading . Thread ( target = self . push_results , args = ( self . _kill_event , ) ) self . _task_puller_thread . start ( ) self . _result_pusher_thread . start ( ) logger . info ( "Loop start" ) self . _kill_event . wait ( ) logger . critical ( "[MAIN] Received kill event, terminating worker processes" ) self . _task_puller_thread . join ( ) self . _result_pusher_thread . join ( ) for proc_id in self . procs : self . procs [ proc_id ] . terminate ( ) logger . critical ( "Terminating worker {}:{}" . format ( self . procs [ proc_id ] , self . procs [ proc_id ] . is_alive ( ) ) ) self . procs [ proc_id ] . join ( ) logger . debug ( "Worker:{} joined successfully" . format ( self . procs [ proc_id ] ) ) self . task_incoming . close ( ) self . result_outgoing . close ( ) self . context . term ( ) delta = time . time ( ) - start logger . info ( "process_worker_pool ran for {} seconds" . format ( delta ) ) return
8697	def __clear_buffers ( self ) : try : self . _port . reset_input_buffer ( ) self . _port . reset_output_buffer ( ) except AttributeError : self . _port . flushInput ( ) self . _port . flushOutput ( )
11261	def resplit ( prev , pattern , * args , ** kw ) : maxsplit = 0 if 'maxsplit' not in kw else kw . pop ( 'maxsplit' ) pattern_obj = re . compile ( pattern , * args , ** kw ) for s in prev : yield pattern_obj . split ( s , maxsplit = maxsplit )
10908	def circles ( st , layer , axis , ax = None , talpha = 1.0 , cedge = 'white' , cface = 'white' ) : pos = st . obj_get_positions ( ) rad = st . obj_get_radii ( ) shape = st . ishape . shape . tolist ( ) shape . pop ( axis ) if ax is None : fig = plt . figure ( ) axisbg = 'white' if cface == 'black' else 'black' sx , sy = ( ( 1 , shape [ 1 ] / float ( shape [ 0 ] ) ) if shape [ 0 ] > shape [ 1 ] else ( shape [ 0 ] / float ( shape [ 1 ] ) , 1 ) ) ax = fig . add_axes ( ( 0 , 0 , sx , sy ) , axisbg = axisbg ) particles = np . arange ( len ( pos ) ) [ np . abs ( pos [ : , axis ] - layer ) < rad ] scale = 1.0 for i in particles : p = pos [ i ] . copy ( ) r = 2 * np . sqrt ( rad [ i ] ** 2 - ( p [ axis ] - layer ) ** 2 ) if axis == 0 : ix = 1 iy = 2 elif axis == 1 : ix = 0 iy = 2 elif axis == 2 : ix = 0 iy = 1 c = Circle ( ( p [ ix ] / scale , p [ iy ] / scale ) , radius = r / 2 / scale , fc = cface , ec = cedge , alpha = talpha ) ax . add_patch ( c ) plt . axis ( 'equal' ) return ax
1032	def b16decode ( s , casefold = False ) : if casefold : s = s . upper ( ) if re . search ( '[^0-9A-F]' , s ) : raise TypeError ( 'Non-base16 digit found' ) return binascii . unhexlify ( s )
2621	def security_group ( self , vpc ) : sg = vpc . create_security_group ( GroupName = "private-subnet" , Description = "security group for remote executors" ) ip_ranges = [ { 'CidrIp' : '10.0.0.0/16' } ] in_permissions = [ { 'IpProtocol' : 'TCP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : ip_ranges , } , { 'IpProtocol' : 'UDP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : ip_ranges , } , { 'IpProtocol' : 'ICMP' , 'FromPort' : - 1 , 'ToPort' : - 1 , 'IpRanges' : [ { 'CidrIp' : '0.0.0.0/0' } ] , } , { 'IpProtocol' : 'TCP' , 'FromPort' : 22 , 'ToPort' : 22 , 'IpRanges' : [ { 'CidrIp' : '0.0.0.0/0' } ] , } ] out_permissions = [ { 'IpProtocol' : 'TCP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : [ { 'CidrIp' : '0.0.0.0/0' } ] , } , { 'IpProtocol' : 'TCP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : ip_ranges , } , { 'IpProtocol' : 'UDP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : ip_ranges , } , ] sg . authorize_ingress ( IpPermissions = in_permissions ) sg . authorize_egress ( IpPermissions = out_permissions ) self . sg_id = sg . id return sg
7830	def _new_from_xml ( cls , xmlnode ) : field_type = xmlnode . prop ( "type" ) label = from_utf8 ( xmlnode . prop ( "label" ) ) name = from_utf8 ( xmlnode . prop ( "var" ) ) child = xmlnode . children values = [ ] options = [ ] required = False desc = None while child : if child . type != "element" or child . ns ( ) . content != DATAFORM_NS : pass elif child . name == "required" : required = True elif child . name == "desc" : desc = from_utf8 ( child . getContent ( ) ) elif child . name == "value" : values . append ( from_utf8 ( child . getContent ( ) ) ) elif child . name == "option" : options . append ( Option . _new_from_xml ( child ) ) child = child . next if field_type and not field_type . endswith ( "-multi" ) and len ( values ) > 1 : raise BadRequestProtocolError ( "Multiple values for a single-value field" ) return cls ( name , values , field_type , label , options , required , desc )
7501	def get_shape ( spans , loci ) : width = 0 for idx in xrange ( loci . shape [ 0 ] ) : width += spans [ loci [ idx ] , 1 ] - spans [ loci [ idx ] , 0 ] return width
9732	def get_6d ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . body_count ) : component_position , position = QRTPacket . _get_exact ( RT6DBodyPosition , data , component_position ) component_position , matrix = QRTPacket . _get_tuple ( RT6DBodyRotation , data , component_position ) append_components ( ( position , matrix ) ) return components
11324	def fix_name_capitalization ( lastname , givennames ) : lastnames = lastname . split ( ) if len ( lastnames ) == 1 : if '-' in lastname : names = lastname . split ( '-' ) names = map ( lambda a : a [ 0 ] + a [ 1 : ] . lower ( ) , names ) lastname = '-' . join ( names ) else : lastname = lastname [ 0 ] + lastname [ 1 : ] . lower ( ) else : names = [ ] for name in lastnames : if re . search ( r'[A-Z]\.' , name ) : names . append ( name ) else : names . append ( name [ 0 ] + name [ 1 : ] . lower ( ) ) lastname = ' ' . join ( names ) lastname = collapse_initials ( lastname ) names = [ ] for name in givennames : if re . search ( r'[A-Z]\.' , name ) : names . append ( name ) else : names . append ( name [ 0 ] + name [ 1 : ] . lower ( ) ) givennames = ' ' . join ( names ) return lastname , givennames
7581	def parse ( self , psearch , dsearch ) : stable = "" with open ( self . repfile ) as orep : dat = orep . readlines ( ) for line in dat : if "Estimated Ln Prob of Data" in line : self . est_lnlik = float ( line . split ( ) [ - 1 ] ) if "Mean value of ln likelihood" in line : self . mean_lnlik = float ( line . split ( ) [ - 1 ] ) if "Variance of ln likelihood" in line : self . var_lnlik = float ( line . split ( ) [ - 1 ] ) if "Mean value of alpha" in line : self . alpha = float ( line . split ( ) [ - 1 ] ) nonline = psearch . search ( line ) popline = dsearch . search ( line ) if nonline : abc = line . strip ( ) . split ( ) outstr = "{}{}{}" . format ( " " . join ( [ abc [ 0 ] , abc [ 0 ] , abc [ 2 ] , abc [ 0 ] . split ( '.' ) [ 0 ] ] ) , " : " , " " . join ( abc [ 4 : ] ) ) self . inds += 1 stable += outstr + "\n" elif popline : abc = line . strip ( ) . split ( ) prop = [ "0.000" ] * self . kpop pidx = int ( abc [ 3 ] ) - 1 prop [ pidx ] = "1.000" outstr = "{}{}{}" . format ( " " . join ( [ abc [ 0 ] , abc [ 0 ] , abc [ 2 ] , abc [ 0 ] . split ( '.' ) [ 0 ] ] ) , " : " , " " . join ( prop ) ) self . inds += 1 stable += outstr + "\n" stable += "\n" return stable
12294	def annotate_metadata_platform ( repo ) : print ( "Added platform information" ) package = repo . package mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'instrumentation' , name = 'platform' ) package [ 'platform' ] = repomgr . get_metadata ( )
