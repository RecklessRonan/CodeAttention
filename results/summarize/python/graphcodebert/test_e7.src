5899	def get_double_or_single_prec_mdrun ( ) : try : gromacs . mdrun_d ( h = True , stdout = False , stderr = False ) logger . debug ( "using double precision gromacs.mdrun_d" ) return gromacs . mdrun_d except ( AttributeError , GromacsError , OSError ) : wmsg = "No 'mdrun_d' binary found so trying 'mdrun' instead.\n" "(Note that energy minimization runs better with mdrun_d.)" logger . warn ( wmsg ) warnings . warn ( wmsg , category = AutoCorrectionWarning ) return gromacs . mdrun
3138	def get ( self , app_id , ** queryparams ) : self . app_id = app_id return self . _mc_client . _get ( url = self . _build_path ( app_id ) , ** queryparams )
7286	def has_delete_permission ( self , request ) : return request . user . is_authenticated and request . user . is_active and request . user . is_superuser
8154	def create ( self , name , overwrite = True ) : self . _name = name . rstrip ( ".db" ) from os import unlink if overwrite : try : unlink ( self . _name + ".db" ) except : pass self . _con = sqlite . connect ( self . _name + ".db" ) self . _cur = self . _con . cursor ( )
2380	def _load_rule_file ( self , filename ) : if not ( os . path . exists ( filename ) ) : sys . stderr . write ( "rflint: %s: No such file or directory\n" % filename ) return try : basename = os . path . basename ( filename ) ( name , ext ) = os . path . splitext ( basename ) imp . load_source ( name , filename ) except Exception as e : sys . stderr . write ( "rflint: %s: exception while loading: %s\n" % ( filename , str ( e ) ) )
8859	def calltips ( request_data ) : code = request_data [ 'code' ] line = request_data [ 'line' ] + 1 column = request_data [ 'column' ] path = request_data [ 'path' ] encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) signatures = script . call_signatures ( ) for sig in signatures : results = ( str ( sig . module_name ) , str ( sig . name ) , [ p . description for p in sig . params ] , sig . index , sig . bracket_start , column ) return results return [ ]
11080	def get_user ( self , username ) : if hasattr ( self . _bot , 'user_manager' ) : user = self . _bot . user_manager . get_by_username ( username ) if user : return user user = SlackUser . get_user ( self . _bot . sc , username ) self . _bot . user_manager . set ( user ) return user return SlackUser . get_user ( self . _bot . sc , username )
6628	def unpublish ( namespace , name , version , registry = None ) : registry = registry or Registry_Base_URL url = '%s/%s/%s/versions/%s' % ( registry , namespace , name , version ) headers = _headersForRegistry ( registry ) response = requests . delete ( url , headers = headers ) response . raise_for_status ( ) return None
9071	def _initialize ( self ) : r if self . _mean is None or self . _cov is None : return Q = self . _cov [ "QS" ] [ 0 ] [ 0 ] S = self . _cov [ "QS" ] [ 1 ] if S . size > 0 : self . tau [ : ] = 1 / npsum ( ( Q * sqrt ( S ) ) ** 2 , axis = 1 ) else : self . tau [ : ] = 0.0 self . eta [ : ] = self . _mean self . eta [ : ] *= self . tau
534	def _getRegions ( self ) : def makeRegion ( name , r ) : r = Region ( r , self ) return r regions = CollectionWrapper ( engine_internal . Network . getRegions ( self ) , makeRegion ) return regions
8052	def parse_theme ( self , xml ) : kt = KulerTheme ( ) kt . author = xml . getElementsByTagName ( "author" ) [ 0 ] kt . author = kt . author . childNodes [ 1 ] . childNodes [ 0 ] . nodeValue kt . id = int ( self . parse_tag ( xml , "id" ) ) kt . label = self . parse_tag ( xml , "label" ) mode = self . parse_tag ( xml , "mode" ) for swatch in xml . getElementsByTagName ( "swatch" ) : c1 = float ( self . parse_tag ( swatch , "c1" ) ) c2 = float ( self . parse_tag ( swatch , "c2" ) ) c3 = float ( self . parse_tag ( swatch , "c3" ) ) c4 = float ( self . parse_tag ( swatch , "c4" ) ) if mode == "rgb" : kt . append ( ( c1 , c2 , c3 ) ) if mode == "cmyk" : kt . append ( cmyk_to_rgb ( c1 , c2 , c3 , c4 ) ) if mode == "hsv" : kt . append ( colorsys . hsv_to_rgb ( c1 , c2 , c3 ) ) if mode == "hex" : kt . append ( hex_to_rgb ( c1 ) ) if mode == "lab" : kt . append ( lab_to_rgb ( c1 , c2 , c3 ) ) if self . _cache . exists ( self . id_string + str ( kt . id ) ) : xml = self . _cache . read ( self . id_string + str ( kt . id ) ) xml = minidom . parseString ( xml ) for tags in xml . getElementsByTagName ( "tag" ) : tags = self . parse_tag ( tags , "label" ) tags = tags . split ( " " ) kt . tags . extend ( tags ) return kt
3088	def _delete_entity ( self ) : if self . _is_ndb ( ) : _NDB_KEY ( self . _model , self . _key_name ) . delete ( ) else : entity_key = db . Key . from_path ( self . _model . kind ( ) , self . _key_name ) db . delete ( entity_key )
8070	def replace_entities ( ustring , placeholder = " " ) : def _repl_func ( match ) : try : if match . group ( 1 ) : return unichr ( int ( match . group ( 2 ) ) ) else : try : return cp1252 [ unichr ( int ( match . group ( 3 ) ) ) ] . strip ( ) except : return unichr ( name2codepoint [ match . group ( 3 ) ] ) except : return placeholder if not isinstance ( ustring , unicode ) : ustring = UnicodeDammit ( ustring ) . unicode ustring = ustring . replace ( "&nbsp;" , " " ) _entity_re = re . compile ( r'&(?:(#)(\d+)|([^;^> ]+));' ) return _entity_re . sub ( _repl_func , ustring )
2731	def create ( self ) : data = { "name" : self . name , "ip_address" : self . ip_address , } domain = self . get_data ( "domains" , type = POST , params = data ) return domain
10689	def _create_air ( ) : name = "Air" namel = name . lower ( ) mm = 28.9645 ds_dict = _create_ds_dict ( [ "dataset-air-lienhard2015" , "dataset-air-lienhard2018" ] ) active_ds = "dataset-air-lienhard2018" model_dict = { "rho" : IgRhoT ( mm , 101325.0 ) , "beta" : IgBetaT ( ) } model_type = "polynomialmodelt" for property in [ "Cp" , "mu" , "k" ] : name = f"data/{namel}-{property.lower()}-{model_type}-{active_ds}.json" model_dict [ property ] = PolynomialModelT . read ( _path ( name ) ) material = Material ( name , StateOfMatter . gas , model_dict ) return material , ds_dict
9496	def parse_module ( path , excludes = None ) : file = path / MODULE_FILENAME if not file . exists ( ) : raise MissingFile ( file ) id = _parse_document_id ( etree . parse ( file . open ( ) ) ) excludes = excludes or [ ] excludes . extend ( [ lambda filepath : filepath . name == MODULE_FILENAME , ] ) resources_paths = _find_resources ( path , excludes = excludes ) resources = tuple ( _resource_from_path ( res ) for res in resources_paths ) return Module ( id , file , resources )
2394	def calc_list_average ( l ) : total = 0.0 for value in l : total += value return total / len ( l )
3604	def _authenticate ( self , params , headers ) : if self . authentication : user = self . authentication . get_user ( ) params . update ( { 'auth' : user . firebase_auth_token } ) headers . update ( self . authentication . authenticator . HEADERS )
431	def save_image ( image , image_path = '_temp.png' ) : try : imageio . imwrite ( image_path , image ) except Exception : imageio . imwrite ( image_path , image [ : , : , 0 ] )
7238	def iterwindows ( self , count = 64 , window_shape = ( 256 , 256 ) ) : if count is None : while True : yield self . randwindow ( window_shape ) else : for i in xrange ( count ) : yield self . randwindow ( window_shape )
5256	def disassemble_all ( bytecode , pc = 0 , fork = DEFAULT_FORK ) : if isinstance ( bytecode , bytes ) : bytecode = bytearray ( bytecode ) if isinstance ( bytecode , str ) : bytecode = bytearray ( bytecode . encode ( 'latin-1' ) ) bytecode = iter ( bytecode ) while True : instr = disassemble_one ( bytecode , pc = pc , fork = fork ) if not instr : return pc += instr . size yield instr
12237	def doublewell ( theta ) : k0 , k1 , depth = 0.01 , 100 , 0.5 shallow = 0.5 * k0 * theta ** 2 + depth deep = 0.5 * k1 * theta ** 2 obj = float ( np . minimum ( shallow , deep ) ) grad = np . where ( deep < shallow , k1 * theta , k0 * theta ) return obj , grad
8183	def edge ( self , id1 , id2 ) : if id1 in self and id2 in self and self [ id2 ] in self [ id1 ] . links : return self [ id1 ] . links . edge ( id2 ) return None
929	def _getFuncPtrAndParams ( self , funcName ) : params = None if isinstance ( funcName , basestring ) : if funcName == 'sum' : fp = _aggr_sum elif funcName == 'first' : fp = _aggr_first elif funcName == 'last' : fp = _aggr_last elif funcName == 'mean' : fp = _aggr_mean elif funcName == 'max' : fp = max elif funcName == 'min' : fp = min elif funcName == 'mode' : fp = _aggr_mode elif funcName . startswith ( 'wmean:' ) : fp = _aggr_weighted_mean paramsName = funcName [ 6 : ] params = [ f [ 0 ] for f in self . _inputFields ] . index ( paramsName ) else : fp = funcName return ( fp , params )
3178	def get ( self , list_id , merge_id ) : self . list_id = list_id self . merge_id = merge_id return self . _mc_client . _get ( url = self . _build_path ( list_id , 'merge-fields' , merge_id ) )
3146	def _build_path ( self , * args ) : return '/' . join ( chain ( ( self . endpoint , ) , map ( str , args ) ) )
1411	def filter_spouts ( table , header ) : spouts_info = [ ] for row in table : if row [ 0 ] == 'spout' : spouts_info . append ( row ) return spouts_info , header
11541	def write ( self , pin , value , pwm = False ) : if type ( pin ) is list : for p in pin : self . write ( p , value , pwm ) return if pwm and type ( value ) is not int and type ( value ) is not float : raise TypeError ( 'pwm is set, but value is not a float or int' ) pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : lpin = self . _pin_lin . get ( pin , None ) if lpin and type ( lpin [ 'write' ] ) is tuple : write_range = lpin [ 'write' ] value = self . _linear_interpolation ( value , * write_range ) self . _write ( pin_id , value , pwm ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
1593	def prepare ( self , context ) : for stream_id , targets in self . targets . items ( ) : for target in targets : target . prepare ( context , stream_id )
9613	def element ( self , using , value ) : return self . _execute ( Command . FIND_CHILD_ELEMENT , { 'using' : using , 'value' : value } )
3718	def estimate ( self ) : self . mul ( 300 ) self . Cpig ( 300 ) estimates = { 'Tb' : self . Tb ( self . counts ) , 'Tm' : self . Tm ( self . counts ) , 'Tc' : self . Tc ( self . counts , self . Tb_estimated ) , 'Pc' : self . Pc ( self . counts , self . atom_count ) , 'Vc' : self . Vc ( self . counts ) , 'Hf' : self . Hf ( self . counts ) , 'Gf' : self . Gf ( self . counts ) , 'Hfus' : self . Hfus ( self . counts ) , 'Hvap' : self . Hvap ( self . counts ) , 'mul' : self . mul , 'mul_coeffs' : self . calculated_mul_coeffs , 'Cpig' : self . Cpig , 'Cpig_coeffs' : self . calculated_Cpig_coeffs } return estimates
5386	def _format_task_name ( job_id , task_id , task_attempt ) : docker_name = '%s.%s' % ( job_id , 'task' if task_id is None else task_id ) if task_attempt is not None : docker_name += '.' + str ( task_attempt ) return 'dsub-{}' . format ( _convert_suffix_to_docker_chars ( docker_name ) )
13449	def authed_post ( self , url , data , response_code = 200 , follow = False , headers = { } ) : if not self . authed : self . authorize ( ) response = self . client . post ( url , data , follow = follow , ** headers ) self . assertEqual ( response_code , response . status_code ) return response
9732	def get_6d ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . body_count ) : component_position , position = QRTPacket . _get_exact ( RT6DBodyPosition , data , component_position ) component_position , matrix = QRTPacket . _get_tuple ( RT6DBodyRotation , data , component_position ) append_components ( ( position , matrix ) ) return components
6878	def _validate_sqlitecurve_filters ( filterstring , lccolumns ) : stringelems = _squeeze ( filterstring ) . lower ( ) stringelems = filterstring . replace ( '(' , '' ) stringelems = stringelems . replace ( ')' , '' ) stringelems = stringelems . replace ( ',' , '' ) stringelems = stringelems . replace ( "'" , '"' ) stringelems = stringelems . replace ( '\n' , ' ' ) stringelems = stringelems . replace ( '\t' , ' ' ) stringelems = _squeeze ( stringelems ) stringelems = stringelems . split ( ' ' ) stringelems = [ x . strip ( ) for x in stringelems ] stringwords = [ ] for x in stringelems : try : float ( x ) except ValueError as e : stringwords . append ( x ) stringwords2 = [ ] for x in stringwords : if not ( x . startswith ( '"' ) and x . endswith ( '"' ) ) : stringwords2 . append ( x ) stringwords2 = [ x for x in stringwords2 if len ( x ) > 0 ] wordset = set ( stringwords2 ) allowedwords = SQLITE_ALLOWED_WORDS + lccolumns checkset = set ( allowedwords ) validatecheck = list ( wordset - checkset ) if len ( validatecheck ) > 0 : LOGWARNING ( "provided SQL filter string '%s' " "contains non-allowed keywords" % filterstring ) return None else : return filterstring
6551	def from_configurations ( cls , configurations , variables , vartype , name = None ) : def func ( * args ) : return args in configurations return cls ( func , configurations , variables , vartype , name )
7693	def _check_authorization ( self , properties , stream ) : authzid = properties . get ( "authzid" ) if not authzid : return True try : jid = JID ( authzid ) except ValueError : return False if "username" not in properties : result = False elif jid . local != properties [ "username" ] : result = False elif jid . domain != stream . me . domain : result = False elif jid . resource : result = False else : result = True return result
7698	def as_xml ( self , parent = None ) : if parent is not None : element = ElementTree . SubElement ( parent , ITEM_TAG ) else : element = ElementTree . Element ( ITEM_TAG ) element . set ( "jid" , unicode ( self . jid ) ) if self . name is not None : element . set ( "name" , self . name ) if self . subscription is not None : element . set ( "subscription" , self . subscription ) if self . ask : element . set ( "ask" , self . ask ) if self . approved : element . set ( "approved" , "true" ) for group in self . groups : ElementTree . SubElement ( element , GROUP_TAG ) . text = group return element
13811	def GetTopLevelContainingType ( self ) : desc = self while desc . containing_type is not None : desc = desc . containing_type return desc
9867	async def rt_unsubscribe ( self ) : if self . _subscription_id is None : _LOGGER . error ( "Not subscribed." ) return await self . _tibber_control . sub_manager . unsubscribe ( self . _subscription_id )
458	def alphas ( shape , alpha_value , name = None ) : with ops . name_scope ( name , "alphas" , [ shape ] ) as name : alpha_tensor = convert_to_tensor ( alpha_value ) alpha_dtype = dtypes . as_dtype ( alpha_tensor . dtype ) . base_dtype if not isinstance ( shape , ops . Tensor ) : try : shape = constant_op . _tensor_shape_tensor_conversion_function ( tensor_shape . TensorShape ( shape ) ) except ( TypeError , ValueError ) : shape = ops . convert_to_tensor ( shape , dtype = dtypes . int32 ) if not shape . _shape_tuple ( ) : shape = reshape ( shape , [ - 1 ] ) try : output = constant ( alpha_value , shape = shape , dtype = alpha_dtype , name = name ) except ( TypeError , ValueError ) : output = fill ( shape , constant ( alpha_value , dtype = alpha_dtype ) , name = name ) if output . dtype . base_dtype != alpha_dtype : raise AssertionError ( "Dtypes do not corresponds: %s and %s" % ( output . dtype . base_dtype , alpha_dtype ) ) return output
2215	def _join_itemstrs ( itemstrs , itemsep , newlines , _leaf_info , nobraces , trailing_sep , compact_brace , lbr , rbr ) : use_newline = newlines > 0 if newlines < 0 : use_newline = ( - newlines ) < _leaf_info [ 'max_height' ] if use_newline : sep = ',\n' if nobraces : body_str = sep . join ( itemstrs ) if trailing_sep and len ( itemstrs ) > 0 : body_str += ',' retstr = body_str else : if compact_brace : indented = itemstrs else : import ubelt as ub prefix = ' ' * 4 indented = [ ub . indent ( s , prefix ) for s in itemstrs ] body_str = sep . join ( indented ) if trailing_sep and len ( itemstrs ) > 0 : body_str += ',' if compact_brace : braced_body_str = ( lbr + body_str . replace ( '\n' , '\n ' ) + rbr ) else : braced_body_str = ( lbr + '\n' + body_str + '\n' + rbr ) retstr = braced_body_str else : sep = ',' + itemsep body_str = sep . join ( itemstrs ) if trailing_sep and len ( itemstrs ) > 0 : body_str += ',' retstr = ( lbr + body_str + rbr ) return retstr
12613	def is_unique ( self , table_name , sample , unique_fields = None ) : try : eid = find_unique ( self . table ( table_name ) , sample = sample , unique_fields = unique_fields ) except : return False else : return eid is not None
9986	def get_description ( ) : with open ( path . join ( here , 'README.rst' ) , 'r' ) as f : data = f . read ( ) return data
8005	def from_xml ( self , xmlnode ) : if xmlnode . type != "element" : raise ValueError ( "XML node is not a jabber:x:delay element (not an element)" ) ns = get_node_ns_uri ( xmlnode ) if ns and ns != DELAY_NS or xmlnode . name != "x" : raise ValueError ( "XML node is not a jabber:x:delay element" ) stamp = xmlnode . prop ( "stamp" ) if stamp . endswith ( "Z" ) : stamp = stamp [ : - 1 ] if "-" in stamp : stamp = stamp . split ( "-" , 1 ) [ 0 ] try : tm = time . strptime ( stamp , "%Y%m%dT%H:%M:%S" ) except ValueError : raise BadRequestProtocolError ( "Bad timestamp" ) tm = tm [ 0 : 8 ] + ( 0 , ) self . timestamp = datetime . datetime . fromtimestamp ( time . mktime ( tm ) ) delay_from = from_utf8 ( xmlnode . prop ( "from" ) ) if delay_from : try : self . delay_from = JID ( delay_from ) except JIDError : raise JIDMalformedProtocolError ( "Bad JID in the jabber:x:delay 'from' attribute" ) else : self . delay_from = None self . reason = from_utf8 ( xmlnode . getContent ( ) )
444	def roi_pooling ( input , rois , pool_height , pool_width ) : out = roi_pooling_module . roi_pooling ( input , rois , pool_height = pool_height , pool_width = pool_width ) output , argmax_output = out [ 0 ] , out [ 1 ] return output
13686	def embed_data ( request ) : result = _EmbedDataFixture ( request ) result . delete_data_dir ( ) result . create_data_dir ( ) yield result result . delete_data_dir ( )
13802	def _auth ( self , client_id , key , method , callback ) : available = auth_methods . keys ( ) if method not in available : raise Proauth2Error ( 'invalid_request' , 'unsupported authentication method: %s' 'available methods: %s' % ( method , '\n' . join ( available ) ) ) client = yield Task ( self . data_store . fetch , 'applications' , client_id = client_id ) if not client : raise Proauth2Error ( 'access_denied' ) if not auth_methods [ method ] ( key , client [ 'client_secret' ] ) : raise Proauth2Error ( 'access_denied' ) callback ( )
10727	def _handle_array ( toks ) : if len ( toks ) == 5 and toks [ 1 ] == '{' and toks [ 4 ] == '}' : subtree = toks [ 2 : 4 ] signature = '' . join ( s for ( _ , s ) in subtree ) [ key_func , value_func ] = [ f for ( f , _ ) in subtree ] def the_dict_func ( a_dict , variant = 0 ) : elements = [ ( key_func ( x ) , value_func ( y ) ) for ( x , y ) in a_dict . items ( ) ] level = 0 if elements == [ ] else max ( max ( x , y ) for ( ( _ , x ) , ( _ , y ) ) in elements ) ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( level , variant ) return ( dbus . types . Dictionary ( ( ( x , y ) for ( ( x , _ ) , ( y , _ ) ) in elements ) , signature = signature , variant_level = obj_level ) , func_level ) return ( the_dict_func , 'a{' + signature + '}' ) if len ( toks ) == 2 : ( func , sig ) = toks [ 1 ] def the_array_func ( a_list , variant = 0 ) : if isinstance ( a_list , dict ) : raise IntoDPValueError ( a_list , "a_list" , "is a dict, must be an array" ) elements = [ func ( x ) for x in a_list ] level = 0 if elements == [ ] else max ( x for ( _ , x ) in elements ) ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( level , variant ) return ( dbus . types . Array ( ( x for ( x , _ ) in elements ) , signature = sig , variant_level = obj_level ) , func_level ) return ( the_array_func , 'a' + sig ) raise IntoDPValueError ( toks , "toks" , "unexpected tokens" )
4977	def course_or_program_exist ( self , course_id , program_uuid ) : course_exists = course_id and CourseApiClient ( ) . get_course_details ( course_id ) program_exists = program_uuid and CourseCatalogApiServiceClient ( ) . program_exists ( program_uuid ) return course_exists or program_exists
7691	def validate ( schema_file = None , jams_files = None ) : schema = load_json ( schema_file ) for jams_file in jams_files : try : jams = load_json ( jams_file ) jsonschema . validate ( jams , schema ) print '{:s} was successfully validated' . format ( jams_file ) except jsonschema . ValidationError as exc : print '{:s} was NOT successfully validated' . format ( jams_file ) print exc
13728	def balance ( address ) : txhistory = Address . transactions ( address ) balance = 0 for i in txhistory : if i . recipientId == address : balance += i . amount if i . senderId == address : balance -= ( i . amount + i . fee ) delegates = Delegate . delegates ( ) for i in delegates : if address == i . address : forged_blocks = Delegate . blocks ( i . pubkey ) for block in forged_blocks : balance += ( block . reward + block . totalFee ) if balance < 0 : height = Node . height ( ) logger . fatal ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) raise NegativeBalanceError ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) return balance
3606	def get_async ( self , url , name , callback = None , params = None , headers = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) process_pool . apply_async ( make_get_request , args = ( endpoint , params , headers ) , callback = callback )
1125	def Rule ( name , loc = None ) : @ llrule ( loc , lambda parser : getattr ( parser , name ) . expected ( parser ) ) def rule ( parser ) : return getattr ( parser , name ) ( ) return rule
9585	def write_compressed_var_array ( fd , array , name ) : bd = BytesIO ( ) write_var_array ( bd , array , name ) data = zlib . compress ( bd . getvalue ( ) ) bd . close ( ) fd . write ( struct . pack ( 'b3xI' , etypes [ 'miCOMPRESSED' ] [ 'n' ] , len ( data ) ) ) fd . write ( data )
9435	def _read_a_packet ( file_h , hdrp , layers = 0 ) : raw_packet_header = file_h . read ( 16 ) if not raw_packet_header or len ( raw_packet_header ) != 16 : return None if hdrp [ 0 ] . byteorder == 'big' : packet_header = struct . unpack ( '>IIII' , raw_packet_header ) else : packet_header = struct . unpack ( '<IIII' , raw_packet_header ) ( timestamp , timestamp_us , capture_len , packet_len ) = packet_header raw_packet_data = file_h . read ( capture_len ) if not raw_packet_data or len ( raw_packet_data ) != capture_len : return None if layers > 0 : layers -= 1 raw_packet = linklayer . clookup ( hdrp [ 0 ] . ll_type ) ( raw_packet_data , layers = layers ) else : raw_packet = raw_packet_data packet = pcap_packet ( hdrp , timestamp , timestamp_us , capture_len , packet_len , raw_packet ) return packet
2675	def build ( src , requirements = None , local_package = None , config_file = 'config.yaml' , profile_name = None , ) : path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) dist_directory = cfg . get ( 'dist_directory' , 'dist' ) path_to_dist = os . path . join ( src , dist_directory ) mkdir ( path_to_dist ) function_name = cfg . get ( 'function_name' ) output_filename = '{0}-{1}.zip' . format ( timestamp ( ) , function_name ) path_to_temp = mkdtemp ( prefix = 'aws-lambda' ) pip_install_to_target ( path_to_temp , requirements = requirements , local_package = local_package , ) if 'zope' in os . listdir ( path_to_temp ) : print ( 'Zope packages detected; fixing Zope package paths to ' 'make them importable.' , ) with open ( os . path . join ( path_to_temp , 'zope/__init__.py' ) , 'wb' ) : pass output_filename = ( '{0}.zip' . format ( output_filename ) if not output_filename . endswith ( '.zip' ) else output_filename ) build_config = defaultdict ( ** cfg . get ( 'build' , { } ) ) build_source_directories = build_config . get ( 'source_directories' , '' ) build_source_directories = ( build_source_directories if build_source_directories is not None else '' ) source_directories = [ d . strip ( ) for d in build_source_directories . split ( ',' ) ] files = [ ] for filename in os . listdir ( src ) : if os . path . isfile ( filename ) : if filename == '.DS_Store' : continue if filename == config_file : continue print ( 'Bundling: %r' % filename ) files . append ( os . path . join ( src , filename ) ) elif os . path . isdir ( filename ) and filename in source_directories : print ( 'Bundling directory: %r' % filename ) files . append ( os . path . join ( src , filename ) ) os . chdir ( path_to_temp ) for f in files : if os . path . isfile ( f ) : _ , filename = os . path . split ( f ) copyfile ( f , os . path . join ( path_to_temp , filename ) ) copystat ( f , os . path . join ( path_to_temp , filename ) ) elif os . path . isdir ( f ) : destination_folder = os . path . join ( path_to_temp , f [ len ( src ) + 1 : ] ) copytree ( f , destination_folder ) path_to_zip_file = archive ( './' , path_to_dist , output_filename ) return path_to_zip_file
9078	def make_df_getter ( data_url : str , data_path : str , ** kwargs ) -> Callable [ [ Optional [ str ] , bool , bool ] , pd . DataFrame ] : download_function = make_downloader ( data_url , data_path ) def get_df ( url : Optional [ str ] = None , cache : bool = True , force_download : bool = False ) -> pd . DataFrame : if url is None and cache : url = download_function ( force_download = force_download ) return pd . read_csv ( url or data_url , ** kwargs ) return get_df
7015	def concatenate_textlcs_for_objectid ( lcbasedir , objectid , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , recursive = True ) : LOGINFO ( 'looking for light curves for %s, aperture %s in directory: %s' % ( objectid , aperture , lcbasedir ) ) if recursive is False : matching = glob . glob ( os . path . join ( lcbasedir , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) ) else : if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( lcbasedir , '**' , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) , recursive = True ) LOGINFO ( 'found %s files: %s' % ( len ( matching ) , repr ( matching ) ) ) else : walker = os . walk ( lcbasedir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) LOGINFO ( 'found %s in dir: %s' % ( repr ( foundfiles ) , os . path . join ( root , sdir ) ) ) if matching and len ( matching ) > 0 : clcdict = concatenate_textlcs ( matching , sortby = sortby , normalize = normalize ) return clcdict else : LOGERROR ( 'did not find any light curves for %s and aperture %s' % ( objectid , aperture ) ) return None
6346	def _language ( self , name , name_mode ) : name = name . strip ( ) . lower ( ) rules = BMDATA [ name_mode ] [ 'language_rules' ] all_langs = ( sum ( _LANG_DICT [ _ ] for _ in BMDATA [ name_mode ] [ 'languages' ] ) - 1 ) choices_remaining = all_langs for rule in rules : letters , languages , accept = rule if search ( letters , name ) is not None : if accept : choices_remaining &= languages else : choices_remaining &= ( ~ languages ) % ( all_langs + 1 ) if choices_remaining == L_NONE : choices_remaining = L_ANY return choices_remaining
5515	def setlocale ( name ) : with LOCALE_LOCK : old_locale = locale . setlocale ( locale . LC_ALL ) try : yield locale . setlocale ( locale . LC_ALL , name ) finally : locale . setlocale ( locale . LC_ALL , old_locale )
180	def to_bounding_box ( self ) : from . bbs import BoundingBox if len ( self . coords ) == 0 : return None return BoundingBox ( x1 = np . min ( self . xx ) , y1 = np . min ( self . yy ) , x2 = np . max ( self . xx ) , y2 = np . max ( self . yy ) , label = self . label )
10817	def can_see_members ( self , user ) : if self . privacy_policy == PrivacyPolicy . PUBLIC : return True elif self . privacy_policy == PrivacyPolicy . MEMBERS : return self . is_member ( user ) or self . is_admin ( user ) elif self . privacy_policy == PrivacyPolicy . ADMINS : return self . is_admin ( user )
5175	def facts ( self , ** kwargs ) : return self . __api . facts ( query = EqualsOperator ( "certname" , self . name ) , ** kwargs )
8144	def flip ( self , axis = HORIZONTAL ) : if axis == HORIZONTAL : self . img = self . img . transpose ( Image . FLIP_LEFT_RIGHT ) if axis == VERTICAL : self . img = self . img . transpose ( Image . FLIP_TOP_BOTTOM )
3662	def calculate_integral ( self , T1 , T2 , method ) : r if method == ZABRANSKY_SPLINE : return self . Zabransky_spline . calculate_integral ( T1 , T2 ) elif method == ZABRANSKY_SPLINE_C : return self . Zabransky_spline_iso . calculate_integral ( T1 , T2 ) elif method == ZABRANSKY_SPLINE_SAT : return self . Zabransky_spline_sat . calculate_integral ( T1 , T2 ) elif method == ZABRANSKY_QUASIPOLYNOMIAL : return self . Zabransky_quasipolynomial . calculate_integral ( T1 , T2 ) elif method == ZABRANSKY_QUASIPOLYNOMIAL_C : return self . Zabransky_quasipolynomial_iso . calculate_integral ( T1 , T2 ) elif method == ZABRANSKY_QUASIPOLYNOMIAL_SAT : return self . Zabransky_quasipolynomial_sat . calculate_integral ( T1 , T2 ) elif method == POLING_CONST : return ( T2 - T1 ) * self . POLING_constant elif method == CRCSTD : return ( T2 - T1 ) * self . CRCSTD_constant elif method == DADGOSTAR_SHAW : dH = ( Dadgostar_Shaw_integral ( T2 , self . similarity_variable ) - Dadgostar_Shaw_integral ( T1 , self . similarity_variable ) ) return property_mass_to_molar ( dH , self . MW ) elif method in self . tabular_data or method == COOLPROP or method in [ ROWLINSON_POLING , ROWLINSON_BONDI ] : return float ( quad ( self . calculate , T1 , T2 , args = ( method ) ) [ 0 ] ) else : raise Exception ( 'Method not valid' )
4816	def create_n_gram_df ( df , n_pad ) : n_pad_2 = int ( ( n_pad - 1 ) / 2 ) for i in range ( n_pad_2 ) : df [ 'char-{}' . format ( i + 1 ) ] = df [ 'char' ] . shift ( i + 1 ) df [ 'type-{}' . format ( i + 1 ) ] = df [ 'type' ] . shift ( i + 1 ) df [ 'char{}' . format ( i + 1 ) ] = df [ 'char' ] . shift ( - i - 1 ) df [ 'type{}' . format ( i + 1 ) ] = df [ 'type' ] . shift ( - i - 1 ) return df [ n_pad_2 : - n_pad_2 ]
4940	def link_user ( self , enterprise_customer , user_email ) : try : existing_user = User . objects . get ( email = user_email ) self . get_or_create ( enterprise_customer = enterprise_customer , user_id = existing_user . id ) except User . DoesNotExist : PendingEnterpriseCustomerUser . objects . get_or_create ( enterprise_customer = enterprise_customer , user_email = user_email )
798	def modelsClearAll ( self ) : self . _logger . info ( 'Deleting all rows from models table %r' , self . modelsTableName ) with ConnectionFactory . get ( ) as conn : query = 'DELETE FROM %s' % ( self . modelsTableName ) conn . cursor . execute ( query )
10018	def application_exists ( self ) : response = self . ebs . describe_applications ( application_names = [ self . app_name ] ) return len ( response [ 'DescribeApplicationsResponse' ] [ 'DescribeApplicationsResult' ] [ 'Applications' ] ) > 0
2379	def _get_rules ( self , cls ) : result = [ ] for rule_class in cls . __subclasses__ ( ) : rule_name = rule_class . __name__ . lower ( ) if rule_name not in self . _rules : rule = rule_class ( self ) self . _rules [ rule_name ] = rule result . append ( self . _rules [ rule_name ] ) return result
4439	async def _playnow ( self , ctx , * , query : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue and not player . is_playing : return await ctx . invoke ( self . _play , query = query ) query = query . strip ( '<>' ) if not url_rx . match ( query ) : query = f'ytsearch:{query}' results = await self . bot . lavalink . get_tracks ( query ) if not results or not results [ 'tracks' ] : return await ctx . send ( 'Nothing found!' ) tracks = results [ 'tracks' ] track = tracks . pop ( 0 ) if results [ 'loadType' ] == 'PLAYLIST_LOADED' : for _track in tracks : player . add ( requester = ctx . author . id , track = _track ) await player . play_now ( requester = ctx . author . id , track = track )
8006	def handle_read ( self ) : with self . _lock : logger . debug ( "handle_read()" ) if self . _socket is None : return while True : try : sock , address = self . _socket . accept ( ) except socket . error , err : if err . args [ 0 ] in BLOCKING_ERRORS : break else : raise logger . debug ( "Accepted connection from: {0!r}" . format ( address ) ) self . _target ( sock , address )
2655	def isdir ( self , path ) : result = True try : self . sftp_client . lstat ( path ) except FileNotFoundError : result = False return result
10225	def get_correlation_graph ( graph : BELGraph ) -> Graph : result = Graph ( ) for u , v , d in graph . edges ( data = True ) : if d [ RELATION ] not in CORRELATIVE_RELATIONS : continue if not result . has_edge ( u , v ) : result . add_edge ( u , v , ** { d [ RELATION ] : True } ) elif d [ RELATION ] not in result [ u ] [ v ] : log . log ( 5 , 'broken correlation relation for %s, %s' , u , v ) result [ u ] [ v ] [ d [ RELATION ] ] = True result [ v ] [ u ] [ d [ RELATION ] ] = True return result
5829	def check_for_rate_limiting ( response , response_lambda , timeout = 1 , attempts = 0 ) : if attempts >= 3 : raise RateLimitingException ( ) if response . status_code == 429 : sleep ( timeout ) new_timeout = timeout + 1 new_attempts = attempts + 1 return check_for_rate_limiting ( response_lambda ( timeout , attempts ) , response_lambda , timeout = new_timeout , attempts = new_attempts ) return response
11917	def render ( template , ** data ) : try : return renderer . render ( template , ** data ) except JinjaTemplateNotFound as e : logger . error ( e . __doc__ + ', Template: %r' % template ) sys . exit ( e . exit_code )
6203	def em_rates_from_E_DA_mix ( em_rates_tot , E_values ) : em_rates_d , em_rates_a = [ ] , [ ] for em_rate_tot , E_value in zip ( em_rates_tot , E_values ) : em_rate_di , em_rate_ai = em_rates_from_E_DA ( em_rate_tot , E_value ) em_rates_d . append ( em_rate_di ) em_rates_a . append ( em_rate_ai ) return em_rates_d , em_rates_a
577	def rUpdate ( original , updates ) : dictPairs = [ ( original , updates ) ] while len ( dictPairs ) > 0 : original , updates = dictPairs . pop ( ) for k , v in updates . iteritems ( ) : if k in original and isinstance ( original [ k ] , dict ) and isinstance ( v , dict ) : dictPairs . append ( ( original [ k ] , v ) ) else : original [ k ] = v
1780	def AAD ( cpu , imm = None ) : if imm is None : imm = 10 else : imm = imm . read ( ) cpu . AL += cpu . AH * imm cpu . AH = 0 cpu . _calculate_logic_flags ( 8 , cpu . AL )
6979	def filter_kepler_lcdict ( lcdict , filterflags = True , nanfilter = 'sap,pdc' , timestoignore = None ) : cols = lcdict [ 'columns' ] if filterflags : nbefore = lcdict [ 'time' ] . size filterind = lcdict [ 'sap_quality' ] == 0 for col in cols : if '.' in col : key , subkey = col . split ( '.' ) lcdict [ key ] [ subkey ] = lcdict [ key ] [ subkey ] [ filterind ] else : lcdict [ col ] = lcdict [ col ] [ filterind ] nafter = lcdict [ 'time' ] . size LOGINFO ( 'applied quality flag filter, ndet before = %s, ndet after = %s' % ( nbefore , nafter ) ) if nanfilter and nanfilter == 'sap,pdc' : notnanind = ( npisfinite ( lcdict [ 'sap' ] [ 'sap_flux' ] ) & npisfinite ( lcdict [ 'pdc' ] [ 'pdcsap_flux' ] ) & npisfinite ( lcdict [ 'time' ] ) ) elif nanfilter and nanfilter == 'sap' : notnanind = ( npisfinite ( lcdict [ 'sap' ] [ 'sap_flux' ] ) & npisfinite ( lcdict [ 'time' ] ) ) elif nanfilter and nanfilter == 'pdc' : notnanind = ( npisfinite ( lcdict [ 'pdc' ] [ 'pdcsap_flux' ] ) & npisfinite ( lcdict [ 'time' ] ) ) if nanfilter : nbefore = lcdict [ 'time' ] . size for col in cols : if '.' in col : key , subkey = col . split ( '.' ) lcdict [ key ] [ subkey ] = lcdict [ key ] [ subkey ] [ notnanind ] else : lcdict [ col ] = lcdict [ col ] [ notnanind ] nafter = lcdict [ 'time' ] . size LOGINFO ( 'removed nans, ndet before = %s, ndet after = %s' % ( nbefore , nafter ) ) if ( timestoignore and isinstance ( timestoignore , list ) and len ( timestoignore ) > 0 ) : exclind = npfull_like ( lcdict [ 'time' ] , True , dtype = np . bool_ ) nbefore = exclind . size for ignoretime in timestoignore : time0 , time1 = ignoretime [ 0 ] , ignoretime [ 1 ] thismask = ~ ( ( lcdict [ 'time' ] >= time0 ) & ( lcdict [ 'time' ] <= time1 ) ) exclind = exclind & thismask for col in cols : if '.' in col : key , subkey = col . split ( '.' ) lcdict [ key ] [ subkey ] = lcdict [ key ] [ subkey ] [ exclind ] else : lcdict [ col ] = lcdict [ col ] [ exclind ] nafter = lcdict [ 'time' ] . size LOGINFO ( 'removed timestoignore, ndet before = %s, ndet after = %s' % ( nbefore , nafter ) ) return lcdict
4969	def get_catalog_options ( self ) : if hasattr ( self . instance , 'site' ) : catalog_api = CourseCatalogApiClient ( self . user , self . instance . site ) else : catalog_api = CourseCatalogApiClient ( self . user ) catalogs = catalog_api . get_all_catalogs ( ) catalogs = sorted ( catalogs , key = lambda catalog : catalog . get ( 'name' , '' ) . lower ( ) ) return BLANK_CHOICE_DASH + [ ( catalog [ 'id' ] , catalog [ 'name' ] , ) for catalog in catalogs ]
7430	def _resolveambig ( subseq ) : N = [ ] for col in subseq : rand = np . random . binomial ( 1 , 0.5 ) N . append ( [ _AMBIGS [ i ] [ rand ] for i in col ] ) return np . array ( N )
11302	def provider_for_url ( self , url ) : for provider , regex in self . get_registry ( ) . items ( ) : if re . match ( regex , url ) is not None : return provider raise OEmbedMissingEndpoint ( 'No endpoint matches URL: %s' % url )
3928	def _replace_words ( replacements , string ) : output_lines = [ ] for line in string . split ( '\n' ) : output_words = [ ] for word in line . split ( ' ' ) : new_word = replacements . get ( word , word ) output_words . append ( new_word ) output_lines . append ( output_words ) return '\n' . join ( ' ' . join ( output_words ) for output_words in output_lines )
8124	def draw_cornu_flat ( x0 , y0 , t0 , t1 , s0 , c0 , flip , cs , ss , cmd ) : for j in range ( 0 , 100 ) : t = j * .01 s , c = eval_cornu ( t0 + t * ( t1 - t0 ) ) s *= flip s -= s0 c -= c0 x = c * cs - s * ss y = s * cs + c * ss print_pt ( x0 + x , y0 + y , cmd ) cmd = 'lineto' return cmd
10932	def get_termination_stats ( self , get_cos = True ) : delta_vals = self . _last_vals - self . param_vals delta_err = self . _last_error - self . error frac_err = delta_err / self . error to_return = { 'delta_vals' : delta_vals , 'delta_err' : delta_err , 'num_iter' : 1 * self . _num_iter , 'frac_err' : frac_err , 'error' : self . error , 'exp_err' : self . _exp_err } if get_cos : model_cosine = self . calc_model_cosine ( ) to_return . update ( { 'model_cosine' : model_cosine } ) return to_return
3115	def _get_flow_for_token ( csrf_token , request ) : flow_pickle = request . session . get ( _FLOW_KEY . format ( csrf_token ) , None ) return None if flow_pickle is None else jsonpickle . decode ( flow_pickle )
10797	def users ( ) : from invenio_groups . models import Group , Membership , PrivacyPolicy , SubscriptionPolicy admin = accounts . datastore . create_user ( email = 'admin@inveniosoftware.org' , password = encrypt_password ( '123456' ) , active = True , ) reader = accounts . datastore . create_user ( email = 'reader@inveniosoftware.org' , password = encrypt_password ( '123456' ) , active = True , ) admins = Group . create ( name = 'admins' , admins = [ admin ] ) for i in range ( 10 ) : Group . create ( name = 'group-{0}' . format ( i ) , admins = [ admin ] ) Membership . create ( admins , reader ) db . session . commit ( )
2801	def convert_reduce_sum ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting reduce_sum ...' ) keepdims = params [ 'keepdims' ] > 0 axis = params [ 'axes' ] def target_layer ( x , keepdims = keepdims , axis = axis ) : import keras . backend as K return K . sum ( x , keepdims = keepdims , axis = axis ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
6222	def _update_yaw_and_pitch ( self ) : front = Vector3 ( [ 0.0 , 0.0 , 0.0 ] ) front . x = cos ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) front . y = sin ( radians ( self . pitch ) ) front . z = sin ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) self . dir = vector . normalise ( front ) self . right = vector . normalise ( vector3 . cross ( self . dir , self . _up ) ) self . up = vector . normalise ( vector3 . cross ( self . right , self . dir ) )
1043	def generic_visit ( self , node ) : for field_name in node . _fields : setattr ( node , field_name , self . visit ( getattr ( node , field_name ) ) ) return node
380	def get_zca_whitening_principal_components_img ( X ) : flatX = np . reshape ( X , ( X . shape [ 0 ] , X . shape [ 1 ] * X . shape [ 2 ] * X . shape [ 3 ] ) ) tl . logging . info ( "zca : computing sigma .." ) sigma = np . dot ( flatX . T , flatX ) / flatX . shape [ 0 ] tl . logging . info ( "zca : computing U, S and V .." ) U , S , _ = linalg . svd ( sigma ) tl . logging . info ( "zca : computing principal components .." ) principal_components = np . dot ( np . dot ( U , np . diag ( 1. / np . sqrt ( S + 10e-7 ) ) ) , U . T ) return principal_components
8536	def pop_data ( self , nbytes ) : last_timestamp = 0 data = [ ] for packet in self . pop ( nbytes ) : last_timestamp = packet . timestamp data . append ( packet . data . data ) return '' . join ( data ) , last_timestamp
11708	def print_parents ( self ) : if self . gender == female : title = 'Daughter' elif self . gender == male : title = 'Son' else : title = 'Child' p1 = self . parents [ 0 ] p2 = self . parents [ 1 ] template = '%s of %s, the %s, and %s, the %s.' print ( template % ( title , p1 . name , p1 . epithet , p2 . name , p2 . epithet ) )
8512	def _create_kernel ( self ) : kernels = self . kernel_params if not isinstance ( kernels , list ) : raise RuntimeError ( 'Must provide enumeration of kernels' ) for kernel in kernels : if sorted ( list ( kernel . keys ( ) ) ) != [ 'name' , 'options' , 'params' ] : raise RuntimeError ( 'strategy/params/kernels must contain keys: "name", "options", "params"' ) kernels = [ ] for kern in self . kernel_params : params = kern [ 'params' ] options = kern [ 'options' ] name = kern [ 'name' ] kernel_ep = load_entry_point ( name , 'strategy/params/kernels' ) if issubclass ( kernel_ep , KERNEL_BASE_CLASS ) : if options [ 'independent' ] : kernel = np . sum ( [ kernel_ep ( 1 , active_dims = [ i ] , ** params ) for i in range ( self . n_dims ) ] ) else : kernel = kernel_ep ( self . n_dims , ** params ) if not isinstance ( kernel , KERNEL_BASE_CLASS ) : raise RuntimeError ( 'strategy/params/kernel must load a' 'GPy derived Kernel' ) kernels . append ( kernel ) self . kernel = np . sum ( kernels )
1424	def validate_state_locations ( self ) : names = map ( lambda loc : loc [ "name" ] , self . locations ) assert len ( names ) == len ( set ( names ) ) , "Names of state locations must be unique"
4965	def clean_notify ( self ) : return self . cleaned_data . get ( self . Fields . NOTIFY , self . NotificationTypes . DEFAULT )
12206	def raise_for_status ( response ) : for err_name in web_exceptions . __all__ : err = getattr ( web_exceptions , err_name ) if err . status_code == response . status : payload = dict ( headers = response . headers , reason = response . reason , ) if issubclass ( err , web_exceptions . _HTTPMove ) : raise err ( response . headers [ 'Location' ] , ** payload ) raise err ( ** payload )
953	def closenessScores ( self , expValues , actValues , ** kwargs ) : ratio = 1.0 esum = int ( expValues . sum ( ) ) asum = int ( actValues . sum ( ) ) if asum > esum : diff = asum - esum if diff < esum : ratio = 1 - diff / float ( esum ) else : ratio = 1 / float ( diff ) olap = expValues & actValues osum = int ( olap . sum ( ) ) if esum == 0 : r = 0.0 else : r = osum / float ( esum ) r = r * ratio return numpy . array ( [ r ] )
3356	def extend ( self , iterable ) : if not hasattr ( self , "_dict" ) or self . _dict is None : self . _dict = { } _dict = self . _dict current_length = len ( self ) list . extend ( self , iterable ) for i , obj in enumerate ( islice ( self , current_length , None ) , current_length ) : the_id = obj . id if the_id not in _dict : _dict [ the_id ] = i else : self = self [ : current_length ] self . _check ( the_id ) raise ValueError ( "id '%s' at index %d is non-unique. " "Is it present twice?" % ( str ( the_id ) , i ) )
2759	def get_all_load_balancers ( self ) : data = self . get_data ( "load_balancers" ) load_balancers = list ( ) for jsoned in data [ 'load_balancers' ] : load_balancer = LoadBalancer ( ** jsoned ) load_balancer . token = self . token load_balancer . health_check = HealthCheck ( ** jsoned [ 'health_check' ] ) load_balancer . sticky_sessions = StickySesions ( ** jsoned [ 'sticky_sessions' ] ) forwarding_rules = list ( ) for rule in jsoned [ 'forwarding_rules' ] : forwarding_rules . append ( ForwardingRule ( ** rule ) ) load_balancer . forwarding_rules = forwarding_rules load_balancers . append ( load_balancer ) return load_balancers
5451	def convert_to_label_chars ( s ) : accepted_characters = string . ascii_lowercase + string . digits + '-' def label_char_transform ( char ) : if char in accepted_characters : return char if char in string . ascii_uppercase : return char . lower ( ) return '-' return '' . join ( label_char_transform ( c ) for c in s )
4452	def aggregate ( self , query ) : if isinstance ( query , AggregateRequest ) : has_schema = query . _with_schema has_cursor = bool ( query . _cursor ) cmd = [ self . AGGREGATE_CMD , self . index_name ] + query . build_args ( ) elif isinstance ( query , Cursor ) : has_schema = False has_cursor = True cmd = [ self . CURSOR_CMD , 'READ' , self . index_name ] + query . build_args ( ) else : raise ValueError ( 'Bad query' , query ) raw = self . redis . execute_command ( * cmd ) if has_cursor : if isinstance ( query , Cursor ) : query . cid = raw [ 1 ] cursor = query else : cursor = Cursor ( raw [ 1 ] ) raw = raw [ 0 ] else : cursor = None if query . _with_schema : schema = raw [ 0 ] rows = raw [ 2 : ] else : schema = None rows = raw [ 1 : ] res = AggregateResult ( rows , cursor , schema ) return res
3696	def Watson ( T , Hvap_ref , T_Ref , Tc , exponent = 0.38 ) : Tr = T / Tc Trefr = T_Ref / Tc H2 = Hvap_ref * ( ( 1 - Tr ) / ( 1 - Trefr ) ) ** exponent return H2
13080	def register ( self ) : if self . app is not None : if not self . blueprint : self . blueprint = self . create_blueprint ( ) self . app . register_blueprint ( self . blueprint ) if self . cache is None : setattr ( self . app . jinja_env , "_fake_cache_extension" , self ) self . app . jinja_env . add_extension ( FakeCacheExtension ) return self . blueprint return None
9838	def __gridpositions ( self ) : try : tok = self . __consume ( ) except DXParserNoTokens : return if tok . equals ( 'counts' ) : shape = [ ] try : while True : self . __peek ( ) . value ( 'INTEGER' ) tok = self . __consume ( ) shape . append ( tok . value ( 'INTEGER' ) ) except ( DXParserNoTokens , ValueError ) : pass if len ( shape ) == 0 : raise DXParseError ( 'gridpositions: no shape parameters' ) self . currentobject [ 'shape' ] = shape elif tok . equals ( 'origin' ) : origin = [ ] try : while ( self . __peek ( ) . iscode ( 'INTEGER' ) or self . __peek ( ) . iscode ( 'REAL' ) ) : tok = self . __consume ( ) origin . append ( tok . value ( ) ) except DXParserNoTokens : pass if len ( origin ) == 0 : raise DXParseError ( 'gridpositions: no origin parameters' ) self . currentobject [ 'origin' ] = origin elif tok . equals ( 'delta' ) : d = [ ] try : while ( self . __peek ( ) . iscode ( 'INTEGER' ) or self . __peek ( ) . iscode ( 'REAL' ) ) : tok = self . __consume ( ) d . append ( tok . value ( ) ) except DXParserNoTokens : pass if len ( d ) == 0 : raise DXParseError ( 'gridpositions: missing delta parameters' ) try : self . currentobject [ 'delta' ] . append ( d ) except KeyError : self . currentobject [ 'delta' ] = [ d ] else : raise DXParseError ( 'gridpositions: ' + str ( tok ) + ' not recognized.' )
13426	def update_message ( self , message ) : url = "/2/messages/%s" % message . message_id data = self . _put_resource ( url , message . json_data ( ) ) return self . message_from_json ( data )
3352	def query ( self , search_function , attribute = None ) : def select_attribute ( x ) : if attribute is None : return x else : return getattr ( x , attribute ) try : regex_searcher = re . compile ( search_function ) if attribute is not None : matches = ( i for i in self if regex_searcher . findall ( select_attribute ( i ) ) != [ ] ) else : matches = ( i for i in self if regex_searcher . findall ( getattr ( i , 'id' ) ) != [ ] ) except TypeError : matches = ( i for i in self if search_function ( select_attribute ( i ) ) ) results = self . __class__ ( ) results . _extend_nocheck ( matches ) return results
8656	def get_threads ( session , query ) : response = make_get_request ( session , 'threads' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ThreadsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
6221	def set_position ( self , x , y , z ) : self . position = Vector3 ( [ x , y , z ] )
1058	def remove_extension ( module , name , code ) : key = ( module , name ) if ( _extension_registry . get ( key ) != code or _inverted_registry . get ( code ) != key ) : raise ValueError ( "key %s is not registered with code %s" % ( key , code ) ) del _extension_registry [ key ] del _inverted_registry [ code ] if code in _extension_cache : del _extension_cache [ code ]
9571	def discovery_view ( self , message ) : for handler in self . registered_handlers : if handler . check ( message ) : return handler . view return None
6594	def run_multiple ( self , eventLoops ) : self . nruns += len ( eventLoops ) return self . communicationChannel . put_multiple ( eventLoops )
3942	async def _longpoll_request ( self ) : params = { 'VER' : 8 , 'gsessionid' : self . _gsessionid_param , 'RID' : 'rpc' , 't' : 1 , 'SID' : self . _sid_param , 'CI' : 0 , 'ctype' : 'hangouts' , 'TYPE' : 'xmlhttp' , } logger . info ( 'Opening new long-polling request' ) try : async with self . _session . fetch_raw ( 'GET' , CHANNEL_URL , params = params ) as res : if res . status != 200 : if res . status == 400 and res . reason == 'Unknown SID' : raise ChannelSessionError ( 'SID became invalid' ) raise exceptions . NetworkError ( 'Request return unexpected status: {}: {}' . format ( res . status , res . reason ) ) while True : async with async_timeout . timeout ( PUSH_TIMEOUT ) : chunk = await res . content . read ( MAX_READ_BYTES ) if not chunk : break await self . _on_push_data ( chunk ) except asyncio . TimeoutError : raise exceptions . NetworkError ( 'Request timed out' ) except aiohttp . ServerDisconnectedError as err : raise exceptions . NetworkError ( 'Server disconnected error: %s' % err ) except aiohttp . ClientPayloadError : raise ChannelSessionError ( 'SID is about to expire' ) except aiohttp . ClientError as err : raise exceptions . NetworkError ( 'Request connection error: %s' % err )
6815	def enable_mods ( self ) : r = self . local_renderer for mod_name in r . env . mods_enabled : with self . settings ( warn_only = True ) : self . enable_mod ( mod_name )
7337	def predict_subsequences ( self , sequence_dict , peptide_lengths = None ) : sequence_dict = check_sequence_dictionary ( sequence_dict ) peptide_lengths = self . _check_peptide_lengths ( peptide_lengths ) binding_predictions = [ ] expected_peptides = set ( [ ] ) normalized_alleles = [ ] for key , amino_acid_sequence in sequence_dict . items ( ) : for l in peptide_lengths : for i in range ( len ( amino_acid_sequence ) - l + 1 ) : expected_peptides . add ( amino_acid_sequence [ i : i + l ] ) self . _check_peptide_inputs ( expected_peptides ) for allele in self . alleles : allele = normalize_allele_name ( allele , omit_dra1 = True ) normalized_alleles . append ( allele ) request = self . _get_iedb_request_params ( amino_acid_sequence , allele ) logger . info ( "Calling IEDB (%s) with request %s" , self . url , request ) response_df = _query_iedb ( request , self . url ) for _ , row in response_df . iterrows ( ) : binding_predictions . append ( BindingPrediction ( source_sequence_name = key , offset = row [ 'start' ] - 1 , allele = row [ 'allele' ] , peptide = row [ 'peptide' ] , affinity = row [ 'ic50' ] , percentile_rank = row [ 'rank' ] , prediction_method_name = "iedb-" + self . prediction_method ) ) self . _check_results ( binding_predictions , alleles = normalized_alleles , peptides = expected_peptides ) return BindingPredictionCollection ( binding_predictions )
9548	def add_unique_check ( self , key , code = UNIQUE_CHECK_FAILED , message = MESSAGES [ UNIQUE_CHECK_FAILED ] ) : if isinstance ( key , basestring ) : assert key in self . _field_names , 'unexpected field name: %s' % key else : for f in key : assert f in self . _field_names , 'unexpected field name: %s' % key t = key , code , message self . _unique_checks . append ( t )
4728	def gen_to_dev ( self , address ) : cmd = [ "nvm_addr gen2dev" , self . envs [ "DEV_PATH" ] , "0x{:x}" . format ( address ) ] status , stdout , _ = cij . ssh . command ( cmd , shell = True ) if status : raise RuntimeError ( "cij.liblight.gen_to_dev: cmd fail" ) return int ( re . findall ( r"dev: ([0-9a-fx]+)" , stdout ) [ 0 ] , 16 )
662	def checkMatch ( input , prediction , sparse = True , verbosity = 0 ) : if sparse : activeElementsInInput = set ( input ) activeElementsInPrediction = set ( prediction ) else : activeElementsInInput = set ( input . nonzero ( ) [ 0 ] ) activeElementsInPrediction = set ( prediction . nonzero ( ) [ 0 ] ) totalActiveInPrediction = len ( activeElementsInPrediction ) totalActiveInInput = len ( activeElementsInInput ) foundInInput = len ( activeElementsInPrediction . intersection ( activeElementsInInput ) ) missingFromInput = len ( activeElementsInPrediction . difference ( activeElementsInInput ) ) missingFromPrediction = len ( activeElementsInInput . difference ( activeElementsInPrediction ) ) if verbosity >= 1 : print "preds. found in input:" , foundInInput , "out of" , totalActiveInPrediction , print "; preds. missing from input:" , missingFromInput , "out of" , totalActiveInPrediction , print "; unexpected active in input:" , missingFromPrediction , "out of" , totalActiveInInput return ( foundInInput , totalActiveInInput , missingFromInput , totalActiveInPrediction )
4294	def supported_versions ( django , cms ) : cms_version = None django_version = None try : cms_version = Decimal ( cms ) except ( ValueError , InvalidOperation ) : try : cms_version = CMS_VERSION_MATRIX [ str ( cms ) ] except KeyError : pass try : django_version = Decimal ( django ) except ( ValueError , InvalidOperation ) : try : django_version = DJANGO_VERSION_MATRIX [ str ( django ) ] except KeyError : pass try : if ( cms_version and django_version and not ( LooseVersion ( VERSION_MATRIX [ compat . unicode ( cms_version ) ] [ 0 ] ) <= LooseVersion ( compat . unicode ( django_version ) ) <= LooseVersion ( VERSION_MATRIX [ compat . unicode ( cms_version ) ] [ 1 ] ) ) ) : raise RuntimeError ( 'Django and django CMS versions doesn\'t match: ' 'Django {0} is not supported by django CMS {1}' . format ( django_version , cms_version ) ) except KeyError : raise RuntimeError ( 'Django and django CMS versions doesn\'t match: ' 'Django {0} is not supported by django CMS {1}' . format ( django_version , cms_version ) ) return ( compat . unicode ( django_version ) if django_version else django_version , compat . unicode ( cms_version ) if cms_version else cms_version )
657	def averageOnTime ( vectors , numSamples = None ) : if vectors . ndim == 1 : vectors . shape = ( - 1 , 1 ) numTimeSteps = len ( vectors ) numElements = len ( vectors [ 0 ] ) if numSamples is None : numSamples = numElements countOn = range ( numElements ) else : countOn = numpy . random . randint ( 0 , numElements , numSamples ) sumOfLengths = 0.0 onTimeFreqCounts = None n = 0 for i in countOn : ( onTime , segments , durations ) = _listOfOnTimesInVec ( vectors [ : , i ] ) if onTime != 0.0 : sumOfLengths += onTime n += segments onTimeFreqCounts = _accumulateFrequencyCounts ( durations , onTimeFreqCounts ) if n > 0 : return ( sumOfLengths / n , onTimeFreqCounts ) else : return ( 0.0 , onTimeFreqCounts )
2270	def _win32_symlink2 ( path , link , allow_fallback = True , verbose = 0 ) : if _win32_can_symlink ( ) : return _win32_symlink ( path , link , verbose ) else : return _win32_junction ( path , link , verbose )
5364	def stderr ( self ) : if self . _streaming : stderr = [ ] while not self . __stderr . empty ( ) : try : line = self . __stderr . get_nowait ( ) stderr . append ( line ) except : pass else : stderr = self . __stderr return stderr
9682	def set_fan_power ( self , power ) : if power > 255 : raise ValueError ( "The fan power should be a single byte (0-255)." ) a = self . cnxn . xfer ( [ 0x42 ] ) [ 0 ] sleep ( 10e-3 ) b = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] c = self . cnxn . xfer ( [ power ] ) [ 0 ] sleep ( 0.1 ) return True if a == 0xF3 and b == 0x42 and c == 0x00 else False
720	def getOptimizationMetricInfo ( cls , searchJobParams ) : if searchJobParams [ "hsVersion" ] == "v2" : search = HypersearchV2 ( searchParams = searchJobParams ) else : raise RuntimeError ( "Unsupported hypersearch version \"%s\"" % ( searchJobParams [ "hsVersion" ] ) ) info = search . getOptimizationMetricInfo ( ) return info
11442	def _warning ( code ) : if isinstance ( code , str ) : return code message = '' if isinstance ( code , tuple ) : if isinstance ( code [ 0 ] , str ) : message = code [ 1 ] code = code [ 0 ] return CFG_BIBRECORD_WARNING_MSGS . get ( code , '' ) + message
12858	def from_date ( datetime_date ) : return BusinessDate . from_ymd ( datetime_date . year , datetime_date . month , datetime_date . day )
13540	def chisq_red ( self ) : if self . _chisq_red is None : self . _chisq_red = chisquare ( self . y_unweighted . transpose ( ) , _np . dot ( self . X_unweighted , self . beta ) , self . y_error , ddof = 3 , verbose = False ) return self . _chisq_red
10342	def overlay_data ( graph : BELGraph , data : Mapping [ BaseEntity , Any ] , label : Optional [ str ] = None , overwrite : bool = False , ) -> None : if label is None : label = WEIGHT for node , value in data . items ( ) : if node not in graph : log . debug ( '%s not in graph' , node ) continue if label in graph . nodes [ node ] and not overwrite : log . debug ( '%s already on %s' , label , node ) continue graph . nodes [ node ] [ label ] = value
7519	def write_snps_map ( data ) : start = time . time ( ) tmparrs = os . path . join ( data . dirs . outfiles , "tmp-{}.h5" . format ( data . name ) ) with h5py . File ( tmparrs , 'r' ) as io5 : maparr = io5 [ "maparr" ] [ : ] end = np . where ( np . all ( maparr [ : ] == 0 , axis = 1 ) ) [ 0 ] if np . any ( end ) : end = end . min ( ) else : end = maparr . shape [ 0 ] outchunk = [ ] with open ( data . outfiles . snpsmap , 'w' ) as out : for idx in xrange ( end ) : line = maparr [ idx , : ] outchunk . append ( "{}\trad{}_snp{}\t{}\t{}\n" . format ( line [ 0 ] , line [ 1 ] , line [ 2 ] , 0 , line [ 3 ] ) ) if not idx % 10000 : out . write ( "" . join ( outchunk ) ) outchunk = [ ] out . write ( "" . join ( outchunk ) ) LOGGER . debug ( "finished writing snps_map in: %s" , time . time ( ) - start )
8780	def delete_locks ( context , network_ids , addresses ) : addresses_no_longer_null_routed = _find_addresses_to_be_unlocked ( context , network_ids , addresses ) LOG . info ( "Deleting %s lock holders on IPAddress with ids: %s" , len ( addresses_no_longer_null_routed ) , [ addr . id for addr in addresses_no_longer_null_routed ] ) for address in addresses_no_longer_null_routed : lock_holder = None try : lock_holder = db_api . lock_holder_find ( context , lock_id = address . lock_id , name = LOCK_NAME , scope = db_api . ONE ) if lock_holder : db_api . lock_holder_delete ( context , address , lock_holder ) except Exception : LOG . exception ( "Failed to delete lock holder %s" , lock_holder ) continue context . session . flush ( )
10702	def get_modes ( _id ) : url = MODES_URL % _id arequest = requests . get ( url , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
12947	def saveToExternal ( self , redisCon ) : if type ( redisCon ) == dict : conn = redis . Redis ( ** redisCon ) elif hasattr ( conn , '__class__' ) and issubclass ( conn . __class__ , redis . Redis ) : conn = redisCon else : raise ValueError ( 'saveToExternal "redisCon" param must either be a dictionary of connection parameters, or redis.Redis, or extension thereof' ) saver = self . saver forceID = saver . _getNextID ( conn ) myCopy = self . copy ( False ) return saver . save ( myCopy , usePipeline = True , forceID = forceID , conn = conn )
13368	def is_git_repo ( path ) : if path . startswith ( 'git@' ) or path . startswith ( 'https://' ) : return True if os . path . exists ( unipath ( path , '.git' ) ) : return True return False
1267	def _fly ( self , board , layers , things , the_plot ) : if ( self . character in the_plot [ 'bunker_hitters' ] or self . character in the_plot [ 'marauder_hitters' ] ) : return self . _teleport ( ( - 1 , - 1 ) ) self . _north ( board , the_plot )
6833	def version ( self ) : r = self . local_renderer with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : res = r . local ( 'vagrant --version' , capture = True ) if res . failed : return None line = res . splitlines ( ) [ - 1 ] version = re . match ( r'Vagrant (?:v(?:ersion )?)?(.*)' , line ) . group ( 1 ) return tuple ( _to_int ( part ) for part in version . split ( '.' ) )
13463	def add_event ( request ) : form = AddEventForm ( request . POST or None ) if form . is_valid ( ) : instance = form . save ( commit = False ) instance . sites = settings . SITE_ID instance . submitted_by = request . user instance . approved = True instance . slug = slugify ( instance . name ) instance . save ( ) messages . success ( request , 'Your event has been added.' ) return HttpResponseRedirect ( reverse ( 'events_index' ) ) return render ( request , 'happenings/event_form.html' , { 'form' : form , 'form_title' : 'Add an event' } )
1817	def SETNS ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . SF == False , 1 , 0 ) )
4489	def might_need_auth ( f ) : @ wraps ( f ) def wrapper ( cli_args ) : try : return_value = f ( cli_args ) except UnauthorizedException as e : config = config_from_env ( config_from_file ( ) ) username = _get_username ( cli_args , config ) if username is None : sys . exit ( "Please set a username (run `osf -h` for details)." ) else : sys . exit ( "You are not authorized to access this project." ) return return_value return wrapper
8684	def load ( self , origin_passphrase , keys = None , key_file = None ) : self . _assert_valid_stash ( ) if not ( bool ( keys ) ^ bool ( key_file ) ) : raise GhostError ( 'You must either provide a path to an exported stash file ' 'or a list of key dicts to import' ) if key_file : with open ( key_file ) as stash_file : keys = json . loads ( stash_file . read ( ) ) decrypt = origin_passphrase != self . passphrase if decrypt : stub = Stash ( TinyDBStorage ( 'stub' ) , origin_passphrase ) for key in keys : self . put ( name = key [ 'name' ] , value = stub . _decrypt ( key [ 'value' ] ) if decrypt else key [ 'value' ] , metadata = key [ 'metadata' ] , description = key [ 'description' ] , lock = key . get ( 'lock' ) , key_type = key . get ( 'type' ) , encrypt = decrypt )
4108	def chirp ( t , f0 = 0. , t1 = 1. , f1 = 100. , form = 'linear' , phase = 0 ) : r valid_forms = [ 'linear' , 'quadratic' , 'logarithmic' ] if form not in valid_forms : raise ValueError ( "Invalid form. Valid form are %s" % valid_forms ) t = numpy . array ( t ) phase = 2. * pi * phase / 360. if form == "linear" : a = pi * ( f1 - f0 ) / t1 b = 2. * pi * f0 y = numpy . cos ( a * t ** 2 + b * t + phase ) elif form == "quadratic" : a = ( 2 / 3. * pi * ( f1 - f0 ) / t1 / t1 ) b = 2. * pi * f0 y = numpy . cos ( a * t ** 3 + b * t + phase ) elif form == "logarithmic" : a = 2. * pi * t1 / numpy . log ( f1 - f0 ) b = 2. * pi * f0 x = ( f1 - f0 ) ** ( 1. / t1 ) y = numpy . cos ( a * x ** t + b * t + phase ) return y
1585	def send_buffered_messages ( self ) : while not self . out_stream . is_empty ( ) and self . _stmgr_client . is_registered : tuple_set = self . out_stream . poll ( ) if isinstance ( tuple_set , tuple_pb2 . HeronTupleSet ) : tuple_set . src_task_id = self . my_pplan_helper . my_task_id self . gateway_metrics . update_sent_packet ( tuple_set . ByteSize ( ) ) self . _stmgr_client . send_message ( tuple_set )
6382	def sim_hamming ( src , tar , diff_lens = True ) : return Hamming ( ) . sim ( src , tar , diff_lens )
4878	def validate ( self , data ) : lms_user_id = data . get ( 'lms_user_id' ) tpa_user_id = data . get ( 'tpa_user_id' ) user_email = data . get ( 'user_email' ) if not lms_user_id and not tpa_user_id and not user_email : raise serializers . ValidationError ( 'At least one of the following fields must be specified and map to an EnterpriseCustomerUser: ' 'lms_user_id, tpa_user_id, user_email' ) return data
7770	def base_handlers_factory ( self ) : tls_handler = StreamTLSHandler ( self . settings ) sasl_handler = StreamSASLHandler ( self . settings ) session_handler = SessionHandler ( ) binding_handler = ResourceBindingHandler ( self . settings ) return [ tls_handler , sasl_handler , binding_handler , session_handler ]
3692	def a_alpha_and_derivatives ( self , T , full = True , quick = True ) : r return TWU_a_alpha_common ( T , self . Tc , self . omega , self . a , full = full , quick = quick , method = 'SRK' )
3494	def total_yield ( input_fluxes , input_elements , output_flux , output_elements ) : carbon_input_flux = sum ( total_components_flux ( flux , components , consumption = True ) for flux , components in zip ( input_fluxes , input_elements ) ) carbon_output_flux = total_components_flux ( output_flux , output_elements , consumption = False ) try : return carbon_output_flux / carbon_input_flux except ZeroDivisionError : return nan
5851	def get_dataset_files ( self , dataset_id , glob = "." , is_dir = False , version_number = None ) : if version_number is None : latest = True else : latest = False data = { "download_request" : { "glob" : glob , "isDir" : is_dir , "latest" : latest } } failure_message = "Failed to get matched files in dataset {}" . format ( dataset_id ) versions = self . _get_success_json ( self . _post_json ( routes . matched_files ( dataset_id ) , data , failure_message = failure_message ) ) [ 'versions' ] if version_number is None : version = versions [ 0 ] else : try : version = list ( filter ( lambda v : v [ 'number' ] == version_number , versions ) ) [ 0 ] except IndexError : raise ResourceNotFoundException ( ) return list ( map ( lambda f : DatasetFile ( path = f [ 'filename' ] , url = f [ 'url' ] ) , version [ 'files' ] ) )
13342	def concatenate ( tup , axis = 0 ) : from distob import engine if len ( tup ) is 0 : raise ValueError ( 'need at least one array to concatenate' ) first = tup [ 0 ] others = tup [ 1 : ] if ( hasattr ( first , 'concatenate' ) and hasattr ( type ( first ) , '__array_interface__' ) ) : return first . concatenate ( others , axis ) arrays = [ ] for ar in tup : if isinstance ( ar , DistArray ) : if axis == ar . _distaxis : arrays . extend ( ar . _subarrays ) else : arrays . append ( gather ( ar ) ) elif isinstance ( ar , RemoteArray ) : arrays . append ( ar ) elif isinstance ( ar , Remote ) : arrays . append ( _remote_to_array ( ar ) ) elif hasattr ( type ( ar ) , '__array_interface__' ) : arrays . append ( ar ) else : arrays . append ( np . array ( ar ) ) if all ( isinstance ( ar , np . ndarray ) for ar in arrays ) : return np . concatenate ( arrays , axis ) total_length = 0 commonshape = list ( arrays [ 0 ] . shape ) commonshape [ axis ] = None for ar in arrays : total_length += ar . shape [ axis ] shp = list ( ar . shape ) shp [ axis ] = None if shp != commonshape : raise ValueError ( 'incompatible shapes for concatenation' ) blocksize = ( ( total_length - 1 ) // engine . nengines ) + 1 rarrays = [ ] for ar in arrays : if isinstance ( ar , DistArray ) : rarrays . extend ( ar . _subarrays ) elif isinstance ( ar , RemoteArray ) : rarrays . append ( ar ) else : da = _scatter_ndarray ( ar , axis , blocksize ) for ra in da . _subarrays : rarrays . append ( ra ) del da del arrays eid = rarrays [ 0 ] . _id . engine if all ( ra . _id . engine == eid for ra in rarrays ) : if eid == engine . eid : return concatenate ( [ gather ( r ) for r in rarrays ] , axis ) else : return call ( concatenate , rarrays , axis ) else : return DistArray ( rarrays , axis )
7109	def get_from_cache ( url : str , cache_dir : Path = None ) -> Path : cache_dir . mkdir ( parents = True , exist_ok = True ) filename = re . sub ( r'.+/' , '' , url ) cache_path = cache_dir / filename if cache_path . exists ( ) : return cache_path response = requests . head ( url ) if response . status_code != 200 : if "www.dropbox.com" in url : pass else : raise IOError ( "HEAD request failed for url {}" . format ( url ) ) if not cache_path . exists ( ) : fd , temp_filename = tempfile . mkstemp ( ) logger . info ( "%s not found in cache, downloading to %s" , url , temp_filename ) req = requests . get ( url , stream = True ) content_length = req . headers . get ( 'Content-Length' ) total = int ( content_length ) if content_length is not None else None progress = Tqdm . tqdm ( unit = "B" , total = total ) with open ( temp_filename , 'wb' ) as temp_file : for chunk in req . iter_content ( chunk_size = 1024 ) : if chunk : progress . update ( len ( chunk ) ) temp_file . write ( chunk ) progress . close ( ) logger . info ( "copying %s to cache at %s" , temp_filename , cache_path ) shutil . copyfile ( temp_filename , str ( cache_path ) ) logger . info ( "removing temp file %s" , temp_filename ) os . close ( fd ) os . remove ( temp_filename ) return cache_path
7926	def shuffle_srv ( records ) : if not records : return [ ] ret = [ ] while len ( records ) > 1 : weight_sum = 0 for rrecord in records : weight_sum += rrecord . weight + 0.1 thres = random . random ( ) * weight_sum weight_sum = 0 for rrecord in records : weight_sum += rrecord . weight + 0.1 if thres < weight_sum : records . remove ( rrecord ) ret . append ( rrecord ) break ret . append ( records [ 0 ] ) return ret
7030	def specwindow_lsp_value ( times , mags , errs , omega ) : norm_times = times - times . min ( ) tau = ( ( 1.0 / ( 2.0 * omega ) ) * nparctan ( npsum ( npsin ( 2.0 * omega * norm_times ) ) / npsum ( npcos ( 2.0 * omega * norm_times ) ) ) ) lspval_top_cos = ( npsum ( 1.0 * npcos ( omega * ( norm_times - tau ) ) ) * npsum ( 1.0 * npcos ( omega * ( norm_times - tau ) ) ) ) lspval_bot_cos = npsum ( ( npcos ( omega * ( norm_times - tau ) ) ) * ( npcos ( omega * ( norm_times - tau ) ) ) ) lspval_top_sin = ( npsum ( 1.0 * npsin ( omega * ( norm_times - tau ) ) ) * npsum ( 1.0 * npsin ( omega * ( norm_times - tau ) ) ) ) lspval_bot_sin = npsum ( ( npsin ( omega * ( norm_times - tau ) ) ) * ( npsin ( omega * ( norm_times - tau ) ) ) ) lspval = 0.5 * ( ( lspval_top_cos / lspval_bot_cos ) + ( lspval_top_sin / lspval_bot_sin ) ) return lspval
6393	def dist ( self , src , tar ) : if src == tar : return 0.0 src = src . encode ( 'utf-8' ) tar = tar . encode ( 'utf-8' ) if lzma is not None : src_comp = lzma . compress ( src ) [ 14 : ] tar_comp = lzma . compress ( tar ) [ 14 : ] concat_comp = lzma . compress ( src + tar ) [ 14 : ] concat_comp2 = lzma . compress ( tar + src ) [ 14 : ] else : raise ValueError ( 'Install the PylibLZMA module in order to use LZMA' ) return ( min ( len ( concat_comp ) , len ( concat_comp2 ) ) - min ( len ( src_comp ) , len ( tar_comp ) ) ) / max ( len ( src_comp ) , len ( tar_comp ) )
2624	def get_instance_state ( self , instances = None ) : if instances : desc = self . client . describe_instances ( InstanceIds = instances ) else : desc = self . client . describe_instances ( InstanceIds = self . instances ) for i in range ( len ( desc [ 'Reservations' ] ) ) : instance = desc [ 'Reservations' ] [ i ] [ 'Instances' ] [ 0 ] self . instance_states [ instance [ 'InstanceId' ] ] = instance [ 'State' ] [ 'Name' ] return self . instance_states
12962	def exists ( self , pk ) : conn = self . _get_connection ( ) key = self . _get_key_for_id ( pk ) return conn . exists ( key )
569	def _getReportItem ( itemName , results ) : subKeys = itemName . split ( ':' ) subResults = results for subKey in subKeys : subResults = subResults [ subKey ] return subResults
9634	def numeric ( _ , n ) : try : nt = n . as_tuple ( ) except AttributeError : raise TypeError ( 'numeric field requires Decimal value (got %r)' % n ) digits = [ ] if isinstance ( nt . exponent , str ) : ndigits = 0 weight = 0 sign = 0xC000 dscale = 0 else : decdigits = list ( reversed ( nt . digits + ( nt . exponent % 4 ) * ( 0 , ) ) ) weight = 0 while decdigits : if any ( decdigits [ : 4 ] ) : break weight += 1 del decdigits [ : 4 ] while decdigits : digits . insert ( 0 , ndig ( decdigits [ : 4 ] ) ) del decdigits [ : 4 ] ndigits = len ( digits ) weight += nt . exponent // 4 + ndigits - 1 sign = nt . sign * 0x4000 dscale = - min ( 0 , nt . exponent ) data = [ ndigits , weight , sign , dscale ] + digits return ( 'ihhHH%dH' % ndigits , [ 2 * len ( data ) ] + data )
10209	def check_write_permissions ( file ) : try : open ( file , 'a' ) except IOError : print ( "Can't open file {}. " "Please grant write permissions or change the path in your config" . format ( file ) ) sys . exit ( 1 )
6904	def hms_to_decimal ( hours , minutes , seconds , returndeg = True ) : if hours > 24 : return None else : dec_hours = fabs ( hours ) + fabs ( minutes ) / 60.0 + fabs ( seconds ) / 3600.0 if returndeg : dec_deg = dec_hours * 15.0 if dec_deg < 0 : dec_deg = dec_deg + 360.0 dec_deg = dec_deg % 360.0 return dec_deg else : return dec_hours
8295	def cliques ( graph , threshold = 3 ) : cliques = [ ] for n in graph . nodes : c = clique ( graph , n . id ) if len ( c ) >= threshold : c . sort ( ) if c not in cliques : cliques . append ( c ) return cliques
8322	def sanitize ( self , val ) : if self . type == NUMBER : try : return clamp ( self . min , self . max , float ( val ) ) except ValueError : return 0.0 elif self . type == TEXT : try : return unicode ( str ( val ) , "utf_8" , "replace" ) except : return "" elif self . type == BOOLEAN : if unicode ( val ) . lower ( ) in ( "true" , "1" , "yes" ) : return True else : return False
10764	def _random_token ( self , bits = 128 ) : alphabet = string . ascii_letters + string . digits + '-_' num_letters = int ( math . ceil ( bits / 6.0 ) ) return '' . join ( random . choice ( alphabet ) for i in range ( num_letters ) )
4700	def get_sizeof_descriptor_table ( version = "Denali" ) : if version == "Denali" : return sizeof ( DescriptorTableDenali ) elif version == "Spec20" : return sizeof ( DescriptorTableSpec20 ) elif version == "Spec12" : return 0 else : raise RuntimeError ( "Error version!" )
6248	def get_texture ( self , label : str ) -> Union [ moderngl . Texture , moderngl . TextureArray , moderngl . Texture3D , moderngl . TextureCube ] : return self . _project . get_texture ( label )
416	def find_top_dataset ( self , dataset_name = None , sort = None , ** kwargs ) : self . _fill_project_info ( kwargs ) if dataset_name is None : raise Exception ( "dataset_name is None, please give a dataset name" ) kwargs . update ( { 'dataset_name' : dataset_name } ) s = time . time ( ) d = self . db . Dataset . find_one ( filter = kwargs , sort = sort ) if d is not None : dataset_id = d [ 'dataset_id' ] else : print ( "[Database] FAIL! Cannot find dataset: {}" . format ( kwargs ) ) return False try : dataset = self . _deserialization ( self . dataset_fs . get ( dataset_id ) . read ( ) ) pc = self . db . Dataset . find ( kwargs ) print ( "[Database] Find one dataset SUCCESS, {} took: {}s" . format ( kwargs , round ( time . time ( ) - s , 2 ) ) ) dataset_id_list = pc . distinct ( 'dataset_id' ) n_dataset = len ( dataset_id_list ) if n_dataset != 1 : print ( " Note that there are {} datasets match the requirement" . format ( n_dataset ) ) return dataset except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) return False
11522	def add_condor_job ( self , token , batchmaketaskid , jobdefinitionfilename , outputfilename , errorfilename , logfilename , postfilename ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'batchmaketaskid' ] = batchmaketaskid parameters [ 'jobdefinitionfilename' ] = jobdefinitionfilename parameters [ 'outputfilename' ] = outputfilename parameters [ 'errorfilename' ] = errorfilename parameters [ 'logfilename' ] = logfilename parameters [ 'postfilename' ] = postfilename response = self . request ( 'midas.batchmake.add.condor.job' , parameters ) return response
4407	async def listen ( self ) : while not self . _shutdown : try : data = json . loads ( await self . _ws . recv ( ) ) except websockets . ConnectionClosed as error : log . warning ( 'Disconnected from Lavalink: {}' . format ( str ( error ) ) ) for g in self . _lavalink . players . _players . copy ( ) . keys ( ) : ws = self . _lavalink . bot . _connection . _get_websocket ( int ( g ) ) await ws . voice_state ( int ( g ) , None ) self . _lavalink . players . clear ( ) if self . _shutdown : break if await self . _attempt_reconnect ( ) : return log . warning ( 'Unable to reconnect to Lavalink!' ) break op = data . get ( 'op' , None ) log . debug ( 'Received WebSocket data {}' . format ( str ( data ) ) ) if not op : return log . debug ( 'Received WebSocket message without op {}' . format ( str ( data ) ) ) if op == 'event' : log . debug ( 'Received event of type {}' . format ( data [ 'type' ] ) ) player = self . _lavalink . players [ int ( data [ 'guildId' ] ) ] event = None if data [ 'type' ] == 'TrackEndEvent' : event = TrackEndEvent ( player , data [ 'track' ] , data [ 'reason' ] ) elif data [ 'type' ] == 'TrackExceptionEvent' : event = TrackExceptionEvent ( player , data [ 'track' ] , data [ 'error' ] ) elif data [ 'type' ] == 'TrackStuckEvent' : event = TrackStuckEvent ( player , data [ 'track' ] , data [ 'thresholdMs' ] ) if event : await self . _lavalink . dispatch_event ( event ) elif op == 'playerUpdate' : await self . _lavalink . update_state ( data ) elif op == 'stats' : self . _lavalink . stats . _update ( data ) await self . _lavalink . dispatch_event ( StatsUpdateEvent ( self . _lavalink . stats ) ) log . debug ( 'Closing WebSocket...' ) await self . _ws . close ( )
1046	def context ( self , * notes ) : self . _appended_notes += notes yield del self . _appended_notes [ - len ( notes ) : ]
4930	def transform_courserun_title ( self , content_metadata_item ) : title = content_metadata_item . get ( 'title' ) or '' course_run_start = content_metadata_item . get ( 'start' ) if course_run_start : if course_available_for_enrollment ( content_metadata_item ) : title += ' ({starts}: {:%B %Y})' . format ( parse_lms_api_datetime ( course_run_start ) , starts = _ ( 'Starts' ) ) else : title += ' ({:%B %Y} - {enrollment_closed})' . format ( parse_lms_api_datetime ( course_run_start ) , enrollment_closed = _ ( 'Enrollment Closed' ) ) title_with_locales = [ ] content_metadata_language_code = transform_language_code ( content_metadata_item . get ( 'content_language' , '' ) ) for locale in self . enterprise_configuration . get_locales ( default_locale = content_metadata_language_code ) : title_with_locales . append ( { 'locale' : locale , 'value' : title } ) return title_with_locales
5852	def get_dataset_file ( self , dataset_id , file_path , version = None ) : return self . get_dataset_files ( dataset_id , "^{}$" . format ( file_path ) , version_number = version ) [ 0 ]
8274	def colors ( self , n = 10 , d = 0.035 ) : s = sum ( [ w for clr , rng , w in self . ranges ] ) colors = colorlist ( ) for i in _range ( n ) : r = random ( ) for clr , rng , weight in self . ranges : if weight / s >= r : break r -= weight / s colors . append ( rng ( clr , d ) ) return colors
6258	def update ( self , aspect_ratio = None , fov = None , near = None , far = None ) : self . aspect_ratio = aspect_ratio or self . aspect_ratio self . fov = fov or self . fov self . near = near or self . near self . far = far or self . far self . matrix = Matrix44 . perspective_projection ( self . fov , self . aspect_ratio , self . near , self . far )
9387	def get_urls_from_seed ( url ) : if not url or type ( url ) != str or not naarad . utils . is_valid_url ( url ) : logger . error ( "get_urls_from_seed() does not have valid seeding url." ) return base_index = url . find ( '/' , len ( "https://" ) ) base_url = url [ : base_index ] urls = [ ] try : response = urllib2 . urlopen ( url ) hp = HTMLLinkExtractor ( ) hp . feed ( response . read ( ) ) urls = hp . links hp . close ( ) except urllib2 . HTTPError : logger . error ( "Got HTTPError when opening the url of %s" % url ) return urls for i in range ( len ( urls ) ) : if not urls [ i ] . startswith ( "http://" ) and not urls [ i ] . startswith ( "https://" ) : urls [ i ] = base_url + urls [ i ] return urls
4909	def _delete ( self , url , data , scope ) : self . _create_session ( scope ) response = self . session . delete ( url , data = data ) return response . status_code , response . text
10301	def count_defaultdict ( dict_of_lists : Mapping [ X , List [ Y ] ] ) -> Mapping [ X , typing . Counter [ Y ] ] : return { k : Counter ( v ) for k , v in dict_of_lists . items ( ) }
3166	def cancel ( self , campaign_id ) : self . campaign_id = campaign_id return self . _mc_client . _post ( url = self . _build_path ( campaign_id , 'actions/cancel-send' ) )
8913	def save_service ( self , service , overwrite = True ) : name = namesgenerator . get_sane_name ( service . name ) if not name : name = namesgenerator . get_random_name ( ) if name in self . name_index : name = namesgenerator . get_random_name ( retry = True ) if name in self . name_index : if overwrite : self . _delete ( name = name ) else : raise Exception ( "service name already registered." ) self . _insert ( Service ( name = name , url = baseurl ( service . url ) , type = service . type , purl = service . purl , public = service . public , auth = service . auth , verify = service . verify ) ) return self . fetch_by_name ( name = name )
6777	def iter_dict_differences ( a , b ) : common_keys = set ( a ) . union ( b ) for k in common_keys : a_value = a . get ( k ) b_value = b . get ( k ) if a_value != b_value : yield k , ( a_value , b_value )
7787	def _try_backup_item ( self ) : if not self . _backup_state : return False item = self . cache . get_item ( self . address , self . _backup_state ) if item : self . _object_handler ( item . address , item . value , item . state ) return True else : False
13795	def handle_reduce ( self , reduce_function_names , mapped_docs ) : reduce_functions = [ ] for reduce_function_name in reduce_function_names : try : reduce_function = get_function ( reduce_function_name ) if getattr ( reduce_function , 'view_decorated' , None ) : reduce_function = reduce_function ( self . log ) reduce_functions . append ( reduce_function ) except Exception , exc : self . log ( repr ( exc ) ) reduce_functions . append ( lambda * args , ** kwargs : None ) keys , values = zip ( ( key , value ) for ( ( key , doc_id ) , value ) in mapped_docs ) results = [ ] for reduce_function in reduce_functions : try : results . append ( reduce_function ( keys , values , rereduce = False ) ) except Exception , exc : self . log ( repr ( exc ) ) results . append ( None ) return [ True , results ]
8451	def is_temple_project ( ) : if not os . path . exists ( temple . constants . TEMPLE_CONFIG_FILE ) : msg = 'No {} file found in repository.' . format ( temple . constants . TEMPLE_CONFIG_FILE ) raise temple . exceptions . InvalidTempleProjectError ( msg )
11236	def reusable ( func ) : sig = signature ( func ) origin = func while hasattr ( origin , '__wrapped__' ) : origin = origin . __wrapped__ return type ( origin . __name__ , ( ReusableGenerator , ) , dict ( [ ( '__doc__' , origin . __doc__ ) , ( '__module__' , origin . __module__ ) , ( '__signature__' , sig ) , ( '__wrapped__' , staticmethod ( func ) ) , ] + [ ( name , property ( compose ( itemgetter ( name ) , attrgetter ( '_bound_args.arguments' ) ) ) ) for name in sig . parameters ] + ( [ ( '__qualname__' , origin . __qualname__ ) , ] if sys . version_info > ( 3 , ) else [ ] ) ) )
1358	def get_argument_component ( self ) : try : component = self . get_argument ( constants . PARAM_COMPONENT ) return component except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
12501	def _smooth_data_array ( arr , affine , fwhm , copy = True ) : if arr . dtype . kind == 'i' : if arr . dtype == np . int64 : arr = arr . astype ( np . float64 ) else : arr = arr . astype ( np . float32 ) if copy : arr = arr . copy ( ) arr [ np . logical_not ( np . isfinite ( arr ) ) ] = 0 try : affine = affine [ : 3 , : 3 ] fwhm_sigma_ratio = np . sqrt ( 8 * np . log ( 2 ) ) vox_size = np . sqrt ( np . sum ( affine ** 2 , axis = 0 ) ) sigma = fwhm / ( fwhm_sigma_ratio * vox_size ) for n , s in enumerate ( sigma ) : ndimage . gaussian_filter1d ( arr , s , output = arr , axis = n ) except : raise ValueError ( 'Error smoothing the array.' ) else : return arr
9931	def get_repr ( self , obj , referent = None ) : objtype = type ( obj ) typename = str ( objtype . __module__ ) + "." + objtype . __name__ prettytype = typename . replace ( "__builtin__." , "" ) name = getattr ( obj , "__name__" , "" ) if name : prettytype = "%s %r" % ( prettytype , name ) key = "" if referent : key = self . get_refkey ( obj , referent ) url = reverse ( 'dowser_trace_object' , args = ( typename , id ( obj ) ) ) return ( '<a class="objectid" href="%s">%s</a> ' '<span class="typename">%s</span>%s<br />' '<span class="repr">%s</span>' % ( url , id ( obj ) , prettytype , key , get_repr ( obj , 100 ) ) )
1889	def min ( self , constraints , X : BitVec , M = 10000 ) : assert isinstance ( X , BitVec ) return self . optimize ( constraints , X , 'minimize' , M )
7260	def get_data_location ( self , catalog_id ) : try : record = self . get ( catalog_id ) except : return None if 'Landsat8' in record [ 'type' ] and 'LandsatAcquisition' in record [ 'type' ] : bucket = record [ 'properties' ] [ 'bucketName' ] prefix = record [ 'properties' ] [ 'bucketPrefix' ] return 's3://' + bucket + '/' + prefix if 'DigitalGlobeAcquisition' in record [ 'type' ] : o = Ordering ( ) res = o . location ( [ catalog_id ] ) return res [ 'acquisitions' ] [ 0 ] [ 'location' ] return None
1210	def table ( self , header , body ) : table = '\n.. list-table::\n' if header and not header . isspace ( ) : table = ( table + self . indent + ':header-rows: 1\n\n' + self . _indent_block ( header ) + '\n' ) else : table = table + '\n' table = table + self . _indent_block ( body ) + '\n\n' return table
7809	def from_ssl_socket ( cls , ssl_socket ) : try : data = ssl_socket . getpeercert ( True ) except AttributeError : data = None if not data : logger . debug ( "No certificate infromation" ) return cls ( ) result = cls . from_der_data ( data ) result . validated = bool ( ssl_socket . getpeercert ( ) ) return result
8832	def if_ ( * args ) : for i in range ( 0 , len ( args ) - 1 , 2 ) : if args [ i ] : return args [ i + 1 ] if len ( args ) % 2 : return args [ - 1 ] else : return None
9695	def findall ( text ) : results = TIMESTRING_RE . findall ( text ) dates = [ ] for date in results : if re . compile ( '((next|last)\s(\d+|couple(\sof))\s(weeks|months|quarters|years))|(between|from)' , re . I ) . match ( date [ 0 ] ) : dates . append ( ( date [ 0 ] . strip ( ) , Range ( date [ 0 ] ) ) ) else : dates . append ( ( date [ 0 ] . strip ( ) , Date ( date [ 0 ] ) ) ) return dates
7451	def get_quart_iter ( tups ) : if tups [ 0 ] . endswith ( ".gz" ) : ofunc = gzip . open else : ofunc = open ofile1 = ofunc ( tups [ 0 ] , 'r' ) fr1 = iter ( ofile1 ) quart1 = itertools . izip ( fr1 , fr1 , fr1 , fr1 ) if tups [ 1 ] : ofile2 = ofunc ( tups [ 1 ] , 'r' ) fr2 = iter ( ofile2 ) quart2 = itertools . izip ( fr2 , fr2 , fr2 , fr2 ) quarts = itertools . izip ( quart1 , quart2 ) else : ofile2 = 0 quarts = itertools . izip ( quart1 , iter ( int , 1 ) ) def feedme ( quarts ) : for quart in quarts : yield quart genquarts = feedme ( quarts ) return genquarts , ofile1 , ofile2
3130	def check_url ( url ) : URL_REGEX = re . compile ( u"^" u"(?:(?:https?|ftp)://)" u"(?:\S+(?::\S*)?@)?" u"(?:" u"(?!(?:10|127)(?:\.\d{1,3}){3})" u"(?!(?:169\.254|192\.168)(?:\.\d{1,3}){2})" u"(?!172\.(?:1[6-9]|2\d|3[0-1])(?:\.\d{1,3}){2})" u"(?:[1-9]\d?|1\d\d|2[01]\d|22[0-3])" u"(?:\.(?:1?\d{1,2}|2[0-4]\d|25[0-5])){2}" u"(?:\.(?:[1-9]\d?|1\d\d|2[0-4]\d|25[0-4]))" u"|" u"(?:(?:[a-z\u00a1-\uffff0-9]-?)*[a-z\u00a1-\uffff0-9]+)" u"(?:\.(?:[a-z\u00a1-\uffff0-9]-?)*[a-z\u00a1-\uffff0-9]+)*" u"(?:\.(?:[a-z\u00a1-\uffff]{2,}))" u")" u"(?::\d{2,5})?" u"(?:/\S*)?" u"$" , re . UNICODE ) if not re . match ( URL_REGEX , url ) : raise ValueError ( 'String passed is not a valid url' ) return
13876	def CopyFilesX ( file_mapping ) : files = [ ] for i_target_path , i_source_path_mask in file_mapping : tree_recurse , flat_recurse , dirname , in_filters , out_filters = ExtendedPathMask . Split ( i_source_path_mask ) _AssertIsLocal ( dirname ) filenames = FindFiles ( dirname , in_filters , out_filters , tree_recurse ) for i_source_filename in filenames : if os . path . isdir ( i_source_filename ) : continue i_target_filename = i_source_filename [ len ( dirname ) + 1 : ] if flat_recurse : i_target_filename = os . path . basename ( i_target_filename ) i_target_filename = os . path . join ( i_target_path , i_target_filename ) files . append ( ( StandardizePath ( i_source_filename ) , StandardizePath ( i_target_filename ) ) ) for i_source_filename , i_target_filename in files : target_dir = os . path . dirname ( i_target_filename ) CreateDirectory ( target_dir ) CopyFile ( i_source_filename , i_target_filename ) return files
10789	def guess_invert ( st ) : pos = st . obj_get_positions ( ) pxinds_ar = np . round ( pos ) . astype ( 'int' ) inim = st . ishape . translate ( - st . pad ) . contains ( pxinds_ar ) pxinds_tuple = tuple ( pxinds_ar [ inim ] . T ) pxvals = st . data [ pxinds_tuple ] invert = np . median ( pxvals ) < np . median ( st . data ) return invert
6722	def list_instances ( show = 1 , name = None , group = None , release = None , except_release = None ) : from burlap . common import shelf , OrderedDict , get_verbose verbose = get_verbose ( ) require ( 'vm_type' , 'vm_group' ) assert env . vm_type , 'No VM type specified.' env . vm_type = ( env . vm_type or '' ) . lower ( ) _name = name _group = group _release = release if verbose : print ( 'name=%s, group=%s, release=%s' % ( _name , _group , _release ) ) env . vm_elastic_ip_mappings = shelf . get ( 'vm_elastic_ip_mappings' ) data = type ( env ) ( ) if env . vm_type == EC2 : if verbose : print ( 'Checking EC2...' ) for instance in get_all_running_ec2_instances ( ) : name = instance . tags . get ( env . vm_name_tag ) group = instance . tags . get ( env . vm_group_tag ) release = instance . tags . get ( env . vm_release_tag ) if env . vm_group and env . vm_group != group : if verbose : print ( ( 'Skipping instance %s because its group "%s" ' 'does not match env.vm_group "%s".' ) % ( instance . public_dns_name , group , env . vm_group ) ) continue if _group and group != _group : if verbose : print ( ( 'Skipping instance %s because its group "%s" ' 'does not match local group "%s".' ) % ( instance . public_dns_name , group , _group ) ) continue if _name and name != _name : if verbose : print ( ( 'Skipping instance %s because its name "%s" ' 'does not match name "%s".' ) % ( instance . public_dns_name , name , _name ) ) continue if _release and release != _release : if verbose : print ( ( 'Skipping instance %s because its release "%s" ' 'does not match release "%s".' ) % ( instance . public_dns_name , release , _release ) ) continue if except_release and release == except_release : continue if verbose : print ( 'Adding instance %s (%s).' % ( name , instance . public_dns_name ) ) data . setdefault ( name , type ( env ) ( ) ) data [ name ] [ 'id' ] = instance . id data [ name ] [ 'public_dns_name' ] = instance . public_dns_name if verbose : print ( 'Public DNS: %s' % instance . public_dns_name ) if env . vm_elastic_ip_mappings and name in env . vm_elastic_ip_mappings : data [ name ] [ 'ip' ] = env . vm_elastic_ip_mappings [ name ] else : data [ name ] [ 'ip' ] = socket . gethostbyname ( instance . public_dns_name ) if int ( show ) : pprint ( data , indent = 4 ) return data elif env . vm_type == KVM : pass else : raise NotImplementedError
12671	def aggregate ( self , clazz , new_col , * args ) : if is_callable ( clazz ) and not is_none ( new_col ) and has_elements ( * args ) : return self . __do_aggregate ( clazz , new_col , * args )
12774	def inverse_kinematics ( self , start = 0 , end = 1e100 , states = None , max_force = 20 ) : zeros = None if max_force > 0 : self . skeleton . enable_motors ( max_force ) zeros = np . zeros ( self . skeleton . num_dofs ) for _ in self . follow_markers ( start , end , states ) : if zeros is not None : self . skeleton . set_target_angles ( zeros ) yield self . skeleton . joint_angles
6578	def _base_repr ( self , and_also = None ) : items = [ "=" . join ( ( key , repr ( getattr ( self , key ) ) ) ) for key in sorted ( self . _fields . keys ( ) ) ] if items : output = ", " . join ( items ) else : output = None if and_also : return "{}({}, {})" . format ( self . __class__ . __name__ , output , and_also ) else : return "{}({})" . format ( self . __class__ . __name__ , output )
1183	def push_new_context ( self , pattern_offset ) : child_context = _MatchContext ( self . state , self . pattern_codes [ self . code_position + pattern_offset : ] ) self . state . context_stack . append ( child_context ) return child_context
438	def read_and_decode ( filename , is_train = None ) : filename_queue = tf . train . string_input_producer ( [ filename ] ) reader = tf . TFRecordReader ( ) _ , serialized_example = reader . read ( filename_queue ) features = tf . parse_single_example ( serialized_example , features = { 'label' : tf . FixedLenFeature ( [ ] , tf . int64 ) , 'img_raw' : tf . FixedLenFeature ( [ ] , tf . string ) , } ) img = tf . decode_raw ( features [ 'img_raw' ] , tf . float32 ) img = tf . reshape ( img , [ 32 , 32 , 3 ] ) if is_train == True : img = tf . random_crop ( img , [ 24 , 24 , 3 ] ) img = tf . image . random_flip_left_right ( img ) img = tf . image . random_brightness ( img , max_delta = 63 ) img = tf . image . random_contrast ( img , lower = 0.2 , upper = 1.8 ) img = tf . image . per_image_standardization ( img ) elif is_train == False : img = tf . image . resize_image_with_crop_or_pad ( img , 24 , 24 ) img = tf . image . per_image_standardization ( img ) elif is_train == None : img = img label = tf . cast ( features [ 'label' ] , tf . int32 ) return img , label
3324	def lock_string ( lock_dict ) : if not lock_dict : return "Lock: None" if lock_dict [ "expire" ] < 0 : expire = "Infinite ({})" . format ( lock_dict [ "expire" ] ) else : expire = "{} (in {} seconds)" . format ( util . get_log_time ( lock_dict [ "expire" ] ) , lock_dict [ "expire" ] - time . time ( ) ) return "Lock(<{}..>, '{}', {}, {}, depth-{}, until {}" . format ( lock_dict . get ( "token" , "?" * 30 ) [ 18 : 22 ] , lock_dict . get ( "root" ) , lock_dict . get ( "principal" ) , lock_dict . get ( "scope" ) , lock_dict . get ( "depth" ) , expire , )
10734	def fork ( self , name ) : fork = deepcopy ( self ) self [ name ] = fork return fork
7740	def hold_exception ( method ) : @ functools . wraps ( method ) def wrapper ( self , * args , ** kwargs ) : try : return method ( self , * args , ** kwargs ) except Exception : if self . exc_info : raise if not self . _stack : logger . debug ( '@hold_exception wrapped method {0!r} called' ' from outside of the main loop' . format ( method ) ) raise self . exc_info = sys . exc_info ( ) logger . debug ( u"exception in glib main loop callback:" , exc_info = self . exc_info ) main_loop = self . _stack [ - 1 ] if main_loop is not None : main_loop . quit ( ) return False return wrapper
9164	def includeme ( config ) : api_key_authn_policy = APIKeyAuthenticationPolicy ( ) config . include ( 'openstax_accounts' ) openstax_authn_policy = config . registry . getUtility ( IOpenstaxAccountsAuthenticationPolicy ) policies = [ api_key_authn_policy , openstax_authn_policy ] authn_policy = MultiAuthenticationPolicy ( policies ) config . set_authentication_policy ( authn_policy ) authz_policy = ACLAuthorizationPolicy ( ) config . set_authorization_policy ( authz_policy )
9169	def parse_archive_uri ( uri ) : parsed = urlparse ( uri ) path = parsed . path . rstrip ( '/' ) . split ( '/' ) ident_hash = path [ - 1 ] ident_hash = unquote ( ident_hash ) return ident_hash
9349	def date ( past = False , min_delta = 0 , max_delta = 20 ) : timedelta = dt . timedelta ( days = _delta ( past , min_delta , max_delta ) ) return dt . date . today ( ) + timedelta
6711	def disk ( self ) : r = self . local_renderer r . run ( r . env . disk_usage_command )
744	def requireAnomalyModel ( func ) : @ wraps ( func ) def _decorator ( self , * args , ** kwargs ) : if not self . getInferenceType ( ) == InferenceType . TemporalAnomaly : raise RuntimeError ( "Method required a TemporalAnomaly model." ) if self . _getAnomalyClassifier ( ) is None : raise RuntimeError ( "Model does not support this command. Model must" "be an active anomalyDetector model." ) return func ( self , * args , ** kwargs ) return _decorator
11079	def stop_timer ( self , func ) : if func in self . _timer_callbacks : t = self . _timer_callbacks [ func ] t . cancel ( ) del self . _timer_callbacks [ func ]
8856	def on_current_tab_changed ( self ) : self . menuEdit . clear ( ) self . menuModes . clear ( ) self . menuPanels . clear ( ) editor = self . tabWidget . current_widget ( ) self . menuEdit . setEnabled ( editor is not None ) self . menuModes . setEnabled ( editor is not None ) self . menuPanels . setEnabled ( editor is not None ) self . actionSave . setEnabled ( editor is not None ) self . actionSave_as . setEnabled ( editor is not None ) self . actionConfigure_run . setEnabled ( editor is not None ) self . actionRun . setEnabled ( editor is not None ) if editor is not None : self . setup_mnu_edit ( editor ) self . setup_mnu_modes ( editor ) self . setup_mnu_panels ( editor ) self . widgetOutline . set_editor ( editor ) self . _update_status_bar ( editor )
7562	def _run_qmc ( self , boot ) : self . _tmp = os . path . join ( self . dirs , ".tmptre" ) cmd = [ ip . bins . qmc , "qrtt=" + self . files . qdump , "otre=" + self . _tmp ] proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) res = proc . communicate ( ) if proc . returncode : raise IPyradWarningExit ( res [ 1 ] ) with open ( self . _tmp , 'r' ) as intree : tre = ete3 . Tree ( intree . read ( ) . strip ( ) ) names = tre . get_leaves ( ) for name in names : name . name = self . samples [ int ( name . name ) ] tmptre = tre . write ( format = 9 ) if boot : self . trees . boots = os . path . join ( self . dirs , self . name + ".boots" ) with open ( self . trees . boots , 'a' ) as outboot : outboot . write ( tmptre + "\n" ) else : self . trees . tree = os . path . join ( self . dirs , self . name + ".tree" ) with open ( self . trees . tree , 'w' ) as outtree : outtree . write ( tmptre ) self . _save ( )
7115	def config_sources ( app , environment , cluster , configs_dirs , app_dir , local = False , build = False ) : sources = [ ( configs_dirs , 'hostname' ) , ( configs_dirs , 'hostname-local' ) , ( configs_dirs , 'hostname-build' ) , ( configs_dirs , 'common' ) , ( configs_dirs , 'common-%s' % environment ) , ( configs_dirs , 'common-%s-%s' % ( environment , cluster ) ) , ( configs_dirs , 'common-local' ) , ( configs_dirs , 'common-build' ) , ( configs_dirs , 'common-overrides' ) , ( [ app_dir ] , '%s-default' % app ) , ( [ app_dir ] , '%s-%s' % ( app , environment ) ) , ( [ app_dir ] , '%s-%s-%s' % ( app , environment , cluster ) ) , ( configs_dirs , app ) , ( configs_dirs , '%s-%s' % ( app , environment ) ) , ( configs_dirs , '%s-%s-%s' % ( app , environment , cluster ) ) , ( [ app_dir ] , '%s-local' % app ) , ( [ app_dir ] , '%s-build' % app ) , ( configs_dirs , '%s-local' % app ) , ( configs_dirs , '%s-build' % app ) , ( configs_dirs , '%s-overrides' % app ) , ] if not build : sources = [ source for source in sources if not source [ 1 ] . endswith ( '-build' ) ] if not local : sources = [ source for source in sources if not source [ 1 ] . endswith ( '-local' ) ] return available_sources ( sources )
1352	def make_success_response ( self , result ) : response = self . make_response ( constants . RESPONSE_STATUS_SUCCESS ) response [ constants . RESPONSE_KEY_RESULT ] = result return response
533	def setParameter ( self , paramName , value ) : ( setter , getter ) = self . _getParameterMethods ( paramName ) if setter is None : import exceptions raise exceptions . Exception ( "setParameter -- parameter name '%s' does not exist in region %s of type %s" % ( paramName , self . name , self . type ) ) setter ( paramName , value )
13536	def prune_list ( self ) : targets = self . descendents_root ( ) try : targets . remove ( self . graph . root ) except ValueError : pass targets . append ( self ) return targets
10132	def parse_grid ( grid_data ) : try : grid_parts = NEWLINE_RE . split ( grid_data ) if len ( grid_parts ) < 2 : raise ZincParseException ( 'Malformed grid received' , grid_data , 1 , 1 ) grid_meta_str = grid_parts . pop ( 0 ) col_meta_str = grid_parts . pop ( 0 ) ver_match = VERSION_RE . match ( grid_meta_str ) if ver_match is None : raise ZincParseException ( 'Could not determine version from %r' % grid_meta_str , grid_data , 1 , 1 ) version = Version ( ver_match . group ( 1 ) ) try : grid_meta = hs_gridMeta [ version ] . parseString ( grid_meta_str , parseAll = True ) [ 0 ] except pp . ParseException as pe : raise ZincParseException ( 'Failed to parse grid metadata: %s' % pe , grid_data , 1 , pe . col ) except : LOG . debug ( 'Failed to parse grid meta: %r' , grid_meta_str ) raise try : col_meta = hs_cols [ version ] . parseString ( col_meta_str , parseAll = True ) [ 0 ] except pp . ParseException as pe : raise ZincParseException ( 'Failed to parse column metadata: %s' % reformat_exception ( pe , 2 ) , grid_data , 2 , pe . col ) except : LOG . debug ( 'Failed to parse column meta: %r' , col_meta_str ) raise row_grammar = hs_row [ version ] def _parse_row ( row_num_and_data ) : ( row_num , row ) = row_num_and_data line_num = row_num + 3 try : return dict ( zip ( col_meta . keys ( ) , row_grammar . parseString ( row , parseAll = True ) [ 0 ] . asList ( ) ) ) except pp . ParseException as pe : raise ZincParseException ( 'Failed to parse row: %s' % reformat_exception ( pe , line_num ) , grid_data , line_num , pe . col ) except : LOG . debug ( 'Failed to parse row: %r' , row ) raise g = Grid ( version = grid_meta . pop ( 'ver' ) , metadata = grid_meta , columns = list ( col_meta . items ( ) ) ) g . extend ( map ( _parse_row , filter ( lambda gp : bool ( gp [ 1 ] ) , enumerate ( grid_parts ) ) ) ) return g except : LOG . debug ( 'Failing grid: %r' , grid_data ) raise
3268	def md_dynamic_default_values_info ( name , node ) : configurations = node . find ( "configurations" ) if configurations is not None : configurations = [ ] for n in node . findall ( "configuration" ) : dimension = n . find ( "dimension" ) dimension = dimension . text if dimension is not None else None policy = n . find ( "policy" ) policy = policy . text if policy is not None else None defaultValueExpression = n . find ( "defaultValueExpression" ) defaultValueExpression = defaultValueExpression . text if defaultValueExpression is not None else None configurations . append ( DynamicDefaultValuesConfiguration ( dimension , policy , defaultValueExpression ) ) return DynamicDefaultValues ( name , configurations )
372	def shift ( x , wrg = 0.1 , hrg = 0.1 , is_random = False , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , order = 1 ) : h , w = x . shape [ row_index ] , x . shape [ col_index ] if is_random : tx = np . random . uniform ( - hrg , hrg ) * h ty = np . random . uniform ( - wrg , wrg ) * w else : tx , ty = hrg * h , wrg * w translation_matrix = np . array ( [ [ 1 , 0 , tx ] , [ 0 , 1 , ty ] , [ 0 , 0 , 1 ] ] ) transform_matrix = translation_matrix x = affine_transform ( x , transform_matrix , channel_index , fill_mode , cval , order ) return x
11166	def unusedoptions ( self , sections ) : unused = set ( [ ] ) for section in _list ( sections ) : if not self . has_section ( section ) : continue options = self . options ( section ) raw_values = [ self . get ( section , option , raw = True ) for option in options ] for option in options : formatter = "%(" + option + ")s" for raw_value in raw_values : if formatter in raw_value : break else : unused . add ( option ) return list ( unused )
4884	def handle_transmission_error ( self , learner_data , request_exception ) : try : sys_msg = request_exception . response . content except AttributeError : pass else : if 'user account is inactive' in sys_msg : ecu = EnterpriseCustomerUser . objects . get ( enterprise_enrollments__id = learner_data . enterprise_course_enrollment_id ) ecu . active = False ecu . save ( ) LOGGER . warning ( 'User %s with ID %s and email %s is a former employee of %s ' 'and has been marked inactive in SAPSF. Now marking inactive internally.' , ecu . username , ecu . user_id , ecu . user_email , ecu . enterprise_customer ) return super ( SapSuccessFactorsLearnerTransmitter , self ) . handle_transmission_error ( learner_data , request_exception )
10700	def paginate_link_tag ( item ) : a_tag = Page . default_link_tag ( item ) if item [ 'type' ] == 'current_page' : return make_html_tag ( 'li' , a_tag , ** { 'class' : 'blue white-text' } ) return make_html_tag ( 'li' , a_tag )
8054	def handler ( self , conn , * args ) : self . shell . stdout . write ( self . shell . prompt ) line = self . shell . stdin . readline ( ) if not len ( line ) : line = 'EOF' return False else : line = line . rstrip ( '\r\n' ) line = self . shell . precmd ( line ) stop = self . shell . onecmd ( line ) stop = self . shell . postcmd ( stop , line ) self . shell . stdout . flush ( ) self . shell . postloop ( ) if stop : self . shell = None conn . close ( ) return not stop
2716	def __extract_resources_from_droplets ( self , data ) : resources = [ ] if not isinstance ( data , list ) : return data for a_droplet in data : res = { } try : if isinstance ( a_droplet , unicode ) : res = { "resource_id" : a_droplet , "resource_type" : "droplet" } except NameError : pass if isinstance ( a_droplet , str ) or isinstance ( a_droplet , int ) : res = { "resource_id" : str ( a_droplet ) , "resource_type" : "droplet" } elif isinstance ( a_droplet , Droplet ) : res = { "resource_id" : str ( a_droplet . id ) , "resource_type" : "droplet" } if len ( res ) > 0 : resources . append ( res ) return resources
7613	def get_arena_image ( self , obj : BaseAttrDict ) : badge_id = obj . arena . id for i in self . constants . arenas : if i . id == badge_id : return 'https://royaleapi.github.io/cr-api-assets/arenas/arena{}.png' . format ( i . arena_id )
9759	def statuses ( ctx , job , page ) : def get_experiment_statuses ( ) : try : response = PolyaxonClient ( ) . experiment . get_statuses ( user , project_name , _experiment , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could get status for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Statuses for experiment `{}`.' . format ( _experiment ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No statuses found for experiment `{}`.' . format ( _experiment ) ) objects = list_dicts_to_tabulate ( [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) , status_key = 'status' ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Statuses:" ) objects . pop ( 'experiment' , None ) dict_tabulate ( objects , is_list_dict = True ) def get_experiment_job_statuses ( ) : try : response = PolyaxonClient ( ) . experiment_job . get_statuses ( user , project_name , _experiment , _job , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get status for job `{}`.' . format ( job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Statuses for Job `{}`.' . format ( _job ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No statuses found for job `{}`.' . format ( _job ) ) objects = list_dicts_to_tabulate ( [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) , status_key = 'status' ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Statuses:" ) objects . pop ( 'job' , None ) dict_tabulate ( objects , is_list_dict = True ) page = page or 1 user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if job : _job = get_experiment_job_or_local ( job ) get_experiment_job_statuses ( ) else : get_experiment_statuses ( )
9384	def parse_xml_jtl ( self , granularity ) : data = defaultdict ( list ) processed_data = defaultdict ( lambda : defaultdict ( lambda : defaultdict ( list ) ) ) for input_file in self . infile_list : logger . info ( 'Processing : %s' , input_file ) timestamp_format = None tree = ElementTree . parse ( input_file ) samples = tree . findall ( './httpSample' ) + tree . findall ( './sample' ) for sample in samples : if not timestamp_format or timestamp_format == 'unknown' : timestamp_format = naarad . utils . detect_timestamp_format ( sample . get ( 'ts' ) ) if timestamp_format == 'unknown' : continue ts = naarad . utils . get_standardized_timestamp ( sample . get ( 'ts' ) , timestamp_format ) if ts == - 1 : continue ts = naarad . utils . reconcile_timezones ( ts , self . timezone , self . graph_timezone ) aggregate_timestamp , averaging_factor = self . get_aggregation_timestamp ( ts , granularity ) self . aggregate_count_over_time ( processed_data , sample , [ self . _sanitize_label ( sample . get ( 'lb' ) ) , 'Overall_Summary' ] , aggregate_timestamp ) self . aggregate_values_over_time ( processed_data , sample , [ self . _sanitize_label ( sample . get ( 'lb' ) ) , 'Overall_Summary' ] , [ 't' , 'by' ] , aggregate_timestamp ) logger . info ( 'Finished parsing : %s' , input_file ) logger . info ( 'Processing metrics for output to csv' ) self . average_values_for_plot ( processed_data , data , averaging_factor ) logger . info ( 'Writing time series csv' ) for csv in data . keys ( ) : self . csv_files . append ( csv ) with open ( csv , 'w' ) as csvf : csvf . write ( '\n' . join ( sorted ( data [ csv ] ) ) ) logger . info ( 'Processing raw data for stats' ) self . calculate_key_stats ( processed_data ) return True
6638	def getScript ( self , scriptname ) : script = self . description . get ( 'scripts' , { } ) . get ( scriptname , None ) if script is not None : if isinstance ( script , str ) or isinstance ( script , type ( u'unicode string' ) ) : import shlex script = shlex . split ( script ) if len ( script ) and script [ 0 ] . lower ( ) . endswith ( '.py' ) : if not os . path . isabs ( script [ 0 ] ) : absscript = os . path . abspath ( os . path . join ( self . path , script [ 0 ] ) ) logger . debug ( 'rewriting script %s to be absolute path %s' , script [ 0 ] , absscript ) script [ 0 ] = absscript import sys script = [ sys . executable ] + script return script
8841	def jsonLogic ( tests , data = None ) : if tests is None or not isinstance ( tests , dict ) : return tests data = data or { } operator = list ( tests . keys ( ) ) [ 0 ] values = tests [ operator ] if not isinstance ( values , list ) and not isinstance ( values , tuple ) : values = [ values ] values = [ jsonLogic ( val , data ) for val in values ] if operator == 'var' : return get_var ( data , * values ) if operator == 'missing' : return missing ( data , * values ) if operator == 'missing_some' : return missing_some ( data , * values ) if operator not in operations : raise ValueError ( "Unrecognized operation %s" % operator ) return operations [ operator ] ( * values )
2019	def SMOD ( self , a , b ) : s0 , s1 = to_signed ( a ) , to_signed ( b ) sign = Operators . ITEBV ( 256 , s0 < 0 , - 1 , 1 ) try : result = ( Operators . ABS ( s0 ) % Operators . ABS ( s1 ) ) * sign except ZeroDivisionError : result = 0 return Operators . ITEBV ( 256 , s1 == 0 , 0 , result )
7258	def search_address ( self , address , filters = None , startDate = None , endDate = None , types = None ) : lat , lng = self . get_address_coords ( address ) return self . search_point ( lat , lng , filters = filters , startDate = startDate , endDate = endDate , types = types )
1113	def _split_line ( self , data_list , line_num , text ) : if not line_num : data_list . append ( ( line_num , text ) ) return size = len ( text ) max = self . _wrapcolumn if ( size <= max ) or ( ( size - ( text . count ( '\0' ) * 3 ) ) <= max ) : data_list . append ( ( line_num , text ) ) return i = 0 n = 0 mark = '' while n < max and i < size : if text [ i ] == '\0' : i += 1 mark = text [ i ] i += 1 elif text [ i ] == '\1' : i += 1 mark = '' else : i += 1 n += 1 line1 = text [ : i ] line2 = text [ i : ] if mark : line1 = line1 + '\1' line2 = '\0' + mark + line2 data_list . append ( ( line_num , line1 ) ) self . _split_line ( data_list , '>' , line2 )
8853	def on_open ( self ) : filename , filter = QtWidgets . QFileDialog . getOpenFileName ( self , 'Open' ) if filename : self . open_file ( filename ) self . actionRun . setEnabled ( True ) self . actionConfigure_run . setEnabled ( True )
10994	def randomize_parameters ( self , ptp = 0.2 , fourier = False , vmin = None , vmax = None ) : if vmin is not None and vmax is not None : ptp = vmax - vmin elif vmax is not None and vmin is None : vmin = vmax - ptp elif vmin is not None and vmax is None : vmax = vmin + ptp else : vmax = 1.0 vmin = vmax - ptp self . set_values ( self . category + '-scale' , 1.0 ) self . set_values ( self . category + '-off' , 0.0 ) for k , v in iteritems ( self . poly_params ) : norm = ( self . zorder + 1.0 ) * 2 self . set_values ( k , ptp * ( np . random . rand ( ) - 0.5 ) / norm ) for i , p in enumerate ( self . barnes_params ) : N = len ( p ) if fourier : t = ( ( np . random . rand ( N ) - 0.5 ) + 1.j * ( np . random . rand ( N ) - 0.5 ) ) / ( np . arange ( N ) + 1 ) q = np . real ( np . fft . ifftn ( t ) ) / ( i + 1 ) else : t = ptp * np . sqrt ( N ) * ( np . random . rand ( N ) - 0.5 ) q = np . cumsum ( t ) / ( i + 1 ) q = ptp * q / q . ptp ( ) / len ( self . barnes_params ) q -= q . mean ( ) self . set_values ( p , q ) self . _norm_stat = [ ptp , vmin ] if self . shape : self . initialize ( ) if self . _parent : param = self . category + '-scale' self . trigger_update ( param , self . get_values ( param ) )
5463	def ddel_tasks ( provider , user_ids = None , job_ids = None , task_ids = None , labels = None , create_time_min = None , create_time_max = None ) : deleted_tasks , error_messages = provider . delete_jobs ( user_ids , job_ids , task_ids , labels , create_time_min , create_time_max ) for msg in error_messages : print ( msg ) return deleted_tasks
11021	def _generate_circle ( self ) : total_weight = 0 for node in self . nodes : total_weight += self . weights . get ( node , 1 ) for node in self . nodes : weight = 1 if node in self . weights : weight = self . weights . get ( node ) factor = math . floor ( ( 40 * len ( self . nodes ) * weight ) / total_weight ) for j in range ( 0 , int ( factor ) ) : b_key = bytearray ( self . _hash_digest ( '%s-%s' % ( node , j ) ) ) for i in range ( 0 , 3 ) : key = self . _hash_val ( b_key , lambda x : x + i * 4 ) self . ring [ key ] = node self . _sorted_keys . append ( key ) self . _sorted_keys . sort ( )
8839	def missing ( data , * args ) : not_found = object ( ) if args and isinstance ( args [ 0 ] , list ) : args = args [ 0 ] ret = [ ] for arg in args : if get_var ( data , arg , not_found ) is not_found : ret . append ( arg ) return ret
1569	def invoke_hook_bolt_fail ( self , heron_tuple , fail_latency_ns ) : if len ( self . task_hooks ) > 0 : bolt_fail_info = BoltFailInfo ( heron_tuple = heron_tuple , failing_task_id = self . get_task_id ( ) , fail_latency_ms = fail_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . bolt_fail ( bolt_fail_info )
10144	def from_schema ( self , schema_node , base_name = None ) : return self . _ref_recursive ( self . type_converter ( schema_node ) , self . ref , base_name )
13471	def apply_changesets ( args , changesets , catalog ) : tmpdir = tempfile . mkdtemp ( ) tmp_patch = join ( tmpdir , "tmp.patch" ) tmp_lcat = join ( tmpdir , "tmp.lcat" ) for node in changesets : remove ( tmp_patch ) copy ( node . mfile [ 'changeset' ] [ 'filename' ] , tmp_patch ) logging . info ( "mv %s %s" % ( catalog , tmp_lcat ) ) shutil . move ( catalog , tmp_lcat ) cmd = args . patch_cmd . replace ( "$in1" , tmp_lcat ) . replace ( "$patch" , tmp_patch ) . replace ( "$out" , catalog ) logging . info ( "Patch: %s" % cmd ) subprocess . check_call ( cmd , shell = True ) shutil . rmtree ( tmpdir , ignore_errors = True )
3652	def run ( self ) : while self . _base . is_running : if self . _worker : self . _worker ( ) time . sleep ( self . _sleep_duration )
2940	def deserialize_condition ( self , workflow , start_node ) : condition = None spec_name = None for node in start_node . childNodes : if node . nodeType != minidom . Node . ELEMENT_NODE : continue if node . nodeName . lower ( ) == 'successor' : if spec_name is not None : _exc ( 'Duplicate task name %s' % spec_name ) if node . firstChild is None : _exc ( 'Successor tag without a task name' ) spec_name = node . firstChild . nodeValue elif node . nodeName . lower ( ) in _op_map : if condition is not None : _exc ( 'Multiple conditions are not yet supported' ) condition = self . deserialize_logical ( node ) else : _exc ( 'Unknown node: %s' % node . nodeName ) if condition is None : _exc ( 'Missing condition in conditional statement' ) if spec_name is None : _exc ( 'A %s has no task specified' % start_node . nodeName ) return condition , spec_name
5442	def parse_file_provider ( uri ) : providers = { 'gs' : job_model . P_GCS , 'file' : job_model . P_LOCAL } provider_found = re . match ( r'^([A-Za-z][A-Za-z0-9+.-]{0,29})://' , uri ) if provider_found : prefix = provider_found . group ( 1 ) . lower ( ) else : prefix = 'file' if prefix in providers : return providers [ prefix ] else : raise ValueError ( 'File prefix not supported: %s://' % prefix )
5735	def _get_or_create_subscription ( self ) : topic_path = self . _get_topic_path ( ) subscription_name = '{}-{}-{}-worker' . format ( queue . PUBSUB_OBJECT_PREFIX , self . name , uuid4 ( ) . hex ) subscription_path = self . subscriber_client . subscription_path ( self . project , subscription_name ) try : self . subscriber_client . get_subscription ( subscription_path ) except google . cloud . exceptions . NotFound : logger . info ( "Creating worker subscription {}" . format ( subscription_name ) ) self . subscriber_client . create_subscription ( subscription_path , topic_path ) return subscription_path
6496	def _get_mappings ( self , doc_type ) : mapping = ElasticSearchEngine . get_mappings ( self . index_name , doc_type ) if not mapping : mapping = self . _es . indices . get_mapping ( index = self . index_name , doc_type = doc_type , ) . get ( self . index_name , { } ) . get ( 'mappings' , { } ) . get ( doc_type , { } ) if mapping : ElasticSearchEngine . set_mappings ( self . index_name , doc_type , mapping ) return mapping
6718	def virtualenv_exists ( self , virtualenv_dir = None ) : r = self . local_renderer ret = True with self . settings ( warn_only = True ) : ret = r . run_or_local ( 'ls {virtualenv_dir}' ) or '' ret = 'cannot access' not in ret . strip ( ) . lower ( ) if self . verbose : if ret : print ( 'Yes' ) else : print ( 'No' ) return ret
3820	async def create_conversation ( self , create_conversation_request ) : response = hangouts_pb2 . CreateConversationResponse ( ) await self . _pb_request ( 'conversations/createconversation' , create_conversation_request , response ) return response
11161	def autopep8 ( self , ** kwargs ) : self . assert_is_dir_and_exists ( ) for p in self . select_by_ext ( ".py" ) : with open ( p . abspath , "rb" ) as f : code = f . read ( ) . decode ( "utf-8" ) formatted_code = autopep8 . fix_code ( code , ** kwargs ) with open ( p . abspath , "wb" ) as f : f . write ( formatted_code . encode ( "utf-8" ) )
5614	def segmentize_geometry ( geometry , segmentize_value ) : if geometry . geom_type != "Polygon" : raise TypeError ( "segmentize geometry type must be Polygon" ) return Polygon ( LinearRing ( [ p for l in map ( lambda x : LineString ( [ x [ 0 ] , x [ 1 ] ] ) , zip ( geometry . exterior . coords [ : - 1 ] , geometry . exterior . coords [ 1 : ] ) ) for p in [ l . interpolate ( segmentize_value * i ) . coords [ 0 ] for i in range ( int ( l . length / segmentize_value ) ) ] + [ l . coords [ 1 ] ] ] ) )
2542	def set_file_chksum ( self , doc , chk_sum ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_chksum_set : self . file_chksum_set = True self . file ( doc ) . chk_sum = checksum . Algorithm ( 'SHA1' , chk_sum ) return True else : raise CardinalityError ( 'File::CheckSum' ) else : raise OrderError ( 'File::CheckSum' )
12451	def deref ( self , data ) : deref = copy . deepcopy ( jsonref . JsonRef . replace_refs ( data ) ) self . write_template ( deref , filename = 'swagger.json' ) return deref
9459	def send_digits ( self , call_params ) : path = '/' + self . api_version + '/SendDigits/' method = 'POST' return self . request ( path , method , call_params )
5150	def parse ( self , native ) : if not hasattr ( self , 'parser' ) or not self . parser : raise NotImplementedError ( 'Parser class not specified' ) parser = self . parser ( native ) self . intermediate_data = parser . intermediate_data del parser self . to_netjson ( )
11603	def parse_byteranges ( cls , environ ) : r = [ ] s = environ . get ( cls . header_range , '' ) . replace ( ' ' , '' ) . lower ( ) if s : l = s . split ( '=' ) if len ( l ) == 2 : unit , vals = tuple ( l ) if unit == 'bytes' and vals : gen_rng = ( tuple ( rng . split ( '-' ) ) for rng in vals . split ( ',' ) if '-' in rng ) for start , end in gen_rng : if start or end : r . append ( ( int ( start ) if start else None , int ( end ) if end else None ) ) return r
9271	def filter_since_tag ( self , all_tags ) : tag = self . detect_since_tag ( ) if not tag or tag == REPO_CREATED_TAG_NAME : return copy . deepcopy ( all_tags ) filtered_tags = [ ] tag_names = [ t [ "name" ] for t in all_tags ] try : idx = tag_names . index ( tag ) except ValueError : self . warn_if_tag_not_found ( tag , "since-tag" ) return copy . deepcopy ( all_tags ) since_tag = all_tags [ idx ] since_date = self . get_time_of_tag ( since_tag ) for t in all_tags : tag_date = self . get_time_of_tag ( t ) if since_date <= tag_date : filtered_tags . append ( t ) return filtered_tags
11272	def register_default_types ( ) : register_type ( type , pipe . map ) register_type ( types . FunctionType , pipe . map ) register_type ( types . MethodType , pipe . map ) register_type ( tuple , seq ) register_type ( list , seq ) register_type ( types . GeneratorType , seq ) register_type ( string_type , sh ) register_type ( unicode_type , sh ) register_type ( file_type , fileobj ) if is_py3 : register_type ( range , seq ) register_type ( map , seq )
13622	def many ( func ) : def _many ( result ) : if _isSequenceTypeNotText ( result ) : return map ( func , result ) return [ ] return maybe ( _many , default = [ ] )
9170	def declare_api_routes ( config ) : add_route = config . add_route add_route ( 'get-content' , '/contents/{ident_hash}' ) add_route ( 'get-resource' , '/resources/{hash}' ) add_route ( 'license-request' , '/contents/{uuid}/licensors' ) add_route ( 'roles-request' , '/contents/{uuid}/roles' ) add_route ( 'acl-request' , '/contents/{uuid}/permissions' ) add_route ( 'publications' , '/publications' ) add_route ( 'get-publication' , '/publications/{id}' ) add_route ( 'publication-license-acceptance' , '/publications/{id}/license-acceptances/{uid}' ) add_route ( 'publication-role-acceptance' , '/publications/{id}/role-acceptances/{uid}' ) add_route ( 'collate-content' , '/contents/{ident_hash}/collate-content' ) add_route ( 'bake-content' , '/contents/{ident_hash}/baked' ) add_route ( 'moderation' , '/moderations' ) add_route ( 'moderate' , '/moderations/{id}' ) add_route ( 'moderation-rss' , '/feeds/moderations.rss' ) add_route ( 'api-keys' , '/api-keys' ) add_route ( 'api-key' , '/api-keys/{id}' )
2123	def disassociate_failure_node ( self , parent , child ) : return self . _disassoc ( self . _forward_rel_name ( 'failure' ) , parent , child )
8649	def delete_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'delete' , } endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
7163	def answer_display ( self , s = '' ) : padding = len ( max ( self . questions . keys ( ) , key = len ) ) + 5 for key in list ( self . answers . keys ( ) ) : s += '{:>{}} : {}\n' . format ( key , padding , self . answers [ key ] ) return s
13790	def MessageSetItemDecoder ( extensions_by_number ) : type_id_tag_bytes = encoder . TagBytes ( 2 , wire_format . WIRETYPE_VARINT ) message_tag_bytes = encoder . TagBytes ( 3 , wire_format . WIRETYPE_LENGTH_DELIMITED ) item_end_tag_bytes = encoder . TagBytes ( 1 , wire_format . WIRETYPE_END_GROUP ) local_ReadTag = ReadTag local_DecodeVarint = _DecodeVarint local_SkipField = SkipField def DecodeItem ( buffer , pos , end , message , field_dict ) : message_set_item_start = pos type_id = - 1 message_start = - 1 message_end = - 1 while 1 : ( tag_bytes , pos ) = local_ReadTag ( buffer , pos ) if tag_bytes == type_id_tag_bytes : ( type_id , pos ) = local_DecodeVarint ( buffer , pos ) elif tag_bytes == message_tag_bytes : ( size , message_start ) = local_DecodeVarint ( buffer , pos ) pos = message_end = message_start + size elif tag_bytes == item_end_tag_bytes : break else : pos = SkipField ( buffer , pos , end , tag_bytes ) if pos == - 1 : raise _DecodeError ( 'Missing group end tag.' ) if pos > end : raise _DecodeError ( 'Truncated message.' ) if type_id == - 1 : raise _DecodeError ( 'MessageSet item missing type_id.' ) if message_start == - 1 : raise _DecodeError ( 'MessageSet item missing message.' ) extension = extensions_by_number . get ( type_id ) if extension is not None : value = field_dict . get ( extension ) if value is None : value = field_dict . setdefault ( extension , extension . message_type . _concrete_class ( ) ) if value . _InternalParse ( buffer , message_start , message_end ) != message_end : raise _DecodeError ( 'Unexpected end-group tag.' ) else : if not message . _unknown_fields : message . _unknown_fields = [ ] message . _unknown_fields . append ( ( MESSAGE_SET_ITEM_TAG , buffer [ message_set_item_start : pos ] ) ) return pos return DecodeItem
6985	def parallel_timebin ( lclist , binsizesec , maxobjects = None , outdir = None , lcformat = 'hat-sql' , lcformatdir = None , timecols = None , magcols = None , errcols = None , minbinelems = 7 , nworkers = NCPUS , maxworkertasks = 1000 ) : if outdir and not os . path . exists ( outdir ) : os . mkdir ( outdir ) if maxobjects is not None : lclist = lclist [ : maxobjects ] tasks = [ ( x , binsizesec , { 'outdir' : outdir , 'lcformat' : lcformat , 'lcformatdir' : lcformatdir , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'minbinelems' : minbinelems } ) for x in lclist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( timebinlc_worker , tasks ) pool . close ( ) pool . join ( ) resdict = { os . path . basename ( x ) : y for ( x , y ) in zip ( lclist , results ) } return resdict
3784	def TP_dependent_property ( self , T , P ) : r if self . method_P : if self . test_method_validity_P ( T , P , self . method_P ) : try : prop = self . calculate_P ( T , P , self . method_P ) if self . test_property_validity ( prop ) : return prop except : pass self . sorted_valid_methods_P = self . select_valid_methods_P ( T , P ) for method_P in self . sorted_valid_methods_P : try : prop = self . calculate_P ( T , P , method_P ) if self . test_property_validity ( prop ) : self . method_P = method_P return prop except : pass return None
4018	def _get_app_libs_volume_mounts ( app_name , assembled_specs ) : volumes = [ ] for lib_name in assembled_specs [ 'apps' ] [ app_name ] [ 'depends' ] [ 'libs' ] : lib_spec = assembled_specs [ 'libs' ] [ lib_name ] volumes . append ( "{}:{}" . format ( Repo ( lib_spec [ 'repo' ] ) . vm_path , container_code_path ( lib_spec ) ) ) return volumes
6010	def load_poisson_noise_map ( poisson_noise_map_path , poisson_noise_map_hdu , pixel_scale , convert_poisson_noise_map_from_weight_map , convert_poisson_noise_map_from_inverse_noise_map , poisson_noise_map_from_image , image , exposure_time_map , convert_from_electrons , gain , convert_from_adus ) : poisson_noise_map_options = sum ( [ convert_poisson_noise_map_from_weight_map , convert_poisson_noise_map_from_inverse_noise_map , poisson_noise_map_from_image ] ) if poisson_noise_map_options == 0 and poisson_noise_map_path is not None : return PoissonNoiseMap . from_fits_with_pixel_scale ( file_path = poisson_noise_map_path , hdu = poisson_noise_map_hdu , pixel_scale = pixel_scale ) elif poisson_noise_map_from_image : if not ( convert_from_electrons or convert_from_adus ) and exposure_time_map is None : raise exc . DataException ( 'Cannot compute the Poisson noise-map from the image if an ' 'exposure-time (or exposure time map) is not supplied to convert to adus' ) if convert_from_adus and gain is None : raise exc . DataException ( 'Cannot compute the Poisson noise-map from the image if a' 'gain is not supplied to convert from adus' ) return PoissonNoiseMap . from_image_and_exposure_time_map ( pixel_scale = pixel_scale , image = image , exposure_time_map = exposure_time_map , convert_from_electrons = convert_from_electrons , gain = gain , convert_from_adus = convert_from_adus ) elif convert_poisson_noise_map_from_weight_map and poisson_noise_map_path is not None : weight_map = Array . from_fits ( file_path = poisson_noise_map_path , hdu = poisson_noise_map_hdu ) return PoissonNoiseMap . from_weight_map ( weight_map = weight_map , pixel_scale = pixel_scale ) elif convert_poisson_noise_map_from_inverse_noise_map and poisson_noise_map_path is not None : inverse_noise_map = Array . from_fits ( file_path = poisson_noise_map_path , hdu = poisson_noise_map_hdu ) return PoissonNoiseMap . from_inverse_noise_map ( inverse_noise_map = inverse_noise_map , pixel_scale = pixel_scale ) else : return None
3650	def num2hex ( self , num ) : temp = '' for i in range ( 0 , 4 ) : x = self . hexChars [ ( num >> ( i * 8 + 4 ) ) & 0x0F ] y = self . hexChars [ ( num >> ( i * 8 ) ) & 0x0F ] temp += ( x + y ) return temp
6554	def flip_variable ( self , v ) : try : idx = self . variables . index ( v ) except ValueError : raise ValueError ( "variable {} is not a variable in constraint {}" . format ( v , self . name ) ) if self . vartype is dimod . BINARY : original_func = self . func def func ( * args ) : new_args = list ( args ) new_args [ idx ] = 1 - new_args [ idx ] return original_func ( * new_args ) self . func = func self . configurations = frozenset ( config [ : idx ] + ( 1 - config [ idx ] , ) + config [ idx + 1 : ] for config in self . configurations ) else : original_func = self . func def func ( * args ) : new_args = list ( args ) new_args [ idx ] = - new_args [ idx ] return original_func ( * new_args ) self . func = func self . configurations = frozenset ( config [ : idx ] + ( - config [ idx ] , ) + config [ idx + 1 : ] for config in self . configurations ) self . name = '{} ({} flipped)' . format ( self . name , v )
1669	def FlagCxx14Features ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] include = Match ( r'\s*#\s*include\s+[<"]([^<"]+)[">]' , line ) if include and include . group ( 1 ) in ( 'scoped_allocator' , 'shared_mutex' ) : error ( filename , linenum , 'build/c++14' , 5 , ( '<%s> is an unapproved C++14 header.' ) % include . group ( 1 ) )
2104	def create ( self , fail_on_found = False , force_on_exists = False , ** kwargs ) : jt_id = kwargs . pop ( 'job_template' , None ) old_endpoint = self . endpoint if jt_id is not None : jt = get_resource ( 'job_template' ) jt . get ( pk = jt_id ) try : label_id = self . get ( name = kwargs . get ( 'name' , None ) , organization = kwargs . get ( 'organization' , None ) ) [ 'id' ] except exc . NotFound : pass else : if fail_on_found : raise exc . TowerCLIError ( 'Label already exists and fail-on-found is switched on. Please use' ' "associate_label" method of job_template instead.' ) else : debug . log ( 'Label already exists, associating with job template.' , header = 'details' ) return jt . associate_label ( job_template = jt_id , label = label_id ) self . endpoint = '/job_templates/%d/labels/' % jt_id result = super ( Resource , self ) . create ( fail_on_found = fail_on_found , force_on_exists = force_on_exists , ** kwargs ) self . endpoint = old_endpoint return result
4835	def get_paginated_catalog_courses ( self , catalog_id , querystring = None ) : return self . _load_data ( self . CATALOGS_COURSES_ENDPOINT . format ( catalog_id ) , default = [ ] , querystring = querystring , traverse_pagination = False , many = False , )
12154	def timeit ( timer = None ) : if timer is None : return time . time ( ) else : took = time . time ( ) - timer if took < 1 : return "%.02f ms" % ( took * 1000.0 ) elif took < 60 : return "%.02f s" % ( took ) else : return "%.02f min" % ( took / 60.0 )
13542	def update ( self , server ) : return server . put ( 'task_admin' , self . as_payload ( ) , replacements = { 'slug' : self . __challenge__ . slug , 'identifier' : self . identifier } )
321	def get_max_drawdown ( returns ) : returns = returns . copy ( ) df_cum = cum_returns ( returns , 1.0 ) running_max = np . maximum . accumulate ( df_cum ) underwater = df_cum / running_max - 1 return get_max_drawdown_underwater ( underwater )
6341	def sim ( self , src , tar ) : if src == tar : return 1.0 if not src or not tar : return 0.0 min_word , max_word = ( src , tar ) if len ( src ) < len ( tar ) else ( tar , src ) min_len = len ( min_word ) for i in range ( min_len , 0 , - 1 ) : if min_word [ : i ] == max_word [ : i ] : return i / min_len return 0.0
6073	def mass_within_circle_in_units ( self , radius , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : if self . has_mass_profile : return sum ( map ( lambda p : p . mass_within_circle_in_units ( radius = radius , unit_mass = unit_mass , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) , self . mass_profiles ) ) else : return None
430	def read_images ( img_list , path = '' , n_threads = 10 , printable = True ) : imgs = [ ] for idx in range ( 0 , len ( img_list ) , n_threads ) : b_imgs_list = img_list [ idx : idx + n_threads ] b_imgs = tl . prepro . threading_data ( b_imgs_list , fn = read_image , path = path ) imgs . extend ( b_imgs ) if printable : tl . logging . info ( 'read %d from %s' % ( len ( imgs ) , path ) ) return imgs
3131	def merge_results ( x , y ) : z = x . copy ( ) for key , value in y . items ( ) : if isinstance ( value , list ) and isinstance ( z . get ( key ) , list ) : z [ key ] += value else : z [ key ] = value return z
351	def download_file_from_google_drive ( ID , destination ) : def save_response_content ( response , destination , chunk_size = 32 * 1024 ) : total_size = int ( response . headers . get ( 'content-length' , 0 ) ) with open ( destination , "wb" ) as f : for chunk in tqdm ( response . iter_content ( chunk_size ) , total = total_size , unit = 'B' , unit_scale = True , desc = destination ) : if chunk : f . write ( chunk ) def get_confirm_token ( response ) : for key , value in response . cookies . items ( ) : if key . startswith ( 'download_warning' ) : return value return None URL = "https://docs.google.com/uc?export=download" session = requests . Session ( ) response = session . get ( URL , params = { 'id' : ID } , stream = True ) token = get_confirm_token ( response ) if token : params = { 'id' : ID , 'confirm' : token } response = session . get ( URL , params = params , stream = True ) save_response_content ( response , destination )
1408	def _is_continue_to_work ( self ) : if not self . _is_topology_running ( ) : return False max_spout_pending = self . pplan_helper . context . get_cluster_config ( ) . get ( api_constants . TOPOLOGY_MAX_SPOUT_PENDING ) if not self . acking_enabled and self . output_helper . is_out_queue_available ( ) : return True elif self . acking_enabled and self . output_helper . is_out_queue_available ( ) and len ( self . in_flight_tuples ) < max_spout_pending : return True elif self . acking_enabled and not self . in_stream . is_empty ( ) : return True else : return False
5371	def load_file ( file_path , credentials = None ) : if file_path . startswith ( 'gs://' ) : return _load_file_from_gcs ( file_path , credentials ) else : return open ( file_path , 'r' )
9907	def send_confirmation ( self ) : confirmation = EmailConfirmation . objects . create ( email = self ) confirmation . send ( )
5976	def total_regular_pixels_from_mask ( mask ) : total_regular_pixels = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : total_regular_pixels += 1 return total_regular_pixels
7769	def _stream_disconnected ( self , event ) : with self . lock : if event . stream != self . stream : return if self . stream is not None and event . stream == self . stream : if self . stream . transport in self . _ml_handlers : self . _ml_handlers . remove ( self . stream . transport ) self . main_loop . remove_handler ( self . stream . transport ) self . stream = None self . uplink = None
6137	def add_model_file ( self , model_fpath , position = 1 , file_id = None ) : if file_id is None : file_id = self . make_unique_id ( 'file_input' ) ret_data = self . file_create ( File . from_file ( model_fpath , position , file_id ) ) return ret_data
3055	def from_string ( cls , key , password = 'notasecret' ) : key = _helpers . _from_bytes ( key ) marker_id , key_bytes = pem . readPemBlocksFromFile ( six . StringIO ( key ) , _PKCS1_MARKER , _PKCS8_MARKER ) if marker_id == 0 : pkey = rsa . key . PrivateKey . load_pkcs1 ( key_bytes , format = 'DER' ) elif marker_id == 1 : key_info , remaining = decoder . decode ( key_bytes , asn1Spec = _PKCS8_SPEC ) if remaining != b'' : raise ValueError ( 'Unused bytes' , remaining ) pkey_info = key_info . getComponentByName ( 'privateKey' ) pkey = rsa . key . PrivateKey . load_pkcs1 ( pkey_info . asOctets ( ) , format = 'DER' ) else : raise ValueError ( 'No key could be detected.' ) return cls ( pkey )
4114	def rc2lar ( k ) : assert numpy . isrealobj ( k ) , 'Log area ratios not defined for complex reflection coefficients.' if max ( numpy . abs ( k ) ) >= 1 : raise ValueError ( 'All reflection coefficients should have magnitude less than unity.' ) return - 2 * numpy . arctanh ( - numpy . array ( k ) )
6032	def grid_stack_from_mask_sub_grid_size_and_psf_shape ( cls , mask , sub_grid_size , psf_shape ) : regular_grid = RegularGrid . from_mask ( mask ) sub_grid = SubGrid . from_mask_and_sub_grid_size ( mask , sub_grid_size ) blurring_grid = RegularGrid . blurring_grid_from_mask_and_psf_shape ( mask , psf_shape ) return GridStack ( regular_grid , sub_grid , blurring_grid )
5849	def list_files ( self , dataset_id , glob = "." , is_dir = False ) : data = { "list" : { "glob" : glob , "isDir" : is_dir } } return self . _get_success_json ( self . _post_json ( routes . list_files ( dataset_id ) , data , failure_message = "Failed to list files for dataset {}" . format ( dataset_id ) ) ) [ 'files' ]
10279	def neurommsig_topology ( graph : BELGraph , nodes : List [ BaseEntity ] ) -> float : nodes = list ( nodes ) number_nodes = len ( nodes ) if number_nodes <= 1 : return 0.0 unnormalized_sum = sum ( u in graph [ v ] for u , v in itt . product ( nodes , repeat = 2 ) if v in graph and u != v ) return unnormalized_sum / ( number_nodes * ( number_nodes - 1.0 ) )
369	def crop_multi ( x , wrg , hrg , is_random = False , row_index = 0 , col_index = 1 ) : h , w = x [ 0 ] . shape [ row_index ] , x [ 0 ] . shape [ col_index ] if ( h < hrg ) or ( w < wrg ) : raise AssertionError ( "The size of cropping should smaller than or equal to the original image" ) if is_random : h_offset = int ( np . random . uniform ( 0 , h - hrg ) ) w_offset = int ( np . random . uniform ( 0 , w - wrg ) ) results = [ ] for data in x : results . append ( data [ h_offset : hrg + h_offset , w_offset : wrg + w_offset ] ) return np . asarray ( results ) else : h_offset = ( h - hrg ) / 2 w_offset = ( w - wrg ) / 2 results = [ ] for data in x : results . append ( data [ h_offset : h - h_offset , w_offset : w - w_offset ] ) return np . asarray ( results )
7212	def layers ( self ) : layers = [ self . _layer_def ( style ) for style in self . styles ] return layers
6721	def get_combined_requirements ( self , requirements = None ) : requirements = requirements or self . env . requirements def iter_lines ( fn ) : with open ( fn , 'r' ) as fin : for line in fin . readlines ( ) : line = line . strip ( ) if not line or line . startswith ( '#' ) : continue yield line content = [ ] if isinstance ( requirements , ( tuple , list ) ) : for f in requirements : f = self . find_template ( f ) content . extend ( list ( iter_lines ( f ) ) ) else : assert isinstance ( requirements , six . string_types ) f = self . find_template ( requirements ) content . extend ( list ( iter_lines ( f ) ) ) return '\n' . join ( content )
13569	def selected_exercise ( func ) : @ wraps ( func ) def inner ( * args , ** kwargs ) : exercise = Exercise . get_selected ( ) return func ( exercise , * args , ** kwargs ) return inner
3010	def _get_scopes ( self ) : if _credentials_from_request ( self . request ) : return ( self . _scopes | _credentials_from_request ( self . request ) . scopes ) else : return self . _scopes
2047	def has_storage ( self , address ) : storage = self . _world_state [ address ] [ 'storage' ] array = storage . array while not isinstance ( array , ArrayVariable ) : if isinstance ( array , ArrayStore ) : return True array = array . array return False
5030	def get_learner_data_records ( self , enterprise_enrollment , completed_date = None , is_passing = False , ** kwargs ) : completed_timestamp = completed_date . strftime ( "%F" ) if isinstance ( completed_date , datetime ) else None if enterprise_enrollment . enterprise_customer_user . get_remote_id ( ) is not None : DegreedLearnerDataTransmissionAudit = apps . get_model ( 'degreed' , 'DegreedLearnerDataTransmissionAudit' ) return [ DegreedLearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , degreed_user_email = enterprise_enrollment . enterprise_customer_user . user_email , course_id = parse_course_key ( enterprise_enrollment . course_id ) , course_completed = completed_date is not None and is_passing , completed_timestamp = completed_timestamp , ) , DegreedLearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , degreed_user_email = enterprise_enrollment . enterprise_customer_user . user_email , course_id = enterprise_enrollment . course_id , course_completed = completed_date is not None and is_passing , completed_timestamp = completed_timestamp , ) ] else : LOGGER . debug ( 'No learner data was sent for user [%s] because a Degreed user ID could not be found.' , enterprise_enrollment . enterprise_customer_user . username )
13820	def _ConvertListValueMessage ( value , message ) : if not isinstance ( value , list ) : raise ParseError ( 'ListValue must be in [] which is {0}.' . format ( value ) ) message . ClearField ( 'values' ) for item in value : _ConvertValueMessage ( item , message . values . add ( ) )
10524	def login ( self , username = None , password = None , android_id = None ) : cls_name = type ( self ) . __name__ if username is None : username = input ( "Enter your Google username or email address: " ) if password is None : password = getpass . getpass ( "Enter your Google Music password: " ) if android_id is None : android_id = Mobileclient . FROM_MAC_ADDRESS try : self . api . login ( username , password , android_id ) except OSError : logger . exception ( "{} authentication failed." . format ( cls_name ) ) if not self . is_authenticated : logger . warning ( "{} authentication failed." . format ( cls_name ) ) return False logger . info ( "{} authentication succeeded.\n" . format ( cls_name ) ) return True
10447	def activatewindow ( self , window_name ) : window_handle = self . _get_window_handle ( window_name ) self . _grabfocus ( window_handle ) return 1
8010	def webhook_handler ( * event_types ) : event_types_to_register = set ( ) for event_type in event_types : event_type = event_type . lower ( ) if "*" in event_type : for t in WEBHOOK_EVENT_TYPES : if fnmatch ( t , event_type ) : event_types_to_register . add ( t ) elif event_type not in WEBHOOK_EVENT_TYPES : raise ValueError ( "Unknown webhook event: %r" % ( event_type ) ) else : event_types_to_register . add ( event_type ) def decorator ( func ) : for event_type in event_types_to_register : WEBHOOK_SIGNALS [ event_type ] . connect ( func ) return func return decorator
6745	def iter_sites ( sites = None , site = None , renderer = None , setter = None , no_secure = False , verbose = None ) : if verbose is None : verbose = get_verbose ( ) hostname = get_current_hostname ( ) target_sites = env . available_sites_by_host . get ( hostname , None ) if sites is None : site = site or env . SITE or ALL if site == ALL : sites = list ( six . iteritems ( env . sites ) ) else : sys . stderr . flush ( ) sites = [ ( site , env . sites . get ( site ) ) ] renderer = renderer env_default = save_env ( ) for _site , site_data in sorted ( sites ) : if no_secure and _site . endswith ( '_secure' ) : continue if target_sites is None : pass else : assert isinstance ( target_sites , ( tuple , list ) ) if _site not in target_sites : if verbose : print ( 'Skipping site %s because not in among target sites.' % _site ) continue env . update ( env_default ) env . update ( env . sites . get ( _site , { } ) ) env . SITE = _site if callable ( renderer ) : renderer ( ) if setter : setter ( _site ) yield _site , site_data env . update ( env_default ) added_keys = set ( env ) . difference ( env_default ) for key in added_keys : if key . startswith ( '_' ) : continue del env [ key ]
3323	def set_last_modified ( self , dest_path , time_stamp , dry_run ) : secs = util . parse_time_string ( time_stamp ) if not dry_run : os . utime ( self . _file_path , ( secs , secs ) ) return True
2966	def _sm_to_pain ( self , * args , ** kwargs ) : _logger . info ( "Starting chaos for blockade %s" % self . _blockade_name ) self . _do_blockade_event ( ) millisec = random . randint ( self . _run_min_time , self . _run_max_time ) self . _timer = threading . Timer ( millisec / 1000.0 , self . event_timeout ) self . _timer . start ( )
11366	def _do_unzip ( zipped_file , output_directory ) : z = zipfile . ZipFile ( zipped_file ) for path in z . namelist ( ) : relative_path = os . path . join ( output_directory , path ) dirname , dummy = os . path . split ( relative_path ) try : if relative_path . endswith ( os . sep ) and not os . path . exists ( dirname ) : os . makedirs ( relative_path ) elif not os . path . exists ( relative_path ) : dirname = os . path . join ( output_directory , os . path . dirname ( path ) ) if os . path . dirname ( path ) and not os . path . exists ( dirname ) : os . makedirs ( dirname ) fd = open ( relative_path , "w" ) fd . write ( z . read ( path ) ) fd . close ( ) except IOError , e : raise e return output_directory
85	def ContrastNormalization ( alpha = 1.0 , per_channel = False , name = None , deterministic = False , random_state = None ) : from . import contrast as contrast_lib return contrast_lib . LinearContrast ( alpha = alpha , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
2280	def create_graph_from_data ( self , data , ** kwargs ) : self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) results = self . _run_ccdr ( data , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
11925	def parse ( self , source ) : rt , title , title_pic , markdown = libparser . parse ( source ) if rt == - 1 : raise SeparatorNotFound elif rt == - 2 : raise PostTitleNotFound title , title_pic , markdown = map ( to_unicode , ( title , title_pic , markdown ) ) html = self . markdown . render ( markdown ) summary = self . markdown . render ( markdown [ : 200 ] ) return { 'title' : title , 'markdown' : markdown , 'html' : html , 'summary' : summary , 'title_pic' : title_pic }
4081	def get_directory ( ) : try : language_check_dir = cache [ 'language_check_dir' ] except KeyError : def version_key ( string ) : return [ int ( e ) if e . isdigit ( ) else e for e in re . split ( r"(\d+)" , string ) ] def get_lt_dir ( base_dir ) : paths = [ path for path in glob . glob ( os . path . join ( base_dir , 'LanguageTool*' ) ) if os . path . isdir ( path ) ] return max ( paths , key = version_key ) if paths else None base_dir = os . path . dirname ( sys . argv [ 0 ] ) language_check_dir = get_lt_dir ( base_dir ) if not language_check_dir : try : base_dir = os . path . dirname ( os . path . abspath ( __file__ ) ) except NameError : pass else : language_check_dir = get_lt_dir ( base_dir ) if not language_check_dir : raise PathError ( "can't find LanguageTool directory in {!r}" . format ( base_dir ) ) cache [ 'language_check_dir' ] = language_check_dir return language_check_dir
8668	def lock_key ( key_name , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Locking key...' ) stash . lock ( key_name = key_name ) click . echo ( 'Key locked successfully' ) except GhostError as ex : sys . exit ( ex )
808	def handleLogOutput ( self , output ) : if self . _tapFileOut is not None : for k in range ( len ( output ) ) : print >> self . _tapFileOut , output [ k ] , print >> self . _tapFileOut
6500	def perform_search ( search_term , user = None , size = 10 , from_ = 0 , course_id = None ) : ( field_dictionary , filter_dictionary , exclude_dictionary ) = SearchFilterGenerator . generate_field_filters ( user = user , course_id = course_id ) searcher = SearchEngine . get_search_engine ( getattr ( settings , "COURSEWARE_INDEX_NAME" , "courseware_index" ) ) if not searcher : raise NoSearchEngineError ( "No search engine specified in settings.SEARCH_ENGINE" ) results = searcher . search_string ( search_term , field_dictionary = field_dictionary , filter_dictionary = filter_dictionary , exclude_dictionary = exclude_dictionary , size = size , from_ = from_ , doc_type = "courseware_content" , ) for result in results [ "results" ] : result [ "data" ] = SearchResultProcessor . process_result ( result [ "data" ] , search_term , user ) results [ "access_denied_count" ] = len ( [ r for r in results [ "results" ] if r [ "data" ] is None ] ) results [ "results" ] = [ r for r in results [ "results" ] if r [ "data" ] is not None ] return results
11597	def _rc_dbsize ( self ) : "Returns the number of keys in the current database" result = 0 for alias , redisent in iteritems ( self . redises ) : if alias . find ( '_slave' ) == - 1 : continue result += redisent . dbsize ( ) return result
8369	def create_canvas ( src , format = None , outputfile = None , multifile = False , buff = None , window = False , title = None , fullscreen = None , show_vars = False ) : from core import CairoCanvas , CairoImageSink if outputfile : sink = CairoImageSink ( outputfile , format , multifile , buff ) elif window or show_vars : from gui import ShoebotWindow if not title : if src and os . path . isfile ( src ) : title = os . path . splitext ( os . path . basename ( src ) ) [ 0 ] + ' - Shoebot' else : title = 'Untitled - Shoebot' sink = ShoebotWindow ( title , show_vars , fullscreen = fullscreen ) else : if src and isinstance ( src , cairo . Surface ) : outputfile = src format = 'surface' elif src and os . path . isfile ( src ) : outputfile = os . path . splitext ( os . path . basename ( src ) ) [ 0 ] + '.' + ( format or 'svg' ) else : outputfile = 'output.svg' sink = CairoImageSink ( outputfile , format , multifile , buff ) canvas = CairoCanvas ( sink ) return canvas
12498	def xfm_atlas_to_functional ( atlas_filepath , anatbrain_filepath , meanfunc_filepath , atlas2anat_nonlin_xfm_filepath , is_atlas2anat_inverted , anat2func_lin_xfm_filepath , atlasinanat_out_filepath , atlasinfunc_out_filepath , interp = 'nn' , rewrite = True , parallel = False ) : if is_atlas2anat_inverted : anat_to_mni_nl_inv = atlas2anat_nonlin_xfm_filepath else : output_dir = op . abspath ( op . dirname ( atlasinanat_out_filepath ) ) ext = get_extension ( atlas2anat_nonlin_xfm_filepath ) anat_to_mni_nl_inv = op . join ( output_dir , remove_ext ( op . basename ( atlas2anat_nonlin_xfm_filepath ) ) + '_inv' + ext ) invwarp_cmd = op . join ( '${FSLDIR}' , 'bin' , 'invwarp' ) applywarp_cmd = op . join ( '${FSLDIR}' , 'bin' , 'applywarp' ) fslsub_cmd = op . join ( '${FSLDIR}' , 'bin' , 'fsl_sub' ) if parallel : invwarp_cmd = fslsub_cmd + ' ' + invwarp_cmd applywarp_cmd = fslsub_cmd + ' ' + applywarp_cmd if rewrite or ( not is_atlas2anat_inverted and not op . exists ( anat_to_mni_nl_inv ) ) : log . debug ( 'Creating {}.\n' . format ( anat_to_mni_nl_inv ) ) cmd = invwarp_cmd + ' ' cmd += '-w {} ' . format ( atlas2anat_nonlin_xfm_filepath ) cmd += '-o {} ' . format ( anat_to_mni_nl_inv ) cmd += '-r {} ' . format ( anatbrain_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd ) if rewrite or not op . exists ( atlasinanat_out_filepath ) : log . debug ( 'Creating {}.\n' . format ( atlasinanat_out_filepath ) ) cmd = applywarp_cmd + ' ' cmd += '--in={} ' . format ( atlas_filepath ) cmd += '--ref={} ' . format ( anatbrain_filepath ) cmd += '--warp={} ' . format ( anat_to_mni_nl_inv ) cmd += '--interp={} ' . format ( interp ) cmd += '--out={} ' . format ( atlasinanat_out_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd ) if rewrite or not op . exists ( atlasinfunc_out_filepath ) : log . debug ( 'Creating {}.\n' . format ( atlasinfunc_out_filepath ) ) cmd = applywarp_cmd + ' ' cmd += '--in={} ' . format ( atlasinanat_out_filepath ) cmd += '--ref={} ' . format ( meanfunc_filepath ) cmd += '--premat={} ' . format ( anat2func_lin_xfm_filepath ) cmd += '--interp={} ' . format ( interp ) cmd += '--out={} ' . format ( atlasinfunc_out_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd )
8659	def filter_by ( zips = _zips , ** kwargs ) : return [ z for z in zips if all ( [ k in z and z [ k ] == v for k , v in kwargs . items ( ) ] ) ]
6865	def get_time_flux_errs_from_Ames_lightcurve ( infile , lctype , cadence_min = 2 ) : warnings . warn ( "Use the astrotess.read_tess_fitslc and " "astrotess.consolidate_tess_fitslc functions instead of this function. " "This function will be removed in astrobase v0.4.2." , FutureWarning ) if lctype not in ( 'PDCSAP' , 'SAP' ) : raise ValueError ( 'unknown light curve type requested: %s' % lctype ) hdulist = pyfits . open ( infile ) main_hdr = hdulist [ 0 ] . header lc_hdr = hdulist [ 1 ] . header lc = hdulist [ 1 ] . data if ( ( 'Ames' not in main_hdr [ 'ORIGIN' ] ) or ( 'LIGHTCURVE' not in lc_hdr [ 'EXTNAME' ] ) ) : raise ValueError ( 'could not understand input LC format. ' 'Is it a TESS TOI LC file?' ) time = lc [ 'TIME' ] flux = lc [ '{:s}_FLUX' . format ( lctype ) ] err_flux = lc [ '{:s}_FLUX_ERR' . format ( lctype ) ] sel = ( lc [ 'QUALITY' ] == 0 ) sel &= np . isfinite ( time ) sel &= np . isfinite ( flux ) sel &= np . isfinite ( err_flux ) sel &= ~ np . isnan ( time ) sel &= ~ np . isnan ( flux ) sel &= ~ np . isnan ( err_flux ) sel &= ( time != 0 ) sel &= ( flux != 0 ) sel &= ( err_flux != 0 ) time = time [ sel ] flux = flux [ sel ] err_flux = err_flux [ sel ] lc_cadence_diff = np . abs ( np . nanmedian ( np . diff ( time ) ) * 24 * 60 - cadence_min ) if lc_cadence_diff > 1.0e-2 : raise ValueError ( 'the light curve is not at the required cadence specified: %.2f' % cadence_min ) fluxmedian = np . nanmedian ( flux ) flux /= fluxmedian err_flux /= fluxmedian return time , flux , err_flux
9128	def store_populate ( cls , resource : str , session : Optional [ Session ] = None ) -> 'Action' : action = cls . make_populate ( resource ) _store_helper ( action , session = session ) return action
10375	def calculate_concordance_helper ( graph : BELGraph , key : str , cutoff : Optional [ float ] = None , ) -> Tuple [ int , int , int , int ] : scores = defaultdict ( int ) for u , v , k , d in graph . edges ( keys = True , data = True ) : c = edge_concords ( graph , u , v , k , d , key , cutoff = cutoff ) scores [ c ] += 1 return ( scores [ Concordance . correct ] , scores [ Concordance . incorrect ] , scores [ Concordance . ambiguous ] , scores [ Concordance . unassigned ] , )
222	def is_not_modified ( self , response_headers : Headers , request_headers : Headers ) -> bool : try : if_none_match = request_headers [ "if-none-match" ] etag = response_headers [ "etag" ] if if_none_match == etag : return True except KeyError : pass try : if_modified_since = parsedate ( request_headers [ "if-modified-since" ] ) last_modified = parsedate ( response_headers [ "last-modified" ] ) if ( if_modified_since is not None and last_modified is not None and if_modified_since >= last_modified ) : return True except KeyError : pass return False
12622	def have_same_shape ( array1 , array2 , nd_to_check = None ) : shape1 = array1 . shape shape2 = array2 . shape if nd_to_check is not None : if len ( shape1 ) < nd_to_check : msg = 'Number of dimensions to check {} is out of bounds for the shape of the first image: \n{}\n.' . format ( shape1 ) raise ValueError ( msg ) elif len ( shape2 ) < nd_to_check : msg = 'Number of dimensions to check {} is out of bounds for the shape of the second image: \n{}\n.' . format ( shape2 ) raise ValueError ( msg ) shape1 = shape1 [ : nd_to_check ] shape2 = shape2 [ : nd_to_check ] return shape1 == shape2
4486	def remove ( self ) : response = self . _delete ( self . _delete_url ) if response . status_code != 204 : raise RuntimeError ( 'Could not delete {}.' . format ( self . path ) )
8849	def setup_actions ( self ) : self . actionOpen . triggered . connect ( self . on_open ) self . actionNew . triggered . connect ( self . on_new ) self . actionSave . triggered . connect ( self . on_save ) self . actionSave_as . triggered . connect ( self . on_save_as ) self . actionQuit . triggered . connect ( QtWidgets . QApplication . instance ( ) . quit ) self . tabWidget . current_changed . connect ( self . on_current_tab_changed ) self . tabWidget . last_tab_closed . connect ( self . on_last_tab_closed ) self . actionAbout . triggered . connect ( self . on_about ) self . actionRun . triggered . connect ( self . on_run ) self . interactiveConsole . process_finished . connect ( self . on_process_finished ) self . actionConfigure_run . triggered . connect ( self . on_configure_run )
11453	def convert_all ( cls , records ) : out = [ "<collection>" ] for rec in records : conversion = cls ( rec ) out . append ( conversion . convert ( ) ) out . append ( "</collection>" ) return "\n" . join ( out )
1387	def set_execution_state ( self , execution_state ) : if not execution_state : self . execution_state = None self . cluster = None self . environ = None else : self . execution_state = execution_state cluster , environ = self . get_execution_state_dc_environ ( execution_state ) self . cluster = cluster self . environ = environ self . zone = cluster self . trigger_watches ( )
5343	def compose_mbox ( projects ) : mbox_archives = '/home/bitergia/mboxes' mailing_lists_projects = [ project for project in projects if 'mailing_lists' in projects [ project ] ] for mailing_lists in mailing_lists_projects : projects [ mailing_lists ] [ 'mbox' ] = [ ] for mailing_list in projects [ mailing_lists ] [ 'mailing_lists' ] : if 'listinfo' in mailing_list : name = mailing_list . split ( 'listinfo/' ) [ 1 ] elif 'mailing-list' in mailing_list : name = mailing_list . split ( 'mailing-list/' ) [ 1 ] else : name = mailing_list . split ( '@' ) [ 0 ] list_new = "%s %s/%s.mbox/%s.mbox" % ( name , mbox_archives , name , name ) projects [ mailing_lists ] [ 'mbox' ] . append ( list_new ) return projects
13034	def write_triples ( filename , triples , delimiter = DEFAULT_DELIMITER , triple_order = "hrt" ) : with open ( filename , 'w' ) as f : for t in triples : line = t . serialize ( delimiter , triple_order ) f . write ( line + "\n" )
2262	def find_duplicates ( items , k = 2 , key = None ) : duplicates = defaultdict ( list ) if key is None : for count , item in enumerate ( items ) : duplicates [ item ] . append ( count ) else : for count , item in enumerate ( items ) : duplicates [ key ( item ) ] . append ( count ) for key in list ( duplicates . keys ( ) ) : if len ( duplicates [ key ] ) < k : del duplicates [ key ] duplicates = dict ( duplicates ) return duplicates
3243	def get_rules ( security_group , ** kwargs ) : rules = security_group . pop ( 'security_group_rules' , [ ] ) for rule in rules : rule [ 'ip_protocol' ] = rule . pop ( 'protocol' ) rule [ 'from_port' ] = rule . pop ( 'port_range_max' ) rule [ 'to_port' ] = rule . pop ( 'port_range_min' ) rule [ 'cidr_ip' ] = rule . pop ( 'remote_ip_prefix' ) rule [ 'rule_type' ] = rule . pop ( 'direction' ) security_group [ 'rules' ] = sorted ( rules ) return security_group
12776	def forward_dynamics ( self , torques , start = 0 , states = None ) : if states is not None : self . skeleton . set_body_states ( states ) for frame_no , torque in enumerate ( torques ) : if frame_no < start : continue if frame_no >= end : break self . ode_space . collide ( None , self . on_collision ) self . skeleton . add_torques ( torque ) self . ode_world . step ( self . dt ) yield self . ode_contactgroup . empty ( )
4496	def login ( self , username , password = None , token = None ) : self . session . basic_auth ( username , password )
13556	def all_comments ( self ) : ctype = ContentType . objects . get ( app_label__exact = "happenings" , model__exact = 'event' ) update_ctype = ContentType . objects . get ( app_label__exact = "happenings" , model__exact = 'update' ) update_ids = self . update_set . values_list ( 'id' , flat = True ) return Comment . objects . filter ( Q ( content_type = ctype . id , object_pk = self . id ) | Q ( content_type = update_ctype . id , object_pk__in = update_ids ) )
12791	def delete ( self , url = None , post_data = { } , parse_data = False , key = None , parameters = None ) : return self . _fetch ( "DELETE" , url , post_data = post_data , parse_data = parse_data , key = key , parameters = parameters , full_return = True )
10384	def remove_inconsistent_edges ( graph : BELGraph ) -> None : for u , v in get_inconsistent_edges ( graph ) : edges = [ ( u , v , k ) for k in graph [ u ] [ v ] ] graph . remove_edges_from ( edges )
10238	def count_citations ( graph : BELGraph , ** annotations ) -> Counter : citations = defaultdict ( set ) annotation_dict_filter = build_edge_data_filter ( annotations ) for u , v , _ , d in filter_edges ( graph , annotation_dict_filter ) : if CITATION not in d : continue citations [ u , v ] . add ( ( d [ CITATION ] [ CITATION_TYPE ] , d [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) ) ) return Counter ( itt . chain . from_iterable ( citations . values ( ) ) )
2452	def set_pkg_home ( self , doc , location ) : self . assert_package_exists ( ) if not self . package_home_set : self . package_home_set = True if validations . validate_pkg_homepage ( location ) : doc . package . homepage = location return True else : raise SPDXValueError ( 'Package::HomePage' ) else : raise CardinalityError ( 'Package::HomePage' )
3126	def get ( self , template_id , ** queryparams ) : self . template_id = template_id return self . _mc_client . _get ( url = self . _build_path ( template_id ) , ** queryparams )
5815	def _read_callback ( connection_id , data_buffer , data_length_pointer ) : self = None try : self = _connection_refs . get ( connection_id ) if not self : socket = _socket_refs . get ( connection_id ) else : socket = self . _socket if not self and not socket : return 0 bytes_requested = deref ( data_length_pointer ) timeout = socket . gettimeout ( ) error = None data = b'' try : while len ( data ) < bytes_requested : if timeout is not None and timeout > 0.0 : read_ready , _ , _ = select . select ( [ socket ] , [ ] , [ ] , timeout ) if len ( read_ready ) == 0 : raise socket_ . error ( errno . EAGAIN , 'timed out' ) chunk = socket . recv ( bytes_requested - len ( data ) ) data += chunk if chunk == b'' : if len ( data ) == 0 : if timeout is None : return SecurityConst . errSSLClosedNoNotify return SecurityConst . errSSLClosedAbort break except ( socket_ . error ) as e : error = e . errno if error is not None and error != errno . EAGAIN : if error == errno . ECONNRESET or error == errno . EPIPE : return SecurityConst . errSSLClosedNoNotify return SecurityConst . errSSLClosedAbort if self and not self . _done_handshake : if len ( data ) >= 3 and len ( self . _server_hello ) == 0 : valid_record_type = data [ 0 : 1 ] in set ( [ b'\x15' , b'\x16' ] ) valid_protocol_version = data [ 1 : 3 ] in set ( [ b'\x03\x00' , b'\x03\x01' , b'\x03\x02' , b'\x03\x03' , b'\x03\x04' ] ) if not valid_record_type or not valid_protocol_version : self . _server_hello += data + _read_remaining ( socket ) return SecurityConst . errSSLProtocol self . _server_hello += data write_to_buffer ( data_buffer , data ) pointer_set ( data_length_pointer , len ( data ) ) if len ( data ) != bytes_requested : return SecurityConst . errSSLWouldBlock return 0 except ( KeyboardInterrupt ) as e : if self : self . _exception = e return SecurityConst . errSSLClosedAbort
5857	def get_available_columns ( self , dataset_ids ) : if not isinstance ( dataset_ids , list ) : dataset_ids = [ dataset_ids ] data = { "dataset_ids" : dataset_ids } failure_message = "Failed to get available columns in dataset(s) {}" . format ( dataset_ids ) return self . _get_success_json ( self . _post_json ( 'v1/datasets/get-available-columns' , data , failure_message = failure_message ) ) [ 'data' ]
9240	def fetch_events_for_issues_and_pr ( self ) : self . fetcher . fetch_events_async ( self . issues , "issues" ) self . fetcher . fetch_events_async ( self . pull_requests , "pull requests" )
11852	def parse ( self , words , S = 'S' ) : self . chart = [ [ ] for i in range ( len ( words ) + 1 ) ] self . add_edge ( [ 0 , 0 , 'S_' , [ ] , [ S ] ] ) for i in range ( len ( words ) ) : self . scanner ( i , words [ i ] ) return self . chart
8231	def speed ( self , framerate = None ) : if framerate is not None : self . _speed = framerate self . _dynamic = True else : return self . _speed
2762	def get_all_certificates ( self ) : data = self . get_data ( "certificates" ) certificates = list ( ) for jsoned in data [ 'certificates' ] : cert = Certificate ( ** jsoned ) cert . token = self . token certificates . append ( cert ) return certificates
13552	def _post_resource ( self , url , body ) : headers = { "Content-Type" : "application/json" , "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = WhenIWork_DAO ( ) . postURL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 204 ) : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
1600	def str_cmd ( cmd , cwd , env ) : process = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE , cwd = cwd , env = env ) stdout_builder , stderr_builder = proc . async_stdout_stderr_builder ( process ) process . wait ( ) stdout , stderr = stdout_builder . result ( ) , stderr_builder . result ( ) return { 'command' : ' ' . join ( cmd ) , 'stderr' : stderr , 'stdout' : stdout }
604	def _addBase ( self , position , xlabel = None , ylabel = None ) : ax = self . _fig . add_subplot ( position ) ax . set_xlabel ( xlabel ) ax . set_ylabel ( ylabel ) return ax
7175	def main ( src , pyi_dir , target_dir , incremental , quiet , replace_any , hg , traceback ) : Config . incremental = incremental Config . replace_any = replace_any returncode = 0 for src_entry in src : for file , error , exc_type , tb in retype_path ( Path ( src_entry ) , pyi_dir = Path ( pyi_dir ) , targets = Path ( target_dir ) , src_explicitly_given = True , quiet = quiet , hg = hg , ) : print ( f'error: {file}: {error}' , file = sys . stderr ) if traceback : print ( 'Traceback (most recent call last):' , file = sys . stderr ) for line in tb : print ( line , file = sys . stderr , end = '' ) print ( f'{exc_type.__name__}: {error}' , file = sys . stderr ) returncode += 1 if not src and not quiet : print ( 'warning: no sources given' , file = sys . stderr ) sys . exit ( min ( returncode , 125 ) )
4026	def _get_host_only_ip ( ) : mac = _get_host_only_mac_address ( ) ip_addr_show = check_output_demoted ( [ 'ssh' , '-o' , 'StrictHostKeyChecking=no' , '-o' , 'UserKnownHostsFile=/dev/null' , '-i' , _vm_key_path ( ) , '-p' , _get_localhost_ssh_port ( ) , 'docker@127.0.0.1' , 'ip addr show' ] ) return _ip_for_mac_from_ip_addr_show ( ip_addr_show , mac )
1485	def run ( self , name , config , builder ) : if not isinstance ( name , str ) : raise RuntimeError ( "Name has to be a string type" ) if not isinstance ( config , Config ) : raise RuntimeError ( "config has to be a Config type" ) if not isinstance ( builder , Builder ) : raise RuntimeError ( "builder has to be a Builder type" ) bldr = TopologyBuilder ( name = name ) builder . build ( bldr ) bldr . set_config ( config . _api_config ) bldr . build_and_submit ( )
354	def load_and_assign_npz ( sess = None , name = None , network = None ) : if network is None : raise ValueError ( "network is None." ) if sess is None : raise ValueError ( "session is None." ) if not os . path . exists ( name ) : logging . error ( "file {} doesn't exist." . format ( name ) ) return False else : params = load_npz ( name = name ) assign_params ( sess , params , network ) logging . info ( "[*] Load {} SUCCESS!" . format ( name ) ) return network
7920	def __prepare_local ( data ) : if not data : return None data = unicode ( data ) try : local = NODEPREP . prepare ( data ) except StringprepError , err : raise JIDError ( u"Local part invalid: {0}" . format ( err ) ) if len ( local . encode ( "utf-8" ) ) > 1023 : raise JIDError ( u"Local part too long" ) return local
221	async def check_config ( self ) -> None : if self . directory is None : return try : stat_result = await aio_stat ( self . directory ) except FileNotFoundError : raise RuntimeError ( f"StaticFiles directory '{self.directory}' does not exist." ) if not ( stat . S_ISDIR ( stat_result . st_mode ) or stat . S_ISLNK ( stat_result . st_mode ) ) : raise RuntimeError ( f"StaticFiles path '{self.directory}' is not a directory." )
391	def keypoint_random_flip ( image , annos , mask = None , prob = 0.5 , flip_list = ( 0 , 1 , 5 , 6 , 7 , 2 , 3 , 4 , 11 , 12 , 13 , 8 , 9 , 10 , 15 , 14 , 17 , 16 , 18 ) ) : _prob = np . random . uniform ( 0 , 1.0 ) if _prob < prob : return image , annos , mask _ , width , _ = np . shape ( image ) image = cv2 . flip ( image , 1 ) mask = cv2 . flip ( mask , 1 ) new_joints = [ ] for people in annos : new_keypoints = [ ] for k in flip_list : point = people [ k ] if point [ 0 ] < 0 or point [ 1 ] < 0 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue if point [ 0 ] > image . shape [ 1 ] - 1 or point [ 1 ] > image . shape [ 0 ] - 1 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue if ( width - point [ 0 ] ) > image . shape [ 1 ] - 1 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue new_keypoints . append ( ( width - point [ 0 ] , point [ 1 ] ) ) new_joints . append ( new_keypoints ) annos = new_joints return image , annos , mask
1322	def MoveToCenter ( self ) -> bool : if self . IsTopLevel ( ) : rect = self . BoundingRectangle screenWidth , screenHeight = GetScreenSize ( ) x , y = ( screenWidth - rect . width ( ) ) // 2 , ( screenHeight - rect . height ( ) ) // 2 if x < 0 : x = 0 if y < 0 : y = 0 return SetWindowPos ( self . NativeWindowHandle , SWP . HWND_Top , x , y , 0 , 0 , SWP . SWP_NoSize ) return False
2369	def keywords ( self ) : for table in self . tables : if isinstance ( table , KeywordTable ) : for keyword in table . keywords : yield keyword
1105	def set_seq2 ( self , b ) : if b is self . b : return self . b = b self . matching_blocks = self . opcodes = None self . fullbcount = None self . __chain_b ( )
387	def obj_box_imresize ( im , coords = None , size = None , interp = 'bicubic' , mode = None , is_rescale = False ) : if coords is None : coords = [ ] if size is None : size = [ 100 , 100 ] imh , imw = im . shape [ 0 : 2 ] imh = imh * 1.0 imw = imw * 1.0 im = imresize ( im , size = size , interp = interp , mode = mode ) if is_rescale is False : coords_new = list ( ) for coord in coords : if len ( coord ) != 4 : raise AssertionError ( "coordinate should be 4 values : [x, y, w, h]" ) x = int ( coord [ 0 ] * ( size [ 1 ] / imw ) ) y = int ( coord [ 1 ] * ( size [ 0 ] / imh ) ) w = int ( coord [ 2 ] * ( size [ 1 ] / imw ) ) h = int ( coord [ 3 ] * ( size [ 0 ] / imh ) ) coords_new . append ( [ x , y , w , h ] ) return im , coords_new else : return im , coords
4444	def get_suggestions ( self , prefix , fuzzy = False , num = 10 , with_scores = False , with_payloads = False ) : args = [ AutoCompleter . SUGGET_COMMAND , self . key , prefix , 'MAX' , num ] if fuzzy : args . append ( AutoCompleter . FUZZY ) if with_scores : args . append ( AutoCompleter . WITHSCORES ) if with_payloads : args . append ( AutoCompleter . WITHPAYLOADS ) ret = self . redis . execute_command ( * args ) results = [ ] if not ret : return results parser = SuggestionParser ( with_scores , with_payloads , ret ) return [ s for s in parser ]
7247	def status ( self , workflow_id ) : self . logger . debug ( 'Get status of workflow: ' + workflow_id ) url = '%(wf_url)s/%(wf_id)s' % { 'wf_url' : self . workflows_url , 'wf_id' : workflow_id } r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( ) [ 'state' ]
7421	def fetch_cluster_se ( data , samfile , chrom , rstart , rend ) : overlap_buffer = data . _hackersonly [ "min_SE_refmap_overlap" ] rstart_buff = rstart + overlap_buffer rend_buff = rend - overlap_buffer if rstart_buff > rend_buff : tmp = rstart_buff rstart_buff = rend_buff rend_buff = tmp if rstart_buff == rend_buff : rend_buff += 1 rdict = { } clust = [ ] iterreg = [ ] iterreg = samfile . fetch ( chrom , rstart_buff , rend_buff ) for read in iterreg : if read . qname not in rdict : rdict [ read . qname ] = read sfunc = lambda x : int ( x . split ( ";size=" ) [ 1 ] . split ( ";" ) [ 0 ] ) rkeys = sorted ( rdict . keys ( ) , key = sfunc , reverse = True ) try : read1 = rdict [ rkeys [ 0 ] ] except ValueError : LOGGER . error ( "Found bad cluster, skipping - key:{} rdict:{}" . format ( rkeys [ 0 ] , rdict ) ) return "" poss = read1 . get_reference_positions ( full_length = True ) seed_r1start = min ( poss ) seed_r1end = max ( poss ) if read1 . is_reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq size = sfunc ( rkeys [ 0 ] ) clust . append ( ">{}:{}:{};size={};*\n{}" . format ( chrom , seed_r1start , seed_r1end , size , seq ) ) if len ( rkeys ) > 1 : for key in rkeys [ 1 : ] : skip = False try : read1 = rdict [ key ] except ValueError : read1 = rdict [ key ] [ 0 ] skip = True if not skip : poss = read1 . get_reference_positions ( full_length = True ) minpos = min ( poss ) maxpos = max ( poss ) if read1 . is_reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq size = sfunc ( key ) clust . append ( ">{}:{}:{};size={};+\n{}" . format ( chrom , minpos , maxpos , size , seq ) ) else : pass return clust
13789	def marv ( ctx , config , loglevel , logfilter , verbosity ) : if config is None : cwd = os . path . abspath ( os . path . curdir ) while cwd != os . path . sep : config = os . path . join ( cwd , 'marv.conf' ) if os . path . exists ( config ) : break cwd = os . path . dirname ( cwd ) else : config = '/etc/marv/marv.conf' if not os . path . exists ( config ) : config = None ctx . obj = config setup_logging ( loglevel , verbosity , logfilter )
12525	def condor_call ( cmd , shell = True ) : log . info ( cmd ) ret = condor_submit ( cmd ) if ret != 0 : subprocess . call ( cmd , shell = shell )
9479	def node ( self , node ) : if node == self . node1 : return self . node2 elif node == self . node2 : return self . node1 else : return None
29	def get_session ( config = None ) : sess = tf . get_default_session ( ) if sess is None : sess = make_session ( config = config , make_default = True ) return sess
6573	def formatter ( self , api_client , data , newval ) : url_map = data . get ( "audioUrlMap" ) audio_url = data . get ( "audioUrl" ) if audio_url and not url_map : url_map = { BaseAPIClient . HIGH_AUDIO_QUALITY : { "audioUrl" : audio_url , "bitrate" : 64 , "encoding" : "aacplus" , } } elif not url_map : return None valid_audio_formats = [ BaseAPIClient . HIGH_AUDIO_QUALITY , BaseAPIClient . MED_AUDIO_QUALITY , BaseAPIClient . LOW_AUDIO_QUALITY ] preferred_quality = api_client . default_audio_quality if preferred_quality in valid_audio_formats : i = valid_audio_formats . index ( preferred_quality ) valid_audio_formats = valid_audio_formats [ i : ] for quality in valid_audio_formats : audio_url = url_map . get ( quality ) if audio_url : return audio_url [ self . field ] return audio_url [ self . field ] if audio_url else None
13723	def log_file ( self , url = None ) : if url is None : url = self . url f = re . sub ( "file://" , "" , url ) try : with open ( f , "a" ) as of : of . write ( str ( self . store . get_json_tuples ( True ) ) ) except IOError as e : print ( e ) print ( "Could not write the content to the file.." )
11804	def assign ( self , var , val , assignment ) : "Assign var, and keep track of conflicts." oldval = assignment . get ( var , None ) if val != oldval : if oldval is not None : self . record_conflict ( assignment , var , oldval , - 1 ) self . record_conflict ( assignment , var , val , + 1 ) CSP . assign ( self , var , val , assignment )
4767	def is_not_same_as ( self , other ) : if self . val is other : self . _err ( 'Expected <%s> to be not identical to <%s>, but was.' % ( self . val , other ) ) return self
527	def _getColumnNeighborhood ( self , centerColumn ) : if self . _wrapAround : return topology . wrappingNeighborhood ( centerColumn , self . _inhibitionRadius , self . _columnDimensions ) else : return topology . neighborhood ( centerColumn , self . _inhibitionRadius , self . _columnDimensions )
11372	def get_temporary_file ( prefix = "tmp_" , suffix = "" , directory = None ) : try : file_fd , filepath = mkstemp ( prefix = prefix , suffix = suffix , dir = directory ) os . close ( file_fd ) except IOError , e : try : os . remove ( filepath ) except Exception : pass raise e return filepath
4049	def fulltext_item ( self , itemkey , ** kwargs ) : query_string = "/{t}/{u}/items/{itemkey}/fulltext" . format ( t = self . library_type , u = self . library_id , itemkey = itemkey ) return self . _build_query ( query_string )
3932	def _auth_with_code ( session , authorization_code ) : token_request_data = { 'client_id' : OAUTH2_CLIENT_ID , 'client_secret' : OAUTH2_CLIENT_SECRET , 'code' : authorization_code , 'grant_type' : 'authorization_code' , 'redirect_uri' : 'urn:ietf:wg:oauth:2.0:oob' , } res = _make_token_request ( session , token_request_data ) return res [ 'access_token' ] , res [ 'refresh_token' ]
806	def enableTap ( self , tapPath ) : self . _tapFileIn = open ( tapPath + '.in' , 'w' ) self . _tapFileOut = open ( tapPath + '.out' , 'w' )
12814	def startProducing ( self , consumer ) : self . _consumer = consumer self . _current_deferred = defer . Deferred ( ) self . _sent = 0 self . _paused = False if not hasattr ( self , "_chunk_headers" ) : self . _build_chunk_headers ( ) if self . _data : block = "" for field in self . _data : block += self . _chunk_headers [ field ] block += self . _data [ field ] block += "\r\n" self . _send_to_consumer ( block ) if self . _files : self . _files_iterator = self . _files . iterkeys ( ) self . _files_sent = 0 self . _files_length = len ( self . _files ) self . _current_file_path = None self . _current_file_handle = None self . _current_file_length = None self . _current_file_sent = 0 result = self . _produce ( ) if result : return result else : return defer . succeed ( None ) return self . _current_deferred
10002	def add_path ( self , nodes , ** attr ) : if nx . __version__ [ 0 ] == "1" : return super ( ) . add_path ( nodes , ** attr ) else : return nx . add_path ( self , nodes , ** attr )
3496	def reaction_weight ( reaction ) : if len ( reaction . metabolites ) != 1 : raise ValueError ( 'Reaction weight is only defined for single ' 'metabolite products or educts.' ) met , coeff = next ( iteritems ( reaction . metabolites ) ) return [ coeff * met . formula_weight ]
1863	def STOS ( cpu , dest , src ) : size = src . size dest . write ( src . read ( ) ) dest_reg = dest . mem . base increment = Operators . ITEBV ( { 'RDI' : 64 , 'EDI' : 32 , 'DI' : 16 } [ dest_reg ] , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( dest_reg , cpu . read_register ( dest_reg ) + increment )
4777	def is_empty ( self ) : if len ( self . val ) != 0 : if isinstance ( self . val , str_types ) : self . _err ( 'Expected <%s> to be empty string, but was not.' % self . val ) else : self . _err ( 'Expected <%s> to be empty, but was not.' % self . val ) return self
10866	def update ( self , params , values ) : params = listify ( params ) values = listify ( values ) for i , p in enumerate ( params ) : if ( p [ - 2 : ] == '-a' ) and ( values [ i ] < 0 ) : values [ i ] = 0.0 super ( PlatonicSpheresCollection , self ) . update ( params , values )
12651	def where_is ( strings , pattern , n = 1 , lookup_func = re . match ) : count = 0 for idx , item in enumerate ( strings ) : if lookup_func ( pattern , item ) : count += 1 if count == n : return idx return - 1
1951	def deprecated ( message : str ) : assert isinstance ( message , str ) , "The deprecated decorator requires a message string argument." def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : warnings . warn ( f"`{func.__qualname__}` is deprecated. {message}" , category = ManticoreDeprecationWarning , stacklevel = 2 ) return func ( * args , ** kwargs ) return wrapper return decorator
8647	def accept_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'accept' , } endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotAcceptedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
12715	def positions ( self ) : return [ self . ode_obj . getPosition ( i ) for i in range ( self . LDOF ) ]
13249	def get_url_from_entry ( entry ) : if 'url' in entry . fields : return entry . fields [ 'url' ] elif entry . type . lower ( ) == 'docushare' : return 'https://ls.st/' + entry . fields [ 'handle' ] elif 'adsurl' in entry . fields : return entry . fields [ 'adsurl' ] elif 'doi' in entry . fields : return 'https://doi.org/' + entry . fields [ 'doi' ] else : raise NoEntryUrlError ( )
10454	def stateenabled ( self , window_name , object_name ) : try : object_handle = self . _get_object_handle ( window_name , object_name ) if object_handle . AXEnabled : return 1 except LdtpServerException : pass return 0
7713	def handle_roster_push ( self , stanza ) : if self . server is None and stanza . from_jid : logger . debug ( u"Server address not known, cannot verify roster push" " from {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) if self . server and stanza . from_jid and stanza . from_jid != self . server : logger . debug ( u"Roster push from invalid source: {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) payload = stanza . get_payload ( RosterPayload ) if len ( payload ) != 1 : logger . warning ( "Bad roster push received ({0} items)" . format ( len ( payload ) ) ) return stanza . make_error_response ( u"bad-request" ) if self . roster is None : logger . debug ( "Dropping roster push - no roster here" ) return True item = payload [ 0 ] item . verify_roster_push ( True ) old_item = self . roster . get ( item . jid ) if item . subscription == "remove" : if old_item : self . roster . remove_item ( item . jid ) else : self . roster . add_item ( item , replace = True ) self . _event_queue . put ( RosterUpdatedEvent ( self , old_item , item ) ) return stanza . make_result_response ( )
4759	def env ( ) : ssh = cij . env_to_dict ( PREFIX , REQUIRED ) if "KEY" in ssh : ssh [ "KEY" ] = cij . util . expand_path ( ssh [ "KEY" ] ) if cij . ENV . get ( "SSH_PORT" ) is None : cij . ENV [ "SSH_PORT" ] = "22" cij . warn ( "cij.ssh.env: SSH_PORT was not set, assigned: %r" % ( cij . ENV . get ( "SSH_PORT" ) ) ) if cij . ENV . get ( "SSH_CMD_TIME" ) is None : cij . ENV [ "SSH_CMD_TIME" ] = "1" cij . warn ( "cij.ssh.env: SSH_CMD_TIME was not set, assigned: %r" % ( cij . ENV . get ( "SSH_CMD_TIME" ) ) ) return 0
11967	def _BYTES_TO_BITS ( ) : the_table = 256 * [ None ] bits_per_byte = list ( range ( 7 , - 1 , - 1 ) ) for n in range ( 256 ) : l = n bits = 8 * [ None ] for i in bits_per_byte : bits [ i ] = '01' [ n & 1 ] n >>= 1 the_table [ l ] = '' . join ( bits ) return the_table
5853	def get_pif ( self , dataset_id , uid , dataset_version = None ) : failure_message = "An error occurred retrieving PIF {}" . format ( uid ) if dataset_version == None : response = self . _get ( routes . pif_dataset_uid ( dataset_id , uid ) , failure_message = failure_message ) else : response = self . _get ( routes . pif_dataset_version_uid ( dataset_id , uid , dataset_version ) , failure_message = failure_message ) return pif . loads ( response . content . decode ( "utf-8" ) )
7287	def set_form_fields ( self , form_field_dict , parent_key = None , field_type = None ) : for form_key , field_value in form_field_dict . items ( ) : form_key = make_key ( parent_key , form_key ) if parent_key is not None else form_key if isinstance ( field_value , tuple ) : set_list_class = False base_key = form_key if ListField in ( field_value . field_type , field_type ) : if parent_key is None or ListField == field_value . field_type : if field_type != EmbeddedDocumentField : field_value . widget . attrs [ 'class' ] += ' listField {0}' . format ( form_key ) set_list_class = True else : field_value . widget . attrs [ 'class' ] += ' listField' list_keys = [ field_key for field_key in self . form . fields . keys ( ) if has_digit ( field_key ) ] key_int = 0 while form_key in list_keys : key_int += 1 form_key = make_key ( form_key , key_int ) if parent_key is not None : valid_base_keys = [ model_key for model_key in self . model_map_dict . keys ( ) if not model_key . startswith ( "_" ) ] while base_key not in valid_base_keys and base_key : base_key = make_key ( base_key , exclude_last_string = True ) embedded_key_class = None if set_list_class : field_value . widget . attrs [ 'class' ] += " listField" . format ( base_key ) embedded_key_class = make_key ( field_key , exclude_last_string = True ) field_value . widget . attrs [ 'class' ] += " embeddedField" if base_key == parent_key : field_value . widget . attrs [ 'class' ] += ' {0}' . format ( base_key ) else : field_value . widget . attrs [ 'class' ] += ' {0} {1}' . format ( base_key , parent_key ) if embedded_key_class is not None : field_value . widget . attrs [ 'class' ] += ' {0}' . format ( embedded_key_class ) default_value = self . get_field_value ( form_key ) if isinstance ( default_value , list ) and len ( default_value ) > 0 : key_index = int ( form_key . split ( "_" ) [ - 1 ] ) new_base_key = make_key ( form_key , exclude_last_string = True ) for list_value in default_value : list_widget = deepcopy ( field_value . widget ) new_key = make_key ( new_base_key , six . text_type ( key_index ) ) list_widget . attrs [ 'class' ] += " {0}" . format ( make_key ( base_key , key_index ) ) self . set_form_field ( list_widget , field_value . document_field , new_key , list_value ) key_index += 1 else : self . set_form_field ( field_value . widget , field_value . document_field , form_key , default_value ) elif isinstance ( field_value , dict ) : self . set_form_fields ( field_value , form_key , field_value . get ( "_field_type" , None ) )
13332	def localize ( name ) : env = cpenv . get_active_env ( ) if not env : click . echo ( 'You need to activate an environment first.' ) return try : r = cpenv . resolve ( name ) except cpenv . ResolveError as e : click . echo ( '\n' + str ( e ) ) module = r . resolved [ 0 ] if isinstance ( module , cpenv . VirtualEnvironment ) : click . echo ( '\nCan only localize a module not an environment' ) return active_modules = cpenv . get_active_modules ( ) if module in active_modules : click . echo ( '\nCan not localize an active module.' ) return if module in env . get_modules ( ) : click . echo ( '\n{} is already local to {}' . format ( module . name , env . name ) ) return if click . confirm ( '\nAdd {} to env {}?' . format ( module . name , env . name ) ) : click . echo ( 'Adding module...' , nl = False ) try : module = env . add_module ( module . name , module . path ) except : click . echo ( bold_red ( 'FAILED' ) ) raise else : click . echo ( bold_green ( 'OK!' ) ) click . echo ( '\nActivate the localize module:' ) click . echo ( ' cpenv activate {} {}' . format ( env . name , module . name ) )
263	def plot_factor_contribution_to_perf ( perf_attrib_data , ax = None , title = 'Cumulative common returns attribution' , ) : if ax is None : ax = plt . gca ( ) factors_to_plot = perf_attrib_data . drop ( [ 'total_returns' , 'common_returns' ] , axis = 'columns' , errors = 'ignore' ) factors_cumulative = pd . DataFrame ( ) for factor in factors_to_plot : factors_cumulative [ factor ] = ep . cum_returns ( factors_to_plot [ factor ] ) for col in factors_cumulative : ax . plot ( factors_cumulative [ col ] ) ax . axhline ( 0 , color = 'k' ) configure_legend ( ax , change_colors = True ) ax . set_ylabel ( 'Cumulative returns by factor' ) ax . set_title ( title ) return ax
8392	def usable_class_name ( node ) : name = node . qname ( ) for prefix in [ "__builtin__." , "builtins." , "." ] : if name . startswith ( prefix ) : name = name [ len ( prefix ) : ] return name
13359	def validate ( self ) : for env in list ( self ) : if not env . exists : self . remove ( env )
8285	def _get_length ( self , segmented = False , precision = 10 ) : if not segmented : return sum ( self . _segment_lengths ( n = precision ) , 0.0 ) else : return self . _segment_lengths ( relative = True , n = precision )
6377	def sim_typo ( src , tar , metric = 'euclidean' , cost = ( 1 , 1 , 0.5 , 0.5 ) , layout = 'QWERTY' ) : return Typo ( ) . sim ( src , tar , metric , cost , layout )
11497	def create_community ( self , token , name , ** kwargs ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'name' ] = name optional_keys = [ 'description' , 'uuid' , 'privacy' , 'can_join' ] for key in optional_keys : if key in kwargs : if key == 'can_join' : parameters [ 'canjoin' ] = kwargs [ key ] continue parameters [ key ] = kwargs [ key ] response = self . request ( 'midas.community.create' , parameters ) return response
12169	def _dispatch ( self , event , listener , * args , ** kwargs ) : if ( asyncio . iscoroutinefunction ( listener ) or isinstance ( listener , functools . partial ) and asyncio . iscoroutinefunction ( listener . func ) ) : return self . _dispatch_coroutine ( event , listener , * args , ** kwargs ) return self . _dispatch_function ( event , listener , * args , ** kwargs )
9473	def DFS_prefix ( self , root = None ) : if not root : root = self . _root return self . _DFS_prefix ( root )
7107	def fit ( self , X , y , coef_init = None , intercept_init = None , sample_weight = None ) : super ( SGDClassifier , self ) . fit ( X , y , coef_init , intercept_init , sample_weight )
13632	def _handleRenderResult ( self , request , result ) : def _requestFinished ( result , cancel ) : cancel ( ) return result if not isinstance ( result , Deferred ) : result = succeed ( result ) def _whenDone ( result ) : render = getattr ( result , 'render' , lambda request : result ) renderResult = render ( request ) if renderResult != NOT_DONE_YET : request . write ( renderResult ) request . finish ( ) return result request . notifyFinish ( ) . addBoth ( _requestFinished , result . cancel ) result . addCallback ( self . _adaptToResource ) result . addCallback ( _whenDone ) result . addErrback ( request . processingFailed ) return NOT_DONE_YET
9010	def index_of_first_produced_mesh_in_row ( self ) : index = 0 for instruction in self . row_instructions : if instruction is self : break index += instruction . number_of_produced_meshes else : self . _raise_not_found_error ( ) return index
7979	def _try_auth ( self ) : if self . authenticated : self . __logger . debug ( "try_auth: already authenticated" ) return self . __logger . debug ( "trying auth: %r" % ( self . _auth_methods_left , ) ) if not self . _auth_methods_left : raise LegacyAuthenticationError ( "No allowed authentication methods available" ) method = self . _auth_methods_left [ 0 ] if method . startswith ( "sasl:" ) : return ClientStream . _try_auth ( self ) elif method not in ( "plain" , "digest" ) : self . _auth_methods_left . pop ( 0 ) self . __logger . debug ( "Skipping unknown auth method: %s" % method ) return self . _try_auth ( ) elif self . available_auth_methods is not None : if method in self . available_auth_methods : self . _auth_methods_left . pop ( 0 ) self . auth_method_used = method if method == "digest" : self . _digest_auth_stage2 ( self . auth_stanza ) else : self . _plain_auth_stage2 ( self . auth_stanza ) self . auth_stanza = None return else : self . __logger . debug ( "Skipping unavailable auth method: %s" % method ) else : self . _auth_stage1 ( )
11539	def set_pin_type ( self , pin , ptype ) : if type ( pin ) is list : for p in pin : self . set_pin_type ( p , ptype ) return pin_id = self . _pin_mapping . get ( pin , None ) if type ( ptype ) is not ahio . PortType : raise KeyError ( 'ptype must be of type ahio.PortType' ) elif pin_id : self . _set_pin_type ( pin_id , ptype ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
12183	def method_exists ( cls , method ) : methods = cls . API_METHODS for key in method . split ( '.' ) : methods = methods . get ( key ) if methods is None : break if isinstance ( methods , str ) : logger . debug ( '%r: %r' , method , methods ) return True return False
3640	def tradeStatus ( self , trade_id ) : method = 'GET' url = 'trade/status' if not isinstance ( trade_id , ( list , tuple ) ) : trade_id = ( trade_id , ) trade_id = ( str ( i ) for i in trade_id ) params = { 'tradeIds' : ',' . join ( trade_id ) } rc = self . __request__ ( method , url , params = params ) return [ itemParse ( i , full = False ) for i in rc [ 'auctionInfo' ] ]
2649	def monitor ( pid , task_id , monitoring_hub_url , run_id , sleep_dur = 10 ) : import psutil radio = UDPRadio ( monitoring_hub_url , source_id = task_id ) simple = [ "cpu_num" , 'cpu_percent' , 'create_time' , 'cwd' , 'exe' , 'memory_percent' , 'nice' , 'name' , 'num_threads' , 'pid' , 'ppid' , 'status' , 'username' ] summable_values = [ 'cpu_percent' , 'memory_percent' , 'num_threads' ] pm = psutil . Process ( pid ) pm . cpu_percent ( ) first_msg = True while True : try : d = { "psutil_process_" + str ( k ) : v for k , v in pm . as_dict ( ) . items ( ) if k in simple } d [ "run_id" ] = run_id d [ "task_id" ] = task_id d [ 'resource_monitoring_interval' ] = sleep_dur d [ 'first_msg' ] = first_msg d [ 'timestamp' ] = datetime . datetime . now ( ) children = pm . children ( recursive = True ) d [ "psutil_cpu_count" ] = psutil . cpu_count ( ) d [ 'psutil_process_memory_virtual' ] = pm . memory_info ( ) . vms d [ 'psutil_process_memory_resident' ] = pm . memory_info ( ) . rss d [ 'psutil_process_time_user' ] = pm . cpu_times ( ) . user d [ 'psutil_process_time_system' ] = pm . cpu_times ( ) . system d [ 'psutil_process_children_count' ] = len ( children ) try : d [ 'psutil_process_disk_write' ] = pm . io_counters ( ) . write_bytes d [ 'psutil_process_disk_read' ] = pm . io_counters ( ) . read_bytes except psutil . _exceptions . AccessDenied : d [ 'psutil_process_disk_write' ] = 0 d [ 'psutil_process_disk_read' ] = 0 for child in children : for k , v in child . as_dict ( attrs = summable_values ) . items ( ) : d [ 'psutil_process_' + str ( k ) ] += v d [ 'psutil_process_time_user' ] += child . cpu_times ( ) . user d [ 'psutil_process_time_system' ] += child . cpu_times ( ) . system d [ 'psutil_process_memory_virtual' ] += child . memory_info ( ) . vms d [ 'psutil_process_memory_resident' ] += child . memory_info ( ) . rss try : d [ 'psutil_process_disk_write' ] += child . io_counters ( ) . write_bytes d [ 'psutil_process_disk_read' ] += child . io_counters ( ) . read_bytes except psutil . _exceptions . AccessDenied : d [ 'psutil_process_disk_write' ] += 0 d [ 'psutil_process_disk_read' ] += 0 finally : radio . send ( MessageType . TASK_INFO , task_id , d ) time . sleep ( sleep_dur ) first_msg = False
4517	def fillScreen ( self , color = None ) : md . fill_rect ( self . set , 0 , 0 , self . width , self . height , color )
5379	def build_pipeline ( cls , project , zones , min_cores , min_ram , disk_size , boot_disk_size , preemptible , accelerator_type , accelerator_count , image , script_name , envs , inputs , outputs , pipeline_name ) : if min_cores is None : min_cores = job_model . DEFAULT_MIN_CORES if min_ram is None : min_ram = job_model . DEFAULT_MIN_RAM if disk_size is None : disk_size = job_model . DEFAULT_DISK_SIZE if boot_disk_size is None : boot_disk_size = job_model . DEFAULT_BOOT_DISK_SIZE if preemptible is None : preemptible = job_model . DEFAULT_PREEMPTIBLE docker_command = cls . _build_pipeline_docker_command ( script_name , inputs , outputs , envs ) input_envs = [ { 'name' : SCRIPT_VARNAME } ] + [ { 'name' : env . name } for env in envs if env . value ] input_files = [ cls . _build_pipeline_input_file_param ( var . name , var . docker_path ) for var in inputs if not var . recursive and var . value ] output_files = [ cls . _build_pipeline_file_param ( var . name , var . docker_path ) for var in outputs if not var . recursive and var . value ] return { 'ephemeralPipeline' : { 'projectId' : project , 'name' : pipeline_name , 'resources' : { 'minimumCpuCores' : min_cores , 'minimumRamGb' : min_ram , 'bootDiskSizeGb' : boot_disk_size , 'preemptible' : preemptible , 'zones' : google_base . get_zones ( zones ) , 'acceleratorType' : accelerator_type , 'acceleratorCount' : accelerator_count , 'disks' : [ { 'name' : 'datadisk' , 'autoDelete' : True , 'sizeGb' : disk_size , 'mountPoint' : providers_util . DATA_MOUNT_POINT , } ] , } , 'inputParameters' : input_envs + input_files , 'outputParameters' : output_files , 'docker' : { 'imageName' : image , 'cmd' : docker_command , } } }
7891	def set_stream ( self , stream ) : _unused = stream if self . joined and self . handler : self . handler . user_left ( self . me , None ) self . joined = False
10244	def get_citation_years ( graph : BELGraph ) -> List [ Tuple [ int , int ] ] : return create_timeline ( count_citation_years ( graph ) )
6731	def create_module ( name , code = None ) : if name not in sys . modules : sys . modules [ name ] = imp . new_module ( name ) module = sys . modules [ name ] if code : print ( 'executing code for %s: %s' % ( name , code ) ) exec ( code in module . __dict__ ) exec ( "from %s import %s" % ( name , '*' ) ) return module
7160	def next_question ( self ) : for key , questions in self . questions . items ( ) : if key in self . answers : continue for question in questions : if self . check_condition ( question . _condition ) : return question return None
1116	def _convert_flags ( self , fromlist , tolist , flaglist , context , numlines ) : toprefix = self . _prefix [ 1 ] next_id = [ '' ] * len ( flaglist ) next_href = [ '' ] * len ( flaglist ) num_chg , in_change = 0 , False last = 0 for i , flag in enumerate ( flaglist ) : if flag : if not in_change : in_change = True last = i i = max ( [ 0 , i - numlines ] ) next_id [ i ] = ' id="difflib_chg_%s_%d"' % ( toprefix , num_chg ) num_chg += 1 next_href [ last ] = '<a href="#difflib_chg_%s_%d">n</a>' % ( toprefix , num_chg ) else : in_change = False if not flaglist : flaglist = [ False ] next_id = [ '' ] next_href = [ '' ] last = 0 if context : fromlist = [ '<td></td><td>&nbsp;No Differences Found&nbsp;</td>' ] tolist = fromlist else : fromlist = tolist = [ '<td></td><td>&nbsp;Empty File&nbsp;</td>' ] if not flaglist [ 0 ] : next_href [ 0 ] = '<a href="#difflib_chg_%s_0">f</a>' % toprefix next_href [ last ] = '<a href="#difflib_chg_%s_top">t</a>' % ( toprefix ) return fromlist , tolist , flaglist , next_href , next_id
12804	def get_user ( self , id = None ) : if not id : id = self . _user . id if id not in self . _users : self . _users [ id ] = self . _user if id == self . _user . id else User ( self , id ) return self . _users [ id ]
9326	def refresh ( self ) : response = self . __raw = self . _conn . get ( self . url ) self . _populate_fields ( ** response ) self . _loaded = True
5823	def to_dict ( self ) : return { "type" : self . type , "name" : self . name , "group_by_key" : self . group_by_key , "role" : self . role , "units" : self . units , "options" : self . build_options ( ) }
981	def _initializeBucketMap ( self , maxBuckets , offset ) : self . _maxBuckets = maxBuckets self . minIndex = self . _maxBuckets / 2 self . maxIndex = self . _maxBuckets / 2 self . _offset = offset self . bucketMap = { } def _permutation ( n ) : r = numpy . arange ( n , dtype = numpy . uint32 ) self . random . shuffle ( r ) return r self . bucketMap [ self . minIndex ] = _permutation ( self . n ) [ 0 : self . w ] self . numTries = 0
6347	def _redo_language ( self , term , name_mode , rules , final_rules1 , final_rules2 , concat ) : language_arg = self . _language ( term , name_mode ) return self . _phonetic ( term , name_mode , rules , final_rules1 , final_rules2 , language_arg , concat , )
2366	def walk ( self , * types ) : requested = types if len ( types ) > 0 else [ SuiteFile , ResourceFile , SuiteFolder , Testcase , Keyword ] for thing in self . robot_files : if thing . __class__ in requested : yield thing if isinstance ( thing , SuiteFolder ) : for child in thing . walk ( ) : if child . __class__ in requested : yield child else : for child in thing . walk ( * types ) : yield child
8610	def get_resource ( self , resource_type , resource_id , depth = 1 ) : response = self . _perform_request ( '/um/resources/%s/%s?depth=%s' % ( resource_type , resource_id , str ( depth ) ) ) return response
5816	def _read_remaining ( socket ) : output = b'' old_timeout = socket . gettimeout ( ) try : socket . settimeout ( 0.0 ) output += socket . recv ( 8192 ) except ( socket_ . error ) : pass finally : socket . settimeout ( old_timeout ) return output
8448	def _has_branch ( branch ) : ret = temple . utils . shell ( 'git rev-parse --verify {}' . format ( branch ) , stderr = subprocess . DEVNULL , stdout = subprocess . DEVNULL , check = False ) return ret . returncode == 0
31	def adjust_shape ( placeholder , data ) : if not isinstance ( data , np . ndarray ) and not isinstance ( data , list ) : return data if isinstance ( data , list ) : data = np . array ( data ) placeholder_shape = [ x or - 1 for x in placeholder . shape . as_list ( ) ] assert _check_shape ( placeholder_shape , data . shape ) , 'Shape of data {} is not compatible with shape of the placeholder {}' . format ( data . shape , placeholder_shape ) return np . reshape ( data , placeholder_shape )
11916	def render_to ( self , path , template , ** data ) : html = self . render ( template , ** data ) with open ( path , 'w' ) as f : f . write ( html . encode ( charset ) )
13001	def hr_diagram_from_data ( data , x_range , y_range ) : _ , color_mapper = hr_diagram_color_helper ( [ ] ) data_dict = { 'x' : list ( data [ 'temperature' ] ) , 'y' : list ( data [ 'luminosity' ] ) , 'color' : list ( data [ 'color' ] ) } source = ColumnDataSource ( data = data_dict ) pf = figure ( y_axis_type = 'log' , x_range = x_range , y_range = y_range ) _diagram ( source = source , plot_figure = pf , color = { 'field' : 'color' , 'transform' : color_mapper } , xaxis_label = 'Temperature (Kelvin)' , yaxis_label = 'Luminosity (solar units)' ) show_with_bokeh_server ( pf )
7606	def search_clans ( self , ** params : clansearch ) : url = self . api . CLAN return self . _get_model ( url , PartialClan , ** params )
10381	def _get_drug_target_interactions ( manager : Optional [ 'bio2bel_drugbank.manager' ] = None ) -> Mapping [ str , List [ str ] ] : if manager is None : import bio2bel_drugbank manager = bio2bel_drugbank . Manager ( ) if not manager . is_populated ( ) : manager . populate ( ) return manager . get_drug_to_hgnc_symbols ( )
9616	def is_displayed ( target ) : is_displayed = getattr ( target , 'is_displayed' , None ) if not is_displayed or not callable ( is_displayed ) : raise TypeError ( 'Target has no attribute \'is_displayed\' or not callable' ) if not is_displayed ( ) : raise WebDriverException ( 'element not visible' )
1206	def from_spec ( spec , kwargs ) : env = tensorforce . util . get_object ( obj = spec , predefined_objects = tensorforce . environments . environments , kwargs = kwargs ) assert isinstance ( env , Environment ) return env
6612	def put_multiple ( self , task_args_kwargs_list ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return packages = [ ] for t in task_args_kwargs_list : try : task = t [ 'task' ] args = t . get ( 'args' , ( ) ) kwargs = t . get ( 'kwargs' , { } ) package = TaskPackage ( task = task , args = args , kwargs = kwargs ) except TypeError : package = TaskPackage ( task = t , args = ( ) , kwargs = { } ) packages . append ( package ) return self . dropbox . put_multiple ( packages )
12082	def clampfit_rename ( path , char ) : assert len ( char ) == 1 and type ( char ) == str , "replacement character must be a single character" assert os . path . exists ( path ) , "path doesn't exist" files = sorted ( os . listdir ( path ) ) files = [ x for x in files if len ( x ) > 18 and x [ 4 ] + x [ 7 ] + x [ 10 ] == ' ' ] for fname in files : fname2 = list ( fname ) fname2 [ 11 ] = char fname2 = "" . join ( fname2 ) if fname == fname2 : print ( fname , "==" , fname2 ) else : print ( fname , "->" , fname2 ) return
2880	def get_event_definition ( self ) : messageEventDefinition = first ( self . xpath ( './/bpmn:messageEventDefinition' ) ) if messageEventDefinition is not None : return self . get_message_event_definition ( messageEventDefinition ) timerEventDefinition = first ( self . xpath ( './/bpmn:timerEventDefinition' ) ) if timerEventDefinition is not None : return self . get_timer_event_definition ( timerEventDefinition ) raise NotImplementedError ( 'Unsupported Intermediate Catch Event: %r' , ET . tostring ( self . node ) )
4901	def get_course_enrollments ( self , enterprise_customer , days ) : return CourseEnrollment . objects . filter ( created__gt = datetime . datetime . now ( ) - datetime . timedelta ( days = days ) ) . filter ( user_id__in = enterprise_customer . enterprise_customer_users . values_list ( 'user_id' , flat = True ) )
6149	def firwin_bpf ( N_taps , f1 , f2 , fs = 1.0 , pass_zero = False ) : return signal . firwin ( N_taps , 2 * ( f1 , f2 ) / fs , pass_zero = pass_zero )
13376	def walk_dn ( start_dir , depth = 10 ) : start_depth = len ( os . path . split ( start_dir ) ) end_depth = start_depth + depth for root , subdirs , files in os . walk ( start_dir ) : yield root , subdirs , files if len ( os . path . split ( root ) ) >= end_depth : break
11334	def table ( * columns , ** kwargs ) : ret = [ ] prefix = kwargs . get ( 'prefix' , '' ) buf_count = kwargs . get ( 'buf_count' , 2 ) if len ( columns ) == 1 : columns = list ( columns [ 0 ] ) else : columns = list ( zip ( * columns ) ) headers = kwargs . get ( "headers" , [ ] ) if headers : columns . insert ( 0 , headers ) widths = kwargs . get ( "widths" , [ ] ) row_counts = Counter ( ) for i in range ( len ( widths ) ) : row_counts [ i ] = int ( widths [ i ] ) width = int ( kwargs . get ( "width" , 0 ) ) for row in columns : for i , c in enumerate ( row ) : if isinstance ( c , basestring ) : cl = len ( c ) else : cl = len ( str ( c ) ) if cl > row_counts [ i ] : row_counts [ i ] = cl width = int ( kwargs . get ( "width" , 0 ) ) if width : for i in row_counts : if row_counts [ i ] < width : row_counts [ i ] = width def colstr ( c ) : if isinstance ( c , basestring ) : return c return str ( c ) def rowstr ( row , prefix , row_counts ) : row_format = prefix cols = list ( map ( colstr , row ) ) for i in range ( len ( row_counts ) ) : c = cols [ i ] if re . match ( r"^\d+(?:\.\d+)?$" , c ) : if i == 0 : row_format += "{:>" + str ( row_counts [ i ] ) + "}" else : row_format += "{:>" + str ( row_counts [ i ] + buf_count ) + "}" else : row_format += "{:<" + str ( row_counts [ i ] + buf_count ) + "}" return row_format . format ( * cols ) for row in columns : ret . append ( rowstr ( row , prefix , row_counts ) ) out ( os . linesep . join ( ret ) )
3763	def Pt ( CASRN , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in Staveley_data . index and not np . isnan ( Staveley_data . at [ CASRN , 'Pt' ] ) : methods . append ( STAVELEY ) if Tt ( CASRN ) and VaporPressure ( CASRN = CASRN ) . T_dependent_property ( T = Tt ( CASRN ) ) : methods . append ( DEFINITION ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == STAVELEY : Pt = Staveley_data . at [ CASRN , 'Pt' ] elif Method == DEFINITION : Pt = VaporPressure ( CASRN = CASRN ) . T_dependent_property ( T = Tt ( CASRN ) ) elif Method == NONE : Pt = None else : raise Exception ( 'Failure in in function' ) return Pt
2032	def MLOAD ( self , address ) : self . _allocate ( address , 32 ) value = self . _load ( address , 32 ) return value
1313	def ControlFromPoint ( x : int , y : int ) -> Control : element = _AutomationClient . instance ( ) . IUIAutomation . ElementFromPoint ( ctypes . wintypes . POINT ( x , y ) ) return Control . CreateControlFromElement ( element )
6310	def load ( self ) : self . _open_image ( ) components , data = image_data ( self . image ) texture = self . ctx . texture ( self . image . size , components , data , ) texture . extra = { 'meta' : self . meta } if self . meta . mipmap : texture . build_mipmaps ( ) self . _close_image ( ) return texture
5308	def hex_to_rgb ( value ) : value = value . lstrip ( '#' ) check_hex ( value ) length = len ( value ) step = int ( length / 3 ) return tuple ( int ( value [ i : i + step ] , 16 ) for i in range ( 0 , length , step ) )
13852	def age ( self ) : if self . rounds == 1 : self . do_run = False elif self . rounds > 1 : self . rounds -= 1
2828	def convert_selu ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting selu ...' ) if names == 'short' : tf_name = 'SELU' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) selu = keras . layers . Activation ( 'selu' , name = tf_name ) layers [ scope_name ] = selu ( layers [ inputs [ 0 ] ] )
40	def add ( self , * args , ** kwargs ) : idx = self . _next_idx super ( ) . add ( * args , ** kwargs ) self . _it_sum [ idx ] = self . _max_priority ** self . _alpha self . _it_min [ idx ] = self . _max_priority ** self . _alpha
1509	def stop_cluster ( cl_args ) : Log . info ( "Terminating cluster..." ) roles = read_and_parse_roles ( cl_args ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] dist_nodes = masters . union ( slaves ) if masters : try : single_master = list ( masters ) [ 0 ] jobs = get_jobs ( cl_args , single_master ) for job in jobs : job_id = job [ "ID" ] Log . info ( "Terminating job %s" % job_id ) delete_job ( cl_args , job_id , single_master ) except : Log . debug ( "Error stopping jobs" ) Log . debug ( sys . exc_info ( ) [ 0 ] ) for node in dist_nodes : Log . info ( "Terminating processes on %s" % node ) if not is_self ( node ) : cmd = "ps aux | grep heron-nomad | awk '{print \$2}' " "| xargs kill" cmd = ssh_remote_execute ( cmd , node , cl_args ) else : cmd = "ps aux | grep heron-nomad | awk '{print $2}' " "| xargs kill" Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) Log . info ( "Cleaning up directories on %s" % node ) cmd = "rm -rf /tmp/slave ; rm -rf /tmp/master" if not is_self ( node ) : cmd = ssh_remote_execute ( cmd , node , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) )
6319	def initial_sanity_check ( self ) : self . try_import ( self . project_name ) self . validate_name ( self . project_name ) if os . path . exists ( self . project_name ) : print ( "Directory {} already exist. Aborting." . format ( self . project_name ) ) return False if os . path . exists ( 'manage.py' ) : print ( "A manage.py file already exist in the current directory. Aborting." ) return False return True
12749	def load_asf ( self , source , ** kwargs ) : if hasattr ( source , 'read' ) : p = parser . parse_asf ( source , self . world , self . jointgroup , ** kwargs ) else : with open ( source ) as handle : p = parser . parse_asf ( handle , self . world , self . jointgroup , ** kwargs ) self . bodies = p . bodies self . joints = p . joints self . set_pid_params ( kp = 0.999 / self . world . dt )
5344	def compose_gerrit ( projects ) : git_projects = [ project for project in projects if 'git' in projects [ project ] ] for project in git_projects : repos = [ repo for repo in projects [ project ] [ 'git' ] if 'gitroot' in repo ] if len ( repos ) > 0 : projects [ project ] [ 'gerrit' ] = [ ] for repo in repos : gerrit_project = repo . replace ( "http://git.eclipse.org/gitroot/" , "" ) gerrit_project = gerrit_project . replace ( ".git" , "" ) projects [ project ] [ 'gerrit' ] . append ( "git.eclipse.org_" + gerrit_project ) return projects
10553	def create_helpingmaterial ( project_id , info , media_url = None , file_path = None ) : try : helping = dict ( project_id = project_id , info = info , media_url = None , ) if file_path : files = { 'file' : open ( file_path , 'rb' ) } payload = { 'project_id' : project_id } res = _pybossa_req ( 'post' , 'helpingmaterial' , payload = payload , files = files ) else : res = _pybossa_req ( 'post' , 'helpingmaterial' , payload = helping ) if res . get ( 'id' ) : return HelpingMaterial ( res ) else : return res except : raise
1123	def Tok ( kind , loc = None ) : @ llrule ( loc , lambda parser : [ kind ] ) def rule ( parser ) : return parser . _accept ( kind ) return rule
262	def plot_returns ( perf_attrib_data , cost = None , ax = None ) : if ax is None : ax = plt . gca ( ) returns = perf_attrib_data [ 'total_returns' ] total_returns_label = 'Total returns' cumulative_returns_less_costs = _cumulative_returns_less_costs ( returns , cost ) if cost is not None : total_returns_label += ' (adjusted)' specific_returns = perf_attrib_data [ 'specific_returns' ] common_returns = perf_attrib_data [ 'common_returns' ] ax . plot ( cumulative_returns_less_costs , color = 'b' , label = total_returns_label ) ax . plot ( ep . cum_returns ( specific_returns ) , color = 'g' , label = 'Cumulative specific returns' ) ax . plot ( ep . cum_returns ( common_returns ) , color = 'r' , label = 'Cumulative common returns' ) if cost is not None : ax . plot ( - ep . cum_returns ( cost ) , color = 'k' , label = 'Cumulative cost spent' ) ax . set_title ( 'Time series of cumulative returns' ) ax . set_ylabel ( 'Returns' ) configure_legend ( ax ) return ax
617	def parseTimestamp ( s ) : s = s . strip ( ) for pattern in DATETIME_FORMATS : try : return datetime . datetime . strptime ( s , pattern ) except ValueError : pass raise ValueError ( 'The provided timestamp %s is malformed. The supported ' 'formats are: [%s]' % ( s , ', ' . join ( DATETIME_FORMATS ) ) )
2320	def check_cuda_devices ( ) : import ctypes CUDA_SUCCESS = 0 libnames = ( 'libcuda.so' , 'libcuda.dylib' , 'cuda.dll' ) for libname in libnames : try : cuda = ctypes . CDLL ( libname ) except OSError : continue else : break else : return 0 nGpus = ctypes . c_int ( ) error_str = ctypes . c_char_p ( ) result = cuda . cuInit ( 0 ) if result != CUDA_SUCCESS : cuda . cuGetErrorString ( result , ctypes . byref ( error_str ) ) return 0 result = cuda . cuDeviceGetCount ( ctypes . byref ( nGpus ) ) if result != CUDA_SUCCESS : cuda . cuGetErrorString ( result , ctypes . byref ( error_str ) ) return 0 return nGpus . value
2260	def group_items ( items , groupids ) : r if callable ( groupids ) : keyfunc = groupids pair_list = ( ( keyfunc ( item ) , item ) for item in items ) else : pair_list = zip ( groupids , items ) groupid_to_items = defaultdict ( list ) for key , item in pair_list : groupid_to_items [ key ] . append ( item ) return groupid_to_items
6674	def get_owner ( self , path , use_sudo = False ) : func = use_sudo and run_as_root or self . run with self . settings ( hide ( 'running' , 'stdout' ) , warn_only = True ) : result = func ( 'stat -c %%U "%(path)s"' % locals ( ) ) if result . failed and 'stat: illegal option' in result : return func ( 'stat -f %%Su "%(path)s"' % locals ( ) ) return result
8042	def parse ( self , filelike , filename ) : self . log = log self . source = filelike . readlines ( ) src = "" . join ( self . source ) compile ( src , filename , "exec" ) self . stream = TokenStream ( StringIO ( src ) ) self . filename = filename self . all = None self . future_imports = set ( ) self . _accumulated_decorators = [ ] return self . parse_module ( )
6830	def get_logs_between_commits ( self , a , b ) : print ( 'REAL' ) ret = self . local ( 'git --no-pager log --pretty=oneline %s...%s' % ( a , b ) , capture = True ) if self . verbose : print ( ret ) return str ( ret )
4253	def org_by_name ( self , hostname ) : addr = self . _gethostbyname ( hostname ) return self . org_by_addr ( addr )
471	def read_analogies_file ( eval_file = 'questions-words.txt' , word2id = None ) : if word2id is None : word2id = { } questions = [ ] questions_skipped = 0 with open ( eval_file , "rb" ) as analogy_f : for line in analogy_f : if line . startswith ( b":" ) : continue words = line . strip ( ) . lower ( ) . split ( b" " ) ids = [ word2id . get ( w . strip ( ) ) for w in words ] if None in ids or len ( ids ) != 4 : questions_skipped += 1 else : questions . append ( np . array ( ids ) ) tl . logging . info ( "Eval analogy file: %s" % eval_file ) tl . logging . info ( "Questions: %d" , len ( questions ) ) tl . logging . info ( "Skipped: %d" , questions_skipped ) analogy_questions = np . array ( questions , dtype = np . int32 ) return analogy_questions
13132	def parse_user ( entry , domain_groups ) : result = { } distinguished_name = get_field ( entry , 'distinguishedName' ) result [ 'domain' ] = "." . join ( distinguished_name . split ( ',DC=' ) [ 1 : ] ) result [ 'name' ] = get_field ( entry , 'name' ) result [ 'username' ] = get_field ( entry , 'sAMAccountName' ) result [ 'description' ] = get_field ( entry , 'description' ) result [ 'sid' ] = get_field ( entry , 'objectSid' ) . split ( '-' ) [ - 1 ] primary_group = get_field ( entry , 'primaryGroupID' ) member_of = entry [ 'attributes' ] . get ( 'memberOf' , [ ] ) groups = [ ] for member in member_of : for e in member . split ( ',' ) : if e . startswith ( 'CN=' ) : groups . append ( e [ 3 : ] ) groups . append ( domain_groups . get ( primary_group , '' ) ) result [ 'groups' ] = groups flags = [ ] try : uac = int ( get_field ( entry , 'userAccountControl' ) ) for flag , value in uac_flags . items ( ) : if uac & value : flags . append ( flag ) except ValueError : pass result [ 'flags' ] = flags return result
6414	def aghmean ( nums ) : m_a = amean ( nums ) m_g = gmean ( nums ) m_h = hmean ( nums ) if math . isnan ( m_a ) or math . isnan ( m_g ) or math . isnan ( m_h ) : return float ( 'nan' ) while round ( m_a , 12 ) != round ( m_g , 12 ) and round ( m_g , 12 ) != round ( m_h , 12 ) : m_a , m_g , m_h = ( ( m_a + m_g + m_h ) / 3 , ( m_a * m_g * m_h ) ** ( 1 / 3 ) , 3 / ( 1 / m_a + 1 / m_g + 1 / m_h ) , ) return m_a
10008	def get_dynamic_base ( self , bases : tuple ) : try : return self . _dynamic_bases_inverse [ bases ] except KeyError : name = self . _dynamic_base_namer . get_next ( self . _dynamic_bases ) base = self . _new_space ( name = name ) self . spacegraph . add_space ( base ) self . _dynamic_bases [ name ] = base self . _dynamic_bases_inverse [ bases ] = base base . add_bases ( bases ) return base
347	def load_imdb_dataset ( path = 'data' , nb_words = None , skip_top = 0 , maxlen = None , test_split = 0.2 , seed = 113 , start_char = 1 , oov_char = 2 , index_from = 3 ) : path = os . path . join ( path , 'imdb' ) filename = "imdb.pkl" url = 'https://s3.amazonaws.com/text-datasets/' maybe_download_and_extract ( filename , path , url ) if filename . endswith ( ".gz" ) : f = gzip . open ( os . path . join ( path , filename ) , 'rb' ) else : f = open ( os . path . join ( path , filename ) , 'rb' ) X , labels = cPickle . load ( f ) f . close ( ) np . random . seed ( seed ) np . random . shuffle ( X ) np . random . seed ( seed ) np . random . shuffle ( labels ) if start_char is not None : X = [ [ start_char ] + [ w + index_from for w in x ] for x in X ] elif index_from : X = [ [ w + index_from for w in x ] for x in X ] if maxlen : new_X = [ ] new_labels = [ ] for x , y in zip ( X , labels ) : if len ( x ) < maxlen : new_X . append ( x ) new_labels . append ( y ) X = new_X labels = new_labels if not X : raise Exception ( 'After filtering for sequences shorter than maxlen=' + str ( maxlen ) + ', no sequence was kept. ' 'Increase maxlen.' ) if not nb_words : nb_words = max ( [ max ( x ) for x in X ] ) if oov_char is not None : X = [ [ oov_char if ( w >= nb_words or w < skip_top ) else w for w in x ] for x in X ] else : nX = [ ] for x in X : nx = [ ] for w in x : if ( w >= nb_words or w < skip_top ) : nx . append ( w ) nX . append ( nx ) X = nX X_train = np . array ( X [ : int ( len ( X ) * ( 1 - test_split ) ) ] ) y_train = np . array ( labels [ : int ( len ( X ) * ( 1 - test_split ) ) ] ) X_test = np . array ( X [ int ( len ( X ) * ( 1 - test_split ) ) : ] ) y_test = np . array ( labels [ int ( len ( X ) * ( 1 - test_split ) ) : ] ) return X_train , y_train , X_test , y_test
5640	def remove_all_trips_fully_outside_buffer ( db_conn , center_lat , center_lon , buffer_km , update_secondary_data = True ) : distance_function_str = add_wgs84_distance_function_to_db ( db_conn ) stops_within_buffer_query_sql = "SELECT stop_I FROM stops WHERE CAST(" + distance_function_str + "(lat, lon, {lat} , {lon}) AS INT) < {d_m}" . format ( lat = float ( center_lat ) , lon = float ( center_lon ) , d_m = int ( 1000 * buffer_km ) ) select_all_trip_Is_where_stop_I_is_within_buffer_sql = "SELECT distinct(trip_I) FROM stop_times WHERE stop_I IN (" + stops_within_buffer_query_sql + ")" trip_Is_to_remove_sql = "SELECT trip_I FROM trips WHERE trip_I NOT IN ( " + select_all_trip_Is_where_stop_I_is_within_buffer_sql + ")" trip_Is_to_remove = pandas . read_sql ( trip_Is_to_remove_sql , db_conn ) [ "trip_I" ] . values trip_Is_to_remove_string = "," . join ( [ str ( trip_I ) for trip_I in trip_Is_to_remove ] ) remove_all_trips_fully_outside_buffer_sql = "DELETE FROM trips WHERE trip_I IN (" + trip_Is_to_remove_string + ")" remove_all_stop_times_where_trip_I_fully_outside_buffer_sql = "DELETE FROM stop_times WHERE trip_I IN (" + trip_Is_to_remove_string + ")" db_conn . execute ( remove_all_trips_fully_outside_buffer_sql ) db_conn . execute ( remove_all_stop_times_where_trip_I_fully_outside_buffer_sql ) delete_stops_not_in_stop_times_and_not_as_parent_stop ( db_conn ) db_conn . execute ( DELETE_ROUTES_NOT_PRESENT_IN_TRIPS_SQL ) db_conn . execute ( DELETE_SHAPES_NOT_REFERENCED_IN_TRIPS_SQL ) db_conn . execute ( DELETE_DAYS_ENTRIES_NOT_PRESENT_IN_TRIPS_SQL ) db_conn . execute ( DELETE_DAY_TRIPS2_ENTRIES_NOT_PRESENT_IN_TRIPS_SQL ) db_conn . execute ( DELETE_CALENDAR_ENTRIES_FOR_NON_REFERENCE_SERVICE_IS_SQL ) db_conn . execute ( DELETE_CALENDAR_DATES_ENTRIES_FOR_NON_REFERENCE_SERVICE_IS_SQL ) db_conn . execute ( DELETE_FREQUENCIES_ENTRIES_NOT_PRESENT_IN_TRIPS ) db_conn . execute ( DELETE_AGENCIES_NOT_REFERENCED_IN_ROUTES_SQL ) if update_secondary_data : update_secondary_data_copies ( db_conn )
7834	def make_submit ( self , keep_types = False ) : result = Form ( "submit" ) for field in self . fields : if field . type == "fixed" : continue if not field . values : if field . required : raise ValueError ( "Required field with no value!" ) continue if keep_types : result . add_field ( field . name , field . values , field . type ) else : result . add_field ( field . name , field . values ) return result
9260	def include_issues_by_labels ( self , all_issues ) : included_by_labels = self . filter_by_include_labels ( all_issues ) wo_labels = self . filter_wo_labels ( all_issues ) il = set ( [ f [ "number" ] for f in included_by_labels ] ) wl = set ( [ w [ "number" ] for w in wo_labels ] ) filtered_issues = [ ] for issue in all_issues : if issue [ "number" ] in il or issue [ "number" ] in wl : filtered_issues . append ( issue ) return filtered_issues
9621	def gamepad ( self ) : state = _xinput_state ( ) _xinput . XInputGetState ( self . ControllerID - 1 , pointer ( state ) ) self . dwPacketNumber = state . dwPacketNumber return state . XINPUT_GAMEPAD
8403	def squish_infinite ( x , range = ( 0 , 1 ) ) : xtype = type ( x ) if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) x [ x == - np . inf ] = range [ 0 ] x [ x == np . inf ] = range [ 1 ] if not isinstance ( x , xtype ) : x = xtype ( x ) return x
11237	def sendreturn ( gen , value ) : try : gen . send ( value ) except StopIteration as e : return stopiter_value ( e ) else : raise RuntimeError ( 'generator did not return as expected' )
3859	def update_conversation ( self , conversation ) : new_state = conversation . self_conversation_state old_state = self . _conversation . self_conversation_state self . _conversation = conversation if not new_state . delivery_medium_option : new_state . delivery_medium_option . extend ( old_state . delivery_medium_option ) old_timestamp = old_state . self_read_state . latest_read_timestamp new_timestamp = new_state . self_read_state . latest_read_timestamp if new_timestamp == 0 : new_state . self_read_state . latest_read_timestamp = old_timestamp for new_entry in conversation . read_state : tstamp = parsers . from_timestamp ( new_entry . latest_read_timestamp ) if tstamp == 0 : continue uid = parsers . from_participantid ( new_entry . participant_id ) if uid not in self . _watermarks or self . _watermarks [ uid ] < tstamp : self . _watermarks [ uid ] = tstamp
1205	def target_optimizer_arguments ( self ) : variables = self . target_network . get_variables ( ) + [ variable for name in sorted ( self . target_distributions ) for variable in self . target_distributions [ name ] . get_variables ( ) ] source_variables = self . network . get_variables ( ) + [ variable for name in sorted ( self . distributions ) for variable in self . distributions [ name ] . get_variables ( ) ] arguments = dict ( time = self . global_timestep , variables = variables , source_variables = source_variables ) if self . global_model is not None : arguments [ 'global_variables' ] = self . global_model . target_network . get_variables ( ) + [ variable for name in sorted ( self . global_model . target_distributions ) for variable in self . global_model . target_distributions [ name ] . get_variables ( ) ] return arguments
10918	def find_best_step ( err_vals ) : if np . all ( np . isnan ( err_vals ) ) : raise ValueError ( 'All err_vals are nans!' ) return np . nanargmin ( err_vals )
1249	def _is_action_available_left ( self , state ) : for row in range ( 4 ) : has_empty = False for col in range ( 4 ) : has_empty |= state [ row , col ] == 0 if state [ row , col ] != 0 and has_empty : return True if ( state [ row , col ] != 0 and col > 0 and state [ row , col ] == state [ row , col - 1 ] ) : return True return False
11963	def _dec_to_dot ( ip ) : first = int ( ( ip >> 24 ) & 255 ) second = int ( ( ip >> 16 ) & 255 ) third = int ( ( ip >> 8 ) & 255 ) fourth = int ( ip & 255 ) return '%d.%d.%d.%d' % ( first , second , third , fourth )
13850	def ensure_dir_exists ( func ) : "wrap a function that returns a dir, making sure it exists" @ functools . wraps ( func ) def make_if_not_present ( ) : dir = func ( ) if not os . path . isdir ( dir ) : os . makedirs ( dir ) return dir return make_if_not_present
3656	def add_cti_file ( self , file_path : str ) : if not os . path . exists ( file_path ) : self . _logger . warning ( 'Attempted to add {0} which does not exist.' . format ( file_path ) ) if file_path not in self . _cti_files : self . _cti_files . append ( file_path ) self . _logger . info ( 'Added {0} to the CTI file list.' . format ( file_path ) )
6387	def _sb_r2 ( self , term , r1_prefixes = None ) : r1_start = self . _sb_r1 ( term , r1_prefixes ) return r1_start + self . _sb_r1 ( term [ r1_start : ] )
172	def draw_points_heatmap_array ( self , image_shape , alpha = 1.0 , size = 1 , raise_if_out_of_image = False ) : assert len ( image_shape ) == 2 or ( len ( image_shape ) == 3 and image_shape [ - 1 ] == 1 ) , ( "Expected (H,W) or (H,W,1) as image_shape, got %s." % ( image_shape , ) ) arr = self . draw_points_on_image ( np . zeros ( image_shape , dtype = np . uint8 ) , color = 255 , alpha = alpha , size = size , raise_if_out_of_image = raise_if_out_of_image ) return arr . astype ( np . float32 ) / 255.0
7226	def paint ( self ) : snippet = { 'line-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'line-color' : VectorStyle . get_style_value ( self . color ) , 'line-width' : VectorStyle . get_style_value ( self . width ) , } if self . translate : snippet [ 'line-translate' ] = self . translate if self . dasharray : snippet [ 'line-dasharray' ] = VectorStyle . get_style_value ( self . dasharray ) return snippet
10525	def get_google_playlist ( self , playlist ) : logger . info ( "Loading playlist {0}" . format ( playlist ) ) for google_playlist in self . api . get_all_user_playlist_contents ( ) : if google_playlist [ 'name' ] == playlist or google_playlist [ 'id' ] == playlist : return google_playlist else : logger . warning ( "Playlist {0} does not exist." . format ( playlist ) ) return { }
869	def clear ( cls , persistent = False ) : if persistent : try : os . unlink ( cls . getPath ( ) ) except OSError , e : if e . errno != errno . ENOENT : _getLogger ( ) . exception ( "Error %s while trying to remove dynamic " "configuration file: %s" , e . errno , cls . getPath ( ) ) raise cls . _path = None
4201	def modcovar ( x , order ) : from spectrum import corrmtx import scipy . linalg X = corrmtx ( x , order , 'modified' ) Xc = np . matrix ( X [ : , 1 : ] ) X1 = np . array ( X [ : , 0 ] ) a , residues , rank , singular_values = scipy . linalg . lstsq ( - Xc , X1 ) Cz = np . dot ( X1 . conj ( ) . transpose ( ) , Xc ) e = np . dot ( X1 . conj ( ) . transpose ( ) , X1 ) + np . dot ( Cz , a ) assert e . imag < 1e-4 , 'wierd behaviour' e = float ( e . real ) return a , e
9624	def autodiscover ( ) : from django . conf import settings for application in settings . INSTALLED_APPS : module = import_module ( application ) if module_has_submodule ( module , 'emails' ) : emails = import_module ( '%s.emails' % application ) try : import_module ( '%s.emails.previews' % application ) except ImportError : if module_has_submodule ( emails , 'previews' ) : raise
10258	def count_top_centrality ( graph : BELGraph , number : Optional [ int ] = 30 ) -> Mapping [ BaseEntity , int ] : dd = nx . betweenness_centrality ( graph ) dc = Counter ( dd ) return dict ( dc . most_common ( number ) )
4897	def get_course_duration ( self , obj ) : duration = obj . end - obj . start if obj . start and obj . end else None if duration : return strfdelta ( duration , '{W} weeks {D} days.' ) return ''
9576	def read_header ( fd , endian ) : flag_class , nzmax = read_elements ( fd , endian , [ 'miUINT32' ] ) header = { 'mclass' : flag_class & 0x0FF , 'is_logical' : ( flag_class >> 9 & 1 ) == 1 , 'is_global' : ( flag_class >> 10 & 1 ) == 1 , 'is_complex' : ( flag_class >> 11 & 1 ) == 1 , 'nzmax' : nzmax } header [ 'dims' ] = read_elements ( fd , endian , [ 'miINT32' ] ) header [ 'n_dims' ] = len ( header [ 'dims' ] ) if header [ 'n_dims' ] != 2 : raise ParseError ( 'Only matrices with dimension 2 are supported.' ) header [ 'name' ] = read_elements ( fd , endian , [ 'miINT8' ] , is_name = True ) return header
12491	def as_float_array ( X , copy = True , force_all_finite = True ) : if isinstance ( X , np . matrix ) or ( not isinstance ( X , np . ndarray ) and not sp . issparse ( X ) ) : return check_array ( X , [ 'csr' , 'csc' , 'coo' ] , dtype = np . float64 , copy = copy , force_all_finite = force_all_finite , ensure_2d = False ) elif sp . issparse ( X ) and X . dtype in [ np . float32 , np . float64 ] : return X . copy ( ) if copy else X elif X . dtype in [ np . float32 , np . float64 ] : return X . copy ( 'F' if X . flags [ 'F_CONTIGUOUS' ] else 'C' ) if copy else X else : return X . astype ( np . float32 if X . dtype == np . int32 else np . float64 )
9161	def delete_acl_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] posted = request . json permissions = [ ( x [ 'uid' ] , x [ 'permission' ] , ) for x in posted ] with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : remove_acl ( cursor , uuid_ , permissions ) resp = request . response resp . status_int = 200 return resp
9881	def _reliability_data_to_value_counts ( reliability_data , value_domain ) : return np . array ( [ [ sum ( 1 for rate in unit if rate == v ) for v in value_domain ] for unit in reliability_data . T ] )
5583	def get_path ( self , tile ) : return os . path . join ( * [ self . path , str ( tile . zoom ) , str ( tile . row ) , str ( tile . col ) + self . file_extension ] )
2413	def order_error ( self , first_tag , second_tag , line ) : self . error = True msg = ERROR_MESSAGES [ 'A_BEFORE_B' ] . format ( first_tag , second_tag , line ) self . logger . log ( msg )
10952	def build_funcs ( self ) : def m ( inds = None , slicer = None , flat = True ) : return sample ( self . model , inds = inds , slicer = slicer , flat = flat ) . copy ( ) def r ( inds = None , slicer = None , flat = True ) : return sample ( self . residuals , inds = inds , slicer = slicer , flat = flat ) . copy ( ) def l ( ) : return self . loglikelihood def r_e ( ** kwargs ) : return r ( ** kwargs ) , np . copy ( self . error ) def m_e ( ** kwargs ) : return m ( ** kwargs ) , np . copy ( self . error ) self . fisherinformation = partial ( self . _jtj , funct = m ) self . gradloglikelihood = partial ( self . _grad , funct = l ) self . hessloglikelihood = partial ( self . _hess , funct = l ) self . gradmodel = partial ( self . _grad , funct = m ) self . hessmodel = partial ( self . _hess , funct = m ) self . JTJ = partial ( self . _jtj , funct = r ) self . J = partial ( self . _grad , funct = r ) self . J_e = partial ( self . _grad , funct = r_e , nout = 2 ) self . gradmodel_e = partial ( self . _grad , funct = m_e , nout = 2 ) self . fisherinformation . __doc__ = _graddoc + _sampledoc self . gradloglikelihood . __doc__ = _graddoc self . hessloglikelihood . __doc__ = _graddoc self . gradmodel . __doc__ = _graddoc + _sampledoc self . hessmodel . __doc__ = _graddoc + _sampledoc self . JTJ . __doc__ = _graddoc + _sampledoc self . J . __doc__ = _graddoc + _sampledoc self . _dograddoc ( self . _grad_one_param ) self . _dograddoc ( self . _hess_two_param ) self . _dograddoc ( self . _grad ) self . _dograddoc ( self . _hess ) class _Statewrap ( object ) : def __init__ ( self , obj ) : self . obj = obj def __getitem__ ( self , d = None ) : if d is None : d = self . obj . params return util . delistify ( self . obj . get_values ( d ) , d ) self . state = _Statewrap ( self )
9707	def value_from_datadict ( self , * args , ** kwargs ) : value = super ( RichTextWidget , self ) . value_from_datadict ( * args , ** kwargs ) if value is not None : value = self . get_sanitizer ( ) ( value ) return value
5615	def read_vector_window ( input_files , tile , validity_check = True ) : if not isinstance ( input_files , list ) : input_files = [ input_files ] return [ feature for feature in chain . from_iterable ( [ _read_vector_window ( path , tile , validity_check = validity_check ) for path in input_files ] ) ]
10515	def verifyscrollbarvertical ( self , window_name , object_name ) : try : object_handle = self . _get_object_handle ( window_name , object_name ) if object_handle . AXOrientation == "AXVerticalOrientation" : return 1 except : pass return 0
7407	def populate ( publications ) : customlinks = CustomLink . objects . filter ( publication__in = publications ) customfiles = CustomFile . objects . filter ( publication__in = publications ) publications_ = { } for publication in publications : publication . links = [ ] publication . files = [ ] publications_ [ publication . id ] = publication for link in customlinks : publications_ [ link . publication_id ] . links . append ( link ) for file in customfiles : publications_ [ file . publication_id ] . files . append ( file )
11333	def banner ( * lines , ** kwargs ) : sep = kwargs . get ( "sep" , "*" ) count = kwargs . get ( "width" , globals ( ) [ "WIDTH" ] ) out ( sep * count ) if lines : out ( sep ) for line in lines : out ( "{} {}" . format ( sep , line ) ) out ( sep ) out ( sep * count )
6783	def lock ( self ) : self . init ( ) r = self . local_renderer if self . file_exists ( r . env . lockfile_path ) : raise exceptions . AbortDeployment ( 'Lock file %s exists. Perhaps another deployment is currently underway?' % r . env . lockfile_path ) else : self . vprint ( 'Locking %s.' % r . env . lockfile_path ) r . env . hostname = socket . gethostname ( ) r . run_or_local ( 'echo "{hostname}" > {lockfile_path}' )
11736	def _validate_schema ( obj ) : if obj is not None and not isinstance ( obj , Schema ) : raise IncompatibleSchema ( 'Schema must be of type {0}' . format ( Schema ) ) return obj
1081	def tzname ( self ) : if self . _tzinfo is None : return None name = self . _tzinfo . tzname ( None ) _check_tzname ( name ) return name
9154	def bezier ( self , points ) : coordinates = pgmagick . CoordinateList ( ) for point in points : x , y = float ( point [ 0 ] ) , float ( point [ 1 ] ) coordinates . append ( pgmagick . Coordinate ( x , y ) ) self . drawer . append ( pgmagick . DrawableBezier ( coordinates ) )
3093	def _create_flow ( self , request_handler ) : if self . flow is None : redirect_uri = request_handler . request . relative_url ( self . _callback_path ) self . flow = client . OAuth2WebServerFlow ( self . _client_id , self . _client_secret , self . _scope , redirect_uri = redirect_uri , user_agent = self . _user_agent , auth_uri = self . _auth_uri , token_uri = self . _token_uri , revoke_uri = self . _revoke_uri , ** self . _kwargs )
3853	def get_conv_name ( conv , truncate = False , show_unread = False ) : num_unread = len ( [ conv_event for conv_event in conv . unread_events if isinstance ( conv_event , hangups . ChatMessageEvent ) and not conv . get_user ( conv_event . user_id ) . is_self ] ) if show_unread and num_unread > 0 : postfix = ' ({})' . format ( num_unread ) else : postfix = '' if conv . name is not None : return conv . name + postfix else : participants = sorted ( ( user for user in conv . users if not user . is_self ) , key = lambda user : user . id_ ) names = [ user . first_name for user in participants ] if not participants : return "Empty Conversation" + postfix if len ( participants ) == 1 : return participants [ 0 ] . full_name + postfix elif truncate and len ( participants ) > 2 : return ( ', ' . join ( names [ : 2 ] + [ '+{}' . format ( len ( names ) - 2 ) ] ) + postfix ) else : return ', ' . join ( names ) + postfix
4207	def arcovar ( x , order ) : r from spectrum import corrmtx import scipy . linalg X = corrmtx ( x , order , 'covariance' ) Xc = np . matrix ( X [ : , 1 : ] ) X1 = np . array ( X [ : , 0 ] ) a , _residues , _rank , _singular_values = scipy . linalg . lstsq ( - Xc , X1 ) Cz = np . dot ( X1 . conj ( ) . transpose ( ) , Xc ) e = np . dot ( X1 . conj ( ) . transpose ( ) , X1 ) + np . dot ( Cz , a ) assert e . imag < 1e-4 , 'wierd behaviour' e = float ( e . real ) return a , e
7705	def remove_item ( self , jid ) : if jid not in self . _jids : raise KeyError ( jid ) index = self . _jids [ jid ] for i in range ( index , len ( self . _jids ) ) : self . _jids [ self . _items [ i ] . jid ] -= 1 del self . _jids [ jid ] del self . _items [ index ]
4869	def to_representation ( self , instance ) : updated_program = copy . deepcopy ( instance ) enterprise_customer_catalog = self . context [ 'enterprise_customer_catalog' ] updated_program [ 'enrollment_url' ] = enterprise_customer_catalog . get_program_enrollment_url ( updated_program [ 'uuid' ] ) for course in updated_program [ 'courses' ] : course [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_enrollment_url ( course [ 'key' ] ) for course_run in course [ 'course_runs' ] : course_run [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_run_enrollment_url ( course_run [ 'key' ] ) return updated_program
4646	def exists ( self ) : query = ( "SELECT name FROM sqlite_master " + "WHERE type='table' AND name=?" , ( self . __tablename__ , ) , ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( * query ) return True if cursor . fetchone ( ) else False
5871	def fetch_course_organizations ( course_key ) : queryset = internal . OrganizationCourse . objects . filter ( course_id = text_type ( course_key ) , active = True ) . select_related ( 'organization' ) return [ serializers . serialize_organization_with_course ( organization ) for organization in queryset ]
10016	def swap_environment_cnames ( self , from_env_name , to_env_name ) : self . ebs . swap_environment_cnames ( source_environment_name = from_env_name , destination_environment_name = to_env_name )
4334	def norm ( self , db_level = - 3.0 ) : if not is_number ( db_level ) : raise ValueError ( 'db_level must be a number.' ) effect_args = [ 'norm' , '{:f}' . format ( db_level ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'norm' ) return self
6104	def luminosities_of_galaxies_within_ellipses_in_units ( self , major_axis : dim . Length , unit_luminosity = 'eps' , exposure_time = None ) : return list ( map ( lambda galaxy : galaxy . luminosity_within_ellipse_in_units ( major_axis = major_axis , unit_luminosity = unit_luminosity , kpc_per_arcsec = self . kpc_per_arcsec , exposure_time = exposure_time ) , self . galaxies ) )
12492	def indexable ( * iterables ) : result = [ ] for X in iterables : if sp . issparse ( X ) : result . append ( X . tocsr ( ) ) elif hasattr ( X , "__getitem__" ) or hasattr ( X , "iloc" ) : result . append ( X ) elif X is None : result . append ( X ) else : result . append ( np . array ( X ) ) check_consistent_length ( * result ) return result
9518	def count_sequences ( infile ) : seq_reader = sequences . file_reader ( infile ) n = 0 for seq in seq_reader : n += 1 return n
6708	def get_file_hash ( fin , block_size = 2 ** 20 ) : if isinstance ( fin , six . string_types ) : fin = open ( fin ) h = hashlib . sha512 ( ) while True : data = fin . read ( block_size ) if not data : break try : h . update ( data ) except TypeError : h . update ( data . encode ( 'utf-8' ) ) return h . hexdigest ( )
13829	def remove ( self , collection , ** kwargs ) : callback = kwargs . pop ( 'callback' ) yield Op ( self . db [ collection ] . remove , kwargs ) callback ( )
4552	def fill_triangle ( setter , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : a = b = y = last = 0 if y0 > y1 : y0 , y1 = y1 , y0 x0 , x1 = x1 , x0 if y1 > y2 : y2 , y1 = y1 , y2 x2 , x1 = x1 , x2 if y0 > y1 : y0 , y1 = y1 , y0 x0 , x1 = x1 , x0 if y0 == y2 : a = b = x0 if x1 < a : a = x1 elif x1 > b : b = x1 if x2 < a : a = x2 elif x2 > b : b = x2 _draw_fast_hline ( setter , a , y0 , b - a + 1 , color , aa ) dx01 = x1 - x0 dy01 = y1 - y0 dx02 = x2 - x0 dy02 = y2 - y0 dx12 = x2 - x1 dy12 = y2 - y1 sa = 0 sb = 0 if y1 == y2 : last = y1 else : last = y1 - 1 for y in range ( y , last + 1 ) : a = x0 + sa / dy01 b = x0 + sb / dy02 sa += dx01 sb += dx02 if a > b : a , b = b , a _draw_fast_hline ( setter , a , y , b - a + 1 , color , aa ) sa = dx12 * ( y - y1 ) sb = dx02 * ( y - y0 ) for y in range ( y , y2 + 1 ) : a = x1 + sa / dy12 b = x0 + sb / dy02 sa += dx12 sb += dx02 if a > b : a , b = b , a _draw_fast_hline ( setter , a , y , b - a + 1 , color , aa )
1786	def CMPXCHG8B ( cpu , dest ) : size = dest . size cmp_reg_name_l = { 64 : 'EAX' , 128 : 'RAX' } [ size ] cmp_reg_name_h = { 64 : 'EDX' , 128 : 'RDX' } [ size ] src_reg_name_l = { 64 : 'EBX' , 128 : 'RBX' } [ size ] src_reg_name_h = { 64 : 'ECX' , 128 : 'RCX' } [ size ] cmph = cpu . read_register ( cmp_reg_name_h ) cmpl = cpu . read_register ( cmp_reg_name_l ) srch = cpu . read_register ( src_reg_name_h ) srcl = cpu . read_register ( src_reg_name_l ) cmp0 = Operators . CONCAT ( size , cmph , cmpl ) src0 = Operators . CONCAT ( size , srch , srcl ) arg_dest = dest . read ( ) cpu . ZF = arg_dest == cmp0 dest . write ( Operators . ITEBV ( size , cpu . ZF , Operators . CONCAT ( size , srch , srcl ) , arg_dest ) ) cpu . write_register ( cmp_reg_name_l , Operators . ITEBV ( size // 2 , cpu . ZF , cmpl , Operators . EXTRACT ( arg_dest , 0 , size // 2 ) ) ) cpu . write_register ( cmp_reg_name_h , Operators . ITEBV ( size // 2 , cpu . ZF , cmph , Operators . EXTRACT ( arg_dest , size // 2 , size // 2 ) ) )
12218	def _bind_args ( sig , param_matchers , args , kwargs ) : bound = sig . bind ( * args , ** kwargs ) if not all ( param_matcher ( bound . arguments [ param_name ] ) for param_name , param_matcher in param_matchers ) : raise TypeError return bound
4903	def link_to_modal ( link_text , index , autoescape = True ) : link = ( '<a' ' href="#!"' ' class="text-underline view-course-details-link"' ' id="view-course-details-link-{index}"' ' data-toggle="modal"' ' data-target="#course-details-modal-{index}"' '>{link_text}</a>' ) . format ( index = index , link_text = link_text , ) return mark_safe ( link )
12207	def truncate ( text , max_len = 350 , end = '...' ) : if len ( text ) <= max_len : return text return text [ : max_len ] . rsplit ( ' ' , maxsplit = 1 ) [ 0 ] + end
8697	def __clear_buffers ( self ) : try : self . _port . reset_input_buffer ( ) self . _port . reset_output_buffer ( ) except AttributeError : self . _port . flushInput ( ) self . _port . flushOutput ( )
9038	def bounding_box ( self ) : min_x , min_y , max_x , max_y = zip ( * list ( self . walk_rows ( lambda row : row . bounding_box ) ) ) return min ( min_x ) , min ( min_y ) , max ( max_x ) , max ( max_y )
8623	def get_self_user_id ( session ) : response = make_get_request ( session , 'self' ) if response . status_code == 200 : return response . json ( ) [ 'result' ] [ 'id' ] else : raise UserIdNotRetrievedException ( 'Error retrieving user id: %s' % response . text , response . text )
9545	def add_value_predicate ( self , field_name , value_predicate , code = VALUE_PREDICATE_FALSE , message = MESSAGES [ VALUE_PREDICATE_FALSE ] , modulus = 1 ) : assert field_name in self . _field_names , 'unexpected field name: %s' % field_name assert callable ( value_predicate ) , 'value predicate must be a callable function' t = field_name , value_predicate , code , message , modulus self . _value_predicates . append ( t )
13021	def process_columns ( self , columns ) : if type ( columns ) == list : self . columns = columns elif type ( columns ) == str : self . columns = [ c . strip ( ) for c in columns . split ( ) ] elif type ( columns ) == IntEnum : self . columns = [ str ( c ) for c in columns ] else : raise RawlException ( "Unknown format for columns" )
12327	def update ( globalvars ) : global config profileini = getprofileini ( ) config = configparser . ConfigParser ( ) config . read ( profileini ) defaults = { } if globalvars is not None : defaults = { a [ 0 ] : a [ 1 ] for a in globalvars } generic_configs = [ { 'name' : 'User' , 'nature' : 'generic' , 'description' : "General information" , 'variables' : [ 'user.email' , 'user.name' , 'user.fullname' ] , 'defaults' : { 'user.email' : { 'value' : defaults . get ( 'user.email' , '' ) , 'description' : "Email address" , 'validator' : EmailValidator ( ) } , 'user.fullname' : { 'value' : defaults . get ( 'user.fullname' , '' ) , 'description' : "Full Name" , 'validator' : NonEmptyValidator ( ) } , 'user.name' : { 'value' : defaults . get ( 'user.name' , getpass . getuser ( ) ) , 'description' : "Name" , 'validator' : NonEmptyValidator ( ) } , } } ] mgr = plugins_get_mgr ( ) extra_configs = mgr . gather_configs ( ) allconfigs = generic_configs + extra_configs for c in allconfigs : name = c [ 'name' ] for v in c [ 'variables' ] : try : c [ 'defaults' ] [ v ] [ 'value' ] = config [ name ] [ v ] except : continue for c in allconfigs : print ( "" ) print ( c [ 'description' ] ) print ( "==================" ) if len ( c [ 'variables' ] ) == 0 : print ( "Nothing to do. Enabled by default" ) continue name = c [ 'name' ] config [ name ] = { } config [ name ] [ 'nature' ] = c [ 'nature' ] for v in c [ 'variables' ] : value = '' description = v + " " helptext = "" validator = None if v in c [ 'defaults' ] : value = c [ 'defaults' ] [ v ] . get ( 'value' , '' ) helptext = c [ 'defaults' ] [ v ] . get ( "description" , "" ) validator = c [ 'defaults' ] [ v ] . get ( 'validator' , None ) if helptext != "" : description += "(" + helptext + ")" while True : choice = input_with_default ( description , value ) if validator is not None : if validator . is_valid ( choice ) : break else : print ( "Invalid input. Expected input is {}" . format ( validator . message ) ) else : break config [ name ] [ v ] = choice if v == 'enable' and choice == 'n' : break with open ( profileini , 'w' ) as fd : config . write ( fd ) print ( "Updated profile file:" , config )
1693	def map ( self , map_function ) : from heronpy . streamlet . impl . mapbolt import MapStreamlet map_streamlet = MapStreamlet ( map_function , self ) self . _add_child ( map_streamlet ) return map_streamlet
1928	def load_overrides ( path = None ) : if path is not None : names = [ path ] else : possible_names = [ 'mcore.yml' , 'manticore.yml' ] names = [ os . path . join ( '.' , '' . join ( x ) ) for x in product ( [ '' , '.' ] , possible_names ) ] for name in names : try : with open ( name , 'r' ) as yml_f : logger . info ( f'Reading configuration from {name}' ) parse_config ( yml_f ) break except FileNotFoundError : pass else : if path is not None : raise FileNotFoundError ( f"'{path}' not found for config overrides" )
4684	def getPublicKeys ( self , current = False ) : pubkeys = self . store . getPublicKeys ( ) if not current : return pubkeys pubs = [ ] for pubkey in pubkeys : if pubkey [ : len ( self . prefix ) ] == self . prefix : pubs . append ( pubkey ) return pubs
7621	def hierarchy ( ref , est , ** kwargs ) : r namespace = 'multi_segment' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_hier , ref_hier_lab = hierarchy_flatten ( ref ) est_hier , est_hier_lab = hierarchy_flatten ( est ) return mir_eval . hierarchy . evaluate ( ref_hier , ref_hier_lab , est_hier , est_hier_lab , ** kwargs )
10726	def _handle_variant ( self ) : def the_func ( a_tuple , variant = 0 ) : ( signature , an_obj ) = a_tuple ( func , sig ) = self . COMPLETE . parseString ( signature ) [ 0 ] assert sig == signature ( xformed , _ ) = func ( an_obj , variant = variant + 1 ) return ( xformed , xformed . variant_level ) return ( the_func , 'v' )
3842	async def sync_recent_conversations ( self , sync_recent_conversations_request ) : response = hangouts_pb2 . SyncRecentConversationsResponse ( ) await self . _pb_request ( 'conversations/syncrecentconversations' , sync_recent_conversations_request , response ) return response
36	def get_local_rank_size ( comm ) : this_node = platform . node ( ) ranks_nodes = comm . allgather ( ( comm . Get_rank ( ) , this_node ) ) node2rankssofar = defaultdict ( int ) local_rank = None for ( rank , node ) in ranks_nodes : if rank == comm . Get_rank ( ) : local_rank = node2rankssofar [ node ] node2rankssofar [ node ] += 1 assert local_rank is not None return local_rank , node2rankssofar [ this_node ]
10728	def _handle_struct ( toks ) : subtrees = toks [ 1 : - 1 ] signature = '' . join ( s for ( _ , s ) in subtrees ) funcs = [ f for ( f , _ ) in subtrees ] def the_func ( a_list , variant = 0 ) : if isinstance ( a_list , dict ) : raise IntoDPValueError ( a_list , "a_list" , "must be a simple sequence, is a dict" ) if len ( a_list ) != len ( funcs ) : raise IntoDPValueError ( a_list , "a_list" , "must have exactly %u items, has %u" % ( len ( funcs ) , len ( a_list ) ) ) elements = [ f ( x ) for ( f , x ) in zip ( funcs , a_list ) ] level = 0 if elements == [ ] else max ( x for ( _ , x ) in elements ) ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( level , variant ) return ( dbus . types . Struct ( ( x for ( x , _ ) in elements ) , signature = signature , variant_level = obj_level ) , func_level ) return ( the_func , '(' + signature + ')' )
13859	def contents ( self , f , text ) : text += self . _read ( f . abs_path ) + "\r\n" return text
11162	def size ( self ) : try : return self . _stat . st_size except : self . _stat = self . stat ( ) return self . size
9874	def aggregate ( l ) : tree = radix . Radix ( ) for item in l : try : tree . add ( item ) except ( ValueError ) as err : raise Exception ( "ERROR: invalid IP prefix: {}" . format ( item ) ) return aggregate_tree ( tree ) . prefixes ( )
1255	def setup_scaffold ( self ) : if self . execution_type == "single" : global_variables = self . get_variables ( include_submodules = True , include_nontrainable = True ) init_op = tf . variables_initializer ( var_list = global_variables ) if self . summarizer_init_op is not None : init_op = tf . group ( init_op , self . summarizer_init_op ) if self . graph_summary is None : ready_op = tf . report_uninitialized_variables ( var_list = global_variables ) ready_for_local_init_op = None local_init_op = None else : ready_op = None ready_for_local_init_op = tf . report_uninitialized_variables ( var_list = global_variables ) local_init_op = self . graph_summary else : global_variables = self . global_model . get_variables ( include_submodules = True , include_nontrainable = True ) local_variables = self . get_variables ( include_submodules = True , include_nontrainable = True ) init_op = tf . variables_initializer ( var_list = global_variables ) if self . summarizer_init_op is not None : init_op = tf . group ( init_op , self . summarizer_init_op ) ready_op = tf . report_uninitialized_variables ( var_list = ( global_variables + local_variables ) ) ready_for_local_init_op = tf . report_uninitialized_variables ( var_list = global_variables ) if self . graph_summary is None : local_init_op = tf . group ( tf . variables_initializer ( var_list = local_variables ) , * ( tf . assign ( ref = local_var , value = global_var ) for local_var , global_var in zip ( self . get_variables ( include_submodules = True ) , self . global_model . get_variables ( include_submodules = True ) ) ) ) else : local_init_op = tf . group ( tf . variables_initializer ( var_list = local_variables ) , self . graph_summary , * ( tf . assign ( ref = local_var , value = global_var ) for local_var , global_var in zip ( self . get_variables ( include_submodules = True ) , self . global_model . get_variables ( include_submodules = True ) ) ) ) def init_fn ( scaffold , session ) : if self . saver_spec is not None and self . saver_spec . get ( 'load' , True ) : directory = self . saver_spec [ 'directory' ] file = self . saver_spec . get ( 'file' ) if file is None : file = tf . train . latest_checkpoint ( checkpoint_dir = directory , latest_filename = None ) elif not os . path . isfile ( file ) : file = os . path . join ( directory , file ) if file is not None : try : scaffold . saver . restore ( sess = session , save_path = file ) session . run ( fetches = self . list_buffer_index_reset_op ) except tf . errors . NotFoundError : raise TensorForceError ( "Error: Existing checkpoint could not be loaded! Set \"load\" to false in saver_spec." ) self . scaffold = tf . train . Scaffold ( init_op = init_op , init_feed_dict = None , init_fn = init_fn , ready_op = ready_op , ready_for_local_init_op = ready_for_local_init_op , local_init_op = local_init_op , summary_op = None , saver = self . saver , copy_from_scaffold = None )
9782	def delete ( ctx ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) if not click . confirm ( "Are sure you want to delete build job `{}`" . format ( _build ) ) : click . echo ( 'Existing without deleting build job.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . build_job . delete_build ( user , project_name , _build ) BuildJobManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Build job `{}` was deleted successfully" . format ( _build ) )
3170	def send ( self , campaign_id ) : self . campaign_id = campaign_id return self . _mc_client . _post ( url = self . _build_path ( campaign_id , 'actions/send' ) )
1833	def JCXZ ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CX == 0 , target . read ( ) , cpu . PC )
11013	def lint ( context ) : config = context . obj try : run ( 'flake8 {dir} --exclude={exclude}' . format ( dir = config [ 'CWD' ] , exclude = ',' . join ( EXCLUDE ) , ) ) except SubprocessError : context . exit ( 1 )
13818	def _ConvertMessage ( value , message ) : message_descriptor = message . DESCRIPTOR full_name = message_descriptor . full_name if _IsWrapperMessage ( message_descriptor ) : _ConvertWrapperMessage ( value , message ) elif full_name in _WKTJSONMETHODS : _WKTJSONMETHODS [ full_name ] [ 1 ] ( value , message ) else : _ConvertFieldValuePair ( value , message )
13727	def set_delegate ( address = None , pubkey = None , secret = None ) : c . DELEGATE [ 'ADDRESS' ] = address c . DELEGATE [ 'PUBKEY' ] = pubkey c . DELEGATE [ 'PASSPHRASE' ] = secret
9171	def declare_browsable_routes ( config ) : config . add_notfound_view ( default_exceptionresponse_view , append_slash = True ) add_route = config . add_route add_route ( 'admin-index' , '/a/' ) add_route ( 'admin-moderation' , '/a/moderation/' ) add_route ( 'admin-api-keys' , '/a/api-keys/' ) add_route ( 'admin-add-site-messages' , '/a/site-messages/' , request_method = 'GET' ) add_route ( 'admin-add-site-messages-POST' , '/a/site-messages/' , request_method = 'POST' ) add_route ( 'admin-delete-site-messages' , '/a/site-messages/' , request_method = 'DELETE' ) add_route ( 'admin-edit-site-message' , '/a/site-messages/{id}/' , request_method = 'GET' ) add_route ( 'admin-edit-site-message-POST' , '/a/site-messages/{id}/' , request_method = 'POST' ) add_route ( 'admin-content-status' , '/a/content-status/' ) add_route ( 'admin-content-status-single' , '/a/content-status/{uuid}' ) add_route ( 'admin-print-style' , '/a/print-style/' ) add_route ( 'admin-print-style-single' , '/a/print-style/{style}' )
2756	def get_all_tags ( self ) : data = self . get_data ( "tags" ) return [ Tag ( token = self . token , ** tag ) for tag in data [ 'tags' ] ]
12957	def _get_key_for_index ( self , indexedField , val ) : if hasattr ( indexedField , 'toIndex' ) : val = indexedField . toIndex ( val ) else : val = self . fields [ indexedField ] . toIndex ( val ) return '' . join ( [ INDEXED_REDIS_PREFIX , self . keyName , ':idx:' , indexedField , ':' , val ] )
7502	def fill_boot ( seqarr , newboot , newmap , spans , loci ) : cidx = 0 for i in xrange ( loci . shape [ 0 ] ) : x1 = spans [ loci [ i ] ] [ 0 ] x2 = spans [ loci [ i ] ] [ 1 ] cols = seqarr [ : , x1 : x2 ] cord = np . random . choice ( cols . shape [ 1 ] , cols . shape [ 1 ] , replace = False ) rcols = cols [ : , cord ] newboot [ : , cidx : cidx + cols . shape [ 1 ] ] = rcols newmap [ cidx : cidx + cols . shape [ 1 ] , 0 ] = i + 1 cidx += cols . shape [ 1 ] return newboot , newmap
3269	def md_jdbc_virtual_table ( key , node ) : name = node . find ( "name" ) sql = node . find ( "sql" ) escapeSql = node . find ( "escapeSql" ) escapeSql = escapeSql . text if escapeSql is not None else None keyColumn = node . find ( "keyColumn" ) keyColumn = keyColumn . text if keyColumn is not None else None n_g = node . find ( "geometry" ) geometry = JDBCVirtualTableGeometry ( n_g . find ( "name" ) , n_g . find ( "type" ) , n_g . find ( "srid" ) ) parameters = [ ] for n_p in node . findall ( "parameter" ) : p_name = n_p . find ( "name" ) p_defaultValue = n_p . find ( "defaultValue" ) p_defaultValue = p_defaultValue . text if p_defaultValue is not None else None p_regexpValidator = n_p . find ( "regexpValidator" ) p_regexpValidator = p_regexpValidator . text if p_regexpValidator is not None else None parameters . append ( JDBCVirtualTableParam ( p_name , p_defaultValue , p_regexpValidator ) ) return JDBCVirtualTable ( name , sql , escapeSql , geometry , keyColumn , parameters )
13792	def get_function ( function_name ) : module , basename = str ( function_name ) . rsplit ( '.' , 1 ) try : return getattr ( __import__ ( module , fromlist = [ basename ] ) , basename ) except ( ImportError , AttributeError ) : raise FunctionNotFound ( function_name )
11461	def add_control_number ( self , tag , value ) : record_add_field ( self . record , tag , controlfield_value = value )
2795	def create ( self ) : params = { 'name' : self . name , 'region' : self . region , 'url' : self . url , 'distribution' : self . distribution , 'description' : self . description , 'tags' : self . tags } data = self . get_data ( 'images' , type = POST , params = params ) if data : for attr in data [ 'image' ] . keys ( ) : setattr ( self , attr , data [ 'image' ] [ attr ] ) return self
13583	def admin_obj_link ( obj , display = '' ) : url = reverse ( 'admin:%s_%s_changelist' % ( obj . _meta . app_label , obj . _meta . model_name ) ) url += '?id__exact=%s' % obj . id text = str ( obj ) if display : text = display return format_html ( '<a href="{}">{}</a>' , url , text )
11046	def init_logging ( log_level ) : log_level_filter = LogLevelFilterPredicate ( LogLevel . levelWithName ( log_level ) ) log_level_filter . setLogLevelForNamespace ( 'twisted.web.client._HTTP11ClientFactory' , LogLevel . warn ) log_observer = FilteringLogObserver ( textFileLogObserver ( sys . stdout ) , [ log_level_filter ] ) globalLogPublisher . addObserver ( log_observer )
10266	def collapse_consistent_edges ( graph : BELGraph ) : for u , v in graph . edges ( ) : relation = pair_is_consistent ( graph , u , v ) if not relation : continue edges = [ ( u , v , k ) for k in graph [ u ] [ v ] ] graph . remove_edges_from ( edges ) graph . add_edge ( u , v , attr_dict = { RELATION : relation } )
4180	def _coeff4 ( N , a0 , a1 , a2 , a3 ) : if N == 1 : return ones ( 1 ) n = arange ( 0 , N ) N1 = N - 1. w = a0 - a1 * cos ( 2. * pi * n / N1 ) + a2 * cos ( 4. * pi * n / N1 ) - a3 * cos ( 6. * pi * n / N1 ) return w
2279	def retrieve_adjacency_matrix ( graph , order_nodes = None , weight = False ) : if isinstance ( graph , np . ndarray ) : return graph elif isinstance ( graph , nx . DiGraph ) : if order_nodes is None : order_nodes = graph . nodes ( ) if not weight : return np . array ( nx . adjacency_matrix ( graph , order_nodes , weight = None ) . todense ( ) ) else : return np . array ( nx . adjacency_matrix ( graph , order_nodes ) . todense ( ) ) else : raise TypeError ( "Only networkx.DiGraph and np.ndarray (adjacency matrixes) are supported." )
5140	def new_noncomment ( self , start_lineno , end_lineno ) : block = NonComment ( start_lineno , end_lineno ) self . blocks . append ( block ) self . current_block = block
8612	def list_volumes ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/volumes?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
8577	def get_server ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s?depth=%s' % ( datacenter_id , server_id , str ( depth ) ) ) return response
4197	def HERMTOEP ( T0 , T , Z ) : assert len ( T ) > 0 M = len ( T ) X = numpy . zeros ( M + 1 , dtype = complex ) A = numpy . zeros ( M , dtype = complex ) P = T0 if P == 0 : raise ValueError ( "P must be different from zero" ) X [ 0 ] = Z [ 0 ] / T0 for k in range ( 0 , M ) : save = T [ k ] beta = X [ 0 ] * T [ k ] if k == 0 : temp = - save / P else : for j in range ( 0 , k ) : save = save + A [ j ] * T [ k - j - 1 ] beta = beta + X [ j + 1 ] * T [ k - j - 1 ] temp = - save / P P = P * ( 1. - ( temp . real ** 2 + temp . imag ** 2 ) ) if P <= 0 : raise ValueError ( "singular matrix" ) A [ k ] = temp alpha = ( Z [ k + 1 ] - beta ) / P if k == 0 : X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * A [ k - j ] . conjugate ( ) continue khalf = ( k + 1 ) // 2 for j in range ( 0 , khalf ) : kj = k - j - 1 save = A [ j ] A [ j ] = save + temp * A [ kj ] . conjugate ( ) if j != kj : A [ kj ] = A [ kj ] + temp * save . conjugate ( ) X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * A [ k - j ] . conjugate ( ) return X
1557	def _get_stream_id ( comp_name , stream_id ) : proto_stream_id = topology_pb2 . StreamId ( ) proto_stream_id . id = stream_id proto_stream_id . component_name = comp_name return proto_stream_id
8908	def fetch_by_name ( self , name ) : service = self . collection . find_one ( { 'name' : name } ) if not service : raise ServiceNotFound return Service ( service )
2007	def _deserialize_uint ( data , nbytes = 32 , padding = 0 , offset = 0 ) : assert isinstance ( data , ( bytearray , Array ) ) value = ABI . _readBE ( data , nbytes , padding = True , offset = offset ) value = Operators . ZEXTEND ( value , ( nbytes + padding ) * 8 ) return value
7721	def get_history ( self ) : for child in xml_element_iter ( self . xmlnode . children ) : if get_node_ns_uri ( child ) == MUC_NS and child . name == "history" : maxchars = from_utf8 ( child . prop ( "maxchars" ) ) if maxchars is not None : maxchars = int ( maxchars ) maxstanzas = from_utf8 ( child . prop ( "maxstanzas" ) ) if maxstanzas is not None : maxstanzas = int ( maxstanzas ) maxseconds = from_utf8 ( child . prop ( "maxseconds" ) ) if maxseconds is not None : maxseconds = int ( maxseconds ) since = None return HistoryParameters ( maxchars , maxstanzas , maxseconds , since )
8728	def strftime ( fmt , t ) : if isinstance ( t , ( time . struct_time , tuple ) ) : t = datetime . datetime ( * t [ : 6 ] ) assert isinstance ( t , ( datetime . datetime , datetime . time , datetime . date ) ) try : year = t . year if year < 1900 : t = t . replace ( year = 1900 ) except AttributeError : year = 1900 subs = ( ( '%Y' , '%04d' % year ) , ( '%y' , '%02d' % ( year % 100 ) ) , ( '%s' , '%03d' % ( t . microsecond // 1000 ) ) , ( '%u' , '%03d' % ( t . microsecond % 1000 ) ) ) def doSub ( s , sub ) : return s . replace ( * sub ) def doSubs ( s ) : return functools . reduce ( doSub , subs , s ) fmt = '%%' . join ( map ( doSubs , fmt . split ( '%%' ) ) ) return t . strftime ( fmt )
12271	def get_schema ( self , filename ) : table_set = self . read_file ( filename ) if table_set is None : return [ ] row_set = table_set . tables [ 0 ] offset , headers = headers_guess ( row_set . sample ) row_set . register_processor ( headers_processor ( headers ) ) row_set . register_processor ( offset_processor ( offset + 1 ) ) types = type_guess ( row_set . sample , strict = True ) sample = next ( row_set . sample ) clean = lambda v : str ( v ) if not isinstance ( v , str ) else v schema = [ ] for i , h in enumerate ( headers ) : schema . append ( [ h , str ( types [ i ] ) , clean ( sample [ i ] . value ) ] ) return schema
4280	def watermark ( im , mark , position , opacity = 1 ) : if opacity < 1 : mark = reduce_opacity ( mark , opacity ) if im . mode != 'RGBA' : im = im . convert ( 'RGBA' ) layer = Image . new ( 'RGBA' , im . size , ( 0 , 0 , 0 , 0 ) ) if position == 'tile' : for y in range ( 0 , im . size [ 1 ] , mark . size [ 1 ] ) : for x in range ( 0 , im . size [ 0 ] , mark . size [ 0 ] ) : layer . paste ( mark , ( x , y ) ) elif position == 'scale' : ratio = min ( float ( im . size [ 0 ] ) / mark . size [ 0 ] , float ( im . size [ 1 ] ) / mark . size [ 1 ] ) w = int ( mark . size [ 0 ] * ratio ) h = int ( mark . size [ 1 ] * ratio ) mark = mark . resize ( ( w , h ) ) layer . paste ( mark , ( int ( ( im . size [ 0 ] - w ) / 2 ) , int ( ( im . size [ 1 ] - h ) / 2 ) ) ) else : layer . paste ( mark , position ) return Image . composite ( layer , im , layer )
9868	def rt_subscription_running ( self ) : return ( self . _tibber_control . sub_manager is not None and self . _tibber_control . sub_manager . is_running and self . _subscription_id is not None )
260	def create_perf_attrib_stats ( perf_attrib , risk_exposures ) : summary = OrderedDict ( ) total_returns = perf_attrib [ 'total_returns' ] specific_returns = perf_attrib [ 'specific_returns' ] common_returns = perf_attrib [ 'common_returns' ] summary [ 'Annualized Specific Return' ] = ep . annual_return ( specific_returns ) summary [ 'Annualized Common Return' ] = ep . annual_return ( common_returns ) summary [ 'Annualized Total Return' ] = ep . annual_return ( total_returns ) summary [ 'Specific Sharpe Ratio' ] = ep . sharpe_ratio ( specific_returns ) summary [ 'Cumulative Specific Return' ] = ep . cum_returns_final ( specific_returns ) summary [ 'Cumulative Common Return' ] = ep . cum_returns_final ( common_returns ) summary [ 'Total Returns' ] = ep . cum_returns_final ( total_returns ) summary = pd . Series ( summary , name = '' ) annualized_returns_by_factor = [ ep . annual_return ( perf_attrib [ c ] ) for c in risk_exposures . columns ] cumulative_returns_by_factor = [ ep . cum_returns_final ( perf_attrib [ c ] ) for c in risk_exposures . columns ] risk_exposure_summary = pd . DataFrame ( data = OrderedDict ( [ ( 'Average Risk Factor Exposure' , risk_exposures . mean ( axis = 'rows' ) ) , ( 'Annualized Return' , annualized_returns_by_factor ) , ( 'Cumulative Return' , cumulative_returns_by_factor ) , ] ) , index = risk_exposures . columns , ) return summary , risk_exposure_summary
203	def to_heatmaps ( self , only_nonempty = False , not_none_if_no_nonempty = False ) : from imgaug . augmentables . heatmaps import HeatmapsOnImage if not only_nonempty : return HeatmapsOnImage . from_0to1 ( self . arr , self . shape , min_value = 0.0 , max_value = 1.0 ) else : nonempty_mask = np . sum ( self . arr , axis = ( 0 , 1 ) ) > 0 + 1e-4 if np . sum ( nonempty_mask ) == 0 : if not_none_if_no_nonempty : nonempty_mask [ 0 ] = True else : return None , [ ] class_indices = np . arange ( self . arr . shape [ 2 ] ) [ nonempty_mask ] channels = self . arr [ ... , class_indices ] return HeatmapsOnImage ( channels , self . shape , min_value = 0.0 , max_value = 1.0 ) , class_indices
12784	def get_xdg_dirs ( self ) : config_dirs = getenv ( 'XDG_CONFIG_DIRS' , '' ) if config_dirs : self . _log . debug ( 'XDG_CONFIG_DIRS is set to %r' , config_dirs ) output = [ ] for path in reversed ( config_dirs . split ( ':' ) ) : output . append ( join ( path , self . group_name , self . app_name ) ) return output return [ '/etc/xdg/%s/%s' % ( self . group_name , self . app_name ) ]
2156	def launch ( self , monitor = False , wait = False , timeout = None , ** kwargs ) : r = client . get ( '/' ) if 'ad_hoc_commands' not in r . json ( ) : raise exc . TowerCLIError ( 'Your host is running an outdated version' 'of Ansible Tower that can not run ' 'ad-hoc commands (2.2 or earlier)' ) self . _pop_none ( kwargs ) debug . log ( 'Launching the ad-hoc command.' , header = 'details' ) result = client . post ( self . endpoint , data = kwargs ) command = result . json ( ) command_id = command [ 'id' ] if monitor : return self . monitor ( command_id , timeout = timeout ) elif wait : return self . wait ( command_id , timeout = timeout ) answer = OrderedDict ( ( ( 'changed' , True ) , ( 'id' , command_id ) , ) ) answer . update ( result . json ( ) ) return answer
1080	def isocalendar ( self ) : year = self . _year week1monday = _isoweek1monday ( year ) today = _ymd2ord ( self . _year , self . _month , self . _day ) week , day = divmod ( today - week1monday , 7 ) if week < 0 : year -= 1 week1monday = _isoweek1monday ( year ) week , day = divmod ( today - week1monday , 7 ) elif week >= 52 : if today >= _isoweek1monday ( year + 1 ) : year += 1 week = 0 return year , week + 1 , day + 1
9863	def get_homes ( self , only_active = True ) : return [ self . get_home ( home_id ) for home_id in self . get_home_ids ( only_active ) ]
12308	def get_files_to_commit ( autooptions ) : workingdir = autooptions [ 'working-directory' ] includes = autooptions [ 'track' ] [ 'includes' ] excludes = autooptions [ 'track' ] [ 'excludes' ] includes = r'|' . join ( [ fnmatch . translate ( x ) for x in includes ] ) excludes = r'|' . join ( [ fnmatch . translate ( x ) for x in excludes ] ) or r'$.' matched_files = [ ] for root , dirs , files in os . walk ( workingdir ) : dirs [ : ] = [ d for d in dirs if not re . match ( excludes , d ) ] files = [ f for f in files if not re . match ( excludes , f ) ] files = [ f for f in files if re . match ( includes , f ) ] files = [ os . path . join ( root , f ) for f in files ] matched_files . extend ( files ) return matched_files
6616	def receive ( self ) : pkgidx_result_pairs = self . receive_all ( ) if pkgidx_result_pairs is None : return results = [ r for _ , r in pkgidx_result_pairs ] return results
10442	def getobjectlist ( self , window_name ) : try : window_handle , name , app = self . _get_window_handle ( window_name , True ) object_list = self . _get_appmap ( window_handle , name , True ) except atomac . _a11y . ErrorInvalidUIElement : self . _windows = { } window_handle , name , app = self . _get_window_handle ( window_name , True ) object_list = self . _get_appmap ( window_handle , name , True ) return object_list . keys ( )
10150	def generate ( self , title = None , version = None , base_path = None , info = None , swagger = None , ** kwargs ) : title = title or self . api_title version = version or self . api_version info = info or self . swagger . get ( 'info' , { } ) swagger = swagger or self . swagger base_path = base_path or self . base_path swagger = swagger . copy ( ) info . update ( title = title , version = version ) swagger . update ( swagger = '2.0' , info = info , basePath = base_path ) paths , tags = self . _build_paths ( ) if tags : swagger . setdefault ( 'tags' , [ ] ) tag_names = { t [ 'name' ] for t in swagger [ 'tags' ] } for tag in tags : if tag [ 'name' ] not in tag_names : swagger [ 'tags' ] . append ( tag ) if paths : swagger . setdefault ( 'paths' , { } ) merge_dicts ( swagger [ 'paths' ] , paths ) definitions = self . definitions . definition_registry if definitions : swagger . setdefault ( 'definitions' , { } ) merge_dicts ( swagger [ 'definitions' ] , definitions ) parameters = self . parameters . parameter_registry if parameters : swagger . setdefault ( 'parameters' , { } ) merge_dicts ( swagger [ 'parameters' ] , parameters ) responses = self . responses . response_registry if responses : swagger . setdefault ( 'responses' , { } ) merge_dicts ( swagger [ 'responses' ] , responses ) return swagger
11984	async def copy_storage_object ( self , source_bucket , source_key , bucket , key ) : info = await self . head_object ( Bucket = source_bucket , Key = source_key ) size = info [ 'ContentLength' ] if size > MULTI_PART_SIZE : result = await _multipart_copy ( self , source_bucket , source_key , bucket , key , size ) else : result = await self . copy_object ( Bucket = bucket , Key = key , CopySource = _source_string ( source_bucket , source_key ) ) return result
3423	def resettable ( f ) : def wrapper ( self , new_value ) : context = get_context ( self ) if context : old_value = getattr ( self , f . __name__ ) if old_value == new_value : return context ( partial ( f , self , old_value ) ) f ( self , new_value ) return wrapper
12547	def abs_img ( img ) : bool_img = np . abs ( read_img ( img ) . get_data ( ) ) return bool_img . astype ( int )
3891	def html ( tag ) : return ( HTML_START . format ( tag = tag ) , HTML_END . format ( tag = tag ) )
8784	def create_port ( self , context , network_id , port_id , ** kwargs ) : LOG . info ( "create_port %s %s %s" % ( context . tenant_id , network_id , port_id ) ) if not kwargs . get ( 'base_net_driver' ) : raise IronicException ( msg = 'base_net_driver required.' ) base_net_driver = kwargs [ 'base_net_driver' ] if not kwargs . get ( 'device_id' ) : raise IronicException ( msg = 'device_id required.' ) device_id = kwargs [ 'device_id' ] if not kwargs . get ( 'instance_node_id' ) : raise IronicException ( msg = 'instance_node_id required.' ) instance_node_id = kwargs [ 'instance_node_id' ] if not kwargs . get ( 'mac_address' ) : raise IronicException ( msg = 'mac_address is required.' ) mac_address = str ( netaddr . EUI ( kwargs [ "mac_address" ] [ "address" ] ) ) mac_address = mac_address . replace ( '-' , ':' ) if kwargs . get ( 'security_groups' ) : msg = 'ironic driver does not support security group operations.' raise IronicException ( msg = msg ) fixed_ips = [ ] addresses = kwargs . get ( 'addresses' ) if not isinstance ( addresses , list ) : addresses = [ addresses ] for address in addresses : fixed_ips . append ( self . _make_fixed_ip_dict ( context , address ) ) body = { "id" : port_id , "network_id" : network_id , "device_id" : device_id , "device_owner" : kwargs . get ( 'device_owner' , '' ) , "tenant_id" : context . tenant_id or "quark" , "roles" : context . roles , "mac_address" : mac_address , "fixed_ips" : fixed_ips , "switch:hardware_id" : instance_node_id , "dynamic_network" : not STRATEGY . is_provider_network ( network_id ) } net_info = self . _get_base_network_info ( context , network_id , base_net_driver ) body . update ( net_info ) try : LOG . info ( "creating downstream port: %s" % ( body ) ) port = self . _create_port ( context , body ) LOG . info ( "created downstream port: %s" % ( port ) ) return { "uuid" : port [ 'port' ] [ 'id' ] , "vlan_id" : port [ 'port' ] [ 'vlan_id' ] } except Exception as e : msg = "failed to create downstream port. Exception: %s" % ( e ) raise IronicException ( msg = msg )
9940	def set_options ( self , ** options ) : self . interactive = options [ 'interactive' ] self . verbosity = options [ 'verbosity' ] self . symlink = options [ 'link' ] self . clear = options [ 'clear' ] self . dry_run = options [ 'dry_run' ] ignore_patterns = options [ 'ignore_patterns' ] if options [ 'use_default_ignore_patterns' ] : ignore_patterns += [ 'CVS' , '.*' , '*~' ] self . ignore_patterns = list ( set ( ignore_patterns ) ) self . post_process = options [ 'post_process' ]
13183	def dict_to_row ( cls , observation_data ) : row = [ ] row . append ( observation_data [ 'name' ] ) row . append ( observation_data [ 'date' ] ) row . append ( observation_data [ 'magnitude' ] ) comment_code = observation_data . get ( 'comment_code' , 'na' ) if not comment_code : comment_code = 'na' row . append ( comment_code ) comp1 = observation_data . get ( 'comp1' , 'na' ) if not comp1 : comp1 = 'na' row . append ( comp1 ) comp2 = observation_data . get ( 'comp2' , 'na' ) if not comp2 : comp2 = 'na' row . append ( comp2 ) chart = observation_data . get ( 'chart' , 'na' ) if not chart : chart = 'na' row . append ( chart ) notes = observation_data . get ( 'notes' , 'na' ) if not notes : notes = 'na' row . append ( notes ) return row
3506	def loopless_fva_iter ( model , reaction , solution = False , zero_cutoff = None ) : zero_cutoff = normalize_cutoff ( model , zero_cutoff ) current = model . objective . value sol = get_solution ( model ) objective_dir = model . objective . direction if reaction . boundary : if solution : return sol else : return current with model : _add_cycle_free ( model , sol . fluxes ) model . slim_optimize ( ) if abs ( reaction . flux - current ) < zero_cutoff : if solution : return sol return current ll_sol = get_solution ( model ) . fluxes reaction . bounds = ( current , current ) model . slim_optimize ( ) almost_ll_sol = get_solution ( model ) . fluxes with model : for rxn in model . reactions : rid = rxn . id if ( ( abs ( ll_sol [ rid ] ) < zero_cutoff ) and ( abs ( almost_ll_sol [ rid ] ) > zero_cutoff ) ) : rxn . bounds = max ( 0 , rxn . lower_bound ) , min ( 0 , rxn . upper_bound ) if solution : best = model . optimize ( ) else : model . slim_optimize ( ) best = reaction . flux model . objective . direction = objective_dir return best
12695	def contains_all ( set1 , set2 , warn ) : for elem in set2 : if elem not in set1 : raise ValueError ( warn ) return True
11131	def stop ( self ) : with self . _status_lock : if self . _running : assert self . _observer is not None self . _observer . stop ( ) self . _running = False self . _origin_mapped_data = dict ( )
13793	def handle_add_fun ( self , function_name ) : function_name = function_name . strip ( ) try : function = get_function ( function_name ) except Exception , exc : self . wfile . write ( js_error ( exc ) + NEWLINE ) return if not getattr ( function , 'view_decorated' , None ) : self . functions [ function_name ] = ( self . function_counter , function ) else : self . functions [ function_name ] = ( self . function_counter , function ( self . log ) ) self . function_counter += 1 return True
9222	def reverse_guard ( lst ) : rev = { '<' : '>=' , '>' : '=<' , '>=' : '<' , '=<' : '>' } return [ rev [ l ] if l in rev else l for l in lst ]
4882	def create_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . get_or_create ( name = 'SAP_USE_ENTERPRISE_ENROLLMENT_PAGE' , defaults = { 'active' : False } )
9721	async def start ( self , rtfromfile = False ) : cmd = "start" + ( " rtfromfile" if rtfromfile else "" ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
7444	def _step2func ( self , samples , force , ipyclient ) : if self . _headers : print ( "\n Step 2: Filtering reads " ) if not self . samples . keys ( ) : raise IPyradWarningExit ( FIRST_RUN_1 ) samples = _get_samples ( self , samples ) if not force : if all ( [ i . stats . state >= 2 for i in samples ] ) : print ( EDITS_EXIST . format ( len ( samples ) ) ) return assemble . rawedit . run2 ( self , samples , force , ipyclient )
1295	def import_demo_experience ( self , states , internals , actions , terminal , reward ) : fetches = self . import_demo_experience_output feed_dict = self . get_feed_dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) self . monitored_session . run ( fetches = fetches , feed_dict = feed_dict )
2258	def argsort ( indexable , key = None , reverse = False ) : if isinstance ( indexable , collections_abc . Mapping ) : vk_iter = ( ( v , k ) for k , v in indexable . items ( ) ) else : vk_iter = ( ( v , k ) for k , v in enumerate ( indexable ) ) if key is None : indices = [ k for v , k in sorted ( vk_iter , reverse = reverse ) ] else : indices = [ k for v , k in sorted ( vk_iter , key = lambda vk : key ( vk [ 0 ] ) , reverse = reverse ) ] return indices
2251	def color_text ( text , color ) : r if color is None : return text try : import pygments import pygments . console if sys . platform . startswith ( 'win32' ) : import colorama colorama . init ( ) ansi_text = pygments . console . colorize ( color , text ) return ansi_text except ImportError : import warnings warnings . warn ( 'pygments is not installed, text will not be colored' ) return text
952	def trainTM ( sequence , timeSteps , noiseLevel ) : currentColumns = np . zeros ( tm . numberOfColumns ( ) , dtype = "uint32" ) predictedColumns = np . zeros ( tm . numberOfColumns ( ) , dtype = "uint32" ) ts = 0 for t in range ( timeSteps ) : tm . reset ( ) for k in range ( 4 ) : v = corruptVector ( sequence [ k ] [ : ] , noiseLevel , sparseCols ) tm . compute ( set ( v [ : ] . nonzero ( ) [ 0 ] . tolist ( ) ) , learn = True ) activeColumnsIndices = [ tm . columnForCell ( i ) for i in tm . getActiveCells ( ) ] predictedColumnIndices = [ tm . columnForCell ( i ) for i in tm . getPredictiveCells ( ) ] currentColumns = [ 1 if i in activeColumnsIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ] acc = accuracy ( currentColumns , predictedColumns ) x . append ( ts ) y . append ( acc ) ts += 1 predictedColumns = [ 1 if i in predictedColumnIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ]
13882	def GetFileContents ( filename , binary = False , encoding = None , newline = None ) : source_file = OpenFile ( filename , binary = binary , encoding = encoding , newline = newline ) try : contents = source_file . read ( ) finally : source_file . close ( ) return contents
12262	def add ( self , operator , * args ) : if isinstance ( operator , str ) : op = getattr ( proxops , operator ) ( * args ) elif isinstance ( operator , proxops . ProximalOperatorBaseClass ) : op = operator else : raise ValueError ( "operator must be a string or a subclass of ProximalOperator" ) self . operators . append ( op ) return self
8797	def apply_rules ( self , device_id , mac_address , rules ) : LOG . info ( "Applying security group rules for device %s with MAC %s" % ( device_id , mac_address ) ) rule_dict = { SECURITY_GROUP_RULE_KEY : rules } redis_key = self . vif_key ( device_id , mac_address ) self . set_field ( redis_key , SECURITY_GROUP_HASH_ATTR , rule_dict ) self . set_field_raw ( redis_key , SECURITY_GROUP_ACK , False )
12037	def matrixValues ( matrix , key ) : assert key in matrix . dtype . names col = matrix . dtype . names . index ( key ) values = np . empty ( len ( matrix ) ) * np . nan for i in range ( len ( matrix ) ) : values [ i ] = matrix [ i ] [ col ] return values
4357	def remove_namespace ( self , namespace ) : if namespace in self . active_ns : del self . active_ns [ namespace ] if len ( self . active_ns ) == 0 and self . connected : self . kill ( detach = True )
10341	def main ( graph : BELGraph , xlsx : str , tsvs : str ) : if not xlsx and not tsvs : click . secho ( 'Specify at least one option --xlsx or --tsvs' , fg = 'red' ) sys . exit ( 1 ) spia_matrices = bel_to_spia_matrices ( graph ) if xlsx : spia_matrices_to_excel ( spia_matrices , xlsx ) if tsvs : spia_matrices_to_tsvs ( spia_matrices , tsvs )
878	def newPosition ( self , whichVars = None ) : globalBestPosition = None if self . _hsObj . _speculativeParticles : genIdx = self . genIdx else : genIdx = self . genIdx - 1 if genIdx >= 0 : ( bestModelId , _ ) = self . _resultsDB . bestModelIdAndErrScore ( self . swarmId , genIdx ) if bestModelId is not None : ( particleState , _ , _ , _ , _ ) = self . _resultsDB . getParticleInfo ( bestModelId ) globalBestPosition = Particle . getPositionFromState ( particleState ) for ( varName , var ) in self . permuteVars . iteritems ( ) : if whichVars is not None and varName not in whichVars : continue if globalBestPosition is None : var . newPosition ( None , self . _rng ) else : var . newPosition ( globalBestPosition [ varName ] , self . _rng ) position = self . getPosition ( ) if self . logger . getEffectiveLevel ( ) <= logging . DEBUG : msg = StringIO . StringIO ( ) print >> msg , "New particle position: \n%s" % ( pprint . pformat ( position , indent = 4 ) ) print >> msg , "Particle variables:" for ( varName , var ) in self . permuteVars . iteritems ( ) : print >> msg , " %s: %s" % ( varName , str ( var ) ) self . logger . debug ( msg . getvalue ( ) ) msg . close ( ) return position
4333	def noisered ( self , profile_path , amount = 0.5 ) : if not os . path . exists ( profile_path ) : raise IOError ( "profile_path {} does not exist." . format ( profile_path ) ) if not is_number ( amount ) or amount < 0 or amount > 1 : raise ValueError ( "amount must be a number between 0 and 1." ) effect_args = [ 'noisered' , profile_path , '{:f}' . format ( amount ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'noisered' ) return self
2227	def _digest_hasher ( hasher , hashlen , base ) : hex_text = hasher . hexdigest ( ) base_text = _convert_hexstr_base ( hex_text , base ) text = base_text [ : hashlen ] return text
13801	def revoke_token ( self , token , callback ) : yield Task ( self . data_store . remove , 'tokens' , token = token ) callback ( )
3214	def get_stats ( self ) : expired = sum ( [ x [ 'expired' ] for _ , x in self . _CACHE_STATS [ 'access_stats' ] . items ( ) ] ) miss = sum ( [ x [ 'miss' ] for _ , x in self . _CACHE_STATS [ 'access_stats' ] . items ( ) ] ) hit = sum ( [ x [ 'hit' ] for _ , x in self . _CACHE_STATS [ 'access_stats' ] . items ( ) ] ) return { 'totals' : { 'keys' : len ( self . _CACHE_STATS [ 'access_stats' ] ) , 'expired' : expired , 'miss' : miss , 'hit' : hit , } }
12513	def _crop_img_to ( image , slices , copy = True ) : img = check_img ( image ) data = img . get_data ( ) affine = img . get_affine ( ) cropped_data = data [ slices ] if copy : cropped_data = cropped_data . copy ( ) linear_part = affine [ : 3 , : 3 ] old_origin = affine [ : 3 , 3 ] new_origin_voxel = np . array ( [ s . start for s in slices ] ) new_origin = old_origin + linear_part . dot ( new_origin_voxel ) new_affine = np . eye ( 4 ) new_affine [ : 3 , : 3 ] = linear_part new_affine [ : 3 , 3 ] = new_origin new_img = nib . Nifti1Image ( cropped_data , new_affine ) return new_img
9589	def init ( self ) : resp = self . _execute ( Command . NEW_SESSION , { 'desiredCapabilities' : self . desired_capabilities } , False ) resp . raise_for_status ( ) self . session_id = str ( resp . session_id ) self . capabilities = resp . value
5659	def _validate_danglers ( self ) : for query , warning in zip ( DANGLER_QUERIES , DANGLER_WARNINGS ) : dangler_count = self . gtfs . execute_custom_query ( query ) . fetchone ( ) [ 0 ] if dangler_count > 0 : if self . verbose : print ( str ( dangler_count ) + " " + warning ) self . warnings_container . add_warning ( warning , self . location , count = dangler_count )
871	def edit ( cls , properties ) : copyOfProperties = copy ( properties ) configFilePath = cls . getPath ( ) try : with open ( configFilePath , 'r' ) as fp : contents = fp . read ( ) except IOError , e : if e . errno != errno . ENOENT : _getLogger ( ) . exception ( "Error %s reading custom configuration store " "from %s, while editing properties %s." , e . errno , configFilePath , properties ) raise contents = '<configuration/>' try : elements = ElementTree . XML ( contents ) ElementTree . tostring ( elements ) except Exception , e : msg = "File contents of custom configuration is corrupt. File " "location: %s; Contents: '%s'. Original Error (%s): %s." % ( configFilePath , contents , type ( e ) , e ) _getLogger ( ) . exception ( msg ) raise RuntimeError ( msg ) , None , sys . exc_info ( ) [ 2 ] if elements . tag != 'configuration' : e = "Expected top-level element to be 'configuration' but got '%s'" % ( elements . tag ) _getLogger ( ) . error ( e ) raise RuntimeError ( e ) for propertyItem in elements . findall ( './property' ) : propInfo = dict ( ( attr . tag , attr . text ) for attr in propertyItem ) name = propInfo [ 'name' ] if name in copyOfProperties : foundValues = propertyItem . findall ( './value' ) if len ( foundValues ) > 0 : foundValues [ 0 ] . text = str ( copyOfProperties . pop ( name ) ) if not copyOfProperties : break else : e = "Property %s missing value tag." % ( name , ) _getLogger ( ) . error ( e ) raise RuntimeError ( e ) for propertyName , value in copyOfProperties . iteritems ( ) : newProp = ElementTree . Element ( 'property' ) nameTag = ElementTree . Element ( 'name' ) nameTag . text = propertyName newProp . append ( nameTag ) valueTag = ElementTree . Element ( 'value' ) valueTag . text = str ( value ) newProp . append ( valueTag ) elements . append ( newProp ) try : makeDirectoryFromAbsolutePath ( os . path . dirname ( configFilePath ) ) with open ( configFilePath , 'w' ) as fp : fp . write ( ElementTree . tostring ( elements ) ) except Exception , e : _getLogger ( ) . exception ( "Error while saving custom configuration " "properties %s in %s." , properties , configFilePath ) raise
13241	def daily_periods ( self , range_start = datetime . date . min , range_end = datetime . date . max , exclude_dates = tuple ( ) ) : tz = self . timezone period = self . period weekdays = self . weekdays current_date = max ( range_start , self . start_date ) end_date = range_end if self . end_date : end_date = min ( end_date , self . end_date ) while current_date <= end_date : if current_date . weekday ( ) in weekdays and current_date not in exclude_dates : yield Period ( tz . localize ( datetime . datetime . combine ( current_date , period . start ) ) , tz . localize ( datetime . datetime . combine ( current_date , period . end ) ) ) current_date += datetime . timedelta ( days = 1 )
6362	def encode ( self , word , max_length = - 1 ) : word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = '' . join ( c for c in word if c in self . _uc_set ) word = word . replace ( 'LL' , 'L' ) word = word . replace ( 'R' , 'R' ) sdx = word . translate ( self . _trans ) if max_length > 0 : sdx = ( sdx + ( '0' * max_length ) ) [ : max_length ] return sdx
4415	def add_at ( self , index : int , requester : int , track : dict ) : self . queue . insert ( min ( index , len ( self . queue ) - 1 ) , AudioTrack ( ) . build ( track , requester ) )
10763	def get_unique_token ( self ) : if self . _unique_token is None : self . _unique_token = self . _random_token ( ) return self . _unique_token
5	def clear_mpi_env_vars ( ) : removed_environment = { } for k , v in list ( os . environ . items ( ) ) : for prefix in [ 'OMPI_' , 'PMI_' ] : if k . startswith ( prefix ) : removed_environment [ k ] = v del os . environ [ k ] try : yield finally : os . environ . update ( removed_environment )
2287	def parallel_graph_evaluation ( data , adj_matrix , nb_runs = 16 , nb_jobs = None , ** kwargs ) : nb_jobs = SETTINGS . get_default ( nb_jobs = nb_jobs ) if nb_runs == 1 : return graph_evaluation ( data , adj_matrix , ** kwargs ) else : output = Parallel ( n_jobs = nb_jobs ) ( delayed ( graph_evaluation ) ( data , adj_matrix , idx = run , gpu_id = run % SETTINGS . GPU , ** kwargs ) for run in range ( nb_runs ) ) return np . mean ( output )
13174	def next ( self , name = None ) : if self . parent is None or self . index is None : return None for idx in xrange ( self . index + 1 , len ( self . parent ) ) : if name is None or self . parent [ idx ] . tagname == name : return self . parent [ idx ]
3295	def set_share_path ( self , share_path ) : assert share_path == "" or share_path . startswith ( "/" ) if share_path == "/" : share_path = "" assert share_path in ( "" , "/" ) or not share_path . endswith ( "/" ) self . share_path = share_path
1516	def make_tarfile ( output_filename , source_dir ) : with tarfile . open ( output_filename , "w:gz" ) as tar : tar . add ( source_dir , arcname = os . path . basename ( source_dir ) )
8511	def load ( self ) : from pylearn2 . config import yaml_parse from pylearn2 . datasets import Dataset dataset = yaml_parse . load ( self . yaml_string ) assert isinstance ( dataset , Dataset ) data = dataset . iterator ( mode = 'sequential' , num_batches = 1 , data_specs = dataset . data_specs , return_tuple = True ) . next ( ) if len ( data ) == 2 : X , y = data y = np . squeeze ( y ) if self . one_hot : y = np . argmax ( y , axis = 1 ) else : X = data y = None return X , y
8435	def map ( cls , x , palette , limits , na_value = None , oob = censor ) : x = oob ( rescale ( x , _from = limits ) ) pal = palette ( x ) try : pal [ pd . isnull ( x ) ] = na_value except TypeError : pal = [ v if not pd . isnull ( v ) else na_value for v in pal ] return pal
12095	def indexImages ( folder , fname = "index.html" ) : html = "<html><body>" for item in glob . glob ( folder + "/*.*" ) : if item . split ( "." ) [ - 1 ] in [ 'jpg' , 'png' ] : html += "<h3>%s</h3>" % os . path . basename ( item ) html += '<img src="%s">' % os . path . basename ( item ) html += '<br>' * 10 html += "</html></body>" f = open ( folder + "/" + fname , 'w' ) f . write ( html ) f . close print ( "indexed:" ) print ( " " , os . path . abspath ( folder + "/" + fname ) ) return
10433	def selectlastrow ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) cell = object_handle . AXRows [ - 1 ] if not cell . AXSelected : object_handle . activate ( ) cell . AXSelected = True else : pass return 1
6898	def serial_periodicfeatures ( pfpkl_list , lcbasedir , outdir , starfeaturesdir = None , fourierorder = 5 , transitparams = ( - 0.01 , 0.1 , 0.1 ) , ebparams = ( - 0.2 , 0.3 , 0.7 , 0.5 ) , pdiff_threshold = 1.0e-4 , sidereal_threshold = 1.0e-4 , sampling_peak_multiplier = 5.0 , sampling_startp = None , sampling_endp = None , starfeatures = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , sigclip = 10.0 , verbose = False , maxobjects = None ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( fileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if maxobjects : pfpkl_list = pfpkl_list [ : maxobjects ] LOGINFO ( '%s periodfinding pickles to process' % len ( pfpkl_list ) ) if starfeaturesdir and os . path . exists ( starfeaturesdir ) : starfeatures_list = [ ] LOGINFO ( 'collecting starfeatures pickles...' ) for pfpkl in pfpkl_list : sfpkl1 = os . path . basename ( pfpkl ) . replace ( 'periodfinding' , 'starfeatures' ) sfpkl2 = sfpkl1 . replace ( '.gz' , '' ) sfpath1 = os . path . join ( starfeaturesdir , sfpkl1 ) sfpath2 = os . path . join ( starfeaturesdir , sfpkl2 ) if os . path . exists ( sfpath1 ) : starfeatures_list . append ( sfpkl1 ) elif os . path . exists ( sfpath2 ) : starfeatures_list . append ( sfpkl2 ) else : starfeatures_list . append ( None ) else : starfeatures_list = [ None for x in pfpkl_list ] kwargs = { 'fourierorder' : fourierorder , 'transitparams' : transitparams , 'ebparams' : ebparams , 'pdiff_threshold' : pdiff_threshold , 'sidereal_threshold' : sidereal_threshold , 'sampling_peak_multiplier' : sampling_peak_multiplier , 'sampling_startp' : sampling_startp , 'sampling_endp' : sampling_endp , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'lcformat' : lcformat , 'lcformatdir' : lcformatdir , 'sigclip' : sigclip , 'verbose' : verbose } tasks = [ ( x , lcbasedir , outdir , y , kwargs ) for ( x , y ) in zip ( pfpkl_list , starfeatures_list ) ] LOGINFO ( 'processing periodfinding pickles...' ) for task in tqdm ( tasks ) : _periodicfeatures_worker ( task )
581	def _setRandomEncoderResolution ( minResolution = 0.001 ) : encoder = ( model_params . MODEL_PARAMS [ "modelParams" ] [ "sensorParams" ] [ "encoders" ] [ "value" ] ) if encoder [ "type" ] == "RandomDistributedScalarEncoder" : rangePadding = abs ( _INPUT_MAX - _INPUT_MIN ) * 0.2 minValue = _INPUT_MIN - rangePadding maxValue = _INPUT_MAX + rangePadding resolution = max ( minResolution , ( maxValue - minValue ) / encoder . pop ( "numBuckets" ) ) encoder [ "resolution" ] = resolution
12303	def url_is_valid ( self , url ) : if url . startswith ( "file://" ) : url = url . replace ( "file://" , "" ) return os . path . exists ( url )
8109	def search_images ( q , start = 0 , size = "" , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_IMAGES return GoogleSearch ( q , start , service , size , wait , asynchronous , cached )
9718	async def stream_frames_stop ( self ) : self . _protocol . set_on_packet ( None ) cmd = "streamframes stop" await self . _protocol . send_command ( cmd , callback = False )
3528	def get_required_setting ( setting , value_re , invalid_msg ) : try : value = getattr ( settings , setting ) except AttributeError : raise AnalyticalException ( "%s setting: not found" % setting ) if not value : raise AnalyticalException ( "%s setting is not set" % setting ) value = str ( value ) if not value_re . search ( value ) : raise AnalyticalException ( "%s setting: %s: '%s'" % ( setting , invalid_msg , value ) ) return value
7674	def slice ( self , start_time , end_time , strict = False ) : if self . file_metadata . duration is None : raise JamsError ( 'Duration must be set (jam.file_metadata.duration) before ' 'slicing can be performed.' ) if ( start_time < 0 or start_time > float ( self . file_metadata . duration ) or end_time < start_time or end_time > float ( self . file_metadata . duration ) ) : raise ParameterError ( 'start_time and end_time must be within the original file ' 'duration ({:f}) and end_time cannot be smaller than ' 'start_time.' . format ( float ( self . file_metadata . duration ) ) ) jam_sliced = JAMS ( annotations = None , file_metadata = self . file_metadata , sandbox = self . sandbox ) jam_sliced . annotations = self . annotations . slice ( start_time , end_time , strict = strict ) jam_sliced . file_metadata . duration = end_time - start_time if 'slice' not in jam_sliced . sandbox . keys ( ) : jam_sliced . sandbox . update ( slice = [ { 'start_time' : start_time , 'end_time' : end_time } ] ) else : jam_sliced . sandbox . slice . append ( { 'start_time' : start_time , 'end_time' : end_time } ) return jam_sliced
10892	def translate ( self , dr ) : tile = self . copy ( ) tile . l += dr tile . r += dr return tile
11660	def fit ( self , X , y = None ) : self . kmeans_fit_ = copy ( self . kmeans ) X = as_features ( X , stack = True ) self . kmeans_fit_ . fit ( X . stacked_features ) return self
11341	def set_target_temperature ( self , temperature , mode = config . SCHEDULE_HOLD ) : if temperature < self . min_temperature : temperature = self . min_temperature if temperature > self . max_temperature : temperature = self . max_temperature modes = [ config . SCHEDULE_TEMPORARY_HOLD , config . SCHEDULE_HOLD ] if mode not in modes : raise Exception ( "Invalid mode. Please use one of: {}" . format ( modes ) ) self . set_data ( { "SetPointTemp" : temperature , "ScheduleMode" : mode } )
10880	def listify ( a ) : if a is None : return [ ] elif not isinstance ( a , ( tuple , list , np . ndarray ) ) : return [ a ] return list ( a )
10131	def _unescape ( s , uri = False ) : out = '' while len ( s ) > 0 : c = s [ 0 ] if c == '\\' : esc_c = s [ 1 ] if esc_c in ( 'u' , 'U' ) : out += six . unichr ( int ( s [ 2 : 6 ] , base = 16 ) ) s = s [ 6 : ] continue else : if esc_c == 'b' : out += '\b' elif esc_c == 'f' : out += '\f' elif esc_c == 'n' : out += '\n' elif esc_c == 'r' : out += '\r' elif esc_c == 't' : out += '\t' else : if uri and ( esc_c == '#' ) : out += '\\' out += esc_c s = s [ 2 : ] continue else : out += c s = s [ 1 : ] return out
2496	def create_package_node ( self , package ) : package_node = BNode ( ) type_triple = ( package_node , RDF . type , self . spdx_namespace . Package ) self . graph . add ( type_triple ) self . handle_pkg_optional_fields ( package , package_node ) name_triple = ( package_node , self . spdx_namespace . name , Literal ( package . name ) ) self . graph . add ( name_triple ) down_loc_node = ( package_node , self . spdx_namespace . downloadLocation , self . to_special_value ( package . download_location ) ) self . graph . add ( down_loc_node ) verif_node = self . package_verif_node ( package ) verif_triple = ( package_node , self . spdx_namespace . packageVerificationCode , verif_node ) self . graph . add ( verif_triple ) conc_lic_node = self . license_or_special ( package . conc_lics ) conc_lic_triple = ( package_node , self . spdx_namespace . licenseConcluded , conc_lic_node ) self . graph . add ( conc_lic_triple ) decl_lic_node = self . license_or_special ( package . license_declared ) decl_lic_triple = ( package_node , self . spdx_namespace . licenseDeclared , decl_lic_node ) self . graph . add ( decl_lic_triple ) licenses_from_files_nodes = map ( lambda el : self . license_or_special ( el ) , package . licenses_from_files ) lic_from_files_predicate = self . spdx_namespace . licenseInfoFromFiles lic_from_files_triples = [ ( package_node , lic_from_files_predicate , node ) for node in licenses_from_files_nodes ] for triple in lic_from_files_triples : self . graph . add ( triple ) cr_text_node = self . to_special_value ( package . cr_text ) cr_text_triple = ( package_node , self . spdx_namespace . copyrightText , cr_text_node ) self . graph . add ( cr_text_triple ) self . handle_package_has_file ( package , package_node ) return package_node
9521	def mean_length ( infile , limit = None ) : total = 0 count = 0 seq_reader = sequences . file_reader ( infile ) for seq in seq_reader : total += len ( seq ) count += 1 if limit is not None and count >= limit : break assert count > 0 return total / count
5509	def get_permissions ( self , path ) : path = pathlib . PurePosixPath ( path ) parents = filter ( lambda p : p . is_parent ( path ) , self . permissions ) perm = min ( parents , key = lambda p : len ( path . relative_to ( p . path ) . parts ) , default = Permission ( ) , ) return perm
7704	def add_item ( self , item , replace = False ) : if item . jid in self . _jids : if replace : self . remove_item ( item . jid ) else : raise ValueError ( "JID already in the roster" ) index = len ( self . _items ) self . _items . append ( item ) self . _jids [ item . jid ] = index
7146	def to_atomic ( amount ) : if not isinstance ( amount , ( Decimal , float ) + _integer_types ) : raise ValueError ( "Amount '{}' doesn't have numeric type. Only Decimal, int, long and " "float (not recommended) are accepted as amounts." ) return int ( amount * 10 ** 12 )
6516	def execute_tools ( config , path , progress = None ) : progress = progress or QuietProgress ( ) progress . on_start ( ) manager = SyncManager ( ) manager . start ( ) num_tools = 0 tools = manager . Queue ( ) for name , cls in iteritems ( get_tools ( ) ) : if config [ name ] [ 'use' ] and cls . can_be_used ( ) : num_tools += 1 tools . put ( { 'name' : name , 'config' : config [ name ] , } ) collector = Collector ( config ) if not num_tools : progress . on_finish ( ) return collector notifications = manager . Queue ( ) environment = manager . dict ( { 'finder' : Finder ( path , config ) , } ) workers = [ ] for _ in range ( config [ 'workers' ] ) : worker = Worker ( args = ( tools , notifications , environment , ) , ) worker . start ( ) workers . append ( worker ) while num_tools : try : notification = notifications . get ( True , 0.25 ) except Empty : pass else : if notification [ 'type' ] == 'start' : progress . on_tool_start ( notification [ 'tool' ] ) elif notification [ 'type' ] == 'complete' : collector . add_issues ( notification [ 'issues' ] ) progress . on_tool_finish ( notification [ 'tool' ] ) num_tools -= 1 progress . on_finish ( ) return collector
9847	def _load_cpp4 ( self , filename ) : ccp4 = CCP4 . CCP4 ( ) ccp4 . read ( filename ) grid , edges = ccp4 . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
5555	def _element_at_zoom ( name , element , zoom ) : if isinstance ( element , dict ) : if "format" in element : return element out_elements = { } for sub_name , sub_element in element . items ( ) : out_element = _element_at_zoom ( sub_name , sub_element , zoom ) if name == "input" : out_elements [ sub_name ] = out_element elif out_element is not None : out_elements [ sub_name ] = out_element if len ( out_elements ) == 1 and name != "input" : return next ( iter ( out_elements . values ( ) ) ) if len ( out_elements ) == 0 : return None return out_elements elif isinstance ( name , str ) : if name . startswith ( "zoom" ) : return _filter_by_zoom ( conf_string = name . strip ( "zoom" ) . strip ( ) , zoom = zoom , element = element ) else : return element else : return element
1450	def get_all_file_state_managers ( conf ) : state_managers = [ ] state_locations = conf . get_state_locations_of_type ( "file" ) for location in state_locations : name = location [ 'name' ] rootpath = os . path . expanduser ( location [ 'rootpath' ] ) LOG . info ( "Connecting to file state with rootpath: " + rootpath ) state_manager = FileStateManager ( name , rootpath ) state_managers . append ( state_manager ) return state_managers
11367	def locate ( pattern , root = os . curdir ) : for path , dummy , files in os . walk ( os . path . abspath ( root ) ) : for filename in fnmatch . filter ( files , pattern ) : yield os . path . join ( path , filename )
667	def logProbability ( self , distn ) : x = numpy . asarray ( distn ) n = x . sum ( ) return ( logFactorial ( n ) - numpy . sum ( [ logFactorial ( k ) for k in x ] ) + numpy . sum ( x * numpy . log ( self . dist . pmf ) ) )
7783	def _deactivate ( self ) : self . cache . remove_fetcher ( self ) if self . active : self . _deactivated ( )
9004	def to_svg ( self , zoom ) : def on_dump ( ) : knitting_pattern = self . patterns . at ( 0 ) layout = GridLayout ( knitting_pattern ) instruction_to_svg = default_instruction_svg_cache ( ) builder = SVGBuilder ( ) kp_to_svg = KnittingPatternToSVG ( knitting_pattern , layout , instruction_to_svg , builder , zoom ) return kp_to_svg . build_SVG_dict ( ) return XMLDumper ( on_dump )
11056	def ensure_backrefs ( obj , fields = None ) : for ref in _collect_refs ( obj , fields ) : updated = ref [ 'value' ] . _update_backref ( ref [ 'field_instance' ] . _backref_field_name , obj , ref [ 'field_name' ] , ) if updated : logging . debug ( 'Updated reference {}:{}:{}:{}:{}' . format ( obj . _name , obj . _primary_key , ref [ 'field_name' ] , ref [ 'value' ] . _name , ref [ 'value' ] . _primary_key , ) )
1592	def add ( self , stream_id , task_ids , grouping , source_comp_name ) : if stream_id not in self . targets : self . targets [ stream_id ] = [ ] self . targets [ stream_id ] . append ( Target ( task_ids , grouping , source_comp_name ) )
6604	def result_relpath ( self , package_index ) : dirname = 'task_{:05d}' . format ( package_index ) ret = os . path . join ( 'results' , dirname , 'result.p.gz' ) return ret
1059	def update_wrapper ( wrapper , wrapped , assigned = WRAPPER_ASSIGNMENTS , updated = WRAPPER_UPDATES ) : for attr in assigned : setattr ( wrapper , attr , getattr ( wrapped , attr ) ) for attr in updated : getattr ( wrapper , attr ) . update ( getattr ( wrapped , attr , { } ) ) return wrapper
9633	def render_to_message ( self , extra_context = None , * args , ** kwargs ) : message = super ( TemplatedHTMLEmailMessageView , self ) . render_to_message ( extra_context , * args , ** kwargs ) if extra_context is None : extra_context = { } context = self . get_context_data ( ** extra_context ) content = self . render_html_body ( context ) message . attach_alternative ( content , mimetype = 'text/html' ) return message
8857	def on_run ( self ) : filename = self . tabWidget . current_widget ( ) . file . path wd = os . path . dirname ( filename ) args = Settings ( ) . get_run_config_for_file ( filename ) self . interactiveConsole . start_process ( Settings ( ) . interpreter , args = [ filename ] + args , cwd = wd ) self . dockWidget . show ( ) self . actionRun . setEnabled ( False ) self . actionConfigure_run . setEnabled ( False )
13372	def expandpath ( path ) : return os . path . abspath ( os . path . expandvars ( os . path . expanduser ( path ) ) )
13291	def get_variables_by_attributes ( self , ** kwargs ) : vs = [ ] has_value_flag = False for vname in self . variables : var = self . variables [ vname ] for k , v in kwargs . items ( ) : if callable ( v ) : has_value_flag = v ( getattr ( var , k , None ) ) if has_value_flag is False : break elif hasattr ( var , k ) and getattr ( var , k ) == v : has_value_flag = True else : has_value_flag = False break if has_value_flag is True : vs . append ( self . variables [ vname ] ) return vs
5492	def write_config ( self ) : with open ( self . config_file , "w" ) as config_file : self . cfg . write ( config_file )
1385	def set_physical_plan ( self , physical_plan ) : if not physical_plan : self . physical_plan = None self . id = None else : self . physical_plan = physical_plan self . id = physical_plan . topology . id self . trigger_watches ( )
11906	def to_permutation_matrix ( matches ) : n = len ( matches ) P = np . zeros ( ( n , n ) ) P [ list ( zip ( * ( matches . items ( ) ) ) ) ] = 1 return P
11899	def _get_src_from_image ( img , fallback_image_file ) : if img is None : return fallback_image_file target_format = img . format if target_format . lower ( ) in [ 'tif' , 'tiff' ] : target_format = 'JPEG' try : bytesio = io . BytesIO ( ) img . save ( bytesio , target_format ) byte_value = bytesio . getvalue ( ) b64 = base64 . b64encode ( byte_value ) return 'data:image/%s;base64,%s' % ( target_format . lower ( ) , b64 ) except IOError as exptn : print ( 'IOError while saving image bytes: %s' % exptn ) return fallback_image_file
7023	def _base64_to_file ( b64str , outfpath , writetostrio = False ) : try : filebytes = base64 . b64decode ( b64str ) if writetostrio : outobj = StrIO ( filebytes ) return outobj else : with open ( outfpath , 'wb' ) as outfd : outfd . write ( filebytes ) if os . path . exists ( outfpath ) : return outfpath else : LOGERROR ( 'could not write output file: %s' % outfpath ) return None except Exception as e : LOGEXCEPTION ( 'failed while trying to convert ' 'b64 string to file %s' % outfpath ) return None
7506	def _dump_qmc ( self ) : io5 = h5py . File ( self . database . output , 'r' ) self . files . qdump = os . path . join ( self . dirs , self . name + ".quartets.txt" ) LOGGER . info ( "qdump file %s" , self . files . qdump ) outfile = open ( self . files . qdump , 'w' ) for idx in xrange ( 0 , self . params . nquartets , self . _chunksize ) : masked_quartets = io5 [ "quartets" ] [ idx : idx + self . _chunksize , : ] quarts = [ list ( j ) for j in masked_quartets ] chunk = [ "{},{}|{},{}" . format ( * i ) for i in quarts ] outfile . write ( "\n" . join ( chunk ) + "\n" ) outfile . close ( ) io5 . close ( )
831	def decode ( self , encoded , parentFieldName = '' ) : fieldsDict = dict ( ) fieldsOrder = [ ] if parentFieldName == '' : parentName = self . name else : parentName = "%s.%s" % ( parentFieldName , self . name ) if self . encoders is not None : for i in xrange ( len ( self . encoders ) ) : ( name , encoder , offset ) = self . encoders [ i ] if i < len ( self . encoders ) - 1 : nextOffset = self . encoders [ i + 1 ] [ 2 ] else : nextOffset = self . width fieldOutput = encoded [ offset : nextOffset ] ( subFieldsDict , subFieldsOrder ) = encoder . decode ( fieldOutput , parentFieldName = parentName ) fieldsDict . update ( subFieldsDict ) fieldsOrder . extend ( subFieldsOrder ) return ( fieldsDict , fieldsOrder )
5384	def _operation_status_message ( self ) : metadata = self . _op [ 'metadata' ] if not self . _op [ 'done' ] : if 'events' in metadata and metadata [ 'events' ] : last_event = metadata [ 'events' ] [ - 1 ] msg = last_event [ 'description' ] ds = last_event [ 'startTime' ] else : msg = 'Pending' ds = metadata [ 'createTime' ] else : ds = metadata [ 'endTime' ] if 'error' in self . _op : msg = self . _op [ 'error' ] [ 'message' ] else : msg = 'Success' return ( msg , google_base . parse_rfc3339_utc_string ( ds ) )
9251	def generate_log_for_all_tags ( self ) : if self . options . verbose : print ( "Generating log..." ) self . issues2 = copy . deepcopy ( self . issues ) log1 = "" if self . options . with_unreleased : log1 = self . generate_unreleased_section ( ) log = "" for index in range ( len ( self . filtered_tags ) - 1 ) : log += self . do_generate_log_for_all_tags_part1 ( log , index ) if self . options . tag_separator and log1 : log = log1 + self . options . tag_separator + log else : log = log1 + log if len ( self . filtered_tags ) != 0 : log += self . do_generate_log_for_all_tags_part2 ( log ) return log
181	def to_polygon ( self ) : from . polys import Polygon return Polygon ( self . coords , label = self . label )
4946	def send_course_enrollment_statement ( lrs_configuration , course_enrollment ) : user_details = LearnerInfoSerializer ( course_enrollment . user ) course_details = CourseInfoSerializer ( course_enrollment . course ) statement = LearnerCourseEnrollmentStatement ( course_enrollment . user , course_enrollment . course , user_details . data , course_details . data , ) EnterpriseXAPIClient ( lrs_configuration ) . save_statement ( statement )
5021	def get_enterprise_customer_from_catalog_id ( catalog_id ) : try : return str ( EnterpriseCustomerCatalog . objects . get ( pk = catalog_id ) . enterprise_customer . uuid ) except EnterpriseCustomerCatalog . DoesNotExist : return None
1675	def _ExpandDirectories ( filenames ) : expanded = set ( ) for filename in filenames : if not os . path . isdir ( filename ) : expanded . add ( filename ) continue for root , _ , files in os . walk ( filename ) : for loopfile in files : fullname = os . path . join ( root , loopfile ) if fullname . startswith ( '.' + os . path . sep ) : fullname = fullname [ len ( '.' + os . path . sep ) : ] expanded . add ( fullname ) filtered = [ ] for filename in expanded : if os . path . splitext ( filename ) [ 1 ] [ 1 : ] in GetAllExtensions ( ) : filtered . append ( filename ) return filtered
7700	def verify_roster_set ( self , fix = False , settings = None ) : try : self . _verify ( ( None , u"remove" ) , fix ) except ValueError , err : raise BadRequestProtocolError ( unicode ( err ) ) if self . ask : if fix : self . ask = None else : raise BadRequestProtocolError ( "'ask' in roster set" ) if self . approved : if fix : self . approved = False else : raise BadRequestProtocolError ( "'approved' in roster set" ) if settings is None : settings = XMPPSettings ( ) name_length_limit = settings [ "roster_name_length_limit" ] if self . name and len ( self . name ) > name_length_limit : raise NotAcceptableProtocolError ( u"Roster item name too long" ) group_length_limit = settings [ "roster_group_name_length_limit" ] for group in self . groups : if not group : raise NotAcceptableProtocolError ( u"Roster group name empty" ) if len ( group ) > group_length_limit : raise NotAcceptableProtocolError ( u"Roster group name too long" ) if self . _duplicate_group : raise BadRequestProtocolError ( u"Item group duplicated" )
4657	def clear ( self ) : self . ops = [ ] self . wifs = set ( ) self . signing_accounts = [ ] self [ "expiration" ] = None dict . __init__ ( self , { } )
420	def save_validation_log ( self , ** kwargs ) : self . _fill_project_info ( kwargs ) kwargs . update ( { 'time' : datetime . utcnow ( ) } ) _result = self . db . ValidLog . insert_one ( kwargs ) _log = self . _print_dict ( kwargs ) logging . info ( "[Database] valid log: " + _log )
2090	def _disassoc ( self , url_fragment , me , other ) : url = self . endpoint + '%d/%s/' % ( me , url_fragment ) r = client . get ( url , params = { 'id' : other } ) . json ( ) if r [ 'count' ] == 0 : return { 'changed' : False } r = client . post ( url , data = { 'disassociate' : True , 'id' : other } ) return { 'changed' : True }
12111	def save ( self , filename , metadata = { } , ** data ) : intersection = set ( metadata . keys ( ) ) & set ( data . keys ( ) ) if intersection : msg = 'Key(s) overlap between data and metadata: %s' raise Exception ( msg % ',' . join ( intersection ) )
12112	def _savepath ( self , filename ) : ( basename , ext ) = os . path . splitext ( filename ) basename = basename if ( ext in self . extensions ) else filename ext = ext if ( ext in self . extensions ) else self . extensions [ 0 ] savepath = os . path . abspath ( os . path . join ( self . directory , '%s%s' % ( basename , ext ) ) ) return ( tempfile . mkstemp ( ext , basename + "_" , self . directory ) [ 1 ] if self . hash_suffix else savepath )
3753	def Ceiling ( CASRN , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in _OntarioExposureLimits and ( _OntarioExposureLimits [ CASRN ] [ "Ceiling (ppm)" ] or _OntarioExposureLimits [ CASRN ] [ "Ceiling (mg/m^3)" ] ) : methods . append ( ONTARIO ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ONTARIO : if _OntarioExposureLimits [ CASRN ] [ "Ceiling (ppm)" ] : _Ceiling = ( _OntarioExposureLimits [ CASRN ] [ "Ceiling (ppm)" ] , 'ppm' ) elif _OntarioExposureLimits [ CASRN ] [ "Ceiling (mg/m^3)" ] : _Ceiling = ( _OntarioExposureLimits [ CASRN ] [ "Ceiling (mg/m^3)" ] , 'mg/m^3' ) elif Method == NONE : _Ceiling = None else : raise Exception ( 'Failure in in function' ) return _Ceiling
2100	def read ( self , * args , ** kwargs ) : if 'actor' in kwargs : kwargs [ 'actor' ] = kwargs . pop ( 'actor' ) r = super ( Resource , self ) . read ( * args , ** kwargs ) if 'results' in r : for d in r [ 'results' ] : self . _promote_actor ( d ) else : self . _promote_actor ( d ) return r
7559	def set_mkl_thread_limit ( cores ) : if "linux" in sys . platform : mkl_rt = ctypes . CDLL ( 'libmkl_rt.so' ) else : mkl_rt = ctypes . CDLL ( 'libmkl_rt.dylib' ) oldlimit = mkl_rt . mkl_get_max_threads ( ) mkl_rt . mkl_set_num_threads ( ctypes . byref ( ctypes . c_int ( cores ) ) ) return oldlimit
8033	def find_dupes ( paths , exact = False , ignores = None , min_size = 0 ) : groups = { '' : getPaths ( paths , ignores ) } groups = groupBy ( groups , sizeClassifier , 'sizes' , min_size = min_size ) groups = groupBy ( groups , hashClassifier , 'header hashes' , limit = HEAD_SIZE ) if exact : groups = groupBy ( groups , groupByContent , fun_desc = 'contents' ) else : groups = groupBy ( groups , hashClassifier , fun_desc = 'hashes' ) return groups
782	def jobReactivateRunningJobs ( self ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET _eng_cjm_conn_id=%%s, ' ' _eng_allocate_new_workers=TRUE ' ' WHERE status=%%s ' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . _connectionID , self . STATUS_RUNNING ] ) return
12235	def generate_versionwarning_data_json ( app , config = None , ** kwargs ) : config = config or kwargs . pop ( 'config' , None ) if config is None : config = app . config if config . versionwarning_project_version in config . versionwarning_messages : custom = True message = config . versionwarning_messages . get ( config . versionwarning_project_version ) else : custom = False message = config . versionwarning_default_message banner_html = config . versionwarning_banner_html . format ( id_div = config . versionwarning_banner_id_div , banner_title = config . versionwarning_banner_title , message = message . format ( ** { config . versionwarning_message_placeholder : '<a href="#"></a>' } , ) , admonition_type = config . versionwarning_admonition_type , ) data = json . dumps ( { 'meta' : { 'api_url' : config . versionwarning_api_url , } , 'banner' : { 'html' : banner_html , 'id_div' : config . versionwarning_banner_id_div , 'body_selector' : config . versionwarning_body_selector , 'custom' : custom , } , 'project' : { 'slug' : config . versionwarning_project_slug , } , 'version' : { 'slug' : config . versionwarning_project_version , } , } , indent = 4 ) data_path = os . path . join ( STATIC_PATH , 'data' ) if not os . path . exists ( data_path ) : os . mkdir ( data_path ) with open ( os . path . join ( data_path , JSON_DATA_FILENAME ) , 'w' ) as f : f . write ( data ) config . html_static_path . append ( STATIC_PATH )
5870	def fetch_organization_courses ( organization ) : organization_obj = serializers . deserialize_organization ( organization ) queryset = internal . OrganizationCourse . objects . filter ( organization = organization_obj , active = True ) . select_related ( 'organization' ) return [ serializers . serialize_organization_with_course ( organization ) for organization in queryset ]
6422	def _synoname_strip_punct ( self , word ) : stripped = '' for char in word : if char not in set ( ',-./:;"&\'()!{|}?$%*+<=>[\\]^_`~' ) : stripped += char return stripped . strip ( )
10185	def _events_config ( self ) : result = { } for ep in iter_entry_points ( group = self . entry_point_group_events ) : for cfg in ep . load ( ) ( ) : if cfg [ 'event_type' ] not in self . enabled_events : continue elif cfg [ 'event_type' ] in result : raise DuplicateEventError ( 'Duplicate event {0} in entry point ' '{1}' . format ( cfg [ 'event_type' ] , ep . name ) ) cfg . update ( self . enabled_events [ cfg [ 'event_type' ] ] or { } ) result [ cfg [ 'event_type' ] ] = cfg return result
2534	def validate_str_fields ( self , fields , optional , messages ) : for field_str in fields : field = getattr ( self , field_str ) if field is not None : attr = getattr ( field , '__str__' , None ) if not callable ( attr ) : messages = messages + [ '{0} must provide __str__ method.' . format ( field ) ] elif not optional : messages = messages + [ 'Package {0} can not be None.' . format ( field_str ) ] return messages
1085	def timetz ( self ) : "Return the time part, with same tzinfo." return time ( self . hour , self . minute , self . second , self . microsecond , self . _tzinfo )
3407	def eval_gpr ( expr , knockouts ) : if isinstance ( expr , Expression ) : return eval_gpr ( expr . body , knockouts ) elif isinstance ( expr , Name ) : return expr . id not in knockouts elif isinstance ( expr , BoolOp ) : op = expr . op if isinstance ( op , Or ) : return any ( eval_gpr ( i , knockouts ) for i in expr . values ) elif isinstance ( op , And ) : return all ( eval_gpr ( i , knockouts ) for i in expr . values ) else : raise TypeError ( "unsupported operation " + op . __class__ . __name__ ) elif expr is None : return True else : raise TypeError ( "unsupported operation " + repr ( expr ) )
9820	def project ( ctx , project ) : if ctx . invoked_subcommand not in [ 'create' , 'list' ] : ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project
9436	def strip_ip ( packet ) : if not isinstance ( packet , IP ) : packet = IP ( packet ) payload = packet . payload return payload
13185	def get_default_tag ( app ) : view_func = get_view_function ( app , request . path , request . method ) if view_func : return view_func . __name__
2797	def transfer ( self , new_region_slug ) : return self . get_data ( "images/%s/actions/" % self . id , type = POST , params = { "type" : "transfer" , "region" : new_region_slug } )
9037	def walk_connections ( self , mapping = identity ) : for start in self . walk_instructions ( ) : for stop_instruction in start . instruction . consuming_instructions : if stop_instruction is None : continue stop = self . _walk . instruction_in_grid ( stop_instruction ) connection = Connection ( start , stop ) if connection . is_visible ( ) : yield mapping ( connection )
7777	def __make_fn ( self ) : s = [ ] if self . n . prefix : s . append ( self . n . prefix ) if self . n . given : s . append ( self . n . given ) if self . n . middle : s . append ( self . n . middle ) if self . n . family : s . append ( self . n . family ) if self . n . suffix : s . append ( self . n . suffix ) s = u" " . join ( s ) self . content [ "FN" ] = VCardString ( "FN" , s , empty_ok = True )
2771	def get_object ( cls , api_token , id ) : load_balancer = cls ( token = api_token , id = id ) load_balancer . load ( ) return load_balancer
382	def drop ( x , keep = 0.5 ) : if len ( x . shape ) == 3 : if x . shape [ - 1 ] == 3 : img_size = x . shape mask = np . random . binomial ( n = 1 , p = keep , size = x . shape [ : - 1 ] ) for i in range ( 3 ) : x [ : , : , i ] = np . multiply ( x [ : , : , i ] , mask ) elif x . shape [ - 1 ] == 1 : img_size = x . shape x = np . multiply ( x , np . random . binomial ( n = 1 , p = keep , size = img_size ) ) else : raise Exception ( "Unsupported shape {}" . format ( x . shape ) ) elif len ( x . shape ) == 2 or 1 : img_size = x . shape x = np . multiply ( x , np . random . binomial ( n = 1 , p = keep , size = img_size ) ) else : raise Exception ( "Unsupported shape {}" . format ( x . shape ) ) return x
13455	def _parse_args ( args ) : parser = argparse . ArgumentParser ( description = "Remove and/or rearrange " + "sections from each line of a file(s)." , usage = _usage ( ) [ len ( 'usage: ' ) : ] ) parser . add_argument ( '-b' , "--bytes" , action = 'store' , type = lst , default = [ ] , help = "Bytes to select" ) parser . add_argument ( '-c' , "--chars" , action = 'store' , type = lst , default = [ ] , help = "Character to select" ) parser . add_argument ( '-f' , "--fields" , action = 'store' , type = lst , default = [ ] , help = "Fields to select" ) parser . add_argument ( '-d' , "--delimiter" , action = 'store' , default = "\t" , help = "Sets field delimiter(default is TAB)" ) parser . add_argument ( '-e' , "--regex" , action = 'store_true' , help = 'Enable regular expressions to be used as input ' + 'delimiter' ) parser . add_argument ( '-s' , '--skip' , action = 'store_true' , help = "Skip lines that do not contain input delimiter." ) parser . add_argument ( '-S' , "--separator" , action = 'store' , default = "\t" , help = "Sets field separator for output." ) parser . add_argument ( 'file' , nargs = '*' , default = "-" , help = "File(s) to cut" ) return parser . parse_args ( args )
10043	def create_blueprint ( endpoints ) : from invenio_records_ui . views import create_url_rule blueprint = Blueprint ( 'invenio_deposit_ui' , __name__ , static_folder = '../static' , template_folder = '../templates' , url_prefix = '' , ) @ blueprint . errorhandler ( PIDDeletedError ) def tombstone_errorhandler ( error ) : return render_template ( current_app . config [ 'DEPOSIT_UI_TOMBSTONE_TEMPLATE' ] , pid = error . pid , record = error . record or { } , ) , 410 for endpoint , options in ( endpoints or { } ) . items ( ) : options = deepcopy ( options ) options . pop ( 'jsonschema' , None ) options . pop ( 'schemaform' , None ) blueprint . add_url_rule ( ** create_url_rule ( endpoint , ** options ) ) @ blueprint . route ( '/deposit' ) @ login_required def index ( ) : return render_template ( current_app . config [ 'DEPOSIT_UI_INDEX_TEMPLATE' ] ) @ blueprint . route ( '/deposit/new' ) @ login_required def new ( ) : deposit_type = request . values . get ( 'type' ) return render_template ( current_app . config [ 'DEPOSIT_UI_NEW_TEMPLATE' ] , record = { '_deposit' : { 'id' : None } } , jsonschema = current_deposit . jsonschemas [ deposit_type ] , schemaform = current_deposit . schemaforms [ deposit_type ] , ) return blueprint
12071	def update ( self , tids , info ) : outputs_dir = os . path . join ( info [ 'root_directory' ] , 'streams' ) pattern = '%s_*_tid_*{tid}.o.{tid}*' % info [ 'batch_name' ] flist = os . listdir ( outputs_dir ) try : outputs = [ ] for tid in tids : matches = fnmatch . filter ( flist , pattern . format ( tid = tid ) ) if len ( matches ) != 1 : self . warning ( "No unique output file for tid %d" % tid ) contents = open ( os . path . join ( outputs_dir , matches [ 0 ] ) , 'r' ) . read ( ) outputs . append ( self . output_extractor ( contents ) ) self . _next_val = self . _update_state ( outputs ) self . trace . append ( ( outputs , self . _next_val ) ) except : self . warning ( "Cannot load required output files. Cannot continue." ) self . _next_val = StopIteration
12587	def all_childnodes_to_nifti1img ( h5group ) : child_nodes = [ ] def append_parent_if_dataset ( name , obj ) : if isinstance ( obj , h5py . Dataset ) : if name . split ( '/' ) [ - 1 ] == 'data' : child_nodes . append ( obj . parent ) vols = [ ] h5group . visititems ( append_parent_if_dataset ) for c in child_nodes : vols . append ( hdfgroup_to_nifti1image ( c ) ) return vols
1493	def _get_next_timeout_interval ( self ) : if len ( self . timer_tasks ) == 0 : return sys . maxsize else : next_timeout_interval = self . timer_tasks [ 0 ] [ 0 ] - time . time ( ) return next_timeout_interval
8509	def fit ( self , X , y = None ) : from pylearn2 . config import yaml_parse from pylearn2 . train import Train params = self . get_params ( ) yaml_string = Template ( self . yaml_string ) . substitute ( params ) self . trainer = yaml_parse . load ( yaml_string ) assert isinstance ( self . trainer , Train ) if self . trainer . dataset is not None : raise ValueError ( 'Train YAML database must evaluate to None.' ) self . trainer . dataset = self . _get_dataset ( X , y ) if ( hasattr ( self . trainer . algorithm , 'monitoring_dataset' ) and self . trainer . algorithm . monitoring_dataset is not None ) : monitoring_dataset = self . trainer . algorithm . monitoring_dataset if len ( monitoring_dataset ) == 1 and '' in monitoring_dataset : monitoring_dataset [ '' ] = self . trainer . dataset else : monitoring_dataset [ 'train' ] = self . trainer . dataset self . trainer . algorithm . _set_monitoring_dataset ( monitoring_dataset ) else : self . trainer . algorithm . _set_monitoring_dataset ( self . trainer . dataset ) self . trainer . main_loop ( )
6437	def dist_abs ( self , src , tar , weights = 'exponential' , max_length = 8 , normalized = False ) : xored = eudex ( src , max_length = max_length ) ^ eudex ( tar , max_length = max_length ) if not weights : binary = bin ( xored ) distance = binary . count ( '1' ) if normalized : return distance / ( len ( binary ) - 2 ) return distance if callable ( weights ) : weights = weights ( ) elif weights == 'exponential' : weights = Eudex . gen_exponential ( ) elif weights == 'fibonacci' : weights = Eudex . gen_fibonacci ( ) if isinstance ( weights , GeneratorType ) : weights = [ next ( weights ) for _ in range ( max_length ) ] [ : : - 1 ] distance = 0 max_distance = 0 while ( xored or normalized ) and weights : max_distance += 8 * weights [ - 1 ] distance += bin ( xored & 0xFF ) . count ( '1' ) * weights . pop ( ) xored >>= 8 if normalized : distance /= max_distance return distance
4718	def tsuite_enter ( trun , tsuite ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:enter { name: %r }" % tsuite [ "name" ] ) rcode = 0 for hook in tsuite [ "hooks" ] [ "enter" ] : rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:enter { rcode: %r } " % rcode , rcode ) return rcode
3664	def calculate ( self , T , method ) : r if method == PERRY151 : Cp = ( self . PERRY151_const + self . PERRY151_lin * T + self . PERRY151_quadinv / T ** 2 + self . PERRY151_quad * T ** 2 ) * calorie elif method == CRCSTD : Cp = self . CRCSTD_Cp elif method == LASTOVKA_S : Cp = Lastovka_solid ( T , self . similarity_variable ) Cp = property_mass_to_molar ( Cp , self . MW ) elif method in self . tabular_data : Cp = self . interpolate ( T , method ) return Cp
9575	def read_elements ( fd , endian , mtps , is_name = False ) : mtpn , num_bytes , data = read_element_tag ( fd , endian ) if mtps and mtpn not in [ etypes [ mtp ] [ 'n' ] for mtp in mtps ] : raise ParseError ( 'Got type {}, expected {}' . format ( mtpn , ' / ' . join ( '{} ({})' . format ( etypes [ mtp ] [ 'n' ] , mtp ) for mtp in mtps ) ) ) if not data : data = fd . read ( num_bytes ) mod8 = num_bytes % 8 if mod8 : fd . seek ( 8 - mod8 , 1 ) if is_name : fmt = 's' val = [ unpack ( endian , fmt , s ) for s in data . split ( b'\0' ) if s ] if len ( val ) == 0 : val = '' elif len ( val ) == 1 : val = asstr ( val [ 0 ] ) else : val = [ asstr ( s ) for s in val ] else : fmt = etypes [ inv_etypes [ mtpn ] ] [ 'fmt' ] val = unpack ( endian , fmt , data ) return val
2835	def write ( self , data , assert_ss = True , deassert_ss = True ) : if self . _mosi is None : raise RuntimeError ( 'Write attempted with no MOSI pin specified.' ) if assert_ss and self . _ss is not None : self . _gpio . set_low ( self . _ss ) for byte in data : for i in range ( 8 ) : if self . _write_shift ( byte , i ) & self . _mask : self . _gpio . set_high ( self . _mosi ) else : self . _gpio . set_low ( self . _mosi ) self . _gpio . output ( self . _sclk , not self . _clock_base ) self . _gpio . output ( self . _sclk , self . _clock_base ) if deassert_ss and self . _ss is not None : self . _gpio . set_high ( self . _ss )
7243	def geotiff ( self , ** kwargs ) : if 'proj' not in kwargs : kwargs [ 'proj' ] = self . proj return to_geotiff ( self , ** kwargs )
2710	def make_sentence ( sent_text ) : lex = [ ] idx = 0 for word in sent_text : if len ( word ) > 0 : if ( idx > 0 ) and not ( word [ 0 ] in ",.:;!?-\"'" ) : lex . append ( " " ) lex . append ( word ) idx += 1 return "" . join ( lex )
2385	def from_spec_resolver ( cls , spec_resolver ) : deref = DerefValidatorDecorator ( spec_resolver ) for key , validator_callable in iteritems ( cls . validators ) : yield key , deref ( validator_callable )
11641	def yaml_get_data ( filename ) : with open ( filename , 'rb' ) as fd : yaml_data = yaml . load ( fd ) return yaml_data return False
4092	def addSearchers ( self , * searchers ) : self . _searchers . extend ( searchers ) debug . logger & debug . flagCompiler and debug . logger ( 'current compiled MIBs location(s): %s' % ', ' . join ( [ str ( x ) for x in self . _searchers ] ) ) return self
3701	def solubility_eutectic ( T , Tm , Hm , Cpl = 0 , Cps = 0 , gamma = 1 ) : r dCp = Cpl - Cps x = exp ( - Hm / R / T * ( 1 - T / Tm ) + dCp * ( Tm - T ) / R / T - dCp / R * log ( Tm / T ) ) / gamma return x
839	def getClosest ( self , inputPattern , topKCategories = 3 ) : inferenceResult = numpy . zeros ( max ( self . _categoryList ) + 1 ) dist = self . _getDistances ( inputPattern ) sorted = dist . argsort ( ) validVectorCount = len ( self . _categoryList ) - self . _categoryList . count ( - 1 ) for j in sorted [ : min ( self . k , validVectorCount ) ] : inferenceResult [ self . _categoryList [ j ] ] += 1.0 winner = inferenceResult . argmax ( ) topNCats = [ ] for i in range ( topKCategories ) : topNCats . append ( ( self . _categoryList [ sorted [ i ] ] , dist [ sorted [ i ] ] ) ) return winner , dist , topNCats
3406	def ast2str ( expr , level = 0 , names = None ) : if isinstance ( expr , Expression ) : return ast2str ( expr . body , 0 , names ) if hasattr ( expr , "body" ) else "" elif isinstance ( expr , Name ) : return names . get ( expr . id , expr . id ) if names else expr . id elif isinstance ( expr , BoolOp ) : op = expr . op if isinstance ( op , Or ) : str_exp = " or " . join ( ast2str ( i , level + 1 , names ) for i in expr . values ) elif isinstance ( op , And ) : str_exp = " and " . join ( ast2str ( i , level + 1 , names ) for i in expr . values ) else : raise TypeError ( "unsupported operation " + op . __class__ . __name ) return "(" + str_exp + ")" if level else str_exp elif expr is None : return "" else : raise TypeError ( "unsupported operation " + repr ( expr ) )
6695	def upgrade ( safe = True ) : manager = MANAGER if safe : cmd = 'upgrade' else : cmd = 'dist-upgrade' run_as_root ( "%(manager)s --assume-yes %(cmd)s" % locals ( ) , pty = False )
10852	def otsu_threshold ( data , bins = 255 ) : h0 , x0 = np . histogram ( data . ravel ( ) , bins = bins ) h = h0 . astype ( 'float' ) / h0 . sum ( ) x = 0.5 * ( x0 [ 1 : ] + x0 [ : - 1 ] ) wk = np . array ( [ h [ : i + 1 ] . sum ( ) for i in range ( h . size ) ] ) mk = np . array ( [ sum ( x [ : i + 1 ] * h [ : i + 1 ] ) for i in range ( h . size ) ] ) mt = mk [ - 1 ] sb = ( mt * wk - mk ) ** 2 / ( wk * ( 1 - wk ) + 1e-15 ) ind = sb . argmax ( ) return 0.5 * ( x0 [ ind ] + x0 [ ind + 1 ] )
4396	def adsSyncWriteByNameEx ( port , address , data_name , value , data_type ) : handle = adsSyncReadWriteReqEx2 ( port , address , ADSIGRP_SYM_HNDBYNAME , 0x0 , PLCTYPE_UDINT , data_name , PLCTYPE_STRING , ) adsSyncWriteReqEx ( port , address , ADSIGRP_SYM_VALBYHND , handle , value , data_type ) adsSyncWriteReqEx ( port , address , ADSIGRP_SYM_RELEASEHND , 0 , handle , PLCTYPE_UDINT )
535	def writeToProto ( self , proto ) : proto . implementation = self . implementation proto . steps = self . steps proto . alpha = self . alpha proto . verbosity = self . verbosity proto . maxCategoryCount = self . maxCategoryCount proto . learningMode = self . learningMode proto . inferenceMode = self . inferenceMode proto . recordNum = self . recordNum self . _sdrClassifier . write ( proto . sdrClassifier )
1317	def GetChildren ( self ) -> list : children = [ ] child = self . GetFirstChildControl ( ) while child : children . append ( child ) child = child . GetNextSiblingControl ( ) return children
6394	def sim_levenshtein ( src , tar , mode = 'lev' , cost = ( 1 , 1 , 1 , 1 ) ) : return Levenshtein ( ) . sim ( src , tar , mode , cost )
9824	def update ( ctx , name , description , tags , private ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description if private is not None : update_dict [ 'is_public' ] = not private tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the project.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . project . update_project ( user , project_name , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Project updated." ) get_project_details ( response )
11328	def select ( options = None ) : if not options : return None width = len ( str ( len ( options ) ) ) for x , option in enumerate ( options ) : sys . stdout . write ( '{:{width}}) {}\n' . format ( x + 1 , option , width = width ) ) sys . stdout . write ( '{:>{width}} ' . format ( '#?' , width = width + 1 ) ) sys . stdout . flush ( ) if sys . stdin . isatty ( ) : try : response = raw_input ( ) . strip ( ) except ( EOFError , KeyboardInterrupt ) : response = '' else : sys . stdin = open ( "/dev/tty" ) try : response = '' while True : response += sys . stdin . read ( 1 ) if response . endswith ( '\n' ) : break except ( EOFError , KeyboardInterrupt ) : sys . stdout . flush ( ) pass try : response = int ( response ) - 1 except ValueError : return None if response < 0 or response >= len ( options ) : return None return options [ response ]
176	def extract_from_image ( self , image , size = 1 , pad = True , pad_max = None , antialiased = True , prevent_zero_size = True ) : from . bbs import BoundingBox assert image . ndim in [ 2 , 3 ] , ( "Expected image of shape (H,W,[C]), " "got shape %s." % ( image . shape , ) ) if len ( self . coords ) == 0 or size <= 0 : if prevent_zero_size : return np . zeros ( ( 1 , 1 ) + image . shape [ 2 : ] , dtype = image . dtype ) return np . zeros ( ( 0 , 0 ) + image . shape [ 2 : ] , dtype = image . dtype ) xx = self . xx_int yy = self . yy_int sizeh = ( size - 1 ) / 2 x1 = np . min ( xx ) - sizeh y1 = np . min ( yy ) - sizeh x2 = np . max ( xx ) + 1 + sizeh y2 = np . max ( yy ) + 1 + sizeh bb = BoundingBox ( x1 = x1 , y1 = y1 , x2 = x2 , y2 = y2 ) if len ( self . coords ) == 1 : return bb . extract_from_image ( image , pad = pad , pad_max = pad_max , prevent_zero_size = prevent_zero_size ) heatmap = self . draw_lines_heatmap_array ( image . shape [ 0 : 2 ] , alpha = 1.0 , size = size , antialiased = antialiased ) if image . ndim == 3 : heatmap = np . atleast_3d ( heatmap ) image_masked = image . astype ( np . float32 ) * heatmap extract = bb . extract_from_image ( image_masked , pad = pad , pad_max = pad_max , prevent_zero_size = prevent_zero_size ) return np . clip ( np . round ( extract ) , 0 , 255 ) . astype ( np . uint8 )
8609	def list_resources ( self , resource_type = None , depth = 1 ) : if resource_type is not None : response = self . _perform_request ( '/um/resources/%s?depth=%s' % ( resource_type , str ( depth ) ) ) else : response = self . _perform_request ( '/um/resources?depth=' + str ( depth ) ) return response
5117	def get_agent_data ( self , queues = None , edge = None , edge_type = None , return_header = False ) : queues = _get_queues ( self . g , queues , edge , edge_type ) data = { } for qid in queues : for agent_id , dat in self . edge2queue [ qid ] . data . items ( ) : datum = np . zeros ( ( len ( dat ) , 6 ) ) datum [ : , : 5 ] = np . array ( dat ) datum [ : , 5 ] = qid if agent_id in data : data [ agent_id ] = np . vstack ( ( data [ agent_id ] , datum ) ) else : data [ agent_id ] = datum dType = [ ( 'a' , float ) , ( 's' , float ) , ( 'd' , float ) , ( 'q' , float ) , ( 'n' , float ) , ( 'id' , float ) ] for agent_id , dat in data . items ( ) : datum = np . array ( [ tuple ( d ) for d in dat . tolist ( ) ] , dtype = dType ) datum = np . sort ( datum , order = 'a' ) data [ agent_id ] = np . array ( [ tuple ( d ) for d in datum ] ) if return_header : return data , 'arrival,service,departure,num_queued,num_total,q_id' return data
3414	def model_to_dict ( model , sort = False ) : obj = OrderedDict ( ) obj [ "metabolites" ] = list ( map ( metabolite_to_dict , model . metabolites ) ) obj [ "reactions" ] = list ( map ( reaction_to_dict , model . reactions ) ) obj [ "genes" ] = list ( map ( gene_to_dict , model . genes ) ) obj [ "id" ] = model . id _update_optional ( model , obj , _OPTIONAL_MODEL_ATTRIBUTES , _ORDERED_OPTIONAL_MODEL_KEYS ) if sort : get_id = itemgetter ( "id" ) obj [ "metabolites" ] . sort ( key = get_id ) obj [ "reactions" ] . sort ( key = get_id ) obj [ "genes" ] . sort ( key = get_id ) return obj
2061	def _declare ( self , var ) : if var . name in self . _declarations : raise ValueError ( 'Variable already declared' ) self . _declarations [ var . name ] = var return var
4196	def TOEPLITZ ( T0 , TC , TR , Z ) : assert len ( TC ) > 0 assert len ( TC ) == len ( TR ) M = len ( TC ) X = numpy . zeros ( M + 1 , dtype = complex ) A = numpy . zeros ( M , dtype = complex ) B = numpy . zeros ( M , dtype = complex ) P = T0 if P == 0 : raise ValueError ( "P must be different from zero" ) if P == 0 : raise ValueError ( "P must be different from zero" ) X [ 0 ] = Z [ 0 ] / T0 for k in range ( 0 , M ) : save1 = TC [ k ] save2 = TR [ k ] beta = X [ 0 ] * TC [ k ] if k == 0 : temp1 = - save1 / P temp2 = - save2 / P else : for j in range ( 0 , k ) : save1 = save1 + A [ j ] * TC [ k - j - 1 ] save2 = save2 + B [ j ] * TR [ k - j - 1 ] beta = beta + X [ j + 1 ] * TC [ k - j - 1 ] temp1 = - save1 / P temp2 = - save2 / P P = P * ( 1. - ( temp1 * temp2 ) ) if P <= 0 : raise ValueError ( "singular matrix" ) A [ k ] = temp1 B [ k ] = temp2 alpha = ( Z [ k + 1 ] - beta ) / P if k == 0 : X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * B [ k - j ] continue for j in range ( 0 , k ) : kj = k - j - 1 save1 = A [ j ] A [ j ] = save1 + temp1 * B [ kj ] B [ kj ] = B [ kj ] + temp2 * save1 X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * B [ k - j ] return X
8871	def match ( self , subsetLines , offsetOfSubset , fileName ) : for ( offset , l ) in enumerate ( subsetLines ) : column = l . find ( self . literal ) if column != - 1 : truePosition = offset + offsetOfSubset _logger . debug ( 'Found match on line {}, col {}' . format ( str ( truePosition + 1 ) , column ) ) _logger . debug ( 'Line is {}' . format ( l ) ) self . matchLocation = CheckFileParser . FileLocation ( fileName , truePosition + 1 ) return truePosition self . failed = True raise DirectiveException ( self )
7	def observation_placeholder ( ob_space , batch_size = None , name = 'Ob' ) : assert isinstance ( ob_space , Discrete ) or isinstance ( ob_space , Box ) or isinstance ( ob_space , MultiDiscrete ) , 'Can only deal with Discrete and Box observation spaces for now' dtype = ob_space . dtype if dtype == np . int8 : dtype = np . uint8 return tf . placeholder ( shape = ( batch_size , ) + ob_space . shape , dtype = dtype , name = name )
7570	def fastq_touchup_for_vsearch_merge ( read , outfile , reverse = False ) : counts = 0 with open ( outfile , 'w' ) as out : if read . endswith ( ".gz" ) : fr1 = gzip . open ( read , 'rb' ) else : fr1 = open ( read , 'rb' ) quarts = itertools . izip ( * [ iter ( fr1 ) ] * 4 ) writing = [ ] while 1 : try : lines = quarts . next ( ) except StopIteration : break if reverse : seq = lines [ 1 ] . strip ( ) [ : : - 1 ] else : seq = lines [ 1 ] . strip ( ) writing . append ( "" . join ( [ lines [ 0 ] , seq + "\n" , lines [ 2 ] , "B" * len ( seq ) ] ) ) counts += 1 if not counts % 1000 : out . write ( "\n" . join ( writing ) + "\n" ) writing = [ ] if writing : out . write ( "\n" . join ( writing ) ) out . close ( ) fr1 . close ( )
5010	def _call_post_with_user_override ( self , sap_user_id , url , payload ) : SAPSuccessFactorsEnterpriseCustomerConfiguration = apps . get_model ( 'sap_success_factors' , 'SAPSuccessFactorsEnterpriseCustomerConfiguration' ) oauth_access_token , _ = SAPSuccessFactorsAPIClient . get_oauth_access_token ( self . enterprise_configuration . sapsf_base_url , self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . sapsf_company_id , sap_user_id , SAPSuccessFactorsEnterpriseCustomerConfiguration . USER_TYPE_USER ) response = requests . post ( url , data = payload , headers = { 'Authorization' : 'Bearer {}' . format ( oauth_access_token ) , 'content-type' : 'application/json' } ) return response . status_code , response . text
12916	def prune ( self , regex = r".*" ) : return filetree ( self . root , ignore = self . ignore , regex = regex )
9987	def to_frame ( self , * args ) : if sys . version_info < ( 3 , 6 , 0 ) : from collections import OrderedDict impls = OrderedDict ( ) for name , obj in self . items ( ) : impls [ name ] = obj . _impl else : impls = get_impls ( self ) return _to_frame_inner ( impls , args )
2672	def upload ( src , requirements = None , local_package = None , config_file = 'config.yaml' , profile_name = None , ) : path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) path_to_zip_file = build ( src , config_file = config_file , requirements = requirements , local_package = local_package , ) upload_s3 ( cfg , path_to_zip_file )
8929	def pylint ( ctx , skip_tests = False , skip_root = False , reports = False ) : cfg = config . load ( ) add_dir2pypath ( cfg . project_root ) if not os . path . exists ( cfg . testjoin ( '__init__.py' ) ) : add_dir2pypath ( cfg . testjoin ( ) ) namelist = set ( ) for package in cfg . project . get ( 'packages' , [ ] ) : if '.' not in package : namelist . add ( cfg . srcjoin ( package ) ) for module in cfg . project . get ( 'py_modules' , [ ] ) : namelist . add ( module + '.py' ) if not skip_tests : test_py = antglob . FileSet ( cfg . testdir , '**/*.py' ) test_py = [ cfg . testjoin ( i ) for i in test_py ] if test_py : namelist |= set ( test_py ) if not skip_root : root_py = antglob . FileSet ( '.' , '*.py' ) if root_py : namelist |= set ( root_py ) namelist = set ( [ i [ len ( os . getcwd ( ) ) + 1 : ] if i . startswith ( os . getcwd ( ) + os . sep ) else i for i in namelist ] ) cmd = 'pylint' cmd += ' "{}"' . format ( '" "' . join ( sorted ( namelist ) ) ) cmd += ' --reports={0}' . format ( 'y' if reports else 'n' ) for cfgfile in ( '.pylintrc' , 'pylint.rc' , 'pylint.cfg' , 'project.d/pylint.cfg' ) : if os . path . exists ( cfgfile ) : cmd += ' --rcfile={0}' . format ( cfgfile ) break try : shell . run ( cmd , report_error = False , runner = ctx . run ) notify . info ( "OK - No problems found by pylint." ) except exceptions . Failure as exc : if exc . result . return_code & 32 : notify . error ( "Usage error, bad arguments in {}?!" . format ( repr ( cmd ) ) ) raise else : bits = { 1 : "fatal" , 2 : "error" , 4 : "warning" , 8 : "refactor" , 16 : "convention" , } notify . warning ( "Some messages of type {} issued by pylint." . format ( ", " . join ( [ text for bit , text in bits . items ( ) if exc . result . return_code & bit ] ) ) ) if exc . result . return_code & 3 : notify . error ( "Exiting due to fatal / error message." ) raise
8059	def do_vars ( self , line ) : if self . bot . _vars : max_name_len = max ( [ len ( name ) for name in self . bot . _vars ] ) for i , ( name , v ) in enumerate ( self . bot . _vars . items ( ) ) : keep = i < len ( self . bot . _vars ) - 1 self . print_response ( "%s = %s" % ( name . ljust ( max_name_len ) , v . value ) , keep = keep ) else : self . print_response ( "No vars" )
2300	def predict_undirected_graph ( self , data ) : graph = Graph ( ) for idx_i , i in enumerate ( data . columns ) : for idx_j , j in enumerate ( data . columns [ idx_i + 1 : ] ) : score = self . predict ( data [ i ] . values , data [ j ] . values ) if abs ( score ) > 0.001 : graph . add_edge ( i , j , weight = score ) return graph
10382	def multi_run_epicom ( graphs : Iterable [ BELGraph ] , path : Union [ None , str , TextIO ] ) -> None : if isinstance ( path , str ) : with open ( path , 'w' ) as file : _multi_run_helper_file_wrapper ( graphs , file ) else : _multi_run_helper_file_wrapper ( graphs , path )
7043	def lightcurve_moments ( ftimes , fmags , ferrs ) : ndet = len ( fmags ) if ndet > 9 : series_median = npmedian ( fmags ) series_wmean = ( npsum ( fmags * ( 1.0 / ( ferrs * ferrs ) ) ) / npsum ( 1.0 / ( ferrs * ferrs ) ) ) series_mad = npmedian ( npabs ( fmags - series_median ) ) series_stdev = 1.483 * series_mad series_skew = spskew ( fmags ) series_kurtosis = spkurtosis ( fmags ) series_above1std = len ( fmags [ fmags > ( series_median + series_stdev ) ] ) series_below1std = len ( fmags [ fmags < ( series_median - series_stdev ) ] ) series_beyond1std = ( series_above1std + series_below1std ) / float ( ndet ) series_mag_percentiles = nppercentile ( fmags , [ 5.0 , 10 , 17.5 , 25 , 32.5 , 40 , 60 , 67.5 , 75 , 82.5 , 90 , 95 ] ) return { 'median' : series_median , 'wmean' : series_wmean , 'mad' : series_mad , 'stdev' : series_stdev , 'skew' : series_skew , 'kurtosis' : series_kurtosis , 'beyond1std' : series_beyond1std , 'mag_percentiles' : series_mag_percentiles , 'mag_iqr' : series_mag_percentiles [ 8 ] - series_mag_percentiles [ 3 ] , } else : LOGERROR ( 'not enough detections in this magseries ' 'to calculate light curve moments' ) return None
633	def createSynapse ( self , segment , presynapticCell , permanence ) : idx = len ( segment . _synapses ) synapse = Synapse ( segment , presynapticCell , permanence , self . _nextSynapseOrdinal ) self . _nextSynapseOrdinal += 1 segment . _synapses . add ( synapse ) self . _synapsesForPresynapticCell [ presynapticCell ] . add ( synapse ) self . _numSynapses += 1 return synapse
9515	def to_Fastq ( self , qual_scores ) : if len ( self ) != len ( qual_scores ) : raise Error ( 'Error making Fastq from Fasta, lengths differ.' , self . id ) return Fastq ( self . id , self . seq , '' . join ( [ chr ( max ( 0 , min ( x , 93 ) ) + 33 ) for x in qual_scores ] ) )
9030	def _expand_consumed_mesh ( self , mesh , mesh_index , row_position , passed ) : if not mesh . is_produced ( ) : return row = mesh . producing_row position = Point ( row_position . x + mesh . index_in_producing_row - mesh_index , row_position . y - INSTRUCTION_HEIGHT ) self . _expand ( row , position , passed )
12738	def create_bodies ( self , translate = ( 0 , 1 , 0 ) , size = 0.1 ) : stack = [ ( 'root' , 0 , self . root [ 'position' ] + translate ) ] while stack : name , depth , end = stack . pop ( ) for child in self . hierarchy . get ( name , ( ) ) : stack . append ( ( child , depth + 1 , end + self . bones [ child ] . end ) ) if name not in self . bones : continue bone = self . bones [ name ] body = self . world . create_body ( 'box' , name = bone . name , density = self . density , lengths = ( size , size , bone . length ) ) body . color = self . color x , y , z = end - bone . direction * bone . length / 2 body . position = x , z , y u = bone . direction v = np . cross ( u , [ 0 , 1 , 0 ] ) l = np . linalg . norm ( v ) if l > 0 : v /= l rot = np . vstack ( [ np . cross ( u , v ) , v , u ] ) . T swizzle = [ [ 1 , 0 , 0 ] , [ 0 , 0 , 1 ] , [ 0 , - 1 , 0 ] ] body . rotation = np . dot ( swizzle , rot ) self . bodies . append ( body )
4006	def pty_fork ( * args ) : updated_env = copy ( os . environ ) updated_env . update ( get_docker_env ( ) ) args += ( updated_env , ) executable = args [ 0 ] demote_fn = demote_to_user ( get_config_value ( constants . CONFIG_MAC_USERNAME_KEY ) ) child_pid , pty_fd = pty . fork ( ) if child_pid == 0 : demote_fn ( ) os . execle ( _executable_path ( executable ) , * args ) else : child_process = psutil . Process ( child_pid ) terminal = os . fdopen ( pty_fd , 'r' , 0 ) with streaming_to_client ( ) : while child_process . status ( ) == 'running' : output = terminal . read ( 1 ) log_to_client ( output ) _ , exit_code = os . waitpid ( child_pid , 0 ) if exit_code != 0 : raise subprocess . CalledProcessError ( exit_code , ' ' . join ( args [ : - 1 ] ) )
7994	def _send ( self , stanza ) : self . fix_out_stanza ( stanza ) element = stanza . as_xml ( ) self . _write_element ( element )
5998	def plot_grid ( grid_arcsec , array , units , kpc_per_arcsec , pointsize , zoom_offset_arcsec ) : if grid_arcsec is not None : if zoom_offset_arcsec is not None : grid_arcsec -= zoom_offset_arcsec grid_units = convert_grid_units ( grid_arcsec = grid_arcsec , array = array , units = units , kpc_per_arcsec = kpc_per_arcsec ) plt . scatter ( y = np . asarray ( grid_units [ : , 0 ] ) , x = np . asarray ( grid_units [ : , 1 ] ) , s = pointsize , c = 'k' )
1009	def compute ( self , bottomUpInput , enableLearn , enableInference = None ) : if enableInference is None : if enableLearn : enableInference = False else : enableInference = True assert ( enableLearn or enableInference ) activeColumns = bottomUpInput . nonzero ( ) [ 0 ] if enableLearn : self . lrnIterationIdx += 1 self . iterationIdx += 1 if self . verbosity >= 3 : print "\n==== PY Iteration: %d =====" % ( self . iterationIdx ) print "Active cols:" , activeColumns if enableLearn : if self . lrnIterationIdx in Segment . dutyCycleTiers : for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : for segment in self . cells [ c ] [ i ] : segment . dutyCycle ( ) if self . avgInputDensity is None : self . avgInputDensity = len ( activeColumns ) else : self . avgInputDensity = ( 0.99 * self . avgInputDensity + 0.01 * len ( activeColumns ) ) if enableInference : self . _updateInferenceState ( activeColumns ) if enableLearn : self . _updateLearningState ( activeColumns ) if self . globalDecay > 0.0 and ( ( self . lrnIterationIdx % self . maxAge ) == 0 ) : for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : segsToDel = [ ] for segment in self . cells [ c ] [ i ] : age = self . lrnIterationIdx - segment . lastActiveIteration if age <= self . maxAge : continue synsToDel = [ ] for synapse in segment . syns : synapse [ 2 ] = synapse [ 2 ] - self . globalDecay if synapse [ 2 ] <= 0 : synsToDel . append ( synapse ) if len ( synsToDel ) == segment . getNumSynapses ( ) : segsToDel . append ( segment ) elif len ( synsToDel ) > 0 : for syn in synsToDel : segment . syns . remove ( syn ) for seg in segsToDel : self . _cleanUpdatesList ( c , i , seg ) self . cells [ c ] [ i ] . remove ( seg ) if self . collectStats : if enableInference : predictedState = self . infPredictedState [ 't-1' ] else : predictedState = self . lrnPredictedState [ 't-1' ] self . _updateStatsInferEnd ( self . _internalStats , activeColumns , predictedState , self . colConfidence [ 't-1' ] ) output = self . _computeOutput ( ) self . printComputeEnd ( output , learn = enableLearn ) self . resetCalled = False return output
13146	def remove_direct_link_triples ( train , valid , test ) : pairs = set ( ) merged = valid + test for t in merged : pairs . add ( ( t . head , t . tail ) ) filtered = filterfalse ( lambda t : ( t . head , t . tail ) in pairs or ( t . tail , t . head ) in pairs , train ) return list ( filtered )
10571	def template_to_filepath ( template , metadata , template_patterns = None ) : if template_patterns is None : template_patterns = TEMPLATE_PATTERNS metadata = metadata if isinstance ( metadata , dict ) else _mutagen_fields_to_single_value ( metadata ) assert isinstance ( metadata , dict ) suggested_filename = get_suggested_filename ( metadata ) . replace ( '.mp3' , '' ) if template == os . getcwd ( ) or template == '%suggested%' : filepath = suggested_filename else : t = template . replace ( '%suggested%' , suggested_filename ) filepath = _replace_template_patterns ( t , metadata , template_patterns ) return filepath
12864	def days_in_month ( year , month ) : eom = _days_per_month [ month - 1 ] if is_leap_year ( year ) and month == 2 : eom += 1 return eom
4360	def _receiver_loop ( self ) : while True : rawdata = self . get_server_msg ( ) if not rawdata : continue try : pkt = packet . decode ( rawdata , self . json_loads ) except ( ValueError , KeyError , Exception ) as e : self . error ( 'invalid_packet' , "There was a decoding error when dealing with packet " "with event: %s... (%s)" % ( rawdata [ : 20 ] , e ) ) continue if pkt [ 'type' ] == 'heartbeat' : continue if pkt [ 'type' ] == 'disconnect' and pkt [ 'endpoint' ] == '' : self . kill ( detach = True ) continue endpoint = pkt [ 'endpoint' ] if endpoint not in self . namespaces : self . error ( "no_such_namespace" , "The endpoint you tried to connect to " "doesn't exist: %s" % endpoint , endpoint = endpoint ) continue elif endpoint in self . active_ns : pkt_ns = self . active_ns [ endpoint ] else : new_ns_class = self . namespaces [ endpoint ] pkt_ns = new_ns_class ( self . environ , endpoint , request = self . request ) for cls in type ( pkt_ns ) . __mro__ : if hasattr ( cls , 'initialize' ) : cls . initialize ( pkt_ns ) self . active_ns [ endpoint ] = pkt_ns retval = pkt_ns . process_packet ( pkt ) if pkt . get ( 'ack' ) == "data" and pkt . get ( 'id' ) : if type ( retval ) is tuple : args = list ( retval ) else : args = [ retval ] returning_ack = dict ( type = 'ack' , ackId = pkt [ 'id' ] , args = args , endpoint = pkt . get ( 'endpoint' , '' ) ) self . send_packet ( returning_ack ) if not self . connected : self . kill ( detach = True ) return
4178	def window_lanczos ( N ) : r if N == 1 : return ones ( 1 ) n = linspace ( - N / 2. , N / 2. , N ) win = sinc ( 2 * n / ( N - 1. ) ) return win
848	def _convertNonNumericData ( self , spatialOutput , temporalOutput , output ) : encoders = self . encoder . getEncoderList ( ) types = self . encoder . getDecoderOutputFieldTypes ( ) for i , ( encoder , type ) in enumerate ( zip ( encoders , types ) ) : spatialData = spatialOutput [ i ] temporalData = temporalOutput [ i ] if type != FieldMetaType . integer and type != FieldMetaType . float : spatialData = encoder . getScalars ( spatialData ) [ 0 ] temporalData = encoder . getScalars ( temporalData ) [ 0 ] assert isinstance ( spatialData , ( float , int ) ) assert isinstance ( temporalData , ( float , int ) ) output [ 'spatialTopDownOut' ] [ i ] = spatialData output [ 'temporalTopDownOut' ] [ i ] = temporalData
3502	def assess_products ( model , reaction , flux_coefficient_cutoff = 0.001 , solver = None ) : warn ( 'use assess_component instead' , DeprecationWarning ) return assess_component ( model , reaction , 'products' , flux_coefficient_cutoff , solver )
1031	def b32decode ( s , casefold = False , map01 = None ) : quanta , leftover = divmod ( len ( s ) , 8 ) if leftover : raise TypeError ( 'Incorrect padding' ) if map01 : s = s . translate ( string . maketrans ( b'01' , b'O' + map01 ) ) if casefold : s = s . upper ( ) padchars = 0 mo = re . search ( '(?P<pad>[=]*)$' , s ) if mo : padchars = len ( mo . group ( 'pad' ) ) if padchars > 0 : s = s [ : - padchars ] parts = [ ] acc = 0 shift = 35 for c in s : val = _b32rev . get ( c ) if val is None : raise TypeError ( 'Non-base32 digit found' ) acc += _b32rev [ c ] << shift shift -= 5 if shift < 0 : parts . append ( binascii . unhexlify ( '%010x' % acc ) ) acc = 0 shift = 35 last = binascii . unhexlify ( '%010x' % acc ) if padchars == 0 : last = '' elif padchars == 1 : last = last [ : - 1 ] elif padchars == 3 : last = last [ : - 2 ] elif padchars == 4 : last = last [ : - 3 ] elif padchars == 6 : last = last [ : - 4 ] else : raise TypeError ( 'Incorrect padding' ) parts . append ( last ) return EMPTYSTRING . join ( parts )
10876	def calculate_linescan_ilm_psf ( y , z , polar_angle = 0. , nlpts = 1 , pinhole_width = 1 , use_laggauss = False , ** kwargs ) : if use_laggauss : x_vals , wts = calc_pts_lag ( ) else : x_vals , wts = calc_pts_hg ( ) xg , yg , zg = [ np . zeros ( list ( y . shape ) + [ x_vals . size ] ) for a in range ( 3 ) ] hilm = np . zeros ( xg . shape ) for a in range ( x_vals . size ) : xg [ ... , a ] = x_vals [ a ] yg [ ... , a ] = y . copy ( ) zg [ ... , a ] = z . copy ( ) y_pinhole , wts_pinhole = np . polynomial . hermite . hermgauss ( nlpts ) y_pinhole *= np . sqrt ( 2 ) * pinhole_width wts_pinhole /= np . sqrt ( np . pi ) for yp , wp in zip ( y_pinhole , wts_pinhole ) : rho = np . sqrt ( xg * xg + ( yg - yp ) * ( yg - yp ) ) phi = np . arctan2 ( yg , xg ) hsym , hasym = get_hsym_asym ( rho , zg , get_hdet = False , ** kwargs ) hilm += wp * ( hsym + np . cos ( 2 * ( phi - polar_angle ) ) * hasym ) for a in range ( x_vals . size ) : hilm [ ... , a ] *= wts [ a ] return hilm . sum ( axis = - 1 ) * 2.
3435	def slim_optimize ( self , error_value = float ( 'nan' ) , message = None ) : self . solver . optimize ( ) if self . solver . status == optlang . interface . OPTIMAL : return self . solver . objective . value elif error_value is not None : return error_value else : assert_optimal ( self , message )
12406	def serialize ( self , data = None ) : if data is not None and self . response is not None : self . response [ 'Content-Type' ] = self . media_types [ 0 ] self . response . write ( data ) return data
6921	def _autocorr_func3 ( mags , lag , maglen , magmed , magstd ) : result = npcorrelate ( mags , mags , mode = 'full' ) result = result / npmax ( result ) return result [ int ( result . size / 2 ) : ]
5562	def effective_bounds ( self ) : return snap_bounds ( bounds = clip_bounds ( bounds = self . init_bounds , clip = self . process_pyramid . bounds ) , pyramid = self . process_pyramid , zoom = min ( self . baselevels [ "zooms" ] ) if self . baselevels else min ( self . init_zoom_levels ) )
2946	def get_ready_user_tasks ( self ) : return [ t for t in self . get_tasks ( Task . READY ) if not self . _is_engine_task ( t . task_spec ) ]
1664	def CheckMakePairUsesDeduction ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] match = _RE_PATTERN_EXPLICIT_MAKEPAIR . search ( line ) if match : error ( filename , linenum , 'build/explicit_make_pair' , 4 , 'For C++11-compatibility, omit template arguments from make_pair' ' OR use pair directly OR if appropriate, construct a pair directly' )
2844	def enable_FTDI_driver ( ) : logger . debug ( 'Enabling FTDI driver.' ) if sys . platform == 'darwin' : logger . debug ( 'Detected Mac OSX' ) _check_running_as_root ( ) subprocess . check_call ( 'kextload -b com.apple.driver.AppleUSBFTDI' , shell = True ) subprocess . check_call ( 'kextload /System/Library/Extensions/FTDIUSBSerialDriver.kext' , shell = True ) elif sys . platform . startswith ( 'linux' ) : logger . debug ( 'Detected Linux' ) _check_running_as_root ( ) subprocess . check_call ( 'modprobe -q ftdi_sio' , shell = True ) subprocess . check_call ( 'modprobe -q usbserial' , shell = True )
4921	def list ( self , request ) : catalog_api = CourseCatalogApiClient ( request . user ) catalogs = catalog_api . get_paginated_catalogs ( request . GET ) self . ensure_data_exists ( request , catalogs ) serializer = serializers . ResponsePaginationSerializer ( catalogs ) return get_paginated_response ( serializer . data , request )
1026	def encode ( input , output , quotetabs , header = 0 ) : if b2a_qp is not None : data = input . read ( ) odata = b2a_qp ( data , quotetabs = quotetabs , header = header ) output . write ( odata ) return def write ( s , output = output , lineEnd = '\n' ) : if s and s [ - 1 : ] in ' \t' : output . write ( s [ : - 1 ] + quote ( s [ - 1 ] ) + lineEnd ) elif s == '.' : output . write ( quote ( s ) + lineEnd ) else : output . write ( s + lineEnd ) prevline = None while 1 : line = input . readline ( ) if not line : break outline = [ ] stripped = '' if line [ - 1 : ] == '\n' : line = line [ : - 1 ] stripped = '\n' for c in line : if needsquoting ( c , quotetabs , header ) : c = quote ( c ) if header and c == ' ' : outline . append ( '_' ) else : outline . append ( c ) if prevline is not None : write ( prevline ) thisline = EMPTYSTRING . join ( outline ) while len ( thisline ) > MAXLINESIZE : write ( thisline [ : MAXLINESIZE - 1 ] , lineEnd = '=\n' ) thisline = thisline [ MAXLINESIZE - 1 : ] prevline = thisline if prevline is not None : write ( prevline , lineEnd = stripped )
13472	def clean ( self ) : cleaned = super ( EventForm , self ) . clean ( ) if Event . objects . filter ( name = cleaned [ 'name' ] , start_date = cleaned [ 'start_date' ] ) . count ( ) : raise forms . ValidationError ( u'This event appears to be in the database already.' ) return cleaned
1132	def _replace ( _self , ** kwds ) : 'Return a new SplitResult object replacing specified fields with new values' result = _self . _make ( map ( kwds . pop , ( 'scheme' , 'netloc' , 'path' , 'query' , 'fragment' ) , _self ) ) if kwds : raise ValueError ( 'Got unexpected field names: %r' % kwds . keys ( ) ) return result
1548	def init_rotating_logger ( level , logfile , max_files , max_bytes ) : logging . basicConfig ( ) root_logger = logging . getLogger ( ) log_format = "[%(asctime)s] [%(levelname)s] %(filename)s: %(message)s" root_logger . setLevel ( level ) handler = RotatingFileHandler ( logfile , maxBytes = max_bytes , backupCount = max_files ) handler . setFormatter ( logging . Formatter ( fmt = log_format , datefmt = date_format ) ) root_logger . addHandler ( handler ) for handler in root_logger . handlers : root_logger . debug ( "Associated handlers - " + str ( handler ) ) if isinstance ( handler , logging . StreamHandler ) : root_logger . debug ( "Removing StreamHandler: " + str ( handler ) ) root_logger . handlers . remove ( handler )
5079	def strip_html_tags ( text , allowed_tags = None ) : if text is None : return if allowed_tags is None : allowed_tags = ALLOWED_TAGS return bleach . clean ( text , tags = allowed_tags , attributes = [ 'id' , 'class' , 'style' , 'href' , 'title' ] , strip = True )
11217	def _pop_claims_from_payload ( self ) : claims_in_payload = [ k for k in self . payload . keys ( ) if k in registered_claims . values ( ) ] for name in claims_in_payload : self . registered_claims [ name ] = self . payload . pop ( name )
3508	def nullspace ( A , atol = 1e-13 , rtol = 0 ) : A = np . atleast_2d ( A ) u , s , vh = np . linalg . svd ( A ) tol = max ( atol , rtol * s [ 0 ] ) nnz = ( s >= tol ) . sum ( ) ns = vh [ nnz : ] . conj ( ) . T return ns
8103	def load_profiles ( self ) : _profiles = { } for name , klass in inspect . getmembers ( profiles ) : if inspect . isclass ( klass ) and name . endswith ( 'Profile' ) and name != 'TuioProfile' : profile = klass ( ) _profiles [ profile . address ] = profile try : setattr ( self , profile . list_label , profile . objs ) except AttributeError : continue self . manager . add ( self . callback , profile . address ) return _profiles
13719	def create_tree ( endpoints ) : tree = { } for method , url , doc in endpoints : path = [ p for p in url . strip ( '/' ) . split ( '/' ) ] here = tree version = path [ 0 ] here . setdefault ( version , { } ) here = here [ version ] for p in path [ 1 : ] : part = _camelcase_to_underscore ( p ) here . setdefault ( part , { } ) here = here [ part ] if not 'METHODS' in here : here [ 'METHODS' ] = [ [ method , doc ] ] else : if not method in here [ 'METHODS' ] : here [ 'METHODS' ] . append ( [ method , doc ] ) return tree
4971	def clean ( self ) : super ( EnterpriseCustomerIdentityProviderAdminForm , self ) . clean ( ) provider_id = self . cleaned_data . get ( 'provider_id' , None ) enterprise_customer = self . cleaned_data . get ( 'enterprise_customer' , None ) if provider_id is None or enterprise_customer is None : return identity_provider = utils . get_identity_provider ( provider_id ) if not identity_provider : message = _ ( "The specified Identity Provider does not exist. For more " "information, contact a system administrator." , ) logger . exception ( message ) raise ValidationError ( message ) if identity_provider and identity_provider . site != enterprise_customer . site : raise ValidationError ( _ ( "The site for the selected identity provider " "({identity_provider_site}) does not match the site for " "this enterprise customer ({enterprise_customer_site}). " "To correct this problem, select a site that has a domain " "of '{identity_provider_site}', or update the identity " "provider to '{enterprise_customer_site}'." ) . format ( enterprise_customer_site = enterprise_customer . site , identity_provider_site = identity_provider . site , ) , )
7611	def get_top_clans ( self , location_id = 'global' , ** params : keys ) : url = self . api . LOCATIONS + '/' + str ( location_id ) + '/rankings/clans' return self . _get_model ( url , PartialClan , ** params )
1829	def JA ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . AND ( cpu . CF == False , cpu . ZF == False ) , target . read ( ) , cpu . PC )
928	def _getEndTime ( self , t ) : assert isinstance ( t , datetime . datetime ) if self . _aggTimeDelta : return t + self . _aggTimeDelta else : year = t . year + self . _aggYears + ( t . month - 1 + self . _aggMonths ) / 12 month = ( t . month - 1 + self . _aggMonths ) % 12 + 1 return t . replace ( year = year , month = month )
8242	def outline ( path , colors , precision = 0.4 , continuous = True ) : def _point_count ( path , precision ) : return max ( int ( path . length * precision * 0.5 ) , 10 ) n = sum ( [ _point_count ( contour , precision ) for contour in path . contours ] ) contour_i = 0 contour_n = len ( path . contours ) - 1 if contour_n == 0 : continuous = False i = 0 for contour in path . contours : if not continuous : i = 0 j = _point_count ( contour , precision ) first = True for pt in contour . points ( j ) : if first : first = False else : if not continuous : clr = float ( i ) / j * len ( colors ) else : clr = float ( i ) / n * len ( colors ) - 1 * contour_i / contour_n _ctx . stroke ( colors [ int ( clr ) ] ) _ctx . line ( x0 , y0 , pt . x , pt . y ) x0 = pt . x y0 = pt . y i += 1 pt = contour . point ( 0.9999999 ) _ctx . line ( x0 , y0 , pt . x , pt . y ) contour_i += 1
6523	def issue_count ( self , include_unclean = False ) : if include_unclean : return len ( self . _all_issues ) self . _ensure_cleaned_issues ( ) return len ( self . _cleaned_issues )
13746	def create_item ( self , hash_key , start = 0 , extra_attrs = None ) : table = self . get_table ( ) now = datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) attrs = { 'created_on' : now , 'modified_on' : now , 'count' : start , } if extra_attrs : attrs . update ( extra_attrs ) item = table . new_item ( hash_key = hash_key , attrs = attrs , ) return item
11938	def add_message_for ( users , level , message_text , extra_tags = '' , date = None , url = None , fail_silently = False ) : BackendClass = stored_messages_settings . STORAGE_BACKEND backend = BackendClass ( ) m = backend . create_message ( level , message_text , extra_tags , date , url ) backend . archive_store ( users , m ) backend . inbox_store ( users , m )
3997	def copy_between_containers ( source_name , source_path , dest_name , dest_path ) : if not container_path_exists ( source_name , source_path ) : raise RuntimeError ( 'ERROR: Path {} does not exist inside container {}.' . format ( source_path , source_name ) ) temp_path = os . path . join ( tempfile . mkdtemp ( ) , str ( uuid . uuid1 ( ) ) ) with _cleanup_path ( temp_path ) : copy_to_local ( temp_path , source_name , source_path , demote = False ) copy_from_local ( temp_path , dest_name , dest_path , demote = False )
7907	def __error_message ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : return False rs . process_error_message ( stanza ) return True
1118	def _MakeParallelBenchmark ( p , work_func , * args ) : def Benchmark ( b ) : e = threading . Event ( ) def Target ( ) : e . wait ( ) for _ in xrange ( b . N / p ) : work_func ( * args ) threads = [ ] for _ in xrange ( p ) : t = threading . Thread ( target = Target ) t . start ( ) threads . append ( t ) b . ResetTimer ( ) e . set ( ) for t in threads : t . join ( ) return Benchmark
9985	def _reload ( self , module = None ) : if self . module is None : raise RuntimeError elif module is None : import importlib module = ModuleSource ( importlib . reload ( module ) ) elif module . name != self . module : raise RuntimeError if self . name in module . funcs : func = module . funcs [ self . name ] self . __init__ ( func = func ) else : self . __init__ ( func = NULL_FORMULA ) return self
8069	def randomChildElement ( self , node ) : choices = [ e for e in node . childNodes if e . nodeType == e . ELEMENT_NODE ] chosen = random . choice ( choices ) if _debug : sys . stderr . write ( '%s available choices: %s\n' % ( len ( choices ) , [ e . toxml ( ) for e in choices ] ) ) sys . stderr . write ( 'Chosen: %s\n' % chosen . toxml ( ) ) return chosen
1325	def _saliency_map ( self , a , image , target , labels , mask , fast = False ) : alphas = a . gradient ( image , target ) * mask if fast : betas = - np . ones_like ( alphas ) else : betas = np . sum ( [ a . gradient ( image , label ) * mask - alphas for label in labels ] , 0 ) salmap = np . abs ( alphas ) * np . abs ( betas ) * np . sign ( alphas * betas ) idx = np . argmin ( salmap ) idx = np . unravel_index ( idx , mask . shape ) pix_sign = np . sign ( alphas ) [ idx ] return idx , pix_sign
5683	def tripI_takes_place_on_dsut ( self , trip_I , day_start_ut ) : query = "SELECT * FROM days WHERE trip_I=? AND day_start_ut=?" params = ( trip_I , day_start_ut ) cur = self . conn . cursor ( ) rows = list ( cur . execute ( query , params ) ) if len ( rows ) == 0 : return False else : assert len ( rows ) == 1 , 'On a day, a trip_I should be present at most once' return True
12304	def post ( self , repo ) : datapackage = repo . package url = self . url token = self . token headers = { 'Authorization' : 'Token {}' . format ( token ) , 'Content-Type' : 'application/json' } try : r = requests . post ( url , data = json . dumps ( datapackage ) , headers = headers ) return r except Exception as e : raise NetworkError ( ) return ""
6318	def _find_last_of ( self , path , finders ) : found_path = None for finder in finders : result = finder . find ( path ) if result : found_path = result return found_path
5086	def has_implicit_access_to_catalog ( user , obj ) : request = get_request_or_stub ( ) decoded_jwt = get_decoded_jwt_from_request ( request ) return request_user_has_implicit_access_via_jwt ( decoded_jwt , ENTERPRISE_CATALOG_ADMIN_ROLE , obj )
11639	def json_write_data ( json_data , filename ) : with open ( filename , 'w' ) as fp : json . dump ( json_data , fp , indent = 4 , sort_keys = True , ensure_ascii = False ) return True return False
3934	def _get_session_cookies ( session , access_token ) : headers = { 'Authorization' : 'Bearer {}' . format ( access_token ) } try : r = session . get ( ( 'https://accounts.google.com/accounts/OAuthLogin' '?source=hangups&issueuberauth=1' ) , headers = headers ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'OAuthLogin request failed: {}' . format ( e ) ) uberauth = r . text try : r = session . get ( ( 'https://accounts.google.com/MergeSession?' 'service=mail&' 'continue=http://www.google.com&uberauth={}' ) . format ( uberauth ) , headers = headers ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'MergeSession request failed: {}' . format ( e ) ) cookies = session . cookies . get_dict ( domain = '.google.com' ) if cookies == { } : raise GoogleAuthError ( 'Failed to find session cookies' ) return cookies
7091	def _cpinfo_key_worker ( task ) : cpfile , keyspeclist = task keystoget = [ x [ 0 ] for x in keyspeclist ] nonesubs = [ x [ - 2 ] for x in keyspeclist ] nansubs = [ x [ - 1 ] for x in keyspeclist ] for i , k in enumerate ( keystoget ) : thisk = k . split ( '.' ) if sys . version_info [ : 2 ] < ( 3 , 4 ) : thisk = [ ( int ( x ) if x . isdigit ( ) else x ) for x in thisk ] else : thisk = [ ( int ( x ) if x . isdecimal ( ) else x ) for x in thisk ] keystoget [ i ] = thisk keystoget . insert ( 0 , [ 'objectid' ] ) nonesubs . insert ( 0 , '' ) nansubs . insert ( 0 , '' ) vals = checkplot_infokey_worker ( ( cpfile , keystoget ) ) for val , nonesub , nansub , valind in zip ( vals , nonesubs , nansubs , range ( len ( vals ) ) ) : if val is None : outval = nonesub elif isinstance ( val , float ) and not np . isfinite ( val ) : outval = nansub elif isinstance ( val , ( list , tuple ) ) : outval = ', ' . join ( val ) else : outval = val vals [ valind ] = outval return vals
6617	def expand_path_cfg ( path_cfg , alias_dict = { } , overriding_kargs = { } ) : if isinstance ( path_cfg , str ) : return _expand_str ( path_cfg , alias_dict , overriding_kargs ) if isinstance ( path_cfg , dict ) : return _expand_dict ( path_cfg , alias_dict ) return _expand_tuple ( path_cfg , alias_dict , overriding_kargs )
8401	def rescale_mid ( x , to = ( 0 , 1 ) , _from = None , mid = 0 ) : array_like = True try : len ( x ) except TypeError : array_like = False x = [ x ] if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) if _from is None : _from = np . array ( [ np . min ( x ) , np . max ( x ) ] ) else : _from = np . asarray ( _from ) if ( zero_range ( _from ) or zero_range ( to ) ) : out = np . repeat ( np . mean ( to ) , len ( x ) ) else : extent = 2 * np . max ( np . abs ( _from - mid ) ) out = ( x - mid ) / extent * np . diff ( to ) + np . mean ( to ) if not array_like : out = out [ 0 ] return out
9112	def replies ( self ) : fs_reply_path = join ( self . fs_replies_path , 'message_001.txt' ) if exists ( fs_reply_path ) : return [ load ( open ( fs_reply_path , 'r' ) ) ] else : return [ ]
9393	def check_important_sub_metrics ( self , sub_metric ) : if not self . important_sub_metrics : return False if sub_metric in self . important_sub_metrics : return True items = sub_metric . split ( '.' ) if items [ - 1 ] in self . important_sub_metrics : return True return False
10188	def publish ( self , event_type , events ) : assert event_type in self . events current_queues . queues [ 'stats-{}' . format ( event_type ) ] . publish ( events )
7467	def summarize_results ( self , individual_results = False ) : if ( not self . params . infer_delimit ) & ( not self . params . infer_sptree ) : if individual_results : return [ _parse_00 ( i ) for i in self . files . outfiles ] else : return pd . concat ( [ pd . read_csv ( i , sep = '\t' , index_col = 0 ) for i in self . files . mcmcfiles ] ) . describe ( ) . T if self . params . infer_delimit & ( not self . params . infer_sptree ) : return _parse_01 ( self . files . outfiles , individual = individual_results ) else : return "summary function not yet ready for this type of result"
8725	def at_time ( cls , at , target ) : at = cls . _from_timestamp ( at ) cmd = cls . from_datetime ( at ) cmd . delay = at - now ( ) cmd . target = target return cmd
3118	def _create_file_if_needed ( self ) : if not os . path . exists ( self . _filename ) : old_umask = os . umask ( 0o177 ) try : open ( self . _filename , 'a+b' ) . close ( ) finally : os . umask ( old_umask )
12174	def genIndex ( folder , forceIDs = [ ] ) : if not os . path . exists ( folder + "/swhlab4/" ) : print ( " !! cannot index if no /swhlab4/" ) return timestart = cm . timethis ( ) files = glob . glob ( folder + "/*.*" ) files . extend ( glob . glob ( folder + "/swhlab4/*.*" ) ) print ( " -- indexing glob took %.02f ms" % ( cm . timethis ( timestart ) * 1000 ) ) files . extend ( genPNGs ( folder , files ) ) files = sorted ( files ) timestart = cm . timethis ( ) d = cm . getIDfileDict ( files ) print ( " -- filedict length:" , len ( d ) ) print ( " -- generating ID dict took %.02f ms" % ( cm . timethis ( timestart ) * 1000 ) ) groups = cm . getABFgroups ( files ) print ( " -- groups length:" , len ( groups ) ) for ID in sorted ( list ( groups . keys ( ) ) ) : overwrite = False for abfID in groups [ ID ] : if abfID in forceIDs : overwrite = True try : htmlABF ( ID , groups [ ID ] , d , folder , overwrite ) except : print ( "~~ HTML GENERATION FAILED!!!" ) menu = expMenu ( groups , folder ) makeSplash ( menu , folder ) makeMenu ( menu , folder ) htmlFrames ( d , folder ) makeMenu ( menu , folder ) makeSplash ( menu , folder )
13345	def _valid_distaxis ( shapes , ax ) : compare_shapes = np . vstack ( shapes ) if ax < compare_shapes . shape [ 1 ] : compare_shapes [ : , ax ] = - 1 return np . count_nonzero ( compare_shapes - compare_shapes [ 0 ] ) == 0
8798	def get_security_group_states ( self , interfaces ) : LOG . debug ( "Getting security groups from Redis for {0}" . format ( interfaces ) ) interfaces = tuple ( interfaces ) vif_keys = [ self . vif_key ( vif . device_id , vif . mac_address ) for vif in interfaces ] sec_grp_all = self . get_fields_all ( vif_keys ) ret = { } for vif , group in zip ( interfaces , sec_grp_all ) : if group : ret [ vif ] = { SECURITY_GROUP_ACK : None , SECURITY_GROUP_HASH_ATTR : [ ] } temp_ack = group [ SECURITY_GROUP_ACK ] . lower ( ) temp_rules = group [ SECURITY_GROUP_HASH_ATTR ] if temp_rules : temp_rules = json . loads ( temp_rules ) ret [ vif ] [ SECURITY_GROUP_HASH_ATTR ] = temp_rules [ "rules" ] if "true" in temp_ack : ret [ vif ] [ SECURITY_GROUP_ACK ] = True elif "false" in temp_ack : ret [ vif ] [ SECURITY_GROUP_ACK ] = False else : ret . pop ( vif , None ) LOG . debug ( "Skipping bad ack value %s" % temp_ack ) return ret
10344	def load_differential_gene_expression ( path : str , gene_symbol_column : str = 'Gene.symbol' , logfc_column : str = 'logFC' , aggregator : Optional [ Callable [ [ List [ float ] ] , float ] ] = None , ) -> Mapping [ str , float ] : if aggregator is None : aggregator = np . median df = pd . read_csv ( path ) assert gene_symbol_column in df . columns assert logfc_column in df . columns df = df . loc [ df [ gene_symbol_column ] . notnull ( ) , [ gene_symbol_column , logfc_column ] ] values = defaultdict ( list ) for _ , gene_symbol , log_fold_change in df . itertuples ( ) : values [ gene_symbol ] . append ( log_fold_change ) return { gene_symbol : aggregator ( log_fold_changes ) for gene_symbol , log_fold_changes in values . items ( ) }
2322	def read_causal_pairs ( filename , scale = True , ** kwargs ) : def convert_row ( row , scale ) : a = row [ "A" ] . split ( " " ) b = row [ "B" ] . split ( " " ) if a [ 0 ] == "" : a . pop ( 0 ) b . pop ( 0 ) if a [ - 1 ] == "" : a . pop ( - 1 ) b . pop ( - 1 ) a = array ( [ float ( i ) for i in a ] ) b = array ( [ float ( i ) for i in b ] ) if scale : a = scaler ( a ) b = scaler ( b ) return row [ 'SampleID' ] , a , b if isinstance ( filename , str ) : data = read_csv ( filename , ** kwargs ) elif isinstance ( filename , DataFrame ) : data = filename else : raise TypeError ( "Type not supported." ) conv_data = [ ] for idx , row in data . iterrows ( ) : conv_data . append ( convert_row ( row , scale ) ) df = DataFrame ( conv_data , columns = [ 'SampleID' , 'A' , 'B' ] ) df = df . set_index ( "SampleID" ) return df
266	def format_asset ( asset ) : try : import zipline . assets except ImportError : return asset if isinstance ( asset , zipline . assets . Asset ) : return asset . symbol else : return asset
207	def draw ( self , size = None , cmap = "jet" ) : heatmaps_uint8 = self . to_uint8 ( ) heatmaps_drawn = [ ] for c in sm . xrange ( heatmaps_uint8 . shape [ 2 ] ) : heatmap_c = heatmaps_uint8 [ ... , c : c + 1 ] if size is not None : heatmap_c_rs = ia . imresize_single_image ( heatmap_c , size , interpolation = "nearest" ) else : heatmap_c_rs = heatmap_c heatmap_c_rs = np . squeeze ( heatmap_c_rs ) . astype ( np . float32 ) / 255.0 if cmap is not None : import matplotlib . pyplot as plt cmap_func = plt . get_cmap ( cmap ) heatmap_cmapped = cmap_func ( heatmap_c_rs ) heatmap_cmapped = np . delete ( heatmap_cmapped , 3 , 2 ) else : heatmap_cmapped = np . tile ( heatmap_c_rs [ ... , np . newaxis ] , ( 1 , 1 , 3 ) ) heatmap_cmapped = np . clip ( heatmap_cmapped * 255 , 0 , 255 ) . astype ( np . uint8 ) heatmaps_drawn . append ( heatmap_cmapped ) return heatmaps_drawn
10678	def H ( self , T ) : result = 0.0 if T < self . Tmax : lT = T else : lT = self . Tmax Tref = self . Tmin for c , e in zip ( self . _coefficients , self . _exponents ) : if e == - 1.0 : result += c * math . log ( lT / Tref ) else : result += c * ( lT ** ( e + 1.0 ) - Tref ** ( e + 1.0 ) ) / ( e + 1.0 ) return result
4117	def poly2lsf ( a ) : a = numpy . array ( a ) if a [ 0 ] != 1 : a /= a [ 0 ] if max ( numpy . abs ( numpy . roots ( a ) ) ) >= 1.0 : error ( 'The polynomial must have all roots inside of the unit circle.' ) p = len ( a ) - 1 a1 = numpy . concatenate ( ( a , numpy . array ( [ 0 ] ) ) ) a2 = a1 [ - 1 : : - 1 ] P1 = a1 - a2 Q1 = a1 + a2 if p % 2 : P , r = deconvolve ( P1 , [ 1 , 0 , - 1 ] ) Q = Q1 else : P , r = deconvolve ( P1 , [ 1 , - 1 ] ) Q , r = deconvolve ( Q1 , [ 1 , 1 ] ) rP = numpy . roots ( P ) rQ = numpy . roots ( Q ) aP = numpy . angle ( rP [ 1 : : 2 ] ) aQ = numpy . angle ( rQ [ 1 : : 2 ] ) lsf = sorted ( numpy . concatenate ( ( - aP , - aQ ) ) ) return lsf
4082	def set_directory ( path = None ) : old_path = get_directory ( ) terminate_server ( ) cache . clear ( ) if path : cache [ 'language_check_dir' ] = path try : get_jar_info ( ) except Error : cache [ 'language_check_dir' ] = old_path raise
9694	def replace ( self , ** k ) : if self . date != 'infinity' : return Date ( self . date . replace ( ** k ) ) else : return Date ( 'infinity' )
13233	def load ( directory_name , module_name ) : directory_name = os . path . expanduser ( directory_name ) if os . path . isdir ( directory_name ) and directory_name not in sys . path : sys . path . append ( directory_name ) try : return importlib . import_module ( module_name ) except ImportError : pass
7412	def run_tree_inference ( self , nexus , idx ) : tmpdir = tempfile . tempdir tmpfile = os . path . join ( tempfile . NamedTemporaryFile ( delete = False , prefix = str ( idx ) , dir = tmpdir , ) ) tmpfile . write ( nexus ) tmpfile . flush ( ) rax = raxml ( name = str ( idx ) , data = tmpfile . name , workdir = tmpdir , N = 1 , T = 2 ) rax . run ( force = True , block = True , quiet = True ) tmpfile . close ( ) order = get_order ( toytree . tree ( rax . trees . bestTree ) ) return "" . join ( order )
13201	def format_short_title ( self , format = 'html5' , deparagraph = True , mathjax = False , smart = True , extra_args = None ) : if self . short_title is None : return None output_text = convert_lsstdoc_tex ( self . short_title , 'html5' , deparagraph = deparagraph , mathjax = mathjax , smart = smart , extra_args = extra_args ) return output_text
5731	def advance_past_string_with_gdb_escapes ( self , chars_to_remove_gdb_escape = None ) : if chars_to_remove_gdb_escape is None : chars_to_remove_gdb_escape = [ '"' ] buf = "" while True : c = self . raw_text [ self . index ] self . index += 1 logging . debug ( "%s" , fmt_cyan ( c ) ) if c == "\\" : c2 = self . raw_text [ self . index ] self . index += 1 buf += c2 elif c == '"' : break else : buf += c return buf
8300	def handle ( self , data , source = None ) : decoded = decodeOSC ( data ) self . dispatch ( decoded , source )
6670	def task ( * args , ** kwargs ) : precursors = kwargs . pop ( 'precursors' , None ) post_callback = kwargs . pop ( 'post_callback' , False ) if args and callable ( args [ 0 ] ) : return _task ( * args ) def wrapper ( meth ) : if precursors : meth . deploy_before = list ( precursors ) if post_callback : meth . is_post_callback = True return _task ( meth ) return wrapper
4924	def get_required_query_params ( self , request ) : email = get_request_value ( request , self . REQUIRED_PARAM_EMAIL , '' ) enterprise_name = get_request_value ( request , self . REQUIRED_PARAM_ENTERPRISE_NAME , '' ) number_of_codes = get_request_value ( request , self . OPTIONAL_PARAM_NUMBER_OF_CODES , '' ) if not ( email and enterprise_name ) : raise CodesAPIRequestError ( self . get_missing_params_message ( [ ( self . REQUIRED_PARAM_EMAIL , bool ( email ) ) , ( self . REQUIRED_PARAM_ENTERPRISE_NAME , bool ( enterprise_name ) ) , ] ) ) return email , enterprise_name , number_of_codes
5464	def get_action_by_id ( op , action_id ) : actions = get_actions ( op ) if actions and 1 <= action_id < len ( actions ) : return actions [ action_id - 1 ]
6417	def stem ( self , word ) : word = normalize ( 'NFKD' , text_type ( word . lower ( ) ) ) word = '' . join ( c for c in word if c in { 'a' , 'b' , 'c' , 'd' , 'e' , 'f' , 'g' , 'h' , 'i' , 'j' , 'k' , 'l' , 'm' , 'n' , 'o' , 'p' , 'q' , 'r' , 's' , 't' , 'u' , 'v' , 'w' , 'x' , 'y' , 'z' , } ) word = word . replace ( 'j' , 'i' ) . replace ( 'v' , 'u' ) if word [ - 3 : ] == 'que' : if word [ : - 3 ] in self . _keep_que or word == 'que' : return { 'n' : word , 'v' : word } else : word = word [ : - 3 ] noun = word verb = word for endlen in range ( 4 , 0 , - 1 ) : if word [ - endlen : ] in self . _n_endings [ endlen ] : if len ( word ) - 2 >= endlen : noun = word [ : - endlen ] else : noun = word break for endlen in range ( 6 , 0 , - 1 ) : if word [ - endlen : ] in self . _v_endings_strip [ endlen ] : if len ( word ) - 2 >= endlen : verb = word [ : - endlen ] else : verb = word break if word [ - endlen : ] in self . _v_endings_alter [ endlen ] : if word [ - endlen : ] in { 'iuntur' , 'erunt' , 'untur' , 'iunt' , 'unt' , } : new_word = word [ : - endlen ] + 'i' addlen = 1 elif word [ - endlen : ] in { 'beris' , 'bor' , 'bo' } : new_word = word [ : - endlen ] + 'bi' addlen = 2 else : new_word = word [ : - endlen ] + 'eri' addlen = 3 if len ( new_word ) >= 2 + addlen : verb = new_word else : verb = word break return { 'n' : noun , 'v' : verb }
10534	def delete_project ( project_id ) : try : res = _pybossa_req ( 'delete' , 'project' , project_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : raise
1041	def source_lines ( self ) : return [ self . source_buffer . source_line ( line ) for line in range ( self . line ( ) , self . end ( ) . line ( ) + 1 ) ]
2986	def get_cors_options ( appInstance , * dicts ) : options = DEFAULT_OPTIONS . copy ( ) options . update ( get_app_kwarg_dict ( appInstance ) ) if dicts : for d in dicts : options . update ( d ) return serialize_options ( options )
229	def compute_style_factor_exposures ( positions , risk_factor ) : positions_wo_cash = positions . drop ( 'cash' , axis = 'columns' ) gross_exposure = positions_wo_cash . abs ( ) . sum ( axis = 'columns' ) style_factor_exposure = positions_wo_cash . multiply ( risk_factor ) . divide ( gross_exposure , axis = 'index' ) tot_style_factor_exposure = style_factor_exposure . sum ( axis = 'columns' , skipna = True ) return tot_style_factor_exposure
12137	def _load_expansion ( self , key , root , pattern ) : path_pattern = os . path . join ( root , pattern ) expanded_paths = self . _expand_pattern ( path_pattern ) specs = [ ] for ( path , tags ) in expanded_paths : filelist = [ os . path . join ( path , f ) for f in os . listdir ( path ) ] if os . path . isdir ( path ) else [ path ] for filepath in filelist : specs . append ( dict ( tags , ** { key : os . path . abspath ( filepath ) } ) ) return sorted ( specs , key = lambda s : s [ key ] )
7189	def fix_line_numbers ( body ) : r maxline = 0 for node in body . pre_order ( ) : maxline += node . prefix . count ( '\n' ) if isinstance ( node , Leaf ) : node . lineno = maxline maxline += str ( node . value ) . count ( '\n' )
896	def read ( cls , proto ) : tm = object . __new__ ( cls ) tm . columnDimensions = tuple ( proto . columnDimensions ) tm . cellsPerColumn = int ( proto . cellsPerColumn ) tm . activationThreshold = int ( proto . activationThreshold ) tm . initialPermanence = round ( proto . initialPermanence , EPSILON_ROUND ) tm . connectedPermanence = round ( proto . connectedPermanence , EPSILON_ROUND ) tm . minThreshold = int ( proto . minThreshold ) tm . maxNewSynapseCount = int ( proto . maxNewSynapseCount ) tm . permanenceIncrement = round ( proto . permanenceIncrement , EPSILON_ROUND ) tm . permanenceDecrement = round ( proto . permanenceDecrement , EPSILON_ROUND ) tm . predictedSegmentDecrement = round ( proto . predictedSegmentDecrement , EPSILON_ROUND ) tm . maxSegmentsPerCell = int ( proto . maxSegmentsPerCell ) tm . maxSynapsesPerSegment = int ( proto . maxSynapsesPerSegment ) tm . connections = Connections . read ( proto . connections ) tm . _random = Random ( ) tm . _random . read ( proto . random ) tm . activeCells = [ int ( x ) for x in proto . activeCells ] tm . winnerCells = [ int ( x ) for x in proto . winnerCells ] flatListLength = tm . connections . segmentFlatListLength ( ) tm . numActiveConnectedSynapsesForSegment = [ 0 ] * flatListLength tm . numActivePotentialSynapsesForSegment = [ 0 ] * flatListLength tm . lastUsedIterationForSegment = [ 0 ] * flatListLength tm . activeSegments = [ ] tm . matchingSegments = [ ] for protoSegment in proto . activeSegments : tm . activeSegments . append ( tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) ) for protoSegment in proto . matchingSegments : tm . matchingSegments . append ( tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) ) for protoSegment in proto . numActivePotentialSynapsesForSegment : segment = tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) tm . numActivePotentialSynapsesForSegment [ segment . flatIdx ] = ( int ( protoSegment . number ) ) tm . iteration = long ( proto . iteration ) for protoSegment in proto . lastUsedIterationForSegment : segment = tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) tm . lastUsedIterationForSegment [ segment . flatIdx ] = ( long ( protoSegment . number ) ) return tm
6704	def togroups ( self , user , groups ) : r = self . local_renderer if isinstance ( groups , six . string_types ) : groups = [ _ . strip ( ) for _ in groups . split ( ',' ) if _ . strip ( ) ] for group in groups : r . env . username = user r . env . group = group r . sudo ( 'groupadd --force {group}' ) r . sudo ( 'adduser {username} {group}' )
8051	def _darkest ( self ) : rgb , n = ( 1.0 , 1.0 , 1.0 ) , 3.0 for r , g , b in self : if r + g + b < n : rgb , n = ( r , g , b ) , r + g + b return rgb
5863	def add_organization_course ( organization_data , course_key ) : _validate_course_key ( course_key ) _validate_organization_data ( organization_data ) data . create_organization_course ( organization = organization_data , course_key = course_key )
2596	def _import_mapping ( mapping , original = None ) : for key , value in list ( mapping . items ( ) ) : if isinstance ( key , string_types ) : try : cls = import_item ( key ) except Exception : if original and key not in original : print ( "ERROR: canning class not importable: %r" , key , exc_info = True ) mapping . pop ( key ) else : mapping [ cls ] = mapping . pop ( key )
6757	def reboot_or_dryrun ( self , * args , ** kwargs ) : warnings . warn ( 'Use self.run() instead.' , DeprecationWarning , stacklevel = 2 ) self . reboot ( * args , ** kwargs )
12213	def update_field_from_proxy ( field_obj , pref_proxy ) : attr_names = ( 'verbose_name' , 'help_text' , 'default' ) for attr_name in attr_names : setattr ( field_obj , attr_name , getattr ( pref_proxy , attr_name ) )
4626	def decrypt ( self , wif ) : if not self . unlocked ( ) : raise WalletLocked return format ( bip38 . decrypt ( wif , self . masterkey ) , "wif" )
13538	def get_location ( self , location_id ) : url = "/2/locations/%s" % location_id return self . location_from_json ( self . _get_resource ( url ) [ "location" ] )
12895	def get_modes ( self ) : if not self . __modes : self . __modes = yield from self . handle_list ( self . API . get ( 'valid_modes' ) ) return self . __modes
12162	async def _try_catch_coro ( emitter , event , listener , coro ) : try : await coro except Exception as exc : if event == emitter . LISTENER_ERROR_EVENT : raise emitter . emit ( emitter . LISTENER_ERROR_EVENT , event , listener , exc )
9744	def on_packet ( packet ) : print ( "Framenumber: {}" . format ( packet . framenumber ) ) header , markers = packet . get_3d_markers ( ) print ( "Component info: {}" . format ( header ) ) for marker in markers : print ( "\t" , marker )
2278	def parse ( config ) : if not isinstance ( config , basestring ) : raise TypeError ( "Contains input must be a simple string" ) validator = ContainsValidator ( ) validator . contains_string = config return validator
2042	def human_transactions ( self ) : txs = [ ] for tx in self . transactions : if tx . depth == 0 : txs . append ( tx ) return tuple ( txs )
11411	def record_delete_subfield ( rec , tag , subfield_code , ind1 = ' ' , ind2 = ' ' ) : ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) for field in rec . get ( tag , [ ] ) : if field [ 1 ] == ind1 and field [ 2 ] == ind2 : field [ 0 ] [ : ] = [ subfield for subfield in field [ 0 ] if subfield_code != subfield [ 0 ] ]
4604	def history ( self , first = 0 , last = 0 , limit = - 1 , only_ops = [ ] , exclude_ops = [ ] ) : _limit = 100 cnt = 0 if first < 0 : first = 0 while True : txs = self . blockchain . rpc . get_account_history ( self [ "id" ] , "1.11.{}" . format ( last ) , _limit , "1.11.{}" . format ( first - 1 ) , api = "history" , ) for i in txs : if ( exclude_ops and self . operations . getOperationNameForId ( i [ "op" ] [ 0 ] ) in exclude_ops ) : continue if ( not only_ops or self . operations . getOperationNameForId ( i [ "op" ] [ 0 ] ) in only_ops ) : cnt += 1 yield i if limit >= 0 and cnt >= limit : return if not txs : log . info ( "No more history returned from API node" ) break if len ( txs ) < _limit : log . info ( "Less than {} have been returned." . format ( _limit ) ) break first = int ( txs [ - 1 ] [ "id" ] . split ( "." ) [ 2 ] )
11903	def static ( ** kwargs ) : def wrap ( fn ) : fn . func_globals [ 'static' ] = fn fn . __dict__ . update ( kwargs ) return fn return wrap
9775	def logs ( ctx , past , follow , hide_time ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) if past : try : response = PolyaxonClient ( ) . job . logs ( user , project_name , _job , stream = False ) get_logs_handler ( handle_job_info = False , show_timestamp = not hide_time , stream = False ) ( response . content . decode ( ) . split ( '\n' ) ) print ( ) if not follow : return except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : if not follow : Printer . print_error ( 'Could not get logs for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) try : PolyaxonClient ( ) . job . logs ( user , project_name , _job , message_handler = get_logs_handler ( handle_job_info = False , show_timestamp = not hide_time ) ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get logs for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
6326	def corpus_importer ( self , corpus , n_val = 1 , bos = '_START_' , eos = '_END_' ) : r if not corpus or not isinstance ( corpus , Corpus ) : raise TypeError ( 'Corpus argument of the Corpus class required.' ) sentences = corpus . sents ( ) for sent in sentences : ngs = Counter ( sent ) for key in ngs . keys ( ) : self . _add_to_ngcorpus ( self . ngcorpus , [ key ] , ngs [ key ] ) if n_val > 1 : if bos and bos != '' : sent = [ bos ] + sent if eos and eos != '' : sent += [ eos ] for i in range ( 2 , n_val + 1 ) : for j in range ( len ( sent ) - i + 1 ) : self . _add_to_ngcorpus ( self . ngcorpus , sent [ j : j + i ] , 1 )
2277	def parse_generator_doubling ( config ) : start = 1 if 'start' in config : start = int ( config [ 'start' ] ) def generator ( ) : val = start while ( True ) : yield val val = val * 2 return generator ( )
5100	def _dict2dict ( adj_dict ) : item = adj_dict . popitem ( ) adj_dict [ item [ 0 ] ] = item [ 1 ] if not isinstance ( item [ 1 ] , dict ) : new_dict = { } for key , value in adj_dict . items ( ) : new_dict [ key ] = { v : { } for v in value } adj_dict = new_dict return adj_dict
10913	def find_particles_in_tile ( positions , tile ) : bools = tile . contains ( positions ) return np . arange ( bools . size ) [ bools ]
729	def prettyPrintPattern ( self , bits , verbosity = 1 ) : numberMap = self . numberMapForBits ( bits ) text = "" numberList = [ ] numberItems = sorted ( numberMap . iteritems ( ) , key = lambda ( number , bits ) : len ( bits ) , reverse = True ) for number , bits in numberItems : if verbosity > 2 : strBits = [ str ( n ) for n in bits ] numberText = "{0} (bits: {1})" . format ( number , "," . join ( strBits ) ) elif verbosity > 1 : numberText = "{0} ({1} bits)" . format ( number , len ( bits ) ) else : numberText = str ( number ) numberList . append ( numberText ) text += "[{0}]" . format ( ", " . join ( numberList ) ) return text
10613	def H ( self , H ) : self . _H = H self . _T = self . _calculate_T ( H )
12821	def _str_to_path ( s , result_type ) : assert isinstance ( s , str ) if isinstance ( s , bytes ) and result_type is text_type : return s . decode ( 'ascii' ) elif isinstance ( s , text_type ) and result_type is bytes : return s . encode ( 'ascii' ) return s
12052	def getNotesForABF ( abfFile ) : parent = getParent ( abfFile ) parent = os . path . basename ( parent ) . replace ( ".abf" , "" ) expFile = os . path . dirname ( abfFile ) + "/experiment.txt" if not os . path . exists ( expFile ) : return "no experiment file" with open ( expFile ) as f : raw = f . readlines ( ) for line in raw : if line [ 0 ] == '~' : line = line [ 1 : ] . strip ( ) if line . startswith ( parent ) : while "\t\t" in line : line = line . replace ( "\t\t" , "\t" ) line = line . replace ( "\t" , "\n" ) return line return "experiment.txt found, but didn't contain %s" % parent
1470	def getStmgrsRegSummary ( self , tmaster , callback = None ) : if not tmaster or not tmaster . host or not tmaster . stats_port : return reg_request = tmaster_pb2 . StmgrsRegistrationSummaryRequest ( ) request_str = reg_request . SerializeToString ( ) port = str ( tmaster . stats_port ) host = tmaster . host url = "http://{0}:{1}/stmgrsregistrationsummary" . format ( host , port ) request = tornado . httpclient . HTTPRequest ( url , body = request_str , method = 'POST' , request_timeout = 5 ) Log . debug ( 'Making HTTP call to fetch stmgrsregistrationsummary url: %s' , url ) try : client = tornado . httpclient . AsyncHTTPClient ( ) result = yield client . fetch ( request ) Log . debug ( "HTTP call complete." ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) ) responseCode = result . code if responseCode >= 400 : message = "Error in getting exceptions from Tmaster, code: " + responseCode Log . error ( message ) raise tornado . gen . Return ( { "message" : message } ) reg_response = tmaster_pb2 . StmgrsRegistrationSummaryResponse ( ) reg_response . ParseFromString ( result . body ) ret = { } for stmgr in reg_response . registered_stmgrs : ret [ stmgr ] = True for stmgr in reg_response . absent_stmgrs : ret [ stmgr ] = False raise tornado . gen . Return ( ret )
1936	def constructor_abi ( self ) -> Dict [ str , Any ] : item = self . _constructor_abi_item if item : return dict ( item ) return { 'inputs' : [ ] , 'payable' : False , 'stateMutability' : 'nonpayable' , 'type' : 'constructor' }
2515	def p_file_contributor ( self , f_term , predicate ) : for _ , _ , contributor in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . add_file_contribution ( self . doc , six . text_type ( contributor ) )
4900	def handle ( self , * args , ** options ) : if not CourseEnrollment : raise NotConnectedToOpenEdX ( "This package must be installed in an OpenEdX environment." ) days , enterprise_customer = self . parse_arguments ( * args , ** options ) if enterprise_customer : try : lrs_configuration = XAPILRSConfiguration . objects . get ( active = True , enterprise_customer = enterprise_customer ) except XAPILRSConfiguration . DoesNotExist : raise CommandError ( 'No xAPI Configuration found for "{enterprise_customer}"' . format ( enterprise_customer = enterprise_customer . name ) ) self . send_xapi_statements ( lrs_configuration , days ) else : for lrs_configuration in XAPILRSConfiguration . objects . filter ( active = True ) : self . send_xapi_statements ( lrs_configuration , days )
10	def save_policy ( self , path ) : with open ( path , 'wb' ) as f : pickle . dump ( self . policy , f )
5149	def _add_file ( self , tar , name , contents , mode = DEFAULT_FILE_MODE ) : byte_contents = BytesIO ( contents . encode ( 'utf8' ) ) info = tarfile . TarInfo ( name = name ) info . size = len ( contents ) info . mtime = 0 info . type = tarfile . REGTYPE info . mode = int ( mode , 8 ) tar . addfile ( tarinfo = info , fileobj = byte_contents )
13240	def includes ( self , query_date , query_time = None ) : if self . start_date and query_date < self . start_date : return False if self . end_date and query_date > self . end_date : return False if query_date . weekday ( ) not in self . weekdays : return False if not query_time : return True if query_time >= self . period . start and query_time <= self . period . end : return True return False
7822	def _make_response ( self , nonce , salt , iteration_count ) : self . _salted_password = self . Hi ( self . Normalize ( self . password ) , salt , iteration_count ) self . password = None if self . channel_binding : channel_binding = b"c=" + standard_b64encode ( self . _gs2_header + self . _cb_data ) else : channel_binding = b"c=" + standard_b64encode ( self . _gs2_header ) client_final_message_without_proof = ( channel_binding + b",r=" + nonce ) client_key = self . HMAC ( self . _salted_password , b"Client Key" ) stored_key = self . H ( client_key ) auth_message = ( self . _client_first_message_bare + b"," + self . _server_first_message + b"," + client_final_message_without_proof ) self . _auth_message = auth_message client_signature = self . HMAC ( stored_key , auth_message ) client_proof = self . XOR ( client_key , client_signature ) proof = b"p=" + standard_b64encode ( client_proof ) client_final_message = ( client_final_message_without_proof + b"," + proof ) return Response ( client_final_message )
9755	def delete ( ctx ) : user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if not click . confirm ( "Are sure you want to delete experiment `{}`" . format ( _experiment ) ) : click . echo ( 'Existing without deleting experiment.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . experiment . delete_experiment ( user , project_name , _experiment ) ExperimentManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Experiment `{}` was delete successfully" . format ( _experiment ) )
9462	def conference_hangup ( self , call_params ) : path = '/' + self . api_version + '/ConferenceHangup/' method = 'POST' return self . request ( path , method , call_params )
13265	def get_plugin_instance ( plugin_class , * args , ** kwargs ) : assert issubclass ( plugin_class , BasePlugin ) , type ( plugin_class ) global _yaz_plugin_instance_cache qualname = plugin_class . __qualname__ if not qualname in _yaz_plugin_instance_cache : plugin_class = get_plugin_list ( ) [ qualname ] _yaz_plugin_instance_cache [ qualname ] = plugin = plugin_class ( * args , ** kwargs ) funcs = [ func for _ , func in inspect . getmembers ( plugin ) if inspect . ismethod ( func ) and hasattr ( func , "yaz_dependency_config" ) ] for func in funcs : signature = inspect . signature ( func ) assert all ( parameter . kind is parameter . POSITIONAL_OR_KEYWORD and issubclass ( parameter . annotation , BasePlugin ) for parameter in signature . parameters . values ( ) ) , "All parameters for {} must type hint to a BasePlugin" . format ( func ) func ( * [ get_plugin_instance ( parameter . annotation ) for parameter in signature . parameters . values ( ) ] ) return _yaz_plugin_instance_cache [ qualname ]
13077	def main_collections ( self , lang = None ) : return sorted ( [ { "id" : member . id , "label" : str ( member . get_label ( lang = lang ) ) , "model" : str ( member . model ) , "type" : str ( member . type ) , "size" : member . size } for member in self . resolver . getMetadata ( ) . members ] , key = itemgetter ( "label" ) )
10060	def jsonschemas ( self ) : _jsonschemas = { k : v [ 'jsonschema' ] for k , v in self . app . config [ 'DEPOSIT_RECORDS_UI_ENDPOINTS' ] . items ( ) if 'jsonschema' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT_DEFAULT_JSONSCHEMA' ] , _jsonschemas )
12212	def get_field_for_proxy ( pref_proxy ) : field = { bool : models . BooleanField , int : models . IntegerField , float : models . FloatField , datetime : models . DateTimeField , } . get ( type ( pref_proxy . default ) , models . TextField ) ( ) update_field_from_proxy ( field , pref_proxy ) return field
13644	def append_arguments ( klass , sub_parsers , default_epilog , general_arguments ) : entry_name = hump_to_underscore ( klass . __name__ ) . replace ( '_component' , '' ) epilog = default_epilog if default_epilog else 'This tool generate by `cliez` ' 'https://www.github.com/wangwenpei/cliez' sub_parser = sub_parsers . add_parser ( entry_name , help = klass . __doc__ , epilog = epilog ) sub_parser . description = klass . add_arguments . __doc__ if hasattr ( klass , 'add_slot_args' ) : slot_args = klass . add_slot_args ( ) or [ ] for v in slot_args : sub_parser . add_argument ( * v [ 0 ] , ** v [ 1 ] ) sub_parser . description = klass . add_slot_args . __doc__ pass user_arguments = klass . add_arguments ( ) or [ ] for v in user_arguments : sub_parser . add_argument ( * v [ 0 ] , ** v [ 1 ] ) if not klass . exclude_global_option : for v in general_arguments : sub_parser . add_argument ( * v [ 0 ] , ** v [ 1 ] ) return sub_parser
134	def extract_from_image ( self , image ) : ia . do_assert ( image . ndim in [ 2 , 3 ] ) if len ( self . exterior ) <= 2 : raise Exception ( "Polygon must be made up of at least 3 points to extract its area from an image." ) bb = self . to_bounding_box ( ) bb_area = bb . extract_from_image ( image ) if self . is_out_of_image ( image , fully = True , partly = False ) : return bb_area xx = self . xx_int yy = self . yy_int xx_mask = xx - np . min ( xx ) yy_mask = yy - np . min ( yy ) height_mask = np . max ( yy_mask ) width_mask = np . max ( xx_mask ) rr_face , cc_face = skimage . draw . polygon ( yy_mask , xx_mask , shape = ( height_mask , width_mask ) ) mask = np . zeros ( ( height_mask , width_mask ) , dtype = np . bool ) mask [ rr_face , cc_face ] = True if image . ndim == 3 : mask = np . tile ( mask [ : , : , np . newaxis ] , ( 1 , 1 , image . shape [ 2 ] ) ) return bb_area * mask
4135	def check_md5sum_change ( src_file ) : src_md5 = get_md5sum ( src_file ) src_md5_file = src_file + '.md5' src_file_changed = True if os . path . exists ( src_md5_file ) : with open ( src_md5_file , 'r' ) as file_checksum : ref_md5 = file_checksum . read ( ) if src_md5 == ref_md5 : src_file_changed = False if src_file_changed : with open ( src_md5_file , 'w' ) as file_checksum : file_checksum . write ( src_md5 ) return src_file_changed
7047	def _parallel_bls_worker ( task ) : try : return _bls_runner ( * task ) except Exception as e : LOGEXCEPTION ( 'BLS failed for task %s' % repr ( task [ 2 : ] ) ) return { 'power' : nparray ( [ npnan for x in range ( task [ 2 ] ) ] ) , 'bestperiod' : npnan , 'bestpower' : npnan , 'transdepth' : npnan , 'transduration' : npnan , 'transingressbin' : npnan , 'transegressbin' : npnan }
7039	def object_info ( lcc_server , objectid , db_collection_id ) : urlparams = { 'objectid' : objectid , 'collection' : db_collection_id } urlqs = urlencode ( urlparams ) url = '%s/api/object?%s' % ( lcc_server , urlqs ) try : LOGINFO ( 'getting info for %s in collection %s from %s' % ( objectid , db_collection_id , lcc_server ) ) have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } req = Request ( url , data = None , headers = headers ) resp = urlopen ( req ) objectinfo = json . loads ( resp . read ( ) ) [ 'result' ] return objectinfo except HTTPError as e : if e . code == 404 : LOGERROR ( 'additional info for object %s not ' 'found in collection: %s' % ( objectid , db_collection_id ) ) else : LOGERROR ( 'could not retrieve object info, ' 'URL used: %s, error code: %s, reason: %s' % ( url , e . code , e . reason ) ) return None
5519	def append ( self , name , data , start ) : for throttle in self . throttles . values ( ) : getattr ( throttle , name ) . append ( data , start )
12969	def get ( self , pk , cascadeFetch = False ) : conn = self . _get_connection ( ) key = self . _get_key_for_id ( pk ) res = conn . hgetall ( key ) if type ( res ) != dict or not len ( res . keys ( ) ) : return None res [ '_id' ] = pk ret = self . _redisResultToObj ( res ) if cascadeFetch is True : self . _doCascadeFetch ( ret ) return ret
8807	def _make_job_dict ( job ) : body = { "id" : job . get ( 'id' ) , "action" : job . get ( 'action' ) , "completed" : job . get ( 'completed' ) , "tenant_id" : job . get ( 'tenant_id' ) , "created_at" : job . get ( 'created_at' ) , "transaction_id" : job . get ( 'transaction_id' ) , "parent_id" : job . get ( 'parent_id' , None ) } if not body [ 'transaction_id' ] : body [ 'transaction_id' ] = job . get ( 'id' ) completed = 0 for sub in job . subtransactions : if sub . get ( 'completed' ) : completed += 1 pct = 100 if job . get ( 'completed' ) else 0 if len ( job . subtransactions ) > 0 : pct = float ( completed ) / len ( job . subtransactions ) * 100.0 body [ 'transaction_percent' ] = int ( pct ) body [ 'completed_subtransactions' ] = completed body [ 'subtransactions' ] = len ( job . subtransactions ) return body
11450	def get_date ( self , filename ) : try : self . document = parse ( filename ) return self . _get_date ( ) except DateNotFoundException : print ( "Date problem found in {0}" . format ( filename ) ) return datetime . datetime . strftime ( datetime . datetime . now ( ) , "%Y-%m-%d" )
9187	def includeme ( config ) : settings = config . registry . settings session_factory = SignedCookieSessionFactory ( settings [ 'session_key' ] ) config . set_session_factory ( session_factory )
5162	def __intermediate_interface ( self , interface , uci_name ) : interface . update ( { '.type' : 'interface' , '.name' : uci_name , 'ifname' : interface . pop ( 'name' ) } ) if 'network' in interface : del interface [ 'network' ] if 'mac' in interface : if interface . get ( 'type' ) != 'wireless' : interface [ 'macaddr' ] = interface [ 'mac' ] del interface [ 'mac' ] if 'autostart' in interface : interface [ 'auto' ] = interface [ 'autostart' ] del interface [ 'autostart' ] if 'disabled' in interface : interface [ 'enabled' ] = not interface [ 'disabled' ] del interface [ 'disabled' ] if 'wireless' in interface : del interface [ 'wireless' ] if 'addresses' in interface : del interface [ 'addresses' ] return interface
12171	def count ( self , event ) : return len ( self . _listeners [ event ] ) + len ( self . _once [ event ] )
9767	def job ( ctx , project , job ) : ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'job' ] = job
6241	def combine ( self ) : self . gbuffer . color_attachments [ 0 ] . use ( location = 0 ) self . combine_shader [ "diffuse_buffer" ] . value = 0 self . lightbuffer . color_attachments [ 0 ] . use ( location = 1 ) self . combine_shader [ "light_buffer" ] . value = 1 self . quad . render ( self . combine_shader )
4981	def set_final_prices ( self , modes , request ) : result = [ ] for mode in modes : if mode [ 'premium' ] : mode [ 'final_price' ] = EcommerceApiClient ( request . user ) . get_course_final_price ( mode = mode , enterprise_catalog_uuid = request . GET . get ( 'catalog' ) if request . method == 'GET' else None , ) result . append ( mode ) return result
4224	def _load_keyring_class ( keyring_name ) : module_name , sep , class_name = keyring_name . rpartition ( '.' ) __import__ ( module_name ) module = sys . modules [ module_name ] return getattr ( module , class_name )
5121	def reset_colors ( self ) : for k , e in enumerate ( self . g . edges ( ) ) : self . g . set_ep ( e , 'edge_color' , self . edge2queue [ k ] . colors [ 'edge_color' ] ) for v in self . g . nodes ( ) : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_fill_color' ] )
8738	def get_ports_count ( context , filters = None ) : LOG . info ( "get_ports_count for tenant %s filters %s" % ( context . tenant_id , filters ) ) return db_api . port_count_all ( context , join_security_groups = True , ** filters )
13762	def _check_next ( self ) : if self . is_initial : return True if self . before : if self . before_cursor : return True else : return False else : if self . after_cursor : return True else : return False
10049	def create_error_handlers ( blueprint ) : blueprint . errorhandler ( PIDInvalidAction ) ( create_api_errorhandler ( status = 403 , message = 'Invalid action' ) ) records_rest_error_handlers ( blueprint )
1562	def get_component_tasks ( self , component_id ) : ret = [ ] for task_id , comp_id in self . task_to_component_map . items ( ) : if comp_id == component_id : ret . append ( task_id ) return ret
12429	def create_virtualenv ( self ) : if check_command ( 'virtualenv' ) : ve_dir = os . path . join ( self . _ve_dir , self . _project_name ) if os . path . exists ( ve_dir ) : if self . _force : logging . warn ( 'Removing existing virtualenv' ) shutil . rmtree ( ve_dir ) else : logging . warn ( 'Found existing virtualenv; not creating (use --force to overwrite)' ) return logging . info ( 'Creating virtualenv' ) p = subprocess . Popen ( 'virtualenv --no-site-packages {0} > /dev/null' . format ( ve_dir ) , shell = True ) os . waitpid ( p . pid , 0 ) for m in self . _modules : self . log . info ( 'Installing module {0}' . format ( m ) ) p = subprocess . Popen ( '{0} install {1} > /dev/null' . format ( os . path . join ( self . _ve_dir , self . _project_name ) + os . sep + 'bin' + os . sep + 'pip' , m ) , shell = True ) os . waitpid ( p . pid , 0 )
5668	def stop_to_stop_network_for_route_type ( gtfs , route_type , link_attributes = None , start_time_ut = None , end_time_ut = None ) : if link_attributes is None : link_attributes = DEFAULT_STOP_TO_STOP_LINK_ATTRIBUTES assert ( route_type in route_types . TRANSIT_ROUTE_TYPES ) stops_dataframe = gtfs . get_stops_for_route_type ( route_type ) net = networkx . DiGraph ( ) _add_stops_to_net ( net , stops_dataframe ) events_df = gtfs . get_transit_events ( start_time_ut = start_time_ut , end_time_ut = end_time_ut , route_type = route_type ) if len ( net . nodes ( ) ) < 2 : assert events_df . shape [ 0 ] == 0 link_event_groups = events_df . groupby ( [ 'from_stop_I' , 'to_stop_I' ] , sort = False ) for key , link_events in link_event_groups : from_stop_I , to_stop_I = key assert isinstance ( link_events , pd . DataFrame ) if link_attributes is None : net . add_edge ( from_stop_I , to_stop_I ) else : link_data = { } if "duration_min" in link_attributes : link_data [ 'duration_min' ] = float ( link_events [ 'duration' ] . min ( ) ) if "duration_max" in link_attributes : link_data [ 'duration_max' ] = float ( link_events [ 'duration' ] . max ( ) ) if "duration_median" in link_attributes : link_data [ 'duration_median' ] = float ( link_events [ 'duration' ] . median ( ) ) if "duration_avg" in link_attributes : link_data [ 'duration_avg' ] = float ( link_events [ 'duration' ] . mean ( ) ) if "n_vehicles" in link_attributes : link_data [ 'n_vehicles' ] = int ( link_events . shape [ 0 ] ) if "capacity_estimate" in link_attributes : link_data [ 'capacity_estimate' ] = route_types . ROUTE_TYPE_TO_APPROXIMATE_CAPACITY [ route_type ] * int ( link_events . shape [ 0 ] ) if "d" in link_attributes : from_lat = net . node [ from_stop_I ] [ 'lat' ] from_lon = net . node [ from_stop_I ] [ 'lon' ] to_lat = net . node [ to_stop_I ] [ 'lat' ] to_lon = net . node [ to_stop_I ] [ 'lon' ] distance = wgs84_distance ( from_lat , from_lon , to_lat , to_lon ) link_data [ 'd' ] = int ( distance ) if "distance_shape" in link_attributes : assert "shape_id" in link_events . columns . values found = None for i , shape_id in enumerate ( link_events [ "shape_id" ] . values ) : if shape_id is not None : found = i break if found is None : link_data [ "distance_shape" ] = None else : link_event = link_events . iloc [ found ] distance = gtfs . get_shape_distance_between_stops ( link_event [ "trip_I" ] , int ( link_event [ "from_seq" ] ) , int ( link_event [ "to_seq" ] ) ) link_data [ 'distance_shape' ] = distance if "route_I_counts" in link_attributes : link_data [ "route_I_counts" ] = link_events . groupby ( "route_I" ) . size ( ) . to_dict ( ) net . add_edge ( from_stop_I , to_stop_I , attr_dict = link_data ) return net
649	def generateSimpleCoincMatrix ( nCoinc = 10 , length = 500 , activity = 50 ) : assert nCoinc * activity <= length , "can't generate non-overlapping coincidences" coincMatrix = SM32 ( 0 , length ) coinc = numpy . zeros ( length , dtype = 'int32' ) for i in xrange ( nCoinc ) : coinc [ : ] = 0 coinc [ i * activity : ( i + 1 ) * activity ] = 1 coincMatrix . addRow ( coinc ) return coincMatrix
9965	def _to_attrdict ( self , attrs = None ) : result = self . _baseattrs for attr in attrs : if hasattr ( self , attr ) : result [ attr ] = getattr ( self , attr ) . _to_attrdict ( attrs ) return result
8263	def swatch ( self , x , y , w = 35 , h = 35 , padding = 0 , roundness = 0 ) : for clr in self : clr . swatch ( x , y , w , h , roundness ) y += h + padding
5861	def _keys_to_camel_case ( self , obj ) : return dict ( ( to_camel_case ( key ) , value ) for ( key , value ) in obj . items ( ) )
6262	def resize ( self , width , height ) : self . width = width self . height = height self . buffer_width , self . buffer_height = glfw . get_framebuffer_size ( self . window ) self . set_default_viewport ( )
35	def setup_mpi_gpus ( ) : if 'CUDA_VISIBLE_DEVICES' not in os . environ : if sys . platform == 'darwin' : ids = [ ] else : lrank , _lsize = get_local_rank_size ( MPI . COMM_WORLD ) ids = [ lrank ] os . environ [ "CUDA_VISIBLE_DEVICES" ] = "," . join ( map ( str , ids ) )
2307	def reset_parameters ( self ) : stdv = 1. / math . sqrt ( self . weight . size ( 1 ) ) self . weight . data . uniform_ ( - stdv , stdv ) if self . bias is not None : self . bias . data . uniform_ ( - stdv , stdv )
13736	def get_param_values ( request , model = None ) : if type ( request ) == dict : return request params = get_payload ( request ) try : del params [ 'pk' ] params [ params . pop ( 'name' ) ] = params . pop ( 'value' ) except KeyError : pass return { k . rstrip ( '[]' ) : safe_eval ( v ) if not type ( v ) == list else [ safe_eval ( sv ) for sv in v ] for k , v in params . items ( ) }
4327	def downsample ( self , factor = 2 ) : if not isinstance ( factor , int ) or factor < 1 : raise ValueError ( 'factor must be a positive integer.' ) effect_args = [ 'downsample' , '{}' . format ( factor ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'downsample' ) return self
2087	def _convert_pagenum ( self , kwargs ) : for key in ( 'next' , 'previous' ) : if not kwargs . get ( key ) : continue match = re . search ( r'page=(?P<num>[\d]+)' , kwargs [ key ] ) if match is None and key == 'previous' : kwargs [ key ] = 1 continue kwargs [ key ] = int ( match . groupdict ( ) [ 'num' ] )
13002	def _filter_cluster_data ( self ) : min_temp = self . temperature_range_slider . value [ 0 ] max_temp = self . temperature_range_slider . value [ 1 ] temp_mask = np . logical_and ( self . cluster . catalog [ 'temperature' ] >= min_temp , self . cluster . catalog [ 'temperature' ] <= max_temp ) min_lum = self . luminosity_range_slider . value [ 0 ] max_lum = self . luminosity_range_slider . value [ 1 ] lum_mask = np . logical_and ( self . cluster . catalog [ 'luminosity' ] >= min_lum , self . cluster . catalog [ 'luminosity' ] <= max_lum ) selected_mask = np . isin ( self . cluster . catalog [ 'id' ] , self . selection_ids ) filter_mask = temp_mask & lum_mask & selected_mask self . filtered_data = self . cluster . catalog [ filter_mask ] . data self . source . data = { 'id' : list ( self . filtered_data [ 'id' ] ) , 'temperature' : list ( self . filtered_data [ 'temperature' ] ) , 'luminosity' : list ( self . filtered_data [ 'luminosity' ] ) , 'color' : list ( self . filtered_data [ 'color' ] ) } logging . debug ( "Selected data is now: %s" , self . filtered_data )
7322	def addattachments ( message , template_path ) : if 'attachment' not in message : return message , 0 message = make_message_multipart ( message ) attachment_filepaths = message . get_all ( 'attachment' , failobj = [ ] ) template_parent_dir = os . path . dirname ( template_path ) for attachment_filepath in attachment_filepaths : attachment_filepath = os . path . expanduser ( attachment_filepath . strip ( ) ) if not attachment_filepath : continue if not os . path . isabs ( attachment_filepath ) : attachment_filepath = os . path . join ( template_parent_dir , attachment_filepath ) normalized_path = os . path . abspath ( attachment_filepath ) if not os . path . exists ( normalized_path ) : print ( "Error: can't find attachment " + normalized_path ) sys . exit ( 1 ) filename = os . path . basename ( normalized_path ) with open ( normalized_path , "rb" ) as attachment : part = email . mime . application . MIMEApplication ( attachment . read ( ) , Name = filename ) part . add_header ( 'Content-Disposition' , 'attachment; filename="{}"' . format ( filename ) ) message . attach ( part ) print ( ">>> attached {}" . format ( normalized_path ) ) del message [ 'attachment' ] return message , len ( attachment_filepaths )
9935	def find ( self , path , all = False ) : matches = [ ] for prefix , root in self . locations : if root not in searched_locations : searched_locations . append ( root ) matched_path = self . find_location ( root , path , prefix ) if matched_path : if not all : return matched_path matches . append ( matched_path ) return matches
7604	def get_player_chests ( self , tag : crtag , timeout : int = None ) : url = self . api . PLAYER + '/' + tag + '/upcomingchests' return self . _get_model ( url , timeout = timeout )
1188	def roundfrac ( intpart , fraction , digs ) : f = len ( fraction ) if f <= digs : return intpart , fraction + '0' * ( digs - f ) i = len ( intpart ) if i + digs < 0 : return '0' * - digs , '' total = intpart + fraction nextdigit = total [ i + digs ] if nextdigit >= '5' : n = i + digs - 1 while n >= 0 : if total [ n ] != '9' : break n = n - 1 else : total = '0' + total i = i + 1 n = 0 total = total [ : n ] + chr ( ord ( total [ n ] ) + 1 ) + '0' * ( len ( total ) - n - 1 ) intpart , fraction = total [ : i ] , total [ i : ] if digs >= 0 : return intpart , fraction [ : digs ] else : return intpart [ : digs ] + '0' * - digs , ''
3525	def uservoice ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return UserVoiceNode ( )
603	def add2DArray ( self , data , position = 111 , xlabel = None , ylabel = None , cmap = None , aspect = "auto" , interpolation = "nearest" , name = None ) : if cmap is None : cmap = cm . Greys ax = self . _addBase ( position , xlabel = xlabel , ylabel = ylabel ) ax . imshow ( data , cmap = cmap , aspect = aspect , interpolation = interpolation ) if self . _show : plt . draw ( ) if name is not None : if not os . path . exists ( "log" ) : os . mkdir ( "log" ) plt . savefig ( "log/{name}.png" . format ( name = name ) , bbox_inches = "tight" , figsize = ( 8 , 6 ) , dpi = 400 )
11954	def upload_gif ( gif ) : client_id = os . environ . get ( 'IMGUR_API_ID' ) client_secret = os . environ . get ( 'IMGUR_API_SECRET' ) if client_id is None or client_secret is None : click . echo ( 'Cannot upload - could not find IMGUR_API_ID or IMGUR_API_SECRET environment variables' ) return client = ImgurClient ( client_id , client_secret ) click . echo ( 'Uploading file {}' . format ( click . format_filename ( gif ) ) ) response = client . upload_from_path ( gif ) click . echo ( 'File uploaded - see your gif at {}' . format ( response [ 'link' ] ) )
13212	def build_jsonld ( self , url = None , code_url = None , ci_url = None , readme_url = None , license_id = None ) : jsonld = { '@context' : [ "https://raw.githubusercontent.com/codemeta/codemeta/2.0-rc/" "codemeta.jsonld" , "http://schema.org" ] , '@type' : [ 'Report' , 'SoftwareSourceCode' ] , 'language' : 'TeX' , 'reportNumber' : self . handle , 'name' : self . plain_title , 'description' : self . plain_abstract , 'author' : [ { '@type' : 'Person' , 'name' : author_name } for author_name in self . plain_authors ] , 'dateModified' : self . revision_datetime } try : jsonld [ 'articleBody' ] = self . plain_content jsonld [ 'fileFormat' ] = 'text/plain' except RuntimeError : self . _logger . exception ( 'Could not convert latex body to plain ' 'text for articleBody.' ) self . _logger . warning ( 'Falling back to tex source for articleBody' ) jsonld [ 'articleBody' ] = self . _tex jsonld [ 'fileFormat' ] = 'text/plain' if url is not None : jsonld [ '@id' ] = url jsonld [ 'url' ] = url else : jsonld [ '@id' ] = self . handle if code_url is not None : jsonld [ 'codeRepository' ] = code_url if ci_url is not None : jsonld [ 'contIntegration' ] = ci_url if readme_url is not None : jsonld [ 'readme' ] = readme_url if license_id is not None : jsonld [ 'license_id' ] = None return jsonld
7445	def _step4func ( self , samples , force , ipyclient ) : if self . _headers : print ( "\n Step 4: Joint estimation of error rate and heterozygosity" ) samples = _get_samples ( self , samples ) if not self . _samples_precheck ( samples , 4 , force ) : raise IPyradError ( FIRST_RUN_3 ) elif not force : if all ( [ i . stats . state >= 4 for i in samples ] ) : print ( JOINTS_EXIST . format ( len ( samples ) ) ) return assemble . jointestimate . run ( self , samples , force , ipyclient )
7164	def add_intent ( self , name , lines , reload_cache = False ) : self . intents . add ( name , lines , reload_cache ) self . padaos . add_intent ( name , lines ) self . must_train = True
12426	def _expand_targets ( self , targets , base_dir = None ) : all_targets = [ ] for target in targets : target_dirs = [ p for p in [ base_dir , os . path . dirname ( target ) ] if p ] target_dir = target_dirs and os . path . join ( * target_dirs ) or '' target = os . path . basename ( target ) target_path = os . path . join ( target_dir , target ) if os . path . exists ( target_path ) : all_targets . append ( target_path ) with open ( target_path ) as fp : for line in fp : if line . startswith ( '-r ' ) : _ , new_target = line . split ( ' ' , 1 ) all_targets . extend ( self . _expand_targets ( [ new_target . strip ( ) ] , base_dir = target_dir ) ) return all_targets
11840	def score ( self ) : "The total score for the words found, according to the rules." return sum ( [ self . scores [ len ( w ) ] for w in self . words ( ) ] )
4622	def _new_masterpassword ( self , password ) : if self . config_key in self . config and self . config [ self . config_key ] : raise Exception ( "Storage already has a masterpassword!" ) self . decrypted_master = hexlify ( os . urandom ( 32 ) ) . decode ( "ascii" ) self . password = password self . _save_encrypted_masterpassword ( ) return self . masterkey
490	def acquireConnection ( self ) : self . _logger . debug ( "Acquiring connection" ) self . _conn . _ping_check ( ) connWrap = ConnectionWrapper ( dbConn = self . _conn , cursor = self . _conn . cursor ( ) , releaser = self . _releaseConnection , logger = self . _logger ) return connWrap
8267	def copy ( self , clr = None , d = 0.0 ) : cr = ColorRange ( ) cr . name = self . name cr . h = deepcopy ( self . h ) cr . s = deepcopy ( self . s ) cr . b = deepcopy ( self . b ) cr . a = deepcopy ( self . a ) cr . grayscale = self . grayscale if not self . grayscale : cr . black = self . black . copy ( ) cr . white = self . white . copy ( ) if clr != None : cr . h , cr . a = clr . h + d * ( random ( ) * 2 - 1 ) , clr . a return cr
13809	def get_version ( relpath ) : from os . path import dirname , join if '__file__' not in globals ( ) : root = '.' else : root = dirname ( __file__ ) for line in open ( join ( root , relpath ) , 'rb' ) : line = line . decode ( 'cp437' ) if '__version__' in line : if '"' in line : return line . split ( '"' ) [ 1 ] elif "'" in line : return line . split ( "'" ) [ 1 ]
13362	def prompt ( text , default = None , hide_input = False , confirmation_prompt = False , type = None , value_proc = None , prompt_suffix = ': ' , show_default = True , err = False ) : result = None def prompt_func ( text ) : f = hide_input and hidden_prompt_func or visible_prompt_func try : echo ( text , nl = False , err = err ) return f ( '' ) except ( KeyboardInterrupt , EOFError ) : if hide_input : echo ( None , err = err ) raise Abort ( ) if value_proc is None : value_proc = convert_type ( type , default ) prompt = _build_prompt ( text , prompt_suffix , show_default , default ) while 1 : while 1 : value = prompt_func ( prompt ) if value : break elif default is not None : return default try : result = value_proc ( value ) except UsageError as e : echo ( 'Error: %s' % e . message , err = err ) continue if not confirmation_prompt : return result while 1 : value2 = prompt_func ( 'Repeat for confirmation: ' ) if value2 : break if value == value2 : return result echo ( 'Error: the two entered values do not match' , err = err )
12210	def cache_result ( func ) : def cache_set ( key , value ) : cache . set ( key , value , AVATAR_CACHE_TIMEOUT ) return value def cached_func ( user , size ) : prefix = func . __name__ cached_funcs . add ( prefix ) key = get_cache_key ( user , size , prefix = prefix ) return cache . get ( key ) or cache_set ( key , func ( user , size ) ) return cached_func
11525	def create_big_thumbnail ( self , token , bitstream_id , item_id , width = 575 ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'bitstreamId' ] = bitstream_id parameters [ 'itemId' ] = item_id parameters [ 'width' ] = width response = self . request ( 'midas.thumbnailcreator.create.big.thumbnail' , parameters ) return response
12253	def _delete_key_internal ( self , * args , ** kwargs ) : mimicdb . backend . srem ( tpl . bucket % self . name , args [ 0 ] ) mimicdb . backend . delete ( tpl . key % ( self . name , args [ 0 ] ) ) return super ( Bucket , self ) . _delete_key_internal ( * args , ** kwargs )
9890	def _boottime_linux ( ) : global __boottime try : f = open ( '/proc/stat' , 'r' ) for line in f : if line . startswith ( 'btime' ) : __boottime = int ( line . split ( ) [ 1 ] ) if datetime is None : raise NotImplementedError ( 'datetime module required.' ) return datetime . fromtimestamp ( __boottime ) except ( IOError , IndexError ) : return None
4566	def euclidean ( c1 , c2 ) : diffs = ( ( i - j ) for i , j in zip ( c1 , c2 ) ) return sum ( x * x for x in diffs )
7283	def trim_field_key ( document , field_key ) : trimming = True left_over_key_values = [ ] current_key = field_key while trimming and current_key : if hasattr ( document , current_key ) : trimming = False else : key_array = current_key . split ( "_" ) left_over_key_values . append ( key_array . pop ( ) ) current_key = u"_" . join ( key_array ) left_over_key_values . reverse ( ) return current_key , left_over_key_values
12507	def voxspace_to_mmspace ( img ) : shape , affine = img . shape [ : 3 ] , img . affine coords = np . array ( np . meshgrid ( * ( range ( i ) for i in shape ) , indexing = 'ij' ) ) coords = np . rollaxis ( coords , 0 , len ( shape ) + 1 ) mm_coords = nib . affines . apply_affine ( affine , coords ) return mm_coords
1422	def loads ( string ) : f = StringIO . StringIO ( string ) marshaller = JavaObjectUnmarshaller ( f ) marshaller . add_transformer ( DefaultObjectTransformer ( ) ) return marshaller . readObject ( )
9209	def get_codec ( bytes_ ) : prefix = extract_prefix ( bytes_ ) try : return CODE_TABLE [ prefix ] except KeyError : raise ValueError ( 'Prefix {} not present in the lookup table' . format ( prefix ) )
3109	def locked_put ( self , credentials ) : entity , _ = self . model_class . objects . get_or_create ( ** { self . key_name : self . key_value } ) setattr ( entity , self . property_name , credentials ) entity . save ( )
2022	def SIGNEXTEND ( self , size , value ) : testbit = Operators . ITEBV ( 256 , size <= 31 , size * 8 + 7 , 257 ) result1 = ( value | ( TT256 - ( 1 << testbit ) ) ) result2 = ( value & ( ( 1 << testbit ) - 1 ) ) result = Operators . ITEBV ( 256 , ( value & ( 1 << testbit ) ) != 0 , result1 , result2 ) return Operators . ITEBV ( 256 , size <= 31 , result , value )
621	def parseStringList ( s ) : assert isinstance ( s , basestring ) return [ int ( i ) for i in s . split ( ) ]
5411	def build_machine ( network = None , machine_type = None , preemptible = None , service_account = None , boot_disk_size_gb = None , disks = None , accelerators = None , labels = None , cpu_platform = None , nvidia_driver_version = None ) : return { 'network' : network , 'machineType' : machine_type , 'preemptible' : preemptible , 'serviceAccount' : service_account , 'bootDiskSizeGb' : boot_disk_size_gb , 'disks' : disks , 'accelerators' : accelerators , 'labels' : labels , 'cpuPlatform' : cpu_platform , 'nvidiaDriverVersion' : nvidia_driver_version , }
8197	def bezier_arc ( x1 , y1 , x2 , y2 , start_angle = 0 , extent = 90 ) : x1 , y1 , x2 , y2 = min ( x1 , x2 ) , max ( y1 , y2 ) , max ( x1 , x2 ) , min ( y1 , y2 ) if abs ( extent ) <= 90 : frag_angle = float ( extent ) nfrag = 1 else : nfrag = int ( ceil ( abs ( extent ) / 90. ) ) if nfrag == 0 : warnings . warn ( 'Invalid value for extent: %r' % extent ) return [ ] frag_angle = float ( extent ) / nfrag x_cen = ( x1 + x2 ) / 2. y_cen = ( y1 + y2 ) / 2. rx = ( x2 - x1 ) / 2. ry = ( y2 - y1 ) / 2. half_angle = radians ( frag_angle ) / 2 kappa = abs ( 4. / 3. * ( 1. - cos ( half_angle ) ) / sin ( half_angle ) ) if frag_angle < 0 : sign = - 1 else : sign = 1 point_list = [ ] for i in range ( nfrag ) : theta0 = radians ( start_angle + i * frag_angle ) theta1 = radians ( start_angle + ( i + 1 ) * frag_angle ) c0 = cos ( theta0 ) c1 = cos ( theta1 ) s0 = sin ( theta0 ) s1 = sin ( theta1 ) if frag_angle > 0 : signed_kappa = - kappa else : signed_kappa = kappa point_list . append ( ( x_cen + rx * c0 , y_cen - ry * s0 , x_cen + rx * ( c0 + signed_kappa * s0 ) , y_cen - ry * ( s0 - signed_kappa * c0 ) , x_cen + rx * ( c1 - signed_kappa * s1 ) , y_cen - ry * ( s1 + signed_kappa * c1 ) , x_cen + rx * c1 , y_cen - ry * s1 ) ) return point_list
5867	def _inactivate_organization ( organization ) : [ _inactivate_organization_course_relationship ( record ) for record in internal . OrganizationCourse . objects . filter ( organization_id = organization . id , active = True ) ] [ _inactivate_record ( record ) for record in internal . Organization . objects . filter ( id = organization . id , active = True ) ]
6630	def get ( self , path ) : path = _splitPath ( path ) for config in self . configs . values ( ) : cur = config for el in path : if el in cur : cur = cur [ el ] else : cur = None break if cur is not None : return cur return None
4668	def encrypt ( privkey , passphrase ) : if isinstance ( privkey , str ) : privkey = PrivateKey ( privkey ) else : privkey = PrivateKey ( repr ( privkey ) ) privkeyhex = repr ( privkey ) addr = format ( privkey . bitcoin . address , "BTC" ) a = _bytes ( addr ) salt = hashlib . sha256 ( hashlib . sha256 ( a ) . digest ( ) ) . digest ( ) [ 0 : 4 ] if SCRYPT_MODULE == "scrypt" : key = scrypt . hash ( passphrase , salt , 16384 , 8 , 8 ) elif SCRYPT_MODULE == "pylibscrypt" : key = scrypt . scrypt ( bytes ( passphrase , "utf-8" ) , salt , 16384 , 8 , 8 ) else : raise ValueError ( "No scrypt module loaded" ) ( derived_half1 , derived_half2 ) = ( key [ : 32 ] , key [ 32 : ] ) aes = AES . new ( derived_half2 , AES . MODE_ECB ) encrypted_half1 = _encrypt_xor ( privkeyhex [ : 32 ] , derived_half1 [ : 16 ] , aes ) encrypted_half2 = _encrypt_xor ( privkeyhex [ 32 : ] , derived_half1 [ 16 : ] , aes ) " flag byte is forced 0xc0 because Graphene only uses compressed keys " payload = b"\x01" + b"\x42" + b"\xc0" + salt + encrypted_half1 + encrypted_half2 " Checksum " checksum = hashlib . sha256 ( hashlib . sha256 ( payload ) . digest ( ) ) . digest ( ) [ : 4 ] privatkey = hexlify ( payload + checksum ) . decode ( "ascii" ) return Base58 ( privatkey )
7088	def jd_to_datetime ( jd , returniso = False ) : tt = astime . Time ( jd , format = 'jd' , scale = 'utc' ) if returniso : return tt . iso else : return tt . datetime
8158	def sql ( self , sql ) : self . _cur . execute ( sql ) if sql . lower ( ) . find ( "select" ) >= 0 : matches = [ ] for r in self . _cur : matches . append ( r ) return matches
245	def get_low_liquidity_transactions ( transactions , market_data , last_n_days = None ) : txn_daily_w_bar = daily_txns_with_bar_data ( transactions , market_data ) txn_daily_w_bar . index . name = 'date' txn_daily_w_bar = txn_daily_w_bar . reset_index ( ) if last_n_days is not None : md = txn_daily_w_bar . date . max ( ) - pd . Timedelta ( days = last_n_days ) txn_daily_w_bar = txn_daily_w_bar [ txn_daily_w_bar . date > md ] bar_consumption = txn_daily_w_bar . assign ( max_pct_bar_consumed = ( txn_daily_w_bar . amount / txn_daily_w_bar . volume ) * 100 ) . sort_values ( 'max_pct_bar_consumed' , ascending = False ) max_bar_consumption = bar_consumption . groupby ( 'symbol' ) . first ( ) return max_bar_consumption [ [ 'date' , 'max_pct_bar_consumed' ] ]
9830	def write ( self , filename ) : maxcol = 80 with open ( filename , 'w' ) as outfile : for line in self . comments : comment = '# ' + str ( line ) outfile . write ( comment [ : maxcol ] + '\n' ) for component , object in self . sorted_components ( ) : object . write ( outfile ) DXclass . write ( self , outfile , quote = True ) for component , object in self . sorted_components ( ) : outfile . write ( 'component "%s" value %s\n' % ( component , str ( object . id ) ) )
5915	def _process_range ( self , selection , name = None ) : try : first , last , gmx_atomname = selection except ValueError : try : first , last = selection gmx_atomname = '*' except : logger . error ( "%r is not a valid range selection" , selection ) raise if name is None : name = "{first!s}-{last!s}_{gmx_atomname!s}" . format ( ** vars ( ) ) _first = self . _translate_residue ( first , default_atomname = gmx_atomname ) _last = self . _translate_residue ( last , default_atomname = gmx_atomname ) _selection = 'r {0:d} - {1:d} & & a {2!s}' . format ( _first [ 'resid' ] , _last [ 'resid' ] , gmx_atomname ) cmd = [ 'keep 0' , 'del 0' , _selection , 'name 0 {name!s}' . format ( ** vars ( ) ) , 'q' ] fd , ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = name + '__' ) rc , out , err = self . make_ndx ( n = self . ndx , o = ndx , input = cmd ) self . check_output ( out , "No atoms found for " "%(selection)r % vars ( ) ) return name , ndx
9168	def post_publication_processing ( event , cursor ) : module_ident , ident_hash = event . module_ident , event . ident_hash celery_app = get_current_registry ( ) . celery_app cursor . execute ( 'SELECT result_id::text ' 'FROM document_baking_result_associations ' 'WHERE module_ident = %s' , ( module_ident , ) ) for result in cursor . fetchall ( ) : state = celery_app . AsyncResult ( result [ 0 ] ) . state if state in ( 'QUEUED' , 'STARTED' , 'RETRY' ) : logger . debug ( 'Already queued module_ident={} ident_hash={}' . format ( module_ident , ident_hash ) ) return logger . debug ( 'Queued for processing module_ident={} ident_hash={}' . format ( module_ident , ident_hash ) ) recipe_ids = _get_recipe_ids ( module_ident , cursor ) update_module_state ( cursor , module_ident , 'processing' , recipe_ids [ 0 ] ) cursor . connection . commit ( ) task_name = 'cnxpublishing.subscribers.baking_processor' baking_processor = celery_app . tasks [ task_name ] result = baking_processor . delay ( module_ident , ident_hash ) baking_processor . backend . store_result ( result . id , None , 'QUEUED' ) track_baking_proc_state ( result , module_ident , cursor )
2807	def convert_gemm ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting Linear ...' ) if names == 'short' : tf_name = 'FC' + random_string ( 6 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) bias_name = '{0}.bias' . format ( w_name ) weights_name = '{0}.weight' . format ( w_name ) W = weights [ weights_name ] . numpy ( ) . transpose ( ) input_channels , output_channels = W . shape keras_weights = [ W ] has_bias = False if bias_name in weights : bias = weights [ bias_name ] . numpy ( ) keras_weights = [ W , bias ] has_bias = True dense = keras . layers . Dense ( output_channels , weights = keras_weights , use_bias = has_bias , name = tf_name , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , ) layers [ scope_name ] = dense ( layers [ inputs [ 0 ] ] )
1939	def get_func_return_types ( self , hsh : bytes ) -> str : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) abi = self . get_abi ( hsh ) outputs = abi . get ( 'outputs' ) return '()' if outputs is None else SolidityMetadata . tuple_signature_for_components ( outputs )
2962	def expand_partitions ( containers , partitions ) : all_names = frozenset ( c . name for c in containers if not c . holy ) holy_names = frozenset ( c . name for c in containers if c . holy ) neutral_names = frozenset ( c . name for c in containers if c . neutral ) partitions = [ frozenset ( p ) for p in partitions ] unknown = set ( ) holy = set ( ) union = set ( ) for partition in partitions : unknown . update ( partition - all_names - holy_names ) holy . update ( partition - all_names ) union . update ( partition ) if unknown : raise BlockadeError ( 'Partitions contain unknown containers: %s' % list ( unknown ) ) if holy : raise BlockadeError ( 'Partitions contain holy containers: %s' % list ( holy ) ) leftover = all_names . difference ( union ) if leftover : partitions . append ( leftover ) if not neutral_names . issubset ( leftover ) : partitions . append ( neutral_names ) return partitions
6547	def wait_for_field ( self ) : self . exec_command ( "Wait({0}, InputField)" . format ( self . timeout ) . encode ( "ascii" ) ) if self . status . keyboard != b"U" : raise KeyboardStateError ( "keyboard not unlocked, state was: {0}" . format ( self . status . keyboard . decode ( "ascii" ) ) )
376	def imresize ( x , size = None , interp = 'bicubic' , mode = None ) : if size is None : size = [ 100 , 100 ] if x . shape [ - 1 ] == 1 : x = scipy . misc . imresize ( x [ : , : , 0 ] , size , interp = interp , mode = mode ) return x [ : , : , np . newaxis ] else : return scipy . misc . imresize ( x , size , interp = interp , mode = mode )
9237	def open ( self ) : if self . is_open : return try : os . chdir ( self . working_directory ) if self . chroot_directory : os . chroot ( self . chroot_directory ) os . setgid ( self . gid ) os . setuid ( self . uid ) os . umask ( self . umask ) except OSError as err : raise DaemonError ( 'Setting up Environment failed: {0}' . format ( err ) ) if self . prevent_core : try : resource . setrlimit ( resource . RLIMIT_CORE , ( 0 , 0 ) ) except Exception as err : raise DaemonError ( 'Could not disable core files: {0}' . format ( err ) ) if self . detach_process : try : if os . fork ( ) > 0 : os . _exit ( 0 ) except OSError as err : raise DaemonError ( 'First fork failed: {0}' . format ( err ) ) os . setsid ( ) try : if os . fork ( ) > 0 : os . _exit ( 0 ) except OSError as err : raise DaemonError ( 'Second fork failed: {0}' . format ( err ) ) for ( signal_number , handler ) in self . _signal_handler_map . items ( ) : signal . signal ( signal_number , handler ) close_filenos ( self . _files_preserve ) redirect_stream ( sys . stdin , self . stdin ) redirect_stream ( sys . stdout , self . stdout ) redirect_stream ( sys . stderr , self . stderr ) if self . pidfile : self . pidfile . acquire ( ) self . _is_open = True
9376	def extract_diff_sla_from_config_file ( obj , options_file ) : rule_strings = { } config_obj = ConfigParser . ConfigParser ( ) config_obj . optionxform = str config_obj . read ( options_file ) for section in config_obj . sections ( ) : rule_strings , kwargs = get_rule_strings ( config_obj , section ) for ( key , val ) in rule_strings . iteritems ( ) : set_sla ( obj , section , key , val )
1225	def setup_components_and_tf_funcs ( self , custom_getter = None ) : custom_getter = super ( MemoryModel , self ) . setup_components_and_tf_funcs ( custom_getter ) self . memory = Memory . from_spec ( spec = self . memory_spec , kwargs = dict ( states = self . states_spec , internals = self . internals_spec , actions = self . actions_spec , summary_labels = self . summary_labels ) ) self . optimizer = Optimizer . from_spec ( spec = self . optimizer_spec , kwargs = dict ( summary_labels = self . summary_labels ) ) self . fn_discounted_cumulative_reward = tf . make_template ( name_ = 'discounted-cumulative-reward' , func_ = self . tf_discounted_cumulative_reward , custom_getter_ = custom_getter ) self . fn_reference = tf . make_template ( name_ = 'reference' , func_ = self . tf_reference , custom_getter_ = custom_getter ) self . fn_loss_per_instance = tf . make_template ( name_ = 'loss-per-instance' , func_ = self . tf_loss_per_instance , custom_getter_ = custom_getter ) self . fn_regularization_losses = tf . make_template ( name_ = 'regularization-losses' , func_ = self . tf_regularization_losses , custom_getter_ = custom_getter ) self . fn_loss = tf . make_template ( name_ = 'loss' , func_ = self . tf_loss , custom_getter_ = custom_getter ) self . fn_optimization = tf . make_template ( name_ = 'optimization' , func_ = self . tf_optimization , custom_getter_ = custom_getter ) self . fn_import_experience = tf . make_template ( name_ = 'import-experience' , func_ = self . tf_import_experience , custom_getter_ = custom_getter ) return custom_getter
1597	def format_prefix ( filename , sres ) : try : pwent = pwd . getpwuid ( sres . st_uid ) user = pwent . pw_name except KeyError : user = sres . st_uid try : grent = grp . getgrgid ( sres . st_gid ) group = grent . gr_name except KeyError : group = sres . st_gid return '%s %3d %10s %10s %10d %s' % ( format_mode ( sres ) , sres . st_nlink , user , group , sres . st_size , format_mtime ( sres . st_mtime ) , )
10545	def delete_task ( task_id ) : try : res = _pybossa_req ( 'delete' , 'task' , task_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : raise
9054	def posteriori_covariance ( self ) : r K = GLMM . covariance ( self ) tau = self . _ep . _posterior . tau return pinv ( pinv ( K ) + diag ( 1 / tau ) )
4010	def get_docker_client ( ) : env = get_docker_env ( ) host , cert_path , tls_verify = env [ 'DOCKER_HOST' ] , env [ 'DOCKER_CERT_PATH' ] , env [ 'DOCKER_TLS_VERIFY' ] params = { 'base_url' : host . replace ( 'tcp://' , 'https://' ) , 'timeout' : None , 'version' : 'auto' } if tls_verify and cert_path : params [ 'tls' ] = docker . tls . TLSConfig ( client_cert = ( os . path . join ( cert_path , 'cert.pem' ) , os . path . join ( cert_path , 'key.pem' ) ) , ca_cert = os . path . join ( cert_path , 'ca.pem' ) , verify = True , ssl_version = None , assert_hostname = False ) return docker . Client ( ** params )
4476	def slice_clip ( filename , start , stop , n_samples , sr , mono = True ) : with psf . SoundFile ( str ( filename ) , mode = 'r' ) as soundf : n_target = stop - start soundf . seek ( start ) y = soundf . read ( n_target ) . T if mono : y = librosa . to_mono ( y ) y = librosa . resample ( y , soundf . samplerate , sr ) y = librosa . util . fix_length ( y , n_samples ) return y
11832	def mate ( self , other ) : "Return a new individual crossing self and other." c = random . randrange ( len ( self . genes ) ) return self . __class__ ( self . genes [ : c ] + other . genes [ c : ] )
11655	def transform ( self , X , ** params ) : X = as_features ( X , stack = True ) X_new = self . transformer . transform ( X . stacked_features , ** params ) return self . _gather_outputs ( X , X_new )
7995	def uplink_receive ( self , stanza ) : with self . lock : if self . stanza_route : self . stanza_route . uplink_receive ( stanza ) else : logger . debug ( u"Stanza dropped (no route): {0!r}" . format ( stanza ) )
6091	def cache ( func ) : def wrapper ( instance : GeometryProfile , grid : np . ndarray , * args , ** kwargs ) : if not hasattr ( instance , "cache" ) : instance . cache = { } key = ( func . __name__ , grid . tobytes ( ) ) if key not in instance . cache : instance . cache [ key ] = func ( instance , grid ) return instance . cache [ key ] return wrapper
13842	def arkt_to_unixt ( ark_timestamp ) : res = datetime . datetime ( 2017 , 3 , 21 , 15 , 55 , 44 ) + datetime . timedelta ( seconds = ark_timestamp ) return res . timestamp ( )
5834	def create_ml_configuration ( self , search_template , extract_as_keys , dataset_ids ) : data = { "search_template" : search_template , "extract_as_keys" : extract_as_keys } failure_message = "ML Configuration creation failed" config_job_id = self . _get_success_json ( self . _post_json ( 'v1/descriptors/builders/simple/default/trigger' , data , failure_message = failure_message ) ) [ 'data' ] [ 'result' ] [ 'uid' ] while True : config_status = self . __get_ml_configuration_status ( config_job_id ) print ( 'Configuration status: ' , config_status ) if config_status [ 'status' ] == 'Finished' : ml_config = self . __convert_response_to_configuration ( config_status [ 'result' ] , dataset_ids ) return ml_config time . sleep ( 5 )
659	def populationStability ( vectors , numSamples = None ) : numVectors = len ( vectors ) if numSamples is None : numSamples = numVectors - 1 countOn = range ( numVectors - 1 ) else : countOn = numpy . random . randint ( 0 , numVectors - 1 , numSamples ) sigmap = 0.0 for i in countOn : match = checkMatch ( vectors [ i ] , vectors [ i + 1 ] , sparse = False ) if match [ 1 ] != 0 : sigmap += float ( match [ 0 ] ) / match [ 1 ] return sigmap / numSamples
8235	def split_complementary ( clr ) : clr = color ( clr ) colors = colorlist ( clr ) clr = clr . complement colors . append ( clr . rotate_ryb ( - 30 ) . lighten ( 0.1 ) ) colors . append ( clr . rotate_ryb ( 30 ) . lighten ( 0.1 ) ) return colors
10977	def members ( group_id ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) q = request . args . get ( 'q' , '' ) s = request . args . get ( 's' , '' ) group = Group . query . get_or_404 ( group_id ) if group . can_see_members ( current_user ) : members = Membership . query_by_group ( group_id , with_invitations = True ) if q : members = Membership . search ( members , q ) if s : members = Membership . order ( members , Membership . state , s ) members = members . paginate ( page , per_page = per_page ) return render_template ( "invenio_groups/members.html" , group = group , members = members , page = page , per_page = per_page , q = q , s = s , ) flash ( _ ( 'You are not allowed to see members of this group %(group_name)s.' , group_name = group . name ) , 'error' ) return redirect ( url_for ( '.index' ) )
8651	def get_jobs ( session , job_ids , seo_details , lang ) : get_jobs_data = { 'jobs[]' : job_ids , 'seo_details' : seo_details , 'lang' : lang , } response = make_get_request ( session , 'jobs' , params_data = get_jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise JobsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
571	def runModelGivenBaseAndParams ( modelID , jobID , baseDescription , params , predictedField , reportKeys , optimizeKey , jobsDAO , modelCheckpointGUID , logLevel = None , predictionCacheMaxRecords = None ) : from nupic . swarming . ModelRunner import OPFModelRunner logger = logging . getLogger ( 'com.numenta.nupic.hypersearch.utils' ) experimentDir = tempfile . mkdtemp ( ) try : logger . info ( "Using experiment directory: %s" % ( experimentDir ) ) paramsFilePath = os . path . join ( experimentDir , 'description.py' ) paramsFile = open ( paramsFilePath , 'wb' ) paramsFile . write ( _paramsFileHead ( ) ) items = params . items ( ) items . sort ( ) for ( key , value ) in items : quotedKey = _quoteAndEscape ( key ) if isinstance ( value , basestring ) : paramsFile . write ( " %s : '%s',\n" % ( quotedKey , value ) ) else : paramsFile . write ( " %s : %s,\n" % ( quotedKey , value ) ) paramsFile . write ( _paramsFileTail ( ) ) paramsFile . close ( ) baseParamsFile = open ( os . path . join ( experimentDir , 'base.py' ) , 'wb' ) baseParamsFile . write ( baseDescription ) baseParamsFile . close ( ) fd = open ( paramsFilePath ) expDescription = fd . read ( ) fd . close ( ) jobsDAO . modelSetFields ( modelID , { 'genDescription' : expDescription } ) try : runner = OPFModelRunner ( modelID = modelID , jobID = jobID , predictedField = predictedField , experimentDir = experimentDir , reportKeyPatterns = reportKeys , optimizeKeyPattern = optimizeKey , jobsDAO = jobsDAO , modelCheckpointGUID = modelCheckpointGUID , logLevel = logLevel , predictionCacheMaxRecords = predictionCacheMaxRecords ) signal . signal ( signal . SIGINT , runner . handleWarningSignal ) ( completionReason , completionMsg ) = runner . run ( ) except InvalidConnectionException : raise except Exception , e : ( completionReason , completionMsg ) = _handleModelRunnerException ( jobID , modelID , jobsDAO , experimentDir , logger , e ) finally : shutil . rmtree ( experimentDir ) signal . signal ( signal . SIGINT , signal . default_int_handler ) return ( completionReason , completionMsg )
11940	def mark_read ( user , message ) : BackendClass = stored_messages_settings . STORAGE_BACKEND backend = BackendClass ( ) backend . inbox_delete ( user , message )
8064	def precmd ( self , line ) : args = shlex . split ( line or "" ) if args and 'cookie=' in args [ - 1 ] : cookie_index = line . index ( 'cookie=' ) cookie = line [ cookie_index + 7 : ] line = line [ : cookie_index ] . strip ( ) self . cookie = cookie if line . startswith ( '#' ) : return '' elif '=' in line : cmdname = line . partition ( " " ) [ 0 ] if hasattr ( self , "do_%s" % cmdname ) : return line if not line . startswith ( "set " ) : return "set " + line else : return line if len ( args ) and args [ 0 ] in self . shortcuts : return "%s %s" % ( self . shortcuts [ args [ 0 ] ] , " " . join ( args [ 1 : ] ) ) else : return line
6615	def receive_all ( self ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return return self . dropbox . receive ( )
7305	def process_post_form ( self , success_message = None ) : if not hasattr ( self , 'document' ) or self . document is None : self . document = self . document_type ( ) self . form = MongoModelForm ( model = self . document_type , instance = self . document , form_post_data = self . request . POST ) . get_form ( ) self . form . is_bound = True if self . form . is_valid ( ) : self . document_map_dict = MongoModelForm ( model = self . document_type ) . create_document_dictionary ( self . document_type ) self . new_document = self . document_type self . embedded_list_docs = { } if self . new_document is None : messages . error ( self . request , u"Failed to save document" ) else : self . new_document = self . new_document ( ) for form_key in self . form . cleaned_data . keys ( ) : if form_key == 'id' and hasattr ( self , 'document' ) : self . new_document . id = self . document . id continue self . process_document ( self . new_document , form_key , None ) self . new_document . save ( ) if success_message : messages . success ( self . request , success_message ) return self . form
13653	def Text ( name , encoding = None ) : def _match ( request , value ) : return name , query . Text ( value , encoding = contentEncoding ( request . requestHeaders , encoding ) ) return _match
5353	def retain_identities ( self , retention_time ) : enrich_es = self . conf [ 'es_enrichment' ] [ 'url' ] sortinghat_db = self . db current_data_source = self . get_backend ( self . backend_section ) active_data_sources = self . config . get_active_data_sources ( ) if retention_time is None : logger . debug ( "[identities retention] Retention policy disabled, no identities will be deleted." ) return if retention_time <= 0 : logger . debug ( "[identities retention] Retention time must be greater than 0." ) return retain_identities ( retention_time , enrich_es , sortinghat_db , current_data_source , active_data_sources )
9645	def _get_detail_value ( var , attr ) : value = getattr ( var , attr ) kls = getattr ( getattr ( value , '__class__' , '' ) , '__name__' , '' ) if kls in ( 'ManyRelatedManager' , 'RelatedManager' , 'EmptyManager' ) : return kls if callable ( value ) : return 'routine' return value
11277	def run_program ( prog_list , debug , shell ) : try : if not shell : process = Popen ( prog_list , stdout = PIPE , stderr = PIPE ) stdout , stderr = process . communicate ( ) retcode = process . returncode if debug >= 1 : print ( "Program : " , " " . join ( prog_list ) ) print ( "Return Code: " , retcode ) print ( "Stdout: " , stdout ) print ( "Stderr: " , stderr ) return bool ( retcode ) else : command = " " . join ( prog_list ) os . system ( command ) return True except : return False
4395	def adsSyncReadByNameEx ( port , address , data_name , data_type , return_ctypes = False ) : handle = adsSyncReadWriteReqEx2 ( port , address , ADSIGRP_SYM_HNDBYNAME , 0x0 , PLCTYPE_UDINT , data_name , PLCTYPE_STRING , ) value = adsSyncReadReqEx2 ( port , address , ADSIGRP_SYM_VALBYHND , handle , data_type , return_ctypes ) adsSyncWriteReqEx ( port , address , ADSIGRP_SYM_RELEASEHND , 0 , handle , PLCTYPE_UDINT ) return value
7443	def _step1func ( self , force , ipyclient ) : sfiles = self . paramsdict [ "sorted_fastq_path" ] rfiles = self . paramsdict [ "raw_fastq_path" ] if sfiles and rfiles : raise IPyradWarningExit ( NOT_TWO_PATHS ) if not ( sfiles or rfiles ) : raise IPyradWarningExit ( NO_SEQ_PATH_FOUND ) if self . _headers : if sfiles : print ( "\n{}Step 1: Loading sorted fastq data to Samples" . format ( self . _spacer ) ) else : print ( "\n{}Step 1: Demultiplexing fastq data to Samples" . format ( self . _spacer ) ) if self . samples : if not force : print ( SAMPLES_EXIST . format ( len ( self . samples ) , self . name ) ) else : if glob . glob ( sfiles ) : self . _link_fastqs ( ipyclient = ipyclient , force = force ) else : assemble . demultiplex . run2 ( self , ipyclient , force ) else : if glob . glob ( sfiles ) : self . _link_fastqs ( ipyclient = ipyclient ) else : assemble . demultiplex . run2 ( self , ipyclient , force )
10968	def setup_passthroughs ( self ) : self . _nopickle = [ ] for c in self . comps : funcs = inspect . getmembers ( c , predicate = inspect . ismethod ) for func in funcs : if func [ 0 ] . startswith ( 'param_' ) : setattr ( self , func [ 0 ] , func [ 1 ] ) self . _nopickle . append ( func [ 0 ] ) funcs = c . exports ( ) for func in funcs : newname = c . category + '_' + func . __func__ . __name__ setattr ( self , newname , func ) self . _nopickle . append ( newname )
8090	def textheight ( self , txt , width = None ) : w = width return self . textmetrics ( txt , width = w ) [ 1 ]
4454	def group_by ( self , fields , * reducers ) : group = Group ( fields , reducers ) self . _groups . append ( group ) return self
7910	def __presence_unavailable ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : return False rs . process_unavailable_presence ( MucPresence ( stanza ) ) return True
7460	def save_json2 ( data ) : datadict = OrderedDict ( [ ( "outfiles" , data . __dict__ [ "outfiles" ] ) , ( "stats_files" , dict ( data . __dict__ [ "stats_files" ] ) ) , ( "stats_dfs" , data . __dict__ [ "stats_dfs" ] ) ] )
1784	def CMP ( cpu , src1 , src2 ) : arg0 = src1 . read ( ) arg1 = Operators . SEXTEND ( src2 . read ( ) , src2 . size , src1 . size ) cpu . _calculate_CMP_flags ( src1 . size , arg0 - arg1 , arg0 , arg1 )
12408	def _merge ( options , name , bases , default = None ) : result = None for base in bases : if base is None : continue value = getattr ( base , name , None ) if value is None : continue result = utils . cons ( result , value ) value = options . get ( name ) if value is not None : result = utils . cons ( result , value ) return result or default
12032	def average ( self , t1 = 0 , t2 = None , setsweep = False ) : if setsweep : self . setsweep ( setsweep ) if t2 is None or t2 > self . sweepLength : t2 = self . sweepLength self . log . debug ( "resetting t2 to [%f]" , t2 ) t1 = max ( t1 , 0 ) if t1 > t2 : self . log . error ( "t1 cannot be larger than t2" ) return False I1 , I2 = int ( t1 * self . pointsPerSec ) , int ( t2 * self . pointsPerSec ) if I1 == I2 : return np . nan return np . average ( self . sweepY [ I1 : I2 ] )
6967	def smooth_magseries_gaussfilt ( mags , windowsize , windowfwhm = 7 ) : convkernel = Gaussian1DKernel ( windowfwhm , x_size = windowsize ) smoothed = convolve ( mags , convkernel , boundary = 'extend' ) return smoothed
7561	def get_sampled ( data , totn , node ) : names = sorted ( totn ) cdict = { name : idx for idx , name in enumerate ( names ) } if ( node . is_leaf ( ) or node . is_root ( ) ) : return 0 else : if len ( node . children ) > 2 : down_r = node . children [ 0 ] down_l = node . children [ 1 ] for child in node . children [ 2 : ] : down_l += child else : down_r , down_l = node . children lendr = set ( cdict [ i ] for i in down_r . get_leaf_names ( ) ) lendl = set ( cdict [ i ] for i in down_l . get_leaf_names ( ) ) up_r = node . get_sisters ( ) [ 0 ] lenur = set ( cdict [ i ] for i in up_r . get_leaf_names ( ) ) lenul = set ( cdict [ i ] for i in totn ) - set . union ( lendr , lendl , lenur ) idx = 0 sampled = 0 with h5py . File ( data . database . output , 'r' ) as io5 : end = io5 [ "quartets" ] . shape [ 0 ] while 1 : if idx >= end : break qrts = io5 [ "quartets" ] [ idx : idx + data . _chunksize ] for qrt in qrts : sqrt = set ( qrt ) if all ( [ sqrt . intersection ( i ) for i in [ lendr , lendl , lenur , lenul ] ] ) : sampled += 1 idx += data . _chunksize return sampled
6315	def _add_resource_descriptions_to_pools ( self , meta_list ) : if not meta_list : return for meta in meta_list : getattr ( resources , meta . resource_type ) . add ( meta )
8901	def authenticate_credentials ( self , userargs , password , request = None ) : credentials = { 'password' : password } if "=" not in userargs : credentials [ get_user_model ( ) . USERNAME_FIELD ] = userargs else : for arg in userargs . split ( "&" ) : key , val = arg . split ( "=" ) credentials [ key ] = val user = authenticate ( ** credentials ) if user is None : raise exceptions . AuthenticationFailed ( 'Invalid credentials.' ) if not user . is_active : raise exceptions . AuthenticationFailed ( 'User inactive or deleted.' ) return ( user , None )
5470	def _prepare_summary_table ( rows ) : if not rows : return [ ] key_field = 'job-name' if key_field not in rows [ 0 ] : key_field = 'job-id' grouped = collections . defaultdict ( lambda : collections . defaultdict ( lambda : [ ] ) ) for row in rows : grouped [ row . get ( key_field , '' ) ] [ row . get ( 'status' , '' ) ] += [ row ] new_rows = [ ] for job_key in sorted ( grouped . keys ( ) ) : group = grouped . get ( job_key , None ) canonical_status = [ 'RUNNING' , 'SUCCESS' , 'FAILURE' , 'CANCEL' ] for status in canonical_status + sorted ( group . keys ( ) ) : if status not in group : continue task_count = len ( group [ status ] ) del group [ status ] if task_count : summary_row = collections . OrderedDict ( ) summary_row [ key_field ] = job_key summary_row [ 'status' ] = status summary_row [ 'task-count' ] = task_count new_rows . append ( summary_row ) return new_rows
4761	def assert_that ( val , description = '' ) : global _soft_ctx if _soft_ctx : return AssertionBuilder ( val , description , 'soft' ) return AssertionBuilder ( val , description )
2490	def create_file_node ( self , doc_file ) : file_node = URIRef ( 'http://www.spdx.org/files#{id}' . format ( id = str ( doc_file . spdx_id ) ) ) type_triple = ( file_node , RDF . type , self . spdx_namespace . File ) self . graph . add ( type_triple ) name_triple = ( file_node , self . spdx_namespace . fileName , Literal ( doc_file . name ) ) self . graph . add ( name_triple ) if doc_file . has_optional_field ( 'comment' ) : comment_triple = ( file_node , RDFS . comment , Literal ( doc_file . comment ) ) self . graph . add ( comment_triple ) if doc_file . has_optional_field ( 'type' ) : ftype = self . spdx_namespace [ self . FILE_TYPES [ doc_file . type ] ] ftype_triple = ( file_node , self . spdx_namespace . fileType , ftype ) self . graph . add ( ftype_triple ) self . graph . add ( ( file_node , self . spdx_namespace . checksum , self . create_checksum_node ( doc_file . chk_sum ) ) ) conc_lic_node = self . license_or_special ( doc_file . conc_lics ) conc_lic_triple = ( file_node , self . spdx_namespace . licenseConcluded , conc_lic_node ) self . graph . add ( conc_lic_triple ) license_info_nodes = map ( self . license_or_special , doc_file . licenses_in_file ) for lic in license_info_nodes : triple = ( file_node , self . spdx_namespace . licenseInfoInFile , lic ) self . graph . add ( triple ) if doc_file . has_optional_field ( 'license_comment' ) : comment_triple = ( file_node , self . spdx_namespace . licenseComments , Literal ( doc_file . license_comment ) ) self . graph . add ( comment_triple ) cr_text_node = self . to_special_value ( doc_file . copyright ) cr_text_triple = ( file_node , self . spdx_namespace . copyrightText , cr_text_node ) self . graph . add ( cr_text_triple ) if doc_file . has_optional_field ( 'notice' ) : notice_triple = ( file_node , self . spdx_namespace . noticeText , doc_file . notice ) self . graph . add ( notice_triple ) contrib_nodes = map ( lambda c : Literal ( c ) , doc_file . contributors ) contrib_triples = [ ( file_node , self . spdx_namespace . fileContributor , node ) for node in contrib_nodes ] for triple in contrib_triples : self . graph . add ( triple ) return file_node
1806	def SETB ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . CF , 1 , 0 ) )
7489	def importvcf ( vcffile , locifile ) : try : with open ( invcffile , 'r' ) as invcf : for line in invcf : if line . split ( ) [ 0 ] == "#CHROM" : names_col = line . split ( ) . index ( "FORMAT" ) + 1 names = line . split ( ) [ names_col : ] LOGGER . debug ( "Got names - %s" , names ) break print ( "wat" ) except Exception : print ( "wat" )
9009	def next_instruction_in_row ( self ) : index = self . index_in_row + 1 if index >= len ( self . row_instructions ) : return None return self . row_instructions [ index ]
4687	def decrypt ( self , message ) : if not message : return None try : memo_wif = self . blockchain . wallet . getPrivateKeyForPublicKey ( message [ "to" ] ) pubkey = message [ "from" ] except KeyNotFound : try : memo_wif = self . blockchain . wallet . getPrivateKeyForPublicKey ( message [ "from" ] ) pubkey = message [ "to" ] except KeyNotFound : raise MissingKeyError ( "None of the required memo keys are installed!" "Need any of {}" . format ( [ message [ "to" ] , message [ "from" ] ] ) ) if not hasattr ( self , "chain_prefix" ) : self . chain_prefix = self . blockchain . prefix return memo . decode_memo ( self . privatekey_class ( memo_wif ) , self . publickey_class ( pubkey , prefix = self . chain_prefix ) , message . get ( "nonce" ) , message . get ( "message" ) , )
8313	def draw_table ( table , x , y , w , padding = 5 ) : try : from web import _ctx except : pass f = _ctx . fill ( ) _ctx . stroke ( f ) h = _ctx . textheight ( " " ) + padding * 2 row_y = y if table . title != "" : _ctx . fill ( f ) _ctx . rect ( x , row_y , w , h ) _ctx . fill ( 1 ) _ctx . text ( table . title , x + padding , row_y + _ctx . fontsize ( ) + padding ) row_y += h rowspans = [ 1 for i in range ( 10 ) ] previous_cell_w = 0 for row in table : cell_x = x cell_w = 1.0 * w cell_w -= previous_cell_w * len ( [ n for n in rowspans if n > 1 ] ) cell_w /= len ( row ) cell_h = 0 for cell in row : this_h = _ctx . textheight ( cell , width = cell_w - padding * 2 ) + padding * 2 cell_h = max ( cell_h , this_h ) i = 0 for cell in row : if rowspans [ i ] > 1 : rowspans [ i ] -= 1 cell_x += previous_cell_w i += 1 m = re . search ( "rowspan=\"(.*?)\"" , cell . properties ) if m : rowspan = int ( m . group ( 1 ) ) rowspans [ i ] = rowspan else : rowspan = 1 _ctx . fill ( f ) _ctx . text ( cell , cell_x + padding , row_y + _ctx . fontsize ( ) + padding , cell_w - padding * 2 ) _ctx . line ( cell_x , row_y , cell_x + cell_w , row_y ) if cell_x > x : _ctx . nofill ( ) _ctx . line ( cell_x , row_y , cell_x , row_y + cell_h ) cell_x += cell_w i += 1 row_y += cell_h previous_cell_w = cell_w _ctx . nofill ( ) _ctx . rect ( x , y , w , row_y - y )
8561	def get_loadbalancer ( self , datacenter_id , loadbalancer_id ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) ) return response
11295	def make_request_data ( self , zipcode , city , state ) : data = { 'key' : self . api_key , 'postalcode' : str ( zipcode ) , 'city' : city , 'state' : state } data = ZipTaxClient . _clean_request_data ( data ) return data
3061	def positional ( max_positional_args ) : def positional_decorator ( wrapped ) : @ functools . wraps ( wrapped ) def positional_wrapper ( * args , ** kwargs ) : if len ( args ) > max_positional_args : plural_s = '' if max_positional_args != 1 : plural_s = 's' message = ( '{function}() takes at most {args_max} positional ' 'argument{plural} ({args_given} given)' . format ( function = wrapped . __name__ , args_max = max_positional_args , args_given = len ( args ) , plural = plural_s ) ) if positional_parameters_enforcement == POSITIONAL_EXCEPTION : raise TypeError ( message ) elif positional_parameters_enforcement == POSITIONAL_WARNING : logger . warning ( message ) return wrapped ( * args , ** kwargs ) return positional_wrapper if isinstance ( max_positional_args , six . integer_types ) : return positional_decorator else : args , _ , _ , defaults = inspect . getargspec ( max_positional_args ) return positional ( len ( args ) - len ( defaults ) ) ( max_positional_args )
4856	def _delete_transmissions ( self , content_metadata_item_ids ) : ContentMetadataItemTransmission = apps . get_model ( 'integrated_channel' , 'ContentMetadataItemTransmission' ) ContentMetadataItemTransmission . objects . filter ( enterprise_customer = self . enterprise_configuration . enterprise_customer , integrated_channel_code = self . enterprise_configuration . channel_code ( ) , content_id__in = content_metadata_item_ids ) . delete ( )
3954	def remove_exited_dusty_containers ( ) : client = get_docker_client ( ) exited_containers = get_exited_dusty_containers ( ) removed_containers = [ ] for container in exited_containers : log_to_client ( "Removing container {}" . format ( container [ 'Names' ] [ 0 ] ) ) try : client . remove_container ( container [ 'Id' ] , v = True ) removed_containers . append ( container ) except Exception as e : log_to_client ( e . message or str ( e ) ) return removed_containers
9254	def issue_line_with_user ( self , line , issue ) : if not issue . get ( "pull_request" ) or not self . options . author : return line if not issue . get ( "user" ) : line += u" (Null user)" elif self . options . username_as_tag : line += u" (@{0})" . format ( issue [ "user" ] [ "login" ] ) else : line += u" ([{0}]({1}))" . format ( issue [ "user" ] [ "login" ] , issue [ "user" ] [ "html_url" ] ) return line
12733	def move_next_to ( self , body_a , body_b , offset_a , offset_b ) : ba = self . get_body ( body_a ) bb = self . get_body ( body_b ) if ba is None : return bb . relative_offset_to_world ( offset_b ) if bb is None : return ba . relative_offset_to_world ( offset_a ) anchor = ba . relative_offset_to_world ( offset_a ) offset = bb . relative_offset_to_world ( offset_b ) bb . position = bb . position + anchor - offset return anchor
82	def SaltAndPepper ( p = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return ReplaceElementwise ( mask = p , replacement = iap . Beta ( 0.5 , 0.5 ) * 255 , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
5748	def history ( self , ip , days_limit = None ) : all_dates = sorted ( self . routing_db . smembers ( 'imported_dates' ) , reverse = True ) if days_limit is not None : all_dates = all_dates [ : days_limit ] return [ self . date_asn_block ( ip , date ) for date in all_dates ]
11604	def check_ranges ( cls , ranges , length ) : result = [ ] for start , end in ranges : if isinstance ( start , int ) or isinstance ( end , int ) : if isinstance ( start , int ) and not ( 0 <= start < length ) : continue elif isinstance ( start , int ) and isinstance ( end , int ) and not ( start <= end ) : continue elif start is None and end == 0 : continue result . append ( ( start , end ) ) return result
1177	def search ( self , string , pos = 0 , endpos = sys . maxint ) : state = _State ( string , pos , endpos , self . flags ) if state . search ( self . _code ) : return SRE_Match ( self , state ) else : return None
9532	def dumps ( obj , key = None , salt = 'django.core.signing' , serializer = JSONSerializer , compress = False ) : data = serializer ( ) . dumps ( obj ) is_compressed = False if compress : compressed = zlib . compress ( data ) if len ( compressed ) < ( len ( data ) - 1 ) : data = compressed is_compressed = True base64d = b64_encode ( data ) if is_compressed : base64d = b'.' + base64d return TimestampSigner ( key , salt = salt ) . sign ( base64d )
3208	def _reformat_policy ( policy ) : policy_name = policy [ 'PolicyName' ] ret = { } ret [ 'type' ] = policy [ 'PolicyTypeName' ] attrs = policy [ 'PolicyAttributeDescriptions' ] if ret [ 'type' ] != 'SSLNegotiationPolicyType' : return policy_name , ret attributes = dict ( ) for attr in attrs : attributes [ attr [ 'AttributeName' ] ] = attr [ 'AttributeValue' ] ret [ 'protocols' ] = dict ( ) ret [ 'protocols' ] [ 'sslv2' ] = bool ( attributes . get ( 'Protocol-SSLv2' ) ) ret [ 'protocols' ] [ 'sslv3' ] = bool ( attributes . get ( 'Protocol-SSLv3' ) ) ret [ 'protocols' ] [ 'tlsv1' ] = bool ( attributes . get ( 'Protocol-TLSv1' ) ) ret [ 'protocols' ] [ 'tlsv1_1' ] = bool ( attributes . get ( 'Protocol-TLSv1.1' ) ) ret [ 'protocols' ] [ 'tlsv1_2' ] = bool ( attributes . get ( 'Protocol-TLSv1.2' ) ) ret [ 'server_defined_cipher_order' ] = bool ( attributes . get ( 'Server-Defined-Cipher-Order' ) ) ret [ 'reference_security_policy' ] = attributes . get ( 'Reference-Security-Policy' , None ) non_ciphers = [ 'Server-Defined-Cipher-Order' , 'Protocol-SSLv2' , 'Protocol-SSLv3' , 'Protocol-TLSv1' , 'Protocol-TLSv1.1' , 'Protocol-TLSv1.2' , 'Reference-Security-Policy' ] ciphers = [ ] for cipher in attributes : if attributes [ cipher ] == 'true' and cipher not in non_ciphers : ciphers . append ( cipher ) ciphers . sort ( ) ret [ 'supported_ciphers' ] = ciphers return policy_name , ret
12421	def dump ( obj , fp , startindex = 1 , separator = DEFAULT , index_separator = DEFAULT ) : if startindex < 0 : raise ValueError ( 'startindex must be non-negative, but was {}' . format ( startindex ) ) try : firstkey = next ( iter ( obj . keys ( ) ) ) except StopIteration : return if isinstance ( firstkey , six . text_type ) : converter = six . u else : converter = six . b default_separator = converter ( '|' ) default_index_separator = converter ( '_' ) newline = converter ( '\n' ) if separator is DEFAULT : separator = default_separator if index_separator is DEFAULT : index_separator = default_index_separator for key , value in six . iteritems ( obj ) : if isinstance ( value , ( list , tuple , set ) ) : for index , item in enumerate ( value , start = startindex ) : fp . write ( key ) fp . write ( index_separator ) fp . write ( converter ( str ( index ) ) ) fp . write ( separator ) fp . write ( item ) fp . write ( newline ) else : fp . write ( key ) fp . write ( separator ) fp . write ( value ) fp . write ( newline )
6074	def mass_within_ellipse_in_units ( self , major_axis , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : if self . has_mass_profile : return sum ( map ( lambda p : p . mass_within_ellipse_in_units ( major_axis = major_axis , unit_mass = unit_mass , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) , self . mass_profiles ) ) else : return None
397	def cross_entropy ( output , target , name = None ) : if name is None : raise Exception ( "Please give a unique name to tl.cost.cross_entropy for TF1.0+" ) return tf . reduce_mean ( tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = target , logits = output ) , name = name )
8983	def get_instruction_id ( self , instruction_or_id ) : if isinstance ( instruction_or_id , tuple ) : return _InstructionId ( instruction_or_id ) return _InstructionId ( instruction_or_id . type , instruction_or_id . hex_color )
12463	def print_error ( message , wrap = True ) : if wrap : message = 'ERROR: {0}. Exit...' . format ( message . rstrip ( '.' ) ) colorizer = ( _color_wrap ( colorama . Fore . RED ) if colorama else lambda message : message ) return print ( colorizer ( message ) , file = sys . stderr )
758	def generateRandomInput ( numRecords , elemSize = 400 , numSet = 42 ) : inputs = [ ] for _ in xrange ( numRecords ) : input = np . zeros ( elemSize , dtype = realDType ) for _ in range ( 0 , numSet ) : ind = np . random . random_integers ( 0 , elemSize - 1 , 1 ) [ 0 ] input [ ind ] = 1 while abs ( input . sum ( ) - numSet ) > 0.1 : ind = np . random . random_integers ( 0 , elemSize - 1 , 1 ) [ 0 ] input [ ind ] = 1 inputs . append ( input ) return inputs
4151	def power ( self ) : r if self . scale_by_freq == False : return sum ( self . psd ) * len ( self . psd ) else : return sum ( self . psd ) * self . df / ( 2. * numpy . pi )
4650	def appendSigner ( self , accounts , permission ) : assert permission in self . permission_types , "Invalid permission" if self . blockchain . wallet . locked ( ) : raise WalletLocked ( ) if not isinstance ( accounts , ( list , tuple , set ) ) : accounts = [ accounts ] for account in accounts : if account not in self . signing_accounts : if isinstance ( account , self . publickey_class ) : self . appendWif ( self . blockchain . wallet . getPrivateKeyForPublicKey ( str ( account ) ) ) else : accountObj = self . account_class ( account , blockchain_instance = self . blockchain ) required_treshold = accountObj [ permission ] [ "weight_threshold" ] keys = self . _fetchkeys ( accountObj , permission , required_treshold = required_treshold ) if not keys and permission != "owner" : keys . extend ( self . _fetchkeys ( accountObj , "owner" , required_treshold = required_treshold ) ) for x in keys : self . appendWif ( x [ 0 ] ) self . signing_accounts . append ( account )
10467	def getAnyAppWithWindow ( cls ) : apps = cls . _getRunningApps ( ) for app in apps : pid = app . processIdentifier ( ) ref = cls . getAppRefByPid ( pid ) if hasattr ( ref , 'windows' ) and len ( ref . windows ( ) ) > 0 : return ref raise ValueError ( 'No GUI application found.' )
723	def getTerminationCallbacks ( self , terminationFunc ) : activities = [ None ] * len ( ModelTerminator . _MILESTONES ) for index , ( iteration , _ ) in enumerate ( ModelTerminator . _MILESTONES ) : cb = functools . partial ( terminationFunc , index = index ) activities [ index ] = PeriodicActivityRequest ( repeating = False , period = iteration , cb = cb )
3717	def economic_status ( CASRN , Method = None , AvailableMethods = False ) : load_economic_data ( ) CASi = CAS2int ( CASRN ) def list_methods ( ) : methods = [ ] methods . append ( 'Combined' ) if CASRN in _EPACDRDict : methods . append ( EPACDR ) if CASRN in _ECHATonnageDict : methods . append ( ECHA ) if CASi in HPV_data . index : methods . append ( OECD ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == EPACDR : status = 'US public: ' + str ( _EPACDRDict [ CASRN ] ) elif Method == ECHA : status = _ECHATonnageDict [ CASRN ] elif Method == OECD : status = 'OECD HPV Chemicals' elif Method == 'Combined' : status = [ ] if CASRN in _EPACDRDict : status += [ 'US public: ' + str ( _EPACDRDict [ CASRN ] ) ] if CASRN in _ECHATonnageDict : status += _ECHATonnageDict [ CASRN ] if CASi in HPV_data . index : status += [ 'OECD HPV Chemicals' ] elif Method == NONE : status = None else : raise Exception ( 'Failure in in function' ) return status
9372	def read_stream ( schema , stream , * , buffer_size = io . DEFAULT_BUFFER_SIZE ) : reader = _lancaster . Reader ( schema ) buf = stream . read ( buffer_size ) remainder = b'' while len ( buf ) > 0 : values , n = reader . read_seq ( buf ) yield from values remainder = buf [ n : ] buf = stream . read ( buffer_size ) if len ( buf ) > 0 and len ( remainder ) > 0 : ba = bytearray ( ) ba . extend ( remainder ) ba . extend ( buf ) buf = memoryview ( ba ) . tobytes ( ) if len ( remainder ) > 0 : raise EOFError ( '{} bytes remaining but could not continue reading ' 'from stream' . format ( len ( remainder ) ) )
108	def draw_grid ( images , rows = None , cols = None ) : nb_images = len ( images ) do_assert ( nb_images > 0 ) if is_np_array ( images ) : do_assert ( images . ndim == 4 ) else : do_assert ( is_iterable ( images ) and is_np_array ( images [ 0 ] ) and images [ 0 ] . ndim == 3 ) dts = [ image . dtype . name for image in images ] nb_dtypes = len ( set ( dts ) ) do_assert ( nb_dtypes == 1 , ( "All images provided to draw_grid() must have the same dtype, " + "found %d dtypes (%s)" ) % ( nb_dtypes , ", " . join ( dts ) ) ) cell_height = max ( [ image . shape [ 0 ] for image in images ] ) cell_width = max ( [ image . shape [ 1 ] for image in images ] ) channels = set ( [ image . shape [ 2 ] for image in images ] ) do_assert ( len ( channels ) == 1 , "All images are expected to have the same number of channels, " + "but got channel set %s with length %d instead." % ( str ( channels ) , len ( channels ) ) ) nb_channels = list ( channels ) [ 0 ] if rows is None and cols is None : rows = cols = int ( math . ceil ( math . sqrt ( nb_images ) ) ) elif rows is not None : cols = int ( math . ceil ( nb_images / rows ) ) elif cols is not None : rows = int ( math . ceil ( nb_images / cols ) ) do_assert ( rows * cols >= nb_images ) width = cell_width * cols height = cell_height * rows dt = images . dtype if is_np_array ( images ) else images [ 0 ] . dtype grid = np . zeros ( ( height , width , nb_channels ) , dtype = dt ) cell_idx = 0 for row_idx in sm . xrange ( rows ) : for col_idx in sm . xrange ( cols ) : if cell_idx < nb_images : image = images [ cell_idx ] cell_y1 = cell_height * row_idx cell_y2 = cell_y1 + image . shape [ 0 ] cell_x1 = cell_width * col_idx cell_x2 = cell_x1 + image . shape [ 1 ] grid [ cell_y1 : cell_y2 , cell_x1 : cell_x2 , : ] = image cell_idx += 1 return grid
2730	def get_object ( cls , api_token , domain_name ) : domain = cls ( token = api_token , name = domain_name ) domain . load ( ) return domain
10899	def update ( self , value = 0 ) : self . _deltas . append ( time . time ( ) ) self . value = value self . _percent = 100.0 * self . value / self . num if self . bar : self . _bars = self . _bar_symbol * int ( np . round ( self . _percent / 100. * self . _barsize ) ) if ( len ( self . _deltas ) < 2 ) or ( self . _deltas [ - 1 ] - self . _deltas [ - 2 ] ) > 1e-1 : self . _estimate_time ( ) self . _draw ( ) if self . value == self . num : self . end ( )
11203	def picknthweekday ( year , month , dayofweek , hour , minute , whichweek ) : first = datetime . datetime ( year , month , 1 , hour , minute ) weekdayone = first . replace ( day = ( ( dayofweek - first . isoweekday ( ) ) % 7 ) + 1 ) wd = weekdayone + ( ( whichweek - 1 ) * ONEWEEK ) if ( wd . month != month ) : wd -= ONEWEEK return wd
9498	def parse_litezip ( path ) : struct = [ parse_collection ( path ) ] struct . extend ( [ parse_module ( x ) for x in path . iterdir ( ) if x . is_dir ( ) and x . name . startswith ( 'm' ) ] ) return tuple ( sorted ( struct ) )
6785	def fake ( self , components = None ) : self . init ( ) if components : current_tp = self . get_previous_thumbprint ( ) or { } current_tp . update ( self . get_current_thumbprint ( components = components ) or { } ) else : current_tp = self . get_current_thumbprint ( components = components ) or { } tp_text = yaml . dump ( current_tp ) r = self . local_renderer r . upload_content ( content = tp_text , fn = self . manifest_filename ) self . reset_all_satchels ( )
10586	def _create_account_ ( self , name , number , account_type ) : new_acc = GeneralLedgerAccount ( name , None , number , account_type ) self . accounts . append ( new_acc ) return new_acc
11499	def get_community_by_id ( self , community_id , token = None ) : parameters = dict ( ) parameters [ 'id' ] = community_id if token : parameters [ 'token' ] = token response = self . request ( 'midas.community.get' , parameters ) return response
12810	def connectionMade ( self ) : headers = [ "GET %s HTTP/1.1" % ( "/room/%s/live.json" % self . factory . get_stream ( ) . get_room_id ( ) ) ] connection_headers = self . factory . get_stream ( ) . get_connection ( ) . get_headers ( ) for header in connection_headers : headers . append ( "%s: %s" % ( header , connection_headers [ header ] ) ) headers . append ( "Host: streaming.campfirenow.com" ) self . transport . write ( "\r\n" . join ( headers ) + "\r\n\r\n" ) self . factory . get_stream ( ) . set_protocol ( self )
9606	def check_unused_args ( self , used_args , args , kwargs ) : for k , v in kwargs . items ( ) : if k in used_args : self . _used_kwargs . update ( { k : v } ) else : self . _unused_kwargs . update ( { k : v } )
8639	def accept_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'accept' } endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotAcceptedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
2006	def _serialize_int ( value , size = 32 , padding = 0 ) : if size <= 0 or size > 32 : raise ValueError if not isinstance ( value , ( int , BitVec ) ) : raise ValueError if issymbolic ( value ) : buf = ArrayVariable ( index_bits = 256 , index_max = 32 , value_bits = 8 , name = 'temp{}' . format ( uuid . uuid1 ( ) ) ) value = Operators . SEXTEND ( value , value . size , size * 8 ) buf = ArrayProxy ( buf . write_BE ( padding , value , size ) ) else : value = int ( value ) buf = bytearray ( ) for _ in range ( padding ) : buf . append ( 0 ) for position in reversed ( range ( size ) ) : buf . append ( Operators . EXTRACT ( value , position * 8 , 8 ) ) return buf
13349	def launch ( prompt_prefix = None ) : if prompt_prefix : os . environ [ 'PROMPT' ] = prompt ( prompt_prefix ) subprocess . call ( cmd ( ) , env = os . environ . data )
580	def getSpec ( cls ) : spec = { "description" : IdentityRegion . __doc__ , "singleNodeOnly" : True , "inputs" : { "in" : { "description" : "The input vector." , "dataType" : "Real32" , "count" : 0 , "required" : True , "regionLevel" : False , "isDefaultInput" : True , "requireSplitterMap" : False } , } , "outputs" : { "out" : { "description" : "A copy of the input vector." , "dataType" : "Real32" , "count" : 0 , "regionLevel" : True , "isDefaultOutput" : True } , } , "parameters" : { "dataWidth" : { "description" : "Size of inputs" , "accessMode" : "Read" , "dataType" : "UInt32" , "count" : 1 , "constraints" : "" } , } , } return spec
6880	def _parse_csv_header ( header ) : headerlines = header . split ( '\n' ) headerlines = [ x . lstrip ( '# ' ) for x in headerlines ] objectstart = headerlines . index ( 'OBJECT' ) metadatastart = headerlines . index ( 'METADATA' ) camfilterstart = headerlines . index ( 'CAMFILTERS' ) photaperturestart = headerlines . index ( 'PHOTAPERTURES' ) columnstart = headerlines . index ( 'COLUMNS' ) lcstart = headerlines . index ( 'LIGHTCURVE' ) objectinfo = headerlines [ objectstart + 1 : metadatastart - 1 ] metadatainfo = headerlines [ metadatastart + 1 : camfilterstart - 1 ] camfilterinfo = headerlines [ camfilterstart + 1 : photaperturestart - 1 ] photapertureinfo = headerlines [ photaperturestart + 1 : columnstart - 1 ] columninfo = headerlines [ columnstart + 1 : lcstart - 1 ] metadict = { 'objectinfo' : { } } objectinfo = [ x . split ( ';' ) for x in objectinfo ] for elem in objectinfo : for kvelem in elem : key , val = kvelem . split ( ' = ' , 1 ) metadict [ 'objectinfo' ] [ key . strip ( ) ] = ( _smartcast ( val , METAKEYS [ key . strip ( ) ] ) ) metadict [ 'objectid' ] = metadict [ 'objectinfo' ] [ 'objectid' ] [ : ] del metadict [ 'objectinfo' ] [ 'objectid' ] metadatainfo = [ x . split ( ';' ) for x in metadatainfo ] for elem in metadatainfo : for kvelem in elem : try : key , val = kvelem . split ( ' = ' , 1 ) if key . strip ( ) == 'lcbestaperture' : val = json . loads ( val ) if key . strip ( ) in ( 'datarelease' , 'lcversion' ) : val = int ( val ) if key . strip ( ) == 'lastupdated' : val = float ( val ) metadict [ key . strip ( ) ] = val except Exception as e : LOGWARNING ( 'could not understand header element "%s",' ' skipped.' % kvelem ) metadict [ 'filters' ] = [ ] for row in camfilterinfo : filterid , filtername , filterdesc = row . split ( ' - ' ) metadict [ 'filters' ] . append ( ( int ( filterid ) , filtername , filterdesc ) ) metadict [ 'lcapertures' ] = { } for row in photapertureinfo : apnum , appix = row . split ( ' - ' ) appix = float ( appix . rstrip ( ' px' ) ) metadict [ 'lcapertures' ] [ apnum . strip ( ) ] = appix metadict [ 'columns' ] = [ ] for row in columninfo : colnum , colname , coldesc = row . split ( ' - ' ) metadict [ 'columns' ] . append ( colname ) return metadict
392	def keypoint_random_resize ( image , annos , mask = None , zoom_range = ( 0.8 , 1.2 ) ) : height = image . shape [ 0 ] width = image . shape [ 1 ] _min , _max = zoom_range scalew = np . random . uniform ( _min , _max ) scaleh = np . random . uniform ( _min , _max ) neww = int ( width * scalew ) newh = int ( height * scaleh ) dst = cv2 . resize ( image , ( neww , newh ) , interpolation = cv2 . INTER_AREA ) if mask is not None : mask = cv2 . resize ( mask , ( neww , newh ) , interpolation = cv2 . INTER_AREA ) adjust_joint_list = [ ] for joint in annos : adjust_joint = [ ] for point in joint : if point [ 0 ] < - 100 or point [ 1 ] < - 100 : adjust_joint . append ( ( - 1000 , - 1000 ) ) continue adjust_joint . append ( ( int ( point [ 0 ] * scalew + 0.5 ) , int ( point [ 1 ] * scaleh + 0.5 ) ) ) adjust_joint_list . append ( adjust_joint ) if mask is not None : return dst , adjust_joint_list , mask else : return dst , adjust_joint_list , None
5649	def write_gtfs ( gtfs , output ) : output = os . path . abspath ( output ) uuid_str = "tmp_" + str ( uuid . uuid1 ( ) ) if output [ - 4 : ] == '.zip' : zip = True out_basepath = os . path . dirname ( os . path . abspath ( output ) ) if not os . path . exists ( out_basepath ) : raise IOError ( out_basepath + " does not exist, cannot write gtfs as a zip" ) tmp_dir = os . path . join ( out_basepath , str ( uuid_str ) ) else : zip = False out_basepath = output tmp_dir = os . path . join ( out_basepath + "_" + str ( uuid_str ) ) os . makedirs ( tmp_dir , exist_ok = True ) gtfs_table_to_writer = { "agency" : _write_gtfs_agencies , "calendar" : _write_gtfs_calendar , "calendar_dates" : _write_gtfs_calendar_dates , "feed_info" : _write_gtfs_feed_info , "routes" : _write_gtfs_routes , "shapes" : _write_gtfs_shapes , "stops" : _write_gtfs_stops , "stop_times" : _write_gtfs_stop_times , "transfers" : _write_gtfs_transfers , "trips" : _write_gtfs_trips , } for table , writer in gtfs_table_to_writer . items ( ) : fname_to_write = os . path . join ( tmp_dir , table + '.txt' ) print ( fname_to_write ) writer ( gtfs , open ( os . path . join ( tmp_dir , table + '.txt' ) , 'w' ) ) if zip : shutil . make_archive ( output [ : - 4 ] , 'zip' , tmp_dir ) shutil . rmtree ( tmp_dir ) else : print ( "moving " + str ( tmp_dir ) + " to " + out_basepath ) os . rename ( tmp_dir , out_basepath )
2190	def _product_file_hash ( self , product = None ) : if self . hasher is None : return None else : products = self . _rectify_products ( product ) product_file_hash = [ util_hash . hash_file ( p , hasher = self . hasher , base = 'hex' ) for p in products ] return product_file_hash
6770	def install_yum ( self , fn = None , package_name = None , update = 0 , list_only = 0 ) : assert self . genv [ ROLE ] yum_req_fn = fn or self . find_template ( self . genv . yum_requirments_fn ) if not yum_req_fn : return [ ] assert os . path . isfile ( yum_req_fn ) update = int ( update ) if list_only : return [ _ . strip ( ) for _ in open ( yum_req_fn ) . readlines ( ) if _ . strip ( ) and not _ . strip . startswith ( '#' ) and ( not package_name or _ . strip ( ) == package_name ) ] if update : self . sudo_or_dryrun ( 'yum update --assumeyes' ) if package_name : self . sudo_or_dryrun ( 'yum install --assumeyes %s' % package_name ) else : if self . genv . is_local : self . put_or_dryrun ( local_path = yum_req_fn ) yum_req_fn = self . genv . put_remote_fn self . sudo_or_dryrun ( 'yum install --assumeyes $(cat %(yum_req_fn)s)' % yum_req_fn )
508	def match ( self , record ) : for field , meta in self . filterDict . iteritems ( ) : index = meta [ 'index' ] categories = meta [ 'categories' ] for category in categories : if not record : continue if record [ index ] . find ( category ) != - 1 : return True return False
3518	def kiss_insights ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return KissInsightsNode ( )
2197	def log_part ( self ) : self . cap_stdout . seek ( self . _pos ) text = self . cap_stdout . read ( ) self . _pos = self . cap_stdout . tell ( ) self . parts . append ( text ) self . text = text
10346	def merge_namespaces ( input_locations , output_path , namespace_name , namespace_keyword , namespace_domain , author_name , citation_name , namespace_description = None , namespace_species = None , namespace_version = None , namespace_query_url = None , namespace_created = None , author_contact = None , author_copyright = None , citation_description = None , citation_url = None , citation_version = None , citation_date = None , case_sensitive = True , delimiter = '|' , cacheable = True , functions = None , value_prefix = '' , sort_key = None , check_keywords = True ) : results = get_merged_namespace_names ( input_locations , check_keywords = check_keywords ) with open ( output_path , 'w' ) as file : write_namespace ( namespace_name = namespace_name , namespace_keyword = namespace_keyword , namespace_domain = namespace_domain , author_name = author_name , citation_name = citation_name , values = results , namespace_species = namespace_species , namespace_description = namespace_description , namespace_query_url = namespace_query_url , namespace_version = namespace_version , namespace_created = namespace_created , author_contact = author_contact , author_copyright = author_copyright , citation_description = citation_description , citation_url = citation_url , citation_version = citation_version , citation_date = citation_date , case_sensitive = case_sensitive , delimiter = delimiter , cacheable = cacheable , functions = functions , value_prefix = value_prefix , sort_key = sort_key , file = file )
2203	def startfile ( fpath , verbose = True ) : from ubelt import util_cmd if verbose : print ( '[ubelt] startfile("{}")' . format ( fpath ) ) fpath = normpath ( fpath ) if not exists ( fpath ) : raise Exception ( 'Cannot start nonexistant file: %r' % fpath ) if not WIN32 : import pipes fpath = pipes . quote ( fpath ) if LINUX : info = util_cmd . cmd ( ( 'xdg-open' , fpath ) , detach = True , verbose = verbose ) elif DARWIN : info = util_cmd . cmd ( ( 'open' , fpath ) , detach = True , verbose = verbose ) elif WIN32 : os . startfile ( fpath ) info = None else : raise RuntimeError ( 'Unknown Platform' ) if info is not None : if not info [ 'proc' ] : raise Exception ( 'startfile failed' )
12269	def evaluate ( self , repo , spec , args ) : status = [ ] with cd ( repo . rootdir ) : files = spec . get ( 'files' , [ '*' ] ) resource_files = repo . find_matching_files ( files ) files = glob2 . glob ( "**/*" ) disk_files = [ f for f in files if os . path . isfile ( f ) and f != "datapackage.json" ] allfiles = list ( set ( resource_files + disk_files ) ) allfiles . sort ( ) for f in allfiles : if f in resource_files and f in disk_files : r = repo . get_resource ( f ) coded_sha256 = r [ 'sha256' ] computed_sha256 = compute_sha256 ( f ) if computed_sha256 != coded_sha256 : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "Mismatch in checksum on disk and in datapackage.json" } ) else : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'OK' , 'message' : "" } ) elif f in resource_files : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "In datapackage.json but not in repo" } ) else : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "In repo but not in datapackage.json" } ) return status
9395	def _set_scores ( self ) : anom_scores = { } self . _compute_derivatives ( ) derivatives_ema = utils . compute_ema ( self . smoothing_factor , self . derivatives ) for i , ( timestamp , value ) in enumerate ( self . time_series_items ) : anom_scores [ timestamp ] = abs ( self . derivatives [ i ] - derivatives_ema [ i ] ) stdev = numpy . std ( anom_scores . values ( ) ) if stdev : for timestamp in anom_scores . keys ( ) : anom_scores [ timestamp ] /= stdev self . anom_scores = TimeSeries ( self . _denoise_scores ( anom_scores ) )
12554	def sav_to_pandas_rpy2 ( input_file ) : import pandas . rpy . common as com w = com . robj . r ( 'foreign::read.spss("%s", to.data.frame=TRUE)' % input_file ) return com . convert_robj ( w )
11009	def subscribe ( self , event , bet_ids ) : if not self . _subscriptions . get ( event ) : self . _subscriptions [ event ] = set ( ) self . _subscriptions [ event ] = self . _subscriptions [ event ] . union ( bet_ids )
2722	def _perform_action ( self , params , return_dict = True ) : action = self . get_data ( "droplets/%s/actions/" % self . id , type = POST , params = params ) if return_dict : return action else : action = action [ u'action' ] return_action = Action ( token = self . token ) for attr in action . keys ( ) : setattr ( return_action , attr , action [ attr ] ) return return_action
5219	def ref_file ( ticker : str , fld : str , has_date = False , cache = False , ext = 'parq' , ** kwargs ) -> str : data_path = os . environ . get ( assist . BBG_ROOT , '' ) . replace ( '\\' , '/' ) if ( not data_path ) or ( not cache ) : return '' proper_ticker = ticker . replace ( '/' , '_' ) cache_days = kwargs . pop ( 'cache_days' , 10 ) root = f'{data_path}/{ticker.split()[-1]}/{proper_ticker}/{fld}' if len ( kwargs ) > 0 : info = utils . to_str ( kwargs ) [ 1 : - 1 ] . replace ( '|' , '_' ) else : info = 'ovrd=None' if has_date : cur_dt = utils . cur_time ( ) missing = f'{root}/asof={cur_dt}, {info}.{ext}' to_find = re . compile ( rf'{root}/asof=(.*), {info}\.pkl' ) cur_files = list ( filter ( to_find . match , sorted ( files . all_files ( path_name = root , keyword = info , ext = ext ) ) ) ) if len ( cur_files ) > 0 : upd_dt = to_find . match ( cur_files [ - 1 ] ) . group ( 1 ) diff = pd . Timestamp ( 'today' ) - pd . Timestamp ( upd_dt ) if diff >= pd . Timedelta ( days = cache_days ) : return missing return sorted ( cur_files ) [ - 1 ] else : return missing else : return f'{root}/{info}.{ext}'
2903	def ref ( function , callback = None ) : try : function . __func__ except AttributeError : return _WeakMethodFree ( function , callback ) return _WeakMethodBound ( function , callback )
9076	def begin ( self ) : self . connect ( self . host , self . port ) if self . user : self . starttls ( ) self . login ( self . user , self . password )
12159	def abfGroupFiles ( groups , folder ) : assert os . path . exists ( folder ) files = os . listdir ( folder ) group2 = { } for parent in groups . keys ( ) : if not parent in group2 . keys ( ) : group2 [ parent ] = [ ] for ID in groups [ parent ] : for fname in [ x . lower ( ) for x in files if ID in x . lower ( ) ] : group2 [ parent ] . extend ( [ fname ] ) return group2
11137	def path_required ( func ) : @ wraps ( func ) def wrapper ( self , * args , ** kwargs ) : if self . path is None : warnings . warn ( 'Must load (Repository.load_repository) or initialize (Repository.create_repository) the repository first !' ) return return func ( self , * args , ** kwargs ) return wrapper
1321	def Maximize ( self , waitTime : float = OPERATION_WAIT_TIME ) -> bool : if self . IsTopLevel ( ) : return self . ShowWindow ( SW . ShowMaximized , waitTime ) return False
2644	def push_file ( self , source , dest_dir ) : local_dest = dest_dir + '/' + os . path . basename ( source ) if os . path . dirname ( source ) != dest_dir : try : shutil . copyfile ( source , local_dest ) os . chmod ( local_dest , 0o777 ) except OSError as e : raise FileCopyException ( e , self . hostname ) return local_dest
9883	def inquire ( self ) : name = copy . deepcopy ( self . fname ) stats = fortran_cdf . inquire ( name ) status = stats [ 0 ] if status == 0 : self . _num_dims = stats [ 1 ] self . _dim_sizes = stats [ 2 ] self . _encoding = stats [ 3 ] self . _majority = stats [ 4 ] self . _max_rec = stats [ 5 ] self . _num_r_vars = stats [ 6 ] self . _num_z_vars = stats [ 7 ] self . _num_attrs = stats [ 8 ] else : raise IOError ( fortran_cdf . statusreporter ( status ) )
7709	def handle_authorized_event ( self , event ) : self . server = event . authorized_jid . bare ( ) if "versioning" in self . server_features : if self . roster is not None and self . roster . version is not None : version = self . roster . version else : version = u"" else : version = None self . request_roster ( version )
4186	def window_taylor ( N , nbar = 4 , sll = - 30 ) : B = 10 ** ( - sll / 20 ) A = log ( B + sqrt ( B ** 2 - 1 ) ) / pi s2 = nbar ** 2 / ( A ** 2 + ( nbar - 0.5 ) ** 2 ) ma = arange ( 1 , nbar ) def calc_Fm ( m ) : numer = ( - 1 ) ** ( m + 1 ) * prod ( 1 - m ** 2 / s2 / ( A ** 2 + ( ma - 0.5 ) ** 2 ) ) denom = 2 * prod ( [ 1 - m ** 2 / j ** 2 for j in ma if j != m ] ) return numer / denom Fm = array ( [ calc_Fm ( m ) for m in ma ] ) def W ( n ) : return 2 * np . sum ( Fm * cos ( 2 * pi * ma * ( n - N / 2 + 1 / 2 ) / N ) ) + 1 w = array ( [ W ( n ) for n in range ( N ) ] ) scale = W ( ( N - 1 ) / 2 ) w /= scale return w
8733	def parse_timedelta ( str ) : deltas = ( _parse_timedelta_part ( part . strip ( ) ) for part in str . split ( ',' ) ) return sum ( deltas , datetime . timedelta ( ) )
560	def anyGoodSprintsActive ( self ) : if self . _state [ 'lastGoodSprint' ] is not None : goodSprints = self . _state [ 'sprints' ] [ 0 : self . _state [ 'lastGoodSprint' ] + 1 ] else : goodSprints = self . _state [ 'sprints' ] for sprint in goodSprints : if sprint [ 'status' ] == 'active' : anyActiveSprints = True break else : anyActiveSprints = False return anyActiveSprints
6396	def sim_minkowski ( src , tar , qval = 2 , pval = 1 , alphabet = None ) : return Minkowski ( ) . sim ( src , tar , qval , pval , alphabet )
365	def affine_transform_keypoints ( coords_list , transform_matrix ) : coords_result_list = [ ] for coords in coords_list : coords = np . asarray ( coords ) coords = coords . transpose ( [ 1 , 0 ] ) coords = np . insert ( coords , 2 , 1 , axis = 0 ) coords_result = np . matmul ( transform_matrix , coords ) coords_result = coords_result [ 0 : 2 , : ] . transpose ( [ 1 , 0 ] ) coords_result_list . append ( coords_result ) return coords_result_list
2377	def list_rules ( self ) : for rule in sorted ( self . all_rules , key = lambda rule : rule . name ) : print ( rule ) if self . args . verbose : for line in rule . doc . split ( "\n" ) : print ( " " , line )
3062	def string_to_scopes ( scopes ) : if not scopes : return [ ] elif isinstance ( scopes , six . string_types ) : return scopes . split ( ' ' ) else : return scopes
4185	def window_flattop ( N , mode = 'symmetric' , precision = None ) : r assert mode in [ 'periodic' , 'symmetric' ] t = arange ( 0 , N ) if mode == 'periodic' : x = 2 * pi * t / float ( N ) else : if N == 1 : return ones ( 1 ) x = 2 * pi * t / float ( N - 1 ) a0 = 0.21557895 a1 = 0.41663158 a2 = 0.277263158 a3 = 0.083578947 a4 = 0.006947368 if precision == 'octave' : d = 4.6402 a0 = 1. / d a1 = 1.93 / d a2 = 1.29 / d a3 = 0.388 / d a4 = 0.0322 / d w = a0 - a1 * cos ( x ) + a2 * cos ( 2 * x ) - a3 * cos ( 3 * x ) + a4 * cos ( 4 * x ) return w
9418	def document_func_view ( serializer_class = None , response_serializer_class = None , filter_backends = None , permission_classes = None , authentication_classes = None , doc_format_args = list ( ) , doc_format_kwargs = dict ( ) ) : def decorator ( func ) : if serializer_class : func . cls . serializer_class = func . view_class . serializer_class = serializer_class if response_serializer_class : func . cls . response_serializer_class = func . view_class . response_serializer_class = response_serializer_class if filter_backends : func . cls . filter_backends = func . view_class . filter_backends = filter_backends if permission_classes : func . cls . permission_classes = func . view_class . permission_classes = permission_classes if authentication_classes : func . cls . authentication_classes = func . view_class . authentication_classes = authentication_classes if doc_format_args or doc_format_kwargs : func . cls . __doc__ = func . view_class . __doc__ = getdoc ( func ) . format ( * doc_format_args , ** doc_format_kwargs ) return func return decorator
6742	def render_to_string ( template , extra = None ) : from jinja2 import Template extra = extra or { } final_fqfn = find_template ( template ) assert final_fqfn , 'Template not found: %s' % template template_content = open ( final_fqfn , 'r' ) . read ( ) t = Template ( template_content ) if extra : context = env . copy ( ) context . update ( extra ) else : context = env rendered_content = t . render ( ** context ) rendered_content = rendered_content . replace ( '&quot;' , '"' ) return rendered_content
5360	def execute_batch_tasks ( self , tasks_cls , big_delay = 0 , small_delay = 0 , wait_for_threads = True ) : def _split_tasks ( tasks_cls ) : backend_t = [ ] global_t = [ ] for t in tasks_cls : if t . is_backend_task ( t ) : backend_t . append ( t ) else : global_t . append ( t ) return backend_t , global_t backend_tasks , global_tasks = _split_tasks ( tasks_cls ) logger . debug ( 'backend_tasks = %s' % ( backend_tasks ) ) logger . debug ( 'global_tasks = %s' % ( global_tasks ) ) threads = [ ] stopper = threading . Event ( ) if len ( backend_tasks ) > 0 : repos_backend = self . _get_repos_by_backend ( ) for backend in repos_backend : t = TasksManager ( backend_tasks , backend , stopper , self . config , small_delay ) threads . append ( t ) t . start ( ) if len ( global_tasks ) > 0 : gt = TasksManager ( global_tasks , "Global tasks" , stopper , self . config , big_delay ) threads . append ( gt ) gt . start ( ) if big_delay > 0 : when = datetime . now ( ) + timedelta ( seconds = big_delay ) when_str = when . strftime ( '%a, %d %b %Y %H:%M:%S %Z' ) logger . info ( "%s will be executed on %s" % ( global_tasks , when_str ) ) if wait_for_threads : time . sleep ( 1 ) stopper . set ( ) for t in threads : t . join ( ) self . __check_queue_for_errors ( ) logger . debug ( "[thread:main] All threads (and their tasks) are finished" )
11313	def update_cnum ( self ) : if "ConferencePaper" not in self . collections : cnums = record_get_field_values ( self . record , '773' , code = "w" ) for cnum in cnums : cnum_subs = [ ( "9" , "INSPIRE-CNUM" ) , ( "a" , cnum ) ] record_add_field ( self . record , "035" , subfields = cnum_subs )
6527	def get_tools ( ) : if not hasattr ( get_tools , '_CACHE' ) : get_tools . _CACHE = dict ( ) for entry in pkg_resources . iter_entry_points ( 'tidypy.tools' ) : try : get_tools . _CACHE [ entry . name ] = entry . load ( ) except ImportError as exc : output_error ( 'Could not load tool "%s" defined by "%s": %s' % ( entry , entry . dist , exc , ) , ) return get_tools . _CACHE
6148	def unique_cpx_roots ( rlist , tol = 0.001 ) : uniq = [ rlist [ 0 ] ] mult = [ 1 ] for k in range ( 1 , len ( rlist ) ) : N_uniq = len ( uniq ) for m in range ( N_uniq ) : if abs ( rlist [ k ] - uniq [ m ] ) <= tol : mult [ m ] += 1 uniq [ m ] = ( uniq [ m ] * ( mult [ m ] - 1 ) + rlist [ k ] ) / float ( mult [ m ] ) break uniq = np . hstack ( ( uniq , rlist [ k ] ) ) mult = np . hstack ( ( mult , [ 1 ] ) ) return np . array ( uniq ) , np . array ( mult )
12903	def _parse_genotype ( self , vcf_fields ) : format_col = vcf_fields [ 8 ] . split ( ':' ) genome_data = vcf_fields [ 9 ] . split ( ':' ) try : gt_idx = format_col . index ( 'GT' ) except ValueError : return [ ] return [ int ( x ) for x in re . split ( r'[\|/]' , genome_data [ gt_idx ] ) if x != '.' ]
2954	def initialize ( self , containers ) : self . _containers = deepcopy ( containers ) self . __write ( containers , initialize = True )
874	def initStateFrom ( self , particleId , particleState , newBest ) : if newBest : ( bestResult , bestPosition ) = self . _resultsDB . getParticleBest ( particleId ) else : bestResult = bestPosition = None varStates = particleState [ 'varStates' ] for varName in varStates . keys ( ) : varState = copy . deepcopy ( varStates [ varName ] ) if newBest : varState [ 'bestResult' ] = bestResult if bestPosition is not None : varState [ 'bestPosition' ] = bestPosition [ varName ] self . permuteVars [ varName ] . setState ( varState )
10243	def count_citation_years ( graph : BELGraph ) -> typing . Counter [ int ] : result = defaultdict ( set ) for _ , _ , data in graph . edges ( data = True ) : if CITATION not in data or CITATION_DATE not in data [ CITATION ] : continue try : dt = _ensure_datetime ( data [ CITATION ] [ CITATION_DATE ] ) result [ dt . year ] . add ( ( data [ CITATION ] [ CITATION_TYPE ] , data [ CITATION ] [ CITATION_REFERENCE ] ) ) except Exception : continue return count_dict_values ( result )
3285	def end_write ( self , with_errors ) : if not with_errors : commands . add ( self . provider . ui , self . provider . repo , self . localHgPath )
12143	async def _push ( self , * args , ** kwargs ) : self . _data . append ( ( args , kwargs ) ) if self . _future is not None : future , self . _future = self . _future , None future . set_result ( True )
13444	def _rindex ( mylist : Sequence [ T ] , x : T ) -> int : return len ( mylist ) - mylist [ : : - 1 ] . index ( x ) - 1
8715	def file_print ( self , filename ) : log . info ( 'Printing ' + filename ) res = self . __exchange ( PRINT_FILE . format ( filename = filename ) ) log . info ( res ) return res
12735	def step ( self , substeps = 2 ) : self . frame_no += 1 dt = self . dt / substeps for _ in range ( substeps ) : self . ode_contactgroup . empty ( ) self . ode_space . collide ( None , self . on_collision ) self . ode_world . step ( dt )
11003	def psffunc ( self , * args , ** kwargs ) : if self . polychromatic : func = psfcalc . calculate_polychrome_linescan_psf else : func = psfcalc . calculate_linescan_psf return func ( * args , ** kwargs )
10276	def get_neurommsig_scores ( graph : BELGraph , genes : List [ Gene ] , annotation : str = 'Subgraph' , ora_weight : Optional [ float ] = None , hub_weight : Optional [ float ] = None , top_percent : Optional [ float ] = None , topology_weight : Optional [ float ] = None , preprocess : bool = False ) -> Optional [ Mapping [ str , float ] ] : if preprocess : graph = neurommsig_graph_preprocessor . run ( graph ) if not any ( gene in graph for gene in genes ) : logger . debug ( 'no genes mapping to graph' ) return subgraphs = get_subgraphs_by_annotation ( graph , annotation = annotation ) return get_neurommsig_scores_prestratified ( subgraphs = subgraphs , genes = genes , ora_weight = ora_weight , hub_weight = hub_weight , top_percent = top_percent , topology_weight = topology_weight , )
5444	def parse_uri ( self , raw_uri , recursive ) : if recursive : raw_uri = directory_fmt ( raw_uri ) file_provider = self . parse_file_provider ( raw_uri ) self . _validate_paths_or_fail ( raw_uri , recursive ) uri , docker_uri = self . rewrite_uris ( raw_uri , file_provider ) uri_parts = job_model . UriParts ( directory_fmt ( os . path . dirname ( uri ) ) , os . path . basename ( uri ) ) return docker_uri , uri_parts , file_provider
127	def area ( self ) : if len ( self . exterior ) < 3 : raise Exception ( "Cannot compute the polygon's area because it contains less than three points." ) poly = self . to_shapely_polygon ( ) return poly . area
977	def _newRepresentationOK ( self , newRep , newIndex ) : if newRep . size != self . w : return False if ( newIndex < self . minIndex - 1 ) or ( newIndex > self . maxIndex + 1 ) : raise ValueError ( "newIndex must be within one of existing indices" ) newRepBinary = numpy . array ( [ False ] * self . n ) newRepBinary [ newRep ] = True midIdx = self . _maxBuckets / 2 runningOverlap = self . _countOverlap ( self . bucketMap [ self . minIndex ] , newRep ) if not self . _overlapOK ( self . minIndex , newIndex , overlap = runningOverlap ) : return False for i in range ( self . minIndex + 1 , midIdx + 1 ) : newBit = ( i - 1 ) % self . w if newRepBinary [ self . bucketMap [ i - 1 ] [ newBit ] ] : runningOverlap -= 1 if newRepBinary [ self . bucketMap [ i ] [ newBit ] ] : runningOverlap += 1 if not self . _overlapOK ( i , newIndex , overlap = runningOverlap ) : return False for i in range ( midIdx + 1 , self . maxIndex + 1 ) : newBit = i % self . w if newRepBinary [ self . bucketMap [ i - 1 ] [ newBit ] ] : runningOverlap -= 1 if newRepBinary [ self . bucketMap [ i ] [ newBit ] ] : runningOverlap += 1 if not self . _overlapOK ( i , newIndex , overlap = runningOverlap ) : return False return True
12732	def join ( self , shape , body_a , body_b = None , name = None , ** kwargs ) : ba = self . get_body ( body_a ) bb = self . get_body ( body_b ) shape = shape . lower ( ) if name is None : name = '{}^{}^{}' . format ( ba . name , shape , bb . name if bb else '' ) self . _joints [ name ] = Joint . build ( shape , name , self , body_a = ba , body_b = bb , ** kwargs ) return self . _joints [ name ]
12042	def XMLtoPython ( xmlStr = r"C:\Apps\pythonModules\GSTemp.xml" ) : if os . path . exists ( xmlStr ) : with open ( xmlStr ) as f : xmlStr = f . read ( ) print ( xmlStr ) print ( "DONE" ) return
12456	def install ( env , requirements , args , ignore_activated = False , install_dev_requirements = False , quiet = False ) : if os . path . isfile ( requirements ) : args += ( '-r' , requirements ) label = 'project' else : args += ( '-U' , '-e' , '.' ) label = 'library' if install_dev_requirements : dev_requirements = None dirname = os . path . dirname ( requirements ) basename , ext = os . path . splitext ( os . path . basename ( requirements ) ) for delimiter in ( '-' , '_' , '' ) : filename = os . path . join ( dirname , '' . join ( ( basename , delimiter , 'dev' , ext ) ) ) if os . path . isfile ( filename ) : dev_requirements = filename break filename = os . path . join ( dirname , '' . join ( ( 'dev' , delimiter , basename , ext ) ) ) if os . path . isfile ( filename ) : dev_requirements = filename break if dev_requirements : args += ( '-r' , dev_requirements ) if not quiet : print_message ( '== Step 2. Install {0} ==' . format ( label ) ) result = not pip_cmd ( env , ( 'install' , ) + args , ignore_activated , echo = not quiet ) if not quiet : print_message ( ) return result
3800	def Bahadori_gas ( T , MW ) : r A = [ 4.3931323468E-1 , - 3.88001122207E-2 , 9.28616040136E-4 , - 6.57828995724E-6 ] B = [ - 2.9624238519E-3 , 2.67956145820E-4 , - 6.40171884139E-6 , 4.48579040207E-8 ] C = [ 7.54249790107E-6 , - 6.46636219509E-7 , 1.5124510261E-8 , - 1.0376480449E-10 ] D = [ - 6.0988433456E-9 , 5.20752132076E-10 , - 1.19425545729E-11 , 8.0136464085E-14 ] X , Y = T , MW a = A [ 0 ] + B [ 0 ] * X + C [ 0 ] * X ** 2 + D [ 0 ] * X ** 3 b = A [ 1 ] + B [ 1 ] * X + C [ 1 ] * X ** 2 + D [ 1 ] * X ** 3 c = A [ 2 ] + B [ 2 ] * X + C [ 2 ] * X ** 2 + D [ 2 ] * X ** 3 d = A [ 3 ] + B [ 3 ] * X + C [ 3 ] * X ** 2 + D [ 3 ] * X ** 3 return a + b * Y + c * Y ** 2 + d * Y ** 3
4941	def unlink_user ( self , enterprise_customer , user_email ) : try : existing_user = User . objects . get ( email = user_email ) link_record = self . get ( enterprise_customer = enterprise_customer , user_id = existing_user . id ) link_record . delete ( ) if update_user : update_user . delay ( sailthru_vars = { 'is_enterprise_learner' : False , 'enterprise_name' : None , } , email = user_email ) except User . DoesNotExist : pending_link = PendingEnterpriseCustomerUser . objects . get ( enterprise_customer = enterprise_customer , user_email = user_email ) pending_link . delete ( ) LOGGER . info ( 'Enterprise learner {%s} successfully unlinked from Enterprise Customer {%s}' , user_email , enterprise_customer . name )
7531	def _plot_dag ( dag , results , snames ) : try : import matplotlib . pyplot as plt from matplotlib . dates import date2num from matplotlib . cm import gist_rainbow plt . figure ( "dag_layout" , figsize = ( 10 , 10 ) ) nx . draw ( dag , pos = nx . spring_layout ( dag ) , node_color = 'pink' , with_labels = True ) plt . savefig ( "./dag_layout.png" , bbox_inches = 'tight' , dpi = 200 ) pos = { } colors = { } for node in dag : mtd = results [ node ] . metadata start = date2num ( mtd . started ) _ , _ , sname = node . split ( "-" , 2 ) sid = snames . index ( sname ) pos [ node ] = ( start + sid , start * 1e6 ) colors [ node ] = mtd . engine_id plt . figure ( "dag_starttimes" , figsize = ( 10 , 16 ) ) nx . draw ( dag , pos , node_list = colors . keys ( ) , node_color = colors . values ( ) , cmap = gist_rainbow , with_labels = True ) plt . savefig ( "./dag_starttimes.png" , bbox_inches = 'tight' , dpi = 200 ) except Exception as inst : LOGGER . warning ( inst )
4721	def trun_exit ( trun ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun:exit" ) rcode = 0 for hook in reversed ( trun [ "hooks" ] [ "exit" ] ) : rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun::exit { rcode: %r }" % rcode , rcode ) return rcode
9489	def generate_bytecode_from_obb ( obb : object , previous : bytes ) -> bytes : if isinstance ( obb , pyte . superclasses . _PyteOp ) : return obb . to_bytes ( previous ) elif isinstance ( obb , ( pyte . superclasses . _PyteAugmentedComparator , pyte . superclasses . _PyteAugmentedValidator . _FakeMathematicalOP ) ) : return obb . to_bytes ( previous ) elif isinstance ( obb , pyte . superclasses . _PyteAugmentedValidator ) : obb . validate ( ) return obb . to_load ( ) elif isinstance ( obb , int ) : return obb . to_bytes ( ( obb . bit_length ( ) + 7 ) // 8 , byteorder = "little" ) or b'' elif isinstance ( obb , bytes ) : return obb else : raise TypeError ( "`{}` was not a valid bytecode-encodable item" . format ( obb ) )
1391	def convert_pb_kvs ( kvs , include_non_primitives = True ) : config = { } for kv in kvs : if kv . value : config [ kv . key ] = kv . value elif kv . serialized_value : if topology_pb2 . JAVA_SERIALIZED_VALUE == kv . type : jv = _convert_java_value ( kv , include_non_primitives = include_non_primitives ) if jv is not None : config [ kv . key ] = jv else : config [ kv . key ] = _raw_value ( kv ) return config
10886	def corners ( self ) : corners = [ ] for ind in itertools . product ( * ( ( 0 , 1 ) , ) * self . dim ) : ind = np . array ( ind ) corners . append ( self . l + ind * self . r ) return np . array ( corners )
11830	def child_node ( self , problem , action ) : "Fig. 3.10" next = problem . result ( self . state , action ) return Node ( next , self , action , problem . path_cost ( self . path_cost , self . state , action , next ) )
10484	def _matchOther ( self , obj , ** kwargs ) : if obj is not None : if self . _findFirstR ( ** kwargs ) : return obj . _match ( ** kwargs ) return False
6828	def fetch ( self , path , use_sudo = False , user = None , remote = None ) : if path is None : raise ValueError ( "Path to the working copy is needed to fetch from a remote repository." ) if remote is not None : cmd = 'git fetch %s' % remote else : cmd = 'git fetch' with cd ( path ) : if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
7538	def getassembly ( args , parsedict ) : project_dir = ip . core . assembly . _expander ( parsedict [ 'project_dir' ] ) assembly_name = parsedict [ 'assembly_name' ] assembly_file = os . path . join ( project_dir , assembly_name ) if not os . path . exists ( project_dir ) : os . mkdir ( project_dir ) try : if ( '1' in args . steps ) and args . force : data = ip . Assembly ( assembly_name , cli = True ) else : data = ip . load_json ( assembly_file , cli = True ) data . _cli = True except IPyradWarningExit as _ : if '1' not in args . steps : raise IPyradWarningExit ( " Error: You must first run step 1 on the assembly: {}" . format ( assembly_file ) ) else : data = ip . Assembly ( assembly_name , cli = True ) for param in parsedict : if param == "assembly_name" : if parsedict [ param ] != data . name : data . set_params ( param , parsedict [ param ] ) else : try : data . set_params ( param , parsedict [ param ] ) except IndexError as _ : print ( " Malformed params file: {}" . format ( args . params ) ) print ( " Bad parameter {} - {}" . format ( param , parsedict [ param ] ) ) sys . exit ( - 1 ) return data
10347	def run_rcr ( graph , tag = 'dgxp' ) : hypotheses = defaultdict ( set ) increases = defaultdict ( set ) decreases = defaultdict ( set ) for u , v , d in graph . edges ( data = True ) : hypotheses [ u ] . add ( v ) if d [ RELATION ] in CAUSAL_INCREASE_RELATIONS : increases [ u ] . add ( v ) elif d [ RELATION ] in CAUSAL_DECREASE_RELATIONS : decreases [ u ] . add ( v ) correct = defaultdict ( int ) contra = defaultdict ( int ) ambiguous = defaultdict ( int ) missing = defaultdict ( int ) for controller , downstream_nodes in hypotheses . items ( ) : if len ( downstream_nodes ) < 4 : continue for node in downstream_nodes : if node in increases [ controller ] and node in decreases [ controller ] : ambiguous [ controller ] += 1 elif node in increases [ controller ] : if graph . node [ node ] [ tag ] == 1 : correct [ controller ] += 1 elif graph . node [ node ] [ tag ] == - 1 : contra [ controller ] += 1 elif node in decreases [ controller ] : if graph . node [ node ] [ tag ] == 1 : contra [ controller ] += 1 elif graph . node [ node ] [ tag ] == - 1 : correct [ controller ] += 1 else : missing [ controller ] += 1 controllers = { controller for controller , downstream_nodes in hypotheses . items ( ) if 4 <= len ( downstream_nodes ) } concordance_scores = { controller : scipy . stats . beta ( 0.5 , correct [ controller ] , contra [ controller ] ) for controller in controllers } population = { node for controller in controllers for node in hypotheses [ controller ] } population_size = len ( population ) return pandas . DataFrame ( { 'contra' : contra , 'correct' : correct , 'concordance' : concordance_scores } )
9308	def get_canonical_headers ( cls , req , include = None ) : if include is None : include = cls . default_include_headers include = [ x . lower ( ) for x in include ] headers = req . headers . copy ( ) if 'host' not in headers : headers [ 'host' ] = urlparse ( req . url ) . netloc . split ( ':' ) [ 0 ] cano_headers_dict = { } for hdr , val in headers . items ( ) : hdr = hdr . strip ( ) . lower ( ) val = cls . amz_norm_whitespace ( val ) . strip ( ) if ( hdr in include or '*' in include or ( 'x-amz-*' in include and hdr . startswith ( 'x-amz-' ) and not hdr == 'x-amz-client-context' ) ) : vals = cano_headers_dict . setdefault ( hdr , [ ] ) vals . append ( val ) cano_headers = '' signed_headers_list = [ ] for hdr in sorted ( cano_headers_dict ) : vals = cano_headers_dict [ hdr ] val = ',' . join ( sorted ( vals ) ) cano_headers += '{}:{}\n' . format ( hdr , val ) signed_headers_list . append ( hdr ) signed_headers = ';' . join ( signed_headers_list ) return ( cano_headers , signed_headers )
4698	def fmt ( lbaf = 3 ) : if env ( ) : cij . err ( "cij.nvme.exists: Invalid NVMe ENV." ) return 1 nvme = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) cmd = [ "nvme" , "format" , nvme [ "DEV_PATH" ] , "-l" , str ( lbaf ) ] rcode , _ , _ = cij . ssh . command ( cmd , shell = True ) return rcode
5807	def parse_session_info ( server_handshake_bytes , client_handshake_bytes ) : protocol = None cipher_suite = None compression = False session_id = None session_ticket = None server_session_id = None client_session_id = None for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type != b'\x02' : continue protocol = { b'\x03\x00' : "SSLv3" , b'\x03\x01' : "TLSv1" , b'\x03\x02' : "TLSv1.1" , b'\x03\x03' : "TLSv1.2" , b'\x03\x04' : "TLSv1.3" , } [ message_data [ 0 : 2 ] ] session_id_length = int_from_bytes ( message_data [ 34 : 35 ] ) if session_id_length > 0 : server_session_id = message_data [ 35 : 35 + session_id_length ] cipher_suite_start = 35 + session_id_length cipher_suite_bytes = message_data [ cipher_suite_start : cipher_suite_start + 2 ] cipher_suite = CIPHER_SUITE_MAP [ cipher_suite_bytes ] compression_start = cipher_suite_start + 2 compression = message_data [ compression_start : compression_start + 1 ] != b'\x00' extensions_length_start = compression_start + 1 extensions_data = message_data [ extensions_length_start : ] for extension_type , extension_data in _parse_hello_extensions ( extensions_data ) : if extension_type == 35 : session_ticket = "new" break break for record_type , _ , record_data in parse_tls_records ( client_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type != b'\x01' : continue session_id_length = int_from_bytes ( message_data [ 34 : 35 ] ) if session_id_length > 0 : client_session_id = message_data [ 35 : 35 + session_id_length ] cipher_suite_start = 35 + session_id_length cipher_suite_length = int_from_bytes ( message_data [ cipher_suite_start : cipher_suite_start + 2 ] ) compression_start = cipher_suite_start + 2 + cipher_suite_length compression_length = int_from_bytes ( message_data [ compression_start : compression_start + 1 ] ) if server_session_id is None and session_ticket is None : extensions_length_start = compression_start + 1 + compression_length extensions_data = message_data [ extensions_length_start : ] for extension_type , extension_data in _parse_hello_extensions ( extensions_data ) : if extension_type == 35 : session_ticket = "reused" break break if server_session_id is not None : if client_session_id is None : session_id = "new" else : if client_session_id != server_session_id : session_id = "new" else : session_id = "reused" return { "protocol" : protocol , "cipher_suite" : cipher_suite , "compression" : compression , "session_id" : session_id , "session_ticket" : session_ticket , }
11509	def delete_item ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = item_id response = self . request ( 'midas.item.delete' , parameters ) return response
10210	def _is_root ( ) : import os import ctypes try : return os . geteuid ( ) == 0 except AttributeError : return ctypes . windll . shell32 . IsUserAnAdmin ( ) != 0 return False
3315	def _find ( self , url ) : vr = self . db . view ( "properties/by_url" , key = url , include_docs = True ) _logger . debug ( "find(%r) returned %s" % ( url , len ( vr ) ) ) assert len ( vr ) <= 1 , "Found multiple matches for %r" % url for row in vr : assert row . doc return row . doc return None
3044	def _generate_refresh_request_body ( self ) : body = urllib . parse . urlencode ( { 'grant_type' : 'refresh_token' , 'client_id' : self . client_id , 'client_secret' : self . client_secret , 'refresh_token' : self . refresh_token , } ) return body
4182	def window_blackman_nuttall ( N ) : r a0 = 0.3635819 a1 = 0.4891775 a2 = 0.1365995 a3 = 0.0106411 return _coeff4 ( N , a0 , a1 , a2 , a3 )
2885	def n_subscribers ( self ) : hard = self . hard_subscribers and len ( self . hard_subscribers ) or 0 weak = self . weak_subscribers and len ( self . weak_subscribers ) or 0 return hard + weak
3555	def power_on ( self , timeout_sec = TIMEOUT_SEC ) : self . _powered_on . clear ( ) IOBluetoothPreferenceSetControllerPowerState ( 1 ) if not self . _powered_on . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to power on!' )
3921	def get_menu_widget ( self , close_callback ) : return ConversationMenu ( self . _coroutine_queue , self . _conversation , close_callback , self . _keys )
9769	def delete ( ctx ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) if not click . confirm ( "Are sure you want to delete job `{}`" . format ( _job ) ) : click . echo ( 'Existing without deleting job.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . job . delete_job ( user , project_name , _job ) JobManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Job `{}` was delete successfully" . format ( _job ) )
7976	def _reset ( self ) : ClientStream . _reset ( self ) self . available_auth_methods = None self . auth_stanza = None self . registration_callback = None
4733	def generate_steady_rt_pic ( process_data , para_meter , scale , steady_time ) : pic_path_steady = para_meter [ 'filename' ] + '_steady.png' plt . figure ( figsize = ( 4 * scale , 2.5 * scale ) ) for key in process_data . keys ( ) : if len ( process_data [ key ] ) < steady_time : steady_time = len ( process_data [ key ] ) plt . scatter ( process_data [ key ] [ - 1 * steady_time : , 0 ] , process_data [ key ] [ - 1 * steady_time : , 1 ] , label = str ( key ) , s = 10 ) steady_value = np . mean ( process_data [ key ] [ - 1 * steady_time : , 1 ] ) steady_value_5 = steady_value * ( 1 + 0.05 ) steady_value_10 = steady_value * ( 1 + 0.1 ) steady_value_ng_5 = steady_value * ( 1 - 0.05 ) steady_value_ng_10 = steady_value * ( 1 - 0.1 ) plt . plot ( process_data [ key ] [ - 1 * steady_time : , 0 ] , [ steady_value ] * steady_time , 'b' ) plt . plot ( process_data [ key ] [ - 1 * steady_time : , 0 ] , [ steady_value_5 ] * steady_time , 'g' ) plt . plot ( process_data [ key ] [ - 1 * steady_time : , 0 ] , [ steady_value_ng_5 ] * steady_time , 'g' ) plt . plot ( process_data [ key ] [ - 1 * steady_time : , 0 ] , [ steady_value_10 ] * steady_time , 'r' ) plt . plot ( process_data [ key ] [ - 1 * steady_time : , 0 ] , [ steady_value_ng_10 ] * steady_time , 'r' ) plt . title ( para_meter [ 'title' ] + '(steady)' ) plt . xlabel ( para_meter [ 'x_axis_name' ] + '(steady)' ) plt . ylabel ( para_meter [ 'y_axis_name' ] + '(steady)' ) plt . legend ( loc = 'upper left' ) plt . savefig ( pic_path_steady ) return pic_path_steady
4675	def removeAccount ( self , account ) : accounts = self . getAccounts ( ) for a in accounts : if a [ "name" ] == account : self . store . delete ( a [ "pubkey" ] )
4556	def all_named_colors ( ) : yield from _TO_COLOR_USER . items ( ) for name , color in _TO_COLOR . items ( ) : if name not in _TO_COLOR_USER : yield name , color
436	def draw_weights ( W = None , second = 10 , saveable = True , shape = None , name = 'mnist' , fig_idx = 2396512 ) : if shape is None : shape = [ 28 , 28 ] import matplotlib . pyplot as plt if saveable is False : plt . ion ( ) fig = plt . figure ( fig_idx ) n_units = W . shape [ 1 ] num_r = int ( np . sqrt ( n_units ) ) num_c = int ( np . ceil ( n_units / num_r ) ) count = int ( 1 ) for _row in range ( 1 , num_r + 1 ) : for _col in range ( 1 , num_c + 1 ) : if count > n_units : break fig . add_subplot ( num_r , num_c , count ) feature = W [ : , count - 1 ] / np . sqrt ( ( W [ : , count - 1 ] ** 2 ) . sum ( ) ) plt . imshow ( np . reshape ( feature , ( shape [ 0 ] , shape [ 1 ] ) ) , cmap = 'gray' , interpolation = "nearest" ) plt . gca ( ) . xaxis . set_major_locator ( plt . NullLocator ( ) ) plt . gca ( ) . yaxis . set_major_locator ( plt . NullLocator ( ) ) count = count + 1 if saveable : plt . savefig ( name + '.pdf' , format = 'pdf' ) else : plt . draw ( ) plt . pause ( second )
13844	def process_macros ( self , content : str ) -> str : def _sub ( macro ) : name = macro . group ( 'body' ) params = self . get_options ( macro . group ( 'options' ) ) return self . options [ 'macros' ] . get ( name , '' ) . format_map ( params ) return self . pattern . sub ( _sub , content )
6174	def reset_lock ( self ) : redis_key = self . CELERY_LOCK . format ( task_id = self . task_identifier ) self . celery_self . backend . client . delete ( redis_key )
11985	def upload_folder ( self , bucket , folder , key = None , skip = None , content_types = None ) : uploader = FolderUploader ( self , bucket , folder , key , skip , content_types ) return uploader . start ( )
1606	def run_containers ( command , parser , cl_args , unknown_args ) : cluster , role , env = cl_args [ 'cluster' ] , cl_args [ 'role' ] , cl_args [ 'environ' ] topology = cl_args [ 'topology-name' ] container_id = cl_args [ 'id' ] try : result = tracker_access . get_topology_info ( cluster , env , topology , role ) except : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False containers = result [ 'physical_plan' ] [ 'stmgrs' ] all_bolts , all_spouts = set ( ) , set ( ) for _ , bolts in result [ 'physical_plan' ] [ 'bolts' ] . items ( ) : all_bolts = all_bolts | set ( bolts ) for _ , spouts in result [ 'physical_plan' ] [ 'spouts' ] . items ( ) : all_spouts = all_spouts | set ( spouts ) stmgrs = containers . keys ( ) stmgrs . sort ( ) if container_id is not None : try : normalized_cid = container_id - 1 if normalized_cid < 0 : raise stmgrs = [ stmgrs [ normalized_cid ] ] except : Log . error ( 'Invalid container id: %d' % container_id ) return False table = [ ] for sid , name in enumerate ( stmgrs ) : cid = sid + 1 host = containers [ name ] [ "host" ] port = containers [ name ] [ "port" ] pid = containers [ name ] [ "pid" ] instances = containers [ name ] [ "instance_ids" ] bolt_nums = len ( [ instance for instance in instances if instance in all_bolts ] ) spout_nums = len ( [ instance for instance in instances if instance in all_spouts ] ) table . append ( [ cid , host , port , pid , bolt_nums , spout_nums , len ( instances ) ] ) headers = [ "container" , "host" , "port" , "pid" , "#bolt" , "#spout" , "#instance" ] sys . stdout . flush ( ) print ( tabulate ( table , headers = headers ) ) return True
8029	def sizeClassifier ( path , min_size = DEFAULTS [ 'min_size' ] ) : filestat = _stat ( path ) if stat . S_ISLNK ( filestat . st_mode ) : return if filestat . st_size < min_size : return return filestat . st_size
8195	def load ( self , id ) : self . clear ( ) self . add_node ( id , root = True ) for w , id2 in self . get_links ( id ) : self . add_edge ( id , id2 , weight = w ) if len ( self ) > self . max : break for w , id2 , links in self . get_cluster ( id ) : for id3 in links : self . add_edge ( id3 , id2 , weight = w ) self . add_edge ( id , id3 , weight = w ) if len ( self ) > self . max : break if self . event . clicked : g . add_node ( self . event . clicked )
3240	def get_group_policy_document ( group_name , policy_name , client = None , ** kwargs ) : return client . get_group_policy ( GroupName = group_name , PolicyName = policy_name , ** kwargs ) [ 'PolicyDocument' ]
13791	def get_app_name ( ) : fn = getattr ( sys . modules [ '__main__' ] , '__file__' , None ) if fn is None : return '__main__' return os . path . splitext ( os . path . basename ( fn ) ) [ 0 ]
9541	def datetime_range_inclusive ( min , max , format ) : dmin = datetime . strptime ( min , format ) dmax = datetime . strptime ( max , format ) def checker ( v ) : dv = datetime . strptime ( v , format ) if dv < dmin or dv > dmax : raise ValueError ( v ) return checker
5143	def search_for_comment ( self , lineno , default = None ) : if not self . index : self . make_index ( ) block = self . index . get ( lineno , None ) text = getattr ( block , 'text' , default ) return text
5419	def _google_v2_parse_arguments ( args ) : if ( args . zones and args . regions ) or ( not args . zones and not args . regions ) : raise ValueError ( 'Exactly one of --regions and --zones must be specified' ) if args . machine_type and ( args . min_cores or args . min_ram ) : raise ValueError ( '--machine-type not supported together with --min-cores or --min-ram.' )
12937	def depricated_name ( newmethod ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : warnings . simplefilter ( 'always' , DeprecationWarning ) warnings . warn ( "Function {} is depricated, please use {} instead." . format ( func . __name__ , newmethod ) , category = DeprecationWarning , stacklevel = 2 ) warnings . simplefilter ( 'default' , DeprecationWarning ) return func ( * args , ** kwargs ) return wrapper return decorator
5888	def __crawl ( self , crawl_candidate ) : def crawler_wrapper ( parser , parsers_lst , crawl_candidate ) : try : crawler = Crawler ( self . config , self . fetcher ) article = crawler . crawl ( crawl_candidate ) except ( UnicodeDecodeError , ValueError ) as ex : if parsers_lst : parser = parsers_lst . pop ( 0 ) return crawler_wrapper ( parser , parsers_lst , crawl_candidate ) else : raise ex return article parsers = list ( self . config . available_parsers ) parsers . remove ( self . config . parser_class ) return crawler_wrapper ( self . config . parser_class , parsers , crawl_candidate )
5397	def _delocalize_outputs_commands ( self , task_dir , outputs , user_project ) : commands = [ ] for o in outputs : if o . recursive or not o . value : continue dest_path = o . uri . path local_path = task_dir + '/' + _DATA_SUBDIR + '/' + o . docker_path if o . file_provider == job_model . P_LOCAL : commands . append ( 'mkdir -p "%s"' % dest_path ) if o . file_provider in [ job_model . P_LOCAL , job_model . P_GCS ] : if user_project : command = 'gsutil -u %s -mq cp "%s" "%s"' % ( user_project , local_path , dest_path ) else : command = 'gsutil -mq cp "%s" "%s"' % ( local_path , dest_path ) commands . append ( command ) return '\n' . join ( commands )
8637	def award_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'award' } endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotAwardedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
5459	def find_task_descriptor ( self , task_id ) : for task_descriptor in self . task_descriptors : if task_descriptor . task_metadata . get ( 'task-id' ) == task_id : return task_descriptor return None
5929	def getLogLevel ( self , section , option ) : return logging . getLevelName ( self . get ( section , option ) . upper ( ) )
8243	def guess_name ( clr ) : clr = Color ( clr ) if clr . is_transparent : return "transparent" if clr . is_black : return "black" if clr . is_white : return "white" if clr . is_black : return "black" for name in named_colors : try : r , g , b = named_colors [ name ] except : continue if r == clr . r and g == clr . g and b == clr . b : return name for shade in shades : if clr in shade : return shade . name + " " + clr . nearest_hue ( ) break return clr . nearest_hue ( )
3930	def _get_authorization_code ( session , credentials_prompt ) : browser = Browser ( session , OAUTH2_LOGIN_URL ) email = credentials_prompt . get_email ( ) browser . submit_form ( FORM_SELECTOR , { EMAIL_SELECTOR : email } ) password = credentials_prompt . get_password ( ) browser . submit_form ( FORM_SELECTOR , { PASSWORD_SELECTOR : password } ) if browser . has_selector ( TOTP_CHALLENGE_SELECTOR ) : browser . submit_form ( TOTP_CHALLENGE_SELECTOR , { } ) elif browser . has_selector ( PHONE_CHALLENGE_SELECTOR ) : browser . submit_form ( PHONE_CHALLENGE_SELECTOR , { } ) if browser . has_selector ( VERIFICATION_FORM_SELECTOR ) : if browser . has_selector ( TOTP_CODE_SELECTOR ) : input_selector = TOTP_CODE_SELECTOR elif browser . has_selector ( PHONE_CODE_SELECTOR ) : input_selector = PHONE_CODE_SELECTOR else : raise GoogleAuthError ( 'Unknown verification code input' ) verfification_code = credentials_prompt . get_verification_code ( ) browser . submit_form ( VERIFICATION_FORM_SELECTOR , { input_selector : verfification_code } ) try : return browser . get_cookie ( 'oauth_code' ) except KeyError : raise GoogleAuthError ( 'Authorization code cookie not found' )
2857	def read ( self , length ) : if ( 1 > length > 65536 ) : print ( 'the FTDI chip is limited to 65536 bytes (64 KB) of input/output per command!' ) print ( 'use for loops for larger reads' ) exit ( 1 ) command = 0x20 | ( self . lsbfirst << 3 ) | ( self . read_clock_ve << 2 ) logger . debug ( 'SPI read with command {0:2X}.' . format ( command ) ) lengthR = length if length % 2 == 1 : lengthR += 1 lengthR = lengthR / 2 lenremain = length - lengthR len_low = ( lengthR - 1 ) & 0xFF len_high = ( ( lengthR - 1 ) >> 8 ) & 0xFF self . _assert_cs ( ) self . _ft232h . _write ( str ( bytearray ( ( command , len_low , len_high ) ) ) ) payload1 = self . _ft232h . _poll_read ( lengthR ) self . _ft232h . _write ( str ( bytearray ( ( command , len_low , len_high ) ) ) ) payload2 = self . _ft232h . _poll_read ( lenremain ) self . _deassert_cs ( ) return bytearray ( payload1 + payload2 )
8972	def connect_to ( self , other_mesh ) : other_mesh . disconnect ( ) self . disconnect ( ) self . _connect_to ( other_mesh )
7748	def _process_iq_response ( self , stanza ) : stanza_id = stanza . stanza_id from_jid = stanza . from_jid if from_jid : ufrom = from_jid . as_unicode ( ) else : ufrom = None res_handler = err_handler = None try : res_handler , err_handler = self . _iq_response_handlers . pop ( ( stanza_id , ufrom ) ) except KeyError : logger . debug ( "No response handler for id={0!r} from={1!r}" . format ( stanza_id , ufrom ) ) logger . debug ( " from_jid: {0!r} peer: {1!r} me: {2!r}" . format ( from_jid , self . peer , self . me ) ) if ( ( from_jid == self . peer or from_jid == self . me or self . me and from_jid == self . me . bare ( ) ) ) : try : logger . debug ( " trying id={0!r} from=None" . format ( stanza_id ) ) res_handler , err_handler = self . _iq_response_handlers . pop ( ( stanza_id , None ) ) except KeyError : pass if stanza . stanza_type == "result" : if res_handler : response = res_handler ( stanza ) else : return False else : if err_handler : response = err_handler ( stanza ) else : return False self . _process_handler_result ( response ) return True
170	def draw_mask ( self , image_shape , size_lines = 1 , size_points = 0 , raise_if_out_of_image = False ) : heatmap = self . draw_heatmap_array ( image_shape , alpha_lines = 1.0 , alpha_points = 1.0 , size_lines = size_lines , size_points = size_points , antialiased = False , raise_if_out_of_image = raise_if_out_of_image ) return heatmap > 0.5
2462	def set_file_spdx_id ( self , doc , spdx_id ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_spdx_id_set : self . file_spdx_id_set = True if validations . validate_file_spdx_id ( spdx_id ) : self . file ( doc ) . spdx_id = spdx_id return True else : raise SPDXValueError ( 'File::SPDXID' ) else : raise CardinalityError ( 'File::SPDXID' ) else : raise OrderError ( 'File::SPDXID' )
5695	def import_ ( self , conn ) : if self . print_progress : print ( 'Beginning' , self . __class__ . __name__ ) self . _conn = conn self . create_table ( conn ) if self . mode in ( 'all' , 'import' ) and self . fname and self . exists ( ) and self . table not in ignore_tables : self . insert_data ( conn ) if self . mode in ( 'all' , 'index' ) and hasattr ( self , 'index' ) : self . create_index ( conn ) if self . mode in ( 'all' , 'import' ) and hasattr ( self , 'post_import' ) : self . run_post_import ( conn ) conn . commit ( )
3539	def hubspot ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return HubSpotNode ( )
9635	def execute_from_command_line ( argv = None ) : parser = argparse . ArgumentParser ( description = __doc__ ) parser . add_argument ( '--monitors-dir' , default = MONITORS_DIR ) parser . add_argument ( '--alerts-dir' , default = ALERTS_DIR ) parser . add_argument ( '--config' , default = SMA_INI_FILE ) parser . add_argument ( '--warning' , help = 'set logging to warning' , action = 'store_const' , dest = 'loglevel' , const = logging . WARNING , default = logging . INFO ) parser . add_argument ( '--quiet' , help = 'set logging to ERROR' , action = 'store_const' , dest = 'loglevel' , const = logging . ERROR , default = logging . INFO ) parser . add_argument ( '--debug' , help = 'set logging to DEBUG' , action = 'store_const' , dest = 'loglevel' , const = logging . DEBUG , default = logging . INFO ) parser . add_argument ( '--verbose' , help = 'set logging to COMM' , action = 'store_const' , dest = 'loglevel' , const = 5 , default = logging . INFO ) parser . sub = parser . add_subparsers ( ) parse_service = parser . sub . add_parser ( 'service' , help = 'Run SMA as service (daemon).' ) parse_service . set_defaults ( which = 'service' ) parse_oneshot = parser . sub . add_parser ( 'one-shot' , help = 'Run SMA once and exit' ) parse_oneshot . set_defaults ( which = 'one-shot' ) parse_alerts = parser . sub . add_parser ( 'alerts' , help = 'Alerts options.' ) parse_alerts . set_defaults ( which = 'alerts' ) parse_alerts . add_argument ( '--test' , help = 'Test alert' , action = 'store_true' ) parse_alerts . add_argument ( 'alert_section' , nargs = '?' , help = 'Alert section to see' ) parse_results = parser . sub . add_parser ( 'results' , help = 'Monitors results' ) parse_results . set_defaults ( which = 'results' ) parser . set_default_subparser ( 'one-shot' ) args = parser . parse_args ( argv [ 1 : ] ) create_logger ( 'sma' , args . loglevel ) if not getattr ( args , 'which' , None ) or args . which == 'one-shot' : sma = SMA ( args . monitors_dir , args . alerts_dir , args . config ) sma . evaluate_and_alert ( ) elif args . which == 'service' : sma = SMAService ( args . monitors_dir , args . alerts_dir , args . config ) sma . start ( ) elif args . which == 'alerts' and args . test : sma = SMA ( args . monitors_dir , args . alerts_dir , args . config ) sma . alerts . test ( ) elif args . which == 'results' : print ( SMA ( args . monitors_dir , args . alerts_dir , args . config ) . results )
5545	def pyramid ( input_raster , output_dir , pyramid_type = None , output_format = None , resampling_method = None , scale_method = None , zoom = None , bounds = None , overwrite = False , debug = False ) : bounds = bounds if bounds else None options = dict ( pyramid_type = pyramid_type , scale_method = scale_method , output_format = output_format , resampling = resampling_method , zoom = zoom , bounds = bounds , overwrite = overwrite ) raster2pyramid ( input_raster , output_dir , options )
7323	def sendmail ( message , sender , recipients , config_filename ) : if not hasattr ( sendmail , "host" ) : config = configparser . RawConfigParser ( ) config . read ( config_filename ) sendmail . host = config . get ( "smtp_server" , "host" ) sendmail . port = config . getint ( "smtp_server" , "port" ) sendmail . username = config . get ( "smtp_server" , "username" ) sendmail . security = config . get ( "smtp_server" , "security" ) print ( ">>> Read SMTP server configuration from {}" . format ( config_filename ) ) print ( ">>> host = {}" . format ( sendmail . host ) ) print ( ">>> port = {}" . format ( sendmail . port ) ) print ( ">>> username = {}" . format ( sendmail . username ) ) print ( ">>> security = {}" . format ( sendmail . security ) ) if not hasattr ( sendmail , "password" ) : if sendmail . security == "Dummy" or sendmail . username == "None" : sendmail . password = None else : prompt = ">>> password for {} on {}: " . format ( sendmail . username , sendmail . host ) sendmail . password = getpass . getpass ( prompt ) if sendmail . security == "SSL/TLS" : smtp = smtplib . SMTP_SSL ( sendmail . host , sendmail . port ) elif sendmail . security == "STARTTLS" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) smtp . ehlo ( ) smtp . starttls ( ) smtp . ehlo ( ) elif sendmail . security == "Never" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) elif sendmail . security == "Dummy" : smtp = smtp_dummy . SMTP_dummy ( ) else : raise configparser . Error ( "Unrecognized security type: {}" . format ( sendmail . security ) ) if sendmail . username != "None" : smtp . login ( sendmail . username , sendmail . password ) smtp . sendmail ( sender , recipients , message . as_string ( ) ) smtp . close ( )
11939	def broadcast_message ( level , message_text , extra_tags = '' , date = None , url = None , fail_silently = False ) : from django . contrib . auth import get_user_model users = get_user_model ( ) . objects . all ( ) add_message_for ( users , level , message_text , extra_tags = extra_tags , date = date , url = url , fail_silently = fail_silently )
9348	def year ( past = False , min_delta = 0 , max_delta = 20 ) : return dt . date . today ( ) . year + _delta ( past , min_delta , max_delta )
3901	def _exception_handler ( self , _loop , context ) : self . _coroutine_queue . put ( self . _client . disconnect ( ) ) default_exception = Exception ( context . get ( 'message' ) ) self . _exception = context . get ( 'exception' , default_exception )
2036	def SSTORE ( self , offset , value ) : storage_address = self . address self . _publish ( 'will_evm_write_storage' , storage_address , offset , value ) if istainted ( self . pc ) : for taint in get_taints ( self . pc ) : value = taint_with ( value , taint ) self . world . set_storage_data ( storage_address , offset , value ) self . _publish ( 'did_evm_write_storage' , storage_address , offset , value )
9714	async def connect ( host , port = 22223 , version = "1.19" , on_event = None , on_disconnect = None , timeout = 5 , loop = None , ) -> QRTConnection : loop = loop or asyncio . get_event_loop ( ) try : _ , protocol = await loop . create_connection ( lambda : QTMProtocol ( loop = loop , on_event = on_event , on_disconnect = on_disconnect ) , host , port , ) except ( ConnectionRefusedError , TimeoutError , OSError ) as exception : LOG . error ( exception ) return None try : await protocol . set_version ( version ) except QRTCommandException as exception : LOG . error ( Exception ) return None except TypeError as exception : LOG . error ( exception ) return None return QRTConnection ( protocol , timeout = timeout )
2943	def validate ( self ) : results = [ ] from . . specs import Join def recursive_find_loop ( task , history ) : current = history [ : ] current . append ( task ) if isinstance ( task , Join ) : if task in history : msg = "Found loop with '%s': %s then '%s' again" % ( task . name , '->' . join ( [ p . name for p in history ] ) , task . name ) raise Exception ( msg ) for predecessor in task . inputs : recursive_find_loop ( predecessor , current ) for parent in task . inputs : recursive_find_loop ( parent , current ) for task_id , task in list ( self . task_specs . items ( ) ) : try : recursive_find_loop ( task , [ ] ) except Exception as exc : results . append ( exc . __str__ ( ) ) if not task . inputs and task . name not in [ 'Start' , 'Root' ] : if task . outputs : results . append ( "Task '%s' is disconnected (no inputs)" % task . name ) else : LOG . debug ( "Task '%s' is not being used" % task . name ) return results
5673	def from_directory_as_inmemory_db ( cls , gtfs_directory ) : from gtfspy . import_gtfs import import_gtfs conn = sqlite3 . connect ( ":memory:" ) import_gtfs ( gtfs_directory , conn , preserve_connection = True , print_progress = False ) return cls ( conn )
5055	def get_idp_choices ( ) : try : from third_party_auth . provider import Registry except ImportError as exception : LOGGER . warning ( "Could not import Registry from third_party_auth.provider" ) LOGGER . warning ( exception ) Registry = None first = [ ( "" , "-" * 7 ) ] if Registry : return first + [ ( idp . provider_id , idp . name ) for idp in Registry . enabled ( ) ] return None
10772	def filled_contour ( self , min = None , max = None ) : if min is None : min = np . finfo ( np . float64 ) . min if max is None : max = np . finfo ( np . float64 ) . max vertices , codes = ( self . _contour_generator . create_filled_contour ( min , max ) ) return self . formatter ( ( min , max ) , vertices , codes )
4046	def num_tagitems ( self , tag ) : query = "/{t}/{u}/tags/{ta}/items" . format ( u = self . library_id , t = self . library_type , ta = tag ) return self . _totals ( query )
613	def _generateExtraMetricSpecs ( options ) : _metricSpecSchema = { 'properties' : { } } results = [ ] for metric in options [ 'metrics' ] : for propertyName in _metricSpecSchema [ 'properties' ] . keys ( ) : _getPropertyValue ( _metricSpecSchema , propertyName , metric ) specString , label = _generateMetricSpecString ( field = metric [ 'field' ] , metric = metric [ 'metric' ] , params = metric [ 'params' ] , inferenceElement = metric [ 'inferenceElement' ] , returnLabel = True ) if metric [ 'logged' ] : options [ 'loggedMetrics' ] . append ( label ) results . append ( specString ) return results
5840	def check_predict_status ( self , view_id , predict_request_id ) : failure_message = "Get status on predict failed" bare_response = self . _get_success_json ( self . _get ( 'v1/data_views/' + str ( view_id ) + '/predict/' + str ( predict_request_id ) + '/status' , None , failure_message = failure_message ) ) result = bare_response [ "data" ] return result
2802	def convert_concat ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting concat ...' ) concat_nodes = [ layers [ i ] for i in inputs ] if len ( concat_nodes ) == 1 : layers [ scope_name ] = concat_nodes [ 0 ] return if names == 'short' : tf_name = 'CAT' + random_string ( 5 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) cat = keras . layers . Concatenate ( name = tf_name , axis = params [ 'axis' ] ) layers [ scope_name ] = cat ( concat_nodes )
11855	def predictor ( self , ( i , j , A , alpha , Bb ) ) : "Add to chart any rules for B that could help extend this edge." B = Bb [ 0 ] if B in self . grammar . rules : for rhs in self . grammar . rewrites_for ( B ) : self . add_edge ( [ j , j , B , [ ] , rhs ] )
5762	def write_groovy_script_and_configs ( filename , content , job_configs , view_configs = None ) : with open ( filename , 'w' ) as h : h . write ( content ) if view_configs : view_config_dir = os . path . join ( os . path . dirname ( filename ) , 'view_configs' ) if not os . path . isdir ( view_config_dir ) : os . makedirs ( view_config_dir ) for config_name , config_body in view_configs . items ( ) : config_filename = os . path . join ( view_config_dir , config_name ) with open ( config_filename , 'w' ) as config_fh : config_fh . write ( config_body ) job_config_dir = os . path . join ( os . path . dirname ( filename ) , 'job_configs' ) if not os . path . isdir ( job_config_dir ) : os . makedirs ( job_config_dir ) format_str = '%0' + str ( len ( str ( len ( job_configs ) ) ) ) + 'd' i = 0 for config_name , config_body in job_configs . items ( ) : i += 1 config_filename = os . path . join ( job_config_dir , format_str % i + ' ' + config_name ) with open ( config_filename , 'w' ) as config_fh : config_fh . write ( config_body )
9212	def get_channel_image ( self , channel , img_size = 300 , skip_cache = False ) : from bs4 import BeautifulSoup from wikipedia . exceptions import PageError import re import wikipedia wikipedia . set_lang ( 'fr' ) if not channel : _LOGGER . error ( 'Channel is not set. Could not retrieve image.' ) return if channel in self . _cache_channel_img and not skip_cache : img = self . _cache_channel_img [ channel ] _LOGGER . debug ( 'Cache hit: %s -> %s' , channel , img ) return img channel_info = self . get_channel_info ( channel ) query = channel_info [ 'wiki_page' ] if not query : _LOGGER . debug ( 'Wiki page is not set for channel %s' , channel ) return _LOGGER . debug ( 'Query: %s' , query ) if 'max_img_size' in channel_info : if img_size > channel_info [ 'max_img_size' ] : _LOGGER . info ( 'Requested image size is bigger than the max, ' 'setting it to %s' , channel_info [ 'max_img_size' ] ) img_size = channel_info [ 'max_img_size' ] try : page = wikipedia . page ( query ) _LOGGER . debug ( 'Wikipedia article title: %s' , page . title ) soup = BeautifulSoup ( page . html ( ) , 'html.parser' ) images = soup . find_all ( 'img' ) img_src = None for i in images : if i [ 'alt' ] . startswith ( 'Image illustrative' ) : img_src = re . sub ( r'\d+px' , '{}px' . format ( img_size ) , i [ 'src' ] ) img = 'https:{}' . format ( img_src ) if img_src else None self . _cache_channel_img [ channel ] = img return img except PageError : _LOGGER . error ( 'Could not fetch channel image for %s' , channel )
5218	def hist_file ( ticker : str , dt , typ = 'TRADE' ) -> str : data_path = os . environ . get ( assist . BBG_ROOT , '' ) . replace ( '\\' , '/' ) if not data_path : return '' asset = ticker . split ( ) [ - 1 ] proper_ticker = ticker . replace ( '/' , '_' ) cur_dt = pd . Timestamp ( dt ) . strftime ( '%Y-%m-%d' ) return f'{data_path}/{asset}/{proper_ticker}/{typ}/{cur_dt}.parq'
2157	def _auto_help_text ( self , help_text ) : api_doc_delimiter = '=====API DOCS=====' begin_api_doc = help_text . find ( api_doc_delimiter ) if begin_api_doc >= 0 : end_api_doc = help_text . rfind ( api_doc_delimiter ) + len ( api_doc_delimiter ) help_text = help_text [ : begin_api_doc ] + help_text [ end_api_doc : ] an_prefix = ( 'a' , 'e' , 'i' , 'o' ) if not self . resource_name . lower ( ) . startswith ( an_prefix ) : help_text = help_text . replace ( 'an object' , 'a %s' % self . resource_name ) if self . resource_name . lower ( ) . endswith ( 'y' ) : help_text = help_text . replace ( 'objects' , '%sies' % self . resource_name [ : - 1 ] , ) help_text = help_text . replace ( 'object' , self . resource_name ) help_text = help_text . replace ( 'keyword argument' , 'option' ) help_text = help_text . replace ( 'raise an exception' , 'abort with an error' ) for match in re . findall ( r'`([\w_]+)`' , help_text ) : option = '--%s' % match . replace ( '_' , '-' ) help_text = help_text . replace ( '`%s`' % match , option ) return help_text
121	def get_batch ( self ) : if self . all_finished ( ) : return None batch_str = self . queue_result . get ( ) batch = pickle . loads ( batch_str ) if batch is not None : return batch else : self . nb_workers_finished += 1 if self . nb_workers_finished >= self . nb_workers : try : self . queue_source . get ( timeout = 0.001 ) except QueueEmpty : pass return None else : return self . get_batch ( )
12660	def copy ( configfile = '' , destpath = '' , overwrite = False , sub_node = '' ) : log . info ( 'Running {0} {1} {2}' . format ( os . path . basename ( __file__ ) , whoami ( ) , locals ( ) ) ) assert ( os . path . isfile ( configfile ) ) if os . path . exists ( destpath ) : if os . listdir ( destpath ) : raise FolderAlreadyExists ( 'Folder {0} already exists. Please clean ' 'it or change destpath.' . format ( destpath ) ) else : log . info ( 'Creating folder {0}' . format ( destpath ) ) path ( destpath ) . makedirs_p ( ) from boyle . files . file_tree_map import FileTreeMap file_map = FileTreeMap ( ) try : file_map . from_config_file ( configfile ) except Exception as e : raise FileTreeMapError ( str ( e ) ) if sub_node : sub_map = file_map . get_node ( sub_node ) if not sub_map : raise FileTreeMapError ( 'Could not find sub node ' '{0}' . format ( sub_node ) ) file_map . _filetree = { } file_map . _filetree [ sub_node ] = sub_map try : file_map . copy_to ( destpath , overwrite = overwrite ) except Exception as e : raise FileTreeMapError ( str ( e ) )
13869	def _GetNativeEolStyle ( platform = sys . platform ) : _NATIVE_EOL_STYLE_MAP = { 'win32' : EOL_STYLE_WINDOWS , 'linux2' : EOL_STYLE_UNIX , 'linux' : EOL_STYLE_UNIX , 'darwin' : EOL_STYLE_MAC , } result = _NATIVE_EOL_STYLE_MAP . get ( platform ) if result is None : from . _exceptions import UnknownPlatformError raise UnknownPlatformError ( platform ) return result
844	def _rebuildPartitionIdMap ( self , partitionIdList ) : self . _partitionIdMap = { } for row , partitionId in enumerate ( partitionIdList ) : indices = self . _partitionIdMap . get ( partitionId , [ ] ) indices . append ( row ) self . _partitionIdMap [ partitionId ] = indices
7173	def calc_intent ( self , query ) : matches = self . calc_intents ( query ) if len ( matches ) == 0 : return MatchData ( '' , '' ) best_match = max ( matches , key = lambda x : x . conf ) best_matches = ( match for match in matches if match . conf == best_match . conf ) return min ( best_matches , key = lambda x : sum ( map ( len , x . matches . values ( ) ) ) )
128	def project ( self , from_shape , to_shape ) : if from_shape [ 0 : 2 ] == to_shape [ 0 : 2 ] : return self . copy ( ) ls_proj = self . to_line_string ( closed = False ) . project ( from_shape , to_shape ) return self . copy ( exterior = ls_proj . coords )
804	def modelAdoptNextOrphan ( self , jobId , maxUpdateInterval ) : @ g_retrySQL def findCandidateModelWithRetries ( ) : modelID = None with ConnectionFactory . get ( ) as conn : query = 'SELECT model_id FROM %s ' ' WHERE status=%%s ' ' AND job_id=%%s ' ' AND TIMESTAMPDIFF(SECOND, ' ' _eng_last_update_time, ' ' UTC_TIMESTAMP()) > %%s ' ' LIMIT 1 ' % ( self . modelsTableName , ) sqlParams = [ self . STATUS_RUNNING , jobId , maxUpdateInterval ] numRows = conn . cursor . execute ( query , sqlParams ) rows = conn . cursor . fetchall ( ) assert numRows <= 1 , "Unexpected numRows: %r" % numRows if numRows == 1 : ( modelID , ) = rows [ 0 ] return modelID @ g_retrySQL def adoptModelWithRetries ( modelID ) : adopted = False with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET _eng_worker_conn_id=%%s, ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE model_id=%%s ' ' AND status=%%s' ' AND TIMESTAMPDIFF(SECOND, ' ' _eng_last_update_time, ' ' UTC_TIMESTAMP()) > %%s ' ' LIMIT 1 ' % ( self . modelsTableName , ) sqlParams = [ self . _connectionID , modelID , self . STATUS_RUNNING , maxUpdateInterval ] numRowsAffected = conn . cursor . execute ( query , sqlParams ) assert numRowsAffected <= 1 , 'Unexpected numRowsAffected=%r' % ( numRowsAffected , ) if numRowsAffected == 1 : adopted = True else : ( status , connectionID ) = self . _getOneMatchingRowNoRetries ( self . _models , conn , { 'model_id' : modelID } , [ 'status' , '_eng_worker_conn_id' ] ) adopted = ( status == self . STATUS_RUNNING and connectionID == self . _connectionID ) return adopted adoptedModelID = None while True : modelID = findCandidateModelWithRetries ( ) if modelID is None : break if adoptModelWithRetries ( modelID ) : adoptedModelID = modelID break return adoptedModelID
7573	def progressbar ( njobs , finished , msg = "" , spacer = " " ) : if njobs : progress = 100 * ( finished / float ( njobs ) ) else : progress = 100 hashes = '#' * int ( progress / 5. ) nohash = ' ' * int ( 20 - len ( hashes ) ) if not ipyrad . __interactive__ : msg = msg . rsplit ( "|" , 2 ) [ 0 ] args = [ spacer , hashes + nohash , int ( progress ) , msg ] print ( "\r{}[{}] {:>3}% {} " . format ( * args ) , end = "" ) sys . stdout . flush ( )
8441	def _parse_link_header ( headers ) : links = { } if 'link' in headers : link_headers = headers [ 'link' ] . split ( ', ' ) for link_header in link_headers : ( url , rel ) = link_header . split ( '; ' ) url = url [ 1 : - 1 ] rel = rel [ 5 : - 1 ] links [ rel ] = url return links
5535	def read ( self , output_tile ) : if self . config . mode not in [ "readonly" , "continue" , "overwrite" ] : raise ValueError ( "process mode must be readonly, continue or overwrite" ) if isinstance ( output_tile , tuple ) : output_tile = self . config . output_pyramid . tile ( * output_tile ) elif isinstance ( output_tile , BufferedTile ) : pass else : raise TypeError ( "output_tile must be tuple or BufferedTile" ) return self . config . output . read ( output_tile )
11293	def oembed_schema ( request ) : current_domain = Site . objects . get_current ( ) . domain url_schemes = [ ] endpoint = reverse ( 'oembed_json' ) providers = oembed . site . get_providers ( ) for provider in providers : if not provider . provides : continue match = None if isinstance ( provider , DjangoProvider ) : url_pattern = resolver . reverse_dict . get ( provider . _meta . named_view ) if url_pattern : regex = re . sub ( r'%\(.+?\)s' , '*' , url_pattern [ 0 ] [ 0 ] [ 0 ] ) match = 'http://%s/%s' % ( current_domain , regex ) elif isinstance ( provider , HTTPProvider ) : match = provider . url_scheme else : match = provider . regex if match : url_schemes . append ( { 'type' : provider . resource_type , 'matches' : match , 'endpoint' : endpoint } ) url_schemes . sort ( key = lambda item : item [ 'matches' ] ) response = HttpResponse ( mimetype = 'application/json' ) response . write ( simplejson . dumps ( url_schemes ) ) return response
13657	def _forObject ( self , obj ) : router = type ( self ) ( ) router . _routes = list ( self . _routes ) router . _self = obj return router
5593	def tiles_from_geom ( self , geometry , zoom ) : for tile in self . tile_pyramid . tiles_from_geom ( geometry , zoom ) : yield self . tile ( * tile . id )
12582	def get_3D_from_4D ( filename , vol_idx = 0 ) : def remove_4th_element_from_hdr_string ( hdr , fieldname ) : if fieldname in hdr : hdr [ fieldname ] = ' ' . join ( hdr [ fieldname ] . split ( ) [ : 3 ] ) vol , hdr = load_raw_data_with_mhd ( filename ) if vol . ndim != 4 : raise ValueError ( 'Volume in {} does not have 4 dimensions.' . format ( op . join ( op . dirname ( filename ) , hdr [ 'ElementDataFile' ] ) ) ) if not 0 <= vol_idx < vol . shape [ 3 ] : raise IndexError ( 'IndexError: 4th dimension in volume {} has {} volumes, not {}.' . format ( filename , vol . shape [ 3 ] , vol_idx ) ) new_vol = vol [ : , : , : , vol_idx ] . copy ( ) hdr [ 'NDims' ] = 3 remove_4th_element_from_hdr_string ( hdr , 'ElementSpacing' ) remove_4th_element_from_hdr_string ( hdr , 'DimSize' ) return new_vol , hdr
12217	def print_file_info ( ) : tpl = TableLogger ( columns = 'file,created,modified,size' ) for f in os . listdir ( '.' ) : size = os . stat ( f ) . st_size date_created = datetime . fromtimestamp ( os . path . getctime ( f ) ) date_modified = datetime . fromtimestamp ( os . path . getmtime ( f ) ) tpl ( f , date_created , date_modified , size )
7504	def _parse_names ( self ) : self . samples = [ ] with iter ( open ( self . files . data , 'r' ) ) as infile : infile . next ( ) . strip ( ) . split ( ) while 1 : try : self . samples . append ( infile . next ( ) . split ( ) [ 0 ] ) except StopIteration : break
6200	def _sim_timestamps ( self , max_rate , bg_rate , emission , i_start , rs , ip_start = 0 , scale = 10 , sort = True ) : counts_chunk = sim_timetrace_bg ( emission , max_rate , bg_rate , self . t_step , rs = rs ) nrows = emission . shape [ 0 ] if bg_rate is not None : nrows += 1 assert counts_chunk . shape == ( nrows , emission . shape [ 1 ] ) max_counts = counts_chunk . max ( ) if max_counts == 0 : return np . array ( [ ] , dtype = np . int64 ) , np . array ( [ ] , dtype = np . int64 ) time_start = i_start * scale time_stop = time_start + counts_chunk . shape [ 1 ] * scale ts_range = np . arange ( time_start , time_stop , scale , dtype = 'int64' ) times_chunk_p = [ ] par_index_chunk_p = [ ] for ip , counts_chunk_ip in enumerate ( counts_chunk ) : times_c_ip = [ ] for v in range ( 1 , max_counts + 1 ) : times_c_ip . append ( ts_range [ counts_chunk_ip >= v ] ) t = np . hstack ( times_c_ip ) times_chunk_p . append ( t ) par_index_chunk_p . append ( np . full ( t . size , ip + ip_start , dtype = 'u1' ) ) times_chunk = np . hstack ( times_chunk_p ) par_index_chunk = np . hstack ( par_index_chunk_p ) if sort : index_sort = times_chunk . argsort ( kind = 'mergesort' ) times_chunk = times_chunk [ index_sort ] par_index_chunk = par_index_chunk [ index_sort ] return times_chunk , par_index_chunk
10367	def complex_increases_activity ( graph : BELGraph , u : BaseEntity , v : BaseEntity , key : str ) -> bool : return ( isinstance ( u , ( ComplexAbundance , NamedComplexAbundance ) ) and complex_has_member ( graph , u , v ) and part_has_modifier ( graph [ u ] [ v ] [ key ] , OBJECT , ACTIVITY ) )
9948	def new_space_from_excel ( self , book , range_ , sheet = None , name = None , names_row = None , param_cols = None , space_param_order = None , cells_param_order = None , transpose = False , names_col = None , param_rows = None , ) : space = self . _impl . new_space_from_excel ( book , range_ , sheet , name , names_row , param_cols , space_param_order , cells_param_order , transpose , names_col , param_rows , ) return get_interfaces ( space )
12105	def _qsub_block ( self , output_dir , error_dir , tid_specs ) : processes = [ ] job_names = [ ] for ( tid , spec ) in tid_specs : job_name = "%s_%s_tid_%d" % ( self . batch_name , self . job_timestamp , tid ) job_names . append ( job_name ) cmd_args = self . command ( self . command . _formatter ( spec ) , tid , self . _launchinfo ) popen_args = self . _qsub_args ( [ ( "-e" , error_dir ) , ( '-N' , job_name ) , ( "-o" , output_dir ) ] , cmd_args ) p = subprocess . Popen ( popen_args , stdout = subprocess . PIPE ) ( stdout , stderr ) = p . communicate ( ) self . debug ( stdout ) if p . poll ( ) != 0 : raise EnvironmentError ( "qsub command exit with code: %d" % p . poll ( ) ) processes . append ( p ) self . message ( "Invoked qsub for %d commands" % len ( processes ) ) if ( self . reduction_fn is not None ) or self . dynamic : self . _qsub_collate_and_launch ( output_dir , error_dir , job_names )
13451	def imgmax ( self ) : if not hasattr ( self , '_imgmax' ) : imgmax = _np . max ( self . images [ 0 ] ) for img in self . images : imax = _np . max ( img ) if imax > imgmax : imgmax = imax self . _imgmax = imgmax return self . _imgmax
664	def makeCloneMap ( columnsShape , outputCloningWidth , outputCloningHeight = - 1 ) : if outputCloningHeight < 0 : outputCloningHeight = outputCloningWidth columnsHeight , columnsWidth = columnsShape numDistinctMasters = outputCloningWidth * outputCloningHeight a = numpy . empty ( ( columnsHeight , columnsWidth ) , 'uint32' ) for row in xrange ( columnsHeight ) : for col in xrange ( columnsWidth ) : a [ row , col ] = ( col % outputCloningWidth ) + ( row % outputCloningHeight ) * outputCloningWidth return a , numDistinctMasters
4230	def argparser ( ) : parser = ArgumentParser ( prog = 'pynetgear' ) parser . add_argument ( "--format" , choices = [ 'json' , 'prettyjson' , 'py' ] , default = 'prettyjson' ) router_args = parser . add_argument_group ( "router connection config" ) router_args . add_argument ( "--host" , help = "Hostname for the router" ) router_args . add_argument ( "--user" , help = "Account for login" ) router_args . add_argument ( "--port" , help = "Port exposed on the router" ) router_args . add_argument ( "--login-v2" , help = "Force the use of the cookie-based authentication" , dest = "force_login_v2" , default = False , action = "store_true" ) router_args . add_argument ( "--password" , help = "Not required with a wired connection." + "Optionally, set the PYNETGEAR_PASSWORD environment variable" ) router_args . add_argument ( "--url" , help = "Overrides host:port and ssl with url to router" ) router_args . add_argument ( "--no-ssl" , dest = "ssl" , default = True , action = "store_false" , help = "Connect with https" ) subparsers = parser . add_subparsers ( description = "Runs subcommand against the specified router" , dest = "subcommand" ) block_parser = subparsers . add_parser ( "block_device" , help = "Blocks a device from connecting by mac address" ) block_parser . add_argument ( "--mac-addr" ) allow_parser = subparsers . add_parser ( "allow_device" , help = "Allows a device with the mac address to connect" ) allow_parser . add_argument ( "--mac-addr" ) subparsers . add_parser ( "login" , help = "Attempts to login to router." ) attached_devices = subparsers . add_parser ( "attached_devices" , help = "Outputs all attached devices" ) attached_devices . add_argument ( "-v" , "--verbose" , action = "store_true" , default = False , help = "Choose between verbose and slower or terse and fast." ) subparsers . add_parser ( "traffic_meter" , help = "Output router's traffic meter data" ) return parser
3461	def single_gene_deletion ( model , gene_list = None , method = "fba" , solution = None , processes = None , ** kwargs ) : return _multi_deletion ( model , 'gene' , element_lists = _element_lists ( model . genes , gene_list ) , method = method , solution = solution , processes = processes , ** kwargs )
5571	def execute ( mp ) : with mp . open ( "file1" , resampling = "bilinear" ) as raster_file : if raster_file . is_empty ( ) : return "empty" dem = raster_file . read ( ) return dem
8226	def _makeInstance ( self , clazz , args , kwargs ) : inst = clazz ( self , * args , ** kwargs ) return inst
238	def create_full_tear_sheet ( returns , positions = None , transactions = None , market_data = None , benchmark_rets = None , slippage = None , live_start_date = None , sector_mappings = None , bayesian = False , round_trips = False , estimate_intraday = 'infer' , hide_positions = False , cone_std = ( 1.0 , 1.5 , 2.0 ) , bootstrap = False , unadjusted_returns = None , style_factor_panel = None , sectors = None , caps = None , shares_held = None , volumes = None , percentile = None , turnover_denom = 'AGB' , set_context = True , factor_returns = None , factor_loadings = None , pos_in_dollars = True , header_rows = None , factor_partitions = FACTOR_PARTITIONS ) : if ( unadjusted_returns is None ) and ( slippage is not None ) and ( transactions is not None ) : unadjusted_returns = returns . copy ( ) returns = txn . adjust_returns_for_slippage ( returns , positions , transactions , slippage ) positions = utils . check_intraday ( estimate_intraday , returns , positions , transactions ) create_returns_tear_sheet ( returns , positions = positions , transactions = transactions , live_start_date = live_start_date , cone_std = cone_std , benchmark_rets = benchmark_rets , bootstrap = bootstrap , turnover_denom = turnover_denom , header_rows = header_rows , set_context = set_context ) create_interesting_times_tear_sheet ( returns , benchmark_rets = benchmark_rets , set_context = set_context ) if positions is not None : create_position_tear_sheet ( returns , positions , hide_positions = hide_positions , set_context = set_context , sector_mappings = sector_mappings , estimate_intraday = False ) if transactions is not None : create_txn_tear_sheet ( returns , positions , transactions , unadjusted_returns = unadjusted_returns , estimate_intraday = False , set_context = set_context ) if round_trips : create_round_trip_tear_sheet ( returns = returns , positions = positions , transactions = transactions , sector_mappings = sector_mappings , estimate_intraday = False ) if market_data is not None : create_capacity_tear_sheet ( returns , positions , transactions , market_data , liquidation_daily_vol_limit = 0.2 , last_n_days = 125 , estimate_intraday = False ) if style_factor_panel is not None : create_risk_tear_sheet ( positions , style_factor_panel , sectors , caps , shares_held , volumes , percentile ) if factor_returns is not None and factor_loadings is not None : create_perf_attrib_tear_sheet ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars = pos_in_dollars , factor_partitions = factor_partitions ) if bayesian : create_bayesian_tear_sheet ( returns , live_start_date = live_start_date , benchmark_rets = benchmark_rets , set_context = set_context )
6581	def play ( self , song ) : self . _callbacks . play ( song ) self . _load_track ( song ) time . sleep ( 2 ) while True : try : self . _callbacks . pre_poll ( ) self . _ensure_started ( ) self . _loop_hook ( ) readers , _ , _ = select . select ( self . _get_select_readers ( ) , [ ] , [ ] , 1 ) for handle in readers : if handle . fileno ( ) == self . _control_fd : self . _callbacks . input ( handle . readline ( ) . strip ( ) , song ) else : value = self . _read_from_process ( handle ) if self . _player_stopped ( value ) : return finally : self . _callbacks . post_poll ( )
1494	def _trigger_timers ( self ) : current = time . time ( ) while len ( self . timer_tasks ) > 0 and ( self . timer_tasks [ 0 ] [ 0 ] - current <= 0 ) : task = heappop ( self . timer_tasks ) [ 1 ] task ( )
11726	def format_seconds ( self , n_seconds ) : func = self . ok if n_seconds >= 60 : n_minutes , n_seconds = divmod ( n_seconds , 60 ) return "%s minutes %s seconds" % ( func ( "%d" % n_minutes ) , func ( "%.3f" % n_seconds ) ) else : return "%s seconds" % ( func ( "%.3f" % n_seconds ) )
3390	def validate ( self , samples ) : samples = np . atleast_2d ( samples ) prob = self . problem if samples . shape [ 1 ] == len ( self . model . reactions ) : S = create_stoichiometric_matrix ( self . model ) b = np . array ( [ self . model . constraints [ m . id ] . lb for m in self . model . metabolites ] ) bounds = np . array ( [ r . bounds for r in self . model . reactions ] ) . T elif samples . shape [ 1 ] == len ( self . model . variables ) : S = prob . equalities b = prob . b bounds = prob . variable_bounds else : raise ValueError ( "Wrong number of columns. samples must have a " "column for each flux or variable defined in the " "model!" ) feasibility = np . abs ( S . dot ( samples . T ) . T - b ) . max ( axis = 1 ) lb_error = ( samples - bounds [ 0 , ] ) . min ( axis = 1 ) ub_error = ( bounds [ 1 , ] - samples ) . min ( axis = 1 ) if ( samples . shape [ 1 ] == len ( self . model . variables ) and prob . inequalities . shape [ 0 ] ) : consts = prob . inequalities . dot ( samples . T ) lb_error = np . minimum ( lb_error , ( consts - prob . bounds [ 0 , ] ) . min ( axis = 1 ) ) ub_error = np . minimum ( ub_error , ( prob . bounds [ 1 , ] - consts ) . min ( axis = 1 ) ) valid = ( ( feasibility < self . feasibility_tol ) & ( lb_error > - self . bounds_tol ) & ( ub_error > - self . bounds_tol ) ) codes = np . repeat ( "" , valid . shape [ 0 ] ) . astype ( np . dtype ( ( str , 3 ) ) ) codes [ valid ] = "v" codes [ lb_error <= - self . bounds_tol ] = np . char . add ( codes [ lb_error <= - self . bounds_tol ] , "l" ) codes [ ub_error <= - self . bounds_tol ] = np . char . add ( codes [ ub_error <= - self . bounds_tol ] , "u" ) codes [ feasibility > self . feasibility_tol ] = np . char . add ( codes [ feasibility > self . feasibility_tol ] , "e" ) return codes
6179	def merge_ph_times ( times_list , times_par_list , time_block ) : offsets = np . arange ( len ( times_list ) ) * time_block cum_sizes = np . cumsum ( [ ts . size for ts in times_list ] ) times = np . zeros ( cum_sizes [ - 1 ] ) times_par = np . zeros ( cum_sizes [ - 1 ] , dtype = 'uint8' ) i1 = 0 for i2 , ts , ts_par , offset in zip ( cum_sizes , times_list , times_par_list , offsets ) : times [ i1 : i2 ] = ts + offset times_par [ i1 : i2 ] = ts_par i1 = i2 return times , times_par
3249	def get_base ( managed_policy , ** conn ) : managed_policy [ '_version' ] = 1 arn = _get_name_from_structure ( managed_policy , 'Arn' ) policy = get_policy ( arn , ** conn ) document = get_managed_policy_document ( arn , policy_metadata = policy , ** conn ) managed_policy . update ( policy [ 'Policy' ] ) managed_policy [ 'Document' ] = document managed_policy [ 'CreateDate' ] = get_iso_string ( managed_policy [ 'CreateDate' ] ) managed_policy [ 'UpdateDate' ] = get_iso_string ( managed_policy [ 'UpdateDate' ] ) return managed_policy
7852	def get_identities ( self ) : ret = [ ] l = self . xpath_ctxt . xpathEval ( "d:identity" ) if l is not None : for i in l : ret . append ( DiscoIdentity ( self , i ) ) return ret
867	def clear ( cls ) : super ( Configuration , cls ) . clear ( ) _CustomConfigurationFileWrapper . clear ( persistent = False )
12402	def require ( self , req ) : reqs = req if isinstance ( req , list ) else [ req ] for req in reqs : if not isinstance ( req , BumpRequirement ) : req = BumpRequirement ( req ) req . required = True req . required_by = self self . requirements . append ( req )
10494	def clickMouseButtonRight ( self , coord ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonRight , modFlags ) self . _postQueuedEvents ( )
9583	def write_var_header ( fd , header ) : fd . write ( struct . pack ( 'b3xI' , etypes [ 'miUINT32' ] [ 'n' ] , 8 ) ) fd . write ( struct . pack ( 'b3x4x' , mclasses [ header [ 'mclass' ] ] ) ) write_elements ( fd , 'miINT32' , header [ 'dims' ] ) write_elements ( fd , 'miINT8' , asbytes ( header [ 'name' ] ) , is_name = True )
4254	def time_zone_by_country_and_region ( country_code , region_code = None ) : timezone = country_dict . get ( country_code ) if not timezone : return None if isinstance ( timezone , str ) : return timezone return timezone . get ( region_code )
10372	def node_has_namespace ( node : BaseEntity , namespace : str ) -> bool : ns = node . get ( NAMESPACE ) return ns is not None and ns == namespace
5878	def get_video ( self , node ) : video = Video ( ) video . _embed_code = self . get_embed_code ( node ) video . _embed_type = self . get_embed_type ( node ) video . _width = self . get_width ( node ) video . _height = self . get_height ( node ) video . _src = self . get_src ( node ) video . _provider = self . get_provider ( video . src ) return video
3175	def create ( self , campaign_id , data , ** queryparams ) : self . campaign_id = campaign_id if 'message' not in data : raise KeyError ( 'The campaign feedback must have a message' ) response = self . _mc_client . _post ( url = self . _build_path ( campaign_id , 'feedback' ) , data = data , ** queryparams ) if response is not None : self . feedback_id = response [ 'feedback_id' ] else : self . feedback_id = None return response
12628	def get_all_files ( folder ) : for path , dirlist , filelist in os . walk ( folder ) : for fn in filelist : yield op . join ( path , fn )
12998	def round_teff_luminosity ( cluster ) : temps = [ round ( t , - 1 ) for t in teff ( cluster ) ] lums = [ round ( l , 3 ) for l in luminosity ( cluster ) ] return temps , lums
10747	def fetch ( self , url , path , filename ) : logger . debug ( 'initializing download in ' , url ) remote_file_size = self . get_remote_file_size ( url ) if exists ( join ( path , filename ) ) : size = getsize ( join ( path , filename ) ) if size == remote_file_size : logger . error ( '%s already exists on your system' % filename ) print ( '%s already exists on your system' % filename ) return [ join ( path , filename ) , size ] logger . debug ( 'Downloading: %s' % filename ) print ( 'Downloading: %s' % filename ) fetch ( url , path ) print ( 'stored at %s' % path ) logger . debug ( 'stored at %s' % path ) return [ join ( path , filename ) , remote_file_size ]
13847	def splitext_files_only ( filepath ) : "Custom version of splitext that doesn't perform splitext on directories" return ( ( filepath , '' ) if os . path . isdir ( filepath ) else os . path . splitext ( filepath ) )
11536	def map_pin ( self , abstract_pin_id , physical_pin_id ) : if physical_pin_id : self . _pin_mapping [ abstract_pin_id ] = physical_pin_id else : self . _pin_mapping . pop ( abstract_pin_id , None )
12711	def add_force ( self , force , relative = False , position = None , relative_position = None ) : b = self . ode_body if relative_position is not None : op = b . addRelForceAtRelPos if relative else b . addForceAtRelPos op ( force , relative_position ) elif position is not None : op = b . addRelForceAtPos if relative else b . addForceAtPos op ( force , position ) else : op = b . addRelForce if relative else b . addForce op ( force )
13257	def save ( self , entry , with_location = True , debug = False ) : entry_dict = { } if isinstance ( entry , DayOneEntry ) : entry_dict = entry . as_dict ( ) else : entry_dict = entry entry_dict [ 'UUID' ] = uuid . uuid4 ( ) . get_hex ( ) if with_location and not entry_dict [ 'Location' ] : entry_dict [ 'Location' ] = self . get_location ( ) if not all ( ( entry_dict [ 'UUID' ] , entry_dict [ 'Time Zone' ] , entry_dict [ 'Entry Text' ] ) ) : print "You must provide: Time zone, UUID, Creation Date, Entry Text" return False if debug is False : file_path = self . _file_path ( entry_dict [ 'UUID' ] ) plistlib . writePlist ( entry_dict , file_path ) else : plist = plistlib . writePlistToString ( entry_dict ) print plist return True
2572	def dbm_starter ( priority_msgs , resource_msgs , * args , ** kwargs ) : dbm = DatabaseManager ( * args , ** kwargs ) dbm . start ( priority_msgs , resource_msgs )
9369	def legal_ogrn ( ) : ogrn = "" . join ( map ( str , [ random . randint ( 1 , 9 ) for _ in range ( 12 ) ] ) ) ogrn += str ( ( int ( ogrn ) % 11 % 10 ) ) return ogrn
10554	def get_helping_materials ( project_id , limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) params [ 'project_id' ] = project_id try : res = _pybossa_req ( 'get' , 'helpingmaterial' , params = params ) if type ( res ) . __name__ == 'list' : return [ HelpingMaterial ( helping ) for helping in res ] else : return res except : raise
460	def evaluation ( y_test = None , y_predict = None , n_classes = None ) : c_mat = confusion_matrix ( y_test , y_predict , labels = [ x for x in range ( n_classes ) ] ) f1 = f1_score ( y_test , y_predict , average = None , labels = [ x for x in range ( n_classes ) ] ) f1_macro = f1_score ( y_test , y_predict , average = 'macro' ) acc = accuracy_score ( y_test , y_predict ) tl . logging . info ( 'confusion matrix: \n%s' % c_mat ) tl . logging . info ( 'f1-score : %s' % f1 ) tl . logging . info ( 'f1-score(macro) : %f' % f1_macro ) tl . logging . info ( 'accuracy-score : %f' % acc ) return c_mat , f1 , acc , f1_macro
11301	def populate ( self ) : self . _registry = { } for provider_class in self . _registered_providers : instance = provider_class ( ) self . _registry [ instance ] = instance . regex for stored_provider in StoredProvider . objects . active ( ) : self . _registry [ stored_provider ] = stored_provider . regex self . _populated = True
6648	def _loadConfig ( self ) : config_dicts = [ self . additional_config , self . app_config ] + [ t . getConfig ( ) for t in self . hierarchy ] config_blame = [ _mirrorStructure ( self . additional_config , 'command-line config' ) , _mirrorStructure ( self . app_config , 'application\'s config.json' ) , ] + [ _mirrorStructure ( t . getConfig ( ) , t . getName ( ) ) for t in self . hierarchy ] self . config = _mergeDictionaries ( * config_dicts ) self . config_blame = _mergeDictionaries ( * config_blame )
10106	def _process_tabs ( self , tabs , current_tab , group_current_tab ) : for t in tabs : t . current_tab = current_tab t . group_current_tab = group_current_tab tabs = list ( filter ( lambda t : t . tab_visible , tabs ) ) tabs . sort ( key = lambda t : t . weight ) return tabs
10887	def _format_vector ( self , vecs , form = 'broadcast' ) : if form == 'meshed' : return np . meshgrid ( * vecs , indexing = 'ij' ) elif form == 'vector' : vecs = np . meshgrid ( * vecs , indexing = 'ij' ) return np . rollaxis ( np . array ( np . broadcast_arrays ( * vecs ) ) , 0 , self . dim + 1 ) elif form == 'flat' : return vecs else : return [ v [ self . _coord_slicers [ i ] ] for i , v in enumerate ( vecs ) ]
9424	def _open ( self , archive ) : try : handle = unrarlib . RAROpenArchiveEx ( ctypes . byref ( archive ) ) except unrarlib . UnrarException : raise BadRarFile ( "Invalid RAR file." ) return handle
6408	def lmean ( nums ) : r if len ( nums ) != len ( set ( nums ) ) : raise AttributeError ( 'No two values in the nums list may be equal' ) rolling_sum = 0 for i in range ( len ( nums ) ) : rolling_prod = 1 for j in range ( len ( nums ) ) : if i != j : rolling_prod *= math . log ( nums [ i ] / nums [ j ] ) rolling_sum += nums [ i ] / rolling_prod return math . factorial ( len ( nums ) - 1 ) * rolling_sum
13661	def _tempfile ( filename ) : return tempfile . NamedTemporaryFile ( mode = 'w' , dir = os . path . dirname ( filename ) , prefix = os . path . basename ( filename ) , suffix = os . fsencode ( '.tmp' ) , delete = False )
9478	def parse_string ( self , string ) : dom = minidom . parseString ( string ) return self . parse_dom ( dom )
9607	def vformat ( self , format_string , args , kwargs ) : self . _used_kwargs = { } self . _unused_kwargs = { } return super ( MemorizeFormatter , self ) . vformat ( format_string , args , kwargs )
10884	def slicer ( self ) : return tuple ( np . s_ [ l : r ] for l , r in zip ( * self . bounds ) )
12719	def axes ( self ) : return [ np . array ( self . ode_obj . getAxis ( i ) ) for i in range ( self . ADOF or self . LDOF ) ]
7938	def _connect ( self , addr , port , service ) : self . _dst_name = addr self . _dst_port = port family = None try : res = socket . getaddrinfo ( addr , port , socket . AF_UNSPEC , socket . SOCK_STREAM , 0 , socket . AI_NUMERICHOST ) family = res [ 0 ] [ 0 ] sockaddr = res [ 0 ] [ 4 ] except socket . gaierror : family = None sockaddr = None if family is not None : if not port : raise ValueError ( "No port number given with literal IP address" ) self . _dst_service = None self . _family = family self . _dst_addrs = [ ( family , sockaddr ) ] self . _set_state ( "connect" ) elif service is not None : self . _dst_service = service self . _set_state ( "resolve-srv" ) self . _dst_name = addr elif port : self . _dst_nameports = [ ( self . _dst_name , self . _dst_port ) ] self . _dst_service = None self . _set_state ( "resolve-hostname" ) else : raise ValueError ( "No port number and no SRV service name given" )
2822	def convert_relu ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting relu ...' ) if names == 'short' : tf_name = 'RELU' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) relu = keras . layers . Activation ( 'relu' , name = tf_name ) layers [ scope_name ] = relu ( layers [ inputs [ 0 ] ] )
378	def samplewise_norm ( x , rescale = None , samplewise_center = False , samplewise_std_normalization = False , channel_index = 2 , epsilon = 1e-7 ) : if rescale : x *= rescale if x . shape [ channel_index ] == 1 : if samplewise_center : x = x - np . mean ( x ) if samplewise_std_normalization : x = x / np . std ( x ) return x elif x . shape [ channel_index ] == 3 : if samplewise_center : x = x - np . mean ( x , axis = channel_index , keepdims = True ) if samplewise_std_normalization : x = x / ( np . std ( x , axis = channel_index , keepdims = True ) + epsilon ) return x else : raise Exception ( "Unsupported channels %d" % x . shape [ channel_index ] )
814	def pickByDistribution ( distribution , r = None ) : if r is None : r = random x = r . uniform ( 0 , sum ( distribution ) ) for i , d in enumerate ( distribution ) : if x <= d : return i x -= d
7653	def serialize_obj ( obj ) : if isinstance ( obj , np . integer ) : return int ( obj ) elif isinstance ( obj , np . floating ) : return float ( obj ) elif isinstance ( obj , np . ndarray ) : return obj . tolist ( ) elif isinstance ( obj , list ) : return [ serialize_obj ( x ) for x in obj ] elif isinstance ( obj , Observation ) : return { k : serialize_obj ( v ) for k , v in six . iteritems ( obj . _asdict ( ) ) } return obj
2955	def update ( self , containers ) : self . _containers = deepcopy ( containers ) self . __write ( containers , initialize = False )
12401	def satisfied_by_checked ( self , req ) : req_man = RequirementsManager ( [ req ] ) return any ( req_man . check ( * checked ) for checked in self . checked )
9530	def encrypt ( base_field , key = None , ttl = None ) : if not isinstance ( base_field , models . Field ) : assert key is None assert ttl is None return get_encrypted_field ( base_field ) name , path , args , kwargs = base_field . deconstruct ( ) kwargs . update ( { 'key' : key , 'ttl' : ttl } ) return get_encrypted_field ( base_field . __class__ ) ( * args , ** kwargs )
7775	def rfc2425encode ( name , value , parameters = None , charset = "utf-8" ) : if not parameters : parameters = { } if type ( value ) is unicode : value = value . replace ( u"\r\n" , u"\\n" ) value = value . replace ( u"\n" , u"\\n" ) value = value . replace ( u"\r" , u"\\n" ) value = value . encode ( charset , "replace" ) elif type ( value ) is not str : raise TypeError ( "Bad type for rfc2425 value" ) elif not valid_string_re . match ( value ) : parameters [ "encoding" ] = "b" value = binascii . b2a_base64 ( value ) ret = str ( name ) . lower ( ) for k , v in parameters . items ( ) : ret += ";%s=%s" % ( str ( k ) , str ( v ) ) ret += ":" while ( len ( value ) > 70 ) : ret += value [ : 70 ] + "\r\n " value = value [ 70 : ] ret += value + "\r\n" return ret
5135	def minimal_random_graph ( num_vertices , seed = None , ** kwargs ) : if isinstance ( seed , numbers . Integral ) : np . random . seed ( seed ) points = np . random . random ( ( num_vertices , 2 ) ) * 10 edges = [ ] for k in range ( num_vertices - 1 ) : for j in range ( k + 1 , num_vertices ) : v = points [ k ] - points [ j ] edges . append ( ( k , j , v [ 0 ] ** 2 + v [ 1 ] ** 2 ) ) mytype = [ ( 'n1' , int ) , ( 'n2' , int ) , ( 'distance' , np . float ) ] edges = np . array ( edges , dtype = mytype ) edges = np . sort ( edges , order = 'distance' ) unionF = UnionFind ( [ k for k in range ( num_vertices ) ] ) g = nx . Graph ( ) for n1 , n2 , dummy in edges : unionF . union ( n1 , n2 ) g . add_edge ( n1 , n2 ) if unionF . nClusters == 1 : break pos = { j : p for j , p in enumerate ( points ) } g = QueueNetworkDiGraph ( g . to_directed ( ) ) g . set_pos ( pos ) return g
4483	def create_file ( self , path , fp , force = False , update = False ) : if 'b' not in fp . mode : raise ValueError ( "File has to be opened in binary mode." ) path = norm_remote_path ( path ) directory , fname = os . path . split ( path ) directories = directory . split ( os . path . sep ) parent = self for directory in directories : if directory : parent = parent . create_folder ( directory , exist_ok = True ) url = parent . _new_file_url connection_error = False if file_empty ( fp ) : response = self . _put ( url , params = { 'name' : fname } , data = b'' ) else : try : response = self . _put ( url , params = { 'name' : fname } , data = fp ) except ConnectionError : connection_error = True if connection_error or response . status_code == 409 : if not force and not update : file_size_bytes = get_local_file_size ( fp ) large_file_cutoff = 2 ** 20 if connection_error and file_size_bytes < large_file_cutoff : msg = ( "There was a connection error which might mean {} " + "already exists. Try again with the `--force` flag " + "specified." ) . format ( path ) raise RuntimeError ( msg ) else : raise FileExistsError ( path ) else : for file_ in self . files : if norm_remote_path ( file_ . path ) == path : if not force : if checksum ( path ) == file_ . hashes . get ( 'md5' ) : break fp . seek ( 0 ) file_ . update ( fp ) break else : raise RuntimeError ( "Could not create a new file at " "({}) nor update it." . format ( path ) )
1359	def get_argument_instance ( self ) : try : instance = self . get_argument ( constants . PARAM_INSTANCE ) return instance except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
11942	def stored_messages_archive ( context , num_elements = 10 ) : if "user" in context : user = context [ "user" ] if user . is_authenticated ( ) : qs = MessageArchive . objects . select_related ( "message" ) . filter ( user = user ) return { "messages" : qs [ : num_elements ] , "count" : qs . count ( ) , }
12859	def to_date ( self ) : y , m , d = self . to_ymd ( ) return date ( y , m , d )
3174	def create_or_update ( self , store_id , product_id , variant_id , data ) : self . store_id = store_id self . product_id = product_id self . variant_id = variant_id if 'id' not in data : raise KeyError ( 'The product variant must have an id' ) if 'title' not in data : raise KeyError ( 'The product variant must have a title' ) return self . _mc_client . _put ( url = self . _build_path ( store_id , 'products' , product_id , 'variants' , variant_id ) , data = data )
8020	async def websocket_accept ( self , message , stream_name ) : is_first = not self . applications_accepting_frames self . applications_accepting_frames . add ( stream_name ) if is_first : await self . accept ( )
553	def readStateFromDB ( self ) : self . _priorStateJSON = self . _hsObj . _cjDAO . jobGetFields ( self . _hsObj . _jobID , [ 'engWorkerState' ] ) [ 0 ] if self . _priorStateJSON is None : swarms = dict ( ) if self . _hsObj . _fixedFields is not None : print self . _hsObj . _fixedFields encoderSet = [ ] for field in self . _hsObj . _fixedFields : if field == '_classifierInput' : continue encoderName = self . getEncoderKeyFromName ( field ) assert encoderName in self . _hsObj . _encoderNames , "The field '%s' " " specified in the fixedFields list is not present in this " " model." % ( field ) encoderSet . append ( encoderName ) encoderSet . sort ( ) swarms [ '.' . join ( encoderSet ) ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } elif self . _hsObj . _searchType == HsSearchType . temporal : for encoderName in self . _hsObj . _encoderNames : swarms [ encoderName ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } elif self . _hsObj . _searchType == HsSearchType . classification : for encoderName in self . _hsObj . _encoderNames : if encoderName == self . _hsObj . _predictedFieldEncoder : continue swarms [ encoderName ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } elif self . _hsObj . _searchType == HsSearchType . legacyTemporal : swarms [ self . _hsObj . _predictedFieldEncoder ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } else : raise RuntimeError ( "Unsupported search type: %s" % ( self . _hsObj . _searchType ) ) self . _state = dict ( lastUpdateTime = time . time ( ) , lastGoodSprint = None , searchOver = False , activeSwarms = swarms . keys ( ) , swarms = swarms , sprints = [ { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None } ] , blackListedEncoders = [ ] , ) self . _hsObj . _cjDAO . jobSetFieldIfEqual ( self . _hsObj . _jobID , 'engWorkerState' , json . dumps ( self . _state ) , None ) self . _priorStateJSON = self . _hsObj . _cjDAO . jobGetFields ( self . _hsObj . _jobID , [ 'engWorkerState' ] ) [ 0 ] assert ( self . _priorStateJSON is not None ) self . _state = json . loads ( self . _priorStateJSON ) self . _dirty = False
466	def generate_skip_gram_batch ( data , batch_size , num_skips , skip_window , data_index = 0 ) : if batch_size % num_skips != 0 : raise Exception ( "batch_size should be able to be divided by num_skips." ) if num_skips > 2 * skip_window : raise Exception ( "num_skips <= 2 * skip_window" ) batch = np . ndarray ( shape = ( batch_size ) , dtype = np . int32 ) labels = np . ndarray ( shape = ( batch_size , 1 ) , dtype = np . int32 ) span = 2 * skip_window + 1 buffer = collections . deque ( maxlen = span ) for _ in range ( span ) : buffer . append ( data [ data_index ] ) data_index = ( data_index + 1 ) % len ( data ) for i in range ( batch_size // num_skips ) : target = skip_window targets_to_avoid = [ skip_window ] for j in range ( num_skips ) : while target in targets_to_avoid : target = random . randint ( 0 , span - 1 ) targets_to_avoid . append ( target ) batch [ i * num_skips + j ] = buffer [ skip_window ] labels [ i * num_skips + j , 0 ] = buffer [ target ] buffer . append ( data [ data_index ] ) data_index = ( data_index + 1 ) % len ( data ) return batch , labels , data_index
2017	def DIV ( self , a , b ) : try : result = Operators . UDIV ( a , b ) except ZeroDivisionError : result = 0 return Operators . ITEBV ( 256 , b == 0 , 0 , result )
7726	def __init ( self , code ) : code = int ( code ) if code < 0 or code > 999 : raise ValueError ( "Bad status code" ) self . code = code
7408	def worker ( self ) : fullseqs = self . sample_loci ( ) liters = itertools . product ( * self . imap . values ( ) ) hashval = uuid . uuid4 ( ) . hex weights = [ ] for ridx , lidx in enumerate ( liters ) : a , b , c , d = lidx sub = { } for i in lidx : if self . rmap [ i ] == "p1" : sub [ "A" ] = fullseqs [ i ] elif self . rmap [ i ] == "p2" : sub [ "B" ] = fullseqs [ i ] elif self . rmap [ i ] == "p3" : sub [ "C" ] = fullseqs [ i ] else : sub [ "D" ] = fullseqs [ i ] nex = [ ] for tax in list ( "ABCD" ) : nex . append ( ">{} {}" . format ( tax , sub [ tax ] ) ) nsites , nvar = count_var ( nex ) if nvar > self . minsnps : nexus = "{} {}\n" . format ( 4 , len ( fullseqs [ a ] ) ) + "\n" . join ( nex ) treeorder = self . run_tree_inference ( nexus , "{}.{}" . format ( hashval , ridx ) ) weights . append ( treeorder ) rfiles = glob . glob ( os . path . join ( tempfile . tempdir , "*{}*" . format ( hashval ) ) ) for rfile in rfiles : if os . path . exists ( rfile ) : os . remove ( rfile ) trees = [ "ABCD" , "ACBD" , "ADBC" ] wdict = { i : float ( weights . count ( i ) ) / len ( weights ) for i in trees } return wdict
2021	def EXP_gas ( self , base , exponent ) : EXP_SUPPLEMENTAL_GAS = 10 def nbytes ( e ) : result = 0 for i in range ( 32 ) : result = Operators . ITEBV ( 512 , Operators . EXTRACT ( e , i * 8 , 8 ) != 0 , i + 1 , result ) return result return EXP_SUPPLEMENTAL_GAS * nbytes ( exponent )
500	def _deleteRangeFromKNN ( self , start = 0 , end = None ) : prototype_idx = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) if end is None : end = prototype_idx . max ( ) + 1 idsIdxToDelete = numpy . logical_and ( prototype_idx >= start , prototype_idx < end ) idsToDelete = prototype_idx [ idsIdxToDelete ] nProtos = self . _knnclassifier . _knn . _numPatterns self . _knnclassifier . _knn . removeIds ( idsToDelete . tolist ( ) ) assert self . _knnclassifier . _knn . _numPatterns == nProtos - len ( idsToDelete )
10642	def Ra ( L : float , Ts : float , Tf : float , alpha : float , beta : float , nu : float ) -> float : return g * beta * ( Ts - Tinf ) * L ** 3.0 / ( nu * alpha )
10473	def _sendKey ( self , keychr , modFlags = 0 , globally = False ) : escapedChrs = { '\n' : AXKeyCodeConstants . RETURN , '\r' : AXKeyCodeConstants . RETURN , '\t' : AXKeyCodeConstants . TAB , } if keychr in escapedChrs : keychr = escapedChrs [ keychr ] self . _addKeyToQueue ( keychr , modFlags , globally = globally ) self . _postQueuedEvents ( )
10665	def stoichiometry_coefficient ( compound , element ) : stoichiometry = parse_compound ( compound . strip ( ) ) . count ( ) return stoichiometry [ element ]
7746	def stanza_factory ( element , return_path = None , language = None ) : tag = element . tag if tag . endswith ( "}iq" ) or tag == "iq" : return Iq ( element , return_path = return_path , language = language ) if tag . endswith ( "}message" ) or tag == "message" : return Message ( element , return_path = return_path , language = language ) if tag . endswith ( "}presence" ) or tag == "presence" : return Presence ( element , return_path = return_path , language = language ) else : return Stanza ( element , return_path = return_path , language = language )
10704	def get_device ( _id ) : url = DEVICE_URL % _id arequest = requests . get ( url , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
10950	def sample ( field , inds = None , slicer = None , flat = True ) : if inds is not None : out = field . ravel ( ) [ inds ] elif slicer is not None : out = field [ slicer ] . ravel ( ) else : out = field if flat : return out . ravel ( ) return out
13123	def argparser ( self ) : core_parser = self . core_parser core_parser . add_argument ( '-r' , '--range' , type = str , help = "The range to search for use" ) return core_parser
12704	def make_quaternion ( theta , * axis ) : x , y , z = axis r = np . sqrt ( x * x + y * y + z * z ) st = np . sin ( theta / 2. ) ct = np . cos ( theta / 2. ) return [ x * st / r , y * st / r , z * st / r , ct ]
9747	def send_discovery_packet ( self ) : if self . port is None : return self . transport . sendto ( QRTDiscoveryP1 . pack ( QRTDiscoveryPacketSize , QRTPacketType . PacketDiscover . value ) + QRTDiscoveryP2 . pack ( self . port ) , ( "<broadcast>" , 22226 ) , )
11105	def sync_required ( func ) : @ wraps ( func ) def wrapper ( self , * args , ** kwargs ) : if not self . _keepSynchronized : r = func ( self , * args , ** kwargs ) else : state = self . _load_state ( ) if state is None : r = func ( self , * args , ** kwargs ) elif state == self . state : r = func ( self , * args , ** kwargs ) else : warnings . warn ( "Repository at '%s' is out of date. Need to load it again to avoid conflict." % self . path ) r = None return r return wrapper
8171	def alignment ( self , d = 5 ) : vx = vy = vz = 0 for b in self . boids : if b != self : vx , vy , vz = vx + b . vx , vy + b . vy , vz + b . vz n = len ( self . boids ) - 1 vx , vy , vz = vx / n , vy / n , vz / n return ( vx - self . vx ) / d , ( vy - self . vy ) / d , ( vz - self . vz ) / d
13458	def download_s3 ( bucket_name , file_key , file_path , force = False ) : file_path = path ( file_path ) bucket = open_s3 ( bucket_name ) file_dir = file_path . dirname ( ) file_dir . makedirs ( ) s3_key = bucket . get_key ( file_key ) if file_path . exists ( ) : file_data = file_path . bytes ( ) file_md5 , file_md5_64 = s3_key . get_md5_from_hexdigest ( hashlib . md5 ( file_data ) . hexdigest ( ) ) try : s3_md5 = s3_key . etag . replace ( '"' , '' ) except KeyError : pass else : if s3_md5 == file_md5 : info ( 'Hash is the same. Skipping %s' % file_path ) return elif not force : s3_datetime = datetime . datetime ( * time . strptime ( s3_key . last_modified , '%a, %d %b %Y %H:%M:%S %Z' ) [ 0 : 6 ] ) local_datetime = datetime . datetime . utcfromtimestamp ( file_path . stat ( ) . st_mtime ) if s3_datetime < local_datetime : info ( "File at %s is less recent than the local version." % ( file_key ) ) return info ( "Downloading %s..." % ( file_key ) ) try : with open ( file_path , 'w' ) as fo : s3_key . get_contents_to_file ( fo ) except Exception as e : error ( "Failed: %s" % e ) raise
11213	def decode ( secret : Union [ str , bytes ] , token : Union [ str , bytes ] , alg : str = default_alg ) -> Tuple [ dict , dict ] : secret = util . to_bytes ( secret ) token = util . to_bytes ( token ) pre_signature , signature_segment = token . rsplit ( b'.' , 1 ) header_b64 , payload_b64 = pre_signature . split ( b'.' ) try : header_json = util . b64_decode ( header_b64 ) header = json . loads ( util . from_bytes ( header_json ) ) except ( json . decoder . JSONDecodeError , UnicodeDecodeError , ValueError ) : raise InvalidHeaderError ( 'Invalid header' ) try : payload_json = util . b64_decode ( payload_b64 ) payload = json . loads ( util . from_bytes ( payload_json ) ) except ( json . decoder . JSONDecodeError , UnicodeDecodeError , ValueError ) : raise InvalidPayloadError ( 'Invalid payload' ) if not isinstance ( header , dict ) : raise InvalidHeaderError ( 'Invalid header: {}' . format ( header ) ) if not isinstance ( payload , dict ) : raise InvalidPayloadError ( 'Invalid payload: {}' . format ( payload ) ) signature = util . b64_decode ( signature_segment ) calculated_signature = _hash ( secret , pre_signature , alg ) if not compare_signature ( signature , calculated_signature ) : raise InvalidSignatureError ( 'Invalid signature' ) return header , payload
8710	def __write_chunk ( self , chunk ) : log . debug ( 'writing %d bytes chunk' , len ( chunk ) ) data = BLOCK_START + chr ( len ( chunk ) ) + chunk if len ( chunk ) < 128 : padding = 128 - len ( chunk ) log . debug ( 'pad with %d characters' , padding ) data = data + ( ' ' * padding ) log . debug ( "packet size %d" , len ( data ) ) self . __write ( data ) self . _port . flush ( ) return self . __got_ack ( )
1783	def ADC ( cpu , dest , src ) : cpu . _ADD ( dest , src , carry = True )
7973	def start ( self , daemon = False ) : self . daemon = daemon self . io_threads = [ ] self . event_thread = EventDispatcherThread ( self . event_dispatcher , daemon = daemon , exc_queue = self . exc_queue ) self . event_thread . start ( ) for handler in self . io_handlers : self . _run_io_threads ( handler ) for handler in self . timeout_handlers : self . _run_timeout_threads ( handler )
9160	def delete_roles_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] posted_roles = request . json with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : remove_role_requests ( cursor , uuid_ , posted_roles ) resp = request . response resp . status_int = 200 return resp
1497	def parse_query_string ( self , query ) : if not query : return None if query [ 0 ] == '(' : index = self . find_closing_braces ( query ) if index != len ( query ) - 1 : raise Exception ( "Invalid syntax" ) else : return self . parse_query_string ( query [ 1 : - 1 ] ) start_index = query . find ( "(" ) if start_index < 0 : try : constant = float ( query ) return constant except ValueError : raise Exception ( "Invalid syntax" ) token = query [ : start_index ] if token not in self . operators : raise Exception ( "Invalid token: " + token ) rest_of_the_query = query [ start_index : ] braces_end_index = self . find_closing_braces ( rest_of_the_query ) if braces_end_index != len ( rest_of_the_query ) - 1 : raise Exception ( "Invalid syntax" ) parts = self . get_sub_parts ( rest_of_the_query [ 1 : - 1 ] ) if token == "TS" : return self . operators [ token ] ( parts ) children = [ ] for part in parts : children . append ( self . parse_query_string ( part ) ) node = self . operators [ token ] ( children ) return node
6342	def docs_of_words ( self ) : r return [ [ words for sents in doc for words in sents ] for doc in self . corpus ]
2740	def add_tags ( self , tags ) : return self . get_data ( "firewalls/%s/tags" % self . id , type = POST , params = { "tags" : tags } )
7464	def _parse_00 ( ofile ) : with open ( ofile ) as infile : arr = np . array ( [ " " ] + infile . read ( ) . split ( "Summary of MCMC results\n\n\n" ) [ 1 : ] [ 0 ] . strip ( ) . split ( ) ) rows = 12 cols = ( arr . shape [ 0 ] + 1 ) / rows arr = arr . reshape ( rows , cols ) df = pd . DataFrame ( data = arr [ 1 : , 1 : ] , columns = arr [ 0 , 1 : ] , index = arr [ 1 : , 0 ] , ) . T return df
7493	def n_choose_k ( n , k ) : return int ( reduce ( MUL , ( Fraction ( n - i , i + 1 ) for i in range ( k ) ) , 1 ) )
160	def height ( self ) : if len ( self . coords ) <= 1 : return 0 return np . max ( self . yy ) - np . min ( self . yy )
4485	def write_to ( self , fp ) : if 'b' not in fp . mode : raise ValueError ( "File has to be opened in binary mode." ) response = self . _get ( self . _download_url , stream = True ) if response . status_code == 200 : response . raw . decode_content = True copyfileobj ( response . raw , fp , int ( response . headers [ 'Content-Length' ] ) ) else : raise RuntimeError ( "Response has status " "code {}." . format ( response . status_code ) )
3486	def _check_required ( sbase , value , attribute ) : if ( value is None ) or ( value == "" ) : msg = "Required attribute '%s' cannot be found or parsed in '%s'" % ( attribute , sbase ) if hasattr ( sbase , "getId" ) and sbase . getId ( ) : msg += " with id '%s'" % sbase . getId ( ) elif hasattr ( sbase , "getName" ) and sbase . getName ( ) : msg += " with name '%s'" % sbase . getName ( ) elif hasattr ( sbase , "getMetaId" ) and sbase . getMetaId ( ) : msg += " with metaId '%s'" % sbase . getName ( ) raise CobraSBMLError ( msg ) return value
1679	def CheckNextIncludeOrder ( self , header_type ) : error_message = ( 'Found %s after %s' % ( self . _TYPE_NAMES [ header_type ] , self . _SECTION_NAMES [ self . _section ] ) ) last_section = self . _section if header_type == _C_SYS_HEADER : if self . _section <= self . _C_SECTION : self . _section = self . _C_SECTION else : self . _last_header = '' return error_message elif header_type == _CPP_SYS_HEADER : if self . _section <= self . _CPP_SECTION : self . _section = self . _CPP_SECTION else : self . _last_header = '' return error_message elif header_type == _LIKELY_MY_HEADER : if self . _section <= self . _MY_H_SECTION : self . _section = self . _MY_H_SECTION else : self . _section = self . _OTHER_H_SECTION elif header_type == _POSSIBLE_MY_HEADER : if self . _section <= self . _MY_H_SECTION : self . _section = self . _MY_H_SECTION else : self . _section = self . _OTHER_H_SECTION else : assert header_type == _OTHER_HEADER self . _section = self . _OTHER_H_SECTION if last_section != self . _section : self . _last_header = '' return ''
8123	def textpath ( self , txt , x , y , width = None , height = 1000000 , enableRendering = False , ** kwargs ) : txt = self . Text ( txt , x , y , width , height , ** kwargs ) path = txt . path if draw : path . draw ( ) return path
1756	def emulate_until ( self , target : int ) : self . _concrete = True self . _break_unicorn_at = target if self . emu : self . emu . _stop_at = target
860	def getTemporalDelay ( inferenceElement , key = None ) : if inferenceElement in ( InferenceElement . prediction , InferenceElement . encodings ) : return 1 if inferenceElement in ( InferenceElement . anomalyScore , InferenceElement . anomalyLabel , InferenceElement . classification , InferenceElement . classConfidences ) : return 0 if inferenceElement in ( InferenceElement . multiStepPredictions , InferenceElement . multiStepBestPredictions ) : return int ( key ) return 0
13356	def run_global_hook ( hook_name , * args ) : hook_finder = HookFinder ( get_global_hook_path ( ) ) hook = hook_finder ( hook_name ) if hook : hook . run ( * args )
11077	def send_message ( self , channel , text ) : if isinstance ( channel , SlackIM ) or isinstance ( channel , SlackUser ) : self . _bot . send_im ( channel , text ) elif isinstance ( channel , SlackRoom ) : self . _bot . send_message ( channel , text ) elif isinstance ( channel , basestring ) : if channel [ 0 ] == '@' : self . _bot . send_im ( channel [ 1 : ] , text ) elif channel [ 0 ] == '#' : self . _bot . send_message ( channel [ 1 : ] , text ) else : self . _bot . send_message ( channel , text ) else : self . _bot . send_message ( channel , text )
6739	def check_settings_for_differences ( old , new , as_bool = False , as_tri = False ) : assert not as_bool or not as_tri old = old or { } new = new or { } changes = set ( k for k in set ( new . iterkeys ( ) ) . intersection ( old . iterkeys ( ) ) if new [ k ] != old [ k ] ) if changes and as_bool : return True added_keys = set ( new . iterkeys ( ) ) . difference ( old . iterkeys ( ) ) if added_keys and as_bool : return True if not as_tri : changes . update ( added_keys ) deled_keys = set ( old . iterkeys ( ) ) . difference ( new . iterkeys ( ) ) if deled_keys and as_bool : return True if as_bool : return False if not as_tri : changes . update ( deled_keys ) if as_tri : return added_keys , changes , deled_keys return changes
5989	def grid_stack_from_deflection_stack ( grid_stack , deflection_stack ) : if deflection_stack is not None : def minus ( grid , deflections ) : return grid - deflections return grid_stack . map_function ( minus , deflection_stack )
1323	def SetActive ( self , waitTime : float = OPERATION_WAIT_TIME ) -> bool : if self . IsTopLevel ( ) : handle = self . NativeWindowHandle if IsIconic ( handle ) : ret = ShowWindow ( handle , SW . Restore ) elif not IsWindowVisible ( handle ) : ret = ShowWindow ( handle , SW . Show ) ret = SetForegroundWindow ( handle ) time . sleep ( waitTime ) return ret return False
5156	def _get_install_context ( self ) : config = self . config l2vpn = [ ] for vpn in self . config . get ( 'openvpn' , [ ] ) : if vpn . get ( 'dev_type' ) != 'tap' : continue tap = vpn . copy ( ) l2vpn . append ( tap ) bridges = [ ] for interface in self . config . get ( 'interfaces' , [ ] ) : if interface [ 'type' ] != 'bridge' : continue bridge = interface . copy ( ) if bridge . get ( 'addresses' ) : bridge [ 'proto' ] = interface [ 'addresses' ] [ 0 ] . get ( 'proto' ) bridge [ 'ip' ] = interface [ 'addresses' ] [ 0 ] . get ( 'address' ) bridges . append ( bridge ) cron = False for _file in config . get ( 'files' , [ ] ) : path = _file [ 'path' ] if path . startswith ( '/crontabs' ) or path . startswith ( 'crontabs' ) : cron = True break return dict ( hostname = config [ 'general' ] [ 'hostname' ] , l2vpn = l2vpn , bridges = bridges , radios = config . get ( 'radios' , [ ] ) , cron = cron )
523	def _updateBoostFactorsLocal ( self ) : targetDensity = numpy . zeros ( self . _numColumns , dtype = realDType ) for i in xrange ( self . _numColumns ) : maskNeighbors = self . _getColumnNeighborhood ( i ) targetDensity [ i ] = numpy . mean ( self . _activeDutyCycles [ maskNeighbors ] ) self . _boostFactors = numpy . exp ( ( targetDensity - self . _activeDutyCycles ) * self . _boostStrength )
8717	def file_compile ( self , path ) : log . info ( 'Compile ' + path ) cmd = 'node.compile("%s")' % path res = self . __exchange ( cmd ) log . info ( res ) return res
12122	def filter_gaussian ( self , sigmaMs = 100 , applyFiltered = False , applyBaseline = False ) : if sigmaMs == 0 : return self . dataY filtered = cm . filter_gaussian ( self . dataY , sigmaMs ) if applyBaseline : self . dataY = self . dataY - filtered elif applyFiltered : self . dataY = filtered else : return filtered
4680	def getAccountsFromPublicKey ( self , pub ) : names = self . rpc . get_key_references ( [ str ( pub ) ] ) [ 0 ] for name in names : yield name
7488	def make ( data , samples ) : invcffile = os . path . join ( data . dirs . consens , data . name + ".vcf" ) outlocifile = os . path . join ( data . dirs . outfiles , data . name + ".loci" ) importvcf ( invcffile , outlocifile )
8534	def read ( cls , data , protocol = None , fallback_protocol = TBinaryProtocol , finagle_thrift = False , max_fields = MAX_FIELDS , max_list_size = MAX_LIST_SIZE , max_map_size = MAX_MAP_SIZE , max_set_size = MAX_SET_SIZE , read_values = False ) : if len ( data ) < cls . MIN_MESSAGE_SIZE : raise ValueError ( 'not enough data' ) if protocol is None : protocol = cls . detect_protocol ( data , fallback_protocol ) trans = TTransport . TMemoryBuffer ( data ) proto = protocol ( trans ) header = None if finagle_thrift : try : header = ThriftStruct . read ( proto , max_fields , max_list_size , max_map_size , max_set_size , read_values ) except : trans = TTransport . TMemoryBuffer ( data ) proto = protocol ( trans ) method , mtype , seqid = proto . readMessageBegin ( ) mtype = cls . message_type_to_str ( mtype ) if len ( method ) == 0 or method . isspace ( ) or method . startswith ( ' ' ) : raise ValueError ( 'no method name' ) if len ( method ) > cls . MAX_METHOD_LENGTH : raise ValueError ( 'method name too long' ) valid = range ( 33 , 127 ) if any ( ord ( char ) not in valid for char in method ) : raise ValueError ( 'invalid method name' % method ) args = ThriftStruct . read ( proto , max_fields , max_list_size , max_map_size , max_set_size , read_values ) proto . readMessageEnd ( ) msglen = trans . _buffer . tell ( ) return cls ( method , mtype , seqid , args , header , msglen ) , msglen
1971	def check_timers ( self ) : if self . _current is None : advance = min ( [ self . clocks ] + [ x for x in self . timers if x is not None ] ) + 1 logger . debug ( f"Advancing the clock from {self.clocks} to {advance}" ) self . clocks = advance for procid in range ( len ( self . timers ) ) : if self . timers [ procid ] is not None : if self . clocks > self . timers [ procid ] : self . procs [ procid ] . PC += self . procs [ procid ] . instruction . size self . awake ( procid )
2037	def JUMPI ( self , dest , cond ) : self . pc = Operators . ITEBV ( 256 , cond != 0 , dest , self . pc + self . instruction . size ) self . _set_check_jmpdest ( cond != 0 )
7375	def code ( self , code ) : def decorator ( exception ) : self [ code ] = exception return exception return decorator
5581	def _get_contour_values ( min_val , max_val , base = 0 , interval = 100 ) : i = base out = [ ] if min_val < base : while i >= min_val : i -= interval while i <= max_val : if i >= min_val : out . append ( i ) i += interval return out
415	def save_dataset ( self , dataset = None , dataset_name = None , ** kwargs ) : self . _fill_project_info ( kwargs ) if dataset_name is None : raise Exception ( "dataset_name is None, please give a dataset name" ) kwargs . update ( { 'dataset_name' : dataset_name } ) s = time . time ( ) try : dataset_id = self . dataset_fs . put ( self . _serialization ( dataset ) ) kwargs . update ( { 'dataset_id' : dataset_id , 'time' : datetime . utcnow ( ) } ) self . db . Dataset . insert_one ( kwargs ) print ( "[Database] Save dataset: SUCCESS, took: {}s" . format ( round ( time . time ( ) - s , 2 ) ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) print ( "[Database] Save dataset: FAIL" ) return False
8297	def hexDump ( bytes ) : for i in range ( len ( bytes ) ) : sys . stdout . write ( "%2x " % ( ord ( bytes [ i ] ) ) ) if ( i + 1 ) % 8 == 0 : print repr ( bytes [ i - 7 : i + 1 ] ) if ( len ( bytes ) % 8 != 0 ) : print string . rjust ( "" , 11 ) , repr ( bytes [ i - len ( bytes ) % 8 : i + 1 ] )
10471	def _queueEvent ( self , event , args ) : if not hasattr ( self , 'eventList' ) : self . eventList = deque ( [ ( event , args ) ] ) return self . eventList . append ( ( event , args ) )
6737	def get_component_settings ( prefixes = None ) : prefixes = prefixes or [ ] assert isinstance ( prefixes , ( tuple , list ) ) , 'Prefixes must be a sequence type, not %s.' % type ( prefixes ) data = { } for name in prefixes : name = name . lower ( ) . strip ( ) for k in sorted ( env ) : if k . startswith ( '%s_' % name ) : new_k = k [ len ( name ) + 1 : ] data [ new_k ] = env [ k ] return data
11489	def _download_folder_recursive ( folder_id , path = '.' ) : session . token = verify_credentials ( ) cur_folder = session . communicator . folder_get ( session . token , folder_id ) folder_path = os . path . join ( path , cur_folder [ 'name' ] . replace ( '/' , '_' ) ) print ( 'Creating folder at {0}' . format ( folder_path ) ) try : os . mkdir ( folder_path ) except OSError as e : if e . errno == errno . EEXIST and session . allow_existing_download_paths : pass else : raise cur_children = session . communicator . folder_children ( session . token , folder_id ) for item in cur_children [ 'items' ] : _download_item ( item [ 'item_id' ] , folder_path , item = item ) for folder in cur_children [ 'folders' ] : _download_folder_recursive ( folder [ 'folder_id' ] , folder_path ) for callback in session . folder_download_callbacks : callback ( session . communicator , session . token , cur_folder , folder_path )
3380	def add_lexicographic_constraints ( model , objectives , objective_direction = 'max' ) : if type ( objective_direction ) is not list : objective_direction = [ objective_direction ] * len ( objectives ) constraints = [ ] for rxn_id , obj_dir in zip ( objectives , objective_direction ) : model . objective = model . reactions . get_by_id ( rxn_id ) model . objective_direction = obj_dir constraints . append ( fix_objective_as_constraint ( model ) ) return pd . Series ( constraints , index = objectives )
5944	def isstream ( obj ) : signature_methods = ( "close" , ) alternative_methods = ( ( "read" , "readline" , "readlines" ) , ( "write" , "writeline" , "writelines" ) ) for m in signature_methods : if not hasmethod ( obj , m ) : return False alternative_results = [ numpy . all ( [ hasmethod ( obj , m ) for m in alternatives ] ) for alternatives in alternative_methods ] return numpy . any ( alternative_results )
3521	def performable ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return PerformableNode ( )
1343	def onehot_like ( a , index , value = 1 ) : x = np . zeros_like ( a ) x [ index ] = value return x
6791	def loaddata ( self , path , site = None ) : site = site or self . genv . SITE r = self . local_renderer r . env . _loaddata_path = path for _site , site_data in self . iter_sites ( site = site , no_secure = True ) : try : self . set_db ( site = _site ) r . env . SITE = _site r . sudo ( 'export SITE={SITE}; export ROLE={ROLE}; ' 'cd {project_dir}; ' '{manage_cmd} loaddata {_loaddata_path}' ) except KeyError : pass
10808	def delete ( self ) : with db . session . begin_nested ( ) : Membership . query_by_group ( self ) . delete ( ) GroupAdmin . query_by_group ( self ) . delete ( ) GroupAdmin . query_by_admin ( self ) . delete ( ) db . session . delete ( self )
5539	def read ( self , ** kwargs ) : if self . tile . pixelbuffer > self . config . output . pixelbuffer : output_tiles = list ( self . config . output_pyramid . tiles_from_bounds ( self . tile . bounds , self . tile . zoom ) ) else : output_tiles = self . config . output_pyramid . intersecting ( self . tile ) return self . config . output . extract_subset ( input_data_tiles = [ ( output_tile , self . config . output . read ( output_tile ) ) for output_tile in output_tiles ] , out_tile = self . tile , )
5168	def __intermediate_htmode ( self , radio ) : protocol = radio . pop ( 'protocol' ) channel_width = radio . pop ( 'channel_width' ) if 'htmode' in radio : return radio [ 'htmode' ] if protocol == '802.11n' : return 'HT{0}' . format ( channel_width ) elif protocol == '802.11ac' : return 'VHT{0}' . format ( channel_width ) return 'NONE'
13020	def _execute ( self , query , commit = False , working_columns = None ) : log . debug ( "RawlBase._execute()" ) result = [ ] if working_columns is None : working_columns = self . columns with RawlConnection ( self . dsn ) as conn : query_id = random . randrange ( 9999 ) curs = conn . cursor ( ) try : log . debug ( "Executing(%s): %s" % ( query_id , query . as_string ( curs ) ) ) except : log . exception ( "LOGGING EXCEPTION LOL" ) curs . execute ( query ) log . debug ( "Executed" ) if commit == True : log . debug ( "COMMIT(%s)" % query_id ) conn . commit ( ) log . debug ( "curs.rowcount: %s" % curs . rowcount ) if curs . rowcount > 0 : result_rows = curs . fetchall ( ) for row in result_rows : i = 0 row_dict = { } for col in working_columns : try : col = col . replace ( '.' , '_' ) row_dict [ col ] = row [ i ] except IndexError : pass i += 1 log . debug ( "Appending dict to result: %s" % row_dict ) rr = RawlResult ( working_columns , row_dict ) result . append ( rr ) curs . close ( ) return result
10105	def get_group_tabs ( self ) : if self . tab_group is None : raise ImproperlyConfigured ( "%s requires a definition of 'tab_group'" % self . __class__ . __name__ ) group_members = [ t for t in self . _registry if t . tab_group == self . tab_group ] return [ t ( ) for t in group_members ]
9270	def get_temp_tag_for_repo_creation ( self ) : tag_date = self . tag_times_dict . get ( REPO_CREATED_TAG_NAME , None ) if not tag_date : tag_name , tag_date = self . fetcher . fetch_repo_creation_date ( ) self . tag_times_dict [ tag_name ] = timestring_to_datetime ( tag_date ) return REPO_CREATED_TAG_NAME
12332	def get_diffs ( history ) : mgr = plugins_get_mgr ( ) keys = mgr . search ( 'representation' ) [ 'representation' ] representations = [ mgr . get_by_key ( 'representation' , k ) for k in keys ] for i in range ( len ( history ) ) : if i + 1 > len ( history ) - 1 : continue prev = history [ i ] curr = history [ i + 1 ] for c in curr [ 'changes' ] : path = c [ 'path' ] if c [ 'path' ] . endswith ( 'datapackage.json' ) : continue handler = None for r in representations : if r . can_process ( path ) : handler = r break if handler is None : continue v1_hex = prev [ 'commit' ] v2_hex = curr [ 'commit' ] temp1 = tempfile . mkdtemp ( prefix = "dgit-diff-" ) try : for h in [ v1_hex , v2_hex ] : filename = '{}/{}/checkout.tar' . format ( temp1 , h ) try : os . makedirs ( os . path . dirname ( filename ) ) except : pass extractcmd = [ 'git' , 'archive' , '-o' , filename , h , path ] output = run ( extractcmd ) if 'fatal' in output : raise Exception ( "File not present in commit" ) with cd ( os . path . dirname ( filename ) ) : cmd = [ 'tar' , 'xvf' , 'checkout.tar' ] output = run ( cmd ) if 'fatal' in output : print ( "Cleaning up - fatal 1" , temp1 ) shutil . rmtree ( temp1 ) continue path1 = os . path . join ( temp1 , v1_hex , path ) path2 = os . path . join ( temp1 , v2_hex , path ) if not os . path . exists ( path1 ) or not os . path . exists ( path2 ) : shutil . rmtree ( temp1 ) continue diff = handler . get_diff ( path1 , path2 ) c [ 'diff' ] = diff except Exception as e : shutil . rmtree ( temp1 )
13813	def MessageToJson ( message , including_default_value_fields = False ) : js = _MessageToJsonObject ( message , including_default_value_fields ) return json . dumps ( js , indent = 2 )
4451	def search ( self , query ) : args , query = self . _mk_query_args ( query ) st = time . time ( ) res = self . redis . execute_command ( self . SEARCH_CMD , * args ) return Result ( res , not query . _no_content , duration = ( time . time ( ) - st ) * 1000.0 , has_payload = query . _with_payloads )
5023	def get_integrated_channels ( self , options ) : channel_classes = self . get_channel_classes ( options . get ( 'channel' ) ) filter_kwargs = { 'active' : True , 'enterprise_customer__active' : True , } enterprise_customer = self . get_enterprise_customer ( options . get ( 'enterprise_customer' ) ) if enterprise_customer : filter_kwargs [ 'enterprise_customer' ] = enterprise_customer for channel_class in channel_classes : for integrated_channel in channel_class . objects . filter ( ** filter_kwargs ) : yield integrated_channel
13325	def activate ( paths , skip_local , skip_shared ) : if not paths : ctx = click . get_current_context ( ) if cpenv . get_active_env ( ) : ctx . invoke ( info ) return click . echo ( ctx . get_help ( ) ) examples = ( '\nExamples: \n' ' cpenv activate my_env\n' ' cpenv activate ./relative/path/to/my_env\n' ' cpenv activate my_env my_module\n' ) click . echo ( examples ) return if skip_local : cpenv . module_resolvers . remove ( cpenv . resolver . module_resolver ) cpenv . module_resolvers . remove ( cpenv . resolver . active_env_module_resolver ) if skip_shared : cpenv . module_resolvers . remove ( cpenv . resolver . modules_path_resolver ) try : r = cpenv . resolve ( * paths ) except cpenv . ResolveError as e : click . echo ( '\n' + str ( e ) ) return resolved = set ( r . resolved ) active_modules = set ( ) env = cpenv . get_active_env ( ) if env : active_modules . add ( env ) active_modules . update ( cpenv . get_active_modules ( ) ) new_modules = resolved - active_modules old_modules = active_modules & resolved if old_modules and not new_modules : click . echo ( '\nModules already active: ' + bold ( ' ' . join ( [ obj . name for obj in old_modules ] ) ) ) return if env and contains_env ( new_modules ) : click . echo ( '\nUse bold(exit) to leave your active environment first.' ) return click . echo ( '\nResolved the following modules...' ) click . echo ( format_objects ( r . resolved ) ) r . activate ( ) click . echo ( blue ( '\nLaunching subshell...' ) ) modules = sorted ( resolved | active_modules , key = _type_and_name ) prompt = ':' . join ( [ obj . name for obj in modules ] ) shell . launch ( prompt )
10677	def Cp ( self , T ) : result = 0.0 for c , e in zip ( self . _coefficients , self . _exponents ) : result += c * T ** e return result
8959	def build ( ctx , dput = '' , opts = '' ) : with io . open ( 'debian/changelog' , encoding = 'utf-8' ) as changes : metadata = re . match ( r'^([^ ]+) \(([^)]+)\) ([^;]+); urgency=(.+)$' , changes . readline ( ) . rstrip ( ) ) if not metadata : notify . failure ( 'Badly formatted top entry in changelog' ) name , version , _ , _ = metadata . groups ( ) ctx . run ( 'dpkg-buildpackage {} {}' . format ( ctx . rituals . deb . build . opts , opts ) ) if not os . path . exists ( 'dist' ) : os . makedirs ( 'dist' ) artifact_pattern = '{}?{}*' . format ( name , re . sub ( r'[^-_.a-zA-Z0-9]' , '?' , version ) ) changes_files = [ ] for debfile in glob . glob ( '../' + artifact_pattern ) : shutil . move ( debfile , 'dist' ) if debfile . endswith ( '.changes' ) : changes_files . append ( os . path . join ( 'dist' , os . path . basename ( debfile ) ) ) ctx . run ( 'ls -l dist/{}' . format ( artifact_pattern ) ) if dput : ctx . run ( 'dput {} {}' . format ( dput , ' ' . join ( changes_files ) ) )
13220	def settings ( self ) : stmt = "select {fields} from pg_settings" . format ( fields = ', ' . join ( SETTINGS_FIELDS ) ) settings = [ ] for row in self . _iter_results ( stmt ) : row [ 'setting' ] = self . _vartype_map [ row [ 'vartype' ] ] ( row [ 'setting' ] ) settings . append ( Settings ( ** row ) ) return settings
11311	def get_record ( self ) : self . recid = self . get_recid ( ) self . remove_controlfields ( ) self . update_system_numbers ( ) self . add_systemnumber ( "Inspire" , recid = self . recid ) self . add_control_number ( "003" , "SzGeCERN" ) self . update_collections ( ) self . update_languages ( ) self . update_reportnumbers ( ) self . update_authors ( ) self . update_journals ( ) self . update_subject_categories ( "INSPIRE" , "SzGeCERN" , "categories_cds" ) self . update_pagenumber ( ) self . update_notes ( ) self . update_experiments ( ) self . update_isbn ( ) self . update_dois ( ) self . update_links_and_ffts ( ) self . update_date ( ) self . update_date_year ( ) self . update_hidden_notes ( ) self . update_oai_info ( ) self . update_cnum ( ) self . update_conference_info ( ) self . fields_list = [ "909" , "541" , "961" , "970" , "690" , "695" , "981" , ] self . strip_fields ( ) if "ANNOUNCEMENT" in self . collections : self . update_conference_111 ( ) self . update_conference_links ( ) record_add_field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CONFERENCE" ) ] ) if "THESIS" in self . collections : self . update_thesis_information ( ) self . update_thesis_supervisors ( ) if "PROCEEDINGS" in self . collections : self . update_title_to_proceeding ( ) self . update_author_to_proceeding ( ) record_add_field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CONFERENCE" ) ] ) if self . tag_as_cern : record_add_field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CERN" ) ] ) return self . record
6678	def uncommented_lines ( self , filename , use_sudo = False ) : func = run_as_root if use_sudo else self . run res = func ( 'cat %s' % quote ( filename ) , quiet = True ) if res . succeeded : return [ line for line in res . splitlines ( ) if line and not line . startswith ( '#' ) ] return [ ]
9286	def sendall ( self , line ) : if isinstance ( line , APRSPacket ) : line = str ( line ) elif not isinstance ( line , string_type ) : raise TypeError ( "Expected line to be str or APRSPacket, got %s" , type ( line ) ) if not self . _connected : raise ConnectionError ( "not connected" ) if line == "" : return line = line . rstrip ( "\r\n" ) + "\r\n" try : self . sock . setblocking ( 1 ) self . sock . settimeout ( 5 ) self . _sendall ( line ) except socket . error as exp : self . close ( ) raise ConnectionError ( str ( exp ) )
7951	def wait_for_readability ( self ) : with self . lock : while True : if self . _socket is None or self . _eof : return False if self . _state in ( "connected" , "closing" ) : return True if self . _state == "tls-handshake" and self . _tls_state == "want_read" : return True self . _state_cond . wait ( )
6456	def sim ( src , tar , method = sim_levenshtein ) : if callable ( method ) : return method ( src , tar ) else : raise AttributeError ( 'Unknown similarity function: ' + str ( method ) )
9720	async def release_control ( self ) : cmd = "releasecontrol" return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
12764	def attach ( self , frame_no ) : assert not self . joints for label , j in self . channels . items ( ) : target = self . targets . get ( label ) if target is None : continue if self . visibility [ frame_no , j ] < 0 : continue if np . linalg . norm ( self . velocities [ frame_no , j ] ) > 10 : continue joint = ode . BallJoint ( self . world . ode_world , self . jointgroup ) joint . attach ( self . bodies [ label ] . ode_body , target . ode_body ) joint . setAnchor1Rel ( [ 0 , 0 , 0 ] ) joint . setAnchor2Rel ( self . offsets [ label ] ) joint . setParam ( ode . ParamCFM , self . cfms [ frame_no , j ] ) joint . setParam ( ode . ParamERP , self . erp ) joint . name = label self . joints [ label ] = joint self . _frame_no = frame_no
9017	def _pattern ( self , base ) : rows = self . _rows ( base . get ( ROWS , [ ] ) ) self . _finish_inheritance ( ) self . _finish_instructions ( ) self . _connect_rows ( base . get ( CONNECTIONS , [ ] ) ) id_ = self . _to_id ( base [ ID ] ) name = base [ NAME ] return self . new_pattern ( id_ , name , rows )
1663	def UpdateIncludeState ( filename , include_dict , io = codecs ) : headerfile = None try : headerfile = io . open ( filename , 'r' , 'utf8' , 'replace' ) except IOError : return False linenum = 0 for line in headerfile : linenum += 1 clean_line = CleanseComments ( line ) match = _RE_PATTERN_INCLUDE . search ( clean_line ) if match : include = match . group ( 2 ) include_dict . setdefault ( include , linenum ) return True
864	def makeDirectoryFromAbsolutePath ( absDirPath ) : assert os . path . isabs ( absDirPath ) try : os . makedirs ( absDirPath ) except OSError , e : if e . errno != os . errno . EEXIST : raise return absDirPath
4929	def transform_launch_points ( self , content_metadata_item ) : return [ { 'providerID' : self . enterprise_configuration . provider_id , 'launchURL' : content_metadata_item [ 'enrollment_url' ] , 'contentTitle' : content_metadata_item [ 'title' ] , 'contentID' : self . get_content_id ( content_metadata_item ) , 'launchType' : 3 , 'mobileEnabled' : True , 'mobileLaunchURL' : content_metadata_item [ 'enrollment_url' ] , } ]
4709	def power_btn ( self , interval = 200 ) : if self . __power_btn_port is None : cij . err ( "cij.usb.relay: Invalid USB_RELAY_POWER_BTN" ) return 1 return self . __press ( self . __power_btn_port , interval = interval )
8630	def get_projects ( session , query ) : response = make_get_request ( session , 'projects' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
13204	def _parse_documentclass ( self ) : command = LatexCommand ( 'documentclass' , { 'name' : 'options' , 'required' : False , 'bracket' : '[' } , { 'name' : 'class_name' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no documentclass' ) self . _document_options = [ ] try : content = parsed [ 'options' ] self . _document_options = [ opt . strip ( ) for opt in content . split ( ',' ) ] except KeyError : self . _logger . warning ( 'lsstdoc has no documentclass options' ) self . _document_options = [ ]
10752	def download ( self , bands , download_dir = None , metadata = False ) : super ( AWSDownloader , self ) . validate_bands ( bands ) if download_dir is None : download_dir = DOWNLOAD_DIR dest_dir = check_create_folder ( join ( download_dir , self . sceneInfo . name ) ) downloaded = [ ] for band in bands : if band == 'BQA' : filename = '%s_%s.%s' % ( self . sceneInfo . name , band , self . __remote_file_ext ) else : filename = '%s_B%s.%s' % ( self . sceneInfo . name , band , self . __remote_file_ext ) band_url = join ( self . base_url , filename ) downloaded . append ( self . fetch ( band_url , dest_dir , filename ) ) if metadata : filename = '%s_MTL.txt' % ( self . sceneInfo . name ) url = join ( self . base_url , filename ) self . fetch ( url , dest_dir , filename ) return downloaded
2141	def parse_kv ( var_string ) : return_dict = { } if var_string is None : return { } fix_encoding_26 = False if sys . version_info < ( 2 , 7 ) and '\x00' in shlex . split ( u'a' ) [ 0 ] : fix_encoding_26 = True is_unicode = False if fix_encoding_26 or not isinstance ( var_string , str ) : if isinstance ( var_string , six . text_type ) : var_string = var_string . encode ( 'UTF-8' ) is_unicode = True else : var_string = str ( var_string ) for token in shlex . split ( var_string ) : if ( is_unicode ) : token = token . decode ( 'UTF-8' ) if fix_encoding_26 : token = six . text_type ( token ) if '=' in token : ( k , v ) = token . split ( '=' , 1 ) if len ( k ) == 0 or len ( v ) == 0 : raise Exception try : return_dict [ k ] = ast . literal_eval ( v ) except Exception : return_dict [ k ] = v else : raise Exception return return_dict
5860	def default ( self , obj ) : if obj is None : return [ ] elif isinstance ( obj , list ) : return [ i . as_dictionary ( ) for i in obj ] elif isinstance ( obj , dict ) : return self . _keys_to_camel_case ( obj ) else : return obj . as_dictionary ( )
7456	def splitfiles ( data , raws , ipyclient ) : tmpdir = os . path . join ( data . paramsdict [ "project_dir" ] , "tmp-chunks-" + data . name ) if os . path . exists ( tmpdir ) : shutil . rmtree ( tmpdir ) os . makedirs ( tmpdir ) totalreads = estimate_optim ( data , raws [ 0 ] [ 0 ] , ipyclient ) optim = int ( 8e6 ) njobs = int ( totalreads / ( optim / 4. ) ) * len ( raws ) nosplit = 0 if ( len ( raws ) > len ( ipyclient ) ) or ( totalreads < optim ) : nosplit = 1 start = time . time ( ) chunkfiles = { } for fidx , tups in enumerate ( raws ) : handle = os . path . splitext ( os . path . basename ( tups [ 0 ] ) ) [ 0 ] if nosplit : chunkfiles [ handle ] = [ tups ] else : chunklist = zcat_make_temps ( data , tups , fidx , tmpdir , optim , njobs , start ) chunkfiles [ handle ] = chunklist if not nosplit : print ( "" ) return chunkfiles
4030	def load ( self ) : con = sqlite3 . connect ( self . tmp_cookie_file ) cur = con . cursor ( ) try : cur . execute ( 'SELECT host_key, path, secure, expires_utc, name, value, encrypted_value ' 'FROM cookies WHERE host_key like "%{}%";' . format ( self . domain_name ) ) except sqlite3 . OperationalError : cur . execute ( 'SELECT host_key, path, is_secure, expires_utc, name, value, encrypted_value ' 'FROM cookies WHERE host_key like "%{}%";' . format ( self . domain_name ) ) cj = http . cookiejar . CookieJar ( ) for item in cur . fetchall ( ) : host , path , secure , expires , name = item [ : 5 ] value = self . _decrypt ( item [ 5 ] , item [ 6 ] ) c = create_cookie ( host , path , secure , expires , name , value ) cj . set_cookie ( c ) con . close ( ) return cj
7404	def below ( self , ref ) : if not self . _valid_ordering_reference ( ref ) : raise ValueError ( "%r can only be moved below instances of %r which %s equals %r." % ( self , self . __class__ , self . order_with_respect_to , self . _get_order_with_respect_to ( ) ) ) if self . order == ref . order : return if self . order > ref . order : o = self . get_ordering_queryset ( ) . filter ( order__gt = ref . order ) . aggregate ( Min ( 'order' ) ) . get ( 'order__min' ) or 0 else : o = ref . order self . to ( o )
12125	def pprint_args ( self , pos_args , keyword_args , infix_operator = None , extra_params = { } ) : if infix_operator and not ( len ( pos_args ) == 2 and keyword_args == [ ] ) : raise Exception ( 'Infix format requires exactly two' ' positional arguments and no keywords' ) ( kwargs , _ , _ , _ ) = self . _pprint_args self . _pprint_args = ( keyword_args + kwargs , pos_args , infix_operator , extra_params )
9885	def load_all_variables ( self ) : self . data = { } file_var_names = self . z_variable_info . keys ( ) dim_sizes = [ ] rec_nums = [ ] data_types = [ ] names = [ ] for i , name in enumerate ( file_var_names ) : dim_sizes . extend ( self . z_variable_info [ name ] [ 'dim_sizes' ] ) rec_nums . append ( self . z_variable_info [ name ] [ 'rec_num' ] ) data_types . append ( self . z_variable_info [ name ] [ 'data_type' ] ) names . append ( name . ljust ( 256 ) ) dim_sizes = np . array ( dim_sizes ) rec_nums = np . array ( rec_nums ) data_types = np . array ( data_types ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'real4' ] , fortran_cdf . get_multi_z_real4 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'float' ] , fortran_cdf . get_multi_z_real4 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'real8' ] , fortran_cdf . get_multi_z_real8 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'double' ] , fortran_cdf . get_multi_z_real8 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'int4' ] , fortran_cdf . get_multi_z_int4 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'uint4' ] , fortran_cdf . get_multi_z_int4 , data_offset = 2 ** 32 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'int2' ] , fortran_cdf . get_multi_z_int2 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'uint2' ] , fortran_cdf . get_multi_z_int2 , data_offset = 2 ** 16 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'int1' ] , fortran_cdf . get_multi_z_int1 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'uint1' ] , fortran_cdf . get_multi_z_int1 , data_offset = 2 ** 8 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'byte' ] , fortran_cdf . get_multi_z_int1 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'epoch' ] , fortran_cdf . get_multi_z_real8 , epoch = True ) self . _call_multi_fortran_z ( names , data_types , rec_nums , 2 * dim_sizes , self . cdf_data_types [ 'epoch16' ] , fortran_cdf . get_multi_z_epoch16 , epoch16 = True ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'TT2000' ] , fortran_cdf . get_multi_z_tt2000 , epoch = True ) self . data_loaded = True
5304	def sanitize_color_palette ( colorpalette ) : new_palette = { } def __make_valid_color_name ( name ) : if len ( name ) == 1 : name = name [ 0 ] return name [ : 1 ] . lower ( ) + name [ 1 : ] return name [ 0 ] . lower ( ) + '' . join ( word . capitalize ( ) for word in name [ 1 : ] ) for key , value in colorpalette . items ( ) : if isinstance ( value , str ) : value = utils . hex_to_rgb ( value ) new_palette [ __make_valid_color_name ( key . split ( ) ) ] = value return new_palette
5028	def transmit_learner_data ( self , user ) : exporter = self . get_learner_data_exporter ( user ) transmitter = self . get_learner_data_transmitter ( ) transmitter . transmit ( exporter )
9091	def _make_namespace ( self ) -> Namespace : namespace = Namespace ( name = self . _get_namespace_name ( ) , keyword = self . _get_namespace_keyword ( ) , url = self . _get_namespace_url ( ) , version = str ( time . asctime ( ) ) , ) self . session . add ( namespace ) entries = self . _get_namespace_entries ( namespace ) self . session . add_all ( entries ) t = time . time ( ) log . info ( 'committing models' ) self . session . commit ( ) log . info ( 'committed models in %.2f seconds' , time . time ( ) - t ) return namespace
9704	def checkSerial ( self ) : for item in self . rxSerial ( self . _TUN . _tun . mtu ) : try : self . _TUN . _tun . write ( item ) except pytun . Error as error : print ( "pytun error writing: {0}" . format ( item ) ) print ( error )
6020	def simulate_as_gaussian ( cls , shape , pixel_scale , sigma , centre = ( 0.0 , 0.0 ) , axis_ratio = 1.0 , phi = 0.0 ) : from autolens . model . profiles . light_profiles import EllipticalGaussian gaussian = EllipticalGaussian ( centre = centre , axis_ratio = axis_ratio , phi = phi , intensity = 1.0 , sigma = sigma ) grid_1d = grid_util . regular_grid_1d_masked_from_mask_pixel_scales_and_origin ( mask = np . full ( shape , False ) , pixel_scales = ( pixel_scale , pixel_scale ) ) gaussian_1d = gaussian . intensities_from_grid ( grid = grid_1d ) gaussian_2d = mapping_util . map_unmasked_1d_array_to_2d_array_from_array_1d_and_shape ( array_1d = gaussian_1d , shape = shape ) return PSF ( array = gaussian_2d , pixel_scale = pixel_scale , renormalize = True )
7198	def describe_images ( self , idaho_image_results ) : results = idaho_image_results [ 'results' ] results = [ r for r in results if 'IDAHOImage' in r [ 'type' ] ] self . logger . debug ( 'Describing %s IDAHO images.' % len ( results ) ) catids = set ( [ r [ 'properties' ] [ 'catalogID' ] for r in results ] ) description = { } for catid in catids : description [ catid ] = { } description [ catid ] [ 'parts' ] = { } images = [ r for r in results if r [ 'properties' ] [ 'catalogID' ] == catid ] for image in images : description [ catid ] [ 'sensorPlatformName' ] = image [ 'properties' ] [ 'sensorPlatformName' ] part = int ( image [ 'properties' ] [ 'vendorDatasetIdentifier' ] . split ( ':' ) [ 1 ] [ - 3 : ] ) color = image [ 'properties' ] [ 'colorInterpretation' ] bucket = image [ 'properties' ] [ 'tileBucketName' ] identifier = image [ 'identifier' ] boundstr = image [ 'properties' ] [ 'footprintWkt' ] try : description [ catid ] [ 'parts' ] [ part ] except : description [ catid ] [ 'parts' ] [ part ] = { } description [ catid ] [ 'parts' ] [ part ] [ color ] = { } description [ catid ] [ 'parts' ] [ part ] [ color ] [ 'id' ] = identifier description [ catid ] [ 'parts' ] [ part ] [ color ] [ 'bucket' ] = bucket description [ catid ] [ 'parts' ] [ part ] [ color ] [ 'boundstr' ] = boundstr return description
13400	def addLogbook ( self , physDef = "LCLS" , mccDef = "MCC" , initialInstance = False ) : if self . logMenuCount < 5 : self . logMenus . append ( LogSelectMenu ( self . logui . multiLogLayout , initialInstance ) ) self . logMenus [ - 1 ] . addLogbooks ( self . logTypeList [ 1 ] , self . physics_programs , physDef ) self . logMenus [ - 1 ] . addLogbooks ( self . logTypeList [ 0 ] , self . mcc_programs , mccDef ) self . logMenus [ - 1 ] . show ( ) self . logMenuCount += 1 if initialInstance : QObject . connect ( self . logMenus [ - 1 ] . logButton , SIGNAL ( "clicked()" ) , self . addLogbook ) else : from functools import partial QObject . connect ( self . logMenus [ - 1 ] . logButton , SIGNAL ( "clicked()" ) , partial ( self . removeLogbook , self . logMenus [ - 1 ] ) )
9018	def new_pattern ( self , id_ , name , rows = None ) : if rows is None : rows = self . new_row_collection ( ) return self . _spec . new_pattern ( id_ , name , rows , self )
4066	def item_fields ( self ) : if self . templates . get ( "item_fields" ) and not self . _updated ( "/itemFields" , self . templates [ "item_fields" ] , "item_fields" ) : return self . templates [ "item_fields" ] [ "tmplt" ] query_string = "/itemFields" retrieved = self . _retrieve_data ( query_string ) return self . _cache ( retrieved , "item_fields" )
3646	def sendToWatchlist ( self , trade_id ) : method = 'PUT' url = 'watchlist' data = { 'auctionInfo' : [ { 'id' : trade_id } ] } return self . __request__ ( method , url , data = json . dumps ( data ) )
8928	def prep ( ctx , commit = True ) : cfg = config . load ( ) scm = scm_provider ( cfg . project_root , commit = commit , ctx = ctx ) if not scm . workdir_is_clean ( ) : notify . failure ( "You have uncommitted changes, please commit or stash them!" ) setup_cfg = cfg . rootjoin ( 'setup.cfg' ) if os . path . exists ( setup_cfg ) : with io . open ( setup_cfg , encoding = 'utf-8' ) as handle : data = handle . readlines ( ) changed = False for i , line in enumerate ( data ) : if any ( line . startswith ( i ) for i in ( 'tag_build' , 'tag_date' ) ) : data [ i ] = '#' + data [ i ] changed = True if changed and commit : notify . info ( "Rewriting 'setup.cfg'..." ) with io . open ( setup_cfg , 'w' , encoding = 'utf-8' ) as handle : handle . write ( '' . join ( data ) ) scm . add_file ( 'setup.cfg' ) elif changed : notify . warning ( "WOULD rewrite 'setup.cfg', but --no-commit was passed" ) else : notify . warning ( "Cannot rewrite 'setup.cfg', none found!" ) ctx . run ( 'python setup.py -q develop -U' ) version = capture ( 'python setup.py --version' ) ctx . run ( 'invoke clean --all build --docs release.dist' ) for distfile in os . listdir ( 'dist' ) : trailer = distfile . split ( '-' + version ) [ 1 ] trailer , _ = os . path . splitext ( trailer ) if trailer and trailer [ 0 ] not in '.-' : notify . failure ( "The version found in 'dist' seems to be" " a pre-release one! [{}{}]" . format ( version , trailer ) ) scm . commit ( ctx . rituals . release . commit . message . format ( version = version ) ) scm . tag ( ctx . rituals . release . tag . name . format ( version = version ) , ctx . rituals . release . tag . message . format ( version = version ) )
12238	def rosenbrock ( theta ) : x , y = theta obj = ( 1 - x ) ** 2 + 100 * ( y - x ** 2 ) ** 2 grad = np . zeros ( 2 ) grad [ 0 ] = 2 * x - 400 * ( x * y - x ** 3 ) - 2 grad [ 1 ] = 200 * ( y - x ** 2 ) return obj , grad
6194	def datafile_from_hash ( hash_ , prefix , path ) : pattern = '%s_%s*.h*' % ( prefix , hash_ ) datafiles = list ( path . glob ( pattern ) ) if len ( datafiles ) == 0 : raise NoMatchError ( 'No matches for "%s"' % pattern ) if len ( datafiles ) > 1 : raise MultipleMatchesError ( 'More than 1 match for "%s"' % pattern ) return datafiles [ 0 ]
13283	def _parse_whitespace_argument ( source , name ) : r command_pattern = r'\\(' + name + r')(?:[\s{[%])' command_match = re . search ( command_pattern , source ) if command_match is not None : source = source [ command_match . end ( 1 ) : ] pattern = r'(?P<content>\S+)(?:[ %\t\n]+)' match = re . search ( pattern , source ) if match is None : message = ( 'When parsing {}, did not find whitespace-delimited command ' 'argument' ) raise CommandParserError ( message . format ( name ) ) content = match . group ( 'content' ) content . strip ( ) return content
12013	def generate_panel ( self , img ) : plt . figure ( figsize = ( 14 , 6 ) ) ax = plt . gca ( ) fig = plt . gcf ( ) plt . subplot ( 122 ) data_save = np . zeros_like ( self . postcard ) self . roll_best = np . zeros ( ( 4 , 2 ) ) for i in range ( 4 ) : g = np . where ( self . qs == i ) [ 0 ] wh = np . where ( self . times [ g ] > 54947 ) self . roll_best [ i ] = self . do_rolltest ( g , wh ) self . do_photometry ( ) for i in range ( 4 ) : g = np . where ( self . qs == i ) [ 0 ] plt . errorbar ( self . times [ g ] , self . obs_flux [ g ] , yerr = self . flux_uncert [ i ] , fmt = fmt [ i ] ) plt . xlabel ( 'Time' , fontsize = 20 ) plt . ylabel ( 'Relative Flux' , fontsize = 20 ) plt . subplot ( 121 ) implot = plt . imshow ( img , interpolation = 'nearest' , cmap = 'gray' , vmin = 98000 * 52 , vmax = 104000 * 52 ) cid = fig . canvas . mpl_connect ( 'button_press_event' , self . onclick ) plt . show ( block = True )
10967	def sync_params ( self ) : def _normalize ( comps , param ) : vals = [ c . get_values ( param ) for c in comps ] diff = any ( [ vals [ i ] != vals [ i + 1 ] for i in range ( len ( vals ) - 1 ) ] ) if diff : for c in comps : c . set_values ( param , vals [ 0 ] ) for param , comps in iteritems ( self . lmap ) : if isinstance ( comps , list ) and len ( comps ) > 1 : _normalize ( comps , param )
5789	def handle_openssl_error ( result , exception_class = None ) : if result > 0 : return if exception_class is None : exception_class = OSError error_num = libcrypto . ERR_get_error ( ) buffer = buffer_from_bytes ( 120 ) libcrypto . ERR_error_string ( error_num , buffer ) error_string = byte_string_from_buffer ( buffer ) raise exception_class ( _try_decode ( error_string ) )
12686	def find ( self , * args ) : curr_node = self . __root return self . __traverse ( curr_node , 0 , * args )
6884	def find_lc_timegroups ( lctimes , mingap = 4.0 ) : lc_time_diffs = [ ( lctimes [ x ] - lctimes [ x - 1 ] ) for x in range ( 1 , len ( lctimes ) ) ] lc_time_diffs = np . array ( lc_time_diffs ) group_start_indices = np . where ( lc_time_diffs > mingap ) [ 0 ] if len ( group_start_indices ) > 0 : group_indices = [ ] for i , gindex in enumerate ( group_start_indices ) : if i == 0 : group_indices . append ( slice ( 0 , gindex + 1 ) ) else : group_indices . append ( slice ( group_start_indices [ i - 1 ] + 1 , gindex + 1 ) ) group_indices . append ( slice ( group_start_indices [ - 1 ] + 1 , len ( lctimes ) ) ) else : group_indices = [ slice ( 0 , len ( lctimes ) ) ] return len ( group_indices ) , group_indices
11794	def mac ( csp , var , value , assignment , removals ) : "Maintain arc consistency." return AC3 ( csp , [ ( X , var ) for X in csp . neighbors [ var ] ] , removals )
5666	def _run ( self ) : if self . _has_run : raise RuntimeError ( "This spreader instance has already been run: " "create a new Spreader object for a new run." ) i = 1 while self . event_heap . size ( ) > 0 and len ( self . _uninfected_stops ) > 0 : event = self . event_heap . pop_next_event ( ) this_stop = self . _stop_I_to_spreading_stop [ event . from_stop_I ] if event . arr_time_ut > self . start_time_ut + self . max_duration_ut : break if this_stop . can_infect ( event ) : target_stop = self . _stop_I_to_spreading_stop [ event . to_stop_I ] already_visited = target_stop . has_been_visited ( ) target_stop . visit ( event ) if not already_visited : self . _uninfected_stops . remove ( event . to_stop_I ) print ( i , self . event_heap . size ( ) ) transfer_distances = self . gtfs . get_straight_line_transfer_distances ( event . to_stop_I ) self . event_heap . add_walk_events_to_heap ( transfer_distances , event , self . start_time_ut , self . walk_speed , self . _uninfected_stops , self . max_duration_ut ) i += 1 self . _has_run = True
12448	def from_cookie_string ( self , cookie_string ) : for key_value in cookie_string . split ( ';' ) : if '=' in key_value : key , value = key_value . split ( '=' , 1 ) else : key = key_value strip_key = key . strip ( ) if strip_key and strip_key . lower ( ) not in COOKIE_ATTRIBUTE_NAMES : self [ strip_key ] = value . strip ( )
585	def _deleteRangeFromKNN ( self , start = 0 , end = None ) : classifier = self . htm_prediction_model . _getAnomalyClassifier ( ) knn = classifier . getSelf ( ) . _knn prototype_idx = numpy . array ( classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) ) if end is None : end = prototype_idx . max ( ) + 1 idsIdxToDelete = numpy . logical_and ( prototype_idx >= start , prototype_idx < end ) idsToDelete = prototype_idx [ idsIdxToDelete ] nProtos = knn . _numPatterns knn . removeIds ( idsToDelete . tolist ( ) ) assert knn . _numPatterns == nProtos - len ( idsToDelete )
2183	def existing_versions ( self ) : import glob pattern = join ( self . dpath , self . fname + '_*' + self . ext ) for fname in glob . iglob ( pattern ) : data_fpath = join ( self . dpath , fname ) yield data_fpath
3801	def calculate ( self , T , method ) : r if method == SHEFFY_JOHNSON : kl = Sheffy_Johnson ( T , self . MW , self . Tm ) elif method == SATO_RIEDEL : kl = Sato_Riedel ( T , self . MW , self . Tb , self . Tc ) elif method == GHARAGHEIZI_L : kl = Gharagheizi_liquid ( T , self . MW , self . Tb , self . Pc , self . omega ) elif method == NICOLA : kl = Nicola ( T , self . MW , self . Tc , self . Pc , self . omega ) elif method == NICOLA_ORIGINAL : kl = Nicola_original ( T , self . MW , self . Tc , self . omega , self . Hfus ) elif method == LAKSHMI_PRASAD : kl = Lakshmi_Prasad ( T , self . MW ) elif method == BAHADORI_L : kl = Bahadori_liquid ( T , self . MW ) elif method == DIPPR_PERRY_8E : kl = EQ100 ( T , * self . Perrys2_315_coeffs ) elif method == VDI_PPDS : kl = horner ( self . VDI_PPDS_coeffs , T ) elif method == COOLPROP : kl = CoolProp_T_dependent_property ( T , self . CASRN , 'L' , 'l' ) elif method in self . tabular_data : kl = self . interpolate ( T , method ) return kl
8924	def verify ( self ) : value = self . get ( 'verify' , 'true' ) if isinstance ( value , bool ) : verify = value elif value . lower ( ) == 'true' : verify = True elif value . lower ( ) == 'false' : verify = False else : verify = value return verify
3469	def copy ( self ) : model = self . _model self . _model = None for i in self . _metabolites : i . _model = None for i in self . _genes : i . _model = None new_reaction = deepcopy ( self ) self . _model = model for i in self . _metabolites : i . _model = model for i in self . _genes : i . _model = model return new_reaction
2492	def create_review_node ( self , review ) : review_node = BNode ( ) type_triple = ( review_node , RDF . type , self . spdx_namespace . Review ) self . graph . add ( type_triple ) reviewer_node = Literal ( review . reviewer . to_value ( ) ) self . graph . add ( ( review_node , self . spdx_namespace . reviewer , reviewer_node ) ) reviewed_date_node = Literal ( review . review_date_iso_format ) reviewed_triple = ( review_node , self . spdx_namespace . reviewDate , reviewed_date_node ) self . graph . add ( reviewed_triple ) if review . has_comment : comment_node = Literal ( review . comment ) comment_triple = ( review_node , RDFS . comment , comment_node ) self . graph . add ( comment_triple ) return review_node
9597	def save_screenshot ( self , filename , quietly = False ) : imgData = self . take_screenshot ( ) try : with open ( filename , "wb" ) as f : f . write ( b64decode ( imgData . encode ( 'ascii' ) ) ) except IOError as err : if not quietly : raise err
2864	def readS8 ( self , register ) : result = self . readU8 ( register ) if result > 127 : result -= 256 return result
1299	def ResetConsoleColor ( ) -> bool : if sys . stdout : sys . stdout . flush ( ) bool ( ctypes . windll . kernel32 . SetConsoleTextAttribute ( _ConsoleOutputHandle , _DefaultConsoleColor ) )
12949	def copyModel ( mdl ) : copyNum = _modelCopyMap [ mdl ] _modelCopyMap [ mdl ] += 1 mdlCopy = type ( mdl . __name__ + '_Copy' + str ( copyNum ) , mdl . __bases__ , copy . deepcopy ( dict ( mdl . __dict__ ) ) ) mdlCopy . FIELDS = [ field . copy ( ) for field in mdl . FIELDS ] mdlCopy . INDEXED_FIELDS = [ str ( idxField ) for idxField in mdl . INDEXED_FIELDS ] mdlCopy . validateModel ( ) return mdlCopy
10835	def all ( self ) : response = self . api . get ( url = PATHS [ 'GET_PROFILES' ] ) for raw_profile in response : self . append ( Profile ( self . api , raw_profile ) ) return self
6713	def install_setuptools ( python_cmd = 'python' , use_sudo = True ) : setuptools_version = package_version ( 'setuptools' , python_cmd ) distribute_version = package_version ( 'distribute' , python_cmd ) if setuptools_version is None : _install_from_scratch ( python_cmd , use_sudo ) else : if distribute_version is None : _upgrade_from_setuptools ( python_cmd , use_sudo ) else : _upgrade_from_distribute ( python_cmd , use_sudo )
6587	def iterate_forever ( func , * args , ** kwargs ) : output = func ( * args , ** kwargs ) while True : try : playlist_item = next ( output ) playlist_item . prepare_playback ( ) yield playlist_item except StopIteration : output = func ( * args , ** kwargs )
11697	def count ( self ) : xml = get_changeset ( self . id ) actions = [ action . tag for action in xml . getchildren ( ) ] self . create = actions . count ( 'create' ) self . modify = actions . count ( 'modify' ) self . delete = actions . count ( 'delete' ) self . verify_editor ( ) try : if ( self . create / len ( actions ) > self . percentage and self . create > self . create_threshold and ( self . powerfull_editor or self . create > self . top_threshold ) ) : self . label_suspicious ( 'possible import' ) elif ( self . modify / len ( actions ) > self . percentage and self . modify > self . modify_threshold ) : self . label_suspicious ( 'mass modification' ) elif ( ( self . delete / len ( actions ) > self . percentage and self . delete > self . delete_threshold ) or self . delete > self . top_threshold ) : self . label_suspicious ( 'mass deletion' ) except ZeroDivisionError : print ( 'It seems this changeset was redacted' )
13639	def bind ( mod_path , with_path = None ) : if with_path : if os . path . isdir ( with_path ) : sys . path . insert ( 0 , with_path ) else : sys . path . insert ( 0 , with_path . rsplit ( '/' , 2 ) [ 0 ] ) pass mod = importlib . import_module ( mod_path ) settings = Settings ( ) for v in dir ( mod ) : if v [ 0 ] == '_' or type ( getattr ( mod , v ) ) . __name__ == 'module' : continue setattr ( settings , v , getattr ( mod , v ) ) pass Settings . _path = mod_path Settings . _wrapped = settings return settings
6751	def unregister ( self ) : for k in list ( env . keys ( ) ) : if k . startswith ( self . env_prefix ) : del env [ k ] try : del all_satchels [ self . name . upper ( ) ] except KeyError : pass try : del manifest_recorder [ self . name ] except KeyError : pass try : del manifest_deployers [ self . name . upper ( ) ] except KeyError : pass try : del manifest_deployers_befores [ self . name . upper ( ) ] except KeyError : pass try : del required_system_packages [ self . name . upper ( ) ] except KeyError : pass
6841	def supported_locales ( ) : family = distrib_family ( ) if family == 'debian' : return _parse_locales ( '/usr/share/i18n/SUPPORTED' ) elif family == 'arch' : return _parse_locales ( '/etc/locale.gen' ) elif family == 'redhat' : return _supported_locales_redhat ( ) else : raise UnsupportedFamily ( supported = [ 'debian' , 'arch' , 'redhat' ] )
512	def _updateMinDutyCyclesLocal ( self ) : for column in xrange ( self . _numColumns ) : neighborhood = self . _getColumnNeighborhood ( column ) maxActiveDuty = self . _activeDutyCycles [ neighborhood ] . max ( ) maxOverlapDuty = self . _overlapDutyCycles [ neighborhood ] . max ( ) self . _minOverlapDutyCycles [ column ] = ( maxOverlapDuty * self . _minPctOverlapDutyCycles )
2956	def load ( self ) : try : with open ( self . _state_file ) as f : state = yaml . safe_load ( f ) self . _containers = state [ 'containers' ] except ( IOError , OSError ) as err : if err . errno == errno . ENOENT : raise NotInitializedError ( "No blockade exists in this context" ) raise InconsistentStateError ( "Failed to load Blockade state: " + str ( err ) ) except Exception as err : raise InconsistentStateError ( "Failed to load Blockade state: " + str ( err ) )
11753	def make_tables ( grammar , precedence ) : ACTION = { } GOTO = { } labels = { } def get_label ( closure ) : if closure not in labels : labels [ closure ] = len ( labels ) return labels [ closure ] def resolve_shift_reduce ( lookahead , s_action , r_action ) : s_assoc , s_level = precedence [ lookahead ] r_assoc , r_level = precedence [ r_action [ 1 ] ] if s_level < r_level : return r_action elif s_level == r_level and r_assoc == LEFT : return r_action else : return s_action initial , closures , goto = grammar . closures ( ) for closure in closures : label = get_label ( closure ) for rule in closure : new_action , lookahead = None , rule . lookahead if not rule . at_end : symbol = rule . rhs [ rule . pos ] is_terminal = symbol in grammar . terminals has_goto = symbol in goto [ closure ] if is_terminal and has_goto : next_state = get_label ( goto [ closure ] [ symbol ] ) new_action , lookahead = ( 'shift' , next_state ) , symbol elif rule . production == grammar . start and rule . at_end : new_action = ( 'accept' , ) elif rule . at_end : new_action = ( 'reduce' , rule . production ) if new_action is None : continue prev_action = ACTION . get ( ( label , lookahead ) ) if prev_action is None or prev_action == new_action : ACTION [ label , lookahead ] = new_action else : types = ( prev_action [ 0 ] , new_action [ 0 ] ) if types == ( 'shift' , 'reduce' ) : chosen = resolve_shift_reduce ( lookahead , prev_action , new_action ) elif types == ( 'reduce' , 'shift' ) : chosen = resolve_shift_reduce ( lookahead , new_action , prev_action ) else : raise TableConflictError ( prev_action , new_action ) ACTION [ label , lookahead ] = chosen for symbol in grammar . nonterminals : if symbol in goto [ closure ] : GOTO [ label , symbol ] = get_label ( goto [ closure ] [ symbol ] ) return get_label ( initial ) , ACTION , GOTO
1717	def replacement_template ( rep , source , span , npar ) : n = 0 res = '' while n < len ( rep ) - 1 : char = rep [ n ] if char == '$' : if rep [ n + 1 ] == '$' : res += '$' n += 2 continue elif rep [ n + 1 ] == '`' : res += source [ : span [ 0 ] ] n += 2 continue elif rep [ n + 1 ] == '\'' : res += source [ span [ 1 ] : ] n += 2 continue elif rep [ n + 1 ] in DIGS : dig = rep [ n + 1 ] if n + 2 < len ( rep ) and rep [ n + 2 ] in DIGS : dig += rep [ n + 2 ] num = int ( dig ) if not num or num > len ( npar ) : res += '$' + dig else : res += npar [ num - 1 ] if npar [ num - 1 ] else '' n += 1 + len ( dig ) continue res += char n += 1 if n < len ( rep ) : res += rep [ - 1 ] return res
5613	def reproject_geometry ( geometry , src_crs = None , dst_crs = None , error_on_clip = False , validity_check = True , antimeridian_cutting = False ) : src_crs = _validated_crs ( src_crs ) dst_crs = _validated_crs ( dst_crs ) def _reproject_geom ( geometry , src_crs , dst_crs ) : if geometry . is_empty : return geometry else : out_geom = to_shape ( transform_geom ( src_crs . to_dict ( ) , dst_crs . to_dict ( ) , mapping ( geometry ) , antimeridian_cutting = antimeridian_cutting ) ) return _repair ( out_geom ) if validity_check else out_geom if src_crs == dst_crs or geometry . is_empty : return _repair ( geometry ) elif ( dst_crs . is_epsg_code and dst_crs . get ( "init" ) in CRS_BOUNDS and dst_crs . get ( "init" ) != "epsg:4326" ) : wgs84_crs = CRS ( ) . from_epsg ( 4326 ) crs_bbox = box ( * CRS_BOUNDS [ dst_crs . get ( "init" ) ] ) geometry_4326 = _reproject_geom ( geometry , src_crs , wgs84_crs ) if error_on_clip and not geometry_4326 . within ( crs_bbox ) : raise RuntimeError ( "geometry outside target CRS bounds" ) return _reproject_geom ( crs_bbox . intersection ( geometry_4326 ) , wgs84_crs , dst_crs ) else : return _reproject_geom ( geometry , src_crs , dst_crs )
6932	def colormagdiagram_cplist ( cplist , outpkl , color_mag1 = [ 'gaiamag' , 'sdssg' ] , color_mag2 = [ 'kmag' , 'kmag' ] , yaxis_mag = [ 'gaia_absmag' , 'rpmj' ] ) : cplist_objectids = [ ] cplist_mags = [ ] cplist_colors = [ ] for cpf in cplist : cpd = _read_checkplot_picklefile ( cpf ) cplist_objectids . append ( cpd [ 'objectid' ] ) thiscp_mags = [ ] thiscp_colors = [ ] for cm1 , cm2 , ym in zip ( color_mag1 , color_mag2 , yaxis_mag ) : if ( ym in cpd [ 'objectinfo' ] and cpd [ 'objectinfo' ] [ ym ] is not None ) : thiscp_mags . append ( cpd [ 'objectinfo' ] [ ym ] ) else : thiscp_mags . append ( np . nan ) if ( cm1 in cpd [ 'objectinfo' ] and cpd [ 'objectinfo' ] [ cm1 ] is not None and cm2 in cpd [ 'objectinfo' ] and cpd [ 'objectinfo' ] [ cm2 ] is not None ) : thiscp_colors . append ( cpd [ 'objectinfo' ] [ cm1 ] - cpd [ 'objectinfo' ] [ cm2 ] ) else : thiscp_colors . append ( np . nan ) cplist_mags . append ( thiscp_mags ) cplist_colors . append ( thiscp_colors ) cplist_objectids = np . array ( cplist_objectids ) cplist_mags = np . array ( cplist_mags ) cplist_colors = np . array ( cplist_colors ) cmddict = { 'objectids' : cplist_objectids , 'mags' : cplist_mags , 'colors' : cplist_colors , 'color_mag1' : color_mag1 , 'color_mag2' : color_mag2 , 'yaxis_mag' : yaxis_mag } with open ( outpkl , 'wb' ) as outfd : pickle . dump ( cmddict , outfd , pickle . HIGHEST_PROTOCOL ) plt . close ( 'all' ) return cmddict
6458	def _m_degree ( self , term ) : mdeg = 0 last_was_vowel = False for letter in term : if letter in self . _vowels : last_was_vowel = True else : if last_was_vowel : mdeg += 1 last_was_vowel = False return mdeg
3732	def mixture_from_any ( ID ) : if type ( ID ) == list : if len ( ID ) == 1 : ID = ID [ 0 ] else : raise Exception ( 'If the input is a list, the list must contain only one item.' ) ID = ID . lower ( ) . strip ( ) ID2 = ID . replace ( ' ' , '' ) ID3 = ID . replace ( '-' , '' ) for i in [ ID , ID2 , ID3 ] : if i in _MixtureDictLookup : return _MixtureDictLookup [ i ] raise Exception ( 'Mixture name not recognized' )
10520	def oneup ( self , window_name , object_name , iterations ) : if not self . verifyscrollbarvertical ( window_name , object_name ) : raise LdtpServerException ( 'Object not vertical scrollbar' ) object_handle = self . _get_object_handle ( window_name , object_name ) i = 0 minValue = 1.0 / 8 flag = False while i < iterations : if object_handle . AXValue <= 0 : raise LdtpServerException ( 'Minimum limit reached' ) object_handle . AXValue -= minValue time . sleep ( 1.0 / 100 ) flag = True i += 1 if flag : return 1 else : raise LdtpServerException ( 'Unable to decrease scrollbar' )
11553	def disable_analog_reporting ( self , pin ) : command = [ self . _command_handler . REPORT_ANALOG + pin , self . REPORTING_DISABLE ] self . _command_handler . send_command ( command )
2470	def set_file_notice ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_notice_set : self . file_notice_set = True if validations . validate_file_notice ( text ) : self . file ( doc ) . notice = str_from_text ( text ) else : raise SPDXValueError ( 'File::Notice' ) else : raise CardinalityError ( 'File::Notice' ) else : raise OrderError ( 'File::Notice' )
10745	def _validate_many ( args , specs , defaults , passed_conditions , value_conditions , allow_unknowns , unknowns_spec ) : validated_args = builtins . dict ( ) passed_but_not_specified = set ( args . keys ( ) ) - set ( specs . keys ( ) ) if passed_but_not_specified : if not allow_unknowns : raise ValueError ( ( 'Arguments {} were passed but not specified (use ' + '`allow_unknowns=True` to avoid this error)' . format ( passed_but_not_specified ) ) ) else : for arg in passed_but_not_specified : if unknowns_spec is not None : specs [ arg ] = unknowns_spec if passed_conditions : validate ( args , Dict ( passed_conditions = passed_conditions ) ) for arg in specs : if ( not arg in args ) or NotPassed ( args [ arg ] ) : if arg in defaults : if isinstance ( defaults [ arg ] , DefaultGenerator ) : validated_args [ arg ] = defaults [ arg ] ( ) else : validated_args [ arg ] = defaults [ arg ] else : validated_args [ arg ] = NotPassed else : validated_args [ arg ] = validate ( args [ arg ] , specs [ arg ] ) if value_conditions : validated_args = validate ( validated_args , value_conditions ) return validated_args
12688	def send_now ( users , label , extra_context = None , sender = None ) : sent = False if extra_context is None : extra_context = { } notice_type = NoticeType . objects . get ( label = label ) current_language = get_language ( ) for user in users : try : language = get_notification_language ( user ) except LanguageStoreNotAvailable : language = None if language is not None : activate ( language ) for backend in NOTIFICATION_BACKENDS . values ( ) : if backend . can_send ( user , notice_type ) : backend . deliver ( user , sender , notice_type , extra_context ) sent = True activate ( current_language ) return sent
8778	def _check_collisions ( self , new_range , existing_ranges ) : def _contains ( num , r1 ) : return ( num >= r1 [ 0 ] and num <= r1 [ 1 ] ) def _is_overlap ( r1 , r2 ) : return ( _contains ( r1 [ 0 ] , r2 ) or _contains ( r1 [ 1 ] , r2 ) or _contains ( r2 [ 0 ] , r1 ) or _contains ( r2 [ 1 ] , r1 ) ) for existing_range in existing_ranges : if _is_overlap ( new_range , existing_range ) : return True return False
13616	def scaffold ( ) : click . echo ( "A whole new site? Awesome." ) title = click . prompt ( "What's the title?" ) url = click . prompt ( "Great. What's url? http://" ) click . echo ( "Got it. Creating %s..." % url )
10674	def load_data_auxi ( path = '' ) : compounds . clear ( ) if path == '' : path = default_data_path if not os . path . exists ( path ) : warnings . warn ( 'The specified data file path does not exist. (%s)' % path ) return files = glob . glob ( os . path . join ( path , 'Compound_*.json' ) ) for file in files : compound = Compound . read ( file ) compounds [ compound . formula ] = compound
3215	def get_vpc_flow_logs ( vpc , ** conn ) : fl_result = describe_flow_logs ( Filters = [ { "Name" : "resource-id" , "Values" : [ vpc [ "id" ] ] } ] , ** conn ) fl_ids = [ ] for fl in fl_result : fl_ids . append ( fl [ "FlowLogId" ] ) return fl_ids
10928	def _run2 ( self ) : if self . check_update_J ( ) : self . update_J ( ) else : if self . check_Broyden_J ( ) : self . update_Broyden_J ( ) if self . check_update_eig_J ( ) : self . update_eig_J ( ) _last_residuals = self . calc_residuals ( ) . copy ( ) _last_error = 1 * self . error _last_vals = self . param_vals . copy ( ) delta_params_1 = self . find_LM_updates ( self . calc_grad ( ) , do_correct_damping = False ) self . decrease_damping ( ) delta_params_2 = self . find_LM_updates ( self . calc_grad ( ) , do_correct_damping = False ) self . decrease_damping ( undo_decrease = True ) er1 = self . update_function ( self . param_vals + delta_params_1 ) er2 = self . update_function ( self . param_vals + delta_params_2 ) triplet = ( self . error , er1 , er2 ) best_step = find_best_step ( triplet ) if best_step == 0 : _ = self . update_function ( self . param_vals . copy ( ) ) grad = self . calc_grad ( ) CLOG . debug ( 'Bad step, increasing damping' ) CLOG . debug ( '%f\t%f\t%f' % triplet ) for _try in range ( self . _max_inner_loop ) : self . increase_damping ( ) delta_vals = self . find_LM_updates ( grad ) er_new = self . update_function ( self . param_vals + delta_vals ) good_step = er_new < self . error if good_step : self . update_param_vals ( delta_vals , incremental = True ) self . error = er_new CLOG . debug ( 'Sufficiently increased damping' ) CLOG . debug ( '%f\t%f' % ( triplet [ 0 ] , self . error ) ) break else : CLOG . warn ( 'Stuck!' ) self . error = self . update_function ( self . param_vals . copy ( ) ) elif best_step == 1 : good_step = True CLOG . debug ( 'Good step, same damping' ) CLOG . debug ( '%f\t%f\t%f' % triplet ) er1_1 = self . update_function ( self . param_vals + delta_params_1 ) if np . abs ( er1_1 - er1 ) > 1e-6 : raise RuntimeError ( 'Function updates are not exact.' ) self . update_param_vals ( delta_params_1 , incremental = True ) self . error = er1 elif best_step == 2 : good_step = True self . error = er2 CLOG . debug ( 'Good step, decreasing damping' ) CLOG . debug ( '%f\t%f\t%f' % triplet ) self . update_param_vals ( delta_params_2 , incremental = True ) self . decrease_damping ( ) if good_step : self . _last_residuals = _last_residuals self . _last_error = _last_error self . _last_vals = _last_vals self . error self . do_internal_run ( initial_count = 1 )
11929	def watch_files ( self ) : try : while 1 : sleep ( 1 ) try : files_stat = self . get_files_stat ( ) except SystemExit : logger . error ( "Error occurred, server shut down" ) self . shutdown_server ( ) if self . files_stat != files_stat : logger . info ( "Changes detected, start rebuilding.." ) try : generator . re_generate ( ) global _root _root = generator . root except SystemExit : logger . error ( "Error occurred, server shut down" ) self . shutdown_server ( ) self . files_stat = files_stat except KeyboardInterrupt : logger . info ( "^C received, shutting down watcher" ) self . shutdown_watcher ( )
6659	def _bias_correction ( V_IJ , inbag , pred_centered , n_trees ) : n_train_samples = inbag . shape [ 0 ] n_var = np . mean ( np . square ( inbag [ 0 : n_trees ] ) . mean ( axis = 1 ) . T . view ( ) - np . square ( inbag [ 0 : n_trees ] . mean ( axis = 1 ) ) . T . view ( ) ) boot_var = np . square ( pred_centered ) . sum ( axis = 1 ) / n_trees bias_correction = n_train_samples * n_var * boot_var / n_trees V_IJ_unbiased = V_IJ - bias_correction return V_IJ_unbiased
2934	def merge_option_and_config_str ( cls , option_name , config , options ) : opt = getattr ( options , option_name , None ) if opt : config . set ( CONFIG_SECTION_NAME , option_name , opt ) elif config . has_option ( CONFIG_SECTION_NAME , option_name ) : setattr ( options , option_name , config . get ( CONFIG_SECTION_NAME , option_name ) )
12367	def create ( self , name , ip_address ) : return ( self . post ( name = name , ip_address = ip_address ) . get ( self . singular , None ) )
8861	def defined_names ( request_data ) : global _old_definitions ret_val = [ ] path = request_data [ 'path' ] toplvl_definitions = jedi . names ( request_data [ 'code' ] , path , 'utf-8' ) for d in toplvl_definitions : definition = _extract_def ( d , path ) if d . type != 'import' : ret_val . append ( definition ) ret_val = [ d . to_dict ( ) for d in ret_val ] return ret_val
11425	def record_match_subfields ( rec , tag , ind1 = " " , ind2 = " " , sub_key = None , sub_value = '' , sub_key2 = None , sub_value2 = '' , case_sensitive = True ) : if sub_key is None : raise TypeError ( "None object passed for parameter sub_key." ) if sub_key2 is not None and sub_value2 is '' : raise TypeError ( "Parameter sub_key2 defined but sub_value2 is None, " + "function requires a value for comparrison." ) ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) if not case_sensitive : sub_value = sub_value . lower ( ) sub_value2 = sub_value2 . lower ( ) for field in record_get_field_instances ( rec , tag , ind1 , ind2 ) : subfields = dict ( field_get_subfield_instances ( field ) ) if not case_sensitive : for k , v in subfields . iteritems ( ) : subfields [ k ] = v . lower ( ) if sub_key in subfields : if sub_value is '' : return field [ 4 ] else : if sub_value == subfields [ sub_key ] : if sub_key2 is None : return field [ 4 ] else : if sub_key2 in subfields : if sub_value2 == subfields [ sub_key2 ] : return field [ 4 ] return False
5699	def write_stats_as_csv ( gtfs , path_to_csv , re_write = False ) : stats_dict = get_stats ( gtfs ) if re_write : os . remove ( path_to_csv ) is_new = True mode = 'r' if os . path . exists ( path_to_csv ) else 'w+' with open ( path_to_csv , mode ) as csvfile : for line in csvfile : if line : is_new = False else : is_new = True with open ( path_to_csv , 'a' ) as csvfile : if ( sys . version_info > ( 3 , 0 ) ) : delimiter = u"," else : delimiter = b"," statswriter = csv . writer ( csvfile , delimiter = delimiter ) if is_new : statswriter . writerow ( [ key for key in sorted ( stats_dict . keys ( ) ) ] ) row_to_write = [ ] for key in sorted ( stats_dict . keys ( ) ) : row_to_write . append ( stats_dict [ key ] ) statswriter . writerow ( row_to_write )
7877	def _bind_success ( self , stanza ) : payload = stanza . get_payload ( ResourceBindingPayload ) jid = payload . jid if not jid : raise BadRequestProtocolError ( u"<jid/> element mising in" " the bind response" ) self . stream . me = jid self . stream . event ( AuthorizedEvent ( self . stream . me ) )
8892	def get_default ( self ) : if self . has_default ( ) : if callable ( self . default ) : default = self . default ( ) if isinstance ( default , uuid . UUID ) : return default . hex return default if isinstance ( self . default , uuid . UUID ) : return self . default . hex return self . default return None
11846	def add_thing ( self , thing , location = None ) : if not isinstance ( thing , Thing ) : thing = Agent ( thing ) assert thing not in self . things , "Don't add the same thing twice" thing . location = location or self . default_location ( thing ) self . things . append ( thing ) if isinstance ( thing , Agent ) : thing . performance = 0 self . agents . append ( thing )
7893	def leave ( self ) : if self . joined : p = MucPresence ( to_jid = self . room_jid , stanza_type = "unavailable" ) self . manager . stream . send ( p )
4427	async def _seek ( self , ctx , * , time : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is_playing : return await ctx . send ( 'Not playing.' ) seconds = time_rx . search ( time ) if not seconds : return await ctx . send ( 'You need to specify the amount of seconds to skip!' ) seconds = int ( seconds . group ( ) ) * 1000 if time . startswith ( '-' ) : seconds *= - 1 track_time = player . position + seconds await player . seek ( track_time ) await ctx . send ( f'Moved track to **{lavalink.Utils.format_time(track_time)}**' )
5138	def process_file ( self , file ) : if sys . version_info [ 0 ] >= 3 : nxt = file . __next__ else : nxt = file . next for token in tokenize . generate_tokens ( nxt ) : self . process_token ( * token ) self . make_index ( )
4419	async def play_previous ( self ) : if not self . previous : raise NoPreviousTrack self . queue . insert ( 0 , self . previous ) await self . play ( ignore_shuffle = True )
9134	def get_module_config_cls ( module_name : str ) -> Type [ _AbstractModuleConfig ] : class ModuleConfig ( _AbstractModuleConfig ) : NAME = f'bio2bel:{module_name}' FILES = DEFAULT_CONFIG_PATHS + [ os . path . join ( DEFAULT_CONFIG_DIRECTORY , module_name , 'config.ini' ) ] return ModuleConfig
11927	def run_server ( self , port ) : try : self . server = MultiThreadedHTTPServer ( ( '0.0.0.0' , port ) , Handler ) except socket . error , e : logger . error ( str ( e ) ) sys . exit ( 1 ) logger . info ( "HTTP serve at http://0.0.0.0:%d (ctrl-c to stop) ..." % port ) try : self . server . serve_forever ( ) except KeyboardInterrupt : logger . info ( "^C received, shutting down server" ) self . shutdown_server ( )
13908	def create_subparsers ( self , parser ) : subparsers = parser . add_subparsers ( ) for name in self . config [ 'subparsers' ] : subparser = subparsers . add_parser ( name ) self . create_commands ( self . config [ 'subparsers' ] [ name ] , subparser )
7480	def sub_build_clustbits ( data , usort , nseeds ) : LOGGER . info ( "loading full _catcons file into memory" ) allcons = { } conshandle = os . path . join ( data . dirs . across , data . name + "_catcons.tmp" ) with gzip . open ( conshandle , 'rb' ) as iocons : cons = itertools . izip ( * [ iter ( iocons ) ] * 2 ) for namestr , seq in cons : nnn , sss = [ i . strip ( ) for i in namestr , seq ] allcons [ nnn [ 1 : ] ] = sss optim = ( ( nseeds // ( data . cpus * 4 ) ) + ( nseeds % ( data . cpus * 4 ) ) ) LOGGER . info ( "building clustbits, optim=%s, nseeds=%s, cpus=%s" , optim , nseeds , data . cpus ) with open ( usort , 'rb' ) as insort : isort = iter ( insort ) loci = 0 lastseed = 0 fseqs = [ ] seqlist = [ ] seqsize = 0 while 1 : try : hit , seed , ori = isort . next ( ) . strip ( ) . split ( ) except StopIteration : break try : if seed != lastseed : if fseqs : seqlist . append ( "\n" . join ( fseqs ) ) seqsize += 1 fseqs = [ ] if seqsize >= optim : if seqlist : loci += seqsize with open ( os . path . join ( data . tmpdir , data . name + ".chunk_{}" . format ( loci ) ) , 'w' ) as clustsout : LOGGER . debug ( "writing chunk - seqsize {} loci {} {}" . format ( seqsize , loci , clustsout . name ) ) clustsout . write ( "\n//\n//\n" . join ( seqlist ) + "\n//\n//\n" ) seqlist = [ ] seqsize = 0 fseqs . append ( ">{}\n{}" . format ( seed , allcons [ seed ] ) ) lastseed = seed seq = allcons [ hit ] if ori == "-" : seq = fullcomp ( seq ) [ : : - 1 ] fseqs . append ( ">{}\n{}" . format ( hit , seq ) ) except KeyError as inst : LOGGER . error ( "Bad Seed/Hit: seqsize {}\tloci {}\tseed {}\thit {}" . format ( seqsize , loci , seed , hit ) ) if fseqs : seqlist . append ( "\n" . join ( fseqs ) ) seqsize += 1 loci += seqsize if seqlist : with open ( os . path . join ( data . tmpdir , data . name + ".chunk_{}" . format ( loci ) ) , 'w' ) as clustsout : clustsout . write ( "\n//\n//\n" . join ( seqlist ) + "\n//\n//\n" ) del allcons clustbits = glob . glob ( os . path . join ( data . tmpdir , data . name + ".chunk_*" ) ) return clustbits , loci
1657	def IsInitializerList ( clean_lines , linenum ) : for i in xrange ( linenum , 1 , - 1 ) : line = clean_lines . elided [ i ] if i == linenum : remove_function_body = Match ( r'^(.*)\{\s*$' , line ) if remove_function_body : line = remove_function_body . group ( 1 ) if Search ( r'\s:\s*\w+[({]' , line ) : return True if Search ( r'\}\s*,\s*$' , line ) : return True if Search ( r'[{};]\s*$' , line ) : return False return False
7944	def _connected ( self ) : self . _auth_properties [ 'remote-ip' ] = self . _dst_addr [ 0 ] if self . _dst_service : self . _auth_properties [ 'service-domain' ] = self . _dst_name if self . _dst_hostname is not None : self . _auth_properties [ 'service-hostname' ] = self . _dst_hostname else : self . _auth_properties [ 'service-hostname' ] = self . _dst_addr [ 0 ] self . _auth_properties [ 'security-layer' ] = None self . event ( ConnectedEvent ( self . _dst_addr ) ) self . _set_state ( "connected" ) self . _stream . transport_connected ( )
10833	def query_by_admin ( cls , admin ) : return cls . query . filter_by ( admin_type = resolve_admin_type ( admin ) , admin_id = admin . get_id ( ) )
8606	def list_group_users ( self , group_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s/users?depth=%s' % ( group_id , str ( depth ) ) ) return response
11206	def gettz ( name ) : warnings . warn ( "zoneinfo.gettz() will be removed in future versions, " "to use the dateutil-provided zoneinfo files, instantiate a " "ZoneInfoFile object and use ZoneInfoFile.zones.get() " "instead. See the documentation for details." , DeprecationWarning ) if len ( _CLASS_ZONE_INSTANCE ) == 0 : _CLASS_ZONE_INSTANCE . append ( ZoneInfoFile ( getzoneinfofile_stream ( ) ) ) return _CLASS_ZONE_INSTANCE [ 0 ] . zones . get ( name )
1944	def _hook_syscall ( self , uc , data ) : logger . debug ( f"Stopping emulation at {hex(uc.reg_read(self._to_unicorn_id('RIP')))} to perform syscall" ) self . sync_unicorn_to_manticore ( ) from . . native . cpu . abstractcpu import Syscall self . _to_raise = Syscall ( ) uc . emu_stop ( )
12197	def get_task_options ( ) : options = ( ) task_classes = get_tasks ( ) for cls in task_classes : options += cls . option_list return options
6528	def get_reports ( ) : if not hasattr ( get_reports , '_CACHE' ) : get_reports . _CACHE = dict ( ) for entry in pkg_resources . iter_entry_points ( 'tidypy.reports' ) : try : get_reports . _CACHE [ entry . name ] = entry . load ( ) except ImportError as exc : output_error ( 'Could not load report "%s" defined by "%s": %s' % ( entry , entry . dist , exc , ) , ) return get_reports . _CACHE
1283	def autolink ( self , link , is_email = False ) : text = link = escape ( link ) if is_email : link = 'mailto:%s' % link return '<a href="%s">%s</a>' % ( link , text )
12548	def icc_img_to_zscore ( icc , center_image = False ) : vol = read_img ( icc ) . get_data ( ) v2 = vol [ vol != 0 ] if center_image : v2 = detrend ( v2 , axis = 0 ) vstd = np . linalg . norm ( v2 , ord = 2 ) / np . sqrt ( np . prod ( v2 . shape ) - 1 ) eps = np . finfo ( vstd . dtype ) . eps vol /= ( eps + vstd ) return vol
4810	def train_model ( best_processed_path , weight_path = '../weight/model_weight.h5' , verbose = 2 ) : x_train_char , x_train_type , y_train = prepare_feature ( best_processed_path , option = 'train' ) x_test_char , x_test_type , y_test = prepare_feature ( best_processed_path , option = 'test' ) validation_set = False if os . path . isdir ( os . path . join ( best_processed_path , 'val' ) ) : validation_set = True x_val_char , x_val_type , y_val = prepare_feature ( best_processed_path , option = 'val' ) if not os . path . isdir ( os . path . dirname ( weight_path ) ) : os . makedirs ( os . path . dirname ( weight_path ) ) callbacks_list = [ ReduceLROnPlateau ( ) , ModelCheckpoint ( weight_path , save_best_only = True , save_weights_only = True , monitor = 'val_loss' , mode = 'min' , verbose = 1 ) ] model = get_convo_nn2 ( ) train_params = [ ( 10 , 256 ) , ( 3 , 512 ) , ( 3 , 2048 ) , ( 3 , 4096 ) , ( 3 , 8192 ) ] for ( epochs , batch_size ) in train_params : print ( "train with {} epochs and {} batch size" . format ( epochs , batch_size ) ) if validation_set : model . fit ( [ x_train_char , x_train_type ] , y_train , epochs = epochs , batch_size = batch_size , verbose = verbose , callbacks = callbacks_list , validation_data = ( [ x_val_char , x_val_type ] , y_val ) ) else : model . fit ( [ x_train_char , x_train_type ] , y_train , epochs = epochs , batch_size = batch_size , verbose = verbose , callbacks = callbacks_list ) return model
1734	def remove_objects ( code , count = 1 ) : replacements = { } br = bracket_split ( code , [ '{}' , '[]' ] ) res = '' last = '' for e in br : if e [ 0 ] == '{' : n , temp_rep , cand_count = remove_objects ( e [ 1 : - 1 ] , count ) if is_object ( n , last ) : res += ' ' + OBJECT_LVAL % count replacements [ OBJECT_LVAL % count ] = e count += 1 else : res += '{%s}' % n count = cand_count replacements . update ( temp_rep ) elif e [ 0 ] == '[' : if is_array ( last ) : res += e else : n , rep , count = remove_objects ( e [ 1 : - 1 ] , count ) res += '[%s]' % n replacements . update ( rep ) else : res += e last = e return res , replacements , count
3988	def parallel_task_queue ( pool_size = multiprocessing . cpu_count ( ) ) : task_queue = TaskQueue ( pool_size ) yield task_queue task_queue . execute ( )
3468	def _update_awareness ( self ) : for x in self . _metabolites : x . _reaction . add ( self ) for x in self . _genes : x . _reaction . add ( self )
8075	def rect ( self , x , y , width , height , roundness = 0.0 , draw = True , ** kwargs ) : path = self . BezierPath ( ** kwargs ) path . rect ( x , y , width , height , roundness , self . rectmode ) if draw : path . draw ( ) return path
4996	def default_content_filter ( sender , instance , ** kwargs ) : if kwargs [ 'created' ] and not instance . content_filter : instance . content_filter = get_default_catalog_content_filter ( ) instance . save ( )
3694	def Tm ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ ] ) : r def list_methods ( ) : methods = [ ] if CASRN in Tm_ON_data . index : methods . append ( OPEN_NTBKM ) if CASRN in CRC_inorganic_data . index and not np . isnan ( CRC_inorganic_data . at [ CASRN , 'Tm' ] ) : methods . append ( CRC_INORG ) if CASRN in CRC_organic_data . index and not np . isnan ( CRC_organic_data . at [ CASRN , 'Tm' ] ) : methods . append ( CRC_ORG ) if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == OPEN_NTBKM : return float ( Tm_ON_data . at [ CASRN , 'Tm' ] ) elif Method == CRC_INORG : return float ( CRC_inorganic_data . at [ CASRN , 'Tm' ] ) elif Method == CRC_ORG : return float ( CRC_organic_data . at [ CASRN , 'Tm' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
124	def to_normalized_batch ( self ) : assert all ( [ attr is None for attr_name , attr in self . __dict__ . items ( ) if attr_name . endswith ( "_aug" ) ] ) , "Expected UnnormalizedBatch to not contain any augmented data " "before normalization, but at least one '*_aug' attribute was " "already set." images_unaug = nlib . normalize_images ( self . images_unaug ) shapes = None if images_unaug is not None : shapes = [ image . shape for image in images_unaug ] return Batch ( images = images_unaug , heatmaps = nlib . normalize_heatmaps ( self . heatmaps_unaug , shapes ) , segmentation_maps = nlib . normalize_segmentation_maps ( self . segmentation_maps_unaug , shapes ) , keypoints = nlib . normalize_keypoints ( self . keypoints_unaug , shapes ) , bounding_boxes = nlib . normalize_bounding_boxes ( self . bounding_boxes_unaug , shapes ) , polygons = nlib . normalize_polygons ( self . polygons_unaug , shapes ) , line_strings = nlib . normalize_line_strings ( self . line_strings_unaug , shapes ) , data = self . data )
7171	def train_subprocess ( self , * args , ** kwargs ) : ret = call ( [ sys . executable , '-m' , 'padatious' , 'train' , self . cache_dir , '-d' , json . dumps ( self . serialized_args ) , '-a' , json . dumps ( args ) , '-k' , json . dumps ( kwargs ) , ] ) if ret == 2 : raise TypeError ( 'Invalid train arguments: {} {}' . format ( args , kwargs ) ) data = self . serialized_args self . clear ( ) self . apply_training_args ( data ) self . padaos . compile ( ) if ret == 0 : self . must_train = False return True elif ret == 10 : return False else : raise ValueError ( 'Training failed and returned code: {}' . format ( ret ) )
13776	def get_abs_and_rel_paths ( self , root_path , file_name , input_dir ) : relative_dir = root_path . replace ( input_dir , '' ) return os . path . join ( root_path , file_name ) , relative_dir + '/' + file_name
5374	def _prefix_exists_in_gcs ( gcs_prefix , credentials = None ) : gcs_service = _get_storage_service ( credentials ) bucket_name , prefix = gcs_prefix [ len ( 'gs://' ) : ] . split ( '/' , 1 ) request = gcs_service . objects ( ) . list ( bucket = bucket_name , prefix = prefix , maxResults = 1 ) response = request . execute ( ) return response . get ( 'items' , None )
6625	def availableTags ( self ) : return [ GithubComponentVersion ( '' , t [ 0 ] , t [ 1 ] , self . name , cache_key = _createCacheKey ( 'tag' , t [ 0 ] , t [ 1 ] , self . name ) ) for t in self . _getTags ( ) ]
10325	def _binomial_pmf ( n , p ) : n = int ( n ) ret = np . empty ( n + 1 ) nmax = int ( np . round ( p * n ) ) ret [ nmax ] = 1.0 old_settings = np . seterr ( under = 'ignore' ) for i in range ( nmax + 1 , n + 1 ) : ret [ i ] = ret [ i - 1 ] * ( n - i + 1.0 ) / i * p / ( 1.0 - p ) for i in range ( nmax - 1 , - 1 , - 1 ) : ret [ i ] = ret [ i + 1 ] * ( i + 1.0 ) / ( n - i ) * ( 1.0 - p ) / p np . seterr ( ** old_settings ) return ret / ret . sum ( )
10607	def run ( self ) : self . prepare_to_run ( ) for i in range ( 0 , self . period_count ) : for e in self . entities : e . run ( self . clock ) self . clock . tick ( )
4660	def finalizeOp ( self , ops , account , permission , ** kwargs ) : if "append_to" in kwargs and kwargs [ "append_to" ] : if self . proposer : log . warning ( "You may not use append_to and self.proposer at " "the same time. Append new_proposal(..) instead" ) append_to = kwargs [ "append_to" ] parent = append_to . get_parent ( ) assert isinstance ( append_to , ( self . transactionbuilder_class , self . proposalbuilder_class ) ) append_to . appendOps ( ops ) if isinstance ( append_to , self . proposalbuilder_class ) : parent . appendSigner ( append_to . proposer , permission ) else : parent . appendSigner ( account , permission ) return append_to . get_parent ( ) elif self . proposer : proposal = self . proposal ( ) proposal . set_proposer ( self . proposer ) proposal . set_expiration ( self . proposal_expiration ) proposal . set_review ( self . proposal_review ) proposal . appendOps ( ops ) else : self . txbuffer . appendOps ( ops ) if "fee_asset" in kwargs and kwargs [ "fee_asset" ] : self . txbuffer . set_fee_asset ( kwargs [ "fee_asset" ] ) if self . unsigned : self . txbuffer . addSigningInformation ( account , permission ) return self . txbuffer elif self . bundle : self . txbuffer . appendSigner ( account , permission ) return self . txbuffer . json ( ) else : self . txbuffer . appendSigner ( account , permission ) self . txbuffer . sign ( ) return self . txbuffer . broadcast ( )
1038	def end ( self ) : return Range ( self . source_buffer , self . end_pos , self . end_pos , expanded_from = self . expanded_from )
4470	def _get_param_names ( cls ) : init = cls . __init__ args , varargs = inspect . getargspec ( init ) [ : 2 ] if varargs is not None : raise RuntimeError ( 'BaseTransformer objects cannot have varargs' ) args . pop ( 0 ) args . sort ( ) return args
9526	def to_fastg ( infile , outfile , circular = None ) : if circular is None : to_circularise = set ( ) elif type ( circular ) is not set : f = utils . open_file_read ( circular ) to_circularise = set ( [ x . rstrip ( ) for x in f . readlines ( ) ] ) utils . close ( f ) else : to_circularise = circular seq_reader = sequences . file_reader ( infile ) fout = utils . open_file_write ( outfile ) nodes = 1 for seq in seq_reader : new_id = '_' . join ( [ 'NODE' , str ( nodes ) , 'length' , str ( len ( seq ) ) , 'cov' , '1' , 'ID' , seq . id ] ) if seq . id in to_circularise : seq . id = new_id + ':' + new_id + ';' print ( seq , file = fout ) seq . revcomp ( ) seq . id = new_id + "':" + new_id + "';" print ( seq , file = fout ) else : seq . id = new_id + ';' print ( seq , file = fout ) seq . revcomp ( ) seq . id = new_id + "';" print ( seq , file = fout ) nodes += 1 utils . close ( fout )
5333	def config_logging ( debug ) : if debug : logging . basicConfig ( level = logging . DEBUG , format = '%(asctime)s %(message)s' ) logging . debug ( "Debug mode activated" ) else : logging . basicConfig ( level = logging . INFO , format = '%(asctime)s %(message)s' )
9893	def _uptime_plan9 ( ) : try : f = open ( '/dev/time' , 'r' ) s , ns , ct , cf = f . read ( ) . split ( ) f . close ( ) return float ( ct ) / float ( cf ) except ( IOError , ValueError ) : return None
13469	def error ( self , error_code , value , ** kwargs ) : code = self . error_code_map . get ( error_code , error_code ) try : message = Template ( self . error_messages [ code ] ) except KeyError : message = Template ( self . error_messages [ error_code ] ) placeholders = { "value" : self . hidden_value if self . hidden else value } placeholders . update ( kwargs ) placeholders . update ( self . message_values ) self . messages [ code ] = message . safe_substitute ( placeholders )
3989	def _nginx_location_spec ( port_spec , bridge_ip ) : location_string_spec = "\t \t location / { \n" for location_setting in [ 'proxy_http_version 1.1;' , 'proxy_set_header Upgrade $http_upgrade;' , 'proxy_set_header Connection "upgrade";' , 'proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;' , 'proxy_set_header Host $http_host;' , _nginx_proxy_string ( port_spec , bridge_ip ) ] : location_string_spec += "\t \t \t {} \n" . format ( location_setting ) location_string_spec += "\t \t } \n" return location_string_spec
2355	def find_elements ( self , strategy , locator ) : return self . driver_adapter . find_elements ( strategy , locator , root = self . root )
6570	def last_arg_decorator ( func ) : @ wraps ( func ) def decorator ( * args , ** kwargs ) : if signature_matches ( func , args , kwargs ) : return func ( * args , ** kwargs ) else : return lambda last : func ( * ( args + ( last , ) ) , ** kwargs ) return decorator
4490	def init ( args ) : config = config_from_file ( ) config_ = configparser . ConfigParser ( ) config_ . add_section ( 'osf' ) if 'username' not in config . keys ( ) : config_ . set ( 'osf' , 'username' , '' ) else : config_ . set ( 'osf' , 'username' , config [ 'username' ] ) if 'project' not in config . keys ( ) : config_ . set ( 'osf' , 'project' , '' ) else : config_ . set ( 'osf' , 'project' , config [ 'project' ] ) print ( 'Provide a username for the config file [current username: {}]:' . format ( config_ . get ( 'osf' , 'username' ) ) ) username = input ( ) if username : config_ . set ( 'osf' , 'username' , username ) print ( 'Provide a project for the config file [current project: {}]:' . format ( config_ . get ( 'osf' , 'project' ) ) ) project = input ( ) if project : config_ . set ( 'osf' , 'project' , project ) cfgfile = open ( ".osfcli.config" , "w" ) config_ . write ( cfgfile ) cfgfile . close ( )
2837	def transfer ( self , data , assert_ss = True , deassert_ss = True ) : if self . _mosi is None : raise RuntimeError ( 'Write attempted with no MOSI pin specified.' ) if self . _miso is None : raise RuntimeError ( 'Read attempted with no MISO pin specified.' ) if assert_ss and self . _ss is not None : self . _gpio . set_low ( self . _ss ) result = bytearray ( len ( data ) ) for i in range ( len ( data ) ) : for j in range ( 8 ) : if self . _write_shift ( data [ i ] , j ) & self . _mask : self . _gpio . set_high ( self . _mosi ) else : self . _gpio . set_low ( self . _mosi ) self . _gpio . output ( self . _sclk , not self . _clock_base ) if self . _read_leading : if self . _gpio . is_high ( self . _miso ) : result [ i ] |= self . _read_shift ( self . _mask , j ) else : result [ i ] &= ~ self . _read_shift ( self . _mask , j ) self . _gpio . output ( self . _sclk , self . _clock_base ) if not self . _read_leading : if self . _gpio . is_high ( self . _miso ) : result [ i ] |= self . _read_shift ( self . _mask , j ) else : result [ i ] &= ~ self . _read_shift ( self . _mask , j ) if deassert_ss and self . _ss is not None : self . _gpio . set_high ( self . _ss ) return result
8393	def parse_pylint_output ( pylint_output ) : for line in pylint_output : if not line . strip ( ) : continue if line [ 0 : 5 ] in ( "-" * 5 , "*" * 5 ) : continue parsed = PYLINT_PARSEABLE_REGEX . search ( line ) if parsed is None : LOG . warning ( u"Unable to parse %r. If this is a lint failure, please re-run pylint with the " u"--output-format=parseable option, otherwise, you can ignore this message." , line ) continue parsed_dict = parsed . groupdict ( ) parsed_dict [ 'linenum' ] = int ( parsed_dict [ 'linenum' ] ) yield PylintError ( ** parsed_dict )
8361	def create_rcontext ( self , size , frame ) : self . frame = frame width , height = size meta_surface = cairo . RecordingSurface ( cairo . CONTENT_COLOR_ALPHA , ( 0 , 0 , width , height ) ) ctx = cairo . Context ( meta_surface ) return ctx
12928	def _parse_info ( self , info_field ) : info = dict ( ) for item in info_field . split ( ';' ) : info_item_data = item . split ( '=' ) if len ( info_item_data ) == 1 : info [ info_item_data [ 0 ] ] = True elif len ( info_item_data ) == 2 : info [ info_item_data [ 0 ] ] = info_item_data [ 1 ] return info
5277	def query ( self , i , j ) : "Query the oracle to find out whether i and j should be must-linked" if self . queries_cnt < self . max_queries_cnt : self . queries_cnt += 1 return self . labels [ i ] == self . labels [ j ] else : raise MaximumQueriesExceeded
8201	def settings ( self , ** kwargs ) : for k , v in kwargs . items ( ) : setattr ( self , k , v )
10083	def edit ( self , pid = None ) : pid = pid or self . pid with db . session . begin_nested ( ) : before_record_update . send ( current_app . _get_current_object ( ) , record = self ) record_pid , record = self . fetch_published ( ) assert PIDStatus . REGISTERED == record_pid . status assert record [ '_deposit' ] == self [ '_deposit' ] self . model . json = self . _prepare_edit ( record ) flag_modified ( self . model , 'json' ) db . session . merge ( self . model ) after_record_update . send ( current_app . _get_current_object ( ) , record = self ) return self . __class__ ( self . model . json , model = self . model )
7682	def display ( annotation , meta = True , ** kwargs ) : for namespace , func in six . iteritems ( VIZ_MAPPING ) : try : ann = coerce_annotation ( annotation , namespace ) axes = func ( ann , ** kwargs ) axes . set_title ( annotation . namespace ) if meta : description = pprint_jobject ( annotation . annotation_metadata , indent = 2 ) anchored_box = AnchoredText ( description . strip ( '\n' ) , loc = 2 , frameon = True , bbox_to_anchor = ( 1.02 , 1.0 ) , bbox_transform = axes . transAxes , borderpad = 0.0 ) axes . add_artist ( anchored_box ) axes . figure . subplots_adjust ( right = 0.8 ) return axes except NamespaceError : pass raise NamespaceError ( 'Unable to visualize annotation of namespace="{:s}"' . format ( annotation . namespace ) )
5803	def extract_chain ( server_handshake_bytes ) : output = [ ] chain_bytes = None for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type == b'\x0b' : chain_bytes = message_data break if chain_bytes : break if chain_bytes : pointer = 3 while pointer < len ( chain_bytes ) : cert_length = int_from_bytes ( chain_bytes [ pointer : pointer + 3 ] ) cert_start = pointer + 3 cert_end = cert_start + cert_length pointer = cert_end cert_bytes = chain_bytes [ cert_start : cert_end ] output . append ( Certificate . load ( cert_bytes ) ) return output
5332	def get_panels ( config ) : task = TaskPanels ( config ) task . execute ( ) task = TaskPanelsMenu ( config ) task . execute ( ) logging . info ( "Panels creation finished!" )
6329	def gng_importer ( self , corpus_file ) : with c_open ( corpus_file , 'r' , encoding = 'utf-8' ) as gng : for line in gng : line = line . rstrip ( ) . split ( '\t' ) words = line [ 0 ] . split ( ) self . _add_to_ngcorpus ( self . ngcorpus , words , int ( line [ 2 ] ) )
7021	def parallel_gen_binnedlc_pkls ( binnedpkldir , textlcdir , timebinsec , binnedpklglob = '*binned*sec*.pkl' , textlcglob = '*.tfalc.TF1*' ) : binnedpkls = sorted ( glob . glob ( os . path . join ( binnedpkldir , binnedpklglob ) ) ) textlcs = [ ] for bpkl in binnedpkls : objectid = HATIDREGEX . findall ( bpkl ) if objectid is not None : objectid = objectid [ 0 ] searchpath = os . path . join ( textlcdir , '%s-%s' % ( objectid , textlcglob ) ) textlcf = glob . glob ( searchpath ) if textlcf : textlcs . append ( textlcf ) else : textlcs . append ( None )
7849	def has_feature ( self , var ) : if not var : raise ValueError ( "var is None" ) if '"' not in var : expr = u'd:feature[@var="%s"]' % ( var , ) elif "'" not in var : expr = u"d:feature[@var='%s']" % ( var , ) else : raise ValueError ( "Invalid feature name" ) l = self . xpath_ctxt . xpathEval ( to_utf8 ( expr ) ) if l : return True else : return False
12988	def remote_jupyter_proxy_url ( port ) : base_url = os . environ [ 'EXTERNAL_URL' ] host = urllib . parse . urlparse ( base_url ) . netloc if port is None : return host service_url_path = os . environ [ 'JUPYTERHUB_SERVICE_PREFIX' ] proxy_url_path = 'proxy/%d' % port user_url = urllib . parse . urljoin ( base_url , service_url_path ) full_url = urllib . parse . urljoin ( user_url , proxy_url_path ) return full_url
8238	def analogous ( clr , angle = 10 , contrast = 0.25 ) : contrast = max ( 0 , min ( contrast , 1.0 ) ) clr = color ( clr ) colors = colorlist ( clr ) for i , j in [ ( 1 , 2.2 ) , ( 2 , 1 ) , ( - 1 , - 0.5 ) , ( - 2 , 1 ) ] : c = clr . rotate_ryb ( angle * i ) t = 0.44 - j * 0.1 if clr . brightness - contrast * j < t : c . brightness = t else : c . brightness = clr . brightness - contrast * j c . saturation -= 0.05 colors . append ( c ) return colors
11674	def copy ( self , stack = False , copy_meta = False , memo = None ) : if self . stacked : fs = deepcopy ( self . stacked_features , memo ) n_pts = self . n_pts . copy ( ) elif stack : fs = np . vstack ( self . features ) n_pts = self . n_pts . copy ( ) else : fs = deepcopy ( self . features , memo ) n_pts = None meta = deepcopy ( self . meta , memo ) if copy_meta else self . meta return Features ( fs , n_pts , copy = False , ** meta )
3148	def all ( self , workflow_id ) : self . workflow_id = workflow_id return self . _mc_client . _get ( url = self . _build_path ( workflow_id , 'removed-subscribers' ) )
12447	def render_to_string ( self ) : values = '' for key , value in self . items ( ) : values += '{}={};' . format ( key , value ) return values
6679	def getmtime ( self , path , use_sudo = False ) : func = use_sudo and run_as_root or self . run with self . settings ( hide ( 'running' , 'stdout' ) ) : return int ( func ( 'stat -c %%Y "%(path)s" ' % locals ( ) ) . strip ( ) )
4879	def get_paginated_response ( data , request ) : url = urlparse ( request . build_absolute_uri ( ) ) . _replace ( query = None ) . geturl ( ) next_page = None previous_page = None if data [ 'next' ] : next_page = "{base_url}?{query_parameters}" . format ( base_url = url , query_parameters = urlparse ( data [ 'next' ] ) . query , ) next_page = next_page . rstrip ( '?' ) if data [ 'previous' ] : previous_page = "{base_url}?{query_parameters}" . format ( base_url = url , query_parameters = urlparse ( data [ 'previous' ] or "" ) . query , ) previous_page = previous_page . rstrip ( '?' ) return Response ( OrderedDict ( [ ( 'count' , data [ 'count' ] ) , ( 'next' , next_page ) , ( 'previous' , previous_page ) , ( 'results' , data [ 'results' ] ) ] ) )
875	def copyVarStatesFrom ( self , particleState , varNames ) : allowedToMove = True for varName in particleState [ 'varStates' ] : if varName in varNames : if varName not in self . permuteVars : continue state = copy . deepcopy ( particleState [ 'varStates' ] [ varName ] ) state [ '_position' ] = state [ 'position' ] state [ 'bestPosition' ] = state [ 'position' ] if not allowedToMove : state [ 'velocity' ] = 0 self . permuteVars [ varName ] . setState ( state ) if allowedToMove : self . permuteVars [ varName ] . resetVelocity ( self . _rng )
930	def _createAggregateRecord ( self ) : record = [ ] for i , ( fieldIdx , aggFP , paramIdx ) in enumerate ( self . _fields ) : if aggFP is None : continue values = self . _slice [ i ] refIndex = None if paramIdx is not None : record . append ( aggFP ( values , self . _slice [ paramIdx ] ) ) else : record . append ( aggFP ( values ) ) return record
12550	def write_meta_header ( filename , meta_dict ) : header = '' for tag in MHD_TAGS : if tag in meta_dict . keys ( ) : header += '{} = {}\n' . format ( tag , meta_dict [ tag ] ) with open ( filename , 'w' ) as f : f . write ( header )
11890	def set_brightness ( self , brightness ) : command = "C {},,,,{},\r\n" . format ( self . _zid , brightness ) response = self . _hub . send_command ( command ) _LOGGER . debug ( "Set brightness %s: %s" , repr ( command ) , response ) return response
7667	def search ( self , ** kwargs ) : results = AnnotationArray ( ) for annotation in self : if annotation . search ( ** kwargs ) : results . append ( annotation ) return results
1556	def get_out_streamids ( self ) : if self . outputs is None : return set ( ) if not isinstance ( self . outputs , ( list , tuple ) ) : raise TypeError ( "Argument to outputs must be either list or tuple, given: %s" % str ( type ( self . outputs ) ) ) ret_lst = [ ] for output in self . outputs : if not isinstance ( output , ( str , Stream ) ) : raise TypeError ( "Outputs must be a list of strings or Streams, given: %s" % str ( output ) ) ret_lst . append ( Stream . DEFAULT_STREAM_ID if isinstance ( output , str ) else output . stream_id ) return set ( ret_lst )
11400	def update_collaboration ( self ) : for field in record_get_field_instances ( self . record , '710' ) : subs = field_get_subfield_instances ( field ) for idx , ( key , value ) in enumerate ( subs [ : ] ) : if key == '5' : subs . pop ( idx ) elif value . startswith ( 'CERN. Geneva' ) : subs . pop ( idx ) if len ( subs ) == 0 : record_delete_field ( self . record , tag = '710' , field_position_global = field [ 4 ] )
7424	def check_insert_size ( data , sample ) : cmd1 = [ ipyrad . bins . samtools , "stats" , sample . files . mapped_reads ] cmd2 = [ "grep" , "SN" ] proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = sps . PIPE ) proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = sps . PIPE , stdin = proc1 . stdout ) res = proc2 . communicate ( ) [ 0 ] if proc2 . returncode : raise IPyradWarningExit ( "error in %s: %s" , cmd2 , res ) avg_insert = 0 stdv_insert = 0 avg_len = 0 for line in res . split ( "\n" ) : if "insert size average" in line : avg_insert = float ( line . split ( ":" ) [ - 1 ] . strip ( ) ) elif "insert size standard deviation" in line : stdv_insert = float ( line . split ( ":" ) [ - 1 ] . strip ( ) ) + 0.1 elif "average length" in line : avg_len = float ( line . split ( ":" ) [ - 1 ] . strip ( ) ) LOGGER . debug ( "avg {} stdv {} avg_len {}" . format ( avg_insert , stdv_insert , avg_len ) ) if all ( [ avg_insert , stdv_insert , avg_len ] ) : if stdv_insert < 5 : stdv_insert = 5. if ( 2 * avg_len ) < avg_insert : hack = avg_insert + ( 3 * np . math . ceil ( stdv_insert ) ) - ( 2 * avg_len ) else : hack = ( avg_insert - avg_len ) + ( 3 * np . math . ceil ( stdv_insert ) ) LOGGER . info ( "stdv: hacked insert size is %s" , hack ) data . _hackersonly [ "max_inner_mate_distance" ] = int ( np . math . ceil ( hack ) ) else : data . _hackersonly [ "max_inner_mate_distance" ] = 300 LOGGER . debug ( "inner mate distance for {} - {}" . format ( sample . name , data . _hackersonly [ "max_inner_mate_distance" ] ) )
6228	def init ( window = None , project = None , timeline = None ) : from demosys . effects . registry import Effect from demosys . scene import camera window . timeline = timeline setattr ( Effect , '_window' , window ) setattr ( Effect , '_ctx' , window . ctx ) setattr ( Effect , '_project' , project ) window . sys_camera = camera . SystemCamera ( aspect = window . aspect_ratio , fov = 60.0 , near = 1 , far = 1000 ) setattr ( Effect , '_sys_camera' , window . sys_camera ) print ( "Loading started at" , time . time ( ) ) project . load ( ) timer_cls = import_string ( settings . TIMER ) window . timer = timer_cls ( ) window . timer . start ( )
5796	def handle_sec_error ( error , exception_class = None ) : if error == 0 : return if error in set ( [ SecurityConst . errSSLClosedNoNotify , SecurityConst . errSSLClosedAbort ] ) : raise TLSDisconnectError ( 'The remote end closed the connection' ) if error == SecurityConst . errSSLClosedGraceful : raise TLSGracefulDisconnectError ( 'The remote end closed the connection' ) cf_error_string = Security . SecCopyErrorMessageString ( error , null ( ) ) output = CFHelpers . cf_string_to_unicode ( cf_error_string ) CoreFoundation . CFRelease ( cf_error_string ) if output is None or output == '' : output = 'OSStatus %s' % error if exception_class is None : exception_class = OSError raise exception_class ( output )
4836	def get_paginated_catalogs ( self , querystring = None ) : return self . _load_data ( self . CATALOGS_ENDPOINT , default = [ ] , querystring = querystring , traverse_pagination = False , many = False )
1082	def replace ( self , hour = None , minute = None , second = None , microsecond = None , tzinfo = True ) : if hour is None : hour = self . hour if minute is None : minute = self . minute if second is None : second = self . second if microsecond is None : microsecond = self . microsecond if tzinfo is True : tzinfo = self . tzinfo return time . __new__ ( type ( self ) , hour , minute , second , microsecond , tzinfo )
4441	async def _find ( self , ctx , * , query ) : if not query . startswith ( 'ytsearch:' ) and not query . startswith ( 'scsearch:' ) : query = 'ytsearch:' + query results = await self . bot . lavalink . get_tracks ( query ) if not results or not results [ 'tracks' ] : return await ctx . send ( 'Nothing found' ) tracks = results [ 'tracks' ] [ : 10 ] o = '' for index , track in enumerate ( tracks , start = 1 ) : track_title = track [ "info" ] [ "title" ] track_uri = track [ "info" ] [ "uri" ] o += f'`{index}.` [{track_title}]({track_uri})\n' embed = discord . Embed ( color = discord . Color . blurple ( ) , description = o ) await ctx . send ( embed = embed )
4058	def _bib_processor ( self , retrieved ) : items = [ ] for bib in retrieved . entries : items . append ( bib [ "content" ] [ 0 ] [ "value" ] ) self . url_params = None return items
10699	def set ( self , key , value ) : if self . in_memory : self . _memory_db [ key ] = value else : db = self . _read_file ( ) db [ key ] = value with open ( self . db_path , 'w' ) as f : f . write ( json . dumps ( db , ensure_ascii = False , indent = 2 ) )
1375	def parse_override_config ( namespace ) : overrides = dict ( ) for config in namespace : kv = config . split ( "=" ) if len ( kv ) != 2 : raise Exception ( "Invalid config property format (%s) expected key=value" % config ) if kv [ 1 ] in [ 'true' , 'True' , 'TRUE' ] : overrides [ kv [ 0 ] ] = True elif kv [ 1 ] in [ 'false' , 'False' , 'FALSE' ] : overrides [ kv [ 0 ] ] = False else : overrides [ kv [ 0 ] ] = kv [ 1 ] return overrides
8596	def update_group ( self , group_id , ** kwargs ) : properties = { } if 'create_datacenter' in kwargs : kwargs [ 'create_data_center' ] = kwargs . pop ( 'create_datacenter' ) for attr , value in kwargs . items ( ) : properties [ self . _underscore_to_camelcase ( attr ) ] = value data = { "properties" : properties } response = self . _perform_request ( url = '/um/groups/%s' % group_id , method = 'PUT' , data = json . dumps ( data ) ) return response
11154	def auto_complete_choices ( self , case_sensitive = False ) : self_basename = self . basename self_basename_lower = self . basename . lower ( ) if case_sensitive : def match ( basename ) : return basename . startswith ( self_basename ) else : def match ( basename ) : return basename . lower ( ) . startswith ( self_basename_lower ) choices = list ( ) if self . is_dir ( ) : choices . append ( self ) for p in self . sort_by_abspath ( self . select ( recursive = False ) ) : choices . append ( p ) else : p_parent = self . parent if p_parent . is_dir ( ) : for p in self . sort_by_abspath ( p_parent . select ( recursive = False ) ) : if match ( p . basename ) : choices . append ( p ) else : raise ValueError ( "'%s' directory does not exist!" % p_parent ) return choices
7815	def run ( self ) : if self . args . roster_cache and os . path . exists ( self . args . roster_cache ) : logging . info ( u"Loading roster from {0!r}" . format ( self . args . roster_cache ) ) try : self . client . roster_client . load_roster ( self . args . roster_cache ) except ( IOError , ValueError ) , err : logging . error ( u"Could not load the roster: {0!r}" . format ( err ) ) self . client . connect ( ) self . client . run ( )
13012	def pprint ( arr , columns = ( 'temperature' , 'luminosity' ) , names = ( 'Temperature (Kelvin)' , 'Luminosity (solar units)' ) , max_rows = 32 , precision = 2 ) : if max_rows is True : pd . set_option ( 'display.max_rows' , 1000 ) elif type ( max_rows ) is int : pd . set_option ( 'display.max_rows' , max_rows ) pd . set_option ( 'precision' , precision ) df = pd . DataFrame ( arr . flatten ( ) , index = arr [ 'id' ] . flatten ( ) , columns = columns ) df . columns = names return df . style . format ( { names [ 0 ] : '{:.0f}' , names [ 1 ] : '{:.2f}' } )
11243	def add_newlines ( f , output , char ) : line_count = get_line_count ( f ) f = open ( f , 'r+' ) output = open ( output , 'r+' ) for line in range ( line_count ) : string = f . readline ( ) string = re . sub ( char , char + '\n' , string ) output . write ( string )
6856	def query ( query , use_sudo = True , ** kwargs ) : func = use_sudo and run_as_root or run user = kwargs . get ( 'mysql_user' ) or env . get ( 'mysql_user' ) password = kwargs . get ( 'mysql_password' ) or env . get ( 'mysql_password' ) options = [ '--batch' , '--raw' , '--skip-column-names' , ] if user : options . append ( '--user=%s' % quote ( user ) ) if password : options . append ( '--password=%s' % quote ( password ) ) options = ' ' . join ( options ) return func ( 'mysql %(options)s --execute=%(query)s' % { 'options' : options , 'query' : quote ( query ) , } )
5570	def profile ( self ) : with rasterio . open ( self . path , "r" ) as src : return deepcopy ( src . meta )
12809	def received ( self , messages ) : if messages : if self . _queue : self . _queue . put_nowait ( messages ) if self . _callback : self . _callback ( messages )
3713	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : Vms = [ i ( T , P ) for i in self . VolumeGases ] return mixing_simple ( zs , Vms ) elif method == IDEAL : return ideal_gas ( T , P ) elif method == EOS : self . eos [ 0 ] = self . eos [ 0 ] . to_TP_zs ( T = T , P = P , zs = zs ) return self . eos [ 0 ] . V_g else : raise Exception ( 'Method not valid' )
3744	def _round_whole_even ( i ) : r if i % .5 == 0 : if ( i + 0.5 ) % 2 == 0 : i = i + 0.5 else : i = i - 0.5 else : i = round ( i , 0 ) return int ( i )
13071	def r_first_passage ( self , objectId ) : collection , reffs = self . get_reffs ( objectId = objectId , export_collection = True ) first , _ = reffs [ 0 ] return redirect ( url_for ( ".r_passage_semantic" , objectId = objectId , subreference = first , semantic = self . semantic ( collection ) ) )
6448	def dist ( self , src , tar ) : if src == tar : return 0.0 src = src . encode ( 'utf-8' ) tar = tar . encode ( 'utf-8' ) self . _compressor . compress ( src ) src_comp = self . _compressor . flush ( zlib . Z_FULL_FLUSH ) self . _compressor . compress ( tar ) tar_comp = self . _compressor . flush ( zlib . Z_FULL_FLUSH ) self . _compressor . compress ( src + tar ) concat_comp = self . _compressor . flush ( zlib . Z_FULL_FLUSH ) self . _compressor . compress ( tar + src ) concat_comp2 = self . _compressor . flush ( zlib . Z_FULL_FLUSH ) return ( min ( len ( concat_comp ) , len ( concat_comp2 ) ) - min ( len ( src_comp ) , len ( tar_comp ) ) ) / max ( len ( src_comp ) , len ( tar_comp ) )
2611	def serialize_object ( obj , buffer_threshold = MAX_BYTES , item_threshold = MAX_ITEMS ) : buffers = [ ] if istype ( obj , sequence_types ) and len ( obj ) < item_threshold : cobj = can_sequence ( obj ) for c in cobj : buffers . extend ( _extract_buffers ( c , buffer_threshold ) ) elif istype ( obj , dict ) and len ( obj ) < item_threshold : cobj = { } for k in sorted ( obj ) : c = can ( obj [ k ] ) buffers . extend ( _extract_buffers ( c , buffer_threshold ) ) cobj [ k ] = c else : cobj = can ( obj ) buffers . extend ( _extract_buffers ( cobj , buffer_threshold ) ) buffers . insert ( 0 , pickle . dumps ( cobj , PICKLE_PROTOCOL ) ) return buffers
9736	def get_3d_markers_residual ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPositionResidual , component_info , data , component_position )
9570	async def _get_response ( self , message ) : view = self . discovery_view ( message ) if not view : return if inspect . iscoroutinefunction ( view ) : response = await view ( message ) else : response = view ( message ) return self . prepare_response ( response , message )
11780	def ContinuousXor ( n ) : "2 inputs are chosen uniformly from (0.0 .. 2.0]; output is xor of ints." examples = [ ] for i in range ( n ) : x , y = [ random . uniform ( 0.0 , 2.0 ) for i in '12' ] examples . append ( [ x , y , int ( x ) != int ( y ) ] ) return DataSet ( name = "continuous xor" , examples = examples )
2323	def forward ( self , pred , target ) : loss = th . FloatTensor ( [ 0 ] ) for i in range ( 1 , self . moments ) : mk_pred = th . mean ( th . pow ( pred , i ) , 0 ) mk_tar = th . mean ( th . pow ( target , i ) , 0 ) loss . add_ ( th . mean ( ( mk_pred - mk_tar ) ** 2 ) ) return loss
8978	def _binary_file ( self , file ) : if self . __text_is_expected : file = TextWrapper ( file , self . __encoding ) self . __dump_to_file ( file )
6843	def set_permissions ( self ) : r = self . local_renderer for path in r . env . paths_owned : r . env . path_owned = path r . sudo ( 'chown {celery_daemon_user}:{celery_daemon_user} {celery_path_owned}' )
2977	def cmd_kill ( opts ) : kill_signal = opts . signal if hasattr ( opts , 'signal' ) else "SIGKILL" __with_containers ( opts , Blockade . kill , signal = kill_signal )
6973	def epd_magseries ( times , mags , errs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd , magsarefluxes = False , epdsmooth_sigclip = 3.0 , epdsmooth_windowsize = 21 , epdsmooth_func = smooth_magseries_savgol , epdsmooth_extraparams = None ) : finind = np . isfinite ( times ) & np . isfinite ( mags ) & np . isfinite ( errs ) ftimes , fmags , ferrs = times [ : : ] [ finind ] , mags [ : : ] [ finind ] , errs [ : : ] [ finind ] ffsv , ffdv , ffkv , fxcc , fycc , fbgv , fbge , fiha , fizd = ( fsv [ : : ] [ finind ] , fdv [ : : ] [ finind ] , fkv [ : : ] [ finind ] , xcc [ : : ] [ finind ] , ycc [ : : ] [ finind ] , bgv [ : : ] [ finind ] , bge [ : : ] [ finind ] , iha [ : : ] [ finind ] , izd [ : : ] [ finind ] , ) stimes , smags , serrs , separams = sigclip_magseries_with_extparams ( times , mags , errs , [ fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ] , sigclip = epdsmooth_sigclip , magsarefluxes = magsarefluxes ) sfsv , sfdv , sfkv , sxcc , sycc , sbgv , sbge , siha , sizd = separams if isinstance ( epdsmooth_extraparams , dict ) : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize , ** epdsmooth_extraparams ) else : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize ) initcoeffs = np . zeros ( 22 ) leastsqfit = leastsq ( _epd_residual , initcoeffs , args = ( smoothedmags , sfsv , sfdv , sfkv , sxcc , sycc , sbgv , sbge , siha , sizd ) , full_output = True ) if leastsqfit [ - 1 ] in ( 1 , 2 , 3 , 4 ) : fitcoeffs = leastsqfit [ 0 ] epdfit = _epd_function ( fitcoeffs , ffsv , ffdv , ffkv , fxcc , fycc , fbgv , fbge , fiha , fizd ) epdmags = npmedian ( fmags ) + fmags - epdfit retdict = { 'times' : ftimes , 'mags' : epdmags , 'errs' : ferrs , 'fitcoeffs' : fitcoeffs , 'fitinfo' : leastsqfit , 'fitmags' : epdfit , 'mags_median' : npmedian ( epdmags ) , 'mags_mad' : npmedian ( npabs ( epdmags - npmedian ( epdmags ) ) ) } return retdict else : LOGERROR ( 'EPD fit did not converge' ) return None
7087	def get_epochs_given_midtimes_and_period ( t_mid , period , err_t_mid = None , t0_fixed = None , t0_percentile = None , verbose = False ) : kwargarr = np . array ( [ isinstance ( err_t_mid , np . ndarray ) , t0_fixed , t0_percentile ] ) if not _single_true ( kwargarr ) and not np . all ( ~ kwargarr . astype ( bool ) ) : raise AssertionError ( 'can have at most one of err_t_mid, t0_fixed, t0_percentile' ) t_mid = t_mid [ np . isfinite ( t_mid ) ] N_midtimes = len ( t_mid ) if t0_fixed : t0 = t0_fixed elif isinstance ( err_t_mid , np . ndarray ) : t0_avg = np . average ( t_mid , weights = 1 / err_t_mid ** 2 ) t0_options = np . arange ( min ( t_mid ) , max ( t_mid ) + period , period ) t0 = t0_options [ np . argmin ( np . abs ( t0_options - t0_avg ) ) ] else : if not t0_percentile : if N_midtimes % 2 == 1 : t0 = np . median ( t_mid ) else : t0 = t_mid [ int ( N_midtimes / 2 ) ] else : t0 = np . sort ( t_mid ) [ int ( N_midtimes * t0_percentile / 100 ) ] epoch = ( t_mid - t0 ) / period int_epoch = np . round ( epoch , 0 ) if verbose : LOGINFO ( 'epochs before rounding' ) LOGINFO ( '\n{:s}' . format ( repr ( epoch ) ) ) LOGINFO ( 'epochs after rounding' ) LOGINFO ( '\n{:s}' . format ( repr ( int_epoch ) ) ) return int_epoch , t0
7447	def _step6func ( self , samples , noreverse , force , randomseed , ipyclient , ** kwargs ) : samples = _get_samples ( self , samples ) csamples = self . _samples_precheck ( samples , 6 , force ) if self . _headers : print ( "\n Step 6: Clustering at {} similarity across {} samples" . format ( self . paramsdict [ "clust_threshold" ] , len ( csamples ) ) ) if not csamples : raise IPyradError ( FIRST_RUN_5 ) elif not force : if all ( [ i . stats . state >= 6 for i in csamples ] ) : print ( DATABASE_EXISTS . format ( len ( samples ) ) ) return assemble . cluster_across . run ( self , csamples , noreverse , force , randomseed , ipyclient , ** kwargs )
12374	def take_snapshot ( droplet , name ) : print "powering off" droplet . power_off ( ) droplet . wait ( ) print "taking snapshot" droplet . take_snapshot ( name ) droplet . wait ( ) snapshots = droplet . snapshots ( ) print "Current snapshots" print snapshots
7214	def preview ( image , ** kwargs ) : try : from IPython . display import Javascript , HTML , display from gbdxtools . rda . interface import RDA from gbdxtools import Interface gbdx = Interface ( ) except : print ( "IPython is required to produce maps." ) return zoom = kwargs . get ( "zoom" , 16 ) bands = kwargs . get ( "bands" ) if bands is None : bands = image . _rgb_bands wgs84_bounds = kwargs . get ( "bounds" , list ( loads ( image . metadata [ "image" ] [ "imageBoundsWGS84" ] ) . bounds ) ) center = kwargs . get ( "center" , list ( shape ( image ) . centroid . bounds [ 0 : 2 ] ) ) if image . proj != 'EPSG:4326' : code = image . proj . split ( ':' ) [ 1 ] conn = gbdx . gbdx_connection proj_info = conn . get ( 'https://ughlicoordinates.geobigdata.io/ughli/v1/projinfo/{}' . format ( code ) ) . json ( ) tfm = partial ( pyproj . transform , pyproj . Proj ( init = 'EPSG:4326' ) , pyproj . Proj ( init = image . proj ) ) bounds = list ( ops . transform ( tfm , box ( * wgs84_bounds ) ) . bounds ) else : proj_info = { } bounds = wgs84_bounds if not image . options . get ( 'dra' ) : rda = RDA ( ) dra = rda . HistogramDRA ( image ) image = dra . aoi ( bbox = image . bounds ) graph_id = image . rda_id node_id = image . rda . graph ( ) [ 'nodes' ] [ 0 ] [ 'id' ] map_id = "map_{}" . format ( str ( int ( time . time ( ) ) ) ) scales = ',' . join ( [ '1' ] * len ( bands ) ) offsets = ',' . join ( [ '0' ] * len ( bands ) ) display ( HTML ( Template ( ) . substitute ( { "map_id" : map_id } ) ) ) js = Template ( ) . substitute ( { "map_id" : map_id , "proj" : image . proj , "projInfo" : json . dumps ( proj_info ) , "graphId" : graph_id , "bounds" : bounds , "bands" : "," . join ( map ( str , bands ) ) , "nodeId" : node_id , "md" : json . dumps ( image . metadata [ "image" ] ) , "georef" : json . dumps ( image . metadata [ "georef" ] ) , "center" : center , "zoom" : zoom , "token" : gbdx . gbdx_connection . access_token , "scales" : scales , "offsets" : offsets , "url" : VIRTUAL_RDA_URL } ) display ( Javascript ( js ) )
4027	def create_local_copy ( cookie_file ) : if isinstance ( cookie_file , list ) : cookie_file = cookie_file [ 0 ] if os . path . exists ( cookie_file ) : tmp_cookie_file = tempfile . NamedTemporaryFile ( suffix = '.sqlite' ) . name open ( tmp_cookie_file , 'wb' ) . write ( open ( cookie_file , 'rb' ) . read ( ) ) return tmp_cookie_file else : raise BrowserCookieError ( 'Can not find cookie file at: ' + cookie_file )
8830	def segment_allocation_find ( context , lock_mode = False , ** filters ) : range_ids = filters . pop ( "segment_allocation_range_ids" , None ) query = context . session . query ( models . SegmentAllocation ) if lock_mode : query = query . with_lockmode ( "update" ) query = query . filter_by ( ** filters ) if range_ids : query . filter ( models . SegmentAllocation . segment_allocation_range_id . in_ ( range_ids ) ) return query
2210	def parse_requirements ( fname = 'requirements.txt' ) : from os . path import dirname , join , exists import re require_fpath = join ( dirname ( __file__ ) , fname ) def parse_line ( line ) : info = { } if line . startswith ( '-e ' ) : info [ 'package' ] = line . split ( '#egg=' ) [ 1 ] else : pat = '(' + '|' . join ( [ '>=' , '==' , '>' ] ) + ')' parts = re . split ( pat , line , maxsplit = 1 ) parts = [ p . strip ( ) for p in parts ] info [ 'package' ] = parts [ 0 ] if len ( parts ) > 1 : op , rest = parts [ 1 : ] if ';' in rest : version , platform_deps = map ( str . strip , rest . split ( ';' ) ) info [ 'platform_deps' ] = platform_deps else : version = rest info [ 'version' ] = ( op , version ) return info if exists ( require_fpath ) : with open ( require_fpath , 'r' ) as f : packages = [ ] for line in f . readlines ( ) : line = line . strip ( ) if line and not line . startswith ( '#' ) : info = parse_line ( line ) package = info [ 'package' ] if not sys . version . startswith ( '3.4' ) : platform_deps = info . get ( 'platform_deps' ) if platform_deps is not None : package += ';' + platform_deps packages . append ( package ) return packages return [ ]
5479	def _cancel_batch ( batch_fn , cancel_fn , ops ) : canceled = [ ] failed = [ ] def handle_cancel_response ( request_id , response , exception ) : del response if exception : msg = 'error %s: %s' % ( exception . resp . status , exception . resp . reason ) if exception . resp . status == FAILED_PRECONDITION_CODE : detail = json . loads ( exception . content ) status = detail . get ( 'error' , { } ) . get ( 'status' ) if status == FAILED_PRECONDITION_STATUS : msg = 'Not running' failed . append ( { 'name' : request_id , 'msg' : msg } ) else : canceled . append ( { 'name' : request_id } ) return batch = batch_fn ( callback = handle_cancel_response ) ops_by_name = { } for op in ops : op_name = op . get_field ( 'internal-id' ) ops_by_name [ op_name ] = op batch . add ( cancel_fn ( name = op_name , body = { } ) , request_id = op_name ) batch . execute ( ) canceled_ops = [ ops_by_name [ op [ 'name' ] ] for op in canceled ] error_messages = [ ] for fail in failed : op = ops_by_name [ fail [ 'name' ] ] error_messages . append ( "Error canceling '%s': %s" % ( get_operation_full_job_id ( op ) , fail [ 'msg' ] ) ) return canceled_ops , error_messages
8735	def date_range ( start = None , stop = None , step = None ) : if step is None : step = datetime . timedelta ( days = 1 ) if start is None : start = datetime . datetime . now ( ) while start < stop : yield start start += step
3265	def build_url ( base , seg , query = None ) : def clean_segment ( segment ) : segment = segment . strip ( '/' ) if isinstance ( segment , basestring ) : segment = segment . encode ( 'utf-8' ) return segment seg = ( quote ( clean_segment ( s ) ) for s in seg ) if query is None or len ( query ) == 0 : query_string = '' else : query_string = "?" + urlencode ( query ) path = '/' . join ( seg ) + query_string adjusted_base = base . rstrip ( '/' ) + '/' return urljoin ( str ( adjusted_base ) , str ( path ) )
7454	def estimate_optim ( data , testfile , ipyclient ) : insize = os . path . getsize ( testfile ) tmp_file_name = os . path . join ( data . paramsdict [ "project_dir" ] , "tmp-step1-count.fq" ) if testfile . endswith ( ".gz" ) : infile = gzip . open ( testfile ) outfile = gzip . open ( tmp_file_name , 'wb' , compresslevel = 5 ) else : infile = open ( testfile ) outfile = open ( tmp_file_name , 'w' ) outfile . write ( "" . join ( itertools . islice ( infile , 40000 ) ) ) outfile . close ( ) infile . close ( ) tmp_size = os . path . getsize ( tmp_file_name ) inputreads = int ( insize / tmp_size ) * 10000 os . remove ( tmp_file_name ) return inputreads
1812	def SETNB ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . CF == False , 1 , 0 ) )
6196	def compact_name ( self , hashsize = 6 ) : s = self . compact_name_core ( hashsize , t_max = True ) s += "_ID%d-%d" % ( self . ID , self . EID ) return s
2154	def _read ( self , fp , fpname ) : if os . path . isfile ( fpname ) : file_permission = os . stat ( fpname ) if fpname != os . path . join ( tower_dir , 'tower_cli.cfg' ) and ( ( file_permission . st_mode & stat . S_IRGRP ) or ( file_permission . st_mode & stat . S_IROTH ) ) : warnings . warn ( 'File {0} readable by group or others.' . format ( fpname ) , RuntimeWarning ) try : return configparser . ConfigParser . _read ( self , fp , fpname ) except configparser . MissingSectionHeaderError : fp . seek ( 0 ) string = '[general]\n%s' % fp . read ( ) flo = StringIO ( string ) return configparser . ConfigParser . _read ( self , flo , fpname )
5706	def clean ( self ) : cleaned_data = super ( AuthForm , self ) . clean ( ) user = self . get_user ( ) if self . staff_only and ( not user or not user . is_staff ) : raise forms . ValidationError ( 'Sorry, only staff are allowed.' ) if self . superusers_only and ( not user or not user . is_superuser ) : raise forms . ValidationError ( 'Sorry, only superusers are allowed.' ) return cleaned_data
6042	def sparse_to_unmasked_sparse ( self ) : return mapping_util . sparse_to_unmasked_sparse_from_mask_and_pixel_centres ( total_sparse_pixels = self . total_sparse_pixels , mask = self . regular_grid . mask , unmasked_sparse_grid_pixel_centres = self . unmasked_sparse_grid_pixel_centres ) . astype ( 'int' )
6524	def get_issues ( self , sortby = None ) : self . _ensure_cleaned_issues ( ) return self . _sort_issues ( self . _cleaned_issues , sortby )
5067	def get_cache_key ( ** kwargs ) : key = '__' . join ( [ '{}:{}' . format ( item , value ) for item , value in iteritems ( kwargs ) ] ) return hashlib . md5 ( key . encode ( 'utf-8' ) ) . hexdigest ( )
10493	def clickMouseButtonLeft ( self , coord , interval = None ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags ) if interval : self . _postQueuedEvents ( interval = interval ) else : self . _postQueuedEvents ( )
722	def getData ( self , n ) : records = [ self . getNext ( ) for x in range ( n ) ] return records
9494	def compile ( code : list , consts : list , names : list , varnames : list , func_name : str = "<unknown, compiled>" , arg_count : int = 0 , kwarg_defaults : Tuple [ Any ] = ( ) , use_safety_wrapper : bool = True ) : varnames = tuple ( varnames ) consts = tuple ( consts ) names = tuple ( names ) code = util . flatten ( code ) if arg_count > len ( varnames ) : raise CompileError ( "arg_count > len(varnames)" ) if len ( kwarg_defaults ) > len ( varnames ) : raise CompileError ( "len(kwarg_defaults) > len(varnames)" ) bc = compile_bytecode ( code ) dis . dis ( bc ) if PY36 : pass else : if bc [ - 1 ] != tokens . RETURN_VALUE : raise CompileError ( "No default RETURN_VALUE. Add a `pyte.tokens.RETURN_VALUE` to the end of your " "bytecode if you don't need one." ) flags = 1 | 2 | 64 frame_data = inspect . stack ( ) [ 1 ] if sys . version_info [ 0 : 2 ] > ( 3 , 3 ) : stack_size = _simulate_stack ( dis . _get_instructions_bytes ( bc , constants = consts , names = names , varnames = varnames ) ) else : warnings . warn ( "Cannot check stack for safety." ) stack_size = 99 _optimize_warn_pass ( dis . _get_instructions_bytes ( bc , constants = consts , names = names , varnames = varnames ) ) obb = types . CodeType ( arg_count , 0 , len ( varnames ) , stack_size , flags , bc , consts , names , varnames , frame_data [ 1 ] , func_name , frame_data [ 2 ] , b'' , ( ) , ( ) ) f_globals = frame_data [ 0 ] . f_globals f = types . FunctionType ( obb , f_globals ) f . __name__ = func_name f . __defaults__ = kwarg_defaults if use_safety_wrapper : def __safety_wrapper ( * args , ** kwargs ) : try : return f ( * args , ** kwargs ) except SystemError as e : if 'opcode' not in ' ' . join ( e . args ) : raise msg = "Bytecode exception!" "\nFunction {} returned an invalid opcode." "\nFunction dissection:\n\n" . format ( f . __name__ ) file = io . StringIO ( ) with contextlib . redirect_stdout ( file ) : dis . dis ( f ) msg += file . getvalue ( ) raise SystemError ( msg ) from e returned_func = __safety_wrapper returned_func . wrapped = f else : returned_func = f return returned_func
7996	def process_stream_error ( self , error ) : logger . debug ( "Unhandled stream error: condition: {0} {1!r}" . format ( error . condition_name , error . serialize ( ) ) )
8252	def image_to_rgb ( self , path , n = 10 ) : from PIL import Image img = Image . open ( path ) p = img . getdata ( ) f = lambda p : choice ( p ) for i in _range ( n ) : rgba = f ( p ) rgba = _list ( rgba ) if len ( rgba ) == 3 : rgba . append ( 255 ) r , g , b , a = [ v / 255.0 for v in rgba ] clr = color ( r , g , b , a , mode = "rgb" ) self . append ( clr )
10092	def _parse_response ( self , response ) : if not self . _raise_errors : return response is_4xx_error = str ( response . status_code ) [ 0 ] == '4' is_5xx_error = str ( response . status_code ) [ 0 ] == '5' content = response . content if response . status_code == 403 : raise AuthenticationError ( content ) elif is_4xx_error : raise APIError ( content ) elif is_5xx_error : raise ServerError ( content ) return response
3591	def toBigInt ( byteArray ) : array = byteArray [ : : - 1 ] out = 0 for key , value in enumerate ( array ) : decoded = struct . unpack ( "B" , bytes ( [ value ] ) ) [ 0 ] out = out | decoded << key * 8 return out
6283	def set_default_viewport ( self ) : expected_height = int ( self . buffer_width / self . aspect_ratio ) blank_space = self . buffer_height - expected_height self . fbo . viewport = ( 0 , blank_space // 2 , self . buffer_width , expected_height )
7100	def on_marker ( self , marker ) : mid , pos = marker self . marker = Marker ( __id__ = mid ) mapview = self . parent ( ) mapview . markers [ mid ] = self self . marker . setTag ( mid ) for w in self . child_widgets ( ) : mapview . init_info_window_adapter ( ) break d = self . declaration if d . show_info : self . set_show_info ( d . show_info ) del self . options
10330	def rank_edges ( edges , edge_ranking = None ) : edge_ranking = default_edge_ranking if edge_ranking is None else edge_ranking edges_scores = [ ( edge_id , edge_data [ RELATION ] , edge_ranking [ edge_data [ RELATION ] ] ) for edge_id , edge_data in edges . items ( ) ] return max ( edges_scores , key = itemgetter ( 2 ) )
4748	def import_parms ( self , args ) : for key , val in args . items ( ) : self . set_parm ( key , val )
13370	def is_redirecting ( path ) : candidate = unipath ( path , '.cpenv' ) return os . path . exists ( candidate ) and os . path . isfile ( candidate )
8645	def get_track_by_id ( session , track_id , track_point_limit = None , track_point_offset = None ) : tracking_data = { } if track_point_limit : tracking_data [ 'track_point_limit' ] = track_point_limit if track_point_offset : tracking_data [ 'track_point_offset' ] = track_point_offset response = make_get_request ( session , 'tracks/{}' . format ( track_id ) , params_data = tracking_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise TrackNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
6391	def encode ( self , word , max_length = 8 ) : word = '' . join ( char for char in word . lower ( ) if char in self . _initial_phones ) if not word : word = '' values = [ self . _initial_phones [ word [ 0 ] ] ] values += [ self . _trailing_phones [ char ] for char in word [ 1 : ] ] shifted_values = [ _ >> 1 for _ in values ] condensed_values = [ values [ 0 ] ] for n in range ( 1 , len ( shifted_values ) ) : if shifted_values [ n ] != shifted_values [ n - 1 ] : condensed_values . append ( values [ n ] ) values = ( [ condensed_values [ 0 ] ] + [ 0 ] * max ( 0 , max_length - len ( condensed_values ) ) + condensed_values [ 1 : max_length ] ) hash_value = 0 for val in values : hash_value = ( hash_value << 8 ) | val return hash_value
2585	def migrate_tasks_to_internal ( self , kill_event ) : logger . info ( "[TASK_PULL_THREAD] Starting" ) task_counter = 0 poller = zmq . Poller ( ) poller . register ( self . task_incoming , zmq . POLLIN ) while not kill_event . is_set ( ) : try : msg = self . task_incoming . recv_pyobj ( ) except zmq . Again : logger . debug ( "[TASK_PULL_THREAD] {} tasks in internal queue" . format ( self . pending_task_queue . qsize ( ) ) ) continue if msg == 'STOP' : kill_event . set ( ) break else : self . pending_task_queue . put ( msg ) task_counter += 1 logger . debug ( "[TASK_PULL_THREAD] Fetched task:{}" . format ( task_counter ) )
7428	def _subsample ( self ) : spans = self . maparr samp = np . zeros ( spans . shape [ 0 ] , dtype = np . uint64 ) for i in xrange ( spans . shape [ 0 ] ) : samp [ i ] = np . random . randint ( spans [ i , 0 ] , spans [ i , 1 ] , 1 ) return samp
11689	def get_metadata ( changeset ) : url = 'https://www.openstreetmap.org/api/0.6/changeset/{}' . format ( changeset ) return ET . fromstring ( requests . get ( url ) . content ) . getchildren ( ) [ 0 ]
3355	def union ( self , iterable ) : _dict = self . _dict append = self . append for i in iterable : if i . id not in _dict : append ( i )
13478	def _sentence_to_interstitial_spacing ( self ) : not_sentence_end_chars = [ ' ' ] abbreviations = [ 'i.e.' , 'e.g.' , ' v.' , ' w.' , ' wh.' ] titles = [ 'Prof.' , 'Mr.' , 'Mrs.' , 'Messrs.' , 'Mmes.' , 'Msgr.' , 'Ms.' , 'Fr.' , 'Rev.' , 'St.' , 'Dr.' , 'Lieut.' , 'Lt.' , 'Capt.' , 'Cptn.' , 'Sgt.' , 'Sjt.' , 'Gen.' , 'Hon.' , 'Cpl.' , 'L-Cpl.' , 'Pvt.' , 'Dvr.' , 'Gnr.' , 'Spr.' , 'Col.' , 'Lt-Col' , 'Lt-Gen.' , 'Mx.' ] for abbrev in abbreviations : for x in not_sentence_end_chars : self . _str_replacement ( abbrev + x , abbrev + '\ ' ) for title in titles : for x in not_sentence_end_chars : self . _str_replacement ( title + x , title + '~' )
3171	def create ( self , store_id , data ) : self . store_id = store_id if 'id' not in data : raise KeyError ( 'The store customer must have an id' ) if 'email_address' not in data : raise KeyError ( 'The store customer must have an email_address' ) check_email ( data [ 'email_address' ] ) if 'opt_in_status' not in data : raise KeyError ( 'The store customer must have an opt_in_status' ) if data [ 'opt_in_status' ] not in [ True , False ] : raise TypeError ( 'The opt_in_status must be True or False' ) response = self . _mc_client . _post ( url = self . _build_path ( store_id , 'customers' ) , data = data ) if response is not None : self . customer_id = response [ 'id' ] else : self . customer_id = None return response
12523	def check_call ( cmd_args ) : p = subprocess . Popen ( cmd_args , stdout = subprocess . PIPE ) ( output , err ) = p . communicate ( ) return output
8323	def isList ( l ) : return hasattr ( l , '__iter__' ) or ( type ( l ) in ( types . ListType , types . TupleType ) )
11460	def add_systemnumber ( self , source , recid = None ) : if not recid : recid = self . get_recid ( ) if not self . hidden and recid : record_add_field ( self . record , tag = '035' , subfields = [ ( '9' , source ) , ( 'a' , recid ) ] )
3622	def __post_save_receiver ( self , instance , ** kwargs ) : logger . debug ( 'RECEIVE post_save FOR %s' , instance . __class__ ) self . save_record ( instance , ** kwargs )
1134	def updatecache ( filename , module_globals = None ) : if filename in cache : del cache [ filename ] if not filename or ( filename . startswith ( '<' ) and filename . endswith ( '>' ) ) : return [ ] fullname = filename try : stat = os . stat ( fullname ) except OSError : basename = filename if module_globals and '__loader__' in module_globals : name = module_globals . get ( '__name__' ) loader = module_globals [ '__loader__' ] get_source = getattr ( loader , 'get_source' , None ) if name and get_source : try : data = get_source ( name ) except ( ImportError , IOError ) : pass else : if data is None : return [ ] cache [ filename ] = ( len ( data ) , None , [ line + '\n' for line in data . splitlines ( ) ] , fullname ) return cache [ filename ] [ 2 ] if os . path . isabs ( filename ) : return [ ] for dirname in sys . path : try : fullname = os . path . join ( dirname , basename ) except ( TypeError , AttributeError ) : continue try : stat = os . stat ( fullname ) break except os . error : pass else : return [ ] try : with open ( fullname , 'rU' ) as fp : lines = fp . readlines ( ) except IOError : return [ ] if lines and not lines [ - 1 ] . endswith ( '\n' ) : lines [ - 1 ] += '\n' size , mtime = stat . st_size , stat . st_mtime cache [ filename ] = size , mtime , lines , fullname return lines
12514	def crop_img ( image , rtol = 1e-8 , copy = True ) : img = check_img ( image ) data = img . get_data ( ) infinity_norm = max ( - data . min ( ) , data . max ( ) ) passes_threshold = np . logical_or ( data < - rtol * infinity_norm , data > rtol * infinity_norm ) if data . ndim == 4 : passes_threshold = np . any ( passes_threshold , axis = - 1 ) coords = np . array ( np . where ( passes_threshold ) ) start = coords . min ( axis = 1 ) end = coords . max ( axis = 1 ) + 1 start = np . maximum ( start - 1 , 0 ) end = np . minimum ( end + 1 , data . shape [ : 3 ] ) slices = [ slice ( s , e ) for s , e in zip ( start , end ) ] return _crop_img_to ( img , slices , copy = copy )
6354	def _apply_rule_if_compat ( self , phonetic , target , language_arg ) : candidate = phonetic + target if '[' not in candidate : return candidate candidate = self . _expand_alternates ( candidate ) candidate_array = candidate . split ( '|' ) candidate = '' found = False for i in range ( len ( candidate_array ) ) : this_candidate = candidate_array [ i ] if language_arg != 1 : this_candidate = self . _normalize_lang_attrs ( this_candidate + '[' + str ( language_arg ) + ']' , False ) if this_candidate != '[0]' : found = True if candidate : candidate += '|' candidate += this_candidate if not found : return None if '|' in candidate : candidate = '(' + candidate + ')' return candidate
10881	def delistify ( a , b = None ) : if isinstance ( b , ( tuple , list , np . ndarray ) ) : if isinstance ( a , ( tuple , list , np . ndarray ) ) : return type ( b ) ( a ) return type ( b ) ( [ a ] ) else : if isinstance ( a , ( tuple , list , np . ndarray ) ) and len ( a ) == 1 : return a [ 0 ] return a return a
9350	def check_digit ( num ) : sum = 0 digits = str ( num ) [ : - 1 ] [ : : - 1 ] for i , n in enumerate ( digits ) : if ( i + 1 ) % 2 != 0 : digit = int ( n ) * 2 if digit > 9 : sum += ( digit - 9 ) else : sum += digit else : sum += int ( n ) return ( ( divmod ( sum , 10 ) [ 0 ] + 1 ) * 10 - sum ) % 10
4229	def make_formatter ( format_name ) : if "json" in format_name : from json import dumps import datetime def jsonhandler ( obj ) : obj . isoformat ( ) if isinstance ( obj , ( datetime . datetime , datetime . date ) ) else obj if format_name == "prettyjson" : def jsondumps ( data ) : return dumps ( data , default = jsonhandler , indent = 2 , separators = ( ',' , ': ' ) ) else : def jsondumps ( data ) : return dumps ( data , default = jsonhandler ) def jsonify ( data ) : if isinstance ( data , dict ) : print ( jsondumps ( data ) ) elif isinstance ( data , list ) : print ( jsondumps ( [ device . _asdict ( ) for device in data ] ) ) else : print ( dumps ( { 'result' : data } ) ) return jsonify else : def printer ( data ) : if isinstance ( data , dict ) : print ( data ) else : for row in data : print ( row ) return printer
7851	def remove_feature ( self , var ) : if not var : raise ValueError ( "var is None" ) if '"' not in var : expr = 'd:feature[@var="%s"]' % ( var , ) elif "'" not in var : expr = "d:feature[@var='%s']" % ( var , ) else : raise ValueError ( "Invalid feature name" ) l = self . xpath_ctxt . xpathEval ( expr ) if not l : return for f in l : f . unlinkNode ( ) f . freeNode ( )
6304	def find_effect_class ( self , path ) -> Type [ Effect ] : package_name , class_name = parse_package_string ( path ) if package_name : package = self . get_package ( package_name ) return package . find_effect_class ( class_name , raise_for_error = True ) for package in self . packages : effect_cls = package . find_effect_class ( class_name ) if effect_cls : return effect_cls raise EffectError ( "No effect class '{}' found in any packages" . format ( class_name ) )
2443	def add_annotation_type ( self , doc , annotation_type ) : if len ( doc . annotations ) != 0 : if not self . annotation_type_set : self . annotation_type_set = True if validations . validate_annotation_type ( annotation_type ) : doc . annotations [ - 1 ] . annotation_type = annotation_type return True else : raise SPDXValueError ( 'Annotation::AnnotationType' ) else : raise CardinalityError ( 'Annotation::AnnotationType' ) else : raise OrderError ( 'Annotation::AnnotationType' )
11419	def record_move_subfield ( rec , tag , subfield_position , new_subfield_position , field_position_global = None , field_position_local = None ) : subfields = record_get_subfields ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) try : subfield = subfields . pop ( subfield_position ) subfields . insert ( new_subfield_position , subfield ) except IndexError : raise InvenioBibRecordFieldError ( "There is no subfield with position '%d'." % subfield_position )
2594	def interactive ( f ) : if isinstance ( f , FunctionType ) : mainmod = __import__ ( '__main__' ) f = FunctionType ( f . __code__ , mainmod . __dict__ , f . __name__ , f . __defaults__ , ) f . __module__ = '__main__' return f
8139	def desaturate ( self ) : alpha = self . img . split ( ) [ 3 ] self . img = self . img . convert ( "L" ) self . img = self . img . convert ( "RGBA" ) self . img . putalpha ( alpha )
11487	def _search_folder_for_item_or_folder ( name , folder_id ) : session . token = verify_credentials ( ) children = session . communicator . folder_children ( session . token , folder_id ) for folder in children [ 'folders' ] : if folder [ 'name' ] == name : return False , folder [ 'folder_id' ] for item in children [ 'items' ] : if item [ 'name' ] == name : return True , item [ 'item_id' ] return False , - 1
10443	def getobjectinfo ( self , window_name , object_name ) : try : obj_info = self . _get_object_map ( window_name , object_name , wait_for_object = False ) except atomac . _a11y . ErrorInvalidUIElement : self . _windows = { } obj_info = self . _get_object_map ( window_name , object_name , wait_for_object = False ) props = [ ] if obj_info : for obj_prop in obj_info . keys ( ) : if not obj_info [ obj_prop ] or obj_prop == "obj" : continue props . append ( obj_prop ) return props
4672	def newWallet ( self , pwd ) : if self . created ( ) : raise WalletExists ( "You already have created a wallet!" ) self . store . unlock ( pwd )
7110	def fit ( self , X , y ) : trainer = pycrfsuite . Trainer ( verbose = True ) for xseq , yseq in zip ( X , y ) : trainer . append ( xseq , yseq ) trainer . set_params ( self . params ) if self . filename : filename = self . filename else : filename = 'model.tmp' trainer . train ( filename ) tagger = pycrfsuite . Tagger ( ) tagger . open ( filename ) self . estimator = tagger
9501	def _disassemble ( self , lineno_width = 3 , mark_as_current = False ) : fields = [ ] if lineno_width : if self . starts_line is not None : lineno_fmt = "%%%dd" % lineno_width fields . append ( lineno_fmt % self . starts_line ) else : fields . append ( ' ' * lineno_width ) if mark_as_current : fields . append ( ' ) else : fields . append ( ' ' ) if self . is_jump_target : fields . append ( '>>' ) else : fields . append ( ' ' ) fields . append ( repr ( self . offset ) . rjust ( 4 ) ) fields . append ( self . opname . ljust ( 20 ) ) if self . arg is not None : fields . append ( repr ( self . arg ) . rjust ( 5 ) ) if self . argrepr : fields . append ( '(' + self . argrepr + ')' ) return ' ' . join ( fields ) . rstrip ( )
1474	def _get_tmaster_processes ( self ) : retval = { } tmaster_cmd_lst = [ self . tmaster_binary , '--topology_name=%s' % self . topology_name , '--topology_id=%s' % self . topology_id , '--zkhostportlist=%s' % self . state_manager_connection , '--zkroot=%s' % self . state_manager_root , '--myhost=%s' % self . master_host , '--master_port=%s' % str ( self . master_port ) , '--controller_port=%s' % str ( self . tmaster_controller_port ) , '--stats_port=%s' % str ( self . tmaster_stats_port ) , '--config_file=%s' % self . heron_internals_config_file , '--override_config_file=%s' % self . override_config_file , '--metrics_sinks_yaml=%s' % self . metrics_sinks_config_file , '--metricsmgr_port=%s' % str ( self . metrics_manager_port ) , '--ckptmgr_port=%s' % str ( self . checkpoint_manager_port ) ] tmaster_env = self . shell_env . copy ( ) if self . shell_env is not None else { } tmaster_cmd = Command ( tmaster_cmd_lst , tmaster_env ) if os . environ . get ( 'ENABLE_HEAPCHECK' ) is not None : tmaster_cmd . env . update ( { 'LD_PRELOAD' : "/usr/lib/libtcmalloc.so" , 'HEAPCHECK' : "normal" } ) retval [ "heron-tmaster" ] = tmaster_cmd if self . metricscache_manager_mode . lower ( ) != "disabled" : retval [ "heron-metricscache" ] = self . _get_metrics_cache_cmd ( ) if self . health_manager_mode . lower ( ) != "disabled" : retval [ "heron-healthmgr" ] = self . _get_healthmgr_cmd ( ) retval [ self . metricsmgr_ids [ 0 ] ] = self . _get_metricsmgr_cmd ( self . metricsmgr_ids [ 0 ] , self . metrics_sinks_config_file , self . metrics_manager_port ) if self . is_stateful_topology : retval . update ( self . _get_ckptmgr_process ( ) ) return retval
1017	def _adaptSegment ( self , segUpdate ) : trimSegment = False c , i , segment = segUpdate . columnIdx , segUpdate . cellIdx , segUpdate . segment activeSynapses = segUpdate . activeSynapses synToUpdate = set ( [ syn for syn in activeSynapses if type ( syn ) == int ] ) if segment is not None : if self . verbosity >= 4 : print "Reinforcing segment #%d for cell[%d,%d]" % ( segment . segID , c , i ) print " before:" , segment . debugPrint ( ) segment . lastActiveIteration = self . lrnIterationIdx segment . positiveActivations += 1 segment . dutyCycle ( active = True ) lastSynIndex = len ( segment . syns ) - 1 inactiveSynIndices = [ s for s in xrange ( 0 , lastSynIndex + 1 ) if s not in synToUpdate ] trimSegment = segment . updateSynapses ( inactiveSynIndices , - self . permanenceDec ) activeSynIndices = [ syn for syn in synToUpdate if syn <= lastSynIndex ] segment . updateSynapses ( activeSynIndices , self . permanenceInc ) synsToAdd = [ syn for syn in activeSynapses if type ( syn ) != int ] if self . maxSynapsesPerSegment > 0 and len ( synsToAdd ) + len ( segment . syns ) > self . maxSynapsesPerSegment : numToFree = ( len ( segment . syns ) + len ( synsToAdd ) - self . maxSynapsesPerSegment ) segment . freeNSynapses ( numToFree , inactiveSynIndices , self . verbosity ) for newSyn in synsToAdd : segment . addSynapse ( newSyn [ 0 ] , newSyn [ 1 ] , self . initialPerm ) if self . verbosity >= 4 : print " after:" , segment . debugPrint ( ) else : newSegment = Segment ( tm = self , isSequenceSeg = segUpdate . sequenceSegment ) for synapse in activeSynapses : newSegment . addSynapse ( synapse [ 0 ] , synapse [ 1 ] , self . initialPerm ) if self . verbosity >= 3 : print "New segment #%d for cell[%d,%d]" % ( self . segID - 1 , c , i ) , newSegment . debugPrint ( ) self . cells [ c ] [ i ] . append ( newSegment ) return trimSegment
6778	def get_component_order ( component_names ) : assert isinstance ( component_names , ( tuple , list ) ) component_dependences = { } for _name in component_names : deps = set ( manifest_deployers_befores . get ( _name , [ ] ) ) deps = deps . intersection ( component_names ) component_dependences [ _name ] = deps component_order = list ( topological_sort ( component_dependences . items ( ) ) ) return component_order
3372	def choose_solver ( model , solver = None , qp = False ) : if solver is None : solver = model . problem else : model . solver = solver if qp and interface_to_str ( solver ) not in qp_solvers : solver = solvers [ get_solver_name ( qp = True ) ] return solver
8450	def has_env_vars ( * env_vars ) : for env_var in env_vars : if not os . environ . get ( env_var ) : msg = ( 'Must set {} environment variable. View docs for setting up environment at {}' ) . format ( env_var , temple . constants . TEMPLE_DOCS_URL ) raise temple . exceptions . InvalidEnvironmentError ( msg )
12589	def treefall ( iterable ) : num_elems = len ( iterable ) for i in range ( num_elems , - 1 , - 1 ) : for c in combinations ( iterable , i ) : yield c
10495	def clickMouseButtonRightWithMods ( self , coord , modifiers ) : modFlags = self . _pressModifiers ( modifiers ) self . _queueMouseButton ( coord , Quartz . kCGMouseButtonRight , modFlags ) self . _releaseModifiers ( modifiers ) self . _postQueuedEvents ( )
4052	def dump ( self , itemkey , filename = None , path = None ) : if not filename : filename = self . item ( itemkey ) [ "data" ] [ "filename" ] if path : pth = os . path . join ( path , filename ) else : pth = filename file = self . file ( itemkey ) if self . snapshot : self . snapshot = False pth = pth + ".zip" with open ( pth , "wb" ) as f : f . write ( file )
7798	def sasl_mechanism ( name , secure , preference = 50 ) : def decorator ( klass ) : klass . _pyxmpp_sasl_secure = secure klass . _pyxmpp_sasl_preference = preference if issubclass ( klass , ClientAuthenticator ) : _register_client_authenticator ( klass , name ) elif issubclass ( klass , ServerAuthenticator ) : _register_server_authenticator ( klass , name ) else : raise TypeError ( "Not a ClientAuthenticator" " or ServerAuthenticator class" ) return klass return decorator
1192	def translate ( pat ) : i , n = 0 , len ( pat ) res = '' while i < n : c = pat [ i ] i = i + 1 if c == '*' : res = res + '.*' elif c == '?' : res = res + '.' elif c == '[' : j = i if j < n and pat [ j ] == '!' : j = j + 1 if j < n and pat [ j ] == ']' : j = j + 1 while j < n and pat [ j ] != ']' : j = j + 1 if j >= n : res = res + '\\[' else : stuff = pat [ i : j ] . replace ( '\\' , '\\\\' ) i = j + 1 if stuff [ 0 ] == '!' : stuff = '^' + stuff [ 1 : ] elif stuff [ 0 ] == '^' : stuff = '\\' + stuff res = '%s[%s]' % ( res , stuff ) else : res = res + re . escape ( c ) return res + '\Z(?ms)'
2506	def get_extr_lic_name ( self , extr_lic ) : extr_name_list = list ( self . graph . triples ( ( extr_lic , self . spdx_namespace [ 'licenseName' ] , None ) ) ) if len ( extr_name_list ) > 1 : self . more_than_one_error ( 'extracted license name' ) return elif len ( extr_name_list ) == 0 : return return self . to_special_value ( extr_name_list [ 0 ] [ 2 ] )
1237	def from_spec ( spec , kwargs = None ) : network = util . get_object ( obj = spec , default_object = LayeredNetwork , kwargs = kwargs ) assert isinstance ( network , Network ) return network
11595	def _rc_renamenx ( self , src , dst ) : "Rename key ``src`` to ``dst`` if ``dst`` doesn't already exist" if self . exists ( dst ) : return False return self . _rc_rename ( src , dst )
4847	def _load_data ( self , resource , detail_resource = None , resource_id = None , querystring = None , traverse_pagination = False , default = DEFAULT_VALUE_SAFEGUARD , ) : default_val = default if default != self . DEFAULT_VALUE_SAFEGUARD else { } querystring = querystring if querystring else { } cache_key = utils . get_cache_key ( resource = resource , querystring = querystring , traverse_pagination = traverse_pagination , resource_id = resource_id ) response = cache . get ( cache_key ) if not response : endpoint = getattr ( self . client , resource ) ( resource_id ) endpoint = getattr ( endpoint , detail_resource ) if detail_resource else endpoint response = endpoint . get ( ** querystring ) if traverse_pagination : results = utils . traverse_pagination ( response , endpoint ) response = { 'count' : len ( results ) , 'next' : 'None' , 'previous' : 'None' , 'results' : results , } if response : cache . set ( cache_key , response , settings . ENTERPRISE_API_CACHE_TIMEOUT ) return response or default_val
2691	def iter_cython ( path ) : for dir_path , dir_names , file_names in os . walk ( path ) : for file_name in file_names : if file_name . startswith ( '.' ) : continue if os . path . splitext ( file_name ) [ 1 ] not in ( '.pyx' , '.pxd' ) : continue yield os . path . join ( dir_path , file_name )
12984	def keywords ( func ) : @ wraps ( func ) def decorator ( * args , ** kwargs ) : idx = 0 if inspect . ismethod ( func ) else 1 if len ( args ) > idx : if isinstance ( args [ idx ] , ( dict , composite ) ) : for key in args [ idx ] : kwargs [ key ] = args [ idx ] [ key ] args = args [ : idx ] return func ( * args , ** kwargs ) return decorator
9389	def check_sla ( self , sla , diff_metric ) : try : if sla . display is '%' : diff_val = float ( diff_metric [ 'percent_diff' ] ) else : diff_val = float ( diff_metric [ 'absolute_diff' ] ) except ValueError : return False if not ( sla . check_sla_passed ( diff_val ) ) : self . sla_failures += 1 self . sla_failure_list . append ( DiffSLAFailure ( sla , diff_metric ) ) return True
1551	def _get_bolt ( self ) : bolt = topology_pb2 . Bolt ( ) bolt . comp . CopyFrom ( self . _get_base_component ( ) ) self . _add_in_streams ( bolt ) self . _add_out_streams ( bolt ) return bolt
11563	def set_digital_latch ( self , pin , threshold_type , cb = None ) : if 0 <= threshold_type <= 1 : self . _command_handler . set_digital_latch ( pin , threshold_type , cb ) return True else : return False
10067	def json_files_serializer ( objs , status = None ) : files = [ file_serializer ( obj ) for obj in objs ] return make_response ( json . dumps ( files ) , status )
2456	def set_pkg_license_from_file ( self , doc , lic ) : self . assert_package_exists ( ) if validations . validate_lics_from_file ( lic ) : doc . package . licenses_from_files . append ( lic ) return True else : raise SPDXValueError ( 'Package::LicensesFromFile' )
12993	def level_chunker ( text , getreffs , level = 1 ) : references = getreffs ( level = level ) return [ ( ref . split ( ":" ) [ - 1 ] , ref . split ( ":" ) [ - 1 ] ) for ref in references ]
7513	def locichunk ( args ) : data , optim , pnames , snppad , smask , start , samplecov , locuscov , upper = args hslice = [ start , start + optim ] co5 = h5py . File ( data . database , 'r' ) afilt = co5 [ "filters" ] [ hslice [ 0 ] : hslice [ 1 ] , ] aedge = co5 [ "edges" ] [ hslice [ 0 ] : hslice [ 1 ] , ] asnps = co5 [ "snps" ] [ hslice [ 0 ] : hslice [ 1 ] , ] io5 = h5py . File ( data . clust_database , 'r' ) if upper : aseqs = np . char . upper ( io5 [ "seqs" ] [ hslice [ 0 ] : hslice [ 1 ] , ] ) else : aseqs = io5 [ "seqs" ] [ hslice [ 0 ] : hslice [ 1 ] , ] keep = np . where ( np . sum ( afilt , axis = 1 ) == 0 ) [ 0 ] store = [ ] for iloc in keep : edg = aedge [ iloc ] args = [ iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ] if edg [ 4 ] : outstr , samplecov , locuscov = enter_pairs ( * args ) store . append ( outstr ) else : outstr , samplecov , locuscov = enter_singles ( * args ) store . append ( outstr ) tmpo = os . path . join ( data . dirs . outfiles , data . name + ".loci.{}" . format ( start ) ) with open ( tmpo , 'w' ) as tmpout : tmpout . write ( "\n" . join ( store ) + "\n" ) io5 . close ( ) co5 . close ( ) return samplecov , locuscov , start
7466	def _load_existing_results ( self , name , workdir ) : path = os . path . realpath ( os . path . join ( self . workdir , self . name ) ) mcmcs = glob . glob ( path + "_r*.mcmc.txt" ) outs = glob . glob ( path + "_r*.out.txt" ) trees = glob . glob ( path + "_r*.tre" ) for mcmcfile in mcmcs : if mcmcfile not in self . files . mcmcfiles : self . files . mcmcfiles . append ( mcmcfile ) for outfile in outs : if outfile not in self . files . outfiles : self . files . outfiles . append ( outfile ) for tree in trees : if tree not in self . files . treefiles : self . files . treefiles . append ( tree )
5557	def _strip_zoom ( input_string , strip_string ) : try : return int ( input_string . strip ( strip_string ) ) except Exception as e : raise MapcheteConfigError ( "zoom level could not be determined: %s" % e )
11196	def compress ( obj , level = 6 , return_type = "bytes" ) : if isinstance ( obj , binary_type ) : b = zlib . compress ( obj , level ) elif isinstance ( obj , string_types ) : b = zlib . compress ( obj . encode ( "utf-8" ) , level ) else : b = zlib . compress ( pickle . dumps ( obj , protocol = 2 ) , level ) if return_type == "bytes" : return b elif return_type == "str" : return base64 . b64encode ( b ) . decode ( "utf-8" ) else : raise ValueError ( "'return_type' has to be one of 'bytes', 'str'!" )
6632	def islast ( generator ) : next_x = None first = True for x in generator : if not first : yield ( next_x , False ) next_x = x first = False if not first : yield ( next_x , True )
293	def plot_gross_leverage ( returns , positions , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) gl = timeseries . gross_lev ( positions ) gl . plot ( lw = 0.5 , color = 'limegreen' , legend = False , ax = ax , ** kwargs ) ax . axhline ( gl . mean ( ) , color = 'g' , linestyle = '--' , lw = 3 ) ax . set_title ( 'Gross leverage' ) ax . set_ylabel ( 'Gross leverage' ) ax . set_xlabel ( '' ) return ax
265	def _cumulative_returns_less_costs ( returns , costs ) : if costs is None : return ep . cum_returns ( returns ) return ep . cum_returns ( returns - costs )
8306	def close ( self ) : self . process . stdout . close ( ) self . process . stderr . close ( ) self . running = False
1197	def nested ( * managers ) : warn ( "With-statements now directly support multiple context managers" , DeprecationWarning , 3 ) exits = [ ] vars = [ ] exc = ( None , None , None ) try : for mgr in managers : exit = mgr . __exit__ enter = mgr . __enter__ vars . append ( enter ( ) ) exits . append ( exit ) yield vars except : exc = sys . exc_info ( ) finally : while exits : exit = exits . pop ( ) try : if exit ( * exc ) : exc = ( None , None , None ) except : exc = sys . exc_info ( ) if exc != ( None , None , None ) : raise exc [ 0 ] , exc [ 1 ] , exc [ 2 ]
12103	def summary ( self ) : print ( "Type: %s" % self . __class__ . __name__ ) print ( "Batch Name: %r" % self . batch_name ) if self . tag : print ( "Tag: %s" % self . tag ) print ( "Root directory: %r" % self . get_root_directory ( ) ) print ( "Maximum concurrency: %s" % self . max_concurrency ) if self . description : print ( "Description: %s" % self . description )
12976	def compat_convertHashedIndexes ( self , objs , conn = None ) : if conn is None : conn = self . _get_connection ( ) fields = [ ] for indexedField in self . indexedFields : origField = self . fields [ indexedField ] if 'hashIndex' not in origField . __class__ . __new__ . __code__ . co_varnames : continue if indexedField . hashIndex is True : hashingField = origField regField = origField . copy ( ) regField . hashIndex = False else : regField = origField hashingField = origField . copy ( ) hashingField . hashIndex = True fields . append ( ( origField , regField , hashingField ) ) objDicts = [ obj . asDict ( True , forStorage = True ) for obj in objs ] for objDict in objDicts : pipeline = conn . pipeline ( ) pk = objDict [ '_id' ] for origField , regField , hashingField in fields : val = objDict [ indexedField ] self . _rem_id_from_index ( regField , pk , val , pipeline ) self . _rem_id_from_index ( hashingField , pk , val , pipeline ) self . _add_id_to_index ( origField , pk , val , pipeline ) pipeline . execute ( )
1910	def run ( self , procs = 1 , timeout = None , should_profile = False ) : assert not self . running , "Manticore is already running." self . _start_run ( ) self . _last_run_stats [ 'time_started' ] = time . time ( ) with self . shutdown_timeout ( timeout ) : self . _start_workers ( procs , profiling = should_profile ) self . _join_workers ( ) self . _finish_run ( profiling = should_profile )
1014	def _getBestMatchingSegment ( self , c , i , activeState ) : maxActivity , which = self . minThreshold , - 1 for j , s in enumerate ( self . cells [ c ] [ i ] ) : activity = self . _getSegmentActivityLevel ( s , activeState , connectedSynapsesOnly = False ) if activity >= maxActivity : maxActivity , which = activity , j if which == - 1 : return None else : return self . cells [ c ] [ i ] [ which ]
12861	def add_months ( self , month_int ) : month_int += self . month while month_int > 12 : self = BusinessDate . add_years ( self , 1 ) month_int -= 12 while month_int < 1 : self = BusinessDate . add_years ( self , - 1 ) month_int += 12 l = monthrange ( self . year , month_int ) [ 1 ] return BusinessDate . from_ymd ( self . year , month_int , min ( l , self . day ) )
7862	def handle_tls_connected_event ( self , event ) : if self . settings [ "tls_verify_peer" ] : valid = self . settings [ "tls_verify_callback" ] ( event . stream , event . peer_certificate ) if not valid : raise SSLError ( "Certificate verification failed" ) event . stream . tls_established = True with event . stream . lock : event . stream . _restart_stream ( )
8181	def remove_node ( self , id ) : if self . has_key ( id ) : n = self [ id ] self . nodes . remove ( n ) del self [ id ] for e in list ( self . edges ) : if n in ( e . node1 , e . node2 ) : if n in e . node1 . links : e . node1 . links . remove ( n ) if n in e . node2 . links : e . node2 . links . remove ( n ) self . edges . remove ( e )
254	def add_closing_transactions ( positions , transactions ) : closed_txns = transactions [ [ 'symbol' , 'amount' , 'price' ] ] pos_at_end = positions . drop ( 'cash' , axis = 1 ) . iloc [ - 1 ] open_pos = pos_at_end . replace ( 0 , np . nan ) . dropna ( ) end_dt = open_pos . name + pd . Timedelta ( seconds = 1 ) for sym , ending_val in open_pos . iteritems ( ) : txn_sym = transactions [ transactions . symbol == sym ] ending_amount = txn_sym . amount . sum ( ) ending_price = ending_val / ending_amount closing_txn = { 'symbol' : sym , 'amount' : - ending_amount , 'price' : ending_price } closing_txn = pd . DataFrame ( closing_txn , index = [ end_dt ] ) closed_txns = closed_txns . append ( closing_txn ) closed_txns = closed_txns [ closed_txns . amount != 0 ] return closed_txns
13785	def generate ( length = DEFAULT_LENGTH ) : return '' . join ( random . SystemRandom ( ) . choice ( ALPHABET ) for _ in range ( length ) )
12641	def get_config_value ( name , fallback = None ) : cli_config = CLIConfig ( SF_CLI_CONFIG_DIR , SF_CLI_ENV_VAR_PREFIX ) return cli_config . get ( 'servicefabric' , name , fallback )
10071	def pid ( self ) : pid = self . deposit_fetcher ( self . id , self ) return PersistentIdentifier . get ( pid . pid_type , pid . pid_value )
5651	def _scan_footpaths_to_departure_stop ( self , connection_dep_stop , connection_dep_time , arrival_time_target ) : for _ , neighbor , data in self . _walk_network . edges_iter ( nbunch = [ connection_dep_stop ] , data = True ) : d_walk = data [ 'd_walk' ] neighbor_dep_time = connection_dep_time - d_walk / self . _walk_speed pt = LabelTimeSimple ( departure_time = neighbor_dep_time , arrival_time_target = arrival_time_target ) self . _stop_profiles [ neighbor ] . update_pareto_optimal_tuples ( pt )
10391	def workflow_aggregate ( graph : BELGraph , node : BaseEntity , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , aggregator : Optional [ Callable [ [ Iterable [ float ] ] , float ] ] = None , ) -> Optional [ float ] : runners = workflow ( graph , node , key = key , tag = tag , default_score = default_score , runs = runs ) scores = [ runner . get_final_score ( ) for runner in runners ] if not scores : log . warning ( 'Unable to run the heat diffusion workflow for %s' , node ) return if aggregator is None : return np . average ( scores ) return aggregator ( scores )
4033	def parse ( s ) : if IS_PY3 : r = sre_parse . parse ( s , flags = U ) else : r = sre_parse . parse ( s . decode ( 'utf-8' ) , flags = U ) return list ( r )
618	def parseBool ( s ) : l = s . lower ( ) if l in ( "true" , "t" , "1" ) : return True if l in ( "false" , "f" , "0" ) : return False raise Exception ( "Unable to convert string '%s' to a boolean value" % s )
4149	def onesided_gen ( self ) : if self . N % 2 == 0 : for n in range ( 0 , self . N // 2 + 1 ) : yield n * self . df else : for n in range ( 0 , ( self . N + 1 ) // 2 ) : yield n * self . df
526	def _inhibitColumnsLocal ( self , overlaps , density ) : activeArray = numpy . zeros ( self . _numColumns , dtype = "bool" ) for column , overlap in enumerate ( overlaps ) : if overlap >= self . _stimulusThreshold : neighborhood = self . _getColumnNeighborhood ( column ) neighborhoodOverlaps = overlaps [ neighborhood ] numBigger = numpy . count_nonzero ( neighborhoodOverlaps > overlap ) ties = numpy . where ( neighborhoodOverlaps == overlap ) tiedNeighbors = neighborhood [ ties ] numTiesLost = numpy . count_nonzero ( activeArray [ tiedNeighbors ] ) numActive = int ( 0.5 + density * len ( neighborhood ) ) if numBigger + numTiesLost < numActive : activeArray [ column ] = True return activeArray . nonzero ( ) [ 0 ]
3572	def peripheral_didDiscoverServices_ ( self , peripheral , services ) : logger . debug ( 'peripheral_didDiscoverServices called' ) for service in peripheral . services ( ) : if service_list ( ) . get ( service ) is None : service_list ( ) . add ( service , CoreBluetoothGattService ( service ) ) peripheral . discoverCharacteristics_forService_ ( None , service )
1862	def SCAS ( cpu , dest , src ) : dest_reg = dest . reg mem_reg = src . mem . base size = dest . size arg0 = dest . read ( ) arg1 = src . read ( ) res = arg0 - arg1 cpu . _calculate_CMP_flags ( size , res , arg0 , arg1 ) increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( mem_reg , cpu . read_register ( mem_reg ) + increment )
10410	def finalize_canonical_averages ( number_of_nodes , ps , canonical_averages , alpha , ) : spanning_cluster = ( ( 'percolation_probability_mean' in canonical_averages . dtype . names ) and 'percolation_probability_m2' in canonical_averages . dtype . names ) ret = np . empty_like ( canonical_averages , dtype = finalized_canonical_averages_dtype ( spanning_cluster = spanning_cluster ) , ) n = canonical_averages [ 'number_of_runs' ] sqrt_n = np . sqrt ( canonical_averages [ 'number_of_runs' ] ) ret [ 'number_of_runs' ] = n ret [ 'p' ] = ps ret [ 'alpha' ] = alpha def _transform ( original_key , final_key = None , normalize = False , transpose = False , ) : if final_key is None : final_key = original_key keys_mean = [ '{}_mean' . format ( key ) for key in [ original_key , final_key ] ] keys_std = [ '{}_m2' . format ( original_key ) , '{}_std' . format ( final_key ) , ] key_ci = '{}_ci' . format ( final_key ) ret [ keys_mean [ 1 ] ] = canonical_averages [ keys_mean [ 0 ] ] if normalize : ret [ keys_mean [ 1 ] ] /= number_of_nodes array = canonical_averages [ keys_std [ 0 ] ] result = np . sqrt ( ( array . T if transpose else array ) / ( n - 1 ) ) ret [ keys_std [ 1 ] ] = ( result . T if transpose else result ) if normalize : ret [ keys_std [ 1 ] ] /= number_of_nodes array = ret [ keys_std [ 1 ] ] scale = ( array . T if transpose else array ) / sqrt_n array = ret [ keys_mean [ 1 ] ] mean = ( array . T if transpose else array ) result = scipy . stats . t . interval ( 1 - alpha , df = n - 1 , loc = mean , scale = scale , ) ( ret [ key_ci ] [ ... , 0 ] , ret [ key_ci ] [ ... , 1 ] ) = ( [ my_array . T for my_array in result ] if transpose else result ) if spanning_cluster : _transform ( 'percolation_probability' ) _transform ( 'max_cluster_size' , 'percolation_strength' , normalize = True ) _transform ( 'moments' , normalize = True , transpose = True ) return ret
3135	def create ( self , store_id , order_id , data ) : self . store_id = store_id self . order_id = order_id if 'id' not in data : raise KeyError ( 'The order line must have an id' ) if 'product_id' not in data : raise KeyError ( 'The order line must have a product_id' ) if 'product_variant_id' not in data : raise KeyError ( 'The order line must have a product_variant_id' ) if 'quantity' not in data : raise KeyError ( 'The order line must have a quantity' ) if 'price' not in data : raise KeyError ( 'The order line must have a price' ) response = self . _mc_client . _post ( url = self . _build_path ( store_id , 'orders' , order_id , 'lines' ) ) if response is not None : self . line_id = response [ 'id' ] else : self . line_id = None return response
1446	def poll ( self ) : try : ret = self . _buffer . get ( block = False ) if self . _producer_callback is not None : self . _producer_callback ( ) return ret except Queue . Empty : Log . debug ( "%s: Empty in poll()" % str ( self ) ) raise Queue . Empty
8002	def __from_xml ( self , xmlnode ) : self . __logger . debug ( "Converting jabber:iq:register element from XML" ) if xmlnode . type != "element" : raise ValueError ( "XML node is not a jabber:iq:register element (not an element)" ) ns = get_node_ns_uri ( xmlnode ) if ns and ns != REGISTER_NS or xmlnode . name != "query" : raise ValueError ( "XML node is not a jabber:iq:register element" ) for element in xml_element_iter ( xmlnode . children ) : ns = get_node_ns_uri ( element ) if ns == DATAFORM_NS and element . name == "x" and not self . form : self . form = Form ( element ) elif ns != REGISTER_NS : continue name = element . name if name == "instructions" and not self . instructions : self . instructions = from_utf8 ( element . getContent ( ) ) elif name == "registered" : self . registered = True elif name == "remove" : self . remove = True elif name in legacy_fields and not getattr ( self , name ) : value = from_utf8 ( element . getContent ( ) ) if value is None : value = u"" self . __logger . debug ( u"Setting legacy field %r to %r" % ( name , value ) ) setattr ( self , name , value )
11304	def embed ( self , url , ** kwargs ) : try : provider = self . provider_for_url ( url ) except OEmbedMissingEndpoint : raise else : try : stored_match = StoredOEmbed . objects . filter ( match = url , maxwidth = kwargs . get ( 'maxwidth' , None ) , maxheight = kwargs . get ( 'maxheight' , None ) , date_expires__gte = datetime . datetime . now ( ) ) [ 0 ] return OEmbedResource . create_json ( stored_match . response_json ) except IndexError : params = dict ( [ ( k , v ) for k , v in kwargs . items ( ) if v ] ) resource = provider . request_resource ( url , ** params ) try : cache_age = int ( resource . cache_age ) if cache_age < MIN_OEMBED_TTL : cache_age = MIN_OEMBED_TTL except : cache_age = DEFAULT_OEMBED_TTL date_expires = datetime . datetime . now ( ) + datetime . timedelta ( seconds = cache_age ) stored_oembed , created = StoredOEmbed . objects . get_or_create ( match = url , maxwidth = kwargs . get ( 'maxwidth' , None ) , maxheight = kwargs . get ( 'maxheight' , None ) ) stored_oembed . response_json = resource . json stored_oembed . resource_type = resource . type stored_oembed . date_expires = date_expires if resource . content_object : stored_oembed . content_object = resource . content_object stored_oembed . save ( ) return resource
7181	def fix_remaining_type_comments ( node ) : assert node . type == syms . file_input last_n = None for n in node . post_order ( ) : if last_n is not None : if n . type == token . NEWLINE and is_assignment ( last_n ) : fix_variable_annotation_type_comment ( n , last_n ) elif n . type == syms . funcdef and last_n . type == syms . suite : fix_signature_annotation_type_comment ( n , last_n , offset = 1 ) elif n . type == syms . async_funcdef and last_n . type == syms . suite : fix_signature_annotation_type_comment ( n , last_n , offset = 2 ) last_n = n
11658	def transform ( self , X ) : X = check_array ( X , copy = self . copy ) X *= self . scale_ X += self . min_ if self . truncate : np . maximum ( self . feature_range [ 0 ] , X , out = X ) np . minimum ( self . feature_range [ 1 ] , X , out = X ) return X
13767	def add_bundle ( self , * args ) : for bundle in args : if not self . multitype and self . has_bundles ( ) : first_bundle = self . get_first_bundle ( ) if first_bundle . get_type ( ) != bundle . get_type ( ) : raise Exception ( 'Different bundle types for one Asset: %s[%s -> %s]' 'check types or set multitype parameter to True' % ( self . name , first_bundle . get_type ( ) , bundle . get_type ( ) ) ) self . bundles . append ( bundle ) return self
7795	def unregister_fetcher ( self , object_class ) : self . _lock . acquire ( ) try : cache = self . _caches . get ( object_class ) if not cache : return cache . set_fetcher ( None ) finally : self . _lock . release ( )
13805	def merge_ordered ( ordereds : typing . Iterable [ typing . Any ] ) -> typing . Iterable [ typing . Any ] : seen_set = set ( ) add_seen = seen_set . add return reversed ( tuple ( map ( lambda obj : add_seen ( obj ) or obj , filterfalse ( seen_set . __contains__ , chain . from_iterable ( map ( reversed , reversed ( ordereds ) ) ) , ) , ) ) )
10622	def get_element_mass ( self , element ) : result = numpy . zeros ( 1 ) for compound in self . material . compounds : result += self . get_compound_mass ( compound ) * numpy . array ( stoich . element_mass_fractions ( compound , [ element ] ) ) return result [ 0 ]
10412	def summarize_node_filter ( graph : BELGraph , node_filters : NodePredicates ) -> None : passed = count_passed_node_filter ( graph , node_filters ) print ( '{}/{} nodes passed' . format ( passed , graph . number_of_nodes ( ) ) )
190	def deepcopy ( self , line_strings = None , shape = None ) : lss = self . line_strings if line_strings is None else line_strings shape = self . shape if shape is None else shape return LineStringsOnImage ( line_strings = [ ls . deepcopy ( ) for ls in lss ] , shape = tuple ( shape ) )
8094	def edges ( s , edges , alpha = 1.0 , weighted = False , directed = False ) : p = s . _ctx . BezierPath ( ) if directed and s . stroke : pd = s . _ctx . BezierPath ( ) if weighted and s . fill : pw = [ s . _ctx . BezierPath ( ) for i in range ( 11 ) ] if len ( edges ) == 0 : return for e in edges : try : s2 = e . node1 . graph . styles [ e . node1 . style ] except : s2 = s if s2 . edge : s2 . edge ( s2 , p , e , alpha ) if directed and s . stroke : s2 . edge_arrow ( s2 , pd , e , radius = 10 ) if weighted and s . fill : s2 . edge ( s2 , pw [ int ( e . weight * 10 ) ] , e , alpha ) s . _ctx . autoclosepath ( False ) s . _ctx . nofill ( ) s . _ctx . nostroke ( ) if weighted and s . fill : r = e . node1 . __class__ ( None ) . r s . _ctx . stroke ( s . fill . r , s . fill . g , s . fill . b , s . fill . a * 0.65 * alpha ) for w in range ( 1 , len ( pw ) ) : s . _ctx . strokewidth ( r * w * 0.1 ) s . _ctx . drawpath ( pw [ w ] . copy ( ) ) if s . stroke : s . _ctx . strokewidth ( s . strokewidth ) s . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * 0.65 * alpha ) s . _ctx . drawpath ( p . copy ( ) ) if directed and s . stroke : clr = s . _ctx . color ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * 0.65 * alpha ) clr . a *= 1.3 s . _ctx . stroke ( clr ) s . _ctx . drawpath ( pd . copy ( ) ) for e in edges : try : s2 = self . styles [ e . node1 . style ] except : s2 = s if s2 . edge_label : s2 . edge_label ( s2 , e , alpha )
680	def getAllRecords ( self ) : values = [ ] numRecords = self . fields [ 0 ] . numRecords assert ( all ( field . numRecords == numRecords for field in self . fields ) ) for x in range ( numRecords ) : values . append ( self . getRecord ( x ) ) return values
574	def clippedObj ( obj , maxElementSize = 64 ) : if hasattr ( obj , '_asdict' ) : obj = obj . _asdict ( ) if isinstance ( obj , dict ) : objOut = dict ( ) for key , val in obj . iteritems ( ) : objOut [ key ] = clippedObj ( val ) elif hasattr ( obj , '__iter__' ) : objOut = [ ] for val in obj : objOut . append ( clippedObj ( val ) ) else : objOut = str ( obj ) if len ( objOut ) > maxElementSize : objOut = objOut [ 0 : maxElementSize ] + '...' return objOut
9581	def eof ( fd ) : b = fd . read ( 1 ) end = len ( b ) == 0 if not end : curpos = fd . tell ( ) fd . seek ( curpos - 1 ) return end
3966	def case_insensitive_rename ( src , dst ) : temp_dir = tempfile . mkdtemp ( ) shutil . rmtree ( temp_dir ) shutil . move ( src , temp_dir ) shutil . move ( temp_dir , dst )
9968	def copy ( self , space = None , name = None ) : return Cells ( space = space , name = name , formula = self . formula )
6012	def load_exposure_time_map ( exposure_time_map_path , exposure_time_map_hdu , pixel_scale , shape , exposure_time , exposure_time_map_from_inverse_noise_map , inverse_noise_map ) : exposure_time_map_options = sum ( [ exposure_time_map_from_inverse_noise_map ] ) if exposure_time is not None and exposure_time_map_path is not None : raise exc . DataException ( 'You have supplied both a exposure_time_map_path to an exposure time map and an exposure time. Only' 'one quantity should be supplied.' ) if exposure_time_map_options == 0 : if exposure_time is not None and exposure_time_map_path is None : return ExposureTimeMap . single_value ( value = exposure_time , pixel_scale = pixel_scale , shape = shape ) elif exposure_time is None and exposure_time_map_path is not None : return ExposureTimeMap . from_fits_with_pixel_scale ( file_path = exposure_time_map_path , hdu = exposure_time_map_hdu , pixel_scale = pixel_scale ) else : if exposure_time_map_from_inverse_noise_map : return ExposureTimeMap . from_exposure_time_and_inverse_noise_map ( pixel_scale = pixel_scale , exposure_time = exposure_time , inverse_noise_map = inverse_noise_map )
7900	def process_configuration_success ( self , stanza ) : _unused = stanza self . configured = True self . handler . room_configured ( )
12580	def to_file ( self , outpath ) : if not self . has_mask ( ) and not self . is_smoothed ( ) : save_niigz ( outpath , self . img ) else : save_niigz ( outpath , self . get_data ( masked = True , smoothed = True ) , self . get_header ( ) , self . get_affine ( ) )
13647	def get_fuel_prices ( self ) -> GetFuelPricesResponse : response = requests . get ( '{}/prices' . format ( API_URL_BASE ) , headers = self . _get_headers ( ) , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) return GetFuelPricesResponse . deserialize ( response . json ( ) )
5835	def __convert_response_to_configuration ( self , result_blob , dataset_ids ) : builder = DataViewBuilder ( ) builder . dataset_ids ( dataset_ids ) for i , ( k , v ) in enumerate ( result_blob [ 'descriptors' ] . items ( ) ) : try : descriptor = self . __snake_case ( v [ 0 ] ) print ( json . dumps ( descriptor ) ) descriptor [ 'descriptor_key' ] = k builder . add_raw_descriptor ( descriptor ) except IndexError : pass for i , ( k , v ) in enumerate ( result_blob [ 'types' ] . items ( ) ) : builder . set_role ( k , v . lower ( ) ) return builder . build ( )
13029	def exploit_single ( self , ip , operating_system ) : result = None if "Windows Server 2008" in operating_system or "Windows 7" in operating_system : result = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'eternalblue_exploit7.py' ) , str ( ip ) , os . path . join ( self . datadir , 'final_combined.bin' ) , "12" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) elif "Windows Server 2012" in operating_system or "Windows 10" in operating_system or "Windows 8.1" in operating_system : result = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'eternalblue_exploit8.py' ) , str ( ip ) , os . path . join ( self . datadir , 'final_combined.bin' ) , "12" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) else : return [ "System target could not be automatically identified" ] return result . stdout . decode ( 'utf-8' ) . split ( '\n' )
4527	def report ( function , * args , ** kwds ) : try : function ( * args , ** kwds ) except Exception : traceback . print_exc ( )
9239	def timestring_to_datetime ( timestring ) : with warnings . catch_warnings ( ) : warnings . filterwarnings ( "ignore" , category = UnicodeWarning ) result = dateutil_parser ( timestring ) return result
1260	def restore_component ( self , component_name , save_path ) : component = self . get_component ( component_name = component_name ) self . _validate_savable ( component = component , component_name = component_name ) component . restore ( sess = self . session , save_path = save_path )
11681	def send ( self , command , timeout = 5 ) : logger . info ( u'Sending %s' % command ) _ , writable , __ = select . select ( [ ] , [ self . sock ] , [ ] , timeout ) if not writable : raise SendTimeoutError ( ) writable [ 0 ] . sendall ( command + '\n' )
1735	def _ensure_regexp ( source , n ) : markers = '(+~"\'=[%:?!*^|&-,;/\\' k = 0 while True : k += 1 if n - k < 0 : return True char = source [ n - k ] if char in markers : return True if char != ' ' and char != '\n' : break return False
13016	def hook ( name ) : def hookTarget ( wrapped ) : if not hasattr ( wrapped , '__hook__' ) : wrapped . __hook__ = [ name ] else : wrapped . __hook__ . append ( name ) return wrapped return hookTarget
6649	def inheritsFrom ( self , target_name ) : for t in self . hierarchy : if t and t . getName ( ) == target_name or target_name in t . description . get ( 'inherits' , { } ) : return True return False
10978	def leave ( group_id ) : group = Group . query . get_or_404 ( group_id ) if group . can_leave ( current_user ) : try : group . remove_member ( current_user ) except Exception as e : flash ( str ( e ) , "error" ) return redirect ( url_for ( '.index' ) ) flash ( _ ( 'You have successfully left %(group_name)s group.' , group_name = group . name ) , 'success' ) return redirect ( url_for ( '.index' ) ) flash ( _ ( 'You cannot leave the group %(group_name)s' , group_name = group . name ) , 'error' ) return redirect ( url_for ( '.index' ) )
5648	def _write_stop_to_stop_network_edges ( net , file_name , data = True , fmt = None ) : if fmt is None : fmt = "edg" if fmt == "edg" : if data : networkx . write_edgelist ( net , file_name , data = True ) else : networkx . write_edgelist ( net , file_name ) elif fmt == "csv" : with open ( file_name , 'w' ) as f : edge_iter = net . edges_iter ( data = True ) _ , _ , edg_data = next ( edge_iter ) edg_data_keys = list ( sorted ( edg_data . keys ( ) ) ) header = ";" . join ( [ "from_stop_I" , "to_stop_I" ] + edg_data_keys ) f . write ( header ) for from_node_I , to_node_I , data in net . edges_iter ( data = True ) : f . write ( "\n" ) values = [ str ( from_node_I ) , str ( to_node_I ) ] data_values = [ ] for key in edg_data_keys : if key == "route_I_counts" : route_I_counts_string = str ( data [ key ] ) . replace ( " " , "" ) [ 1 : - 1 ] data_values . append ( route_I_counts_string ) else : data_values . append ( str ( data [ key ] ) ) all_values = values + data_values f . write ( ";" . join ( all_values ) )
9654	def take_shas_of_all_files ( G , settings ) : global ERROR_FN sprint = settings [ "sprint" ] error = settings [ "error" ] ERROR_FN = error sha_dict = { } all_files = [ ] for target in G . nodes ( data = True ) : sprint ( "About to take shas of files in target '{}'" . format ( target [ 0 ] ) , level = "verbose" ) if 'dependencies' in target [ 1 ] : sprint ( "It has dependencies" , level = "verbose" ) deplist = [ ] for dep in target [ 1 ] [ 'dependencies' ] : glist = glob . glob ( dep ) if glist : for oneglob in glist : deplist . append ( oneglob ) else : deplist . append ( dep ) target [ 1 ] [ 'dependencies' ] = list ( deplist ) for dep in target [ 1 ] [ 'dependencies' ] : sprint ( " - {}" . format ( dep ) , level = "verbose" ) all_files . append ( dep ) if 'output' in target [ 1 ] : sprint ( "It has outputs" , level = "verbose" ) for out in acts . get_all_outputs ( target [ 1 ] ) : sprint ( " - {}" . format ( out ) , level = "verbose" ) all_files . append ( out ) if len ( all_files ) : sha_dict [ 'files' ] = { } extant_files = [ ] for item in all_files : if item not in extant_files and os . path . isfile ( item ) : extant_files . append ( item ) pool = Pool ( ) results = pool . map ( get_sha , extant_files ) pool . close ( ) pool . join ( ) for fn , sha in zip ( extant_files , results ) : sha_dict [ 'files' ] [ fn ] = { 'sha' : sha } return sha_dict sprint ( "No dependencies" , level = "verbose" )
9172	def includeme ( config ) : config . include ( 'pyramid_jinja2' ) config . add_jinja2_renderer ( '.html' ) config . add_jinja2_renderer ( '.rss' ) config . add_static_view ( name = '/a/static' , path = "cnxpublishing:static/" ) config . commit ( ) from cnxdb . ident_hash import join_ident_hash for ext in ( '.html' , '.rss' , ) : jinja2_env = config . get_jinja2_environment ( ext ) jinja2_env . globals . update ( join_ident_hash = join_ident_hash , ) declare_api_routes ( config ) declare_browsable_routes ( config )
9247	def generate_sub_section ( self , issues , prefix ) : log = "" if issues : if not self . options . simple_list : log += u"{0}\n\n" . format ( prefix ) for issue in issues : merge_string = self . get_string_for_issue ( issue ) log += u"- {0}\n" . format ( merge_string ) log += "\n" return log
11667	def linear ( Ks , dim , num_q , rhos , nus ) : r return _get_linear ( Ks , dim ) ( num_q , rhos , nus )
11176	def parse ( self , argv ) : if len ( argv ) < self . nargs : raise BadNumberOfArguments ( self . nargs , len ( argv ) ) if self . nargs == 1 : return self . parse_argument ( argv . pop ( 0 ) ) return [ self . parse_argument ( argv . pop ( 0 ) ) for tmp in range ( self . nargs ) ]
1774	def invalidate_cache ( cpu , address , size ) : cache = cpu . instruction_cache for offset in range ( size ) : if address + offset in cache : del cache [ address + offset ]
9629	def split_docstring ( value ) : docstring = textwrap . dedent ( getattr ( value , '__doc__' , '' ) ) if not docstring : return None pieces = docstring . strip ( ) . split ( '\n\n' , 1 ) try : body = pieces [ 1 ] except IndexError : body = None return Docstring ( pieces [ 0 ] , body )
6859	def create_database ( name , owner = None , owner_host = 'localhost' , charset = 'utf8' , collate = 'utf8_general_ci' , ** kwargs ) : with settings ( hide ( 'running' ) ) : query ( "CREATE DATABASE %(name)s CHARACTER SET %(charset)s COLLATE %(collate)s;" % { 'name' : name , 'charset' : charset , 'collate' : collate } , ** kwargs ) if owner : query ( "GRANT ALL PRIVILEGES ON %(name)s.* TO '%(owner)s'@'%(owner_host)s' WITH GRANT OPTION;" % { 'name' : name , 'owner' : owner , 'owner_host' : owner_host } , ** kwargs ) puts ( "Created MySQL database '%s'." % name )
6019	def from_inverse_noise_map ( cls , pixel_scale , inverse_noise_map ) : noise_map = 1.0 / inverse_noise_map return NoiseMap ( array = noise_map , pixel_scale = pixel_scale )
8771	def _add_default_tz_bindings ( self , context , switch , network_id ) : default_tz = CONF . NVP . default_tz if not default_tz : LOG . warn ( "additional_default_tz_types specified, " "but no default_tz. Skipping " "_add_default_tz_bindings()." ) return if not network_id : LOG . warn ( "neutron network_id not specified, skipping " "_add_default_tz_bindings()" ) return for net_type in CONF . NVP . additional_default_tz_types : if net_type in TZ_BINDINGS : binding = TZ_BINDINGS [ net_type ] binding . add ( context , switch , default_tz , network_id ) else : LOG . warn ( "Unknown default tz type %s" % ( net_type ) )
13768	def collect_files ( self ) : self . files = [ ] for bundle in self . bundles : bundle . init_build ( self , self . builder ) bundle_files = bundle . prepare ( ) self . files . extend ( bundle_files ) return self
8550	def update_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value if attr == 'source_mac' : data [ 'sourceMac' ] = value elif attr == 'source_ip' : data [ 'sourceIp' ] = value elif attr == 'target_ip' : data [ 'targetIp' ] = value elif attr == 'port_range_start' : data [ 'portRangeStart' ] = value elif attr == 'port_range_end' : data [ 'portRangeEnd' ] = value elif attr == 'icmp_type' : data [ 'icmpType' ] = value elif attr == 'icmp_code' : data [ 'icmpCode' ] = value else : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s/firewallrules/%s' % ( datacenter_id , server_id , nic_id , firewall_rule_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
5315	def colorpalette ( self , colorpalette ) : if isinstance ( colorpalette , str ) : colorpalette = colors . parse_colors ( colorpalette ) self . _colorpalette = colors . sanitize_color_palette ( colorpalette )
11307	def map_attr ( self , mapping , attr , obj ) : if attr not in mapping and hasattr ( self , attr ) : if not callable ( getattr ( self , attr ) ) : mapping [ attr ] = getattr ( self , attr ) else : mapping [ attr ] = getattr ( self , attr ) ( obj )
5906	def create_portable_topology ( topol , struct , ** kwargs ) : _topoldir , _topol = os . path . split ( topol ) processed = kwargs . pop ( 'processed' , os . path . join ( _topoldir , 'pp_' + _topol ) ) grompp_kwargs , mdp_kwargs = filter_grompp_options ( ** kwargs ) mdp_kwargs = add_mdp_includes ( topol , mdp_kwargs ) with tempfile . NamedTemporaryFile ( suffix = '.mdp' ) as mdp : mdp . write ( '; empty mdp file\ninclude = {include!s}\n' . format ( ** mdp_kwargs ) ) mdp . flush ( ) grompp_kwargs [ 'p' ] = topol grompp_kwargs [ 'pp' ] = processed grompp_kwargs [ 'f' ] = mdp . name grompp_kwargs [ 'c' ] = struct grompp_kwargs [ 'v' ] = False try : gromacs . grompp ( ** grompp_kwargs ) finally : utilities . unlink_gmx ( 'topol.tpr' , 'mdout.mdp' ) return utilities . realpath ( processed )
297	def plot_return_quantiles ( returns , live_start_date = None , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) is_returns = returns if live_start_date is None else returns . loc [ returns . index < live_start_date ] is_weekly = ep . aggregate_returns ( is_returns , 'weekly' ) is_monthly = ep . aggregate_returns ( is_returns , 'monthly' ) sns . boxplot ( data = [ is_returns , is_weekly , is_monthly ] , palette = [ "#4c72B0" , "#55A868" , "#CCB974" ] , ax = ax , ** kwargs ) if live_start_date is not None : oos_returns = returns . loc [ returns . index >= live_start_date ] oos_weekly = ep . aggregate_returns ( oos_returns , 'weekly' ) oos_monthly = ep . aggregate_returns ( oos_returns , 'monthly' ) sns . swarmplot ( data = [ oos_returns , oos_weekly , oos_monthly ] , ax = ax , color = "red" , marker = "d" , ** kwargs ) red_dots = matplotlib . lines . Line2D ( [ ] , [ ] , color = "red" , marker = "d" , label = "Out-of-sample data" , linestyle = '' ) ax . legend ( handles = [ red_dots ] , frameon = True , framealpha = 0.5 ) ax . set_xticklabels ( [ 'Daily' , 'Weekly' , 'Monthly' ] ) ax . set_title ( 'Return quantiles' ) return ax
440	def print_layers ( self ) : for i , layer in enumerate ( self . all_layers ) : logging . info ( " layer {:3}: {:20} {:15} {}" . format ( i , layer . name , str ( layer . get_shape ( ) ) , layer . dtype . name ) )
12691	def write_table_pair_potential ( func , dfunc = None , bounds = ( 1.0 , 10.0 ) , samples = 1000 , tollerance = 1e-6 , keyword = 'PAIR' ) : r_min , r_max = bounds if dfunc is None : dfunc = lambda r : ( func ( r + tollerance ) - func ( r - tollerance ) ) / ( 2 * tollerance ) i = np . arange ( 1 , samples + 1 ) r = np . linspace ( r_min , r_max , samples ) forces = func ( r ) energies = dfunc ( r ) lines = [ '%d %f %f %f\n' % ( index , radius , force , energy ) for index , radius , force , energy in zip ( i , r , forces , energies ) ] return "%s\nN %d\n\n" % ( keyword , samples ) + '' . join ( lines )
1684	def Begin ( self , function_name ) : self . in_a_function = True self . lines_in_function = 0 self . current_function = function_name
11679	def connect ( self ) : try : logger . info ( u'Connecting %s:%d' % ( self . host , self . port ) ) self . sock . connect ( ( self . host , self . port ) ) except socket . error : raise ConnectionError ( ) self . state = CONNECTED
10007	def get_object ( self , name ) : parts = name . split ( "." ) space = self . spaces [ parts . pop ( 0 ) ] if parts : return space . get_object ( "." . join ( parts ) ) else : return space
6420	def readfile ( fn ) : with open ( path . join ( HERE , fn ) , 'r' , encoding = 'utf-8' ) as f : return f . read ( )
8289	def sbot_executable ( ) : gsettings = load_gsettings ( ) venv = gsettings . get_string ( 'current-virtualenv' ) if venv == 'Default' : sbot = which ( 'sbot' ) elif venv == 'System' : env_venv = os . environ . get ( 'VIRTUAL_ENV' ) if not env_venv : return which ( 'sbot' ) for p in os . environ [ 'PATH' ] . split ( os . path . pathsep ) : sbot = '%s/sbot' % p if not p . startswith ( env_venv ) and os . path . isfile ( sbot ) : return sbot else : sbot = os . path . join ( venv , 'bin/sbot' ) if not os . path . isfile ( sbot ) : print ( 'Shoebot not found, reverting to System shoebot' ) sbot = which ( 'sbot' ) return os . path . realpath ( sbot )
10801	def _distance_matrix ( self , a , b ) : def sq ( x ) : return ( x * x ) matrix = sq ( a [ : , 0 ] [ : , None ] - b [ : , 0 ] [ None , : ] ) for x , y in zip ( a . T [ 1 : ] , b . T [ 1 : ] ) : matrix += sq ( x [ : , None ] - y [ None , : ] ) return matrix
4629	def from_pubkey ( cls , pubkey , compressed = True , version = 56 , prefix = None ) : pubkey = PublicKey ( pubkey , prefix = prefix or Prefix . prefix ) if compressed : pubkey_plain = pubkey . compressed ( ) else : pubkey_plain = pubkey . uncompressed ( ) sha = hashlib . sha256 ( unhexlify ( pubkey_plain ) ) . hexdigest ( ) rep = hexlify ( ripemd160 ( sha ) ) . decode ( "ascii" ) s = ( "%.2x" % version ) + rep result = s + hexlify ( doublesha256 ( s ) [ : 4 ] ) . decode ( "ascii" ) result = hexlify ( ripemd160 ( result ) ) . decode ( "ascii" ) return cls ( result , prefix = pubkey . prefix )
13696	def parse_int ( s ) : try : val = int ( s ) except ValueError : print_err ( '\nInvalid integer: {}' . format ( s ) ) sys . exit ( 1 ) return val
9378	def is_valid_file ( filename ) : if os . path . exists ( filename ) : if not os . path . getsize ( filename ) : logger . warning ( '%s : file is empty.' , filename ) return False else : logger . warning ( '%s : file does not exist.' , filename ) return False return True
1250	def do_action ( self , action ) : temp_state = np . rot90 ( self . _state , action ) reward = self . _do_action_left ( temp_state ) self . _state = np . rot90 ( temp_state , - action ) self . _score += reward self . add_random_tile ( ) return reward
9145	def clear ( skip ) : for name in sorted ( MODULES ) : if name in skip : continue click . secho ( f'clearing cache for {name}' , fg = 'cyan' , bold = True ) clear_cache ( name )
7945	def _continue_connect ( self ) : try : self . _socket . connect ( self . _dst_addr ) except socket . error , err : logger . debug ( "Connect error: {0}" . format ( err ) ) if err . args [ 0 ] == errno . EISCONN : pass elif err . args [ 0 ] in BLOCKING_ERRORS : return None elif self . _dst_addrs : self . _set_state ( "connect" ) return None elif self . _dst_nameports : self . _set_state ( "resolve-hostname" ) return None else : self . _socket . close ( ) self . _socket = None self . _set_state ( "aborted" ) raise self . _connected ( )
12536	def get_dcm_reader ( store_metadata = True , header_fields = None ) : if not store_metadata : return lambda fpath : fpath if header_fields is None : build_dcm = lambda fpath : DicomFile ( fpath ) else : dicom_header = namedtuple ( 'DicomHeader' , header_fields ) build_dcm = lambda fpath : dicom_header . _make ( DicomFile ( fpath ) . get_attributes ( header_fields ) ) return build_dcm
475	def sentence_to_token_ids ( sentence , vocabulary , tokenizer = None , normalize_digits = True , UNK_ID = 3 , _DIGIT_RE = re . compile ( br"\d" ) ) : if tokenizer : words = tokenizer ( sentence ) else : words = basic_tokenizer ( sentence ) if not normalize_digits : return [ vocabulary . get ( w , UNK_ID ) for w in words ] return [ vocabulary . get ( re . sub ( _DIGIT_RE , b"0" , w ) , UNK_ID ) for w in words ]
2319	def autoset_settings ( set_var ) : try : devices = ast . literal_eval ( os . environ [ "CUDA_VISIBLE_DEVICES" ] ) if type ( devices ) != list and type ( devices ) != tuple : devices = [ devices ] if len ( devices ) != 0 : set_var . GPU = len ( devices ) set_var . NB_JOBS = len ( devices ) warnings . warn ( "Detecting CUDA devices : {}" . format ( devices ) ) except KeyError : set_var . GPU = check_cuda_devices ( ) set_var . NB_JOBS = set_var . GPU warnings . warn ( "Detecting {} CUDA devices." . format ( set_var . GPU ) ) if not set_var . GPU : warnings . warn ( "No GPU automatically detected. Setting SETTINGS.GPU to 0, " + "and SETTINGS.NB_JOBS to cpu_count." ) set_var . GPU = 0 set_var . NB_JOBS = multiprocessing . cpu_count ( ) return set_var
6930	def xmatch_cplist_external_catalogs ( cplist , xmatchpkl , xmatchradiusarcsec = 2.0 , updateexisting = True , resultstodir = None ) : with open ( xmatchpkl , 'rb' ) as infd : xmd = pickle . load ( infd ) status_dict = { } for cpf in cplist : cpd = _read_checkplot_picklefile ( cpf ) try : xmatch_external_catalogs ( cpd , xmd , xmatchradiusarcsec = xmatchradiusarcsec , updatexmatch = updateexisting ) for xmi in cpd [ 'xmatch' ] : if cpd [ 'xmatch' ] [ xmi ] [ 'found' ] : LOGINFO ( 'checkplot %s: %s matched to %s, ' 'match dist: %s arcsec' % ( os . path . basename ( cpf ) , cpd [ 'objectid' ] , cpd [ 'xmatch' ] [ xmi ] [ 'name' ] , cpd [ 'xmatch' ] [ xmi ] [ 'distarcsec' ] ) ) if not resultstodir : outcpf = _write_checkplot_picklefile ( cpd , outfile = cpf ) else : xcpf = os . path . join ( resultstodir , os . path . basename ( cpf ) ) outcpf = _write_checkplot_picklefile ( cpd , outfile = xcpf ) status_dict [ cpf ] = outcpf except Exception as e : LOGEXCEPTION ( 'failed to match objects for %s' % cpf ) status_dict [ cpf ] = None return status_dict
2339	def weighted_mean_and_std ( values , weights ) : average = np . average ( values , weights = weights , axis = 0 ) variance = np . dot ( weights , ( values - average ) ** 2 ) / weights . sum ( ) return ( average , np . sqrt ( variance ) )
9140	def find_best_label_for_type ( labels , language , labeltype ) : typelabels = [ l for l in labels if l . type == labeltype ] if not typelabels : return False if language == 'any' : return typelabels [ 0 ] exact = filter_labels_by_language ( typelabels , language ) if exact : return exact [ 0 ] inexact = filter_labels_by_language ( typelabels , language , True ) if inexact : return inexact [ 0 ] return False
5302	def parse_rgb_txt_file ( path ) : color_dict = { } with open ( path , 'r' ) as rgb_txt : for line in rgb_txt : line = line . strip ( ) if not line or line . startswith ( '!' ) : continue parts = line . split ( ) color_dict [ " " . join ( parts [ 3 : ] ) ] = ( int ( parts [ 0 ] ) , int ( parts [ 1 ] ) , int ( parts [ 2 ] ) ) return color_dict
10511	def onwindowcreate ( self , window_name , fn_name , * args ) : self . _pollEvents . _callback [ window_name ] = [ "onwindowcreate" , fn_name , args ] return self . _remote_onwindowcreate ( window_name )
1184	def match ( self , context ) : while context . remaining_codes ( ) > 0 and context . has_matched is None : opcode = context . peek_code ( ) if not self . dispatch ( opcode , context ) : return None if context . has_matched is None : context . has_matched = False return context . has_matched
573	def rApply ( d , f ) : remainingDicts = [ ( d , ( ) ) ] while len ( remainingDicts ) > 0 : current , prevKeys = remainingDicts . pop ( ) for k , v in current . iteritems ( ) : keys = prevKeys + ( k , ) if isinstance ( v , dict ) : remainingDicts . insert ( 0 , ( v , keys ) ) else : f ( v , keys )
11251	def get_percentage ( a , b , i = False , r = False ) : if i is False and r is True : percentage = round ( 100.0 * ( float ( a ) / b ) , 2 ) elif ( i is True and r is True ) or ( i is True and r is False ) : percentage = int ( round ( 100 * ( float ( a ) / b ) ) ) if r is False : warnings . warn ( "If integer is set to True and Round is set to False, you will still get a rounded number if you pass floating point numbers as arguments." ) else : percentage = 100.0 * ( float ( a ) / b ) return percentage
9005	def add_new_pattern ( self , id_ , name = None ) : if name is None : name = id_ pattern = self . _parser . new_pattern ( id_ , name ) self . _patterns . append ( pattern ) return pattern
162	def get_pointwise_inside_image_mask ( self , image ) : if len ( self . coords ) == 0 : return np . zeros ( ( 0 , ) , dtype = bool ) shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] x_within = np . logical_and ( 0 <= self . xx , self . xx < width ) y_within = np . logical_and ( 0 <= self . yy , self . yy < height ) return np . logical_and ( x_within , y_within )
660	def percentOutputsStableOverNTimeSteps ( vectors , numSamples = None ) : totalSamples = len ( vectors ) windowSize = numSamples numWindows = 0 pctStable = 0 for wStart in range ( 0 , totalSamples - windowSize + 1 ) : data = vectors [ wStart : wStart + windowSize ] outputSums = data . sum ( axis = 0 ) stableOutputs = ( outputSums == windowSize ) . sum ( ) samplePctStable = float ( stableOutputs ) / data [ 0 ] . sum ( ) print samplePctStable pctStable += samplePctStable numWindows += 1 return float ( pctStable ) / numWindows
12309	def auto_add ( repo , autooptions , files ) : mapping = { "." : "" } if ( ( 'import' in autooptions ) and ( 'directory-mapping' in autooptions [ 'import' ] ) ) : mapping = autooptions [ 'import' ] [ 'directory-mapping' ] keys = mapping . keys ( ) keys = sorted ( keys , key = lambda k : len ( k ) , reverse = True ) count = 0 params = [ ] for f in files : relativepath = f for k in keys : v = mapping [ k ] if f . startswith ( k + "/" ) : relativepath = f . replace ( k + "/" , v ) break count += files_add ( repo = repo , args = [ f ] , targetdir = os . path . dirname ( relativepath ) ) return count
7029	def generalized_lsp_value_notau ( times , mags , errs , omega ) : one_over_errs2 = 1.0 / ( errs * errs ) W = npsum ( one_over_errs2 ) wi = one_over_errs2 / W sin_omegat = npsin ( omega * times ) cos_omegat = npcos ( omega * times ) sin2_omegat = sin_omegat * sin_omegat cos2_omegat = cos_omegat * cos_omegat sincos_omegat = sin_omegat * cos_omegat Y = npsum ( wi * mags ) C = npsum ( wi * cos_omegat ) S = npsum ( wi * sin_omegat ) YpY = npsum ( wi * mags * mags ) YpC = npsum ( wi * mags * cos_omegat ) YpS = npsum ( wi * mags * sin_omegat ) CpC = npsum ( wi * cos2_omegat ) CpS = npsum ( wi * sincos_omegat ) YY = YpY - Y * Y YC = YpC - Y * C YS = YpS - Y * S CC = CpC - C * C SS = 1 - CpC - S * S CS = CpS - C * S Domega = CC * SS - CS * CS lspval = ( SS * YC * YC + CC * YS * YS - 2.0 * CS * YC * YS ) / ( YY * Domega ) return lspval
10413	def node_inclusion_filter_builder ( nodes : Iterable [ BaseEntity ] ) -> NodePredicate : node_set = set ( nodes ) def inclusion_filter ( _ : BELGraph , node : BaseEntity ) -> bool : return node in node_set return inclusion_filter
11040	def write ( self , path , ** data ) : d = self . request ( 'PUT' , '/v1/' + path , json = data ) return d . addCallback ( self . _handle_response , check_cas = True )
5074	def track_enrollment ( pathway , user_id , course_run_id , url_path = None ) : track_event ( user_id , 'edx.bi.user.enterprise.onboarding' , { 'pathway' : pathway , 'url_path' : url_path , 'course_run_id' : course_run_id , } )
10793	def separate_particles_into_groups ( s , region_size = 40 , bounds = None ) : imtile = ( s . oshape . translate ( - s . pad ) if bounds is None else util . Tile ( bounds [ 0 ] , bounds [ 1 ] ) ) region = util . Tile ( region_size , dim = s . dim ) trange = np . ceil ( imtile . shape . astype ( 'float' ) / region . shape ) translations = util . Tile ( trange ) . coords ( form = 'vector' ) translations = translations . reshape ( - 1 , translations . shape [ - 1 ] ) groups = [ ] positions = s . obj_get_positions ( ) for v in translations : tmptile = region . copy ( ) . translate ( region . shape * v - s . pad ) groups . append ( find_particles_in_tile ( positions , tmptile ) ) return [ g for g in groups if len ( g ) > 0 ]
3563	def find_descriptor ( self , uuid ) : for desc in self . list_descriptors ( ) : if desc . uuid == uuid : return desc return None
12372	def chop ( list_ , n ) : "Chop list_ into n chunks. Returns a list." size = len ( list_ ) each = size // n if each == 0 : return [ list_ ] chopped = [ ] for i in range ( n ) : start = i * each end = ( i + 1 ) * each if i == ( n - 1 ) : end = size chopped . append ( list_ [ start : end ] ) return chopped
7628	def namespace ( ns_key ) : if ns_key not in __NAMESPACE__ : raise NamespaceError ( 'Unknown namespace: {:s}' . format ( ns_key ) ) sch = copy . deepcopy ( JAMS_SCHEMA [ 'definitions' ] [ 'SparseObservation' ] ) for key in [ 'value' , 'confidence' ] : try : sch [ 'properties' ] [ key ] = __NAMESPACE__ [ ns_key ] [ key ] except KeyError : pass return sch
12938	def setDefaultRedisConnectionParams ( connectionParams ) : global _defaultRedisConnectionParams _defaultRedisConnectionParams . clear ( ) for key , value in connectionParams . items ( ) : _defaultRedisConnectionParams [ key ] = value clearRedisPools ( )
10197	def _handle_request ( self , scheme , netloc , path , headers , body = None , method = "GET" ) : backend_url = "{}://{}{}" . format ( scheme , netloc , path ) try : response = self . http_request . request ( backend_url , method = method , body = body , headers = dict ( headers ) ) self . _return_response ( response ) except Exception as e : body = "Invalid response from backend: '{}' Server might be busy" . format ( e . message ) logging . debug ( body ) self . send_error ( httplib . SERVICE_UNAVAILABLE , body )
7923	def as_unicode ( self ) : result = self . domain if self . local : result = self . local + u'@' + result if self . resource : result = result + u'/' + self . resource if not JID . cache . has_key ( result ) : JID . cache [ result ] = self return result
3743	def ViswanathNatarajan2 ( T , A , B ) : mu = exp ( A + B / T ) mu = mu / 1000. mu = mu * 10 return mu
5155	def type_cast ( self , item , schema = None ) : if schema is None : schema = self . _schema properties = schema [ 'properties' ] for key , value in item . items ( ) : if key not in properties : continue try : json_type = properties [ key ] [ 'type' ] except KeyError : json_type = None if json_type == 'integer' and not isinstance ( value , int ) : value = int ( value ) elif json_type == 'boolean' and not isinstance ( value , bool ) : value = value == '1' item [ key ] = value return item
7736	def map ( self , data ) : result = [ ] for char in data : ret = None for lookup in self . mapping : ret = lookup ( char ) if ret is not None : break if ret is not None : result . append ( ret ) else : result . append ( char ) return result
12879	def sep1 ( parser , separator ) : first = [ parser ( ) ] def inner ( ) : separator ( ) return parser ( ) return first + many ( tri ( inner ) )
13329	def remove ( path ) : r = cpenv . resolve ( path ) if isinstance ( r . resolved [ 0 ] , cpenv . VirtualEnvironment ) : EnvironmentCache . discard ( r . resolved [ 0 ] ) EnvironmentCache . save ( )
3918	def _handle_event ( self , conv_event ) : if not self . _is_scrolling : self . set_focus ( conv_event . id_ ) else : self . _modified ( )
12605	def _to_string ( data ) : sdata = data . copy ( ) for k , v in data . items ( ) : if isinstance ( v , datetime ) : sdata [ k ] = timestamp_to_date_str ( v ) elif not isinstance ( v , ( string_types , float , int ) ) : sdata [ k ] = str ( v ) return sdata
8691	def put ( self , key ) : self . client . write ( self . _key_path ( key [ 'name' ] ) , ** key ) return self . _key_path ( key [ 'name' ] )
9138	def drop_all ( self , check_first : bool = True ) : self . _metadata . drop_all ( self . engine , checkfirst = check_first ) self . _store_drop ( )
10128	def update ( self , dt ) : self . translate ( dt * self . velocity ) self . rotate ( dt * self . angular_velocity )
12415	def send ( self , * args , ** kwargs ) : self . write ( * args , ** kwargs ) self . flush ( )
8576	def get_request ( self , request_id , status = False ) : if status : response = self . _perform_request ( '/requests/' + request_id + '/status' ) else : response = self . _perform_request ( '/requests/%s' % request_id ) return response
3401	def find_external_compartment ( model ) : if model . boundary : counts = pd . Series ( tuple ( r . compartments ) [ 0 ] for r in model . boundary ) most = counts . value_counts ( ) most = most . index [ most == most . max ( ) ] . to_series ( ) else : most = None like_external = compartment_shortlist [ "e" ] + [ "e" ] matches = pd . Series ( [ co in like_external for co in model . compartments ] , index = model . compartments ) if matches . sum ( ) == 1 : compartment = matches . index [ matches ] [ 0 ] LOGGER . info ( "Compartment `%s` sounds like an external compartment. " "Using this one without counting boundary reactions" % compartment ) return compartment elif most is not None and matches . sum ( ) > 1 and matches [ most ] . sum ( ) == 1 : compartment = most [ matches [ most ] ] [ 0 ] LOGGER . warning ( "There are several compartments that look like an " "external compartment but `%s` has the most boundary " "reactions, so using that as the external " "compartment." % compartment ) return compartment elif matches . sum ( ) > 1 : raise RuntimeError ( "There are several compartments (%s) that look " "like external compartments but we can't tell " "which one to use. Consider renaming your " "compartments please." ) if most is not None : return most [ 0 ] LOGGER . warning ( "Could not identify an external compartment by name and" " choosing one with the most boundary reactions. That " "might be complete nonsense or change suddenly. " "Consider renaming your compartments using " "`Model.compartments` to fix this." ) raise RuntimeError ( "The heuristic for discovering an external compartment " "relies on names and boundary reactions. Yet, there " "are neither compartments with recognized names nor " "boundary reactions in the model." )
803	def modelsGetResultAndStatus ( self , modelIDs ) : assert isinstance ( modelIDs , self . _SEQUENCE_TYPES ) , ( "Wrong modelIDs type: %r" ) % type ( modelIDs ) assert len ( modelIDs ) >= 1 , "modelIDs is empty" rows = self . _getMatchingRowsWithRetries ( self . _models , { 'model_id' : modelIDs } , [ self . _models . pubToDBNameDict [ f ] for f in self . _models . getResultAndStatusNamedTuple . _fields ] ) assert len ( rows ) == len ( modelIDs ) , "Didn't find modelIDs: %r" % ( ( set ( modelIDs ) - set ( r [ 0 ] for r in rows ) ) , ) return [ self . _models . getResultAndStatusNamedTuple . _make ( r ) for r in rows ]
3620	def unregister ( self , model ) : if not self . is_registered ( model ) : raise RegistrationError ( '{} is not registered with Algolia engine' . format ( model ) ) del self . __registered_models [ model ] post_save . disconnect ( self . __post_save_receiver , model ) pre_delete . disconnect ( self . __pre_delete_receiver , model ) logger . info ( 'UNREGISTER %s' , model )
11892	def update ( self ) : bulbs = self . _hub . get_lights ( ) if not bulbs : _LOGGER . debug ( "%s is offline, send command failed" , self . _zid ) self . _online = False
9748	async def choose_qtm_instance ( interface ) : instances = { } print ( "Available QTM instances:" ) async for i , qtm_instance in AsyncEnumerate ( qtm . Discover ( interface ) , start = 1 ) : instances [ i ] = qtm_instance print ( "{} - {}" . format ( i , qtm_instance . info ) ) try : choice = int ( input ( "Connect to: " ) ) if choice not in instances : raise ValueError except ValueError : LOG . error ( "Invalid choice" ) return None return instances [ choice ] . host
13485	def showhtml ( ) : import webbrowser opts = options docroot = path ( opts . get ( 'docroot' , 'docs' ) ) if not docroot . exists ( ) : raise BuildFailure ( "Sphinx documentation root (%s) does not exist." % docroot ) builddir = docroot / opts . get ( "builddir" , ".build" ) builddir = builddir / 'html' if not builddir . exists ( ) : raise BuildFailure ( "Sphinx build directory (%s) does not exist." % builddir ) webbrowser . open ( builddir / 'index.html' )
3698	def Hsub ( T = 298.15 , P = 101325 , MW = None , AvailableMethods = False , Method = None , CASRN = '' ) : def list_methods ( ) : methods = [ ] if CASRN in GharagheiziHsub_data . index : methods . append ( 'Ghazerati Appendix, at 298K' ) methods . append ( 'None' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'Ghazerati Appendix, at 298K' : _Hsub = float ( GharagheiziHsub_data . at [ CASRN , 'Hsub' ] ) elif Method == 'None' or not _Hsub or not MW : return None else : raise Exception ( 'Failure in in function' ) _Hsub = property_molar_to_mass ( _Hsub , MW ) return _Hsub
2661	def hold_worker ( self , worker_id ) : c = self . command_client . run ( "HOLD_WORKER;{}" . format ( worker_id ) ) logger . debug ( "Sent hold request to worker: {}" . format ( worker_id ) ) return c
4244	def _get_record ( self , ipnum ) : seek_country = self . _seek_country ( ipnum ) if seek_country == self . _databaseSegments : return { } read_length = ( 2 * self . _recordLength - 1 ) * self . _databaseSegments try : self . _lock . acquire ( ) self . _fp . seek ( seek_country + read_length , os . SEEK_SET ) buf = self . _fp . read ( const . FULL_RECORD_LENGTH ) finally : self . _lock . release ( ) if PY3 and type ( buf ) is bytes : buf = buf . decode ( ENCODING ) record = { 'dma_code' : 0 , 'area_code' : 0 , 'metro_code' : None , 'postal_code' : None } latitude = 0 longitude = 0 char = ord ( buf [ 0 ] ) record [ 'country_code' ] = const . COUNTRY_CODES [ char ] record [ 'country_code3' ] = const . COUNTRY_CODES3 [ char ] record [ 'country_name' ] = const . COUNTRY_NAMES [ char ] record [ 'continent' ] = const . CONTINENT_NAMES [ char ] def read_data ( buf , pos ) : cur = pos while buf [ cur ] != '\0' : cur += 1 return cur , buf [ pos : cur ] if cur > pos else None offset , record [ 'region_code' ] = read_data ( buf , 1 ) offset , record [ 'city' ] = read_data ( buf , offset + 1 ) offset , record [ 'postal_code' ] = read_data ( buf , offset + 1 ) offset = offset + 1 for j in range ( 3 ) : latitude += ( ord ( buf [ offset + j ] ) << ( j * 8 ) ) for j in range ( 3 ) : longitude += ( ord ( buf [ offset + j + 3 ] ) << ( j * 8 ) ) record [ 'latitude' ] = ( latitude / 10000.0 ) - 180.0 record [ 'longitude' ] = ( longitude / 10000.0 ) - 180.0 if self . _databaseType in ( const . CITY_EDITION_REV1 , const . CITY_EDITION_REV1_V6 ) : if record [ 'country_code' ] == 'US' : dma_area = 0 for j in range ( 3 ) : dma_area += ord ( buf [ offset + j + 6 ] ) << ( j * 8 ) record [ 'dma_code' ] = int ( floor ( dma_area / 1000 ) ) record [ 'area_code' ] = dma_area % 1000 record [ 'metro_code' ] = const . DMA_MAP . get ( record [ 'dma_code' ] ) params = ( record [ 'country_code' ] , record [ 'region_code' ] ) record [ 'time_zone' ] = time_zone_by_country_and_region ( * params ) return record
2501	def value_error ( self , key , bad_value ) : msg = ERROR_MESSAGES [ key ] . format ( bad_value ) self . logger . log ( msg ) self . error = True
5551	def snap_bounds ( bounds = None , pyramid = None , zoom = None ) : if not isinstance ( bounds , ( tuple , list ) ) : raise TypeError ( "bounds must be either a tuple or a list" ) if len ( bounds ) != 4 : raise ValueError ( "bounds has to have exactly four values" ) if not isinstance ( pyramid , BufferedTilePyramid ) : raise TypeError ( "pyramid has to be a BufferedTilePyramid" ) bounds = Bounds ( * bounds ) lb = pyramid . tile_from_xy ( bounds . left , bounds . bottom , zoom , on_edge_use = "rt" ) . bounds rt = pyramid . tile_from_xy ( bounds . right , bounds . top , zoom , on_edge_use = "lb" ) . bounds return Bounds ( lb . left , lb . bottom , rt . right , rt . top )
499	def _deleteRecordsFromKNN ( self , recordsToDelete ) : prototype_idx = self . _knnclassifier . getParameter ( 'categoryRecencyList' ) idsToDelete = ( [ r . ROWID for r in recordsToDelete if not r . setByUser and r . ROWID in prototype_idx ] ) nProtos = self . _knnclassifier . _knn . _numPatterns self . _knnclassifier . _knn . removeIds ( idsToDelete ) assert self . _knnclassifier . _knn . _numPatterns == nProtos - len ( idsToDelete )
1571	def submit_tar ( cl_args , unknown_args , tmp_dir ) : topology_file = cl_args [ 'topology-file-name' ] java_defines = cl_args [ 'topology_main_jvm_property' ] main_class = cl_args [ 'topology-class-name' ] res = execute . heron_tar ( main_class , topology_file , tuple ( unknown_args ) , tmp_dir , java_defines ) result . render ( res ) if not result . is_successful ( res ) : err_context = ( "Failed to create topology definition " "file when executing class '%s' of file '%s'" ) % ( main_class , topology_file ) res . add_context ( err_context ) return res return launch_topologies ( cl_args , topology_file , tmp_dir )
1074	def getphraselist ( self ) : plist = [ ] while self . pos < len ( self . field ) : if self . field [ self . pos ] in self . LWS : self . pos += 1 elif self . field [ self . pos ] == '"' : plist . append ( self . getquote ( ) ) elif self . field [ self . pos ] == '(' : self . commentlist . append ( self . getcomment ( ) ) elif self . field [ self . pos ] in self . phraseends : break else : plist . append ( self . getatom ( self . phraseends ) ) return plist
55	def copy ( self , keypoints = None , shape = None ) : result = copy . copy ( self ) if keypoints is not None : result . keypoints = keypoints if shape is not None : result . shape = shape return result
9661	def find_standard_sakefile ( settings ) : error = settings [ "error" ] if settings [ "customsake" ] : custom = settings [ "customsake" ] if not os . path . isfile ( custom ) : error ( "Specified sakefile '{}' doesn't exist" , custom ) sys . exit ( 1 ) return custom for name in [ "Sakefile" , "Sakefile.yaml" , "Sakefile.yml" ] : if os . path . isfile ( name ) : return name error ( "Error: there is no Sakefile to read" ) sys . exit ( 1 )
12925	def match_to_clinvar ( genome_file , clin_file ) : clin_curr_line = _next_line ( clin_file ) genome_curr_line = _next_line ( genome_file ) while clin_curr_line . startswith ( '#' ) : clin_curr_line = _next_line ( clin_file ) while genome_curr_line . startswith ( '#' ) : genome_curr_line = _next_line ( genome_file ) while clin_curr_line and genome_curr_line : clin_curr_pos = VCFLine . get_pos ( clin_curr_line ) genome_curr_pos = VCFLine . get_pos ( genome_curr_line ) try : if clin_curr_pos [ 'chrom' ] > genome_curr_pos [ 'chrom' ] : genome_curr_line = _next_line ( genome_file ) continue elif clin_curr_pos [ 'chrom' ] < genome_curr_pos [ 'chrom' ] : clin_curr_line = _next_line ( clin_file ) continue if clin_curr_pos [ 'pos' ] > genome_curr_pos [ 'pos' ] : genome_curr_line = _next_line ( genome_file ) continue elif clin_curr_pos [ 'pos' ] < genome_curr_pos [ 'pos' ] : clin_curr_line = _next_line ( clin_file ) continue except StopIteration : break genome_vcf_line = GenomeVCFLine ( vcf_line = genome_curr_line , skip_info = True ) if not genome_vcf_line . genotype_allele_indexes : genome_curr_line = _next_line ( genome_file ) continue clinvar_vcf_line = ClinVarVCFLine ( vcf_line = clin_curr_line ) if not genome_vcf_line . ref_allele == clinvar_vcf_line . ref_allele : try : genome_curr_line = _next_line ( genome_file ) clin_curr_line = _next_line ( clin_file ) continue except StopIteration : break genotype_allele_indexes = genome_vcf_line . genotype_allele_indexes genome_alleles = [ genome_vcf_line . alleles [ x ] for x in genotype_allele_indexes ] if len ( genome_alleles ) == 1 : zygosity = 'Hem' elif len ( genome_alleles ) == 2 : if genome_alleles [ 0 ] . sequence == genome_alleles [ 1 ] . sequence : zygosity = 'Hom' genome_alleles = [ genome_alleles [ 0 ] ] else : zygosity = 'Het' else : raise ValueError ( 'This code only expects to work on genomes ' + 'with one or two alleles called at each ' + 'location. The following line violates this:' + str ( genome_vcf_line ) ) for genome_allele in genome_alleles : for allele in clinvar_vcf_line . alleles : if genome_allele . sequence == allele . sequence : if hasattr ( allele , 'records' ) : yield ( genome_vcf_line , allele , zygosity ) try : genome_curr_line = _next_line ( genome_file ) clin_curr_line = _next_line ( clin_file ) except StopIteration : break
9625	def register ( self , cls ) : preview = cls ( site = self ) logger . debug ( 'Registering %r with %r' , preview , self ) index = self . __previews . setdefault ( preview . module , { } ) index [ cls . __name__ ] = preview
5461	def get_job_and_task_param ( job_params , task_params , field ) : return job_params . get ( field , set ( ) ) | task_params . get ( field , set ( ) )
13337	def active_env_module_resolver ( resolver , path ) : from . api import get_active_env env = get_active_env ( ) if not env : raise ResolveError mod = env . get_module ( path ) if not mod : raise ResolveError return mod
13281	def parse ( self , source ) : command_regex = self . _make_command_regex ( self . name ) for match in re . finditer ( command_regex , source ) : self . _logger . debug ( match ) start_index = match . start ( 0 ) yield self . _parse_command ( source , start_index )
12904	def toIndex ( self , value ) : if self . _isIrNull ( value ) : ret = IR_NULL_STR else : ret = self . _toIndex ( value ) if self . isIndexHashed is False : return ret return md5 ( tobytes ( ret ) ) . hexdigest ( )
2276	def _win32_dir ( path , star = '' ) : from ubelt import util_cmd import re wrapper = 'cmd /S /C "{}"' command = 'dir /-C "{}"{}' . format ( path , star ) wrapped = wrapper . format ( command ) info = util_cmd . cmd ( wrapped , shell = True ) if info [ 'ret' ] != 0 : from ubelt import util_format print ( 'Failed command:' ) print ( info [ 'command' ] ) print ( util_format . repr2 ( info , nl = 1 ) ) raise OSError ( str ( info ) ) lines = info [ 'out' ] . split ( '\n' ) [ 5 : - 3 ] splitter = re . compile ( '( +)' ) for line in lines : parts = splitter . split ( line ) date , sep , time , sep , ampm , sep , type_or_size , sep = parts [ : 8 ] name = '' . join ( parts [ 8 : ] ) if name == '.' or name == '..' : continue if type_or_size in [ '<JUNCTION>' , '<SYMLINKD>' , '<SYMLINK>' ] : pos = name . find ( ':' ) bpos = name [ : pos ] . rfind ( '[' ) name = name [ : bpos - 1 ] pointed = name [ bpos + 1 : - 1 ] yield type_or_size , name , pointed else : yield type_or_size , name , None
1647	def CheckCheck ( filename , clean_lines , linenum , error ) : lines = clean_lines . elided ( check_macro , start_pos ) = FindCheckMacro ( lines [ linenum ] ) if not check_macro : return ( last_line , end_line , end_pos ) = CloseExpression ( clean_lines , linenum , start_pos ) if end_pos < 0 : return if not Match ( r'\s*;' , last_line [ end_pos : ] ) : return if linenum == end_line : expression = lines [ linenum ] [ start_pos + 1 : end_pos - 1 ] else : expression = lines [ linenum ] [ start_pos + 1 : ] for i in xrange ( linenum + 1 , end_line ) : expression += lines [ i ] expression += last_line [ 0 : end_pos - 1 ] lhs = '' rhs = '' operator = None while expression : matched = Match ( r'^\s*(<<|<<=|>>|>>=|->\*|->|&&|\|\||' r'==|!=|>=|>|<=|<|\()(.*)$' , expression ) if matched : token = matched . group ( 1 ) if token == '(' : expression = matched . group ( 2 ) ( end , _ ) = FindEndOfExpressionInLine ( expression , 0 , [ '(' ] ) if end < 0 : return lhs += '(' + expression [ 0 : end ] expression = expression [ end : ] elif token in ( '&&' , '||' ) : return elif token in ( '<<' , '<<=' , '>>' , '>>=' , '->*' , '->' ) : lhs += token expression = matched . group ( 2 ) else : operator = token rhs = matched . group ( 2 ) break else : matched = Match ( r'^([^-=!<>()&|]+)(.*)$' , expression ) if not matched : matched = Match ( r'^(\s*\S)(.*)$' , expression ) if not matched : break lhs += matched . group ( 1 ) expression = matched . group ( 2 ) if not ( lhs and operator and rhs ) : return if rhs . find ( '&&' ) > - 1 or rhs . find ( '||' ) > - 1 : return lhs = lhs . strip ( ) rhs = rhs . strip ( ) match_constant = r'^([-+]?(\d+|0[xX][0-9a-fA-F]+)[lLuU]{0,3}|".*"|\'.*\')$' if Match ( match_constant , lhs ) or Match ( match_constant , rhs ) : error ( filename , linenum , 'readability/check' , 2 , 'Consider using %s instead of %s(a %s b)' % ( _CHECK_REPLACEMENT [ check_macro ] [ operator ] , check_macro , operator ) )
8819	def get_networks_count ( context , filters = None ) : LOG . info ( "get_networks_count for tenant %s filters %s" % ( context . tenant_id , filters ) ) return db_api . network_count_all ( context )
7794	def register_fetcher ( self , object_class , fetcher_class ) : self . _lock . acquire ( ) try : cache = self . _caches . get ( object_class ) if not cache : cache = Cache ( self . max_items , self . default_freshness_period , self . default_expiration_period , self . default_purge_period ) self . _caches [ object_class ] = cache cache . set_fetcher ( fetcher_class ) finally : self . _lock . release ( )
7542	def storealleles ( consens , hidx , alleles ) : bigbase = PRIORITY [ consens [ hidx [ 0 ] ] ] bigallele = [ i for i in alleles if i [ 0 ] == bigbase ] [ 0 ] for hsite , pbase in zip ( hidx [ 1 : ] , bigallele [ 1 : ] ) : if PRIORITY [ consens [ hsite ] ] != pbase : consens [ hsite ] = consens [ hsite ] . lower ( ) return consens
4596	def pid_context ( pid_filename = None ) : pid_filename = pid_filename or DEFAULT_PID_FILENAME if os . path . exists ( pid_filename ) : contents = open ( pid_filename ) . read ( 16 ) log . warning ( 'pid_filename %s already exists with contents %s' , pid_filename , contents ) with open ( pid_filename , 'w' ) as fp : fp . write ( str ( os . getpid ( ) ) ) fp . write ( '\n' ) try : yield finally : try : os . remove ( pid_filename ) except Exception as e : log . error ( 'Got an exception %s deleting the pid_filename %s' , e , pid_filename )
9788	def bookmark ( ctx , username ) : ctx . obj = ctx . obj or { } ctx . obj [ 'username' ] = username
9523	def scaffolds_to_contigs ( infile , outfile , number_contigs = False ) : seq_reader = sequences . file_reader ( infile ) fout = utils . open_file_write ( outfile ) for seq in seq_reader : contigs = seq . contig_coords ( ) counter = 1 for contig in contigs : if number_contigs : name = seq . id + '.' + str ( counter ) counter += 1 else : name = '.' . join ( [ seq . id , str ( contig . start + 1 ) , str ( contig . end + 1 ) ] ) print ( sequences . Fasta ( name , seq [ contig . start : contig . end + 1 ] ) , file = fout ) utils . close ( fout )
728	def numberMapForBits ( self , bits ) : numberMap = dict ( ) for bit in bits : numbers = self . numbersForBit ( bit ) for number in numbers : if not number in numberMap : numberMap [ number ] = set ( ) numberMap [ number ] . add ( bit ) return numberMap
11055	def rm_back_refs ( obj ) : for ref in _collect_refs ( obj ) : ref [ 'value' ] . _remove_backref ( ref [ 'field_instance' ] . _backref_field_name , obj , ref [ 'field_name' ] , strict = False )
10157	def get_transition_viewset_method ( transition_name , ** kwargs ) : @ detail_route ( methods = [ 'post' ] , ** kwargs ) def inner_func ( self , request , pk = None , ** kwargs ) : object = self . get_object ( ) transition_method = getattr ( object , transition_name ) transition_method ( by = self . request . user ) if self . save_after_transition : object . save ( ) serializer = self . get_serializer ( object ) return Response ( serializer . data ) return inner_func
1199	def get_variables ( self , include_nontrainable = False ) : if include_nontrainable : return [ self . all_variables [ key ] for key in sorted ( self . all_variables ) ] else : return [ self . variables [ key ] for key in sorted ( self . variables ) ]
3123	def _check_audience ( payload_dict , audience ) : if audience is None : return audience_in_payload = payload_dict . get ( 'aud' ) if audience_in_payload is None : raise AppIdentityError ( 'No aud field in token: {0}' . format ( payload_dict ) ) if audience_in_payload != audience : raise AppIdentityError ( 'Wrong recipient, {0} != {1}: {2}' . format ( audience_in_payload , audience , payload_dict ) )
10286	def get_subgraph_peripheral_nodes ( graph : BELGraph , subgraph : Iterable [ BaseEntity ] , node_predicates : NodePredicates = None , edge_predicates : EdgePredicates = None , ) : node_filter = concatenate_node_predicates ( node_predicates = node_predicates ) edge_filter = and_edge_predicates ( edge_predicates = edge_predicates ) result = defaultdict ( lambda : defaultdict ( lambda : defaultdict ( list ) ) ) for u , v , k , d in get_peripheral_successor_edges ( graph , subgraph ) : if not node_filter ( graph , v ) or not node_filter ( graph , u ) or not edge_filter ( graph , u , v , k ) : continue result [ v ] [ 'predecessor' ] [ u ] . append ( ( k , d ) ) for u , v , k , d in get_peripheral_predecessor_edges ( graph , subgraph ) : if not node_filter ( graph , v ) or not node_filter ( graph , u ) or not edge_filter ( graph , u , v , k ) : continue result [ u ] [ 'successor' ] [ v ] . append ( ( k , d ) ) return result
12204	def auto_constraints ( self , component = None ) : if not component : for table in self . tables : self . auto_constraints ( table ) return if not component . tableSchema . primaryKey : idcol = component . get_column ( term_uri ( 'id' ) ) if idcol : component . tableSchema . primaryKey = [ idcol . name ] self . _auto_foreign_keys ( component ) try : table_type = self . get_tabletype ( component ) except ValueError : return for table in self . tables : self . _auto_foreign_keys ( table , component = component , table_type = table_type )
10310	def safe_add_edge ( graph , u , v , key , attr_dict , ** attr ) : if key < 0 : graph . add_edge ( u , v , key = key , attr_dict = attr_dict , ** attr ) else : graph . add_edge ( u , v , attr_dict = attr_dict , ** attr )
92	def _quokka_normalize_extract ( extract ) : from imgaug . augmentables . bbs import BoundingBox , BoundingBoxesOnImage if extract == "square" : bb = BoundingBox ( x1 = 0 , y1 = 0 , x2 = 643 , y2 = 643 ) elif isinstance ( extract , tuple ) and len ( extract ) == 4 : bb = BoundingBox ( x1 = extract [ 0 ] , y1 = extract [ 1 ] , x2 = extract [ 2 ] , y2 = extract [ 3 ] ) elif isinstance ( extract , BoundingBox ) : bb = extract elif isinstance ( extract , BoundingBoxesOnImage ) : do_assert ( len ( extract . bounding_boxes ) == 1 ) do_assert ( extract . shape [ 0 : 2 ] == ( 643 , 960 ) ) bb = extract . bounding_boxes [ 0 ] else : raise Exception ( "Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage " + "for parameter 'extract', got %s." % ( type ( extract ) , ) ) return bb
11663	def as_integer_type ( ary ) : ary = np . asanyarray ( ary ) if is_integer_type ( ary ) : return ary rounded = np . rint ( ary ) if np . any ( rounded != ary ) : raise ValueError ( "argument array must contain only integers" ) return rounded . astype ( int )
2522	def p_file_chk_sum ( self , f_term , predicate ) : try : for _s , _p , checksum in self . graph . triples ( ( f_term , predicate , None ) ) : for _ , _ , value in self . graph . triples ( ( checksum , self . spdx_namespace [ 'checksumValue' ] , None ) ) : self . builder . set_file_chksum ( self . doc , six . text_type ( value ) ) except CardinalityError : self . more_than_one_error ( 'File checksum' )
1981	def sys_transmit ( self , cpu , fd , buf , count , tx_bytes ) : if issymbolic ( fd ) : logger . info ( "Ask to write to a symbolic file descriptor!!" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 0 ) if issymbolic ( buf ) : logger . info ( "Ask to write to a symbolic buffer" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 1 ) if issymbolic ( count ) : logger . info ( "Ask to write a symbolic number of bytes " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 2 ) if issymbolic ( tx_bytes ) : logger . info ( "Ask to return size to a symbolic address " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 3 ) return super ( ) . sys_transmit ( cpu , fd , buf , count , tx_bytes )
12006	def _read_version ( self , data ) : version = ord ( data [ 0 ] ) if version not in self . VERSIONS : raise Exception ( 'Version not defined: %d' % version ) return version
6637	def unpublish ( self , registry = None ) : return registry_access . unpublish ( self . getRegistryNamespace ( ) , self . getName ( ) , self . getVersion ( ) , registry = registry )
3001	def cryptoDF ( token = '' , version = '' ) : df = pd . DataFrame ( crypto ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
6839	def distrib_release ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : kernel = ( run ( 'uname -s' ) or '' ) . strip ( ) . lower ( ) if kernel == LINUX : return run ( 'lsb_release -r --short' ) elif kernel == SUNOS : return run ( 'uname -v' )
3781	def calculate ( self , T , method ) : r if method == TEST_METHOD_1 : prop = self . TEST_METHOD_1_coeffs [ 0 ] + self . TEST_METHOD_1_coeffs [ 1 ] * T elif method == TEST_METHOD_2 : prop = self . TEST_METHOD_2_coeffs [ 0 ] + self . TEST_METHOD_2_coeffs [ 1 ] * T elif method in self . tabular_data : prop = self . interpolate ( T , method ) return prop
8065	def drawdaisy ( x , y , color = '#fefefe' ) : _ctx . push ( ) _fill = _ctx . fill ( ) _stroke = _ctx . stroke ( ) sc = ( 1.0 / _ctx . HEIGHT ) * float ( y * 0.5 ) * 4.0 _ctx . strokewidth ( sc * 2.0 ) _ctx . stroke ( '#3B240B' ) _ctx . line ( x + ( sin ( x * 0.1 ) * 10.0 ) , y + 80 , x + sin ( _ctx . FRAME * 0.1 ) , y ) _ctx . translate ( - 20 , 0 ) _ctx . scale ( sc ) _ctx . fill ( color ) _ctx . nostroke ( ) for angle in xrange ( 0 , 360 , 45 ) : _ctx . rotate ( degrees = 45 ) _ctx . rect ( x , y , 40 , 8 , 1 ) _ctx . fill ( '#F7FE2E' ) _ctx . ellipse ( x + 15 , y , 10 , 10 ) _ctx . fill ( _fill ) _ctx . stroke ( _stroke ) _ctx . pop ( )
1635	def CheckSpacingForFunctionCall ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] fncall = line for pattern in ( r'\bif\s*\((.*)\)\s*{' , r'\bfor\s*\((.*)\)\s*{' , r'\bwhile\s*\((.*)\)\s*[{;]' , r'\bswitch\s*\((.*)\)\s*{' ) : match = Search ( pattern , line ) if match : fncall = match . group ( 1 ) break if ( not Search ( r'\b(if|for|while|switch|return|new|delete|catch|sizeof)\b' , fncall ) and not Search ( r' \([^)]+\)\([^)]*(\)|,$)' , fncall ) and not Search ( r' \([^)]+\)\[[^\]]+\]' , fncall ) ) : if Search ( r'\w\s*\(\s(?!\s*\\$)' , fncall ) : error ( filename , linenum , 'whitespace/parens' , 4 , 'Extra space after ( in function call' ) elif Search ( r'\(\s+(?!(\s*\\)|\()' , fncall ) : error ( filename , linenum , 'whitespace/parens' , 2 , 'Extra space after (' ) if ( Search ( r'\w\s+\(' , fncall ) and not Search ( r'_{0,2}asm_{0,2}\s+_{0,2}volatile_{0,2}\s+\(' , fncall ) and not Search ( r'#\s*define|typedef|using\s+\w+\s*=' , fncall ) and not Search ( r'\w\s+\((\w+::)*\*\w+\)\(' , fncall ) and not Search ( r'\bcase\s+\(' , fncall ) ) : if Search ( r'\boperator_*\b' , line ) : error ( filename , linenum , 'whitespace/parens' , 0 , 'Extra space before ( in function call' ) else : error ( filename , linenum , 'whitespace/parens' , 4 , 'Extra space before ( in function call' ) if Search ( r'[^)]\s+\)\s*[^{\s]' , fncall ) : if Search ( r'^\s+\)' , fncall ) : error ( filename , linenum , 'whitespace/parens' , 2 , 'Closing ) should be moved to the previous line' ) else : error ( filename , linenum , 'whitespace/parens' , 2 , 'Extra space before )' )
4696	def env ( ) : if cij . ssh . env ( ) : cij . err ( "board.env: invalid SSH environment" ) return 1 board = cij . env_to_dict ( PREFIX , REQUIRED ) if board is None : cij . err ( "board.env: invalid BOARD environment" ) return 1 board [ "CLASS" ] = "_" . join ( [ board [ r ] for r in REQUIRED [ : - 1 ] ] ) board [ "IDENT" ] = "-" . join ( [ board [ "CLASS" ] , board [ "ALIAS" ] ] ) cij . env_export ( PREFIX , EXPORTED , board ) return 0
5084	def unlink_learners ( self ) : sap_inactive_learners = self . client . get_inactive_sap_learners ( ) enterprise_customer = self . enterprise_configuration . enterprise_customer if not sap_inactive_learners : LOGGER . info ( 'Enterprise customer {%s} has no SAPSF inactive learners' , enterprise_customer . name ) return provider_id = enterprise_customer . identity_provider tpa_provider = get_identity_provider ( provider_id ) if not tpa_provider : LOGGER . info ( 'Enterprise customer {%s} has no associated identity provider' , enterprise_customer . name ) return None for sap_inactive_learner in sap_inactive_learners : social_auth_user = get_user_from_social_auth ( tpa_provider , sap_inactive_learner [ 'studentID' ] ) if not social_auth_user : continue try : EnterpriseCustomerUser . objects . unlink_user ( enterprise_customer = enterprise_customer , user_email = social_auth_user . email , ) except ( EnterpriseCustomerUser . DoesNotExist , PendingEnterpriseCustomerUser . DoesNotExist ) : LOGGER . info ( 'Learner with email {%s} is not associated with Enterprise Customer {%s}' , social_auth_user . email , enterprise_customer . name )
4595	def make_object ( * args , typename = None , python_path = None , datatype = None , ** kwds ) : datatype = datatype or import_symbol ( typename , python_path ) field_types = getattr ( datatype , 'FIELD_TYPES' , fields . FIELD_TYPES ) return datatype ( * args , ** fields . component ( kwds , field_types ) )
3777	def calculate_integral ( self , T1 , T2 , method ) : r return float ( quad ( self . calculate , T1 , T2 , args = ( method ) ) [ 0 ] )
11801	def restore ( self , removals ) : "Undo a supposition and all inferences from it." for B , b in removals : self . curr_domains [ B ] . append ( b )
957	def _genLoggingFilePath ( ) : appName = os . path . splitext ( os . path . basename ( sys . argv [ 0 ] ) ) [ 0 ] or 'UnknownApp' appLogDir = os . path . abspath ( os . path . join ( os . environ [ 'NTA_LOG_DIR' ] , 'numenta-logs-%s' % ( os . environ [ 'USER' ] , ) , appName ) ) appLogFileName = '%s-%s-%s.log' % ( appName , long ( time . mktime ( time . gmtime ( ) ) ) , os . getpid ( ) ) return os . path . join ( appLogDir , appLogFileName )
13433	def showfig ( fig , aspect = "auto" ) : ax = fig . gca ( ) alim = list ( ax . axis ( ) ) if alim [ 3 ] < alim [ 2 ] : temp = alim [ 2 ] alim [ 2 ] = alim [ 3 ] alim [ 3 ] = temp ax . axis ( alim ) ax . set_aspect ( aspect ) fig . show ( )
12223	def execute ( self , args , kwargs ) : return self . lookup_explicit ( args , kwargs ) ( * args , ** kwargs )
4937	def transform_description ( self , content_metadata_item ) : full_description = content_metadata_item . get ( 'full_description' ) or '' if 0 < len ( full_description ) <= self . LONG_STRING_LIMIT : return full_description return content_metadata_item . get ( 'short_description' ) or content_metadata_item . get ( 'title' ) or ''
5093	def refresh_maps ( self ) : for robot in self . robots : resp2 = ( requests . get ( urljoin ( self . ENDPOINT , 'users/me/robots/{}/maps' . format ( robot . serial ) ) , headers = self . _headers ) ) resp2 . raise_for_status ( ) self . _maps . update ( { robot . serial : resp2 . json ( ) } )
1463	def build ( self , bldr ) : stage_names = sets . Set ( ) for source in self . _sources : source . _build ( bldr , stage_names ) for source in self . _sources : if not source . _all_built ( ) : raise RuntimeError ( "Topology cannot be fully built! Are all sources added?" )
2098	def relaunch ( self , pk = None , ** kwargs ) : if not pk : existing_data = self . get ( ** kwargs ) pk = existing_data [ 'id' ] relaunch_endpoint = '%s%s/relaunch/' % ( self . endpoint , pk ) data = { } answer = { } try : result = client . post ( relaunch_endpoint , data = data ) . json ( ) if 'id' in result : answer . update ( result ) answer [ 'changed' ] = True except exc . MethodNotAllowed : answer [ 'changed' ] = False return answer
2147	def _configuration ( self , kwargs , config_item ) : if 'notification_configuration' not in config_item : if 'notification_type' not in kwargs : return nc = kwargs [ 'notification_configuration' ] = { } for field in Resource . configuration [ kwargs [ 'notification_type' ] ] : if field not in config_item : raise exc . TowerCLIError ( 'Required config field %s not' ' provided.' % field ) else : nc [ field ] = config_item [ field ] else : kwargs [ 'notification_configuration' ] = config_item [ 'notification_configuration' ]
5429	def _name_for_command ( command ) : r lines = command . splitlines ( ) for line in lines : line = line . strip ( ) if line and not line . startswith ( '#' ) and line != '\\' : return os . path . basename ( re . split ( r'\s' , line ) [ 0 ] ) return 'command'
8116	def line_line_intersection ( x1 , y1 , x2 , y2 , x3 , y3 , x4 , y4 , infinite = False ) : ua = ( x4 - x3 ) * ( y1 - y3 ) - ( y4 - y3 ) * ( x1 - x3 ) ub = ( x2 - x1 ) * ( y1 - y3 ) - ( y2 - y1 ) * ( x1 - x3 ) d = ( y4 - y3 ) * ( x2 - x1 ) - ( x4 - x3 ) * ( y2 - y1 ) if d == 0 : if ua == ub == 0 : return [ ] else : return [ ] ua /= float ( d ) ub /= float ( d ) if not infinite and not ( 0 <= ua <= 1 and 0 <= ub <= 1 ) : return None , None return [ ( x1 + ua * ( x2 - x1 ) , y1 + ua * ( y2 - y1 ) ) ]
7597	def get_top_war_clans ( self , country_key = '' , ** params : keys ) : url = self . api . TOP + '/war/' + str ( country_key ) return self . _get_model ( url , PartialClan , ** params )
13423	def line ( self , line ) : return [ x for x in re . split ( self . delimiter , line . rstrip ( ) ) if x != '' ]
8236	def left_complement ( clr ) : left = split_complementary ( clr ) [ 1 ] colors = complementary ( clr ) colors [ 3 ] . h = left . h colors [ 4 ] . h = left . h colors [ 5 ] . h = left . h colors = colorlist ( colors [ 0 ] , colors [ 2 ] , colors [ 1 ] , colors [ 3 ] , colors [ 4 ] , colors [ 5 ] ) return colors
7651	def query_pop ( query , prefix , sep = '.' ) : terms = query . split ( sep ) if terms [ 0 ] == prefix : terms = terms [ 1 : ] return sep . join ( terms )
6165	def sqrt_rc_imp ( Ns , alpha , M = 6 ) : n = np . arange ( - M * Ns , M * Ns + 1 ) b = np . zeros ( len ( n ) ) Ns *= 1.0 a = alpha for i in range ( len ( n ) ) : if abs ( 1 - 16 * a ** 2 * ( n [ i ] / Ns ) ** 2 ) <= np . finfo ( np . float ) . eps / 2 : b [ i ] = 1 / 2. * ( ( 1 + a ) * np . sin ( ( 1 + a ) * np . pi / ( 4. * a ) ) - ( 1 - a ) * np . cos ( ( 1 - a ) * np . pi / ( 4. * a ) ) + ( 4 * a ) / np . pi * np . sin ( ( 1 - a ) * np . pi / ( 4. * a ) ) ) else : b [ i ] = 4 * a / ( np . pi * ( 1 - 16 * a ** 2 * ( n [ i ] / Ns ) ** 2 ) ) b [ i ] = b [ i ] * ( np . cos ( ( 1 + a ) * np . pi * n [ i ] / Ns ) + np . sinc ( ( 1 - a ) * n [ i ] / Ns ) * ( 1 - a ) * np . pi / ( 4. * a ) ) return b
9809	def version ( cli , platform ) : version_client = PolyaxonClient ( ) . version cli = cli or not any ( [ cli , platform ] ) if cli : try : server_version = version_client . get_cli_version ( ) except AuthorizationError : session_expired ( ) sys . exit ( 1 ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get cli version.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) cli_version = get_version ( PROJECT_CLI_NAME ) Printer . print_header ( 'Current cli version: {}.' . format ( cli_version ) ) Printer . print_header ( 'Supported cli versions:' ) dict_tabulate ( server_version . to_dict ( ) ) if platform : try : platform_version = version_client . get_platform_version ( ) except AuthorizationError : session_expired ( ) sys . exit ( 1 ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get platform version.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) chart_version = version_client . get_chart_version ( ) Printer . print_header ( 'Current platform version: {}.' . format ( chart_version . version ) ) Printer . print_header ( 'Supported platform versions:' ) dict_tabulate ( platform_version . to_dict ( ) )
4080	def get_languages ( ) -> set : try : languages = cache [ 'languages' ] except KeyError : languages = LanguageTool . _get_languages ( ) cache [ 'languages' ] = languages return languages
13186	def download_observations ( observer_code ) : page_number = 1 observations = [ ] while True : logger . info ( 'Downloading page %d...' , page_number ) response = requests . get ( WEBOBS_RESULTS_URL , params = { 'obscode' : observer_code , 'num_results' : 200 , 'obs_types' : 'all' , 'page' : page_number , } ) logger . debug ( response . request . url ) parser = WebObsResultsParser ( response . text ) observations . extend ( parser . get_observations ( ) ) if '>Next</a>' not in response . text : break page_number += 1 return observations
13108	def to_dict ( self , include_meta = False ) : result = super ( JackalDoc , self ) . to_dict ( include_meta = include_meta ) if include_meta : source = result . pop ( '_source' ) return { ** result , ** source } else : return result
4862	def save ( self ) : course_id = self . validated_data [ 'course_id' ] __ , created = models . EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = self . enterprise_customer_user , course_id = course_id , ) if created : track_enrollment ( 'rest-api-enrollment' , self . enterprise_customer_user . user_id , course_id )
6504	def decorate_matches ( match_in , match_word ) : matches = re . finditer ( match_word , match_in , re . IGNORECASE ) for matched_string in set ( [ match . group ( ) for match in matches ] ) : match_in = match_in . replace ( matched_string , getattr ( settings , "SEARCH_MATCH_DECORATION" , u"<b>{}</b>" ) . format ( matched_string ) ) return match_in
4190	def window_poisson_hanning ( N , alpha = 2 ) : r w1 = window_hann ( N ) w2 = window_poisson ( N , alpha = alpha ) return w1 * w2
10012	def parse_option_settings ( option_settings ) : ret = [ ] for namespace , params in list ( option_settings . items ( ) ) : for key , value in list ( params . items ( ) ) : ret . append ( ( namespace , key , value ) ) return ret
6425	def tanimoto_coeff ( self , src , tar , qval = 2 ) : coeff = self . sim ( src , tar , qval ) if coeff != 0 : return log ( coeff , 2 ) return float ( '-inf' )
1830	def JB ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CF == True , target . read ( ) , cpu . PC )
13863	def ts ( when , tz = None ) : if not when : return None when = totz ( when , tz ) return calendar . timegm ( when . timetuple ( ) )
13104	def start_scan ( self , scan_id ) : requests . post ( self . url + 'scans/{}/launch' . format ( scan_id ) , verify = False , headers = self . headers )
8071	def not_found ( url , wait = 10 ) : try : connection = open ( url , wait ) except HTTP404NotFound : return True except : return False return False
11703	def set_gender ( self , gender = None ) : if gender and gender in genders : self . gender = gender else : if not self . chromosomes : self . set_chromosomes ( ) self . gender = npchoice ( genders , 1 , p = p_gender [ self . chromosomes ] ) [ 0 ]
1723	def translate_file ( input_path , output_path ) : js = get_file_contents ( input_path ) py_code = translate_js ( js ) lib_name = os . path . basename ( output_path ) . split ( '.' ) [ 0 ] head = '__all__ = [%s]\n\n# Don\'t look below, you will not understand this Python code :) I don\'t.\n\n' % repr ( lib_name ) tail = '\n\n# Add lib to the module scope\n%s = var.to_python()' % lib_name out = head + py_code + tail write_file_contents ( output_path , out )
4109	def mexican ( lb , ub , n ) : r if n <= 0 : raise ValueError ( "n must be strictly positive" ) x = numpy . linspace ( lb , ub , n ) psi = ( 1. - x ** 2. ) * ( 2. / ( numpy . sqrt ( 3. ) * pi ** 0.25 ) ) * numpy . exp ( - x ** 2 / 2. ) return psi
8680	def list ( self , key_name = None , max_suggestions = 100 , cutoff = 0.5 , locked_only = False , key_type = None ) : self . _assert_valid_stash ( ) key_list = [ k for k in self . _storage . list ( ) if k [ 'name' ] != 'stored_passphrase' and ( k . get ( 'lock' ) if locked_only else True ) ] if key_type : types = ( 'secret' , None ) if key_type == 'secret' else [ key_type ] key_list = [ k for k in key_list if k . get ( 'type' ) in types ] key_list = [ k [ 'name' ] for k in key_list ] if key_name : if key_name . startswith ( '~' ) : key_list = difflib . get_close_matches ( key_name . lstrip ( '~' ) , key_list , max_suggestions , cutoff ) else : key_list = [ k for k in key_list if key_name in k ] audit ( storage = self . _storage . db_path , action = 'LIST' + ( '[LOCKED]' if locked_only else '' ) , message = json . dumps ( dict ( ) ) ) return key_list
10273	def remove_unweighted_sources ( graph : BELGraph , key : Optional [ str ] = None ) -> None : nodes = list ( get_unweighted_sources ( graph , key = key ) ) graph . remove_nodes_from ( nodes )
7298	def get_form_field_class ( model_field ) : FIELD_MAPPING = { IntField : forms . IntegerField , StringField : forms . CharField , FloatField : forms . FloatField , BooleanField : forms . BooleanField , DateTimeField : forms . DateTimeField , DecimalField : forms . DecimalField , URLField : forms . URLField , EmailField : forms . EmailField } return FIELD_MAPPING . get ( model_field . __class__ , forms . CharField )
7732	def get_join_info ( self ) : x = self . get_muc_child ( ) if not x : return None if not isinstance ( x , MucX ) : return None return x
11052	def sync ( self ) : self . log . info ( 'Starting a sync...' ) def log_success ( result ) : self . log . info ( 'Sync completed successfully' ) return result def log_failure ( failure ) : self . log . failure ( 'Sync failed' , failure , LogLevel . error ) return failure return ( self . marathon_client . get_apps ( ) . addCallback ( self . _apps_acme_domains ) . addCallback ( self . _filter_new_domains ) . addCallback ( self . _issue_certs ) . addCallbacks ( log_success , log_failure ) )
904	def read ( cls , proto ) : anomalyLikelihood = object . __new__ ( cls ) anomalyLikelihood . _iteration = proto . iteration anomalyLikelihood . _historicalScores = collections . deque ( maxlen = proto . historicWindowSize ) for i , score in enumerate ( proto . historicalScores ) : anomalyLikelihood . _historicalScores . append ( ( i , score . value , score . anomalyScore ) ) if proto . distribution . name : anomalyLikelihood . _distribution = dict ( ) anomalyLikelihood . _distribution [ 'distribution' ] = dict ( ) anomalyLikelihood . _distribution [ 'distribution' ] [ "name" ] = proto . distribution . name anomalyLikelihood . _distribution [ 'distribution' ] [ "mean" ] = proto . distribution . mean anomalyLikelihood . _distribution [ 'distribution' ] [ "variance" ] = proto . distribution . variance anomalyLikelihood . _distribution [ 'distribution' ] [ "stdev" ] = proto . distribution . stdev anomalyLikelihood . _distribution [ "movingAverage" ] = { } anomalyLikelihood . _distribution [ "movingAverage" ] [ "windowSize" ] = proto . distribution . movingAverage . windowSize anomalyLikelihood . _distribution [ "movingAverage" ] [ "historicalValues" ] = [ ] for value in proto . distribution . movingAverage . historicalValues : anomalyLikelihood . _distribution [ "movingAverage" ] [ "historicalValues" ] . append ( value ) anomalyLikelihood . _distribution [ "movingAverage" ] [ "total" ] = proto . distribution . movingAverage . total anomalyLikelihood . _distribution [ "historicalLikelihoods" ] = [ ] for likelihood in proto . distribution . historicalLikelihoods : anomalyLikelihood . _distribution [ "historicalLikelihoods" ] . append ( likelihood ) else : anomalyLikelihood . _distribution = None anomalyLikelihood . _probationaryPeriod = proto . probationaryPeriod anomalyLikelihood . _learningPeriod = proto . learningPeriod anomalyLikelihood . _reestimationPeriod = proto . reestimationPeriod return anomalyLikelihood
8082	def relcurveto ( self , h1x , h1y , h2x , h2y , x , y ) : if self . _path is None : raise ShoebotError ( _ ( "No current path. Use beginpath() first." ) ) self . _path . relcurveto ( h1x , h1y , h2x , h2y , x , y )
4670	def setKeys ( self , loadkeys ) : log . debug ( "Force setting of private keys. Not using the wallet database!" ) if isinstance ( loadkeys , dict ) : loadkeys = list ( loadkeys . values ( ) ) elif not isinstance ( loadkeys , ( list , set ) ) : loadkeys = [ loadkeys ] for wif in loadkeys : pub = self . publickey_from_wif ( wif ) self . store . add ( str ( wif ) , pub )
677	def getDescription ( self ) : description = { 'name' : self . name , 'fields' : [ f . name for f in self . fields ] , 'numRecords by field' : [ f . numRecords for f in self . fields ] } return description
4522	def run ( function , * args , use_subprocess = False , daemon = True , ** kwds ) : if use_subprocess : Creator , Queue = multiprocessing . Process , multiprocessing . Queue else : Creator , Queue = threading . Thread , queue . Queue input , output = Queue ( ) , Queue ( ) args = input , output , function , args sub = Creator ( target = _run_locally , args = args , kwargs = kwds , daemon = daemon ) sub . start ( ) return sub , input , output
1702	def outer_right_join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . OUTER_RIGHT , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
5101	def adjacency2graph ( adjacency , edge_type = None , adjust = 1 , ** kwargs ) : if isinstance ( adjacency , np . ndarray ) : adjacency = _matrix2dict ( adjacency ) elif isinstance ( adjacency , dict ) : adjacency = _dict2dict ( adjacency ) else : msg = ( "If the adjacency parameter is supplied it must be a " "dict, or a numpy.ndarray." ) raise TypeError ( msg ) if edge_type is None : edge_type = { } else : if isinstance ( edge_type , np . ndarray ) : edge_type = _matrix2dict ( edge_type , etype = True ) elif isinstance ( edge_type , dict ) : edge_type = _dict2dict ( edge_type ) for u , ty in edge_type . items ( ) : for v , et in ty . items ( ) : adjacency [ u ] [ v ] [ 'edge_type' ] = et g = nx . from_dict_of_dicts ( adjacency , create_using = nx . DiGraph ( ) ) adjacency = nx . to_dict_of_dicts ( g ) adjacency = _adjacency_adjust ( adjacency , adjust , True ) return nx . from_dict_of_dicts ( adjacency , create_using = nx . DiGraph ( ) )
642	def readConfigFile ( cls , filename , path = None ) : properties = cls . _readConfigFile ( filename , path ) if cls . _properties is None : cls . _properties = dict ( ) for name in properties : if 'value' in properties [ name ] : cls . _properties [ name ] = properties [ name ] [ 'value' ]
12766	def distances ( self ) : distances = [ ] for label in self . labels : joint = self . joints . get ( label ) distances . append ( [ np . nan , np . nan , np . nan ] if joint is None else np . array ( joint . getAnchor ( ) ) - joint . getAnchor2 ( ) ) return np . array ( distances )
3385	def _reproject ( self , p ) : nulls = self . problem . nullspace equalities = self . problem . equalities if np . allclose ( equalities . dot ( p ) , self . problem . b , rtol = 0 , atol = self . feasibility_tol ) : new = p else : LOGGER . info ( "feasibility violated in sample" " %d, trying to reproject" % self . n_samples ) new = nulls . dot ( nulls . T . dot ( p ) ) if any ( new != p ) : LOGGER . info ( "reprojection failed in sample" " %d, using random point in space" % self . n_samples ) new = self . _random_point ( ) return new
11865	def pointwise_product ( self , other , bn ) : "Multiply two factors, combining their variables." vars = list ( set ( self . vars ) | set ( other . vars ) ) cpt = dict ( ( event_values ( e , vars ) , self . p ( e ) * other . p ( e ) ) for e in all_events ( vars , bn , { } ) ) return Factor ( vars , cpt )
6961	def _time_independent_equals ( a , b ) : if len ( a ) != len ( b ) : return False result = 0 if isinstance ( a [ 0 ] , int ) : for x , y in zip ( a , b ) : result |= x ^ y else : for x , y in zip ( a , b ) : result |= ord ( x ) ^ ord ( y ) return result == 0
11675	def bare ( self ) : "Make a Features object with no metadata; points to the same features." if not self . meta : return self elif self . stacked : return Features ( self . stacked_features , self . n_pts , copy = False ) else : return Features ( self . features , copy = False )
6278	def draw ( self , current_time , frame_time ) : self . set_default_viewport ( ) self . timeline . draw ( current_time , frame_time , self . fbo )
7350	def predict ( self , sequences ) : with tempfile . NamedTemporaryFile ( suffix = ".fsa" , mode = "w" ) as input_fd : for ( i , sequence ) in enumerate ( sequences ) : input_fd . write ( "> %d\n" % i ) input_fd . write ( sequence ) input_fd . write ( "\n" ) input_fd . flush ( ) try : output = subprocess . check_output ( [ "netChop" , input_fd . name ] ) except subprocess . CalledProcessError as e : logging . error ( "Error calling netChop: %s:\n%s" % ( e , e . output ) ) raise parsed = self . parse_netchop ( output ) assert len ( parsed ) == len ( sequences ) , "Expected %d results but got %d" % ( len ( sequences ) , len ( parsed ) ) assert [ len ( x ) for x in parsed ] == [ len ( x ) for x in sequences ] return parsed
3851	async def lookup_entities ( client , args ) : lookup_spec = _get_lookup_spec ( args . entity_identifier ) request = hangups . hangouts_pb2 . GetEntityByIdRequest ( request_header = client . get_request_header ( ) , batch_lookup_spec = [ lookup_spec ] , ) res = await client . get_entity_by_id ( request ) for entity_result in res . entity_result : for entity in entity_result . entity : print ( entity )
9327	def valid_content_type ( self , content_type , accept ) : accept_tokens = accept . replace ( ' ' , '' ) . split ( ';' ) content_type_tokens = content_type . replace ( ' ' , '' ) . split ( ';' ) return ( all ( elem in content_type_tokens for elem in accept_tokens ) and ( content_type_tokens [ 0 ] == 'application/vnd.oasis.taxii+json' or content_type_tokens [ 0 ] == 'application/vnd.oasis.stix+json' ) )
7946	def _write ( self , data ) : OUT_LOGGER . debug ( "OUT: %r" , data ) if self . _hup or not self . _socket : raise PyXMPPIOError ( u"Connection closed." ) try : while data : try : sent = self . _socket . send ( data ) except ssl . SSLError , err : if err . args [ 0 ] == ssl . SSL_ERROR_WANT_WRITE : continue else : raise except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue if err . args [ 0 ] in BLOCKING_ERRORS : wait_for_write ( self . _socket ) continue raise data = data [ sent : ] except ( IOError , OSError , socket . error ) , err : raise PyXMPPIOError ( u"IO Error: {0}" . format ( err ) )
9699	def event_choices ( events ) : if events is None : msg = "Please add some events in settings.WEBHOOK_EVENTS." raise ImproperlyConfigured ( msg ) try : choices = [ ( x , x ) for x in events ] except TypeError : msg = "settings.WEBHOOK_EVENTS must be an iterable object." raise ImproperlyConfigured ( msg ) return choices
3771	def mixing_logarithmic ( fracs , props ) : r if not none_and_length_check ( [ fracs , props ] ) : return None return exp ( sum ( frac * log ( prop ) for frac , prop in zip ( fracs , props ) ) )
2206	def compressuser ( path , home = '~' ) : path = normpath ( path ) userhome_dpath = userhome ( ) if path . startswith ( userhome_dpath ) : if len ( path ) == len ( userhome_dpath ) : path = home elif path [ len ( userhome_dpath ) ] == os . path . sep : path = home + path [ len ( userhome_dpath ) : ] return path
10123	def rotate ( self , angle , center = None ) : args = [ angle ] if center is not None : args . extend ( center ) self . poly . rotate ( * args ) return self
10812	def query_by_user ( cls , user , with_pending = False , eager = False ) : q1 = Group . query . join ( Membership ) . filter_by ( user_id = user . get_id ( ) ) if not with_pending : q1 = q1 . filter_by ( state = MembershipState . ACTIVE ) if eager : q1 = q1 . options ( joinedload ( Group . members ) ) q2 = Group . query . join ( GroupAdmin ) . filter_by ( admin_id = user . get_id ( ) , admin_type = resolve_admin_type ( user ) ) if eager : q2 = q2 . options ( joinedload ( Group . members ) ) query = q1 . union ( q2 ) . with_entities ( Group . id ) return Group . query . filter ( Group . id . in_ ( query ) )
8768	def add_job_to_context ( context , job_id ) : db_job = db_api . async_transaction_find ( context , id = job_id , scope = db_api . ONE ) if not db_job : return context . async_job = { "job" : v . _make_job_dict ( db_job ) }
10800	def _newcall ( self , rvecs ) : sigma = 1 * self . filter_size out = self . _eval_firstorder ( rvecs , self . d , sigma ) ondata = self . _eval_firstorder ( self . x , self . d , sigma ) for i in range ( self . iterations ) : out += self . _eval_firstorder ( rvecs , self . d - ondata , sigma ) ondata += self . _eval_firstorder ( self . x , self . d - ondata , sigma ) sigma *= self . damp return out
9333	def full_like ( array , value , dtype = None ) : shared = empty_like ( array , dtype ) shared [ : ] = value return shared
8293	def unique ( list ) : unique = [ ] [ unique . append ( x ) for x in list if x not in unique ] return unique
4961	def paginated_list ( object_list , page , page_size = 25 ) : paginator = CustomPaginator ( object_list , page_size ) try : object_list = paginator . page ( page ) except PageNotAnInteger : object_list = paginator . page ( 1 ) except EmptyPage : object_list = paginator . page ( paginator . num_pages ) page_range = [ ] page_num = object_list . number if paginator . num_pages <= 10 : page_range = range ( paginator . num_pages ) else : if page_num > ( PAGES_ON_EACH_SIDE + PAGES_ON_ENDS + 1 ) : page_range . extend ( range ( 1 , PAGES_ON_ENDS + 1 ) ) page_range . append ( DOT ) page_range . extend ( range ( page_num - PAGES_ON_EACH_SIDE , page_num + 1 ) ) else : page_range . extend ( range ( 1 , page_num + 1 ) ) if page_num < ( paginator . num_pages - PAGES_ON_EACH_SIDE - PAGES_ON_ENDS ) : page_range . extend ( range ( page_num + 1 , page_num + PAGES_ON_EACH_SIDE + 1 ) ) page_range . append ( DOT ) page_range . extend ( range ( paginator . num_pages + 1 - PAGES_ON_ENDS , paginator . num_pages + 1 ) ) else : page_range . extend ( range ( page_num + 1 , paginator . num_pages + 1 ) ) object_list . paginator . page_range = page_range return object_list
1869	def MOVZX ( cpu , op0 , op1 ) : op0 . write ( Operators . ZEXTEND ( op1 . read ( ) , op0 . size ) )
3399	def add_switches_and_objective ( self ) : constraints = list ( ) big_m = max ( max ( abs ( b ) for b in r . bounds ) for r in self . model . reactions ) prob = self . model . problem for rxn in self . model . reactions : if not hasattr ( rxn , 'gapfilling_type' ) : continue indicator = prob . Variable ( name = 'indicator_{}' . format ( rxn . id ) , lb = 0 , ub = 1 , type = 'binary' ) if rxn . id in self . penalties : indicator . cost = self . penalties [ rxn . id ] else : indicator . cost = self . penalties [ rxn . gapfilling_type ] indicator . rxn_id = rxn . id self . indicators . append ( indicator ) constraint_lb = prob . Constraint ( rxn . flux_expression - big_m * indicator , ub = 0 , name = 'constraint_lb_{}' . format ( rxn . id ) , sloppy = True ) constraint_ub = prob . Constraint ( rxn . flux_expression + big_m * indicator , lb = 0 , name = 'constraint_ub_{}' . format ( rxn . id ) , sloppy = True ) constraints . extend ( [ constraint_lb , constraint_ub ] ) self . model . add_cons_vars ( self . indicators ) self . model . add_cons_vars ( constraints , sloppy = True ) self . model . objective = prob . Objective ( Zero , direction = 'min' , sloppy = True ) self . model . objective . set_linear_coefficients ( { i : 1 for i in self . indicators } ) self . update_costs ( )
6397	def sim ( self , src , tar , qval = 2 ) : r if src == tar : return 1.0 if not src or not tar : return 0.0 q_src , q_tar = self . _get_qgrams ( src , tar , qval ) q_src_mag = sum ( q_src . values ( ) ) q_tar_mag = sum ( q_tar . values ( ) ) q_intersection_mag = sum ( ( q_src & q_tar ) . values ( ) ) return q_intersection_mag / sqrt ( q_src_mag * q_tar_mag )
13626	def Delimited ( value , parser = Text , delimiter = u',' , encoding = None ) : value = Text ( value , encoding ) if value is None or value == u'' : return [ ] return map ( parser , value . split ( delimiter ) )
10582	def set_parent_path ( self , value ) : self . _parent_path = value self . path = value + r'/' + self . name self . _update_childrens_parent_path ( )
9068	def _lml_arbitrary_scale ( self ) : s = self . scale D = self . _D n = len ( self . _y ) lml = - self . _df * log2pi - n * log ( s ) lml -= sum ( npsum ( log ( d ) ) for d in D ) d = ( mTQ - yTQ for ( mTQ , yTQ ) in zip ( self . _mTQ , self . _yTQ ) ) lml -= sum ( ( i / j ) @ i for ( i , j ) in zip ( d , D ) ) / s return lml / 2
10063	def process_minter ( value ) : try : return current_pidstore . minters [ value ] except KeyError : raise click . BadParameter ( 'Unknown minter {0}. Please use one of {1}.' . format ( value , ', ' . join ( current_pidstore . minters . keys ( ) ) ) )
2427	def set_doc_comment ( self , doc , comment ) : if not self . doc_comment_set : self . doc_comment_set = True if validations . validate_doc_comment ( comment ) : doc . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'Document::Comment' ) else : raise CardinalityError ( 'Document::Comment' )
2451	def set_pkg_down_location ( self , doc , location ) : self . assert_package_exists ( ) if not self . package_down_location_set : self . package_down_location_set = True doc . package . download_location = location return True else : raise CardinalityError ( 'Package::DownloadLocation' )
6317	def image_data ( image ) : data = image . tobytes ( ) components = len ( data ) // ( image . size [ 0 ] * image . size [ 1 ] ) return components , data
6631	def set ( self , path , value = None , filename = None ) : if filename is None : config = self . _firstConfig ( ) [ 1 ] else : config = self . configs [ filename ] path = _splitPath ( path ) for el in path [ : - 1 ] : if el in config : config = config [ el ] else : config [ el ] = OrderedDict ( ) config = config [ el ] config [ path [ - 1 ] ] = value
2736	def create ( self , * args , ** kwargs ) : data = self . get_data ( 'floating_ips/' , type = POST , params = { 'droplet_id' : self . droplet_id } ) if data : self . ip = data [ 'floating_ip' ] [ 'ip' ] self . region = data [ 'floating_ip' ] [ 'region' ] return self
601	def addGraph ( self , data , position = 111 , xlabel = None , ylabel = None ) : ax = self . _addBase ( position , xlabel = xlabel , ylabel = ylabel ) ax . plot ( data ) plt . draw ( )
10867	def rmatrix ( self ) : t = self . param_dict [ self . lbl_theta ] r0 = np . array ( [ [ np . cos ( t ) , - np . sin ( t ) , 0 ] , [ np . sin ( t ) , np . cos ( t ) , 0 ] , [ 0 , 0 , 1 ] ] ) p = self . param_dict [ self . lbl_phi ] r1 = np . array ( [ [ np . cos ( p ) , 0 , np . sin ( p ) ] , [ 0 , 1 , 0 ] , [ - np . sin ( p ) , 0 , np . cos ( p ) ] ] ) return np . dot ( r1 , r0 )
10215	def rank_subgraph_by_node_filter ( graph : BELGraph , node_predicates : Union [ NodePredicate , Iterable [ NodePredicate ] ] , annotation : str = 'Subgraph' , reverse : bool = True , ) -> List [ Tuple [ str , int ] ] : r1 = group_nodes_by_annotation_filtered ( graph , node_predicates = node_predicates , annotation = annotation ) r2 = count_dict_values ( r1 ) return sorted ( r2 . items ( ) , key = itemgetter ( 1 ) , reverse = reverse )
9305	def handle_date_mismatch ( self , req ) : req_datetime = self . get_request_date ( req ) new_key_date = req_datetime . strftime ( '%Y%m%d' ) self . regenerate_signing_key ( date = new_key_date )
12455	def error_handler ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : try : return func ( * args , ** kwargs ) except BaseException as err : if BOOTSTRAPPER_TEST_KEY in os . environ : raise if ERROR_HANDLER_DISABLED : return True return save_traceback ( err ) return wrapper
5598	def execute ( mp , resampling = "nearest" , scale_method = None , scales_minmax = None ) : with mp . open ( "raster" , resampling = resampling ) as raster_file : if raster_file . is_empty ( ) : return "empty" scaled = ( ) mask = ( ) raster_data = raster_file . read ( ) if raster_data . ndim == 2 : raster_data = ma . expand_dims ( raster_data , axis = 0 ) if not scale_method : scales_minmax = [ ( i , i ) for i in range ( len ( raster_data ) ) ] for band , ( scale_min , scale_max ) in zip ( raster_data , scales_minmax ) : if scale_method in [ "dtype_scale" , "minmax_scale" ] : scaled += ( _stretch_array ( band , scale_min , scale_max ) , ) elif scale_method == "crop" : scaled += ( np . clip ( band , scale_min , scale_max ) , ) else : scaled += ( band , ) mask += ( band . mask , ) return ma . masked_array ( np . stack ( scaled ) , np . stack ( mask ) )
519	def _initPermConnected ( self ) : p = self . _synPermConnected + ( self . _synPermMax - self . _synPermConnected ) * self . _random . getReal64 ( ) p = int ( p * 100000 ) / 100000.0 return p
7965	def start ( self , tag , attrs ) : if self . _level == 0 : self . _root = ElementTree . Element ( tag , attrs ) self . _handler . stream_start ( self . _root ) if self . _level < 2 : self . _builder = ElementTree . TreeBuilder ( ) self . _level += 1 return self . _builder . start ( tag , attrs )
7716	def remove_item ( self , jid , callback = None , error_callback = None ) : item = self . roster [ jid ] if jid not in self . roster : raise KeyError ( jid ) item = RosterItem ( jid , subscription = "remove" ) self . _roster_set ( item , callback , error_callback )
3580	def disconnect_devices ( self , service_uuids = [ ] ) : service_uuids = set ( service_uuids ) for device in self . list_devices ( ) : if not device . is_connected : continue device_uuids = set ( map ( lambda x : x . uuid , device . list_services ( ) ) ) if device_uuids >= service_uuids : device . disconnect ( )
7765	def disconnect ( self ) : with self . lock : if self . stream : if self . settings [ u"initial_presence" ] : self . send ( Presence ( stanza_type = "unavailable" ) ) self . stream . disconnect ( )
13539	def get_locations ( self ) : url = "/2/locations" data = self . _get_resource ( url ) locations = [ ] for entry in data [ 'locations' ] : locations . append ( self . location_from_json ( entry ) ) return locations
5568	def bounds_at_zoom ( self , zoom = None ) : return ( ) if self . area_at_zoom ( zoom ) . is_empty else Bounds ( * self . area_at_zoom ( zoom ) . bounds )
7462	def encode ( self , obj ) : def hint_tuples ( item ) : if isinstance ( item , tuple ) : return { '__tuple__' : True , 'items' : item } if isinstance ( item , list ) : return [ hint_tuples ( e ) for e in item ] if isinstance ( item , dict ) : return { key : hint_tuples ( val ) for key , val in item . iteritems ( ) } else : return item return super ( Encoder , self ) . encode ( hint_tuples ( obj ) )
13371	def redirect_to_env_paths ( path ) : with open ( path , 'r' ) as f : redirected = f . read ( ) return shlex . split ( redirected )
11275	def check_pid ( pid , debug ) : try : os . kill ( pid , 0 ) if debug > 1 : print ( "Script has a PIDFILE where the process is still running" ) return True except OSError : if debug > 1 : print ( "Script does not appear to be running" ) return False
1549	def set_logging_level ( cl_args ) : if 'verbose' in cl_args and cl_args [ 'verbose' ] : configure ( logging . DEBUG ) else : configure ( logging . INFO )
6511	def _eat_name_line ( self , line ) : if line [ 0 ] not in "#=" : parts = line . split ( ) country_values = line [ 30 : - 1 ] name = map_name ( parts [ 1 ] ) if not self . case_sensitive : name = name . lower ( ) if parts [ 0 ] == "M" : self . _set ( name , u"male" , country_values ) elif parts [ 0 ] == "1M" or parts [ 0 ] == "?M" : self . _set ( name , u"mostly_male" , country_values ) elif parts [ 0 ] == "F" : self . _set ( name , u"female" , country_values ) elif parts [ 0 ] == "1F" or parts [ 0 ] == "?F" : self . _set ( name , u"mostly_female" , country_values ) elif parts [ 0 ] == "?" : self . _set ( name , self . unknown_value , country_values ) else : raise "Not sure what to do with a sex of %s" % parts [ 0 ]
8675	def load_keys ( key_file , origin_passphrase , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) click . echo ( 'Importing all keys from {0}...' . format ( key_file ) ) stash . load ( origin_passphrase , key_file = key_file ) click . echo ( 'Import complete!' )
10963	def set_shape ( self , shape , inner ) : if self . shape != shape or self . inner != inner : self . shape = shape self . inner = inner self . initialize ( )
868	def resetCustomConfig ( cls ) : _getLogger ( ) . info ( "Resetting all custom configuration properties; " "caller=%r" , traceback . format_stack ( ) ) super ( Configuration , cls ) . clear ( ) _CustomConfigurationFileWrapper . clear ( persistent = True )
9647	def is_valid_in_template ( var , attr ) : if attr . startswith ( '_' ) : return False try : value = getattr ( var , attr ) except : return False if isroutine ( value ) : if getattr ( value , 'alters_data' , False ) : return False else : try : argspec = getargspec ( value ) num_args = len ( argspec . args ) if argspec . args else 0 num_defaults = len ( argspec . defaults ) if argspec . defaults else 0 if num_args - num_defaults > 1 : return False except TypeError : pass return True
8732	def divide_timedelta_float ( td , divisor ) : dsm = [ getattr ( td , attr ) for attr in ( 'days' , 'seconds' , 'microseconds' ) ] dsm = map ( lambda elem : elem / divisor , dsm ) return datetime . timedelta ( * dsm )
1987	def save_state ( self , state , key ) : with self . save_stream ( key , binary = True ) as f : self . _serializer . serialize ( state , f )
13858	def curl ( self , url , post ) : try : req = urllib2 . Request ( url ) req . add_header ( "Content-type" , "application/xml" ) data = urllib2 . urlopen ( req , post . encode ( 'utf-8' ) ) . read ( ) except urllib2 . URLError , v : raise AmbientSMSError ( v ) return dictFromXml ( data )
4798	def exists ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a path' ) if not os . path . exists ( self . val ) : self . _err ( 'Expected <%s> to exist, but was not found.' % self . val ) return self
2218	def _rectify_countdown_or_bool ( count_or_bool ) : if count_or_bool is True or count_or_bool is False : count_or_bool_ = count_or_bool elif isinstance ( count_or_bool , int ) : if count_or_bool == 0 : return 0 elif count_or_bool > 0 : count_or_bool_ = count_or_bool - 1 else : count_or_bool_ = count_or_bool else : count_or_bool_ = False return count_or_bool_
3430	def add_reactions ( self , reaction_list ) : def existing_filter ( rxn ) : if rxn . id in self . reactions : LOGGER . warning ( "Ignoring reaction '%s' since it already exists." , rxn . id ) return False return True pruned = DictList ( filter ( existing_filter , reaction_list ) ) context = get_context ( self ) for reaction in pruned : reaction . _model = self for metabolite in list ( reaction . metabolites ) : if metabolite not in self . metabolites : self . add_metabolites ( metabolite ) else : stoichiometry = reaction . _metabolites . pop ( metabolite ) model_metabolite = self . metabolites . get_by_id ( metabolite . id ) reaction . _metabolites [ model_metabolite ] = stoichiometry model_metabolite . _reaction . add ( reaction ) if context : context ( partial ( model_metabolite . _reaction . remove , reaction ) ) for gene in list ( reaction . _genes ) : if not self . genes . has_id ( gene . id ) : self . genes += [ gene ] gene . _model = self if context : context ( partial ( self . genes . __isub__ , [ gene ] ) ) context ( partial ( setattr , gene , '_model' , None ) ) else : model_gene = self . genes . get_by_id ( gene . id ) if model_gene is not gene : reaction . _dissociate_gene ( gene ) reaction . _associate_gene ( model_gene ) self . reactions += pruned if context : context ( partial ( self . reactions . __isub__ , pruned ) ) self . _populate_solver ( pruned )
10572	def walk_depth ( path , max_depth = float ( 'inf' ) ) : start_level = os . path . abspath ( path ) . count ( os . path . sep ) for dir_entry in os . walk ( path ) : root , dirs , _ = dir_entry level = root . count ( os . path . sep ) - start_level yield dir_entry if level >= max_depth : dirs [ : ] = [ ]
2466	def set_concluded_license ( self , doc , lic ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_conc_lics_set : self . file_conc_lics_set = True if validations . validate_lics_conc ( lic ) : self . file ( doc ) . conc_lics = lic return True else : raise SPDXValueError ( 'File::ConcludedLicense' ) else : raise CardinalityError ( 'File::ConcludedLicense' ) else : raise OrderError ( 'File::ConcludedLicense' )
9676	def _calculate_period ( self , vals ) : if len ( vals ) < 4 : return None if self . firmware [ 'major' ] < 16 : return ( ( vals [ 3 ] << 24 ) | ( vals [ 2 ] << 16 ) | ( vals [ 1 ] << 8 ) | vals [ 0 ] ) / 12e6 else : return self . _calculate_float ( vals )
8130	def layer ( self , img , x = 0 , y = 0 , name = "" ) : from types import StringType if isinstance ( img , Image . Image ) : img = img . convert ( "RGBA" ) self . layers . append ( Layer ( self , img , x , y , name ) ) return len ( self . layers ) - 1 if isinstance ( img , Layer ) : img . canvas = self self . layers . append ( img ) return len ( self . layers ) - 1 if type ( img ) == StringType : img = Image . open ( img ) img = img . convert ( "RGBA" ) self . layers . append ( Layer ( self , img , x , y , name ) ) return len ( self . layers ) - 1
5936	def col ( self , c ) : m = self . COLOUR . search ( c ) if not m : self . logger . fatal ( "Cannot parse colour specification %r." , c ) raise ParseError ( "XPM reader: Cannot parse colour specification {0!r}." . format ( c ) ) value = m . group ( 'value' ) color = m . group ( 'symbol' ) self . logger . debug ( "%s: %s %s\n" , c . strip ( ) , color , value ) return color , value
10129	def _map_timezones ( ) : tz_map = { } todo = HAYSTACK_TIMEZONES_SET . copy ( ) for full_tz in pytz . all_timezones : if not bool ( todo ) : break if full_tz in todo : tz_map [ full_tz ] = full_tz todo . discard ( full_tz ) continue if '/' not in full_tz : continue ( prefix , suffix ) = full_tz . split ( '/' , 1 ) if '/' in suffix : continue if suffix in todo : tz_map [ suffix ] = full_tz todo . discard ( suffix ) continue return tz_map
12883	def _run_supervisor ( self ) : import time still_supervising = lambda : ( multiprocessing . active_children ( ) or not self . log_queue . empty ( ) or not self . exception_queue . empty ( ) ) try : while still_supervising ( ) : try : record = self . log_queue . get_nowait ( ) logger = logging . getLogger ( record . name ) logger . handle ( record ) except queue . Empty : pass try : exception = self . exception_queue . get_nowait ( ) except queue . Empty : pass else : raise exception time . sleep ( 1 / self . frame_rate ) self . elapsed_time += 1 / self . frame_rate if self . time_limit and self . elapsed_time > self . time_limit : raise RuntimeError ( "timeout" ) finally : for process in multiprocessing . active_children ( ) : process . terminate ( )
8795	def serialize_rules ( self , rules ) : serialized = [ ] for rule in rules : direction = rule [ "direction" ] source = '' destination = '' if rule . get ( "remote_ip_prefix" ) : prefix = rule [ "remote_ip_prefix" ] if direction == "ingress" : source = self . _convert_remote_network ( prefix ) else : if ( Capabilities . EGRESS not in CONF . QUARK . environment_capabilities ) : raise q_exc . EgressSecurityGroupRulesNotEnabled ( ) else : destination = self . _convert_remote_network ( prefix ) optional_fields = { } protocol_map = protocols . PROTOCOL_MAP [ rule [ "ethertype" ] ] if rule [ "protocol" ] == protocol_map [ "icmp" ] : optional_fields [ "icmp type" ] = rule [ "port_range_min" ] optional_fields [ "icmp code" ] = rule [ "port_range_max" ] else : optional_fields [ "port start" ] = rule [ "port_range_min" ] optional_fields [ "port end" ] = rule [ "port_range_max" ] payload = { "ethertype" : rule [ "ethertype" ] , "protocol" : rule [ "protocol" ] , "source network" : source , "destination network" : destination , "action" : "allow" , "direction" : direction } payload . update ( optional_fields ) serialized . append ( payload ) return serialized
10709	def _authenticate ( self ) : auth_url = BASE_URL + "/auth/token" payload = { 'username' : self . email , 'password' : self . password , 'grant_type' : 'password' } arequest = requests . post ( auth_url , data = payload , headers = BASIC_HEADERS ) status = arequest . status_code if status != 200 : _LOGGER . error ( "Authentication request failed, please check credintials. " + str ( status ) ) return False response = arequest . json ( ) _LOGGER . debug ( str ( response ) ) self . token = response . get ( "access_token" ) self . refresh_token = response . get ( "refresh_token" ) _auth = HEADERS . get ( "Authorization" ) _auth = _auth % self . token HEADERS [ "Authorization" ] = _auth _LOGGER . info ( "Authentication was successful, token set." ) return True
6965	def get ( self ) : if 'reviewed' not in self . currentproject : self . currentproject [ 'reviewed' ] = { } self . write ( self . currentproject )
12710	def relative_offset_to_world ( self , offset ) : return np . array ( self . body_to_world ( offset * self . dimensions / 2 ) )
6392	def _get_qgrams ( self , src , tar , qval = 0 , skip = 0 ) : if isinstance ( src , Counter ) and isinstance ( tar , Counter ) : return src , tar if qval > 0 : return QGrams ( src , qval , '$#' , skip ) , QGrams ( tar , qval , '$#' , skip ) return Counter ( src . strip ( ) . split ( ) ) , Counter ( tar . strip ( ) . split ( ) )
2304	def _run_gies ( self , data , fixedGaps = None , verbose = True ) : id = str ( uuid . uuid4 ( ) ) os . makedirs ( '/tmp/cdt_gies' + id + '/' ) self . arguments [ '{FOLDER}' ] = '/tmp/cdt_gies' + id + '/' def retrieve_result ( ) : return read_csv ( '/tmp/cdt_gies' + id + '/result.csv' , delimiter = ',' ) . values try : data . to_csv ( '/tmp/cdt_gies' + id + '/data.csv' , header = False , index = False ) if fixedGaps is not None : fixedGaps . to_csv ( '/tmp/cdt_gies' + id + '/fixedgaps.csv' , index = False , header = False ) self . arguments [ '{SKELETON}' ] = 'TRUE' else : self . arguments [ '{SKELETON}' ] = 'FALSE' gies_result = launch_R_script ( "{}/R_templates/gies.R" . format ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) , self . arguments , output_function = retrieve_result , verbose = verbose ) except Exception as e : rmtree ( '/tmp/cdt_gies' + id + '' ) raise e except KeyboardInterrupt : rmtree ( '/tmp/cdt_gies' + id + '/' ) raise KeyboardInterrupt rmtree ( '/tmp/cdt_gies' + id + '' ) return gies_result
6098	def luminosity_within_ellipse_in_units ( self , major_axis , unit_luminosity = 'eps' , kpc_per_arcsec = None , exposure_time = None ) : if not isinstance ( major_axis , dim . Length ) : major_axis = dim . Length ( major_axis , 'arcsec' ) profile = self . new_profile_with_units_converted ( unit_length = major_axis . unit_length , unit_luminosity = unit_luminosity , kpc_per_arcsec = kpc_per_arcsec , exposure_time = exposure_time ) luminosity = quad ( profile . luminosity_integral , a = 0.0 , b = major_axis , args = ( self . axis_ratio , ) ) [ 0 ] return dim . Luminosity ( luminosity , unit_luminosity )
12750	def set_pid_params ( self , * args , ** kwargs ) : for joint in self . joints : joint . target_angles = [ None ] * joint . ADOF joint . controllers = [ pid ( * args , ** kwargs ) for i in range ( joint . ADOF ) ]
7330	def stream_request ( self , method , url , headers = None , _session = None , * args , ** kwargs ) : return StreamResponse ( method = method , url = url , client = self , headers = headers , session = _session , proxy = self . proxy , ** kwargs )
3874	def _add_conversation ( self , conversation , events = [ ] , event_cont_token = None ) : conv_id = conversation . conversation_id . id logger . debug ( 'Adding new conversation: {}' . format ( conv_id ) ) conv = Conversation ( self . _client , self . _user_list , conversation , events , event_cont_token ) self . _conv_dict [ conv_id ] = conv return conv
9200	def _sort_lows_and_highs ( func ) : "Decorator for extract_cycles" @ functools . wraps ( func ) def wrapper ( * args , ** kwargs ) : for low , high , mult in func ( * args , ** kwargs ) : if low < high : yield low , high , mult else : yield high , low , mult return wrapper
9323	def refresh_information ( self , accept = MEDIA_TYPE_TAXII_V20 ) : response = self . __raw = self . _conn . get ( self . url , headers = { "Accept" : accept } ) self . _populate_fields ( ** response ) self . _loaded_information = True
9601	def wait_for_element ( self , using , value , timeout = 10000 , interval = 1000 , asserter = is_displayed ) : if not callable ( asserter ) : raise TypeError ( 'Asserter must be callable.' ) @ retry ( retry_on_exception = lambda ex : isinstance ( ex , WebDriverException ) , stop_max_delay = timeout , wait_fixed = interval ) def _wait_for_element ( ctx , using , value ) : el = ctx . element ( using , value ) asserter ( el ) return el return _wait_for_element ( self , using , value )
3856	def unread_events ( self ) : return [ conv_event for conv_event in self . _events if conv_event . timestamp > self . latest_read_timestamp ]
9862	async def update_info ( self , * _ ) : query = gql ( ) res = await self . _execute ( query ) if res is None : return errors = res . get ( "errors" , [ ] ) if errors : msg = errors [ 0 ] . get ( "message" , "failed to login" ) _LOGGER . error ( msg ) raise InvalidLogin ( msg ) data = res . get ( "data" ) if not data : return viewer = data . get ( "viewer" ) if not viewer : return self . _name = viewer . get ( "name" ) homes = viewer . get ( "homes" , [ ] ) self . _home_ids = [ ] for _home in homes : home_id = _home . get ( "id" ) self . _all_home_ids += [ home_id ] subs = _home . get ( "subscriptions" ) if subs : status = subs [ 0 ] . get ( "status" , "ended" ) . lower ( ) if not home_id or status != "running" : continue self . _home_ids += [ home_id ]
6642	def satisfyDependenciesRecursive ( self , available_components = None , search_dirs = None , update_installed = False , traverse_links = False , target = None , test = False ) : def provider ( dspec , available_components , search_dirs , working_directory , update_installed , dep_of = None ) : r = access . satisfyFromAvailable ( dspec . name , available_components ) if r : if r . isTestDependency ( ) and not dspec . is_test_dependency : logger . debug ( 'test dependency subsequently occurred as real dependency: %s' , r . getName ( ) ) r . setTestDependency ( False ) return r update_if_installed = False if update_installed is True : update_if_installed = True elif update_installed : update_if_installed = dspec . name in update_installed r = access . satisfyVersionFromSearchPaths ( dspec . name , dspec . versionReq ( ) , search_dirs , update_if_installed , inherit_shrinkwrap = dep_of . getShrinkwrap ( ) ) if r : r . setTestDependency ( dspec . is_test_dependency ) return r default_path = os . path . join ( self . modulesPath ( ) , dspec . name ) if fsutils . isLink ( default_path ) : r = Component ( default_path , test_dependency = dspec . is_test_dependency , installed_linked = fsutils . isLink ( default_path ) , inherit_shrinkwrap = dep_of . getShrinkwrap ( ) ) if r : assert ( r . installedLinked ( ) ) return r else : logger . error ( 'linked module %s is invalid: %s' , dspec . name , r . getError ( ) ) return r r = access . satisfyVersionByInstalling ( dspec . name , dspec . versionReq ( ) , self . modulesPath ( ) , inherit_shrinkwrap = dep_of . getShrinkwrap ( ) ) if not r : logger . error ( 'could not install %s' % dspec . name ) if r is not None : r . setTestDependency ( dspec . is_test_dependency ) return r return self . __getDependenciesRecursiveWithProvider ( available_components = available_components , search_dirs = search_dirs , target = target , traverse_links = traverse_links , update_installed = update_installed , provider = provider , test = test )
8758	def get_subnet ( context , id , fields = None ) : LOG . info ( "get_subnet %s for tenant %s with fields %s" % ( id , context . tenant_id , fields ) ) subnet = db_api . subnet_find ( context = context , limit = None , page_reverse = False , sorts = [ 'id' ] , marker_obj = None , fields = None , id = id , join_dns = True , join_routes = True , scope = db_api . ONE ) if not subnet : raise n_exc . SubnetNotFound ( subnet_id = id ) cache = subnet . get ( "_allocation_pool_cache" ) if not cache : new_cache = subnet . allocation_pools db_api . subnet_update_set_alloc_pool_cache ( context , subnet , new_cache ) return v . _make_subnet_dict ( subnet )
987	def mmPrettyPrintSequenceCellRepresentations ( self , sortby = "Column" ) : self . _mmComputeTransitionTraces ( ) table = PrettyTable ( [ "Pattern" , "Column" , "predicted=>active cells" ] ) for sequenceLabel , predictedActiveCells in ( self . _mmData [ "predictedActiveCellsForSequence" ] . iteritems ( ) ) : cellsForColumn = self . mapCellsToColumns ( predictedActiveCells ) for column , cells in cellsForColumn . iteritems ( ) : table . add_row ( [ sequenceLabel , column , list ( cells ) ] ) return table . get_string ( sortby = sortby ) . encode ( "utf-8" )
13672	def init_build ( self , asset , builder ) : if not self . abs_path : rel_path = utils . prepare_path ( self . rel_bundle_path ) self . abs_bundle_path = utils . prepare_path ( [ builder . config . input_dir , rel_path ] ) self . abs_path = True self . input_dir = builder . config . input_dir
4319	def _parse_stat ( stat_output ) : lines = stat_output . split ( '\n' ) stat_dict = { } for line in lines : split_line = line . split ( ':' ) if len ( split_line ) == 2 : key = split_line [ 0 ] val = split_line [ 1 ] . strip ( ' ' ) try : val = float ( val ) except ValueError : val = None stat_dict [ key ] = val return stat_dict
678	def generateRecords ( self , records ) : if self . verbosity > 0 : print 'Generating' , len ( records ) , 'records...' for record in records : self . generateRecord ( record )
5924	def setup ( filename = CONFIGNAME ) : get_configuration ( ) if not os . path . exists ( filename ) : with open ( filename , 'w' ) as configfile : cfg . write ( configfile ) msg = "NOTE: GromacsWrapper created the configuration file \n\t%r\n" " for you. Edit the file to customize the package." % filename print ( msg ) for d in config_directories : utilities . mkdir_p ( d )
9189	def get_api_keys ( request ) : with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( ) api_keys = [ x [ 0 ] for x in cursor . fetchall ( ) ] return api_keys
6750	def register ( self ) : self . _set_defaults ( ) all_satchels [ self . name . upper ( ) ] = self manifest_recorder [ self . name ] = self . record_manifest if self . required_system_packages : required_system_packages [ self . name . upper ( ) ] = self . required_system_packages
4335	def oops ( self ) : effect_args = [ 'oops' ] self . effects . extend ( effect_args ) self . effects_log . append ( 'oops' ) return self
7118	def merge_dicts ( d1 , d2 , _path = None ) : if _path is None : _path = ( ) if isinstance ( d1 , dict ) and isinstance ( d2 , dict ) : for k , v in d2 . items ( ) : if isinstance ( v , MissingValue ) and v . name is None : v . name = '.' . join ( _path + ( k , ) ) if isinstance ( v , DeletedValue ) : d1 . pop ( k , None ) elif k not in d1 : if isinstance ( v , dict ) : d1 [ k ] = merge_dicts ( { } , v , _path + ( k , ) ) else : d1 [ k ] = v else : if isinstance ( d1 [ k ] , dict ) and isinstance ( v , dict ) : d1 [ k ] = merge_dicts ( d1 [ k ] , v , _path + ( k , ) ) elif isinstance ( d1 [ k ] , list ) and isinstance ( v , list ) : d1 [ k ] += v elif isinstance ( d1 [ k ] , MissingValue ) : d1 [ k ] = v elif d1 [ k ] is None : d1 [ k ] = v elif type ( d1 [ k ] ) == type ( v ) : d1 [ k ] = v else : raise TypeError ( 'Refusing to replace a %s with a %s' % ( type ( d1 [ k ] ) , type ( v ) ) ) else : raise TypeError ( 'Cannot merge a %s with a %s' % ( type ( d1 ) , type ( d2 ) ) ) return d1
4217	def delete_password ( self , service , username ) : if not self . connected ( service ) : raise PasswordDeleteError ( "Cancelled by user" ) if not self . iface . hasEntry ( self . handle , service , username , self . appid ) : raise PasswordDeleteError ( "Password not found" ) self . iface . removeEntry ( self . handle , service , username , self . appid )
6054	def regular_to_sparse_from_sparse_mappings ( regular_to_unmasked_sparse , unmasked_sparse_to_sparse ) : total_regular_pixels = regular_to_unmasked_sparse . shape [ 0 ] regular_to_sparse = np . zeros ( total_regular_pixels ) for regular_index in range ( total_regular_pixels ) : regular_to_sparse [ regular_index ] = unmasked_sparse_to_sparse [ regular_to_unmasked_sparse [ regular_index ] ] return regular_to_sparse
7537	def branch_assembly ( args , parsedict ) : data = getassembly ( args , parsedict ) bargs = args . branch newname = bargs [ 0 ] if newname . endswith ( ".txt" ) : newname = newname [ : - 4 ] if len ( bargs ) > 1 : if any ( [ x . stats . state == 6 for x in data . samples . values ( ) ] ) : pass subsamples = bargs [ 1 : ] if bargs [ 1 ] == "-" : fails = [ i for i in subsamples [ 1 : ] if i not in data . samples . keys ( ) ] if any ( fails ) : raise IPyradWarningExit ( "\ \n Failed: unrecognized names requested, check spelling:\n {}" . format ( "\n " . join ( [ i for i in fails ] ) ) ) print ( " dropping {} samples" . format ( len ( subsamples ) - 1 ) ) subsamples = list ( set ( data . samples . keys ( ) ) - set ( subsamples ) ) if os . path . exists ( bargs [ 1 ] ) : new_data = data . branch ( newname , infile = bargs [ 1 ] ) else : new_data = data . branch ( newname , subsamples ) else : new_data = data . branch ( newname , None ) print ( " creating a new branch called '{}' with {} Samples" . format ( new_data . name , len ( new_data . samples ) ) ) print ( " writing new params file to {}" . format ( "params-" + new_data . name + ".txt\n" ) ) new_data . write_params ( "params-" + new_data . name + ".txt" , force = args . force )
8988	def last_consumed_mesh ( self ) : for instruction in reversed ( self . instructions ) : if instruction . consumes_meshes ( ) : return instruction . last_consumed_mesh raise IndexError ( "{} consumes no meshes" . format ( self ) )
1822	def SETPO ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . PF == False , 1 , 0 ) )
10671	def _finalise_result_ ( compound , value , mass ) : result = value / 3.6E6 result = result / compound . molar_mass result = result * mass return result
4014	def register_consumer ( ) : global _consumers hostname , port = request . form [ 'hostname' ] , request . form [ 'port' ] app_name = _app_name_from_forwarding_info ( hostname , port ) containers = get_dusty_containers ( [ app_name ] , include_exited = True ) if not containers : raise ValueError ( 'No container exists for app {}' . format ( app_name ) ) container = containers [ 0 ] new_id = uuid1 ( ) new_consumer = Consumer ( container [ 'Id' ] , datetime . utcnow ( ) ) _consumers [ str ( new_id ) ] = new_consumer response = jsonify ( { 'app_name' : app_name , 'consumer_id' : new_id } ) response . headers [ 'Access-Control-Allow-Origin' ] = '*' response . headers [ 'Access-Control-Allow-Methods' ] = 'GET, POST' return response
13840	def ConsumeBool ( self ) : try : result = ParseBool ( self . token ) except ValueError as e : raise self . _ParseError ( str ( e ) ) self . NextToken ( ) return result
6081	def potential_of_galaxies_from_grid ( grid , galaxies ) : if galaxies : return sum ( map ( lambda g : g . potential_from_grid ( grid ) , galaxies ) ) else : return np . full ( ( grid . shape [ 0 ] ) , 0.0 )
10654	def prepare_to_run ( self , clock , period_count ) : self . period_count = period_count self . _exec_year_end_datetime = clock . get_datetime_at_period_ix ( period_count ) self . _prev_year_end_datetime = clock . start_datetime self . _curr_year_end_datetime = clock . start_datetime + relativedelta ( years = 1 ) del self . gl . transactions [ : ] for c in self . components : c . prepare_to_run ( clock , period_count ) self . negative_income_tax_total = 0
5885	def get_canonical_link ( self ) : if self . article . final_url : kwargs = { 'tag' : 'link' , 'attr' : 'rel' , 'value' : 'canonical' } meta = self . parser . getElementsByTag ( self . article . doc , ** kwargs ) if meta is not None and len ( meta ) > 0 : href = self . parser . getAttribute ( meta [ 0 ] , 'href' ) if href : href = href . strip ( ) o = urlparse ( href ) if not o . hostname : tmp = urlparse ( self . article . final_url ) domain = '%s://%s' % ( tmp . scheme , tmp . hostname ) href = urljoin ( domain , href ) return href return self . article . final_url
12146	def analyzeSingle ( abfFname ) : assert os . path . exists ( abfFname ) and abfFname . endswith ( ".abf" ) ABFfolder , ABFfname = os . path . split ( abfFname ) abfID = os . path . splitext ( ABFfname ) [ 0 ] IN = INDEX ( ABFfolder ) IN . analyzeABF ( abfID ) IN . scan ( ) IN . html_single_basic ( [ abfID ] , overwrite = True ) IN . html_single_plot ( [ abfID ] , overwrite = True ) IN . scan ( ) IN . html_index ( ) return
5173	def get_install_requires ( ) : requirements = [ ] for line in open ( 'requirements.txt' ) . readlines ( ) : if line . startswith ( '#' ) or line == '' or line . startswith ( 'http' ) or line . startswith ( 'git' ) : continue requirements . append ( line . replace ( '\n' , '' ) ) if sys . version_info . major < 3 : requirements . append ( 'py2-ipaddress' ) return requirements
13130	def parse_single_computer ( entry ) : computer = Computer ( dns_hostname = get_field ( entry , 'dNSHostName' ) , description = get_field ( entry , 'description' ) , os = get_field ( entry , 'operatingSystem' ) , group_id = get_field ( entry , 'primaryGroupID' ) ) try : ip = str ( ipaddress . ip_address ( get_field ( entry , 'IPv4' ) ) ) except ValueError : ip = '' if ip : computer . ip = ip elif computer . dns_hostname : computer . ip = resolve_ip ( computer . dns_hostname ) return computer
8084	def scale ( self , x = 1 , y = None ) : if not y : y = x if x == 0 : x = 1 if y == 0 : y = 1 self . _canvas . scale ( x , y )
2436	def add_reviewer ( self , doc , reviewer ) : self . reset_reviews ( ) if validations . validate_reviewer ( reviewer ) : doc . add_review ( review . Review ( reviewer = reviewer ) ) return True else : raise SPDXValueError ( 'Review::Reviewer' )
5193	def send_select_and_operate_command_set ( self , command_set , callback = asiodnp3 . PrintingCommandCallback . Get ( ) , config = opendnp3 . TaskConfig ( ) . Default ( ) ) : self . master . SelectAndOperate ( command_set , callback , config )
7955	def getpeercert ( self ) : with self . lock : if not self . _socket or self . _tls_state != "connected" : raise ValueError ( "Not TLS-connected" ) return get_certificate_from_ssl_socket ( self . _socket )
13568	def selected_course ( func ) : @ wraps ( func ) def inner ( * args , ** kwargs ) : course = Course . get_selected ( ) return func ( course , * args , ** kwargs ) return inner
6108	def yticks ( self ) : return np . linspace ( np . amin ( self . grid_stack . regular [ : , 0 ] ) , np . amax ( self . grid_stack . regular [ : , 0 ] ) , 4 )
4458	def sort_by ( self , field , asc = True ) : self . _sortby = SortbyField ( field , asc ) return self
8943	def search_file_upwards ( name , base = None ) : base = base or os . getcwd ( ) while base != os . path . dirname ( base ) : if os . path . exists ( os . path . join ( base , name ) ) : return base base = os . path . dirname ( base ) return None
8809	def delete_mac_address_range ( context , id ) : LOG . info ( "delete_mac_address_range %s for tenant %s" % ( id , context . tenant_id ) ) if not context . is_admin : raise n_exc . NotAuthorized ( ) with context . session . begin ( ) : mar = db_api . mac_address_range_find ( context , id = id , scope = db_api . ONE ) if not mar : raise q_exc . MacAddressRangeNotFound ( mac_address_range_id = id ) _delete_mac_address_range ( context , mar )
6591	def receive ( self ) : ret = [ ] while True : if self . runid_pkgidx_map : self . runid_to_return . extend ( self . dispatcher . poll ( ) ) ret . extend ( self . _collect_all_finished_pkgidx_result_pairs ( ) ) if not self . runid_pkgidx_map : break time . sleep ( self . sleep ) ret = sorted ( ret , key = itemgetter ( 0 ) ) return ret
8456	def up_to_date ( version = None ) : temple . check . in_git_repo ( ) temple . check . is_temple_project ( ) temple_config = temple . utils . read_temple_config ( ) old_template_version = temple_config [ '_version' ] new_template_version = version or _get_latest_template_version ( temple_config [ '_template' ] ) return new_template_version == old_template_version
12670	def create ( _ ) : endpoint = client_endpoint ( ) if not endpoint : raise CLIError ( "Connection endpoint not found. " "Before running sfctl commands, connect to a cluster using " "the 'sfctl cluster select' command." ) no_verify = no_verify_setting ( ) if security_type ( ) == 'aad' : auth = AdalAuthentication ( no_verify ) else : cert = cert_info ( ) ca_cert = ca_cert_info ( ) auth = ClientCertAuthentication ( cert , ca_cert , no_verify ) return ServiceFabricClientAPIs ( auth , base_url = endpoint )
13060	def get_reffs ( self , objectId , subreference = None , collection = None , export_collection = False ) : if collection is not None : text = collection else : text = self . get_collection ( objectId ) reffs = self . chunk ( text , lambda level : self . resolver . getReffs ( objectId , level = level , subreference = subreference ) ) if export_collection is True : return text , reffs return reffs
3659	def add_coeffs ( self , Tmin , Tmax , coeffs ) : self . n += 1 if not self . Ts : self . Ts = [ Tmin , Tmax ] self . coeff_sets = [ coeffs ] else : for ind , T in enumerate ( self . Ts ) : if Tmin < T : self . Ts . insert ( ind , Tmin ) self . coeff_sets . insert ( ind , coeffs ) return self . Ts . append ( Tmax ) self . coeff_sets . append ( coeffs )
8984	def to_svg ( self , instruction_or_id , i_promise_not_to_change_the_result = False ) : return self . _new_svg_dumper ( lambda : self . instruction_to_svg_dict ( instruction_or_id , not i_promise_not_to_change_the_result ) )
11089	def _sort_by ( key ) : @ staticmethod def sort_by ( p_list , reverse = False ) : return sorted ( p_list , key = lambda p : getattr ( p , key ) , reverse = reverse , ) return sort_by
3731	def checkCAS ( CASRN ) : try : check = CASRN [ - 1 ] CASRN = CASRN [ : : - 1 ] [ 1 : ] productsum = 0 i = 1 for num in CASRN : if num == '-' : pass else : productsum += i * int ( num ) i += 1 return ( productsum % 10 == int ( check ) ) except : return False
1719	def emit ( self , what , * args ) : if isinstance ( what , basestring ) : return self . exe . emit ( what , * args ) elif isinstance ( what , list ) : self . _emit_statement_list ( what ) else : return getattr ( self , what [ 'type' ] ) ( ** what )
6597	def receive ( self ) : ret = self . communicationChannel . receive_all ( ) self . nruns -= len ( ret ) if self . nruns > 0 : import logging logger = logging . getLogger ( __name__ ) logger . warning ( 'too few results received: {} results received, {} more expected' . format ( len ( ret ) , self . nruns ) ) elif self . nruns < 0 : import logging logger = logging . getLogger ( __name__ ) logger . warning ( 'too many results received: {} results received, {} too many' . format ( len ( ret ) , - self . nruns ) ) return ret
13447	def authorize ( self ) : response = self . client . login ( username = self . USERNAME , password = self . PASSWORD ) self . assertTrue ( response ) self . authed = True
7641	def parse_arguments ( args ) : parser = argparse . ArgumentParser ( description = 'Convert JAMS to .lab files' ) parser . add_argument ( '-c' , '--comma-separated' , dest = 'csv' , action = 'store_true' , default = False , help = 'Output in .csv instead of .lab' ) parser . add_argument ( '--comment' , dest = 'comment_char' , type = str , default = '#' , help = 'Comment character' ) parser . add_argument ( '-n' , '--namespace' , dest = 'namespaces' , nargs = '+' , default = [ '.*' ] , help = 'One or more namespaces to output. Default is all.' ) parser . add_argument ( 'jams_file' , help = 'Path to the input jams file' ) parser . add_argument ( 'output_prefix' , help = 'Prefix for output files' ) return vars ( parser . parse_args ( args ) )
11160	def trail_space ( self , filters = lambda p : p . ext == ".py" ) : self . assert_is_dir_and_exists ( ) for p in self . select_file ( filters ) : try : with open ( p . abspath , "rb" ) as f : lines = list ( ) for line in f : lines . append ( line . decode ( "utf-8" ) . rstrip ( ) ) with open ( p . abspath , "wb" ) as f : f . write ( "\n" . join ( lines ) . encode ( "utf-8" ) ) except Exception as e : raise e
4519	def set_project ( self , project ) : def visit ( x ) : set_project = getattr ( x , 'set_project' , None ) if set_project : set_project ( project ) values = getattr ( x , 'values' , lambda : ( ) ) for v in values ( ) : visit ( v ) visit ( self . routing )
5341	def __get_menu_entries ( self , kibiter_major ) : menu_entries = [ ] for entry in self . panels_menu : if entry [ 'source' ] not in self . data_sources : continue parent_menu_item = { 'name' : entry [ 'name' ] , 'title' : entry [ 'name' ] , 'description' : "" , 'type' : "menu" , 'dashboards' : [ ] } for subentry in entry [ 'menu' ] : try : dash_name = get_dashboard_name ( subentry [ 'panel' ] ) except FileNotFoundError : logging . error ( "Can't open dashboard file %s" , subentry [ 'panel' ] ) continue child_item = { "name" : subentry [ 'name' ] , "title" : subentry [ 'name' ] , "description" : "" , "type" : "entry" , "panel_id" : dash_name } parent_menu_item [ 'dashboards' ] . append ( child_item ) menu_entries . append ( parent_menu_item ) return menu_entries
6384	def mean_pairwise_similarity ( collection , metric = sim , mean_func = hmean , symmetric = False ) : if not callable ( mean_func ) : raise ValueError ( 'mean_func must be a function' ) if not callable ( metric ) : raise ValueError ( 'metric must be a function' ) if hasattr ( collection , 'split' ) : collection = collection . split ( ) if not hasattr ( collection , '__iter__' ) : raise ValueError ( 'collection is neither a string nor iterable type' ) elif len ( collection ) < 2 : raise ValueError ( 'collection has fewer than two members' ) collection = list ( collection ) pairwise_values = [ ] for i in range ( len ( collection ) ) : for j in range ( i + 1 , len ( collection ) ) : pairwise_values . append ( metric ( collection [ i ] , collection [ j ] ) ) if symmetric : pairwise_values . append ( metric ( collection [ j ] , collection [ i ] ) ) return mean_func ( pairwise_values )
712	def _launchWorkers ( self , cmdLine , numWorkers ) : self . _workers = [ ] for i in range ( numWorkers ) : stdout = tempfile . NamedTemporaryFile ( delete = False ) stderr = tempfile . NamedTemporaryFile ( delete = False ) p = subprocess . Popen ( cmdLine , bufsize = 1 , env = os . environ , shell = True , stdin = None , stdout = stdout , stderr = stderr ) p . _stderr_file = stderr p . _stdout_file = stdout self . _workers . append ( p )
961	def initLogger ( obj ) : if inspect . isclass ( obj ) : myClass = obj else : myClass = obj . __class__ logger = logging . getLogger ( "." . join ( [ 'com.numenta' , myClass . __module__ , myClass . __name__ ] ) ) return logger
1618	def IsCppString ( line ) : line = line . replace ( r'\\' , 'XX' ) return ( ( line . count ( '"' ) - line . count ( r'\"' ) - line . count ( "'\"'" ) ) & 1 ) == 1
8848	def mousePressEvent ( self , e ) : super ( PyInteractiveConsole , self ) . mousePressEvent ( e ) cursor = self . cursorForPosition ( e . pos ( ) ) p = cursor . positionInBlock ( ) usd = cursor . block ( ) . userData ( ) if usd and usd . start_pos_in_block <= p <= usd . end_pos_in_block : if e . button ( ) == QtCore . Qt . LeftButton : self . open_file_requested . emit ( usd . filename , usd . line )
3387	def _is_redundant ( self , matrix , cutoff = None ) : cutoff = 1.0 - self . feasibility_tol extra_col = matrix [ : , 0 ] + 1 extra_col [ matrix . sum ( axis = 1 ) == 0 ] = 2 corr = np . corrcoef ( np . c_ [ matrix , extra_col ] ) corr = np . tril ( corr , - 1 ) return ( np . abs ( corr ) > cutoff ) . any ( axis = 1 )
9447	def hangup_all_calls ( self ) : path = '/' + self . api_version + '/HangupAllCalls/' method = 'POST' return self . request ( path , method )
10885	def oslicer ( self , tile ) : mask = None vecs = tile . coords ( form = 'meshed' ) for v in vecs : v [ self . slicer ] = - 1 mask = mask & ( v > 0 ) if mask is not None else ( v > 0 ) return tuple ( np . array ( i ) . astype ( 'int' ) for i in zip ( * [ v [ mask ] for v in vecs ] ) )
1379	def print_build_info ( zipped_pex = False ) : if zipped_pex : release_file = get_zipped_heron_release_file ( ) else : release_file = get_heron_release_file ( ) with open ( release_file ) as release_info : release_map = yaml . load ( release_info ) release_items = sorted ( release_map . items ( ) , key = lambda tup : tup [ 0 ] ) for key , value in release_items : print ( "%s : %s" % ( key , value ) )
672	def getPredictionResults ( network , clRegionName ) : classifierRegion = network . regions [ clRegionName ] actualValues = classifierRegion . getOutputData ( "actualValues" ) probabilities = classifierRegion . getOutputData ( "probabilities" ) steps = classifierRegion . getSelf ( ) . stepsList N = classifierRegion . getSelf ( ) . maxCategoryCount results = { step : { } for step in steps } for i in range ( len ( steps ) ) : stepProbabilities = probabilities [ i * N : ( i + 1 ) * N - 1 ] mostLikelyCategoryIdx = stepProbabilities . argmax ( ) predictedValue = actualValues [ mostLikelyCategoryIdx ] predictionConfidence = stepProbabilities [ mostLikelyCategoryIdx ] results [ steps [ i ] ] [ "predictedValue" ] = predictedValue results [ steps [ i ] ] [ "predictionConfidence" ] = predictionConfidence return results
4749	def get_parm ( self , key ) : if key in self . __parm . keys ( ) : return self . __parm [ key ] return None
3850	def fetch_raw ( self , method , url , params = None , headers = None , data = None ) : if not urllib . parse . urlparse ( url ) . hostname . endswith ( '.google.com' ) : raise Exception ( 'expected google.com domain' ) headers = headers or { } headers . update ( self . _authorization_headers ) return self . _session . request ( method , url , params = params , headers = headers , data = data , proxy = self . _proxy )
840	def closestTrainingPattern ( self , inputPattern , cat ) : dist = self . _getDistances ( inputPattern ) sorted = dist . argsort ( ) for patIdx in sorted : patternCat = self . _categoryList [ patIdx ] if patternCat == cat : if self . useSparseMemory : closestPattern = self . _Memory . getRow ( int ( patIdx ) ) else : closestPattern = self . _M [ patIdx ] return closestPattern return None
9716	async def await_event ( self , event = None , timeout = 30 ) : return await self . _protocol . await_event ( event , timeout = timeout )
3491	def _sbase_annotations ( sbase , annotation ) : if not annotation or len ( annotation ) == 0 : return annotation_data = deepcopy ( annotation ) for key , value in annotation_data . items ( ) : if isinstance ( value , ( float , int ) ) : value = str ( value ) if isinstance ( value , string_types ) : annotation_data [ key ] = [ ( "is" , value ) ] for key , value in annotation_data . items ( ) : for idx , item in enumerate ( value ) : if isinstance ( item , string_types ) : value [ idx ] = ( "is" , item ) meta_id = "meta_{}" . format ( sbase . getId ( ) ) sbase . setMetaId ( meta_id ) for provider , data in iteritems ( annotation_data ) : if provider in [ "SBO" , "sbo" ] : if provider == "SBO" : LOGGER . warning ( "'SBO' provider is deprecated, " "use 'sbo' provider instead" ) sbo_term = data [ 0 ] [ 1 ] _check ( sbase . setSBOTerm ( sbo_term ) , "Setting SBOTerm: {}" . format ( sbo_term ) ) continue for item in data : qualifier_str , entity = item [ 0 ] , item [ 1 ] qualifier = QUALIFIER_TYPES . get ( qualifier_str , None ) if qualifier is None : qualifier = libsbml . BQB_IS LOGGER . error ( "Qualifier type is not supported on " "annotation: '{}'" . format ( qualifier_str ) ) qualifier_type = libsbml . BIOLOGICAL_QUALIFIER if qualifier_str . startswith ( "bqm_" ) : qualifier_type = libsbml . MODEL_QUALIFIER cv = libsbml . CVTerm ( ) cv . setQualifierType ( qualifier_type ) if qualifier_type == libsbml . BIOLOGICAL_QUALIFIER : cv . setBiologicalQualifierType ( qualifier ) elif qualifier_type == libsbml . MODEL_QUALIFIER : cv . setModelQualifierType ( qualifier ) else : raise CobraSBMLError ( 'Unsupported qualifier: ' '%s' % qualifier ) resource = "%s/%s/%s" % ( URL_IDENTIFIERS_PREFIX , provider , entity ) cv . addResource ( resource ) _check ( sbase . addCVTerm ( cv ) , "Setting cvterm: {}, resource: {}" . format ( cv , resource ) )
8373	def widget_changed ( self , widget , v ) : if v . type is NUMBER : self . bot . _namespace [ v . name ] = widget . get_value ( ) self . bot . _vars [ v . name ] . value = widget . get_value ( ) publish_event ( VARIABLE_UPDATED_EVENT , v ) elif v . type is BOOLEAN : self . bot . _namespace [ v . name ] = widget . get_active ( ) self . bot . _vars [ v . name ] . value = widget . get_active ( ) publish_event ( VARIABLE_UPDATED_EVENT , v ) elif v . type is TEXT : self . bot . _namespace [ v . name ] = widget . get_text ( ) self . bot . _vars [ v . name ] . value = widget . get_text ( ) publish_event ( VARIABLE_UPDATED_EVENT , v )
155	def prev_key ( self , key , default = _sentinel ) : item = self . prev_item ( key , default ) return default if item is default else item [ 0 ]
7778	def __from_xml ( self , data ) : ns = get_node_ns ( data ) if ns and ns . getContent ( ) != VCARD_NS : raise ValueError ( "Not in the %r namespace" % ( VCARD_NS , ) ) if data . name != "vCard" : raise ValueError ( "Bad root element name: %r" % ( data . name , ) ) n = data . children dns = get_node_ns ( data ) while n : if n . type != 'element' : n = n . next continue ns = get_node_ns ( n ) if ( ns and dns and ns . getContent ( ) != dns . getContent ( ) ) : n = n . next continue if not self . components . has_key ( n . name ) : n = n . next continue cl , tp = self . components [ n . name ] if tp in ( "required" , "optional" ) : if self . content . has_key ( n . name ) : raise ValueError ( "Duplicate %s" % ( n . name , ) ) try : self . content [ n . name ] = cl ( n . name , n ) except Empty : pass elif tp == "multi" : if not self . content . has_key ( n . name ) : self . content [ n . name ] = [ ] try : self . content [ n . name ] . append ( cl ( n . name , n ) ) except Empty : pass n = n . next
12165	def once ( self , event , listener ) : self . emit ( 'new_listener' , event , listener ) self . _once [ event ] . append ( listener ) self . _check_limit ( event ) return self
762	def getRandomWithMods ( inputSpace , maxChanges ) : size = len ( inputSpace ) ind = np . random . random_integers ( 0 , size - 1 , 1 ) [ 0 ] value = copy . deepcopy ( inputSpace [ ind ] ) if maxChanges == 0 : return value return modifyBits ( value , maxChanges )
4206	def levup ( acur , knxt , ecur = None ) : if acur [ 0 ] != 1 : raise ValueError ( 'At least one of the reflection coefficients is equal to one.' ) acur = acur [ 1 : ] anxt = numpy . concatenate ( ( acur , [ 0 ] ) ) + knxt * numpy . concatenate ( ( numpy . conj ( acur [ - 1 : : - 1 ] ) , [ 1 ] ) ) enxt = None if ecur is not None : enxt = ( 1. - numpy . dot ( numpy . conj ( knxt ) , knxt ) ) * ecur anxt = numpy . insert ( anxt , 0 , 1 ) return anxt , enxt
2816	def convert_maxpool3 ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting pooling ...' ) if names == 'short' : tf_name = 'P' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'kernel_shape' in params : height , width , depth = params [ 'kernel_shape' ] else : height , width , depth = params [ 'kernel_size' ] if 'strides' in params : stride_height , stride_width , stride_depth = params [ 'strides' ] else : stride_height , stride_width , stride_depth = params [ 'stride' ] if 'pads' in params : padding_h , padding_w , padding_d , _ , _ = params [ 'pads' ] else : padding_h , padding_w , padding_d = params [ 'padding' ] input_name = inputs [ 0 ] if padding_h > 0 and padding_w > 0 and padding_d > 0 : padding_name = tf_name + '_pad' padding_layer = keras . layers . ZeroPadding3D ( padding = ( padding_h , padding_w , padding_d ) , name = padding_name ) layers [ padding_name ] = padding_layer ( layers [ inputs [ 0 ] ] ) input_name = padding_name pooling = keras . layers . MaxPooling3D ( pool_size = ( height , width , depth ) , strides = ( stride_height , stride_width , stride_depth ) , padding = 'valid' , name = tf_name ) layers [ scope_name ] = pooling ( layers [ input_name ] )
5842	def get_design_run_status ( self , data_view_id , run_uuid ) : url = routes . get_data_view_design_status ( data_view_id , run_uuid ) response = self . _get ( url ) . json ( ) status = response [ "data" ] return ProcessStatus ( result = status . get ( "result" ) , progress = status . get ( "progress" ) , status = status . get ( "status" ) , messages = status . get ( "messages" ) )
3424	def get_solution ( model , reactions = None , metabolites = None , raise_error = False ) : check_solver_status ( model . solver . status , raise_error = raise_error ) if reactions is None : reactions = model . reactions if metabolites is None : metabolites = model . metabolites rxn_index = list ( ) fluxes = empty ( len ( reactions ) ) reduced = empty ( len ( reactions ) ) var_primals = model . solver . primal_values shadow = empty ( len ( metabolites ) ) if model . solver . is_integer : reduced . fill ( nan ) shadow . fill ( nan ) for ( i , rxn ) in enumerate ( reactions ) : rxn_index . append ( rxn . id ) fluxes [ i ] = var_primals [ rxn . id ] - var_primals [ rxn . reverse_id ] met_index = [ met . id for met in metabolites ] else : var_duals = model . solver . reduced_costs for ( i , rxn ) in enumerate ( reactions ) : forward = rxn . id reverse = rxn . reverse_id rxn_index . append ( forward ) fluxes [ i ] = var_primals [ forward ] - var_primals [ reverse ] reduced [ i ] = var_duals [ forward ] - var_duals [ reverse ] met_index = list ( ) constr_duals = model . solver . shadow_prices for ( i , met ) in enumerate ( metabolites ) : met_index . append ( met . id ) shadow [ i ] = constr_duals [ met . id ] return Solution ( model . solver . objective . value , model . solver . status , Series ( index = rxn_index , data = fluxes , name = "fluxes" ) , Series ( index = rxn_index , data = reduced , name = "reduced_costs" ) , Series ( index = met_index , data = shadow , name = "shadow_prices" ) )
12549	def spatial_map ( icc , thr , mode = '+' ) : return thr_img ( icc_img_to_zscore ( icc ) , thr = thr , mode = mode ) . get_data ( )
5916	def _translate_residue ( self , selection , default_atomname = 'CA' ) : m = self . RESIDUE . match ( selection ) if not m : errmsg = "Selection {selection!r} is not valid." . format ( ** vars ( ) ) logger . error ( errmsg ) raise ValueError ( errmsg ) gmx_resid = self . gmx_resid ( int ( m . group ( 'resid' ) ) ) residue = m . group ( 'aa' ) if len ( residue ) == 1 : gmx_resname = utilities . convert_aa_code ( residue ) else : gmx_resname = residue gmx_atomname = m . group ( 'atom' ) if gmx_atomname is None : gmx_atomname = default_atomname return { 'resname' : gmx_resname , 'resid' : gmx_resid , 'atomname' : gmx_atomname }
7505	def _run_qmc ( self , boot ) : self . _tmp = os . path . join ( self . dirs , ".tmpwtre" ) cmd = [ ip . bins . qmc , "qrtt=" + self . files . qdump , "otre=" + self . _tmp ] proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) res = proc . communicate ( ) if proc . returncode : LOGGER . error ( res ) raise IPyradWarningExit ( res [ 1 ] ) with open ( self . _tmp ) as intree : tmp = ete3 . Tree ( intree . read ( ) . strip ( ) ) tmpwtre = self . _renamer ( tmp ) if boot : self . trees . boots = os . path . join ( self . dirs , self . name + ".boots" ) with open ( self . trees . boots , 'a' ) as outboot : outboot . write ( tmpwtre + "\n" ) else : self . trees . tree = os . path . join ( self . dirs , self . name + ".tree" ) with open ( self . trees . tree , 'w' ) as outtree : outtree . write ( tmpwtre ) self . _save ( )
3070	def request ( http , uri , method = 'GET' , body = None , headers = None , redirections = httplib2 . DEFAULT_MAX_REDIRECTS , connection_type = None ) : http_callable = getattr ( http , 'request' , http ) return http_callable ( uri , method = method , body = body , headers = headers , redirections = redirections , connection_type = connection_type )
2403	def gen_feats ( self , e_set ) : bag_feats = self . gen_bag_feats ( e_set ) length_feats = self . gen_length_feats ( e_set ) prompt_feats = self . gen_prompt_feats ( e_set ) overall_feats = numpy . concatenate ( ( length_feats , prompt_feats , bag_feats ) , axis = 1 ) overall_feats = overall_feats . copy ( ) return overall_feats
6668	def populate_fabfile ( ) : stack = inspect . stack ( ) fab_frame = None for frame_obj , script_fn , line , _ , _ , _ in stack : if 'fabfile.py' in script_fn : fab_frame = frame_obj break if not fab_frame : return try : locals_ = fab_frame . f_locals for module_name , module in sub_modules . items ( ) : locals_ [ module_name ] = module for role_name , role_func in role_commands . items ( ) : assert role_name not in sub_modules , ( 'The role %s conflicts with a built-in submodule. ' 'Please choose a different name.' ) % ( role_name ) locals_ [ role_name ] = role_func locals_ [ 'common' ] = common locals_ [ 'shell' ] = shell for _module_alias in common . post_import_modules : exec ( "import %s" % _module_alias ) locals_ [ _module_alias ] = locals ( ) [ _module_alias ] finally : del stack
7364	async def set_tz ( self ) : settings = await self . api . account . settings . get ( ) tz = settings . time_zone . tzinfo_name os . environ [ 'TZ' ] = tz time . tzset ( )
6887	def parallel_epd_lclist ( lclist , externalparams , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , epdsmooth_sigclip = 3.0 , epdsmooth_windowsize = 21 , epdsmooth_func = smooth_magseries_savgol , epdsmooth_extraparams = None , nworkers = NCPUS , maxworkertasks = 1000 ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( fileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if timecols is None : timecols = dtimecols if magcols is None : magcols = dmagcols if errcols is None : errcols = derrcols outdict = { } for t , m , e in zip ( timecols , magcols , errcols ) : tasks = [ ( x , t , m , e , externalparams , lcformat , lcformatdir , epdsmooth_sigclip , epdsmooth_windowsize , epdsmooth_func , epdsmooth_extraparams ) for x in lclist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( parallel_epd_worker , tasks ) pool . close ( ) pool . join ( ) outdict [ m ] = results return outdict
6167	def to_bin ( data , width ) : data_str = bin ( data & ( 2 ** width - 1 ) ) [ 2 : ] . zfill ( width ) return [ int ( x ) for x in tuple ( data_str ) ]
13250	def get_authoryear_from_entry ( entry , paren = False ) : def _format_last ( person ) : return ' ' . join ( [ n . strip ( '{}' ) for n in person . last_names ] ) if len ( entry . persons [ 'author' ] ) > 0 : persons = entry . persons [ 'author' ] elif len ( entry . persons [ 'editor' ] ) > 0 : persons = entry . persons [ 'editor' ] else : raise AuthorYearError try : year = entry . fields [ 'year' ] except KeyError : raise AuthorYearError if paren and len ( persons ) == 1 : template = '{author} ({year})' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year ) elif not paren and len ( persons ) == 1 : template = '{author} {year}' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year ) elif paren and len ( persons ) == 2 : template = '{author1} and {author2} ({year})' return template . format ( author1 = _format_last ( persons [ 0 ] ) , author2 = _format_last ( persons [ 1 ] ) , year = year ) elif not paren and len ( persons ) == 2 : template = '{author1} and {author2} {year}' return template . format ( author1 = _format_last ( persons [ 0 ] ) , author2 = _format_last ( persons [ 1 ] ) , year = year ) elif not paren and len ( persons ) > 2 : template = '{author} et al {year}' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year ) elif paren and len ( persons ) > 2 : template = '{author} et al ({year})' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year )
2548	def validate ( self , messages ) : messages = self . validate_version ( messages ) messages = self . validate_data_lics ( messages ) messages = self . validate_name ( messages ) messages = self . validate_spdx_id ( messages ) messages = self . validate_namespace ( messages ) messages = self . validate_ext_document_references ( messages ) messages = self . validate_creation_info ( messages ) messages = self . validate_package ( messages ) messages = self . validate_extracted_licenses ( messages ) messages = self . validate_reviews ( messages ) return messages
10435	def gettablerowindex ( self , window_name , object_name , row_text ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) index = 0 for cell in object_handle . AXRows : if re . match ( row_text , cell . AXChildren [ 0 ] . AXValue ) : return index index += 1 raise LdtpServerException ( u"Unable to find row: %s" % row_text )
695	def _loadDescriptionFile ( descriptionPyPath ) : global g_descriptionImportCount if not os . path . isfile ( descriptionPyPath ) : raise RuntimeError ( ( "Experiment description file %s does not exist or " + "is not a file" ) % ( descriptionPyPath , ) ) mod = imp . load_source ( "pf_description%d" % g_descriptionImportCount , descriptionPyPath ) g_descriptionImportCount += 1 if not hasattr ( mod , "descriptionInterface" ) : raise RuntimeError ( "Experiment description file %s does not define %s" % ( descriptionPyPath , "descriptionInterface" ) ) if not isinstance ( mod . descriptionInterface , exp_description_api . DescriptionIface ) : raise RuntimeError ( ( "Experiment description file %s defines %s but it " + "is not DescriptionIface-based" ) % ( descriptionPyPath , name ) ) return mod
5258	def parse_operand ( self , buf ) : buf = iter ( buf ) try : operand = 0 for _ in range ( self . operand_size ) : operand <<= 8 operand |= next ( buf ) self . _operand = operand except StopIteration : raise ParseError ( "Not enough data for decoding" )
3034	def _oauth2_web_server_flow_params ( kwargs ) : params = { 'access_type' : 'offline' , 'response_type' : 'code' , } params . update ( kwargs ) approval_prompt = params . get ( 'approval_prompt' ) if approval_prompt is not None : logger . warning ( 'The approval_prompt parameter for OAuth2WebServerFlow is ' 'deprecated. Please use the prompt parameter instead.' ) if approval_prompt == 'force' : logger . warning ( 'approval_prompt="force" has been adjusted to ' 'prompt="consent"' ) params [ 'prompt' ] = 'consent' del params [ 'approval_prompt' ] return params
8683	def export ( self , output_path = None , decrypt = False ) : self . _assert_valid_stash ( ) all_keys = [ ] for key in self . list ( ) : all_keys . append ( dict ( self . get ( key , decrypt = decrypt ) ) ) if all_keys : if output_path : with open ( output_path , 'w' ) as output_file : output_file . write ( json . dumps ( all_keys , indent = 4 ) ) return all_keys else : raise GhostError ( 'There are no keys to export' )
5846	def load_file_as_yaml ( path ) : with open ( path , "r" ) as f : raw_yaml = f . read ( ) parsed_dict = yaml . load ( raw_yaml ) return parsed_dict
9355	def money ( min = 0 , max = 10 ) : value = random . choice ( range ( min * 100 , max * 100 ) ) return "%1.2f" % ( float ( value ) / 100 )
9174	def bake ( binder , recipe_id , publisher , message , cursor ) : recipe = _get_recipe ( recipe_id , cursor ) includes = _formatter_callback_factory ( ) binder = collate_models ( binder , ruleset = recipe , includes = includes ) def flatten_filter ( model ) : return ( isinstance ( model , cnxepub . CompositeDocument ) or ( isinstance ( model , cnxepub . Binder ) and model . metadata . get ( 'type' ) == 'composite-chapter' ) ) def only_documents_filter ( model ) : return isinstance ( model , cnxepub . Document ) and not isinstance ( model , cnxepub . CompositeDocument ) for doc in cnxepub . flatten_to ( binder , flatten_filter ) : publish_composite_model ( cursor , doc , binder , publisher , message ) for doc in cnxepub . flatten_to ( binder , only_documents_filter ) : publish_collated_document ( cursor , doc , binder ) tree = cnxepub . model_to_tree ( binder ) publish_collated_tree ( cursor , tree ) return [ ]
4173	def window_visu ( N = 51 , name = 'hamming' , ** kargs ) : mindB = kargs . pop ( 'mindB' , - 100 ) maxdB = kargs . pop ( 'maxdB' , None ) norm = kargs . pop ( 'norm' , True ) w = Window ( N , name , ** kargs ) w . plot_time_freq ( mindB = mindB , maxdB = maxdB , norm = norm )
5351	def __autorefresh_studies ( self , cfg ) : if 'studies' not in self . conf [ self . backend_section ] or 'enrich_areas_of_code:git' not in self . conf [ self . backend_section ] [ 'studies' ] : logger . debug ( "Not doing autorefresh for studies, Areas of Code study is not active." ) return aoc_index = self . conf [ 'enrich_areas_of_code:git' ] . get ( 'out_index' , GitEnrich . GIT_AOC_ENRICHED ) if not aoc_index : aoc_index = GitEnrich . GIT_AOC_ENRICHED logger . debug ( "Autorefresh for Areas of Code study index: %s" , aoc_index ) es = Elasticsearch ( [ self . conf [ 'es_enrichment' ] [ 'url' ] ] , timeout = 100 , verify_certs = self . _get_enrich_backend ( ) . elastic . requests . verify ) if not es . indices . exists ( index = aoc_index ) : logger . debug ( "Not doing autorefresh, index doesn't exist for Areas of Code study" ) return logger . debug ( "Doing autorefresh for Areas of Code study" ) aoc_backend = GitEnrich ( self . db_sh , None , cfg [ 'projects' ] [ 'projects_file' ] , self . db_user , self . db_password , self . db_host ) aoc_backend . mapping = None aoc_backend . roles = [ 'author' ] elastic_enrich = get_elastic ( self . conf [ 'es_enrichment' ] [ 'url' ] , aoc_index , clean = False , backend = aoc_backend ) aoc_backend . set_elastic ( elastic_enrich ) self . __autorefresh ( aoc_backend , studies = True )
9976	def alter_freevars ( func , globals_ = None , ** vars ) : if globals_ is None : globals_ = func . __globals__ frees = tuple ( vars . keys ( ) ) oldlocs = func . __code__ . co_names newlocs = tuple ( name for name in oldlocs if name not in frees ) code = _alter_code ( func . __code__ , co_freevars = frees , co_names = newlocs , co_flags = func . __code__ . co_flags | inspect . CO_NESTED ) closure = _create_closure ( * vars . values ( ) ) return FunctionType ( code , globals_ , closure = closure )
4355	def _pop_ack_callback ( self , msgid ) : if msgid not in self . ack_callbacks : return None return self . ack_callbacks . pop ( msgid )
11073	def set_nested ( data , value , * keys ) : if len ( keys ) == 1 : data [ keys [ 0 ] ] = value else : if keys [ 0 ] not in data : data [ keys [ 0 ] ] = { } set_nested ( data [ keys [ 0 ] ] , value , * keys [ 1 : ] )
4535	def fillRGB ( self , r , g , b , start = 0 , end = - 1 ) : self . fill ( ( r , g , b ) , start , end )
1678	def IsInAlphabeticalOrder ( self , clean_lines , linenum , header_path ) : if ( self . _last_header > header_path and Match ( r'^\s*#\s*include\b' , clean_lines . elided [ linenum - 1 ] ) ) : return False return True
3517	def spring_metrics ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return SpringMetricsNode ( )
9884	def _read_all_z_variable_info ( self ) : self . z_variable_info = { } self . z_variable_names_by_num = { } info = fortran_cdf . z_var_all_inquire ( self . fname , self . _num_z_vars , len ( self . fname ) ) status = info [ 0 ] data_types = info [ 1 ] num_elems = info [ 2 ] rec_varys = info [ 3 ] dim_varys = info [ 4 ] num_dims = info [ 5 ] dim_sizes = info [ 6 ] rec_nums = info [ 7 ] var_nums = info [ 8 ] var_names = info [ 9 ] if status == 0 : for i in np . arange ( len ( data_types ) ) : out = { } out [ 'data_type' ] = data_types [ i ] out [ 'num_elems' ] = num_elems [ i ] out [ 'rec_vary' ] = rec_varys [ i ] out [ 'dim_varys' ] = dim_varys [ i ] out [ 'num_dims' ] = num_dims [ i ] out [ 'dim_sizes' ] = dim_sizes [ i , : 1 ] if out [ 'dim_sizes' ] [ 0 ] == 0 : out [ 'dim_sizes' ] [ 0 ] += 1 out [ 'rec_num' ] = rec_nums [ i ] out [ 'var_num' ] = var_nums [ i ] var_name = '' . join ( var_names [ i ] . astype ( 'U' ) ) out [ 'var_name' ] = var_name . rstrip ( ) self . z_variable_info [ out [ 'var_name' ] ] = out self . z_variable_names_by_num [ out [ 'var_num' ] ] = var_name else : raise IOError ( fortran_cdf . statusreporter ( status ) )
3272	def _init ( self ) : self . provider . _count_get_resource_inst_init += 1 tableName , primKey = self . provider . _split_path ( self . path ) display_type = "Unknown" displayTypeComment = "" contentType = "text/html" if tableName is None : display_type = "Database" elif primKey is None : display_type = "Database Table" else : contentType = "text/csv" if primKey == "_ENTIRE_CONTENTS" : display_type = "Database Table Contents" displayTypeComment = "CSV Representation of Table Contents" else : display_type = "Database Record" displayTypeComment = "Attributes available as properties" is_collection = primKey is None self . _cache = { "content_length" : None , "contentType" : contentType , "created" : time . time ( ) , "display_name" : self . name , "etag" : hashlib . md5 ( ) . update ( self . path ) . hexdigest ( ) , "modified" : None , "support_ranges" : False , "display_info" : { "type" : display_type , "typeComment" : displayTypeComment } , } if not is_collection : self . _cache [ "modified" ] = time . time ( ) _logger . debug ( "- % self . provider . _count_initConnection )
13065	def expose_ancestors_or_children ( self , member , collection , lang = None ) : x = { "id" : member . id , "label" : str ( member . get_label ( lang ) ) , "model" : str ( member . model ) , "type" : str ( member . type ) , "size" : member . size , "semantic" : self . semantic ( member , parent = collection ) } if isinstance ( member , ResourceCollection ) : x [ "lang" ] = str ( member . lang ) return x
9427	def getinfo ( self , name ) : rarinfo = self . NameToInfo . get ( name ) if rarinfo is None : raise KeyError ( 'There is no item named %r in the archive' % name ) return rarinfo
10751	def validate_sceneInfo ( self ) : if self . sceneInfo . prefix not in self . __prefixesValid : raise WrongSceneNameError ( 'AWS: Prefix of %s (%s) is invalid' % ( self . sceneInfo . name , self . sceneInfo . prefix ) )
13196	def ensure_format ( doc , format ) : assert format in ( 'xml' , 'json' ) if getattr ( doc , 'tag' , None ) == 'open511' : if format == 'json' : return xml_to_json ( doc ) elif isinstance ( doc , dict ) and 'meta' in doc : if format == 'xml' : return json_doc_to_xml ( doc ) else : raise ValueError ( "Unrecognized input document" ) return doc
3002	def start ( self ) : if self . extra_args : sys . exit ( '{} takes no extra arguments' . format ( self . name ) ) else : if self . _toggle_value : nbextensions . install_nbextension_python ( _pkg_name , overwrite = True , symlink = False , user = self . user , sys_prefix = self . sys_prefix , prefix = None , nbextensions_dir = None , logger = None ) else : nbextensions . uninstall_nbextension_python ( _pkg_name , user = self . user , sys_prefix = self . sys_prefix , prefix = None , nbextensions_dir = None , logger = None ) self . toggle_nbextension_python ( _pkg_name ) self . toggle_server_extension_python ( _pkg_name )
2875	def add_bpmn_files ( self , filenames ) : for filename in filenames : f = open ( filename , 'r' ) try : self . add_bpmn_xml ( ET . parse ( f ) , filename = filename ) finally : f . close ( )
3032	def credentials_from_code ( client_id , client_secret , scope , code , redirect_uri = 'postmessage' , http = None , user_agent = None , token_uri = oauth2client . GOOGLE_TOKEN_URI , auth_uri = oauth2client . GOOGLE_AUTH_URI , revoke_uri = oauth2client . GOOGLE_REVOKE_URI , device_uri = oauth2client . GOOGLE_DEVICE_URI , token_info_uri = oauth2client . GOOGLE_TOKEN_INFO_URI , pkce = False , code_verifier = None ) : flow = OAuth2WebServerFlow ( client_id , client_secret , scope , redirect_uri = redirect_uri , user_agent = user_agent , auth_uri = auth_uri , token_uri = token_uri , revoke_uri = revoke_uri , device_uri = device_uri , token_info_uri = token_info_uri , pkce = pkce , code_verifier = code_verifier ) credentials = flow . step2_exchange ( code , http = http ) return credentials
10181	def _events_process ( event_types = None , eager = False ) : event_types = event_types or list ( current_stats . enabled_events ) if eager : process_events . apply ( ( event_types , ) , throw = True ) click . secho ( 'Events processed successfully.' , fg = 'green' ) else : process_events . delay ( event_types ) click . secho ( 'Events processing task sent...' , fg = 'yellow' )
1316	def GetAllPixelColors ( self ) -> ctypes . Array : return self . GetPixelColorsOfRect ( 0 , 0 , self . Width , self . Height )
11786	def sanitize ( self , example ) : "Return a copy of example, with non-input attributes replaced by None." return [ attr_i if i in self . inputs else None for i , attr_i in enumerate ( example ) ]
5011	def _call_post_with_session ( self , url , payload ) : now = datetime . datetime . utcnow ( ) if now >= self . expires_at : self . session . close ( ) self . _create_session ( ) response = self . session . post ( url , data = payload ) return response . status_code , response . text
7097	def on_map_fragment_created ( self , obj_id ) : self . fragment = MapFragment ( __id__ = obj_id ) self . map . onMapReady . connect ( self . on_map_ready ) self . fragment . getMapAsync ( self . map . getId ( ) ) context = self . get_context ( ) def on_transaction ( id ) : trans = FragmentTransaction ( __id__ = id ) trans . add ( self . widget . getId ( ) , self . fragment ) trans . commit ( ) def on_fragment_manager ( id ) : fm = FragmentManager ( __id__ = id ) fm . beginTransaction ( ) . then ( on_transaction ) context . widget . getSupportFragmentManager ( ) . then ( on_fragment_manager )
2602	def engine_file ( self ) : return os . path . join ( self . ipython_dir , 'profile_{0}' . format ( self . profile ) , 'security/ipcontroller-engine.json' )
7333	async def close ( self ) : tasks = self . _get_close_tasks ( ) if tasks : await asyncio . wait ( tasks ) self . _session = None
9937	def list ( self , ignore_patterns ) : for storage in six . itervalues ( self . storages ) : if storage . exists ( '' ) : for path in utils . get_files ( storage , ignore_patterns ) : yield path , storage
12288	def datapackage_exists ( repo ) : datapath = os . path . join ( repo . rootdir , "datapackage.json" ) return os . path . exists ( datapath )
594	def _cacheSequenceInfoType ( self ) : hasReset = self . resetFieldName is not None hasSequenceId = self . sequenceIdFieldName is not None if hasReset and not hasSequenceId : self . _sequenceInfoType = self . SEQUENCEINFO_RESET_ONLY self . _prevSequenceId = 0 elif not hasReset and hasSequenceId : self . _sequenceInfoType = self . SEQUENCEINFO_SEQUENCEID_ONLY self . _prevSequenceId = None elif hasReset and hasSequenceId : self . _sequenceInfoType = self . SEQUENCEINFO_BOTH else : self . _sequenceInfoType = self . SEQUENCEINFO_NONE
4832	def course_discovery_api_client ( user , catalog_url ) : if JwtBuilder is None : raise NotConnectedToOpenEdX ( _ ( "To get a Catalog API client, this package must be " "installed in an Open edX environment." ) ) jwt = JwtBuilder . create_jwt_for_user ( user ) return EdxRestApiClient ( catalog_url , jwt = jwt )
9896	def uptime ( ) : if __boottime is not None : return time . time ( ) - __boottime return { 'amiga' : _uptime_amiga , 'aros12' : _uptime_amiga , 'beos5' : _uptime_beos , 'cygwin' : _uptime_linux , 'darwin' : _uptime_osx , 'haiku1' : _uptime_beos , 'linux' : _uptime_linux , 'linux-armv71' : _uptime_linux , 'linux2' : _uptime_linux , 'mac' : _uptime_mac , 'minix3' : _uptime_minix , 'riscos' : _uptime_riscos , 'sunos5' : _uptime_solaris , 'syllable' : _uptime_syllable , 'win32' : _uptime_windows , 'wince' : _uptime_windows } . get ( sys . platform , _uptime_bsd ) ( ) or _uptime_bsd ( ) or _uptime_plan9 ( ) or _uptime_linux ( ) or _uptime_windows ( ) or _uptime_solaris ( ) or _uptime_beos ( ) or _uptime_amiga ( ) or _uptime_riscos ( ) or _uptime_posix ( ) or _uptime_syllable ( ) or _uptime_mac ( ) or _uptime_osx ( )
11974	def convert_nm ( nm , notation = IP_DOT , inotation = IP_UNKNOWN , check = True ) : return _convert ( nm , notation , inotation , _check = check , _isnm = True )
11833	def make_undirected ( self ) : "Make a digraph into an undirected graph by adding symmetric edges." for a in self . dict . keys ( ) : for ( b , distance ) in self . dict [ a ] . items ( ) : self . connect1 ( b , a , distance )
12370	def get ( self , id , ** kwargs ) : return super ( DomainRecords , self ) . get ( id , ** kwargs )
10282	def get_peripheral_predecessor_edges ( graph : BELGraph , subgraph : BELGraph ) -> EdgeIterator : for v in subgraph : for u , _ , k in graph . in_edges ( v , keys = True ) : if u not in subgraph : yield u , v , k
12461	def pip_cmd ( env , cmd , ignore_activated = False , ** kwargs ) : r cmd = tuple ( cmd ) dirname = safe_path ( env ) if not ignore_activated : activated_env = os . environ . get ( 'VIRTUAL_ENV' ) if hasattr ( sys , 'real_prefix' ) : dirname = sys . prefix elif activated_env : dirname = activated_env pip_path = os . path . join ( dirname , 'Scripts' if IS_WINDOWS else 'bin' , 'pip' ) if kwargs . pop ( 'return_path' , False ) : return pip_path if not os . path . isfile ( pip_path ) : raise OSError ( 'No pip found at {0!r}' . format ( pip_path ) ) if BOOTSTRAPPER_TEST_KEY in os . environ and cmd [ 0 ] == 'install' : cmd = list ( cmd ) cmd . insert ( 1 , '--disable-pip-version-check' ) cmd = tuple ( cmd ) with disable_error_handler ( ) : return run_cmd ( ( pip_path , ) + cmd , ** kwargs )
11533	def setup ( self , port ) : port = str ( port ) self . _serial = serial . Serial ( port , 115200 , timeout = 2 ) time . sleep ( 2 ) if not self . _serial . is_open : raise RuntimeError ( 'Could not connect to Arduino' ) self . _serial . write ( b'\x01' ) if self . _serial . read ( ) != b'\x06' : raise RuntimeError ( 'Could not connect to Arduino' ) ps = [ p for p in self . available_pins ( ) if p [ 'digital' ] [ 'output' ] ] for pin in ps : self . _set_pin_direction ( pin [ 'id' ] , ahio . Direction . Output )
1460	def resolve_heron_suffix_issue ( abs_pex_path , class_path ) : importer = zipimport . zipimporter ( abs_pex_path ) importer . load_module ( "heron" ) to_load_lst = class_path . split ( '.' ) [ 1 : - 1 ] loaded = [ 'heron' ] loaded_mod = None for to_load in to_load_lst : sub_importer = zipimport . zipimporter ( os . path . join ( abs_pex_path , '/' . join ( loaded ) ) ) loaded_mod = sub_importer . load_module ( to_load ) loaded . append ( to_load ) return loaded_mod
2288	def forward ( self ) : self . noise . data . normal_ ( ) if not self . confounding : for i in self . topological_order : self . generated [ i ] = self . blocks [ i ] ( th . cat ( [ v for c in [ [ self . generated [ j ] for j in np . nonzero ( self . adjacency_matrix [ : , i ] ) [ 0 ] ] , [ self . noise [ : , [ i ] ] ] ] for v in c ] , 1 ) ) else : for i in self . topological_order : self . generated [ i ] = self . blocks [ i ] ( th . cat ( [ v for c in [ [ self . generated [ j ] for j in np . nonzero ( self . adjacency_matrix [ : , i ] ) [ 0 ] ] , [ self . corr_noise [ min ( i , j ) , max ( i , j ) ] for j in np . nonzero ( self . i_adj_matrix [ : , i ] ) [ 0 ] ] [ self . noise [ : , [ i ] ] ] ] for v in c ] , 1 ) ) return th . cat ( self . generated , 1 )
6940	def _gaussian ( x , amp , loc , std ) : return amp * np . exp ( - ( ( x - loc ) * ( x - loc ) ) / ( 2.0 * std * std ) )
3495	def reaction_elements ( reaction ) : c_elements = [ coeff * met . elements . get ( 'C' , 0 ) for met , coeff in iteritems ( reaction . metabolites ) ] return [ elem for elem in c_elements if elem != 0 ]
8443	def ls ( github_user , template = None ) : temple . check . has_env_vars ( temple . constants . GITHUB_API_TOKEN_ENV_VAR ) if template : temple . check . is_git_ssh_path ( template ) search_q = 'user:{} filename:{} {}' . format ( github_user , temple . constants . TEMPLE_CONFIG_FILE , template ) else : search_q = 'user:{} cookiecutter.json in:path' . format ( github_user ) results = _code_search ( search_q , github_user ) return collections . OrderedDict ( sorted ( results . items ( ) ) )
3393	def undelete_model_genes ( cobra_model ) : if cobra_model . _trimmed_genes is not None : for x in cobra_model . _trimmed_genes : x . functional = True if cobra_model . _trimmed_reactions is not None : for the_reaction , ( lower_bound , upper_bound ) in cobra_model . _trimmed_reactions . items ( ) : the_reaction . lower_bound = lower_bound the_reaction . upper_bound = upper_bound cobra_model . _trimmed_genes = [ ] cobra_model . _trimmed_reactions = { } cobra_model . _trimmed = False
4851	def _transmit_update ( self , channel_metadata_item_map , transmission_map ) : for chunk in chunks ( channel_metadata_item_map , self . enterprise_configuration . transmission_chunk_size ) : serialized_chunk = self . _serialize_items ( list ( chunk . values ( ) ) ) try : self . client . update_content_metadata ( serialized_chunk ) except ClientError as exc : LOGGER . error ( 'Failed to update [%s] content metadata items for integrated channel [%s] [%s]' , len ( chunk ) , self . enterprise_configuration . enterprise_customer . name , self . enterprise_configuration . channel_code , ) LOGGER . error ( exc ) else : self . _update_transmissions ( chunk , transmission_map )
8023	def retry ( exceptions = ( Exception , ) , interval = 0 , max_retries = 10 , success = None , timeout = - 1 ) : if not exceptions and success is None : raise TypeError ( '`exceptions` and `success` parameter can not both be None' ) exceptions = exceptions or ( _DummyException , ) _retries_error_msg = ( 'Exceeded maximum number of retries {} at ' 'an interval of {}s for function {}' ) _timeout_error_msg = 'Maximum timeout of {}s reached for function {}' @ decorator def wrapper ( func , * args , ** kwargs ) : signal . signal ( signal . SIGALRM , _timeout ( _timeout_error_msg . format ( timeout , func . __name__ ) ) ) run_func = functools . partial ( func , * args , ** kwargs ) logger = logging . getLogger ( func . __module__ ) if max_retries < 0 : iterator = itertools . count ( ) else : iterator = range ( max_retries ) if timeout > 0 : signal . alarm ( timeout ) for num , _ in enumerate ( iterator , 1 ) : try : result = run_func ( ) if success is None or success ( result ) : signal . alarm ( 0 ) return result except exceptions : logger . exception ( 'Exception experienced when trying function {}' . format ( func . __name__ ) ) if num == max_retries : raise logger . warning ( 'Retrying {} in {}s...' . format ( func . __name__ , interval ) ) time . sleep ( interval ) else : raise MaximumRetriesExceeded ( _retries_error_msg . format ( max_retries , interval , func . __name__ ) ) return wrapper
8194	def _density ( self ) : return 2.0 * len ( self . edges ) / ( len ( self . nodes ) * ( len ( self . nodes ) - 1 ) )
11212	def _hash ( secret : bytes , data : bytes , alg : str ) -> bytes : algorithm = get_algorithm ( alg ) return hmac . new ( secret , msg = data , digestmod = algorithm ) . digest ( )
12436	def traverse ( cls , request , params = None ) : result = cls . parse ( request . path ) if result is None : return cls , { } elif not result : raise http . exceptions . NotFound ( ) resource , data , rest = result if params : data . update ( params ) if resource is None : return cls , data if data . get ( 'path' ) is not None : request . path = data . pop ( 'path' ) elif rest is not None : request . path = rest result = resource . traverse ( request , params = data ) return result
4926	def transform_title ( self , content_metadata_item ) : title_with_locales = [ ] for locale in self . enterprise_configuration . get_locales ( ) : title_with_locales . append ( { 'locale' : locale , 'value' : content_metadata_item . get ( 'title' , '' ) } ) return title_with_locales
1367	def start_connect ( self ) : Log . debug ( "In start_connect() of %s" % self . _get_classname ( ) ) self . create_socket ( socket . AF_INET , socket . SOCK_STREAM ) self . _connecting = True self . connect ( self . endpoint )
456	def ternary_operation ( x ) : g = tf . get_default_graph ( ) with g . gradient_override_map ( { "Sign" : "Identity" } ) : threshold = _compute_threshold ( x ) x = tf . sign ( tf . add ( tf . sign ( tf . add ( x , threshold ) ) , tf . sign ( tf . add ( x , - threshold ) ) ) ) return x
4933	def get_content_id ( self , content_metadata_item ) : content_id = content_metadata_item . get ( 'key' , '' ) if content_metadata_item [ 'content_type' ] == 'program' : content_id = content_metadata_item . get ( 'uuid' , '' ) return content_id
7250	def launch_batch_workflow ( self , batch_workflow ) : url = '%(base_url)s/batch_workflows' % { 'base_url' : self . base_url } try : r = self . gbdx_connection . post ( url , json = batch_workflow ) batch_workflow_id = r . json ( ) [ 'batch_workflow_id' ] return batch_workflow_id except TypeError as e : self . logger . debug ( 'Batch Workflow not launched, reason: {0}' . format ( e ) )
5508	def worker ( f ) : @ functools . wraps ( f ) async def wrapper ( cls , connection , rest ) : try : await f ( cls , connection , rest ) except asyncio . CancelledError : connection . response ( "426" , "transfer aborted" ) connection . response ( "226" , "abort successful" ) return wrapper
11733	def isValidClass ( self , class_ ) : module = inspect . getmodule ( class_ ) valid = ( module in self . _valid_modules or ( hasattr ( module , '__file__' ) and module . __file__ in self . _valid_named_modules ) ) return valid and not private ( class_ )
4893	def _collect_certificate_data ( self , enterprise_enrollment ) : if self . certificates_api is None : self . certificates_api = CertificatesApiClient ( self . user ) course_id = enterprise_enrollment . course_id username = enterprise_enrollment . enterprise_customer_user . user . username try : certificate = self . certificates_api . get_course_certificate ( course_id , username ) completed_date = certificate . get ( 'created_date' ) if completed_date : completed_date = parse_datetime ( completed_date ) else : completed_date = timezone . now ( ) is_passing = certificate . get ( 'is_passing' ) grade = self . grade_passing if is_passing else self . grade_failing except HttpNotFoundError : completed_date = None grade = self . grade_incomplete is_passing = False return completed_date , grade , is_passing
1298	def SetClipboardText ( text : str ) -> bool : if ctypes . windll . user32 . OpenClipboard ( 0 ) : ctypes . windll . user32 . EmptyClipboard ( ) textByteLen = ( len ( text ) + 1 ) * 2 hClipboardData = ctypes . windll . kernel32 . GlobalAlloc ( 0 , textByteLen ) hDestText = ctypes . windll . kernel32 . GlobalLock ( hClipboardData ) ctypes . cdll . msvcrt . wcsncpy ( ctypes . c_wchar_p ( hDestText ) , ctypes . c_wchar_p ( text ) , textByteLen // 2 ) ctypes . windll . kernel32 . GlobalUnlock ( hClipboardData ) ctypes . windll . user32 . SetClipboardData ( 13 , hClipboardData ) ctypes . windll . user32 . CloseClipboard ( ) return True return False
9660	def merge_from_store_and_in_mems ( from_store , in_mem_shas , dont_update_shas_of ) : if not from_store : for item in dont_update_shas_of : if item in in_mem_shas [ 'files' ] : del in_mem_shas [ 'files' ] [ item ] return in_mem_shas for key in from_store [ 'files' ] : if key not in in_mem_shas [ 'files' ] and key not in dont_update_shas_of : in_mem_shas [ 'files' ] [ key ] = from_store [ 'files' ] [ key ] for item in dont_update_shas_of : if item in in_mem_shas [ 'files' ] : del in_mem_shas [ 'files' ] [ item ] return in_mem_shas
10147	def _ref ( self , param , base_name = None ) : name = base_name or param . get ( 'title' , '' ) or param . get ( 'name' , '' ) pointer = self . json_pointer + name self . parameter_registry [ name ] = param return { '$ref' : pointer }
8542	def _get_username ( self , username = None , use_config = True , config_filename = None ) : if not username and use_config : if self . _config is None : self . _read_config ( config_filename ) username = self . _config . get ( "credentials" , "username" , fallback = None ) if not username : username = input ( "Please enter your username: " ) . strip ( ) while not username : username = input ( "No username specified. Please enter your username: " ) . strip ( ) if 'credendials' not in self . _config : self . _config . add_section ( 'credentials' ) self . _config . set ( "credentials" , "username" , username ) self . _save_config ( ) return username
12465	def read_config ( filename , args ) : config = defaultdict ( dict ) splitter = operator . methodcaller ( 'split' , ' ' ) converters = { __script__ : { 'env' : safe_path , 'pre_requirements' : splitter , } , 'pip' : { 'allow_external' : splitter , 'allow_unverified' : splitter , } } default = copy . deepcopy ( CONFIG ) sections = set ( iterkeys ( default ) ) if int ( getattr ( pip , '__version__' , '1.x' ) . split ( '.' ) [ 0 ] ) < 6 : default [ 'pip' ] [ 'download_cache' ] = safe_path ( os . path . expanduser ( os . path . join ( '~' , '.{0}' . format ( __script__ ) , 'pip-cache' ) ) ) is_default = filename == DEFAULT_CONFIG filename = os . path . expandvars ( os . path . expanduser ( filename ) ) if not is_default and not os . path . isfile ( filename ) : print_error ( 'Config file does not exist at {0!r}' . format ( filename ) ) return None parser = ConfigParser ( ) try : parser . read ( filename ) except ConfigParserError : print_error ( 'Cannot parse config file at {0!r}' . format ( filename ) ) return None for section in sections : if not parser . has_section ( section ) : continue items = parser . items ( section ) for key , value in items : try : value = int ( value ) except ( TypeError , ValueError ) : try : value = bool ( strtobool ( value ) ) except ValueError : pass if section in converters and key in converters [ section ] : value = converters [ section ] [ key ] ( value ) config [ section ] [ key ] = value for section , data in iteritems ( default ) : if section not in config : config [ section ] = data else : for key , value in iteritems ( data ) : config [ section ] . setdefault ( key , value ) keys = set ( ( 'env' , 'hook' , 'install_dev_requirements' , 'ignore_activated' , 'pre_requirements' , 'quiet' , 'recreate' , 'requirements' ) ) for key in keys : value = getattr ( args , key ) config [ __script__ ] . setdefault ( key , value ) if key == 'pre_requirements' and not value : continue if value is not None : config [ __script__ ] [ key ] = value return config
11392	def fetch_url ( url , method = 'GET' , user_agent = 'django-oembed' , timeout = SOCKET_TIMEOUT ) : sock = httplib2 . Http ( timeout = timeout ) request_headers = { 'User-Agent' : user_agent , 'Accept-Encoding' : 'gzip' } try : headers , raw = sock . request ( url , headers = request_headers , method = method ) except : raise OEmbedHTTPException ( 'Error fetching %s' % url ) return headers , raw
13072	def r_passage ( self , objectId , subreference , lang = None ) : collection = self . get_collection ( objectId ) if isinstance ( collection , CtsWorkMetadata ) : editions = [ t for t in collection . children . values ( ) if isinstance ( t , CtsEditionMetadata ) ] if len ( editions ) == 0 : raise UnknownCollection ( "This work has no default edition" ) return redirect ( url_for ( ".r_passage" , objectId = str ( editions [ 0 ] . id ) , subreference = subreference ) ) text = self . get_passage ( objectId = objectId , subreference = subreference ) passage = self . transform ( text , text . export ( Mimetypes . PYTHON . ETREE ) , objectId ) prev , next = self . get_siblings ( objectId , subreference , text ) return { "template" : "main::text.html" , "objectId" : objectId , "subreference" : subreference , "collections" : { "current" : { "label" : collection . get_label ( lang ) , "id" : collection . id , "model" : str ( collection . model ) , "type" : str ( collection . type ) , "author" : text . get_creator ( lang ) , "title" : text . get_title ( lang ) , "description" : text . get_description ( lang ) , "citation" : collection . citation , "coins" : self . make_coins ( collection , text , subreference , lang = lang ) } , "parents" : self . make_parents ( collection , lang = lang ) } , "text_passage" : Markup ( passage ) , "prev" : prev , "next" : next }
9774	def resources ( ctx , gpu ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . job . resources ( user , project_name , _job , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
11848	def things_near ( self , location , radius = None ) : "Return all things within radius of location." if radius is None : radius = self . perceptible_distance radius2 = radius * radius return [ thing for thing in self . things if distance2 ( location , thing . location ) <= radius2 ]
5136	def get_class_traits ( klass ) : source = inspect . getsource ( klass ) cb = CommentBlocker ( ) cb . process_file ( StringIO ( source ) ) mod_ast = compiler . parse ( source ) class_ast = mod_ast . node . nodes [ 0 ] for node in class_ast . code . nodes : if isinstance ( node , compiler . ast . Assign ) : name = node . nodes [ 0 ] . name rhs = unparse ( node . expr ) . strip ( ) doc = strip_comment_marker ( cb . search_for_comment ( node . lineno , default = '' ) ) yield name , rhs , doc
4409	async def connect ( self , channel_id : int ) : ws = self . _lavalink . bot . _connection . _get_websocket ( int ( self . guild_id ) ) await ws . voice_state ( self . guild_id , str ( channel_id ) )
2165	def format_commands ( self , ctx , formatter ) : self . format_command_subsection ( ctx , formatter , self . list_misc_commands ( ) , 'Commands' ) self . format_command_subsection ( ctx , formatter , self . list_resource_commands ( ) , 'Resources' )
4219	def get_preferred_collection ( self ) : bus = secretstorage . dbus_init ( ) try : if hasattr ( self , 'preferred_collection' ) : collection = secretstorage . Collection ( bus , self . preferred_collection ) else : collection = secretstorage . get_default_collection ( bus ) except exceptions . SecretStorageException as e : raise InitError ( "Failed to create the collection: %s." % e ) if collection . is_locked ( ) : collection . unlock ( ) if collection . is_locked ( ) : raise KeyringLocked ( "Failed to unlock the collection!" ) return collection
11717	def create ( self , config ) : assert config [ "name" ] == self . name , "Given config is not for this template" data = self . _json_encode ( config ) headers = self . _default_headers ( ) return self . _request ( "" , ok_status = None , data = data , headers = headers )
11838	def result ( self , state , row ) : "Place the next queen at the given row." col = state . index ( None ) new = state [ : ] new [ col ] = row return new
1404	def load_configs ( self ) : self . statemgr_config . set_state_locations ( self . configs [ STATEMGRS_KEY ] ) if EXTRA_LINKS_KEY in self . configs : for extra_link in self . configs [ EXTRA_LINKS_KEY ] : self . extra_links . append ( self . validate_extra_link ( extra_link ) )
5938	def help ( self , long = False ) : print ( "\ncommand: {0!s}\n\n" . format ( self . command_name ) ) print ( self . __doc__ ) if long : print ( "\ncall method: command():\n" ) print ( self . __call__ . __doc__ )
12855	def subtree ( events ) : stack = 0 for obj in events : if obj [ 'type' ] == ENTER : stack += 1 elif obj [ 'type' ] == EXIT : if stack == 0 : break stack -= 1 yield obj
11064	def _ignore_event ( self , message ) : if hasattr ( message , 'subtype' ) and message . subtype in self . ignored_events : return True return False
9346	def adapt ( cls , source , template ) : if not isinstance ( template , packarray ) : raise TypeError ( 'template must be a packarray' ) return cls ( source , template . start , template . end )
5394	def _localize_inputs_recursive_command ( self , task_dir , inputs ) : data_dir = os . path . join ( task_dir , _DATA_SUBDIR ) provider_commands = [ providers_util . build_recursive_localize_command ( data_dir , inputs , file_provider ) for file_provider in _SUPPORTED_INPUT_PROVIDERS ] return '\n' . join ( provider_commands )
7432	def _write_nex ( self , mdict , nlocus ) : max_name_len = max ( [ len ( i ) for i in mdict ] ) namestring = "{:<" + str ( max_name_len + 1 ) + "} {}\n" matrix = "" for i in mdict . items ( ) : matrix += namestring . format ( i [ 0 ] , i [ 1 ] ) minidir = os . path . realpath ( os . path . join ( self . workdir , self . name ) ) if not os . path . exists ( minidir ) : os . makedirs ( minidir ) handle = os . path . join ( minidir , "{}.nex" . format ( nlocus ) ) with open ( handle , 'w' ) as outnex : outnex . write ( NEXBLOCK . format ( ** { "ntax" : len ( mdict ) , "nchar" : len ( mdict . values ( ) [ 0 ] ) , "matrix" : matrix , "ngen" : self . params . mb_mcmc_ngen , "sfreq" : self . params . mb_mcmc_sample_freq , "burnin" : self . params . mb_mcmc_burnin , } ) )
6705	def create ( self , username , groups = None , uid = None , create_home = None , system = False , password = None , home_dir = None ) : r = self . local_renderer r . env . username = username args = [ ] if uid : args . append ( '-u %s' % uid ) if create_home is None : create_home = not system if create_home is True : if home_dir : args . append ( '--home %s' % home_dir ) elif create_home is False : args . append ( '--no-create-home' ) if password is None : pass elif password : crypted_password = _crypt_password ( password ) args . append ( '-p %s' % quote ( crypted_password ) ) else : args . append ( '--disabled-password' ) args . append ( '--gecos ""' ) if system : args . append ( '--system' ) r . env . args = ' ' . join ( args ) r . env . groups = ( groups or '' ) . strip ( ) r . sudo ( 'adduser {args} {username} || true' ) if groups : for group in groups . split ( ' ' ) : group = group . strip ( ) if not group : continue r . sudo ( 'adduser %s %s || true' % ( username , group ) )
11798	def suppose ( self , var , value ) : "Start accumulating inferences from assuming var=value." self . support_pruning ( ) removals = [ ( var , a ) for a in self . curr_domains [ var ] if a != value ] self . curr_domains [ var ] = [ value ] return removals
5727	def _get_responses_windows ( self , timeout_sec ) : timeout_time_sec = time . time ( ) + timeout_sec responses = [ ] while True : try : self . gdb_process . stdout . flush ( ) if PYTHON3 : raw_output = self . gdb_process . stdout . readline ( ) . replace ( b"\r" , b"\n" ) else : raw_output = self . gdb_process . stdout . read ( ) . replace ( b"\r" , b"\n" ) responses += self . _get_responses_list ( raw_output , "stdout" ) except IOError : pass try : self . gdb_process . stderr . flush ( ) if PYTHON3 : raw_output = self . gdb_process . stderr . readline ( ) . replace ( b"\r" , b"\n" ) else : raw_output = self . gdb_process . stderr . read ( ) . replace ( b"\r" , b"\n" ) responses += self . _get_responses_list ( raw_output , "stderr" ) except IOError : pass if time . time ( ) > timeout_time_sec : break return responses
2684	def cached_download ( url , name ) : clean_name = os . path . normpath ( name ) if clean_name != name : raise ValueError ( "{} is not normalized." . format ( name ) ) for dir_ in iter_data_dirs ( ) : path = os . path . join ( dir_ , name ) if os . path . exists ( path ) : return path dir_ = next ( iter_data_dirs ( True ) ) path = os . path . join ( dir_ , name ) log . info ( "Downloading {} to {}" . format ( url , path ) ) response = urlopen ( url ) if response . getcode ( ) != 200 : raise ValueError ( "HTTP {}" . format ( response . getcode ( ) ) ) dir_ = os . path . dirname ( path ) try : os . makedirs ( dir_ ) except OSError as e : if e . errno != errno . EEXIST : raise tmp_path = path + '.tmp' with open ( tmp_path , 'wb' ) as fh : while True : chunk = response . read ( 8196 ) if chunk : fh . write ( chunk ) else : break os . rename ( tmp_path , path ) return path
9846	def resample_factor ( self , factor ) : newlengths = [ ( N - 1 ) * float ( factor ) + 1 for N in self . _len_edges ( ) ] edges = [ numpy . linspace ( start , stop , num = int ( N ) , endpoint = True ) for ( start , stop , N ) in zip ( self . _min_edges ( ) , self . _max_edges ( ) , newlengths ) ] return self . resample ( edges )
12003	def _add_header ( self , data , options ) : version_info = self . _get_version_info ( options [ 'version' ] ) flags = options [ 'flags' ] header_flags = dict ( ( i , str ( int ( j ) ) ) for i , j in options [ 'flags' ] . iteritems ( ) ) header_flags = '' . join ( version_info [ 'flags' ] ( ** header_flags ) ) header_flags = int ( header_flags , 2 ) options [ 'flags' ] = header_flags header = version_info [ 'header' ] header = header ( ** options ) header = pack ( version_info [ 'header_format' ] , * header ) if 'timestamp' in flags and flags [ 'timestamp' ] : timestamp = long ( time ( ) ) timestamp = pack ( version_info [ 'timestamp_format' ] , timestamp ) header = header + timestamp return header + data
5697	def get_median_lat_lon_of_stops ( gtfs ) : stops = gtfs . get_table ( "stops" ) median_lat = numpy . percentile ( stops [ 'lat' ] . values , 50 ) median_lon = numpy . percentile ( stops [ 'lon' ] . values , 50 ) return median_lat , median_lon
10667	def add_to ( self , other ) : if type ( other ) is MaterialPackage : if self . material == other . material : self . size_class_masses = self . size_class_masses + other . size_class_masses else : for size_class in other . material . size_classes : if size_class not in self . material . size_classes : raise Exception ( "Packages of '" + other . material . name + "' cannot be added to packages of '" + self . material . name + "'. The size class '" + size_class + "' was not found in '" + self . material . name + "'." ) self . add_to ( ( size_class , other . get_size_class_mass ( size_class ) ) ) elif self . _is_size_class_mass_tuple ( other ) : size_class = other [ 0 ] compound_index = self . material . get_size_class_index ( size_class ) mass = other [ 1 ] self . size_class_masses [ compound_index ] = self . size_class_masses [ compound_index ] + mass else : raise TypeError ( "Invalid addition argument." )
11384	def module ( self ) : if not hasattr ( self , '_module' ) : if "__main__" in sys . modules : mod = sys . modules [ "__main__" ] path = self . normalize_path ( mod . __file__ ) if os . path . splitext ( path ) == os . path . splitext ( self . path ) : self . _module = mod else : self . _module = imp . load_source ( 'captain_script' , self . path ) return self . _module
2134	def _get_schema ( self , wfjt_id ) : node_res = get_resource ( 'node' ) node_results = node_res . list ( workflow_job_template = wfjt_id , all_pages = True ) [ 'results' ] return self . _workflow_node_structure ( node_results )
4706	def read ( self , path ) : with open ( path , "rb" ) as fout : memmove ( self . m_buf , fout . read ( self . m_size ) , self . m_size )
1089	def indexOf ( a , b ) : "Return the first index of b in a." for i , j in enumerate ( a ) : if j == b : return i else : raise ValueError ( 'sequence.index(x): x not in sequence' )
12775	def inverse_dynamics ( self , angles , start = 0 , end = 1e100 , states = None , max_force = 100 ) : if states is not None : self . skeleton . set_body_states ( states ) for frame_no , frame in enumerate ( angles ) : if frame_no < start : continue if frame_no >= end : break self . ode_space . collide ( None , self . on_collision ) states = self . skeleton . get_body_states ( ) self . skeleton . set_body_states ( states ) self . skeleton . enable_motors ( max_force ) self . skeleton . set_target_angles ( angles [ frame_no ] ) self . ode_world . step ( self . dt ) torques = self . skeleton . joint_torques self . skeleton . disable_motors ( ) self . skeleton . set_body_states ( states ) self . skeleton . add_torques ( torques ) yield torques self . ode_world . step ( self . dt ) self . ode_contactgroup . empty ( )
5889	def smart_unicode ( string , encoding = 'utf-8' , strings_only = False , errors = 'strict' ) : return force_unicode ( string , encoding , strings_only , errors )
7259	def search_point ( self , lat , lng , filters = None , startDate = None , endDate = None , types = None , type = None ) : searchAreaWkt = "POLYGON ((%s %s, %s %s, %s %s, %s %s, %s %s))" % ( lng , lat , lng , lat , lng , lat , lng , lat , lng , lat ) return self . search ( searchAreaWkt = searchAreaWkt , filters = filters , startDate = startDate , endDate = endDate , types = types )
11184	def wrap_state_dict ( self , typename : str , state ) -> Dict [ str , Any ] : return { self . type_key : typename , self . state_key : state }
65	def clip_out_of_image ( self , image ) : shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] ia . do_assert ( height > 0 ) ia . do_assert ( width > 0 ) eps = np . finfo ( np . float32 ) . eps x1 = np . clip ( self . x1 , 0 , width - eps ) x2 = np . clip ( self . x2 , 0 , width - eps ) y1 = np . clip ( self . y1 , 0 , height - eps ) y2 = np . clip ( self . y2 , 0 , height - eps ) return self . copy ( x1 = x1 , y1 = y1 , x2 = x2 , y2 = y2 , label = self . label )
9895	def _uptime_syllable ( ) : global __boottime try : __boottime = os . stat ( '/dev/pty/mst/pty0' ) . st_mtime return time . time ( ) - __boottime except ( NameError , OSError ) : return None
344	def train_and_validate_to_end ( self , validate_step_size = 50 ) : while not self . _sess . should_stop ( ) : self . train_on_batch ( ) if self . global_step % validate_step_size == 0 : log_str = 'step: %d, ' % self . global_step for n , m in self . validation_metrics : log_str += '%s: %f, ' % ( n . name , m ) logging . info ( log_str )
2846	def close ( self ) : if self . _ctx is not None : ftdi . free ( self . _ctx ) self . _ctx = None
12874	def satisfies ( guard ) : i = peek ( ) if ( i is EndOfFile ) or ( not guard ( i ) ) : fail ( [ "<satisfies predicate " + _fun_to_str ( guard ) + ">" ] ) next ( ) return i
4962	def clean_email_or_username ( self ) : email_or_username = self . cleaned_data [ self . Fields . EMAIL_OR_USERNAME ] . strip ( ) if not email_or_username : return email_or_username email = email_or_username__to__email ( email_or_username ) bulk_entry = len ( split_usernames_and_emails ( email ) ) > 1 if bulk_entry : for email in split_usernames_and_emails ( email ) : validate_email_to_link ( email , None , ValidationMessages . INVALID_EMAIL_OR_USERNAME , ignore_existing = True ) email = email_or_username else : validate_email_to_link ( email , email_or_username , ValidationMessages . INVALID_EMAIL_OR_USERNAME , ignore_existing = True ) return email
5787	def _advapi32_encrypt ( cipher , key , data , iv , padding ) : context_handle = None key_handle = None try : context_handle , key_handle = _advapi32_create_handles ( cipher , key , iv ) out_len = new ( advapi32 , 'DWORD *' , len ( data ) ) res = advapi32 . CryptEncrypt ( key_handle , null ( ) , True , 0 , null ( ) , out_len , 0 ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) write_to_buffer ( buffer , data ) pointer_set ( out_len , len ( data ) ) res = advapi32 . CryptEncrypt ( key_handle , null ( ) , True , 0 , buffer , out_len , buffer_len ) handle_error ( res ) output = bytes_from_buffer ( buffer , deref ( out_len ) ) if cipher == 'aes' and not padding : if output [ - 16 : ] != ( b'\x10' * 16 ) : raise ValueError ( 'Invalid padding generated by OS crypto library' ) output = output [ : - 16 ] return output finally : if key_handle : advapi32 . CryptDestroyKey ( key_handle ) if context_handle : close_context_handle ( context_handle )
931	def next ( self , record , curInputBookmark ) : outRecord = None retInputBookmark = None if record is not None : self . _inIdx += 1 if self . _filter != None and not self . _filter [ 0 ] ( self . _filter [ 1 ] , record ) : return ( None , None ) if self . _nullAggregation : return ( record , curInputBookmark ) t = record [ self . _timeFieldIdx ] if self . _firstSequenceStartTime == None : self . _firstSequenceStartTime = t if self . _startTime is None : self . _startTime = t if self . _endTime is None : self . _endTime = self . _getEndTime ( t ) assert self . _endTime > t if self . _resetFieldIdx is not None : resetSignal = record [ self . _resetFieldIdx ] else : resetSignal = None if self . _sequenceIdFieldIdx is not None : currSequenceId = record [ self . _sequenceIdFieldIdx ] else : currSequenceId = None newSequence = ( resetSignal == 1 and self . _inIdx > 0 ) or self . _sequenceId != currSequenceId or self . _inIdx == 0 if newSequence : self . _sequenceId = currSequenceId sliceEnded = ( t >= self . _endTime or t < self . _startTime ) if ( newSequence or sliceEnded ) and len ( self . _slice ) > 0 : for j , f in enumerate ( self . _fields ) : index = f [ 0 ] if index == self . _timeFieldIdx : self . _slice [ j ] [ 0 ] = self . _startTime break outRecord = self . _createAggregateRecord ( ) retInputBookmark = self . _aggrInputBookmark self . _slice = defaultdict ( list ) for j , f in enumerate ( self . _fields ) : index = f [ 0 ] self . _slice [ j ] . append ( record [ index ] ) self . _aggrInputBookmark = curInputBookmark if newSequence : self . _startTime = t self . _endTime = self . _getEndTime ( t ) if sliceEnded : if t < self . _startTime : self . _endTime = self . _firstSequenceStartTime while t >= self . _endTime : self . _startTime = self . _endTime self . _endTime = self . _getEndTime ( self . _endTime ) if outRecord is not None : return ( outRecord , retInputBookmark ) elif self . _slice : for j , f in enumerate ( self . _fields ) : index = f [ 0 ] if index == self . _timeFieldIdx : self . _slice [ j ] [ 0 ] = self . _startTime break outRecord = self . _createAggregateRecord ( ) retInputBookmark = self . _aggrInputBookmark self . _slice = defaultdict ( list ) return ( outRecord , retInputBookmark )
2487	def create_conjunction_node ( self , conjunction ) : node = BNode ( ) type_triple = ( node , RDF . type , self . spdx_namespace . ConjunctiveLicenseSet ) self . graph . add ( type_triple ) licenses = self . licenses_from_tree ( conjunction ) for lic in licenses : member_triple = ( node , self . spdx_namespace . member , lic ) self . graph . add ( member_triple ) return node
768	def getMetricDetails ( self , metricLabel ) : try : metricIndex = self . __metricLabels . index ( metricLabel ) except IndexError : return None return self . __metrics [ metricIndex ] . getMetric ( )
13258	def _file_path ( self , uid ) : file_name = '%s.doentry' % ( uid ) return os . path . join ( self . dayone_journal_path , file_name )
3353	def _replace_on_id ( self , new_object ) : the_id = new_object . id the_index = self . _dict [ the_id ] list . __setitem__ ( self , the_index , new_object )
4860	def force_fresh_session ( view ) : @ wraps ( view ) def wrapper ( request , * args , ** kwargs ) : if not request . GET . get ( FRESH_LOGIN_PARAMETER ) : enterprise_customer = get_enterprise_customer_or_404 ( kwargs . get ( 'enterprise_uuid' ) ) provider_id = enterprise_customer . identity_provider or '' sso_provider = get_identity_provider ( provider_id ) if sso_provider : scheme , netloc , path , params , query , fragment = urlparse ( request . get_full_path ( ) ) redirect_url = urlunparse ( ( scheme , netloc , quote ( path ) , params , query , fragment ) ) return redirect ( '{logout_url}?{params}' . format ( logout_url = '/logout' , params = urlencode ( { 'redirect_url' : redirect_url } ) ) ) return view ( request , * args , ** kwargs ) return wrapper
12398	def gen_methods ( self , * args , ** kwargs ) : token = args [ 0 ] inst = self . inst prefix = self . _method_prefix for method_key in self . gen_method_keys ( * args , ** kwargs ) : method = getattr ( inst , prefix + method_key , None ) if method is not None : yield method typename = type ( token ) . __name__ yield from self . check_basetype ( token , typename , self . builtins . get ( typename ) ) for basetype_name in self . interp_types : yield from self . check_basetype ( token , basetype_name , getattr ( self . types , basetype_name , None ) ) for basetype_name in self . abc_types : yield from self . check_basetype ( token , basetype_name , getattr ( self . collections , basetype_name , None ) ) yield from self . gen_generic ( )
8318	def connect_table ( self , table , chunk , markup ) : k = markup . find ( chunk ) i = markup . rfind ( "\n=" , 0 , k ) j = markup . find ( "\n" , i + 1 ) paragraph_title = markup [ i : j ] . strip ( ) . strip ( "= " ) for paragraph in self . paragraphs : if paragraph . title == paragraph_title : paragraph . tables . append ( table ) table . paragraph = paragraph
4493	def list_ ( args ) : osf = _setup_osf ( args ) project = osf . project ( args . project ) for store in project . storages : prefix = store . name for file_ in store . files : path = file_ . path if path . startswith ( '/' ) : path = path [ 1 : ] print ( os . path . join ( prefix , path ) )
7329	async def request ( self , method , url , future , headers = None , session = None , encoding = None , ** kwargs ) : await self . setup req_kwargs = await self . headers . prepare_request ( method = method , url = url , headers = headers , proxy = self . proxy , ** kwargs ) if encoding is None : encoding = self . encoding session = session if ( session is not None ) else self . _session logger . debug ( "making request with parameters: %s" % req_kwargs ) async with session . request ( ** req_kwargs ) as response : if response . status < 400 : data = await data_processing . read ( response , self . _loads , encoding = encoding ) future . set_result ( data_processing . PeonyResponse ( data = data , headers = response . headers , url = response . url , request = req_kwargs ) ) else : await exceptions . throw ( response , loads = self . _loads , encoding = encoding , url = url )
1727	def parse_identifier ( source , start , throw = True ) : start = pass_white ( source , start ) end = start if not end < len ( source ) : if throw : raise SyntaxError ( 'Missing identifier!' ) return None if source [ end ] not in IDENTIFIER_START : if throw : raise SyntaxError ( 'Invalid identifier start: "%s"' % source [ end ] ) return None end += 1 while end < len ( source ) and source [ end ] in IDENTIFIER_PART : end += 1 if not is_valid_lval ( source [ start : end ] ) : if throw : raise SyntaxError ( 'Invalid identifier name: "%s"' % source [ start : end ] ) return None return source [ start : end ] , end
1405	def validate_extra_link ( self , extra_link ) : if EXTRA_LINK_NAME_KEY not in extra_link or EXTRA_LINK_FORMATTER_KEY not in extra_link : raise Exception ( "Invalid extra.links format. " + "Extra link must include a 'name' and 'formatter' field" ) self . validated_formatter ( extra_link [ EXTRA_LINK_FORMATTER_KEY ] ) return extra_link
5869	def _inactivate_organization_course_relationship ( relationship ) : relationship = internal . OrganizationCourse . objects . get ( id = relationship . id , active = True ) _inactivate_record ( relationship )
9969	def value ( self ) : if self . has_value : return self . _impl [ OBJ ] . get_value ( self . _impl [ KEY ] ) else : raise ValueError ( "Value not found" )
3583	def _print_tree ( self ) : objects = self . _bluez . GetManagedObjects ( ) for path in objects . keys ( ) : print ( "[ %s ]" % ( path ) ) interfaces = objects [ path ] for interface in interfaces . keys ( ) : if interface in [ "org.freedesktop.DBus.Introspectable" , "org.freedesktop.DBus.Properties" ] : continue print ( " %s" % ( interface ) ) properties = interfaces [ interface ] for key in properties . keys ( ) : print ( " %s = %s" % ( key , properties [ key ] ) )
13335	def cache_resolver ( resolver , path ) : env = resolver . cache . find ( path ) if env : return env raise ResolveError
3301	def element_content_as_string ( element ) : if len ( element ) == 0 : return element . text or "" stream = compat . StringIO ( ) for childnode in element : stream . write ( xml_to_bytes ( childnode , pretty_print = False ) + "\n" ) s = stream . getvalue ( ) stream . close ( ) return s
13262	def task ( func , ** config ) : if func . __name__ == func . __qualname__ : assert not func . __qualname__ in _task_list , "Can not define the same task \"{}\" twice" . format ( func . __qualname__ ) logger . debug ( "Found task %s" , func ) _task_list [ func . __qualname__ ] = Task ( plugin_class = None , func = func , config = config ) else : func . yaz_task_config = config return func
1612	def ProcessGlobalSuppresions ( lines ) : for line in lines : if _SEARCH_C_FILE . search ( line ) : for category in _DEFAULT_C_SUPPRESSED_CATEGORIES : _global_error_suppressions [ category ] = True if _SEARCH_KERNEL_FILE . search ( line ) : for category in _DEFAULT_KERNEL_SUPPRESSED_CATEGORIES : _global_error_suppressions [ category ] = True
3845	def to_participantid ( user_id ) : return hangouts_pb2 . ParticipantId ( chat_id = user_id . chat_id , gaia_id = user_id . gaia_id )
2145	def request ( self , method , url , * args , ** kwargs ) : import re url = re . sub ( "^/?api/v[0-9]+/" , "" , url ) use_version = not url . startswith ( '/o/' ) url = '%s%s' % ( self . get_prefix ( use_version ) , url . lstrip ( '/' ) ) kwargs . setdefault ( 'auth' , BasicTowerAuth ( settings . username , settings . password , self ) ) headers = kwargs . get ( 'headers' , { } ) if method . upper ( ) in ( 'PATCH' , 'POST' , 'PUT' ) : headers . setdefault ( 'Content-Type' , 'application/json' ) kwargs [ 'headers' ] = headers debug . log ( '%s %s' % ( method , url ) , fg = 'blue' , bold = True ) if method in ( 'POST' , 'PUT' , 'PATCH' ) : debug . log ( 'Data: %s' % kwargs . get ( 'data' , { } ) , fg = 'blue' , bold = True ) if method == 'GET' or kwargs . get ( 'params' , None ) : debug . log ( 'Params: %s' % kwargs . get ( 'params' , { } ) , fg = 'blue' , bold = True ) debug . log ( '' ) if headers . get ( 'Content-Type' , '' ) == 'application/json' : kwargs [ 'data' ] = json . dumps ( kwargs . get ( 'data' , { } ) ) r = self . _make_request ( method , url , args , kwargs ) if r . status_code >= 500 : raise exc . ServerError ( 'The Tower server sent back a server error. ' 'Please try again later.' ) if r . status_code == 401 : raise exc . AuthError ( 'Invalid Tower authentication credentials (HTTP 401).' ) if r . status_code == 403 : raise exc . Forbidden ( "You don't have permission to do that (HTTP 403)." ) if r . status_code == 404 : raise exc . NotFound ( 'The requested object could not be found.' ) if r . status_code == 405 : raise exc . MethodNotAllowed ( "The Tower server says you can't make a request with the " "%s method to that URL (%s)." % ( method , url ) , ) if r . status_code >= 400 : raise exc . BadRequest ( 'The Tower server claims it was sent a bad request.\n\n' '%s %s\nParams: %s\nData: %s\n\nResponse: %s' % ( method , url , kwargs . get ( 'params' , None ) , kwargs . get ( 'data' , None ) , r . content . decode ( 'utf8' ) ) ) r . __class__ = APIResponse return r
13276	def update_desc_rsib_path ( desc , sibs_len ) : if ( desc [ 'sib_seq' ] < ( sibs_len - 1 ) ) : rsib_path = copy . deepcopy ( desc [ 'path' ] ) rsib_path [ - 1 ] = desc [ 'sib_seq' ] + 1 desc [ 'rsib_path' ] = rsib_path else : pass return ( desc )
7431	def _count_PIS ( seqsamp , N ) : counts = [ Counter ( col ) for col in seqsamp . T if not ( "-" in col or "N" in col ) ] pis = [ i . most_common ( 2 ) [ 1 ] [ 1 ] > 1 for i in counts if len ( i . most_common ( 2 ) ) > 1 ] if sum ( pis ) >= N : return sum ( pis ) else : return 0
13456	def open_s3 ( bucket ) : conn = boto . connect_s3 ( options . paved . s3 . access_id , options . paved . s3 . secret ) try : bucket = conn . get_bucket ( bucket ) except boto . exception . S3ResponseError : bucket = conn . create_bucket ( bucket ) return bucket
7728	def add_item ( self , item ) : if not isinstance ( item , MucItemBase ) : raise TypeError ( "Bad item type for muc#user" ) item . as_xml ( self . xmlnode )
12291	def annotate_metadata_data ( repo , task , patterns = [ "*" ] , size = 0 ) : mgr = plugins_get_mgr ( ) keys = mgr . search ( 'representation' ) [ 'representation' ] representations = [ mgr . get_by_key ( 'representation' , k ) for k in keys ] matching_files = repo . find_matching_files ( patterns ) package = repo . package rootdir = repo . rootdir files = package [ 'resources' ] for f in files : relativepath = f [ 'relativepath' ] if relativepath in matching_files : path = os . path . join ( rootdir , relativepath ) if task == 'preview' : print ( "Adding preview for " , relativepath ) f [ 'content' ] = open ( path ) . read ( ) [ : size ] elif task == 'schema' : for r in representations : if r . can_process ( path ) : print ( "Adding schema for " , path ) f [ 'schema' ] = r . get_schema ( path ) break
13798	def handle ( self ) : while True : try : line = self . rfile . readline ( ) try : cmd = json . loads ( line ) except Exception , exc : self . wfile . write ( repr ( exc ) + NEWLINE ) continue else : handler = getattr ( self , 'handle_' + cmd [ 0 ] , None ) if not handler : self . wfile . write ( repr ( CommandNotFound ( cmd [ 0 ] ) ) + NEWLINE ) continue return_value = handler ( * cmd [ 1 : ] ) if not return_value : continue self . wfile . write ( one_lineify ( json . dumps ( return_value ) ) + NEWLINE ) except Exception , exc : self . wfile . write ( repr ( exc ) + NEWLINE ) continue
4827	def get_course_enrollment ( self , username , course_id ) : endpoint = getattr ( self . client . enrollment , '{username},{course_id}' . format ( username = username , course_id = course_id ) ) try : result = endpoint . get ( ) except HttpNotFoundError : LOGGER . error ( 'Course enrollment details not found for invalid username or course; username=[%s], course=[%s]' , username , course_id ) return None if not result : LOGGER . info ( 'Failed to find course enrollment details for user [%s] and course [%s]' , username , course_id ) return None return result
3132	def create ( self , data ) : if 'name' not in data : raise KeyError ( 'The list must have a name' ) if 'contact' not in data : raise KeyError ( 'The list must have a contact' ) if 'company' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a company' ) if 'address1' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a address1' ) if 'city' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a city' ) if 'state' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a state' ) if 'zip' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a zip' ) if 'country' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a country' ) if 'permission_reminder' not in data : raise KeyError ( 'The list must have a permission_reminder' ) if 'campaign_defaults' not in data : raise KeyError ( 'The list must have a campaign_defaults' ) if 'from_name' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a from_name' ) if 'from_email' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a from_email' ) check_email ( data [ 'campaign_defaults' ] [ 'from_email' ] ) if 'subject' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a subject' ) if 'language' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a language' ) if 'email_type_option' not in data : raise KeyError ( 'The list must have an email_type_option' ) if data [ 'email_type_option' ] not in [ True , False ] : raise TypeError ( 'The list email_type_option must be True or False' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . list_id = response [ 'id' ] else : self . list_id = None return response
216	def setdefault ( self , key : str , value : str ) -> str : set_key = key . lower ( ) . encode ( "latin-1" ) set_value = value . encode ( "latin-1" ) for idx , ( item_key , item_value ) in enumerate ( self . _list ) : if item_key == set_key : return item_value . decode ( "latin-1" ) self . _list . append ( ( set_key , set_value ) ) return value
8088	def fontsize ( self , fontsize = None ) : if fontsize is not None : self . _canvas . fontsize = fontsize else : return self . _canvas . fontsize
3195	def delete ( self , list_id , subscriber_hash ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash return self . _mc_client . _delete ( url = self . _build_path ( list_id , 'members' , subscriber_hash ) )
1195	def calculate_transitive_deps ( modname , script , gopath ) : deps = set ( ) def calc ( modname , script ) : if modname in deps : return deps . add ( modname ) for imp in collect_imports ( modname , script , gopath ) : if imp . is_native : deps . add ( imp . name ) continue parts = imp . name . split ( '.' ) calc ( imp . name , imp . script ) if len ( parts ) == 1 : continue package_dir , filename = os . path . split ( imp . script ) if filename == '__init__.py' : package_dir = os . path . dirname ( package_dir ) for i in xrange ( len ( parts ) - 1 , 0 , - 1 ) : modname = '.' . join ( parts [ : i ] ) script = os . path . join ( package_dir , '__init__.py' ) calc ( modname , script ) package_dir = os . path . dirname ( package_dir ) calc ( modname , script ) deps . remove ( modname ) return deps
2809	def convert_constant ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting constant ...' ) params_list = params [ 'value' ] . numpy ( ) def target_layer ( x , value = params_list ) : return tf . constant ( value . tolist ( ) , shape = value . shape ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name + '_np' ] = params_list layers [ scope_name ] = lambda_layer ( layers [ list ( layers . keys ( ) ) [ 0 ] ] )
3981	def _get_expanded_active_specs ( specs ) : _filter_active ( constants . CONFIG_BUNDLES_KEY , specs ) _filter_active ( 'apps' , specs ) _expand_libs_in_apps ( specs ) _filter_active ( 'libs' , specs ) _filter_active ( 'services' , specs ) _add_active_assets ( specs )
8655	def search_messages ( session , thread_id , query , limit = 20 , offset = 0 , message_context_details = None , window_above = None , window_below = None ) : query = { 'thread_id' : thread_id , 'query' : query , 'limit' : limit , 'offset' : offset } if message_context_details : query [ 'message_context_details' ] = message_context_details if window_above : query [ 'window_above' ] = window_above if window_below : query [ 'window_below' ] = window_below response = make_get_request ( session , 'messages/search' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MessagesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
6355	def _language_index_from_code ( self , code , name_mode ) : if code < 1 or code > sum ( _LANG_DICT [ _ ] for _ in BMDATA [ name_mode ] [ 'languages' ] ) : return L_ANY if ( code & ( code - 1 ) ) != 0 : return L_ANY return code
8835	def less_or_equal ( a , b , * args ) : return ( less ( a , b ) or soft_equals ( a , b ) ) and ( not args or less_or_equal ( b , * args ) )
5196	def configure_stack ( ) : stack_config = asiodnp3 . OutstationStackConfig ( opendnp3 . DatabaseSizes . AllTypes ( 10 ) ) stack_config . outstation . eventBufferConfig = opendnp3 . EventBufferConfig ( ) . AllTypes ( 10 ) stack_config . outstation . params . allowUnsolicited = True stack_config . link . LocalAddr = 10 stack_config . link . RemoteAddr = 1 stack_config . link . KeepAliveTimeout = openpal . TimeDuration ( ) . Max ( ) return stack_config
1602	def parse_topo_loc ( cl_args ) : try : topo_loc = cl_args [ 'cluster/[role]/[env]' ] . split ( '/' ) topo_name = cl_args [ 'topology-name' ] topo_loc . append ( topo_name ) if len ( topo_loc ) != 4 : raise return topo_loc except Exception : Log . error ( 'Invalid topology location' ) raise
2873	def trash ( self , file ) : if self . _should_skipped_by_specs ( file ) : self . reporter . unable_to_trash_dot_entries ( file ) return volume_of_file_to_be_trashed = self . volume_of_parent ( file ) self . reporter . volume_of_file ( volume_of_file_to_be_trashed ) candidates = self . _possible_trash_directories_for ( volume_of_file_to_be_trashed ) self . try_trash_file_using_candidates ( file , volume_of_file_to_be_trashed , candidates )
517	def _bumpUpWeakColumns ( self ) : weakColumns = numpy . where ( self . _overlapDutyCycles < self . _minOverlapDutyCycles ) [ 0 ] for columnIndex in weakColumns : perm = self . _permanences [ columnIndex ] . astype ( realDType ) maskPotential = numpy . where ( self . _potentialPools [ columnIndex ] > 0 ) [ 0 ] perm [ maskPotential ] += self . _synPermBelowStimulusInc self . _updatePermanencesForColumn ( perm , columnIndex , raisePerm = False )
12734	def set_body_states ( self , states ) : for state in states : self . get_body ( state . name ) . state = state
7006	def plot_training_results ( classifier , classlabels , outfile ) : if isinstance ( classifier , str ) and os . path . exists ( classifier ) : with open ( classifier , 'rb' ) as infd : clfdict = pickle . load ( infd ) elif isinstance ( classifier , dict ) : clfdict = classifier else : LOGERROR ( "can't figure out the input classifier arg" ) return None confmatrix = clfdict [ 'best_confmatrix' ] overall_feature_importances = clfdict [ 'best_classifier' ] . feature_importances_ feature_importances_per_tree = np . array ( [ tree . feature_importances_ for tree in clfdict [ 'best_classifier' ] . estimators_ ] ) stdev_feature_importances = np . std ( feature_importances_per_tree , axis = 0 ) feature_names = np . array ( clfdict [ 'feature_names' ] ) plt . figure ( figsize = ( 6.4 * 3.0 , 4.8 ) ) plt . subplot ( 121 ) classes = np . array ( classlabels ) plt . imshow ( confmatrix , interpolation = 'nearest' , cmap = plt . cm . Blues ) tick_marks = np . arange ( len ( classes ) ) plt . xticks ( tick_marks , classes ) plt . yticks ( tick_marks , classes ) plt . title ( 'evaluation set confusion matrix' ) plt . ylabel ( 'predicted class' ) plt . xlabel ( 'actual class' ) thresh = confmatrix . max ( ) / 2. for i , j in itertools . product ( range ( confmatrix . shape [ 0 ] ) , range ( confmatrix . shape [ 1 ] ) ) : plt . text ( j , i , confmatrix [ i , j ] , horizontalalignment = "center" , color = "white" if confmatrix [ i , j ] > thresh else "black" ) plt . subplot ( 122 ) features = np . array ( feature_names ) sorted_ind = np . argsort ( overall_feature_importances ) [ : : - 1 ] features = features [ sorted_ind ] feature_names = feature_names [ sorted_ind ] overall_feature_importances = overall_feature_importances [ sorted_ind ] stdev_feature_importances = stdev_feature_importances [ sorted_ind ] plt . bar ( np . arange ( 0 , features . size ) , overall_feature_importances , yerr = stdev_feature_importances , width = 0.8 , color = 'grey' ) plt . xticks ( np . arange ( 0 , features . size ) , features , rotation = 90 ) plt . yticks ( [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9 , 1.0 ] ) plt . xlim ( - 0.75 , features . size - 1.0 + 0.75 ) plt . ylim ( 0.0 , 0.9 ) plt . ylabel ( 'relative importance' ) plt . title ( 'relative importance of features' ) plt . subplots_adjust ( wspace = 0.1 ) plt . savefig ( outfile , bbox_inches = 'tight' , dpi = 100 ) plt . close ( 'all' ) return outfile
7556	def random_combination ( nsets , n , k ) : sets = set ( ) while len ( sets ) < nsets : newset = tuple ( sorted ( np . random . choice ( n , k , replace = False ) ) ) sets . add ( newset ) return tuple ( sets )
1808	def SETC ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . CF , 1 , 0 ) )
5560	def bounds ( self ) : if self . _raw [ "bounds" ] is None : return self . process_pyramid . bounds else : return Bounds ( * _validate_bounds ( self . _raw [ "bounds" ] ) )
711	def _iterModels ( modelIDs ) : class ModelInfoIterator ( object ) : __CACHE_LIMIT = 1000 debug = False def __init__ ( self , modelIDs ) : self . __modelIDs = tuple ( modelIDs ) if self . debug : _emit ( Verbosity . DEBUG , "MODELITERATOR: __init__; numModelIDs=%s" % len ( self . __modelIDs ) ) self . __nextIndex = 0 self . __modelCache = collections . deque ( ) return def __iter__ ( self ) : return self def next ( self ) : return self . __getNext ( ) def __getNext ( self ) : if self . debug : _emit ( Verbosity . DEBUG , "MODELITERATOR: __getNext(); modelCacheLen=%s" % ( len ( self . __modelCache ) ) ) if not self . __modelCache : self . __fillCache ( ) if not self . __modelCache : raise StopIteration ( ) return self . __modelCache . popleft ( ) def __fillCache ( self ) : assert ( not self . __modelCache ) numModelIDs = len ( self . __modelIDs ) if self . __modelIDs else 0 if self . __nextIndex >= numModelIDs : return idRange = self . __nextIndex + self . __CACHE_LIMIT if idRange > numModelIDs : idRange = numModelIDs lookupIDs = self . __modelIDs [ self . __nextIndex : idRange ] self . __nextIndex += ( idRange - self . __nextIndex ) infoList = _clientJobsDB ( ) . modelsInfo ( lookupIDs ) assert len ( infoList ) == len ( lookupIDs ) , "modelsInfo returned %s elements; expected %s." % ( len ( infoList ) , len ( lookupIDs ) ) for rawInfo in infoList : modelInfo = _NupicModelInfo ( rawInfo = rawInfo ) self . __modelCache . append ( modelInfo ) assert len ( self . __modelCache ) == len ( lookupIDs ) , "Added %s elements to modelCache; expected %s." % ( len ( self . __modelCache ) , len ( lookupIDs ) ) if self . debug : _emit ( Verbosity . DEBUG , "MODELITERATOR: Leaving __fillCache(); modelCacheLen=%s" % ( len ( self . __modelCache ) , ) ) return ModelInfoIterator ( modelIDs )
7589	def call_fastq_dump_on_SRRs ( self , srr , outname , paired ) : fd_cmd = [ "fastq-dump" , srr , "--accession" , outname , "--outdir" , self . workdir , "--gzip" , ] if paired : fd_cmd += [ "--split-files" ] proc = sps . Popen ( fd_cmd , stderr = sps . STDOUT , stdout = sps . PIPE ) o , e = proc . communicate ( ) srafile = os . path . join ( self . workdir , "sra" , srr + ".sra" ) if os . path . exists ( srafile ) : os . remove ( srafile )
13646	def hump_to_underscore ( name ) : new_name = '' pos = 0 for c in name : if pos == 0 : new_name = c . lower ( ) elif 65 <= ord ( c ) <= 90 : new_name += '_' + c . lower ( ) pass else : new_name += c pos += 1 pass return new_name
7961	def disconnect ( self ) : logger . debug ( "TCPTransport.disconnect()" ) with self . lock : if self . _socket is None : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" ) return if self . _hup or not self . _serializer : self . _close ( ) else : self . send_stream_tail ( )
10860	def param_particle ( self , ind ) : ind = self . _vps ( listify ( ind ) ) return [ self . _i2p ( i , j ) for i in ind for j in [ 'z' , 'y' , 'x' , 'a' ] ]
11362	def convert_html_subscripts_to_latex ( text ) : text = re . sub ( "<sub>(.*?)</sub>" , r"$_{\1}$" , text ) text = re . sub ( "<sup>(.*?)</sup>" , r"$^{\1}$" , text ) return text
1215	def _wait_state ( self , state , reward , terminal ) : while state == [ None ] or not state : state , terminal , reward = self . _execute ( dict ( key = 0 ) ) return state , terminal , reward
8309	def pangocairo_create_context ( cr ) : try : return PangoCairo . create_context ( cr ) except KeyError as e : if e . args == ( 'could not find foreign type Context' , ) : raise ShoebotInstallError ( "Error creating PangoCairo missing dependency: python-gi-cairo" ) else : raise
4821	def redirect_if_blocked ( course_run_ids , user = None , ip_address = None , url = None ) : for course_run_id in course_run_ids : redirect_url = embargo_api . redirect_if_blocked ( CourseKey . from_string ( course_run_id ) , user = user , ip_address = ip_address , url = url ) if redirect_url : return redirect_url
7138	def get_type_info ( obj ) : if isinstance ( obj , primitive_types ) : return ( 'primitive' , type ( obj ) . __name__ ) if isinstance ( obj , sequence_types ) : return ( 'sequence' , type ( obj ) . __name__ ) if isinstance ( obj , array_types ) : return ( 'array' , type ( obj ) . __name__ ) if isinstance ( obj , key_value_types ) : return ( 'key-value' , type ( obj ) . __name__ ) if isinstance ( obj , types . ModuleType ) : return ( 'module' , type ( obj ) . __name__ ) if isinstance ( obj , ( types . FunctionType , types . MethodType ) ) : return ( 'function' , type ( obj ) . __name__ ) if isinstance ( obj , type ) : if hasattr ( obj , '__dict__' ) : return ( 'class' , obj . __name__ ) if isinstance ( type ( obj ) , type ) : if hasattr ( obj , '__dict__' ) : cls_name = type ( obj ) . __name__ if cls_name == 'classobj' : cls_name = obj . __name__ return ( 'class' , '{}' . format ( cls_name ) ) if cls_name == 'instance' : cls_name = obj . __class__ . __name__ return ( 'instance' , '{} instance' . format ( cls_name ) ) return ( 'unknown' , type ( obj ) . __name__ )
7806	def verify_jid_against_srv_name ( self , jid , srv_type ) : srv_prefix = u"_" + srv_type + u"." srv_prefix_l = len ( srv_prefix ) for srv in self . alt_names . get ( "SRVName" , [ ] ) : logger . debug ( "checking {0!r} against {1!r}" . format ( jid , srv ) ) if not srv . startswith ( srv_prefix ) : logger . debug ( "{0!r} does not start with {1!r}" . format ( srv , srv_prefix ) ) continue try : srv_jid = JID ( srv [ srv_prefix_l : ] ) except ValueError : continue if srv_jid == jid : logger . debug ( "Match!" ) return True return False
8845	def _at_block_start ( tc , line ) : if tc . atBlockStart ( ) : return True column = tc . columnNumber ( ) indentation = len ( line ) - len ( line . lstrip ( ) ) return column <= indentation
12517	def extract_datasets ( h5file , h5path = '/' ) : if isinstance ( h5file , str ) : _h5file = h5py . File ( h5file , mode = 'r' ) else : _h5file = h5file _datasets = get_datasets ( _h5file , h5path ) datasets = OrderedDict ( ) try : for ds in _datasets : datasets [ ds . name . split ( '/' ) [ - 1 ] ] = ds [ : ] except : raise RuntimeError ( 'Error reading datasets in {}/{}.' . format ( _h5file . filename , h5path ) ) finally : if isinstance ( h5file , str ) : _h5file . close ( ) return datasets
11088	def wake ( self , channel ) : self . log . info ( 'Waking up in %s' , channel ) self . _bot . dispatcher . unignore ( channel ) self . send_message ( channel , 'Hello, how may I be of service?' )
516	def _avgConnectedSpanForColumn2D ( self , columnIndex ) : assert ( self . _inputDimensions . size == 2 ) connected = self . _connectedSynapses [ columnIndex ] ( rows , cols ) = connected . reshape ( self . _inputDimensions ) . nonzero ( ) if rows . size == 0 and cols . size == 0 : return 0 rowSpan = rows . max ( ) - rows . min ( ) + 1 colSpan = cols . max ( ) - cols . min ( ) + 1 return numpy . average ( [ rowSpan , colSpan ] )
4443	def delete ( self , string ) : return self . redis . execute_command ( AutoCompleter . SUGDEL_COMMAND , self . key , string )
4760	def wait ( timeout = 300 ) : if env ( ) : cij . err ( "cij.ssh.wait: Invalid SSH environment" ) return 1 timeout_backup = cij . ENV . get ( "SSH_CMD_TIMEOUT" ) try : time_start = time . time ( ) cij . ENV [ "SSH_CMD_TIMEOUT" ] = "3" while True : time_current = time . time ( ) if ( time_current - time_start ) > timeout : cij . err ( "cij.ssh.wait: Timeout" ) return 1 status , _ , _ = command ( [ "exit" ] , shell = True , echo = False ) if not status : break cij . info ( "cij.ssh.wait: Time elapsed: %d seconds" % ( time_current - time_start ) ) finally : if timeout_backup is None : del cij . ENV [ "SSH_CMD_TIMEOUT" ] else : cij . ENV [ "SSH_CMD_TIMEOUT" ] = timeout_backup return 0
1276	def tf_initialize ( self , x_init , base_value , target_value , estimated_improvement ) : self . base_value = base_value if estimated_improvement is None : estimated_improvement = tf . abs ( x = base_value ) first_step = super ( LineSearch , self ) . tf_initialize ( x_init ) improvement = tf . divide ( x = ( target_value - self . base_value ) , y = tf . maximum ( x = estimated_improvement , y = util . epsilon ) ) last_improvement = improvement - 1.0 if self . mode == 'linear' : deltas = [ - t * self . parameter for t in x_init ] self . estimated_incr = - estimated_improvement * self . parameter elif self . mode == 'exponential' : deltas = [ - t * self . parameter for t in x_init ] return first_step + ( deltas , improvement , last_improvement , estimated_improvement )
51	def deepcopy ( self , x = None , y = None ) : x = self . x if x is None else x y = self . y if y is None else y return Keypoint ( x = x , y = y )
2129	def configure_display ( self , data , kwargs = None , write = False ) : if settings . format != 'human' : return if write : obj , obj_type , res , res_type = self . obj_res ( kwargs ) data [ 'type' ] = kwargs [ 'type' ] data [ obj_type ] = obj data [ res_type ] = res self . set_display_columns ( set_false = [ 'team' if obj_type == 'user' else 'user' ] , set_true = [ 'target_team' if res_type == 'team' else res_type ] ) else : self . set_display_columns ( set_false = [ 'user' , 'team' ] , set_true = [ 'resource_name' , 'resource_type' ] ) if 'results' in data : for i in range ( len ( data [ 'results' ] ) ) : self . populate_resource_columns ( data [ 'results' ] [ i ] ) else : self . populate_resource_columns ( data )
4625	def change_password ( self , newpassword ) : if not self . unlocked ( ) : raise WalletLocked self . password = newpassword self . _save_encrypted_masterpassword ( )
12506	def signed_session ( self , session = None ) : from sfctl . config import ( aad_metadata , aad_cache ) if session : session = super ( AdalAuthentication , self ) . signed_session ( session ) else : session = super ( AdalAuthentication , self ) . signed_session ( ) if self . no_verify : session . verify = False authority_uri , cluster_id , client_id = aad_metadata ( ) existing_token , existing_cache = aad_cache ( ) context = adal . AuthenticationContext ( authority_uri , cache = existing_cache ) new_token = context . acquire_token ( cluster_id , existing_token [ 'userId' ] , client_id ) header = "{} {}" . format ( "Bearer" , new_token [ 'accessToken' ] ) session . headers [ 'Authorization' ] = header return session
13697	def try_read_file ( s ) : try : with open ( s , 'r' ) as f : data = f . read ( ) except FileNotFoundError : return s except EnvironmentError as ex : print_err ( '\nFailed to read file: {}\n {}' . format ( s , ex ) ) return None return data
8636	def get_milestone_by_id ( session , milestone_id , user_details = None ) : endpoint = 'milestones/{}' . format ( milestone_id ) response = make_get_request ( session , endpoint , params_data = user_details ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MilestonesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
13398	def check_docstring ( cls ) : docstring = inspect . getdoc ( cls ) if not docstring : breadcrumbs = " -> " . join ( t . __name__ for t in inspect . getmro ( cls ) [ : - 1 ] [ : : - 1 ] ) msg = "docstring required for plugin '%s' (%s, defined in %s)" args = ( cls . __name__ , breadcrumbs , cls . __module__ ) raise InternalCashewException ( msg % args ) max_line_length = cls . _class_settings . get ( 'max-docstring-length' ) if max_line_length : for i , line in enumerate ( docstring . splitlines ( ) ) : if len ( line ) > max_line_length : msg = "docstring line %s of %s is %s chars too long" args = ( i , cls . __name__ , len ( line ) - max_line_length ) raise Exception ( msg % args ) return docstring
13392	def paginate_update ( update ) : from happenings . models import Update time = update . pub_time event = update . event try : next = Update . objects . filter ( event = event , pub_time__gt = time ) . order_by ( 'pub_time' ) . only ( 'title' ) [ 0 ] except : next = None try : previous = Update . objects . filter ( event = event , pub_time__lt = time ) . order_by ( '-pub_time' ) . only ( 'title' ) [ 0 ] except : previous = None return { 'next' : next , 'previous' : previous , 'event' : event }
6062	def mass_within_circle_in_units ( self , radius : dim . Length , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : self . check_units_of_radius_and_critical_surface_density ( radius = radius , critical_surface_density = critical_surface_density ) profile = self . new_profile_with_units_converted ( unit_length = radius . unit_length , unit_mass = 'angular' , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) mass_angular = dim . Mass ( value = quad ( profile . mass_integral , a = 0.0 , b = radius , args = ( 1.0 , ) ) [ 0 ] , unit_mass = 'angular' ) return mass_angular . convert ( unit_mass = unit_mass , critical_surface_density = critical_surface_density )
11749	def attach_bundle ( self , bundle ) : if not isinstance ( bundle , BlueprintBundle ) : raise IncompatibleBundle ( 'BlueprintBundle object passed to attach_bundle must be of type {0}' . format ( BlueprintBundle ) ) elif len ( bundle . blueprints ) == 0 : raise MissingBlueprints ( "Bundles must contain at least one flask.Blueprint" ) elif self . _bundle_exists ( bundle . path ) : raise ConflictingPath ( "Duplicate bundle path {0}" . format ( bundle . path ) ) elif self . _journey_path == bundle . path == '/' : raise ConflictingPath ( "Bundle path and Journey path cannot both be {0}" . format ( bundle . path ) ) self . _attached_bundles . append ( bundle )
6364	def to_tuple ( self ) : return self . _tp , self . _tn , self . _fp , self . _fn
5529	def _get_zoom_level ( zoom , process ) : if zoom is None : return reversed ( process . config . zoom_levels ) if isinstance ( zoom , int ) : return [ zoom ] elif len ( zoom ) == 2 : return reversed ( range ( min ( zoom ) , max ( zoom ) + 1 ) ) elif len ( zoom ) == 1 : return zoom
11273	def get_dict ( self ) : return dict ( current_page = self . current_page , total_page_count = self . total_page_count , items = self . items , total_item_count = self . total_item_count , page_size = self . page_size )
830	def pprint ( self , output , prefix = "" ) : print prefix , description = self . getDescription ( ) + [ ( "end" , self . getWidth ( ) ) ] for i in xrange ( len ( description ) - 1 ) : offset = description [ i ] [ 1 ] nextoffset = description [ i + 1 ] [ 1 ] print "%s |" % bitsToString ( output [ offset : nextoffset ] ) , print
529	def Array ( dtype , size = None , ref = False ) : def getArrayType ( self ) : return self . _dtype if ref : assert size is None index = basicTypes . index ( dtype ) if index == - 1 : raise Exception ( 'Invalid data type: ' + dtype ) if size and size <= 0 : raise Exception ( 'Array size must be positive' ) suffix = 'ArrayRef' if ref else 'Array' arrayFactory = getattr ( engine_internal , dtype + suffix ) arrayFactory . getType = getArrayType if size : a = arrayFactory ( size ) else : a = arrayFactory ( ) a . _dtype = basicTypes [ index ] return a
6728	def respawn ( name = None , group = None ) : if name is None : name = get_name ( ) delete ( name = name , group = group ) instance = get_or_create ( name = name , group = group ) env . host_string = instance . public_dns_name
13677	def prepare ( self ) : result_files = self . collect_files ( ) chain = self . prepare_handlers_chain if chain is None : chain = [ LessCompilerPrepareHandler ( ) ] for prepare_handler in chain : result_files = prepare_handler . prepare ( result_files , self ) return result_files
4858	def ignore_warning ( warning ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : warnings . simplefilter ( 'ignore' , warning ) return func ( * args , ** kwargs ) return wrapper return decorator
7219	def update ( self , task_name , task_json ) : r = self . gbdx_connection . put ( self . _base_url + '/' + task_name , json = task_json ) raise_for_status ( r ) return r . json ( )
10549	def get_results ( project_id , limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) params [ 'project_id' ] = project_id try : res = _pybossa_req ( 'get' , 'result' , params = params ) if type ( res ) . __name__ == 'list' : return [ Result ( result ) for result in res ] else : return res except : raise
2717	def add_droplets ( self , droplet ) : droplets = droplet if not isinstance ( droplets , list ) : droplets = [ droplet ] resources = self . __extract_resources_from_droplets ( droplets ) if len ( resources ) > 0 : return self . __add_resources ( resources ) return False
6102	def intensities_from_grid_radii ( self , grid_radii ) : return np . multiply ( np . multiply ( self . intensity_prime , np . power ( np . add ( 1 , np . power ( np . divide ( self . radius_break , grid_radii ) , self . alpha ) ) , ( self . gamma / self . alpha ) ) ) , np . exp ( np . multiply ( - self . sersic_constant , ( np . power ( np . divide ( np . add ( np . power ( grid_radii , self . alpha ) , ( self . radius_break ** self . alpha ) ) , ( self . effective_radius ** self . alpha ) ) , ( 1.0 / ( self . alpha * self . sersic_index ) ) ) ) ) ) )
12953	def _add_id_to_keys ( self , pk , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . sadd ( self . _get_ids_key ( ) , pk )
1709	def send ( self , str , end = '\n' ) : return self . _process . stdin . write ( str + end )
4442	def add_suggestions ( self , * suggestions , ** kwargs ) : pipe = self . redis . pipeline ( ) for sug in suggestions : args = [ AutoCompleter . SUGADD_COMMAND , self . key , sug . string , sug . score ] if kwargs . get ( 'increment' ) : args . append ( AutoCompleter . INCR ) if sug . payload : args . append ( 'PAYLOAD' ) args . append ( sug . payload ) pipe . execute_command ( * args ) return pipe . execute ( ) [ - 1 ]
4857	def deprecated ( extra ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : message = 'You called the deprecated function `{function}`. {extra}' . format ( function = func . __name__ , extra = extra ) frame = inspect . currentframe ( ) . f_back warnings . warn_explicit ( message , category = DeprecationWarning , filename = inspect . getfile ( frame . f_code ) , lineno = frame . f_lineno ) return func ( * args , ** kwargs ) return wrapper return decorator
5352	def __studies ( self , retention_time ) : cfg = self . config . get_conf ( ) if 'studies' not in cfg [ self . backend_section ] or not cfg [ self . backend_section ] [ 'studies' ] : logger . debug ( 'No studies for %s' % self . backend_section ) return studies = [ study for study in cfg [ self . backend_section ] [ 'studies' ] if study . strip ( ) != "" ] if not studies : logger . debug ( 'No studies for %s' % self . backend_section ) return logger . debug ( "Executing studies for %s: %s" % ( self . backend_section , studies ) ) time . sleep ( 2 ) enrich_backend = self . _get_enrich_backend ( ) ocean_backend = self . _get_ocean_backend ( enrich_backend ) active_studies = [ ] all_studies = enrich_backend . studies all_studies_names = [ study . __name__ for study in enrich_backend . studies ] logger . debug ( "All studies in %s: %s" , self . backend_section , all_studies_names ) logger . debug ( "Configured studies %s" , studies ) cfg_studies_types = [ study . split ( ":" ) [ 0 ] for study in studies ] if not set ( cfg_studies_types ) . issubset ( set ( all_studies_names ) ) : logger . error ( 'Wrong studies names for %s: %s' , self . backend_section , studies ) raise RuntimeError ( 'Wrong studies names ' , self . backend_section , studies ) for study in enrich_backend . studies : if study . __name__ in cfg_studies_types : active_studies . append ( study ) enrich_backend . studies = active_studies print ( "Executing for %s the studies %s" % ( self . backend_section , [ study for study in studies ] ) ) studies_args = self . __load_studies ( ) do_studies ( ocean_backend , enrich_backend , studies_args , retention_time = retention_time ) enrich_backend . studies = all_studies
6250	def create_projection ( self , fov : float = 75.0 , near : float = 1.0 , far : float = 100.0 , aspect_ratio : float = None ) : return matrix44 . create_perspective_projection_matrix ( fov , aspect_ratio or self . window . aspect_ratio , near , far , dtype = 'f4' , )
4758	def main ( args ) : trun = cij . runner . trun_from_file ( args . trun_fpath ) rehome ( trun [ "conf" ] [ "OUTPUT" ] , args . output , trun ) postprocess ( trun ) cij . emph ( "main: reports are uses tmpl_fpath: %r" % args . tmpl_fpath ) cij . emph ( "main: reports are here args.output: %r" % args . output ) html_fpath = os . sep . join ( [ args . output , "%s.html" % args . tmpl_name ] ) cij . emph ( "html_fpath: %r" % html_fpath ) try : with open ( html_fpath , 'w' ) as html_file : html_file . write ( dset_to_html ( trun , args . tmpl_fpath ) ) except ( IOError , OSError , ValueError ) as exc : import traceback traceback . print_exc ( ) cij . err ( "rprtr:main: exc: %s" % exc ) return 1 return 0
4676	def getOwnerKeyForAccount ( self , name ) : account = self . rpc . get_account ( name ) for authority in account [ "owner" ] [ "key_auths" ] : key = self . getPrivateKeyForPublicKey ( authority [ 0 ] ) if key : return key raise KeyNotFound
11562	def set_analog_latch ( self , pin , threshold_type , threshold_value , cb = None ) : if self . ANALOG_LATCH_GT <= threshold_type <= self . ANALOG_LATCH_LTE : if 0 <= threshold_value <= 1023 : self . _command_handler . set_analog_latch ( pin , threshold_type , threshold_value , cb ) return True else : return False
225	async def send ( self , message : Message ) -> None : if self . application_state == WebSocketState . CONNECTING : message_type = message [ "type" ] assert message_type in { "websocket.accept" , "websocket.close" } if message_type == "websocket.close" : self . application_state = WebSocketState . DISCONNECTED else : self . application_state = WebSocketState . CONNECTED await self . _send ( message ) elif self . application_state == WebSocketState . CONNECTED : message_type = message [ "type" ] assert message_type in { "websocket.send" , "websocket.close" } if message_type == "websocket.close" : self . application_state = WebSocketState . DISCONNECTED await self . _send ( message ) else : raise RuntimeError ( 'Cannot call "send" once a close message has been sent.' )
4968	def _validate_program ( self ) : program = self . cleaned_data . get ( self . Fields . PROGRAM ) if not program : return course_runs = get_course_runs_from_program ( program ) try : client = CourseCatalogApiClient ( self . _user , self . _enterprise_customer . site ) available_modes = client . get_common_course_modes ( course_runs ) course_mode = self . cleaned_data . get ( self . Fields . COURSE_MODE ) except ( HttpClientError , HttpServerError ) : raise ValidationError ( ValidationMessages . FAILED_TO_OBTAIN_COURSE_MODES . format ( program_title = program . get ( "title" ) ) ) if not course_mode : raise ValidationError ( ValidationMessages . COURSE_WITHOUT_COURSE_MODE ) if course_mode not in available_modes : raise ValidationError ( ValidationMessages . COURSE_MODE_NOT_AVAILABLE . format ( mode = course_mode , program_title = program . get ( "title" ) , modes = ", " . join ( available_modes ) ) )
11086	def whoami ( self , msg , args ) : output = [ "Hello %s" % msg . user ] if hasattr ( self . _bot . dispatcher , 'auth_manager' ) and msg . user . is_admin is True : output . append ( "You are a *bot admin*." ) output . append ( "Bot version: %s-%s" % ( self . _bot . version , self . _bot . commit ) ) return '\n' . join ( output )
3953	def get_last_result ( self ) : result = self . _device . readList ( ADS1x15_POINTER_CONVERSION , 2 ) return self . _conversion_value ( result [ 1 ] , result [ 0 ] )
1755	def read_register ( self , register ) : self . _publish ( 'will_read_register' , register ) value = self . _regfile . read ( register ) self . _publish ( 'did_read_register' , register , value ) return value
9437	def strip_ethernet ( packet ) : if not isinstance ( packet , Ethernet ) : packet = Ethernet ( packet ) payload = packet . payload return payload
5753	def get_page_url ( page_num , current_app , url_view_name , url_extra_args , url_extra_kwargs , url_param_name , url_get_params , url_anchor ) : if url_view_name is not None : url_extra_kwargs [ url_param_name ] = page_num try : url = reverse ( url_view_name , args = url_extra_args , kwargs = url_extra_kwargs , current_app = current_app ) except NoReverseMatch as e : if settings . SETTINGS_MODULE : if django . VERSION < ( 1 , 9 , 0 ) : separator = '.' else : separator = ':' project_name = settings . SETTINGS_MODULE . split ( '.' ) [ 0 ] try : url = reverse ( project_name + separator + url_view_name , args = url_extra_args , kwargs = url_extra_kwargs , current_app = current_app ) except NoReverseMatch : raise e else : raise e else : url = '' url_get_params = url_get_params or QueryDict ( url ) url_get_params = url_get_params . copy ( ) url_get_params [ url_param_name ] = str ( page_num ) if len ( url_get_params ) > 0 : if not isinstance ( url_get_params , QueryDict ) : tmp = QueryDict ( mutable = True ) tmp . update ( url_get_params ) url_get_params = tmp url += '?' + url_get_params . urlencode ( ) if ( url_anchor is not None ) : url += '#' + url_anchor return url
7803	def display_name ( self ) : if self . subject_name : return u", " . join ( [ u", " . join ( [ u"{0}={1}" . format ( k , v ) for k , v in dn_tuple ] ) for dn_tuple in self . subject_name ] ) for name_type in ( "XmppAddr" , "DNS" , "SRV" ) : names = self . alt_names . get ( name_type ) if names : return names [ 0 ] return u"<unknown>"
2438	def add_review_comment ( self , doc , comment ) : if len ( doc . reviews ) != 0 : if not self . review_comment_set : self . review_comment_set = True if validations . validate_review_comment ( comment ) : doc . reviews [ - 1 ] . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'ReviewComment::Comment' ) else : raise CardinalityError ( 'ReviewComment' ) else : raise OrderError ( 'ReviewComment' )
5367	def compact_interval_string ( value_list ) : if not value_list : return '' value_list . sort ( ) interval_list = [ ] curr = [ ] for val in value_list : if curr and ( val > curr [ - 1 ] + 1 ) : interval_list . append ( ( curr [ 0 ] , curr [ - 1 ] ) ) curr = [ val ] else : curr . append ( val ) if curr : interval_list . append ( ( curr [ 0 ] , curr [ - 1 ] ) ) return ',' . join ( [ '{}-{}' . format ( pair [ 0 ] , pair [ 1 ] ) if pair [ 0 ] != pair [ 1 ] else str ( pair [ 0 ] ) for pair in interval_list ] )
2168	def list_misc_commands ( self ) : answer = set ( [ ] ) for cmd_name in misc . __all__ : answer . add ( cmd_name ) return sorted ( answer )
1047	def format_list ( extracted_list ) : list = [ ] for filename , lineno , name , line in extracted_list : item = ' File "%s", line %d, in %s\n' % ( filename , lineno , name ) if line : item = item + ' %s\n' % line . strip ( ) list . append ( item ) return list
6302	def add_package ( self , name ) : name , cls_name = parse_package_string ( name ) if name in self . package_map : return package = EffectPackage ( name ) package . load ( ) self . packages . append ( package ) self . package_map [ package . name ] = package self . polulate ( package . effect_packages )
11600	def get_queryset ( self , request ) : qs = super ( GalleryAdmin , self ) . get_queryset ( request ) return qs . annotate ( photo_count = Count ( 'photos' ) )
11180	def exchange_token ( self , code ) : access_token_url = OAUTH_ROOT + '/access_token' params = { 'client_id' : self . client_id , 'client_secret' : self . client_secret , 'redirect_uri' : self . redirect_uri , 'code' : code , } resp = requests . get ( access_token_url , params = params ) if not resp . ok : raise MixcloudOauthError ( "Could not get access token." ) return resp . json ( ) [ 'access_token' ]
8525	def log_callback ( wrapped_function ) : def debug_log ( message ) : logger . debug ( message . encode ( 'unicode_escape' ) . decode ( ) ) @ functools . wraps ( wrapped_function ) def _wrapper ( parser , match , ** kwargs ) : func_name = wrapped_function . __name__ debug_log ( u'{func_name} <- {matched_string}' . format ( func_name = func_name , matched_string = match . group ( ) , ) ) try : result = wrapped_function ( parser , match , ** kwargs ) except IgnoredMatchException : debug_log ( u'{func_name} -> IGNORED' . format ( func_name = func_name ) ) raise debug_log ( u'{func_name} -> {result}' . format ( func_name = func_name , result = result , ) ) return result return _wrapper
10576	def _create_element_list_ ( self ) : element_set = stoich . elements ( self . compounds ) return sorted ( list ( element_set ) )
5980	def bin_up_mask_2d ( mask_2d , bin_up_factor ) : padded_array_2d = array_util . pad_2d_array_for_binning_up_with_bin_up_factor ( array_2d = mask_2d , bin_up_factor = bin_up_factor , pad_value = True ) binned_array_2d = np . zeros ( shape = ( padded_array_2d . shape [ 0 ] // bin_up_factor , padded_array_2d . shape [ 1 ] // bin_up_factor ) ) for y in range ( binned_array_2d . shape [ 0 ] ) : for x in range ( binned_array_2d . shape [ 1 ] ) : value = True for y1 in range ( bin_up_factor ) : for x1 in range ( bin_up_factor ) : padded_y = y * bin_up_factor + y1 padded_x = x * bin_up_factor + x1 if padded_array_2d [ padded_y , padded_x ] == False : value = False binned_array_2d [ y , x ] = value return binned_array_2d
12789	def get ( self , q = None , page = None ) : etag = generate_etag ( current_ext . content_version . encode ( 'utf8' ) ) self . check_etag ( etag , weak = True ) res = jsonify ( current_ext . styles ) res . set_etag ( etag ) return res
5588	def calculate_slope_aspect ( elevation , xres , yres , z = 1.0 , scale = 1.0 ) : z = float ( z ) scale = float ( scale ) height , width = elevation . shape [ 0 ] - 2 , elevation . shape [ 1 ] - 2 window = [ z * elevation [ row : ( row + height ) , col : ( col + width ) ] for ( row , col ) in product ( range ( 3 ) , range ( 3 ) ) ] x = ( ( window [ 0 ] + window [ 3 ] + window [ 3 ] + window [ 6 ] ) - ( window [ 2 ] + window [ 5 ] + window [ 5 ] + window [ 8 ] ) ) / ( 8.0 * xres * scale ) y = ( ( window [ 6 ] + window [ 7 ] + window [ 7 ] + window [ 8 ] ) - ( window [ 0 ] + window [ 1 ] + window [ 1 ] + window [ 2 ] ) ) / ( 8.0 * yres * scale ) slope = math . pi / 2 - np . arctan ( np . sqrt ( x * x + y * y ) ) aspect = np . arctan2 ( x , y ) return slope , aspect
474	def save_vocab ( count = None , name = 'vocab.txt' ) : if count is None : count = [ ] pwd = os . getcwd ( ) vocabulary_size = len ( count ) with open ( os . path . join ( pwd , name ) , "w" ) as f : for i in xrange ( vocabulary_size ) : f . write ( "%s %d\n" % ( tf . compat . as_text ( count [ i ] [ 0 ] ) , count [ i ] [ 1 ] ) ) tl . logging . info ( "%d vocab saved to %s in %s" % ( vocabulary_size , name , pwd ) )
12509	def get_img_info ( image ) : try : img = check_img ( image ) except Exception as exc : raise Exception ( 'Error reading file {0}.' . format ( repr_imgs ( image ) ) ) from exc else : return img . get_header ( ) , img . get_affine ( )
6802	def loadable ( self , src , dst ) : from fabric import state from fabric . task_utils import crawl src_task = crawl ( src , state . commands ) assert src_task , 'Unknown source role: %s' % src dst_task = crawl ( dst , state . commands ) assert dst_task , 'Unknown destination role: %s' % src src_task ( ) env . host_string = env . hosts [ 0 ] src_size_bytes = self . get_size ( ) dst_task ( ) env . host_string = env . hosts [ 0 ] try : dst_size_bytes = self . get_size ( ) except ( ValueError , TypeError ) : dst_size_bytes = 0 free_space_bytes = self . get_free_space ( ) balance_bytes = free_space_bytes + dst_size_bytes - src_size_bytes balance_bytes_scaled , units = pretty_bytes ( balance_bytes ) viable = balance_bytes >= 0 if self . verbose : print ( 'src_db_size:' , pretty_bytes ( src_size_bytes ) ) print ( 'dst_db_size:' , pretty_bytes ( dst_size_bytes ) ) print ( 'dst_free_space:' , pretty_bytes ( free_space_bytes ) ) print if viable : print ( 'Viable! There will be %.02f %s of disk space left.' % ( balance_bytes_scaled , units ) ) else : print ( 'Not viable! We would be %.02f %s short.' % ( balance_bytes_scaled , units ) ) return viable
4440	async def _playat ( self , ctx , index : int ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if index < 1 : return await ctx . send ( 'Invalid specified index.' ) if len ( player . queue ) < index : return await ctx . send ( 'This index exceeds the queue\'s length.' ) await player . play_at ( index - 1 )
10557	def login ( self , oauth_filename = "oauth" , uploader_id = None ) : cls_name = type ( self ) . __name__ oauth_cred = os . path . join ( os . path . dirname ( OAUTH_FILEPATH ) , oauth_filename + '.cred' ) try : if not self . api . login ( oauth_credentials = oauth_cred , uploader_id = uploader_id ) : try : self . api . perform_oauth ( storage_filepath = oauth_cred ) except OSError : logger . exception ( "\nUnable to login with specified oauth code." ) self . api . login ( oauth_credentials = oauth_cred , uploader_id = uploader_id ) except ( OSError , ValueError ) : logger . exception ( "{} authentication failed." . format ( cls_name ) ) return False if not self . is_authenticated : logger . warning ( "{} authentication failed." . format ( cls_name ) ) return False logger . info ( "{} authentication succeeded.\n" . format ( cls_name ) ) return True
12266	def docstring ( docstr ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : return func ( * args , ** kwargs ) wrapper . __doc__ = docstr return wrapper return decorator
8041	def is_public ( self ) : return ( not self . name . startswith ( "_" ) and self . parent . is_class and self . parent . is_public )
3688	def solve_T ( self , P , V , quick = True ) : r Tc , a , b , kappa0 , kappa1 = self . Tc , self . a , self . b , self . kappa0 , self . kappa1 if quick : x0 = V - b R_x0 = R / x0 x3 = ( 100. * ( V * ( V + b ) + b * x0 ) ) x4 = 10. * kappa0 kappa110 = kappa1 * 10. kappa17 = kappa1 * 7. def to_solve ( T ) : x1 = T / Tc x2 = x1 ** 0.5 return ( T * R_x0 - a * ( ( x4 - ( kappa110 * x1 - kappa17 ) * ( x2 + 1. ) ) * ( x2 - 1. ) - 10. ) ** 2 / x3 ) - P else : def to_solve ( T ) : P_calc = R * T / ( V - b ) - a * ( ( kappa0 + kappa1 * ( sqrt ( T / Tc ) + 1 ) * ( - T / Tc + 7 / 10 ) ) * ( - sqrt ( T / Tc ) + 1 ) + 1 ) ** 2 / ( V * ( V + b ) + b * ( V - b ) ) return P_calc - P return newton ( to_solve , Tc * 0.5 )
10135	def dump ( grids , mode = MODE_ZINC ) : if isinstance ( grids , Grid ) : return dump_grid ( grids , mode = mode ) _dump = functools . partial ( dump_grid , mode = mode ) if mode == MODE_ZINC : return '\n' . join ( map ( _dump , grids ) ) elif mode == MODE_JSON : return '[%s]' % ',' . join ( map ( _dump , grids ) ) else : raise NotImplementedError ( 'Format not implemented: %s' % mode )
6579	def _send_cmd ( self , cmd ) : self . _process . stdin . write ( "{}\n" . format ( cmd ) . encode ( "utf-8" ) ) self . _process . stdin . flush ( )
5611	def memory_file ( data = None , profile = None ) : memfile = MemoryFile ( ) profile . update ( width = data . shape [ - 2 ] , height = data . shape [ - 1 ] ) with memfile . open ( ** profile ) as dataset : dataset . write ( data ) return memfile
4484	def copyfileobj ( fsrc , fdst , total , length = 16 * 1024 ) : with tqdm ( unit = 'bytes' , total = total , unit_scale = True ) as pbar : while 1 : buf = fsrc . read ( length ) if not buf : break fdst . write ( buf ) pbar . update ( len ( buf ) )
3587	def remove ( self , cbobject ) : with self . _lock : if cbobject in self . _metadata : del self . _metadata [ cbobject ]
11585	def _getnodenamefor ( self , name ) : "Return the node name where the ``name`` would land to" return 'node_' + str ( ( abs ( binascii . crc32 ( b ( name ) ) & 0xffffffff ) % self . no_servers ) + 1 )
9127	def create_all ( engine , checkfirst = True ) : Base . metadata . create_all ( bind = engine , checkfirst = checkfirst )
7048	def bls_stats_singleperiod ( times , mags , errs , period , magsarefluxes = False , sigclip = 10.0 , perioddeltapercent = 10 , nphasebins = 200 , mintransitduration = 0.01 , maxtransitduration = 0.4 , ingressdurationfraction = 0.1 , verbose = True ) : stimes , smags , serrs = sigclip_magseries ( times , mags , errs , magsarefluxes = magsarefluxes , sigclip = sigclip ) if len ( stimes ) > 9 and len ( smags ) > 9 and len ( serrs ) > 9 : startp = period - perioddeltapercent * period / 100.0 if startp < 0 : startp = period endp = period + perioddeltapercent * period / 100.0 blsres = bls_serial_pfind ( stimes , smags , serrs , verbose = verbose , startp = startp , endp = endp , nphasebins = nphasebins , mintransitduration = mintransitduration , maxtransitduration = maxtransitduration , magsarefluxes = magsarefluxes , get_stats = False , sigclip = None ) if ( not blsres or 'blsresult' not in blsres or blsres [ 'blsresult' ] is None ) : LOGERROR ( "BLS failed during a period-search " "performed around the input best period: %.6f. " "Can't continue. " % period ) return None thistransdepth = blsres [ 'blsresult' ] [ 'transdepth' ] thistransduration = blsres [ 'blsresult' ] [ 'transduration' ] thisbestperiod = blsres [ 'bestperiod' ] thistransingressbin = blsres [ 'blsresult' ] [ 'transingressbin' ] thistransegressbin = blsres [ 'blsresult' ] [ 'transegressbin' ] thisnphasebins = nphasebins stats = _get_bls_stats ( stimes , smags , serrs , thistransdepth , thistransduration , ingressdurationfraction , nphasebins , thistransingressbin , thistransegressbin , thisbestperiod , thisnphasebins , magsarefluxes = magsarefluxes , verbose = verbose ) return stats else : LOGERROR ( 'no good detections for these times and mags, skipping...' ) return None
7764	def connect ( self ) : with self . lock : if self . stream : logger . debug ( "Closing the previously used stream." ) self . _close_stream ( ) transport = TCPTransport ( self . settings ) addr = self . settings [ "server" ] if addr : service = None else : addr = self . jid . domain service = self . settings [ "c2s_service" ] transport . connect ( addr , self . settings [ "c2s_port" ] , service ) handlers = self . _base_handlers [ : ] handlers += self . handlers + [ self ] self . clear_response_handlers ( ) self . setup_stanza_handlers ( handlers , "pre-auth" ) stream = ClientStream ( self . jid , self , handlers , self . settings ) stream . initiate ( transport ) self . main_loop . add_handler ( transport ) self . main_loop . add_handler ( stream ) self . _ml_handlers += [ transport , stream ] self . stream = stream self . uplink = stream
13878	def DeleteFile ( target_filename ) : _AssertIsLocal ( target_filename ) try : if IsLink ( target_filename ) : DeleteLink ( target_filename ) elif IsFile ( target_filename ) : os . remove ( target_filename ) elif IsDir ( target_filename ) : from . _exceptions import FileOnlyActionError raise FileOnlyActionError ( target_filename ) except Exception as e : reraise ( e , 'While executing filesystem.DeleteFile(%s)' % ( target_filename ) )
5119	def initialize ( self , nActive = 1 , queues = None , edges = None , edge_type = None ) : if queues is None and edges is None and edge_type is None : if nActive >= 1 and isinstance ( nActive , numbers . Integral ) : qs = [ q . edge [ 2 ] for q in self . edge2queue if q . edge [ 3 ] != 0 ] n = min ( nActive , len ( qs ) ) queues = np . random . choice ( qs , size = n , replace = False ) elif not isinstance ( nActive , numbers . Integral ) : msg = "If queues is None, then nActive must be an integer." raise TypeError ( msg ) else : msg = ( "If queues is None, then nActive must be a " "positive int." ) raise ValueError ( msg ) else : queues = _get_queues ( self . g , queues , edges , edge_type ) queues = [ e for e in queues if self . edge2queue [ e ] . edge [ 3 ] != 0 ] if len ( queues ) == 0 : raise QueueingToolError ( "There were no queues to initialize." ) if len ( queues ) > self . max_agents : queues = queues [ : self . max_agents ] for ei in queues : self . edge2queue [ ei ] . set_active ( ) self . num_agents [ ei ] = self . edge2queue [ ei ] . _num_total keys = [ q . _key ( ) for q in self . edge2queue if q . _time < np . infty ] self . _fancy_heap = PriorityQueue ( keys , self . nE ) self . _initialized = True
7655	def update ( self , ** kwargs ) : for name , value in six . iteritems ( kwargs ) : setattr ( self , name , value )
1588	def get_topology_config ( self ) : if self . pplan . topology . HasField ( "topology_config" ) : return self . _get_dict_from_config ( self . pplan . topology . topology_config ) else : return { }
9762	def unbookmark ( ctx ) : user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) try : PolyaxonClient ( ) . experiment . unbookmark ( user , project_name , _experiment ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not unbookmark experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiment is unbookmarked." )
4282	def video_size ( source , converter = 'ffmpeg' ) : res = subprocess . run ( [ converter , '-i' , source ] , stderr = subprocess . PIPE ) stderr = res . stderr . decode ( 'utf8' ) pattern = re . compile ( r'Stream.*Video.* ([0-9]+)x([0-9]+)' ) match = pattern . search ( stderr ) rot_pattern = re . compile ( r'rotate\s*:\s*-?(90|270)' ) rot_match = rot_pattern . search ( stderr ) if match : x , y = int ( match . groups ( ) [ 0 ] ) , int ( match . groups ( ) [ 1 ] ) else : x = y = 0 if rot_match : x , y = y , x return x , y
9404	def _get_function_ptr ( self , name ) : func = _make_function_ptr_instance self . _function_ptrs . setdefault ( name , func ( self , name ) ) return self . _function_ptrs [ name ]
7991	def _send_stream_start ( self , stream_id = None , stream_to = None ) : if self . _output_state in ( "open" , "closed" ) : raise StreamError ( "Stream start already sent" ) if not self . language : self . language = self . settings [ "language" ] if stream_to : stream_to = unicode ( stream_to ) elif self . peer and self . initiator : stream_to = unicode ( self . peer ) stream_from = None if self . me and ( self . tls_established or not self . initiator ) : stream_from = unicode ( self . me ) if stream_id : self . stream_id = stream_id else : self . stream_id = None self . transport . send_stream_head ( self . stanza_namespace , stream_from , stream_to , self . stream_id , language = self . language ) self . _output_state = "open"
6173	def single_instance ( func = None , lock_timeout = None , include_args = False ) : if func is None : return partial ( single_instance , lock_timeout = lock_timeout , include_args = include_args ) @ wraps ( func ) def wrapped ( celery_self , * args , ** kwargs ) : timeout = ( lock_timeout or celery_self . soft_time_limit or celery_self . time_limit or celery_self . app . conf . get ( 'CELERYD_TASK_SOFT_TIME_LIMIT' ) or celery_self . app . conf . get ( 'CELERYD_TASK_TIME_LIMIT' ) or ( 60 * 5 ) ) manager_class = _select_manager ( celery_self . backend . __class__ . __name__ ) lock_manager = manager_class ( celery_self , timeout , include_args , args , kwargs ) with lock_manager : ret_value = func ( * args , ** kwargs ) return ret_value return wrapped
11844	def run ( self , steps = 1000 ) : "Run the Environment for given number of time steps." for step in range ( steps ) : if self . is_done ( ) : return self . step ( )
10748	def validate_bands ( self , bands ) : if not isinstance ( bands , list ) : logger . error ( 'Parameter bands must be a "list"' ) raise TypeError ( 'Parameter bands must be a "list"' ) valid_bands = list ( range ( 1 , 12 ) ) + [ 'BQA' ] for band in bands : if band not in valid_bands : logger . error ( '%s is not a valid band' % band ) raise InvalidBandError ( '%s is not a valid band' % band )
3368	def _valid_atoms ( model , expression ) : atoms = expression . atoms ( optlang . interface . Variable ) return all ( a . problem is model . solver for a in atoms )
3181	def delete ( self , batch_webhook_id ) : self . batch_webhook_id = batch_webhook_id return self . _mc_client . _delete ( url = self . _build_path ( batch_webhook_id ) )
12012	def do_photometry ( self ) : std_f = np . zeros ( 4 ) data_save = np . zeros_like ( self . postcard ) self . obs_flux = np . zeros_like ( self . reference_flux ) for i in range ( 4 ) : g = np . where ( self . qs == i ) [ 0 ] wh = np . where ( self . times [ g ] > 54947 ) data_save [ g ] = np . roll ( self . postcard [ g ] , int ( self . roll_best [ i , 0 ] ) , axis = 1 ) data_save [ g ] = np . roll ( data_save [ g ] , int ( self . roll_best [ i , 1 ] ) , axis = 2 ) self . target_flux_pixels = data_save [ : , self . targets == 1 ] self . target_flux = np . sum ( self . target_flux_pixels , axis = 1 ) self . obs_flux [ g ] = self . target_flux [ g ] / self . reference_flux [ g ] self . obs_flux [ g ] /= np . median ( self . obs_flux [ g [ wh ] ] ) fitline = np . polyfit ( self . times [ g ] [ wh ] , self . obs_flux [ g ] [ wh ] , 1 ) std_f [ i ] = np . max ( [ np . std ( self . obs_flux [ g ] [ wh ] / ( fitline [ 0 ] * self . times [ g ] [ wh ] + fitline [ 1 ] ) ) , 0.001 ] ) self . flux_uncert = std_f
10974	def new ( ) : form = GroupForm ( request . form ) if form . validate_on_submit ( ) : try : group = Group . create ( admins = [ current_user ] , ** form . data ) flash ( _ ( 'Group "%(name)s" created' , name = group . name ) , 'success' ) return redirect ( url_for ( ".index" ) ) except IntegrityError : flash ( _ ( 'Group creation failure' ) , 'error' ) return render_template ( "invenio_groups/new.html" , form = form , )
5128	def transitions ( self , return_matrix = True ) : if return_matrix : mat = np . zeros ( ( self . nV , self . nV ) ) for v in self . g . nodes ( ) : ind = [ e [ 1 ] for e in sorted ( self . g . out_edges ( v ) ) ] mat [ v , ind ] = self . _route_probs [ v ] else : mat = { k : { e [ 1 ] : p for e , p in zip ( sorted ( self . g . out_edges ( k ) ) , value ) } for k , value in enumerate ( self . _route_probs ) } return mat
10487	def _findAll ( self , ** kwargs ) : result = [ ] for item in self . _generateFind ( ** kwargs ) : result . append ( item ) return result
2335	def clr ( M , ** kwargs ) : R = np . zeros ( M . shape ) Id = [ [ 0 , 0 ] for i in range ( M . shape [ 0 ] ) ] for i in range ( M . shape [ 0 ] ) : mu_i = np . mean ( M [ i , : ] ) sigma_i = np . std ( M [ i , : ] ) Id [ i ] = [ mu_i , sigma_i ] for i in range ( M . shape [ 0 ] ) : for j in range ( i + 1 , M . shape [ 0 ] ) : z_i = np . max ( [ 0 , ( M [ i , j ] - Id [ i ] [ 0 ] ) / Id [ i ] [ 0 ] ] ) z_j = np . max ( [ 0 , ( M [ i , j ] - Id [ j ] [ 0 ] ) / Id [ j ] [ 0 ] ] ) R [ i , j ] = np . sqrt ( z_i ** 2 + z_j ** 2 ) R [ j , i ] = R [ i , j ] return R
6291	def add_texture_dir ( self , directory ) : dirs = list ( self . TEXTURE_DIRS ) dirs . append ( directory ) self . TEXTURE_DIRS = dirs
10757	def writable_path ( path ) : if os . path . exists ( path ) : return os . access ( path , os . W_OK ) try : with open ( path , 'w' ) : pass except ( OSError , IOError ) : return False else : os . remove ( path ) return True
8788	def get ( self , model ) : for tag in model . tags : if self . is_tag ( tag ) : value = self . deserialize ( tag ) try : self . validate ( value ) return value except TagValidationError : continue return None
6623	def _getTarball ( url , into_directory , cache_key , origin_info = None ) : try : access_common . unpackFromCache ( cache_key , into_directory ) except KeyError as e : tok = settings . getProperty ( 'github' , 'authtoken' ) headers = { } if tok is not None : headers [ 'Authorization' ] = 'token ' + str ( tok ) logger . debug ( 'GET %s' , url ) response = requests . get ( url , allow_redirects = True , stream = True , headers = headers ) response . raise_for_status ( ) logger . debug ( 'getting file: %s' , url ) logger . debug ( 'headers: %s' , response . headers ) response . raise_for_status ( ) access_common . unpackTarballStream ( stream = response , into_directory = into_directory , hash = { } , cache_key = cache_key , origin_info = origin_info )
5872	def serialize_organization ( organization ) : return { 'id' : organization . id , 'name' : organization . name , 'short_name' : organization . short_name , 'description' : organization . description , 'logo' : organization . logo }
5075	def is_course_run_enrollable ( course_run ) : now = datetime . datetime . now ( pytz . UTC ) end = parse_datetime_handle_invalid ( course_run . get ( 'end' ) ) enrollment_start = parse_datetime_handle_invalid ( course_run . get ( 'enrollment_start' ) ) enrollment_end = parse_datetime_handle_invalid ( course_run . get ( 'enrollment_end' ) ) return ( not end or end > now ) and ( not enrollment_start or enrollment_start < now ) and ( not enrollment_end or enrollment_end > now )
11234	def translate_array ( self , string , language , level = 3 , retdata = False ) : language = language . lower ( ) assert self . is_built_in ( language ) or language in self . outer_templates , "Sorry, " + language + " is not a supported language." data = phpserialize . loads ( bytes ( string , 'utf-8' ) , array_hook = list , decode_strings = True ) if self . is_built_in ( language ) : self . get_built_in ( language , level , data ) print ( self ) return self . data_structure if retdata else None def loop_print ( iterable , level = 3 ) : retval = '' indentation = ' ' * level if not self . is_iterable ( iterable ) or isinstance ( iterable , str ) : non_iterable = str ( iterable ) return str ( non_iterable ) for item in iterable : if isinstance ( item , tuple ) and len ( item ) == 2 : key = item [ 0 ] val = loop_print ( item [ 1 ] , level = level + 3 ) val = self . translate_val ( language , val ) if language in self . lang_specific_values and val in self . lang_specific_values [ language ] else val key = str ( key ) if isinstance ( key , int ) else '\'' + str ( key ) + '\'' needs_unpacking = hasattr ( item [ 0 ] , '__iter__' ) == False and hasattr ( item [ 1 ] , '__iter__' ) == True if needs_unpacking : retval += self . get_inner_template ( language , 'iterable' , indentation , key , val ) else : val = str ( val ) if val . isdigit ( ) or val in self . lang_specific_values [ language ] . values ( ) else '\'' + str ( val ) + '\'' retval += self . get_inner_template ( language , 'singular' , indentation , key , val ) return retval self . data_structure = self . outer_templates [ language ] % ( loop_print ( data ) ) print ( self ) return self . data_structure if retdata else None
1110	def _dump ( self , tag , x , lo , hi ) : for i in xrange ( lo , hi ) : yield '%s %s' % ( tag , x [ i ] )
8108	def search ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_SEARCH return GoogleSearch ( q , start , service , "" , wait , asynchronous , cached )
12259	def simplex ( x , rho ) : u = np . flipud ( np . sort ( x . ravel ( ) ) ) lambdas = ( 1 - np . cumsum ( u ) ) / ( 1. + np . arange ( u . size ) ) ix = np . where ( u + lambdas > 0 ) [ 0 ] . max ( ) return np . maximum ( x + lambdas [ ix ] , 0 )
8338	def findParents ( self , name = None , attrs = { } , limit = None , ** kwargs ) : return self . _findAll ( name , attrs , None , limit , self . parentGenerator , ** kwargs )
1932	def get_description ( self , name : str ) -> str : if name not in self . _vars : raise ConfigError ( f"{self.name}.{name} not defined." ) return self . _vars [ name ] . description
2567	def check_tracking_enabled ( self ) : track = True test = False testvar = str ( os . environ . get ( "PARSL_TESTING" , 'None' ) ) . lower ( ) if testvar == 'true' : test = True if not self . config . usage_tracking : track = False envvar = str ( os . environ . get ( "PARSL_TRACKING" , True ) ) . lower ( ) if envvar == "false" : track = False return test , track
10255	def get_causal_source_nodes ( graph : BELGraph , func : str ) -> Set [ BaseEntity ] : return { node for node in graph if node . function == func and is_causal_source ( graph , node ) }
72	def deepcopy ( self ) : bbs = [ bb . deepcopy ( ) for bb in self . bounding_boxes ] return BoundingBoxesOnImage ( bbs , tuple ( self . shape ) )
8416	def precision ( x ) : from . bounds import zero_range rng = min_max ( x , na_rm = True ) if zero_range ( rng ) : span = np . abs ( rng [ 0 ] ) else : span = np . diff ( rng ) [ 0 ] if span == 0 : return 1 else : return 10 ** int ( np . floor ( np . log10 ( span ) ) )
7414	def plot_pairwise_dist ( self , labels = None , ax = None , cmap = None , cdict = None , metric = "euclidean" ) : allele_counts = self . genotypes . to_n_alt ( ) dist = allel . pairwise_distance ( allele_counts , metric = metric ) if not ax : fig = plt . figure ( figsize = ( 5 , 5 ) ) ax = fig . add_subplot ( 1 , 1 , 1 ) if isinstance ( labels , bool ) : if labels : labels = list ( self . samples_vcforder ) elif isinstance ( labels , type ( None ) ) : pass else : if not len ( labels ) == len ( self . samples_vcforder ) : raise IPyradError ( LABELS_LENGTH_ERROR . format ( len ( labels ) , len ( self . samples_vcforder ) ) ) allel . plot . pairwise_distance ( dist , labels = labels , ax = ax , colorbar = False )
7458	def _countmatrix ( lxs ) : share = np . zeros ( ( lxs . shape [ 0 ] , lxs . shape [ 0 ] ) ) names = range ( lxs . shape [ 0 ] ) for row in lxs : for samp1 , samp2 in itertools . combinations ( names , 2 ) : shared = lxs [ samp1 , lxs [ samp2 ] > 0 ] . sum ( ) share [ samp1 , samp2 ] = shared for row in xrange ( len ( names ) ) : share [ row , row ] = lxs [ row ] . sum ( ) return share
3912	def show_message ( self , message_str ) : if self . _message_handle is not None : self . _message_handle . cancel ( ) self . _message_handle = asyncio . get_event_loop ( ) . call_later ( self . _MESSAGE_DELAY_SECS , self . _clear_message ) self . _message = message_str self . _update ( )
6481	def mem_size ( self ) : data_len = self . _data_mem_size node_count = len ( list ( self . xml_doc . iter ( tag = etree . Element ) ) ) if self . compressed : size = 52 * node_count + data_len + 630 else : tags_len = 0 for e in self . xml_doc . iter ( tag = etree . Element ) : e_len = max ( len ( e . tag ) , 8 ) e_len = ( e_len + 3 ) & ~ 3 tags_len += e_len size = 56 * node_count + data_len + 630 + tags_len return ( size + 8 ) & ~ 7
3933	def _make_token_request ( session , token_request_data ) : try : r = session . post ( OAUTH2_TOKEN_REQUEST_URL , data = token_request_data ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'Token request failed: {}' . format ( e ) ) else : res = r . json ( ) if 'error' in res : raise GoogleAuthError ( 'Token request error: {!r}' . format ( res [ 'error' ] ) ) return res
13306	def correlation ( a , b ) : diff1 = a - a . mean ( ) diff2 = b - b . mean ( ) return ( diff1 * diff2 ) . mean ( ) / ( np . sqrt ( np . square ( diff1 ) . mean ( ) * np . square ( diff2 ) . mean ( ) ) )
2211	def inject_method ( self , func , name = None ) : new_method = func . __get__ ( self , self . __class__ ) if name is None : name = func . __name__ setattr ( self , name , new_method )
6845	def check_ok ( self ) : import requests if not self . env . check_ok : return branch_name = self . _local ( 'git rev-parse --abbrev-ref HEAD' , capture = True ) . strip ( ) check_ok_paths = self . env . check_ok_paths or { } if branch_name in check_ok_paths : check = check_ok_paths [ branch_name ] if 'username' in check : auth = ( check [ 'username' ] , check [ 'password' ] ) else : auth = None ret = requests . get ( check [ 'url' ] , auth = auth ) passed = check [ 'text' ] in ret . content assert passed , 'Check failed: %s' % check [ 'url' ]
9725	async def set_qtm_event ( self , event = None ) : cmd = "event%s" % ( "" if event is None else " " + event ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
7396	def get_publication_list ( context , list , template = 'publications/publications.html' ) : list = List . objects . filter ( list__iexact = list ) if not list : return '' list = list [ 0 ] publications = list . publication_set . all ( ) publications = publications . order_by ( '-year' , '-month' , '-id' ) if not publications : return '' populate ( publications ) return render_template ( template , context [ 'request' ] , { 'list' : list , 'publications' : publications } )
13197	def open511_convert ( input_doc , output_format , serialize = True , ** kwargs ) : try : output_format_info = FORMATS [ output_format ] except KeyError : raise ValueError ( "Unrecognized output format %s" % output_format ) input_doc = ensure_format ( input_doc , output_format_info . input_format ) result = output_format_info . func ( input_doc , ** kwargs ) if serialize : result = output_format_info . serializer ( result ) return result
7461	def save_json ( data ) : datadict = OrderedDict ( [ ( "_version" , data . __dict__ [ "_version" ] ) , ( "_checkpoint" , data . __dict__ [ "_checkpoint" ] ) , ( "name" , data . __dict__ [ "name" ] ) , ( "dirs" , data . __dict__ [ "dirs" ] ) , ( "paramsdict" , data . __dict__ [ "paramsdict" ] ) , ( "samples" , data . __dict__ [ "samples" ] . keys ( ) ) , ( "populations" , data . __dict__ [ "populations" ] ) , ( "database" , data . __dict__ [ "database" ] ) , ( "clust_database" , data . __dict__ [ "clust_database" ] ) , ( "outfiles" , data . __dict__ [ "outfiles" ] ) , ( "barcodes" , data . __dict__ [ "barcodes" ] ) , ( "stats_files" , data . __dict__ [ "stats_files" ] ) , ( "_hackersonly" , data . __dict__ [ "_hackersonly" ] ) , ] ) sampledict = OrderedDict ( [ ] ) for key , sample in data . samples . iteritems ( ) : sampledict [ key ] = sample . _to_fulldict ( ) fulldumps = json . dumps ( { "assembly" : datadict , "samples" : sampledict } , cls = Encoder , sort_keys = False , indent = 4 , separators = ( "," , ":" ) , ) assemblypath = os . path . join ( data . dirs . project , data . name + ".json" ) if not os . path . exists ( data . dirs . project ) : os . mkdir ( data . dirs . project ) done = 0 while not done : try : with open ( assemblypath , 'w' ) as jout : jout . write ( fulldumps ) done = 1 except ( KeyboardInterrupt , SystemExit ) : print ( '.' ) continue
7883	def _make_prefixed ( self , name , is_element , declared_prefixes , declarations ) : namespace , name = self . _split_qname ( name , is_element ) if namespace is None : prefix = None elif namespace in declared_prefixes : prefix = declared_prefixes [ namespace ] elif namespace in self . _prefixes : prefix = self . _prefixes [ namespace ] declarations [ namespace ] = prefix declared_prefixes [ namespace ] = prefix else : if is_element : prefix = None else : prefix = self . _make_prefix ( declared_prefixes ) declarations [ namespace ] = prefix declared_prefixes [ namespace ] = prefix if prefix : return prefix + u":" + name else : return name
2217	def _list_itemstrs ( list_ , ** kwargs ) : items = list ( list_ ) kwargs [ '_return_info' ] = True _tups = [ repr2 ( item , ** kwargs ) for item in items ] itemstrs = [ t [ 0 ] for t in _tups ] max_height = max ( [ t [ 1 ] [ 'max_height' ] for t in _tups ] ) if _tups else 0 _leaf_info = { 'max_height' : max_height + 1 , } sort = kwargs . get ( 'sort' , None ) if sort is None : sort = isinstance ( list_ , ( set , frozenset ) ) if sort : itemstrs = _sort_itemstrs ( items , itemstrs ) return itemstrs , _leaf_info
12317	def delete ( self , repo , args = [ ] ) : result = None with cd ( repo . rootdir ) : try : cmd = [ 'rm' ] + list ( args ) result = { 'status' : 'success' , 'message' : self . _run ( cmd ) } except Exception as e : result = { 'status' : 'error' , 'message' : str ( e ) } return result
2131	def get ( self , pk = None , ** kwargs ) : if kwargs . pop ( 'include_debug_header' , True ) : debug . log ( 'Getting the role record.' , header = 'details' ) data , self . endpoint = self . data_endpoint ( kwargs ) response = self . read ( pk = pk , fail_on_no_results = True , fail_on_multiple_results = True , ** data ) item_dict = response [ 'results' ] [ 0 ] self . configure_display ( item_dict ) return item_dict
7393	def mods_genre ( self ) : type2genre = { 'conference' : 'conference publication' , 'book chapter' : 'bibliography' , 'unpublished' : 'article' } tp = str ( self . type ) . lower ( ) return type2genre . get ( tp , tp )
11619	def _setup ( ) : s = str . split if sys . version_info < ( 3 , 0 ) : s = unicode . split def pop_all ( some_dict , some_list ) : for scheme in some_list : some_dict . pop ( scheme ) global SCHEMES SCHEMES = copy . deepcopy ( sanscript . SCHEMES ) pop_all ( SCHEMES , [ sanscript . ORIYA , sanscript . BENGALI , sanscript . GUJARATI ] ) SCHEMES [ HK ] . update ( { 'vowels' : s ( ) + s ( ) , 'marks' : s ( ) + s ( ) , 'consonants' : sanscript . SCHEMES [ HK ] [ 'consonants' ] + s ( ) } ) SCHEMES [ ITRANS ] . update ( { 'vowels' : s ( ) + s ( ) , 'marks' : s ( ) + s ( ) , 'consonants' : sanscript . SCHEMES [ ITRANS ] [ 'consonants' ] + s ( ) } ) pop_all ( SCHEMES [ ITRANS ] . synonym_map , s ( ) ) SCHEMES [ OPTITRANS ] . update ( { 'vowels' : s ( ) + s ( ) , 'marks' : s ( ) + s ( ) , 'consonants' : sanscript . SCHEMES [ OPTITRANS ] [ 'consonants' ] + s ( ) } ) pop_all ( SCHEMES [ OPTITRANS ] . synonym_map , s ( ) )
10489	def _getBundleId ( self ) : ra = AppKit . NSRunningApplication app = ra . runningApplicationWithProcessIdentifier_ ( self . _getPid ( ) ) return app . bundleIdentifier ( )
7202	def deprecate_module_attr ( mod , deprecated ) : deprecated = set ( deprecated ) class Wrapper ( object ) : def __getattr__ ( self , attr ) : if attr in deprecated : warnings . warn ( "Property {} is deprecated" . format ( attr ) , GBDXDeprecation ) return getattr ( mod , attr ) def __setattr__ ( self , attr , value ) : if attr in deprecated : warnings . warn ( "Property {} is deprecated" . format ( attr ) , GBDXDeprecation ) return setattr ( mod , attr , value ) return Wrapper ( )
5778	def _bcrypt_encrypt ( certificate_or_public_key , data , rsa_oaep_padding = False ) : flags = BcryptConst . BCRYPT_PAD_PKCS1 if rsa_oaep_padding is True : flags = BcryptConst . BCRYPT_PAD_OAEP padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_OAEP_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) hash_buffer = buffer_from_unicode ( BcryptConst . BCRYPT_SHA1_ALGORITHM ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info_struct . pbLabel = null ( ) padding_info_struct . cbLabel = 0 padding_info = cast ( bcrypt , 'void *' , padding_info_struct_pointer ) else : padding_info = null ( ) out_len = new ( bcrypt , 'ULONG *' ) res = bcrypt . BCryptEncrypt ( certificate_or_public_key . key_handle , data , len ( data ) , padding_info , null ( ) , 0 , null ( ) , 0 , out_len , flags ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) res = bcrypt . BCryptEncrypt ( certificate_or_public_key . key_handle , data , len ( data ) , padding_info , null ( ) , 0 , buffer , buffer_len , out_len , flags ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) )
10006	def clear_obj ( self , obj ) : removed = self . cellgraph . clear_obj ( obj ) for node in removed : del node [ OBJ ] . data [ node [ KEY ] ]
6290	def add_program_dir ( self , directory ) : dirs = list ( self . PROGRAM_DIRS ) dirs . append ( directory ) self . PROGRAM_DIRS = dirs
9513	def orfs ( self , frame = 0 , revcomp = False ) : assert frame in [ 0 , 1 , 2 ] if revcomp : self . revcomp ( ) aa_seq = self . translate ( frame = frame ) . seq . rstrip ( 'X' ) if revcomp : self . revcomp ( ) orfs = _orfs_from_aa_seq ( aa_seq ) for i in range ( len ( orfs ) ) : if revcomp : start = len ( self ) - ( orfs [ i ] . end * 3 + 3 ) - frame end = len ( self ) - ( orfs [ i ] . start * 3 ) - 1 - frame else : start = orfs [ i ] . start * 3 + frame end = orfs [ i ] . end * 3 + 2 + frame orfs [ i ] = intervals . Interval ( start , end ) return orfs
836	def _removeRows ( self , rowsToRemove ) : removalArray = numpy . array ( rowsToRemove ) self . _categoryList = numpy . delete ( numpy . array ( self . _categoryList ) , removalArray ) . tolist ( ) if self . fixedCapacity : self . _categoryRecencyList = numpy . delete ( numpy . array ( self . _categoryRecencyList ) , removalArray ) . tolist ( ) for row in reversed ( rowsToRemove ) : self . _partitionIdList . pop ( row ) self . _rebuildPartitionIdMap ( self . _partitionIdList ) if self . useSparseMemory : for rowIndex in rowsToRemove [ : : - 1 ] : self . _Memory . deleteRow ( rowIndex ) else : self . _M = numpy . delete ( self . _M , removalArray , 0 ) numRemoved = len ( rowsToRemove ) numRowsExpected = self . _numPatterns - numRemoved if self . useSparseMemory : if self . _Memory is not None : assert self . _Memory . nRows ( ) == numRowsExpected else : assert self . _M . shape [ 0 ] == numRowsExpected assert len ( self . _categoryList ) == numRowsExpected self . _numPatterns -= numRemoved return numRemoved
3129	def get_subscriber_hash ( member_email ) : check_email ( member_email ) member_email = member_email . lower ( ) . encode ( ) m = hashlib . md5 ( member_email ) return m . hexdigest ( )
1382	def register_watch ( self , callback ) : RETRY_COUNT = 5 for _ in range ( RETRY_COUNT ) : uid = uuid . uuid4 ( ) if uid not in self . watches : Log . info ( "Registering a watch with uid: " + str ( uid ) ) try : callback ( self ) except Exception as e : Log . error ( "Caught exception while triggering callback: " + str ( e ) ) Log . debug ( traceback . format_exc ( ) ) return None self . watches [ uid ] = callback return uid return None
7782	def update_state ( self ) : self . _lock . acquire ( ) try : now = datetime . utcnow ( ) if self . state == 'new' : self . state = 'fresh' if self . state == 'fresh' : if now > self . freshness_time : self . state = 'old' if self . state == 'old' : if now > self . expire_time : self . state = 'stale' if self . state == 'stale' : if now > self . purge_time : self . state = 'purged' self . state_value = _state_values [ self . state ] return self . state finally : self . _lock . release ( )
7294	def create_list_dict ( self , document , list_field , doc_key ) : list_dict = { "_document" : document } if isinstance ( list_field . field , EmbeddedDocumentField ) : list_dict . update ( self . create_document_dictionary ( document = list_field . field . document_type_obj , owner_document = document ) ) list_dict . update ( { "_document_field" : list_field . field , "_key" : doc_key , "_field_type" : ListField , "_widget" : get_widget ( list_field . field ) , "_value" : getattr ( document , doc_key , None ) } ) return list_dict
1977	def sys_deallocate ( self , cpu , addr , size ) : logger . info ( "DEALLOCATE(0x%08x, %d)" % ( addr , size ) ) if addr & 0xfff != 0 : logger . info ( "DEALLOCATE: addr is not page aligned" ) return Decree . CGC_EINVAL if size == 0 : logger . info ( "DEALLOCATE:length is zero" ) return Decree . CGC_EINVAL cpu . memory . munmap ( addr , size ) self . syscall_trace . append ( ( "_deallocate" , - 1 , size ) ) return 0
8993	def folder ( self , folder ) : result = [ ] for root , _ , files in os . walk ( folder ) : for file in files : path = os . path . join ( root , file ) if self . _chooses_path ( path ) : result . append ( self . path ( path ) ) return result
6063	def mass_within_ellipse_in_units ( self , major_axis , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : self . check_units_of_radius_and_critical_surface_density ( radius = major_axis , critical_surface_density = critical_surface_density ) profile = self . new_profile_with_units_converted ( unit_length = major_axis . unit_length , unit_mass = 'angular' , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) mass_angular = dim . Mass ( value = quad ( profile . mass_integral , a = 0.0 , b = major_axis , args = ( self . axis_ratio , ) ) [ 0 ] , unit_mass = 'angular' ) return mass_angular . convert ( unit_mass = unit_mass , critical_surface_density = critical_surface_density )
6077	def hyper_noise_from_contributions ( self , noise_map , contributions ) : return self . noise_factor * ( noise_map * contributions ) ** self . noise_power
425	def delete_tasks ( self , ** kwargs ) : self . _fill_project_info ( kwargs ) self . db . Task . delete_many ( kwargs ) logging . info ( "[Database] Delete Task SUCCESS" )
1318	def SetWindowText ( self , text : str ) -> bool : handle = self . NativeWindowHandle if handle : return SetWindowText ( handle , text ) return False
6435	def sim_eudex ( src , tar , weights = 'exponential' , max_length = 8 ) : return Eudex ( ) . sim ( src , tar , weights , max_length )
3206	def get ( self , batch_id , ** queryparams ) : self . batch_id = batch_id self . operation_status = None return self . _mc_client . _get ( url = self . _build_path ( batch_id ) , ** queryparams )
5321	def get_data ( self , reset_device = False ) : try : if reset_device : self . _device . reset ( ) for interface in [ 0 , 1 ] : if self . _device . is_kernel_driver_active ( interface ) : LOGGER . debug ( 'Detaching kernel driver for interface %d ' 'of %r on ports %r' , interface , self . _device , self . _ports ) self . _device . detach_kernel_driver ( interface ) self . _device . set_configuration ( ) usb . util . claim_interface ( self . _device , INTERFACE ) self . _control_transfer ( COMMANDS [ 'temp' ] ) self . _interrupt_read ( ) self . _control_transfer ( COMMANDS [ 'temp' ] ) temp_data = self . _interrupt_read ( ) if self . _device . product == 'TEMPer1F_H1_V1.4' : humidity_data = temp_data else : humidity_data = None data = { 'temp_data' : temp_data , 'humidity_data' : humidity_data } usb . util . dispose_resources ( self . _device ) return data except usb . USBError as err : if not reset_device : LOGGER . warning ( "Encountered %s, resetting %r and trying again." , err , self . _device ) return self . get_data ( True ) if "not permitted" in str ( err ) : raise Exception ( "Permission problem accessing USB. " "Maybe I need to run as root?" ) else : LOGGER . error ( err ) raise
6423	def dist ( self , src , tar , word_approx_min = 0.3 , char_approx_min = 0.73 , tests = 2 ** 12 - 1 , ) : return ( synoname ( src , tar , word_approx_min , char_approx_min , tests , False ) / 14 )
123	def terminate ( self ) : for worker in self . workers : if worker . is_alive ( ) : worker . terminate ( ) self . nb_workers_finished = len ( self . workers ) if not self . queue_result . _closed : self . queue_result . close ( ) time . sleep ( 0.01 )
9750	def find_x ( path1 ) : libs = os . listdir ( path1 ) for lib_dir in libs : if "doublefann" in lib_dir : return True
11157	def print_big_dir_and_big_file ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table1 = sorted ( [ ( p , p . dirsize ) for p in self . select_dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p1 , size1 in size_table1 [ : top_n ] : print ( "{:<9} {:<9}" . format ( repr_data_size ( size1 ) , p1 . abspath ) ) size_table2 = sorted ( [ ( p , p . size ) for p in p1 . select_file ( recursive = True ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p2 , size2 in size_table2 [ : top_n ] : print ( " {:<9} {:<9}" . format ( repr_data_size ( size2 ) , p2 . abspath ) )
11075	def set ( self , user ) : self . log . info ( "Loading user information for %s/%s" , user . id , user . username ) self . load_user_info ( user ) self . log . info ( "Loading user rights for %s/%s" , user . id , user . username ) self . load_user_rights ( user ) self . log . info ( "Added user: %s/%s" , user . id , user . username ) self . _add_user_to_cache ( user ) return user
3119	def get_prep_value ( self , value ) : if value is None : return None else : return encoding . smart_text ( base64 . b64encode ( jsonpickle . encode ( value ) . encode ( ) ) )
8178	def can_reach ( self , node , traversable = lambda node , edge : True ) : if isinstance ( node , str ) : node = self . graph [ node ] for n in self . graph . nodes : n . _visited = False return proximity . depth_first_search ( self , visit = lambda n : node == n , traversable = traversable )
11893	def retrieve_document ( file_path , directory = 'sec_filings' ) : ftp = FTP ( 'ftp.sec.gov' , timeout = None ) ftp . login ( ) name = file_path . replace ( '/' , '_' ) if not os . path . exists ( directory ) : os . makedirs ( directory ) with tempfile . TemporaryFile ( ) as temp : ftp . retrbinary ( 'RETR %s' % file_path , temp . write ) temp . seek ( 0 ) with open ( '{}/{}' . format ( directory , name ) , 'w+' ) as f : f . write ( temp . read ( ) . decode ( "utf-8" ) ) f . closed records = temp retry = False ftp . close ( )
8722	def operation_list ( uploader ) : files = uploader . file_list ( ) for f in files : log . info ( "{file:30s} {size}" . format ( file = f [ 0 ] , size = f [ 1 ] ) )
12696	def to_XML ( self ) : marcxml_template = oai_template = leader = self . leader if self . leader is not None else "" if leader : leader = "<leader>" + leader + "</leader>" if self . oai_marc : leader = "" xml_template = oai_template if self . oai_marc else marcxml_template xml_output = Template ( xml_template ) . substitute ( LEADER = leader . strip ( ) , CONTROL_FIELDS = self . _serialize_ctl_fields ( ) . strip ( ) , DATA_FIELDS = self . _serialize_data_fields ( ) . strip ( ) ) return xml_output
2682	def upload_s3 ( cfg , path_to_zip_file , * use_s3 ) : print ( 'Uploading your new Lambda function' ) profile_name = cfg . get ( 'profile' ) aws_access_key_id = cfg . get ( 'aws_access_key_id' ) aws_secret_access_key = cfg . get ( 'aws_secret_access_key' ) client = get_client ( 's3' , profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' ) , ) byte_stream = b'' with open ( path_to_zip_file , mode = 'rb' ) as fh : byte_stream = fh . read ( ) s3_key_prefix = cfg . get ( 's3_key_prefix' , '/dist' ) checksum = hashlib . new ( 'md5' , byte_stream ) . hexdigest ( ) timestamp = str ( time . time ( ) ) filename = '{prefix}{checksum}-{ts}.zip' . format ( prefix = s3_key_prefix , checksum = checksum , ts = timestamp , ) buck_name = ( os . environ . get ( 'S3_BUCKET_NAME' ) or cfg . get ( 'bucket_name' ) ) func_name = ( os . environ . get ( 'LAMBDA_FUNCTION_NAME' ) or cfg . get ( 'function_name' ) ) kwargs = { 'Bucket' : '{}' . format ( buck_name ) , 'Key' : '{}' . format ( filename ) , 'Body' : byte_stream , } client . put_object ( ** kwargs ) print ( 'Finished uploading {} to S3 bucket {}' . format ( func_name , buck_name ) ) if use_s3 : return filename
2879	def serialize_value_list ( self , list_elem , thelist ) : for value in thelist : value_elem = SubElement ( list_elem , 'value' ) self . serialize_value ( value_elem , value ) return list_elem
9180	def _validate_roles ( model ) : required_roles = ( ATTRIBUTED_ROLE_KEYS [ 0 ] , ATTRIBUTED_ROLE_KEYS [ 4 ] , ) for role_key in ATTRIBUTED_ROLE_KEYS : try : roles = model . metadata [ role_key ] except KeyError : if role_key in required_roles : raise exceptions . MissingRequiredMetadata ( role_key ) else : if role_key in required_roles and len ( roles ) == 0 : raise exceptions . MissingRequiredMetadata ( role_key ) for role in roles : if role . get ( 'type' ) != 'cnx-id' : raise exceptions . InvalidRole ( role_key , role )
3025	def _save_private_file ( filename , json_contents ) : temp_filename = tempfile . mktemp ( ) file_desc = os . open ( temp_filename , os . O_WRONLY | os . O_CREAT , 0o600 ) with os . fdopen ( file_desc , 'w' ) as file_handle : json . dump ( json_contents , file_handle , sort_keys = True , indent = 2 , separators = ( ',' , ': ' ) ) shutil . move ( temp_filename , filename )
9094	def add_namespace_to_graph ( self , graph : BELGraph ) -> Namespace : namespace = self . upload_bel_namespace ( ) graph . namespace_url [ namespace . keyword ] = namespace . url self . _add_annotation_to_graph ( graph ) return namespace
5654	def makedirs ( path ) : if not os . path . isdir ( path ) : os . makedirs ( path ) return path
12537	def scrape_all_files ( self ) : try : for dcmf in self . items : yield self . read_dcm ( dcmf ) except IOError as ioe : raise IOError ( 'Error reading DICOM file: {}.' . format ( dcmf ) ) from ioe
10041	def deposit_minter ( record_uuid , data ) : provider = DepositProvider . create ( object_type = 'rec' , object_uuid = record_uuid , pid_value = uuid . uuid4 ( ) . hex , ) data [ '_deposit' ] = { 'id' : provider . pid . pid_value , 'status' : 'draft' , } return provider . pid
12031	def get_protocol_sequence ( self , sweep ) : self . setsweep ( sweep ) return list ( self . protoSeqX ) , list ( self . protoSeqY )
8734	def divide_timedelta ( td1 , td2 ) : try : return td1 / td2 except TypeError : return td1 . total_seconds ( ) / td2 . total_seconds ( )
11936	def display ( self ) : if not self . is_group ( ) : return self . _display return ( ( force_text ( k ) , v ) for k , v in self . _display )
7486	def run_cutadapt ( data , subsamples , lbview ) : start = time . time ( ) printstr = " processing reads | {} | s2 |" finished = 0 rawedits = { } subsamples . sort ( key = lambda x : x . stats . reads_raw , reverse = True ) LOGGER . info ( [ i . stats . reads_raw for i in subsamples ] ) if "pair" in data . paramsdict [ "datatype" ] : for sample in subsamples : rawedits [ sample . name ] = lbview . apply ( cutadaptit_pairs , * ( data , sample ) ) else : for sample in subsamples : rawedits [ sample . name ] = lbview . apply ( cutadaptit_single , * ( data , sample ) ) while 1 : finished = sum ( [ i . ready ( ) for i in rawedits . values ( ) ] ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( rawedits ) , finished , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if finished == len ( rawedits ) : print ( "" ) break for async in rawedits : if rawedits [ async ] . successful ( ) : res = rawedits [ async ] . result ( ) if "pair" not in data . paramsdict [ "datatype" ] : parse_single_results ( data , data . samples [ async ] , res ) else : parse_pair_results ( data , data . samples [ async ] , res ) else : print ( " found an error in step2; see ipyrad_log.txt" ) LOGGER . error ( "error in run_cutadapt(): %s" , rawedits [ async ] . exception ( ) )
6155	def cruise_control ( wn , zeta , T , vcruise , vmax , tf_mode = 'H' ) : tau = T / 2. * vmax / vcruise g = 9.8 g *= 3 * 60 ** 2 / 5280. Kp = T * ( 2 * zeta * wn - 1 / tau ) / vmax Ki = T * wn ** 2. / vmax K = Kp * vmax / T print ( 'wn = ' , np . sqrt ( K / ( Kp / Ki ) ) ) print ( 'zeta = ' , ( K + 1 / tau ) / ( 2 * wn ) ) a = np . array ( [ 1 , 2 * zeta * wn , wn ** 2 ] ) if tf_mode == 'H' : b = np . array ( [ K , wn ** 2 ] ) elif tf_mode == 'HE' : b = np . array ( [ 1 , 2 * zeta * wn - K , 0. ] ) elif tf_mode == 'HVW' : b = np . array ( [ 1 , wn ** 2 / K + 1 / tau , wn ** 2 / ( K * tau ) ] ) b *= Kp elif tf_mode == 'HED' : b = np . array ( [ g , 0 ] ) else : raise ValueError ( 'tf_mode must be: H, HE, HVU, or HED' ) return b , a
2738	def assign ( self , droplet_id ) : return self . get_data ( "floating_ips/%s/actions/" % self . ip , type = POST , params = { "type" : "assign" , "droplet_id" : droplet_id } )
11978	def get_wildcard ( self ) : return _convert ( self . _ip , notation = NM_WILDCARD , inotation = IP_DOT , _check = False , _isnm = self . _isnm )
1019	def getSimplePatterns ( numOnes , numPatterns , patternOverlap = 0 ) : assert ( patternOverlap < numOnes ) numNewBitsInEachPattern = numOnes - patternOverlap numCols = numNewBitsInEachPattern * numPatterns + patternOverlap p = [ ] for i in xrange ( numPatterns ) : x = numpy . zeros ( numCols , dtype = 'float32' ) startBit = i * numNewBitsInEachPattern nextStartBit = startBit + numOnes x [ startBit : nextStartBit ] = 1 p . append ( x ) return p
3358	def index ( self , id , * args ) : if isinstance ( id , string_types ) : try : return self . _dict [ id ] except KeyError : raise ValueError ( "%s not found" % id ) try : i = self . _dict [ id . id ] if self [ i ] is not id : raise ValueError ( "Another object with the identical id (%s) found" % id . id ) return i except KeyError : raise ValueError ( "%s not found" % str ( id ) )
6470	def consume_line ( self , line ) : data = RE_VALUE_KEY . split ( line . strip ( ) , 1 ) if len ( data ) == 1 : return float ( data [ 0 ] ) , None else : return float ( data [ 0 ] ) , data [ 1 ] . strip ( )
2892	def connect_outgoing ( self , taskspec , sequence_flow_id , sequence_flow_name , documentation ) : self . connect ( taskspec ) s = SequenceFlow ( sequence_flow_id , sequence_flow_name , documentation , taskspec ) self . outgoing_sequence_flows [ taskspec . name ] = s self . outgoing_sequence_flows_by_id [ sequence_flow_id ] = s
12669	def niftilist_mask_to_array ( img_filelist , mask_file = None , outdtype = None ) : img = check_img ( img_filelist [ 0 ] ) if not outdtype : outdtype = img . dtype mask_data , _ = load_mask_data ( mask_file ) indices = np . where ( mask_data ) mask = check_img ( mask_file ) outmat = np . zeros ( ( len ( img_filelist ) , np . count_nonzero ( mask_data ) ) , dtype = outdtype ) for i , img_item in enumerate ( img_filelist ) : img = check_img ( img_item ) if not are_compatible_imgs ( img , mask ) : raise NiftiFilesNotCompatible ( repr_imgs ( img ) , repr_imgs ( mask_file ) ) vol = get_img_data ( img ) outmat [ i , : ] = vol [ indices ] return outmat , mask_data
11169	def _add_positional_argument ( self , posarg ) : if self . positional_args : if self . positional_args [ - 1 ] . recurring : raise ValueError ( "recurring positional arguments must be last" ) if self . positional_args [ - 1 ] . optional and not posarg . optional : raise ValueError ( "required positional arguments must precede optional ones" ) self . positional_args . append ( posarg )
13126	def get_domains ( self ) : search = User . search ( ) search . aggs . bucket ( 'domains' , 'terms' , field = 'domain' , order = { '_count' : 'desc' } , size = 100 ) response = search . execute ( ) return [ entry . key for entry in response . aggregations . domains . buckets ]
8748	def update_scalingip ( context , id , content ) : LOG . info ( 'update_scalingip %s for tenant %s and body %s' % ( id , context . tenant_id , content ) ) requested_ports = content . get ( 'ports' , [ ] ) flip = _update_flip ( context , id , ip_types . SCALING , requested_ports ) return v . _make_scaling_ip_dict ( flip )
13039	def overview ( ) : search = Credential . search ( ) search . aggs . bucket ( 'password_count' , 'terms' , field = 'secret' , order = { '_count' : 'desc' } , size = 20 ) . metric ( 'username_count' , 'cardinality' , field = 'username' ) . metric ( 'host_count' , 'cardinality' , field = 'host_ip' ) . metric ( 'top_hits' , 'top_hits' , docvalue_fields = [ 'username' ] , size = 100 ) response = search . execute ( ) print_line ( "{0:65} {1:5} {2:5} {3:5} {4}" . format ( "Secret" , "Count" , "Hosts" , "Users" , "Usernames" ) ) print_line ( "-" * 100 ) for entry in response . aggregations . password_count . buckets : usernames = [ ] for creds in entry . top_hits : usernames . append ( creds . username [ 0 ] ) usernames = list ( set ( usernames ) ) print_line ( "{0:65} {1:5} {2:5} {3:5} {4}" . format ( entry . key , entry . doc_count , entry . host_count . value , entry . username_count . value , usernames ) )
3228	def gce_list_aggregated ( service = None , key_name = 'name' , ** kwargs ) : resp_list = [ ] req = service . aggregatedList ( ** kwargs ) while req is not None : resp = req . execute ( ) for location , item in resp [ 'items' ] . items ( ) : if key_name in item : resp_list . extend ( item [ key_name ] ) req = service . aggregatedList_next ( previous_request = req , previous_response = resp ) return resp_list
8312	def draw_list ( markup , x , y , w , padding = 5 , callback = None ) : try : from web import _ctx except : pass i = 1 for chunk in markup . split ( "\n" ) : if callback != None : callback ( chunk , i ) m = re . search ( "^([0-9]{1,3}\. )" , chunk . lstrip ( ) ) if m : indent = re . search ( "[0-9]" , chunk ) . start ( ) * padding * 2 bullet = m . group ( 1 ) dx = textwidth ( "000." ) chunk = chunk . lstrip ( m . group ( 1 ) + "\t" ) if chunk . lstrip ( ) . startswith ( "*" ) : indent = chunk . find ( "*" ) * padding * 2 bullet = u"" dx = textwidth ( "*" ) chunk = chunk . lstrip ( "* \t" ) _ctx . text ( bullet , x + indent , y ) dx += padding + indent _ctx . text ( chunk , x + dx , y , width = w - dx ) y += _ctx . textheight ( chunk , width = w - dx ) y += _ctx . textheight ( " " ) * 0.25 i += 1
7587	def taxon_table ( self ) : if self . tests : keys = sorted ( self . tests [ 0 ] . keys ( ) ) if isinstance ( self . tests , list ) : ld = [ [ ( key , i [ key ] ) for key in keys ] for i in self . tests ] dd = [ dict ( i ) for i in ld ] df = pd . DataFrame ( dd ) return df else : return pd . DataFrame ( pd . Series ( self . tests ) ) . T else : return None
5822	def vcl ( self , name , content ) : vcl = VCL ( ) vcl . conn = self . conn vcl . attrs = { 'service_id' : self . attrs [ 'service_id' ] , 'version' : self . attrs [ 'number' ] , 'name' : name , 'content' : content , } vcl . save ( ) return vcl
907	def replaceIterationCycle ( self , phaseSpecs ) : self . __phaseManager = _PhaseManager ( model = self . __model , phaseSpecs = phaseSpecs ) return
5009	def create_course_completion ( self , user_id , payload ) : url = self . enterprise_configuration . sapsf_base_url + self . global_sap_config . completion_status_api_path return self . _call_post_with_user_override ( user_id , url , payload )
10590	def report ( self , format = ReportFormat . printout , output_path = None ) : rpt = GlsRpt ( self , output_path ) return rpt . render ( format )
5806	def parse_alert ( server_handshake_bytes ) : for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x15' : continue if len ( record_data ) != 2 : return None return ( int_from_bytes ( record_data [ 0 : 1 ] ) , int_from_bytes ( record_data [ 1 : 2 ] ) ) return None
1259	def save_component ( self , component_name , save_path ) : component = self . get_component ( component_name = component_name ) self . _validate_savable ( component = component , component_name = component_name ) return component . save ( sess = self . session , save_path = save_path )
5784	def select_write ( self , timeout = None ) : _ , write_ready , _ = select . select ( [ ] , [ self . _socket ] , [ ] , timeout ) return len ( write_ready ) > 0
13321	def add_active_module ( module ) : modules = set ( get_active_modules ( ) ) modules . add ( module ) new_modules_path = os . pathsep . join ( [ m . path for m in modules ] ) os . environ [ 'CPENV_ACTIVE_MODULES' ] = str ( new_modules_path )
2551	def unescape ( data ) : cc = re . compile ( r'&(?:(?:#(\d+))|([^;]+));' ) result = [ ] m = cc . search ( data ) while m : result . append ( data [ 0 : m . start ( ) ] ) d = m . group ( 1 ) if d : d = int ( d ) result . append ( unichr ( d ) ) else : d = _unescape . get ( m . group ( 2 ) , ord ( '?' ) ) result . append ( unichr ( d ) ) data = data [ m . end ( ) : ] m = cc . search ( data ) result . append ( data ) return '' . join ( result )
1064	def isheader ( self , line ) : i = line . find ( ':' ) if i > - 1 : return line [ : i ] . lower ( ) return None
12149	def analyzeAll ( self ) : searchableData = str ( self . files2 ) self . log . debug ( "considering analysis for %d ABFs" , len ( self . IDs ) ) for ID in self . IDs : if not ID + "_" in searchableData : self . log . debug ( "%s needs analysis" , ID ) try : self . analyzeABF ( ID ) except : print ( "EXCEPTION! " * 100 ) else : self . log . debug ( "%s has existing analysis, not overwriting" , ID ) self . log . debug ( "verified analysis of %d ABFs" , len ( self . IDs ) )
2110	def send ( source = None , prevent = None , exclude = None , secret_management = 'default' , no_color = False ) : from tower_cli . cli . transfer . send import Sender sender = Sender ( no_color ) sender . send ( source , prevent , exclude , secret_management )
3367	def linear_reaction_coefficients ( model , reactions = None ) : linear_coefficients = { } reactions = model . reactions if not reactions else reactions try : objective_expression = model . solver . objective . expression coefficients = objective_expression . as_coefficients_dict ( ) except AttributeError : return linear_coefficients for rxn in reactions : forward_coefficient = coefficients . get ( rxn . forward_variable , 0 ) reverse_coefficient = coefficients . get ( rxn . reverse_variable , 0 ) if forward_coefficient != 0 : if forward_coefficient == - reverse_coefficient : linear_coefficients [ rxn ] = float ( forward_coefficient ) return linear_coefficients
3242	def boto3_cached_conn ( service , service_type = 'client' , future_expiration_minutes = 15 , account_number = None , assume_role = None , session_name = 'cloudaux' , region = 'us-east-1' , return_credentials = False , external_id = None , arn_partition = 'aws' ) : key = ( account_number , assume_role , session_name , external_id , region , service_type , service , arn_partition ) if key in CACHE : retval = _get_cached_creds ( key , service , service_type , region , future_expiration_minutes , return_credentials ) if retval : return retval role = None if assume_role : sts = boto3 . session . Session ( ) . client ( 'sts' ) if not all ( [ account_number , assume_role ] ) : raise ValueError ( "Account number and role to assume are both required" ) arn = 'arn:{partition}:iam::{0}:role/{1}' . format ( account_number , assume_role , partition = arn_partition ) assume_role_kwargs = { 'RoleArn' : arn , 'RoleSessionName' : session_name } if external_id : assume_role_kwargs [ 'ExternalId' ] = external_id role = sts . assume_role ( ** assume_role_kwargs ) if service_type == 'client' : conn = _client ( service , region , role ) elif service_type == 'resource' : conn = _resource ( service , region , role ) if role : CACHE [ key ] = role if return_credentials : return conn , role [ 'Credentials' ] return conn
4	def parse_unknown_args ( args ) : retval = { } preceded_by_key = False for arg in args : if arg . startswith ( '--' ) : if '=' in arg : key = arg . split ( '=' ) [ 0 ] [ 2 : ] value = arg . split ( '=' ) [ 1 ] retval [ key ] = value else : key = arg [ 2 : ] preceded_by_key = True elif preceded_by_key : retval [ key ] = arg preceded_by_key = False return retval
6605	def result_fullpath ( self , package_index ) : ret = os . path . join ( self . path , self . result_relpath ( package_index ) ) return ret
5293	def get_context_data ( self , ** kwargs ) : context = { } inlines_names = self . get_inlines_names ( ) if inlines_names : context . update ( zip ( inlines_names , kwargs . get ( 'inlines' , [ ] ) ) ) if 'formset' in kwargs : context [ inlines_names [ 0 ] ] = kwargs [ 'formset' ] context . update ( kwargs ) return super ( NamedFormsetsMixin , self ) . get_context_data ( ** context )
3397	def extend_model ( self , exchange_reactions = False , demand_reactions = True ) : for rxn in self . universal . reactions : rxn . gapfilling_type = 'universal' new_metabolites = self . universal . metabolites . query ( lambda metabolite : metabolite not in self . model . metabolites ) self . model . add_metabolites ( new_metabolites ) existing_exchanges = [ ] for rxn in self . universal . boundary : existing_exchanges = existing_exchanges + [ met . id for met in list ( rxn . metabolites ) ] for met in self . model . metabolites : if exchange_reactions : if met . id not in existing_exchanges : rxn = self . universal . add_boundary ( met , type = 'exchange_smiley' , lb = - 1000 , ub = 0 , reaction_id = 'EX_{}' . format ( met . id ) ) rxn . gapfilling_type = 'exchange' if demand_reactions : rxn = self . universal . add_boundary ( met , type = 'demand_smiley' , lb = 0 , ub = 1000 , reaction_id = 'DM_{}' . format ( met . id ) ) rxn . gapfilling_type = 'demand' new_reactions = self . universal . reactions . query ( lambda reaction : reaction not in self . model . reactions ) self . model . add_reactions ( new_reactions )
12951	def _get_new_connection ( self ) : pool = getRedisPool ( self . mdl . REDIS_CONNECTION_PARAMS ) return redis . Redis ( connection_pool = pool )
13498	def with_revision ( self , label , number ) : t = self . clone ( ) t . revision = Revision ( label , number ) return t
13834	def PrintMessage ( self , message ) : fields = message . ListFields ( ) if self . use_index_order : fields . sort ( key = lambda x : x [ 0 ] . index ) for field , value in fields : if _IsMapEntry ( field ) : for key in sorted ( value ) : entry_submsg = field . message_type . _concrete_class ( key = key , value = value [ key ] ) self . PrintField ( field , entry_submsg ) elif field . label == descriptor . FieldDescriptor . LABEL_REPEATED : for element in value : self . PrintField ( field , element ) else : self . PrintField ( field , value )
6689	def groupuninstall ( group , options = None ) : manager = MANAGER if options is None : options = [ ] elif isinstance ( options , str ) : options = [ options ] options = " " . join ( options ) run_as_root ( '%(manager)s %(options)s groupremove "%(group)s"' % locals ( ) )
7075	def run_periodfinding ( simbasedir , pfmethods = ( 'gls' , 'pdm' , 'bls' ) , pfkwargs = ( { } , { } , { 'startp' : 1.0 , 'maxtransitduration' : 0.3 } ) , getblssnr = False , sigclip = 5.0 , nperiodworkers = 10 , ncontrolworkers = 4 , liststartindex = None , listmaxobjects = None ) : with open ( os . path . join ( simbasedir , 'fakelcs-info.pkl' ) , 'rb' ) as infd : siminfo = pickle . load ( infd ) lcfpaths = siminfo [ 'lcfpath' ] pfdir = os . path . join ( simbasedir , 'periodfinding' ) timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] fakelc_formatkey = 'fake-%s' % siminfo [ 'lcformat' ] lcproc . register_lcformat ( fakelc_formatkey , '*-fakelc.pkl' , timecols , magcols , errcols , 'astrobase.lcproc' , '_read_pklc' , magsarefluxes = siminfo [ 'magsarefluxes' ] ) if liststartindex : lcfpaths = lcfpaths [ liststartindex : ] if listmaxobjects : lcfpaths = lcfpaths [ : listmaxobjects ] pfinfo = periodsearch . parallel_pf ( lcfpaths , pfdir , lcformat = fakelc_formatkey , pfmethods = pfmethods , pfkwargs = pfkwargs , getblssnr = getblssnr , sigclip = sigclip , nperiodworkers = nperiodworkers , ncontrolworkers = ncontrolworkers ) with open ( os . path . join ( simbasedir , 'fakelc-periodsearch.pkl' ) , 'wb' ) as outfd : pickle . dump ( pfinfo , outfd , pickle . HIGHEST_PROTOCOL ) return os . path . join ( simbasedir , 'fakelc-periodsearch.pkl' )
13284	def list_from_document ( cls , doc ) : objs = [ ] for feu in doc . xpath ( '//FEU' ) : detail_els = feu . xpath ( 'event-element-details/event-element-detail' ) for idx , detail in enumerate ( detail_els ) : objs . append ( cls ( feu , detail , id_suffix = idx , number_in_group = len ( detail_els ) ) ) return objs
3471	def add_metabolites ( self , metabolites_to_add , combine = True , reversibly = True ) : old_coefficients = self . metabolites new_metabolites = [ ] _id_to_metabolites = dict ( [ ( x . id , x ) for x in self . _metabolites ] ) for metabolite , coefficient in iteritems ( metabolites_to_add ) : if isinstance ( metabolite , Metabolite ) : if ( ( metabolite . model is not None ) and ( metabolite . model is not self . _model ) ) : metabolite = metabolite . copy ( ) met_id = str ( metabolite ) if met_id in _id_to_metabolites : reaction_metabolite = _id_to_metabolites [ met_id ] if combine : self . _metabolites [ reaction_metabolite ] += coefficient else : self . _metabolites [ reaction_metabolite ] = coefficient else : if self . _model : try : metabolite = self . _model . metabolites . get_by_id ( met_id ) except KeyError as e : if isinstance ( metabolite , Metabolite ) : new_metabolites . append ( metabolite ) else : raise e elif isinstance ( metabolite , string_types ) : raise ValueError ( "Reaction '%s' does not belong to a " "model. Either add the reaction to a " "model or use Metabolite objects instead " "of strings as keys." % self . id ) self . _metabolites [ metabolite ] = coefficient metabolite . _reaction . add ( self ) model = self . model if model is not None : model . add_metabolites ( new_metabolites ) for metabolite , coefficient in self . _metabolites . items ( ) : model . constraints [ metabolite . id ] . set_linear_coefficients ( { self . forward_variable : coefficient , self . reverse_variable : - coefficient } ) for metabolite , the_coefficient in list ( self . _metabolites . items ( ) ) : if the_coefficient == 0 : metabolite . _reaction . remove ( self ) self . _metabolites . pop ( metabolite ) context = get_context ( self ) if context and reversibly : if combine : context ( partial ( self . subtract_metabolites , metabolites_to_add , combine = True , reversibly = False ) ) else : mets_to_reset = { key : old_coefficients [ model . metabolites . get_by_any ( key ) [ 0 ] ] for key in iterkeys ( metabolites_to_add ) } context ( partial ( self . add_metabolites , mets_to_reset , combine = False , reversibly = False ) )
8805	def build_full_day_ips ( query , period_start , period_end ) : ip_list = query . filter ( models . IPAddress . version == 4L ) . filter ( models . IPAddress . network_id == PUBLIC_NETWORK_ID ) . filter ( models . IPAddress . used_by_tenant_id is not None ) . filter ( models . IPAddress . allocated_at != null ( ) ) . filter ( models . IPAddress . allocated_at < period_start ) . filter ( or_ ( models . IPAddress . _deallocated is False , models . IPAddress . deallocated_at == null ( ) , models . IPAddress . deallocated_at >= period_end ) ) . all ( ) return ip_list
3292	def set_property_value ( self , name , value , dry_run = False ) : assert value is None or xml_tools . is_etree_element ( value ) if name in _lockPropertyNames : raise DAVError ( HTTP_FORBIDDEN , err_condition = PRECONDITION_CODE_ProtectedProperty ) config = self . environ [ "wsgidav.config" ] mutableLiveProps = config . get ( "mutable_live_props" , [ ] ) if ( name . startswith ( "{DAV:}" ) and name in _standardLivePropNames and name in mutableLiveProps ) : if name in ( "{DAV:}getlastmodified" , "{DAV:}last_modified" ) : try : return self . set_last_modified ( self . path , value . text , dry_run ) except Exception : _logger . warning ( "Provider does not support set_last_modified on {}." . format ( self . path ) ) raise DAVError ( HTTP_FORBIDDEN ) if name . startswith ( "{urn:schemas-microsoft-com:}" ) : agent = self . environ . get ( "HTTP_USER_AGENT" , "None" ) win32_emu = config . get ( "hotfixes" , { } ) . get ( "emulate_win32_lastmod" , False ) if win32_emu and "MiniRedir/6.1" not in agent : if "Win32LastModifiedTime" in name : return self . set_last_modified ( self . path , value . text , dry_run ) elif "Win32FileAttributes" in name : return True elif "Win32CreationTime" in name : return True elif "Win32LastAccessTime" in name : return True pm = self . provider . prop_manager if pm and not name . startswith ( "{DAV:}" ) : refUrl = self . get_ref_url ( ) if value is None : return pm . remove_property ( refUrl , name , dry_run , self . environ ) else : value = etree . tostring ( value ) return pm . write_property ( refUrl , name , value , dry_run , self . environ ) raise DAVError ( HTTP_FORBIDDEN )
8481	def env ( key , default ) : value = os . environ . get ( key , None ) if value is not None : log . info ( ' %s = %r' , key . lower ( ) . replace ( '_' , '.' ) , value ) return value key = key . lower ( ) . replace ( '_' , '.' ) value = get ( key ) if value is not None : return value return default
12251	def get_all_keys ( self , * args , ** kwargs ) : if kwargs . pop ( 'force' , None ) : headers = kwargs . get ( 'headers' , args [ 0 ] if len ( args ) else None ) or dict ( ) headers [ 'force' ] = True kwargs [ 'headers' ] = headers return super ( Bucket , self ) . get_all_keys ( * args , ** kwargs )
5875	def get_images_bytesize_match ( self , images ) : cnt = 0 max_bytes_size = 15728640 good_images = [ ] for image in images : if cnt > 30 : return good_images src = self . parser . getAttribute ( image , attr = 'src' ) src = self . build_image_path ( src ) src = self . add_schema_if_none ( src ) local_image = self . get_local_image ( src ) if local_image : filesize = local_image . bytes if ( filesize == 0 or filesize > self . images_min_bytes ) and filesize < max_bytes_size : good_images . append ( image ) else : images . remove ( image ) cnt += 1 return good_images if len ( good_images ) > 0 else None
10042	def admin_permission_factory ( ) : try : pkg_resources . get_distribution ( 'invenio-access' ) from invenio_access . permissions import DynamicPermission as Permission except pkg_resources . DistributionNotFound : from flask_principal import Permission return Permission ( action_admin_access )
1158	def release ( self ) : if self . __owner != _get_ident ( ) : raise RuntimeError ( "cannot release un-acquired lock" ) self . __count = count = self . __count - 1 if not count : self . __owner = None self . __block . release ( ) if __debug__ : self . _note ( "%s.release(): final release" , self ) else : if __debug__ : self . _note ( "%s.release(): non-final release" , self )
5257	def block_to_fork ( block_number ) : forks_by_block = { 0 : "frontier" , 1150000 : "homestead" , 2463000 : "tangerine_whistle" , 2675000 : "spurious_dragon" , 4370000 : "byzantium" , 7280000 : "petersburg" , 9999999 : "serenity" } fork_names = list ( forks_by_block . values ( ) ) fork_blocks = list ( forks_by_block . keys ( ) ) return fork_names [ bisect ( fork_blocks , block_number ) - 1 ]
8785	def update_port ( self , context , port_id , ** kwargs ) : LOG . info ( "update_port %s %s" % ( context . tenant_id , port_id ) ) if kwargs . get ( "security_groups" ) : msg = 'ironic driver does not support security group operations.' raise IronicException ( msg = msg ) return { "uuid" : port_id }
1191	def fnmatchcase ( name , pat ) : try : re_pat = _cache [ pat ] except KeyError : res = translate ( pat ) if len ( _cache ) >= _MAXCACHE : globals ( ) [ '_cache' ] = { } _cache [ pat ] = re_pat = re . compile ( res ) return re_pat . match ( name ) is not None
13551	def _put_resource ( self , url , body ) : headers = { "Content-Type" : "application/json" , "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = WhenIWork_DAO ( ) . putURL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 201 or response . status == 204 ) : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
13520	def configure ( self , url = None , token = None , test = False ) : if url is None : url = Config . get_value ( "url" ) if token is None : token = Config . get_value ( "token" ) self . server_url = url self . auth_header = { "Authorization" : "Basic {0}" . format ( token ) } self . configured = True if test : self . test_connection ( ) Config . set ( "url" , url ) Config . set ( "token" , token )
2244	def argflag ( key , argv = None ) : if argv is None : argv = sys . argv keys = [ key ] if isinstance ( key , six . string_types ) else key flag = any ( k in argv for k in keys ) return flag
6916	def add_variability_to_fakelc_collection ( simbasedir , override_paramdists = None , overwrite_existingvar = False ) : infof = os . path . join ( simbasedir , 'fakelcs-info.pkl' ) with open ( infof , 'rb' ) as infd : lcinfo = pickle . load ( infd ) lclist = lcinfo [ 'lcfpath' ] varflag = lcinfo [ 'isvariable' ] vartypes = lcinfo [ 'vartype' ] vartind = 0 varinfo = { } for lc , varf , _lcind in zip ( lclist , varflag , range ( len ( lclist ) ) ) : if varf : thisvartype = vartypes [ vartind ] if ( override_paramdists and isinstance ( override_paramdists , dict ) and thisvartype in override_paramdists and isinstance ( override_paramdists [ thisvartype ] , dict ) ) : thisoverride_paramdists = override_paramdists [ thisvartype ] else : thisoverride_paramdists = None varlc = add_fakelc_variability ( lc , thisvartype , override_paramdists = thisoverride_paramdists , overwrite = overwrite_existingvar ) varinfo [ varlc [ 'objectid' ] ] = { 'params' : varlc [ 'actual_varparams' ] , 'vartype' : varlc [ 'actual_vartype' ] } vartind = vartind + 1 else : varlc = add_fakelc_variability ( lc , None , overwrite = overwrite_existingvar ) varinfo [ varlc [ 'objectid' ] ] = { 'params' : varlc [ 'actual_varparams' ] , 'vartype' : varlc [ 'actual_vartype' ] } lcinfo [ 'varinfo' ] = varinfo tempoutf = '%s.%s' % ( infof , md5 ( npr . bytes ( 4 ) ) . hexdigest ( ) [ - 8 : ] ) with open ( tempoutf , 'wb' ) as outfd : pickle . dump ( lcinfo , outfd , pickle . HIGHEST_PROTOCOL ) if os . path . exists ( tempoutf ) : shutil . copy ( tempoutf , infof ) os . remove ( tempoutf ) else : LOGEXCEPTION ( 'could not write output light curve file to dir: %s' % os . path . dirname ( tempoutf ) ) raise return lcinfo
7591	def run ( self , force = False , ipyclient = None , name_fields = 30 , name_separator = "_" , dry_run = False ) : try : if not os . path . exists ( self . workdir ) : os . makedirs ( self . workdir ) self . _set_vdbconfig_path ( ) if ipyclient : self . _ipcluster [ "pids" ] = { } for eid in ipyclient . ids : engine = ipyclient [ eid ] if not engine . outstanding : pid = engine . apply ( os . getpid ) . get ( ) self . _ipcluster [ "pids" ] [ eid ] = pid self . _submit_jobs ( force = force , ipyclient = ipyclient , name_fields = name_fields , name_separator = name_separator , dry_run = dry_run , ) except IPyradWarningExit as inst : print ( inst ) except KeyboardInterrupt : print ( "keyboard interrupt..." ) except Exception as inst : print ( "Exception in run() - {}" . format ( inst ) ) finally : self . _restore_vdbconfig_path ( ) sradir = os . path . join ( self . workdir , "sra" ) if os . path . exists ( sradir ) and ( not os . listdir ( sradir ) ) : shutil . rmtree ( sradir ) else : try : print ( FAILED_DOWNLOAD . format ( os . listdir ( sradir ) ) ) except OSError as inst : raise IPyradWarningExit ( "Download failed. Exiting." ) for srr in os . listdir ( sradir ) : isrr = srr . split ( "." ) [ 0 ] ipath = os . path . join ( self . workdir , "*_{}*.gz" . format ( isrr ) ) ifile = glob . glob ( ipath ) [ 0 ] if os . path . exists ( ifile ) : os . remove ( ifile ) shutil . rmtree ( sradir ) if ipyclient : try : ipyclient . abort ( ) time . sleep ( 0.5 ) for engine_id , pid in self . _ipcluster [ "pids" ] . items ( ) : if ipyclient . queue_status ( ) [ engine_id ] [ "tasks" ] : os . kill ( pid , 2 ) time . sleep ( 0.1 ) except ipp . NoEnginesRegistered : pass if not ipyclient . outstanding : ipyclient . purge_everything ( ) else : ipyclient . shutdown ( hub = True , block = False ) ipyclient . close ( ) print ( "\nwarning: ipcluster shutdown and must be restarted" )
2617	def read_state_file ( self , state_file ) : try : fh = open ( state_file , 'r' ) state = json . load ( fh ) self . vpc_id = state [ 'vpcID' ] self . sg_id = state [ 'sgID' ] self . sn_ids = state [ 'snIDs' ] self . instances = state [ 'instances' ] except Exception as e : logger . debug ( "Caught exception while reading state file: {0}" . format ( e ) ) raise e logger . debug ( "Done reading state from the local state file." )
7966	def end ( self , tag ) : self . _level -= 1 if self . _level < 0 : self . _handler . stream_parse_error ( u"Unexpected end tag for: {0!r}" . format ( tag ) ) return if self . _level == 0 : if tag != self . _root . tag : self . _handler . stream_parse_error ( u"Unexpected end tag for:" " {0!r} (stream end tag expected)" . format ( tag ) ) return self . _handler . stream_end ( ) return element = self . _builder . end ( tag ) if self . _level == 1 : self . _handler . stream_element ( element )
2161	def get_command ( self , ctx , name ) : if not hasattr ( self . resource , name ) : return None method = getattr ( self . resource , name ) attrs = getattr ( method , '_cli_command_attrs' , { } ) help_text = inspect . getdoc ( method ) attrs [ 'help' ] = self . _auto_help_text ( help_text or '' ) ignore_defaults = attrs . pop ( 'ignore_defaults' , False ) new_method = self . _echo_method ( method ) click_params = getattr ( method , '__click_params__' , [ ] ) new_method . __click_params__ = copy ( click_params ) new_method = with_global_options ( new_method ) fao = attrs . pop ( 'use_fields_as_options' , True ) if fao : for field in reversed ( self . resource . fields ) : if not field . is_option : continue if not isinstance ( fao , bool ) and field . name not in fao : continue args = [ field . option ] if field . key : args . insert ( 0 , field . key ) short_fields = { 'name' : 'n' , 'description' : 'd' , 'inventory' : 'i' , 'extra_vars' : 'e' } if field . name in short_fields : args . append ( '-' + short_fields [ field . name ] ) option_help = field . help if isinstance ( field . type , StructuredInput ) : option_help += ' Use @ to get JSON or YAML from a file.' if field . required : option_help = '[REQUIRED] ' + option_help elif field . read_only : option_help = '[READ ONLY] ' + option_help option_help = '[FIELD]' + option_help click . option ( * args , default = field . default if not ignore_defaults else None , help = option_help , type = field . type , show_default = field . show_default , multiple = field . multiple , is_eager = False ) ( new_method ) cmd = click . command ( name = name , cls = ActionSubcommand , ** attrs ) ( new_method ) code = six . get_function_code ( method ) if 'pk' in code . co_varnames : click . argument ( 'pk' , nargs = 1 , required = False , type = str , metavar = '[ID]' ) ( cmd ) return cmd
1501	def template_slave_hcl ( cl_args , masters ) : slave_config_template = "%s/standalone/templates/slave.template.hcl" % cl_args [ "config_path" ] slave_config_actual = "%s/standalone/resources/slave.hcl" % cl_args [ "config_path" ] masters_in_quotes = [ '"%s"' % master for master in masters ] template_file ( slave_config_template , slave_config_actual , { "<nomad_masters:master_port>" : ", " . join ( masters_in_quotes ) } )
8844	def _handle_indent_between_paren ( self , column , line , parent_impl , tc ) : pre , post = parent_impl next_char = self . _get_next_char ( tc ) prev_char = self . _get_prev_char ( tc ) prev_open = prev_char in [ '[' , '(' , '{' ] next_close = next_char in [ ']' , ')' , '}' ] ( open_line , open_symbol_col ) , ( close_line , close_col ) = self . _get_paren_pos ( tc , column ) open_line_txt = self . _helper . line_text ( open_line ) open_line_indent = len ( open_line_txt ) - len ( open_line_txt . lstrip ( ) ) if prev_open : post = ( open_line_indent + self . editor . tab_length ) * ' ' elif next_close and prev_char != ',' : post = open_line_indent * ' ' elif tc . block ( ) . blockNumber ( ) == open_line : post = open_symbol_col * ' ' if close_line and close_col : txt = self . _helper . line_text ( close_line ) bn = tc . block ( ) . blockNumber ( ) flg = bn == close_line next_indent = self . _helper . line_indent ( bn + 1 ) * ' ' if flg and txt . strip ( ) . endswith ( ':' ) and next_indent == post : post += self . editor . tab_length * ' ' if next_char in [ '"' , "'" ] : tc . movePosition ( tc . Left ) is_string = self . _helper . is_comment_or_string ( tc , formats = [ 'string' ] ) if next_char in [ '"' , "'" ] : tc . movePosition ( tc . Right ) if is_string : trav = QTextCursor ( tc ) while self . _helper . is_comment_or_string ( trav , formats = [ 'string' ] ) : trav . movePosition ( trav . Left ) trav . movePosition ( trav . Right ) symbol = '%s' % self . _get_next_char ( trav ) pre += symbol post += symbol return pre , post
4429	async def _queue ( self , ctx , page : int = 1 ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue : return await ctx . send ( 'There\'s nothing in the queue! Why not queue something?' ) items_per_page = 10 pages = math . ceil ( len ( player . queue ) / items_per_page ) start = ( page - 1 ) * items_per_page end = start + items_per_page queue_list = '' for index , track in enumerate ( player . queue [ start : end ] , start = start ) : queue_list += f'`{index + 1}.` [**{track.title}**]({track.uri})\n' embed = discord . Embed ( colour = discord . Color . blurple ( ) , description = f'**{len(player.queue)} tracks**\n\n{queue_list}' ) embed . set_footer ( text = f'Viewing page {page}/{pages}' ) await ctx . send ( embed = embed )
138	def to_shapely_line_string ( self , closed = False , interpolate = 0 ) : return _convert_points_to_shapely_line_string ( self . exterior , closed = closed , interpolate = interpolate )
7386	def find_node_group_membership ( self , node ) : for group , nodelist in self . nodes . items ( ) : if node in nodelist : return group
6905	def great_circle_dist ( ra1 , dec1 , ra2 , dec2 ) : in_ra1 = ra1 % 360.0 in_ra1 = in_ra1 + 360.0 * ( in_ra1 < 0.0 ) in_ra2 = ra2 % 360.0 in_ra2 = in_ra2 + 360.0 * ( in_ra1 < 0.0 ) ra1_rad , dec1_rad = np . deg2rad ( in_ra1 ) , np . deg2rad ( dec1 ) ra2_rad , dec2_rad = np . deg2rad ( in_ra2 ) , np . deg2rad ( dec2 ) del_dec2 = ( dec2_rad - dec1_rad ) / 2.0 del_ra2 = ( ra2_rad - ra1_rad ) / 2.0 sin_dist = np . sqrt ( np . sin ( del_dec2 ) * np . sin ( del_dec2 ) + np . cos ( dec1_rad ) * np . cos ( dec2_rad ) * np . sin ( del_ra2 ) * np . sin ( del_ra2 ) ) dist_rad = 2.0 * np . arcsin ( sin_dist ) return np . rad2deg ( dist_rad ) * 3600.0
10645	def create ( dataset , symbol , degree ) : x_vals = dataset . data [ 'T' ] . tolist ( ) y_vals = dataset . data [ symbol ] . tolist ( ) coeffs = np . polyfit ( x_vals , y_vals , degree ) result = PolynomialModelT ( dataset . material , dataset . names_dict [ symbol ] , symbol , dataset . display_symbols_dict [ symbol ] , dataset . units_dict [ symbol ] , None , [ dataset . name ] , coeffs ) result . state_schema [ 'T' ] [ 'min' ] = float ( min ( x_vals ) ) result . state_schema [ 'T' ] [ 'max' ] = float ( max ( x_vals ) ) return result
4506	def find_serial_devices ( self ) : if self . devices is not None : return self . devices self . devices = { } hardware_id = "(?i)" + self . hardware_id for ports in serial . tools . list_ports . grep ( hardware_id ) : port = ports [ 0 ] try : id = self . get_device_id ( port ) ver = self . _get_device_version ( port ) except : log . debug ( 'Error getting device_id for %s, %s' , port , self . baudrate ) if True : raise continue if getattr ( ports , '__len__' , lambda : 0 ) ( ) : log . debug ( 'Multi-port device %s:%s:%s with %s ports found' , self . hardware_id , id , ver , len ( ports ) ) if id < 0 : log . debug ( 'Serial device %s:%s:%s with id %s < 0' , self . hardware_id , id , ver ) else : self . devices [ id ] = port , ver return self . devices
2399	def initialize_dictionaries ( self , e_set , max_feats2 = 200 ) : if ( hasattr ( e_set , '_type' ) ) : if ( e_set . _type == "train" ) : nvocab = util_functions . get_vocab ( e_set . _text , e_set . _score , max_feats2 = max_feats2 ) svocab = util_functions . get_vocab ( e_set . _clean_stem_text , e_set . _score , max_feats2 = max_feats2 ) self . _normal_dict = CountVectorizer ( ngram_range = ( 1 , 2 ) , vocabulary = nvocab ) self . _stem_dict = CountVectorizer ( ngram_range = ( 1 , 2 ) , vocabulary = svocab ) self . dict_initialized = True self . _mean_spelling_errors = sum ( e_set . _spelling_errors ) / float ( len ( e_set . _spelling_errors ) ) self . _spell_errors_per_character = sum ( e_set . _spelling_errors ) / float ( sum ( [ len ( t ) for t in e_set . _text ] ) ) good_pos_tags , bad_pos_positions = self . _get_grammar_errors ( e_set . _pos , e_set . _text , e_set . _tokens ) self . _grammar_errors_per_character = ( sum ( good_pos_tags ) / float ( sum ( [ len ( t ) for t in e_set . _text ] ) ) ) bag_feats = self . gen_bag_feats ( e_set ) f_row_sum = numpy . sum ( bag_feats [ : , : ] ) self . _mean_f_prop = f_row_sum / float ( sum ( [ len ( t ) for t in e_set . _text ] ) ) ret = "ok" else : raise util_functions . InputError ( e_set , "needs to be an essay set of the train type." ) else : raise util_functions . InputError ( e_set , "wrong input. need an essay set object" ) return ret
1232	def import_experience ( self , states , internals , actions , terminal , reward ) : fetches = self . import_experience_output feed_dict = self . get_feed_dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) self . monitored_session . run ( fetches = fetches , feed_dict = feed_dict )
1947	def sync_unicorn_to_manticore ( self ) : self . write_backs_disabled = True for reg in self . registers : val = self . _emu . reg_read ( self . _to_unicorn_id ( reg ) ) self . _cpu . write_register ( reg , val ) if len ( self . _mem_delta ) > 0 : logger . debug ( f"Syncing {len(self._mem_delta)} writes back into Manticore" ) for location in self . _mem_delta : value , size = self . _mem_delta [ location ] self . _cpu . write_int ( location , value , size * 8 ) self . write_backs_disabled = False self . _mem_delta = { }
2442	def add_annotation_comment ( self , doc , comment ) : if len ( doc . annotations ) != 0 : if not self . annotation_comment_set : self . annotation_comment_set = True if validations . validate_annotation_comment ( comment ) : doc . annotations [ - 1 ] . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'AnnotationComment::Comment' ) else : raise CardinalityError ( 'AnnotationComment::Comment' ) else : raise OrderError ( 'AnnotationComment::Comment' )
3592	def encryptPassword ( self , login , passwd ) : binaryKey = b64decode ( config . GOOGLE_PUBKEY ) i = utils . readInt ( binaryKey , 0 ) modulus = utils . toBigInt ( binaryKey [ 4 : ] [ 0 : i ] ) j = utils . readInt ( binaryKey , i + 4 ) exponent = utils . toBigInt ( binaryKey [ i + 8 : ] [ 0 : j ] ) digest = hashes . Hash ( hashes . SHA1 ( ) , backend = default_backend ( ) ) digest . update ( binaryKey ) h = b'\x00' + digest . finalize ( ) [ 0 : 4 ] der_data = encode_dss_signature ( modulus , exponent ) publicKey = load_der_public_key ( der_data , backend = default_backend ( ) ) to_be_encrypted = login . encode ( ) + b'\x00' + passwd . encode ( ) ciphertext = publicKey . encrypt ( to_be_encrypted , padding . OAEP ( mgf = padding . MGF1 ( algorithm = hashes . SHA1 ( ) ) , algorithm = hashes . SHA1 ( ) , label = None ) ) return urlsafe_b64encode ( h + ciphertext )
7000	def _runpf_worker ( task ) : ( lcfile , outdir , timecols , magcols , errcols , lcformat , lcformatdir , pfmethods , pfkwargs , getblssnr , sigclip , nworkers , minobservations , excludeprocessed ) = task if os . path . exists ( lcfile ) : pfresult = runpf ( lcfile , outdir , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = lcformatdir , pfmethods = pfmethods , pfkwargs = pfkwargs , getblssnr = getblssnr , sigclip = sigclip , nworkers = nworkers , minobservations = minobservations , excludeprocessed = excludeprocessed ) return pfresult else : LOGERROR ( 'LC does not exist for requested file %s' % lcfile ) return None
6495	def log_indexing_error ( cls , indexing_errors ) : indexing_errors_log = [ ] for indexing_error in indexing_errors : indexing_errors_log . append ( str ( indexing_error ) ) raise exceptions . ElasticsearchException ( ', ' . join ( indexing_errors_log ) )
7669	def slice ( self , start_time , end_time , strict = False ) : sliced_array = AnnotationArray ( ) for ann in self : sliced_array . append ( ann . slice ( start_time , end_time , strict = strict ) ) return sliced_array
1998	def visit ( self , node , use_fixed_point = False ) : cache = self . _cache visited = set ( ) stack = [ ] stack . append ( node ) while stack : node = stack . pop ( ) if node in cache : self . push ( cache [ node ] ) elif isinstance ( node , Operation ) : if node in visited : operands = [ self . pop ( ) for _ in range ( len ( node . operands ) ) ] value = self . _method ( node , * operands ) visited . remove ( node ) self . push ( value ) cache [ node ] = value else : visited . add ( node ) stack . append ( node ) stack . extend ( node . operands ) else : self . push ( self . _method ( node ) ) if use_fixed_point : old_value = None new_value = self . pop ( ) while old_value is not new_value : self . visit ( new_value ) old_value = new_value new_value = self . pop ( ) self . push ( new_value )
3722	def calculate ( self , T , method ) : r if method == CRC : A , B , C , D = self . CRC_coeffs epsilon = A + B * T + C * T ** 2 + D * T ** 3 elif method == CRC_CONSTANT : epsilon = self . CRC_permittivity elif method in self . tabular_data : epsilon = self . interpolate ( T , method ) return epsilon
10192	def get_geoip ( ip ) : reader = geolite2 . reader ( ) ip_data = reader . get ( ip ) or { } return ip_data . get ( 'country' , { } ) . get ( 'iso_code' )
8333	def findPrevious ( self , name = None , attrs = { } , text = None , ** kwargs ) : return self . _findOne ( self . findAllPrevious , name , attrs , text , ** kwargs )
2126	def data_endpoint ( cls , in_data , ignore = [ ] ) : obj , obj_type , res , res_type = cls . obj_res ( in_data , fail_on = [ ] ) data = { } if 'obj' in ignore : obj = None if 'res' in ignore : res = None if obj and obj_type == 'user' : data [ 'members__in' ] = obj if obj and obj_type == 'team' : endpoint = '%s/%s/roles/' % ( grammar . pluralize ( obj_type ) , obj ) if res is not None : data [ 'object_id' ] = res elif res : endpoint = '%s/%s/object_roles/' % ( grammar . pluralize ( res_type ) , res ) else : endpoint = '/roles/' if in_data . get ( 'type' , False ) : data [ 'role_field' ] = '%s_role' % in_data [ 'type' ] . lower ( ) for key , value in in_data . items ( ) : if key not in RESOURCE_FIELDS and key not in [ 'type' , 'user' , 'team' ] : data [ key ] = value return data , endpoint
2898	def get_task ( self , id ) : tasks = [ task for task in self . get_tasks ( ) if task . id == id ] return tasks [ 0 ] if len ( tasks ) == 1 else None
1503	def template_uploader_yaml ( cl_args , masters ) : single_master = masters [ 0 ] uploader_config_template = "%s/standalone/templates/uploader.template.yaml" % cl_args [ "config_path" ] uploader_config_actual = "%s/standalone/uploader.yaml" % cl_args [ "config_path" ] template_file ( uploader_config_template , uploader_config_actual , { "<http_uploader_uri>" : "http://%s:9000/api/v1/file/upload" % single_master } )
3478	def build_reaction_from_string ( self , reaction_str , verbose = True , fwd_arrow = None , rev_arrow = None , reversible_arrow = None , term_split = "+" ) : forward_arrow_finder = _forward_arrow_finder if fwd_arrow is None else re . compile ( re . escape ( fwd_arrow ) ) reverse_arrow_finder = _reverse_arrow_finder if rev_arrow is None else re . compile ( re . escape ( rev_arrow ) ) reversible_arrow_finder = _reversible_arrow_finder if reversible_arrow is None else re . compile ( re . escape ( reversible_arrow ) ) if self . _model is None : warn ( "no model found" ) model = None else : model = self . _model found_compartments = compartment_finder . findall ( reaction_str ) if len ( found_compartments ) == 1 : compartment = found_compartments [ 0 ] reaction_str = compartment_finder . sub ( "" , reaction_str ) else : compartment = "" arrow_match = reversible_arrow_finder . search ( reaction_str ) if arrow_match is not None : self . lower_bound = - 1000 self . upper_bound = 1000 else : arrow_match = forward_arrow_finder . search ( reaction_str ) if arrow_match is not None : self . upper_bound = 1000 self . lower_bound = 0 else : arrow_match = reverse_arrow_finder . search ( reaction_str ) if arrow_match is None : raise ValueError ( "no suitable arrow found in '%s'" % reaction_str ) else : self . upper_bound = 0 self . lower_bound = - 1000 reactant_str = reaction_str [ : arrow_match . start ( ) ] . strip ( ) product_str = reaction_str [ arrow_match . end ( ) : ] . strip ( ) self . subtract_metabolites ( self . metabolites , combine = True ) for substr , factor in ( ( reactant_str , - 1 ) , ( product_str , 1 ) ) : if len ( substr ) == 0 : continue for term in substr . split ( term_split ) : term = term . strip ( ) if term . lower ( ) == "nothing" : continue if " " in term : num_str , met_id = term . split ( ) num = float ( num_str . lstrip ( "(" ) . rstrip ( ")" ) ) * factor else : met_id = term num = factor met_id += compartment try : met = model . metabolites . get_by_id ( met_id ) except KeyError : if verbose : print ( "unknown metabolite '%s' created" % met_id ) met = Metabolite ( met_id ) self . add_metabolites ( { met : num } )
10996	def schedules ( self ) : url = PATHS [ 'GET_SCHEDULES' ] % self . id self . __schedules = self . api . get ( url = url ) return self . __schedules
4631	def point ( self ) : string = unhexlify ( self . unCompressed ( ) ) return ecdsa . VerifyingKey . from_string ( string [ 1 : ] , curve = ecdsa . SECP256k1 ) . pubkey . point
329	def model_returns_t_alpha_beta ( data , bmark , samples = 2000 , progressbar = True ) : data_bmark = pd . concat ( [ data , bmark ] , axis = 1 ) . dropna ( ) with pm . Model ( ) as model : sigma = pm . HalfCauchy ( 'sigma' , beta = 1 ) nu = pm . Exponential ( 'nu_minus_two' , 1. / 10. ) X = data_bmark . iloc [ : , 1 ] y = data_bmark . iloc [ : , 0 ] alpha_reg = pm . Normal ( 'alpha' , mu = 0 , sd = .1 ) beta_reg = pm . Normal ( 'beta' , mu = 0 , sd = 1 ) mu_reg = alpha_reg + beta_reg * X pm . StudentT ( 'returns' , nu = nu + 2 , mu = mu_reg , sd = sigma , observed = y ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace
1190	def filter ( names , pat ) : import os result = [ ] try : re_pat = _cache [ pat ] except KeyError : res = translate ( pat ) if len ( _cache ) >= _MAXCACHE : globals ( ) [ '_cache' ] = { } _cache [ pat ] = re_pat = re . compile ( res ) match = re_pat . match if 1 : for name in names : if match ( name ) : result . append ( name ) else : for name in names : if match ( os . path . normcase ( name ) ) : result . append ( name ) return result
7399	def swap ( self , qs ) : try : replacement = qs [ 0 ] except IndexError : return if not self . _valid_ordering_reference ( replacement ) : raise ValueError ( "%r can only be swapped with instances of %r which %s equals %r." % ( self , self . __class__ , self . order_with_respect_to , self . _get_order_with_respect_to ( ) ) ) self . order , replacement . order = replacement . order , self . order self . save ( ) replacement . save ( )
5243	def tz_convert ( dt , to_tz , from_tz = None ) -> str : logger = logs . get_logger ( tz_convert , level = 'info' ) f_tz , t_tz = get_tz ( from_tz ) , get_tz ( to_tz ) from_dt = pd . Timestamp ( str ( dt ) , tz = f_tz ) logger . debug ( f'converting {str(from_dt)} from {f_tz} to {t_tz} ...' ) return str ( pd . Timestamp ( str ( from_dt ) , tz = t_tz ) )
2550	def system ( cmd , data = None ) : import subprocess s = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stdin = subprocess . PIPE ) out , err = s . communicate ( data ) return out . decode ( 'utf8' )
6122	def zoom_region ( self ) : where = np . array ( np . where ( np . invert ( self . astype ( 'bool' ) ) ) ) y0 , x0 = np . amin ( where , axis = 1 ) y1 , x1 = np . amax ( where , axis = 1 ) return [ y0 , y1 + 1 , x0 , x1 + 1 ]
10792	def tile_overlap ( inner , outer , norm = False ) : div = 1.0 / inner . volume if norm else 1.0 return div * ( inner . volume - util . Tile . intersection ( inner , outer ) . volume )
4820	def refresh_token ( func ) : @ wraps ( func ) def inner ( self , * args , ** kwargs ) : if self . token_expired ( ) : self . connect ( ) return func ( self , * args , ** kwargs ) return inner
8934	def auto_detect ( workdir ) : if os . path . isdir ( os . path . join ( workdir , '.git' ) ) and os . path . isfile ( os . path . join ( workdir , '.git' , 'HEAD' ) ) : return 'git' return 'unknown'
11618	def detect ( text ) : if sys . version_info < ( 3 , 0 ) : try : text = text . decode ( 'utf-8' ) except UnicodeError : pass for L in text : code = ord ( L ) if code >= BRAHMIC_FIRST_CODE_POINT : for name , start_code in BLOCKS : if start_code <= code <= BRAHMIC_LAST_CODE_POINT : return name if Regex . IAST_OR_KOLKATA_ONLY . search ( text ) : if Regex . KOLKATA_ONLY . search ( text ) : return Scheme . Kolkata else : return Scheme . IAST if Regex . ITRANS_ONLY . search ( text ) : return Scheme . ITRANS if Regex . SLP1_ONLY . search ( text ) : return Scheme . SLP1 if Regex . VELTHUIS_ONLY . search ( text ) : return Scheme . Velthuis if Regex . ITRANS_OR_VELTHUIS_ONLY . search ( text ) : return Scheme . ITRANS return Scheme . HK
2255	def unique_flags ( items , key = None ) : len_ = len ( items ) if key is None : item_to_index = dict ( zip ( reversed ( items ) , reversed ( range ( len_ ) ) ) ) indices = item_to_index . values ( ) else : indices = argunique ( items , key = key ) flags = boolmask ( indices , len_ ) return flags
10431	def selectrowpartialmatch ( self , window_name , object_name , row_text ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) for cell in object_handle . AXRows : if re . search ( row_text , cell . AXChildren [ 0 ] . AXValue ) : if not cell . AXSelected : object_handle . activate ( ) cell . AXSelected = True else : pass return 1 raise LdtpServerException ( u"Unable to select row: %s" % row_text )
9957	def setup_ipython ( self ) : if self . is_ipysetup : return from ipykernel . kernelapp import IPKernelApp self . shell = IPKernelApp . instance ( ) . shell if not self . shell and is_ipython ( ) : self . shell = get_ipython ( ) if self . shell : shell_class = type ( self . shell ) shell_class . default_showtraceback = shell_class . showtraceback shell_class . showtraceback = custom_showtraceback self . is_ipysetup = True else : raise RuntimeError ( "IPython shell not found." )
11598	def prepare ( self ) : attributes , elements = OrderedDict ( ) , [ ] nsmap = dict ( [ self . meta . namespace ] ) for name , item in self . _items . items ( ) : if isinstance ( item , Attribute ) : attributes [ name ] = item . prepare ( self ) elif isinstance ( item , Element ) : nsmap . update ( [ item . namespace ] ) elements . append ( item ) return attributes , elements , nsmap
1345	def predictions ( self , image ) : return np . squeeze ( self . batch_predictions ( image [ np . newaxis ] ) , axis = 0 )
8970	def step ( self , other_pub ) : if self . triggersStep ( other_pub ) : self . __wrapOtherPub ( other_pub ) self . __newRootKey ( "receiving" ) self . __newRatchetKey ( ) self . __newRootKey ( "sending" )
9829	def edges ( self ) : return [ self . delta [ d , d ] * numpy . arange ( self . shape [ d ] + 1 ) + self . origin [ d ] - 0.5 * self . delta [ d , d ] for d in range ( self . rank ) ]
2039	def CALLCODE ( self , gas , _ignored_ , value , in_offset , in_size , out_offset , out_size ) : self . world . start_transaction ( 'CALLCODE' , address = self . address , data = self . read_buffer ( in_offset , in_size ) , caller = self . address , value = value , gas = gas ) raise StartTx ( )
5940	def _build_arg_list ( self , ** kwargs ) : arglist = [ ] for flag , value in kwargs . items ( ) : flag = str ( flag ) if flag . startswith ( '_' ) : flag = flag [ 1 : ] if not flag . startswith ( '-' ) : flag = '-' + flag if value is True : arglist . append ( flag ) elif value is False : if flag . startswith ( '-no' ) : arglist . append ( '-' + flag [ 3 : ] ) else : arglist . append ( '-no' + flag [ 1 : ] ) elif value is None : pass else : try : arglist . extend ( [ flag ] + value ) except TypeError : arglist . extend ( [ flag , value ] ) return list ( map ( str , arglist ) )
3141	def get ( self , workflow_id , email_id ) : self . workflow_id = workflow_id self . email_id = email_id return self . _mc_client . _get ( url = self . _build_path ( workflow_id , 'emails' , email_id ) )
11438	def _create_record_lxml ( marcxml , verbose = CFG_BIBRECORD_DEFAULT_VERBOSE_LEVEL , correct = CFG_BIBRECORD_DEFAULT_CORRECT , keep_singletons = CFG_BIBRECORD_KEEP_SINGLETONS ) : parser = etree . XMLParser ( dtd_validation = correct , recover = ( verbose <= 3 ) ) if correct : marcxml = '<?xml version="1.0" encoding="UTF-8"?>\n' '<collection>\n%s\n</collection>' % ( marcxml , ) try : tree = etree . parse ( StringIO ( marcxml ) , parser ) except Exception as e : raise InvenioBibRecordParserError ( str ( e ) ) record = { } field_position_global = 0 controlfield_iterator = tree . iter ( tag = '{*}controlfield' ) for controlfield in controlfield_iterator : tag = controlfield . attrib . get ( 'tag' , '!' ) . encode ( "UTF-8" ) ind1 = ' ' ind2 = ' ' text = controlfield . text if text is None : text = '' else : text = text . encode ( "UTF-8" ) subfields = [ ] if text or keep_singletons : field_position_global += 1 record . setdefault ( tag , [ ] ) . append ( ( subfields , ind1 , ind2 , text , field_position_global ) ) datafield_iterator = tree . iter ( tag = '{*}datafield' ) for datafield in datafield_iterator : tag = datafield . attrib . get ( 'tag' , '!' ) . encode ( "UTF-8" ) ind1 = datafield . attrib . get ( 'ind1' , '!' ) . encode ( "UTF-8" ) ind2 = datafield . attrib . get ( 'ind2' , '!' ) . encode ( "UTF-8" ) if ind1 in ( '' , '_' ) : ind1 = ' ' if ind2 in ( '' , '_' ) : ind2 = ' ' subfields = [ ] subfield_iterator = datafield . iter ( tag = '{*}subfield' ) for subfield in subfield_iterator : code = subfield . attrib . get ( 'code' , '!' ) . encode ( "UTF-8" ) text = subfield . text if text is None : text = '' else : text = text . encode ( "UTF-8" ) if text or keep_singletons : subfields . append ( ( code , text ) ) if subfields or keep_singletons : text = '' field_position_global += 1 record . setdefault ( tag , [ ] ) . append ( ( subfields , ind1 , ind2 , text , field_position_global ) ) return record
8264	def swarm ( self , x , y , r = 100 ) : sc = _ctx . stroke ( 0 , 0 , 0 , 0 ) sw = _ctx . strokewidth ( 0 ) _ctx . push ( ) _ctx . transform ( _ctx . CORNER ) _ctx . translate ( x , y ) for i in _range ( r * 3 ) : clr = choice ( self ) . copy ( ) clr . alpha -= 0.5 * random ( ) _ctx . fill ( clr ) clr = choice ( self ) _ctx . stroke ( clr ) _ctx . strokewidth ( 10 * random ( ) ) _ctx . rotate ( 360 * random ( ) ) r2 = r * 0.5 * random ( ) _ctx . oval ( r * random ( ) , 0 , r2 , r2 ) _ctx . pop ( ) _ctx . strokewidth ( sw ) if sc is None : _ctx . nostroke ( ) else : _ctx . stroke ( sc )
12417	def replaced_directory ( dirname ) : if dirname [ - 1 ] == '/' : dirname = dirname [ : - 1 ] full_path = os . path . abspath ( dirname ) if not os . path . isdir ( full_path ) : raise AttributeError ( 'dir_name must be a directory' ) base , name = os . path . split ( full_path ) tempdir = tempfile . mkdtemp ( ) shutil . move ( full_path , tempdir ) os . mkdir ( full_path ) try : yield tempdir finally : shutil . rmtree ( full_path ) moved = os . path . join ( tempdir , name ) shutil . move ( moved , base ) shutil . rmtree ( tempdir )
8976	def file ( self , file = None ) : if file is None : file = StringIO ( ) self . _file ( file ) return file
7257	def get_address_coords ( self , address ) : url = "https://maps.googleapis.com/maps/api/geocode/json?&address=" + address r = requests . get ( url ) r . raise_for_status ( ) results = r . json ( ) [ 'results' ] lat = results [ 0 ] [ 'geometry' ] [ 'location' ] [ 'lat' ] lng = results [ 0 ] [ 'geometry' ] [ 'location' ] [ 'lng' ] return lat , lng
8573	def delete_nic ( self , datacenter_id , server_id , nic_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s' % ( datacenter_id , server_id , nic_id ) , method = 'DELETE' ) return response
2806	def convert_elementwise_sub ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting elementwise_sub ...' ) model0 = layers [ inputs [ 0 ] ] model1 = layers [ inputs [ 1 ] ] if names == 'short' : tf_name = 'S' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) sub = keras . layers . Subtract ( name = tf_name ) layers [ scope_name ] = sub ( [ model0 , model1 ] )
7731	def make_join_request ( self , password = None , history_maxchars = None , history_maxstanzas = None , history_seconds = None , history_since = None ) : self . clear_muc_child ( ) self . muc_child = MucX ( parent = self . xmlnode ) if ( history_maxchars is not None or history_maxstanzas is not None or history_seconds is not None or history_since is not None ) : history = HistoryParameters ( history_maxchars , history_maxstanzas , history_seconds , history_since ) self . muc_child . set_history ( history ) if password is not None : self . muc_child . set_password ( password )
13503	def create ( self , server ) : return server . post ( 'challenge_admin' , self . as_payload ( ) , replacements = { 'slug' : self . slug } )
2641	def shutdown ( self ) : self . is_alive = False logging . debug ( "Waking management thread" ) self . incoming_q . put ( None ) self . _queue_management_thread . join ( ) logging . debug ( "Exiting thread" ) self . worker . join ( ) return True
1309	def _CreateInput ( structure ) -> INPUT : if isinstance ( structure , MOUSEINPUT ) : return INPUT ( InputType . Mouse , _INPUTUnion ( mi = structure ) ) if isinstance ( structure , KEYBDINPUT ) : return INPUT ( InputType . Keyboard , _INPUTUnion ( ki = structure ) ) if isinstance ( structure , HARDWAREINPUT ) : return INPUT ( InputType . Hardware , _INPUTUnion ( hi = structure ) ) raise TypeError ( 'Cannot create INPUT structure!' )
2008	def _deserialize_int ( data , nbytes = 32 , padding = 0 ) : assert isinstance ( data , ( bytearray , Array ) ) value = ABI . _readBE ( data , nbytes , padding = True ) value = Operators . SEXTEND ( value , nbytes * 8 , ( nbytes + padding ) * 8 ) if not issymbolic ( value ) : if value & ( 1 << ( nbytes * 8 - 1 ) ) : value = - ( ( ( ~ value ) + 1 ) & ( ( 1 << ( nbytes * 8 ) ) - 1 ) ) return value
13651	def get_reference_data ( self , modified_since : Optional [ datetime . datetime ] = None ) -> GetReferenceDataResponse : if modified_since is None : modified_since = datetime . datetime ( year = 2010 , month = 1 , day = 1 ) response = requests . get ( '{}/lovs' . format ( API_URL_BASE ) , headers = { 'if-modified-since' : self . _format_dt ( modified_since ) , ** self . _get_headers ( ) , } , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) return GetReferenceDataResponse . deserialize ( response . json ( ) )
12043	def algo_exp ( x , m , t , b ) : return m * np . exp ( - t * x ) + b
8702	def prepare ( self ) : log . info ( 'Preparing esp for transfer.' ) for func in LUA_FUNCTIONS : detected = self . __exchange ( 'print({0})' . format ( func ) ) if detected . find ( 'function:' ) == - 1 : break else : log . info ( 'Preparation already done. Not adding functions again.' ) return True functions = RECV_LUA + '\n' + SEND_LUA data = functions . format ( baud = self . _port . baudrate ) lines = data . replace ( '\r' , '' ) . split ( '\n' ) for line in lines : line = line . strip ( ) . replace ( ', ' , ',' ) . replace ( ' = ' , '=' ) if len ( line ) == 0 : continue resp = self . __exchange ( line ) if ( 'unexpected' in resp ) or ( 'stdin' in resp ) or len ( resp ) > len ( functions ) + 10 : log . error ( 'error when preparing "%s"' , resp ) return False return True
4455	def apply ( self , ** kwexpr ) : for alias , expr in kwexpr . items ( ) : self . _projections . append ( [ alias , expr ] ) return self
9611	def _request ( self , method , url , body ) : if method != 'POST' and method != 'PUT' : body = None s = Session ( ) LOGGER . debug ( 'Method: {0}, Url: {1}, Body: {2}.' . format ( method , url , body ) ) req = Request ( method , url , json = body ) prepped = s . prepare_request ( req ) res = s . send ( prepped , timeout = self . _timeout or None ) res . raise_for_status ( ) return res . json ( )
4518	def fillTriangle ( self , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : md . fill_triangle ( self . set , x0 , y0 , x1 , y1 , x2 , y2 , color , aa )
13421	def map ( self , ID_s , FROM = None , TO = None , target_as_set = False , no_match_sub = None ) : def io_mode ( ID_s ) : unlist_return = False list_of_lists = False if isinstance ( ID_s , str ) : ID_s = [ ID_s ] unlist_return = True elif isinstance ( ID_s , list ) : if len ( ID_s ) > 0 and isinstance ( ID_s [ 0 ] , list ) : list_of_lists = True return ID_s , unlist_return , list_of_lists if FROM == TO : return ID_s ID_s , unlist_return , list_of_lists = io_mode ( ID_s ) if list_of_lists : mapped_ids = [ self . map ( ID , FROM , TO , target_as_set , no_match_sub ) for ID in ID_s ] else : mapped_ids = self . _map ( ID_s , FROM , TO , target_as_set , no_match_sub ) if unlist_return : return mapped_ids [ 0 ] return Mapping ( ID_s , mapped_ids )
4658	def as_base ( self , base ) : if base == self [ "base" ] [ "symbol" ] : return self . copy ( ) elif base == self [ "quote" ] [ "symbol" ] : return self . copy ( ) . invert ( ) else : raise InvalidAssetException
9869	def cleanup_none ( self ) : for ( prop , default ) in self . defaults . items ( ) : if getattr ( self , prop ) == '_None' : setattr ( self , prop , None )
2274	def _win32_rmtree ( path , verbose = 0 ) : def _rmjunctions ( root ) : subdirs = [ ] for name in os . listdir ( root ) : current = join ( root , name ) if os . path . isdir ( current ) : if _win32_is_junction ( current ) : os . rmdir ( current ) elif not os . path . islink ( current ) : subdirs . append ( current ) for subdir in subdirs : _rmjunctions ( subdir ) if _win32_is_junction ( path ) : if verbose : print ( 'Deleting <JUNCTION> directory="{}"' . format ( path ) ) os . rmdir ( path ) else : if verbose : print ( 'Deleting directory="{}"' . format ( path ) ) _rmjunctions ( path ) import shutil shutil . rmtree ( path )
2457	def set_pkg_license_declared ( self , doc , lic ) : self . assert_package_exists ( ) if not self . package_license_declared_set : self . package_license_declared_set = True if validations . validate_lics_conc ( lic ) : doc . package . license_declared = lic return True else : raise SPDXValueError ( 'Package::LicenseDeclared' ) else : raise CardinalityError ( 'Package::LicenseDeclared' )
6908	def galactic_to_equatorial ( gl , gb ) : gal = SkyCoord ( gl * u . degree , gl * u . degree , frame = 'galactic' ) transformed = gal . transform_to ( 'icrs' ) return transformed . ra . degree , transformed . dec . degree
3885	def get_user ( self , user_id ) : try : return self . _user_dict [ user_id ] except KeyError : logger . warning ( 'UserList returning unknown User for UserID %s' , user_id ) return User ( user_id , None , None , None , [ ] , False )
8903	def add_syncable_models ( ) : import django . apps from morango . models import SyncableModel from morango . manager import SyncableModelManager from morango . query import SyncableModelQuerySet model_list = [ ] for model_class in django . apps . apps . get_models ( ) : if issubclass ( model_class , SyncableModel ) : name = model_class . __name__ if _multiple_self_ref_fk_check ( model_class ) : raise InvalidMorangoModelConfiguration ( "Syncing models with more than 1 self referential ForeignKey is not supported." ) try : from mptt import models from morango . utils . morango_mptt import MorangoMPTTModel , MorangoMPTTTreeManager , MorangoTreeQuerySet if issubclass ( model_class , models . MPTTModel ) : if not issubclass ( model_class , MorangoMPTTModel ) : raise InvalidMorangoModelConfiguration ( "{} that inherits from MPTTModel, should instead inherit from MorangoMPTTModel." . format ( name ) ) if not isinstance ( model_class . objects , MorangoMPTTTreeManager ) : raise InvalidMPTTManager ( "Manager for {} must inherit from MorangoMPTTTreeManager." . format ( name ) ) if not isinstance ( model_class . objects . none ( ) , MorangoTreeQuerySet ) : raise InvalidMPTTQuerySet ( "Queryset for {} model must inherit from MorangoTreeQuerySet." . format ( name ) ) except ImportError : pass if not isinstance ( model_class . objects , SyncableModelManager ) : raise InvalidSyncableManager ( "Manager for {} must inherit from SyncableModelManager." . format ( name ) ) if not isinstance ( model_class . objects . none ( ) , SyncableModelQuerySet ) : raise InvalidSyncableQueryset ( "Queryset for {} model must inherit from SyncableModelQuerySet." . format ( name ) ) if model_class . _meta . many_to_many : raise UnsupportedFieldType ( "{} model with a ManyToManyField is not supported in morango." ) if not hasattr ( model_class , 'morango_model_name' ) : raise InvalidMorangoModelConfiguration ( "{} model must define a morango_model_name attribute" . format ( name ) ) if not hasattr ( model_class , 'morango_profile' ) : raise InvalidMorangoModelConfiguration ( "{} model must define a morango_profile attribute" . format ( name ) ) profile = model_class . morango_profile _profile_models [ profile ] = _profile_models . get ( profile , [ ] ) if model_class . morango_model_name is not None : _insert_model_into_profile_dict ( model_class , profile ) for profile , model_list in iteritems ( _profile_models ) : syncable_models_dict = OrderedDict ( ) for model_class in model_list : syncable_models_dict [ model_class . morango_model_name ] = model_class _profile_models [ profile ] = syncable_models_dict
4283	def generate_video ( source , outname , settings , options = None ) : logger = logging . getLogger ( __name__ ) converter = settings [ 'video_converter' ] w_src , h_src = video_size ( source , converter = converter ) w_dst , h_dst = settings [ 'video_size' ] logger . debug ( 'Video size: %i, %i -> %i, %i' , w_src , h_src , w_dst , h_dst ) base , src_ext = splitext ( source ) base , dst_ext = splitext ( outname ) if dst_ext == src_ext and w_src <= w_dst and h_src <= h_dst : logger . debug ( 'Video is smaller than the max size, copying it instead' ) shutil . copy ( source , outname ) return if h_dst * w_src < h_src * w_dst : resize_opt = [ '-vf' , "scale=trunc(oh*a/2)*2:%i" % h_dst ] else : resize_opt = [ '-vf' , "scale=%i:trunc(ow/a/2)*2" % w_dst ] if w_src <= w_dst and h_src <= h_dst : resize_opt = [ ] cmd = [ converter , '-i' , source , '-y' ] if options is not None : cmd += options cmd += resize_opt + [ outname ] logger . debug ( 'Processing video: %s' , ' ' . join ( cmd ) ) check_subprocess ( cmd , source , outname )
4865	def validate_username ( self , value ) : try : self . user = User . objects . get ( username = value ) except User . DoesNotExist : raise serializers . ValidationError ( "User does not exist" ) return value
12236	def objective ( param_scales = ( 1 , 1 ) , xstar = None , seed = None ) : ndim = len ( param_scales ) def decorator ( func ) : @ wraps ( func ) def wrapper ( theta ) : return func ( theta ) def param_init ( ) : np . random . seed ( seed ) return np . random . randn ( ndim , ) * np . array ( param_scales ) wrapper . ndim = ndim wrapper . param_init = param_init wrapper . xstar = xstar return wrapper return decorator
5426	def _group_tasks_by_jobid ( tasks ) : ret = collections . defaultdict ( list ) for t in tasks : ret [ t . get_field ( 'job-id' ) ] . append ( t ) return ret
13013	def strip_labels ( filename ) : labels = [ ] with open ( filename ) as f , open ( 'processed_labels.txt' , 'w' ) as f1 : for l in f : if l . startswith ( '#' ) : next l = l . replace ( " ." , '' ) l = l . replace ( ">\tskos:prefLabel\t" , ' ' ) l = l . replace ( "<" , '' ) l = l . replace ( ">\trdfs:label\t" , ' ' ) f1 . write ( l )
11530	def __get_rev ( self , key , version , ** kwa ) : if '_doc' in kwa : doc = kwa [ '_doc' ] else : if type ( version ) is int : if version == 0 : order = pymongo . ASCENDING elif version == - 1 : order = pymongo . DESCENDING doc = self . _collection . find_one ( { 'k' : key } , sort = [ [ 'd' , order ] ] ) elif type ( version ) is datetime : ver = self . __round_time ( version ) doc = self . _collection . find_one ( { 'k' : key , 'd' : ver } ) if doc is None : raise KeyError ( 'Supplied key `{0}` or version `{1}` does not exist' . format ( key , str ( version ) ) ) coded_val = doc [ 'v' ] return pickle . loads ( coded_val )
32	def wrap_deepmind ( env , episode_life = True , clip_rewards = True , frame_stack = False , scale = False ) : if episode_life : env = EpisodicLifeEnv ( env ) if 'FIRE' in env . unwrapped . get_action_meanings ( ) : env = FireResetEnv ( env ) env = WarpFrame ( env ) if scale : env = ScaledFloatFrame ( env ) if clip_rewards : env = ClipRewardEnv ( env ) if frame_stack : env = FrameStack ( env , 4 ) return env
5574	def available_output_formats ( ) : output_formats = [ ] for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : driver_ = v . load ( ) if hasattr ( driver_ , "METADATA" ) and ( driver_ . METADATA [ "mode" ] in [ "w" , "rw" ] ) : output_formats . append ( driver_ . METADATA [ "driver_name" ] ) return output_formats
10935	def check_update_J ( self ) : self . _J_update_counter += 1 update = self . _J_update_counter >= self . update_J_frequency return update & ( not self . _fresh_JTJ )
12846	def generate ( request ) : models . DataItem . create ( content = '' . join ( random . choice ( string . ascii_uppercase + string . digits ) for _ in range ( 20 ) ) ) return muffin . HTTPFound ( '/' )
6171	def freq_resp ( self , mode = 'dB' , fs = 8000 , ylim = [ - 100 , 2 ] ) : iir_d . freqz_resp_cas_list ( [ self . sos ] , mode , fs = fs ) pylab . grid ( ) pylab . ylim ( ylim )
3065	def _add_query_parameter ( url , name , value ) : if value is None : return url else : return update_query_params ( url , { name : value } )
9413	def _setup_log ( ) : try : handler = logging . StreamHandler ( stream = sys . stdout ) except TypeError : handler = logging . StreamHandler ( strm = sys . stdout ) log = get_log ( ) log . addHandler ( handler ) log . setLevel ( logging . INFO ) log . propagate = False
7252	def order ( self , image_catalog_ids , batch_size = 100 , callback = None ) : def _order_single_batch ( url_ , ids , results_list ) : data = json . dumps ( ids ) if callback is None else json . dumps ( { "acquisitionIds" : ids , "callback" : callback } ) r = self . gbdx_connection . post ( url_ , data = data ) r . raise_for_status ( ) order_id = r . json ( ) . get ( "order_id" ) if order_id : results_list . append ( order_id ) self . logger . debug ( 'Place order' ) url = ( '%s/order' if callback is None else '%s/ordercb' ) % self . base_url batch_size = min ( 100 , batch_size ) if not isinstance ( image_catalog_ids , list ) : image_catalog_ids = [ image_catalog_ids ] sanitized_ids = list ( set ( ( id for id in ( _id . strip ( ) for _id in image_catalog_ids ) if id ) ) ) res = [ ] acq_ids_by_batch = zip ( * ( [ iter ( sanitized_ids ) ] * batch_size ) ) for ids_batch in acq_ids_by_batch : _order_single_batch ( url , ids_batch , res ) remain_count = len ( sanitized_ids ) % batch_size if remain_count > 0 : _order_single_batch ( url , sanitized_ids [ - remain_count : ] , res ) if len ( res ) == 1 : return res [ 0 ] elif len ( res ) > 1 : return res
13662	def atomic_write ( filename ) : f = _tempfile ( os . fsencode ( filename ) ) try : yield f finally : f . close ( ) os . replace ( f . name , filename )
3529	def get_user_from_context ( context ) : try : return context [ 'user' ] except KeyError : pass try : request = context [ 'request' ] return request . user except ( KeyError , AttributeError ) : pass return None
1815	def SETNLE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . AND ( cpu . ZF == False , cpu . SF == cpu . OF ) , 1 , 0 ) )
10824	def query_by_group ( cls , group_or_id , with_invitations = False , ** kwargs ) : if isinstance ( group_or_id , Group ) : id_group = group_or_id . id else : id_group = group_or_id if not with_invitations : return cls . _filter ( cls . query . filter_by ( id_group = id_group ) , ** kwargs ) else : return cls . query . filter ( Membership . id_group == id_group , db . or_ ( Membership . state == MembershipState . PENDING_USER , Membership . state == MembershipState . ACTIVE ) )
5421	def _get_job_metadata ( provider , user_id , job_name , script , task_ids , user_project , unique_job_id ) : create_time = dsub_util . replace_timezone ( datetime . datetime . now ( ) , tzlocal ( ) ) user_id = user_id or dsub_util . get_os_user ( ) job_metadata = provider . prepare_job_metadata ( script . name , job_name , user_id , create_time ) if unique_job_id : job_metadata [ 'job-id' ] = uuid . uuid4 ( ) . hex job_metadata [ 'create-time' ] = create_time job_metadata [ 'script' ] = script job_metadata [ 'user-project' ] = user_project if task_ids : job_metadata [ 'task-ids' ] = dsub_util . compact_interval_string ( list ( task_ids ) ) return job_metadata
4315	def validate_input_file_list ( input_filepath_list ) : if not isinstance ( input_filepath_list , list ) : raise TypeError ( "input_filepath_list must be a list." ) elif len ( input_filepath_list ) < 2 : raise ValueError ( "input_filepath_list must have at least 2 files." ) for input_filepath in input_filepath_list : validate_input_file ( input_filepath )
7132	def prepare_docset ( source , dest , name , index_page , enable_js , online_redirect_url ) : resources = os . path . join ( dest , "Contents" , "Resources" ) docs = os . path . join ( resources , "Documents" ) os . makedirs ( resources ) db_conn = sqlite3 . connect ( os . path . join ( resources , "docSet.dsidx" ) ) db_conn . row_factory = sqlite3 . Row db_conn . execute ( "CREATE TABLE searchIndex(id INTEGER PRIMARY KEY, name TEXT, " "type TEXT, path TEXT)" ) db_conn . commit ( ) plist_path = os . path . join ( dest , "Contents" , "Info.plist" ) plist_cfg = { "CFBundleIdentifier" : name , "CFBundleName" : name , "DocSetPlatformFamily" : name . lower ( ) , "DashDocSetFamily" : "python" , "isDashDocset" : True , "isJavaScriptEnabled" : enable_js , } if index_page is not None : plist_cfg [ "dashIndexFilePath" ] = index_page if online_redirect_url is not None : plist_cfg [ "DashDocSetFallbackURL" ] = online_redirect_url write_plist ( plist_cfg , plist_path ) shutil . copytree ( source , docs ) return DocSet ( path = dest , docs = docs , plist = plist_path , db_conn = db_conn )
11204	def valuestodict ( key ) : dout = { } size = winreg . QueryInfoKey ( key ) [ 1 ] tz_res = None for i in range ( size ) : key_name , value , dtype = winreg . EnumValue ( key , i ) if dtype == winreg . REG_DWORD or dtype == winreg . REG_DWORD_LITTLE_ENDIAN : if value & ( 1 << 31 ) : value = value - ( 1 << 32 ) elif dtype == winreg . REG_SZ : if value . startswith ( '@tzres' ) : tz_res = tz_res or tzres ( ) value = tz_res . name_from_string ( value ) value = value . rstrip ( '\x00' ) dout [ key_name ] = value return dout
3077	def has_credentials ( self ) : if not self . credentials : return False elif ( self . credentials . access_token_expired and not self . credentials . refresh_token ) : return False else : return True
3831	async def search_entities ( self , search_entities_request ) : response = hangouts_pb2 . SearchEntitiesResponse ( ) await self . _pb_request ( 'contacts/searchentities' , search_entities_request , response ) return response
13129	def initialize_indices ( ) : Host . init ( ) Range . init ( ) Service . init ( ) User . init ( ) Credential . init ( ) Log . init ( )
7370	def permission_check ( data , command_permissions , command = None , permissions = None ) : if permissions : pass elif command : if hasattr ( command , 'permissions' ) : permissions = command . permissions else : return True else : msg = "{name} must be called with command or permissions argument" raise RuntimeError ( msg . format ( name = "_permission_check" ) ) return any ( data [ 'sender' ] [ 'id' ] in command_permissions [ permission ] for permission in permissions if permission in command_permissions )
2869	def setup ( self , pin , mode , pull_up_down = PUD_OFF ) : self . rpi_gpio . setup ( pin , self . _dir_mapping [ mode ] , pull_up_down = self . _pud_mapping [ pull_up_down ] )
2038	def SWAP ( self , * operands ) : a = operands [ 0 ] b = operands [ - 1 ] return ( b , ) + operands [ 1 : - 1 ] + ( a , )
13225	async def process_ltd_doc_products ( session , product_urls , github_api_token , mongo_collection = None ) : tasks = [ asyncio . ensure_future ( process_ltd_doc ( session , github_api_token , product_url , mongo_collection = mongo_collection ) ) for product_url in product_urls ] await asyncio . gather ( * tasks )
9096	def upload_bel_namespace ( self , update : bool = False ) -> Namespace : if not self . is_populated ( ) : self . populate ( ) namespace = self . _get_default_namespace ( ) if namespace is None : log . info ( 'making namespace for %s' , self . _get_namespace_name ( ) ) return self . _make_namespace ( ) if update : self . _update_namespace ( namespace ) return namespace
9756	def update ( ctx , name , description , tags ) : user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the experiment.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . experiment . update_experiment ( user , project_name , _experiment , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiment updated." ) get_experiment_details ( response )
10965	def get ( self ) : fields = [ c . get ( ) for c in self . comps ] return self . field_reduce_func ( fields )
3742	def StielPolar ( Tc = None , Pc = None , omega = None , CASRN = '' , Method = None , AvailableMethods = False ) : r def list_methods ( ) : methods = [ ] if Tc and Pc and omega : methods . append ( 'DEFINITION' ) methods . append ( 'NONE' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'DEFINITION' : P = VaporPressure ( CASRN = CASRN ) . T_dependent_property ( Tc * 0.6 ) if not P : factor = None else : Pr = P / Pc factor = log10 ( Pr ) + 1.70 * omega + 1.552 elif Method == 'NONE' : factor = None else : raise Exception ( 'Failure in in function' ) return factor
5493	def validate_config_key ( ctx , param , value ) : if not value : return value try : section , item = value . split ( "." , 1 ) except ValueError : raise click . BadArgumentUsage ( "Given key does not contain a section name." ) else : return section , item
11215	def compare_token ( expected : Union [ str , bytes ] , actual : Union [ str , bytes ] ) -> bool : expected = util . to_bytes ( expected ) actual = util . to_bytes ( actual ) _ , expected_sig_seg = expected . rsplit ( b'.' , 1 ) _ , actual_sig_seg = actual . rsplit ( b'.' , 1 ) expected_sig = util . b64_decode ( expected_sig_seg ) actual_sig = util . b64_decode ( actual_sig_seg ) return compare_signature ( expected_sig , actual_sig )
3915	def _update ( self ) : typing_users = [ self . _conversation . get_user ( user_id ) for user_id , status in self . _typing_statuses . items ( ) if status == hangups . TYPING_TYPE_STARTED ] displayed_names = [ user . first_name for user in typing_users if not user . is_self ] if displayed_names : typing_message = '{} {} typing...' . format ( ', ' . join ( sorted ( displayed_names ) ) , 'is' if len ( displayed_names ) == 1 else 'are' ) else : typing_message = '' if not self . _is_connected : self . _widget . set_text ( "RECONNECTING..." ) elif self . _message is not None : self . _widget . set_text ( self . _message ) else : self . _widget . set_text ( typing_message )
10655	def run ( self , clock ) : if clock . timestep_ix >= self . period_count : return for c in self . components : c . run ( clock , self . gl ) self . _perform_year_end_procedure ( clock )
13733	def value_to_python_log_level ( config_val , evar ) : if not config_val : config_val = evar . default_val config_val = config_val . upper ( ) return logging . _checkLevel ( config_val )
2648	def make_rundir ( path ) : try : if not os . path . exists ( path ) : os . makedirs ( path ) prev_rundirs = glob ( os . path . join ( path , "[0-9]*" ) ) current_rundir = os . path . join ( path , '000' ) if prev_rundirs : x = sorted ( [ int ( os . path . basename ( x ) ) for x in prev_rundirs ] ) [ - 1 ] current_rundir = os . path . join ( path , '{0:03}' . format ( x + 1 ) ) os . makedirs ( current_rundir ) logger . debug ( "Parsl run initializing in rundir: {0}" . format ( current_rundir ) ) return os . path . abspath ( current_rundir ) except Exception as e : logger . error ( "Failed to create a run directory" ) logger . error ( "Error: {0}" . format ( e ) ) raise
10321	def microcanonical_averages ( graph , runs = 40 , spanning_cluster = True , model = 'bond' , alpha = alpha_1sigma , copy_result = True ) : r try : runs = int ( runs ) except : raise ValueError ( "runs needs to be a positive integer" ) if runs <= 0 : raise ValueError ( "runs needs to be a positive integer" ) try : alpha = float ( alpha ) except : raise ValueError ( "alpha needs to be a float in the interval (0, 1)" ) if alpha <= 0.0 or alpha >= 1.0 : raise ValueError ( "alpha needs to be a float in the interval (0, 1)" ) run_iterators = [ sample_states ( graph , spanning_cluster = spanning_cluster , model = model , copy_result = False ) for _ in range ( runs ) ] ret = dict ( ) for microcanonical_ensemble in zip ( * run_iterators ) : ret [ 'n' ] = microcanonical_ensemble [ 0 ] [ 'n' ] ret [ 'N' ] = microcanonical_ensemble [ 0 ] [ 'N' ] ret [ 'M' ] = microcanonical_ensemble [ 0 ] [ 'M' ] max_cluster_size = np . empty ( runs ) moments = np . empty ( ( runs , 5 ) ) if spanning_cluster : has_spanning_cluster = np . empty ( runs ) for r , state in enumerate ( microcanonical_ensemble ) : assert state [ 'n' ] == ret [ 'n' ] assert state [ 'N' ] == ret [ 'N' ] assert state [ 'M' ] == ret [ 'M' ] max_cluster_size [ r ] = state [ 'max_cluster_size' ] moments [ r ] = state [ 'moments' ] if spanning_cluster : has_spanning_cluster [ r ] = state [ 'has_spanning_cluster' ] ret . update ( _microcanonical_average_max_cluster_size ( max_cluster_size , alpha ) ) ret . update ( _microcanonical_average_moments ( moments , alpha ) ) if spanning_cluster : ret . update ( _microcanonical_average_spanning_cluster ( has_spanning_cluster , alpha ) ) if copy_result : yield copy . deepcopy ( ret ) else : yield ret
6379	def dist_manhattan ( src , tar , qval = 2 , alphabet = None ) : return Manhattan ( ) . dist ( src , tar , qval , alphabet )
4580	def toggle ( s ) : is_numeric = ',' in s or s . startswith ( '0x' ) or s . startswith ( '#' ) c = name_to_color ( s ) return color_to_name ( c ) if is_numeric else str ( c )
698	def getParticleInfo ( self , modelId ) : entry = self . _allResults [ self . _modelIDToIdx [ modelId ] ] return ( entry [ 'modelParams' ] [ 'particleState' ] , modelId , entry [ 'errScore' ] , entry [ 'completed' ] , entry [ 'matured' ] )
7369	def doc ( func ) : stripped_chars = " \t" if hasattr ( func , '__doc__' ) : docstring = func . __doc__ . lstrip ( " \n\t" ) if "\n" in docstring : i = docstring . index ( "\n" ) return docstring [ : i ] . rstrip ( stripped_chars ) elif docstring : return docstring . rstrip ( stripped_chars ) return ""
1137	def commonprefix ( m ) : "Given a list of pathnames, returns the longest common leading component" if not m : return '' s1 = min ( m ) s2 = max ( m ) for i , c in enumerate ( s1 ) : if c != s2 [ i ] : return s1 [ : i ] return s1
1045	def float_pack ( x , size ) : if size == 8 : MIN_EXP = - 1021 MAX_EXP = 1024 MANT_DIG = 53 BITS = 64 elif size == 4 : MIN_EXP = - 125 MAX_EXP = 128 MANT_DIG = 24 BITS = 32 else : raise ValueError ( "invalid size value" ) sign = math . copysign ( 1.0 , x ) < 0.0 if math . isinf ( x ) : mant = 0 exp = MAX_EXP - MIN_EXP + 2 elif math . isnan ( x ) : mant = 1 << ( MANT_DIG - 2 ) exp = MAX_EXP - MIN_EXP + 2 elif x == 0.0 : mant = 0 exp = 0 else : m , e = math . frexp ( abs ( x ) ) exp = e - ( MIN_EXP - 1 ) if exp > 0 : mant = round_to_nearest ( m * ( 1 << MANT_DIG ) ) mant -= 1 << MANT_DIG - 1 else : if exp + MANT_DIG - 1 >= 0 : mant = round_to_nearest ( m * ( 1 << exp + MANT_DIG - 1 ) ) else : mant = 0 exp = 0 assert 0 <= mant <= 1 << MANT_DIG - 1 if mant == 1 << MANT_DIG - 1 : mant = 0 exp += 1 if exp >= MAX_EXP - MIN_EXP + 2 : raise OverflowError ( "float too large to pack in this format" ) assert 0 <= mant < 1 << MANT_DIG - 1 assert 0 <= exp <= MAX_EXP - MIN_EXP + 2 assert 0 <= sign <= 1 return ( ( sign << BITS - 1 ) | ( exp << MANT_DIG - 1 ) ) | mant
8593	def remove_snapshot ( self , snapshot_id ) : response = self . _perform_request ( url = '/snapshots/' + snapshot_id , method = 'DELETE' ) return response
12141	def load_dframe ( self , dframe ) : filename_series = dframe [ self . key ] loaded_data = filename_series . map ( self . filetype . data ) keys = [ list ( el . keys ( ) ) for el in loaded_data . values ] for key in set ( ) . union ( * keys ) : key_exists = key in dframe . columns if key_exists : self . warning ( "Appending '_data' suffix to data key %r to avoid" "overwriting existing metadata with the same name." % key ) suffix = '_data' if key_exists else '' dframe [ key + suffix ] = loaded_data . map ( lambda x : x . get ( key , np . nan ) ) return dframe
7312	def is_local_ip ( ip_address ) : try : ip = ipaddress . ip_address ( u'' + ip_address ) return ip . is_loopback except ValueError as e : return None
7857	def __error ( self , stanza ) : try : self . error ( stanza . get_error ( ) ) except ProtocolError : from . . error import StanzaErrorNode self . error ( StanzaErrorNode ( "undefined-condition" ) )
11221	def get ( self , request , hash , filename ) : if _ws_download is True : return HttpResponseForbidden ( ) upload = Upload . objects . uploaded ( ) . get ( hash = hash , name = filename ) return FileResponse ( upload . file , content_type = upload . type )
3054	def from_string ( cls , key_pem , is_x509_cert ) : key_pem = _helpers . _to_bytes ( key_pem ) if is_x509_cert : der = rsa . pem . load_pem ( key_pem , 'CERTIFICATE' ) asn1_cert , remaining = decoder . decode ( der , asn1Spec = Certificate ( ) ) if remaining != b'' : raise ValueError ( 'Unused bytes' , remaining ) cert_info = asn1_cert [ 'tbsCertificate' ] [ 'subjectPublicKeyInfo' ] key_bytes = _bit_list_to_bytes ( cert_info [ 'subjectPublicKey' ] ) pubkey = rsa . PublicKey . load_pkcs1 ( key_bytes , 'DER' ) else : pubkey = rsa . PublicKey . load_pkcs1 ( key_pem , 'PEM' ) return cls ( pubkey )
6220	def create ( self ) : dtype = NP_COMPONENT_DTYPE [ self . component_type . value ] data = numpy . frombuffer ( self . buffer . read ( byte_length = self . byte_length , byte_offset = self . byte_offset ) , count = self . count * self . components , dtype = dtype , ) return dtype , data
2010	def _get_memfee ( self , address , size = 1 ) : if not issymbolic ( size ) and size == 0 : return 0 address = self . safe_add ( address , size ) allocated = self . allocated GMEMORY = 3 GQUADRATICMEMDENOM = 512 old_size = Operators . ZEXTEND ( Operators . UDIV ( self . safe_add ( allocated , 31 ) , 32 ) , 512 ) new_size = Operators . ZEXTEND ( Operators . UDIV ( self . safe_add ( address , 31 ) , 32 ) , 512 ) old_totalfee = self . safe_mul ( old_size , GMEMORY ) + Operators . UDIV ( self . safe_mul ( old_size , old_size ) , GQUADRATICMEMDENOM ) new_totalfee = self . safe_mul ( new_size , GMEMORY ) + Operators . UDIV ( self . safe_mul ( new_size , new_size ) , GQUADRATICMEMDENOM ) memfee = new_totalfee - old_totalfee flag = Operators . UGT ( new_totalfee , old_totalfee ) return Operators . ITEBV ( 512 , size == 0 , 0 , Operators . ITEBV ( 512 , flag , memfee , 0 ) )
10156	def merge_dicts ( base , changes ) : for k , v in changes . items ( ) : if isinstance ( v , dict ) : merge_dicts ( base . setdefault ( k , { } ) , v ) else : base . setdefault ( k , v )
1161	def acquire ( self , blocking = 1 ) : rc = False with self . __cond : while self . __value == 0 : if not blocking : break if __debug__ : self . _note ( "%s.acquire(%s): blocked waiting, value=%s" , self , blocking , self . __value ) self . __cond . wait ( ) else : self . __value = self . __value - 1 if __debug__ : self . _note ( "%s.acquire: success, value=%s" , self , self . __value ) rc = True return rc
749	def _removeUnlikelyPredictions ( cls , likelihoodsDict , minLikelihoodThreshold , maxPredictionsPerStep ) : maxVal = ( None , None ) for ( k , v ) in likelihoodsDict . items ( ) : if len ( likelihoodsDict ) <= 1 : break if maxVal [ 0 ] is None or v >= maxVal [ 1 ] : if maxVal [ 0 ] is not None and maxVal [ 1 ] < minLikelihoodThreshold : del likelihoodsDict [ maxVal [ 0 ] ] maxVal = ( k , v ) elif v < minLikelihoodThreshold : del likelihoodsDict [ k ] likelihoodsDict = dict ( sorted ( likelihoodsDict . iteritems ( ) , key = itemgetter ( 1 ) , reverse = True ) [ : maxPredictionsPerStep ] ) return likelihoodsDict
9025	def kivy_svg ( self ) : from kivy . graphics . svg import Svg path = self . temporary_path ( ".svg" ) try : return Svg ( path ) finally : remove_file ( path )
2263	def dict_take ( dict_ , keys , default = util_const . NoParam ) : r if default is util_const . NoParam : for key in keys : yield dict_ [ key ] else : for key in keys : yield dict_ . get ( key , default )
9120	def dropbox_fileupload ( dropbox , request ) : attachment = request . POST [ 'attachment' ] attached = dropbox . add_attachment ( attachment ) return dict ( files = [ dict ( name = attached , type = attachment . type , ) ] )
9944	def link_file ( self , path , prefixed_path , source_storage ) : if prefixed_path in self . symlinked_files : return self . log ( "Skipping '%s' (already linked earlier)" % path ) if not self . delete_file ( path , prefixed_path , source_storage ) : return source_path = source_storage . path ( path ) if self . dry_run : self . log ( "Pretending to link '%s'" % source_path , level = 1 ) else : self . log ( "Linking '%s'" % source_path , level = 1 ) full_path = self . storage . path ( prefixed_path ) try : os . makedirs ( os . path . dirname ( full_path ) ) except OSError : pass try : if os . path . lexists ( full_path ) : os . unlink ( full_path ) os . symlink ( source_path , full_path ) except AttributeError : import platform raise CommandError ( "Symlinking is not supported by Python %s." % platform . python_version ( ) ) except NotImplementedError : import platform raise CommandError ( "Symlinking is not supported in this " "platform (%s)." % platform . platform ( ) ) except OSError as e : raise CommandError ( e ) if prefixed_path not in self . symlinked_files : self . symlinked_files . append ( prefixed_path )
5252	def bdib ( self , ticker , start_datetime , end_datetime , event_type , interval , elms = None ) : elms = [ ] if not elms else elms logger = _get_logger ( self . debug ) while ( self . _session . tryNextEvent ( ) ) : pass request = self . refDataService . createRequest ( 'IntradayBarRequest' ) request . set ( 'security' , ticker ) request . set ( 'eventType' , event_type ) request . set ( 'interval' , interval ) request . set ( 'startDateTime' , start_datetime ) request . set ( 'endDateTime' , end_datetime ) for name , val in elms : request . set ( name , val ) logger . info ( 'Sending Request:\n{}' . format ( request ) ) self . _session . sendRequest ( request , identity = self . _identity ) data = [ ] flds = [ 'open' , 'high' , 'low' , 'close' , 'volume' , 'numEvents' ] for msg in self . _receive_events ( ) : d = msg [ 'element' ] [ 'IntradayBarResponse' ] for bar in d [ 'barData' ] [ 'barTickData' ] : data . append ( bar [ 'barTickData' ] ) data = pd . DataFrame ( data ) . set_index ( 'time' ) . sort_index ( ) . loc [ : , flds ] return data
3492	def _error_string ( error , k = None ) : package = error . getPackage ( ) if package == '' : package = 'core' template = 'E{} ({}): {} ({}, L{}); {}; {}' error_str = template . format ( k , error . getSeverityAsString ( ) , error . getCategoryAsString ( ) , package , error . getLine ( ) , error . getShortMessage ( ) , error . getMessage ( ) ) return error_str
8662	def migrate ( src_path , src_passphrase , src_backend , dst_path , dst_passphrase , dst_backend ) : src_storage = STORAGE_MAPPING [ src_backend ] ( ** _parse_path_string ( src_path ) ) dst_storage = STORAGE_MAPPING [ dst_backend ] ( ** _parse_path_string ( dst_path ) ) src_stash = Stash ( src_storage , src_passphrase ) dst_stash = Stash ( dst_storage , dst_passphrase ) keys = src_stash . export ( ) dst_stash . load ( src_passphrase , keys = keys )
6431	def dist_abs ( self , src , tar ) : if src == tar : return 6 if src == '' or tar == '' : return 0 src = list ( mra ( src ) ) tar = list ( mra ( tar ) ) if abs ( len ( src ) - len ( tar ) ) > 2 : return 0 length_sum = len ( src ) + len ( tar ) if length_sum < 5 : min_rating = 5 elif length_sum < 8 : min_rating = 4 elif length_sum < 12 : min_rating = 3 else : min_rating = 2 for _ in range ( 2 ) : new_src = [ ] new_tar = [ ] minlen = min ( len ( src ) , len ( tar ) ) for i in range ( minlen ) : if src [ i ] != tar [ i ] : new_src . append ( src [ i ] ) new_tar . append ( tar [ i ] ) src = new_src + src [ minlen : ] tar = new_tar + tar [ minlen : ] src . reverse ( ) tar . reverse ( ) similarity = 6 - max ( len ( src ) , len ( tar ) ) if similarity >= min_rating : return similarity return 0
6503	def find_matches ( strings , words , length_hoped ) : lower_words = [ w . lower ( ) for w in words ] def has_match ( string ) : lower_string = string . lower ( ) for test_word in lower_words : if test_word in lower_string : return True return False shortened_strings = [ textwrap . wrap ( s ) for s in strings ] short_string_list = list ( chain . from_iterable ( shortened_strings ) ) matches = [ ms for ms in short_string_list if has_match ( ms ) ] cumulative_len = 0 break_at = None for idx , match in enumerate ( matches ) : cumulative_len += len ( match ) if cumulative_len >= length_hoped : break_at = idx break return matches [ 0 : break_at ]
359	def load_file_list ( path = None , regx = '\.jpg' , printable = True , keep_prefix = False ) : r if path is None : path = os . getcwd ( ) file_list = os . listdir ( path ) return_list = [ ] for _ , f in enumerate ( file_list ) : if re . search ( regx , f ) : return_list . append ( f ) if keep_prefix : for i , f in enumerate ( return_list ) : return_list [ i ] = os . path . join ( path , f ) if printable : logging . info ( 'Match file list = %s' % return_list ) logging . info ( 'Number of files = %d' % len ( return_list ) ) return return_list
8620	def getServerStates ( pbclient = None , dc_id = None , serverid = None , servername = None ) : if pbclient is None : raise ValueError ( "argument 'pbclient' must not be None" ) if dc_id is None : raise ValueError ( "argument 'dc_id' must not be None" ) server = None if serverid is None : if servername is None : raise ValueError ( "one of 'serverid' or 'servername' must be specified" ) server_info = select_where ( getServerInfo ( pbclient , dc_id ) , [ 'id' , 'name' , 'state' , 'vmstate' ] , name = servername ) if len ( server_info ) > 1 : raise NameError ( "ambiguous server name '{}'" . format ( servername ) ) if len ( server_info ) == 1 : server = server_info [ 0 ] else : try : server_info = pbclient . get_server ( dc_id , serverid , 1 ) server = dict ( id = server_info [ 'id' ] , name = server_info [ 'properties' ] [ 'name' ] , state = server_info [ 'metadata' ] [ 'state' ] , vmstate = server_info [ 'properties' ] [ 'vmState' ] ) except Exception : ex = sys . exc_info ( ) [ 1 ] if ex . args [ 0 ] is not None and ex . args [ 0 ] == 404 : print ( "Server w/ ID {} not found" . format ( serverid ) ) server = None else : raise ex return server
7610	def get_location ( self , location_id : int , timeout : int = None ) : url = self . api . LOCATIONS + '/' + str ( location_id ) return self . _get_model ( url , timeout = timeout )
11455	def get_config_item ( cls , key , kb_name , allow_substring = True ) : config_dict = cls . kbs . get ( kb_name , None ) if config_dict : if key in config_dict : return config_dict [ key ] elif allow_substring : res = [ v for k , v in config_dict . items ( ) if key in k ] if res : return res [ 0 ] return key
6980	def _epd_function ( coeffs , fluxes , xcc , ycc , bgv , bge ) : epdf = ( coeffs [ 0 ] + coeffs [ 1 ] * npsin ( 2 * MPI * xcc ) + coeffs [ 2 ] * npcos ( 2 * MPI * xcc ) + coeffs [ 3 ] * npsin ( 2 * MPI * ycc ) + coeffs [ 4 ] * npcos ( 2 * MPI * ycc ) + coeffs [ 5 ] * npsin ( 4 * MPI * xcc ) + coeffs [ 6 ] * npcos ( 4 * MPI * xcc ) + coeffs [ 7 ] * npsin ( 4 * MPI * ycc ) + coeffs [ 8 ] * npcos ( 4 * MPI * ycc ) + coeffs [ 9 ] * bgv + coeffs [ 10 ] * bge ) return epdf
9822	def list ( page ) : user = AuthConfigManager . get_value ( 'username' ) if not user : Printer . print_error ( 'Please login first. `polyaxon login --help`' ) page = page or 1 try : response = PolyaxonClient ( ) . project . list_projects ( user , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get list of projects.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Projects for current user' ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No projects found for current user' ) objects = list_dicts_to_tabulate ( [ o . to_light_dict ( humanize_values = True , exclude_attrs = [ 'uuid' , 'experiment_groups' , 'experiments' , 'description' , 'num_experiments' , 'num_independent_experiments' , 'num_experiment_groups' , 'num_jobs' , 'num_builds' , 'unique_name' ] ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Projects:" ) dict_tabulate ( objects , is_list_dict = True )
13884	def ListFiles ( directory ) : from six . moves . urllib . parse import urlparse directory_url = urlparse ( directory ) if _UrlIsLocal ( directory_url ) : if not os . path . isdir ( directory ) : return None return os . listdir ( directory ) elif directory_url . scheme == 'ftp' : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( directory_url . scheme ) else : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( directory_url . scheme )
11445	def _clean_xml ( self , path_to_xml ) : try : if os . path . isfile ( path_to_xml ) : tree = ET . parse ( path_to_xml ) root = tree . getroot ( ) else : root = ET . fromstring ( path_to_xml ) except Exception , e : self . logger . error ( "Could not read OAI XML, aborting filter!" ) raise e strip_xml_namespace ( root ) return root
3631	def md_table ( table , * , padding = DEFAULT_PADDING , divider = '|' , header_div = '-' ) : table = normalize_cols ( table ) table = pad_cells ( table ) header = table [ 0 ] body = table [ 1 : ] col_widths = [ len ( cell ) for cell in header ] horiz = horiz_div ( col_widths , header_div , divider , padding ) header = add_dividers ( header , divider , padding ) body = [ add_dividers ( row , divider , padding ) for row in body ] table = [ header , horiz ] table . extend ( body ) table = [ row . rstrip ( ) for row in table ] return '\n' . join ( table )
11325	def extract_oembeds ( self , text , maxwidth = None , maxheight = None , resource_type = None ) : parser = text_parser ( ) urls = parser . extract_urls ( text ) return self . handle_extracted_urls ( urls , maxwidth , maxheight , resource_type )
1791	def IMUL ( cpu , * operands ) : dest = operands [ 0 ] OperandSize = dest . size reg_name_h = { 8 : 'AH' , 16 : 'DX' , 32 : 'EDX' , 64 : 'RDX' } [ OperandSize ] reg_name_l = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ OperandSize ] arg0 = dest . read ( ) arg1 = None arg2 = None res = None if len ( operands ) == 1 : arg1 = cpu . read_register ( reg_name_l ) temp = ( Operators . SEXTEND ( arg0 , OperandSize , OperandSize * 2 ) * Operators . SEXTEND ( arg1 , OperandSize , OperandSize * 2 ) ) temp = temp & ( ( 1 << ( OperandSize * 2 ) ) - 1 ) cpu . write_register ( reg_name_l , Operators . EXTRACT ( temp , 0 , OperandSize ) ) cpu . write_register ( reg_name_h , Operators . EXTRACT ( temp , OperandSize , OperandSize ) ) res = Operators . EXTRACT ( temp , 0 , OperandSize ) elif len ( operands ) == 2 : arg1 = operands [ 1 ] . read ( ) arg1 = Operators . SEXTEND ( arg1 , OperandSize , OperandSize * 2 ) temp = Operators . SEXTEND ( arg0 , OperandSize , OperandSize * 2 ) * arg1 temp = temp & ( ( 1 << ( OperandSize * 2 ) ) - 1 ) res = dest . write ( Operators . EXTRACT ( temp , 0 , OperandSize ) ) else : arg1 = operands [ 1 ] . read ( ) arg2 = operands [ 2 ] . read ( ) temp = ( Operators . SEXTEND ( arg1 , OperandSize , OperandSize * 2 ) * Operators . SEXTEND ( arg2 , operands [ 2 ] . size , OperandSize * 2 ) ) temp = temp & ( ( 1 << ( OperandSize * 2 ) ) - 1 ) res = dest . write ( Operators . EXTRACT ( temp , 0 , OperandSize ) ) cpu . CF = ( Operators . SEXTEND ( res , OperandSize , OperandSize * 2 ) != temp ) cpu . OF = cpu . CF
2031	def EXTCODECOPY ( self , account , address , offset , size ) : extbytecode = self . world . get_code ( account ) self . _allocate ( address + size ) for i in range ( size ) : if offset + i < len ( extbytecode ) : self . _store ( address + i , extbytecode [ offset + i ] ) else : self . _store ( address + i , 0 )
12893	def get_power ( self ) : power = ( yield from self . handle_int ( self . API . get ( 'power' ) ) ) return bool ( power )
7037	def xmatch_search ( lcc_server , file_to_upload , xmatch_dist_arcsec = 3.0 , result_visibility = 'unlisted' , email_when_done = False , collections = None , columns = None , filters = None , sortspec = None , limitspec = None , samplespec = None , download_data = True , outdir = None , maxtimeout = 300.0 , refresh = 15.0 ) : with open ( file_to_upload ) as infd : xmq = infd . read ( ) xmqlines = len ( xmq . split ( '\n' ) [ : - 1 ] ) if xmqlines > 5000 : LOGERROR ( 'you have more than 5000 lines in the file to upload: %s' % file_to_upload ) return None , None , None params = { 'xmq' : xmq , 'xmd' : xmatch_dist_arcsec } if collections : params [ 'collections' ] = collections if columns : params [ 'columns' ] = columns if filters : params [ 'filters' ] = filters if sortspec : params [ 'sortspec' ] = json . dumps ( [ sortspec ] ) if samplespec : params [ 'samplespec' ] = int ( samplespec ) if limitspec : params [ 'limitspec' ] = int ( limitspec ) params [ 'visibility' ] = result_visibility params [ 'emailwhendone' ] = email_when_done if email_when_done : download_data = False have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) api_url = '%s/api/xmatch' % lcc_server searchresult = submit_post_searchquery ( api_url , params , apikey ) status = searchresult [ 0 ] if download_data : if status == 'ok' : LOGINFO ( 'query complete, downloading associated data...' ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if pkl : return searchresult [ 1 ] , csv , lczip , pkl else : return searchresult [ 1 ] , csv , lczip elif status == 'background' : LOGINFO ( 'query is not yet complete, ' 'waiting up to %.1f minutes, ' 'updates every %s seconds (hit Ctrl+C to cancel)...' % ( maxtimeout / 60.0 , refresh ) ) timewaited = 0.0 while timewaited < maxtimeout : try : time . sleep ( refresh ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if ( csv and os . path . exists ( csv ) and lczip and os . path . exists ( lczip ) ) : LOGINFO ( 'all dataset products collected' ) return searchresult [ 1 ] , csv , lczip timewaited = timewaited + refresh except KeyboardInterrupt : LOGWARNING ( 'abandoned wait for downloading data' ) return searchresult [ 1 ] , None , None LOGERROR ( 'wait timed out.' ) return searchresult [ 1 ] , None , None else : LOGERROR ( 'could not download the data for this query result' ) return searchresult [ 1 ] , None , None else : return searchresult [ 1 ] , None , None
11745	def closure ( self , rules ) : closure = set ( ) todo = set ( rules ) while todo : rule = todo . pop ( ) closure . add ( rule ) if rule . at_end : continue symbol = rule . rhs [ rule . pos ] for production in self . nonterminals [ symbol ] : for first in self . first ( rule . rest ) : if EPSILON in production . rhs : new_rule = DottedRule ( production , 1 , first ) else : new_rule = DottedRule ( production , 0 , first ) if new_rule not in closure : todo . add ( new_rule ) return frozenset ( closure )
3757	def LFL ( Hc = None , atoms = { } , CASRN = '' , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in IEC_2010 . index and not np . isnan ( IEC_2010 . at [ CASRN , 'LFL' ] ) : methods . append ( IEC ) if CASRN in NFPA_2008 . index and not np . isnan ( NFPA_2008 . at [ CASRN , 'LFL' ] ) : methods . append ( NFPA ) if Hc : methods . append ( SUZUKI ) if atoms : methods . append ( CROWLLOUVAR ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IEC : return float ( IEC_2010 . at [ CASRN , 'LFL' ] ) elif Method == NFPA : return float ( NFPA_2008 . at [ CASRN , 'LFL' ] ) elif Method == SUZUKI : return Suzuki_LFL ( Hc = Hc ) elif Method == CROWLLOUVAR : return Crowl_Louvar_LFL ( atoms = atoms ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
3679	def economic_status ( self ) : r if self . __economic_status : return self . __economic_status else : self . __economic_status = economic_status ( self . CAS , Method = 'Combined' ) return self . __economic_status
12646	def set_aad_metadata ( uri , resource , client ) : set_config_value ( 'authority_uri' , uri ) set_config_value ( 'aad_resource' , resource ) set_config_value ( 'aad_client' , client )
8657	def _clean ( zipcode , valid_length = _valid_zipcode_length ) : zipcode = zipcode . split ( "-" ) [ 0 ] if len ( zipcode ) != valid_length : raise ValueError ( 'Invalid format, zipcode must be of the format: "#####" or "#####-####"' ) if _contains_nondigits ( zipcode ) : raise ValueError ( 'Invalid characters, zipcode may only contain digits and "-".' ) return zipcode
185	def almost_equals ( self , other , max_distance = 1e-4 , points_per_edge = 8 ) : if self . label != other . label : return False return self . coords_almost_equals ( other , max_distance = max_distance , points_per_edge = points_per_edge )
2865	def get_i2c_device ( address , busnum = None , i2c_interface = None , ** kwargs ) : if busnum is None : busnum = get_default_bus ( ) return Device ( address , busnum , i2c_interface , ** kwargs )
4889	def update_course ( self , course , enterprise_customer , enterprise_context ) : course [ 'course_runs' ] = self . update_course_runs ( course_runs = course . get ( 'course_runs' ) or [ ] , enterprise_customer = enterprise_customer , enterprise_context = enterprise_context , ) marketing_url = course . get ( 'marketing_url' ) if marketing_url : query_parameters = dict ( enterprise_context , ** utils . get_enterprise_utm_context ( enterprise_customer ) ) course . update ( { 'marketing_url' : utils . update_query_parameters ( marketing_url , query_parameters ) } ) course . update ( enterprise_context ) return course
8858	def on_goto_out_of_doc ( self , assignment ) : editor = self . open_file ( assignment . module_path ) if editor : TextHelper ( editor ) . goto_line ( assignment . line , assignment . column )
1010	def _trimSegmentsInCell ( self , colIdx , cellIdx , segList , minPermanence , minNumSyns ) : if minPermanence is None : minPermanence = self . connectedPerm if minNumSyns is None : minNumSyns = self . activationThreshold nSegsRemoved , nSynsRemoved = 0 , 0 segsToDel = [ ] for segment in segList : synsToDel = [ syn for syn in segment . syns if syn [ 2 ] < minPermanence ] if len ( synsToDel ) == len ( segment . syns ) : segsToDel . append ( segment ) else : if len ( synsToDel ) > 0 : for syn in synsToDel : segment . syns . remove ( syn ) nSynsRemoved += 1 if len ( segment . syns ) < minNumSyns : segsToDel . append ( segment ) nSegsRemoved += len ( segsToDel ) for seg in segsToDel : self . _cleanUpdatesList ( colIdx , cellIdx , seg ) self . cells [ colIdx ] [ cellIdx ] . remove ( seg ) nSynsRemoved += len ( seg . syns ) return nSegsRemoved , nSynsRemoved
5201	def Operate ( self , command , index , op_type ) : OutstationApplication . process_point_value ( 'Operate' , command , index , op_type ) return opendnp3 . CommandStatus . SUCCESS
9780	def build ( ctx , project , build ) : ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'build' ] = build
11403	def create_record ( marcxml = None , verbose = CFG_BIBRECORD_DEFAULT_VERBOSE_LEVEL , correct = CFG_BIBRECORD_DEFAULT_CORRECT , parser = '' , sort_fields_by_indicators = False , keep_singletons = CFG_BIBRECORD_KEEP_SINGLETONS ) : if marcxml is None : return { } try : rec = _create_record_lxml ( marcxml , verbose , correct , keep_singletons = keep_singletons ) except InvenioBibRecordParserError as ex1 : return ( None , 0 , str ( ex1 ) ) if sort_fields_by_indicators : _record_sort_by_indicators ( rec ) errs = [ ] if correct : errs = _correct_record ( rec ) return ( rec , int ( not errs ) , errs )
4750	def start ( self ) : self . __thread = Threads ( target = self . run , args = ( True , True , False ) ) self . __thread . setDaemon ( True ) self . __thread . start ( )
661	def computeSaturationLevels ( outputs , outputsShape , sparseForm = False ) : if not sparseForm : outputs = outputs . reshape ( outputsShape ) spOut = SM32 ( outputs ) else : if len ( outputs ) > 0 : assert ( outputs . max ( ) < outputsShape [ 0 ] * outputsShape [ 1 ] ) spOut = SM32 ( 1 , outputsShape [ 0 ] * outputsShape [ 1 ] ) spOut . setRowFromSparse ( 0 , outputs , [ 1 ] * len ( outputs ) ) spOut . reshape ( outputsShape [ 0 ] , outputsShape [ 1 ] ) regionSize = 15 rows = xrange ( regionSize + 1 , outputsShape [ 0 ] + 1 , regionSize ) cols = xrange ( regionSize + 1 , outputsShape [ 1 ] + 1 , regionSize ) regionSums = spOut . nNonZerosPerBox ( rows , cols ) ( locations , values ) = regionSums . tolist ( ) values /= float ( regionSize * regionSize ) sat = list ( values ) innerSat = [ ] locationSet = set ( locations ) for ( location , value ) in itertools . izip ( locations , values ) : ( row , col ) = location if ( row - 1 , col ) in locationSet and ( row , col - 1 ) in locationSet and ( row + 1 , col ) in locationSet and ( row , col + 1 ) in locationSet : innerSat . append ( value ) return ( sat , innerSat )
2723	def take_snapshot ( self , snapshot_name , return_dict = True , power_off = False ) : if power_off is True and self . status != "off" : action = self . power_off ( return_dict = False ) action . wait ( ) self . load ( ) return self . _perform_action ( { "type" : "snapshot" , "name" : snapshot_name } , return_dict )
9849	def _load_plt ( self , filename ) : g = gOpenMol . Plt ( ) g . read ( filename ) grid , edges = g . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
10838	def interactions ( self ) : interactions = [ ] url = PATHS [ 'GET_INTERACTIONS' ] % self . id response = self . api . get ( url = url ) for interaction in response [ 'interactions' ] : interactions . append ( ResponseObject ( interaction ) ) self . __interactions = interactions return self . __interactions
10872	def get_Kprefactor ( z , cos_theta , zint = 100.0 , n2n1 = 0.95 , get_hdet = False , ** kwargs ) : phase = f_theta ( cos_theta , zint , z , n2n1 = n2n1 , ** kwargs ) to_return = np . exp ( - 1j * phase ) if not get_hdet : to_return *= np . outer ( np . ones_like ( z ) , np . sqrt ( cos_theta ) ) return to_return
6643	def getExtraIncludes ( self ) : if 'extraIncludes' in self . description : return [ os . path . normpath ( x ) for x in self . description [ 'extraIncludes' ] ] else : return [ ]
8131	def merge ( self , layers ) : layers . sort ( ) if layers [ 0 ] == 0 : del layers [ 0 ] self . flatten ( layers )
1916	def get ( self ) : if self . is_shutdown ( ) : return None while len ( self . _states ) == 0 : if self . running == 0 : return None if self . is_shutdown ( ) : return None logger . debug ( "Waiting for available states" ) self . _lock . wait ( ) state_id = self . _policy . choice ( list ( self . _states ) ) if state_id is None : return None del self . _states [ self . _states . index ( state_id ) ] return state_id
12581	def setup_logging ( log_config_file = op . join ( op . dirname ( __file__ ) , 'logger.yml' ) , log_default_level = LOG_LEVEL , env_key = MODULE_NAME . upper ( ) + '_LOG_CFG' ) : path = log_config_file value = os . getenv ( env_key , None ) if value : path = value if op . exists ( path ) : log_cfg = yaml . load ( read ( path ) . format ( MODULE_NAME ) ) logging . config . dictConfig ( log_cfg ) else : logging . basicConfig ( level = log_default_level ) log = logging . getLogger ( __name__ ) log . debug ( 'Start logging.' )
1905	def strcmp ( state , s1 , s2 ) : cpu = state . cpu if issymbolic ( s1 ) : raise ConcretizeArgument ( state . cpu , 1 ) if issymbolic ( s2 ) : raise ConcretizeArgument ( state . cpu , 2 ) s1_zero_idx = _find_zero ( cpu , state . constraints , s1 ) s2_zero_idx = _find_zero ( cpu , state . constraints , s2 ) min_zero_idx = min ( s1_zero_idx , s2_zero_idx ) ret = None for offset in range ( min_zero_idx , - 1 , - 1 ) : s1char = ZEXTEND ( cpu . read_int ( s1 + offset , 8 ) , cpu . address_bit_size ) s2char = ZEXTEND ( cpu . read_int ( s2 + offset , 8 ) , cpu . address_bit_size ) if issymbolic ( s1char ) or issymbolic ( s2char ) : if ret is None or ( not issymbolic ( ret ) and ret == 0 ) : ret = s1char - s2char else : ret = ITEBV ( cpu . address_bit_size , s1char != s2char , s1char - s2char , ret ) else : if s1char != s2char : ret = s1char - s2char elif ret is None : ret = 0 return ret
5767	def _advapi32_interpret_rsa_key_blob ( bit_size , blob_struct , blob ) : len1 = bit_size // 8 len2 = bit_size // 16 prime1_offset = len1 prime2_offset = prime1_offset + len2 exponent1_offset = prime2_offset + len2 exponent2_offset = exponent1_offset + len2 coefficient_offset = exponent2_offset + len2 private_exponent_offset = coefficient_offset + len2 public_exponent = blob_struct . rsapubkey . pubexp modulus = int_from_bytes ( blob [ 0 : prime1_offset ] [ : : - 1 ] ) prime1 = int_from_bytes ( blob [ prime1_offset : prime2_offset ] [ : : - 1 ] ) prime2 = int_from_bytes ( blob [ prime2_offset : exponent1_offset ] [ : : - 1 ] ) exponent1 = int_from_bytes ( blob [ exponent1_offset : exponent2_offset ] [ : : - 1 ] ) exponent2 = int_from_bytes ( blob [ exponent2_offset : coefficient_offset ] [ : : - 1 ] ) coefficient = int_from_bytes ( blob [ coefficient_offset : private_exponent_offset ] [ : : - 1 ] ) private_exponent = int_from_bytes ( blob [ private_exponent_offset : private_exponent_offset + len1 ] [ : : - 1 ] ) public_key_info = keys . PublicKeyInfo ( { 'algorithm' : keys . PublicKeyAlgorithm ( { 'algorithm' : 'rsa' , } ) , 'public_key' : keys . RSAPublicKey ( { 'modulus' : modulus , 'public_exponent' : public_exponent , } ) , } ) rsa_private_key = keys . RSAPrivateKey ( { 'version' : 'two-prime' , 'modulus' : modulus , 'public_exponent' : public_exponent , 'private_exponent' : private_exponent , 'prime1' : prime1 , 'prime2' : prime2 , 'exponent1' : exponent1 , 'exponent2' : exponent2 , 'coefficient' : coefficient , } ) private_key_info = keys . PrivateKeyInfo ( { 'version' : 0 , 'private_key_algorithm' : keys . PrivateKeyAlgorithm ( { 'algorithm' : 'rsa' , } ) , 'private_key' : rsa_private_key , } ) return ( public_key_info , private_key_info )
13898	def IterHashes ( iterator_size , hash_length = 7 ) : if not isinstance ( iterator_size , int ) : raise TypeError ( 'iterator_size must be integer.' ) count = 0 while count != iterator_size : count += 1 yield GetRandomHash ( hash_length )
1598	def read_chunk ( filename , offset = - 1 , length = - 1 , escape_data = False ) : try : length = int ( length ) offset = int ( offset ) except ValueError : return { } if not os . path . isfile ( filename ) : return { } try : fstat = os . stat ( filename ) except Exception : return { } if offset == - 1 : offset = fstat . st_size if length == - 1 : length = fstat . st_size - offset with open ( filename , "r" ) as fp : fp . seek ( offset ) try : data = fp . read ( length ) except IOError : return { } if data : data = _escape_data ( data ) if escape_data else data return dict ( offset = offset , length = len ( data ) , data = data ) return dict ( offset = offset , length = 0 )
7530	def build_dag ( data , samples ) : snames = [ i . name for i in samples ] dag = nx . DiGraph ( ) joborder = JOBORDER [ data . paramsdict [ "assembly_method" ] ] for sname in snames : for func in joborder : dag . add_node ( "{}-{}-{}" . format ( func , 0 , sname ) ) for chunk in xrange ( 10 ) : dag . add_node ( "{}-{}-{}" . format ( "muscle_align" , chunk , sname ) ) dag . add_node ( "{}-{}-{}" . format ( "reconcat" , 0 , sname ) ) for sname in snames : for sname2 in snames : dag . add_edge ( "{}-{}-{}" . format ( joborder [ 0 ] , 0 , sname2 ) , "{}-{}-{}" . format ( joborder [ 1 ] , 0 , sname ) ) for idx in xrange ( 2 , len ( joborder ) ) : dag . add_edge ( "{}-{}-{}" . format ( joborder [ idx - 1 ] , 0 , sname ) , "{}-{}-{}" . format ( joborder [ idx ] , 0 , sname ) ) for sname2 in snames : for chunk in range ( 10 ) : dag . add_edge ( "{}-{}-{}" . format ( "muscle_chunker" , 0 , sname2 ) , "{}-{}-{}" . format ( "muscle_align" , chunk , sname ) ) dag . add_edge ( "{}-{}-{}" . format ( "muscle_align" , chunk , sname ) , "{}-{}-{}" . format ( "reconcat" , 0 , sname ) ) return dag , joborder
2969	def _sm_stop_from_pain ( self , * args , ** kwargs ) : _logger . info ( "Stopping chaos for blockade %s" % self . _blockade_name ) self . _do_reset_all ( )
4325	def dcshift ( self , shift = 0.0 ) : if not is_number ( shift ) or shift < - 2 or shift > 2 : raise ValueError ( 'shift must be a number between -2 and 2.' ) effect_args = [ 'dcshift' , '{:f}' . format ( shift ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'dcshift' ) return self
1160	def notify ( self , n = 1 ) : if not self . _is_owned ( ) : raise RuntimeError ( "cannot notify on un-acquired lock" ) __waiters = self . __waiters waiters = __waiters [ : n ] if not waiters : if __debug__ : self . _note ( "%s.notify(): no waiters" , self ) return self . _note ( "%s.notify(): notifying %d waiter%s" , self , n , n != 1 and "s" or "" ) for waiter in waiters : waiter . release ( ) try : __waiters . remove ( waiter ) except ValueError : pass
6868	def find_lc_timegroups ( lctimes , mingap = 4.0 ) : lc_time_diffs = np . diff ( lctimes ) group_start_indices = np . where ( lc_time_diffs > mingap ) [ 0 ] if len ( group_start_indices ) > 0 : group_indices = [ ] for i , gindex in enumerate ( group_start_indices ) : if i == 0 : group_indices . append ( slice ( 0 , gindex + 1 ) ) else : group_indices . append ( slice ( group_start_indices [ i - 1 ] + 1 , gindex + 1 ) ) group_indices . append ( slice ( group_start_indices [ - 1 ] + 1 , len ( lctimes ) ) ) else : group_indices = [ slice ( 0 , len ( lctimes ) ) ] return len ( group_indices ) , group_indices
9973	def _get_namedrange ( book , rangename , sheetname = None ) : def cond ( namedef ) : if namedef . type . upper ( ) == "RANGE" : if namedef . name . upper ( ) == rangename . upper ( ) : if sheetname is None : if not namedef . localSheetId : return True else : sheet_id = [ sht . upper ( ) for sht in book . sheetnames ] . index ( sheetname . upper ( ) ) if namedef . localSheetId == sheet_id : return True return False def get_destinations ( name_def ) : from openpyxl . formula import Tokenizer from openpyxl . utils . cell import SHEETRANGE_RE if name_def . type == "RANGE" : tok = Tokenizer ( "=" + name_def . value ) for part in tok . items : if part . subtype == "RANGE" : m = SHEETRANGE_RE . match ( part . value ) if m . group ( "quoted" ) : sheet_name = m . group ( "quoted" ) else : sheet_name = m . group ( "notquoted" ) yield sheet_name , m . group ( "cells" ) namedef = next ( ( item for item in book . defined_names . definedName if cond ( item ) ) , None ) if namedef is None : return None dests = get_destinations ( namedef ) xlranges = [ ] sheetnames_upper = [ name . upper ( ) for name in book . sheetnames ] for sht , addr in dests : if sheetname : sht = sheetname index = sheetnames_upper . index ( sht . upper ( ) ) xlranges . append ( book . worksheets [ index ] [ addr ] ) if len ( xlranges ) == 1 : return xlranges [ 0 ] else : return xlranges
8836	def minus ( * args ) : if len ( args ) == 1 : return - to_numeric ( args [ 0 ] ) return to_numeric ( args [ 0 ] ) - to_numeric ( args [ 1 ] )
2992	def mutualFundSymbolsDF ( token = '' , version = '' ) : df = pd . DataFrame ( mutualFundSymbols ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
3943	async def _on_push_data ( self , data_bytes ) : logger . debug ( 'Received chunk:\n{}' . format ( data_bytes ) ) for chunk in self . _chunk_parser . get_chunks ( data_bytes ) : if not self . _is_connected : if self . _on_connect_called : self . _is_connected = True await self . on_reconnect . fire ( ) else : self . _on_connect_called = True self . _is_connected = True await self . on_connect . fire ( ) container_array = json . loads ( chunk ) for inner_array in container_array : array_id , data_array = inner_array logger . debug ( 'Chunk contains data array with id %r:\n%r' , array_id , data_array ) await self . on_receive_array . fire ( data_array )
2790	def get_snapshots ( self ) : data = self . get_data ( "volumes/%s/snapshots/" % self . id ) snapshots = list ( ) for jsond in data [ u'snapshots' ] : snapshot = Snapshot ( ** jsond ) snapshot . token = self . token snapshots . append ( snapshot ) return snapshots
3576	def initialize ( self ) : self . _central_manager = CBCentralManager . alloc ( ) self . _central_manager . initWithDelegate_queue_options_ ( self . _central_delegate , None , None )
8188	def betweenness_centrality ( self , normalized = True ) : bc = proximity . brandes_betweenness_centrality ( self , normalized ) for id , w in bc . iteritems ( ) : self [ id ] . _betweenness = w return bc
7602	def get_known_tournaments ( self , ** params : tournamentfilter ) : url = self . api . TOURNAMENT + '/known' return self . _get_model ( url , PartialTournament , ** params )
9497	def parse_collection ( path , excludes = None ) : file = path / COLLECTION_FILENAME if not file . exists ( ) : raise MissingFile ( file ) id = _parse_document_id ( etree . parse ( file . open ( ) ) ) excludes = excludes or [ ] excludes . extend ( [ lambda filepath : filepath . name == COLLECTION_FILENAME , lambda filepath : filepath . is_dir ( ) , ] ) resources_paths = _find_resources ( path , excludes = excludes ) resources = tuple ( _resource_from_path ( res ) for res in resources_paths ) return Collection ( id , file , resources )
8399	def transform ( x ) : try : x = date2num ( x ) except AttributeError : x = [ pd . Timestamp ( item ) for item in x ] x = date2num ( x ) return x
1677	def ResetSection ( self , directive ) : self . _section = self . _INITIAL_SECTION self . _last_header = '' if directive in ( 'if' , 'ifdef' , 'ifndef' ) : self . include_list . append ( [ ] ) elif directive in ( 'else' , 'elif' ) : self . include_list [ - 1 ] = [ ]
13470	def copy ( src , dst ) : ( szip , dzip ) = ( src . endswith ( ".zip" ) , dst . endswith ( ".zip" ) ) logging . info ( "Copy: %s => %s" % ( src , dst ) ) if szip and dzip : shutil . copy2 ( src , dst ) elif szip : with zipfile . ZipFile ( src , mode = 'r' ) as z : tmpdir = tempfile . mkdtemp ( ) try : z . extractall ( tmpdir ) if len ( z . namelist ( ) ) != 1 : raise RuntimeError ( "The zip file '%s' should only have one " "compressed file" % src ) tmpfile = join ( tmpdir , z . namelist ( ) [ 0 ] ) try : os . remove ( dst ) except OSError : pass shutil . move ( tmpfile , dst ) finally : shutil . rmtree ( tmpdir , ignore_errors = True ) elif dzip : with zipfile . ZipFile ( dst , mode = 'w' , compression = ZIP_DEFLATED ) as z : z . write ( src , arcname = basename ( src ) ) else : shutil . copy2 ( src , dst )
11305	def autodiscover ( self , url ) : headers , response = fetch_url ( url ) if headers [ 'content-type' ] . split ( ';' ) [ 0 ] in ( 'application/json' , 'text/javascript' ) : provider_data = json . loads ( response ) return self . store_providers ( provider_data )
309	def plot_cones ( name , bounds , oos_returns , num_samples = 1000 , ax = None , cone_std = ( 1. , 1.5 , 2. ) , random_seed = None , num_strikes = 3 ) : if ax is None : fig = figure . Figure ( figsize = ( 10 , 8 ) ) FigureCanvasAgg ( fig ) axes = fig . add_subplot ( 111 ) else : axes = ax returns = ep . cum_returns ( oos_returns , starting_value = 1. ) bounds_tmp = bounds . copy ( ) returns_tmp = returns . copy ( ) cone_start = returns . index [ 0 ] colors = [ "green" , "orange" , "orangered" , "darkred" ] for c in range ( num_strikes + 1 ) : if c > 0 : tmp = returns . loc [ cone_start : ] bounds_tmp = bounds_tmp . iloc [ 0 : len ( tmp ) ] bounds_tmp = bounds_tmp . set_index ( tmp . index ) crossing = ( tmp < bounds_tmp [ float ( - 2. ) ] . iloc [ : len ( tmp ) ] ) if crossing . sum ( ) <= 0 : break cone_start = crossing . loc [ crossing ] . index [ 0 ] returns_tmp = returns . loc [ cone_start : ] bounds_tmp = ( bounds - ( 1 - returns . loc [ cone_start ] ) ) for std in cone_std : x = returns_tmp . index y1 = bounds_tmp [ float ( std ) ] . iloc [ : len ( returns_tmp ) ] y2 = bounds_tmp [ float ( - std ) ] . iloc [ : len ( returns_tmp ) ] axes . fill_between ( x , y1 , y2 , color = colors [ c ] , alpha = 0.5 ) label = 'Cumulative returns = {:.2f}%' . format ( ( returns . iloc [ - 1 ] - 1 ) * 100 ) axes . plot ( returns . index , returns . values , color = 'black' , lw = 3. , label = label ) if name is not None : axes . set_title ( name ) axes . axhline ( 1 , color = 'black' , alpha = 0.2 ) axes . legend ( frameon = True , framealpha = 0.5 ) if ax is None : return fig else : return axes
12231	def proxy_settings_module ( depth = 3 ) : proxies = [ ] modules = sys . modules module_name = get_frame_locals ( depth ) [ '__name__' ] module_real = modules [ module_name ] for name , locals_dict in traverse_local_prefs ( depth ) : value = locals_dict [ name ] if isinstance ( value , PrefProxy ) : proxies . append ( name ) new_module = type ( module_name , ( ModuleType , ModuleProxy ) , { } ) ( module_name ) new_module . bind ( module_real , proxies ) modules [ module_name ] = new_module
11897	def _create_index_files ( root_dir , force_no_processing = False ) : created_files = [ ] for here , dirs , files in os . walk ( root_dir ) : print ( 'Processing %s' % here ) dirs = sorted ( dirs ) image_files = [ f for f in files if re . match ( IMAGE_FILE_REGEX , f ) ] image_files = sorted ( image_files ) created_files . append ( _create_index_file ( root_dir , here , image_files , dirs , force_no_processing ) ) return created_files
6915	def collection_worker ( task ) : lcfile , outdir , kwargs = task try : fakelcresults = make_fakelc ( lcfile , outdir , ** kwargs ) return fakelcresults except Exception as e : LOGEXCEPTION ( 'could not process %s into a fakelc' % lcfile ) return None
12892	def handle_long ( self , item ) : doc = yield from self . handle_get ( item ) if doc is None : return None return int ( doc . value . u32 . text ) or None
150	def deepcopy ( self ) : polys = [ poly . deepcopy ( ) for poly in self . polygons ] return PolygonsOnImage ( polys , tuple ( self . shape ) )
1242	def _sample_with_priority ( self , p ) : parent = 0 while True : left = 2 * parent + 1 if left >= len ( self . _memory ) : return parent left_p = self . _memory [ left ] if left < self . _capacity - 1 else ( self . _memory [ left ] . priority or 0 ) if p <= left_p : parent = left else : if left + 1 >= len ( self . _memory ) : raise RuntimeError ( 'Right child is expected to exist.' ) p -= left_p parent = left + 1
4786	def ends_with ( self , suffix ) : if suffix is None : raise TypeError ( 'given suffix arg must not be none' ) if isinstance ( self . val , str_types ) : if not isinstance ( suffix , str_types ) : raise TypeError ( 'given suffix arg must be a string' ) if len ( suffix ) == 0 : raise ValueError ( 'given suffix arg must not be empty' ) if not self . val . endswith ( suffix ) : self . _err ( 'Expected <%s> to end with <%s>, but did not.' % ( self . val , suffix ) ) elif isinstance ( self . val , Iterable ) : if len ( self . val ) == 0 : raise ValueError ( 'val must not be empty' ) last = None for last in self . val : pass if last != suffix : self . _err ( 'Expected %s to end with <%s>, but did not.' % ( self . val , suffix ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
10579	def add_to ( self , other ) : if type ( other ) is MaterialPackage : if self . material == other . material : self . compound_masses += other . compound_masses else : for compound in other . material . compounds : if compound not in self . material . compounds : raise Exception ( "Packages of '" + other . material . name + "' cannot be added to packages of '" + self . material . name + "'. The compound '" + compound + "' was not found in '" + self . material . name + "'." ) self . add_to ( ( compound , other . get_compound_mass ( compound ) ) ) elif self . _is_compound_mass_tuple ( other ) : compound = other [ 0 ] compound_index = self . material . get_compound_index ( compound ) mass = other [ 1 ] self . compound_masses [ compound_index ] += mass else : raise TypeError ( 'Invalid addition argument.' )
4559	def next ( self , length ) : return Segment ( self . strip , length , self . offset + self . length )
122	def _augment_images_worker ( self , augseq , queue_source , queue_result , seedval ) : np . random . seed ( seedval ) random . seed ( seedval ) augseq . reseed ( seedval ) ia . seed ( seedval ) loader_finished = False while not loader_finished : try : batch_str = queue_source . get ( timeout = 0.1 ) batch = pickle . loads ( batch_str ) if batch is None : loader_finished = True queue_source . put ( pickle . dumps ( None , protocol = - 1 ) ) else : batch_aug = augseq . augment_batch ( batch ) batch_str = pickle . dumps ( batch_aug , protocol = - 1 ) queue_result . put ( batch_str ) except QueueEmpty : time . sleep ( 0.01 ) queue_result . put ( pickle . dumps ( None , protocol = - 1 ) ) time . sleep ( 0.01 )
5984	def image_psf_shape_tag_from_image_psf_shape ( image_psf_shape ) : if image_psf_shape is None : return '' else : y = str ( image_psf_shape [ 0 ] ) x = str ( image_psf_shape [ 1 ] ) return ( '_image_psf_' + y + 'x' + x )
9252	def generate_unreleased_section ( self ) : if not self . filtered_tags : return "" now = datetime . datetime . utcnow ( ) now = now . replace ( tzinfo = dateutil . tz . tzutc ( ) ) head_tag = { "name" : self . options . unreleased_label } self . tag_times_dict [ head_tag [ "name" ] ] = now unreleased_log = self . generate_log_between_tags ( self . filtered_tags [ 0 ] , head_tag ) return unreleased_log
9626	def detail_view ( self , request , module , preview ) : try : preview = self . __previews [ module ] [ preview ] except KeyError : raise Http404 return preview . detail_view ( request )
2475	def set_lic_name ( self , doc , name ) : if self . has_extr_lic ( doc ) : if not self . extr_lic_name_set : self . extr_lic_name_set = True if validations . validate_extr_lic_name ( name ) : self . extr_lic ( doc ) . full_name = name return True else : raise SPDXValueError ( 'ExtractedLicense::Name' ) else : raise CardinalityError ( 'ExtractedLicense::Name' ) else : raise OrderError ( 'ExtractedLicense::Name' )
10112	def rewrite ( fname , visitor , ** kw ) : if not isinstance ( fname , pathlib . Path ) : assert isinstance ( fname , string_types ) fname = pathlib . Path ( fname ) assert fname . is_file ( ) with tempfile . NamedTemporaryFile ( delete = False ) as fp : tmp = pathlib . Path ( fp . name ) with UnicodeReader ( fname , ** kw ) as reader_ : with UnicodeWriter ( tmp , ** kw ) as writer : for i , row in enumerate ( reader_ ) : row = visitor ( i , row ) if row is not None : writer . writerow ( row ) shutil . move ( str ( tmp ) , str ( fname ) )
1044	def float_unpack ( Q , size , le ) : if size == 8 : MIN_EXP = - 1021 MAX_EXP = 1024 MANT_DIG = 53 BITS = 64 elif size == 4 : MIN_EXP = - 125 MAX_EXP = 128 MANT_DIG = 24 BITS = 32 else : raise ValueError ( "invalid size value" ) if Q >> BITS : raise ValueError ( "input out of range" ) sign = Q >> BITS - 1 exp = ( Q & ( ( 1 << BITS - 1 ) - ( 1 << MANT_DIG - 1 ) ) ) >> MANT_DIG - 1 mant = Q & ( ( 1 << MANT_DIG - 1 ) - 1 ) if exp == MAX_EXP - MIN_EXP + 2 : result = float ( 'nan' ) if mant else float ( 'inf' ) elif exp == 0 : result = math . ldexp ( float ( mant ) , MIN_EXP - MANT_DIG ) else : mant += 1 << MANT_DIG - 1 result = math . ldexp ( float ( mant ) , exp + MIN_EXP - MANT_DIG - 1 ) return - result if sign else result
7690	def sonify ( annotation , sr = 22050 , duration = None , ** kwargs ) : length = None if duration is None : duration = annotation . duration if duration is not None : length = int ( duration * sr ) if annotation . namespace in SONIFY_MAPPING : ann = coerce_annotation ( annotation , annotation . namespace ) return SONIFY_MAPPING [ annotation . namespace ] ( ann , sr = sr , length = length , ** kwargs ) for namespace , func in six . iteritems ( SONIFY_MAPPING ) : try : ann = coerce_annotation ( annotation , namespace ) return func ( ann , sr = sr , length = length , ** kwargs ) except NamespaceError : pass raise NamespaceError ( 'Unable to sonify annotation of namespace="{:s}"' . format ( annotation . namespace ) )
8408	def expand_range_distinct ( range , expand = ( 0 , 0 , 0 , 0 ) , zero_width = 1 ) : if len ( expand ) == 2 : expand = tuple ( expand ) * 2 lower = expand_range ( range , expand [ 0 ] , expand [ 1 ] , zero_width ) [ 0 ] upper = expand_range ( range , expand [ 2 ] , expand [ 3 ] , zero_width ) [ 1 ] return ( lower , upper )
2381	def parse_and_process_args ( self , args ) : parser = argparse . ArgumentParser ( prog = "python -m rflint" , description = "A style checker for robot framework plain text files." , formatter_class = argparse . RawDescriptionHelpFormatter , epilog = ( "You can use 'all' in place of RULENAME to refer to all rules. \n" "\n" "For example: '--ignore all --warn DuplicateTestNames' will ignore all\n" "rules except DuplicateTestNames.\n" "\n" "FORMAT is a string that performs a substitution on the following \n" "patterns: {severity}, {linenumber}, {char}, {message}, and {rulename}.\n" "\n" "For example: --format 'line: {linenumber}: message: {message}'. \n" "\n" "ARGUMENTFILE is a filename with contents that match the format of \n" "standard robot framework argument files\n" "\n" "If you give a directory as an argument, all files in the directory\n" "with the suffix .txt, .robot or .tsv will be processed. With the \n" "--recursive option, subfolders within the directory will also be\n" "processed." ) ) parser . add_argument ( "--error" , "-e" , metavar = "RULENAME" , action = SetErrorAction , help = "Assign a severity of ERROR to the given RULENAME" ) parser . add_argument ( "--ignore" , "-i" , metavar = "RULENAME" , action = SetIgnoreAction , help = "Ignore the given RULENAME" ) parser . add_argument ( "--warning" , "-w" , metavar = "RULENAME" , action = SetWarningAction , help = "Assign a severity of WARNING for the given RULENAME" ) parser . add_argument ( "--list" , "-l" , action = "store_true" , help = "show a list of known rules and exit" ) parser . add_argument ( "--describe" , "-d" , action = "store_true" , help = "describe the given rules" ) parser . add_argument ( "--no-filenames" , action = "store_false" , dest = "print_filenames" , default = True , help = "suppress the printing of filenames" ) parser . add_argument ( "--format" , "-f" , help = "Define the output format" , default = '{severity}: {linenumber}, {char}: {message} ({rulename})' ) parser . add_argument ( "--version" , action = "store_true" , default = False , help = "Display version number and exit" ) parser . add_argument ( "--verbose" , "-v" , action = "store_true" , default = False , help = "Give verbose output" ) parser . add_argument ( "--configure" , "-c" , action = ConfigureAction , help = "Configure a rule" ) parser . add_argument ( "--recursive" , "-r" , action = "store_true" , default = False , help = "Recursively scan subfolders in a directory" ) parser . add_argument ( "--rulefile" , "-R" , action = RulefileAction , help = "import additional rules from the given RULEFILE" ) parser . add_argument ( "--argumentfile" , "-A" , action = ArgfileLoader , help = "read arguments from the given file" ) parser . add_argument ( 'args' , metavar = "file" , nargs = argparse . REMAINDER ) ns = argparse . Namespace ( ) setattr ( ns , "app" , self ) args = parser . parse_args ( args , ns ) Rule . output_format = args . format return args
8163	def _set_mode ( self , mode ) : if mode == CENTER : self . _call_transform_mode = self . _center_transform elif mode == CORNER : self . _call_transform_mode = self . _corner_transform else : raise ValueError ( 'mode must be CENTER or CORNER' )
10186	def _aggregations_config ( self ) : result = { } for ep in iter_entry_points ( group = self . entry_point_group_aggs ) : for cfg in ep . load ( ) ( ) : if cfg [ 'aggregation_name' ] not in self . enabled_aggregations : continue elif cfg [ 'aggregation_name' ] in result : raise DuplicateAggregationError ( 'Duplicate aggregation {0} in entry point ' '{1}' . format ( cfg [ 'event_type' ] , ep . name ) ) cfg . update ( self . enabled_aggregations [ cfg [ 'aggregation_name' ] ] or { } ) result [ cfg [ 'aggregation_name' ] ] = cfg return result
5365	def format ( self , record ) : if isinstance ( self . fmt , dict ) : self . _fmt = self . fmt [ record . levelname ] if sys . version_info > ( 3 , 2 ) : if self . style not in logging . _STYLES : raise ValueError ( 'Style must be one of: %s' % ',' . join ( list ( logging . _STYLES . keys ( ) ) ) ) self . _style = logging . _STYLES [ self . style ] [ 0 ] ( self . _fmt ) if sys . version_info > ( 2 , 7 ) : message = super ( LevelFormatter , self ) . format ( record ) else : message = ColoredFormatter . format ( self , record ) return message
8821	def make_case2 ( context ) : query = context . session . query ( models . IPAddress ) period_start , period_end = billing . calc_periods ( ) ip_list = billing . build_full_day_ips ( query , period_start , period_end ) import random ind = random . randint ( 0 , len ( ip_list ) - 1 ) address = ip_list [ ind ] address . allocated_at = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = 1 ) context . session . add ( address ) context . session . flush ( )
9763	def upload ( sync = True ) : project = ProjectManager . get_config_or_raise ( ) files = IgnoreManager . get_unignored_file_paths ( ) try : with create_tarfile ( files , project . name ) as file_path : with get_files_in_current_directory ( 'repo' , [ file_path ] ) as ( files , files_size ) : try : PolyaxonClient ( ) . project . upload_repo ( project . user , project . name , files , files_size , sync = sync ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not upload code for project `{}`.' . format ( project . name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) Printer . print_error ( 'Check the project exists, ' 'and that you have access rights, ' 'this could happen as well when uploading large files.' 'If you are running a notebook and mounting the code to the notebook, ' 'you should stop it before uploading.' ) sys . exit ( 1 ) Printer . print_success ( 'Files uploaded.' ) except Exception as e : Printer . print_error ( "Could not upload the file." ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
3667	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : Cpgms = [ i ( T ) for i in self . HeatCapacityGases ] return mixing_simple ( zs , Cpgms ) else : raise Exception ( 'Method not valid' )
48	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , alpha = 1.0 , size = 3 , copy = True , raise_if_out_of_image = False ) : if copy : image = np . copy ( image ) if image . ndim == 2 : assert ia . is_single_number ( color ) , ( "Got a 2D image. Expected then 'color' to be a single number, " "but got %s." % ( str ( color ) , ) ) elif image . ndim == 3 and ia . is_single_number ( color ) : color = [ color ] * image . shape [ - 1 ] input_dtype = image . dtype alpha_color = color if alpha < 0.01 : return image elif alpha > 0.99 : alpha = 1 else : image = image . astype ( np . float32 , copy = False ) alpha_color = alpha * np . array ( color ) height , width = image . shape [ 0 : 2 ] y , x = self . y_int , self . x_int x1 = max ( x - size // 2 , 0 ) x2 = min ( x + 1 + size // 2 , width ) y1 = max ( y - size // 2 , 0 ) y2 = min ( y + 1 + size // 2 , height ) x1_clipped , x2_clipped = np . clip ( [ x1 , x2 ] , 0 , width ) y1_clipped , y2_clipped = np . clip ( [ y1 , y2 ] , 0 , height ) x1_clipped_ooi = ( x1_clipped < 0 or x1_clipped >= width ) x2_clipped_ooi = ( x2_clipped < 0 or x2_clipped >= width + 1 ) y1_clipped_ooi = ( y1_clipped < 0 or y1_clipped >= height ) y2_clipped_ooi = ( y2_clipped < 0 or y2_clipped >= height + 1 ) x_ooi = ( x1_clipped_ooi and x2_clipped_ooi ) y_ooi = ( y1_clipped_ooi and y2_clipped_ooi ) x_zero_size = ( x2_clipped - x1_clipped ) < 1 y_zero_size = ( y2_clipped - y1_clipped ) < 1 if not x_ooi and not y_ooi and not x_zero_size and not y_zero_size : if alpha == 1 : image [ y1_clipped : y2_clipped , x1_clipped : x2_clipped ] = color else : image [ y1_clipped : y2_clipped , x1_clipped : x2_clipped ] = ( ( 1 - alpha ) * image [ y1_clipped : y2_clipped , x1_clipped : x2_clipped ] + alpha_color ) else : if raise_if_out_of_image : raise Exception ( "Cannot draw keypoint x=%.8f, y=%.8f on image with " "shape %s." % ( y , x , image . shape ) ) if image . dtype . name != input_dtype . name : if input_dtype . name == "uint8" : image = np . clip ( image , 0 , 255 , out = image ) image = image . astype ( input_dtype , copy = False ) return image
5290	def construct_inlines ( self ) : inline_formsets = [ ] for inline_class in self . get_inlines ( ) : inline_instance = inline_class ( self . model , self . request , self . object , self . kwargs , self ) inline_formset = inline_instance . construct_formset ( ) inline_formsets . append ( inline_formset ) return inline_formsets
9338	def map ( self , func , sequence , reduce = None , star = False , minlength = 0 ) : def realreduce ( r ) : if reduce : if isinstance ( r , tuple ) : return reduce ( * r ) else : return reduce ( r ) return r def realfunc ( i ) : if star : return func ( * i ) else : return func ( i ) if len ( sequence ) <= 0 or self . np == 0 or get_debug ( ) : self . local = lambda : None self . local . rank = 0 rt = [ realreduce ( realfunc ( i ) ) for i in sequence ] self . local = None return rt np = min ( [ self . np , len ( sequence ) ] ) Q = self . backend . QueueFactory ( 64 ) R = self . backend . QueueFactory ( 64 ) self . ordered . reset ( ) pg = ProcessGroup ( main = self . _main , np = np , backend = self . backend , args = ( Q , R , sequence , realfunc ) ) pg . start ( ) L = [ ] N = [ ] def feeder ( pg , Q , N ) : j = 0 try : for i , work in enumerate ( sequence ) : if not hasattr ( sequence , '__getitem__' ) : pg . put ( Q , ( i , work ) ) else : pg . put ( Q , ( i , ) ) j = j + 1 N . append ( j ) for i in range ( np ) : pg . put ( Q , None ) except StopProcessGroup : return finally : pass feeder = threading . Thread ( None , feeder , args = ( pg , Q , N ) ) feeder . start ( ) count = 0 try : while True : try : capsule = pg . get ( R ) except queue . Empty : continue except StopProcessGroup : raise pg . get_exception ( ) capsule = capsule [ 0 ] , realreduce ( capsule [ 1 ] ) heapq . heappush ( L , capsule ) count = count + 1 if len ( N ) > 0 and count == N [ 0 ] : break rt = [ ] while len ( L ) > 0 : rt . append ( heapq . heappop ( L ) [ 1 ] ) pg . join ( ) feeder . join ( ) assert N [ 0 ] == len ( rt ) return rt except BaseException as e : pg . killall ( ) pg . join ( ) feeder . join ( ) raise
7568	def comp ( seq ) : return seq . replace ( "A" , 't' ) . replace ( 'T' , 'a' ) . replace ( 'C' , 'g' ) . replace ( 'G' , 'c' ) . replace ( 'n' , 'Z' ) . upper ( ) . replace ( "Z" , "n" )
9490	def _get_const_info ( const_index , const_list ) : argval = const_index if const_list is not None : try : argval = const_list [ const_index ] except IndexError : raise ValidationError ( "Consts value out of range: {}" . format ( const_index ) ) from None return argval , repr ( argval )
10426	def enrich_internal_unqualified_edges ( graph , subgraph ) : for u , v in itt . combinations ( subgraph , 2 ) : if not graph . has_edge ( u , v ) : continue for k in graph [ u ] [ v ] : if k < 0 : subgraph . add_edge ( u , v , key = k , ** graph [ u ] [ v ] [ k ] )
11179	def authorize_url ( self ) : auth_url = OAUTH_ROOT + '/authorize' params = { 'client_id' : self . client_id , 'redirect_uri' : self . redirect_uri , } return "{}?{}" . format ( auth_url , urlencode ( params ) )
7873	def get_all_payload ( self , specialize = False ) : if self . _payload is None : self . decode_payload ( specialize ) elif specialize : for i , payload in enumerate ( self . _payload ) : if isinstance ( payload , XMLPayload ) : klass = payload_class_for_element_name ( payload . element . tag ) if klass is not XMLPayload : payload = klass . from_xml ( payload . element ) self . _payload [ i ] = payload return list ( self . _payload )
4910	def _create_session ( self , scope ) : now = datetime . datetime . utcnow ( ) if self . session is None or self . expires_at is None or now >= self . expires_at : if self . session : self . session . close ( ) oauth_access_token , expires_at = self . _get_oauth_access_token ( self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . degreed_user_id , self . enterprise_configuration . degreed_user_password , scope ) session = requests . Session ( ) session . timeout = self . SESSION_TIMEOUT session . headers [ 'Authorization' ] = 'Bearer {}' . format ( oauth_access_token ) session . headers [ 'content-type' ] = 'application/json' self . session = session self . expires_at = expires_at
12014	def calc_centroids ( self ) : self . cm = np . zeros ( ( len ( self . postcard ) , 2 ) ) for i in range ( len ( self . postcard ) ) : target = self . postcard [ i ] target [ self . targets != 1 ] = 0.0 self . cm [ i ] = center_of_mass ( target )
7625	def pattern ( ref , est , ** kwargs ) : r namespace = 'pattern_jku' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_patterns = pattern_to_mireval ( ref ) est_patterns = pattern_to_mireval ( est ) return mir_eval . pattern . evaluate ( ref_patterns , est_patterns , ** kwargs )
13742	def get_conn ( self , aws_access_key = None , aws_secret_key = None ) : return boto . connect_dynamodb ( aws_access_key_id = aws_access_key , aws_secret_access_key = aws_secret_key , )
84	def CoarsePepper ( p = 0 , size_px = None , size_percent = None , per_channel = False , min_size = 4 , name = None , deterministic = False , random_state = None ) : mask = iap . handle_probability_param ( p , "p" , tuple_to_uniform = True , list_to_choice = True ) if size_px is not None : mask_low = iap . FromLowerResolution ( other_param = mask , size_px = size_px , min_size = min_size ) elif size_percent is not None : mask_low = iap . FromLowerResolution ( other_param = mask , size_percent = size_percent , min_size = min_size ) else : raise Exception ( "Either size_px or size_percent must be set." ) replacement01 = iap . ForceSign ( iap . Beta ( 0.5 , 0.5 ) - 0.5 , positive = False , mode = "invert" ) + 0.5 replacement = replacement01 * 255 if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return ReplaceElementwise ( mask = mask_low , replacement = replacement , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
4714	def trun_to_file ( trun , fpath = None ) : if fpath is None : fpath = yml_fpath ( trun [ "conf" ] [ "OUTPUT" ] ) with open ( fpath , 'w' ) as yml_file : data = yaml . dump ( trun , explicit_start = True , default_flow_style = False ) yml_file . write ( data )
2043	def current_human_transaction ( self ) : try : tx , _ , _ , _ , _ = self . _callstack [ 0 ] if tx . result is not None : return None assert tx . depth == 0 return tx except IndexError : return None
10127	def draw ( self ) : if self . enabled : self . _vertex_list . colors = self . _gl_colors self . _vertex_list . vertices = self . _gl_vertices self . _vertex_list . draw ( pyglet . gl . GL_TRIANGLES )
8290	def _description ( self ) : meta = self . find ( "meta" , { "name" : "description" } ) if isinstance ( meta , dict ) and meta . has_key ( "content" ) : return meta [ "content" ] else : return u""
1968	def wait ( self , readfds , writefds , timeout ) : logger . debug ( "WAIT:" ) logger . debug ( f"\tProcess {self._current} is going to wait for [ {readfds!r} {writefds!r} {timeout!r} ]" ) logger . debug ( f"\tProcess: {self.procs!r}" ) logger . debug ( f"\tRunning: {self.running!r}" ) logger . debug ( f"\tRWait: {self.rwait!r}" ) logger . debug ( f"\tTWait: {self.twait!r}" ) logger . debug ( f"\tTimers: {self.timers!r}" ) for fd in readfds : self . rwait [ fd ] . add ( self . _current ) for fd in writefds : self . twait [ fd ] . add ( self . _current ) if timeout is not None : self . timers [ self . _current ] = self . clocks + timeout procid = self . _current next_index = ( self . running . index ( procid ) + 1 ) % len ( self . running ) self . _current = self . running [ next_index ] logger . debug ( f"\tTransfer control from process {procid} to {self._current}" ) logger . debug ( f"\tREMOVING {procid!r} from {self.running!r}. Current: {self._current!r}" ) self . running . remove ( procid ) if self . _current not in self . running : logger . debug ( "\tCurrent not running. Checking for timers..." ) self . _current = None self . check_timers ( )
12018	def disassemble ( self ) : ser_pb = open ( self . input_file , 'rb' ) . read ( ) fd = FileDescriptorProto ( ) fd . ParseFromString ( ser_pb ) self . name = fd . name self . _print ( '// Reversed by pbd (https://github.com/rsc-dev/pbd)' ) self . _print ( 'syntax = "proto2";' ) self . _print ( '' ) if len ( fd . package ) > 0 : self . _print ( 'package {};' . format ( fd . package ) ) self . package = fd . package else : self . _print ( '// Package not defined' ) self . _walk ( fd )
13747	def get_item ( self , hash_key , start = 0 , extra_attrs = None ) : table = self . get_table ( ) try : item = table . get_item ( hash_key = hash_key ) except DynamoDBKeyNotFoundError : item = None if item is None : item = self . create_item ( hash_key = hash_key , start = start , extra_attrs = extra_attrs , ) return item
7457	def putstats ( pfile , handle , statdicts ) : with open ( pfile , 'r' ) as infile : filestats , samplestats = pickle . load ( infile ) perfile , fsamplehits , fbarhits , fmisses , fdbars = statdicts perfile [ handle ] += filestats samplehits , barhits , misses , dbars = samplestats fsamplehits . update ( samplehits ) fbarhits . update ( barhits ) fmisses . update ( misses ) fdbars . update ( dbars ) statdicts = perfile , fsamplehits , fbarhits , fmisses , fdbars return statdicts
462	def exit_tensorflow ( sess = None , port = 6006 ) : text = "[TL] Close tensorboard and nvidia-process if available" text2 = "[TL] Close tensorboard and nvidia-process not yet supported by this function (tl.ops.exit_tf) on " if sess is not None : sess . close ( ) if _platform == "linux" or _platform == "linux2" : tl . logging . info ( 'linux: %s' % text ) os . system ( 'nvidia-smi' ) os . system ( 'fuser ' + port + '/tcp -k' ) os . system ( "nvidia-smi | grep python |awk '{print $3}'|xargs kill" ) _exit ( ) elif _platform == "darwin" : tl . logging . info ( 'OS X: %s' % text ) subprocess . Popen ( "lsof -i tcp:" + str ( port ) + " | grep -v PID | awk '{print $2}' | xargs kill" , shell = True ) elif _platform == "win32" : raise NotImplementedError ( "this function is not supported on the Windows platform" ) else : tl . logging . info ( text2 + _platform )
3945	def _decode_field ( message , field , value ) : if field . type == FieldDescriptor . TYPE_MESSAGE : decode ( getattr ( message , field . name ) , value ) else : try : if field . type == FieldDescriptor . TYPE_BYTES : value = base64 . b64decode ( value ) setattr ( message , field . name , value ) except ( ValueError , TypeError ) as e : logger . warning ( 'Message %r ignoring field %s: %s' , message . __class__ . __name__ , field . name , e )
6502	def strings_in_dictionary ( dictionary ) : strings = [ value for value in six . itervalues ( dictionary ) if not isinstance ( value , dict ) ] for child_dict in [ dv for dv in six . itervalues ( dictionary ) if isinstance ( dv , dict ) ] : strings . extend ( SearchResultProcessor . strings_in_dictionary ( child_dict ) ) return strings
7785	def error ( self , error_data ) : if not self . active : return if not self . _try_backup_item ( ) : self . _error_handler ( self . address , error_data ) self . cache . invalidate_object ( self . address ) self . _deactivate ( )
8736	def construct_datetime ( cls , * args , ** kwargs ) : if len ( args ) == 1 : arg = args [ 0 ] method = cls . __get_dt_constructor ( type ( arg ) . __module__ , type ( arg ) . __name__ , ) result = method ( arg ) try : result = result . replace ( tzinfo = kwargs . pop ( 'tzinfo' ) ) except KeyError : pass if kwargs : first_key = kwargs . keys ( ) [ 0 ] tmpl = ( "{first_key} is an invalid keyword " "argument for this function." ) raise TypeError ( tmpl . format ( ** locals ( ) ) ) else : result = datetime . datetime ( * args , ** kwargs ) return result
1760	def read_bytes ( self , where , size , force = False ) : result = [ ] for i in range ( size ) : result . append ( Operators . CHR ( self . read_int ( where + i , 8 , force ) ) ) return result
3832	async def send_chat_message ( self , send_chat_message_request ) : response = hangouts_pb2 . SendChatMessageResponse ( ) await self . _pb_request ( 'conversations/sendchatmessage' , send_chat_message_request , response ) return response
4849	def _serialize_items ( self , channel_metadata_items ) : return json . dumps ( self . _prepare_items_for_transmission ( channel_metadata_items ) , sort_keys = True ) . encode ( 'utf-8' )
1531	def get_pplan ( self , topologyName , callback = None ) : if callback : self . pplan_watchers [ topologyName ] . append ( callback ) else : pplan_path = self . get_pplan_path ( topologyName ) with open ( pplan_path ) as f : data = f . read ( ) pplan = PhysicalPlan ( ) pplan . ParseFromString ( data ) return pplan
10057	def delete ( self , pid , record , key ) : try : del record . files [ str ( key ) ] record . commit ( ) db . session . commit ( ) return make_response ( '' , 204 ) except KeyError : abort ( 404 , 'The specified object does not exist or has already ' 'been deleted.' )
4366	def process_event ( self , packet ) : args = packet [ 'args' ] name = packet [ 'name' ] if not allowed_event_name_regex . match ( name ) : self . error ( "unallowed_event_name" , "name must only contains alpha numerical characters" ) return method_name = 'on_' + name . replace ( ' ' , '_' ) return self . call_method_with_acl ( method_name , packet , * args )
8358	def shoebot_example ( ** shoebot_kwargs ) : def decorator ( f ) : def run ( ) : from shoebot import ShoebotInstallError print ( " Shoebot - %s:" % f . __name__ . replace ( "_" , " " ) ) try : import shoebot outputfile = "/tmp/shoebot-%s.png" % f . __name__ bot = shoebot . create_bot ( outputfile = outputfile ) f ( bot ) bot . finish ( ) print ( ' [passed] : %s' % outputfile ) print ( '' ) except ShoebotInstallError as e : print ( ' [failed]' , e . args [ 0 ] ) print ( '' ) except Exception : print ( ' [failed] - traceback:' ) for line in traceback . format_exc ( ) . splitlines ( ) : print ( ' %s' % line ) print ( '' ) return run return decorator
314	def rolling_beta ( returns , factor_returns , rolling_window = APPROX_BDAYS_PER_MONTH * 6 ) : if factor_returns . ndim > 1 : return factor_returns . apply ( partial ( rolling_beta , returns ) , rolling_window = rolling_window ) else : out = pd . Series ( index = returns . index ) for beg , end in zip ( returns . index [ 0 : - rolling_window ] , returns . index [ rolling_window : ] ) : out . loc [ end ] = ep . beta ( returns . loc [ beg : end ] , factor_returns . loc [ beg : end ] ) return out
12124	def to_table ( args , vdims = [ ] ) : "Helper function to convet an Args object to a HoloViews Table" if not Table : return "HoloViews Table not available" kdims = [ dim for dim in args . constant_keys + args . varying_keys if dim not in vdims ] items = [ tuple ( [ spec [ k ] for k in kdims + vdims ] ) for spec in args . specs ] return Table ( items , kdims = kdims , vdims = vdims )
10776	def finalize ( self , result = None ) : if not self . settings_path : return from django . test . utils import teardown_test_environment from django . db import connection from django . conf import settings self . call_plugins_method ( 'beforeDestroyTestDb' , settings , connection ) try : connection . creation . destroy_test_db ( self . old_db , verbosity = self . verbosity , ) except Exception : pass self . call_plugins_method ( 'afterDestroyTestDb' , settings , connection ) self . call_plugins_method ( 'beforeTeardownTestEnv' , settings , teardown_test_environment ) teardown_test_environment ( ) self . call_plugins_method ( 'afterTeardownTestEnv' , settings )
10000	def clear_obj ( self , obj ) : obj_nodes = self . get_nodes_with ( obj ) removed = set ( ) for node in obj_nodes : if self . has_node ( node ) : removed . update ( self . clear_descendants ( node ) ) return removed
6526	def parse ( cls , content , is_pyproject = False ) : parsed = pytoml . loads ( content ) if is_pyproject : parsed = parsed . get ( 'tool' , { } ) parsed = parsed . get ( 'tidypy' , { } ) return parsed
2699	def render_ranks ( graph , ranks , dot_file = "graph.dot" ) : if dot_file : write_dot ( graph , ranks , path = dot_file )
10359	def shuffle_relations ( graph : BELGraph , percentage : Optional [ str ] = None ) -> BELGraph : percentage = percentage or 0.3 assert 0 < percentage <= 1 n = graph . number_of_edges ( ) swaps = int ( percentage * n * ( n - 1 ) / 2 ) result : BELGraph = graph . copy ( ) edges = result . edges ( keys = True ) for _ in range ( swaps ) : ( s1 , t1 , k1 ) , ( s2 , t2 , k2 ) = random . sample ( edges , 2 ) result [ s1 ] [ t1 ] [ k1 ] , result [ s2 ] [ t2 ] [ k2 ] = result [ s2 ] [ t2 ] [ k2 ] , result [ s1 ] [ t1 ] [ k1 ] return result
1098	def get_close_matches ( word , possibilities , n = 3 , cutoff = 0.6 ) : if not n > 0 : raise ValueError ( "n must be > 0: %r" % ( n , ) ) if not 0.0 <= cutoff <= 1.0 : raise ValueError ( "cutoff must be in [0.0, 1.0]: %r" % ( cutoff , ) ) result = [ ] s = SequenceMatcher ( ) s . set_seq2 ( word ) for x in possibilities : s . set_seq1 ( x ) if s . real_quick_ratio ( ) >= cutoff and s . quick_ratio ( ) >= cutoff and s . ratio ( ) >= cutoff : result . append ( ( s . ratio ( ) , x ) ) result = heapq . nlargest ( n , result ) return [ x for score , x in result ]
12681	def copy_attributes ( source , destination , ignore_patterns = [ ] ) : for attr in _wildcard_filter ( dir ( source ) , * ignore_patterns ) : setattr ( destination , attr , getattr ( source , attr ) )
8462	def set_cmd_env_var ( value ) : def func_decorator ( function ) : @ functools . wraps ( function ) def wrapper ( * args , ** kwargs ) : previous_cmd_env_var = os . getenv ( temple . constants . TEMPLE_ENV_VAR ) os . environ [ temple . constants . TEMPLE_ENV_VAR ] = value try : ret_val = function ( * args , ** kwargs ) finally : if previous_cmd_env_var is None : del os . environ [ temple . constants . TEMPLE_ENV_VAR ] else : os . environ [ temple . constants . TEMPLE_ENV_VAR ] = previous_cmd_env_var return ret_val return wrapper return func_decorator
3405	def _sample_chain ( args ) : n , idx = args center = sampler . center np . random . seed ( ( sampler . _seed + idx ) % np . iinfo ( np . int32 ) . max ) pi = np . random . randint ( sampler . n_warmup ) prev = sampler . warmup [ pi , ] prev = step ( sampler , center , prev - center , 0.95 ) n_samples = max ( sampler . n_samples , 1 ) samples = np . zeros ( ( n , center . shape [ 0 ] ) ) for i in range ( 1 , sampler . thinning * n + 1 ) : pi = np . random . randint ( sampler . n_warmup ) delta = sampler . warmup [ pi , ] - center prev = step ( sampler , prev , delta ) if sampler . problem . homogeneous and ( n_samples * sampler . thinning % sampler . nproj == 0 ) : prev = sampler . _reproject ( prev ) center = sampler . _reproject ( center ) if i % sampler . thinning == 0 : samples [ i // sampler . thinning - 1 , ] = prev center = ( ( n_samples * center ) / ( n_samples + 1 ) + prev / ( n_samples + 1 ) ) n_samples += 1 return ( sampler . retries , samples )
3403	def find_boundary_types ( model , boundary_type , external_compartment = None ) : if not model . boundary : LOGGER . warning ( "There are no boundary reactions in this model. " "Therefore specific types of boundary reactions such " "as 'exchanges', 'demands' or 'sinks' cannot be " "identified." ) return [ ] if external_compartment is None : external_compartment = find_external_compartment ( model ) return model . reactions . query ( lambda r : is_boundary_type ( r , boundary_type , external_compartment ) )
989	def add ( reader , writer , column , start , stop , value ) : for i , row in enumerate ( reader ) : if i >= start and i <= stop : row [ column ] = type ( value ) ( row [ column ] ) + value writer . appendRecord ( row )
13279	def child_begin_handler ( self , scache , * args ) : pdesc = self . pdesc depth = scache . depth sib_seq = self . sib_seq sibs_len = self . sibs_len pdesc_level = scache . pdesc_level desc = copy . deepcopy ( pdesc ) desc = reset_parent_desc_template ( desc ) desc [ 'depth' ] = depth desc [ 'parent_breadth_path' ] = copy . deepcopy ( desc [ 'breadth_path' ] ) desc [ 'sib_seq' ] = sib_seq desc [ 'parent_path' ] = copy . deepcopy ( desc [ 'path' ] ) desc [ 'path' ] . append ( sib_seq ) update_desc_lsib_path ( desc ) update_desc_rsib_path ( desc , sibs_len ) if ( depth == 1 ) : pass else : update_desc_lcin_path ( desc , pdesc_level ) update_desc_rcin_path ( desc , sibs_len , pdesc_level ) return ( desc )
360	def load_folder_list ( path = "" ) : return [ os . path . join ( path , o ) for o in os . listdir ( path ) if os . path . isdir ( os . path . join ( path , o ) ) ]
9529	def get_encrypted_field ( base_class ) : assert not isinstance ( base_class , models . Field ) field_name = 'Encrypted' + base_class . __name__ if base_class not in FIELD_CACHE : FIELD_CACHE [ base_class ] = type ( field_name , ( EncryptedMixin , base_class ) , { 'base_class' : base_class , } ) return FIELD_CACHE [ base_class ]
13645	def parse ( parser , argv = None , settings_key = 'settings' , no_args_func = None ) : argv = argv or sys . argv commands = command_list ( ) if type ( argv ) not in [ list , tuple ] : raise TypeError ( "argv only can be list or tuple" ) if len ( argv ) >= 2 and argv [ 1 ] in commands : sub_parsers = parser . add_subparsers ( ) class_name = argv [ 1 ] . capitalize ( ) + 'Component' from cliez . conf import ( COMPONENT_ROOT , LOGGING_CONFIG , EPILOG , GENERAL_ARGUMENTS ) sys . path . insert ( 0 , os . path . dirname ( COMPONENT_ROOT ) ) mod = importlib . import_module ( '{}.components.{}' . format ( os . path . basename ( COMPONENT_ROOT ) , argv [ 1 ] ) ) klass = getattr ( mod , class_name ) sub_parser = append_arguments ( klass , sub_parsers , EPILOG , GENERAL_ARGUMENTS ) options = parser . parse_args ( argv [ 1 : ] ) settings = Settings . bind ( getattr ( options , settings_key ) ) if settings_key and hasattr ( options , settings_key ) else None obj = klass ( parser , sub_parser , options , settings ) logger_level = logging . CRITICAL if hasattr ( options , 'verbose' ) : if options . verbose == 1 : logger_level = logging . ERROR elif options . verbose == 2 : logger_level = logging . WARNING elif options . verbose == 3 : logger_level = logging . INFO obj . logger . setLevel ( logging . INFO ) pass if hasattr ( options , 'debug' ) and options . debug : logger_level = logging . DEBUG try : import http . client as http_client http_client . HTTPConnection . debuglevel = 1 except Exception : pass pass loggers = LOGGING_CONFIG [ 'loggers' ] for k , v in loggers . items ( ) : v . setdefault ( 'level' , logger_level ) if logger_level in [ logging . INFO , logging . DEBUG ] : v [ 'handlers' ] = [ 'stdout' ] pass logging_config . dictConfig ( LOGGING_CONFIG ) obj . run ( options ) return obj if not parser . description and len ( commands ) : sub_parsers = parser . add_subparsers ( ) [ sub_parsers . add_parser ( v ) for v in commands ] pass pass options = parser . parse_args ( argv [ 1 : ] ) if no_args_func and callable ( no_args_func ) : return no_args_func ( options ) else : parser . _print_message ( "nothing to do...\n" ) pass
10249	def highlight_nodes ( graph : BELGraph , nodes : Optional [ Iterable [ BaseEntity ] ] = None , color : Optional [ str ] = None ) : color = color or NODE_HIGHLIGHT_DEFAULT_COLOR for node in nodes if nodes is not None else graph : graph . node [ node ] [ NODE_HIGHLIGHT ] = color
10891	def intersection ( tiles , * args ) : tiles = listify ( tiles ) + listify ( args ) if len ( tiles ) < 2 : return tiles [ 0 ] tile = tiles [ 0 ] l , r = tile . l . copy ( ) , tile . r . copy ( ) for tile in tiles [ 1 : ] : l = amax ( l , tile . l ) r = amin ( r , tile . r ) return Tile ( l , r , dtype = l . dtype )
679	def getRecord ( self , n = None ) : if n is None : assert len ( self . fields ) > 0 n = self . fields [ 0 ] . numRecords - 1 assert ( all ( field . numRecords > n for field in self . fields ) ) record = [ field . values [ n ] for field in self . fields ] return record
7101	def on_marker ( self , mid ) : self . marker = Circle ( __id__ = mid ) self . parent ( ) . markers [ mid ] = self self . marker . setTag ( mid ) d = self . declaration if d . clickable : self . set_clickable ( d . clickable ) del self . options
8377	def get_attribute ( element , attribute , default = 0 ) : a = element . getAttribute ( attribute ) if a == "" : return default return a
10591	def create_transaction ( self , name , description = None , tx_date = datetime . min . date ( ) , dt_account = None , cr_account = None , source = None , amount = 0.00 ) : new_tx = Transaction ( name , description , tx_date , dt_account , cr_account , source , amount ) self . transactions . append ( new_tx ) return new_tx
4305	def soxi ( filepath , argument ) : if argument not in SOXI_ARGS : raise ValueError ( "Invalid argument '{}' to SoXI" . format ( argument ) ) args = [ 'sox' , '--i' ] args . append ( "-{}" . format ( argument ) ) args . append ( filepath ) try : shell_output = subprocess . check_output ( args , stderr = subprocess . PIPE ) except CalledProcessError as cpe : logger . info ( "SoXI error message: {}" . format ( cpe . output ) ) raise SoxiError ( "SoXI failed with exit code {}" . format ( cpe . returncode ) ) shell_output = shell_output . decode ( "utf-8" ) return str ( shell_output ) . strip ( '\n' )
8638	def revoke_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'revoke' } endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotRevokedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
1315	def DeleteLog ( ) -> None : if os . path . exists ( Logger . FileName ) : os . remove ( Logger . FileName )
6038	def yticks ( self ) : return np . linspace ( np . min ( self [ : , 0 ] ) , np . max ( self [ : , 0 ] ) , 4 )
10247	def enrich_pubmed_citations ( graph : BELGraph , manager : Manager ) -> Set [ str ] : pmids = get_pubmed_identifiers ( graph ) pmid_data , errors = get_citations_by_pmids ( manager = manager , pmids = pmids ) for u , v , k in filter_edges ( graph , has_pubmed ) : pmid = graph [ u ] [ v ] [ k ] [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) if pmid not in pmid_data : log . warning ( 'Missing data for PubMed identifier: %s' , pmid ) errors . add ( pmid ) continue graph [ u ] [ v ] [ k ] [ CITATION ] . update ( pmid_data [ pmid ] ) return errors
7839	def set_action ( self , action ) : if action is None : if self . xmlnode . hasProp ( "action" ) : self . xmlnode . unsetProp ( "action" ) return if action not in ( "remove" , "update" ) : raise ValueError ( "Action must be 'update' or 'remove'" ) action = unicode ( action ) self . xmlnode . setProp ( "action" , action . encode ( "utf-8" ) )
9603	def from_object ( cls , obj ) : return cls ( obj . get ( 'sessionId' , None ) , obj . get ( 'status' , 0 ) , obj . get ( 'value' , None ) )
3852	def _get_lookup_spec ( identifier ) : if identifier . startswith ( '+' ) : return hangups . hangouts_pb2 . EntityLookupSpec ( phone = identifier , create_offnetwork_gaia = True ) elif '@' in identifier : return hangups . hangouts_pb2 . EntityLookupSpec ( email = identifier , create_offnetwork_gaia = True ) else : return hangups . hangouts_pb2 . EntityLookupSpec ( gaia_id = identifier )
8508	def _get_dataset ( self , X , y = None ) : from pylearn2 . datasets import DenseDesignMatrix X = np . asarray ( X ) assert X . ndim > 1 if y is not None : y = self . _get_labels ( y ) if X . ndim == 2 : return DenseDesignMatrix ( X = X , y = y ) return DenseDesignMatrix ( topo_view = X , y = y )
11904	def rand_blend_mask ( shape , rand = rand . uniform ( - 10 , 10 ) , ** kwargs ) : z = rand ( shape [ 0 ] ) noise = snoise2dz ( ( shape [ 1 ] , shape [ 2 ] ) , z , ** kwargs ) return noise
4358	def send_packet ( self , pkt ) : self . put_client_msg ( packet . encode ( pkt , self . json_dumps ) )
8341	def _invert ( h ) : "Cheap function to invert a hash." i = { } for k , v in h . items ( ) : i [ v ] = k return i
4144	def CHOLESKY ( A , B , method = 'scipy' ) : if method == 'numpy_solver' : X = _numpy_solver ( A , B ) return X elif method == 'numpy' : X , _L = _numpy_cholesky ( A , B ) return X elif method == 'scipy' : import scipy . linalg L = scipy . linalg . cholesky ( A ) X = scipy . linalg . cho_solve ( ( L , False ) , B ) else : raise ValueError ( 'method must be numpy_solver, numpy_cholesky or cholesky_inplace' ) return X
10663	def elements ( compounds ) : elementlist = [ parse_compound ( compound ) . count ( ) . keys ( ) for compound in compounds ] return set ( ) . union ( * elementlist )
10940	def calc_J ( self ) : del self . J self . J = np . zeros ( [ self . param_vals . size , self . data . size ] ) dp = np . zeros_like ( self . param_vals ) f0 = self . model . copy ( ) for a in range ( self . param_vals . size ) : dp *= 0 dp [ a ] = self . dl [ a ] f1 = self . func ( self . param_vals + dp , * self . func_args , ** self . func_kwargs ) grad_func = ( f1 - f0 ) / dp [ a ] self . J [ a ] = - grad_func
3477	def _dissociate_gene ( self , cobra_gene ) : self . _genes . discard ( cobra_gene ) cobra_gene . _reaction . discard ( self )
12457	def iteritems ( data , ** kwargs ) : return iter ( data . items ( ** kwargs ) ) if IS_PY3 else data . iteritems ( ** kwargs )
1563	def add_task_hook ( self , task_hook ) : if not isinstance ( task_hook , ITaskHook ) : raise TypeError ( "In add_task_hook(): attempt to add non ITaskHook instance, given: %s" % str ( type ( task_hook ) ) ) self . task_hooks . append ( task_hook )
10603	def calculate ( self , ** state ) : T = state [ 'T' ] y_C = state [ 'y_C' ] y_H = state [ 'y_H' ] y_O = state [ 'y_O' ] y_N = state [ 'y_N' ] y_S = state [ 'y_S' ] a = self . _calc_a ( y_C , y_H , y_O , y_N , y_S ) / 1000 result = ( R / a ) * ( 380 * self . _calc_g0 ( 380 / T ) + 3600 * self . _calc_g0 ( 1800 / T ) ) return result
8414	def round_any ( x , accuracy , f = np . round ) : if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) return f ( x / accuracy ) * accuracy
4765	def is_not_equal_to ( self , other ) : if self . val == other : self . _err ( 'Expected <%s> to be not equal to <%s>, but was.' % ( self . val , other ) ) return self
11106	def get_pickling_errors ( obj , seen = None ) : if seen == None : seen = [ ] if hasattr ( obj , "__getstate__" ) : state = obj . __getstate__ ( ) else : return None if state == None : return 'object state is None' if isinstance ( state , tuple ) : if not isinstance ( state [ 0 ] , dict ) : state = state [ 1 ] else : state = state [ 0 ] . update ( state [ 1 ] ) result = { } for i in state : try : pickle . dumps ( state [ i ] , protocol = 2 ) except pickle . PicklingError as e : if not state [ i ] in seen : seen . append ( state [ i ] ) result [ i ] = get_pickling_errors ( state [ i ] , seen ) return result
10564	def get_supported_filepaths ( filepaths , supported_extensions , max_depth = float ( 'inf' ) ) : supported_filepaths = [ ] for path in filepaths : if os . name == 'nt' and CYGPATH_RE . match ( path ) : path = convert_cygwin_path ( path ) if os . path . isdir ( path ) : for root , __ , files in walk_depth ( path , max_depth ) : for f in files : if f . lower ( ) . endswith ( supported_extensions ) : supported_filepaths . append ( os . path . join ( root , f ) ) elif os . path . isfile ( path ) and path . lower ( ) . endswith ( supported_extensions ) : supported_filepaths . append ( path ) return supported_filepaths
817	def Distribution ( pos , size , counts , dtype ) : x = numpy . zeros ( size , dtype = dtype ) if hasattr ( pos , '__iter__' ) : total = 0 for i in pos : total += counts [ i ] total = float ( total ) for i in pos : x [ i ] = counts [ i ] / total else : x [ pos ] = 1 return x
10241	def count_authors_by_annotation ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Mapping [ str , typing . Counter [ str ] ] : authors = group_as_dict ( _iter_authors_by_annotation ( graph , annotation = annotation ) ) return count_defaultdict ( authors )
1229	def optimizer_arguments ( self , states , internals , actions , terminal , reward , next_states , next_internals ) : arguments = dict ( time = self . global_timestep , variables = self . get_variables ( ) , arguments = dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals , update = tf . constant ( value = True ) ) , fn_reference = self . fn_reference , fn_loss = self . fn_loss ) if self . global_model is not None : arguments [ 'global_variables' ] = self . global_model . get_variables ( ) return arguments
10496	def leftMouseDragged ( self , stopCoord , strCoord = ( 0 , 0 ) , speed = 1 ) : self . _leftMouseDragged ( stopCoord , strCoord , speed )
8206	def overlap ( self , x1 , y1 , x2 , y2 , r = 5 ) : if abs ( x2 - x1 ) < r and abs ( y2 - y1 ) < r : return True else : return False
13713	def run ( self ) : self . log . debug ( 'consumer is running...' ) self . running = True while self . running : self . upload ( ) self . log . debug ( 'consumer exited.' )
12943	def diff ( firstObj , otherObj , includeMeta = False ) : if not isIndexedRedisModel ( firstObj ) : raise ValueError ( 'Type < %s > does not extend IndexedRedisModel.' % ( type ( firstObj ) . __name__ , ) ) if not isIndexedRedisModel ( otherObj ) : raise ValueError ( 'Type < %s > does not extend IndexedRedisModel.' % ( type ( otherObj ) . __name__ , ) ) firstObj . validateModel ( ) otherObj . validateModel ( ) if getattr ( firstObj , 'FIELDS' ) != getattr ( otherObj , 'FIELDS' ) : raise ValueError ( 'Cannot compare < %s > and < %s > . Must be same model OR have equal FIELDS.' % ( firstObj . __class__ , otherObj . __class__ ) ) diffFields = { } for thisField in firstObj . FIELDS : thisFieldStr = str ( thisField ) firstVal = object . __getattribute__ ( firstObj , thisFieldStr ) otherVal = object . __getattribute__ ( otherObj , thisFieldStr ) if firstVal != otherVal : diffFields [ thisFieldStr ] = ( ( firstVal , otherVal ) ) if includeMeta : firstPk = firstObj . getPk ( ) otherPk = otherObj . getPk ( ) if firstPk != otherPk : diffFields [ '_id' ] = ( firstPk , otherPk ) return diffFields
12698	def _parse_control_fields ( self , fields , tag_id = "tag" ) : for field in fields : params = field . params if tag_id not in params : continue self . controlfields [ params [ tag_id ] ] = field . getContent ( ) . strip ( )
12190	def _format_message ( self , channel , text ) : payload = { 'type' : 'message' , 'id' : next ( self . _msg_ids ) } payload . update ( channel = channel , text = text ) return json . dumps ( payload )
10245	def create_timeline ( year_counter : typing . Counter [ int ] ) -> List [ Tuple [ int , int ] ] : if not year_counter : return [ ] from_year = min ( year_counter ) - 1 until_year = datetime . now ( ) . year + 1 return [ ( year , year_counter . get ( year , 0 ) ) for year in range ( from_year , until_year ) ]
9568	def get_chat_id ( self , message ) : if message . chat . type == 'private' : return message . user . id return message . chat . id
509	def stripUnlearnedColumns ( self , activeArray ) : neverLearned = numpy . where ( self . _activeDutyCycles == 0 ) [ 0 ] activeArray [ neverLearned ] = 0
12381	def delete ( self , request , response ) : if self . slug is None : raise http . exceptions . NotImplemented ( ) self . assert_operations ( 'destroy' ) self . destroy ( ) self . response . status = http . client . NO_CONTENT self . make_response ( )
12730	def axes ( self ) : return [ np . array ( self . ode_obj . getAxis1 ( ) ) , np . array ( self . ode_obj . getAxis2 ( ) ) ]
1275	def tf_retrieve_indices ( self , indices ) : states = dict ( ) for name in sorted ( self . states_memory ) : states [ name ] = tf . gather ( params = self . states_memory [ name ] , indices = indices ) internals = dict ( ) for name in sorted ( self . internals_memory ) : internals [ name ] = tf . gather ( params = self . internals_memory [ name ] , indices = indices ) actions = dict ( ) for name in sorted ( self . actions_memory ) : actions [ name ] = tf . gather ( params = self . actions_memory [ name ] , indices = indices ) terminal = tf . gather ( params = self . terminal_memory , indices = indices ) reward = tf . gather ( params = self . reward_memory , indices = indices ) if self . include_next_states : assert util . rank ( indices ) == 1 next_indices = ( indices + 1 ) % self . capacity next_states = dict ( ) for name in sorted ( self . states_memory ) : next_states [ name ] = tf . gather ( params = self . states_memory [ name ] , indices = next_indices ) next_internals = dict ( ) for name in sorted ( self . internals_memory ) : next_internals [ name ] = tf . gather ( params = self . internals_memory [ name ] , indices = next_indices ) return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals ) else : return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
12703	def _set_params ( target , param , values , dof ) : if not isinstance ( values , ( list , tuple , np . ndarray ) ) : values = [ values ] * dof assert dof == len ( values ) for s , value in zip ( [ '' , '2' , '3' ] [ : dof ] , values ) : target . setParam ( getattr ( ode , 'Param{}{}' . format ( param , s ) ) , value )
231	def compute_sector_exposures ( positions , sectors , sector_dict = SECTORS ) : sector_ids = sector_dict . keys ( ) long_exposures = [ ] short_exposures = [ ] gross_exposures = [ ] net_exposures = [ ] positions_wo_cash = positions . drop ( 'cash' , axis = 'columns' ) long_exposure = positions_wo_cash [ positions_wo_cash > 0 ] . sum ( axis = 'columns' ) short_exposure = positions_wo_cash [ positions_wo_cash < 0 ] . abs ( ) . sum ( axis = 'columns' ) gross_exposure = positions_wo_cash . abs ( ) . sum ( axis = 'columns' ) for sector_id in sector_ids : in_sector = positions_wo_cash [ sectors == sector_id ] long_sector = in_sector [ in_sector > 0 ] . sum ( axis = 'columns' ) . divide ( long_exposure ) short_sector = in_sector [ in_sector < 0 ] . sum ( axis = 'columns' ) . divide ( short_exposure ) gross_sector = in_sector . abs ( ) . sum ( axis = 'columns' ) . divide ( gross_exposure ) net_sector = long_sector . subtract ( short_sector ) long_exposures . append ( long_sector ) short_exposures . append ( short_sector ) gross_exposures . append ( gross_sector ) net_exposures . append ( net_sector ) return long_exposures , short_exposures , gross_exposures , net_exposures
10766	def get_poll ( self , arg , * , request_policy = None ) : if isinstance ( arg , str ) : match = self . _url_re . match ( arg ) if match : arg = match . group ( 'id' ) return self . _http_client . get ( '{}/{}' . format ( self . _POLLS , arg ) , request_policy = request_policy , cls = strawpoll . Poll )
3068	def wrap_http_for_auth ( credentials , http ) : orig_request_method = http . request def new_request ( uri , method = 'GET' , body = None , headers = None , redirections = httplib2 . DEFAULT_MAX_REDIRECTS , connection_type = None ) : if not credentials . access_token : _LOGGER . info ( 'Attempting refresh to obtain ' 'initial access_token' ) credentials . _refresh ( orig_request_method ) headers = _initialize_headers ( headers ) credentials . apply ( headers ) _apply_user_agent ( headers , credentials . user_agent ) body_stream_position = None if all ( getattr ( body , stream_prop , None ) for stream_prop in _STREAM_PROPERTIES ) : body_stream_position = body . tell ( ) resp , content = request ( orig_request_method , uri , method , body , clean_headers ( headers ) , redirections , connection_type ) max_refresh_attempts = 2 for refresh_attempt in range ( max_refresh_attempts ) : if resp . status not in REFRESH_STATUS_CODES : break _LOGGER . info ( 'Refreshing due to a %s (attempt %s/%s)' , resp . status , refresh_attempt + 1 , max_refresh_attempts ) credentials . _refresh ( orig_request_method ) credentials . apply ( headers ) if body_stream_position is not None : body . seek ( body_stream_position ) resp , content = request ( orig_request_method , uri , method , body , clean_headers ( headers ) , redirections , connection_type ) return resp , content http . request = new_request http . request . credentials = credentials
3125	def verify_signed_jwt_with_certs ( jwt , certs , audience = None ) : jwt = _helpers . _to_bytes ( jwt ) if jwt . count ( b'.' ) != 2 : raise AppIdentityError ( 'Wrong number of segments in token: {0}' . format ( jwt ) ) header , payload , signature = jwt . split ( b'.' ) message_to_sign = header + b'.' + payload signature = _helpers . _urlsafe_b64decode ( signature ) payload_bytes = _helpers . _urlsafe_b64decode ( payload ) try : payload_dict = json . loads ( _helpers . _from_bytes ( payload_bytes ) ) except : raise AppIdentityError ( 'Can\'t parse token: {0}' . format ( payload_bytes ) ) _verify_signature ( message_to_sign , signature , certs . values ( ) ) _verify_time_range ( payload_dict ) _check_audience ( payload_dict , audience ) return payload_dict
11975	def _add ( self , other ) : if isinstance ( other , self . __class__ ) : sum_ = self . _ip_dec + other . _ip_dec elif isinstance ( other , int ) : sum_ = self . _ip_dec + other else : other = self . __class__ ( other ) sum_ = self . _ip_dec + other . _ip_dec return sum_
11773	def information_content ( values ) : "Number of bits to represent the probability distribution in values." probabilities = normalize ( removeall ( 0 , values ) ) return sum ( - p * log2 ( p ) for p in probabilities )
7810	def from_der_data ( cls , data ) : logger . debug ( "Decoding DER certificate: {0!r}" . format ( data ) ) if cls . _cert_asn1_type is None : cls . _cert_asn1_type = Certificate ( ) cert = der_decoder . decode ( data , asn1Spec = cls . _cert_asn1_type ) [ 0 ] result = cls ( ) tbs_cert = cert . getComponentByName ( 'tbsCertificate' ) subject = tbs_cert . getComponentByName ( 'subject' ) logger . debug ( "Subject: {0!r}" . format ( subject ) ) result . _decode_subject ( subject ) validity = tbs_cert . getComponentByName ( 'validity' ) result . _decode_validity ( validity ) extensions = tbs_cert . getComponentByName ( 'extensions' ) if extensions : for extension in extensions : logger . debug ( "Extension: {0!r}" . format ( extension ) ) oid = extension . getComponentByName ( 'extnID' ) logger . debug ( "OID: {0!r}" . format ( oid ) ) if oid != SUBJECT_ALT_NAME_OID : continue value = extension . getComponentByName ( 'extnValue' ) logger . debug ( "Value: {0!r}" . format ( value ) ) if isinstance ( value , Any ) : value = der_decoder . decode ( value , asn1Spec = OctetString ( ) ) [ 0 ] alt_names = der_decoder . decode ( value , asn1Spec = GeneralNames ( ) ) [ 0 ] logger . debug ( "SubjectAltName: {0!r}" . format ( alt_names ) ) result . _decode_alt_names ( alt_names ) return result
10535	def get_categories ( limit = 20 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) try : res = _pybossa_req ( 'get' , 'category' , params = params ) if type ( res ) . __name__ == 'list' : return [ Category ( category ) for category in res ] else : raise TypeError except : raise
3201	def update ( self , campaign_id , data ) : self . campaign_id = campaign_id if 'settings' not in data : raise KeyError ( 'The campaign must have settings' ) if 'subject_line' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a subject_line' ) if 'from_name' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a from_name' ) if 'reply_to' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a reply_to' ) check_email ( data [ 'settings' ] [ 'reply_to' ] ) return self . _mc_client . _patch ( url = self . _build_path ( campaign_id ) , data = data )
5680	def get_trip_counts_per_day ( self ) : query = "SELECT date, count(*) AS number_of_trips FROM day_trips GROUP BY date" trip_counts_per_day = pd . read_sql_query ( query , self . conn , index_col = "date" ) max_day = trip_counts_per_day . index . max ( ) min_day = trip_counts_per_day . index . min ( ) min_date = datetime . datetime . strptime ( min_day , '%Y-%m-%d' ) max_date = datetime . datetime . strptime ( max_day , '%Y-%m-%d' ) num_days = ( max_date - min_date ) . days dates = [ min_date + datetime . timedelta ( days = x ) for x in range ( num_days + 1 ) ] trip_counts = [ ] date_strings = [ ] for date in dates : date_string = date . strftime ( "%Y-%m-%d" ) date_strings . append ( date_string ) try : value = trip_counts_per_day . loc [ date_string , 'number_of_trips' ] except KeyError : value = 0 trip_counts . append ( value ) for date_string in trip_counts_per_day . index : assert date_string in date_strings data = { "date" : dates , "date_str" : date_strings , "trip_counts" : trip_counts } return pd . DataFrame ( data )
10791	def save_wisdom ( wisdomfile ) : if wisdomfile is None : return if wisdomfile : pickle . dump ( pyfftw . export_wisdom ( ) , open ( wisdomfile , 'wb' ) , protocol = 2 )
3075	def callback_view ( self ) : if 'error' in request . args : reason = request . args . get ( 'error_description' , request . args . get ( 'error' , '' ) ) reason = markupsafe . escape ( reason ) return ( 'Authorization failed: {0}' . format ( reason ) , httplib . BAD_REQUEST ) try : encoded_state = request . args [ 'state' ] server_csrf = session [ _CSRF_KEY ] code = request . args [ 'code' ] except KeyError : return 'Invalid request' , httplib . BAD_REQUEST try : state = json . loads ( encoded_state ) client_csrf = state [ 'csrf_token' ] return_url = state [ 'return_url' ] except ( ValueError , KeyError ) : return 'Invalid request state' , httplib . BAD_REQUEST if client_csrf != server_csrf : return 'Invalid request state' , httplib . BAD_REQUEST flow = _get_flow_for_token ( server_csrf ) if flow is None : return 'Invalid request state' , httplib . BAD_REQUEST try : credentials = flow . step2_exchange ( code ) except client . FlowExchangeError as exchange_error : current_app . logger . exception ( exchange_error ) content = 'An error occurred: {0}' . format ( exchange_error ) return content , httplib . BAD_REQUEST self . storage . put ( credentials ) if self . authorize_callback : self . authorize_callback ( credentials ) return redirect ( return_url )
12473	def add_extension_if_needed ( filepath , ext , check_if_exists = False ) : if not filepath . endswith ( ext ) : filepath += ext if check_if_exists : if not op . exists ( filepath ) : raise IOError ( 'File not found: ' + filepath ) return filepath
8396	def trans_new ( name , transform , inverse , breaks = None , minor_breaks = None , _format = None , domain = ( - np . inf , np . inf ) , doc = '' , ** kwargs ) : def _get ( func ) : if isinstance ( func , ( classmethod , staticmethod , MethodType ) ) : return func else : return staticmethod ( func ) klass_name = '{}_trans' . format ( name ) d = { 'transform' : _get ( transform ) , 'inverse' : _get ( inverse ) , 'domain' : domain , '__doc__' : doc , ** kwargs } if breaks : d [ 'breaks_' ] = _get ( breaks ) if minor_breaks : d [ 'minor_breaks' ] = _get ( minor_breaks ) if _format : d [ 'format' ] = _get ( _format ) return type ( klass_name , ( trans , ) , d )
11555	def enable_analog_reporting ( self , pin ) : command = [ self . _command_handler . REPORT_ANALOG + pin , self . REPORTING_ENABLE ] self . _command_handler . send_command ( command )
10768	def numpy_formatter ( _ , vertices , codes = None ) : if codes is None : return vertices numpy_vertices = [ ] for vertices_ , codes_ in zip ( vertices , codes ) : starts = np . nonzero ( codes_ == MPLPATHCODE . MOVETO ) [ 0 ] stops = np . nonzero ( codes_ == MPLPATHCODE . CLOSEPOLY ) [ 0 ] for start , stop in zip ( starts , stops ) : numpy_vertices . append ( vertices_ [ start : stop + 1 , : ] ) return numpy_vertices
10629	def HHV ( self , HHV ) : self . _HHV = HHV if self . isCoal : self . _DH298 = self . _calculate_DH298_coal ( )
1272	def create_distributions ( self ) : distributions = dict ( ) for name in sorted ( self . actions_spec ) : action = self . actions_spec [ name ] if self . distributions_spec is not None and name in self . distributions_spec : kwargs = dict ( action ) kwargs [ 'scope' ] = name kwargs [ 'summary_labels' ] = self . summary_labels distributions [ name ] = Distribution . from_spec ( spec = self . distributions_spec [ name ] , kwargs = kwargs ) elif action [ 'type' ] == 'bool' : distributions [ name ] = Bernoulli ( shape = action [ 'shape' ] , scope = name , summary_labels = self . summary_labels ) elif action [ 'type' ] == 'int' : distributions [ name ] = Categorical ( shape = action [ 'shape' ] , num_actions = action [ 'num_actions' ] , scope = name , summary_labels = self . summary_labels ) elif action [ 'type' ] == 'float' : if 'min_value' in action : distributions [ name ] = Beta ( shape = action [ 'shape' ] , min_value = action [ 'min_value' ] , max_value = action [ 'max_value' ] , scope = name , summary_labels = self . summary_labels ) else : distributions [ name ] = Gaussian ( shape = action [ 'shape' ] , scope = name , summary_labels = self . summary_labels ) return distributions
10990	def finish_state ( st , desc = 'finish-state' , invert = 'guess' ) : for minmass in [ None , 0 ] : for _ in range ( 3 ) : npart , poses = addsub . add_subtract_locally ( st , region_depth = 7 , minmass = minmass , invert = invert ) if npart == 0 : break opt . finish ( st , n_loop = 1 , separate_psf = True , desc = desc , dowarn = False ) opt . burn ( st , mode = 'polish' , desc = desc , n_loop = 2 , dowarn = False ) d = opt . finish ( st , desc = desc , n_loop = 4 , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' )
4647	def create ( self ) : query = ( ) . format ( self . __tablename__ , self . __key__ , self . __value__ ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( query ) connection . commit ( )
5992	def plot_figure ( array , as_subplot , units , kpc_per_arcsec , figsize , aspect , cmap , norm , norm_min , norm_max , linthresh , linscale , xticks_manual , yticks_manual ) : fig = plotter_util . setup_figure ( figsize = figsize , as_subplot = as_subplot ) norm_min , norm_max = get_normalization_min_max ( array = array , norm_min = norm_min , norm_max = norm_max ) norm_scale = get_normalization_scale ( norm = norm , norm_min = norm_min , norm_max = norm_max , linthresh = linthresh , linscale = linscale ) extent = get_extent ( array = array , units = units , kpc_per_arcsec = kpc_per_arcsec , xticks_manual = xticks_manual , yticks_manual = yticks_manual ) plt . imshow ( array , aspect = aspect , cmap = cmap , norm = norm_scale , extent = extent ) return fig
9854	def _detect_byteorder ( ccp4file ) : bsaflag = None ccp4file . seek ( 52 * 4 ) mapbin = ccp4file . read ( 4 ) for flag in '@=<>' : mapstr = struct . unpack ( flag + '4s' , mapbin ) [ 0 ] . decode ( 'utf-8' ) if mapstr . upper ( ) == 'MAP ' : bsaflag = flag break else : raise TypeError ( "Cannot decode header --- corrupted or wrong format?" ) ccp4file . seek ( 0 ) return bsaflag
5537	def get_raw_output ( self , tile , _baselevel_readonly = False ) : if not isinstance ( tile , ( BufferedTile , tuple ) ) : raise TypeError ( "'tile' must be a tuple or BufferedTile" ) if isinstance ( tile , tuple ) : tile = self . config . output_pyramid . tile ( * tile ) if _baselevel_readonly : tile = self . config . baselevels [ "tile_pyramid" ] . tile ( * tile . id ) if tile . zoom not in self . config . zoom_levels : return self . config . output . empty ( tile ) if tile . crs != self . config . process_pyramid . crs : raise NotImplementedError ( "reprojection between processes not yet implemented" ) if self . config . mode == "memory" : process_tile = self . config . process_pyramid . intersecting ( tile ) [ 0 ] return self . _extract ( in_tile = process_tile , in_data = self . _execute_using_cache ( process_tile ) , out_tile = tile ) process_tile = self . config . process_pyramid . intersecting ( tile ) [ 0 ] if tile . pixelbuffer > self . config . output . pixelbuffer : output_tiles = list ( self . config . output_pyramid . tiles_from_bounds ( tile . bounds , tile . zoom ) ) else : output_tiles = self . config . output_pyramid . intersecting ( tile ) if self . config . mode == "readonly" or _baselevel_readonly : if self . config . output . tiles_exist ( process_tile ) : return self . _read_existing_output ( tile , output_tiles ) else : return self . config . output . empty ( tile ) elif self . config . mode == "continue" and not _baselevel_readonly : if self . config . output . tiles_exist ( process_tile ) : return self . _read_existing_output ( tile , output_tiles ) else : return self . _process_and_overwrite_output ( tile , process_tile ) elif self . config . mode == "overwrite" and not _baselevel_readonly : return self . _process_and_overwrite_output ( tile , process_tile )
12135	def directory ( cls , directory , root = None , extension = None , ** kwargs ) : root = os . getcwd ( ) if root is None else root suffix = '' if extension is None else '.' + extension . rsplit ( '.' ) [ - 1 ] pattern = directory + os . sep + '*' + suffix key = os . path . join ( root , directory , '*' ) . rsplit ( os . sep ) [ - 2 ] format_parse = list ( string . Formatter ( ) . parse ( key ) ) if not all ( [ el is None for el in zip ( * format_parse ) [ 1 ] ] ) : raise Exception ( 'Directory cannot contain format field specifications' ) return cls ( key , pattern , root , ** kwargs )
1305	def GetConsoleOriginalTitle ( ) -> str : if IsNT6orHigher : arrayType = ctypes . c_wchar * MAX_PATH values = arrayType ( ) ctypes . windll . kernel32 . GetConsoleOriginalTitleW ( values , MAX_PATH ) return values . value else : raise RuntimeError ( 'GetConsoleOriginalTitle is not supported on Windows XP or lower.' )
5125	def simulate ( self , n = 1 , t = None ) : if not self . _initialized : msg = ( "Network has not been initialized. " "Call '.initialize()' first." ) raise QueueingToolError ( msg ) if t is None : for dummy in range ( n ) : self . _simulate_next_event ( slow = False ) else : now = self . _t while self . _t < now + t : self . _simulate_next_event ( slow = False )
11391	def contribute_to_class ( self , cls , name ) : super ( EmbeddedMediaField , self ) . contribute_to_class ( cls , name ) register_field ( cls , self ) cls . _meta . add_virtual_field ( EmbeddedSignalCreator ( self ) )
10104	def execute ( self , timeout = None ) : logger . debug ( ' > Batch API request (length %s)' % len ( self . _commands ) ) auth = self . _build_http_auth ( ) headers = self . _build_request_headers ( ) logger . debug ( '\tbatch headers: %s' % headers ) logger . debug ( '\tbatch command length: %s' % len ( self . _commands ) ) path = self . _build_request_path ( self . BATCH_ENDPOINT ) data = json . dumps ( self . _commands , cls = self . _json_encoder ) r = requests . post ( path , auth = auth , headers = headers , data = data , timeout = ( self . DEFAULT_TIMEOUT if timeout is None else timeout ) ) self . _commands = [ ] logger . debug ( '\tresponse code:%s' % r . status_code ) try : logger . debug ( '\tresponse: %s' % r . json ( ) ) except : logger . debug ( '\tresponse: %s' % r . content ) return r
3637	def clubStaff ( self ) : method = 'GET' url = 'club/stats/staff' rc = self . __request__ ( method , url ) return rc
4510	def get ( name = None ) : if name is None or name == 'default' : return _DEFAULT_PALETTE if isinstance ( name , str ) : return PROJECT_PALETTES . get ( name ) or BUILT_IN_PALETTES . get ( name )
11650	def fit ( self , X , y = None ) : if is_integer ( X ) : dim = X else : X = as_features ( X ) dim = X . dim M = self . smoothness inds = np . mgrid [ ( slice ( M + 1 ) , ) * dim ] . reshape ( dim , ( M + 1 ) ** dim ) . T self . inds_ = inds [ ( inds ** 2 ) . sum ( axis = 1 ) <= M ** 2 ] return self
11495	def list_users ( self , limit = 20 ) : parameters = dict ( ) parameters [ 'limit' ] = limit response = self . request ( 'midas.user.list' , parameters ) return response
10616	def clear ( self ) : self . _compound_masses = self . _compound_masses * 0.0 self . _P = 1.0 self . _T = 25.0 self . _H = 0.0
3128	def delete ( self , template_id ) : self . template_id = template_id return self . _mc_client . _delete ( url = self . _build_path ( template_id ) )
3310	def _run_ext_wsgiutils ( app , config , mode ) : from wsgidav . server import ext_wsgiutils_server _logger . info ( "Running WsgiDAV {} on wsgidav.ext_wsgiutils_server..." . format ( __version__ ) ) _logger . warning ( "WARNING: This single threaded server (ext-wsgiutils) is not meant for production." ) try : ext_wsgiutils_server . serve ( config , app ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
7814	def from_file ( cls , filename ) : with open ( filename , "r" ) as pem_file : data = pem . readPemFromFile ( pem_file ) return cls . from_der_data ( data )
8142	def scale ( self , w = 1.0 , h = 1.0 ) : from types import FloatType w0 , h0 = self . img . size if type ( w ) == FloatType : w = int ( w * w0 ) if type ( h ) == FloatType : h = int ( h * h0 ) self . img = self . img . resize ( ( w , h ) , INTERPOLATION ) self . w = w self . h = h
8230	def size ( self , w = None , h = None ) : if not w : w = self . _canvas . width if not h : h = self . _canvas . height if not w and not h : return ( self . _canvas . width , self . _canvas . height ) w , h = self . _canvas . set_size ( ( w , h ) ) self . _namespace [ 'WIDTH' ] = w self . _namespace [ 'HEIGHT' ] = h self . WIDTH = w self . HEIGHT = h
12356	def wait ( self ) : interval_seconds = 5 while True : actions = self . actions ( ) slept = False for a in actions : if a [ 'status' ] == 'in-progress' : time . sleep ( interval_seconds ) slept = True break if not slept : break
6047	def map_to_2d_keep_padded ( self , padded_array_1d ) : return mapping_util . map_unmasked_1d_array_to_2d_array_from_array_1d_and_shape ( array_1d = padded_array_1d , shape = self . mask . shape )
1840	def JNG ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . OR ( cpu . ZF , cpu . SF != cpu . OF ) , target . read ( ) , cpu . PC )
7980	def auth_timeout ( self ) : self . lock . acquire ( ) try : self . __logger . debug ( "Timeout while waiting for jabber:iq:auth result" ) if self . _auth_methods_left : self . _auth_methods_left . pop ( 0 ) finally : self . lock . release ( )
7598	def get_popular_clans ( self , ** params : keys ) : url = self . api . POPULAR + '/clans' return self . _get_model ( url , PartialClan , ** params )
4963	def clean_course ( self ) : course_id = self . cleaned_data [ self . Fields . COURSE ] . strip ( ) if not course_id : return None try : client = EnrollmentApiClient ( ) return client . get_course_details ( course_id ) except ( HttpClientError , HttpServerError ) : raise ValidationError ( ValidationMessages . INVALID_COURSE_ID . format ( course_id = course_id ) )
9069	def _df ( self ) : if not self . _restricted : return self . nsamples return self . nsamples - self . _X [ "tX" ] . shape [ 1 ]
12380	def put ( self , request , response ) : if self . slug is None : raise http . exceptions . NotImplemented ( ) target = self . read ( ) data = self . _clean ( target , self . request . read ( deserialize = True ) ) if target is not None : self . assert_operations ( 'update' ) try : self . update ( target , data ) except AttributeError : raise http . exceptions . NotImplemented ( ) self . make_response ( target ) else : self . assert_operations ( 'create' ) target = self . create ( data ) self . response . status = http . client . CREATED self . make_response ( target )
4034	def ib64_patched ( self , attrsD , contentparams ) : if attrsD . get ( "mode" , "" ) == "base64" : return 0 if self . contentparams [ "type" ] . startswith ( "text/" ) : return 0 if self . contentparams [ "type" ] . endswith ( "+xml" ) : return 0 if self . contentparams [ "type" ] . endswith ( "/xml" ) : return 0 if self . contentparams [ "type" ] . endswith ( "/json" ) : return 0 return 0
3349	def geometric_fba ( model , epsilon = 1E-06 , max_tries = 200 , processes = None ) : with model : consts = [ ] obj_vars = [ ] updating_vars_cons = [ ] prob = model . problem add_pfba ( model ) model . optimize ( ) fva_sol = flux_variability_analysis ( model , processes = processes ) mean_flux = ( fva_sol [ "maximum" ] + fva_sol [ "minimum" ] ) . abs ( ) / 2 for rxn in model . reactions : var = prob . Variable ( "geometric_fba_" + rxn . id , lb = 0 , ub = mean_flux [ rxn . id ] ) upper_const = prob . Constraint ( rxn . flux_expression - var , ub = mean_flux [ rxn . id ] , name = "geometric_fba_upper_const_" + rxn . id ) lower_const = prob . Constraint ( rxn . flux_expression + var , lb = fva_sol . at [ rxn . id , "minimum" ] , name = "geometric_fba_lower_const_" + rxn . id ) updating_vars_cons . append ( ( rxn . id , var , upper_const , lower_const ) ) consts . extend ( [ var , upper_const , lower_const ] ) obj_vars . append ( var ) model . add_cons_vars ( consts ) model . objective = prob . Objective ( Zero , sloppy = True , direction = "min" ) model . objective . set_linear_coefficients ( { v : 1.0 for v in obj_vars } ) sol = model . optimize ( ) fva_sol = flux_variability_analysis ( model , processes = processes ) mean_flux = ( fva_sol [ "maximum" ] + fva_sol [ "minimum" ] ) . abs ( ) / 2 delta = ( fva_sol [ "maximum" ] - fva_sol [ "minimum" ] ) . max ( ) count = 1 LOGGER . debug ( "Iteration: %d; delta: %.3g; status: %s." , count , delta , sol . status ) while delta > epsilon and count < max_tries : for rxn_id , var , u_c , l_c in updating_vars_cons : var . ub = mean_flux [ rxn_id ] u_c . ub = mean_flux [ rxn_id ] l_c . lb = fva_sol . at [ rxn_id , "minimum" ] sol = model . optimize ( ) fva_sol = flux_variability_analysis ( model , processes = processes ) mean_flux = ( fva_sol [ "maximum" ] + fva_sol [ "minimum" ] ) . abs ( ) / 2 delta = ( fva_sol [ "maximum" ] - fva_sol [ "minimum" ] ) . max ( ) count += 1 LOGGER . debug ( "Iteration: %d; delta: %.3g; status: %s." , count , delta , sol . status ) if count == max_tries : raise RuntimeError ( "The iterations have exceeded the maximum value of {}. " "This is probably due to the increased complexity of the " "model and can lead to inaccurate results. Please set a " "different convergence tolerance and/or increase the " "maximum iterations" . format ( max_tries ) ) return sol
41	def update_priorities ( self , idxes , priorities ) : assert len ( idxes ) == len ( priorities ) for idx , priority in zip ( idxes , priorities ) : assert priority > 0 assert 0 <= idx < len ( self . _storage ) self . _it_sum [ idx ] = priority ** self . _alpha self . _it_min [ idx ] = priority ** self . _alpha self . _max_priority = max ( self . _max_priority , priority )
12852	def scan ( xml ) : if xml . tag is et . Comment : yield { 'type' : COMMENT , 'text' : xml . text } return if xml . tag is et . PI : if xml . text : yield { 'type' : PI , 'target' : xml . target , 'text' : xml . text } else : yield { 'type' : PI , 'target' : xml . target } return obj = _elt2obj ( xml ) obj [ 'type' ] = ENTER yield obj assert type ( xml . tag ) is str , xml if xml . text : yield { 'type' : TEXT , 'text' : xml . text } for c in xml : for x in scan ( c ) : yield x if c . tail : yield { 'type' : TEXT , 'text' : c . tail } yield { 'type' : EXIT }
9872	def start_response ( self , status , response_headers , exc_info = None ) : if exc_info : try : if self . headers_sent : raise finally : exc_info = None elif self . header_set : raise AssertionError ( "Headers already set!" ) if PY3K and not isinstance ( status , str ) : self . status = str ( status , 'ISO-8859-1' ) else : self . status = status try : self . header_set = Headers ( response_headers ) except UnicodeDecodeError : self . error = ( '500 Internal Server Error' , 'HTTP Headers should be bytes' ) self . err_log . error ( 'Received HTTP Headers from client that contain' ' invalid characters for Latin-1 encoding.' ) return self . write_warning
9429	def extract ( self , member , path = None , pwd = None ) : if isinstance ( member , RarInfo ) : member = member . filename if path is None : path = os . getcwd ( ) self . _extract_members ( [ member ] , path , pwd ) return os . path . join ( path , member )
6553	def fix_variable ( self , v , value ) : variables = self . variables try : idx = variables . index ( v ) except ValueError : raise ValueError ( "given variable {} is not part of the constraint" . format ( v ) ) if value not in self . vartype . value : raise ValueError ( "expected value to be in {}, received {} instead" . format ( self . vartype . value , value ) ) configurations = frozenset ( config [ : idx ] + config [ idx + 1 : ] for config in self . configurations if config [ idx ] == value ) if not configurations : raise UnsatError ( "fixing {} to {} makes this constraint unsatisfiable" . format ( v , value ) ) variables = variables [ : idx ] + variables [ idx + 1 : ] self . configurations = configurations self . variables = variables def func ( * args ) : return args in configurations self . func = func self . name = '{} ({} fixed to {})' . format ( self . name , v , value )
3712	def calculate_P ( self , T , P , method ) : r if method == EOS : self . eos [ 0 ] = self . eos [ 0 ] . to_TP ( T = T , P = P ) Vm = self . eos [ 0 ] . V_g elif method == TSONOPOULOS_EXTENDED : B = BVirial_Tsonopoulos_extended ( T , self . Tc , self . Pc , self . omega , dipole = self . dipole ) Vm = ideal_gas ( T , P ) + B elif method == TSONOPOULOS : B = BVirial_Tsonopoulos ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == ABBOTT : B = BVirial_Abbott ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == PITZER_CURL : B = BVirial_Pitzer_Curl ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == CRC_VIRIAL : a1 , a2 , a3 , a4 , a5 = self . CRC_VIRIAL_coeffs t = 298.15 / T - 1. B = ( a1 + a2 * t + a3 * t ** 2 + a4 * t ** 3 + a5 * t ** 4 ) / 1E6 Vm = ideal_gas ( T , P ) + B elif method == IDEAL : Vm = ideal_gas ( T , P ) elif method == COOLPROP : Vm = 1. / PropsSI ( 'DMOLAR' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : Vm = self . interpolate_P ( T , P , method ) return Vm
12316	def init ( self , username , reponame , force , backend = None ) : key = self . key ( username , reponame ) server_repodir = self . server_rootdir ( username , reponame , create = False ) if os . path . exists ( server_repodir ) and not force : raise RepositoryExists ( ) if os . path . exists ( server_repodir ) : shutil . rmtree ( server_repodir ) os . makedirs ( server_repodir ) with cd ( server_repodir ) : git . init ( "." , "--bare" ) if backend is not None : backend . init_repo ( server_repodir ) repodir = self . rootdir ( username , reponame , create = False ) if os . path . exists ( repodir ) and not force : raise Exception ( "Local repo already exists" ) if os . path . exists ( repodir ) : shutil . rmtree ( repodir ) os . makedirs ( repodir ) with cd ( os . path . dirname ( repodir ) ) : git . clone ( server_repodir , '--no-hardlinks' ) url = server_repodir if backend is not None : url = backend . url ( username , reponame ) repo = Repo ( username , reponame ) repo . manager = self repo . remoteurl = url repo . rootdir = self . rootdir ( username , reponame ) self . add ( repo ) return repo
6469	def consume ( self , istream , ostream , batch = False ) : datapoints = [ ] if batch : sleep = max ( 0.01 , self . option . sleep ) fd = istream . fileno ( ) while True : try : if select . select ( [ fd ] , [ ] , [ ] , sleep ) : try : line = istream . readline ( ) if line == '' : break datapoints . append ( self . consume_line ( line ) ) except ValueError : continue if self . option . sort_by_column : datapoints = sorted ( datapoints , key = itemgetter ( self . option . sort_by_column - 1 ) ) if len ( datapoints ) > 1 : datapoints = datapoints [ - self . maximum_points : ] self . update ( [ dp [ 0 ] for dp in datapoints ] , [ dp [ 1 ] for dp in datapoints ] ) self . render ( ostream ) time . sleep ( sleep ) except KeyboardInterrupt : break else : for line in istream : try : datapoints . append ( self . consume_line ( line ) ) except ValueError : pass if self . option . sort_by_column : datapoints = sorted ( datapoints , key = itemgetter ( self . option . sort_by_column - 1 ) ) self . update ( [ dp [ 0 ] for dp in datapoints ] , [ dp [ 1 ] for dp in datapoints ] ) self . render ( ostream )
4740	def emph ( txt , rval = None ) : if rval is None : info ( txt ) elif rval == 0 : good ( txt ) else : err ( txt )
1198	def tf_loss ( self , states , internals , reward , update , reference = None ) : prediction = self . predict ( states = states , internals = internals , update = update ) return tf . nn . l2_loss ( t = ( prediction - reward ) )
4028	def create_cookie ( host , path , secure , expires , name , value ) : return http . cookiejar . Cookie ( 0 , name , value , None , False , host , host . startswith ( '.' ) , host . startswith ( '.' ) , path , True , secure , expires , False , None , None , { } )
9900	def data ( self , data ) : if self . is_caching : self . cache = data else : fcontents = self . file_contents with open ( self . path , "w" ) as f : try : indent = self . indent if self . pretty else None json . dump ( data , f , sort_keys = self . sort_keys , indent = indent ) except Exception as e : f . seek ( 0 ) f . truncate ( ) f . write ( fcontents ) raise e self . _updateType ( )
6706	def expire_password ( self , username ) : r = self . local_renderer r . env . username = username r . sudo ( 'chage -d 0 {username}' )
3568	def stop_scan ( self , timeout_sec = TIMEOUT_SEC ) : self . _scan_stopped . clear ( ) self . _adapter . StopDiscovery ( ) if not self . _scan_stopped . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to stop scanning!' )
13739	def _keep_alive_thread ( self ) : while True : with self . _lock : if self . connected ( ) : self . _ws . ping ( ) else : self . disconnect ( ) self . _thread = None return sleep ( 30 )
6546	def connect ( self , host ) : if not self . app . connect ( host ) : command = "Connect({0})" . format ( host ) . encode ( "ascii" ) self . exec_command ( command ) self . last_host = host
7747	def _process_handler_result ( self , response ) : if response is None or response is False : return False if isinstance ( response , Stanza ) : self . send ( response ) return True try : response = iter ( response ) except TypeError : return bool ( response ) for stanza in response : if isinstance ( stanza , Stanza ) : self . send ( stanza ) else : logger . warning ( u"Unexpected object in stanza handler result:" u" {0!r}" . format ( stanza ) ) return True
4654	def constructTx ( self ) : ops = list ( ) for op in self . ops : if isinstance ( op , ProposalBuilder ) : proposal = op . get_raw ( ) if proposal : ops . append ( proposal ) elif isinstance ( op , self . operation_class ) : ops . extend ( [ op ] ) else : ops . extend ( [ self . operation_class ( op ) ] ) ops = self . add_required_fees ( ops , asset_id = self . fee_asset_id ) expiration = formatTimeFromNow ( self . expiration or self . blockchain . expiration or 30 ) ref_block_num , ref_block_prefix = self . get_block_params ( ) self . tx = self . signed_transaction_class ( ref_block_num = ref_block_num , ref_block_prefix = ref_block_prefix , expiration = expiration , operations = ops , ) dict . update ( self , self . tx . json ( ) ) self . _unset_require_reconstruction ( )
2867	def readU8 ( self , register ) : result = self . _bus . read_byte_data ( self . _address , register ) & 0xFF self . _logger . debug ( "Read 0x%02X from register 0x%02X" , result , register ) return result
5844	def get_data_view ( self , data_view_id ) : url = routes . get_data_view ( data_view_id ) response = self . _get ( url ) . json ( ) result = response [ "data" ] [ "data_view" ] datasets_list = [ ] for dataset in result [ "datasets" ] : datasets_list . append ( Dataset ( name = dataset [ "name" ] , id = dataset [ "id" ] , description = dataset [ "description" ] ) ) columns_list = [ ] for column in result [ "columns" ] : columns_list . append ( ColumnFactory . from_dict ( column ) ) return DataView ( view_id = data_view_id , name = result [ "name" ] , description = result [ "description" ] , datasets = datasets_list , columns = columns_list , )
10227	def get_triangles ( graph : DiGraph ) -> SetOfNodeTriples : return { tuple ( sorted ( [ a , b , c ] , key = str ) ) for a , b in graph . edges ( ) for c in graph . successors ( b ) if graph . has_edge ( c , a ) }
13769	def get_minifier ( self ) : if self . minifier is None : if not self . has_bundles ( ) : raise Exception ( "Unable to get default minifier, no bundles in build group" ) minifier = self . get_first_bundle ( ) . get_default_minifier ( ) else : minifier = self . minifier if minifier : minifier . init_asset ( self ) return minifier
11638	def write_data ( data , filename ) : name , ext = get_file_extension ( filename ) func = json_write_data if ext == '.json' else yaml_write_data return func ( data , filename )
9087	async def update ( self ) -> None : _LOGGER . debug ( "Requesting state update from server (S00, S14)" ) await asyncio . gather ( self . send_command ( 'S00' ) , self . send_command ( 'S14' ) , )
5536	def write ( self , process_tile , data ) : if isinstance ( process_tile , tuple ) : process_tile = self . config . process_pyramid . tile ( * process_tile ) elif not isinstance ( process_tile , BufferedTile ) : raise ValueError ( "invalid process_tile type: %s" % type ( process_tile ) ) if self . config . mode not in [ "continue" , "overwrite" ] : raise ValueError ( "cannot write output in current process mode" ) if self . config . mode == "continue" and ( self . config . output . tiles_exist ( process_tile ) ) : message = "output exists, not overwritten" logger . debug ( ( process_tile . id , message ) ) return ProcessInfo ( tile = process_tile , processed = False , process_msg = None , written = False , write_msg = message ) elif data is None : message = "output empty, nothing written" logger . debug ( ( process_tile . id , message ) ) return ProcessInfo ( tile = process_tile , processed = False , process_msg = None , written = False , write_msg = message ) else : with Timer ( ) as t : self . config . output . write ( process_tile = process_tile , data = data ) message = "output written in %s" % t logger . debug ( ( process_tile . id , message ) ) return ProcessInfo ( tile = process_tile , processed = False , process_msg = None , written = True , write_msg = message )
7208	def task_ids ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get task IDs.' ) if self . batch_values : raise NotImplementedError ( "Query Each Workflow Id within the Batch Workflow for task IDs." ) wf = self . workflow . get ( self . id ) return [ task [ 'id' ] for task in wf [ 'tasks' ] ]
6478	def _normalised_numpy ( self ) : dx = ( self . screen . width / float ( len ( self . points ) ) ) oy = ( self . screen . height ) points = np . array ( self . points ) - self . minimum points = points * 4.0 / self . extents * self . size . y for x , y in enumerate ( points ) : yield Point ( ( dx * x , min ( oy , oy - y ) , ) )
8804	def build_payload ( ipaddress , event_type , event_time = None , start_time = None , end_time = None ) : payload = { 'event_type' : unicode ( event_type ) , 'tenant_id' : unicode ( ipaddress . used_by_tenant_id ) , 'ip_address' : unicode ( ipaddress . address_readable ) , 'ip_version' : int ( ipaddress . version ) , 'ip_type' : unicode ( ipaddress . address_type ) , 'id' : unicode ( ipaddress . id ) } if event_type == IP_EXISTS : if start_time is None or end_time is None : raise ValueError ( 'IP_BILL: {} start_time/end_time cannot be empty' . format ( event_type ) ) payload . update ( { 'startTime' : unicode ( convert_timestamp ( start_time ) ) , 'endTime' : unicode ( convert_timestamp ( end_time ) ) } ) elif event_type in [ IP_ADD , IP_DEL , IP_ASSOC , IP_DISASSOC ] : if event_time is None : raise ValueError ( 'IP_BILL: {}: event_time cannot be NULL' . format ( event_type ) ) payload . update ( { 'eventTime' : unicode ( convert_timestamp ( event_time ) ) , 'subnet_id' : unicode ( ipaddress . subnet_id ) , 'network_id' : unicode ( ipaddress . network_id ) , 'public' : True if ipaddress . network_id == PUBLIC_NETWORK_ID else False , } ) else : raise ValueError ( 'IP_BILL: bad event_type: {}' . format ( event_type ) ) return payload
10832	def delete ( cls , group , admin ) : with db . session . begin_nested ( ) : obj = cls . query . filter ( cls . admin == admin , cls . group == group ) . one ( ) db . session . delete ( obj )
10955	def get_update_io_tiles ( self , params , values ) : otile = self . get_update_tile ( params , values ) if otile is None : return [ None ] * 3 ptile = self . get_padding_size ( otile ) or util . Tile ( 0 , dim = otile . dim ) otile = util . Tile . intersection ( otile , self . oshape ) if ( otile . shape <= 0 ) . any ( ) : raise UpdateError ( "update triggered invalid tile size" ) if ( ptile . shape < 0 ) . any ( ) or ( ptile . shape > self . oshape . shape ) . any ( ) : raise UpdateError ( "update triggered invalid padding tile size" ) outer = otile . pad ( ( ptile . shape + 1 ) // 2 ) inner , outer = outer . reflect_overhang ( self . oshape ) iotile = inner . translate ( - outer . l ) outer = util . Tile . intersection ( outer , self . oshape ) inner = util . Tile . intersection ( inner , self . oshape ) return outer , inner , iotile
5069	def ungettext_min_max ( singular , plural , range_text , min_val , max_val ) : if min_val is None and max_val is None : return None if min_val == max_val or min_val is None or max_val is None : return ungettext ( singular , plural , min_val or max_val ) . format ( min_val or max_val ) return range_text . format ( min_val , max_val )
4997	def assign_enterprise_learner_role ( sender , instance , ** kwargs ) : if kwargs [ 'created' ] and instance . user : enterprise_learner_role , __ = SystemWideEnterpriseRole . objects . get_or_create ( name = ENTERPRISE_LEARNER_ROLE ) SystemWideEnterpriseUserRoleAssignment . objects . get_or_create ( user = instance . user , role = enterprise_learner_role )
94	def quokka ( size = None , extract = None ) : img = imageio . imread ( QUOKKA_FP , pilmode = "RGB" ) if extract is not None : bb = _quokka_normalize_extract ( extract ) img = bb . extract_from_image ( img ) if size is not None : shape_resized = _compute_resized_shape ( img . shape , size ) img = imresize_single_image ( img , shape_resized [ 0 : 2 ] ) return img
550	def __checkCancelation ( self ) : print >> sys . stderr , "reporter:counter:HypersearchWorker,numRecords,50" jobCancel = self . _jobsDAO . jobGetFields ( self . _jobID , [ 'cancel' ] ) [ 0 ] if jobCancel : self . _cmpReason = ClientJobsDAO . CMPL_REASON_KILLED self . _isCanceled = True self . _logger . info ( "Model %s canceled because Job %s was stopped." , self . _modelID , self . _jobID ) else : stopReason = self . _jobsDAO . modelsGetFields ( self . _modelID , [ 'engStop' ] ) [ 0 ] if stopReason is None : pass elif stopReason == ClientJobsDAO . STOP_REASON_KILLED : self . _cmpReason = ClientJobsDAO . CMPL_REASON_KILLED self . _isKilled = True self . _logger . info ( "Model %s canceled because it was killed by hypersearch" , self . _modelID ) elif stopReason == ClientJobsDAO . STOP_REASON_STOPPED : self . _cmpReason = ClientJobsDAO . CMPL_REASON_STOPPED self . _isCanceled = True self . _logger . info ( "Model %s stopped because hypersearch ended" , self . _modelID ) else : raise RuntimeError ( "Unexpected stop reason encountered: %s" % ( stopReason ) )
9701	def send ( self , msg ) : slipDriver = sliplib . Driver ( ) slipData = slipDriver . send ( msg ) res = self . _serialPort . write ( slipData ) return res
12379	def post ( self , request , response ) : if self . slug is not None : raise http . exceptions . NotImplemented ( ) self . assert_operations ( 'create' ) data = self . _clean ( None , self . request . read ( deserialize = True ) ) item = self . create ( data ) self . response . status = http . client . CREATED self . make_response ( item )
4513	def drawCircle ( self , x0 , y0 , r , color = None ) : md . draw_circle ( self . set , x0 , y0 , r , color )
1131	def urldefrag ( url ) : if '#' in url : s , n , p , a , q , frag = urlparse ( url ) defrag = urlunparse ( ( s , n , p , a , q , '' ) ) return defrag , frag else : return url , ''
2085	def format_options ( self , ctx , formatter ) : field_opts = [ ] global_opts = [ ] local_opts = [ ] other_opts = [ ] for param in self . params : if param . name in SETTINGS_PARMS : opts = global_opts elif getattr ( param , 'help' , None ) and param . help . startswith ( '[FIELD]' ) : opts = field_opts param . help = param . help [ len ( '[FIELD]' ) : ] else : opts = local_opts rv = param . get_help_record ( ctx ) if rv is None : continue else : opts . append ( rv ) if self . add_help_option : help_options = self . get_help_option_names ( ctx ) if help_options : other_opts . append ( [ join_options ( help_options ) [ 0 ] , 'Show this message and exit.' ] ) if field_opts : with formatter . section ( 'Field Options' ) : formatter . write_dl ( field_opts ) if local_opts : with formatter . section ( 'Local Options' ) : formatter . write_dl ( local_opts ) if global_opts : with formatter . section ( 'Global Options' ) : formatter . write_dl ( global_opts ) if other_opts : with formatter . section ( 'Other Options' ) : formatter . write_dl ( other_opts )
5423	def _wait_after ( provider , job_ids , poll_interval , stop_on_failure ) : job_ids_to_check = { j for j in job_ids if j != dsub_util . NO_JOB } error_messages = [ ] while job_ids_to_check and ( not error_messages or not stop_on_failure ) : print ( 'Waiting for: %s.' % ( ', ' . join ( job_ids_to_check ) ) ) jobs_left = _wait_for_any_job ( provider , job_ids_to_check , poll_interval ) jobs_completed = job_ids_to_check . difference ( jobs_left ) tasks_completed = provider . lookup_job_tasks ( { '*' } , job_ids = jobs_completed ) dominant_job_tasks = _dominant_task_for_jobs ( tasks_completed ) if len ( dominant_job_tasks ) != len ( jobs_completed ) : jobs_found = dsub_util . tasks_to_job_ids ( dominant_job_tasks ) jobs_not_found = jobs_completed . difference ( jobs_found ) for j in jobs_not_found : error = '%s: not found' % j print_error ( ' %s' % error ) error_messages += [ error ] for t in dominant_job_tasks : job_id = t . get_field ( 'job-id' ) status = t . get_field ( 'task-status' ) print ( ' %s: %s' % ( str ( job_id ) , str ( status ) ) ) if status in [ 'FAILURE' , 'CANCELED' ] : error_messages += [ provider . get_tasks_completion_messages ( [ t ] ) ] job_ids_to_check = jobs_left return error_messages
3158	def _post ( self , url , data = None ) : url = urljoin ( self . base_url , url ) try : r = self . _make_request ( ** dict ( method = 'POST' , url = url , json = data , auth = self . auth , timeout = self . timeout , hooks = self . request_hooks , headers = self . request_headers ) ) except requests . exceptions . RequestException as e : raise e else : if r . status_code >= 400 : try : error_data = r . json ( ) except ValueError : error_data = { "response" : r } raise MailChimpError ( error_data ) if r . status_code == 204 : return None return r . json ( )
4187	def window_riesz ( N ) : r n = linspace ( - N / 2. , ( N ) / 2. , N ) w = 1 - abs ( n / ( N / 2. ) ) ** 2. return w
13678	def filenumber_handle ( self ) : self . __results = [ ] self . __dirs = [ ] self . __files = [ ] self . __ftp = self . connect ( ) self . __ftp . dir ( self . args . path , self . __results . append ) self . logger . debug ( "dir results: {}" . format ( self . __results ) ) self . quit ( ) status = self . ok for data in self . __results : if "<DIR>" in data : self . __dirs . append ( str ( data . split ( ) [ 3 ] ) ) else : self . __files . append ( str ( data . split ( ) [ 2 ] ) ) self . __result = len ( self . __files ) self . logger . debug ( "result: {}" . format ( self . __result ) ) if self . __result > self . args . warning : status = self . warning if self . __result > self . args . critical : status = self . critical self . shortoutput = "Found {0} files in {1}." . format ( self . __result , self . args . path ) [ self . longoutput . append ( line ) for line in self . __results if self . __results ] self . perfdata . append ( "{path}={result};{warn};{crit};0;" . format ( crit = self . args . critical , warn = self . args . warning , result = self . __result , path = self . args . path ) ) self . logger . debug ( "Return status and output." ) status ( self . output ( ) )
13260	def main ( argv = None , white_list = None , load_yaz_extension = True ) : assert argv is None or isinstance ( argv , list ) , type ( argv ) assert white_list is None or isinstance ( white_list , list ) , type ( white_list ) assert isinstance ( load_yaz_extension , bool ) , type ( load_yaz_extension ) argv = sys . argv if argv is None else argv assert len ( argv ) > 0 , len ( argv ) if load_yaz_extension : load ( "~/.yaz" , "yaz_extension" ) parser = Parser ( prog = argv [ 0 ] ) parser . add_task_tree ( get_task_tree ( white_list ) ) task , kwargs = parser . parse_arguments ( argv ) if task : try : result = task ( ** kwargs ) if isinstance ( result , bool ) : code = 0 if result else 1 output = None elif isinstance ( result , int ) : code = result % 256 output = None else : code = 0 output = result except Error as error : code = error . return_code output = error else : code = 1 output = parser . format_help ( ) . rstrip ( ) if output is not None : print ( output ) sys . exit ( code )
8080	def relmoveto ( self , x , y ) : if self . _path is None : raise ShoebotError ( _ ( "No current path. Use beginpath() first." ) ) self . _path . relmoveto ( x , y )
10914	def separate_particles_into_groups ( s , region_size = 40 , bounds = None , doshift = False ) : imtile = s . oshape . translate ( - s . pad ) bounding_tile = ( imtile if bounds is None else Tile ( bounds [ 0 ] , bounds [ 1 ] ) ) rs = ( np . ones ( bounding_tile . dim , dtype = 'int' ) * region_size if np . size ( region_size ) == 1 else np . array ( region_size ) ) n_translate = np . ceil ( bounding_tile . shape . astype ( 'float' ) / rs ) . astype ( 'int' ) particle_groups = [ ] tile = Tile ( left = bounding_tile . l , right = bounding_tile . l + rs ) if doshift == 'rand' : doshift = np . random . choice ( [ True , False ] ) if doshift : shift = rs // 2 n_translate += 1 else : shift = 0 deltas = np . meshgrid ( * [ np . arange ( i ) for i in n_translate ] ) positions = s . obj_get_positions ( ) if bounds is None : positions = np . clip ( positions , imtile . l + 1e-3 , imtile . r - 1e-3 ) groups = list ( map ( lambda * args : find_particles_in_tile ( positions , tile . translate ( np . array ( args ) * rs - shift ) ) , * [ d . ravel ( ) for d in deltas ] ) ) for i in range ( len ( groups ) - 1 , - 1 , - 1 ) : if groups [ i ] . size == 0 : groups . pop ( i ) assert _check_groups ( s , groups ) return groups
3625	def encode ( latitude , longitude , precision = 12 ) : lat_interval , lon_interval = ( - 90.0 , 90.0 ) , ( - 180.0 , 180.0 ) geohash = [ ] bits = [ 16 , 8 , 4 , 2 , 1 ] bit = 0 ch = 0 even = True while len ( geohash ) < precision : if even : mid = ( lon_interval [ 0 ] + lon_interval [ 1 ] ) / 2 if longitude > mid : ch |= bits [ bit ] lon_interval = ( mid , lon_interval [ 1 ] ) else : lon_interval = ( lon_interval [ 0 ] , mid ) else : mid = ( lat_interval [ 0 ] + lat_interval [ 1 ] ) / 2 if latitude > mid : ch |= bits [ bit ] lat_interval = ( mid , lat_interval [ 1 ] ) else : lat_interval = ( lat_interval [ 0 ] , mid ) even = not even if bit < 4 : bit += 1 else : geohash += __base32 [ ch ] bit = 0 ch = 0 return '' . join ( geohash )
8831	def send ( self , s ) : self . _socket . send ( s . encode ( ) ) return self . read ( )
11961	def is_wildcard_nm ( nm ) : try : dec = 0xFFFFFFFF - _dot_to_dec ( nm , check = True ) except ValueError : return False if dec in _NETMASKS_VALUES : return True return False
159	def Grayscale ( alpha = 0 , from_colorspace = "RGB" , name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return ChangeColorspace ( to_colorspace = ChangeColorspace . GRAY , alpha = alpha , from_colorspace = from_colorspace , name = name , deterministic = deterministic , random_state = random_state )
8889	def _self_referential_fk ( klass_model ) : for f in klass_model . _meta . concrete_fields : if f . related_model : if issubclass ( klass_model , f . related_model ) : return f . attname return None
7355	def seq_to_str ( obj , sep = "," ) : if isinstance ( obj , string_classes ) : return obj elif isinstance ( obj , ( list , tuple ) ) : return sep . join ( [ str ( x ) for x in obj ] ) else : return str ( obj )
10191	def get_anonymization_salt ( ts ) : salt_key = 'stats:salt:{}' . format ( ts . date ( ) . isoformat ( ) ) salt = current_cache . get ( salt_key ) if not salt : salt_bytes = os . urandom ( 32 ) salt = b64encode ( salt_bytes ) . decode ( 'utf-8' ) current_cache . set ( salt_key , salt , timeout = 60 * 60 * 24 ) return salt
1355	def get_argument_role ( self ) : try : return self . get_argument ( constants . PARAM_ROLE , default = None ) except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
11096	def select_by_pattern_in_fname ( self , pattern , recursive = True , case_sensitive = False ) : if case_sensitive : def filters ( p ) : return pattern in p . fname else : pattern = pattern . lower ( ) def filters ( p ) : return pattern in p . fname . lower ( ) return self . select_file ( filters , recursive )
3740	def omega ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ 'LK' , 'DEFINITION' ] ) : r def list_methods ( ) : methods = [ ] if CASRN in _crit_PSRKR4 . index and not np . isnan ( _crit_PSRKR4 . at [ CASRN , 'omega' ] ) : methods . append ( 'PSRK' ) if CASRN in _crit_PassutDanner . index and not np . isnan ( _crit_PassutDanner . at [ CASRN , 'omega' ] ) : methods . append ( 'PD' ) if CASRN in _crit_Yaws . index and not np . isnan ( _crit_Yaws . at [ CASRN , 'omega' ] ) : methods . append ( 'YAWS' ) Tcrit , Pcrit = Tc ( CASRN ) , Pc ( CASRN ) if Tcrit and Pcrit : if Tb ( CASRN ) : methods . append ( 'LK' ) if VaporPressure ( CASRN = CASRN ) . T_dependent_property ( Tcrit * 0.7 ) : methods . append ( 'DEFINITION' ) if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( 'NONE' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'PSRK' : _omega = float ( _crit_PSRKR4 . at [ CASRN , 'omega' ] ) elif Method == 'PD' : _omega = float ( _crit_PassutDanner . at [ CASRN , 'omega' ] ) elif Method == 'YAWS' : _omega = float ( _crit_Yaws . at [ CASRN , 'omega' ] ) elif Method == 'LK' : _omega = LK_omega ( Tb ( CASRN ) , Tc ( CASRN ) , Pc ( CASRN ) ) elif Method == 'DEFINITION' : P = VaporPressure ( CASRN = CASRN ) . T_dependent_property ( Tc ( CASRN ) * 0.7 ) _omega = - log10 ( P / Pc ( CASRN ) ) - 1.0 elif Method == 'NONE' : _omega = None else : raise Exception ( 'Failure in in function' ) return _omega
1961	def sys_fsync ( self , fd ) : ret = 0 try : self . files [ fd ] . sync ( ) except IndexError : ret = - errno . EBADF except FdError : ret = - errno . EINVAL return ret
2191	def expired ( self , cfgstr = None , product = None ) : products = self . _rectify_products ( product ) certificate = self . _get_certificate ( cfgstr = cfgstr ) if certificate is None : is_expired = True elif products is None : is_expired = False elif not all ( map ( os . path . exists , products ) ) : is_expired = True else : product_file_hash = self . _product_file_hash ( products ) certificate_hash = certificate . get ( 'product_file_hash' , None ) is_expired = product_file_hash != certificate_hash return is_expired
12428	def check_directories ( self ) : self . log . debug ( 'Checking directories' ) if not os . path . exists ( self . _ve_dir ) : os . makedirs ( self . _ve_dir ) if not os . path . exists ( self . _app_dir ) : os . makedirs ( self . _app_dir ) if not os . path . exists ( self . _conf_dir ) : os . makedirs ( self . _conf_dir ) if not os . path . exists ( self . _var_dir ) : os . makedirs ( self . _var_dir ) if not os . path . exists ( self . _log_dir ) : os . makedirs ( self . _log_dir ) if not os . path . exists ( self . _script_dir ) : os . makedirs ( self . _script_dir ) uwsgi_params = '/etc/nginx/uwsgi_params' if os . path . exists ( uwsgi_params ) : shutil . copy ( uwsgi_params , self . _conf_dir ) else : logging . warning ( 'Unable to find Nginx uwsgi_params. You must manually copy this to {0}.' . format ( self . _conf_dir ) ) mime_types = '/etc/nginx/mime.types' if os . path . exists ( mime_types ) : shutil . copy ( mime_types , self . _conf_dir ) self . _include_mimetypes = True else : logging . warn ( 'Unable to find mime.types for Nginx. You must manually copy this to {0}.' . format ( self . _conf_dir ) )
8187	def prune ( self , depth = 0 ) : for n in list ( self . nodes ) : if len ( n . links ) <= depth : self . remove_node ( n . id )
1844	def JO ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . OF , target . read ( ) , cpu . PC )
5579	def write_output_metadata ( output_params ) : if "path" in output_params : metadata_path = os . path . join ( output_params [ "path" ] , "metadata.json" ) logger . debug ( "check for output %s" , metadata_path ) try : existing_params = read_output_metadata ( metadata_path ) logger . debug ( "%s exists" , metadata_path ) logger . debug ( "existing output parameters: %s" , pformat ( existing_params ) ) existing_tp = existing_params [ "pyramid" ] current_params = params_to_dump ( output_params ) logger . debug ( "current output parameters: %s" , pformat ( current_params ) ) current_tp = BufferedTilePyramid ( ** current_params [ "pyramid" ] ) if existing_tp != current_tp : raise MapcheteConfigError ( "pyramid definitions between existing and new output do not match: " "%s != %s" % ( existing_tp , current_tp ) ) existing_format = existing_params [ "driver" ] [ "format" ] current_format = current_params [ "driver" ] [ "format" ] if existing_format != current_format : raise MapcheteConfigError ( "existing output format does not match new output format: " "%s != %s" % ( ( existing_format , current_format ) ) ) except FileNotFoundError : logger . debug ( "%s does not exist" , metadata_path ) dump_params = params_to_dump ( output_params ) write_json ( metadata_path , dump_params ) else : logger . debug ( "no path parameter found" )
1666	def CheckRedundantOverrideOrFinal ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] declarator_end = line . rfind ( ')' ) if declarator_end >= 0 : fragment = line [ declarator_end : ] else : if linenum > 1 and clean_lines . elided [ linenum - 1 ] . rfind ( ')' ) >= 0 : fragment = line else : return if Search ( r'\boverride\b' , fragment ) and Search ( r'\bfinal\b' , fragment ) : error ( filename , linenum , 'readability/inheritance' , 4 , ( '"override" is redundant since function is ' 'already declared as "final"' ) )
4475	def sample_clip_indices ( filename , n_samples , sr ) : with psf . SoundFile ( str ( filename ) , mode = 'r' ) as soundf : n_target = int ( np . ceil ( n_samples * soundf . samplerate / float ( sr ) ) ) if len ( soundf ) < n_target : raise RuntimeError ( 'Source {} (length={})' . format ( filename , len ( soundf ) ) + ' must be at least the length of the input ({})' . format ( n_target ) ) start = np . random . randint ( 0 , 1 + len ( soundf ) - n_target ) stop = start + n_target return start , stop
2283	def check_R_package ( self , package ) : test_package = not bool ( launch_R_script ( "{}/R_templates/test_import.R" . format ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) , { "{package}" : package } , verbose = True ) ) return test_package
2656	def makedirs ( self , path , mode = 511 , exist_ok = False ) : if exist_ok is False and self . isdir ( path ) : raise OSError ( 'Target directory {} already exists' . format ( path ) ) self . execute_wait ( 'mkdir -p {}' . format ( path ) ) self . sftp_client . chmod ( path , mode )
9153	def get_exif_info ( self ) : _dict = { } for tag in _EXIF_TAGS : ret = self . img . attribute ( "EXIF:%s" % tag ) if ret and ret != 'unknown' : _dict [ tag ] = ret return _dict
1506	def template_heron_tools_hcl ( cl_args , masters , zookeepers ) : heron_tools_hcl_template = "%s/standalone/templates/heron_tools.template.hcl" % cl_args [ "config_path" ] heron_tools_hcl_actual = "%s/standalone/resources/heron_tools.hcl" % cl_args [ "config_path" ] single_master = masters [ 0 ] template_file ( heron_tools_hcl_template , heron_tools_hcl_actual , { "<zookeeper_host:zookeeper_port>" : "," . join ( [ '%s' % zk if ":" in zk else '%s:2181' % zk for zk in zookeepers ] ) , "<heron_tracker_executable>" : '"%s/heron-tracker"' % config . get_heron_bin_dir ( ) , "<heron_tools_hostname>" : '"%s"' % get_hostname ( single_master , cl_args ) , "<heron_ui_executable>" : '"%s/heron-ui"' % config . get_heron_bin_dir ( ) } )
11214	def compare_signature ( expected : Union [ str , bytes ] , actual : Union [ str , bytes ] ) -> bool : expected = util . to_bytes ( expected ) actual = util . to_bytes ( actual ) return hmac . compare_digest ( expected , actual )
11671	def _flann_args ( self , X = None ) : "The dictionary of arguments to give to FLANN." args = { 'cores' : self . _n_jobs } if self . flann_algorithm == 'auto' : if X is None or X . dim > 5 : args [ 'algorithm' ] = 'linear' else : args [ 'algorithm' ] = 'kdtree_single' else : args [ 'algorithm' ] = self . flann_algorithm if self . flann_args : args . update ( self . flann_args ) try : FLANNParameters ( ) . update ( args ) except AttributeError as e : msg = "flann_args contains an invalid argument:\n {}" raise TypeError ( msg . format ( e ) ) return args
4747	def __parse_parms ( self ) : args = list ( ) for key , val in self . __parm . items ( ) : key = key . replace ( "FIO_" , "" ) . lower ( ) if key == "runtime" : args . append ( "--time_based" ) if val is None : args . append ( "--%s" % key ) else : args . append ( "--%s=%s" % ( key , val ) ) return args
12123	def validate_activatable_models ( ) : for model in get_activatable_models ( ) : activatable_field = next ( ( f for f in model . _meta . fields if f . __class__ == models . BooleanField and f . name == model . ACTIVATABLE_FIELD_NAME ) , None ) if activatable_field is None : raise ValidationError ( ( 'Model {0} is an activatable model. It must define an activatable BooleanField that ' 'has a field name of model.ACTIVATABLE_FIELD_NAME (which defaults to is_active)' . format ( model ) ) ) if not model . ALLOW_CASCADE_DELETE : for field in model . _meta . fields : if field . __class__ in ( models . ForeignKey , models . OneToOneField ) : if field . remote_field . on_delete == models . CASCADE : raise ValidationError ( ( 'Model {0} is an activatable model. All ForeignKey and OneToOneFields ' 'must set on_delete methods to something other than CASCADE (the default). ' 'If you want to explicitely allow cascade deletes, then you must set the ' 'ALLOW_CASCADE_DELETE=True class variable on your model.' ) . format ( model ) )
1540	def set_config ( self , config ) : if not isinstance ( config , dict ) : raise TypeError ( "Argument to set_config needs to be dict, given: %s" % str ( config ) ) self . _topology_config = config
554	def getFieldContributions ( self ) : if self . _hsObj . _fixedFields is not None : return dict ( ) , dict ( ) predictedEncoderName = self . _hsObj . _predictedFieldEncoder fieldScores = [ ] for swarmId , info in self . _state [ 'swarms' ] . iteritems ( ) : encodersUsed = swarmId . split ( '.' ) if len ( encodersUsed ) != 1 : continue field = self . getEncoderNameFromKey ( encodersUsed [ 0 ] ) bestScore = info [ 'bestErrScore' ] if bestScore is None : ( _modelId , bestScore ) = self . _hsObj . _resultsDB . bestModelIdAndErrScore ( swarmId ) fieldScores . append ( ( bestScore , field ) ) if self . _hsObj . _searchType == HsSearchType . legacyTemporal : assert ( len ( fieldScores ) == 1 ) ( baseErrScore , baseField ) = fieldScores [ 0 ] for swarmId , info in self . _state [ 'swarms' ] . iteritems ( ) : encodersUsed = swarmId . split ( '.' ) if len ( encodersUsed ) != 2 : continue fields = [ self . getEncoderNameFromKey ( name ) for name in encodersUsed ] fields . remove ( baseField ) fieldScores . append ( ( info [ 'bestErrScore' ] , fields [ 0 ] ) ) else : fieldScores . sort ( reverse = True ) if self . _hsObj . _maxBranching > 0 and len ( fieldScores ) > self . _hsObj . _maxBranching : baseErrScore = fieldScores [ - self . _hsObj . _maxBranching - 1 ] [ 0 ] else : baseErrScore = fieldScores [ 0 ] [ 0 ] pctFieldContributionsDict = dict ( ) absFieldContributionsDict = dict ( ) if baseErrScore is not None : if abs ( baseErrScore ) < 0.00001 : baseErrScore = 0.00001 for ( errScore , field ) in fieldScores : if errScore is not None : pctBetter = ( baseErrScore - errScore ) * 100.0 / baseErrScore else : pctBetter = 0.0 errScore = baseErrScore pctFieldContributionsDict [ field ] = pctBetter absFieldContributionsDict [ field ] = baseErrScore - errScore self . logger . debug ( "FieldContributions: %s" % ( pctFieldContributionsDict ) ) return pctFieldContributionsDict , absFieldContributionsDict
5843	def get_design_run_results ( self , data_view_id , run_uuid ) : url = routes . get_data_view_design_results ( data_view_id , run_uuid ) response = self . _get ( url ) . json ( ) result = response [ "data" ] return DesignResults ( best_materials = result . get ( "best_material_results" ) , next_experiments = result . get ( "next_experiment_results" ) )
1596	def format_mtime ( mtime ) : now = datetime . now ( ) dt = datetime . fromtimestamp ( mtime ) return '%s %2d %5s' % ( dt . strftime ( '%b' ) , dt . day , dt . year if dt . year != now . year else dt . strftime ( '%H:%M' ) )
623	def indexFromCoordinates ( coordinates , dimensions ) : index = 0 for i , dimension in enumerate ( dimensions ) : index *= dimension index += coordinates [ i ] return index
12665	def apply_mask ( image , mask_img ) : img = check_img ( image ) mask = check_img ( mask_img ) check_img_compatibility ( img , mask ) vol = img . get_data ( ) mask_data , _ = load_mask_data ( mask ) return vol [ mask_data ] , mask_data
3398	def update_costs ( self ) : for var in self . indicators : if var not in self . costs : self . costs [ var ] = var . cost else : if var . _get_primal ( ) > self . integer_threshold : self . costs [ var ] += var . cost self . model . objective . set_linear_coefficients ( self . costs )
897	def generateFromNumbers ( self , numbers ) : sequence = [ ] for number in numbers : if number == None : sequence . append ( number ) else : pattern = self . patternMachine . get ( number ) sequence . append ( pattern ) return sequence
4699	def get_meta ( offset , length , output ) : if env ( ) : cij . err ( "cij.nvme.meta: Invalid NVMe ENV." ) return 1 nvme = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) max_size = 0x40000 with open ( output , "wb" ) as fout : for off in range ( offset , length , max_size ) : size = min ( length - off , max_size ) cmd = [ "nvme get-log" , nvme [ "DEV_PATH" ] , "-i 0xca" , "-o 0x%x" % off , "-l 0x%x" % size , "-b" ] status , stdout , _ = cij . ssh . command ( cmd , shell = True ) if status : cij . err ( "cij.nvme.meta: Error get chunk meta" ) return 1 fout . write ( stdout ) return 0
7141	def transfer_multiple ( self , destinations , priority = prio . NORMAL , payment_id = None , unlock_time = 0 , relay = True ) : return self . accounts [ 0 ] . transfer_multiple ( destinations , priority = priority , payment_id = payment_id , unlock_time = unlock_time , relay = relay )
3860	def _wrap_event ( event_ ) : cls = conversation_event . ConversationEvent if event_ . HasField ( 'chat_message' ) : cls = conversation_event . ChatMessageEvent elif event_ . HasField ( 'otr_modification' ) : cls = conversation_event . OTREvent elif event_ . HasField ( 'conversation_rename' ) : cls = conversation_event . RenameEvent elif event_ . HasField ( 'membership_change' ) : cls = conversation_event . MembershipChangeEvent elif event_ . HasField ( 'hangout_event' ) : cls = conversation_event . HangoutEvent elif event_ . HasField ( 'group_link_sharing_modification' ) : cls = conversation_event . GroupLinkSharingModificationEvent return cls ( event_ )
10530	def get_project ( project_id ) : try : res = _pybossa_req ( 'get' , 'project' , project_id ) if res . get ( 'id' ) : return Project ( res ) else : return res except : raise
6198	def print_sizes ( self ) : float_size = 4 MB = 1024 * 1024 size_ = self . n_samples * float_size em_size = size_ * self . num_particles / MB pos_size = 3 * size_ * self . num_particles / MB print ( " Number of particles:" , self . num_particles ) print ( " Number of time steps:" , self . n_samples ) print ( " Emission array - 1 particle (float32): %.1f MB" % ( size_ / MB ) ) print ( " Emission array (float32): %.1f MB" % em_size ) print ( " Position array (float32): %.1f MB " % pos_size )
13256	def as_dict ( self ) : entry_dict = { } entry_dict [ 'UUID' ] = self . uuid entry_dict [ 'Creation Date' ] = self . time entry_dict [ 'Time Zone' ] = self . tz if self . tags : entry_dict [ 'Tags' ] = self . tags entry_dict [ 'Entry Text' ] = self . text entry_dict [ 'Starred' ] = self . starred entry_dict [ 'Location' ] = self . location return entry_dict
506	def getLabels ( self , start = None , end = None ) : if len ( self . _recordsCache ) == 0 : return { 'isProcessing' : False , 'recordLabels' : [ ] } try : start = int ( start ) except Exception : start = 0 try : end = int ( end ) except Exception : end = self . _recordsCache [ - 1 ] . ROWID if end <= start : raise HTMPredictionModelInvalidRangeError ( "Invalid supplied range for 'getLabels'." , debugInfo = { 'requestRange' : { 'startRecordID' : start , 'endRecordID' : end } , 'numRecordsStored' : len ( self . _recordsCache ) } ) results = { 'isProcessing' : False , 'recordLabels' : [ ] } ROWIDX = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) validIdx = numpy . where ( ( ROWIDX >= start ) & ( ROWIDX < end ) ) [ 0 ] . tolist ( ) categories = self . _knnclassifier . getCategoryList ( ) for idx in validIdx : row = dict ( ROWID = int ( ROWIDX [ idx ] ) , labels = self . _categoryToLabelList ( categories [ idx ] ) ) results [ 'recordLabels' ] . append ( row ) return results
13441	def cmd_init_push_to_cloud ( args ) : ( lcat , ccat ) = ( args . local_catalog , args . cloud_catalog ) logging . info ( "[init-push-to-cloud]: %s => %s" % ( lcat , ccat ) ) if not isfile ( lcat ) : args . error ( "[init-push-to-cloud] The local catalog does not exist: %s" % lcat ) if isfile ( ccat ) : args . error ( "[init-push-to-cloud] The cloud catalog already exist: %s" % ccat ) ( lmeta , cmeta ) = ( "%s.lrcloud" % lcat , "%s.lrcloud" % ccat ) if isfile ( lmeta ) : args . error ( "[init-push-to-cloud] The local meta-data already exist: %s" % lmeta ) if isfile ( cmeta ) : args . error ( "[init-push-to-cloud] The cloud meta-data already exist: %s" % cmeta ) logging . info ( "Locking local catalog: %s" % ( lcat ) ) if not lock_file ( lcat ) : raise RuntimeError ( "The catalog %s is locked!" % lcat ) util . copy ( lcat , ccat ) mfile = MetaFile ( lmeta ) utcnow = datetime . utcnow ( ) . strftime ( DATETIME_FORMAT ) [ : - 4 ] mfile [ 'catalog' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'catalog' ] [ 'modification_utc' ] = utcnow mfile [ 'catalog' ] [ 'filename' ] = lcat mfile [ 'last_push' ] [ 'filename' ] = ccat mfile [ 'last_push' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'last_push' ] [ 'modification_utc' ] = utcnow mfile . flush ( ) mfile = MetaFile ( cmeta ) mfile [ 'changeset' ] [ 'is_base' ] = True mfile [ 'changeset' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'changeset' ] [ 'modification_utc' ] = utcnow mfile [ 'changeset' ] [ 'filename' ] = basename ( ccat ) mfile . flush ( ) if not args . no_smart_previews : copy_smart_previews ( lcat , ccat , local2cloud = True ) logging . info ( "Unlocking local catalog: %s" % ( lcat ) ) unlock_file ( lcat ) logging . info ( "[init-push-to-cloud]: Success!" )
441	def count_params ( self ) : n_params = 0 for _i , p in enumerate ( self . all_params ) : n = 1 for s in p . get_shape ( ) : try : s = int ( s ) except Exception : s = 1 if s : n = n * s n_params = n_params + n return n_params
10399	def run_with_graph_transformation ( self ) -> Iterable [ BELGraph ] : yield self . get_remaining_graph ( ) while not self . done_chomping ( ) : while not list ( self . iter_leaves ( ) ) : self . remove_random_edge ( ) yield self . get_remaining_graph ( ) self . score_leaves ( ) yield self . get_remaining_graph ( )
9945	def copy_file ( self , path , prefixed_path , source_storage ) : if prefixed_path in self . copied_files : return self . log ( "Skipping '%s' (already copied earlier)" % path ) if not self . delete_file ( path , prefixed_path , source_storage ) : return source_path = source_storage . path ( path ) if self . dry_run : self . log ( "Pretending to copy '%s'" % source_path , level = 1 ) else : self . log ( "Copying '%s'" % source_path , level = 1 ) with source_storage . open ( path ) as source_file : self . storage . save ( prefixed_path , source_file ) self . copied_files . append ( prefixed_path )
3330	def acquire_write ( self , timeout = None ) : if timeout is not None : endtime = time ( ) + timeout me , upgradewriter = currentThread ( ) , False self . __condition . acquire ( ) try : if self . __writer is me : self . __writercount += 1 return elif me in self . __readers : if self . __upgradewritercount : raise ValueError ( "Inevitable dead lock, denying write lock" ) upgradewriter = True self . __upgradewritercount = self . __readers . pop ( me ) else : self . __pendingwriters . append ( me ) while True : if not self . __readers and self . __writer is None : if self . __upgradewritercount : if upgradewriter : self . __writer = me self . __writercount = self . __upgradewritercount + 1 self . __upgradewritercount = 0 return elif self . __pendingwriters [ 0 ] is me : self . __writer = me self . __writercount = 1 self . __pendingwriters = self . __pendingwriters [ 1 : ] return if timeout is not None : remaining = endtime - time ( ) if remaining <= 0 : if upgradewriter : self . __readers [ me ] = self . __upgradewritercount self . __upgradewritercount = 0 else : self . __pendingwriters . remove ( me ) raise RuntimeError ( "Acquiring write lock timed out" ) self . __condition . wait ( remaining ) else : self . __condition . wait ( ) finally : self . __condition . release ( )
13692	def broadcast_tx ( self , address , amount , secret , secondsecret = None , vendorfield = '' ) : peer = random . choice ( self . PEERS ) park = Park ( peer , 4001 , constants . ARK_NETHASH , '1.1.1' ) return park . transactions ( ) . create ( address , str ( amount ) , vendorfield , secret , secondsecret )
9646	def get_attributes ( var ) : is_valid = partial ( is_valid_in_template , var ) return list ( filter ( is_valid , dir ( var ) ) )
13217	def connection_dsn ( self , name = None ) : return ' ' . join ( "%s=%s" % ( param , value ) for param , value in self . _connect_options ( name ) )
4245	def _gethostbyname ( self , hostname ) : if self . _databaseType in const . IPV6_EDITIONS : response = socket . getaddrinfo ( hostname , 0 , socket . AF_INET6 ) family , socktype , proto , canonname , sockaddr = response [ 0 ] address , port , flow , scope = sockaddr return address else : return socket . gethostbyname ( hostname )
10486	def _generateFindR ( self , ** kwargs ) : for needle in self . _generateChildrenR ( ) : if needle . _match ( ** kwargs ) : yield needle
10851	def local_max_featuring ( im , radius = 2.5 , noise_size = 1. , bkg_size = None , minmass = 1. , trim_edge = False ) : if radius <= 0 : raise ValueError ( '`radius` must be > 0' ) filtered = nd . gaussian_filter ( im , noise_size , mode = 'mirror' ) if bkg_size is None : bkg_size = 2 * radius filtered -= nd . gaussian_filter ( filtered , bkg_size , mode = 'mirror' ) footprint = generate_sphere ( radius ) e = nd . maximum_filter ( filtered , footprint = footprint ) mass_im = nd . convolve ( filtered , footprint , mode = 'mirror' ) good_im = ( e == filtered ) * ( mass_im > minmass ) pos = np . transpose ( np . nonzero ( good_im ) ) if trim_edge : good = np . all ( pos > 0 , axis = 1 ) & np . all ( pos + 1 < im . shape , axis = 1 ) pos = pos [ good , : ] . copy ( ) masses = mass_im [ pos [ : , 0 ] , pos [ : , 1 ] , pos [ : , 2 ] ] . copy ( ) return pos , masses
12798	def _fetch ( self , method , url = None , post_data = None , parse_data = True , key = None , parameters = None , listener = None , full_return = False ) : headers = self . get_headers ( ) headers [ "Content-Type" ] = "application/json" handlers = [ ] debuglevel = int ( self . _settings [ "debug" ] ) handlers . append ( urllib2 . HTTPHandler ( debuglevel = debuglevel ) ) if hasattr ( httplib , "HTTPS" ) : handlers . append ( urllib2 . HTTPSHandler ( debuglevel = debuglevel ) ) handlers . append ( urllib2 . HTTPCookieProcessor ( cookielib . CookieJar ( ) ) ) password_url = self . _get_password_url ( ) if password_url and "Authorization" not in headers : pwd_manager = urllib2 . HTTPPasswordMgrWithDefaultRealm ( ) pwd_manager . add_password ( None , password_url , self . _settings [ "user" ] , self . _settings [ "password" ] ) handlers . append ( HTTPBasicAuthHandler ( pwd_manager ) ) opener = urllib2 . build_opener ( * handlers ) if post_data is not None : post_data = json . dumps ( post_data ) uri = self . _url ( url , parameters ) request = RESTRequest ( uri , method = method , headers = headers ) if post_data is not None : request . add_data ( post_data ) response = None try : response = opener . open ( request ) body = response . read ( ) if password_url and password_url not in self . _settings [ "authorizations" ] and request . has_header ( "Authorization" ) : self . _settings [ "authorizations" ] [ password_url ] = request . get_header ( "Authorization" ) except urllib2 . HTTPError as e : if e . code == 401 : raise AuthenticationError ( "Access denied while trying to access %s" % uri ) elif e . code == 404 : raise ConnectionError ( "URL not found: %s" % uri ) else : raise except urllib2 . URLError as e : raise ConnectionError ( "Error while fetching from %s: %s" % ( uri , e ) ) finally : if response : response . close ( ) opener . close ( ) data = None if parse_data : if not key : key = string . split ( url , "/" ) [ 0 ] data = self . parse ( body , key ) if full_return : info = response . info ( ) if response else None status = int ( string . split ( info [ "status" ] ) [ 0 ] ) if ( info and "status" in info ) else None return { "success" : ( status >= 200 and status < 300 ) , "data" : data , "info" : info , "body" : body } return data
1258	def get_savable_components ( self ) : components = self . get_components ( ) components = [ components [ name ] for name in sorted ( components ) ] return set ( filter ( lambda x : isinstance ( x , util . SavableComponent ) , components ) )
6132	def toJSON ( self ) : return { "id" : self . id , "compile" : self . compile , "position" : self . position , "version" : self . version }
4474	def __serial_transform ( self , jam , steps ) : if six . PY2 : attr = 'next' else : attr = '__next__' pending = len ( steps ) nexts = itertools . cycle ( getattr ( iter ( D . transform ( jam ) ) , attr ) for ( name , D ) in steps ) while pending : try : for next_jam in nexts : yield next_jam ( ) except StopIteration : pending -= 1 nexts = itertools . cycle ( itertools . islice ( nexts , pending ) )
6144	def DSP_callback_tic ( self ) : if self . Tcapture > 0 : self . DSP_tic . append ( time . time ( ) - self . start_time )
6888	def parallel_epd_lcdir ( lcdir , externalparams , lcfileglob = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , epdsmooth_sigclip = 3.0 , epdsmooth_windowsize = 21 , epdsmooth_func = smooth_magseries_savgol , epdsmooth_extraparams = None , nworkers = NCPUS , maxworkertasks = 1000 ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( fileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if lcfileglob is None : lcfileglob = fileglob lclist = sorted ( glob . glob ( os . path . join ( lcdir , lcfileglob ) ) ) return parallel_epd_lclist ( lclist , externalparams , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , epdsmooth_sigclip = epdsmooth_sigclip , epdsmooth_windowsize = epdsmooth_windowsize , epdsmooth_func = epdsmooth_func , epdsmooth_extraparams = epdsmooth_extraparams , nworkers = nworkers , maxworkertasks = maxworkertasks )
9242	def detect_actual_closed_dates ( self , issues , kind ) : if self . options . verbose : print ( "Fetching closed dates for {} {}..." . format ( len ( issues ) , kind ) ) all_issues = copy . deepcopy ( issues ) for issue in all_issues : if self . options . verbose > 2 : print ( "." , end = "" ) if not issues . index ( issue ) % 30 : print ( "" ) self . find_closed_date_by_commit ( issue ) if not issue . get ( 'actual_date' , False ) : if issue . get ( 'closed_at' , False ) : print ( "Skipping closed non-merged issue: #{0} {1}" . format ( issue [ "number" ] , issue [ "title" ] ) ) all_issues . remove ( issue ) if self . options . verbose > 2 : print ( "." ) return all_issues
6271	def sphere ( radius = 0.5 , sectors = 32 , rings = 16 ) -> VAO : R = 1.0 / ( rings - 1 ) S = 1.0 / ( sectors - 1 ) vertices = [ 0 ] * ( rings * sectors * 3 ) normals = [ 0 ] * ( rings * sectors * 3 ) uvs = [ 0 ] * ( rings * sectors * 2 ) v , n , t = 0 , 0 , 0 for r in range ( rings ) : for s in range ( sectors ) : y = math . sin ( - math . pi / 2 + math . pi * r * R ) x = math . cos ( 2 * math . pi * s * S ) * math . sin ( math . pi * r * R ) z = math . sin ( 2 * math . pi * s * S ) * math . sin ( math . pi * r * R ) uvs [ t ] = s * S uvs [ t + 1 ] = r * R vertices [ v ] = x * radius vertices [ v + 1 ] = y * radius vertices [ v + 2 ] = z * radius normals [ n ] = x normals [ n + 1 ] = y normals [ n + 2 ] = z t += 2 v += 3 n += 3 indices = [ 0 ] * rings * sectors * 6 i = 0 for r in range ( rings - 1 ) : for s in range ( sectors - 1 ) : indices [ i ] = r * sectors + s indices [ i + 1 ] = ( r + 1 ) * sectors + ( s + 1 ) indices [ i + 2 ] = r * sectors + ( s + 1 ) indices [ i + 3 ] = r * sectors + s indices [ i + 4 ] = ( r + 1 ) * sectors + s indices [ i + 5 ] = ( r + 1 ) * sectors + ( s + 1 ) i += 6 vbo_vertices = numpy . array ( vertices , dtype = numpy . float32 ) vbo_normals = numpy . array ( normals , dtype = numpy . float32 ) vbo_uvs = numpy . array ( uvs , dtype = numpy . float32 ) vbo_elements = numpy . array ( indices , dtype = numpy . uint32 ) vao = VAO ( "sphere" , mode = mlg . TRIANGLES ) vao . buffer ( vbo_vertices , '3f' , [ 'in_position' ] ) vao . buffer ( vbo_normals , '3f' , [ 'in_normal' ] ) vao . buffer ( vbo_uvs , '2f' , [ 'in_uv' ] ) vao . index_buffer ( vbo_elements , index_element_size = 4 ) return vao
7289	def has_digit ( string_or_list , sep = "_" ) : if isinstance ( string_or_list , ( tuple , list ) ) : list_length = len ( string_or_list ) if list_length : return six . text_type ( string_or_list [ - 1 ] ) . isdigit ( ) else : return False else : return has_digit ( string_or_list . split ( sep ) )
11260	def match ( prev , pattern , * args , ** kw ) : to = 'to' in kw and kw . pop ( 'to' ) pattern_obj = re . compile ( pattern , * args , ** kw ) if to is dict : for data in prev : match = pattern_obj . match ( data ) if match is not None : yield match . groupdict ( ) elif to is tuple : for data in prev : match = pattern_obj . match ( data ) if match is not None : yield match . groups ( ) elif to is list : for data in prev : match = pattern_obj . match ( data ) if match is not None : yield list ( match . groups ( ) ) else : for data in prev : match = pattern_obj . match ( data ) if match is not None : yield match
602	def addHistogram ( self , data , position = 111 , xlabel = None , ylabel = None , bins = None ) : ax = self . _addBase ( position , xlabel = xlabel , ylabel = ylabel ) ax . hist ( data , bins = bins , color = "green" , alpha = 0.8 ) plt . draw ( )
5584	def prepare_path ( self , tile ) : makedirs ( os . path . dirname ( self . get_path ( tile ) ) )
710	def _backupFile ( filePath ) : assert os . path . exists ( filePath ) stampNum = 0 ( prefix , suffix ) = os . path . splitext ( filePath ) while True : backupPath = "%s.%d%s" % ( prefix , stampNum , suffix ) stampNum += 1 if not os . path . exists ( backupPath ) : break shutil . copyfile ( filePath , backupPath ) return backupPath
9265	def fetch_and_filter_tags ( self ) : self . all_tags = self . fetcher . get_all_tags ( ) self . filtered_tags = self . get_filtered_tags ( self . all_tags ) self . fetch_tags_dates ( )
9826	def experiments ( ctx , metrics , declarations , independent , group , query , sort , page ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) page = page or 1 try : response = PolyaxonClient ( ) . project . list_experiments ( username = user , project_name = project_name , independent = independent , group = group , metrics = metrics , declarations = declarations , query = query , sort = sort , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get experiments for project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Experiments for project `{}/{}`.' . format ( user , project_name ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No experiments found for project `{}/{}`.' . format ( user , project_name ) ) if metrics : objects = get_experiments_with_metrics ( response ) elif declarations : objects = get_experiments_with_declarations ( response ) else : objects = [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) ) for o in response [ 'results' ] ] objects = list_dicts_to_tabulate ( objects ) if objects : Printer . print_header ( "Experiments:" ) objects . pop ( 'project_name' , None ) dict_tabulate ( objects , is_list_dict = True )
6561	def _bqm_from_2sat ( constraint ) : configurations = constraint . configurations variables = constraint . variables vartype = constraint . vartype u , v = constraint . variables if len ( configurations ) == 4 : return dimod . BinaryQuadraticModel . empty ( constraint . vartype ) components = irreducible_components ( constraint ) if len ( components ) > 1 : const0 = Constraint . from_configurations ( ( ( config [ 0 ] , ) for config in configurations ) , ( u , ) , vartype ) const1 = Constraint . from_configurations ( ( ( config [ 1 ] , ) for config in configurations ) , ( v , ) , vartype ) bqm = _bqm_from_1sat ( const0 ) bqm . update ( _bqm_from_1sat ( const1 ) ) return bqm assert len ( configurations ) > 1 , "single configurations should be irreducible" bqm = dimod . BinaryQuadraticModel . empty ( vartype ) if all ( operator . eq ( * config ) for config in configurations ) : bqm . add_interaction ( u , v , - 1 , vartype = dimod . SPIN ) elif all ( operator . ne ( * config ) for config in configurations ) : bqm . add_interaction ( u , v , + 1 , vartype = dimod . SPIN ) elif ( 1 , 1 ) not in configurations : bqm . add_interaction ( u , v , 2 , vartype = dimod . BINARY ) elif ( - 1 , + 1 ) not in configurations and ( 0 , 1 ) not in configurations : bqm . add_interaction ( u , v , - 2 , vartype = dimod . BINARY ) bqm . add_variable ( v , 2 , vartype = dimod . BINARY ) elif ( + 1 , - 1 ) not in configurations and ( 1 , 0 ) not in configurations : bqm . add_interaction ( u , v , - 2 , vartype = dimod . BINARY ) bqm . add_variable ( u , 2 , vartype = dimod . BINARY ) else : bqm . add_interaction ( u , v , 2 , vartype = dimod . BINARY ) bqm . add_variable ( u , - 2 , vartype = dimod . BINARY ) bqm . add_variable ( v , - 2 , vartype = dimod . BINARY ) return bqm
8842	def indent ( self ) : if not self . tab_always_indent : super ( PyIndenterMode , self ) . indent ( ) else : cursor = self . editor . textCursor ( ) assert isinstance ( cursor , QtGui . QTextCursor ) if cursor . hasSelection ( ) : self . indent_selection ( cursor ) else : tab_len = self . editor . tab_length cursor . beginEditBlock ( ) if self . editor . use_spaces_instead_of_tabs : cursor . insertText ( tab_len * " " ) else : cursor . insertText ( '\t' ) cursor . endEditBlock ( ) self . editor . setTextCursor ( cursor )
3687	def a_alpha_and_derivatives ( self , T , full = True , quick = True ) : r if not full : return self . a * ( 1 + self . kappa * ( 1 - ( T / self . Tc ) ** 0.5 ) ) ** 2 else : if quick : Tc , kappa = self . Tc , self . kappa x0 = T ** 0.5 x1 = Tc ** - 0.5 x2 = kappa * ( x0 * x1 - 1. ) - 1. x3 = self . a * kappa a_alpha = self . a * x2 * x2 da_alpha_dT = x1 * x2 * x3 / x0 d2a_alpha_dT2 = x3 * ( - 0.5 * T ** - 1.5 * x1 * x2 + 0.5 / ( T * Tc ) * kappa ) else : a_alpha = self . a * ( 1 + self . kappa * ( 1 - ( T / self . Tc ) ** 0.5 ) ) ** 2 da_alpha_dT = - self . a * self . kappa * sqrt ( T / self . Tc ) * ( self . kappa * ( - sqrt ( T / self . Tc ) + 1. ) + 1. ) / T d2a_alpha_dT2 = self . a * self . kappa * ( self . kappa / self . Tc - sqrt ( T / self . Tc ) * ( self . kappa * ( sqrt ( T / self . Tc ) - 1. ) - 1. ) / T ) / ( 2. * T ) return a_alpha , da_alpha_dT , d2a_alpha_dT2
11992	def set_signature_passphrases ( self , signature_passphrases ) : self . signature_passphrases = self . _update_dict ( signature_passphrases , { } , replace_data = True )
6939	def checkplot_infokey_worker ( task ) : cpf , keys = task cpd = _read_checkplot_picklefile ( cpf ) resultkeys = [ ] for k in keys : try : resultkeys . append ( _dict_get ( cpd , k ) ) except Exception as e : resultkeys . append ( np . nan ) return resultkeys
11094	def n_dir ( self ) : self . assert_is_dir_and_exists ( ) n = 0 for _ in self . select_dir ( recursive = True ) : n += 1 return n
9989	def import_funcs ( self , module ) : newcells = self . _impl . new_cells_from_module ( module ) return get_interfaces ( newcells )
9814	def start ( ctx , file , u ) : specification = None job_config = None if file : specification = check_polyaxonfile ( file , log = False ) . specification if u : ctx . invoke ( upload , sync = False ) if specification : check_polyaxonfile_kind ( specification = specification , kind = specification . _NOTEBOOK ) job_config = specification . parsed_data user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) try : response = PolyaxonClient ( ) . project . start_notebook ( user , project_name , job_config ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not start notebook project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 200 : Printer . print_header ( "A notebook for this project is already running on:" ) click . echo ( get_notebook_url ( user , project_name ) ) sys . exit ( 0 ) if response . status_code != 201 : Printer . print_error ( 'Something went wrong, Notebook was not created.' ) sys . exit ( 1 ) Printer . print_success ( 'Notebook is being deployed for project `{}`' . format ( project_name ) ) clint . textui . puts ( "It may take some time before you can access the notebook.\n" ) clint . textui . puts ( "Your notebook will be available on:\n" ) with clint . textui . indent ( 4 ) : clint . textui . puts ( get_notebook_url ( user , project_name ) )
899	def prettyPrintSequence ( self , sequence , verbosity = 1 ) : text = "" for i in xrange ( len ( sequence ) ) : pattern = sequence [ i ] if pattern == None : text += "<reset>" if i < len ( sequence ) - 1 : text += "\n" else : text += self . patternMachine . prettyPrintPattern ( pattern , verbosity = verbosity ) return text
2408	def extract_features_and_generate_model ( essays , algorithm = util_functions . AlgorithmTypes . regression ) : f = feature_extractor . FeatureExtractor ( ) f . initialize_dictionaries ( essays ) train_feats = f . gen_feats ( essays ) set_score = numpy . asarray ( essays . _score , dtype = numpy . int ) if len ( util_functions . f7 ( list ( set_score ) ) ) > 5 : algorithm = util_functions . AlgorithmTypes . regression else : algorithm = util_functions . AlgorithmTypes . classification clf , clf2 = get_algorithms ( algorithm ) cv_error_results = get_cv_error ( clf2 , train_feats , essays . _score ) try : clf . fit ( train_feats , set_score ) except ValueError : log . exception ( "Not enough classes (0,1,etc) in sample." ) set_score [ 0 ] = 1 set_score [ 1 ] = 0 clf . fit ( train_feats , set_score ) return f , clf , cv_error_results
1848	def LJMP ( cpu , cs_selector , target ) : logger . info ( "LJMP: Jumping to: %r:%r" , cs_selector . read ( ) , target . read ( ) ) cpu . CS = cs_selector . read ( ) cpu . PC = target . read ( )
4780	def is_less_than ( self , other ) : self . _validate_compareable ( other ) if self . val >= other : if type ( self . val ) is datetime . datetime : self . _err ( 'Expected <%s> to be less than <%s>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) ) ) else : self . _err ( 'Expected <%s> to be less than <%s>, but was not.' % ( self . val , other ) ) return self
4530	def _addLoggingLevel ( levelName , levelNum , methodName = None ) : if not methodName : methodName = levelName . lower ( ) if hasattr ( logging , levelName ) : raise AttributeError ( '{} already defined in logging module' . format ( levelName ) ) if hasattr ( logging , methodName ) : raise AttributeError ( '{} already defined in logging module' . format ( methodName ) ) if hasattr ( logging . getLoggerClass ( ) , methodName ) : raise AttributeError ( '{} already defined in logger class' . format ( methodName ) ) def logForLevel ( self , message , * args , ** kwargs ) : if self . isEnabledFor ( levelNum ) : self . _log ( levelNum , message , args , ** kwargs ) def logToRoot ( message , * args , ** kwargs ) : logging . log ( levelNum , message , * args , ** kwargs ) logging . addLevelName ( levelNum , levelName ) setattr ( logging , levelName , levelNum ) setattr ( logging . getLoggerClass ( ) , methodName , logForLevel ) setattr ( logging , methodName , logToRoot )
7066	def delete_spot_fleet_cluster ( spot_fleet_reqid , client = None , ) : if not client : client = boto3 . client ( 'ec2' ) resp = client . cancel_spot_fleet_requests ( SpotFleetRequestIds = [ spot_fleet_reqid ] , TerminateInstances = True ) return resp
694	def loadExperimentDescriptionScriptFromDir ( experimentDir ) : descriptionScriptPath = os . path . join ( experimentDir , "description.py" ) module = _loadDescriptionFile ( descriptionScriptPath ) return module
699	def getParticleInfos ( self , swarmId = None , genIdx = None , completed = None , matured = None , lastDescendent = False ) : if swarmId is not None : entryIdxs = self . _swarmIdToIndexes . get ( swarmId , [ ] ) else : entryIdxs = range ( len ( self . _allResults ) ) if len ( entryIdxs ) == 0 : return ( [ ] , [ ] , [ ] , [ ] , [ ] ) particleStates = [ ] modelIds = [ ] errScores = [ ] completedFlags = [ ] maturedFlags = [ ] for idx in entryIdxs : entry = self . _allResults [ idx ] if swarmId is not None : assert ( not entry [ 'hidden' ] ) modelParams = entry [ 'modelParams' ] isCompleted = entry [ 'completed' ] isMatured = entry [ 'matured' ] particleState = modelParams [ 'particleState' ] particleGenIdx = particleState [ 'genIdx' ] particleId = particleState [ 'id' ] if genIdx is not None and particleGenIdx != genIdx : continue if completed is not None and ( completed != isCompleted ) : continue if matured is not None and ( matured != isMatured ) : continue if lastDescendent and ( self . _particleLatestGenIdx [ particleId ] != particleGenIdx ) : continue particleStates . append ( particleState ) modelIds . append ( entry [ 'modelID' ] ) errScores . append ( entry [ 'errScore' ] ) completedFlags . append ( isCompleted ) maturedFlags . append ( isMatured ) return ( particleStates , modelIds , errScores , completedFlags , maturedFlags )
3824	async def get_entity_by_id ( self , get_entity_by_id_request ) : response = hangouts_pb2 . GetEntityByIdResponse ( ) await self . _pb_request ( 'contacts/getentitybyid' , get_entity_by_id_request , response ) return response
9817	def install ( self ) : if not self . is_valid : raise PolyaxonDeploymentConfigError ( 'Deployment type `{}` not supported' . format ( self . deployment_type ) ) if self . is_kubernetes : self . install_on_kubernetes ( ) elif self . is_docker_compose : self . install_on_docker_compose ( ) elif self . is_docker : self . install_on_docker ( ) elif self . is_heroku : self . install_on_heroku ( )
10606	def prepare_to_run ( self ) : self . clock . reset ( ) for e in self . entities : e . prepare_to_run ( self . clock , self . period_count )
3908	def put ( self , coro ) : assert asyncio . iscoroutine ( coro ) self . _queue . put_nowait ( coro )
1331	def batch_predictions ( self , images , greedy = False , strict = True , return_details = False ) : if strict : in_bounds = self . in_bounds ( images ) assert in_bounds self . _total_prediction_calls += len ( images ) predictions = self . __model . batch_predictions ( images ) assert predictions . ndim == 2 assert predictions . shape [ 0 ] == images . shape [ 0 ] if return_details : assert greedy adversarials = [ ] for i in range ( len ( predictions ) ) : if strict : in_bounds_i = True else : in_bounds_i = self . in_bounds ( images [ i ] ) is_adversarial , is_best , distance = self . __is_adversarial ( images [ i ] , predictions [ i ] , in_bounds_i ) if is_adversarial and greedy : if return_details : return predictions , is_adversarial , i , is_best , distance else : return predictions , is_adversarial , i adversarials . append ( is_adversarial ) if greedy : if return_details : return predictions , False , None , False , None else : return predictions , False , None is_adversarial = np . array ( adversarials ) assert is_adversarial . ndim == 1 assert is_adversarial . shape [ 0 ] == images . shape [ 0 ] return predictions , is_adversarial
11999	def _decode ( self , data , algorithm , key = None ) : if algorithm [ 'type' ] == 'hmac' : verify_signature = data [ - algorithm [ 'hash_size' ] : ] data = data [ : - algorithm [ 'hash_size' ] ] signature = self . _hmac_generate ( data , algorithm , key ) if not const_equal ( verify_signature , signature ) : raise Exception ( 'Invalid signature' ) return data elif algorithm [ 'type' ] == 'aes' : return self . _aes_decrypt ( data , algorithm , key ) elif algorithm [ 'type' ] == 'no-serialization' : return data elif algorithm [ 'type' ] == 'json' : return json . loads ( data ) elif algorithm [ 'type' ] == 'no-compression' : return data elif algorithm [ 'type' ] == 'gzip' : return self . _zlib_decompress ( data , algorithm ) else : raise Exception ( 'Algorithm not supported: %s' % algorithm [ 'type' ] )
13194	def remove_comments ( tex_source ) : return re . sub ( r'(?<!\\)%.*$' , r'' , tex_source , flags = re . M )
6266	def stop ( self ) -> float : self . stop_time = time . time ( ) return self . stop_time - self . start_time - self . offset
2976	def cmd_status ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) containers = b . status ( ) print_containers ( containers , opts . json )
13459	def create_ical ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) start = event . start_date start = datetime . datetime ( start . year , start . month , start . day ) if event . end_date : end = event . end_date end = datetime . datetime ( end . year , end . month , end . day ) else : end = start cal = card_me . iCalendar ( ) cal . add ( 'method' ) . value = 'PUBLISH' vevent = cal . add ( 'vevent' ) vevent . add ( 'dtstart' ) . value = start vevent . add ( 'dtend' ) . value = end vevent . add ( 'dtstamp' ) . value = datetime . datetime . now ( ) vevent . add ( 'summary' ) . value = event . name response = HttpResponse ( cal . serialize ( ) , content_type = 'text/calendar' ) response [ 'Filename' ] = 'filename.ics' response [ 'Content-Disposition' ] = 'attachment; filename=filename.ics' return response
7232	def get ( self , ID , index = 'vector-web-s' ) : url = self . get_url % index r = self . gbdx_connection . get ( url + ID ) r . raise_for_status ( ) return r . json ( )
11554	def disable_digital_reporting ( self , pin ) : port = pin // 8 command = [ self . _command_handler . REPORT_DIGITAL + port , self . REPORTING_DISABLE ] self . _command_handler . send_command ( command )
4683	def getAccounts ( self ) : pubkeys = self . getPublicKeys ( ) accounts = [ ] for pubkey in pubkeys : if pubkey [ : len ( self . prefix ) ] == self . prefix : accounts . extend ( self . getAccountsFromPublicKey ( pubkey ) ) return accounts
9275	def apply_exclude_tags_regex ( self , all_tags ) : filtered = [ ] for tag in all_tags : if not re . match ( self . options . exclude_tags_regex , tag [ "name" ] ) : filtered . append ( tag ) if len ( all_tags ) == len ( filtered ) : self . warn_if_nonmatching_regex ( ) return filtered
542	def __getOptimizedMetricLabel ( self ) : matchingKeys = matchPatterns ( [ self . _optimizeKeyPattern ] , self . _getMetricLabels ( ) ) if len ( matchingKeys ) == 0 : raise Exception ( "None of the generated metrics match the specified " "optimization pattern: %s. Available metrics are %s" % ( self . _optimizeKeyPattern , self . _getMetricLabels ( ) ) ) elif len ( matchingKeys ) > 1 : raise Exception ( "The specified optimization pattern '%s' matches more " "than one metric: %s" % ( self . _optimizeKeyPattern , matchingKeys ) ) return matchingKeys [ 0 ]
1291	def setup_components_and_tf_funcs ( self , custom_getter = None ) : custom_getter = super ( QDemoModel , self ) . setup_components_and_tf_funcs ( custom_getter ) self . demo_memory = Replay ( states = self . states_spec , internals = self . internals_spec , actions = self . actions_spec , include_next_states = True , capacity = self . demo_memory_capacity , scope = 'demo-replay' , summary_labels = self . summary_labels ) self . fn_import_demo_experience = tf . make_template ( name_ = 'import-demo-experience' , func_ = self . tf_import_demo_experience , custom_getter_ = custom_getter ) self . fn_demo_loss = tf . make_template ( name_ = 'demo-loss' , func_ = self . tf_demo_loss , custom_getter_ = custom_getter ) self . fn_combined_loss = tf . make_template ( name_ = 'combined-loss' , func_ = self . tf_combined_loss , custom_getter_ = custom_getter ) self . fn_demo_optimization = tf . make_template ( name_ = 'demo-optimization' , func_ = self . tf_demo_optimization , custom_getter_ = custom_getter ) return custom_getter
11727	def ppdict ( dict_to_print , br = '\n' , html = False , key_align = 'l' , sort_keys = True , key_preffix = '' , key_suffix = '' , value_prefix = '' , value_suffix = '' , left_margin = 3 , indent = 2 ) : if dict_to_print : if sort_keys : dic = dict_to_print . copy ( ) keys = list ( dic . keys ( ) ) keys . sort ( ) dict_to_print = OrderedDict ( ) for k in keys : dict_to_print [ k ] = dic [ k ] tmp = [ '{' ] ks = [ type ( x ) == str and "'%s'" % x or x for x in dict_to_print . keys ( ) ] vs = [ type ( x ) == str and "'%s'" % x or x for x in dict_to_print . values ( ) ] max_key_len = max ( [ len ( str ( x ) ) for x in ks ] ) for i in range ( len ( ks ) ) : k = { 1 : str ( ks [ i ] ) . ljust ( max_key_len ) , key_align == 'r' : str ( ks [ i ] ) . rjust ( max_key_len ) } [ 1 ] v = vs [ i ] tmp . append ( ' ' * indent + '{}{}{}:{}{}{},' . format ( key_preffix , k , key_suffix , value_prefix , v , value_suffix ) ) tmp [ - 1 ] = tmp [ - 1 ] [ : - 1 ] tmp . append ( '}' ) if left_margin : tmp = [ ' ' * left_margin + x for x in tmp ] if html : return '<code>{}</code>' . format ( br . join ( tmp ) . replace ( ' ' , '&nbsp;' ) ) else : return br . join ( tmp ) else : return '{}'
6497	def index ( self , doc_type , sources , ** kwargs ) : try : actions = [ ] for source in sources : self . _check_mappings ( doc_type , source ) id_ = source [ 'id' ] if 'id' in source else None log . debug ( "indexing %s object with id %s" , doc_type , id_ ) action = { "_index" : self . index_name , "_type" : doc_type , "_id" : id_ , "_source" : source } actions . append ( action ) _ , indexing_errors = bulk ( self . _es , actions , ** kwargs ) if indexing_errors : ElasticSearchEngine . log_indexing_error ( indexing_errors ) except Exception as ex : log . exception ( "error while indexing - %s" , str ( ex ) ) raise
13623	def Text ( value , encoding = None ) : if encoding is None : encoding = 'utf-8' if isinstance ( value , bytes ) : return value . decode ( encoding ) elif isinstance ( value , unicode ) : return value return None
13783	def _MakeFieldDescriptor ( self , field_proto , message_name , index , is_extension = False ) : if message_name : full_name = '.' . join ( ( message_name , field_proto . name ) ) else : full_name = field_proto . name return descriptor . FieldDescriptor ( name = field_proto . name , full_name = full_name , index = index , number = field_proto . number , type = field_proto . type , cpp_type = None , message_type = None , enum_type = None , containing_type = None , label = field_proto . label , has_default_value = False , default_value = None , is_extension = is_extension , extension_scope = None , options = field_proto . options )
4861	def validate_username ( self , value ) : try : user = User . objects . get ( username = value ) except User . DoesNotExist : raise serializers . ValidationError ( "User does not exist" ) try : enterprise_customer_user = models . EnterpriseCustomerUser . objects . get ( user_id = user . pk ) except models . EnterpriseCustomerUser . DoesNotExist : raise serializers . ValidationError ( "User has no EnterpriseCustomerUser" ) self . enterprise_customer_user = enterprise_customer_user return value
7229	def paint ( self ) : snippet = { 'heatmap-radius' : VectorStyle . get_style_value ( self . radius ) , 'heatmap-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'heatmap-color' : VectorStyle . get_style_value ( self . color ) , 'heatmap-intensity' : VectorStyle . get_style_value ( self . intensity ) , 'heatmap-weight' : VectorStyle . get_style_value ( self . weight ) } return snippet
6955	def _get_value ( quantitystr , fitparams , fixedparams ) : fitparamskeys , fixedparamskeys = fitparams . keys ( ) , fixedparams . keys ( ) if quantitystr in fitparamskeys : quantity = fitparams [ quantitystr ] elif quantitystr in fixedparamskeys : quantity = fixedparams [ quantitystr ] return quantity
12946	def copy ( self , copyPrimaryKey = False , copyValues = False ) : cpy = self . __class__ ( ** self . asDict ( copyPrimaryKey , forStorage = False ) ) if copyValues is True : for fieldName in cpy . FIELDS : setattr ( cpy , fieldName , copy . deepcopy ( getattr ( cpy , fieldName ) ) ) return cpy
12832	def validate_xml_name ( name ) : if len ( name ) == 0 : raise RuntimeError ( 'empty XML name' ) if __INVALID_NAME_CHARS & set ( name ) : raise RuntimeError ( 'XML name contains invalid character' ) if name [ 0 ] in __INVALID_NAME_START_CHARS : raise RuntimeError ( 'XML name starts with invalid character' )
7137	def format ( obj , options ) : formatters = { float_types : lambda x : '{:.{}g}' . format ( x , options . digits ) , } for _types , fmtr in formatters . items ( ) : if isinstance ( obj , _types ) : return fmtr ( obj ) try : if six . PY2 and isinstance ( obj , six . string_types ) : return str ( obj . encode ( 'utf-8' ) ) return str ( obj ) except : return 'OBJECT'
11361	def fix_title_capitalization ( title ) : if re . search ( "[A-Z]" , title ) and re . search ( "[a-z]" , title ) : return title word_list = re . split ( ' +' , title ) final = [ word_list [ 0 ] . capitalize ( ) ] for word in word_list [ 1 : ] : if word . upper ( ) in COMMON_ACRONYMS : final . append ( word . upper ( ) ) elif len ( word ) > 3 : final . append ( word . capitalize ( ) ) else : final . append ( word . lower ( ) ) return " " . join ( final )
1854	def SHLD ( cpu , dest , src , count ) : OperandSize = dest . size tempCount = Operators . ZEXTEND ( count . read ( ) , OperandSize ) & ( OperandSize - 1 ) arg0 = dest . read ( ) arg1 = src . read ( ) MASK = ( ( 1 << OperandSize ) - 1 ) t0 = ( arg0 << tempCount ) t1 = arg1 >> ( OperandSize - tempCount ) res = Operators . ITEBV ( OperandSize , tempCount == 0 , arg0 , t0 | t1 ) res = res & MASK dest . write ( res ) if isinstance ( tempCount , int ) and tempCount == 0 : pass else : SIGN_MASK = 1 << ( OperandSize - 1 ) lastbit = 0 != ( ( arg0 << ( tempCount - 1 ) ) & SIGN_MASK ) cpu . _set_shiftd_flags ( OperandSize , arg0 , res , lastbit , tempCount )
8709	def write_lines ( self , data ) : lines = data . replace ( '\r' , '' ) . split ( '\n' ) for line in lines : self . __exchange ( line )
12788	def load ( self , reload = False , require_load = False ) : if reload : self . config = None if self . config : self . _log . debug ( 'Returning cached config instance. Use ' '``reload=True`` to avoid caching!' ) return path = self . _effective_path ( ) config_filename = self . _effective_filename ( ) self . _active_path = [ join ( _ , config_filename ) for _ in path ] for dirname in path : conf_name = join ( dirname , config_filename ) readable = self . check_file ( conf_name ) if readable : action = 'Updating' if self . _loaded_files else 'Loading initial' self . _log . info ( '%s config from %s' , action , conf_name ) self . read ( conf_name ) if conf_name == expanduser ( "~/.%s/%s/%s" % ( self . group_name , self . app_name , self . filename ) ) : self . _log . warning ( "DEPRECATION WARNING: The file " "'%s/.%s/%s/app.ini' was loaded. The XDG " "Basedir standard requires this file to be in " "'%s/.config/%s/%s/app.ini'! This location " "will no longer be parsed in a future version of " "config_resolver! You can already (and should) move " "the file!" , expanduser ( "~" ) , self . group_name , self . app_name , expanduser ( "~" ) , self . group_name , self . app_name ) self . _loaded_files . append ( conf_name ) if not self . _loaded_files and not require_load : self . _log . warning ( "No config file named %s found! Search path was %r" , config_filename , path ) elif not self . _loaded_files and require_load : raise IOError ( "No config file named %s found! Search path " "was %r" % ( config_filename , path ) )
7217	def get_definition ( self , task_name ) : r = self . gbdx_connection . get ( self . _base_url + '/' + task_name ) raise_for_status ( r ) return r . json ( )
917	def info ( self , msg , * args , ** kwargs ) : self . _baseLogger . info ( self , self . getExtendedMsg ( msg ) , * args , ** kwargs )
4590	def serpentine_y ( x , y , matrix ) : if x % 2 : return x , matrix . rows - 1 - y return x , y
8115	def distance ( x0 , y0 , x1 , y1 ) : return sqrt ( pow ( x1 - x0 , 2 ) + pow ( y1 - y0 , 2 ) )
11531	def _hashkey ( self , method , url , ** kwa ) : to_hash = '' . join ( [ str ( method ) , str ( url ) , str ( kwa . get ( 'data' , '' ) ) , str ( kwa . get ( 'params' , '' ) ) ] ) return hashlib . md5 ( to_hash . encode ( ) ) . hexdigest ( )
9553	def _apply_header_checks ( self , i , r , summarize = False , context = None ) : for code , message in self . _header_checks : if tuple ( r ) != self . _field_names : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = tuple ( r ) p [ 'missing' ] = set ( self . _field_names ) - set ( r ) p [ 'unexpected' ] = set ( r ) - set ( self . _field_names ) if context is not None : p [ 'context' ] = context yield p
9901	def _updateType ( self ) : data = self . _data ( ) if isinstance ( data , dict ) and isinstance ( self , ListFile ) : self . __class__ = DictFile elif isinstance ( data , list ) and isinstance ( self , DictFile ) : self . __class__ = ListFile
9321	def _validate_api_root ( self ) : if not self . _title : msg = "No 'title' in API Root for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if not self . _versions : msg = "No 'versions' in API Root for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _max_content_length is None : msg = "No 'max_content_length' in API Root for request '{}'" raise ValidationError ( msg . format ( self . url ) )
5714	def _validate_zip ( the_zip ) : datapackage_jsons = [ f for f in the_zip . namelist ( ) if f . endswith ( 'datapackage.json' ) ] if len ( datapackage_jsons ) != 1 : msg = 'DataPackage must have only one "datapackage.json" (had {n})' raise exceptions . DataPackageException ( msg . format ( n = len ( datapackage_jsons ) ) )
12989	def setup_notebook ( debug = False ) : output_notebook ( INLINE , hide_banner = True ) if debug : _setup_logging ( logging . DEBUG ) logging . debug ( 'Running notebook in debug mode.' ) else : _setup_logging ( logging . WARNING ) if 'JUPYTERHUB_SERVICE_PREFIX' not in os . environ : global jupyter_proxy_url jupyter_proxy_url = 'localhost:8888' logging . info ( 'Setting jupyter proxy to local mode.' )
2798	def rename ( self , new_name ) : return self . get_data ( "images/%s" % self . id , type = PUT , params = { "name" : new_name } )
7223	def save ( self , recipe ) : if 'id' in recipe and recipe [ 'id' ] is not None : self . logger . debug ( "Updating existing recipe: " + json . dumps ( recipe ) ) url = '%(base_url)s/recipe/json/%(recipe_id)s' % { 'base_url' : self . base_url , 'recipe_id' : recipe [ 'id' ] } r = self . gbdx_connection . put ( url , json = recipe ) try : r . raise_for_status ( ) except : print ( r . text ) raise return recipe [ 'id' ] else : self . logger . debug ( "Creating new recipe: " + json . dumps ( recipe ) ) url = '%(base_url)s/recipe/json' % { 'base_url' : self . base_url } r = self . gbdx_connection . post ( url , json = recipe ) try : r . raise_for_status ( ) except : print ( r . text ) raise recipe_json = r . json ( ) return recipe_json [ 'id' ]
3345	def parse_if_header_dict ( environ ) : if "wsgidav.conditions.if" in environ : return if "HTTP_IF" not in environ : environ [ "wsgidav.conditions.if" ] = None environ [ "wsgidav.ifLockTokenList" ] = [ ] return iftext = environ [ "HTTP_IF" ] . strip ( ) if not iftext . startswith ( "<" ) : iftext = "<*>" + iftext ifDict = dict ( [ ] ) ifLockList = [ ] resource1 = "*" for ( tmpURLVar , URLVar , _tmpContentVar , contentVar ) in reIfSeparator . findall ( iftext ) : if tmpURLVar != "" : resource1 = URLVar else : listTagContents = [ ] testflag = True for listitem in reIfTagListContents . findall ( contentVar ) : if listitem . upper ( ) != "NOT" : if listitem . startswith ( "[" ) : listTagContents . append ( ( testflag , "entity" , listitem . strip ( '"[]' ) ) ) else : listTagContents . append ( ( testflag , "locktoken" , listitem . strip ( "<>" ) ) ) ifLockList . append ( listitem . strip ( "<>" ) ) testflag = listitem . upper ( ) != "NOT" if resource1 in ifDict : listTag = ifDict [ resource1 ] else : listTag = [ ] ifDict [ resource1 ] = listTag listTag . append ( listTagContents ) environ [ "wsgidav.conditions.if" ] = ifDict environ [ "wsgidav.ifLockTokenList" ] = ifLockList _logger . debug ( "parse_if_header_dict\n{}" . format ( pformat ( ifDict ) ) ) return
522	def _updateBoostFactorsGlobal ( self ) : if ( self . _localAreaDensity > 0 ) : targetDensity = self . _localAreaDensity else : inhibitionArea = ( ( 2 * self . _inhibitionRadius + 1 ) ** self . _columnDimensions . size ) inhibitionArea = min ( self . _numColumns , inhibitionArea ) targetDensity = float ( self . _numActiveColumnsPerInhArea ) / inhibitionArea targetDensity = min ( targetDensity , 0.5 ) self . _boostFactors = numpy . exp ( ( targetDensity - self . _activeDutyCycles ) * self . _boostStrength )
38	def dict_gather ( comm , d , op = 'mean' , assert_all_have_data = True ) : if comm is None : return d alldicts = comm . allgather ( d ) size = comm . size k2li = defaultdict ( list ) for d in alldicts : for ( k , v ) in d . items ( ) : k2li [ k ] . append ( v ) result = { } for ( k , li ) in k2li . items ( ) : if assert_all_have_data : assert len ( li ) == size , "only %i out of %i MPI workers have sent '%s'" % ( len ( li ) , size , k ) if op == 'mean' : result [ k ] = np . mean ( li , axis = 0 ) elif op == 'sum' : result [ k ] = np . sum ( li , axis = 0 ) else : assert 0 , op return result
11470	def get_filesize ( self , filename ) : result = [ ] def dir_callback ( val ) : result . append ( val . split ( ) [ 4 ] ) self . _ftp . dir ( filename , dir_callback ) return result [ 0 ]
2735	def load ( self ) : data = self . get_data ( 'floating_ips/%s' % self . ip , type = GET ) floating_ip = data [ 'floating_ip' ] for attr in floating_ip . keys ( ) : setattr ( self , attr , floating_ip [ attr ] ) return self
5705	def timeit ( method ) : def timed ( * args , ** kw ) : time_start = time . time ( ) result = method ( * args , ** kw ) time_end = time . time ( ) print ( 'timeit: %r %2.2f sec (%r, %r) ' % ( method . __name__ , time_end - time_start , str ( args ) [ : 20 ] , kw ) ) return result return timed
11414	def record_get_subfields ( rec , tag , field_position_global = None , field_position_local = None ) : field = record_get_field ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) return field [ 0 ]
13680	def get_translated_data ( self ) : j = { } for k in self . data : d = { } for l in self . data [ k ] : d [ self . translation_keys [ l ] ] = self . data [ k ] [ l ] j [ k ] = d return j
6166	def my_psd ( x , NFFT = 2 ** 10 , Fs = 1 ) : Px , f = pylab . mlab . psd ( x , NFFT , Fs ) return Px . flatten ( ) , f
13209	def _prep_snippet_for_pandoc ( self , latex_text ) : replace_cite = CitationLinker ( self . bib_db ) latex_text = replace_cite ( latex_text ) return latex_text
11822	def is_compatible ( cls , value ) : if not hasattr ( cls , 'value_type' ) : raise NotImplementedError ( 'You must define a `value_type` attribute or override the ' '`is_compatible()` method on `SettingValueModel` subclasses.' ) return isinstance ( value , cls . value_type )
780	def jobInsert ( self , client , cmdLine , clientInfo = '' , clientKey = '' , params = '' , alreadyRunning = False , minimumWorkers = 0 , maximumWorkers = 0 , jobType = '' , priority = DEFAULT_JOB_PRIORITY ) : jobHash = self . _normalizeHash ( uuid . uuid1 ( ) . bytes ) @ g_retrySQL def insertWithRetries ( ) : with ConnectionFactory . get ( ) as conn : return self . _insertOrGetUniqueJobNoRetries ( conn , client = client , cmdLine = cmdLine , jobHash = jobHash , clientInfo = clientInfo , clientKey = clientKey , params = params , minimumWorkers = minimumWorkers , maximumWorkers = maximumWorkers , jobType = jobType , priority = priority , alreadyRunning = alreadyRunning ) try : jobID = insertWithRetries ( ) except : self . _logger . exception ( 'jobInsert FAILED: jobType=%r; client=%r; clientInfo=%r; clientKey=%r;' 'jobHash=%r; cmdLine=%r' , jobType , client , _abbreviate ( clientInfo , 48 ) , clientKey , jobHash , cmdLine ) raise else : self . _logger . info ( 'jobInsert: returning jobID=%s. jobType=%r; client=%r; clientInfo=%r; ' 'clientKey=%r; jobHash=%r; cmdLine=%r' , jobID , jobType , client , _abbreviate ( clientInfo , 48 ) , clientKey , jobHash , cmdLine ) return jobID
7167	def load_intent ( self , name , file_name , reload_cache = False ) : self . intents . load ( name , file_name , reload_cache ) with open ( file_name ) as f : self . padaos . add_intent ( name , f . read ( ) . split ( '\n' ) ) self . must_train = True
4701	def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.lnvm.env: invalid SSH environment" ) return 1 lnvm = cij . env_to_dict ( PREFIX , REQUIRED ) nvme = cij . env_to_dict ( "NVME" , [ "DEV_NAME" ] ) if "BGN" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_BGN" ) return 1 if "END" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_END" ) return 1 if "DEV_TYPE" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_DEV_TYPE" ) return 1 lnvm [ "DEV_NAME" ] = "%sb%03de%03d" % ( nvme [ "DEV_NAME" ] , int ( lnvm [ "BGN" ] ) , int ( lnvm [ "END" ] ) ) lnvm [ "DEV_PATH" ] = "/dev/%s" % lnvm [ "DEV_NAME" ] cij . env_export ( PREFIX , EXPORTED , lnvm ) return 0
10848	def noformat ( self ) : try : formats = { } for h in self . get_handlers ( ) : formats [ h ] = h . formatter self . set_formatter ( formatter = 'quiet' ) yield except Exception as e : raise finally : for k , v in iteritems ( formats ) : k . formatter = v
12339	def compress ( images , delete_tif = False , folder = None ) : if type ( images ) == str : return [ compress_blocking ( images , delete_tif , folder ) ] filenames = copy ( images ) return Parallel ( n_jobs = _pools ) ( delayed ( compress_blocking ) ( image = image , delete_tif = delete_tif , folder = folder ) for image in filenames )
3854	def add_color_to_scheme ( scheme , name , foreground , background , palette_colors ) : if foreground is None and background is None : return scheme new_scheme = [ ] for item in scheme : if item [ 0 ] == name : if foreground is None : foreground = item [ 1 ] if background is None : background = item [ 2 ] if palette_colors > 16 : new_scheme . append ( ( name , '' , '' , '' , foreground , background ) ) else : new_scheme . append ( ( name , foreground , background ) ) else : new_scheme . append ( item ) return new_scheme
12073	def _trace_summary ( self ) : for ( i , ( val , args ) ) in enumerate ( self . trace ) : if args is StopIteration : info = "Terminated" else : pprint = ',' . join ( '{' + ',' . join ( '%s=%r' % ( k , v ) for ( k , v ) in arg . items ( ) ) + '}' for arg in args ) info = ( "exploring arguments [%s]" % pprint ) if i == 0 : print ( "Step %d: Initially %s." % ( i , info ) ) else : print ( "Step %d: %s after receiving input(s) %s." % ( i , info . capitalize ( ) , val ) )
2538	def set_pkg_verif_code ( self , doc , code ) : self . assert_package_exists ( ) if not self . package_verif_set : self . package_verif_set = True doc . package . verif_code = code else : raise CardinalityError ( 'Package::VerificationCode' )
12001	def _unsign_data ( self , data , options ) : if options [ 'signature_algorithm_id' ] not in self . signature_algorithms : raise Exception ( 'Unknown signature algorithm id: %d' % options [ 'signature_algorithm_id' ] ) signature_algorithm = self . signature_algorithms [ options [ 'signature_algorithm_id' ] ] algorithm = self . _get_algorithm_info ( signature_algorithm ) key_salt = '' if algorithm [ 'salt_size' ] : key_salt = data [ - algorithm [ 'salt_size' ] : ] data = data [ : - algorithm [ 'salt_size' ] ] key = self . _generate_key ( options [ 'signature_passphrase_id' ] , self . signature_passphrases , key_salt , algorithm ) data = self . _decode ( data , algorithm , key ) return data
13032	def serve_forever ( self , poll_interval = 0.5 ) : logger . info ( 'Starting server on {}:{}...' . format ( self . server_name , self . server_port ) ) while True : try : self . poll_once ( poll_interval ) except ( KeyboardInterrupt , SystemExit ) : break self . handle_close ( ) logger . info ( 'Server stopped.' )
7183	def parse_type_comment ( type_comment ) : try : result = ast3 . parse ( type_comment , '<type_comment>' , 'eval' ) except SyntaxError : raise ValueError ( f"invalid type comment: {type_comment!r}" ) from None assert isinstance ( result , ast3 . Expression ) return result . body
3472	def subtract_metabolites ( self , metabolites , combine = True , reversibly = True ) : self . add_metabolites ( { k : - v for k , v in iteritems ( metabolites ) } , combine = combine , reversibly = reversibly )
13326	def create ( name_or_path , config ) : if not name_or_path : ctx = click . get_current_context ( ) click . echo ( ctx . get_help ( ) ) examples = ( '\nExamples:\n' ' cpenv create my_env\n' ' cpenv create ./relative/path/to/my_env\n' ' cpenv create my_env --config ./relative/path/to/config\n' ' cpenv create my_env --config git@github.com:user/config.git\n' ) click . echo ( examples ) return click . echo ( blue ( 'Creating a new virtual environment ' + name_or_path ) ) try : env = cpenv . create ( name_or_path , config ) except Exception as e : click . echo ( bold_red ( 'FAILED TO CREATE ENVIRONMENT!' ) ) click . echo ( e ) else : click . echo ( bold_green ( 'Successfully created environment!' ) ) click . echo ( blue ( 'Launching subshell' ) ) cpenv . activate ( env ) shell . launch ( env . name )
562	def addEncoder ( self , name , encoder ) : self . encoders . append ( ( name , encoder , self . width ) ) for d in encoder . getDescription ( ) : self . description . append ( ( d [ 0 ] , d [ 1 ] + self . width ) ) self . width += encoder . getWidth ( )
903	def _calcSkipRecords ( numIngested , windowSize , learningPeriod ) : numShiftedOut = max ( 0 , numIngested - windowSize ) return min ( numIngested , max ( 0 , learningPeriod - numShiftedOut ) )
3154	def get ( self , list_id , segment_id ) : return self . _mc_client . _get ( url = self . _build_path ( list_id , 'segments' , segment_id ) )
934	def writeToCheckpoint ( self , checkpointDir ) : proto = self . getSchema ( ) . new_message ( ) self . write ( proto ) checkpointPath = self . _getModelCheckpointFilePath ( checkpointDir ) if os . path . exists ( checkpointDir ) : if not os . path . isdir ( checkpointDir ) : raise Exception ( ( "Existing filesystem entry <%s> is not a model" " checkpoint -- refusing to delete (not a directory)" ) % checkpointDir ) if not os . path . isfile ( checkpointPath ) : raise Exception ( ( "Existing filesystem entry <%s> is not a model" " checkpoint -- refusing to delete" " (%s missing or not a file)" ) % ( checkpointDir , checkpointPath ) ) shutil . rmtree ( checkpointDir ) self . __makeDirectoryFromAbsolutePath ( checkpointDir ) with open ( checkpointPath , 'wb' ) as f : proto . write ( f )
5469	def get_last_update ( op ) : last_update = get_end_time ( op ) if not last_update : last_event = get_last_event ( op ) if last_event : last_update = last_event [ 'timestamp' ] if not last_update : last_update = get_create_time ( op ) return last_update
7881	def _split_qname ( self , name , is_element ) : if name . startswith ( u"{" ) : namespace , name = name [ 1 : ] . split ( u"}" , 1 ) if namespace in STANZA_NAMESPACES : namespace = self . stanza_namespace elif is_element : raise ValueError ( u"Element with no namespace: {0!r}" . format ( name ) ) else : namespace = None return namespace , name
58	def extend ( self , all_sides = 0 , top = 0 , right = 0 , bottom = 0 , left = 0 ) : return BoundingBox ( x1 = self . x1 - all_sides - left , x2 = self . x2 + all_sides + right , y1 = self . y1 - all_sides - top , y2 = self . y2 + all_sides + bottom )
10021	def get_environments ( self ) : response = self . ebs . describe_environments ( application_name = self . app_name , include_deleted = False ) return response [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ]
5728	def _get_responses_unix ( self , timeout_sec ) : timeout_time_sec = time . time ( ) + timeout_sec responses = [ ] while True : select_timeout = timeout_time_sec - time . time ( ) if select_timeout <= 0 : select_timeout = 0 events , _ , _ = select . select ( self . read_list , [ ] , [ ] , select_timeout ) responses_list = None try : for fileno in events : if fileno == self . stdout_fileno : self . gdb_process . stdout . flush ( ) raw_output = self . gdb_process . stdout . read ( ) stream = "stdout" elif fileno == self . stderr_fileno : self . gdb_process . stderr . flush ( ) raw_output = self . gdb_process . stderr . read ( ) stream = "stderr" else : raise ValueError ( "Developer error. Got unexpected file number %d" % fileno ) responses_list = self . _get_responses_list ( raw_output , stream ) responses += responses_list except IOError : pass if timeout_sec == 0 : break elif responses_list and self . _allow_overwrite_timeout_times : timeout_time_sec = min ( time . time ( ) + self . time_to_check_for_additional_output_sec , timeout_time_sec , ) elif time . time ( ) > timeout_time_sec : break return responses
7533	def concat_multiple_edits ( data , sample ) : if len ( sample . files . edits ) > 1 : cmd1 = [ "cat" ] + [ i [ 0 ] for i in sample . files . edits ] conc1 = os . path . join ( data . dirs . edits , sample . name + "_R1_concatedit.fq.gz" ) with open ( conc1 , 'w' ) as cout1 : proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = cout1 , close_fds = True ) res1 = proc1 . communicate ( ) [ 0 ] if proc1 . returncode : raise IPyradWarningExit ( "error in: %s, %s" , cmd1 , res1 ) conc2 = 0 if os . path . exists ( str ( sample . files . edits [ 0 ] [ 1 ] ) ) : cmd2 = [ "cat" ] + [ i [ 1 ] for i in sample . files . edits ] conc2 = os . path . join ( data . dirs . edits , sample . name + "_R2_concatedit.fq.gz" ) with gzip . open ( conc2 , 'w' ) as cout2 : proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = cout2 , close_fds = True ) res2 = proc2 . communicate ( ) [ 0 ] if proc2 . returncode : raise IPyradWarningExit ( "error in: %s, %s" , cmd2 , res2 ) sample . files . edits = [ ( conc1 , conc2 ) ] return sample . files . edits
1868	def PSRLDQ ( cpu , dest , src ) : temp = Operators . EXTRACT ( src . read ( ) , 0 , 8 ) temp = Operators . ITEBV ( src . size , temp > 15 , 16 , temp ) dest . write ( dest . read ( ) >> ( temp * 8 ) )
8626	def delete_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_delete_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise UserJobsNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
2586	def _command_server ( self , kill_event ) : logger . debug ( "[COMMAND] Command Server Starting" ) while not kill_event . is_set ( ) : try : command_req = self . command_channel . recv_pyobj ( ) logger . debug ( "[COMMAND] Received command request: {}" . format ( command_req ) ) if command_req == "OUTSTANDING_C" : outstanding = self . pending_task_queue . qsize ( ) for manager in self . _ready_manager_queue : outstanding += len ( self . _ready_manager_queue [ manager ] [ 'tasks' ] ) reply = outstanding elif command_req == "WORKERS" : num_workers = 0 for manager in self . _ready_manager_queue : num_workers += self . _ready_manager_queue [ manager ] [ 'worker_count' ] reply = num_workers elif command_req == "MANAGERS" : reply = [ ] for manager in self . _ready_manager_queue : resp = { 'manager' : manager . decode ( 'utf-8' ) , 'block_id' : self . _ready_manager_queue [ manager ] [ 'block_id' ] , 'worker_count' : self . _ready_manager_queue [ manager ] [ 'worker_count' ] , 'tasks' : len ( self . _ready_manager_queue [ manager ] [ 'tasks' ] ) , 'active' : self . _ready_manager_queue [ manager ] [ 'active' ] } reply . append ( resp ) elif command_req . startswith ( "HOLD_WORKER" ) : cmd , s_manager = command_req . split ( ';' ) manager = s_manager . encode ( 'utf-8' ) logger . info ( "[CMD] Received HOLD_WORKER for {}" . format ( manager ) ) if manager in self . _ready_manager_queue : self . _ready_manager_queue [ manager ] [ 'active' ] = False reply = True else : reply = False elif command_req == "SHUTDOWN" : logger . info ( "[CMD] Received SHUTDOWN command" ) kill_event . set ( ) reply = True else : reply = None logger . debug ( "[COMMAND] Reply: {}" . format ( reply ) ) self . command_channel . send_pyobj ( reply ) except zmq . Again : logger . debug ( "[COMMAND] is alive" ) continue
12627	def iter_recursive_find ( folder_path , * regex ) : for root , dirs , files in os . walk ( folder_path ) : if len ( files ) > 0 : outlist = [ ] for f in files : for reg in regex : if re . search ( reg , f ) : outlist . append ( op . join ( root , f ) ) if len ( outlist ) == len ( regex ) : yield outlist
1174	def lock ( self , function , argument ) : if self . testandset ( ) : function ( argument ) else : self . queue . append ( ( function , argument ) )
4382	def is_allowed ( self , role , method , resource ) : return ( role , method , resource ) in self . _allowed
8916	def _retrieve_certificate ( self , access_token , timeout = 3 ) : logger . debug ( "Retrieve certificate with token." ) key_pair = crypto . PKey ( ) key_pair . generate_key ( crypto . TYPE_RSA , 2048 ) private_key = crypto . dump_privatekey ( crypto . FILETYPE_PEM , key_pair ) . decode ( "utf-8" ) cert_request = crypto . X509Req ( ) cert_request . set_pubkey ( key_pair ) cert_request . sign ( key_pair , 'md5' ) der_cert_req = crypto . dump_certificate_request ( crypto . FILETYPE_ASN1 , cert_request ) encoded_cert_req = base64 . b64encode ( der_cert_req ) token = { 'access_token' : access_token , 'token_type' : 'Bearer' } client = OAuth2Session ( token = token ) response = client . post ( self . certificate_url , data = { 'certificate_request' : encoded_cert_req } , verify = False , timeout = timeout , ) if response . ok : content = "{} {}" . format ( response . text , private_key ) with open ( self . esgf_credentials , 'w' ) as fh : fh . write ( content ) logger . debug ( 'Fetched certificate successfully.' ) else : msg = "Could not get certificate: {} {}" . format ( response . status_code , response . reason ) raise Exception ( msg ) return True
10170	def set_scheduled ( self ) : with self . _idle_lock : if self . _idle : self . _idle = False return True return False
10297	def get_undefined_namespace_names ( graph : BELGraph , namespace : str ) -> Set [ str ] : return { exc . name for _ , exc , _ in graph . warnings if isinstance ( exc , UndefinedNamespaceWarning ) and exc . namespace == namespace }
4350	def vol ( self , gain , gain_type = 'amplitude' , limiter_gain = None ) : if not is_number ( gain ) : raise ValueError ( 'gain must be a number.' ) if limiter_gain is not None : if ( not is_number ( limiter_gain ) or limiter_gain <= 0 or limiter_gain >= 1 ) : raise ValueError ( 'limiter gain must be a positive number less than 1' ) if gain_type in [ 'amplitude' , 'power' ] and gain < 0 : raise ValueError ( "If gain_type = amplitude or power, gain must be positive." ) effect_args = [ 'vol' ] effect_args . append ( '{:f}' . format ( gain ) ) if gain_type == 'amplitude' : effect_args . append ( 'amplitude' ) elif gain_type == 'power' : effect_args . append ( 'power' ) elif gain_type == 'db' : effect_args . append ( 'dB' ) else : raise ValueError ( 'gain_type must be one of amplitude power or db' ) if limiter_gain is not None : if gain_type in [ 'amplitude' , 'power' ] and gain > 1 : effect_args . append ( '{:f}' . format ( limiter_gain ) ) elif gain_type == 'db' and gain > 0 : effect_args . append ( '{:f}' . format ( limiter_gain ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'vol' ) return self
6233	def points_random_3d ( count , range_x = ( - 10.0 , 10.0 ) , range_y = ( - 10.0 , 10.0 ) , range_z = ( - 10.0 , 10.0 ) , seed = None ) -> VAO : random . seed ( seed ) def gen ( ) : for _ in range ( count ) : yield random . uniform ( * range_x ) yield random . uniform ( * range_y ) yield random . uniform ( * range_z ) data = numpy . fromiter ( gen ( ) , count = count * 3 , dtype = numpy . float32 ) vao = VAO ( "geometry:points_random_3d" , mode = moderngl . POINTS ) vao . buffer ( data , '3f' , [ 'in_position' ] ) return vao
12896	def get_volume_steps ( self ) : if not self . __volume_steps : self . __volume_steps = yield from self . handle_int ( self . API . get ( 'volume_steps' ) ) return self . __volume_steps
3579	def clear_cached_data ( self ) : for device in self . list_devices ( ) : if device . is_connected : continue adapter = dbus . Interface ( self . _bus . get_object ( 'org.bluez' , device . _adapter ) , _ADAPTER_INTERFACE ) adapter . RemoveDevice ( device . _device . object_path )
484	def enableConcurrencyChecks ( maxConcurrency , raiseException = True ) : global g_max_concurrency , g_max_concurrency_raise_exception assert maxConcurrency >= 0 g_max_concurrency = maxConcurrency g_max_concurrency_raise_exception = raiseException return
13741	def cached_httpbl_exempt ( view_func ) : def wrapped_view ( * args , ** kwargs ) : return view_func ( * args , ** kwargs ) wrapped_view . cached_httpbl_exempt = True return wraps ( view_func , assigned = available_attrs ( view_func ) ) ( wrapped_view )
13033	def write_index_translation ( translation_filename , entity_ids , relation_ids ) : translation = triple_pb . Translation ( ) entities = [ ] for name , index in entity_ids . items ( ) : translation . entities . add ( element = name , index = index ) relations = [ ] for name , index in relation_ids . items ( ) : translation . relations . add ( element = name , index = index ) with open ( translation_filename , "wb" ) as f : f . write ( translation . SerializeToString ( ) )
496	def _classifyState ( self , state ) : if state . ROWID < self . getParameter ( 'trainRecords' ) : if not state . setByUser : state . anomalyLabel = [ ] self . _deleteRecordsFromKNN ( [ state ] ) return label = KNNAnomalyClassifierRegion . AUTO_THRESHOLD_CLASSIFIED_LABEL autoLabel = label + KNNAnomalyClassifierRegion . AUTO_TAG newCategory = self . _recomputeRecordFromKNN ( state ) labelList = self . _categoryToLabelList ( newCategory ) if state . setByUser : if label in state . anomalyLabel : state . anomalyLabel . remove ( label ) if autoLabel in state . anomalyLabel : state . anomalyLabel . remove ( autoLabel ) labelList . extend ( state . anomalyLabel ) if state . anomalyScore >= self . getParameter ( 'anomalyThreshold' ) : labelList . append ( label ) elif label in labelList : ind = labelList . index ( label ) labelList [ ind ] = autoLabel labelList = list ( set ( labelList ) ) if label in labelList and autoLabel in labelList : labelList . remove ( autoLabel ) if state . anomalyLabel == labelList : return state . anomalyLabel = labelList if state . anomalyLabel == [ ] : self . _deleteRecordsFromKNN ( [ state ] ) else : self . _addRecordToKNN ( state )
3402	def is_boundary_type ( reaction , boundary_type , external_compartment ) : sbo_term = reaction . annotation . get ( "sbo" , "" ) if isinstance ( sbo_term , list ) : sbo_term = sbo_term [ 0 ] sbo_term = sbo_term . upper ( ) if sbo_term == sbo_terms [ boundary_type ] : return True if sbo_term in [ sbo_terms [ k ] for k in sbo_terms if k != boundary_type ] : return False correct_compartment = external_compartment in reaction . compartments if boundary_type != "exchange" : correct_compartment = not correct_compartment rev_type = True if boundary_type == "demand" : rev_type = not reaction . reversibility elif boundary_type == "sink" : rev_type = reaction . reversibility return ( reaction . boundary and not any ( ex in reaction . id for ex in excludes [ boundary_type ] ) and correct_compartment and rev_type )
7327	def parse_int_list ( string ) : integers = [ ] for comma_part in string . split ( "," ) : for substring in comma_part . split ( " " ) : if len ( substring ) == 0 : continue if "-" in substring : left , right = substring . split ( "-" ) left_val = int ( left . strip ( ) ) right_val = int ( right . strip ( ) ) integers . extend ( range ( left_val , right_val + 1 ) ) else : integers . append ( int ( substring . strip ( ) ) ) return integers
4612	def block_timestamp ( self , block_num ) : return int ( self . block_class ( block_num , blockchain_instance = self . blockchain ) . time ( ) . timestamp ( ) )
3370	def interface_to_str ( interface ) : if isinstance ( interface , ModuleType ) : interface = interface . __name__ return re . sub ( r"optlang.|.interface" , "" , interface )
525	def _inhibitColumnsGlobal ( self , overlaps , density ) : numActive = int ( density * self . _numColumns ) sortedWinnerIndices = numpy . argsort ( overlaps , kind = 'mergesort' ) start = len ( sortedWinnerIndices ) - numActive while start < len ( sortedWinnerIndices ) : i = sortedWinnerIndices [ start ] if overlaps [ i ] >= self . _stimulusThreshold : break else : start += 1 return sortedWinnerIndices [ start : ] [ : : - 1 ]
11579	def system_reset ( self ) : data = chr ( self . SYSTEM_RESET ) self . pymata . transport . write ( data ) with self . pymata . data_lock : for _ in range ( len ( self . digital_response_table ) ) : self . digital_response_table . pop ( ) for _ in range ( len ( self . analog_response_table ) ) : self . analog_response_table . pop ( ) for pin in range ( 0 , self . total_pins_discovered ) : response_entry = [ self . pymata . INPUT , 0 , None ] self . digital_response_table . append ( response_entry ) for pin in range ( 0 , self . number_of_analog_pins_discovered ) : response_entry = [ self . pymata . INPUT , 0 , None ] self . analog_response_table . append ( response_entry )
5338	def __upload_title ( self , kibiter_major ) : if kibiter_major == "6" : resource = ".kibana/doc/projectname" data = { "projectname" : { "name" : self . project_name } } mapping_resource = ".kibana/_mapping/doc" mapping = { "dynamic" : "true" } url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , resource ) mapping_url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , mapping_resource ) logger . debug ( "Adding mapping for dashboard title" ) res = self . grimoire_con . put ( mapping_url , data = json . dumps ( mapping ) , headers = ES6_HEADER ) try : res . raise_for_status ( ) except requests . exceptions . HTTPError : logger . error ( "Couldn't create mapping for dashboard title." ) logger . error ( res . json ( ) ) logger . debug ( "Uploading dashboard title" ) res = self . grimoire_con . post ( url , data = json . dumps ( data ) , headers = ES6_HEADER ) try : res . raise_for_status ( ) except requests . exceptions . HTTPError : logger . error ( "Couldn't create dashboard title." ) logger . error ( res . json ( ) )
7062	def sqs_put_item ( queue_url , item , delay_seconds = 0 , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 'sqs' ) try : json_msg = json . dumps ( item ) resp = client . send_message ( QueueUrl = queue_url , MessageBody = json_msg , DelaySeconds = delay_seconds , ) if not resp : LOGERROR ( 'could not send item to queue: %s' % queue_url ) return None else : return resp except Exception as e : LOGEXCEPTION ( 'could not send item to queue: %s' % queue_url ) if raiseonfail : raise return None
5855	def update_dataset ( self , dataset_id , name = None , description = None , public = None ) : data = { "public" : _convert_bool_to_public_value ( public ) } if name : data [ "name" ] = name if description : data [ "description" ] = description dataset = { "dataset" : data } failure_message = "Failed to update dataset {}" . format ( dataset_id ) response = self . _get_success_json ( self . _post_json ( routes . update_dataset ( dataset_id ) , data = dataset , failure_message = failure_message ) ) return _dataset_from_response_dict ( response )
6421	def encode ( self , word ) : word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = word . translate ( { 198 : 'AE' , 338 : 'OE' } ) word = '' . join ( c for c in word if c in self . _uc_set ) for rule in self . _rule_order : regex , repl = self . _rule_table [ rule ] if isinstance ( regex , text_type ) : word = word . replace ( regex , repl ) else : word = regex . sub ( repl , word ) return word
2375	def report ( self , obj , message , linenum , char_offset = 0 ) : self . controller . report ( linenumber = linenum , filename = obj . path , severity = self . severity , message = message , rulename = self . __class__ . __name__ , char = char_offset )
4133	def codestr2rst ( codestr , lang = 'python' ) : code_directive = "\n.. code-block:: {0}\n\n" . format ( lang ) indented_block = indent ( codestr , ' ' * 4 ) return code_directive + indented_block
2495	def handle_pkg_optional_fields ( self , package , package_node ) : self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . versionInfo , 'version' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . packageFileName , 'file_name' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . supplier , 'supplier' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . originator , 'originator' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . sourceInfo , 'source_info' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . licenseComments , 'license_comment' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . summary , 'summary' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . description , 'description' ) if package . has_optional_field ( 'check_sum' ) : checksum_node = self . create_checksum_node ( package . check_sum ) self . graph . add ( ( package_node , self . spdx_namespace . checksum , checksum_node ) ) if package . has_optional_field ( 'homepage' ) : homepage_node = URIRef ( self . to_special_value ( package . homepage ) ) homepage_triple = ( package_node , self . doap_namespace . homepage , homepage_node ) self . graph . add ( homepage_triple )
5385	def _get_operation_input_field_values ( self , metadata , file_input ) : input_args = metadata [ 'request' ] [ 'ephemeralPipeline' ] [ 'inputParameters' ] vals_dict = metadata [ 'request' ] [ 'pipelineArgs' ] [ 'inputs' ] names = [ arg [ 'name' ] for arg in input_args if ( 'localCopy' in arg ) == file_input ] return { name : vals_dict [ name ] for name in names if name in vals_dict }
8083	def transform ( self , mode = None ) : if mode : self . _canvas . mode = mode return self . _canvas . mode
8549	def create_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule ) : properties = { "name" : firewall_rule . name } if firewall_rule . protocol : properties [ 'protocol' ] = firewall_rule . protocol if firewall_rule . source_mac : properties [ 'sourceMac' ] = firewall_rule . source_mac if firewall_rule . source_ip : properties [ 'sourceIp' ] = firewall_rule . source_ip if firewall_rule . target_ip : properties [ 'targetIp' ] = firewall_rule . target_ip if firewall_rule . port_range_start : properties [ 'portRangeStart' ] = firewall_rule . port_range_start if firewall_rule . port_range_end : properties [ 'portRangeEnd' ] = firewall_rule . port_range_end if firewall_rule . icmp_type : properties [ 'icmpType' ] = firewall_rule . icmp_type if firewall_rule . icmp_code : properties [ 'icmpCode' ] = firewall_rule . icmp_code data = { "properties" : properties } response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s/firewallrules' % ( datacenter_id , server_id , nic_id ) , method = 'POST' , data = json . dumps ( data ) ) return response
8348	def convert_charref ( self , name ) : try : n = int ( name ) except ValueError : return if not 0 <= n <= 127 : return return self . convert_codepoint ( n )
9981	def is_funcdef ( src ) : module_node = ast . parse ( dedent ( src ) ) if len ( module_node . body ) == 1 and isinstance ( module_node . body [ 0 ] , ast . FunctionDef ) : return True else : return False
3439	def _escape_str_id ( id_str ) : for c in ( "'" , '"' ) : if id_str . startswith ( c ) and id_str . endswith ( c ) and id_str . count ( c ) == 2 : id_str = id_str . strip ( c ) for char , escaped_char in _renames : id_str = id_str . replace ( char , escaped_char ) return id_str
1701	def join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . INNER , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
7072	def matthews_correl_coeff ( ntp , ntn , nfp , nfn ) : mcc_top = ( ntp * ntn - nfp * nfn ) mcc_bot = msqrt ( ( ntp + nfp ) * ( ntp + nfn ) * ( ntn + nfp ) * ( ntn + nfn ) ) if mcc_bot > 0 : return mcc_top / mcc_bot else : return np . nan
8216	def hide_variables_window ( self ) : if self . var_window is not None : self . var_window . window . destroy ( ) self . var_window = None
5839	def submit_predict_request ( self , data_view_id , candidates , prediction_source = 'scalar' , use_prior = True ) : data = { "prediction_source" : prediction_source , "use_prior" : use_prior , "candidates" : candidates } failure_message = "Configuration creation failed" post_url = 'v1/data_views/' + str ( data_view_id ) + '/predict/submit' return self . _get_success_json ( self . _post_json ( post_url , data , failure_message = failure_message ) ) [ 'data' ] [ 'uid' ]
9684	def sn ( self ) : string = [ ] self . cnxn . xfer ( [ 0x10 ] ) sleep ( 9e-3 ) for i in range ( 60 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] string . append ( chr ( resp ) ) sleep ( 0.1 ) return '' . join ( string )
9866	def price_unit ( self ) : currency = self . currency consumption_unit = self . consumption_unit if not currency or not consumption_unit : _LOGGER . error ( "Could not find price_unit." ) return " " return currency + "/" + consumption_unit
885	def activatePredictedColumn ( self , column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells , learn ) : return self . _activatePredictedColumn ( self . connections , self . _random , columnActiveSegments , prevActiveCells , prevWinnerCells , self . numActivePotentialSynapsesForSegment , self . maxNewSynapseCount , self . initialPermanence , self . permanenceIncrement , self . permanenceDecrement , self . maxSynapsesPerSegment , learn )
12648	def filter_objlist ( olist , fieldname , fieldval ) : return [ x for x in olist if getattr ( x , fieldname ) == fieldval ]
6714	def _install_from_scratch ( python_cmd , use_sudo ) : with cd ( "/tmp" ) : download ( EZ_SETUP_URL ) command = '%(python_cmd)s ez_setup.py' % locals ( ) if use_sudo : run_as_root ( command ) else : run ( command ) run ( 'rm -f ez_setup.py' )
1201	def reset ( self ) : self . level . reset ( ) return self . level . observations ( ) [ self . state_attribute ]
6926	def cursor ( self , handle , dictcursor = False ) : if handle in self . cursors : return self . cursors [ handle ] else : if dictcursor : self . cursors [ handle ] = self . connection . cursor ( cursor_factory = psycopg2 . extras . DictCursor ) else : self . cursors [ handle ] = self . connection . cursor ( ) return self . cursors [ handle ]
13638	def settings ( path = None , with_path = None ) : if path : Settings . bind ( path , with_path = with_path ) return Settings . _wrapped
530	def getInputNames ( self ) : inputs = self . getSpec ( ) . inputs return [ inputs . getByIndex ( i ) [ 0 ] for i in xrange ( inputs . getCount ( ) ) ]
12433	def dasherize ( value ) : value = value . strip ( ) value = re . sub ( r'([A-Z])' , r'-\1' , value ) value = re . sub ( r'[-_\s]+' , r'-' , value ) value = re . sub ( r'^-' , r'' , value ) value = value . lower ( ) return value
7131	def setup_paths ( source , destination , name , add_to_global , force ) : if source [ - 1 ] == "/" : source = source [ : - 1 ] if not name : name = os . path . split ( source ) [ - 1 ] elif name . endswith ( ".docset" ) : name = name . replace ( ".docset" , "" ) if add_to_global : destination = DEFAULT_DOCSET_PATH dest = os . path . join ( destination or "" , name + ".docset" ) dst_exists = os . path . lexists ( dest ) if dst_exists and force : shutil . rmtree ( dest ) elif dst_exists : log . error ( 'Destination path "{}" already exists.' . format ( click . format_filename ( dest ) ) ) raise SystemExit ( errno . EEXIST ) return source , dest , name
9554	def _apply_record_length_checks ( self , i , r , summarize = False , context = None ) : for code , message , modulus in self . _record_length_checks : if i % modulus == 0 : if len ( r ) != len ( self . _field_names ) : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'length' ] = len ( r ) if context is not None : p [ 'context' ] = context yield p
8013	async def _create_upstream_applications ( self ) : loop = asyncio . get_event_loop ( ) for steam_name , ApplicationsCls in self . applications . items ( ) : application = ApplicationsCls ( self . scope ) upstream_queue = asyncio . Queue ( ) self . application_streams [ steam_name ] = upstream_queue self . application_futures [ steam_name ] = loop . create_task ( application ( upstream_queue . get , partial ( self . dispatch_downstream , steam_name = steam_name ) ) )
8726	def _localize ( dt ) : try : tz = dt . tzinfo return tz . localize ( dt . replace ( tzinfo = None ) ) except AttributeError : return dt
1576	def make_shell_endpoint ( topologyInfo , instance_id ) : pplan = topologyInfo [ "physical_plan" ] stmgrId = pplan [ "instances" ] [ instance_id ] [ "stmgrId" ] host = pplan [ "stmgrs" ] [ stmgrId ] [ "host" ] shell_port = pplan [ "stmgrs" ] [ stmgrId ] [ "shell_port" ] return "http://%s:%d" % ( host , shell_port )
4067	def update_item ( self , payload , last_modified = None ) : to_send = self . check_items ( [ payload ] ) [ 0 ] if last_modified is None : modified = payload [ "version" ] else : modified = last_modified ident = payload [ "key" ] headers = { "If-Unmodified-Since-Version" : str ( modified ) } headers . update ( self . default_headers ( ) ) req = requests . patch ( url = self . endpoint + "/{t}/{u}/items/{id}" . format ( t = self . library_type , u = self . library_id , id = ident ) , headers = headers , data = json . dumps ( to_send ) , ) self . request = req try : req . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( req ) return True
3335	def byte_number_string ( number , thousandsSep = True , partition = False , base1024 = True , appendBytes = True ) : magsuffix = "" bytesuffix = "" if partition : magnitude = 0 if base1024 : while number >= 1024 : magnitude += 1 number = number >> 10 else : while number >= 1000 : magnitude += 1 number /= 1000.0 magsuffix = [ "" , "K" , "M" , "G" , "T" , "P" ] [ magnitude ] if appendBytes : if number == 1 : bytesuffix = " Byte" else : bytesuffix = " Bytes" if thousandsSep and ( number >= 1000 or magsuffix ) : snum = "{:,d}" . format ( number ) else : snum = str ( number ) return "{}{}{}" . format ( snum , magsuffix , bytesuffix )
9995	def del_attr ( self , name ) : if name in self . namespace : if name in self . cells : self . del_cells ( name ) elif name in self . spaces : self . del_space ( name ) elif name in self . refs : self . del_ref ( name ) else : raise RuntimeError ( "Must not happen" ) else : raise KeyError ( "'%s' not found in Space '%s'" % ( name , self . name ) )
11327	def autodiscover ( ) : import imp from django . conf import settings for app in settings . INSTALLED_APPS : try : app_path = __import__ ( app , { } , { } , [ app . split ( '.' ) [ - 1 ] ] ) . __path__ except AttributeError : continue try : imp . find_module ( 'oembed_providers' , app_path ) except ImportError : continue __import__ ( "%s.oembed_providers" % app )
10082	def _prepare_edit ( self , record ) : data = record . dumps ( ) data [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] = record . revision_id data [ '_deposit' ] [ 'status' ] = 'draft' data [ '$schema' ] = self . build_deposit_schema ( record ) return data
9123	def belns ( keyword : str , file : TextIO , encoding : Optional [ str ] , use_names : bool ) : directory = get_data_dir ( keyword ) obo_url = f'http://purl.obolibrary.org/obo/{keyword}.obo' obo_path = os . path . join ( directory , f'{keyword}.obo' ) obo_cache_path = os . path . join ( directory , f'{keyword}.obo.pickle' ) obo_getter = make_obo_getter ( obo_url , obo_path , preparsed_path = obo_cache_path ) graph = obo_getter ( ) convert_obo_graph_to_belns ( graph , file = file , encoding = encoding , use_names = use_names , )
4723	def trun_setup ( conf ) : declr = None try : with open ( conf [ "TESTPLAN_FPATH" ] ) as declr_fd : declr = yaml . safe_load ( declr_fd ) except AttributeError as exc : cij . err ( "rnr: %r" % exc ) if not declr : return None trun = copy . deepcopy ( TRUN ) trun [ "ver" ] = cij . VERSION trun [ "conf" ] = copy . deepcopy ( conf ) trun [ "res_root" ] = conf [ "OUTPUT" ] trun [ "aux_root" ] = os . sep . join ( [ trun [ "res_root" ] , "_aux" ] ) trun [ "evars" ] . update ( copy . deepcopy ( declr . get ( "evars" , { } ) ) ) os . makedirs ( trun [ "aux_root" ] ) hook_names = declr . get ( "hooks" , [ ] ) if "lock" not in hook_names : hook_names = [ "lock" ] + hook_names if hook_names [ 0 ] != "lock" : return None trun [ "hooks" ] = hooks_setup ( trun , trun , hook_names ) for enum , declr in enumerate ( declr [ "testsuites" ] ) : tsuite = tsuite_setup ( trun , declr , enum ) if tsuite is None : cij . err ( "main::FAILED: setting up tsuite: %r" % tsuite ) return 1 trun [ "testsuites" ] . append ( tsuite ) trun [ "progress" ] [ "UNKN" ] += len ( tsuite [ "testcases" ] ) return trun
9745	def connection_made ( self , transport ) : self . transport = transport sock = transport . get_extra_info ( "socket" ) self . port = sock . getsockname ( ) [ 1 ]
8458	def shell ( cmd , check = True , stdin = None , stdout = None , stderr = None ) : return subprocess . run ( cmd , shell = True , check = check , stdin = stdin , stdout = stdout , stderr = stderr )
5097	def _calculate_distance ( latlon1 , latlon2 ) : lat1 , lon1 = latlon1 lat2 , lon2 = latlon2 dlon = lon2 - lon1 dlat = lat2 - lat1 R = 6371 a = np . sin ( dlat / 2 ) ** 2 + np . cos ( lat1 ) * np . cos ( lat2 ) * ( np . sin ( dlon / 2 ) ) ** 2 c = 2 * np . pi * R * np . arctan2 ( np . sqrt ( a ) , np . sqrt ( 1 - a ) ) / 180 return c
5911	def gmx_resid ( self , resid ) : try : gmx_resid = int ( self . offset [ resid ] ) except ( TypeError , IndexError ) : gmx_resid = resid + self . offset except KeyError : raise KeyError ( "offset must be a dict that contains the gmx resid for {0:d}" . format ( resid ) ) return gmx_resid
10316	def relation_set_has_contradictions ( relations : Set [ str ] ) -> bool : has_increases = any ( relation in CAUSAL_INCREASE_RELATIONS for relation in relations ) has_decreases = any ( relation in CAUSAL_DECREASE_RELATIONS for relation in relations ) has_cnc = any ( relation == CAUSES_NO_CHANGE for relation in relations ) return 1 < sum ( [ has_cnc , has_decreases , has_increases ] )
111	def warn_deprecated ( msg , stacklevel = 2 ) : import warnings warnings . warn ( msg , category = DeprecationWarning , stacklevel = stacklevel )
449	def batch_normalization ( x , mean , variance , offset , scale , variance_epsilon , data_format , name = None ) : with ops . name_scope ( name , 'batchnorm' , [ x , mean , variance , scale , offset ] ) : inv = math_ops . rsqrt ( variance + variance_epsilon ) if scale is not None : inv *= scale a = math_ops . cast ( inv , x . dtype ) b = math_ops . cast ( offset - mean * inv if offset is not None else - mean * inv , x . dtype ) df = { 'channels_first' : 'NCHW' , 'channels_last' : 'NHWC' } return _bias_add ( _bias_scale ( x , a , df [ data_format ] ) , b , df [ data_format ] )
13494	def write ( args ) : logging . info ( "Writing configure file: %s" % args . config_file ) if args . config_file is None : return config = cparser . ConfigParser ( ) config . add_section ( "lrcloud" ) for p in [ x for x in dir ( args ) if not x . startswith ( "_" ) ] : if p in IGNORE_ARGS : continue value = getattr ( args , p ) if value is not None : config . set ( 'lrcloud' , p , str ( value ) ) with open ( args . config_file , 'w' ) as f : config . write ( f )
3599	def delivery ( self , packageName , versionCode = None , offerType = 1 , downloadToken = None , expansion_files = False ) : if versionCode is None : versionCode = self . details ( packageName ) . get ( 'versionCode' ) params = { 'ot' : str ( offerType ) , 'doc' : packageName , 'vc' : str ( versionCode ) } headers = self . getHeaders ( ) if downloadToken is not None : params [ 'dtok' ] = downloadToken response = requests . get ( DELIVERY_URL , headers = headers , params = params , verify = ssl_verify , timeout = 60 , proxies = self . proxies_config ) response = googleplay_pb2 . ResponseWrapper . FromString ( response . content ) if response . commands . displayErrorMessage != "" : raise RequestError ( response . commands . displayErrorMessage ) elif response . payload . deliveryResponse . appDeliveryData . downloadUrl == "" : raise RequestError ( 'App not purchased' ) else : result = { } result [ 'docId' ] = packageName result [ 'additionalData' ] = [ ] downloadUrl = response . payload . deliveryResponse . appDeliveryData . downloadUrl cookie = response . payload . deliveryResponse . appDeliveryData . downloadAuthCookie [ 0 ] cookies = { str ( cookie . name ) : str ( cookie . value ) } result [ 'file' ] = self . _deliver_data ( downloadUrl , cookies ) if not expansion_files : return result for obb in response . payload . deliveryResponse . appDeliveryData . additionalFile : a = { } if obb . fileType == 0 : obbType = 'main' else : obbType = 'patch' a [ 'type' ] = obbType a [ 'versionCode' ] = obb . versionCode a [ 'file' ] = self . _deliver_data ( obb . downloadUrl , None ) result [ 'additionalData' ] . append ( a ) return result
12189	async def from_api_token ( cls , token = None , api_cls = SlackBotApi ) : api = api_cls . from_env ( ) if token is None else api_cls ( api_token = token ) data = await api . execute_method ( cls . API_AUTH_ENDPOINT ) return cls ( data [ 'user_id' ] , data [ 'user' ] , api )
1891	def _solver_version ( self ) -> Version : self . _reset ( ) if self . _received_version is None : self . _send ( '(get-info :version)' ) self . _received_version = self . _recv ( ) key , version = shlex . split ( self . _received_version [ 1 : - 1 ] ) return Version ( * map ( int , version . split ( '.' ) ) )
7659	def append_columns ( self , columns ) : self . append_records ( [ dict ( time = t , duration = d , value = v , confidence = c ) for ( t , d , v , c ) in six . moves . zip ( columns [ 'time' ] , columns [ 'duration' ] , columns [ 'value' ] , columns [ 'confidence' ] ) ] )
1615	def ReplaceAll ( pattern , rep , s ) : if pattern not in _regexp_compile_cache : _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) return _regexp_compile_cache [ pattern ] . sub ( rep , s )
11498	def get_community_by_name ( self , name , token = None ) : parameters = dict ( ) parameters [ 'name' ] = name if token : parameters [ 'token' ] = token response = self . request ( 'midas.community.get' , parameters ) return response
8208	def angle ( self , x0 , y0 , x1 , y1 ) : a = degrees ( atan ( ( y1 - y0 ) / ( x1 - x0 + 0.00001 ) ) ) + 360 if x1 - x0 < 0 : a += 180 return a
2142	def process_extra_vars ( extra_vars_list , force_json = True ) : extra_vars = { } extra_vars_yaml = "" for extra_vars_opt in extra_vars_list : if extra_vars_opt . startswith ( "@" ) : with open ( extra_vars_opt [ 1 : ] , 'r' ) as f : extra_vars_opt = f . read ( ) opt_dict = string_to_dict ( extra_vars_opt , allow_kv = False ) else : opt_dict = string_to_dict ( extra_vars_opt , allow_kv = True ) if any ( line . startswith ( "#" ) for line in extra_vars_opt . split ( '\n' ) ) : extra_vars_yaml += extra_vars_opt + "\n" elif extra_vars_opt != "" : extra_vars_yaml += yaml . dump ( opt_dict , default_flow_style = False ) + "\n" extra_vars . update ( opt_dict ) if not force_json : try : try_dict = yaml . load ( extra_vars_yaml , Loader = yaml . SafeLoader ) assert type ( try_dict ) is dict debug . log ( 'Using unprocessed YAML' , header = 'decision' , nl = 2 ) return extra_vars_yaml . rstrip ( ) except Exception : debug . log ( 'Failed YAML parsing, defaulting to JSON' , header = 'decison' , nl = 2 ) if extra_vars == { } : return "" return json . dumps ( extra_vars , ensure_ascii = False )
7256	def get_strip_metadata ( self , catID ) : self . logger . debug ( 'Retrieving strip catalog metadata' ) url = '%(base_url)s/record/%(catID)s?includeRelationships=false' % { 'base_url' : self . base_url , 'catID' : catID } r = self . gbdx_connection . get ( url ) if r . status_code == 200 : return r . json ( ) [ 'properties' ] elif r . status_code == 404 : self . logger . debug ( 'Strip not found: %s' % catID ) r . raise_for_status ( ) else : self . logger . debug ( 'There was a problem retrieving catid: %s' % catID ) r . raise_for_status ( )
177	def concatenate ( self , other ) : if not isinstance ( other , LineString ) : other = LineString ( other ) return self . deepcopy ( coords = np . concatenate ( [ self . coords , other . coords ] , axis = 0 ) )
6359	def sim ( self , src , tar ) : r if src == tar : return 1.0 elif not src or not tar : return 0.0 return len ( self . lcsstr ( src , tar ) ) / max ( len ( src ) , len ( tar ) )
3299	def xml_to_bytes ( element , pretty_print = False ) : if use_lxml : xml = etree . tostring ( element , encoding = "UTF-8" , xml_declaration = True , pretty_print = pretty_print ) else : xml = etree . tostring ( element , encoding = "UTF-8" ) if not xml . startswith ( b"<?xml " ) : xml = b'<?xml version="1.0" encoding="utf-8" ?>\n' + xml assert xml . startswith ( b"<?xml " ) return xml
5628	def hook ( self , event_type = 'push' ) : def decorator ( func ) : self . _hooks [ event_type ] . append ( func ) return func return decorator
2392	def edit_distance ( s1 , s2 ) : d = { } lenstr1 = len ( s1 ) lenstr2 = len ( s2 ) for i in xrange ( - 1 , lenstr1 + 1 ) : d [ ( i , - 1 ) ] = i + 1 for j in xrange ( - 1 , lenstr2 + 1 ) : d [ ( - 1 , j ) ] = j + 1 for i in xrange ( lenstr1 ) : for j in xrange ( lenstr2 ) : if s1 [ i ] == s2 [ j ] : cost = 0 else : cost = 1 d [ ( i , j ) ] = min ( d [ ( i - 1 , j ) ] + 1 , d [ ( i , j - 1 ) ] + 1 , d [ ( i - 1 , j - 1 ) ] + cost , ) if i and j and s1 [ i ] == s2 [ j - 1 ] and s1 [ i - 1 ] == s2 [ j ] : d [ ( i , j ) ] = min ( d [ ( i , j ) ] , d [ i - 2 , j - 2 ] + cost ) return d [ lenstr1 - 1 , lenstr2 - 1 ]
10988	def _translate_particles ( s , max_mem = 1e9 , desc = '' , min_rad = 'calc' , max_rad = 'calc' , invert = 'guess' , rz_order = 0 , do_polish = True ) : if desc is not None : desc_trans = desc + 'translate-particles' desc_burn = desc + 'addsub_burn' desc_polish = desc + 'addsub_polish' else : desc_trans , desc_burn , desc_polish = [ None ] * 3 RLOG . info ( 'Translate Particles:' ) opt . burn ( s , mode = 'do-particles' , n_loop = 4 , fractol = 0.1 , desc = desc_trans , max_mem = max_mem , include_rad = False , dowarn = False ) opt . burn ( s , mode = 'do-particles' , n_loop = 4 , fractol = 0.05 , desc = desc_trans , max_mem = max_mem , include_rad = True , dowarn = False ) RLOG . info ( 'Start add-subtract' ) addsub . add_subtract ( s , tries = 30 , min_rad = min_rad , max_rad = max_rad , invert = invert ) if desc is not None : states . save ( s , desc = desc + 'translate-addsub' ) if do_polish : RLOG . info ( 'Final Burn:' ) opt . burn ( s , mode = 'burn' , n_loop = 3 , fractol = 3e-4 , desc = desc_burn , max_mem = max_mem , rz_order = rz_order , dowarn = False ) RLOG . info ( 'Final Polish:' ) d = opt . burn ( s , mode = 'polish' , n_loop = 4 , fractol = 3e-4 , desc = desc_polish , max_mem = max_mem , rz_order = rz_order , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' )
6807	def create_raspbian_vagrant_box ( self ) : r = self . local_renderer r . sudo ( 'adduser --disabled-password --gecos "" vagrant' ) r . sudo ( 'echo "vagrant ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/vagrant' ) r . sudo ( 'chmod 0440 /etc/sudoers.d/vagrant' ) r . sudo ( 'apt-get update' ) r . sudo ( 'apt-get install -y openssh-server' ) r . sudo ( 'mkdir -p /home/vagrant/.ssh' ) r . sudo ( 'chmod 0700 /home/vagrant/.ssh' ) r . sudo ( 'wget --no-check-certificate https://raw.github.com/mitchellh/vagrant/master/keys/vagrant.pub -O /home/vagrant/.ssh/authorized_keys' ) r . sudo ( 'chmod 0600 /home/vagrant/.ssh/authorized_keys' ) r . sudo ( 'chown -R vagrant /home/vagrant/.ssh' ) r . sudo ( "sed -i '/AuthorizedKeysFile/s/^#//g' /etc/ssh/sshd_config" ) r . sudo ( "sed -i '/PasswordAuthentication/s/^#//g' /etc/ssh/sshd_config" ) r . sudo ( "sed -i 's/PasswordAuthentication yes/PasswordAuthentication no/g' /etc/ssh/sshd_config" ) r . sudo ( 'apt-get upgrade' ) r . sudo ( 'apt-get install -y gcc build-essential' ) r . sudo ( 'mkdir /tmp/test' ) r . sudo ( 'cp {libvirt_images_dir}/{raspbian_image} /tmp/test' ) r . sudo ( 'cp {libvirt_boot_dir}/{raspbian_kernel} /tmp/test' ) r . render_to_file ( 'rpi/metadata.json' , '/tmp/test/metadata.json' ) r . render_to_file ( 'rpi/Vagrantfile' , '/tmp/test/Vagrantfile' ) r . sudo ( 'qemu-img convert -f raw -O qcow2 {libvirt_images_dir}/{raspbian_image} {libvirt_images_dir}/{raspbian_image}.qcow2' ) r . sudo ( 'mv {libvirt_images_dir}/{raspbian_image}.qcow2 {libvirt_images_dir}/box.img' ) r . sudo ( 'cd /tmp/test; tar cvzf custom_box.box ./metadata.json ./Vagrantfile ./{raspbian_kernel} ./box.img' )
1418	def create_execution_state ( self , topologyName , executionState ) : if not executionState or not executionState . IsInitialized ( ) : raise_ ( StateException ( "Execution State protobuf not init properly" , StateException . EX_TYPE_PROTOBUF_ERROR ) , sys . exc_info ( ) [ 2 ] ) path = self . get_execution_state_path ( topologyName ) LOG . info ( "Adding topology: {0} to path: {1}" . format ( topologyName , path ) ) executionStateString = executionState . SerializeToString ( ) try : self . client . create ( path , value = executionStateString , makepath = True ) return True except NoNodeError : raise_ ( StateException ( "NoNodeError while creating execution state" , StateException . EX_TYPE_NO_NODE_ERROR ) , sys . exc_info ( ) [ 2 ] ) except NodeExistsError : raise_ ( StateException ( "NodeExistsError while creating execution state" , StateException . EX_TYPE_NODE_EXISTS_ERROR ) , sys . exc_info ( ) [ 2 ] ) except ZookeeperError : raise_ ( StateException ( "Zookeeper while creating execution state" , StateException . EX_TYPE_ZOOKEEPER_ERROR ) , sys . exc_info ( ) [ 2 ] ) except Exception : raise
4787	def matches ( self , pattern ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if not isinstance ( pattern , str_types ) : raise TypeError ( 'given pattern arg must be a string' ) if len ( pattern ) == 0 : raise ValueError ( 'given pattern arg must not be empty' ) if re . search ( pattern , self . val ) is None : self . _err ( 'Expected <%s> to match pattern <%s>, but did not.' % ( self . val , pattern ) ) return self
5152	def merge_list ( list1 , list2 , identifiers = None ) : identifiers = identifiers or [ ] dict_map = { 'list1' : OrderedDict ( ) , 'list2' : OrderedDict ( ) } counter = 1 for list_ in [ list1 , list2 ] : container = dict_map [ 'list{0}' . format ( counter ) ] for el in list_ : key = id ( el ) if isinstance ( el , dict ) : for id_key in identifiers : if id_key in el : key = el [ id_key ] break container [ key ] = deepcopy ( el ) counter += 1 merged = merge_config ( dict_map [ 'list1' ] , dict_map [ 'list2' ] ) return list ( merged . values ( ) )
7077	def parallel_periodicvar_recovery ( simbasedir , period_tolerance = 1.0e-3 , liststartind = None , listmaxobjects = None , nworkers = None ) : pfpkldir = os . path . join ( simbasedir , 'periodfinding' ) if not os . path . exists ( pfpkldir ) : LOGERROR ( 'no "periodfinding" subdirectory in %s, can\'t continue' % simbasedir ) return None pfpkl_list = glob . glob ( os . path . join ( pfpkldir , '*periodfinding*pkl*' ) ) if len ( pfpkl_list ) > 0 : if liststartind : pfpkl_list = pfpkl_list [ liststartind : ] if listmaxobjects : pfpkl_list = pfpkl_list [ : listmaxobjects ] tasks = [ ( x , simbasedir , period_tolerance ) for x in pfpkl_list ] pool = mp . Pool ( nworkers ) results = pool . map ( periodrec_worker , tasks ) pool . close ( ) pool . join ( ) resdict = { x [ 'objectid' ] : x for x in results if x is not None } actual_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and x [ 'actual_vartype' ] in PERIODIC_VARTYPES ) ] , dtype = np . unicode_ ) recovered_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and 'actual' in x [ 'best_recovered_status' ] ) ] , dtype = np . unicode_ ) alias_twice_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and 'twice' in x [ 'best_recovered_status' ] ) ] , dtype = np . unicode_ ) alias_half_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and 'half' in x [ 'best_recovered_status' ] ) ] , dtype = np . unicode_ ) all_objectids = [ x [ 'objectid' ] for x in results ] outdict = { 'simbasedir' : os . path . abspath ( simbasedir ) , 'objectids' : all_objectids , 'period_tolerance' : period_tolerance , 'actual_periodicvars' : actual_periodicvars , 'recovered_periodicvars' : recovered_periodicvars , 'alias_twice_periodicvars' : alias_twice_periodicvars , 'alias_half_periodicvars' : alias_half_periodicvars , 'details' : resdict } outfile = os . path . join ( simbasedir , 'periodicvar-recovery.pkl' ) with open ( outfile , 'wb' ) as outfd : pickle . dump ( outdict , outfd , pickle . HIGHEST_PROTOCOL ) return outdict else : LOGERROR ( 'no periodfinding result pickles found in %s, can\'t continue' % pfpkldir ) return None
7905	def forget ( self , rs ) : try : del self . rooms [ rs . room_jid . bare ( ) . as_unicode ( ) ] except KeyError : pass
13004	def modify_input ( ) : doc_mapper = DocMapper ( ) if doc_mapper . is_pipe : objects = [ obj for obj in doc_mapper . get_pipe ( ) ] modified = modify_data ( objects ) for line in modified : obj = doc_mapper . line_to_object ( line ) obj . save ( ) print_success ( "Object(s) successfully changed" ) else : print_error ( "Please use this tool with pipes" )
687	def getAllEncodings ( self ) : numEncodings = self . fields [ 0 ] . numEncodings assert ( all ( field . numEncodings == numEncodings for field in self . fields ) ) encodings = [ self . getEncoding ( index ) for index in range ( numEncodings ) ] return encodings
10207	def file_download_event_builder ( event , sender_app , obj = None , ** kwargs ) : event . update ( dict ( timestamp = datetime . datetime . utcnow ( ) . isoformat ( ) , bucket_id = str ( obj . bucket_id ) , file_id = str ( obj . file_id ) , file_key = obj . key , size = obj . file . size , referrer = request . referrer , ** get_user ( ) ) ) return event
3857	def is_quiet ( self ) : level = self . _conversation . self_conversation_state . notification_level return level == hangouts_pb2 . NOTIFICATION_LEVEL_QUIET
4359	def spawn ( self , fn , * args , ** kwargs ) : log . debug ( "Spawning sub-Socket Greenlet: %s" % fn . __name__ ) job = gevent . spawn ( fn , * args , ** kwargs ) self . jobs . append ( job ) return job
4260	def load_exif ( album ) : if not hasattr ( album . gallery , "exifCache" ) : _restore_cache ( album . gallery ) cache = album . gallery . exifCache for media in album . medias : if media . type == "image" : key = os . path . join ( media . path , media . filename ) if key in cache : media . exif = cache [ key ]
3087	def _get_entity ( self ) : if self . _is_ndb ( ) : return self . _model . get_by_id ( self . _key_name ) else : return self . _model . get_by_key_name ( self . _key_name )
11970	def _wildcard_to_dec ( nm , check = False ) : if check and not is_wildcard_nm ( nm ) : raise ValueError ( '_wildcard_to_dec: invalid netmask: "%s"' % nm ) return 0xFFFFFFFF - _dot_to_dec ( nm , check = False )
10415	def function_namespace_inclusion_builder ( func : str , namespace : Strings ) -> NodePredicate : if isinstance ( namespace , str ) : def function_namespaces_filter ( _ : BELGraph , node : BaseEntity ) -> bool : if func != node [ FUNCTION ] : return False return NAMESPACE in node and node [ NAMESPACE ] == namespace elif isinstance ( namespace , Iterable ) : namespaces = set ( namespace ) def function_namespaces_filter ( _ : BELGraph , node : BaseEntity ) -> bool : if func != node [ FUNCTION ] : return False return NAMESPACE in node and node [ NAMESPACE ] in namespaces else : raise ValueError ( 'Invalid type for argument: {}' . format ( namespace ) ) return function_namespaces_filter
310	def var_cov_var_normal ( P , c , mu = 0 , sigma = 1 ) : alpha = sp . stats . norm . ppf ( 1 - c , mu , sigma ) return P - P * ( alpha + 1 )
6879	def _smartcast ( castee , caster , subval = None ) : try : return caster ( castee ) except Exception as e : if caster is float or caster is int : return nan elif caster is str : return '' else : return subval
2409	def dump_model_to_file ( prompt_string , feature_ext , classifier , text , score , model_path ) : model_file = { 'prompt' : prompt_string , 'extractor' : feature_ext , 'model' : classifier , 'text' : text , 'score' : score } pickle . dump ( model_file , file = open ( model_path , "w" ) )
8887	def fit ( self , x , y = None ) : x = iter2array ( x , dtype = ( MoleculeContainer , CGRContainer ) ) if self . __head_less : warn ( f'{self.__class__.__name__} configured to head less mode. fit unusable' ) return self self . _reset ( ) self . __prepare ( x ) return self
3733	def charge ( self ) : try : return self . _charge except AttributeError : self . _charge = charge_from_formula ( self . formula ) return self . _charge
8510	def _predict ( self , X , method = 'fprop' ) : import theano X_sym = self . trainer . model . get_input_space ( ) . make_theano_batch ( ) y_sym = getattr ( self . trainer . model , method ) ( X_sym ) f = theano . function ( [ X_sym ] , y_sym , allow_input_downcast = True ) return f ( X )
6485	def do_search ( request , course_id = None ) : SearchInitializer . set_search_enviroment ( request = request , course_id = course_id ) results = { "error" : _ ( "Nothing to search" ) } status_code = 500 search_term = request . POST . get ( "search_string" , None ) try : if not search_term : raise ValueError ( _ ( 'No search term provided for search' ) ) size , from_ , page = _process_pagination_values ( request ) track . emit ( 'edx.course.search.initiated' , { "search_term" : search_term , "page_size" : size , "page_number" : page , } ) results = perform_search ( search_term , user = request . user , size = size , from_ = from_ , course_id = course_id ) status_code = 200 track . emit ( 'edx.course.search.results_displayed' , { "search_term" : search_term , "page_size" : size , "page_number" : page , "results_count" : results [ "total" ] , } ) except ValueError as invalid_err : results = { "error" : six . text_type ( invalid_err ) } log . debug ( six . text_type ( invalid_err ) ) except QueryParseError : results = { "error" : _ ( 'Your query seems malformed. Check for unmatched quotes.' ) } except Exception as err : results = { "error" : _ ( 'An error occurred when searching for "{search_string}"' ) . format ( search_string = search_term ) } log . exception ( 'Search view exception when searching for %s for user %s: %r' , search_term , request . user . id , err ) return JsonResponse ( results , status = status_code )
9850	def export ( self , filename , file_format = None , type = None , typequote = '"' ) : exporter = self . _get_exporter ( filename , file_format = file_format ) exporter ( filename , type = type , typequote = typequote )
6608	def wait ( self ) : sleep = 5 while True : if self . clusterprocids_outstanding : self . poll ( ) if not self . clusterprocids_outstanding : break time . sleep ( sleep ) return self . clusterprocids_finished
10984	def feature_from_pos_rad ( statemaker , pos , rad , im_name = None , tile = None , desc = '' , use_full_path = False , statemaker_kwargs = { } , ** kwargs ) : if np . size ( pos ) == 0 : raise ValueError ( '`pos` is an empty array.' ) elif np . shape ( pos ) [ 1 ] != 3 : raise ValueError ( '`pos` must be an [N,3] element numpy.ndarray.' ) _ , im_name = _pick_state_im_name ( '' , im_name , use_full_path = use_full_path ) im = util . RawImage ( im_name , tile = tile ) s = statemaker ( im , pos , rad , ** statemaker_kwargs ) RLOG . info ( 'State Created.' ) if desc is not None : states . save ( s , desc = desc + 'initial' ) optimize_from_initial ( s , desc = desc , ** kwargs ) return s
8357	def _detectEncoding ( self , xml_data , isHTML = False ) : xml_encoding = sniffed_xml_encoding = None try : if xml_data [ : 4 ] == '\x4c\x6f\xa7\x94' : xml_data = self . _ebcdic_to_ascii ( xml_data ) elif xml_data [ : 4 ] == '\x00\x3c\x00\x3f' : sniffed_xml_encoding = 'utf-16be' xml_data = unicode ( xml_data , 'utf-16be' ) . encode ( 'utf-8' ) elif ( len ( xml_data ) >= 4 ) and ( xml_data [ : 2 ] == '\xfe\xff' ) and ( xml_data [ 2 : 4 ] != '\x00\x00' ) : sniffed_xml_encoding = 'utf-16be' xml_data = unicode ( xml_data [ 2 : ] , 'utf-16be' ) . encode ( 'utf-8' ) elif xml_data [ : 4 ] == '\x3c\x00\x3f\x00' : sniffed_xml_encoding = 'utf-16le' xml_data = unicode ( xml_data , 'utf-16le' ) . encode ( 'utf-8' ) elif ( len ( xml_data ) >= 4 ) and ( xml_data [ : 2 ] == '\xff\xfe' ) and ( xml_data [ 2 : 4 ] != '\x00\x00' ) : sniffed_xml_encoding = 'utf-16le' xml_data = unicode ( xml_data [ 2 : ] , 'utf-16le' ) . encode ( 'utf-8' ) elif xml_data [ : 4 ] == '\x00\x00\x00\x3c' : sniffed_xml_encoding = 'utf-32be' xml_data = unicode ( xml_data , 'utf-32be' ) . encode ( 'utf-8' ) elif xml_data [ : 4 ] == '\x3c\x00\x00\x00' : sniffed_xml_encoding = 'utf-32le' xml_data = unicode ( xml_data , 'utf-32le' ) . encode ( 'utf-8' ) elif xml_data [ : 4 ] == '\x00\x00\xfe\xff' : sniffed_xml_encoding = 'utf-32be' xml_data = unicode ( xml_data [ 4 : ] , 'utf-32be' ) . encode ( 'utf-8' ) elif xml_data [ : 4 ] == '\xff\xfe\x00\x00' : sniffed_xml_encoding = 'utf-32le' xml_data = unicode ( xml_data [ 4 : ] , 'utf-32le' ) . encode ( 'utf-8' ) elif xml_data [ : 3 ] == '\xef\xbb\xbf' : sniffed_xml_encoding = 'utf-8' xml_data = unicode ( xml_data [ 3 : ] , 'utf-8' ) . encode ( 'utf-8' ) else : sniffed_xml_encoding = 'ascii' pass except : xml_encoding_match = None xml_encoding_match = re . compile ( '^<\?.*encoding=[\'"](.*?)[\'"].*\?>' ) . match ( xml_data ) if not xml_encoding_match and isHTML : regexp = re . compile ( '<\s*meta[^>]+charset=([^>]*?)[;\'">]' , re . I ) xml_encoding_match = regexp . search ( xml_data ) if xml_encoding_match is not None : xml_encoding = xml_encoding_match . groups ( ) [ 0 ] . lower ( ) if isHTML : self . declaredHTMLEncoding = xml_encoding if sniffed_xml_encoding and ( xml_encoding in ( 'iso-10646-ucs-2' , 'ucs-2' , 'csunicode' , 'iso-10646-ucs-4' , 'ucs-4' , 'csucs4' , 'utf-16' , 'utf-32' , 'utf_16' , 'utf_32' , 'utf16' , 'u16' ) ) : xml_encoding = sniffed_xml_encoding return xml_data , xml_encoding , sniffed_xml_encoding
6891	def _starfeatures_worker ( task ) : try : ( lcfile , outdir , kdtree , objlist , lcflist , neighbor_radius_arcsec , deredden , custom_bandpasses , lcformat , lcformatdir ) = task return get_starfeatures ( lcfile , outdir , kdtree , objlist , lcflist , neighbor_radius_arcsec , deredden = deredden , custom_bandpasses = custom_bandpasses , lcformat = lcformat , lcformatdir = lcformatdir ) except Exception as e : return None
12225	def convertGribToTiff ( listeFile , listParam , listLevel , liststep , grid , startDate , endDate , outFolder ) : dicoValues = { } for l in listeFile : grbs = pygrib . open ( l ) grbs . seek ( 0 ) index = 1 for j in range ( len ( listLevel ) , 0 , - 1 ) : for i in range ( len ( listParam ) - 1 , - 1 , - 1 ) : grb = grbs [ index ] p = grb . name . replace ( ' ' , '_' ) if grb . level != 0 : l = str ( grb . level ) + '_' + grb . typeOfLevel else : l = grb . typeOfLevel if p + '_' + l not in dicoValues . keys ( ) : dicoValues [ p + '_' + l ] = [ ] dicoValues [ p + '_' + l ] . append ( grb . values ) shape = grb . values . shape lat , lon = grb . latlons ( ) geoparam = ( lon . min ( ) , lat . max ( ) , grid , grid ) index += 1 nbJour = ( endDate - startDate ) . days + 1 for s in range ( 0 , ( len ( liststep ) * nbJour - len ( listeFile ) ) ) : for k in dicoValues . keys ( ) : dicoValues [ k ] . append ( np . full ( shape , np . nan ) ) for i in range ( len ( dicoValues . keys ( ) ) - 1 , - 1 , - 1 ) : dictParam = dict ( ( k , dicoValues [ dicoValues . keys ( ) [ i ] ] [ k ] ) for k in range ( 0 , len ( dicoValues [ dicoValues . keys ( ) [ i ] ] ) ) ) sorted ( dictParam . items ( ) , key = lambda x : x [ 0 ] ) outputImg = outFolder + '/' + dicoValues . keys ( ) [ i ] + '_' + startDate . strftime ( '%Y%M%d' ) + '_' + endDate . strftime ( '%Y%M%d' ) + '.tif' writeTiffFromDicoArray ( dictParam , outputImg , shape , geoparam ) for f in listeFile : os . remove ( f )
704	def _okToExit ( self ) : print >> sys . stderr , "reporter:status:In hypersearchV2: _okToExit" if not self . _jobCancelled : ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( matured = False ) if len ( modelIds ) > 0 : self . logger . info ( "Ready to end hyperseach, but not all models have " "matured yet. Sleeping a bit to wait for all models " "to mature." ) time . sleep ( 5.0 * random . random ( ) ) return False ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( completed = False ) for modelId in modelIds : self . logger . info ( "Stopping model %d because the search has ended" % ( modelId ) ) self . _cjDAO . modelSetFields ( modelId , dict ( engStop = ClientJobsDAO . STOP_REASON_STOPPED ) , ignoreUnchanged = True ) self . _hsStatePeriodicUpdate ( ) pctFieldContributions , absFieldContributions = self . _hsState . getFieldContributions ( ) jobResultsStr = self . _cjDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] if jobResultsStr is not None : jobResults = json . loads ( jobResultsStr ) else : jobResults = { } if pctFieldContributions != jobResults . get ( 'fieldContributions' , None ) : jobResults [ 'fieldContributions' ] = pctFieldContributions jobResults [ 'absoluteFieldContributions' ] = absFieldContributions isUpdated = self . _cjDAO . jobSetFieldIfEqual ( self . _jobID , fieldName = 'results' , curValue = jobResultsStr , newValue = json . dumps ( jobResults ) ) if isUpdated : self . logger . info ( 'Successfully updated the field contributions:%s' , pctFieldContributions ) else : self . logger . info ( 'Failed updating the field contributions, ' 'another hypersearch worker must have updated it' ) return True
559	def setSwarmState ( self , swarmId , newStatus ) : assert ( newStatus in [ 'active' , 'completing' , 'completed' , 'killed' ] ) swarmInfo = self . _state [ 'swarms' ] [ swarmId ] if swarmInfo [ 'status' ] == newStatus : return if swarmInfo [ 'status' ] == 'completed' and newStatus == 'completing' : return self . _dirty = True swarmInfo [ 'status' ] = newStatus if newStatus == 'completed' : ( modelId , errScore ) = self . _hsObj . _resultsDB . bestModelIdAndErrScore ( swarmId ) swarmInfo [ 'bestModelId' ] = modelId swarmInfo [ 'bestErrScore' ] = errScore if newStatus != 'active' and swarmId in self . _state [ 'activeSwarms' ] : self . _state [ 'activeSwarms' ] . remove ( swarmId ) if newStatus == 'killed' : self . _hsObj . killSwarmParticles ( swarmId ) sprintIdx = swarmInfo [ 'sprintIdx' ] self . isSprintActive ( sprintIdx ) sprintInfo = self . _state [ 'sprints' ] [ sprintIdx ] statusCounts = dict ( active = 0 , completing = 0 , completed = 0 , killed = 0 ) bestModelIds = [ ] bestErrScores = [ ] for info in self . _state [ 'swarms' ] . itervalues ( ) : if info [ 'sprintIdx' ] != sprintIdx : continue statusCounts [ info [ 'status' ] ] += 1 if info [ 'status' ] == 'completed' : bestModelIds . append ( info [ 'bestModelId' ] ) bestErrScores . append ( info [ 'bestErrScore' ] ) if statusCounts [ 'active' ] > 0 : sprintStatus = 'active' elif statusCounts [ 'completing' ] > 0 : sprintStatus = 'completing' else : sprintStatus = 'completed' sprintInfo [ 'status' ] = sprintStatus if sprintStatus == 'completed' : if len ( bestErrScores ) > 0 : whichIdx = numpy . array ( bestErrScores ) . argmin ( ) sprintInfo [ 'bestModelId' ] = bestModelIds [ whichIdx ] sprintInfo [ 'bestErrScore' ] = bestErrScores [ whichIdx ] else : sprintInfo [ 'bestModelId' ] = 0 sprintInfo [ 'bestErrScore' ] = numpy . inf bestPrior = numpy . inf for idx in range ( sprintIdx ) : if self . _state [ 'sprints' ] [ idx ] [ 'status' ] == 'completed' : ( _ , errScore ) = self . bestModelInCompletedSprint ( idx ) if errScore is None : errScore = numpy . inf else : errScore = numpy . inf if errScore < bestPrior : bestPrior = errScore if sprintInfo [ 'bestErrScore' ] >= bestPrior : self . _state [ 'lastGoodSprint' ] = sprintIdx - 1 if self . _state [ 'lastGoodSprint' ] is not None and not self . anyGoodSprintsActive ( ) : self . _state [ 'searchOver' ] = True
4834	def get_catalog ( self , catalog_id ) : return self . _load_data ( self . CATALOGS_ENDPOINT , default = [ ] , resource_id = catalog_id )
1572	def setup ( self , context ) : myindex = context . get_partition_index ( ) self . _files_to_consume = self . _files [ myindex : : context . get_num_partitions ( ) ] self . logger . info ( "TextFileSpout files to consume %s" % self . _files_to_consume ) self . _lines_to_consume = self . _get_next_lines ( ) self . _emit_count = 0
5039	def is_user_enrolled ( cls , user , course_id , course_mode ) : enrollment_client = EnrollmentApiClient ( ) try : enrollments = enrollment_client . get_course_enrollment ( user . username , course_id ) if enrollments and course_mode == enrollments . get ( 'mode' ) : return True except HttpClientError as exc : logging . error ( 'Error while checking enrollment status of user %(user)s: %(message)s' , dict ( user = user . username , message = str ( exc ) ) ) except KeyError as exc : logging . warning ( 'Error while parsing enrollment data of user %(user)s: %(message)s' , dict ( user = user . username , message = str ( exc ) ) ) return False
1027	def unhex ( s ) : bits = 0 for c in s : if '0' <= c <= '9' : i = ord ( '0' ) elif 'a' <= c <= 'f' : i = ord ( 'a' ) - 10 elif 'A' <= c <= 'F' : i = ord ( 'A' ) - 10 else : break bits = bits * 16 + ( ord ( c ) - i ) return bits
11282	def append ( self , next ) : next . chained = True if self . next : self . next . append ( next ) else : self . next = next
12908	def load ( cls , fh ) : dat = fh . read ( ) try : ret = cls . from_json ( dat ) except : ret = cls . from_yaml ( dat ) return ret
10322	def spanning_1d_chain ( length ) : ret = nx . grid_graph ( dim = [ int ( length + 2 ) ] ) ret . node [ 0 ] [ 'span' ] = 0 ret [ 0 ] [ 1 ] [ 'span' ] = 0 ret . node [ length + 1 ] [ 'span' ] = 1 ret [ length ] [ length + 1 ] [ 'span' ] = 1 return ret
4863	def to_representation ( self , instance ) : request = self . context [ 'request' ] enterprise_customer = instance . enterprise_customer representation = super ( EnterpriseCustomerCatalogDetailSerializer , self ) . to_representation ( instance ) paginated_content = instance . get_paginated_content ( request . GET ) count = paginated_content [ 'count' ] search_results = paginated_content [ 'results' ] for item in search_results : content_type = item [ 'content_type' ] marketing_url = item . get ( 'marketing_url' ) if marketing_url : item [ 'marketing_url' ] = utils . update_query_parameters ( marketing_url , utils . get_enterprise_utm_context ( enterprise_customer ) ) if content_type == 'course' : item [ 'enrollment_url' ] = instance . get_course_enrollment_url ( item [ 'key' ] ) if content_type == 'courserun' : item [ 'enrollment_url' ] = instance . get_course_run_enrollment_url ( item [ 'key' ] ) if content_type == 'program' : item [ 'enrollment_url' ] = instance . get_program_enrollment_url ( item [ 'uuid' ] ) previous_url = None next_url = None page = int ( request . GET . get ( 'page' , '1' ) ) request_uri = request . build_absolute_uri ( ) if paginated_content [ 'previous' ] : previous_url = utils . update_query_parameters ( request_uri , { 'page' : page - 1 } ) if paginated_content [ 'next' ] : next_url = utils . update_query_parameters ( request_uri , { 'page' : page + 1 } ) representation [ 'count' ] = count representation [ 'previous' ] = previous_url representation [ 'next' ] = next_url representation [ 'results' ] = search_results return representation
2003	def function_call ( type_spec , * args ) : m = re . match ( r"(?P<name>[a-zA-Z_][a-zA-Z_0-9]*)(?P<type>\(.*\))" , type_spec ) if not m : raise EthereumError ( "Function signature expected" ) ABI . _check_and_warn_num_args ( type_spec , * args ) result = ABI . function_selector ( type_spec ) result += ABI . serialize ( m . group ( 'type' ) , * args ) return result
7779	def __from_rfc2426 ( self , data ) : data = from_utf8 ( data ) lines = data . split ( "\n" ) started = 0 current = None for l in lines : if not l : continue if l [ - 1 ] == "\r" : l = l [ : - 1 ] if not l : continue if l [ 0 ] in " \t" : if current is None : continue current += l [ 1 : ] continue if not started and current and current . upper ( ) . strip ( ) == "BEGIN:VCARD" : started = 1 elif started and current . upper ( ) . strip ( ) == "END:VCARD" : current = None break elif current and started : self . _process_rfc2425_record ( current ) current = l if started and current : self . _process_rfc2425_record ( current )
5732	def parse_response ( gdb_mi_text ) : stream = StringStream ( gdb_mi_text , debug = _DEBUG ) if _GDB_MI_NOTIFY_RE . match ( gdb_mi_text ) : token , message , payload = _get_notify_msg_and_payload ( gdb_mi_text , stream ) return { "type" : "notify" , "message" : message , "payload" : payload , "token" : token , } elif _GDB_MI_RESULT_RE . match ( gdb_mi_text ) : token , message , payload = _get_result_msg_and_payload ( gdb_mi_text , stream ) return { "type" : "result" , "message" : message , "payload" : payload , "token" : token , } elif _GDB_MI_CONSOLE_RE . match ( gdb_mi_text ) : return { "type" : "console" , "message" : None , "payload" : _GDB_MI_CONSOLE_RE . match ( gdb_mi_text ) . groups ( ) [ 0 ] , } elif _GDB_MI_LOG_RE . match ( gdb_mi_text ) : return { "type" : "log" , "message" : None , "payload" : _GDB_MI_LOG_RE . match ( gdb_mi_text ) . groups ( ) [ 0 ] , } elif _GDB_MI_TARGET_OUTPUT_RE . match ( gdb_mi_text ) : return { "type" : "target" , "message" : None , "payload" : _GDB_MI_TARGET_OUTPUT_RE . match ( gdb_mi_text ) . groups ( ) [ 0 ] , } elif response_is_finished ( gdb_mi_text ) : return { "type" : "done" , "message" : None , "payload" : None } else : return { "type" : "output" , "message" : None , "payload" : gdb_mi_text }
1923	def binary_arch ( binary ) : with open ( binary , 'rb' ) as f : elffile = ELFFile ( f ) if elffile [ 'e_machine' ] == 'EM_X86_64' : return True else : return False
11723	def init_app ( self , app , ** kwargs ) : self . init_config ( app ) self . limiter = Limiter ( app , key_func = get_ipaddr ) if app . config [ 'APP_ENABLE_SECURE_HEADERS' ] : self . talisman = Talisman ( app , ** app . config . get ( 'APP_DEFAULT_SECURE_HEADERS' , { } ) ) if app . config [ 'APP_HEALTH_BLUEPRINT_ENABLED' ] : blueprint = Blueprint ( 'invenio_app_ping' , __name__ ) @ blueprint . route ( '/ping' ) def ping ( ) : return 'OK' ping . talisman_view_options = { 'force_https' : False } app . register_blueprint ( blueprint ) requestid_header = app . config . get ( 'APP_REQUESTID_HEADER' ) if requestid_header : @ app . before_request def set_request_id ( ) : request_id = request . headers . get ( requestid_header ) if request_id : g . request_id = request_id [ : 200 ] try : from flask_debugtoolbar import DebugToolbarExtension app . extensions [ 'flask-debugtoolbar' ] = DebugToolbarExtension ( app ) except ImportError : app . logger . debug ( 'Flask-DebugToolbar extension not installed.' ) app . extensions [ 'invenio-app' ] = self
7050	def _reform_templatelc_for_tfa ( task ) : try : ( lcfile , lcformat , lcformatdir , tcol , mcol , ecol , timebase , interpolate_type , sigclip ) = task try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None lcdict = readerfunc ( lcfile ) if ( ( isinstance ( lcdict , ( list , tuple ) ) ) and ( isinstance ( lcdict [ 0 ] , dict ) ) ) : lcdict = lcdict [ 0 ] outdict = { } if '.' in tcol : tcolget = tcol . split ( '.' ) else : tcolget = [ tcol ] times = _dict_get ( lcdict , tcolget ) if '.' in mcol : mcolget = mcol . split ( '.' ) else : mcolget = [ mcol ] mags = _dict_get ( lcdict , mcolget ) if '.' in ecol : ecolget = ecol . split ( '.' ) else : ecolget = [ ecol ] errs = _dict_get ( lcdict , ecolget ) if normfunc is None : ntimes , nmags = normalize_magseries ( times , mags , magsarefluxes = magsarefluxes ) times , mags , errs = ntimes , nmags , errs stimes , smags , serrs = sigclip_magseries ( times , mags , errs , sigclip = sigclip ) mags_interpolator = spi . interp1d ( stimes , smags , kind = interpolate_type , fill_value = 'extrapolate' ) errs_interpolator = spi . interp1d ( stimes , serrs , kind = interpolate_type , fill_value = 'extrapolate' ) interpolated_mags = mags_interpolator ( timebase ) interpolated_errs = errs_interpolator ( timebase ) magmedian = np . median ( interpolated_mags ) renormed_mags = interpolated_mags - magmedian outdict = { 'mags' : renormed_mags , 'errs' : interpolated_errs , 'origmags' : interpolated_mags } return outdict except Exception as e : LOGEXCEPTION ( 'reform LC task failed: %s' % repr ( task ) ) return None
5025	def get_channel_classes ( channel_code ) : if channel_code : channel_code = channel_code . upper ( ) if channel_code not in INTEGRATED_CHANNEL_CHOICES : raise CommandError ( _ ( 'Invalid integrated channel: {channel}' ) . format ( channel = channel_code ) ) channel_classes = [ INTEGRATED_CHANNEL_CHOICES [ channel_code ] ] else : channel_classes = INTEGRATED_CHANNEL_CHOICES . values ( ) return channel_classes
1013	def _getBestMatchingCell ( self , c , activeState , minThreshold ) : bestActivityInCol = minThreshold bestSegIdxInCol = - 1 bestCellInCol = - 1 for i in xrange ( self . cellsPerColumn ) : maxSegActivity = 0 maxSegIdx = 0 for j , s in enumerate ( self . cells [ c ] [ i ] ) : activity = self . _getSegmentActivityLevel ( s , activeState ) if activity > maxSegActivity : maxSegActivity = activity maxSegIdx = j if maxSegActivity >= bestActivityInCol : bestActivityInCol = maxSegActivity bestSegIdxInCol = maxSegIdx bestCellInCol = i if bestCellInCol == - 1 : return ( None , None , None ) else : return ( bestCellInCol , self . cells [ c ] [ bestCellInCol ] [ bestSegIdxInCol ] , bestActivityInCol )
8952	def get_project_root ( ) : try : tasks_py = sys . modules [ 'tasks' ] except KeyError : return None else : return os . path . abspath ( os . path . dirname ( tasks_py . __file__ ) )
8528	def get_ip_packet ( data , client_port , server_port , is_loopback = False ) : header = _loopback if is_loopback else _ethernet try : header . unpack ( data ) except Exception as ex : raise ValueError ( 'Bad header: %s' % ex ) tcp_p = getattr ( header . data , 'data' , None ) if type ( tcp_p ) != dpkt . tcp . TCP : raise ValueError ( 'Not a TCP packet' ) if tcp_p . dport == server_port : if client_port != 0 and tcp_p . sport != client_port : raise ValueError ( 'Request from different client' ) elif tcp_p . sport == server_port : if client_port != 0 and tcp_p . dport != client_port : raise ValueError ( 'Reply for different client' ) else : raise ValueError ( 'Packet not for/from client/server' ) return header . data
1170	def _format_text ( self , text ) : text_width = max ( self . width - self . current_indent , 11 ) indent = " " * self . current_indent return textwrap . fill ( text , text_width , initial_indent = indent , subsequent_indent = indent )
3375	def add_absolute_expression ( model , expression , name = "abs_var" , ub = None , difference = 0 , add = True ) : Components = namedtuple ( 'Components' , [ 'variable' , 'upper_constraint' , 'lower_constraint' ] ) variable = model . problem . Variable ( name , lb = 0 , ub = ub ) upper_constraint = model . problem . Constraint ( expression - variable , ub = difference , name = "abs_pos_" + name ) , lower_constraint = model . problem . Constraint ( expression + variable , lb = difference , name = "abs_neg_" + name ) to_add = Components ( variable , upper_constraint , lower_constraint ) if add : add_cons_vars_to_problem ( model , to_add ) return to_add
7685	def clicks ( annotation , sr = 22050 , length = None , ** kwargs ) : interval , _ = annotation . to_interval_values ( ) return filter_kwargs ( mir_eval . sonify . clicks , interval [ : , 0 ] , fs = sr , length = length , ** kwargs )
3548	def _descriptor_changed ( self , descriptor ) : desc = descriptor_list ( ) . get ( descriptor ) if desc is not None : desc . _value_read . set ( )
4503	def SPI ( ledtype = None , num = 0 , ** kwargs ) : from . . . project . types . ledtype import make if ledtype is None : raise ValueError ( 'Must provide ledtype value!' ) ledtype = make ( ledtype ) if num == 0 : raise ValueError ( 'Must provide num value >0!' ) if ledtype not in SPI_DRIVERS . keys ( ) : raise ValueError ( '{} is not a valid LED type.' . format ( ledtype ) ) return SPI_DRIVERS [ ledtype ] ( num , ** kwargs )
4791	def is_upper ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if self . val != self . val . upper ( ) : self . _err ( 'Expected <%s> to contain only uppercase chars, but did not.' % self . val ) return self
1141	def dedent ( text ) : margin = None text = _whitespace_only_re . sub ( '' , text ) indents = _leading_whitespace_re . findall ( text ) for indent in indents : if margin is None : margin = indent elif indent . startswith ( margin ) : pass elif margin . startswith ( indent ) : margin = indent else : for i , ( x , y ) in enumerate ( zip ( margin , indent ) ) : if x != y : margin = margin [ : i ] break else : margin = margin [ : len ( indent ) ] if 0 and margin : for line in text . split ( "\n" ) : assert not line or line . startswith ( margin ) , "line = %r, margin = %r" % ( line , margin ) if margin : text = re . sub ( r'(?m)^' + margin , '' , text ) return text
4523	def color_scale ( color , level ) : return tuple ( [ int ( i * level ) >> 8 for i in list ( color ) ] )
10409	def finalized_canonical_averages_dtype ( spanning_cluster = True ) : fields = list ( ) fields . extend ( [ ( 'number_of_runs' , 'uint32' ) , ( 'p' , 'float64' ) , ( 'alpha' , 'float64' ) , ] ) if spanning_cluster : fields . extend ( [ ( 'percolation_probability_mean' , 'float64' ) , ( 'percolation_probability_std' , 'float64' ) , ( 'percolation_probability_ci' , '(2,)float64' ) , ] ) fields . extend ( [ ( 'percolation_strength_mean' , 'float64' ) , ( 'percolation_strength_std' , 'float64' ) , ( 'percolation_strength_ci' , '(2,)float64' ) , ( 'moments_mean' , '(5,)float64' ) , ( 'moments_std' , '(5,)float64' ) , ( 'moments_ci' , '(5,2)float64' ) , ] ) return _ndarray_dtype ( fields )
1231	def tf_import_experience ( self , states , internals , actions , terminal , reward ) : return self . memory . store ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
12739	def create_joints ( self ) : stack = [ 'root' ] while stack : parent = stack . pop ( ) for child in self . hierarchy . get ( parent , ( ) ) : stack . append ( child ) if parent not in self . bones : continue bone = self . bones [ parent ] body = [ b for b in self . bodies if b . name == parent ] [ 0 ] for child in self . hierarchy . get ( parent , ( ) ) : child_bone = self . bones [ child ] child_body = [ b for b in self . bodies if b . name == child ] [ 0 ] shape = ( '' , 'hinge' , 'universal' , 'ball' ) [ len ( child_bone . dof ) ] self . joints . append ( self . world . join ( shape , body , child_body ) )
10680	def Cp_mag ( self , T ) : tau = T / self . Tc_mag if tau <= 1.0 : c = ( self . _B_mag * ( 2 * tau ** 3 + 2 * tau ** 9 / 3 + 2 * tau ** 15 / 5 ) ) / self . _D_mag else : c = ( 2 * tau ** - 5 + 2 * tau ** - 15 / 3 + 2 * tau ** - 25 / 5 ) / self . _D_mag result = R * math . log ( self . beta0_mag + 1 ) * c return result
6281	def keyboard_event ( self , key , action , modifier ) : if key == self . keys . ESCAPE : self . close ( ) return if key == self . keys . SPACE and action == self . keys . ACTION_PRESS : self . timer . toggle_pause ( ) if key == self . keys . D : if action == self . keys . ACTION_PRESS : self . sys_camera . move_right ( True ) elif action == self . keys . ACTION_RELEASE : self . sys_camera . move_right ( False ) elif key == self . keys . A : if action == self . keys . ACTION_PRESS : self . sys_camera . move_left ( True ) elif action == self . keys . ACTION_RELEASE : self . sys_camera . move_left ( False ) elif key == self . keys . W : if action == self . keys . ACTION_PRESS : self . sys_camera . move_forward ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_forward ( False ) elif key == self . keys . S : if action == self . keys . ACTION_PRESS : self . sys_camera . move_backward ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_backward ( False ) elif key == self . keys . Q : if action == self . keys . ACTION_PRESS : self . sys_camera . move_down ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_down ( False ) elif key == self . keys . E : if action == self . keys . ACTION_PRESS : self . sys_camera . move_up ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_up ( False ) if key == self . keys . X and action == self . keys . ACTION_PRESS : screenshot . create ( ) if key == self . keys . R and action == self . keys . ACTION_PRESS : project . instance . reload_programs ( ) if key == self . keys . RIGHT and action == self . keys . ACTION_PRESS : self . timer . set_time ( self . timer . get_time ( ) + 10.0 ) if key == self . keys . LEFT and action == self . keys . ACTION_PRESS : self . timer . set_time ( self . timer . get_time ( ) - 10.0 ) self . timeline . key_event ( key , action , modifier )
5327	def sha_github_file ( cls , config , repo_file , repository_api , repository_branch ) : repo_file_sha = None cfg = config . get_conf ( ) github_token = cfg [ 'sortinghat' ] [ 'identities_api_token' ] headers = { "Authorization" : "token " + github_token } url_dir = repository_api + "/git/trees/" + repository_branch logger . debug ( "Gettting sha data from tree: %s" , url_dir ) raw_repo_file_info = requests . get ( url_dir , headers = headers ) raw_repo_file_info . raise_for_status ( ) for rfile in raw_repo_file_info . json ( ) [ 'tree' ] : if rfile [ 'path' ] == repo_file : logger . debug ( "SHA found: %s, " , rfile [ "sha" ] ) repo_file_sha = rfile [ "sha" ] break return repo_file_sha
8199	def transform_from_local ( xp , yp , cphi , sphi , mx , my ) : x = xp * cphi - yp * sphi + mx y = xp * sphi + yp * cphi + my return ( x , y )
5910	def delete_frames ( self ) : for frame in glob . glob ( self . frameglob ) : os . unlink ( frame )
1257	def create_atomic_observe_operations ( self , states , actions , internals , terminal , reward , index ) : num_episodes = tf . count_nonzero ( input_tensor = terminal , dtype = util . tf_dtype ( 'int' ) ) increment_episode = tf . assign_add ( ref = self . episode , value = tf . to_int64 ( x = num_episodes ) ) increment_global_episode = tf . assign_add ( ref = self . global_episode , value = tf . to_int64 ( x = num_episodes ) ) with tf . control_dependencies ( control_inputs = ( increment_episode , increment_global_episode ) ) : states = util . map_tensors ( fn = tf . stop_gradient , tensors = states ) internals = util . map_tensors ( fn = tf . stop_gradient , tensors = internals ) actions = util . map_tensors ( fn = tf . stop_gradient , tensors = actions ) terminal = tf . stop_gradient ( input = terminal ) reward = tf . stop_gradient ( input = reward ) observation = self . fn_observe_timestep ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) with tf . control_dependencies ( control_inputs = ( observation , ) ) : self . unbuffered_episode_output = self . global_episode + 0
5920	def fit ( self , xy = False , ** kwargs ) : kwargs . setdefault ( 's' , self . tpr ) kwargs . setdefault ( 'n' , self . ndx ) kwargs [ 'f' ] = self . xtc force = kwargs . pop ( 'force' , self . force ) if xy : fitmode = 'rotxy+transxy' kwargs . pop ( 'fit' , None ) infix_default = '_fitxy' else : fitmode = kwargs . pop ( 'fit' , 'rot+trans' ) infix_default = '_fit' dt = kwargs . get ( 'dt' ) if dt : infix_default += '_dt{0:d}ps' . format ( int ( dt ) ) kwargs . setdefault ( 'o' , self . outfile ( self . infix_filename ( None , self . xtc , infix_default , 'xtc' ) ) ) fitgroup = kwargs . pop ( 'fitgroup' , 'backbone' ) kwargs . setdefault ( 'input' , [ fitgroup , "system" ] ) if kwargs . get ( 'center' , False ) : logger . warn ( "Transformer.fit(): center=%(center)r used: centering should not be combined with fitting." , kwargs ) if len ( kwargs [ 'inputs' ] ) != 3 : logger . error ( "If you insist on centering you must provide three groups in the 'input' kwarg: (center, fit, output)" ) raise ValuError ( "Insufficient index groups for centering,fitting,output" ) logger . info ( "Fitting trajectory %r to with xy=%r..." , kwargs [ 'f' ] , xy ) logger . info ( "Fitting on index group %(fitgroup)r" , vars ( ) ) with utilities . in_dir ( self . dirname ) : if self . check_file_exists ( kwargs [ 'o' ] , resolve = "indicate" , force = force ) : logger . warn ( "File %r exists; force regenerating it with force=True." , kwargs [ 'o' ] ) else : gromacs . trjconv ( fit = fitmode , ** kwargs ) logger . info ( "Fitted trajectory (fitmode=%s): %r." , fitmode , kwargs [ 'o' ] ) return { 'tpr' : self . rp ( kwargs [ 's' ] ) , 'xtc' : self . rp ( kwargs [ 'o' ] ) }
11700	def spawn ( self , generations ) : egg_donors = [ god for god in self . gods . values ( ) if god . chromosomes == 'XX' ] sperm_donors = [ god for god in self . gods . values ( ) if god . chromosomes == 'XY' ] for i in range ( generations ) : print ( "\nGENERATION %d\n" % ( i + 1 ) ) gen_xx = [ ] gen_xy = [ ] for egg_donor in egg_donors : sperm_donor = random . choice ( sperm_donors ) brood = self . breed ( egg_donor , sperm_donor ) for child in brood : if child . divinity > human : self . add_god ( child ) if child . chromosomes == 'XX' : gen_xx . append ( child ) else : gen_xy . append ( child ) egg_donors = [ ed for ed in egg_donors if ed . generation > ( i - 2 ) ] sperm_donors = [ sd for sd in sperm_donors if sd . generation > ( i - 3 ) ] egg_donors += gen_xx sperm_donors += gen_xy
5040	def get_users_by_email ( cls , emails ) : users = User . objects . filter ( email__in = emails ) present_emails = users . values_list ( 'email' , flat = True ) missing_emails = list ( set ( emails ) - set ( present_emails ) ) return users , missing_emails
125	def Positive ( other_param , mode = "invert" , reroll_count_max = 2 ) : return ForceSign ( other_param = other_param , positive = True , mode = mode , reroll_count_max = reroll_count_max )
5954	def tool_factory ( clsname , name , driver , base = GromacsCommand ) : clsdict = { 'command_name' : name , 'driver' : driver , '__doc__' : property ( base . _get_gmx_docs ) } return type ( clsname , ( base , ) , clsdict )
7511	def select_samples ( dbsamples , samples , pidx = None ) : samples = [ i . name for i in samples ] if pidx : sidx = [ list ( dbsamples [ pidx ] ) . index ( i ) for i in samples ] else : sidx = [ list ( dbsamples ) . index ( i ) for i in samples ] sidx . sort ( ) return sidx
13683	def get ( self , url , params = { } ) : params . update ( { 'api_key' : self . api_key } ) try : response = requests . get ( self . host + url , params = params ) except RequestException as e : response = e . args return self . json_parse ( response . content )
12782	def set_topic ( self , topic ) : if not topic : topic = '' result = self . _connection . put ( "room/%s" % self . id , { "room" : { "topic" : topic } } ) if result [ "success" ] : self . _load ( ) return result [ "success" ]
11547	def url ( self ) : if len ( self . drivers ) > 0 : return self . drivers [ 0 ] . url else : return self . _url
800	def modelsGetFieldsForJob ( self , jobID , fields , ignoreKilled = False ) : assert len ( fields ) >= 1 , 'fields is empty' dbFields = [ self . _models . pubToDBNameDict [ x ] for x in fields ] dbFieldsStr = ',' . join ( dbFields ) query = 'SELECT model_id, %s FROM %s ' ' WHERE job_id=%%s ' % ( dbFieldsStr , self . modelsTableName ) sqlParams = [ jobID ] if ignoreKilled : query += ' AND (completion_reason IS NULL OR completion_reason != %s)' sqlParams . append ( self . CMPL_REASON_KILLED ) with ConnectionFactory . get ( ) as conn : conn . cursor . execute ( query , sqlParams ) rows = conn . cursor . fetchall ( ) if rows is None : self . _logger . error ( "Unexpected None result from cursor.fetchall; " "query=%r; Traceback=%r" , query , traceback . format_exc ( ) ) return [ ( r [ 0 ] , list ( r [ 1 : ] ) ) for r in rows ]
1582	def read ( self , dispatcher ) : try : if not self . is_header_read : to_read = HeronProtocol . HEADER_SIZE - len ( self . header ) self . header += dispatcher . recv ( to_read ) if len ( self . header ) == HeronProtocol . HEADER_SIZE : self . is_header_read = True else : Log . debug ( "Header read incomplete; read %d bytes of header" % len ( self . header ) ) return if self . is_header_read and not self . is_complete : to_read = self . get_datasize ( ) - len ( self . data ) self . data += dispatcher . recv ( to_read ) if len ( self . data ) == self . get_datasize ( ) : self . is_complete = True except socket . error as e : if e . errno == socket . errno . EAGAIN or e . errno == socket . errno . EWOULDBLOCK : Log . debug ( "Try again error" ) else : Log . debug ( "Fatal error when reading IncomingPacket" ) raise RuntimeError ( "Fatal error occured in IncomingPacket.read()" )
11933	def load_widgets ( context , ** kwargs ) : _soft = kwargs . pop ( '_soft' , False ) try : widgets = context . render_context [ WIDGET_CONTEXT_KEY ] except KeyError : widgets = context . render_context [ WIDGET_CONTEXT_KEY ] = { } for alias , template_name in kwargs . items ( ) : if _soft and alias in widgets : continue with context . render_context . push ( { BLOCK_CONTEXT_KEY : BlockContext ( ) } ) : blocks = resolve_blocks ( template_name , context ) widgets [ alias ] = blocks return ''
2535	def set_doc_comment ( self , doc , comment ) : if not self . doc_comment_set : self . doc_comment_set = True doc . comment = comment else : raise CardinalityError ( 'Document::Comment' )
3553	def start_scan ( self , timeout_sec = TIMEOUT_SEC ) : get_provider ( ) . _central_manager . scanForPeripheralsWithServices_options_ ( None , None ) self . _is_scanning = True
9502	def intersection ( l1 , l2 ) : if len ( l1 ) == 0 or len ( l2 ) == 0 : return [ ] out = [ ] l2_pos = 0 for l in l1 : while l2_pos < len ( l2 ) and l2 [ l2_pos ] . end < l . start : l2_pos += 1 if l2_pos == len ( l2 ) : break while l2_pos < len ( l2 ) and l . intersects ( l2 [ l2_pos ] ) : out . append ( l . intersection ( l2 [ l2_pos ] ) ) l2_pos += 1 l2_pos = max ( 0 , l2_pos - 1 ) return out
12891	def handle_int ( self , item ) : doc = yield from self . handle_get ( item ) if doc is None : return None return int ( doc . value . u8 . text ) or None
10834	def query_admins_by_group_ids ( cls , groups_ids = None ) : assert groups_ids is None or isinstance ( groups_ids , list ) query = db . session . query ( Group . id , func . count ( GroupAdmin . id ) ) . join ( GroupAdmin ) . group_by ( Group . id ) if groups_ids : query = query . filter ( Group . id . in_ ( groups_ids ) ) return query
4174	def window_kaiser ( N , beta = 8.6 , method = 'numpy' ) : r if N == 1 : return ones ( 1 ) if method == 'numpy' : from numpy import kaiser return kaiser ( N , beta ) else : return _kaiser ( N , beta )
834	def run ( self ) : print "-" * 80 + "Computing the SDR" + "-" * 80 self . sp . compute ( self . inputArray , True , self . activeArray ) print self . activeArray . nonzero ( )
13510	def pyflakes ( ) : packages = [ x for x in options . setup . packages if '.' not in x ] sh ( 'pyflakes {param} {files}' . format ( param = options . paved . pycheck . pyflakes . param , files = ' ' . join ( packages ) ) )
11493	def list_user_folders ( self , token ) : parameters = dict ( ) parameters [ 'token' ] = token response = self . request ( 'midas.user.folders' , parameters ) return response
1218	def save ( self , sess , save_path , timestep = None ) : if self . _saver is None : raise TensorForceError ( "register_saver_ops should be called before save" ) return self . _saver . save ( sess = sess , save_path = save_path , global_step = timestep , write_meta_graph = False , write_state = True , )
12208	def add ( self , * entries ) : for entry in entries : if isinstance ( entry , string_types ) : self . _add_entries ( database . parse_string ( entry , bib_format = 'bibtex' ) ) else : self . _add_entries ( entry )
13700	def init_app ( self , app ) : app . config . setdefault ( "TRACY_REQUIRE_CLIENT" , False ) if not hasattr ( app , 'extensions' ) : app . extensions = { } app . extensions [ 'restpoints' ] = self app . before_request ( self . _before ) app . after_request ( self . _after )
3018	def _generate_assertion ( self ) : now = int ( time . time ( ) ) payload = { 'aud' : self . token_uri , 'scope' : self . _scopes , 'iat' : now , 'exp' : now + self . MAX_TOKEN_LIFETIME_SECS , 'iss' : self . _service_account_email , } payload . update ( self . _kwargs ) return crypt . make_signed_jwt ( self . _signer , payload , key_id = self . _private_key_id )
1439	def update_sent_packet ( self , sent_pkt_size_bytes ) : self . update_count ( self . SENT_PKT_COUNT ) self . update_count ( self . SENT_PKT_SIZE , incr_by = sent_pkt_size_bytes )
7829	def add_option ( self , value , label ) : if type ( value ) is list : warnings . warn ( ".add_option() accepts single value now." , DeprecationWarning , stacklevel = 1 ) value = value [ 0 ] if self . type not in ( "list-multi" , "list-single" ) : raise ValueError ( "Options are allowed only for list types." ) option = Option ( value , label ) self . options . append ( option ) return option
6323	def ac_encode ( text , probs ) : coder = Arithmetic ( ) coder . set_probs ( probs ) return coder . encode ( text )
13506	def get_position ( self , position_id ) : url = "/2/positions/%s" % position_id return self . position_from_json ( self . _get_resource ( url ) [ "position" ] )
845	def _calcDistance ( self , inputPattern , distanceNorm = None ) : if distanceNorm is None : distanceNorm = self . distanceNorm if self . useSparseMemory : if self . _protoSizes is None : self . _protoSizes = self . _Memory . rowSums ( ) overlapsWithProtos = self . _Memory . rightVecSumAtNZ ( inputPattern ) inputPatternSum = inputPattern . sum ( ) if self . distanceMethod == "rawOverlap" : dist = inputPattern . sum ( ) - overlapsWithProtos elif self . distanceMethod == "pctOverlapOfInput" : dist = inputPatternSum - overlapsWithProtos if inputPatternSum > 0 : dist /= inputPatternSum elif self . distanceMethod == "pctOverlapOfProto" : overlapsWithProtos /= self . _protoSizes dist = 1.0 - overlapsWithProtos elif self . distanceMethod == "pctOverlapOfLarger" : maxVal = numpy . maximum ( self . _protoSizes , inputPatternSum ) if maxVal . all ( ) > 0 : overlapsWithProtos /= maxVal dist = 1.0 - overlapsWithProtos elif self . distanceMethod == "norm" : dist = self . _Memory . vecLpDist ( self . distanceNorm , inputPattern ) distMax = dist . max ( ) if distMax > 0 : dist /= distMax else : raise RuntimeError ( "Unimplemented distance method %s" % self . distanceMethod ) else : if self . distanceMethod == "norm" : dist = numpy . power ( numpy . abs ( self . _M - inputPattern ) , self . distanceNorm ) dist = dist . sum ( 1 ) dist = numpy . power ( dist , 1.0 / self . distanceNorm ) dist /= dist . max ( ) else : raise RuntimeError ( "Not implemented yet for dense storage...." ) return dist
8308	def ensure_pycairo_context ( self , ctx ) : if self . cairocffi and isinstance ( ctx , self . cairocffi . Context ) : from shoebot . util . cairocffi . cairocffi_to_pycairo import _UNSAFE_cairocffi_context_to_pycairo return _UNSAFE_cairocffi_context_to_pycairo ( ctx ) else : return ctx
11115	def save ( self ) : repoInfoPath = os . path . join ( self . __path , ".pyrepinfo" ) try : fdinfo = open ( repoInfoPath , 'wb' ) except Exception as e : raise Exception ( "unable to open repository info for saving (%s)" % e ) try : pickle . dump ( self , fdinfo , protocol = 2 ) except Exception as e : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) raise Exception ( "Unable to save repository info (%s)" % e ) finally : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) repoTimePath = os . path . join ( self . __path , ".pyrepstate" ) try : self . __state = ( "%.6f" % time . time ( ) ) . encode ( ) with open ( repoTimePath , 'wb' ) as fdtime : fdtime . write ( self . __state ) fdtime . flush ( ) os . fsync ( fdtime . fileno ( ) ) except Exception as e : raise Exception ( "unable to open repository time stamp for saving (%s)" % e )
11510	def get_item_metadata ( self , item_id , token = None , revision = None ) : parameters = dict ( ) parameters [ 'id' ] = item_id if token : parameters [ 'token' ] = token if revision : parameters [ 'revision' ] = revision response = self . request ( 'midas.item.getmetadata' , parameters ) return response
6147	def freqz_cas ( sos , w ) : Ns , Mcol = sos . shape w , Hcas = signal . freqz ( sos [ 0 , : 3 ] , sos [ 0 , 3 : ] , w ) for k in range ( 1 , Ns ) : w , Htemp = signal . freqz ( sos [ k , : 3 ] , sos [ k , 3 : ] , w ) Hcas *= Htemp return w , Hcas
3251	def save ( self , obj , content_type = "application/xml" ) : rest_url = obj . href data = obj . message ( ) headers = { "Content-type" : content_type , "Accept" : content_type } logger . debug ( "{} {}" . format ( obj . save_method , obj . href ) ) resp = self . http_request ( rest_url , method = obj . save_method . lower ( ) , data = data , headers = headers ) if resp . status_code not in ( 200 , 201 ) : raise FailedRequestError ( 'Failed to save to Geoserver catalog: {}, {}' . format ( resp . status_code , resp . text ) ) self . _cache . clear ( ) return resp
1725	def eval ( self , expression , use_compilation_plan = False ) : code = 'PyJsEvalResult = eval(%s)' % json . dumps ( expression ) self . execute ( code , use_compilation_plan = use_compilation_plan ) return self [ 'PyJsEvalResult' ]
4550	def fill_round_rect ( setter , x , y , w , h , r , color = None , aa = False ) : fill_rect ( setter , x + r , y , w - 2 * r , h , color , aa ) _fill_circle_helper ( setter , x + w - r - 1 , y + r , r , 1 , h - 2 * r - 1 , color , aa ) _fill_circle_helper ( setter , x + r , y + r , r , 2 , h - 2 * r - 1 , color , aa )
6541	def parse_python_file ( filepath ) : with _AST_CACHE_LOCK : if filepath not in _AST_CACHE : source = read_file ( filepath ) _AST_CACHE [ filepath ] = ast . parse ( source , filename = filepath ) return _AST_CACHE [ filepath ]
6666	def update_merge ( d , u ) : import collections for k , v in u . items ( ) : if isinstance ( v , collections . Mapping ) : r = update_merge ( d . get ( k , dict ( ) ) , v ) d [ k ] = r else : d [ k ] = u [ k ] return d
8251	def swatch ( self , x , y , w = 35 , h = 35 , roundness = 0 ) : _ctx . fill ( self ) _ctx . rect ( x , y , w , h , roundness )
4462	def transpose ( label , n_semitones ) : match = re . match ( six . text_type ( '(?P<note>[A-G][b#]*)(?P<mod>.*)' ) , six . text_type ( label ) ) if not match : return label note = match . group ( 'note' ) new_note = librosa . midi_to_note ( librosa . note_to_midi ( note ) + n_semitones , octave = False ) return new_note + match . group ( 'mod' )
2028	def CALLDATALOAD ( self , offset ) : if issymbolic ( offset ) : if solver . can_be_true ( self . _constraints , offset == self . _used_calldata_size ) : self . constraints . add ( offset == self . _used_calldata_size ) raise ConcretizeArgument ( 1 , policy = 'SAMPLED' ) self . _use_calldata ( offset , 32 ) data_length = len ( self . data ) bytes = [ ] for i in range ( 32 ) : try : c = Operators . ITEBV ( 8 , offset + i < data_length , self . data [ offset + i ] , 0 ) except IndexError : c = 0 bytes . append ( c ) return Operators . CONCAT ( 256 , * bytes )
6667	def check_version ( ) : global CHECK_VERSION if not CHECK_VERSION : return CHECK_VERSION = 0 from six . moves . urllib . request import urlopen try : response = urlopen ( "https://pypi.org/pypi/burlap/json" ) data = json . loads ( response . read ( ) . decode ( ) ) remote_release = sorted ( tuple ( map ( int , _ . split ( '.' ) ) ) for _ in data [ 'releases' ] . keys ( ) ) [ - 1 ] remote_release_str = '.' . join ( map ( str , remote_release ) ) local_release = VERSION local_release_str = '.' . join ( map ( str , local_release ) ) if remote_release > local_release : print ( '\033[93m' ) print ( "You are using burlap version %s, however version %s is available." % ( local_release_str , remote_release_str ) ) print ( "You should consider upgrading via the 'pip install --upgrade burlap' command." ) print ( '\033[0m' ) except Exception as exc : print ( '\033[93m' ) print ( "Unable to check for updated burlap version: %s" % exc ) print ( '\033[0m' )
390	def keypoint_random_crop ( image , annos , mask = None , size = ( 368 , 368 ) ) : _target_height = size [ 0 ] _target_width = size [ 1 ] target_size = ( _target_width , _target_height ) if len ( np . shape ( image ) ) == 2 : image = cv2 . cvtColor ( image , cv2 . COLOR_GRAY2RGB ) height , width , _ = np . shape ( image ) for _ in range ( 50 ) : x = random . randrange ( 0 , width - target_size [ 0 ] ) if width > target_size [ 0 ] else 0 y = random . randrange ( 0 , height - target_size [ 1 ] ) if height > target_size [ 1 ] else 0 for joint in annos : if x <= joint [ 0 ] [ 0 ] < x + target_size [ 0 ] and y <= joint [ 0 ] [ 1 ] < y + target_size [ 1 ] : break def pose_crop ( image , annos , mask , x , y , w , h ) : target_size = ( w , h ) img = image resized = img [ y : y + target_size [ 1 ] , x : x + target_size [ 0 ] , : ] resized_mask = mask [ y : y + target_size [ 1 ] , x : x + target_size [ 0 ] ] adjust_joint_list = [ ] for joint in annos : adjust_joint = [ ] for point in joint : if point [ 0 ] < - 10 or point [ 1 ] < - 10 : adjust_joint . append ( ( - 1000 , - 1000 ) ) continue new_x , new_y = point [ 0 ] - x , point [ 1 ] - y if new_x > w - 1 or new_y > h - 1 : adjust_joint . append ( ( - 1000 , - 1000 ) ) continue adjust_joint . append ( ( new_x , new_y ) ) adjust_joint_list . append ( adjust_joint ) return resized , adjust_joint_list , resized_mask return pose_crop ( image , annos , mask , x , y , target_size [ 0 ] , target_size [ 1 ] )
7161	def go_back ( self , n = 1 ) : if not self . can_go_back : return N = max ( len ( self . answers ) - abs ( n ) , 0 ) self . answers = OrderedDict ( islice ( self . answers . items ( ) , N ) )
12476	def ux_file_len ( filepath ) : p = subprocess . Popen ( [ 'wc' , '-l' , filepath ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) result , err = p . communicate ( ) if p . returncode != 0 : raise IOError ( err ) l = result . strip ( ) l = int ( l . split ( ) [ 0 ] ) return l
12224	def convertShpToExtend ( pathToShp ) : driver = ogr . GetDriverByName ( 'ESRI Shapefile' ) dataset = driver . Open ( pathToShp ) if dataset is not None : layer = dataset . GetLayer ( ) spatialRef = layer . GetSpatialRef ( ) feature = layer . GetNextFeature ( ) geom = feature . GetGeometryRef ( ) spatialRef = geom . GetSpatialReference ( ) outSpatialRef = osr . SpatialReference ( ) outSpatialRef . ImportFromEPSG ( 4326 ) coordTrans = osr . CoordinateTransformation ( spatialRef , outSpatialRef ) env = geom . GetEnvelope ( ) pointMAX = ogr . Geometry ( ogr . wkbPoint ) pointMAX . AddPoint ( env [ 1 ] , env [ 3 ] ) pointMAX . Transform ( coordTrans ) pointMIN = ogr . Geometry ( ogr . wkbPoint ) pointMIN . AddPoint ( env [ 0 ] , env [ 2 ] ) pointMIN . Transform ( coordTrans ) return [ pointMAX . GetPoint ( ) [ 1 ] , pointMIN . GetPoint ( ) [ 0 ] , pointMIN . GetPoint ( ) [ 1 ] , pointMAX . GetPoint ( ) [ 0 ] ] else : exit ( " shapefile not found. Please verify your path to the shapefile" )
555	def getAllSwarms ( self , sprintIdx ) : swarmIds = [ ] for swarmId , info in self . _state [ 'swarms' ] . iteritems ( ) : if info [ 'sprintIdx' ] == sprintIdx : swarmIds . append ( swarmId ) return swarmIds
5369	def _retry_storage_check ( exception ) : now = datetime . now ( ) . strftime ( '%Y-%m-%d %H:%M:%S.%f' ) print_error ( '%s: Exception %s: %s' % ( now , type ( exception ) . __name__ , str ( exception ) ) ) return isinstance ( exception , oauth2client . client . AccessTokenRefreshError )
646	def generateVectors ( numVectors = 100 , length = 500 , activity = 50 ) : vectors = [ ] coinc = numpy . zeros ( length , dtype = 'int32' ) indexList = range ( length ) for i in xrange ( numVectors ) : coinc [ : ] = 0 coinc [ random . sample ( indexList , activity ) ] = 1 vectors . append ( coinc . copy ( ) ) return vectors
12389	def parse ( specifiers ) : specifiers = "" . join ( specifiers . split ( ) ) for specifier in specifiers . split ( ',' ) : if len ( specifier ) == 0 : raise ValueError ( "Range: Invalid syntax; missing specifier." ) count = specifier . count ( '-' ) if ( count and specifier [ 0 ] == '-' ) or not count : yield int ( specifier ) , int ( specifier ) continue specifier = list ( map ( int , specifier . split ( '-' ) ) ) if len ( specifier ) == 2 : if specifier [ 0 ] < 0 or specifier [ 1 ] < 0 : raise ValueError ( "Range: Invalid syntax; negative indexing " "not supported in a range specifier." ) if specifier [ 1 ] < specifier [ 0 ] : raise ValueError ( "Range: Invalid syntax; stop is less than start." ) yield tuple ( specifier ) continue raise ValueError ( "Range: Invalid syntax." )
12273	def iso_reference_valid_char ( c , raise_error = True ) : if c in ISO_REFERENCE_VALID : return True if raise_error : raise ValueError ( "'%s' is not in '%s'" % ( c , ISO_REFERENCE_VALID ) ) return False
12028	def headerHTML ( header , fname ) : html = "<html><body><code>" html += "<h2>%s</h2>" % ( fname ) html += pprint . pformat ( header , indent = 1 ) html = html . replace ( "\n" , '<br>' ) . replace ( " " , "&nbsp;" ) html = html . replace ( r"\x00" , "" ) html += "</code></body></html>" print ( "saving header file:" , fname ) f = open ( fname , 'w' ) f . write ( html ) f . close ( ) webbrowser . open ( fname )
13105	def cmpToDataStore_uri ( base , ds1 , ds2 ) : ret = difflib . get_close_matches ( base . uri , [ ds1 . uri , ds2 . uri ] , 1 , cutoff = 0.5 ) if len ( ret ) <= 0 : return 0 if ret [ 0 ] == ds1 . uri : return - 1 return 1
9870	def build_environ ( self , sock_file , conn ) : request = self . read_request_line ( sock_file ) environ = self . base_environ . copy ( ) for k , v in self . read_headers ( sock_file ) . items ( ) : environ [ str ( 'HTTP_' + k ) ] = v environ [ 'REQUEST_METHOD' ] = request [ 'method' ] environ [ 'PATH_INFO' ] = request [ 'path' ] environ [ 'SERVER_PROTOCOL' ] = request [ 'protocol' ] environ [ 'SERVER_PORT' ] = str ( conn . server_port ) environ [ 'REMOTE_PORT' ] = str ( conn . client_port ) environ [ 'REMOTE_ADDR' ] = str ( conn . client_addr ) environ [ 'QUERY_STRING' ] = request [ 'query_string' ] if 'HTTP_CONTENT_LENGTH' in environ : environ [ 'CONTENT_LENGTH' ] = environ [ 'HTTP_CONTENT_LENGTH' ] if 'HTTP_CONTENT_TYPE' in environ : environ [ 'CONTENT_TYPE' ] = environ [ 'HTTP_CONTENT_TYPE' ] self . request_method = environ [ 'REQUEST_METHOD' ] if conn . ssl : environ [ 'wsgi.url_scheme' ] = 'https' environ [ 'HTTPS' ] = 'on' else : environ [ 'wsgi.url_scheme' ] = 'http' if environ . get ( 'HTTP_TRANSFER_ENCODING' , '' ) == 'chunked' : environ [ 'wsgi.input' ] = ChunkedReader ( sock_file ) else : environ [ 'wsgi.input' ] = sock_file return environ
11632	def codepointsInNamelist ( namFilename , unique_glyphs = False , cache = None ) : key = 'charset' if not unique_glyphs else 'ownCharset' internals_dir = os . path . dirname ( os . path . abspath ( __file__ ) ) target = os . path . join ( internals_dir , namFilename ) result = readNamelist ( target , unique_glyphs , cache ) return result [ key ]
4074	def get_cfg_value ( config , section , option ) : try : value = config [ section ] [ option ] except KeyError : if ( section , option ) in MULTI_OPTIONS : return [ ] else : return '' if ( section , option ) in MULTI_OPTIONS : value = split_multiline ( value ) if ( section , option ) in ENVIRON_OPTIONS : value = eval_environ ( value ) return value
5922	def create ( logger_name , logfile = 'gromacs.log' ) : logger = logging . getLogger ( logger_name ) logger . setLevel ( logging . DEBUG ) logfile = logging . FileHandler ( logfile ) logfile_formatter = logging . Formatter ( '%(asctime)s %(name)-12s %(levelname)-8s %(message)s' ) logfile . setFormatter ( logfile_formatter ) logger . addHandler ( logfile ) console = logging . StreamHandler ( ) console . setLevel ( logging . INFO ) formatter = logging . Formatter ( '%(name)-12s: %(levelname)-8s %(message)s' ) console . setFormatter ( formatter ) logger . addHandler ( console ) return logger
12222	def dispatch_first ( self , func ) : self . callees . appendleft ( self . _make_dispatch ( func ) ) return self . _make_wrapper ( func )
2587	def start ( self ) : start = time . time ( ) self . _kill_event = threading . Event ( ) self . procs = { } for worker_id in range ( self . worker_count ) : p = multiprocessing . Process ( target = worker , args = ( worker_id , self . uid , self . pending_task_queue , self . pending_result_queue , self . ready_worker_queue , ) ) p . start ( ) self . procs [ worker_id ] = p logger . debug ( "Manager synced with workers" ) self . _task_puller_thread = threading . Thread ( target = self . pull_tasks , args = ( self . _kill_event , ) ) self . _result_pusher_thread = threading . Thread ( target = self . push_results , args = ( self . _kill_event , ) ) self . _task_puller_thread . start ( ) self . _result_pusher_thread . start ( ) logger . info ( "Loop start" ) self . _kill_event . wait ( ) logger . critical ( "[MAIN] Received kill event, terminating worker processes" ) self . _task_puller_thread . join ( ) self . _result_pusher_thread . join ( ) for proc_id in self . procs : self . procs [ proc_id ] . terminate ( ) logger . critical ( "Terminating worker {}:{}" . format ( self . procs [ proc_id ] , self . procs [ proc_id ] . is_alive ( ) ) ) self . procs [ proc_id ] . join ( ) logger . debug ( "Worker:{} joined successfully" . format ( self . procs [ proc_id ] ) ) self . task_incoming . close ( ) self . result_outgoing . close ( ) self . context . term ( ) delta = time . time ( ) - start logger . info ( "process_worker_pool ran for {} seconds" . format ( delta ) ) return
2650	def send ( self , message_type , task_id , message ) : x = 0 try : buffer = pickle . dumps ( ( self . source_id , int ( time . time ( ) ) , message_type , message ) ) except Exception as e : print ( "Exception during pickling {}" . format ( e ) ) return try : x = self . sock . sendto ( buffer , ( self . ip , self . port ) ) except socket . timeout : print ( "Could not send message within timeout limit" ) return False return x
7771	def payload_class_for_element_name ( element_name ) : logger . debug ( " looking up payload class for element: {0!r}" . format ( element_name ) ) logger . debug ( " known: {0!r}" . format ( STANZA_PAYLOAD_CLASSES ) ) if element_name in STANZA_PAYLOAD_CLASSES : return STANZA_PAYLOAD_CLASSES [ element_name ] else : return XMLPayload
1616	def Search ( pattern , s ) : if pattern not in _regexp_compile_cache : _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) return _regexp_compile_cache [ pattern ] . search ( s )
8056	def do_escape_nl ( self , arg ) : if arg . lower ( ) == 'off' : self . escape_nl = False else : self . escape_nl = True
13270	def all_subclasses ( cls ) : for subclass in cls . __subclasses__ ( ) : yield subclass for subc in all_subclasses ( subclass ) : yield subc
2247	def _make_signature_key ( args , kwargs ) : kwitems = kwargs . items ( ) if ( sys . version_info . major , sys . version_info . minor ) < ( 3 , 7 ) : kwitems = sorted ( kwitems ) kwitems = tuple ( kwitems ) try : key = _hashable ( args ) , _hashable ( kwitems ) except TypeError : raise TypeError ( 'Signature is not hashable: args={} kwargs{}' . format ( args , kwargs ) ) return key
8957	def included ( self , path , is_dir = False ) : inclusive = None for pattern in self . patterns : if pattern . is_dir == is_dir and pattern . matches ( path ) : inclusive = pattern . inclusive return inclusive
7637	def smkdirs ( dpath , mode = 0o777 ) : if not os . path . exists ( dpath ) : os . makedirs ( dpath , mode = mode )
3389	def batch ( self , batch_size , batch_num , fluxes = True ) : for i in range ( batch_num ) : yield self . sample ( batch_size , fluxes = fluxes )
13656	def routedResource ( f , routerAttribute = 'router' ) : return wraps ( f ) ( lambda * a , ** kw : getattr ( f ( * a , ** kw ) , routerAttribute ) . resource ( ) )
6099	def luminosity_integral ( self , x , axis_ratio ) : r = x * axis_ratio return 2 * np . pi * r * self . intensities_from_grid_radii ( x )
1809	def SETE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . ZF , 1 , 0 ) )
10163	def load ( self ) : ret = { } with open ( self . get_path ( ) , 'r' ) as f : lines = f . readlines ( ) ret [ 'personalities' ] = self . get_personalities ( lines [ 0 ] ) ret [ 'arrays' ] = self . get_arrays ( lines [ 1 : - 1 ] , ret [ 'personalities' ] ) self . content = reduce ( lambda x , y : x + y , lines ) return ret
6336	def dist_abs ( self , src , tar , * args , ** kwargs ) : return self . dist ( src , tar , * args , ** kwargs )
6008	def load_noise_map ( noise_map_path , noise_map_hdu , pixel_scale , image , background_noise_map , exposure_time_map , convert_noise_map_from_weight_map , convert_noise_map_from_inverse_noise_map , noise_map_from_image_and_background_noise_map , convert_from_electrons , gain , convert_from_adus ) : noise_map_options = sum ( [ convert_noise_map_from_weight_map , convert_noise_map_from_inverse_noise_map , noise_map_from_image_and_background_noise_map ] ) if noise_map_options > 1 : raise exc . DataException ( 'You have specified more than one method to load the noise_map map, e.g.:' 'convert_noise_map_from_weight_map | ' 'convert_noise_map_from_inverse_noise_map |' 'noise_map_from_image_and_background_noise_map' ) if noise_map_options == 0 and noise_map_path is not None : return NoiseMap . from_fits_with_pixel_scale ( file_path = noise_map_path , hdu = noise_map_hdu , pixel_scale = pixel_scale ) elif convert_noise_map_from_weight_map and noise_map_path is not None : weight_map = Array . from_fits ( file_path = noise_map_path , hdu = noise_map_hdu ) return NoiseMap . from_weight_map ( weight_map = weight_map , pixel_scale = pixel_scale ) elif convert_noise_map_from_inverse_noise_map and noise_map_path is not None : inverse_noise_map = Array . from_fits ( file_path = noise_map_path , hdu = noise_map_hdu ) return NoiseMap . from_inverse_noise_map ( inverse_noise_map = inverse_noise_map , pixel_scale = pixel_scale ) elif noise_map_from_image_and_background_noise_map : if background_noise_map is None : raise exc . DataException ( 'Cannot compute the noise-map from the image and background noise_map map if a ' 'background noise_map map is not supplied.' ) if not ( convert_from_electrons or convert_from_adus ) and exposure_time_map is None : raise exc . DataException ( 'Cannot compute the noise-map from the image and background noise_map map if an ' 'exposure-time (or exposure time map) is not supplied to convert to adus' ) if convert_from_adus and gain is None : raise exc . DataException ( 'Cannot compute the noise-map from the image and background noise_map map if a' 'gain is not supplied to convert from adus' ) return NoiseMap . from_image_and_background_noise_map ( pixel_scale = pixel_scale , image = image , background_noise_map = background_noise_map , exposure_time_map = exposure_time_map , convert_from_electrons = convert_from_electrons , gain = gain , convert_from_adus = convert_from_adus ) else : raise exc . DataException ( 'A noise_map map was not loaded, specify a noise_map_path or option to compute a noise_map map.' )
2275	def _win32_is_hardlinked ( fpath1 , fpath2 ) : def get_read_handle ( fpath ) : if os . path . isdir ( fpath ) : dwFlagsAndAttributes = jwfs . api . FILE_FLAG_BACKUP_SEMANTICS else : dwFlagsAndAttributes = 0 hFile = jwfs . api . CreateFile ( fpath , jwfs . api . GENERIC_READ , jwfs . api . FILE_SHARE_READ , None , jwfs . api . OPEN_EXISTING , dwFlagsAndAttributes , None ) return hFile def get_unique_id ( hFile ) : info = jwfs . api . BY_HANDLE_FILE_INFORMATION ( ) res = jwfs . api . GetFileInformationByHandle ( hFile , info ) jwfs . handle_nonzero_success ( res ) unique_id = ( info . volume_serial_number , info . file_index_high , info . file_index_low ) return unique_id hFile1 = get_read_handle ( fpath1 ) hFile2 = get_read_handle ( fpath2 ) try : are_equal = ( get_unique_id ( hFile1 ) == get_unique_id ( hFile2 ) ) except Exception : raise finally : jwfs . api . CloseHandle ( hFile1 ) jwfs . api . CloseHandle ( hFile2 ) return are_equal
10723	def xformers ( sig ) : return [ ( _wrapper ( f ) , l ) for ( f , l ) in _XFORMER . PARSER . parseString ( sig , parseAll = True ) ]
4805	def _err ( self , msg ) : out = '%s%s' % ( '[%s] ' % self . description if len ( self . description ) > 0 else '' , msg ) if self . kind == 'warn' : print ( out ) return self elif self . kind == 'soft' : global _soft_err _soft_err . append ( out ) return self else : raise AssertionError ( out )
5202	def create_connection ( port = _PORT_ , timeout = _TIMEOUT_ , restart = False ) : if _CON_SYM_ in globals ( ) : if not isinstance ( globals ( ) [ _CON_SYM_ ] , pdblp . BCon ) : del globals ( ) [ _CON_SYM_ ] if ( _CON_SYM_ in globals ( ) ) and ( not restart ) : con = globals ( ) [ _CON_SYM_ ] if getattr ( con , '_session' ) . start ( ) : con . start ( ) return con , False else : con = pdblp . BCon ( port = port , timeout = timeout ) globals ( ) [ _CON_SYM_ ] = con con . start ( ) return con , True
6900	def parallel_periodicfeatures_lcdir ( pfpkl_dir , lcbasedir , outdir , pfpkl_glob = 'periodfinding-*.pkl*' , starfeaturesdir = None , fourierorder = 5 , transitparams = ( - 0.01 , 0.1 , 0.1 ) , ebparams = ( - 0.2 , 0.3 , 0.7 , 0.5 ) , pdiff_threshold = 1.0e-4 , sidereal_threshold = 1.0e-4 , sampling_peak_multiplier = 5.0 , sampling_startp = None , sampling_endp = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , sigclip = 10.0 , verbose = False , maxobjects = None , nworkers = NCPUS , recursive = True , ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None fileglob = pfpkl_glob LOGINFO ( 'searching for periodfinding pickles in %s ...' % pfpkl_dir ) if recursive is False : matching = glob . glob ( os . path . join ( pfpkl_dir , fileglob ) ) else : if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( pfpkl_dir , '**' , fileglob ) , recursive = True ) else : walker = os . walk ( pfpkl_dir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , fileglob ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) if matching and len ( matching ) > 0 : LOGINFO ( 'found %s periodfinding pickles, getting periodicfeatures...' % len ( matching ) ) return parallel_periodicfeatures ( matching , lcbasedir , outdir , starfeaturesdir = starfeaturesdir , fourierorder = fourierorder , transitparams = transitparams , ebparams = ebparams , pdiff_threshold = pdiff_threshold , sidereal_threshold = sidereal_threshold , sampling_peak_multiplier = sampling_peak_multiplier , sampling_startp = sampling_startp , sampling_endp = sampling_endp , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = lcformatdir , sigclip = sigclip , verbose = verbose , maxobjects = maxobjects , nworkers = nworkers , ) else : LOGERROR ( 'no periodfinding pickles found in %s' % ( pfpkl_dir ) ) return None
11069	def mongo ( daemon = False , port = 20771 ) : cmd = "mongod --port {0}" . format ( port ) if daemon : cmd += " --fork" run ( cmd )
9991	def _get_dynamic_base ( self , bases_ ) : bases = tuple ( base . bases [ 0 ] if base . is_dynamic ( ) else base for base in bases_ ) if len ( bases ) == 1 : return bases [ 0 ] elif len ( bases ) > 1 : return self . model . get_dynamic_base ( bases ) else : RuntimeError ( "must not happen" )
3923	def _set_title ( self ) : self . title = get_conv_name ( self . _conversation , show_unread = True , truncate = True ) self . _set_title_cb ( self , self . title )
13050	def main ( ) : search = ServiceSearch ( ) services = search . get_services ( up = True , tags = [ '!header_scan' ] ) print_notification ( "Scanning {} services" . format ( len ( services ) ) ) urllib3 . disable_warnings ( urllib3 . exceptions . InsecureRequestWarning ) pool = Pool ( 100 ) count = 0 for service in services : count += 1 if count % 50 == 0 : print_notification ( "Checking {}/{} services" . format ( count , len ( services ) ) ) pool . spawn ( check_service , service ) pool . join ( ) print_notification ( "Completed, 'http' tag added to services that respond to http, 'https' tag added to services that respond to https." )
3882	def upgrade_name ( self , user_ ) : if user_ . name_type > self . name_type : self . full_name = user_ . full_name self . first_name = user_ . first_name self . name_type = user_ . name_type logger . debug ( 'Added %s name to User "%s": %s' , self . name_type . name . lower ( ) , self . full_name , self )
107	def max_pool ( arr , block_size , cval = 0 , preserve_dtype = True ) : return pool ( arr , block_size , np . max , cval = cval , preserve_dtype = preserve_dtype )
2027	def SHA3 ( self , start , size ) : data = self . try_simplify_to_constant ( self . read_buffer ( start , size ) ) if issymbolic ( data ) : known_sha3 = { } self . _publish ( 'on_symbolic_sha3' , data , known_sha3 ) value = 0 known_hashes_cond = False for key , hsh in known_sha3 . items ( ) : assert not issymbolic ( key ) , "Saved sha3 data,hash pairs should be concrete" cond = key == data known_hashes_cond = Operators . OR ( cond , known_hashes_cond ) value = Operators . ITEBV ( 256 , cond , hsh , value ) return value value = sha3 . keccak_256 ( data ) . hexdigest ( ) value = int ( value , 16 ) self . _publish ( 'on_concrete_sha3' , data , value ) logger . info ( "Found a concrete SHA3 example %r -> %x" , data , value ) return value
3708	def COSTALD_mixture ( xs , T , Tcs , Vcs , omegas ) : r cmps = range ( len ( xs ) ) if not none_and_length_check ( [ xs , Tcs , Vcs , omegas ] ) : raise Exception ( 'Function inputs are incorrect format' ) sum1 = sum ( [ xi * Vci for xi , Vci in zip ( xs , Vcs ) ] ) sum2 = sum ( [ xi * Vci ** ( 2 / 3. ) for xi , Vci in zip ( xs , Vcs ) ] ) sum3 = sum ( [ xi * Vci ** ( 1 / 3. ) for xi , Vci in zip ( xs , Vcs ) ] ) Vm = 0.25 * ( sum1 + 3. * sum2 * sum3 ) VijTcij = [ [ ( Tcs [ i ] * Tcs [ j ] * Vcs [ i ] * Vcs [ j ] ) ** 0.5 for j in cmps ] for i in cmps ] omega = mixing_simple ( xs , omegas ) Tcm = sum ( [ xs [ i ] * xs [ j ] * VijTcij [ i ] [ j ] / Vm for j in cmps for i in cmps ] ) return COSTALD ( T , Tcm , Vm , omega )
1864	def SARX ( cpu , dest , src , count ) : OperandSize = dest . size count = count . read ( ) countMask = { 8 : 0x1f , 16 : 0x1f , 32 : 0x1f , 64 : 0x3f } [ OperandSize ] tempCount = count & countMask tempDest = value = src . read ( ) sign = value & ( 1 << ( OperandSize - 1 ) ) while tempCount != 0 : cpu . CF = ( value & 0x1 ) != 0 value = ( value >> 1 ) | sign tempCount = tempCount - 1 res = dest . write ( value )
12542	def get_attributes ( self , attributes , default = '' ) : if isinstance ( attributes , str ) : attributes = [ attributes ] attrs = [ getattr ( self , attr , default ) for attr in attributes ] if len ( attrs ) == 1 : return attrs [ 0 ] return tuple ( attrs )
1839	def JNE ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . ZF , target . read ( ) , cpu . PC )
13607	def pickle ( obj , filepath ) : arr = pkl . dumps ( obj , - 1 ) with open ( filepath , 'wb' ) as f : s = 0 while s < len ( arr ) : e = min ( s + blosc . MAX_BUFFERSIZE , len ( arr ) ) carr = blosc . compress ( arr [ s : e ] , typesize = 8 ) f . write ( carr ) s = e
6285	def toggle_pause ( self ) : self . controller . playing = not self . controller . playing self . music . toggle_pause ( )
11083	def help ( self , msg , args ) : output = [ ] if len ( args ) == 0 : commands = sorted ( self . _bot . dispatcher . commands . items ( ) , key = itemgetter ( 0 ) ) commands = filter ( lambda x : x [ 1 ] . is_subcmd is False , commands ) if self . _should_filter_help_commands ( msg . user ) : commands = filter ( lambda x : x [ 1 ] . admin_only is False , commands ) for name , cmd in commands : output . append ( self . _get_short_help_for_command ( name ) ) else : name = '!' + args [ 0 ] output = [ self . _get_help_for_command ( name ) ] return '\n' . join ( output )
4681	def getAccountFromPublicKey ( self , pub ) : names = list ( self . getAccountsFromPublicKey ( str ( pub ) ) ) if names : return names [ 0 ]
11386	def run ( self , raw_args ) : parser = self . parser args , kwargs = parser . parse_callback_args ( raw_args ) callback = kwargs . pop ( "main_callback" ) if parser . has_injected_quiet ( ) : levels = kwargs . pop ( "quiet_inject" , "" ) logging . inject_quiet ( levels ) try : ret_code = callback ( * args , ** kwargs ) ret_code = int ( ret_code ) if ret_code else 0 except ArgError as e : echo . err ( "{}: error: {}" , parser . prog , str ( e ) ) ret_code = 2 return ret_code
7135	def filter_dict ( d , exclude ) : ret = { } for key , value in d . items ( ) : if key not in exclude : ret . update ( { key : value } ) return ret
12181	def api_subclass_factory ( name , docstring , remove_methods , base = SlackApi ) : methods = deepcopy ( base . API_METHODS ) for parent , to_remove in remove_methods . items ( ) : if to_remove is ALL : del methods [ parent ] else : for method in to_remove : del methods [ parent ] [ method ] return type ( name , ( base , ) , dict ( API_METHODS = methods , __doc__ = docstring ) )
2779	def get_data ( self , url , headers = dict ( ) , params = dict ( ) , render_json = True ) : url = urljoin ( self . end_point , url ) response = requests . get ( url , headers = headers , params = params , timeout = self . get_timeout ( ) ) if render_json : return response . json ( ) return response . content
7242	def pxbounds ( self , geom , clip = False ) : try : if isinstance ( geom , dict ) : if 'geometry' in geom : geom = shape ( geom [ 'geometry' ] ) else : geom = shape ( geom ) elif isinstance ( geom , BaseGeometry ) : geom = shape ( geom ) else : geom = wkt . loads ( geom ) except : raise TypeError ( "Invalid geometry object" ) if geom . disjoint ( shape ( self ) ) : raise ValueError ( "Geometry outside of image bounds" ) ( xmin , ymin , xmax , ymax ) = ops . transform ( self . __geo_transform__ . rev , geom ) . bounds _nbands , ysize , xsize = self . shape if clip : xmin = max ( xmin , 0 ) ymin = max ( ymin , 0 ) xmax = min ( xmax , xsize ) ymax = min ( ymax , ysize ) return ( xmin , ymin , xmax , ymax )
8492	def main ( ) : parser = argparse . ArgumentParser ( description = "Helper for working with " "pyconfigs" ) target_group = parser . add_mutually_exclusive_group ( ) target_group . add_argument ( '-f' , '--filename' , help = "parse an individual file or directory" , metavar = 'F' ) target_group . add_argument ( '-m' , '--module' , help = "parse a package or module, recursively looking inside it" , metavar = 'M' ) parser . add_argument ( '-v' , '--view-call' , help = "show the actual pyconfig call made (default: show namespace)" , action = 'store_true' ) parser . add_argument ( '-l' , '--load-configs' , help = "query the currently set value for each key found" , action = 'store_true' ) key_group = parser . add_mutually_exclusive_group ( ) key_group . add_argument ( '-a' , '--all' , help = "show keys which don't have defaults set" , action = 'store_true' ) key_group . add_argument ( '-k' , '--only-keys' , help = "show a list of discovered keys without values" , action = 'store_true' ) parser . add_argument ( '-n' , '--natural-sort' , help = "sort by filename and line (default: alphabetical by key)" , action = 'store_true' ) parser . add_argument ( '-s' , '--source' , help = "show source annotations (implies --natural-sort)" , action = 'store_true' ) parser . add_argument ( '-c' , '--color' , help = "toggle output colors (default: %s)" % bool ( pygments ) , action = 'store_const' , default = bool ( pygments ) , const = ( not bool ( pygments ) ) ) args = parser . parse_args ( ) if args . color and not pygments : _error ( "Pygments is required for color output.\n" " pip install pygments" ) if args . module : _handle_module ( args ) if args . filename : _handle_file ( args )
902	def updateAnomalyLikelihoods ( anomalyScores , params , verbosity = 0 ) : if verbosity > 3 : print ( "In updateAnomalyLikelihoods." ) print ( "Number of anomaly scores:" , len ( anomalyScores ) ) print ( "First 20:" , anomalyScores [ 0 : min ( 20 , len ( anomalyScores ) ) ] ) print ( "Params:" , params ) if len ( anomalyScores ) == 0 : raise ValueError ( "Must have at least one anomalyScore" ) if not isValidEstimatorParams ( params ) : raise ValueError ( "'params' is not a valid params structure" ) if "historicalLikelihoods" not in params : params [ "historicalLikelihoods" ] = [ 1.0 ] historicalValues = params [ "movingAverage" ] [ "historicalValues" ] total = params [ "movingAverage" ] [ "total" ] windowSize = params [ "movingAverage" ] [ "windowSize" ] aggRecordList = numpy . zeros ( len ( anomalyScores ) , dtype = float ) likelihoods = numpy . zeros ( len ( anomalyScores ) , dtype = float ) for i , v in enumerate ( anomalyScores ) : newAverage , historicalValues , total = ( MovingAverage . compute ( historicalValues , total , v [ 2 ] , windowSize ) ) aggRecordList [ i ] = newAverage likelihoods [ i ] = tailProbability ( newAverage , params [ "distribution" ] ) likelihoods2 = params [ "historicalLikelihoods" ] + list ( likelihoods ) filteredLikelihoods = _filterLikelihoods ( likelihoods2 ) likelihoods [ : ] = filteredLikelihoods [ - len ( likelihoods ) : ] historicalLikelihoods = likelihoods2 [ - min ( windowSize , len ( likelihoods2 ) ) : ] newParams = { "distribution" : params [ "distribution" ] , "movingAverage" : { "historicalValues" : historicalValues , "total" : total , "windowSize" : windowSize , } , "historicalLikelihoods" : historicalLikelihoods , } assert len ( newParams [ "historicalLikelihoods" ] ) <= windowSize if verbosity > 3 : print ( "Number of likelihoods:" , len ( likelihoods ) ) print ( "First 20 likelihoods:" , likelihoods [ 0 : min ( 20 , len ( likelihoods ) ) ] ) print ( "Leaving updateAnomalyLikelihoods." ) return ( likelihoods , aggRecordList , newParams )
12434	def redirect ( cls , request , response ) : if cls . meta . legacy_redirect : if request . method in ( 'GET' , 'HEAD' , ) : response . status = http . client . MOVED_PERMANENTLY else : response . status = http . client . TEMPORARY_REDIRECT else : response . status = http . client . PERMANENT_REDIRECT response . close ( )
8314	def parse ( self , light = False ) : markup = self . markup self . disambiguation = self . parse_disambiguation ( markup ) self . categories = self . parse_categories ( markup ) self . links = self . parse_links ( markup ) if not light : markup = self . convert_pre ( markup ) markup = self . convert_li ( markup ) markup = self . convert_table ( markup ) markup = replace_entities ( markup ) markup = markup . replace ( "{{Cite" , "{{cite" ) markup = re . sub ( "\{\{ {1,2}cite" , "{{cite" , markup ) self . references , markup = self . parse_references ( markup ) markup = re . sub ( "\n+(\{\{legend)" , "\\1" , markup ) self . images , markup = self . parse_images ( markup ) self . images . extend ( self . parse_gallery_images ( markup ) ) self . paragraphs = self . parse_paragraphs ( markup ) self . tables = self . parse_tables ( markup ) self . translations = self . parse_translations ( markup ) self . important = self . parse_important ( markup )
2732	def get_records ( self , params = None ) : if params is None : params = { } records = [ ] data = self . get_data ( "domains/%s/records/" % self . name , type = GET , params = params ) for record_data in data [ 'domain_records' ] : record = Record ( domain_name = self . name , ** record_data ) record . token = self . token records . append ( record ) return records
12033	def averageSweep ( self , sweepFirst = 0 , sweepLast = None ) : if sweepLast is None : sweepLast = self . sweeps - 1 nSweeps = sweepLast - sweepFirst + 1 runningSum = np . zeros ( len ( self . sweepY ) ) self . log . debug ( "averaging sweep %d to %d" , sweepFirst , sweepLast ) for sweep in np . arange ( nSweeps ) + sweepFirst : self . setsweep ( sweep ) runningSum += self . sweepY . flatten ( ) average = runningSum / nSweeps return average
3866	async def rename ( self , name ) : await self . _client . rename_conversation ( hangouts_pb2 . RenameConversationRequest ( request_header = self . _client . get_request_header ( ) , new_name = name , event_request_header = self . _get_event_request_header ( ) , ) )
9214	def t_t_isopen ( self , t ) : r'"|\'' if t . value [ 0 ] == '"' : t . lexer . push_state ( 'istringquotes' ) elif t . value [ 0 ] == '\'' : t . lexer . push_state ( 'istringapostrophe' ) return t
7055	def register_lcformat ( formatkey , fileglob , timecols , magcols , errcols , readerfunc_module , readerfunc , readerfunc_kwargs = None , normfunc_module = None , normfunc = None , normfunc_kwargs = None , magsarefluxes = False , overwrite_existing = False , lcformat_dir = '~/.astrobase/lcformat-jsons' ) : LOGINFO ( 'adding %s to LC format registry...' % formatkey ) lcformat_dpath = os . path . abspath ( os . path . expanduser ( lcformat_dir ) ) if not os . path . exists ( lcformat_dpath ) : os . makedirs ( lcformat_dpath ) lcformat_jsonpath = os . path . join ( lcformat_dpath , '%s.json' % formatkey ) if os . path . exists ( lcformat_jsonpath ) and not overwrite_existing : LOGERROR ( 'There is an existing lcformat JSON: %s ' 'for this formatkey: %s and ' 'overwrite_existing = False, skipping...' % ( lcformat_jsonpath , formatkey ) ) return None readermodule = _check_extmodule ( readerfunc_module , formatkey ) if not readermodule : LOGERROR ( "could not import the required " "module: %s to read %s light curves" % ( readerfunc_module , formatkey ) ) return None try : getattr ( readermodule , readerfunc ) readerfunc_in = readerfunc except AttributeError : LOGEXCEPTION ( 'Could not get the specified reader ' 'function: %s for lcformat: %s ' 'from module: %s' % ( formatkey , readerfunc_module , readerfunc ) ) raise if normfunc_module : normmodule = _check_extmodule ( normfunc_module , formatkey ) if not normmodule : LOGERROR ( "could not import the required " "module: %s to normalize %s light curves" % ( normfunc_module , formatkey ) ) return None else : normmodule = None if normfunc_module and normfunc : try : getattr ( normmodule , normfunc ) normfunc_in = normfunc except AttributeError : LOGEXCEPTION ( 'Could not get the specified norm ' 'function: %s for lcformat: %s ' 'from module: %s' % ( normfunc , formatkey , normfunc_module ) ) raise else : normfunc_in = None formatdict = { 'fileglob' : fileglob , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'magsarefluxes' : magsarefluxes , 'lcreader_module' : readerfunc_module , 'lcreader_func' : readerfunc_in , 'lcreader_kwargs' : readerfunc_kwargs , 'lcnorm_module' : normfunc_module , 'lcnorm_func' : normfunc_in , 'lcnorm_kwargs' : normfunc_kwargs } with open ( lcformat_jsonpath , 'w' ) as outfd : json . dump ( formatdict , outfd , indent = 4 ) return lcformat_jsonpath
11100	def select_by_atime ( self , min_time = 0 , max_time = ts_2100 , recursive = True ) : def filters ( p ) : return min_time <= p . atime <= max_time return self . select_file ( filters , recursive )
6	def cg ( f_Ax , b , cg_iters = 10 , callback = None , verbose = False , residual_tol = 1e-10 ) : p = b . copy ( ) r = b . copy ( ) x = np . zeros_like ( b ) rdotr = r . dot ( r ) fmtstr = "%10i %10.3g %10.3g" titlestr = "%10s %10s %10s" if verbose : print ( titlestr % ( "iter" , "residual norm" , "soln norm" ) ) for i in range ( cg_iters ) : if callback is not None : callback ( x ) if verbose : print ( fmtstr % ( i , rdotr , np . linalg . norm ( x ) ) ) z = f_Ax ( p ) v = rdotr / p . dot ( z ) x += v * p r -= v * z newrdotr = r . dot ( r ) mu = newrdotr / rdotr p = r + mu * p rdotr = newrdotr if rdotr < residual_tol : break if callback is not None : callback ( x ) if verbose : print ( fmtstr % ( i + 1 , rdotr , np . linalg . norm ( x ) ) ) return x
8001	def fix_in_stanza ( self , stanza ) : StreamBase . fix_in_stanza ( self , stanza ) if not self . initiator : if stanza . from_jid != self . peer : stanza . set_from ( self . peer )
11566	def stepper_step ( self , motor_speed , number_of_steps ) : if number_of_steps > 0 : direction = 1 else : direction = 0 abs_number_of_steps = abs ( number_of_steps ) data = [ self . STEPPER_STEP , motor_speed & 0x7f , ( motor_speed >> 7 ) & 0x7f , ( motor_speed >> 14 ) & 0x7f , abs_number_of_steps & 0x7f , ( abs_number_of_steps >> 7 ) & 0x7f , direction ] self . _command_handler . send_sysex ( self . _command_handler . STEPPER_DATA , data )
8275	def recombine ( self , other , d = 0.7 ) : a , b = self , other d1 = max ( 0 , min ( d , 1 ) ) d2 = d1 c = ColorTheme ( name = a . name [ : int ( len ( a . name ) * d1 ) ] + b . name [ int ( len ( b . name ) * d2 ) : ] , ranges = a . ranges [ : int ( len ( a . ranges ) * d1 ) ] + b . ranges [ int ( len ( b . ranges ) * d2 ) : ] , top = a . top , cache = os . path . join ( DEFAULT_CACHE , "recombined" ) , blue = a . blue , length = a . length * d1 + b . length * d2 ) c . tags = a . tags [ : int ( len ( a . tags ) * d1 ) ] c . tags += b . tags [ int ( len ( b . tags ) * d2 ) : ] return c
5960	def _tcorrel ( self , nstep = 100 , ** kwargs ) : t = self . array [ 0 , : : nstep ] r = gromacs . collections . Collection ( [ numkit . timeseries . tcorrel ( t , Y , nstep = 1 , ** kwargs ) for Y in self . array [ 1 : , : : nstep ] ] ) return r
11149	def rename ( self , key : Any , new_key : Any ) : if new_key == key : return required_locks = [ self . _key_locks [ key ] , self . _key_locks [ new_key ] ] ordered_required_locks = sorted ( required_locks , key = lambda x : id ( x ) ) for lock in ordered_required_locks : lock . acquire ( ) try : if key not in self . _data : raise KeyError ( "Attribute to rename \"%s\" does not exist" % key ) self . _data [ new_key ] = self [ key ] del self . _data [ key ] finally : for lock in required_locks : lock . release ( )
9334	def full ( shape , value , dtype = 'f8' ) : shared = empty ( shape , dtype ) shared [ : ] = value return shared
10558	def download ( self , songs , template = None ) : if not template : template = os . getcwd ( ) songnum = 0 total = len ( songs ) results = [ ] errors = { } pad = len ( str ( total ) ) for result in self . _download ( songs , template ) : song_id = songs [ songnum ] [ 'id' ] songnum += 1 downloaded , error = result if downloaded : logger . info ( "({num:>{pad}}/{total}) Successfully downloaded -- {file} ({song_id})" . format ( num = songnum , pad = pad , total = total , file = downloaded [ song_id ] , song_id = song_id ) ) results . append ( { 'result' : 'downloaded' , 'id' : song_id , 'filepath' : downloaded [ song_id ] } ) elif error : title = songs [ songnum ] . get ( 'title' , "<empty>" ) artist = songs [ songnum ] . get ( 'artist' , "<empty>" ) album = songs [ songnum ] . get ( 'album' , "<empty>" ) logger . info ( "({num:>{pad}}/{total}) Error on download -- {title} -- {artist} -- {album} ({song_id})" . format ( num = songnum , pad = pad , total = total , title = title , artist = artist , album = album , song_id = song_id ) ) results . append ( { 'result' : 'error' , 'id' : song_id , 'message' : error [ song_id ] } ) if errors : logger . info ( "\n\nThe following errors occurred:\n" ) for filepath , e in errors . items ( ) : logger . info ( "{file} | {error}" . format ( file = filepath , error = e ) ) logger . info ( "\nThese files may need to be synced again.\n" ) return results
7348	async def get_access_token ( consumer_key , consumer_secret , oauth_token , oauth_token_secret , oauth_verifier , ** kwargs ) : client = BasePeonyClient ( consumer_key = consumer_key , consumer_secret = consumer_secret , access_token = oauth_token , access_token_secret = oauth_token_secret , api_version = "" , suffix = "" ) response = await client . api . oauth . access_token . get ( _suffix = "" , oauth_verifier = oauth_verifier ) return parse_token ( response )
11647	def fit_transform ( self , X , y = None ) : n = X . shape [ 0 ] if X . shape != ( n , n ) : raise TypeError ( "Input must be a square matrix." ) memory = get_memory ( self . memory ) discard_X = not self . copy and self . negatives_likely vals , vecs = memory . cache ( scipy . linalg . eigh , ignore = [ 'overwrite_a' ] ) ( X , overwrite_a = discard_X ) vals = vals [ : , None ] self . clip_ = np . dot ( vecs , np . sign ( vals ) * vecs . T ) if discard_X or vals [ 0 , 0 ] < 0 : del X np . abs ( vals , out = vals ) X = np . dot ( vecs , vals * vecs . T ) del vals , vecs X = Symmetrize ( copy = False ) . fit_transform ( X ) return X
9492	def compile_bytecode ( code : list ) -> bytes : bc = b"" for i , op in enumerate ( code ) : try : if isinstance ( op , _PyteOp ) or isinstance ( op , _PyteAugmentedComparator ) : bc_op = op . to_bytes ( bc ) elif isinstance ( op , int ) : bc_op = op . to_bytes ( 1 , byteorder = "little" ) elif isinstance ( op , bytes ) : bc_op = op else : raise CompileError ( "Could not compile code of type {}" . format ( type ( op ) ) ) bc += bc_op except Exception as e : print ( "Fatal compiliation error on operator {i} ({op})." . format ( i = i , op = op ) ) raise e return bc
8851	def open_file ( self , path , line = None ) : editor = None if path : interpreter , pyserver , args = self . _get_backend_parameters ( ) editor = self . tabWidget . open_document ( path , None , interpreter = interpreter , server_script = pyserver , args = args ) if editor : self . setup_editor ( editor ) self . recent_files_manager . open_file ( path ) self . menu_recents . update_actions ( ) if line is not None : TextHelper ( self . tabWidget . current_widget ( ) ) . goto_line ( line ) return editor
7838	def set_node ( self , node ) : if node is None : if self . xmlnode . hasProp ( "node" ) : self . xmlnode . unsetProp ( "node" ) return node = unicode ( node ) self . xmlnode . setProp ( "node" , node . encode ( "utf-8" ) )
628	def _orderForCoordinate ( cls , coordinate ) : seed = cls . _hashCoordinate ( coordinate ) rng = Random ( seed ) return rng . getReal64 ( )
9560	def _apply_assert_methods ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'assert' ) : rdict = self . _as_dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except AssertionError as e : code = ASSERT_CHECK_FAILED message = MESSAGES [ ASSERT_CHECK_FAILED ] if len ( e . args ) > 0 : custom = e . args [ 0 ] if isinstance ( custom , ( list , tuple ) ) : if len ( custom ) > 0 : code = custom [ 0 ] if len ( custom ) > 1 : message = custom [ 1 ] else : code = custom p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . __name__ , f . __doc__ ) if context is not None : p [ 'context' ] = context yield p
11440	def _get_children_as_string ( node ) : out = [ ] if node : for child in node : if child . nodeType == child . TEXT_NODE : out . append ( child . data ) else : out . append ( _get_children_as_string ( child . childNodes ) ) return '' . join ( out )
9276	def apply_exclude_tags ( self , all_tags ) : filtered = copy . deepcopy ( all_tags ) for tag in all_tags : if tag [ "name" ] not in self . options . exclude_tags : self . warn_if_tag_not_found ( tag , "exclude-tags" ) else : filtered . remove ( tag ) return filtered
12020	def fasta_dict_to_file ( fasta_dict , fasta_file , line_char_limit = None ) : fasta_fp = fasta_file if isinstance ( fasta_file , str ) : fasta_fp = open ( fasta_file , 'wb' ) for key in fasta_dict : seq = fasta_dict [ key ] [ 'seq' ] if line_char_limit : seq = '\n' . join ( [ seq [ i : i + line_char_limit ] for i in range ( 0 , len ( seq ) , line_char_limit ) ] ) fasta_fp . write ( u'{0:s}\n{1:s}\n' . format ( fasta_dict [ key ] [ 'header' ] , seq ) )
13642	def check_confirmations_or_resend ( self , use_open_peers = False , ** kw ) : if self . confirmations ( ) == 0 : self . send ( use_open_peers , ** kw )
11820	def as_dict ( self , default = None ) : settings = SettingDict ( queryset = self , default = default ) return settings
11889	def get_lights ( self ) : now = datetime . datetime . now ( ) if ( now - self . _last_updated ) < datetime . timedelta ( seconds = UPDATE_INTERVAL_SECONDS ) : return self . _bulbs else : self . _last_updated = now light_data = self . get_data ( ) _LOGGER . debug ( "got: %s" , light_data ) if not light_data : return [ ] if self . _bulbs : for bulb in self . _bulbs : try : values = light_data [ bulb . zid ] bulb . _online , bulb . _red , bulb . _green , bulb . _blue , bulb . _level = values except KeyError : pass else : for light_id in light_data : self . _bulbs . append ( Bulb ( self , light_id , * light_data [ light_id ] ) ) return self . _bulbs
8454	def _cookiecutter_configs_have_changed ( template , old_version , new_version ) : temple . check . is_git_ssh_path ( template ) repo_path = temple . utils . get_repo_path ( template ) github_client = temple . utils . GithubClient ( ) api = '/repos/{}/contents/cookiecutter.json' . format ( repo_path ) old_config_resp = github_client . get ( api , params = { 'ref' : old_version } ) old_config_resp . raise_for_status ( ) new_config_resp = github_client . get ( api , params = { 'ref' : new_version } ) new_config_resp . raise_for_status ( ) return old_config_resp . json ( ) [ 'content' ] != new_config_resp . json ( ) [ 'content' ]
8105	def callback ( self , * incoming ) : message = incoming [ 0 ] if message : address , command = message [ 0 ] , message [ 2 ] profile = self . get_profile ( address ) if profile is not None : try : getattr ( profile , command ) ( self , message ) except AttributeError : pass
3684	def solve ( self ) : self . check_sufficient_inputs ( ) if self . V : if self . P : self . T = self . solve_T ( self . P , self . V ) self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 = self . a_alpha_and_derivatives ( self . T ) else : self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 = self . a_alpha_and_derivatives ( self . T ) self . P = R * self . T / ( self . V - self . b ) - self . a_alpha / ( self . V * self . V + self . delta * self . V + self . epsilon ) Vs = [ self . V , 1j , 1j ] else : self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 = self . a_alpha_and_derivatives ( self . T ) Vs = self . volume_solutions ( self . T , self . P , self . b , self . delta , self . epsilon , self . a_alpha ) self . set_from_PT ( Vs )
10379	def calculate_concordance_by_annotation ( graph , annotation , key , cutoff = None ) : return { value : calculate_concordance ( subgraph , key , cutoff = cutoff ) for value , subgraph in get_subgraphs_by_annotation ( graph , annotation ) . items ( ) }
3651	def logger ( name = None , save = False ) : logger = logging . getLogger ( name ) if save : logformat = '%(asctime)s [%(levelname)s] [%(name)s] %(funcName)s: %(message)s (line %(lineno)d)' log_file_path = 'fut.log' open ( log_file_path , 'w' ) . write ( '' ) logger . setLevel ( logging . DEBUG ) logger_handler = logging . FileHandler ( log_file_path ) logger_handler . setFormatter ( logging . Formatter ( logformat ) ) else : logger_handler = NullHandler ( ) logger . addHandler ( logger_handler ) return logger
3160	def update ( self , campaign_id , data ) : self . campaign_id = campaign_id return self . _mc_client . _put ( url = self . _build_path ( campaign_id , 'content' ) , data = data )
12069	def save ( abf , fname = None , tag = None , width = 700 , close = True , facecolor = 'w' , resize = True ) : if len ( pylab . gca ( ) . get_lines ( ) ) == 0 : print ( "can't save, no figure!" ) return if resize : pylab . tight_layout ( ) pylab . subplots_adjust ( bottom = .1 ) annotate ( abf ) if tag : fname = abf . outpath + abf . ID + "_" + tag + ".png" inchesX , inchesY = pylab . gcf ( ) . get_size_inches ( ) dpi = width / inchesX if fname : if not os . path . exists ( abf . outpath ) : os . mkdir ( abf . outpath ) print ( " <- saving [%s] at %d DPI (%dx%d)" % ( os . path . basename ( fname ) , dpi , inchesX * dpi , inchesY * dpi ) ) pylab . savefig ( fname , dpi = dpi , facecolor = facecolor ) else : pylab . show ( ) if close : pylab . close ( )
740	def coordinateForPosition ( self , longitude , latitude , altitude = None ) : coords = PROJ ( longitude , latitude ) if altitude is not None : coords = transform ( PROJ , geocentric , coords [ 0 ] , coords [ 1 ] , altitude ) coordinate = numpy . array ( coords ) coordinate = coordinate / self . scale return coordinate . astype ( int )
9011	def index_of_first_consumed_mesh_in_row ( self ) : index = 0 for instruction in self . row_instructions : if instruction is self : break index += instruction . number_of_consumed_meshes else : self . _raise_not_found_error ( ) return index
12539	def find_all_dicom_files ( root_path ) : dicoms = set ( ) try : for fpath in get_all_files ( root_path ) : if is_dicom_file ( fpath ) : dicoms . add ( fpath ) except IOError as ioe : raise IOError ( 'Error reading file {0}.' . format ( fpath ) ) from ioe return dicoms
4059	def _citation_processor ( self , retrieved ) : items = [ ] for cit in retrieved . entries : items . append ( cit [ "content" ] [ 0 ] [ "value" ] ) self . url_params = None return items
11920	def get_object ( self ) : dataframe = self . filter_dataframe ( self . get_dataframe ( ) ) assert self . lookup_url_kwarg in self . kwargs , ( 'Expected view %s to be called with a URL keyword argument ' 'named "%s". Fix your URL conf, or set the `.lookup_field` ' 'attribute on the view correctly.' % ( self . __class__ . __name__ , self . lookup_url_kwarg ) ) try : obj = self . index_row ( dataframe ) except ( IndexError , KeyError , ValueError ) : raise Http404 self . check_object_permissions ( self . request , obj ) return obj
13493	def read ( args ) : if args . config_file is None or not isfile ( args . config_file ) : return logging . info ( "Reading configure file: %s" % args . config_file ) config = cparser . ConfigParser ( ) config . read ( args . config_file ) if not config . has_section ( 'lrcloud' ) : raise RuntimeError ( "Configure file has no [lrcloud] section!" ) for ( name , value ) in config . items ( 'lrcloud' ) : if value == "True" : value = True elif value == "False" : value = False if getattr ( args , name ) is None : setattr ( args , name , value )
11008	def get_project_slug ( self , bet ) : if bet . get ( 'form_params' ) : params = json . loads ( bet [ 'form_params' ] ) return params . get ( 'project' ) return None
11485	def upload ( file_pattern , destination = 'Private' , leaf_folders_as_items = False , reuse_existing = False ) : session . token = verify_credentials ( ) parent_folder_id = None user_folders = session . communicator . list_user_folders ( session . token ) if destination . startswith ( '/' ) : parent_folder_id = _find_resource_id_from_path ( destination ) else : for cur_folder in user_folders : if cur_folder [ 'name' ] == destination : parent_folder_id = cur_folder [ 'folder_id' ] if parent_folder_id is None : print ( 'Unable to locate specified destination. Defaulting to {0}.' . format ( user_folders [ 0 ] [ 'name' ] ) ) parent_folder_id = user_folders [ 0 ] [ 'folder_id' ] for current_file in glob . iglob ( file_pattern ) : current_file = os . path . normpath ( current_file ) if os . path . isfile ( current_file ) : print ( 'Uploading item from {0}' . format ( current_file ) ) _upload_as_item ( os . path . basename ( current_file ) , parent_folder_id , current_file , reuse_existing ) else : _upload_folder_recursive ( current_file , parent_folder_id , leaf_folders_as_items , reuse_existing )
6911	def generate_eb_lightcurve ( times , mags = None , errs = None , paramdists = { 'period' : sps . uniform ( loc = 0.2 , scale = 99.8 ) , 'pdepth' : sps . uniform ( loc = 1.0e-4 , scale = 0.7 ) , 'pduration' : sps . uniform ( loc = 0.01 , scale = 0.44 ) , 'depthratio' : sps . uniform ( loc = 0.01 , scale = 0.99 ) , 'secphase' : sps . norm ( loc = 0.5 , scale = 0.1 ) } , magsarefluxes = False , ) : if mags is None : mags = np . full_like ( times , 0.0 ) if errs is None : errs = np . full_like ( times , 0.0 ) epoch = npr . random ( ) * ( times . max ( ) - times . min ( ) ) + times . min ( ) period = paramdists [ 'period' ] . rvs ( size = 1 ) pdepth = paramdists [ 'pdepth' ] . rvs ( size = 1 ) pduration = paramdists [ 'pduration' ] . rvs ( size = 1 ) depthratio = paramdists [ 'depthratio' ] . rvs ( size = 1 ) secphase = paramdists [ 'secphase' ] . rvs ( size = 1 ) if magsarefluxes and pdepth < 0.0 : pdepth = - pdepth elif not magsarefluxes and pdepth > 0.0 : pdepth = - pdepth modelmags , phase , ptimes , pmags , perrs = ( eclipses . invgauss_eclipses_func ( [ period , epoch , pdepth , pduration , depthratio , secphase ] , times , mags , errs ) ) timeind = np . argsort ( ptimes ) mtimes = ptimes [ timeind ] mmags = modelmags [ timeind ] merrs = perrs [ timeind ] modeldict = { 'vartype' : 'EB' , 'params' : { x : np . asscalar ( y ) for x , y in zip ( [ 'period' , 'epoch' , 'pdepth' , 'pduration' , 'depthratio' ] , [ period , epoch , pdepth , pduration , depthratio ] ) } , 'times' : mtimes , 'mags' : mmags , 'errs' : merrs , 'varperiod' : period , 'varamplitude' : pdepth , } return modeldict
9199	def reversals ( series , left = False , right = False ) : series = iter ( series ) x_last , x = next ( series ) , next ( series ) d_last = ( x - x_last ) if left : yield x_last for x_next in series : if x_next == x : continue d_next = x_next - x if d_last * d_next < 0 : yield x x_last , x = x , x_next d_last = d_next if right : yield x_next
7522	def concat_vcf ( data , names , full ) : if not full : writer = open ( data . outfiles . vcf , 'w' ) else : writer = gzip . open ( data . outfiles . VCF , 'w' ) vcfheader ( data , names , writer ) writer . close ( ) vcfchunks = glob . glob ( data . outfiles . vcf + ".*" ) vcfchunks . sort ( key = lambda x : int ( x . rsplit ( "." ) [ - 1 ] ) ) if not full : writer = open ( data . outfiles . vcf , 'a' ) else : writer = gzip . open ( data . outfiles . VCF , 'a' ) if data . paramsdict [ "assembly_method" ] in [ "reference" , "denovo+reference" ] : cmd = [ "cat" ] + vcfchunks + [ " | sort -k 2,2 -n | sort -k 1,1 -s" ] cmd = " " . join ( cmd ) proc = sps . Popen ( cmd , shell = True , stderr = sps . STDOUT , stdout = writer , close_fds = True ) else : proc = sps . Popen ( [ "cat" ] + vcfchunks , stderr = sps . STDOUT , stdout = writer , close_fds = True ) err = proc . communicate ( ) [ 0 ] if proc . returncode : raise IPyradWarningExit ( "err in concat_vcf: %s" , err ) writer . close ( ) for chunk in vcfchunks : os . remove ( chunk )
3889	async def fire ( self , * args , ** kwargs ) : logger . debug ( 'Fired {}' . format ( self ) ) for observer in self . _observers : gen = observer ( * args , ** kwargs ) if asyncio . iscoroutinefunction ( observer ) : await gen
7280	def quit ( self ) : if self . _process is None : logger . debug ( 'Quit was called after self._process had already been released' ) return try : logger . debug ( 'Quitting OMXPlayer' ) process_group_id = os . getpgid ( self . _process . pid ) os . killpg ( process_group_id , signal . SIGTERM ) logger . debug ( 'SIGTERM Sent to pid: %s' % process_group_id ) self . _process_monitor . join ( ) except OSError : logger . error ( 'Could not find the process to kill' ) self . _process = None
8521	def plot_4 ( data , * args ) : params = nonconstant_parameters ( data ) scores = np . array ( [ d [ 'mean_test_score' ] for d in data ] ) order = np . argsort ( scores ) for key in params . keys ( ) : if params [ key ] . dtype == np . dtype ( 'bool' ) : params [ key ] = params [ key ] . astype ( np . int ) p_list = [ ] for key in params . keys ( ) : x = params [ key ] [ order ] y = scores [ order ] params = params . loc [ order ] try : radius = ( np . max ( x ) - np . min ( x ) ) / 100.0 except : print ( "error making plot4 for '%s'" % key ) continue p_list . append ( build_scatter_tooltip ( x = x , y = y , radius = radius , add_line = False , tt = params , xlabel = key , title = 'Score vs %s' % key ) ) return p_list
10370	def build_edge_data_filter ( annotations : Mapping , partial_match : bool = True ) -> EdgePredicate : @ edge_predicate def annotation_dict_filter ( data : EdgeData ) -> bool : return subdict_matches ( data , annotations , partial_match = partial_match ) return annotation_dict_filter
2080	def callback ( self , pk = None , host_config_key = '' , extra_vars = None ) : url = self . endpoint + '%s/callback/' % pk if not host_config_key : host_config_key = client . get ( url ) . json ( ) [ 'host_config_key' ] post_data = { 'host_config_key' : host_config_key } if extra_vars : post_data [ 'extra_vars' ] = parser . process_extra_vars ( list ( extra_vars ) , force_json = True ) r = client . post ( url , data = post_data , auth = None ) if r . status_code == 201 : return { 'changed' : True }
10035	def add_arguments ( parser ) : parser . add_argument ( '-e' , '--environment' , help = 'Environment name' , required = False , nargs = '+' ) parser . add_argument ( '-w' , '--dont-wait' , help = 'Skip waiting for the app to be deleted' , action = 'store_true' )
5950	def check_file_exists ( self , filename , resolve = 'exception' , force = None ) : def _warn ( x ) : msg = "File {0!r} already exists." . format ( x ) logger . warn ( msg ) warnings . warn ( msg ) return True def _raise ( x ) : msg = "File {0!r} already exists." . format ( x ) logger . error ( msg ) raise IOError ( errno . EEXIST , x , msg ) solutions = { 'ignore' : lambda x : False , 'indicate' : lambda x : True , 'warn' : _warn , 'warning' : _warn , 'exception' : _raise , 'raise' : _raise , } if force is True : resolve = 'ignore' elif force is False : resolve = 'exception' if not os . path . isfile ( filename ) : return False else : return solutions [ resolve ] ( filename )
7616	def typecasted ( func ) : signature = inspect . signature ( func ) . parameters . items ( ) @ wraps ( func ) def wrapper ( * args , ** kwargs ) : args = list ( args ) new_args = [ ] new_kwargs = { } for _ , param in signature : converter = param . annotation if converter is inspect . _empty : converter = lambda a : a if param . kind is param . POSITIONAL_OR_KEYWORD : if args : to_conv = args . pop ( 0 ) new_args . append ( converter ( to_conv ) ) elif param . kind is param . VAR_POSITIONAL : for a in args : new_args . append ( converter ( a ) ) else : for k , v in kwargs . items ( ) : nk , nv = converter ( k , v ) new_kwargs [ nk ] = nv return func ( * new_args , ** new_kwargs ) return wrapper
13274	def default ( self , obj ) : if isinstance ( obj , np . ndarray ) : if obj . flags [ 'C_CONTIGUOUS' ] : obj_data = obj . data else : cont_obj = np . ascontiguousarray ( obj ) assert ( cont_obj . flags [ 'C_CONTIGUOUS' ] ) obj_data = cont_obj . data data_b64 = base64 . b64encode ( obj_data ) return dict ( __ndarray__ = data_b64 , dtype = str ( obj . dtype ) , shape = obj . shape ) elif isinstance ( obj , np . generic ) : return np . asscalar ( obj ) return json . JSONEncoder ( self , obj )
3248	def _get_base ( group , ** conn ) : group [ '_version' ] = 1 group . update ( get_group_api ( group [ 'GroupName' ] , users = False , ** conn ) [ 'Group' ] ) group [ 'CreateDate' ] = get_iso_string ( group [ 'CreateDate' ] ) return group
4372	def get_socket ( self , sessid = '' ) : socket = self . sockets . get ( sessid ) if sessid and not socket : return None if socket is None : socket = Socket ( self , self . config ) self . sockets [ socket . sessid ] = socket else : socket . incr_hits ( ) return socket
7882	def _make_prefix ( self , declared_prefixes ) : used_prefixes = set ( self . _prefixes . values ( ) ) used_prefixes |= set ( declared_prefixes . values ( ) ) while True : prefix = u"ns{0}" . format ( self . _next_id ) self . _next_id += 1 if prefix not in used_prefixes : break return prefix
11382	def do_url_scheme ( parser , token ) : args = token . split_contents ( ) if len ( args ) != 1 : raise template . TemplateSyntaxError ( '%s takes no parameters.' % args [ 0 ] ) return OEmbedURLSchemeNode ( )
8092	def node ( s , node , alpha = 1.0 ) : if s . depth : try : colors . shadow ( dx = 5 , dy = 5 , blur = 10 , alpha = 0.5 * alpha ) except : pass s . _ctx . nofill ( ) s . _ctx . nostroke ( ) if s . fill : s . _ctx . fill ( s . fill . r , s . fill . g , s . fill . b , s . fill . a * alpha ) if s . stroke : s . _ctx . strokewidth ( s . strokewidth ) s . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * alpha * 3 ) r = node . r s . _ctx . oval ( node . x - r , node . y - r , r * 2 , r * 2 )
9844	def __refill_tokenbuffer ( self ) : if len ( self . tokens ) == 0 : self . __tokenize ( self . dxfile . readline ( ) )
10288	def enrich_composites ( graph : BELGraph ) : nodes = list ( get_nodes_by_function ( graph , COMPOSITE ) ) for u in nodes : for v in u . members : graph . add_has_component ( u , v )
4079	def get_version ( ) : version = _get_attrib ( ) . get ( 'version' ) if not version : match = re . search ( r"LanguageTool-?.*?(\S+)$" , get_directory ( ) ) if match : version = match . group ( 1 ) return version
6488	def _get_filter_field ( field_name , field_value ) : filter_field = None if isinstance ( field_value , ValueRange ) : range_values = { } if field_value . lower : range_values . update ( { "gte" : field_value . lower_string } ) if field_value . upper : range_values . update ( { "lte" : field_value . upper_string } ) filter_field = { "range" : { field_name : range_values } } elif _is_iterable ( field_value ) : filter_field = { "terms" : { field_name : field_value } } else : filter_field = { "term" : { field_name : field_value } } return filter_field
13227	def decorator ( decorator_func ) : assert callable ( decorator_func ) , type ( decorator_func ) def _decorator ( func = None , ** kwargs ) : assert func is None or callable ( func ) , type ( func ) if func : return decorator_func ( func , ** kwargs ) else : def _decorator_helper ( func ) : return decorator_func ( func , ** kwargs ) return _decorator_helper return _decorator
2978	def cmd_partition ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) if opts . random : if opts . partitions : raise BlockadeError ( "Either specify individual partitions " "or --random, but not both" ) b . random_partition ( ) else : partitions = [ ] for partition in opts . partitions : names = [ ] for name in partition . split ( "," ) : name = name . strip ( ) if name : names . append ( name ) partitions . append ( names ) if not partitions : raise BlockadeError ( "Either specify individual partitions " "or random" ) b . partition ( partitions )
3366	def _process_flux_dataframe ( flux_dataframe , fva , threshold , floatfmt ) : abs_flux = flux_dataframe [ 'flux' ] . abs ( ) flux_threshold = threshold * abs_flux . max ( ) if fva is None : flux_dataframe = flux_dataframe . loc [ abs_flux >= flux_threshold , : ] . copy ( ) else : flux_dataframe = flux_dataframe . loc [ ( abs_flux >= flux_threshold ) | ( flux_dataframe [ 'fmin' ] . abs ( ) >= flux_threshold ) | ( flux_dataframe [ 'fmax' ] . abs ( ) >= flux_threshold ) , : ] . copy ( ) if fva is None : flux_dataframe [ 'is_input' ] = ( flux_dataframe [ 'flux' ] >= 0 ) flux_dataframe [ 'flux' ] = flux_dataframe [ 'flux' ] . abs ( ) else : def get_direction ( flux , fmin , fmax ) : if flux < 0 : return - 1 elif flux > 0 : return 1 elif ( fmax > 0 ) & ( fmin <= 0 ) : return 1 elif ( fmax < 0 ) & ( fmin >= 0 ) : return - 1 elif ( ( fmax + fmin ) / 2 ) < 0 : return - 1 else : return 1 sign = flux_dataframe . apply ( lambda x : get_direction ( x . flux , x . fmin , x . fmax ) , 1 ) flux_dataframe [ 'is_input' ] = sign == 1 flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] = flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] . multiply ( sign , 0 ) . astype ( 'float' ) . round ( 6 ) flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] = flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] . applymap ( lambda x : x if abs ( x ) > 1E-6 else 0 ) if fva is not None : flux_dataframe [ 'fva_fmt' ] = flux_dataframe . apply ( lambda x : ( "[{0.fmin:" + floatfmt + "}, {0.fmax:" + floatfmt + "}]" ) . format ( x ) , 1 ) flux_dataframe = flux_dataframe . sort_values ( by = [ 'flux' , 'fmax' , 'fmin' , 'id' ] , ascending = [ False , False , False , True ] ) else : flux_dataframe = flux_dataframe . sort_values ( by = [ 'flux' , 'id' ] , ascending = [ False , True ] ) return flux_dataframe
5817	def _write_callback ( connection_id , data_buffer , data_length_pointer ) : try : self = _connection_refs . get ( connection_id ) if not self : socket = _socket_refs . get ( connection_id ) else : socket = self . _socket if not self and not socket : return 0 data_length = deref ( data_length_pointer ) data = bytes_from_buffer ( data_buffer , data_length ) if self and not self . _done_handshake : self . _client_hello += data error = None try : sent = socket . send ( data ) except ( socket_ . error ) as e : error = e . errno if error is not None and error != errno . EAGAIN : if error == errno . ECONNRESET or error == errno . EPIPE : return SecurityConst . errSSLClosedNoNotify return SecurityConst . errSSLClosedAbort if sent != data_length : pointer_set ( data_length_pointer , sent ) return SecurityConst . errSSLWouldBlock return 0 except ( KeyboardInterrupt ) as e : self . _exception = e return SecurityConst . errSSLPeerUserCancelled
5399	def get_filtered_normalized_events ( self ) : user_image = google_v2_operations . get_action_image ( self . _op , _ACTION_USER_COMMAND ) need_ok = google_v2_operations . is_success ( self . _op ) events = { } for event in google_v2_operations . get_events ( self . _op ) : if self . _filter ( event ) : continue mapped , match = self . _map ( event ) name = mapped [ 'name' ] if name == 'ok' : if not need_ok or 'ok' in events : continue if name == 'pulling-image' : if match . group ( 1 ) != user_image : continue events [ name ] = mapped return sorted ( events . values ( ) , key = operator . itemgetter ( 'start-time' ) )
8482	def env_key ( key , default ) : env = key . upper ( ) . replace ( '.' , '_' ) return os . environ . get ( env , default )
6450	def pydocstyle_color ( score ) : score_cutoffs = ( 0 , 10 , 25 , 50 , 100 ) for i in range ( len ( score_cutoffs ) ) : if score <= score_cutoffs [ i ] : return BADGE_COLORS [ i ] return BADGE_COLORS [ - 1 ]
10550	def find_results ( project_id , ** kwargs ) : try : kwargs [ 'project_id' ] = project_id res = _pybossa_req ( 'get' , 'result' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ Result ( result ) for result in res ] else : return res except : raise
5512	def connect ( ) : ftp_class = ftplib . FTP if not SSL else ftplib . FTP_TLS ftp = ftp_class ( timeout = TIMEOUT ) ftp . connect ( HOST , PORT ) ftp . login ( USER , PASSWORD ) if SSL : ftp . prot_p ( ) return ftp
3796	def setup_a_alpha_and_derivatives ( self , i , T = None ) : r if not hasattr ( self , 'kappas' ) : self . kappas = [ ] for Tc , kappa0 , kappa1 , kappa2 , kappa3 in zip ( self . Tcs , self . kappa0s , self . kappa1s , self . kappa2s , self . kappa3s ) : Tr = T / Tc kappa = kappa0 + ( ( kappa1 + kappa2 * ( kappa3 - Tr ) * ( 1. - Tr ** 0.5 ) ) * ( 1. + Tr ** 0.5 ) * ( 0.7 - Tr ) ) self . kappas . append ( kappa ) ( self . a , self . kappa , self . kappa0 , self . kappa1 , self . kappa2 , self . kappa3 , self . Tc ) = ( self . ais [ i ] , self . kappas [ i ] , self . kappa0s [ i ] , self . kappa1s [ i ] , self . kappa2s [ i ] , self . kappa3s [ i ] , self . Tcs [ i ] )
6477	def render ( self , stream ) : encoding = self . option . encoding or self . term . encoding or "utf8" if self . option . color : ramp = self . color_ramp ( self . size . y ) [ : : - 1 ] else : ramp = None if self . cycle >= 1 and self . lines : stream . write ( self . term . csi ( 'cuu' , self . lines ) ) zero = int ( self . null / 4 ) lines = 0 for y in range ( self . screen . size . y ) : if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) for x in range ( self . screen . size . x ) : point = Point ( ( x , y ) ) if point in self . screen : value = self . screen [ point ] if isinstance ( value , int ) : stream . write ( chr ( self . base + value ) . encode ( encoding ) ) else : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( self . term . csi_wrap ( value . encode ( encoding ) , 'bold' ) ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) else : stream . write ( b' ' ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'rmul' ) ) if ramp : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( b'\n' ) lines += 1 stream . flush ( ) self . cycle = self . cycle + 1 self . lines = lines
12479	def rcfile ( appname , section = None , args = { } , strip_dashes = True ) : if strip_dashes : for k in args . keys ( ) : args [ k . lstrip ( '-' ) ] = args . pop ( k ) environ = get_environment ( appname ) if section is None : section = appname config = get_config ( appname , section , args . get ( 'config' , '' ) , args . get ( 'path' , '' ) ) config = merge ( merge ( args , config ) , environ ) if not config : raise IOError ( 'Could not find any rcfile for application ' '{}.' . format ( appname ) ) return config
6237	def draw_buffers ( self , near , far ) : self . ctx . disable ( moderngl . DEPTH_TEST ) helper . draw ( self . gbuffer . color_attachments [ 0 ] , pos = ( 0.0 , 0.0 ) , scale = ( 0.25 , 0.25 ) ) helper . draw ( self . gbuffer . color_attachments [ 1 ] , pos = ( 0.5 , 0.0 ) , scale = ( 0.25 , 0.25 ) ) helper . draw_depth ( self . gbuffer . depth_attachment , near , far , pos = ( 1.0 , 0.0 ) , scale = ( 0.25 , 0.25 ) ) helper . draw ( self . lightbuffer . color_attachments [ 0 ] , pos = ( 1.5 , 0.0 ) , scale = ( 0.25 , 0.25 ) )
2839	def pullup ( self , pin , enabled ) : self . _validate_pin ( pin ) if enabled : self . gppu [ int ( pin / 8 ) ] |= 1 << ( int ( pin % 8 ) ) else : self . gppu [ int ( pin / 8 ) ] &= ~ ( 1 << ( int ( pin % 8 ) ) ) self . write_gppu ( )
11702	def cosine ( vec1 , vec2 ) : if norm ( vec1 ) > 0 and norm ( vec2 ) > 0 : return dot ( vec1 , vec2 ) / ( norm ( vec1 ) * norm ( vec2 ) ) else : return 0.0
10087	def update ( self , * args , ** kwargs ) : super ( Deposit , self ) . update ( * args , ** kwargs )
12930	def get_pos ( vcf_line ) : if not vcf_line : return None vcf_data = vcf_line . strip ( ) . split ( '\t' ) return_data = dict ( ) return_data [ 'chrom' ] = CHROM_INDEX [ vcf_data [ 0 ] ] return_data [ 'pos' ] = int ( vcf_data [ 1 ] ) return return_data
12126	def spec_formatter ( cls , spec ) : " Formats the elements of an argument set appropriately" return type ( spec ) ( ( k , str ( v ) ) for ( k , v ) in spec . items ( ) )
12419	def capture_stderr ( ) : stderr = sys . stderr try : capture_out = StringIO ( ) sys . stderr = capture_out yield capture_out finally : sys . stderr = stderr
3929	def get_auth ( credentials_prompt , refresh_token_cache , manual_login = False ) : with requests . Session ( ) as session : session . headers = { 'user-agent' : USER_AGENT } try : logger . info ( 'Authenticating with refresh token' ) refresh_token = refresh_token_cache . get ( ) if refresh_token is None : raise GoogleAuthError ( "Refresh token not found" ) access_token = _auth_with_refresh_token ( session , refresh_token ) except GoogleAuthError as e : logger . info ( 'Failed to authenticate using refresh token: %s' , e ) logger . info ( 'Authenticating with credentials' ) if manual_login : authorization_code = ( credentials_prompt . get_authorization_code ( ) ) else : authorization_code = _get_authorization_code ( session , credentials_prompt ) access_token , refresh_token = _auth_with_code ( session , authorization_code ) refresh_token_cache . set ( refresh_token ) logger . info ( 'Authentication successful' ) return _get_session_cookies ( session , access_token )
8464	def deploy ( target ) : if not os . getenv ( CIRCLECI_ENV_VAR ) : raise EnvironmentError ( 'Must be on CircleCI to run this script' ) current_branch = os . getenv ( 'CIRCLE_BRANCH' ) if ( target == 'PROD' ) and ( current_branch != 'master' ) : raise EnvironmentError ( ( 'Refusing to deploy to production from branch {current_branch!r}. ' 'Production deploys can only be made from master.' ) . format ( current_branch = current_branch ) ) if target in ( 'PROD' , 'TEST' ) : pypi_username = os . getenv ( '{target}_PYPI_USERNAME' . format ( target = target ) ) pypi_password = os . getenv ( '{target}_PYPI_PASSWORD' . format ( target = target ) ) else : raise ValueError ( "Deploy target must be 'PROD' or 'TEST', got {target!r}." . format ( target = target ) ) if not ( pypi_username and pypi_password ) : raise EnvironmentError ( ( "Missing '{target}_PYPI_USERNAME' and/or '{target}_PYPI_PASSWORD' " "environment variables. These are required to push to PyPI." ) . format ( target = target ) ) os . environ [ 'TWINE_USERNAME' ] = pypi_username os . environ [ 'TWINE_PASSWORD' ] = pypi_password _shell ( 'git config --global user.email "oss@cloverhealth.com"' ) _shell ( 'git config --global user.name "Circle CI"' ) _shell ( 'git config push.default current' ) ret = _shell ( 'make version' , stdout = subprocess . PIPE ) version = ret . stdout . decode ( 'utf-8' ) . strip ( ) print ( 'Deploying version {version!r}...' . format ( version = version ) ) _shell ( 'git tag -f -a {version} -m "Version {version}"' . format ( version = version ) ) _shell ( 'sed -i.bak "s/^__version__ = .*/__version__ = {version!r}/" */version.py' . format ( version = version ) ) _shell ( 'python setup.py sdist bdist_wheel' ) _shell ( 'git add ChangeLog AUTHORS */version.py' ) _shell ( 'git commit --no-verify -m "Merge autogenerated files [skip ci]"' ) _pypi_push ( 'dist' ) _shell ( 'git push --follow-tags' ) print ( 'Deployment complete. Latest version is {version}.' . format ( version = version ) )
12239	def beale ( theta ) : x , y = theta A = 1.5 - x + x * y B = 2.25 - x + x * y ** 2 C = 2.625 - x + x * y ** 3 obj = A ** 2 + B ** 2 + C ** 2 grad = np . array ( [ 2 * A * ( y - 1 ) + 2 * B * ( y ** 2 - 1 ) + 2 * C * ( y ** 3 - 1 ) , 2 * A * x + 4 * B * x * y + 6 * C * x * y ** 2 ] ) return obj , grad
3522	def _hashable_bytes ( data ) : if isinstance ( data , bytes ) : return data elif isinstance ( data , str ) : return data . encode ( 'ascii' ) else : raise TypeError ( data )
3515	def chartbeat_bottom ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ChartbeatBottomNode ( )
1033	def encode ( input , output ) : while True : s = input . read ( MAXBINSIZE ) if not s : break while len ( s ) < MAXBINSIZE : ns = input . read ( MAXBINSIZE - len ( s ) ) if not ns : break s += ns line = binascii . b2a_base64 ( s ) output . write ( line )
990	def scale ( reader , writer , column , start , stop , multiple ) : for i , row in enumerate ( reader ) : if i >= start and i <= stop : row [ column ] = type ( multiple ) ( row [ column ] ) * multiple writer . appendRecord ( row )
10879	def wrap_and_calc_psf ( xpts , ypts , zpts , func , ** kwargs ) : for t in [ xpts , ypts , zpts ] : if len ( t . shape ) != 1 : raise ValueError ( 'xpts,ypts,zpts must be 1D.' ) dx = 1 if xpts [ 0 ] == 0 else 0 dy = 1 if ypts [ 0 ] == 0 else 0 xg , yg , zg = np . meshgrid ( xpts , ypts , zpts , indexing = 'ij' ) xs , ys , zs = [ pts . size for pts in [ xpts , ypts , zpts ] ] to_return = np . zeros ( [ 2 * xs - dx , 2 * ys - dy , zs ] ) up_corner_psf = func ( xg , yg , zg , ** kwargs ) to_return [ xs - dx : , ys - dy : , : ] = up_corner_psf . copy ( ) if dx == 0 : to_return [ : xs - dx , ys - dy : , : ] = up_corner_psf [ : : - 1 , : , : ] . copy ( ) else : to_return [ : xs - dx , ys - dy : , : ] = up_corner_psf [ - 1 : 0 : - 1 , : , : ] . copy ( ) if dy == 0 : to_return [ xs - dx : , : ys - dy , : ] = up_corner_psf [ : , : : - 1 , : ] . copy ( ) else : to_return [ xs - dx : , : ys - dy , : ] = up_corner_psf [ : , - 1 : 0 : - 1 , : ] . copy ( ) if ( dx == 0 ) and ( dy == 0 ) : to_return [ : xs - dx , : ys - dy , : ] = up_corner_psf [ : : - 1 , : : - 1 , : ] . copy ( ) elif ( dx == 0 ) and ( dy != 0 ) : to_return [ : xs - dx , : ys - dy , : ] = up_corner_psf [ : : - 1 , - 1 : 0 : - 1 , : ] . copy ( ) elif ( dy == 0 ) and ( dx != 0 ) : to_return [ : xs - dx , : ys - dy , : ] = up_corner_psf [ - 1 : 0 : - 1 , : : - 1 , : ] . copy ( ) else : to_return [ : xs - dx , : ys - dy , : ] = up_corner_psf [ - 1 : 0 : - 1 , - 1 : 0 : - 1 , : ] . copy ( ) return to_return
5269	def _label_generalized ( self , node ) : if node . is_leaf ( ) : x = { self . _get_word_start_index ( node . idx ) } else : x = { n for ns in node . transition_links for n in ns [ 0 ] . generalized_idxs } node . generalized_idxs = x
10925	def reset ( self , new_damping = None ) : self . _num_iter = 0 self . _inner_run_counter = 0 self . _J_update_counter = self . update_J_frequency self . _fresh_JTJ = False self . _has_run = False if new_damping is not None : self . damping = np . array ( new_damping ) . astype ( 'float' ) self . _set_err_paramvals ( )
1077	def fromordinal ( cls , n ) : y , m , d = _ord2ymd ( n ) return cls ( y , m , d )
12241	def camel ( theta ) : x , y = theta obj = 2 * x ** 2 - 1.05 * x ** 4 + x ** 6 / 6 + x * y + y ** 2 grad = np . array ( [ 4 * x - 4.2 * x ** 3 + x ** 5 + y , x + 2 * y ] ) return obj , grad
5490	def discover ( cls ) : file = os . path . join ( Config . config_dir , Config . config_name ) return cls . from_file ( file )
7848	def get_features ( self ) : l = self . xpath_ctxt . xpathEval ( "d:feature" ) ret = [ ] for f in l : if f . hasProp ( "var" ) : ret . append ( f . prop ( "var" ) . decode ( "utf-8" ) ) return ret
991	def copy ( reader , writer , start , stop , insertLocation = None , tsCol = None ) : assert stop >= start startRows = [ ] copyRows = [ ] ts = None inc = None if tsCol is None : tsCol = reader . getTimestampFieldIdx ( ) for i , row in enumerate ( reader ) : if ts is None : ts = row [ tsCol ] elif inc is None : inc = row [ tsCol ] - ts if i >= start and i <= stop : copyRows . append ( row ) startRows . append ( row ) if insertLocation is None : insertLocation = stop + 1 startRows [ insertLocation : insertLocation ] = copyRows for row in startRows : row [ tsCol ] = ts writer . appendRecord ( row ) ts += inc
1686	def Split ( self ) : googlename = self . RepositoryName ( ) project , rest = os . path . split ( googlename ) return ( project , ) + os . path . splitext ( rest )
856	def seekFromEnd ( self , numRecords ) : self . _file . seek ( self . _getTotalLineCount ( ) - numRecords ) return self . getBookmark ( )
10183	def _aggregations_delete ( aggregation_types = None , start_date = None , end_date = None ) : aggregation_types = ( aggregation_types or list ( current_stats . enabled_aggregations ) ) for a in aggregation_types : aggr_cfg = current_stats . aggregations [ a ] aggregator = aggr_cfg . aggregator_class ( name = aggr_cfg . name , ** aggr_cfg . aggregator_config ) aggregator . delete ( start_date , end_date )
2719	def get_object ( cls , api_token , action_id ) : action = cls ( token = api_token , id = action_id ) action . load_directly ( ) return action
13205	def _parse_title ( self ) : command = LatexCommand ( 'title' , { 'name' : 'short_title' , 'required' : False , 'bracket' : '[' } , { 'name' : 'long_title' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no title' ) self . _title = None self . _short_title = None self . _title = parsed [ 'long_title' ] try : self . _short_title = parsed [ 'short_title' ] except KeyError : self . _logger . warning ( 'lsstdoc has no short title' ) self . _short_title = None
12088	def proto_01_01_HP010 ( abf = exampleABF ) : swhlab . memtest . memtest ( abf ) swhlab . memtest . checkSweep ( abf ) swhlab . plot . save ( abf , tag = "tau" )
12520	def _load_images_and_labels ( self , images , labels = None ) : if not isinstance ( images , ( list , tuple ) ) : raise ValueError ( 'Expected an iterable (list or tuple) of strings or img-like objects. ' 'Got a {}.' . format ( type ( images ) ) ) if not len ( images ) > 0 : raise ValueError ( 'Expected an iterable (list or tuple) of strings or img-like objects ' 'of size higher than 0. Got {} items.' . format ( len ( images ) ) ) if labels is not None and len ( labels ) != len ( images ) : raise ValueError ( 'Expected the same length for image set ({}) and ' 'labels list ({}).' . format ( len ( images ) , len ( labels ) ) ) first_file = images [ 0 ] if first_file : first_img = NeuroImage ( first_file ) else : raise ( 'Error reading image {}.' . format ( repr_imgs ( first_file ) ) ) for idx , image in enumerate ( images ) : try : img = NeuroImage ( image ) self . check_compatibility ( img , first_img ) except : log . exception ( 'Error reading image {}.' . format ( repr_imgs ( image ) ) ) raise else : self . items . append ( img ) self . set_labels ( labels )
7684	def mkclick ( freq , sr = 22050 , duration = 0.1 ) : times = np . arange ( int ( sr * duration ) ) click = np . sin ( 2 * np . pi * times * freq / float ( sr ) ) click *= np . exp ( - times / ( 1e-2 * sr ) ) return click
6863	def tic_single_object_crossmatch ( ra , dec , radius ) : for val in ra , dec , radius : if not isinstance ( val , float ) : raise AssertionError ( 'plz input ra,dec,radius in decimal degrees' ) crossmatchInput = { "fields" : [ { "name" : "ra" , "type" : "float" } , { "name" : "dec" , "type" : "float" } ] , "data" : [ { "ra" : ra , "dec" : dec } ] } request = { "service" : "Mast.Tic.Crossmatch" , "data" : crossmatchInput , "params" : { "raColumn" : "ra" , "decColumn" : "dec" , "radius" : radius } , "format" : "json" , 'removecache' : True } headers , out_string = _mast_query ( request ) out_data = json . loads ( out_string ) return out_data
6439	def euclidean ( src , tar , qval = 2 , normalized = False , alphabet = None ) : return Euclidean ( ) . dist_abs ( src , tar , qval , normalized , alphabet )
12302	def validate ( repo , validator_name = None , filename = None , rulesfiles = None , args = [ ] ) : mgr = plugins_get_mgr ( ) validator_specs = instantiate ( repo , validator_name , filename , rulesfiles ) allresults = [ ] for v in validator_specs : keys = mgr . search ( what = 'validator' , name = v ) [ 'validator' ] for k in keys : validator = mgr . get_by_key ( 'validator' , k ) result = validator . evaluate ( repo , validator_specs [ v ] , args ) allresults . extend ( result ) return allresults
2634	def status ( self ) : if self . provider : status = self . provider . status ( self . engines ) else : status = [ ] return status
551	def __checkMaturity ( self ) : if self . _currentRecordIndex + 1 < self . _MIN_RECORDS_TO_BE_BEST : return if self . _isMature : return metric = self . _getMetrics ( ) [ self . _optimizedMetricLabel ] self . _metricRegression . addPoint ( x = self . _currentRecordIndex , y = metric ) pctChange , absPctChange = self . _metricRegression . getPctChanges ( ) if pctChange is not None and absPctChange <= self . _MATURITY_MAX_CHANGE : self . _jobsDAO . modelSetFields ( self . _modelID , { 'engMatured' : True } ) self . _cmpReason = ClientJobsDAO . CMPL_REASON_STOPPED self . _isMature = True self . _logger . info ( "Model %d has matured (pctChange=%s, n=%d). \n" "Scores = %s\n" "Stopping execution" , self . _modelID , pctChange , self . _MATURITY_NUM_POINTS , self . _metricRegression . _window )
6871	def estimate_achievable_tmid_precision ( snr , t_ingress_min = 10 , t_duration_hr = 2.14 ) : t_ingress = t_ingress_min * u . minute t_duration = t_duration_hr * u . hour theta = t_ingress / t_duration sigma_tc = ( 1 / snr * t_duration * np . sqrt ( theta / 2 ) ) LOGINFO ( 'assuming t_ingress = {:.1f}' . format ( t_ingress ) ) LOGINFO ( 'assuming t_duration = {:.1f}' . format ( t_duration ) ) LOGINFO ( 'measured SNR={:.2f}\n\t' . format ( snr ) + ' . format ( sigma_tc . to ( u . minute ) , sigma_tc . to ( u . hour ) , sigma_tc . to ( u . day ) ) ) return sigma_tc . to ( u . day ) . value
2308	def forward ( self , input ) : return th . nn . functional . linear ( input , self . weight . div ( self . weight . pow ( 2 ) . sum ( 0 ) . sqrt ( ) ) )
8310	def is_list ( str ) : for chunk in str . split ( "\n" ) : chunk = chunk . replace ( "\t" , "" ) if not chunk . lstrip ( ) . startswith ( "*" ) and not re . search ( r"^([0-9]{1,3}\. )" , chunk . lstrip ( ) ) : return False return True
3133	def update_members ( self , list_id , data ) : self . list_id = list_id if 'members' not in data : raise KeyError ( 'The update must have at least one member' ) else : if not len ( data [ 'members' ] ) <= 500 : raise ValueError ( 'You may only batch sub/unsub 500 members at a time' ) for member in data [ 'members' ] : if 'email_address' not in member : raise KeyError ( 'Each list member must have an email_address' ) check_email ( member [ 'email_address' ] ) if 'status' not in member and 'status_if_new' not in member : raise KeyError ( 'Each list member must have either a status or a status_if_new' ) valid_statuses = [ 'subscribed' , 'unsubscribed' , 'cleaned' , 'pending' ] if 'status' in member and member [ 'status' ] not in valid_statuses : raise ValueError ( 'The list member status must be one of "subscribed", "unsubscribed", "cleaned", or ' '"pending"' ) if 'status_if_new' in member and member [ 'status_if_new' ] not in valid_statuses : raise ValueError ( 'The list member status_if_new must be one of "subscribed", "unsubscribed", ' '"cleaned", or "pending"' ) if 'update_existing' not in data : data [ 'update_existing' ] = False return self . _mc_client . _post ( url = self . _build_path ( list_id ) , data = data )
1894	def _send ( self , cmd : str ) : logger . debug ( '>%s' , cmd ) try : self . _proc . stdout . flush ( ) self . _proc . stdin . write ( f'{cmd}\n' ) except IOError as e : raise SolverError ( str ( e ) )
13648	def get_fuel_prices_for_station ( self , station : int ) -> List [ Price ] : response = requests . get ( '{}/prices/station/{}' . format ( API_URL_BASE , station ) , headers = self . _get_headers ( ) , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) data = response . json ( ) return [ Price . deserialize ( data ) for data in data [ 'prices' ] ]
4871	def create ( self , validated_data ) : ret = [ ] for attrs in validated_data : if 'non_field_errors' not in attrs and not any ( isinstance ( attrs [ field ] , list ) for field in attrs ) : ret . append ( self . child . create ( attrs ) ) else : ret . append ( attrs ) return ret
11643	def transform ( self , X ) : X = check_array ( X ) X_rbf = np . empty_like ( X ) if self . copy else X X_in = X if not self . squared : np . power ( X_in , 2 , out = X_rbf ) X_in = X_rbf if self . scale_by_median : scale = self . median_ if self . squared else self . median_ ** 2 gamma = self . gamma * scale else : gamma = self . gamma np . multiply ( X_in , - gamma , out = X_rbf ) np . exp ( X_rbf , out = X_rbf ) return X_rbf
3179	def get ( self , batch_webhook_id , ** queryparams ) : self . batch_webhook_id = batch_webhook_id return self . _mc_client . _get ( url = self . _build_path ( batch_webhook_id ) , ** queryparams )
9331	def cpu_count ( ) : num = os . getenv ( "OMP_NUM_THREADS" ) if num is None : num = os . getenv ( "PBS_NUM_PPN" ) try : return int ( num ) except : return multiprocessing . cpu_count ( )
4189	def window_poisson ( N , alpha = 2 ) : r n = linspace ( - N / 2. , ( N ) / 2. , N ) w = exp ( - alpha * abs ( n ) / ( N / 2. ) ) return w
5996	def plot_border ( mask , should_plot_border , units , kpc_per_arcsec , pointsize , zoom_offset_pixels ) : if should_plot_border and mask is not None : plt . gca ( ) border_pixels = mask . masked_grid_index_to_pixel [ mask . border_pixels ] if zoom_offset_pixels is not None : border_pixels -= zoom_offset_pixels border_arcsec = mask . grid_pixels_to_grid_arcsec ( grid_pixels = border_pixels ) border_units = convert_grid_units ( array = mask , grid_arcsec = border_arcsec , units = units , kpc_per_arcsec = kpc_per_arcsec ) plt . scatter ( y = border_units [ : , 0 ] , x = border_units [ : , 1 ] , s = pointsize , c = 'y' )
1407	def emit ( self , tup , tup_id = None , stream = Stream . DEFAULT_STREAM_ID , direct_task = None , need_task_ids = False ) : self . pplan_helper . check_output_schema ( stream , tup ) custom_target_task_ids = self . pplan_helper . choose_tasks_for_custom_grouping ( stream , tup ) self . pplan_helper . context . invoke_hook_emit ( tup , stream , None ) data_tuple = tuple_pb2 . HeronDataTuple ( ) data_tuple . key = 0 if direct_task is not None : if not isinstance ( direct_task , int ) : raise TypeError ( "direct_task argument needs to be an integer, given: %s" % str ( type ( direct_task ) ) ) data_tuple . dest_task_ids . append ( direct_task ) elif custom_target_task_ids is not None : for task_id in custom_target_task_ids : data_tuple . dest_task_ids . append ( task_id ) if tup_id is not None : tuple_info = TupleHelper . make_root_tuple_info ( stream , tup_id ) if self . acking_enabled : root = data_tuple . roots . add ( ) root . taskid = self . pplan_helper . my_task_id root . key = tuple_info . key self . in_flight_tuples [ tuple_info . key ] = tuple_info else : self . immediate_acks . append ( tuple_info ) tuple_size_in_bytes = 0 start_time = time . time ( ) for obj in tup : serialized = self . serializer . serialize ( obj ) data_tuple . values . append ( serialized ) tuple_size_in_bytes += len ( serialized ) serialize_latency_ns = ( time . time ( ) - start_time ) * system_constants . SEC_TO_NS self . spout_metrics . serialize_data_tuple ( stream , serialize_latency_ns ) super ( SpoutInstance , self ) . admit_data_tuple ( stream_id = stream , data_tuple = data_tuple , tuple_size_in_bytes = tuple_size_in_bytes ) self . total_tuples_emitted += 1 self . spout_metrics . update_emit_count ( stream ) if need_task_ids : sent_task_ids = custom_target_task_ids or [ ] if direct_task is not None : sent_task_ids . append ( direct_task ) return sent_task_ids
11181	def acquire ( self , * args , ** kwargs ) : with self . _stat_lock : self . _waiting += 1 self . _lock . acquire ( * args , ** kwargs ) with self . _stat_lock : self . _locked = True self . _waiting -= 1
11665	def _build_indices ( X , flann_args ) : "Builds FLANN indices for each bag." logger . info ( "Building indices..." ) indices = [ None ] * len ( X ) for i , bag in enumerate ( plog ( X , name = "index building" ) ) : indices [ i ] = idx = FLANNIndex ( ** flann_args ) idx . build_index ( bag ) return indices
561	def isSprintCompleted ( self , sprintIdx ) : numExistingSprints = len ( self . _state [ 'sprints' ] ) if sprintIdx >= numExistingSprints : return False return ( self . _state [ 'sprints' ] [ sprintIdx ] [ 'status' ] == 'completed' )
10387	def build_database ( manager : pybel . Manager , annotation_url : Optional [ str ] = None ) -> None : annotation_url = annotation_url or NEUROMMSIG_DEFAULT_URL annotation = manager . get_namespace_by_url ( annotation_url ) if annotation is None : raise RuntimeError ( 'no graphs in database with given annotation' ) networks = get_networks_using_annotation ( manager , annotation ) dtis = ... for network in networks : graph = network . as_bel ( ) scores = epicom_on_graph ( graph , dtis ) for ( drug_name , subgraph_name ) , score in scores . items ( ) : drug_model = get_drug_model ( manager , drug_name ) subgraph_model = manager . get_annotation_entry ( annotation_url , subgraph_name ) score_model = Score ( network = network , annotation = subgraph_model , drug = drug_model , score = score ) manager . session . add ( score_model ) t = time . time ( ) logger . info ( 'committing scores' ) manager . session . commit ( ) logger . info ( 'committed scores in %.2f seconds' , time . time ( ) - t )
8445	def ls ( github_user , template , long_format ) : github_urls = temple . ls . ls ( github_user , template = template ) for ssh_path , info in github_urls . items ( ) : if long_format : print ( ssh_path , '-' , info [ 'description' ] or '(no project description found)' ) else : print ( ssh_path )
10670	def _split_compound_string_ ( compound_string ) : formula = compound_string . replace ( ']' , '' ) . split ( '[' ) [ 0 ] phase = compound_string . replace ( ']' , '' ) . split ( '[' ) [ 1 ] return formula , phase
3122	def _verify_signature ( message , signature , certs ) : for pem in certs : verifier = Verifier . from_string ( pem , is_x509_cert = True ) if verifier . verify ( message , signature ) : return raise AppIdentityError ( 'Invalid token signature' )
12438	def deserialize ( self , request = None , text = None , format = None ) : if isinstance ( self , Resource ) : if not request : request = self . _request Deserializer = None if format : Deserializer = self . meta . deserializers [ format ] if not Deserializer : media_ranges = request . get ( 'Content-Type' ) if media_ranges : media_types = six . iterkeys ( self . _deserializer_map ) media_type = mimeparse . best_match ( media_types , media_ranges ) if media_type : format = self . _deserializer_map [ media_type ] Deserializer = self . meta . deserializers [ format ] else : pass if Deserializer : try : deserializer = Deserializer ( ) data = deserializer . deserialize ( request = request , text = text ) return data , deserializer except ValueError : pass raise http . exceptions . UnsupportedMediaType ( )
3008	def _credentials_from_request ( request ) : if ( oauth2_settings . storage_model is None or request . user . is_authenticated ( ) ) : return get_storage ( request ) . get ( ) else : return None
1811	def SETNAE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . CF , 1 , 0 ) )
575	def loadJsonValueFromFile ( inputFilePath ) : with open ( inputFilePath ) as fileObj : value = json . load ( fileObj ) return value
4478	def split_storage ( path , default = 'osfstorage' ) : path = norm_remote_path ( path ) for provider in KNOWN_PROVIDERS : if path . startswith ( provider + '/' ) : if six . PY3 : return path . split ( '/' , maxsplit = 1 ) else : return path . split ( '/' , 1 ) return ( default , path )
13352	def monitor ( self , sleep = 5 ) : manager = FileModificationObjectManager ( ) timestamps = { } filebodies = { } for file in self . f_repository : timestamps [ file ] = self . _get_mtime ( file ) filebodies [ file ] = open ( file ) . read ( ) while True : for file in self . f_repository : mtime = timestamps [ file ] fbody = filebodies [ file ] modified = self . _check_modify ( file , mtime , fbody ) if not modified : continue new_mtime = self . _get_mtime ( file ) new_fbody = open ( file ) . read ( ) obj = FileModificationObject ( file , ( mtime , new_mtime ) , ( fbody , new_fbody ) ) timestamps [ file ] = new_mtime filebodies [ file ] = new_fbody manager . add_object ( obj ) yield obj time . sleep ( sleep )
2957	def _get_blockade_id_from_cwd ( self , cwd = None ) : if not cwd : cwd = os . getcwd ( ) parent_dir = os . path . abspath ( cwd ) basename = os . path . basename ( parent_dir ) . lower ( ) blockade_id = re . sub ( r"[^a-z0-9]" , "" , basename ) if not blockade_id : blockade_id = "default" return blockade_id
10479	def _getActions ( self ) : actions = _a11y . AXUIElement . _getActions ( self ) return [ action [ 2 : ] for action in actions ]
6829	def pull ( self , path , use_sudo = False , user = None , force = False ) : if path is None : raise ValueError ( "Path to the working copy is needed to pull from a remote repository." ) options = [ ] if force : options . append ( '--force' ) options = ' ' . join ( options ) cmd = 'git pull %s' % options with cd ( path ) : if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
11388	def parse ( self ) : if self . parsed : return self . callbacks = { } regex = re . compile ( "^{}_?" . format ( self . function_name ) , flags = re . I ) mains = set ( ) body = self . body ast_tree = ast . parse ( self . body , self . path ) for n in ast_tree . body : if hasattr ( n , 'name' ) : if regex . match ( n . name ) : mains . add ( n . name ) if hasattr ( n , 'value' ) : ns = n . value if hasattr ( ns , 'id' ) : if regex . match ( ns . id ) : mains . add ( ns . id ) if hasattr ( n , 'targets' ) : ns = n . targets [ 0 ] if hasattr ( ns , 'id' ) : if regex . match ( ns . id ) : mains . add ( ns . id ) if hasattr ( n , 'names' ) : for ns in n . names : if hasattr ( ns , 'name' ) : if regex . match ( ns . name ) : mains . add ( ns . name ) if getattr ( ns , 'asname' , None ) : if regex . match ( ns . asname ) : mains . add ( ns . asname ) if len ( mains ) > 0 : module = self . module for function_name in mains : cb = getattr ( module , function_name , None ) if cb and callable ( cb ) : self . callbacks [ function_name ] = cb else : raise ParseError ( "no main function found" ) self . parsed = True return len ( self . callbacks ) > 0
3567	def start_scan ( self , timeout_sec = TIMEOUT_SEC ) : self . _scan_started . clear ( ) self . _adapter . StartDiscovery ( ) if not self . _scan_started . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to start scanning!' )
10464	def verifymenucheck ( self , window_name , object_name ) : try : menu_handle = self . _get_menu_handle ( window_name , object_name , False ) try : if menu_handle . AXMenuItemMarkChar : return 1 except atomac . _a11y . Error : pass except LdtpServerException : pass return 0
44	def parse_cmdline_kwargs ( args ) : def parse ( v ) : assert isinstance ( v , str ) try : return eval ( v ) except ( NameError , SyntaxError ) : return v return { k : parse ( v ) for k , v in parse_unknown_args ( args ) . items ( ) }
4371	def spawn ( self , fn , * args , ** kwargs ) : if hasattr ( self , 'exception_handler_decorator' ) : fn = self . exception_handler_decorator ( fn ) new = gevent . spawn ( fn , * args , ** kwargs ) self . jobs . append ( new ) return new
1402	def setTopologyInfo ( self , topology ) : if not topology . execution_state : Log . info ( "No execution state found for: " + topology . name ) return Log . info ( "Setting topology info for topology: " + topology . name ) has_physical_plan = True if not topology . physical_plan : has_physical_plan = False Log . info ( "Setting topology info for topology: " + topology . name ) has_packing_plan = True if not topology . packing_plan : has_packing_plan = False has_tmaster_location = True if not topology . tmaster : has_tmaster_location = False has_scheduler_location = True if not topology . scheduler_location : has_scheduler_location = False topologyInfo = { "name" : topology . name , "id" : topology . id , "logical_plan" : None , "physical_plan" : None , "packing_plan" : None , "execution_state" : None , "tmaster_location" : None , "scheduler_location" : None , } executionState = self . extract_execution_state ( topology ) executionState [ "has_physical_plan" ] = has_physical_plan executionState [ "has_packing_plan" ] = has_packing_plan executionState [ "has_tmaster_location" ] = has_tmaster_location executionState [ "has_scheduler_location" ] = has_scheduler_location executionState [ "status" ] = topology . get_status ( ) topologyInfo [ "metadata" ] = self . extract_metadata ( topology ) topologyInfo [ "runtime_state" ] = self . extract_runtime_state ( topology ) topologyInfo [ "execution_state" ] = executionState topologyInfo [ "logical_plan" ] = self . extract_logical_plan ( topology ) topologyInfo [ "physical_plan" ] = self . extract_physical_plan ( topology ) topologyInfo [ "packing_plan" ] = self . extract_packing_plan ( topology ) topologyInfo [ "tmaster_location" ] = self . extract_tmaster ( topology ) topologyInfo [ "scheduler_location" ] = self . extract_scheduler_location ( topology ) self . topologyInfos [ ( topology . name , topology . state_manager_name ) ] = topologyInfo
6168	def from_bin ( bin_array ) : width = len ( bin_array ) bin_wgts = 2 ** np . arange ( width - 1 , - 1 , - 1 ) return int ( np . dot ( bin_array , bin_wgts ) )
2785	def get_object ( cls , api_token , volume_id ) : volume = cls ( token = api_token , id = volume_id ) volume . load ( ) return volume
5091	def get_clear_catalog_id_action ( description = None ) : description = description or _ ( "Unlink selected objects from existing course catalogs" ) def clear_catalog_id ( modeladmin , request , queryset ) : queryset . update ( catalog = None ) clear_catalog_id . short_description = description return clear_catalog_id
2	def conv_only ( convs = [ ( 32 , 8 , 4 ) , ( 64 , 4 , 2 ) , ( 64 , 3 , 1 ) ] , ** conv_kwargs ) : def network_fn ( X ) : out = tf . cast ( X , tf . float32 ) / 255. with tf . variable_scope ( "convnet" ) : for num_outputs , kernel_size , stride in convs : out = layers . convolution2d ( out , num_outputs = num_outputs , kernel_size = kernel_size , stride = stride , activation_fn = tf . nn . relu , ** conv_kwargs ) return out return network_fn
10032	def execute ( helper , config , args ) : if not helper . application_exists ( ) : helper . create_application ( get ( config , 'app.description' ) ) else : out ( "Application " + get ( config , 'app.app_name' ) + " exists" ) environment_names = [ ] environments_to_wait_for_green = [ ] for env_name , env_config in list ( get ( config , 'app.environments' ) . items ( ) ) : environment_names . append ( env_name ) env_config = parse_env_config ( config , env_name ) if not helper . environment_exists ( env_name ) : option_settings = parse_option_settings ( env_config . get ( 'option_settings' , { } ) ) helper . create_environment ( env_name , solution_stack_name = env_config . get ( 'solution_stack_name' ) , cname_prefix = env_config . get ( 'cname_prefix' , None ) , description = env_config . get ( 'description' , None ) , option_settings = option_settings , tier_name = env_config . get ( 'tier_name' ) , tier_type = env_config . get ( 'tier_type' ) , tier_version = env_config . get ( 'tier_version' ) , version_label = args . version_label ) environments_to_wait_for_green . append ( env_name ) else : out ( "Environment " + env_name ) environments_to_wait_for_term = [ ] if args . delete : environments = helper . get_environments ( ) for env in environments : if env [ 'EnvironmentName' ] not in environment_names : if env [ 'Status' ] != 'Ready' : out ( "Unable to delete " + env [ 'EnvironmentName' ] + " because it's not in status Ready (" + env [ 'Status' ] + ")" ) else : out ( "Deleting environment: " + env [ 'EnvironmentName' ] ) helper . delete_environment ( env [ 'EnvironmentName' ] ) environments_to_wait_for_term . append ( env [ 'EnvironmentName' ] ) if not args . dont_wait and len ( environments_to_wait_for_green ) > 0 : helper . wait_for_environments ( environments_to_wait_for_green , status = 'Ready' , include_deleted = False ) if not args . dont_wait and len ( environments_to_wait_for_term ) > 0 : helper . wait_for_environments ( environments_to_wait_for_term , status = 'Terminated' , include_deleted = False ) out ( "Application initialized" ) return 0
1850	def LOOPNZ ( cpu , target ) : counter_name = { 16 : 'CX' , 32 : 'ECX' , 64 : 'RCX' } [ cpu . address_bit_size ] counter = cpu . write_register ( counter_name , cpu . read_register ( counter_name ) - 1 ) cpu . PC = Operators . ITEBV ( cpu . address_bit_size , counter != 0 , ( cpu . PC + target . read ( ) ) & ( ( 1 << target . size ) - 1 ) , cpu . PC + cpu . instruction . size )
9117	def reset_jails ( confirm = True , keep_cleanser_master = True ) : if value_asbool ( confirm ) and not yesno ( ) : exit ( "Glad I asked..." ) reset_cleansers ( confirm = False ) jails = [ 'appserver' , 'webserver' , 'worker' ] if not value_asbool ( keep_cleanser_master ) : jails . append ( 'cleanser' ) with fab . warn_only ( ) : for jail in jails : fab . run ( 'ezjail-admin delete -fw {jail}' . format ( jail = jail ) ) fab . run ( 'rm /usr/jails/cleanser/usr/home/cleanser/.ssh/authorized_keys' )
12872	def one_of ( these ) : ch = peek ( ) try : if ( ch is EndOfFile ) or ( ch not in these ) : fail ( list ( these ) ) except TypeError : if ch != these : fail ( [ these ] ) next ( ) return ch
2922	def _clear_celery_task_data ( self , my_task ) : if 'task_id' in my_task . internal_data : history = my_task . _get_internal_data ( 'task_history' , [ ] ) history . append ( my_task . _get_internal_data ( 'task_id' ) ) del my_task . internal_data [ 'task_id' ] my_task . _set_internal_data ( task_history = history ) if 'task_state' in my_task . internal_data : del my_task . internal_data [ 'task_state' ] if 'error' in my_task . internal_data : del my_task . internal_data [ 'error' ] if hasattr ( my_task , 'async_call' ) : delattr ( my_task , 'async_call' ) if hasattr ( my_task , 'deserialized' ) : delattr ( my_task , 'deserialized' )
3975	def _env_vars_from_file ( filename ) : def split_env ( env ) : if '=' in env : return env . split ( '=' , 1 ) else : return env , None env = { } for line in open ( filename , 'r' ) : line = line . strip ( ) if line and not line . startswith ( '#' ) : k , v = split_env ( line ) env [ k ] = v return env
9880	def _distances ( value_domain , distance_metric , n_v ) : return np . array ( [ [ distance_metric ( v1 , v2 , i1 = i1 , i2 = i2 , n_v = n_v ) for i2 , v2 in enumerate ( value_domain ) ] for i1 , v1 in enumerate ( value_domain ) ] )
10827	def create ( cls , group , user , state = MembershipState . ACTIVE ) : with db . session . begin_nested ( ) : membership = cls ( user_id = user . get_id ( ) , id_group = group . id , state = state , ) db . session . add ( membership ) return membership
10710	def get_water_heaters ( self ) : water_heaters = [ ] for location in self . locations : _location_id = location . get ( "id" ) for device in location . get ( "equipment" ) : if device . get ( "type" ) == "Water Heater" : water_heater_modes = self . api_interface . get_modes ( device . get ( "id" ) ) water_heater_usage = self . api_interface . get_usage ( device . get ( "id" ) ) water_heater = self . api_interface . get_device ( device . get ( "id" ) ) vacations = self . api_interface . get_vacations ( ) device_vacations = [ ] for vacation in vacations : for equipment in vacation . get ( "participatingEquipment" ) : if equipment . get ( "id" ) == water_heater . get ( "id" ) : device_vacations . append ( EcoNetVacation ( vacation , self . api_interface ) ) water_heaters . append ( EcoNetWaterHeater ( water_heater , water_heater_modes , water_heater_usage , _location_id , device_vacations , self . api_interface ) ) return water_heaters
12092	def proto_02_03_IVfast ( abf = exampleABF ) : av1 , sd1 = swhlab . plot . IV ( abf , .6 , .9 , True ) swhlab . plot . save ( abf , tag = 'iv1' ) Xs = abf . clampValues ( .6 ) abf . saveThing ( [ Xs , av1 ] , 'iv' )
6893	def parallel_starfeatures ( lclist , outdir , lc_catalog_pickle , neighbor_radius_arcsec , maxobjects = None , deredden = True , custom_bandpasses = None , lcformat = 'hat-sql' , lcformatdir = None , nworkers = NCPUS ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if maxobjects : lclist = lclist [ : maxobjects ] with open ( lc_catalog_pickle , 'rb' ) as infd : kdt_dict = pickle . load ( infd ) kdt = kdt_dict [ 'kdtree' ] objlist = kdt_dict [ 'objects' ] [ 'objectid' ] objlcfl = kdt_dict [ 'objects' ] [ 'lcfname' ] tasks = [ ( x , outdir , kdt , objlist , objlcfl , neighbor_radius_arcsec , deredden , custom_bandpasses , lcformat ) for x in lclist ] with ProcessPoolExecutor ( max_workers = nworkers ) as executor : resultfutures = executor . map ( _starfeatures_worker , tasks ) results = [ x for x in resultfutures ] resdict = { os . path . basename ( x ) : y for ( x , y ) in zip ( lclist , results ) } return resdict
325	def rolling_sharpe ( returns , rolling_sharpe_window ) : return returns . rolling ( rolling_sharpe_window ) . mean ( ) / returns . rolling ( rolling_sharpe_window ) . std ( ) * np . sqrt ( APPROX_BDAYS_PER_YEAR )
2766	def get_volume_snapshots ( self ) : data = self . get_data ( "snapshots?resource_type=volume" ) return [ Snapshot ( token = self . token , ** snapshot ) for snapshot in data [ 'snapshots' ] ]
10813	def search ( cls , query , q ) : return query . filter ( Group . name . like ( '%{0}%' . format ( q ) ) )
11017	def signed_number ( number , precision = 2 ) : prefix = '' if number <= 0 else '+' number_str = '{}{:.{precision}f}' . format ( prefix , number , precision = precision ) return number_str
3226	def get_creds_from_kwargs ( kwargs ) : creds = { 'key_file' : kwargs . pop ( 'key_file' , None ) , 'http_auth' : kwargs . pop ( 'http_auth' , None ) , 'project' : kwargs . get ( 'project' , None ) , 'user_agent' : kwargs . pop ( 'user_agent' , None ) , 'api_version' : kwargs . pop ( 'api_version' , 'v1' ) } return ( creds , kwargs )
5053	def prefetch_users ( persistent_course_grades ) : users = User . objects . filter ( id__in = [ grade . user_id for grade in persistent_course_grades ] ) return { user . id : user for user in users }
11656	def fit_transform ( self , X , y = None , ** params ) : X = as_features ( X , stack = True ) X_new = self . transformer . fit_transform ( X . stacked_features , y , ** params ) return self . _gather_outputs ( X , X_new )
213	def from_0to1 ( arr_0to1 , shape , min_value = 0.0 , max_value = 1.0 ) : heatmaps = HeatmapsOnImage ( arr_0to1 , shape , min_value = 0.0 , max_value = 1.0 ) heatmaps . min_value = min_value heatmaps . max_value = max_value return heatmaps
12070	def tryLoadingFrom ( tryPath , moduleName = 'swhlab' ) : if not 'site-packages' in swhlab . __file__ : print ( "loaded custom swhlab module from" , os . path . dirname ( swhlab . __file__ ) ) return while len ( tryPath ) > 5 : sp = tryPath + "/swhlab/" if os . path . isdir ( sp ) and os . path . exists ( sp + "/__init__.py" ) : if not os . path . dirname ( tryPath ) in sys . path : sys . path . insert ( 0 , os . path . dirname ( tryPath ) ) print ( "#" * 80 ) print ( "# WARNING: using site-packages swhlab module" ) print ( "#" * 80 ) tryPath = os . path . dirname ( tryPath ) return
5866	def organization_data_is_valid ( organization_data ) : if organization_data is None : return False if 'id' in organization_data and not organization_data . get ( 'id' ) : return False if 'name' in organization_data and not organization_data . get ( 'name' ) : return False return True
1336	def name ( self ) : names = ( criterion . name ( ) for criterion in self . _criteria ) return '__' . join ( sorted ( names ) )
6375	def stem ( self , word ) : word = normalize ( 'NFC' , text_type ( word . lower ( ) ) ) word = word . translate ( self . _umlauts ) wlen = len ( word ) - 1 if wlen > 3 : if wlen > 5 : if word [ - 3 : ] == 'nen' : return word [ : - 3 ] if wlen > 4 : if word [ - 2 : ] in { 'en' , 'se' , 'es' , 'er' } : return word [ : - 2 ] if word [ - 1 ] in { 'e' , 'n' , 'r' , 's' } : return word [ : - 1 ] return word
6245	def set_time ( self , value : float ) : if value < 0 : value = 0 self . controller . row = self . rps * value
12729	def axes ( self , axes ) : self . amotor . axes = [ axes [ 0 ] ] self . ode_obj . setAxis ( tuple ( axes [ 0 ] ) )
5528	def open ( config , mode = "continue" , zoom = None , bounds = None , single_input_file = None , with_cache = False , debug = False ) : return Mapchete ( MapcheteConfig ( config , mode = mode , zoom = zoom , bounds = bounds , single_input_file = single_input_file , debug = debug ) , with_cache = with_cache )
3834	async def send_offnetwork_invitation ( self , send_offnetwork_invitation_request ) : response = hangouts_pb2 . SendOffnetworkInvitationResponse ( ) await self . _pb_request ( 'devices/sendoffnetworkinvitation' , send_offnetwork_invitation_request , response ) return response
8699	def __write ( self , output , binary = False ) : if not binary : log . debug ( 'write: %s' , output ) else : log . debug ( 'write binary: %s' , hexify ( output ) ) self . _port . write ( output ) self . _port . flush ( )
10257	def get_causal_sink_nodes ( graph : BELGraph , func ) -> Set [ BaseEntity ] : return { node for node in graph if node . function == func and is_causal_sink ( graph , node ) }
9915	def update ( self , instance , validated_data ) : is_primary = validated_data . pop ( "is_primary" , False ) instance = super ( EmailSerializer , self ) . update ( instance , validated_data ) if is_primary : instance . set_primary ( ) return instance
2674	def init ( src , minimal = False ) : templates_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ ) ) , 'project_templates' , ) for filename in os . listdir ( templates_path ) : if ( minimal and filename == 'event.json' ) or filename . endswith ( '.pyc' ) : continue dest_path = os . path . join ( templates_path , filename ) if not os . path . isdir ( dest_path ) : copy ( dest_path , src )
12878	def many_until1 ( these , term ) : first = [ these ( ) ] these_results , term_result = many_until ( these , term ) return ( first + these_results , term_result )
7823	def _final_challenge ( self , challenge ) : if self . _finished : return Failure ( "extra-challenge" ) match = SERVER_FINAL_MESSAGE_RE . match ( challenge ) if not match : logger . debug ( "Bad final message syntax: {0!r}" . format ( challenge ) ) return Failure ( "bad-challenge" ) error = match . group ( "error" ) if error : logger . debug ( "Server returned SCRAM error: {0!r}" . format ( error ) ) return Failure ( u"scram-" + error . decode ( "utf-8" ) ) verifier = match . group ( "verifier" ) if not verifier : logger . debug ( "No verifier value in the final message" ) return Failure ( "bad-succes" ) server_key = self . HMAC ( self . _salted_password , b"Server Key" ) server_signature = self . HMAC ( server_key , self . _auth_message ) if server_signature != a2b_base64 ( verifier ) : logger . debug ( "Server verifier does not match" ) return Failure ( "bad-succes" ) self . _finished = True return Response ( None )
10337	def build_spia_matrices ( nodes : Set [ str ] ) -> Dict [ str , pd . DataFrame ] : nodes = list ( sorted ( nodes ) ) matrices = OrderedDict ( ) for relation in KEGG_RELATIONS : matrices [ relation ] = pd . DataFrame ( 0 , index = nodes , columns = nodes ) return matrices
349	def load_wmt_en_fr_dataset ( path = 'data' ) : path = os . path . join ( path , 'wmt_en_fr' ) _WMT_ENFR_TRAIN_URL = "http://www.statmt.org/wmt10/" _WMT_ENFR_DEV_URL = "http://www.statmt.org/wmt15/" def gunzip_file ( gz_path , new_path ) : logging . info ( "Unpacking %s to %s" % ( gz_path , new_path ) ) with gzip . open ( gz_path , "rb" ) as gz_file : with open ( new_path , "wb" ) as new_file : for line in gz_file : new_file . write ( line ) def get_wmt_enfr_train_set ( path ) : filename = "training-giga-fren.tar" maybe_download_and_extract ( filename , path , _WMT_ENFR_TRAIN_URL , extract = True ) train_path = os . path . join ( path , "giga-fren.release2.fixed" ) gunzip_file ( train_path + ".fr.gz" , train_path + ".fr" ) gunzip_file ( train_path + ".en.gz" , train_path + ".en" ) return train_path def get_wmt_enfr_dev_set ( path ) : filename = "dev-v2.tgz" dev_file = maybe_download_and_extract ( filename , path , _WMT_ENFR_DEV_URL , extract = False ) dev_name = "newstest2013" dev_path = os . path . join ( path , "newstest2013" ) if not ( gfile . Exists ( dev_path + ".fr" ) and gfile . Exists ( dev_path + ".en" ) ) : logging . info ( "Extracting tgz file %s" % dev_file ) with tarfile . open ( dev_file , "r:gz" ) as dev_tar : fr_dev_file = dev_tar . getmember ( "dev/" + dev_name + ".fr" ) en_dev_file = dev_tar . getmember ( "dev/" + dev_name + ".en" ) fr_dev_file . name = dev_name + ".fr" en_dev_file . name = dev_name + ".en" dev_tar . extract ( fr_dev_file , path ) dev_tar . extract ( en_dev_file , path ) return dev_path logging . info ( "Load or Download WMT English-to-French translation > {}" . format ( path ) ) train_path = get_wmt_enfr_train_set ( path ) dev_path = get_wmt_enfr_dev_set ( path ) return train_path , dev_path
13578	def update ( course = False ) : if course : with Spinner . context ( msg = "Updated course metadata." , waitmsg = "Updating course metadata." ) : for course in api . get_courses ( ) : old = None try : old = Course . get ( Course . tid == course [ "id" ] ) except peewee . DoesNotExist : old = None if old : old . details_url = course [ "details_url" ] old . save ( ) continue Course . create ( tid = course [ "id" ] , name = course [ "name" ] , details_url = course [ "details_url" ] ) else : selected = Course . get_selected ( ) print ( "Updating exercise data." ) for exercise in api . get_exercises ( selected ) : old = None try : old = Exercise . byid ( exercise [ "id" ] ) except peewee . DoesNotExist : old = None if old is not None : old . name = exercise [ "name" ] old . course = selected . id old . is_attempted = exercise [ "attempted" ] old . is_completed = exercise [ "completed" ] old . deadline = exercise . get ( "deadline" ) old . is_downloaded = os . path . isdir ( old . path ( ) ) old . return_url = exercise [ "return_url" ] old . zip_url = exercise [ "zip_url" ] old . submissions_url = exercise [ "exercise_submissions_url" ] old . save ( ) download_exercise ( old , update = True ) else : ex = Exercise . create ( tid = exercise [ "id" ] , name = exercise [ "name" ] , course = selected . id , is_attempted = exercise [ "attempted" ] , is_completed = exercise [ "completed" ] , deadline = exercise . get ( "deadline" ) , return_url = exercise [ "return_url" ] , zip_url = exercise [ "zip_url" ] , submissions_url = exercise [ ( "exercise_" "submissions_" "url" ) ] ) ex . is_downloaded = os . path . isdir ( ex . path ( ) ) ex . save ( )
7249	def cancel ( self , workflow_id ) : self . logger . debug ( 'Canceling workflow: ' + workflow_id ) url = '%(wf_url)s/%(wf_id)s/cancel' % { 'wf_url' : self . workflows_url , 'wf_id' : workflow_id } r = self . gbdx_connection . post ( url , data = '' ) r . raise_for_status ( )
9741	async def await_event ( self , event = None , timeout = None ) : if self . event_future is not None : raise Exception ( "Can't wait on multiple events!" ) result = await asyncio . wait_for ( self . _wait_loop ( event ) , timeout ) return result
7969	def _remove_io_handler ( self , handler ) : if handler not in self . io_handlers : return self . io_handlers . remove ( handler ) for thread in self . io_threads : if thread . io_handler is handler : thread . stop ( )
5199	def process_point_value ( cls , command_type , command , index , op_type ) : _log . debug ( 'Processing received point value for index {}: {}' . format ( index , command ) )
5043	def send_messages ( cls , http_request , message_requests ) : deduplicated_messages = set ( message_requests ) for msg_type , text in deduplicated_messages : message_function = getattr ( messages , msg_type ) message_function ( http_request , text )
1365	def get_required_arguments_metricnames ( self ) : try : metricnames = self . get_arguments ( constants . PARAM_METRICNAME ) if not metricnames : raise tornado . web . MissingArgumentError ( constants . PARAM_METRICNAME ) return metricnames except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
1437	def update_reduced_metric ( self , name , value , key = None ) : if name not in self . metrics : Log . error ( "In update_reduced_metric(): %s is not registered in the metric" , name ) if key is None and isinstance ( self . metrics [ name ] , ReducedMetric ) : self . metrics [ name ] . update ( value ) elif key is not None and isinstance ( self . metrics [ name ] , MultiReducedMetric ) : self . metrics [ name ] . update ( key , value ) else : Log . error ( "In update_count(): %s is registered but not supported with this method" , name )
10692	def rgb_to_yiq ( rgb ) : r , g , b = rgb [ 0 ] / 255 , rgb [ 1 ] / 255 , rgb [ 2 ] / 255 y = ( 0.299 * r ) + ( 0.587 * g ) + ( 0.114 * b ) i = ( 0.596 * r ) - ( 0.275 * g ) - ( 0.321 * b ) q = ( 0.212 * r ) - ( 0.528 * g ) + ( 0.311 * b ) return round ( y , 3 ) , round ( i , 3 ) , round ( q , 3 )
4298	def _convert_config_to_stdin ( config , parser ) : keys_empty_values_not_pass = ( '--extra-settings' , '--languages' , '--requirements' , '--template' , '--timezone' ) args = [ ] for key , val in config . items ( SECTION ) : keyp = '--{0}' . format ( key ) action = parser . _option_string_actions [ keyp ] if action . const : try : if config . getboolean ( SECTION , key ) : args . append ( keyp ) except ValueError : args . extend ( [ keyp , val ] ) elif any ( [ i for i in keys_empty_values_not_pass if i in action . option_strings ] ) : if val != '' : args . extend ( [ keyp , val ] ) else : args . extend ( [ keyp , val ] ) return args
9876	def _ordinal_metric ( _v1 , _v2 , i1 , i2 , n_v ) : if i1 > i2 : i1 , i2 = i2 , i1 return ( np . sum ( n_v [ i1 : ( i2 + 1 ) ] ) - ( n_v [ i1 ] + n_v [ i2 ] ) / 2 ) ** 2
12230	def unpatch_locals ( depth = 3 ) : for name , locals_dict in traverse_local_prefs ( depth ) : if isinstance ( locals_dict [ name ] , PatchedLocal ) : locals_dict [ name ] = locals_dict [ name ] . val del get_frame_locals ( depth ) [ __PATCHED_LOCALS_SENTINEL ]
10224	def get_chaotic_pairs ( graph : BELGraph ) -> SetOfNodePairs : cg = get_causal_subgraph ( graph ) results = set ( ) for u , v , d in cg . edges ( data = True ) : if d [ RELATION ] not in CAUSAL_INCREASE_RELATIONS : continue if cg . has_edge ( v , u ) and any ( dd [ RELATION ] in CAUSAL_INCREASE_RELATIONS for dd in cg [ v ] [ u ] . values ( ) ) : results . add ( tuple ( sorted ( [ u , v ] , key = str ) ) ) return results
11015	def publish ( context ) : header ( 'Recording changes...' ) run ( 'git add -A' ) header ( 'Displaying changes...' ) run ( 'git -c color.status=always status' ) if not click . confirm ( '\nContinue publishing' ) : run ( 'git reset HEAD --' ) abort ( context ) header ( 'Saving changes...' ) try : run ( 'git commit -m "{message}"' . format ( message = 'Publishing {}' . format ( choose_commit_emoji ( ) ) ) , capture = True ) except subprocess . CalledProcessError as e : if 'nothing to commit' not in e . stdout : raise else : click . echo ( 'Nothing to commit.' ) header ( 'Pushing to GitHub...' ) branch = get_branch ( ) run ( 'git push origin {branch}:{branch}' . format ( branch = branch ) ) pr_link = get_pr_link ( branch ) if pr_link : click . launch ( pr_link )
9266	def sort_tags_by_date ( self , tags ) : if self . options . verbose : print ( "Sorting tags..." ) tags . sort ( key = lambda x : self . get_time_of_tag ( x ) ) tags . reverse ( ) return tags
1293	def tf_demo_loss ( self , states , actions , terminal , reward , internals , update , reference = None ) : embedding = self . network . apply ( x = states , internals = internals , update = update ) deltas = list ( ) for name in sorted ( actions ) : action = actions [ name ] distr_params = self . distributions [ name ] . parameterize ( x = embedding ) state_action_value = self . distributions [ name ] . state_action_value ( distr_params = distr_params , action = action ) if self . actions_spec [ name ] [ 'type' ] == 'bool' : num_actions = 2 action = tf . cast ( x = action , dtype = util . tf_dtype ( 'int' ) ) else : num_actions = self . actions_spec [ name ] [ 'num_actions' ] one_hot = tf . one_hot ( indices = action , depth = num_actions ) ones = tf . ones_like ( tensor = one_hot , dtype = tf . float32 ) inverted_one_hot = ones - one_hot state_action_values = self . distributions [ name ] . state_action_value ( distr_params = distr_params ) state_action_values = state_action_values + inverted_one_hot * self . expert_margin supervised_selector = tf . reduce_max ( input_tensor = state_action_values , axis = - 1 ) delta = supervised_selector - state_action_value action_size = util . prod ( self . actions_spec [ name ] [ 'shape' ] ) delta = tf . reshape ( tensor = delta , shape = ( - 1 , action_size ) ) deltas . append ( delta ) loss_per_instance = tf . reduce_mean ( input_tensor = tf . concat ( values = deltas , axis = 1 ) , axis = 1 ) loss_per_instance = tf . square ( x = loss_per_instance ) return tf . reduce_mean ( input_tensor = loss_per_instance , axis = 0 )
223	def build_environ ( scope : Scope , body : bytes ) -> dict : environ = { "REQUEST_METHOD" : scope [ "method" ] , "SCRIPT_NAME" : scope . get ( "root_path" , "" ) , "PATH_INFO" : scope [ "path" ] , "QUERY_STRING" : scope [ "query_string" ] . decode ( "ascii" ) , "SERVER_PROTOCOL" : f"HTTP/{scope['http_version']}" , "wsgi.version" : ( 1 , 0 ) , "wsgi.url_scheme" : scope . get ( "scheme" , "http" ) , "wsgi.input" : io . BytesIO ( body ) , "wsgi.errors" : sys . stdout , "wsgi.multithread" : True , "wsgi.multiprocess" : True , "wsgi.run_once" : False , } server = scope . get ( "server" ) or ( "localhost" , 80 ) environ [ "SERVER_NAME" ] = server [ 0 ] environ [ "SERVER_PORT" ] = server [ 1 ] if scope . get ( "client" ) : environ [ "REMOTE_ADDR" ] = scope [ "client" ] [ 0 ] for name , value in scope . get ( "headers" , [ ] ) : name = name . decode ( "latin1" ) if name == "content-length" : corrected_name = "CONTENT_LENGTH" elif name == "content-type" : corrected_name = "CONTENT_TYPE" else : corrected_name = f"HTTP_{name}" . upper ( ) . replace ( "-" , "_" ) value = value . decode ( "latin1" ) if corrected_name in environ : value = environ [ corrected_name ] + "," + value environ [ corrected_name ] = value return environ
3669	def Rachford_Rice_flash_error ( V_over_F , zs , Ks ) : r return sum ( [ zi * ( Ki - 1. ) / ( 1. + V_over_F * ( Ki - 1. ) ) for Ki , zi in zip ( Ks , zs ) ] )
9292	def python_value ( self , value ) : value = super ( OrderedUUIDField , self ) . python_value ( value ) u = binascii . b2a_hex ( value ) value = u [ 8 : 16 ] + u [ 4 : 8 ] + u [ 0 : 4 ] + u [ 16 : 22 ] + u [ 22 : 32 ] return UUID ( value . decode ( ) )
5792	def _cert_callback ( callback , der_cert , reason ) : if not callback : return callback ( x509 . Certificate . load ( der_cert ) , reason )
3898	def compile_protofile ( proto_file_path ) : out_file = tempfile . mkstemp ( ) [ 1 ] try : subprocess . check_output ( [ 'protoc' , '--include_source_info' , '--descriptor_set_out' , out_file , proto_file_path ] ) except subprocess . CalledProcessError as e : sys . exit ( 'protoc returned status {}' . format ( e . returncode ) ) return out_file
13374	def binpath ( * paths ) : package_root = os . path . dirname ( __file__ ) return os . path . normpath ( os . path . join ( package_root , 'bin' , * paths ) )
12827	def add_data ( self , data ) : if not self . _data : self . _data = { } self . _data . update ( data )
459	def predict ( sess , network , X , x , y_op , batch_size = None ) : if batch_size is None : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X , } feed_dict . update ( dp_dict ) return sess . run ( y_op , feed_dict = feed_dict ) else : result = None for X_a , _ in tl . iterate . minibatches ( X , X , batch_size , shuffle = False ) : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X_a , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) if result is None : result = result_a else : result = np . concatenate ( ( result , result_a ) ) if result is None : if len ( X ) % batch_size != 0 : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X [ - ( len ( X ) % batch_size ) : , : ] , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) result = result_a else : if len ( X ) != len ( result ) and len ( X ) % batch_size != 0 : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X [ - ( len ( X ) % batch_size ) : , : ] , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) result = np . concatenate ( ( result , result_a ) ) return result
6654	def pruneCache ( ) : cache_dir = folders . cacheDirectory ( ) def fullpath ( f ) : return os . path . join ( cache_dir , f ) def getMTimeSafe ( f ) : try : return os . stat ( f ) . st_mtime except FileNotFoundError : import time return time . clock ( ) fsutils . mkDirP ( cache_dir ) max_cached_modules = getMaxCachedModules ( ) for f in sorted ( [ f for f in os . listdir ( cache_dir ) if os . path . isfile ( fullpath ( f ) ) and not f . endswith ( '.json' ) and not f . endswith ( '.locked' ) ] , key = lambda f : getMTimeSafe ( fullpath ( f ) ) , reverse = True ) [ max_cached_modules : ] : cache_logger . debug ( 'cleaning up cache file %s' , f ) removeFromCache ( f ) cache_logger . debug ( 'cache pruned to %s items' , max_cached_modules )
2505	def get_extr_license_text ( self , extr_lic ) : text_tripples = list ( self . graph . triples ( ( extr_lic , self . spdx_namespace [ 'extractedText' ] , None ) ) ) if not text_tripples : self . error = True msg = 'Extracted license must have extractedText property' self . logger . log ( msg ) return if len ( text_tripples ) > 1 : self . more_than_one_error ( 'extracted license text' ) return text_tripple = text_tripples [ 0 ] _s , _p , text = text_tripple return text
2884	def connect ( self , callback , * args , ** kwargs ) : if self . is_connected ( callback ) : raise AttributeError ( 'callback is already connected' ) if self . hard_subscribers is None : self . hard_subscribers = [ ] self . hard_subscribers . append ( ( callback , args , kwargs ) )
10281	def get_peripheral_successor_edges ( graph : BELGraph , subgraph : BELGraph ) -> EdgeIterator : for u in subgraph : for _ , v , k in graph . out_edges ( u , keys = True ) : if v not in subgraph : yield u , v , k
8089	def text ( self , txt , x , y , width = None , height = 1000000 , outline = False , draw = True , ** kwargs ) : txt = self . Text ( txt , x , y , width , height , outline = outline , ctx = None , ** kwargs ) if outline : path = txt . path if draw : path . draw ( ) return path else : return txt
9442	def reload_cache_config ( self , call_params ) : path = '/' + self . api_version + '/ReloadCacheConfig/' method = 'POST' return self . request ( path , method , call_params )
2249	def memoize_property ( fget ) : while hasattr ( fget , 'fget' ) : fget = fget . fget attr_name = '_' + fget . __name__ @ functools . wraps ( fget ) def fget_memoized ( self ) : if not hasattr ( self , attr_name ) : setattr ( self , attr_name , fget ( self ) ) return getattr ( self , attr_name ) return property ( fget_memoized )
5339	def __create_dashboard_menu ( self , dash_menu , kibiter_major ) : logger . info ( "Adding dashboard menu" ) if kibiter_major == "6" : menu_resource = ".kibana/doc/metadashboard" mapping_resource = ".kibana/_mapping/doc" mapping = { "dynamic" : "true" } menu = { 'metadashboard' : dash_menu } else : menu_resource = ".kibana/metadashboard/main" mapping_resource = ".kibana/_mapping/metadashboard" mapping = { "dynamic" : "true" } menu = dash_menu menu_url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , menu_resource ) mapping_url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , mapping_resource ) logger . debug ( "Adding mapping for metadashboard" ) res = self . grimoire_con . put ( mapping_url , data = json . dumps ( mapping ) , headers = ES6_HEADER ) try : res . raise_for_status ( ) except requests . exceptions . HTTPError : logger . error ( "Couldn't create mapping for Kibiter menu." ) res = self . grimoire_con . post ( menu_url , data = json . dumps ( menu ) , headers = ES6_HEADER ) try : res . raise_for_status ( ) except requests . exceptions . HTTPError : logger . error ( "Couldn't create Kibiter menu." ) logger . error ( res . json ( ) ) raise
4210	def csvd ( A ) : U , S , V = numpy . linalg . svd ( A ) return U , S , V
8594	def get_group ( self , group_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s?depth=%s' % ( group_id , str ( depth ) ) ) return response
7635	def import_lab ( namespace , filename , infer_duration = True , ** parse_options ) : r annotation = core . Annotation ( namespace ) parse_options . setdefault ( 'sep' , r'\s+' ) parse_options . setdefault ( 'engine' , 'python' ) parse_options . setdefault ( 'header' , None ) parse_options . setdefault ( 'index_col' , False ) parse_options . setdefault ( 'names' , range ( 20 ) ) data = pd . read_csv ( filename , ** parse_options ) data = data . dropna ( how = 'all' , axis = 1 ) if len ( data . columns ) == 2 : data . insert ( 1 , 'duration' , 0 ) if infer_duration : data [ 'duration' ] [ : - 1 ] = data . loc [ : , 0 ] . diff ( ) [ 1 : ] . values else : if infer_duration : data . loc [ : , 1 ] -= data [ 0 ] for row in data . itertuples ( ) : time , duration = row [ 1 : 3 ] value = [ x for x in row [ 3 : ] if x is not None ] [ - 1 ] annotation . append ( time = time , duration = duration , confidence = 1.0 , value = value ) return annotation
11746	def init_app ( self , app ) : if len ( self . _attached_bundles ) == 0 : raise NoBundlesAttached ( "At least one bundle must be attached before initializing Journey" ) for bundle in self . _attached_bundles : processed_bundle = { 'path' : bundle . path , 'description' : bundle . description , 'blueprints' : [ ] } for ( bp , description ) in bundle . blueprints : blueprint = self . _register_blueprint ( app , bp , bundle . path , self . get_bp_path ( bp ) , description ) processed_bundle [ 'blueprints' ] . append ( blueprint ) self . _registered_bundles . append ( processed_bundle )
5908	def make_ndx_captured ( ** kwargs ) : kwargs [ 'stdout' ] = False user_input = kwargs . pop ( 'input' , [ ] ) user_input = [ cmd for cmd in user_input if cmd != 'q' ] kwargs [ 'input' ] = user_input + [ '' , 'q' ] return gromacs . make_ndx ( ** kwargs )
12264	def _send_file_internal ( self , * args , ** kwargs ) : super ( Key , self ) . _send_file_internal ( * args , ** kwargs ) mimicdb . backend . sadd ( tpl . bucket % self . bucket . name , self . name ) mimicdb . backend . hmset ( tpl . key % ( self . bucket . name , self . name ) , dict ( size = self . size , md5 = self . md5 ) )
6741	def get_os_version ( ) : import warnings warnings . filterwarnings ( "ignore" , category = DeprecationWarning ) common_os_version = get_rc ( 'common_os_version' ) if common_os_version : return common_os_version with settings ( warn_only = True ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : ret = _run_or_local ( 'cat /etc/lsb-release' ) if ret . succeeded : return OS ( type = LINUX , distro = UBUNTU , release = re . findall ( r'DISTRIB_RELEASE=([0-9\.]+)' , ret ) [ 0 ] ) ret = _run_or_local ( 'cat /etc/debian_version' ) if ret . succeeded : return OS ( type = LINUX , distro = DEBIAN , release = re . findall ( r'([0-9\.]+)' , ret ) [ 0 ] ) ret = _run_or_local ( 'cat /etc/fedora-release' ) if ret . succeeded : return OS ( type = LINUX , distro = FEDORA , release = re . findall ( r'release ([0-9]+)' , ret ) [ 0 ] ) raise Exception ( 'Unable to determine OS version.' )
3378	def assert_optimal ( model , message = 'optimization failed' ) : status = model . solver . status if status != OPTIMAL : exception_cls = OPTLANG_TO_EXCEPTIONS_DICT . get ( status , OptimizationError ) raise exception_cls ( "{} ({})" . format ( message , status ) )
12923	def start_tag ( self ) : direct_attributes = ( attribute . render ( self ) for attribute in self . render_attributes ) attributes = ( ) if hasattr ( self , '_attributes' ) : attributes = ( '{0}="{1}"' . format ( key , value ) for key , value in self . attributes . items ( ) if value ) rendered_attributes = " " . join ( filter ( bool , chain ( direct_attributes , attributes ) ) ) return '<{0}{1}{2}{3}>' . format ( self . tag , ' ' if rendered_attributes else '' , rendered_attributes , ' /' if self . tag_self_closes else "" )
8945	def url_as_file ( url , ext = None ) : if ext : ext = '.' + ext . strip ( '.' ) url_hint = 'www-{}-' . format ( urlparse ( url ) . hostname or 'any' ) if url . startswith ( 'file://' ) : url = os . path . abspath ( url [ len ( 'file://' ) : ] ) if os . path . isabs ( url ) : with open ( url , 'rb' ) as handle : content = handle . read ( ) else : content = requests . get ( url ) . content with tempfile . NamedTemporaryFile ( suffix = ext or '' , prefix = url_hint , delete = False ) as handle : handle . write ( content ) try : yield handle . name finally : if os . path . exists ( handle . name ) : os . remove ( handle . name )
4330	def gain ( self , gain_db = 0.0 , normalize = True , limiter = False , balance = None ) : if not is_number ( gain_db ) : raise ValueError ( "gain_db must be a number." ) if not isinstance ( normalize , bool ) : raise ValueError ( "normalize must be a boolean." ) if not isinstance ( limiter , bool ) : raise ValueError ( "limiter must be a boolean." ) if balance not in [ None , 'e' , 'B' , 'b' ] : raise ValueError ( "balance must be one of None, 'e', 'B', or 'b'." ) effect_args = [ 'gain' ] if balance is not None : effect_args . append ( '-{}' . format ( balance ) ) if normalize : effect_args . append ( '-n' ) if limiter : effect_args . append ( '-l' ) effect_args . append ( '{:f}' . format ( gain_db ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'gain' ) return self
7335	async def upload_media ( self , file_ , media_type = None , media_category = None , chunked = None , size_limit = None , ** params ) : if isinstance ( file_ , str ) : url = urlparse ( file_ ) if url . scheme . startswith ( 'http' ) : media = await self . _session . get ( file_ ) else : path = urlparse ( file_ ) . path . strip ( " \"'" ) media = await utils . execute ( open ( path , 'rb' ) ) elif hasattr ( file_ , 'read' ) or isinstance ( file_ , bytes ) : media = file_ else : raise TypeError ( "upload_media input must be a file object or a " "filename or binary data or an aiohttp request" ) media_size = await utils . get_size ( media ) if chunked is not None : size_test = False else : size_test = await self . _size_test ( media_size , size_limit ) if isinstance ( media , aiohttp . ClientResponse ) : media = media . content if chunked or ( size_test and chunked is None ) : args = media , media_size , file_ , media_type , media_category response = await self . _chunked_upload ( * args , ** params ) else : response = await self . upload . media . upload . post ( media = media , ** params ) if not hasattr ( file_ , 'read' ) and not getattr ( media , 'closed' , True ) : media . close ( ) return response
2530	def parse_creation_info ( self , ci_term ) : for _s , _p , o in self . graph . triples ( ( ci_term , self . spdx_namespace [ 'creator' ] , None ) ) : try : ent = self . builder . create_entity ( self . doc , six . text_type ( o ) ) self . builder . add_creator ( self . doc , ent ) except SPDXValueError : self . value_error ( 'CREATOR_VALUE' , o ) for _s , _p , o in self . graph . triples ( ( ci_term , self . spdx_namespace [ 'created' ] , None ) ) : try : self . builder . set_created_date ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'CREATED_VALUE' , o ) except CardinalityError : self . more_than_one_error ( 'created' ) break for _s , _p , o in self . graph . triples ( ( ci_term , RDFS . comment , None ) ) : try : self . builder . set_creation_comment ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'CreationInfo comment' ) break for _s , _p , o in self . graph . triples ( ( ci_term , self . spdx_namespace [ 'licenseListVersion' ] , None ) ) : try : self . builder . set_lics_list_ver ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'licenseListVersion' ) break except SPDXValueError : self . value_error ( 'LL_VALUE' , o )
1526	def add_context ( self , err_context , succ_context = None ) : self . err_context = err_context self . succ_context = succ_context
1754	def write_register ( self , register , value ) : self . _publish ( 'will_write_register' , register , value ) value = self . _regfile . write ( register , value ) self . _publish ( 'did_write_register' , register , value ) return value
3949	def execute ( self , using = None ) : if not using : using = self . get_connection ( ) inserted_entities = { } for klass in self . orders : number = self . quantities [ klass ] if klass not in inserted_entities : inserted_entities [ klass ] = [ ] for i in range ( 0 , number ) : entity = self . entities [ klass ] . execute ( using , inserted_entities ) inserted_entities [ klass ] . append ( entity ) return inserted_entities
12555	def sav_to_pandas_savreader ( input_file ) : from savReaderWriter import SavReader lines = [ ] with SavReader ( input_file , returnHeader = True ) as reader : header = next ( reader ) for line in reader : lines . append ( line ) return pd . DataFrame ( data = lines , columns = header )
3379	def add_lp_feasibility ( model ) : obj_vars = [ ] prob = model . problem for met in model . metabolites : s_plus = prob . Variable ( "s_plus_" + met . id , lb = 0 ) s_minus = prob . Variable ( "s_minus_" + met . id , lb = 0 ) model . add_cons_vars ( [ s_plus , s_minus ] ) model . constraints [ met . id ] . set_linear_coefficients ( { s_plus : 1.0 , s_minus : - 1.0 } ) obj_vars . append ( s_plus ) obj_vars . append ( s_minus ) model . objective = prob . Objective ( Zero , sloppy = True , direction = "min" ) model . objective . set_linear_coefficients ( { v : 1.0 for v in obj_vars } )
57	def project ( self , from_shape , to_shape ) : coords_proj = project_coords ( [ ( self . x1 , self . y1 ) , ( self . x2 , self . y2 ) ] , from_shape , to_shape ) return self . copy ( x1 = coords_proj [ 0 ] [ 0 ] , y1 = coords_proj [ 0 ] [ 1 ] , x2 = coords_proj [ 1 ] [ 0 ] , y2 = coords_proj [ 1 ] [ 1 ] , label = self . label )
4722	def trun_enter ( trun ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun::enter" ) trun [ "stamp" ] [ "begin" ] = int ( time . time ( ) ) rcode = 0 for hook in trun [ "hooks" ] [ "enter" ] : rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun::enter { rcode: %r }" % rcode , rcode ) return rcode
5471	def lookup_job_tasks ( provider , statuses , user_ids = None , job_ids = None , job_names = None , task_ids = None , task_attempts = None , labels = None , create_time_min = None , create_time_max = None , max_tasks = 0 , page_size = 0 , summary_output = False ) : tasks_generator = provider . lookup_job_tasks ( statuses , user_ids = user_ids , job_ids = job_ids , job_names = job_names , task_ids = task_ids , task_attempts = task_attempts , labels = labels , create_time_min = create_time_min , create_time_max = create_time_max , max_tasks = max_tasks , page_size = page_size ) for task in tasks_generator : yield _prepare_row ( task , True , summary_output )
2332	def uniform_noise ( points ) : return np . random . rand ( 1 ) * np . random . uniform ( points , 1 ) + random . sample ( [ 2 , - 2 ] , 1 )
5833	def create_ml_configuration_from_datasets ( self , dataset_ids ) : available_columns = self . search_template_client . get_available_columns ( dataset_ids ) search_template = self . search_template_client . create ( dataset_ids , available_columns ) return self . create_ml_configuration ( search_template , available_columns , dataset_ids )
6463	def usage_function ( parser ) : parser . print_usage ( ) print ( '' ) print ( 'available functions:' ) for function in sorted ( FUNCTION ) : doc = FUNCTION [ function ] . __doc__ . strip ( ) . splitlines ( ) [ 0 ] print ( ' %-12s %s' % ( function + ':' , doc ) ) return 0
10068	def index_deposit_after_publish ( sender , action = None , pid = None , deposit = None ) : if action == 'publish' : _ , record = deposit . fetch_published ( ) index_record . delay ( str ( record . id ) )
2202	def ensure_app_cache_dir ( appname , * args ) : from ubelt import util_path dpath = get_app_cache_dir ( appname , * args ) util_path . ensuredir ( dpath ) return dpath
9596	def add_cookie ( self , cookie_dict ) : if not isinstance ( cookie_dict , dict ) : raise TypeError ( 'Type of the cookie must be a dict.' ) if not cookie_dict . get ( 'name' , None ) or not cookie_dict . get ( 'value' , None ) : raise KeyError ( 'Missing required keys, \'name\' and \'value\' must be provided.' ) self . _execute ( Command . ADD_COOKIE , { 'cookie' : cookie_dict } )
6727	def get_name ( ) : if env . vm_type == EC2 : for instance in get_all_running_ec2_instances ( ) : if env . host_string == instance . public_dns_name : name = instance . tags . get ( env . vm_name_tag ) return name else : raise NotImplementedError
1873	def MOVLPD ( cpu , dest , src ) : value = src . read ( ) if src . size == 64 and dest . size == 128 : value = ( dest . read ( ) & 0xffffffffffffffff0000000000000000 ) | Operators . ZEXTEND ( value , 128 ) dest . write ( value )
6601	def put_package ( self , package ) : self . last_package_index += 1 package_index = self . last_package_index package_fullpath = self . package_fullpath ( package_index ) with gzip . open ( package_fullpath , 'wb' ) as f : pickle . dump ( package , f , protocol = pickle . HIGHEST_PROTOCOL ) f . close ( ) result_fullpath = self . result_fullpath ( package_index ) result_dir = os . path . dirname ( result_fullpath ) alphatwirl . mkdir_p ( result_dir ) return package_index
2897	def cancel ( self , success = False ) : self . success = success cancel = [ ] mask = Task . NOT_FINISHED_MASK for task in Task . Iterator ( self . task_tree , mask ) : cancel . append ( task ) for task in cancel : task . cancel ( )
388	def remove_pad_sequences ( sequences , pad_id = 0 ) : sequences_out = copy . deepcopy ( sequences ) for i , _ in enumerate ( sequences ) : for j in range ( 1 , len ( sequences [ i ] ) ) : if sequences [ i ] [ - j ] != pad_id : sequences_out [ i ] = sequences_out [ i ] [ 0 : - j + 1 ] break return sequences_out
217	def append ( self , key : str , value : str ) -> None : append_key = key . lower ( ) . encode ( "latin-1" ) append_value = value . encode ( "latin-1" ) self . _list . append ( ( append_key , append_value ) )
10669	def _get_default_data_path_ ( ) : module_path = os . path . dirname ( sys . modules [ __name__ ] . __file__ ) data_path = os . path . join ( module_path , r'data/rao' ) data_path = os . path . abspath ( data_path ) return data_path
4194	def plot_window ( self ) : from pylab import plot , xlim , grid , title , ylabel , axis x = linspace ( 0 , 1 , self . N ) xlim ( 0 , 1 ) plot ( x , self . data ) grid ( True ) title ( '%s Window (%s points)' % ( self . name . capitalize ( ) , self . N ) ) ylabel ( 'Amplitude' ) axis ( [ 0 , 1 , 0 , 1.1 ] )
9067	def _lml_optimal_scale ( self ) : assert self . _optimal [ "scale" ] n = len ( self . _y ) lml = - self . _df * log2pi - self . _df - n * log ( self . scale ) lml -= sum ( npsum ( log ( D ) ) for D in self . _D ) return lml / 2
8180	def add_node ( self , id , radius = 8 , style = style . DEFAULT , category = "" , label = None , root = False , properties = { } ) : if self . has_key ( id ) : return self [ id ] if not isinstance ( style , str ) and style . __dict__ . has_key [ "name" ] : style = style . name n = node ( self , id , radius , style , category , label , properties ) self [ n . id ] = n self . nodes . append ( n ) if root : self . root = n return n
12967	def random ( self , cascadeFetch = False ) : matchedKeys = list ( self . getPrimaryKeys ( ) ) obj = None while matchedKeys and not obj : key = matchedKeys . pop ( random . randint ( 0 , len ( matchedKeys ) - 1 ) ) obj = self . get ( key , cascadeFetch = cascadeFetch ) return obj
206	def offer ( self , p , e : Event ) : existing = self . events_scan . setdefault ( p , ( [ ] , [ ] , [ ] , [ ] ) if USE_VERTICAL else ( [ ] , [ ] , [ ] ) ) existing [ e . type ] . append ( e )
5076	def is_course_run_upgradeable ( course_run ) : now = datetime . datetime . now ( pytz . UTC ) for seat in course_run . get ( 'seats' , [ ] ) : if seat . get ( 'type' ) == 'verified' : upgrade_deadline = parse_datetime_handle_invalid ( seat . get ( 'upgrade_deadline' ) ) return not upgrade_deadline or upgrade_deadline > now return False
2327	def orient_undirected_graph ( self , data , graph ) : self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) self . arguments [ '{SCORE}' ] = self . score self . arguments [ '{BETA}' ] = str ( self . beta ) self . arguments [ '{OPTIM}' ] = str ( self . optim ) . upper ( ) self . arguments [ '{ALPHA}' ] = str ( self . alpha ) whitelist = DataFrame ( list ( nx . edges ( graph ) ) , columns = [ "from" , "to" ] ) blacklist = DataFrame ( list ( nx . edges ( nx . DiGraph ( DataFrame ( - nx . adj_matrix ( graph , weight = None ) . to_dense ( ) + 1 , columns = list ( graph . nodes ( ) ) , index = list ( graph . nodes ( ) ) ) ) ) ) , columns = [ "from" , "to" ] ) results = self . _run_bnlearn ( data , whitelist = whitelist , blacklist = blacklist , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
10621	def get_element_mass_dictionary ( self ) : element_symbols = self . material . elements element_masses = self . get_element_masses ( ) return { s : m for s , m in zip ( element_symbols , element_masses ) }
9856	def get_data ( self , ** kwargs ) : limit = int ( kwargs . get ( 'limit' , 288 ) ) end_date = kwargs . get ( 'end_date' , False ) if end_date and isinstance ( end_date , datetime . datetime ) : end_date = self . convert_datetime ( end_date ) if self . mac_address is not None : service_address = 'devices/%s' % self . mac_address self . api_instance . log ( 'SERVICE ADDRESS: %s' % service_address ) data = dict ( limit = limit ) if end_date : data . update ( { 'endDate' : end_date } ) self . api_instance . log ( 'DATA:' ) self . api_instance . log ( data ) return self . api_instance . api_call ( service_address , ** data )
4257	def copy ( src , dst , symlink = False , rellink = False ) : func = os . symlink if symlink else shutil . copy2 if symlink and os . path . lexists ( dst ) : os . remove ( dst ) if rellink : func ( os . path . relpath ( src , os . path . dirname ( dst ) ) , dst ) else : func ( src , dst )
5892	def get_urls ( self ) : urls = patterns ( '' , url ( r'^upload/$' , self . admin_site . admin_view ( self . handle_upload ) , name = 'quill-file-upload' ) , ) return urls + super ( QuillAdmin , self ) . get_urls ( )
5527	def backend_version ( backend , childprocess = None ) : if childprocess is None : childprocess = childprocess_default_value ( ) if not childprocess : return _backend_version ( backend ) else : return run_in_childprocess ( _backend_version , None , backend )
88	def is_float_array ( val ) : return is_np_array ( val ) and issubclass ( val . dtype . type , np . floating )
4598	def read_from ( self , data , pad = 0 ) : for i in range ( self . BEGIN , self . END + 1 ) : index = self . index ( i , len ( data ) ) yield pad if index is None else data [ index ]
11002	def pack_args ( self ) : mapper = { 'psf-kfki' : 'kfki' , 'psf-alpha' : 'alpha' , 'psf-n2n1' : 'n2n1' , 'psf-sigkf' : 'sigkf' , 'psf-sph6-ab' : 'sph6_ab' , 'psf-laser-wavelength' : 'laser_wavelength' , 'psf-pinhole-width' : 'pinhole_width' } bads = [ self . zscale , 'psf-zslab' ] d = { } for k , v in iteritems ( mapper ) : if k in self . param_dict : d [ v ] = self . param_dict [ k ] d . update ( { 'polar_angle' : self . polar_angle , 'normalize' : self . normalize , 'include_K3_det' : self . use_J1 } ) if self . polychromatic : d . update ( { 'nkpts' : self . nkpts } ) d . update ( { 'k_dist' : self . k_dist } ) if self . do_pinhole : d . update ( { 'nlpts' : self . num_line_pts } ) d . update ( { 'use_laggauss' : True } ) return d
5346	def compose_mailing_lists ( projects , data ) : for p in [ project for project in data if len ( data [ project ] [ 'mailing_lists' ] ) > 0 ] : if 'mailing_lists' not in projects [ p ] : projects [ p ] [ 'mailing_lists' ] = [ ] urls = [ url [ 'url' ] . replace ( 'mailto:' , '' ) for url in data [ p ] [ 'mailing_lists' ] if url [ 'url' ] not in projects [ p ] [ 'mailing_lists' ] ] projects [ p ] [ 'mailing_lists' ] += urls for p in [ project for project in data if len ( data [ project ] [ 'dev_list' ] ) > 0 ] : if 'mailing_lists' not in projects [ p ] : projects [ p ] [ 'mailing_lists' ] = [ ] mailing_list = data [ p ] [ 'dev_list' ] [ 'url' ] . replace ( 'mailto:' , '' ) projects [ p ] [ 'mailing_lists' ] . append ( mailing_list ) return projects
2928	def write_to_package_zip ( self , filename , data ) : self . manifest [ filename ] = md5hash ( data ) self . package_zip . writestr ( filename , data )
12337	def pip_r ( self , requirements , raise_on_error = True ) : cmd = "pip install -r %s" % requirements return self . wait ( cmd , raise_on_error = raise_on_error )
12961	def count ( self ) : conn = self . _get_connection ( ) numFilters = len ( self . filters ) numNotFilters = len ( self . notFilters ) if numFilters + numNotFilters == 0 : return conn . scard ( self . _get_ids_key ( ) ) if numNotFilters == 0 : if numFilters == 1 : ( filterFieldName , filterValue ) = self . filters [ 0 ] return conn . scard ( self . _get_key_for_index ( filterFieldName , filterValue ) ) indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] return len ( conn . sinter ( indexKeys ) ) notIndexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . notFilters ] if numFilters == 0 : return len ( conn . sdiff ( self . _get_ids_key ( ) , * notIndexKeys ) ) indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] tempKey = self . _getTempKey ( ) pipeline = conn . pipeline ( ) pipeline . sinterstore ( tempKey , * indexKeys ) pipeline . sdiff ( tempKey , * notIndexKeys ) pipeline . delete ( tempKey ) pks = pipeline . execute ( ) [ 1 ] return len ( pks )
2823	def convert_lrelu ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting lrelu ...' ) if names == 'short' : tf_name = 'lRELU' + random_string ( 3 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) leakyrelu = keras . layers . LeakyReLU ( alpha = params [ 'alpha' ] , name = tf_name ) layers [ scope_name ] = leakyrelu ( layers [ inputs [ 0 ] ] )
5098	def graph2dict ( g , return_dict_of_dict = True ) : if not isinstance ( g , nx . DiGraph ) : g = QueueNetworkDiGraph ( g ) dict_of_dicts = nx . to_dict_of_dicts ( g ) if return_dict_of_dict : return dict_of_dicts else : return { k : list ( val . keys ( ) ) for k , val in dict_of_dicts . items ( ) }
2173	def authorization_url ( self , url , state = None , ** kwargs ) : state = state or self . new_state ( ) return ( self . _client . prepare_request_uri ( url , redirect_uri = self . redirect_uri , scope = self . scope , state = state , ** kwargs ) , state , )
1119	def listdir ( path ) : try : cached_mtime , list = cache [ path ] del cache [ path ] except KeyError : cached_mtime , list = - 1 , [ ] mtime = os . stat ( path ) . st_mtime if mtime != cached_mtime : list = os . listdir ( path ) list . sort ( ) cache [ path ] = mtime , list return list
9423	def _load_metadata ( self , handle ) : rarinfo = self . _read_header ( handle ) while rarinfo : self . filelist . append ( rarinfo ) self . NameToInfo [ rarinfo . filename ] = rarinfo self . _process_current ( handle , constants . RAR_SKIP ) rarinfo = self . _read_header ( handle )
2232	def _register_numpy_extensions ( self ) : import numpy as np numpy_floating_types = ( np . float16 , np . float32 , np . float64 ) if hasattr ( np , 'float128' ) : numpy_floating_types = numpy_floating_types + ( np . float128 , ) @ self . add_iterable_check def is_object_ndarray ( data ) : return isinstance ( data , np . ndarray ) and data . dtype . kind == 'O' @ self . register ( np . ndarray ) def hash_numpy_array ( data ) : if data . dtype . kind == 'O' : msg = 'directly hashing ndarrays with dtype=object is unstable' raise TypeError ( msg ) else : header = b'' . join ( _hashable_sequence ( ( len ( data . shape ) , data . shape ) ) ) dtype = b'' . join ( _hashable_sequence ( data . dtype . descr ) ) hashable = header + dtype + data . tobytes ( ) prefix = b'NDARR' return prefix , hashable @ self . register ( ( np . int64 , np . int32 , np . int16 , np . int8 ) + ( np . uint64 , np . uint32 , np . uint16 , np . uint8 ) ) def _hash_numpy_int ( data ) : return _convert_to_hashable ( int ( data ) ) @ self . register ( numpy_floating_types ) def _hash_numpy_float ( data ) : return _convert_to_hashable ( float ( data ) ) @ self . register ( np . random . RandomState ) def _hash_numpy_random_state ( data ) : hashable = b'' . join ( _hashable_sequence ( data . get_state ( ) ) ) prefix = b'RNG' return prefix , hashable
6404	def get_feature ( vector , feature ) : if feature not in _FEATURE_MASK : raise AttributeError ( "feature must be one of: '" + "', '" . join ( ( 'consonantal' , 'sonorant' , 'syllabic' , 'labial' , 'round' , 'coronal' , 'anterior' , 'distributed' , 'dorsal' , 'high' , 'low' , 'back' , 'tense' , 'pharyngeal' , 'ATR' , 'voice' , 'spread_glottis' , 'constricted_glottis' , 'continuant' , 'strident' , 'lateral' , 'delayed_release' , 'nasal' , ) ) + "'" ) mask = _FEATURE_MASK [ feature ] pos_mask = mask >> 1 retvec = [ ] for char in vector : if char < 0 : retvec . append ( float ( 'NaN' ) ) else : masked = char & mask if masked == 0 : retvec . append ( 0 ) elif masked == mask : retvec . append ( 2 ) elif masked & pos_mask : retvec . append ( 1 ) else : retvec . append ( - 1 ) return retvec
366	def projective_transform_by_points ( x , src , dst , map_args = None , output_shape = None , order = 1 , mode = 'constant' , cval = 0.0 , clip = True , preserve_range = False ) : if map_args is None : map_args = { } if isinstance ( src , list ) : src = np . array ( src ) if isinstance ( dst , list ) : dst = np . array ( dst ) if np . max ( x ) > 1 : x = x / 255 m = transform . ProjectiveTransform ( ) m . estimate ( dst , src ) warped = transform . warp ( x , m , map_args = map_args , output_shape = output_shape , order = order , mode = mode , cval = cval , clip = clip , preserve_range = preserve_range ) return warped
10580	def calculate ( self , ** state ) : super ( ) . calculate ( ** state ) return self . mm * self . P / R / state [ "T" ]
2593	def get_last_checkpoint ( rundir = "runinfo" ) : if not os . path . isdir ( rundir ) : return [ ] dirs = sorted ( os . listdir ( rundir ) ) if len ( dirs ) == 0 : return [ ] last_runid = dirs [ - 1 ] last_checkpoint = os . path . abspath ( '{}/{}/checkpoint' . format ( rundir , last_runid ) ) if ( not ( os . path . isdir ( last_checkpoint ) ) ) : return [ ] return [ last_checkpoint ]
4976	def render_page_with_error_code_message ( request , context_data , error_code , log_message ) : LOGGER . error ( log_message ) messages . add_generic_error_message_with_code ( request , error_code ) return render ( request , ENTERPRISE_GENERAL_ERROR_PAGE , context = context_data , status = 404 , )
4022	def docker_vm_is_running ( ) : running_vms = check_output_demoted ( [ 'VBoxManage' , 'list' , 'runningvms' ] ) for line in running_vms . splitlines ( ) : if '"{}"' . format ( constants . VM_MACHINE_NAME ) in line : return True return False
9997	def del_cells ( self , name ) : if name in self . cells : cells = self . cells [ name ] self . cells . del_item ( name ) self . inherit ( ) self . model . spacegraph . update_subspaces ( self ) elif name in self . dynamic_spaces : cells = self . dynamic_spaces . pop ( name ) self . dynamic_spaces . set_update ( ) else : raise KeyError ( "Cells '%s' does not exist" % name ) NullImpl ( cells )
5685	def increment_day_start_ut ( self , day_start_ut , n_days = 1 ) : old_tz = self . set_current_process_time_zone ( ) day0 = time . localtime ( day_start_ut + 43200 ) dayN = time . mktime ( day0 [ : 2 ] + ( day0 [ 2 ] + n_days , ) + ( 12 , 00 , 0 , 0 , 0 , - 1 ) ) - 43200 set_process_timezone ( old_tz ) return dayN
11059	def run ( self , start = True ) : if not self . is_setup : raise NotSetupError self . webserver . start ( ) first_connect = True try : while self . runnable : if self . reconnect_needed : if not self . sc . rtm_connect ( with_team_state = start ) : return False self . reconnect_needed = False if first_connect : first_connect = False self . plugins . connect ( ) try : events = self . sc . rtm_read ( ) except AttributeError : self . log . exception ( 'Something has failed in the slack rtm library. This is fatal.' ) self . runnable = False events = [ ] except : self . log . exception ( 'Unhandled exception in rtm_read()' ) self . reconnect_needed = True events = [ ] for e in events : try : self . _handle_event ( e ) except KeyboardInterrupt : self . runnable = False except : self . log . exception ( 'Unhandled exception in event handler' ) sleep ( 0.1 ) except KeyboardInterrupt : pass except : self . log . exception ( 'Unhandled exception' )
7382	def has_edge_within_group ( self , group ) : assert group in self . nodes . keys ( ) , "{0} not one of the group of nodes" . format ( group ) nodelist = self . nodes [ group ] for n1 , n2 in self . simplified_edges ( ) : if n1 in nodelist and n2 in nodelist : return True
4119	def twosided_2_onesided ( data ) : assert len ( data ) % 2 == 0 N = len ( data ) psd = np . array ( data [ 0 : N // 2 + 1 ] ) * 2. psd [ 0 ] /= 2. psd [ - 1 ] = data [ - 1 ] return psd
2140	def disassociate ( self , group , parent , ** kwargs ) : parent_id = self . lookup_with_inventory ( parent , kwargs . get ( 'inventory' , None ) ) [ 'id' ] group_id = self . lookup_with_inventory ( group , kwargs . get ( 'inventory' , None ) ) [ 'id' ] return self . _disassoc ( 'children' , parent_id , group_id )
9201	def extract_cycles ( series , left = False , right = False ) : points = deque ( ) for x in reversals ( series , left = left , right = right ) : points . append ( x ) while len ( points ) >= 3 : X = abs ( points [ - 2 ] - points [ - 1 ] ) Y = abs ( points [ - 3 ] - points [ - 2 ] ) if X < Y : break elif len ( points ) == 3 : yield points [ 0 ] , points [ 1 ] , 0.5 points . popleft ( ) else : yield points [ - 3 ] , points [ - 2 ] , 1.0 last = points . pop ( ) points . pop ( ) points . pop ( ) points . append ( last ) else : while len ( points ) > 1 : yield points [ 0 ] , points [ 1 ] , 0.5 points . popleft ( )
10231	def list_abundance_expansion ( graph : BELGraph ) -> None : mapping = { node : flatten_list_abundance ( node ) for node in graph if isinstance ( node , ListAbundance ) } relabel_nodes ( graph , mapping , copy = False )
317	def perf_stats_bootstrap ( returns , factor_returns = None , return_stats = True , ** kwargs ) : bootstrap_values = OrderedDict ( ) for stat_func in SIMPLE_STAT_FUNCS : stat_name = STAT_FUNC_NAMES [ stat_func . __name__ ] bootstrap_values [ stat_name ] = calc_bootstrap ( stat_func , returns ) if factor_returns is not None : for stat_func in FACTOR_STAT_FUNCS : stat_name = STAT_FUNC_NAMES [ stat_func . __name__ ] bootstrap_values [ stat_name ] = calc_bootstrap ( stat_func , returns , factor_returns = factor_returns ) bootstrap_values = pd . DataFrame ( bootstrap_values ) if return_stats : stats = bootstrap_values . apply ( calc_distribution_stats ) return stats . T [ [ 'mean' , 'median' , '5%' , '95%' ] ] else : return bootstrap_values
11692	def set_fields ( self , changeset ) : self . id = int ( changeset . get ( 'id' ) ) self . user = changeset . get ( 'user' ) self . uid = changeset . get ( 'uid' ) self . editor = changeset . get ( 'created_by' , None ) self . review_requested = changeset . get ( 'review_requested' , False ) self . host = changeset . get ( 'host' , 'Not reported' ) self . bbox = changeset . get ( 'bbox' ) . wkt self . comment = changeset . get ( 'comment' , 'Not reported' ) self . source = changeset . get ( 'source' , 'Not reported' ) self . imagery_used = changeset . get ( 'imagery_used' , 'Not reported' ) self . date = datetime . strptime ( changeset . get ( 'created_at' ) , '%Y-%m-%dT%H:%M:%SZ' ) self . suspicion_reasons = [ ] self . is_suspect = False self . powerfull_editor = False
3091	def locked_delete ( self ) : if self . _cache : self . _cache . delete ( self . _key_name ) self . _delete_entity ( )
10599	def create_template ( material , path , show = False ) : file_name = 'dataset-%s.csv' % material . lower ( ) file_path = os . path . join ( path , file_name ) with open ( file_path , 'w' , newline = '' ) as csvfile : writer = csv . writer ( csvfile , delimiter = ',' , quotechar = '"' , quoting = csv . QUOTE_MINIMAL ) writer . writerow ( [ 'Name' , material ] ) writer . writerow ( [ 'Description' , '<Add a data set description ' 'here.>' ] ) writer . writerow ( [ 'Reference' , '<Add a reference to the source of ' 'the data set here.>' ] ) writer . writerow ( [ 'Temperature' , '<parameter 1 name>' , '<parameter 2 name>' , '<parameter 3 name>' ] ) writer . writerow ( [ 'T' , '<parameter 1 display symbol>' , '<parameter 2 display symbol>' , '<parameter 3 display symbol>' ] ) writer . writerow ( [ 'K' , '<parameter 1 units>' , '<parameter 2 units>' , '<parameter 3 units>' ] ) writer . writerow ( [ 'T' , '<parameter 1 symbol>' , '<parameter 2 symbol>' , '<parameter 3 symbol>' ] ) for i in range ( 10 ) : writer . writerow ( [ 100.0 + i * 50 , float ( i ) , 10.0 + i , 100.0 + i ] ) if show is True : webbrowser . open_new ( file_path )
7509	def _save ( self ) : fulldict = copy . deepcopy ( self . __dict__ ) for i , j in fulldict . items ( ) : if isinstance ( j , Params ) : fulldict [ i ] = j . __dict__ fulldumps = json . dumps ( fulldict , sort_keys = False , indent = 4 , separators = ( "," , ":" ) , ) assemblypath = os . path . join ( self . dirs , self . name + ".tet.json" ) if not os . path . exists ( self . dirs ) : os . mkdir ( self . dirs ) done = 0 while not done : try : with open ( assemblypath , 'w' ) as jout : jout . write ( fulldumps ) done = 1 except ( KeyboardInterrupt , SystemExit ) : print ( '.' ) continue
12219	def _make_all_matchers ( cls , parameters ) : for name , param in parameters : annotation = param . annotation if annotation is not Parameter . empty : yield name , cls . _make_param_matcher ( annotation , param . kind )
13817	def _ConvertFieldValuePair ( js , message ) : names = [ ] message_descriptor = message . DESCRIPTOR for name in js : try : field = message_descriptor . fields_by_camelcase_name . get ( name , None ) if not field : raise ParseError ( 'Message type "{0}" has no field named "{1}".' . format ( message_descriptor . full_name , name ) ) if name in names : raise ParseError ( 'Message type "{0}" should not have multiple "{1}" fields.' . format ( message . DESCRIPTOR . full_name , name ) ) names . append ( name ) if field . containing_oneof is not None : oneof_name = field . containing_oneof . name if oneof_name in names : raise ParseError ( 'Message type "{0}" should not have multiple "{1}" ' 'oneof fields.' . format ( message . DESCRIPTOR . full_name , oneof_name ) ) names . append ( oneof_name ) value = js [ name ] if value is None : message . ClearField ( field . name ) continue if _IsMapEntry ( field ) : message . ClearField ( field . name ) _ConvertMapFieldValue ( value , message , field ) elif field . label == descriptor . FieldDescriptor . LABEL_REPEATED : message . ClearField ( field . name ) if not isinstance ( value , list ) : raise ParseError ( 'repeated field {0} must be in [] which is ' '{1}.' . format ( name , value ) ) if field . cpp_type == descriptor . FieldDescriptor . CPPTYPE_MESSAGE : for item in value : sub_message = getattr ( message , field . name ) . add ( ) if ( item is None and sub_message . DESCRIPTOR . full_name != 'google.protobuf.Value' ) : raise ParseError ( 'null is not allowed to be used as an element' ' in a repeated field.' ) _ConvertMessage ( item , sub_message ) else : for item in value : if item is None : raise ParseError ( 'null is not allowed to be used as an element' ' in a repeated field.' ) getattr ( message , field . name ) . append ( _ConvertScalarFieldValue ( item , field ) ) elif field . cpp_type == descriptor . FieldDescriptor . CPPTYPE_MESSAGE : sub_message = getattr ( message , field . name ) _ConvertMessage ( value , sub_message ) else : setattr ( message , field . name , _ConvertScalarFieldValue ( value , field ) ) except ParseError as e : if field and field . containing_oneof is None : raise ParseError ( 'Failed to parse {0} field: {1}' . format ( name , e ) ) else : raise ParseError ( str ( e ) ) except ValueError as e : raise ParseError ( 'Failed to parse {0} field: {1}.' . format ( name , e ) ) except TypeError as e : raise ParseError ( 'Failed to parse {0} field: {1}.' . format ( name , e ) )
13528	def printoptions ( ) : x = json . dumps ( environment . options , indent = 4 , sort_keys = True , skipkeys = True , cls = MyEncoder ) print ( x )
13812	def FindMethodByName ( self , name ) : for method in self . methods : if name == method . name : return method return None
1700	def consume ( self , consume_function ) : from heronpy . streamlet . impl . consumebolt import ConsumeStreamlet consume_streamlet = ConsumeStreamlet ( consume_function , self ) self . _add_child ( consume_streamlet ) return
7196	def ndwi ( self ) : data = self . _read ( self [ self . _ndwi_bands , ... ] ) . astype ( np . float32 ) return ( data [ 1 , : , : ] - data [ 0 , : , : ] ) / ( data [ 0 , : , : ] + data [ 1 , : , : ] )
11964	def _hex_to_dec ( ip , check = True ) : if check and not is_hex ( ip ) : raise ValueError ( '_hex_to_dec: invalid IP: "%s"' % ip ) if isinstance ( ip , int ) : ip = hex ( ip ) return int ( str ( ip ) , 16 )
783	def jobGetDemand ( self , ) : rows = self . _getMatchingRowsWithRetries ( self . _jobs , dict ( status = self . STATUS_RUNNING ) , [ self . _jobs . pubToDBNameDict [ f ] for f in self . _jobs . jobDemandNamedTuple . _fields ] ) return [ self . _jobs . jobDemandNamedTuple . _make ( r ) for r in rows ]
12521	def to_file ( self , output_file , smooth_fwhm = 0 , outdtype = None ) : outmat , mask_indices , mask_shape = self . to_matrix ( smooth_fwhm , outdtype ) exporter = ExportData ( ) content = { 'data' : outmat , 'labels' : self . labels , 'mask_indices' : mask_indices , 'mask_shape' : mask_shape , } if self . others : content . update ( self . others ) log . debug ( 'Creating content in file {}.' . format ( output_file ) ) try : exporter . save_variables ( output_file , content ) except Exception as exc : raise Exception ( 'Error saving variables to file {}.' . format ( output_file ) ) from exc
7111	def predict ( self , X ) : if isinstance ( X [ 0 ] , list ) : return [ self . estimator . tag ( x ) for x in X ] return self . estimator . tag ( X )
4902	def course_modal ( context , course = None ) : if course : context . update ( { 'course_image_uri' : course . get ( 'course_image_uri' , '' ) , 'course_title' : course . get ( 'course_title' , '' ) , 'course_level_type' : course . get ( 'course_level_type' , '' ) , 'course_short_description' : course . get ( 'course_short_description' , '' ) , 'course_effort' : course . get ( 'course_effort' , '' ) , 'course_full_description' : course . get ( 'course_full_description' , '' ) , 'expected_learning_items' : course . get ( 'expected_learning_items' , [ ] ) , 'staff' : course . get ( 'staff' , [ ] ) , 'premium_modes' : course . get ( 'premium_modes' , [ ] ) , } ) return context
9075	def sendMultiPart ( smtp , gpg_context , sender , recipients , subject , text , attachments ) : sent = 0 for to in recipients : if not to . startswith ( '<' ) : uid = '<%s>' % to else : uid = to if not checkRecipient ( gpg_context , uid ) : continue msg = MIMEMultipart ( ) msg [ 'From' ] = sender msg [ 'To' ] = to msg [ 'Subject' ] = subject msg [ "Date" ] = formatdate ( localtime = True ) msg . preamble = u'This is an email in encrypted multipart format.' attach = MIMEText ( str ( gpg_context . encrypt ( text . encode ( 'utf-8' ) , uid , always_trust = True ) ) ) attach . set_charset ( 'UTF-8' ) msg . attach ( attach ) for attachment in attachments : with open ( attachment , 'rb' ) as fp : attach = MIMEBase ( 'application' , 'octet-stream' ) attach . set_payload ( str ( gpg_context . encrypt_file ( fp , uid , always_trust = True ) ) ) attach . add_header ( 'Content-Disposition' , 'attachment' , filename = basename ( '%s.pgp' % attachment ) ) msg . attach ( attach ) smtp . begin ( ) smtp . sendmail ( sender , to , msg . as_string ( ) ) smtp . quit ( ) sent += 1 return sent
2906	def _assign_new_thread_id ( self , recursive = True ) : self . __class__ . thread_id_pool += 1 self . thread_id = self . __class__ . thread_id_pool if not recursive : return self . thread_id for child in self : child . thread_id = self . thread_id return self . thread_id
7058	def s3_put_file ( local_file , bucket , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 's3' ) try : client . upload_file ( local_file , bucket , os . path . basename ( local_file ) ) return 's3://%s/%s' % ( bucket , os . path . basename ( local_file ) ) except Exception as e : LOGEXCEPTION ( 'could not upload %s to bucket: %s' % ( local_file , bucket ) ) if raiseonfail : raise return None
8569	def remove_loadbalanced_nic ( self , datacenter_id , loadbalancer_id , nic_id ) : response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s/balancednics/%s' % ( datacenter_id , loadbalancer_id , nic_id ) , method = 'DELETE' ) return response
8099	def copy ( self , graph ) : s = styles ( graph ) s . guide = self . guide . copy ( graph ) dict . __init__ ( s , [ ( v . name , v . copy ( ) ) for v in self . values ( ) ] ) return s
106	def avg_pool ( arr , block_size , cval = 0 , preserve_dtype = True ) : return pool ( arr , block_size , np . average , cval = cval , preserve_dtype = preserve_dtype )
10855	def sphere_analytical_gaussian_trim ( dr , a , alpha = 0.2765 , cut = 1.6 ) : m = np . abs ( dr ) <= cut rr = dr [ m ] t = - rr / ( alpha * np . sqrt ( 2 ) ) q = 0.5 * ( 1 + erf ( t ) ) - np . sqrt ( 0.5 / np . pi ) * ( alpha / ( rr + a + 1e-10 ) ) * np . exp ( - t * t ) ans = 0 * dr ans [ m ] = q ans [ dr > cut ] = 0 ans [ dr < - cut ] = 1 return ans
1843	def JNS ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . SF , target . read ( ) , cpu . PC )
7158	def add ( self , * args , ** kwargs ) : if 'question' in kwargs and isinstance ( kwargs [ 'question' ] , Question ) : question = kwargs [ 'question' ] else : question = Question ( * args , ** kwargs ) self . questions . setdefault ( question . key , [ ] ) . append ( question ) return question
283	def plot_long_short_holdings ( returns , positions , legend_loc = 'upper left' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) positions = positions . drop ( 'cash' , axis = 'columns' ) positions = positions . replace ( 0 , np . nan ) df_longs = positions [ positions > 0 ] . count ( axis = 1 ) df_shorts = positions [ positions < 0 ] . count ( axis = 1 ) lf = ax . fill_between ( df_longs . index , 0 , df_longs . values , color = 'g' , alpha = 0.5 , lw = 2.0 ) sf = ax . fill_between ( df_shorts . index , 0 , df_shorts . values , color = 'r' , alpha = 0.5 , lw = 2.0 ) bf = patches . Rectangle ( [ 0 , 0 ] , 1 , 1 , color = 'darkgoldenrod' ) leg = ax . legend ( [ lf , sf , bf ] , [ 'Long (max: %s, min: %s)' % ( df_longs . max ( ) , df_longs . min ( ) ) , 'Short (max: %s, min: %s)' % ( df_shorts . max ( ) , df_shorts . min ( ) ) , 'Overlap' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) leg . get_frame ( ) . set_edgecolor ( 'black' ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) ax . set_title ( 'Long and short holdings' ) ax . set_ylabel ( 'Holdings' ) ax . set_xlabel ( '' ) return ax
2625	def submit ( self , command = 'sleep 1' , blocksize = 1 , tasks_per_node = 1 , job_name = "parsl.auto" ) : job_name = "parsl.auto.{0}" . format ( time . time ( ) ) wrapped_cmd = self . launcher ( command , tasks_per_node , self . nodes_per_block ) [ instance , * rest ] = self . spin_up_instance ( command = wrapped_cmd , job_name = job_name ) if not instance : logger . error ( "Failed to submit request to EC2" ) return None logger . debug ( "Started instance_id: {0}" . format ( instance . instance_id ) ) state = translate_table . get ( instance . state [ 'Name' ] , "PENDING" ) self . resources [ instance . instance_id ] = { "job_id" : instance . instance_id , "instance" : instance , "status" : state } return instance . instance_id
2794	def get_object ( cls , api_token , image_id_or_slug ) : if cls . _is_string ( image_id_or_slug ) : image = cls ( token = api_token , slug = image_id_or_slug ) image . load ( use_slug = True ) else : image = cls ( token = api_token , id = image_id_or_slug ) image . load ( ) return image
4342	def reverse ( self ) : effect_args = [ 'reverse' ] self . effects . extend ( effect_args ) self . effects_log . append ( 'reverse' ) return self
13743	def get_schema ( self ) : if not self . schema : raise NotImplementedError ( 'You must provide a schema value or override the get_schema method' ) return self . conn . create_schema ( ** self . schema )
8159	def edit ( self , id , * args , ** kw ) : if args and kw : return if args and type ( args [ 0 ] ) == dict : fields = [ k for k in args [ 0 ] ] v = [ args [ 0 ] [ k ] for k in args [ 0 ] ] if kw : fields = [ k for k in kw ] v = [ kw [ k ] for k in kw ] sql = "update " + self . _name + " set " + "=?, " . join ( fields ) + "=? where " + self . _key + "=" + unicode ( id ) self . _db . _cur . execute ( sql , v ) self . _db . _i += 1 if self . _db . _i >= self . _db . _commit : self . _db . _i = 0 self . _db . _con . commit ( )
13311	def site_path ( self ) : if platform == 'win' : return unipath ( self . path , 'Lib' , 'site-packages' ) py_ver = 'python{0}' . format ( sys . version [ : 3 ] ) return unipath ( self . path , 'lib' , py_ver , 'site-packages' )
12829	def parse_conll ( self , texts : List [ str ] , retry_count : int = 0 ) -> List [ str ] : post_data = { 'texts' : texts , 'output_type' : 'conll' } try : response = requests . post ( f'http://{self.hostname}:{self.port}' , json = post_data , headers = { 'Connection' : 'close' } ) response . raise_for_status ( ) except ( requests . exceptions . ConnectionError , requests . exceptions . Timeout ) as server_error : raise ServerError ( server_error , self . hostname , self . port ) except requests . exceptions . HTTPError as http_error : raise http_error else : try : return response . json ( ) except json . JSONDecodeError as json_exception : if retry_count == self . retries : self . log_error ( response . text ) raise Exception ( 'Json Decoding error cannot parse this ' f':\n{response.text}' ) return self . parse_conll ( texts , retry_count + 1 )
5653	def execute ( cur , * args ) : stmt = args [ 0 ] if len ( args ) > 1 : stmt = stmt . replace ( '%' , '%%' ) . replace ( '?' , '%r' ) print ( stmt % ( args [ 1 ] ) ) return cur . execute ( * args )
8460	def write_temple_config ( temple_config , template , version ) : with open ( temple . constants . TEMPLE_CONFIG_FILE , 'w' ) as temple_config_file : versioned_config = { ** temple_config , ** { '_version' : version , '_template' : template } , } yaml . dump ( versioned_config , temple_config_file , Dumper = yaml . SafeDumper )
1101	def context_diff ( a , b , fromfile = '' , tofile = '' , fromfiledate = '' , tofiledate = '' , n = 3 , lineterm = '\n' ) : r prefix = dict ( insert = '+ ' , delete = '- ' , replace = '! ' , equal = ' ' ) started = False for group in SequenceMatcher ( None , a , b ) . get_grouped_opcodes ( n ) : if not started : started = True fromdate = '\t%s' % ( fromfiledate ) if fromfiledate else '' todate = '\t%s' % ( tofiledate ) if tofiledate else '' yield '*** %s%s%s' % ( fromfile , fromdate , lineterm ) yield '--- %s%s%s' % ( tofile , todate , lineterm ) first , last = group [ 0 ] , group [ - 1 ] yield '***************' + lineterm file1_range = _format_range_context ( first [ 1 ] , last [ 2 ] ) yield '*** %s ****%s' % ( file1_range , lineterm ) if any ( tag in ( 'replace' , 'delete' ) for tag , _ , _ , _ , _ in group ) : for tag , i1 , i2 , _ , _ in group : if tag != 'insert' : for line in a [ i1 : i2 ] : yield prefix [ tag ] + line file2_range = _format_range_context ( first [ 3 ] , last [ 4 ] ) yield '--- %s ----%s' % ( file2_range , lineterm ) if any ( tag in ( 'replace' , 'insert' ) for tag , _ , _ , _ , _ in group ) : for tag , _ , _ , j1 , j2 in group : if tag != 'delete' : for line in b [ j1 : j2 ] : yield prefix [ tag ] + line
4539	def multi ( method ) : @ functools . wraps ( method ) def multi ( self , address = '' ) : values = flask . request . values address = urllib . parse . unquote_plus ( address ) if address and values and not address . endswith ( '.' ) : address += '.' result = { } for a in values or '' : try : if not self . project : raise ValueError ( 'No Project is currently loaded' ) ed = editor . Editor ( address + a , self . project ) result [ address + a ] = { 'value' : method ( self , ed , a ) } except : if self . project : traceback . print_exc ( ) result [ address + a ] = { 'error' : 'Could not multi addr %s' % a } return flask . jsonify ( result ) return multi
6603	def package_fullpath ( self , package_index ) : ret = os . path . join ( self . path , self . package_relpath ( package_index ) ) return ret
11405	def record_drop_duplicate_fields ( record ) : out = { } position = 0 tags = sorted ( record . keys ( ) ) for tag in tags : fields = record [ tag ] out [ tag ] = [ ] current_fields = set ( ) for full_field in fields : field = ( tuple ( full_field [ 0 ] ) , ) + full_field [ 1 : 4 ] if field not in current_fields : current_fields . add ( field ) position += 1 out [ tag ] . append ( full_field [ : 4 ] + ( position , ) ) return out
2170	def command ( method = None , ** kwargs ) : def actual_decorator ( method ) : method . _cli_command = True method . _cli_command_attrs = kwargs return method if method and isinstance ( method , types . FunctionType ) : return actual_decorator ( method ) else : return actual_decorator
9301	def retrieve ( self , cursor ) : assert isinstance ( cursor , dict ) , "expected cursor type 'dict'" query = self . get_query ( ) assert isinstance ( query , peewee . Query ) query return query . get ( ** cursor )
6428	def encode ( self , word , lang = 'en' ) : if lang == 'es' : return self . _phonetic_spanish . encode ( self . _spanish_metaphone . encode ( word ) ) word = self . _soundex . encode ( self . _metaphone . encode ( word ) ) word = word [ 0 ] . translate ( self . _trans ) + word [ 1 : ] return word
5963	def plot ( self , ** kwargs ) : columns = kwargs . pop ( 'columns' , Ellipsis ) maxpoints = kwargs . pop ( 'maxpoints' , self . maxpoints_default ) transform = kwargs . pop ( 'transform' , lambda x : x ) method = kwargs . pop ( 'method' , "mean" ) ax = kwargs . pop ( 'ax' , None ) if columns is Ellipsis or columns is None : columns = numpy . arange ( self . array . shape [ 0 ] ) if len ( columns ) == 0 : raise MissingDataError ( "plot() needs at least one column of data" ) if len ( self . array . shape ) == 1 or self . array . shape [ 0 ] == 1 : a = numpy . ravel ( self . array ) X = numpy . arange ( len ( a ) ) a = numpy . vstack ( ( X , a ) ) columns = [ 0 ] + [ c + 1 for c in columns ] else : a = self . array color = kwargs . pop ( 'color' , self . default_color_cycle ) try : cmap = matplotlib . cm . get_cmap ( color ) colors = cmap ( matplotlib . colors . Normalize ( ) ( numpy . arange ( len ( columns [ 1 : ] ) , dtype = float ) ) ) except TypeError : colors = cycle ( utilities . asiterable ( color ) ) if ax is None : ax = plt . gca ( ) a = self . decimate ( method , numpy . asarray ( transform ( a ) ) [ columns ] , maxpoints = maxpoints ) ma = numpy . ma . MaskedArray ( a , mask = numpy . logical_not ( numpy . isfinite ( a ) ) ) for column , color in zip ( range ( 1 , len ( columns ) ) , colors ) : if len ( ma [ column ] ) == 0 : warnings . warn ( "No data to plot for column {column:d}" . format ( ** vars ( ) ) , category = MissingDataWarning ) kwargs [ 'color' ] = color ax . plot ( ma [ 0 ] , ma [ column ] , ** kwargs ) return ax
184	def coords_almost_equals ( self , other , max_distance = 1e-6 , points_per_edge = 8 ) : if isinstance ( other , LineString ) : pass elif isinstance ( other , tuple ) : other = LineString ( [ other ] ) else : other = LineString ( other ) if len ( self . coords ) == 0 and len ( other . coords ) == 0 : return True elif 0 in [ len ( self . coords ) , len ( other . coords ) ] : return False self_subd = self . subdivide ( points_per_edge ) other_subd = other . subdivide ( points_per_edge ) dist_self2other = self_subd . compute_pointwise_distances ( other_subd ) dist_other2self = other_subd . compute_pointwise_distances ( self_subd ) dist = max ( np . max ( dist_self2other ) , np . max ( dist_other2self ) ) return dist < max_distance
4608	def nolist ( self , account ) : assert callable ( self . blockchain . account_whitelist ) return self . blockchain . account_whitelist ( account , lists = [ ] , account = self )
10242	def get_evidences_by_pmid ( graph : BELGraph , pmids : Union [ str , Iterable [ str ] ] ) : result = defaultdict ( set ) for _ , _ , _ , data in filter_edges ( graph , build_pmid_inclusion_filter ( pmids ) ) : result [ data [ CITATION ] [ CITATION_REFERENCE ] ] . add ( data [ EVIDENCE ] ) return dict ( result )
182	def to_heatmap ( self , image_shape , size_lines = 1 , size_points = 0 , antialiased = True , raise_if_out_of_image = False ) : from . heatmaps import HeatmapsOnImage return HeatmapsOnImage ( self . draw_heatmap_array ( image_shape , size_lines = size_lines , size_points = size_points , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) , shape = image_shape )
13056	def _plugin_endpoint_rename ( fn_name , instance ) : if instance and instance . namespaced : fn_name = "r_{0}_{1}" . format ( instance . name , fn_name [ 2 : ] ) return fn_name
7875	def element_to_unicode ( element ) : if hasattr ( ElementTree , 'tounicode' ) : return ElementTree . tounicode ( "element" ) elif sys . version_info . major < 3 : return unicode ( ElementTree . tostring ( element ) ) else : return ElementTree . tostring ( element , encoding = "unicode" )
13825	def FromJsonString ( self , value ) : if len ( value ) < 1 or value [ - 1 ] != 's' : raise ParseError ( 'Duration must end with letter "s": {0}.' . format ( value ) ) try : pos = value . find ( '.' ) if pos == - 1 : self . seconds = int ( value [ : - 1 ] ) self . nanos = 0 else : self . seconds = int ( value [ : pos ] ) if value [ 0 ] == '-' : self . nanos = int ( round ( float ( '-0{0}' . format ( value [ pos : - 1 ] ) ) * 1e9 ) ) else : self . nanos = int ( round ( float ( '0{0}' . format ( value [ pos : - 1 ] ) ) * 1e9 ) ) except ValueError : raise ParseError ( 'Couldn\'t parse duration: {0}.' . format ( value ) )
13355	def profil_annuel ( df , func = 'mean' ) : func = _get_funky ( func ) res = df . groupby ( lambda x : x . month ) . aggregate ( func ) res . index = [ cal . month_name [ i ] for i in range ( 1 , 13 ) ] return res
2566	def udp_messenger ( domain_name , UDP_IP , UDP_PORT , sock_timeout , message ) : try : if message is None : raise ValueError ( "message was none" ) encoded_message = bytes ( message , "utf-8" ) if encoded_message is None : raise ValueError ( "utf-8 encoding of message failed" ) if domain_name : try : UDP_IP = socket . gethostbyname ( domain_name ) except Exception : pass if UDP_IP is None : raise Exception ( "UDP_IP is None" ) if UDP_PORT is None : raise Exception ( "UDP_PORT is None" ) sock = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) sock . settimeout ( sock_timeout ) sock . sendto ( bytes ( message , "utf-8" ) , ( UDP_IP , UDP_PORT ) ) sock . close ( ) except socket . timeout : logger . debug ( "Failed to send usage tracking data: socket timeout" ) except OSError as e : logger . debug ( "Failed to send usage tracking data: OSError: {}" . format ( e ) ) except Exception as e : logger . debug ( "Failed to send usage tracking data: Exception: {}" . format ( e ) )
3320	def refresh ( self , token , timeout ) : assert token in self . _dict , "Lock must exist" assert timeout == - 1 or timeout > 0 if timeout < 0 or timeout > LockStorageDict . LOCK_TIME_OUT_MAX : timeout = LockStorageDict . LOCK_TIME_OUT_MAX self . _lock . acquire_write ( ) try : lock = self . _dict [ token ] lock [ "timeout" ] = timeout lock [ "expire" ] = time . time ( ) + timeout self . _dict [ token ] = lock self . _flush ( ) finally : self . _lock . release ( ) return lock
13671	def strip_codes ( s : Any ) -> str : return codepat . sub ( '' , str ( s ) if ( s or ( s == 0 ) ) else '' )
12815	def _finish ( self , forced = False ) : if hasattr ( self , "_current_file_handle" ) and self . _current_file_handle : self . _current_file_handle . close ( ) if self . _current_deferred : self . _current_deferred . callback ( self . _sent ) self . _current_deferred = None if not forced and self . _deferred : self . _deferred . callback ( self . _sent )
5658	def _validate_no_null_values ( self ) : for table in DB_TABLE_NAMES : null_not_ok_warning = "Null values in must-have columns in table {table}" . format ( table = table ) null_warn_warning = "Null values in good-to-have columns in table {table}" . format ( table = table ) null_not_ok_fields = DB_TABLE_NAME_TO_FIELDS_WHERE_NULL_NOT_OK [ table ] null_warn_fields = DB_TABLE_NAME_TO_FIELDS_WHERE_NULL_OK_BUT_WARN [ table ] df = self . gtfs . get_table ( table ) for warning , fields in zip ( [ null_not_ok_warning , null_warn_warning ] , [ null_not_ok_fields , null_warn_fields ] ) : null_unwanted_df = df [ fields ] rows_having_null = null_unwanted_df . isnull ( ) . any ( 1 ) if sum ( rows_having_null ) > 0 : rows_having_unwanted_null = df [ rows_having_null . values ] self . warnings_container . add_warning ( warning , rows_having_unwanted_null , len ( rows_having_unwanted_null ) )
11990	def const_equal ( str_a , str_b ) : if len ( str_a ) != len ( str_b ) : return False result = True for i in range ( len ( str_a ) ) : result &= ( str_a [ i ] == str_b [ i ] ) return result
3480	def _f_gene ( sid , prefix = "G_" ) : sid = sid . replace ( SBML_DOT , "." ) return _clip ( sid , prefix )
1530	def monitor ( self ) : def trigger_watches_based_on_files ( watchers , path , directory , ProtoClass ) : for topology , callbacks in watchers . items ( ) : file_path = os . path . join ( path , topology ) data = "" if os . path . exists ( file_path ) : with open ( os . path . join ( path , topology ) ) as f : data = f . read ( ) if topology not in directory or data != directory [ topology ] : proto_object = ProtoClass ( ) proto_object . ParseFromString ( data ) for callback in callbacks : callback ( proto_object ) directory [ topology ] = data while not self . monitoring_thread_stop_signal : topologies_path = self . get_topologies_path ( ) topologies = [ ] if os . path . isdir ( topologies_path ) : topologies = list ( filter ( lambda f : os . path . isfile ( os . path . join ( topologies_path , f ) ) , os . listdir ( topologies_path ) ) ) if set ( topologies ) != set ( self . topologies_directory ) : for callback in self . topologies_watchers : callback ( topologies ) self . topologies_directory = topologies trigger_watches_based_on_files ( self . topology_watchers , topologies_path , self . topologies_directory , Topology ) execution_state_path = os . path . dirname ( self . get_execution_state_path ( "" ) ) trigger_watches_based_on_files ( self . execution_state_watchers , execution_state_path , self . execution_state_directory , ExecutionState ) packing_plan_path = os . path . dirname ( self . get_packing_plan_path ( "" ) ) trigger_watches_based_on_files ( self . packing_plan_watchers , packing_plan_path , self . packing_plan_directory , PackingPlan ) pplan_path = os . path . dirname ( self . get_pplan_path ( "" ) ) trigger_watches_based_on_files ( self . pplan_watchers , pplan_path , self . pplan_directory , PhysicalPlan ) tmaster_path = os . path . dirname ( self . get_tmaster_path ( "" ) ) trigger_watches_based_on_files ( self . tmaster_watchers , tmaster_path , self . tmaster_directory , TMasterLocation ) scheduler_location_path = os . path . dirname ( self . get_scheduler_location_path ( "" ) ) trigger_watches_based_on_files ( self . scheduler_location_watchers , scheduler_location_path , self . scheduler_location_directory , SchedulerLocation ) self . event . wait ( timeout = 5 )
3089	def locked_get ( self ) : credentials = None if self . _cache : json = self . _cache . get ( self . _key_name ) if json : credentials = client . Credentials . new_from_json ( json ) if credentials is None : entity = self . _get_entity ( ) if entity is not None : credentials = getattr ( entity , self . _property_name ) if self . _cache : self . _cache . set ( self . _key_name , credentials . to_json ( ) ) if credentials and hasattr ( credentials , 'set_store' ) : credentials . set_store ( self ) return credentials
1007	def _learnPhase1 ( self , activeColumns , readOnly = False ) : self . lrnActiveState [ 't' ] . fill ( 0 ) numUnpredictedColumns = 0 for c in activeColumns : predictingCells = numpy . where ( self . lrnPredictedState [ 't-1' ] [ c ] == 1 ) [ 0 ] numPredictedCells = len ( predictingCells ) assert numPredictedCells <= 1 if numPredictedCells == 1 : i = predictingCells [ 0 ] self . lrnActiveState [ 't' ] [ c , i ] = 1 continue numUnpredictedColumns += 1 if readOnly : continue i , s , numActive = self . _getBestMatchingCell ( c , self . lrnActiveState [ 't-1' ] , self . minThreshold ) if s is not None and s . isSequenceSegment ( ) : if self . verbosity >= 4 : print "Learn branch 0, found segment match. Learning on col=" , c self . lrnActiveState [ 't' ] [ c , i ] = 1 segUpdate = self . _getSegmentActiveSynapses ( c , i , s , self . lrnActiveState [ 't-1' ] , newSynapses = True ) s . totalActivations += 1 trimSegment = self . _adaptSegment ( segUpdate ) if trimSegment : self . _trimSegmentsInCell ( c , i , [ s ] , minPermanence = 0.00001 , minNumSyns = 0 ) else : i = self . _getCellForNewSegment ( c ) if ( self . verbosity >= 4 ) : print "Learn branch 1, no match. Learning on col=" , c , print ", newCellIdxInCol=" , i self . lrnActiveState [ 't' ] [ c , i ] = 1 segUpdate = self . _getSegmentActiveSynapses ( c , i , None , self . lrnActiveState [ 't-1' ] , newSynapses = True ) segUpdate . sequenceSegment = True self . _adaptSegment ( segUpdate ) numBottomUpColumns = len ( activeColumns ) if numUnpredictedColumns < numBottomUpColumns / 2 : return True else : return False
5115	def copy ( self ) : net = QueueNetwork ( None ) net . g = self . g . copy ( ) net . max_agents = copy . deepcopy ( self . max_agents ) net . nV = copy . deepcopy ( self . nV ) net . nE = copy . deepcopy ( self . nE ) net . num_agents = copy . deepcopy ( self . num_agents ) net . num_events = copy . deepcopy ( self . num_events ) net . _t = copy . deepcopy ( self . _t ) net . _initialized = copy . deepcopy ( self . _initialized ) net . _prev_edge = copy . deepcopy ( self . _prev_edge ) net . _blocking = copy . deepcopy ( self . _blocking ) net . colors = copy . deepcopy ( self . colors ) net . out_edges = copy . deepcopy ( self . out_edges ) net . in_edges = copy . deepcopy ( self . in_edges ) net . edge2queue = copy . deepcopy ( self . edge2queue ) net . _route_probs = copy . deepcopy ( self . _route_probs ) if net . _initialized : keys = [ q . _key ( ) for q in net . edge2queue if q . _time < np . infty ] net . _fancy_heap = PriorityQueue ( keys , net . nE ) return net
10203	def register_queries ( ) : return [ dict ( query_name = 'bucket-file-download-histogram' , query_class = ESDateHistogramQuery , query_config = dict ( index = 'stats-file-download' , doc_type = 'file-download-day-aggregation' , copy_fields = dict ( bucket_id = 'bucket_id' , file_key = 'file_key' , ) , required_filters = dict ( bucket_id = 'bucket_id' , file_key = 'file_key' , ) ) ) , dict ( query_name = 'bucket-file-download-total' , query_class = ESTermsQuery , query_config = dict ( index = 'stats-file-download' , doc_type = 'file-download-day-aggregation' , copy_fields = dict ( ) , required_filters = dict ( bucket_id = 'bucket_id' , ) , aggregated_fields = [ 'file_key' ] ) ) , ]
12276	def barcode ( iban , reference , amount , due = None ) : iban = iban . replace ( ' ' , '' ) reference = reference . replace ( ' ' , '' ) if reference . startswith ( 'RF' ) : version = 5 else : version = 4 if version == 5 : reference = reference [ 2 : ] if len ( reference ) < 23 : reference = reference [ : 2 ] + ( "0" * ( 23 - len ( reference ) ) ) + reference [ 2 : ] elif version == 4 : reference = reference . zfill ( 20 ) if not iban . startswith ( 'FI' ) : raise BarcodeException ( 'Barcodes can be printed only for IBANs starting with FI' ) iban = iban [ 2 : ] amount = "%08d" % ( amount . quantize ( Decimal ( '.01' ) ) . shift ( 2 ) . to_integral_value ( ) ) if len ( amount ) != 8 : raise BarcodeException ( "Barcode payment amount must be less than 1000000.00" ) if due : due = due . strftime ( "%y%m%d" ) else : due = "000000" if version == 4 : barcode = "%s%s%s000%s%s" % ( version , iban , amount , reference , due ) elif version == 5 : barcode = "%s%s%s%s%s" % ( version , iban , amount , reference , due ) return barcode
110	def imshow ( image , backend = IMSHOW_BACKEND_DEFAULT ) : do_assert ( backend in [ "matplotlib" , "cv2" ] , "Expected backend 'matplotlib' or 'cv2', got %s." % ( backend , ) ) if backend == "cv2" : image_bgr = image if image . ndim == 3 and image . shape [ 2 ] in [ 3 , 4 ] : image_bgr = image [ ... , 0 : 3 ] [ ... , : : - 1 ] win_name = "imgaug-default-window" cv2 . namedWindow ( win_name , cv2 . WINDOW_NORMAL ) cv2 . imshow ( win_name , image_bgr ) cv2 . waitKey ( 0 ) cv2 . destroyWindow ( win_name ) else : import matplotlib . pyplot as plt dpi = 96 h , w = image . shape [ 0 ] / dpi , image . shape [ 1 ] / dpi w = max ( w , 6 ) fig , ax = plt . subplots ( figsize = ( w , h ) , dpi = dpi ) fig . canvas . set_window_title ( "imgaug.imshow(%s)" % ( image . shape , ) ) ax . imshow ( image , cmap = "gray" ) plt . show ( )
10985	def optimize_from_initial ( s , max_mem = 1e9 , invert = 'guess' , desc = '' , rz_order = 3 , min_rad = None , max_rad = None ) : RLOG . info ( 'Initial burn:' ) if desc is not None : desc_burn = desc + 'initial-burn' desc_polish = desc + 'addsub-polish' else : desc_burn , desc_polish = [ None ] * 2 opt . burn ( s , mode = 'burn' , n_loop = 3 , fractol = 0.1 , desc = desc_burn , max_mem = max_mem , include_rad = False , dowarn = False ) opt . burn ( s , mode = 'burn' , n_loop = 3 , fractol = 0.1 , desc = desc_burn , max_mem = max_mem , include_rad = True , dowarn = False ) RLOG . info ( 'Start add-subtract' ) rad = s . obj_get_radii ( ) if min_rad is None : min_rad = 0.5 * np . median ( rad ) if max_rad is None : max_rad = 1.5 * np . median ( rad ) addsub . add_subtract ( s , tries = 30 , min_rad = min_rad , max_rad = max_rad , invert = invert ) if desc is not None : states . save ( s , desc = desc + 'initial-addsub' ) RLOG . info ( 'Final polish:' ) d = opt . burn ( s , mode = 'polish' , n_loop = 8 , fractol = 3e-4 , desc = desc_polish , max_mem = max_mem , rz_order = rz_order , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' ) return s
6131	def convert_frames_to_video ( tar_file_path , output_path = "output.mp4" , framerate = 60 , overwrite = False ) : output_path = os . path . abspath ( output_path ) if os . path . isfile ( output_path ) and not overwrite : raise ValueError ( "The output path {:s} already exists. To overwrite that file, you can pass overwrite=True to this function." . format ( output_path ) ) with tempfile . TemporaryDirectory ( ) as tmp_dir : with tarfile . open ( tar_file_path ) as tar : tar . extractall ( tmp_dir ) args = [ "ffmpeg" , "-r" , str ( framerate ) , "-i" , r"%07d.png" , "-vcodec" , "libx264" , "-preset" , "slow" , "-crf" , "18" ] if overwrite : args . append ( "-y" ) args . append ( output_path ) try : subprocess . check_call ( args , cwd = tmp_dir ) except subprocess . CalledProcessError as e : print ( ) raise print ( "Saved output as {:s}" . format ( output_path ) ) return output_path
9757	def stop ( ctx , yes ) : user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if not yes and not click . confirm ( "Are sure you want to stop " "experiment `{}`" . format ( _experiment ) ) : click . echo ( 'Existing without stopping experiment.' ) sys . exit ( 0 ) try : PolyaxonClient ( ) . experiment . stop ( user , project_name , _experiment ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not stop experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiment is being stopped." )
1372	def get_heron_libs ( local_jars ) : heron_lib_dir = get_heron_lib_dir ( ) heron_libs = [ os . path . join ( heron_lib_dir , f ) for f in local_jars ] return heron_libs
3266	def prepare_upload_bundle ( name , data ) : fd , path = mkstemp ( ) zip_file = ZipFile ( path , 'w' ) for ext , stream in data . items ( ) : fname = "%s.%s" % ( name , ext ) if ( isinstance ( stream , basestring ) ) : zip_file . write ( stream , fname ) else : zip_file . writestr ( fname , stream . read ( ) ) zip_file . close ( ) os . close ( fd ) return path
7123	def write_config ( config , app_dir , filename = 'configuration.json' ) : path = os . path . join ( app_dir , filename ) with open ( path , 'w' ) as f : json . dump ( config , f , indent = 4 , cls = DetectMissingEncoder , separators = ( ',' , ': ' ) )
6606	def run_multiple ( self , workingArea , package_indices ) : if not package_indices : return [ ] job_desc = self . _compose_job_desc ( workingArea , package_indices ) clusterprocids = submit_jobs ( job_desc , cwd = workingArea . path ) clusterids = clusterprocids2clusterids ( clusterprocids ) for clusterid in clusterids : change_job_priority ( [ clusterid ] , 10 ) self . clusterprocids_outstanding . extend ( clusterprocids ) return clusterprocids
13267	def gml_to_geojson ( el ) : if el . get ( 'srsName' ) not in ( 'urn:ogc:def:crs:EPSG::4326' , None ) : if el . get ( 'srsName' ) == 'EPSG:4326' : return _gmlv2_to_geojson ( el ) else : raise NotImplementedError ( "Unrecognized srsName %s" % el . get ( 'srsName' ) ) tag = el . tag . replace ( '{%s}' % NS_GML , '' ) if tag == 'Point' : coordinates = _reverse_gml_coords ( el . findtext ( '{%s}pos' % NS_GML ) ) [ 0 ] elif tag == 'LineString' : coordinates = _reverse_gml_coords ( el . findtext ( '{%s}posList' % NS_GML ) ) elif tag == 'Polygon' : coordinates = [ ] for ring in el . xpath ( 'gml:exterior/gml:LinearRing/gml:posList' , namespaces = NSMAP ) + el . xpath ( 'gml:interior/gml:LinearRing/gml:posList' , namespaces = NSMAP ) : coordinates . append ( _reverse_gml_coords ( ring . text ) ) elif tag in ( 'MultiPoint' , 'MultiLineString' , 'MultiPolygon' ) : single_type = tag [ 5 : ] member_tag = single_type [ 0 ] . lower ( ) + single_type [ 1 : ] + 'Member' coordinates = [ gml_to_geojson ( member ) [ 'coordinates' ] for member in el . xpath ( 'gml:%s/gml:%s' % ( member_tag , single_type ) , namespaces = NSMAP ) ] else : raise NotImplementedError return { 'type' : tag , 'coordinates' : coordinates }
8500	def _map_arg ( arg ) : if isinstance ( arg , _ast . Str ) : return repr ( arg . s ) elif isinstance ( arg , _ast . Num ) : return arg . n elif isinstance ( arg , _ast . Name ) : name = arg . id if name == 'True' : return True elif name == 'False' : return False elif name == 'None' : return None return name else : return Unparseable ( )
8541	def _save_config ( self , filename = None ) : if filename is None : filename = self . _config_filename parent_path = os . path . dirname ( filename ) if not os . path . isdir ( parent_path ) : os . makedirs ( parent_path ) with open ( filename , "w" ) as configfile : self . _config . write ( configfile )
13814	def _MessageToJsonObject ( message , including_default_value_fields ) : message_descriptor = message . DESCRIPTOR full_name = message_descriptor . full_name if _IsWrapperMessage ( message_descriptor ) : return _WrapperMessageToJsonObject ( message ) if full_name in _WKTJSONMETHODS : return _WKTJSONMETHODS [ full_name ] [ 0 ] ( message , including_default_value_fields ) js = { } return _RegularMessageToJsonObject ( message , js , including_default_value_fields )
6709	def check ( self ) : self . _validate_settings ( ) r = self . local_renderer r . env . alias = r . env . aliases [ 0 ] r . sudo ( r . env . check_command_template )
12654	def convert_dcm2nii ( input_dir , output_dir , filename ) : if not op . exists ( input_dir ) : raise IOError ( 'Expected an existing folder in {}.' . format ( input_dir ) ) if not op . exists ( output_dir ) : raise IOError ( 'Expected an existing output folder in {}.' . format ( output_dir ) ) tmpdir = tempfile . TemporaryDirectory ( prefix = 'dcm2nii_' ) arguments = '-o "{}" -i y' . format ( tmpdir . name ) try : call_out = call_dcm2nii ( input_dir , arguments ) except : raise else : log . info ( 'Converted "{}" to nifti.' . format ( input_dir ) ) filenames = glob ( op . join ( tmpdir . name , '*.nii*' ) ) cleaned_filenames = remove_dcm2nii_underprocessed ( filenames ) filepaths = [ ] for srcpath in cleaned_filenames : dstpath = op . join ( output_dir , filename ) realpath = copy_w_plus ( srcpath , dstpath ) filepaths . append ( realpath ) basename = op . basename ( remove_ext ( srcpath ) ) aux_files = set ( glob ( op . join ( tmpdir . name , '{}.*' . format ( basename ) ) ) ) - set ( glob ( op . join ( tmpdir . name , '{}.nii*' . format ( basename ) ) ) ) for aux_file in aux_files : aux_dstpath = copy_w_ext ( aux_file , output_dir , remove_ext ( op . basename ( realpath ) ) ) filepaths . append ( aux_dstpath ) return filepaths
12840	async def async_connect ( self ) : if self . _waiters is None : raise Exception ( 'Error, database not properly initialized before async connection' ) if self . _waiters or self . max_connections and ( len ( self . _in_use ) >= self . max_connections ) : waiter = asyncio . Future ( loop = self . _loop ) self . _waiters . append ( waiter ) try : logger . debug ( 'Wait for connection.' ) await waiter finally : self . _waiters . remove ( waiter ) self . connect ( ) return self . _state . conn
7967	def feed ( self , data ) : with self . lock : if self . in_use : raise StreamParseError ( "StreamReader.feed() is not reentrant!" ) self . in_use = True try : if not self . _started : if len ( data ) > 1 : self . parser . feed ( data [ : 1 ] ) data = data [ 1 : ] self . _started = True if data : self . parser . feed ( data ) else : self . parser . close ( ) except ElementTree . ParseError , err : self . handler . stream_parse_error ( unicode ( err ) ) finally : self . in_use = False
5229	def _load_yaml_ ( file_name ) : if not os . path . exists ( file_name ) : return dict ( ) with open ( file_name , 'r' , encoding = 'utf-8' ) as fp : return YAML ( ) . load ( stream = fp )
12919	def save ( self ) : if len ( self ) == 0 : return [ ] mdl = self . getModel ( ) return mdl . saver . save ( self )
2516	def p_file_notice ( self , f_term , predicate ) : try : for _ , _ , notice in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_notice ( self . doc , six . text_type ( notice ) ) except CardinalityError : self . more_than_one_error ( 'file notice' )
5582	def create ( mapchete_file , process_file , out_format , out_path = None , pyramid_type = None , force = False ) : if os . path . isfile ( process_file ) or os . path . isfile ( mapchete_file ) : if not force : raise IOError ( "file(s) already exists" ) out_path = out_path if out_path else os . path . join ( os . getcwd ( ) , "output" ) process_template = pkg_resources . resource_filename ( "mapchete.static" , "process_template.py" ) process_file = os . path . join ( os . getcwd ( ) , process_file ) copyfile ( process_template , process_file ) mapchete_template = pkg_resources . resource_filename ( "mapchete.static" , "mapchete_template.mapchete" ) output_options = dict ( format = out_format , path = out_path , ** FORMAT_MANDATORY [ out_format ] ) pyramid_options = { 'grid' : pyramid_type } substitute_elements = { 'process_file' : process_file , 'output' : dump ( { 'output' : output_options } , default_flow_style = False ) , 'pyramid' : dump ( { 'pyramid' : pyramid_options } , default_flow_style = False ) } with open ( mapchete_template , 'r' ) as config_template : config = Template ( config_template . read ( ) ) customized_config = config . substitute ( substitute_elements ) with open ( mapchete_file , 'w' ) as target_config : target_config . write ( customized_config )
12934	def _parse_frequencies ( self ) : frequencies = OrderedDict ( [ ( 'EXAC' , 'Unknown' ) , ( 'ESP' , 'Unknown' ) , ( 'TGP' , 'Unknown' ) ] ) pref_freq = 'Unknown' for source in frequencies . keys ( ) : freq_key = 'AF_' + source if freq_key in self . info : frequencies [ source ] = self . info [ freq_key ] if pref_freq == 'Unknown' : pref_freq = frequencies [ source ] return pref_freq , frequencies
2118	def convert ( self , value , param , ctx ) : resource = tower_cli . get_resource ( self . resource_name ) if value is None : return None if isinstance ( value , int ) : return value if re . match ( r'^[\d]+$' , value ) : return int ( value ) if value == 'null' : return value try : debug . log ( 'The %s field is given as a name; ' 'looking it up.' % param . name , header = 'details' ) lookup_data = { resource . identity [ - 1 ] : value } rel = resource . get ( ** lookup_data ) except exc . MultipleResults : raise exc . MultipleRelatedError ( 'Cannot look up {0} exclusively by name, because multiple {0} ' 'objects exist with that name.\n' 'Please send an ID. You can get the ID for the {0} you want ' 'with:\n' ' tower-cli {0} list --name "{1}"' . format ( self . resource_name , value ) , ) except exc . TowerCLIError as ex : raise exc . RelatedError ( 'Could not get %s. %s' % ( self . resource_name , str ( ex ) ) ) return rel [ 'id' ]
13754	def read_from_file ( file_path , encoding = "utf-8" ) : with codecs . open ( file_path , "r" , encoding ) as f : return f . read ( )
10930	def find_LM_updates ( self , grad , do_correct_damping = True , subblock = None ) : if subblock is not None : if ( subblock . sum ( ) == 0 ) or ( subblock . size == 0 ) : CLOG . fatal ( 'Empty subblock in find_LM_updates' ) raise ValueError ( 'Empty sub-block' ) j = self . J [ subblock ] JTJ = np . dot ( j , j . T ) damped_JTJ = self . _calc_damped_jtj ( JTJ , subblock = subblock ) grad = grad [ subblock ] else : damped_JTJ = self . _calc_damped_jtj ( self . JTJ , subblock = subblock ) delta = self . _calc_lm_step ( damped_JTJ , grad , subblock = subblock ) if self . use_accel : accel_correction = self . calc_accel_correction ( damped_JTJ , delta ) nrm_d0 = np . sqrt ( np . sum ( delta ** 2 ) ) nrm_corr = np . sqrt ( np . sum ( accel_correction ** 2 ) ) CLOG . debug ( '|correction| / |LM step|\t%e' % ( nrm_corr / nrm_d0 ) ) if nrm_corr / nrm_d0 < self . max_accel_correction : delta += accel_correction elif do_correct_damping : CLOG . debug ( 'Untrustworthy step! Increasing damping...' ) self . increase_damping ( ) damped_JTJ = self . _calc_damped_jtj ( self . JTJ , subblock = subblock ) delta = self . _calc_lm_step ( damped_JTJ , grad , subblock = subblock ) if np . any ( np . isnan ( delta ) ) : CLOG . fatal ( 'Calculated steps have nans!?' ) raise FloatingPointError ( 'Calculated steps have nans!?' ) return delta
10462	def doesmenuitemexist ( self , window_name , object_name ) : try : menu_handle = self . _get_menu_handle ( window_name , object_name , False ) return 1 except LdtpServerException : return 0
8354	def start_meta ( self , attrs ) : httpEquiv = None contentType = None contentTypeIndex = None tagNeedsEncodingSubstitution = False for i in range ( 0 , len ( attrs ) ) : key , value = attrs [ i ] key = key . lower ( ) if key == 'http-equiv' : httpEquiv = value elif key == 'content' : contentType = value contentTypeIndex = i if httpEquiv and contentType : match = self . CHARSET_RE . search ( contentType ) if match : if ( self . declaredHTMLEncoding is not None or self . originalEncoding == self . fromEncoding ) : def rewrite ( match ) : return match . group ( 1 ) + "%SOUP-ENCODING%" newAttr = self . CHARSET_RE . sub ( rewrite , contentType ) attrs [ contentTypeIndex ] = ( attrs [ contentTypeIndex ] [ 0 ] , newAttr ) tagNeedsEncodingSubstitution = True else : newCharset = match . group ( 3 ) if newCharset and newCharset != self . originalEncoding : self . declaredHTMLEncoding = newCharset self . _feed ( self . declaredHTMLEncoding ) raise StopParsing pass tag = self . unknown_starttag ( "meta" , attrs ) if tag and tagNeedsEncodingSubstitution : tag . containsSubstitutions = True
5466	def get_action_environment ( op , name ) : action = _get_action_by_name ( op , name ) if action : return action . get ( 'environment' )
9135	def get_connection ( module_name : str , connection : Optional [ str ] = None ) -> str : if connection is not None : return connection module_name = module_name . lower ( ) module_config_cls = get_module_config_cls ( module_name ) module_config = module_config_cls . load ( ) return module_config . connection or config . connection
10062	def deposit_links_factory ( pid ) : links = default_links_factory ( pid ) def _url ( name , ** kwargs ) : endpoint = '.{0}_{1}' . format ( current_records_rest . default_endpoint_prefixes [ pid . pid_type ] , name , ) return url_for ( endpoint , pid_value = pid . pid_value , _external = True , ** kwargs ) links [ 'files' ] = _url ( 'files' ) ui_endpoint = current_app . config . get ( 'DEPOSIT_UI_ENDPOINT' ) if ui_endpoint is not None : links [ 'html' ] = ui_endpoint . format ( host = request . host , scheme = request . scheme , pid_value = pid . pid_value , ) deposit_cls = Deposit if 'pid_value' in request . view_args : deposit_cls = request . view_args [ 'pid_value' ] . data [ 1 ] . __class__ for action in extract_actions_from_class ( deposit_cls ) : links [ action ] = _url ( 'actions' , action = action ) return links
5035	def get_pending_users_queryset ( self , search_keyword , customer_uuid ) : queryset = PendingEnterpriseCustomerUser . objects . filter ( enterprise_customer__uuid = customer_uuid ) if search_keyword is not None : queryset = queryset . filter ( user_email__icontains = search_keyword ) return queryset
1182	def fast_search ( self , pattern_codes ) : flags = pattern_codes [ 2 ] prefix_len = pattern_codes [ 5 ] prefix_skip = pattern_codes [ 6 ] prefix = pattern_codes [ 7 : 7 + prefix_len ] overlap = pattern_codes [ 7 + prefix_len - 1 : pattern_codes [ 1 ] + 1 ] pattern_codes = pattern_codes [ pattern_codes [ 1 ] + 1 : ] i = 0 string_position = self . string_position while string_position < self . end : while True : if ord ( self . string [ string_position ] ) != prefix [ i ] : if i == 0 : break else : i = overlap [ i ] else : i += 1 if i == prefix_len : self . start = string_position + 1 - prefix_len self . string_position = string_position + 1 - prefix_len + prefix_skip if flags & SRE_INFO_LITERAL : return True if self . match ( pattern_codes [ 2 * prefix_skip : ] ) : return True i = overlap [ i ] break string_position += 1 return False
12036	def dictFlat ( l ) : if type ( l ) is dict : return [ l ] if "numpy" in str ( type ( l ) ) : return l dicts = [ ] for item in l : if type ( item ) == dict : dicts . append ( item ) elif type ( item ) == list : for item2 in item : dicts . append ( item2 ) return dicts
837	def getDistances ( self , inputPattern ) : dist = self . _getDistances ( inputPattern ) return ( dist , self . _categoryList )
2054	def ADR ( cpu , dest , src ) : aligned_pc = ( cpu . instruction . address + 4 ) & 0xfffffffc dest . write ( aligned_pc + src . read ( ) )
10600	def _url ( self , endpoint , url_data = None , parameters = None ) : try : url = '%s/%s' % ( self . base_url , self . endpoints [ endpoint ] ) except KeyError : raise EndPointDoesNotExist ( endpoint ) if url_data : url = url % url_data if parameters : url = '%s?%s' % ( url , urllib . urlencode ( parameters , True ) ) return url
9488	def generate_simple_call ( opcode : int , index : int ) : bs = b"" bs += opcode . to_bytes ( 1 , byteorder = "little" ) if isinstance ( index , int ) : if PY36 : bs += index . to_bytes ( 1 , byteorder = "little" ) else : bs += index . to_bytes ( 2 , byteorder = "little" ) else : bs += index return bs
402	def maxnorm_regularizer ( scale = 1.0 ) : if isinstance ( scale , numbers . Integral ) : raise ValueError ( 'scale cannot be an integer: %s' % scale ) if isinstance ( scale , numbers . Real ) : if scale < 0. : raise ValueError ( 'Setting a scale less than 0 on a regularizer: %g' % scale ) if scale == 0. : tl . logging . info ( 'Scale of 0 disables regularizer.' ) return lambda _ , name = None : None def mn ( weights , name = 'max_regularizer' ) : with tf . name_scope ( name ) as scope : my_scale = ops . convert_to_tensor ( scale , dtype = weights . dtype . base_dtype , name = 'scale' ) standard_ops_fn = standard_ops . multiply return standard_ops_fn ( my_scale , standard_ops . reduce_max ( standard_ops . abs ( weights ) ) , name = scope ) return mn
3549	def rssi ( self , timeout_sec = TIMEOUT_SEC ) : self . _rssi_read . clear ( ) self . _peripheral . readRSSI ( ) if not self . _rssi_read . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for RSSI value!' ) return self . _rssi
99	def angle_between_vectors ( v1 , v2 ) : l1 = np . linalg . norm ( v1 ) l2 = np . linalg . norm ( v2 ) v1_u = ( v1 / l1 ) if l1 > 0 else np . float32 ( v1 ) * 0 v2_u = ( v2 / l2 ) if l2 > 0 else np . float32 ( v2 ) * 0 return np . arccos ( np . clip ( np . dot ( v1_u , v2_u ) , - 1.0 , 1.0 ) )
6634	def displayOutdated ( modules , dependency_specs , use_colours ) : if use_colours : DIM = colorama . Style . DIM NORMAL = colorama . Style . NORMAL BRIGHT = colorama . Style . BRIGHT YELLOW = colorama . Fore . YELLOW RED = colorama . Fore . RED GREEN = colorama . Fore . GREEN RESET = colorama . Style . RESET_ALL else : DIM = BRIGHT = YELLOW = RED = GREEN = RESET = u'' status = 0 from yotta . lib import access from yotta . lib import access_common from yotta . lib import sourceparse for name , m in modules . items ( ) : if m . isTestDependency ( ) : continue try : latest_v = access . latestSuitableVersion ( name , '*' , registry = 'modules' , quiet = True ) except access_common . Unavailable as e : latest_v = None if not m : m_version = u' ' + RESET + BRIGHT + RED + u"missing" + RESET else : m_version = DIM + u'@%s' % ( m . version ) if not latest_v : print ( u'%s%s%s%s not available from the registry%s' % ( RED , name , m_version , NORMAL , RESET ) ) status = 2 continue elif not m or m . version < latest_v : update_prevented_by = '' if m : specs_preventing_update = [ x for x in dependency_specs if x . name == name and not sourceparse . parseSourceURL ( x . nonShrinkwrappedVersionReq ( ) ) . semanticSpecMatches ( latest_v ) ] shrinkwrap_prevents_update = [ x for x in dependency_specs if x . name == name and x . isShrinkwrapped ( ) and not sourceparse . parseSourceURL ( x . versionReq ( ) ) . semanticSpecMatches ( latest_v ) ] if len ( specs_preventing_update ) : update_prevented_by = ' (update prevented by specifications: %s)' % ( ', ' . join ( [ '%s from %s' % ( x . version_req , x . specifying_module ) for x in specs_preventing_update ] ) ) if len ( shrinkwrap_prevents_update ) : update_prevented_by += ' yotta-shrinkwrap.json prevents update' if m . version . major ( ) < latest_v . major ( ) : colour = GREEN elif m . version . minor ( ) < latest_v . minor ( ) : colour = YELLOW else : colour = RED else : colour = RED print ( u'%s%s%s latest: %s%s%s%s' % ( name , m_version , RESET , colour , latest_v . version , update_prevented_by , RESET ) ) if not status : status = 1 return status
9618	def UnPlug ( self , force = False ) : if force : _xinput . UnPlugForce ( c_uint ( self . id ) ) else : _xinput . UnPlug ( c_uint ( self . id ) ) while self . id not in self . available_ids ( ) : if self . id == 0 : break
384	def parse_darknet_ann_str_to_list ( annotations ) : r annotations = annotations . split ( "\n" ) ann = [ ] for a in annotations : a = a . split ( ) if len ( a ) == 5 : for i , _v in enumerate ( a ) : if i == 0 : a [ i ] = int ( a [ i ] ) else : a [ i ] = float ( a [ i ] ) ann . append ( a ) return ann
7689	def piano_roll ( annotation , sr = 22050 , length = None , ** kwargs ) : intervals , pitches = annotation . to_interval_values ( ) pitch_map = { f : idx for idx , f in enumerate ( np . unique ( pitches ) ) } gram = np . zeros ( ( len ( pitch_map ) , len ( intervals ) ) ) for col , f in enumerate ( pitches ) : gram [ pitch_map [ f ] , col ] = 1 return filter_kwargs ( mir_eval . sonify . time_frequency , gram , pitches , intervals , sr , length = length , ** kwargs )
6861	def prep_root_password ( self , password = None , ** kwargs ) : r = self . database_renderer ( ** kwargs ) r . env . root_password = password or r . genv . get ( 'db_root_password' ) r . sudo ( "DEBIAN_FRONTEND=noninteractive dpkg --configure -a" ) r . sudo ( "debconf-set-selections <<< 'mysql-server mysql-server/root_password password {root_password}'" ) r . sudo ( "debconf-set-selections <<< 'mysql-server mysql-server/root_password_again password {root_password}'" )
5708	def process_request ( self , request ) : try : session = request . session except AttributeError : raise ImproperlyConfigured ( 'django-lockdown requires the Django ' 'sessions framework' ) if settings . ENABLED is False : return None if self . remote_addr_exceptions : remote_addr_exceptions = self . remote_addr_exceptions else : remote_addr_exceptions = settings . REMOTE_ADDR_EXCEPTIONS if remote_addr_exceptions : trusted_proxies = self . trusted_proxies or settings . TRUSTED_PROXIES remote_addr = request . META . get ( 'REMOTE_ADDR' ) if remote_addr in remote_addr_exceptions : return None if remote_addr in trusted_proxies : x_forwarded_for = request . META . get ( 'HTTP_X_FORWARDED_FOR' ) if x_forwarded_for : remote_addr = x_forwarded_for . split ( ',' ) [ - 1 ] . strip ( ) if remote_addr in remote_addr_exceptions : return None if self . url_exceptions : url_exceptions = compile_url_exceptions ( self . url_exceptions ) else : url_exceptions = compile_url_exceptions ( settings . URL_EXCEPTIONS ) for pattern in url_exceptions : if pattern . search ( request . path ) : return None try : resolved_path = resolve ( request . path ) except Resolver404 : pass else : if resolved_path . func in settings . VIEW_EXCEPTIONS : return None if self . until_date : until_date = self . until_date else : until_date = settings . UNTIL_DATE if self . after_date : after_date = self . after_date else : after_date = settings . AFTER_DATE if until_date or after_date : locked_date = False if until_date and datetime . datetime . now ( ) < until_date : locked_date = True if after_date and datetime . datetime . now ( ) > after_date : locked_date = True if not locked_date : return None form_data = request . POST if request . method == 'POST' else None if self . form : form_class = self . form else : form_class = get_lockdown_form ( settings . FORM ) form = form_class ( data = form_data , ** self . form_kwargs ) authorized = False token = session . get ( self . session_key ) if hasattr ( form , 'authenticate' ) : if form . authenticate ( token ) : authorized = True elif token is True : authorized = True if authorized and self . logout_key and self . logout_key in request . GET : if self . session_key in session : del session [ self . session_key ] querystring = request . GET . copy ( ) del querystring [ self . logout_key ] return self . redirect ( request ) if authorized : return None if form . is_valid ( ) : if hasattr ( form , 'generate_token' ) : token = form . generate_token ( ) else : token = True session [ self . session_key ] = token return self . redirect ( request ) page_data = { 'until_date' : until_date , 'after_date' : after_date } if not hasattr ( form , 'show_form' ) or form . show_form ( ) : page_data [ 'form' ] = form if self . extra_context : page_data . update ( self . extra_context ) return render ( request , 'lockdown/form.html' , page_data )
1512	def distribute_package ( roles , cl_args ) : Log . info ( "Distributing heron package to nodes (this might take a while)..." ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] tar_file = tempfile . NamedTemporaryFile ( suffix = ".tmp" ) . name Log . debug ( "TAR file %s to %s" % ( cl_args [ "heron_dir" ] , tar_file ) ) make_tarfile ( tar_file , cl_args [ "heron_dir" ] ) dist_nodes = masters . union ( slaves ) scp_package ( tar_file , dist_nodes , cl_args )
6816	def optimize_wsgi_processes ( self ) : r = self . local_renderer r . env . wsgi_server_memory_gb = 8 verbose = self . verbose all_sites = list ( self . iter_sites ( site = ALL , setter = self . set_site_specifics ) )
6395	def fingerprint ( self , word ) : word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = '' . join ( c for c in word if c in self . _letters ) key = '' for char in self . _consonants : if char in word : key += char for char in word : if char not in self . _consonants and char not in key : key += char return key
2333	def predict_dataset ( self , df ) : if len ( list ( df . columns ) ) == 2 : df . columns = [ "A" , "B" ] if self . model is None : raise AssertionError ( "Model has not been trained before predictions" ) df2 = DataFrame ( ) for idx , row in df . iterrows ( ) : df2 = df2 . append ( row , ignore_index = True ) df2 = df2 . append ( { 'A' : row [ "B" ] , 'B' : row [ "A" ] } , ignore_index = True ) return predict . predict ( deepcopy ( df2 ) , deepcopy ( self . model ) ) [ : : 2 ]
3413	def _update_optional ( cobra_object , new_dict , optional_attribute_dict , ordered_keys ) : for key in ordered_keys : default = optional_attribute_dict [ key ] value = getattr ( cobra_object , key ) if value is None or value == default : continue new_dict [ key ] = _fix_type ( value )
11573	def clear_display_buffer ( self ) : for row in range ( 0 , 8 ) : self . firmata . i2c_write ( 0x70 , row * 2 , 0 , 0 ) self . firmata . i2c_write ( 0x70 , ( row * 2 ) + 1 , 0 , 0 ) for column in range ( 0 , 8 ) : self . display_buffer [ row ] [ column ] = 0
6065	def density_between_circular_annuli_in_angular_units ( self , inner_annuli_radius , outer_annuli_radius ) : annuli_area = ( np . pi * outer_annuli_radius ** 2.0 ) - ( np . pi * inner_annuli_radius ** 2.0 ) return ( self . mass_within_circle_in_units ( radius = outer_annuli_radius ) - self . mass_within_circle_in_units ( radius = inner_annuli_radius ) ) / annuli_area
10724	def xformer ( signature ) : funcs = [ f for ( f , _ ) in xformers ( signature ) ] def the_func ( objects ) : if len ( objects ) != len ( funcs ) : raise IntoDPValueError ( objects , "objects" , "must have exactly %u items, has %u" % ( len ( funcs ) , len ( objects ) ) ) return [ x for ( x , _ ) in ( f ( a ) for ( f , a ) in zip ( funcs , objects ) ) ] return the_func
6508	def generate_field_filters ( cls , ** kwargs ) : generator = _load_class ( getattr ( settings , "SEARCH_FILTER_GENERATOR" , None ) , cls ) ( ) return ( generator . field_dictionary ( ** kwargs ) , generator . filter_dictionary ( ** kwargs ) , generator . exclude_dictionary ( ** kwargs ) , )
7612	def get_clan_image ( self , obj : BaseAttrDict ) : try : badge_id = obj . clan . badge_id except AttributeError : try : badge_id = obj . badge_id except AttributeError : return 'https://i.imgur.com/Y3uXsgj.png' if badge_id is None : return 'https://i.imgur.com/Y3uXsgj.png' for i in self . constants . alliance_badges : if i . id == badge_id : return 'https://royaleapi.github.io/cr-api-assets/badges/' + i . name + '.png'
109	def show_grid ( images , rows = None , cols = None ) : grid = draw_grid ( images , rows = rows , cols = cols ) imshow ( grid )
11876	def getch ( ) : try : termios . tcsetattr ( _fd , termios . TCSANOW , _new_settings ) ch = sys . stdin . read ( 1 ) finally : termios . tcsetattr ( _fd , termios . TCSADRAIN , _old_settings ) return ch
3415	def model_from_dict ( obj ) : if 'reactions' not in obj : raise ValueError ( 'Object has no reactions attribute. Cannot load.' ) model = Model ( ) model . add_metabolites ( [ metabolite_from_dict ( metabolite ) for metabolite in obj [ 'metabolites' ] ] ) model . genes . extend ( [ gene_from_dict ( gene ) for gene in obj [ 'genes' ] ] ) model . add_reactions ( [ reaction_from_dict ( reaction , model ) for reaction in obj [ 'reactions' ] ] ) objective_reactions = [ rxn for rxn in obj [ 'reactions' ] if rxn . get ( 'objective_coefficient' , 0 ) != 0 ] coefficients = { model . reactions . get_by_id ( rxn [ 'id' ] ) : rxn [ 'objective_coefficient' ] for rxn in objective_reactions } set_objective ( model , coefficients ) for k , v in iteritems ( obj ) : if k in { 'id' , 'name' , 'notes' , 'compartments' , 'annotation' } : setattr ( model , k , v ) return model
7855	def fetch ( self ) : from . . iq import Iq jid , node = self . address iq = Iq ( to_jid = jid , stanza_type = "get" ) disco = self . disco_class ( node ) iq . add_content ( disco . xmlnode ) self . stream . set_response_handlers ( iq , self . __response , self . __error , self . __timeout ) self . stream . send ( iq )
5278	def preprocess_constraints ( ml , cl , n ) : "Create a graph of constraints for both must- and cannot-links" ml_graph , cl_graph = { } , { } for i in range ( n ) : ml_graph [ i ] = set ( ) cl_graph [ i ] = set ( ) def add_both ( d , i , j ) : d [ i ] . add ( j ) d [ j ] . add ( i ) for ( i , j ) in ml : ml_graph [ i ] . add ( j ) ml_graph [ j ] . add ( i ) for ( i , j ) in cl : cl_graph [ i ] . add ( j ) cl_graph [ j ] . add ( i ) def dfs ( i , graph , visited , component ) : visited [ i ] = True for j in graph [ i ] : if not visited [ j ] : dfs ( j , graph , visited , component ) component . append ( i ) visited = [ False ] * n neighborhoods = [ ] for i in range ( n ) : if not visited [ i ] and ml_graph [ i ] : component = [ ] dfs ( i , ml_graph , visited , component ) for x1 in component : for x2 in component : if x1 != x2 : ml_graph [ x1 ] . add ( x2 ) neighborhoods . append ( component ) for ( i , j ) in cl : for x in ml_graph [ i ] : add_both ( cl_graph , x , j ) for y in ml_graph [ j ] : add_both ( cl_graph , i , y ) for x in ml_graph [ i ] : for y in ml_graph [ j ] : add_both ( cl_graph , x , y ) for i in ml_graph : for j in ml_graph [ i ] : if j != i and j in cl_graph [ i ] : raise InconsistentConstraintsException ( 'Inconsistent constraints between {} and {}' . format ( i , j ) ) return ml_graph , cl_graph , neighborhoods
3554	def stop_scan ( self , timeout_sec = TIMEOUT_SEC ) : get_provider ( ) . _central_manager . stopScan ( ) self . _is_scanning = False
11112	def load_repository ( self , path ) : if path . strip ( ) in ( '' , '.' ) : path = os . getcwd ( ) repoPath = os . path . realpath ( os . path . expanduser ( path ) ) if not self . is_repository ( repoPath ) : raise Exception ( "no repository found in '%s'" % str ( repoPath ) ) repoInfoPath = os . path . join ( repoPath , ".pyrepinfo" ) try : fd = open ( repoInfoPath , 'rb' ) except Exception as e : raise Exception ( "unable to open repository file(%s)" % e ) L = Locker ( filePath = None , lockPass = str ( uuid . uuid1 ( ) ) , lockPath = os . path . join ( repoPath , ".pyreplock" ) ) acquired , code = L . acquire_lock ( ) if not acquired : warnings . warn ( "code %s. Unable to aquire the lock when calling 'load_repository'. You may try again!" % ( code , ) ) return try : try : repo = pickle . load ( fd ) except Exception as e : fd . close ( ) raise Exception ( "unable to pickle load repository (%s)" % e ) finally : fd . close ( ) if not isinstance ( repo , Repository ) : raise Exception ( ".pyrepinfo in '%s' is not a repository instance." % s ) else : self . __reset_repository ( ) self . __update_repository ( repo ) self . __path = repoPath self . __state = self . _get_or_create_state ( ) except Exception as e : L . release_lock ( ) raise Exception ( e ) finally : L . release_lock ( ) self . __locker = L return self
194	def AssertLambda ( func_images = None , func_heatmaps = None , func_keypoints = None , func_polygons = None , name = None , deterministic = False , random_state = None ) : def func_images_assert ( images , random_state , parents , hooks ) : ia . do_assert ( func_images ( images , random_state , parents , hooks ) , "Input images did not fulfill user-defined assertion in AssertLambda." ) return images def func_heatmaps_assert ( heatmaps , random_state , parents , hooks ) : ia . do_assert ( func_heatmaps ( heatmaps , random_state , parents , hooks ) , "Input heatmaps did not fulfill user-defined assertion in AssertLambda." ) return heatmaps def func_keypoints_assert ( keypoints_on_images , random_state , parents , hooks ) : ia . do_assert ( func_keypoints ( keypoints_on_images , random_state , parents , hooks ) , "Input keypoints did not fulfill user-defined assertion in AssertLambda." ) return keypoints_on_images def func_polygons_assert ( polygons_on_images , random_state , parents , hooks ) : ia . do_assert ( func_polygons ( polygons_on_images , random_state , parents , hooks ) , "Input polygons did not fulfill user-defined assertion in AssertLambda." ) return polygons_on_images if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Lambda ( func_images_assert if func_images is not None else None , func_heatmaps_assert if func_heatmaps is not None else None , func_keypoints_assert if func_keypoints is not None else None , func_polygons_assert if func_polygons is not None else None , name = name , deterministic = deterministic , random_state = random_state )
8185	def offset ( self , node ) : x = self . x + node . x - _ctx . WIDTH / 2 y = self . y + node . y - _ctx . HEIGHT / 2 return x , y
5284	def formset_valid ( self , formset ) : self . object_list = formset . save ( ) return super ( ModelFormSetMixin , self ) . formset_valid ( formset )
2199	def platform_config_dir ( ) : if LINUX : dpath_ = os . environ . get ( 'XDG_CONFIG_HOME' , '~/.config' ) elif DARWIN : dpath_ = '~/Library/Application Support' elif WIN32 : dpath_ = os . environ . get ( 'APPDATA' , '~/AppData/Roaming' ) else : raise NotImplementedError ( 'Unknown Platform %r' % ( sys . platform , ) ) dpath = normpath ( expanduser ( dpath_ ) ) return dpath
7410	def count_var ( nex ) : arr = np . array ( [ list ( i . split ( ) [ - 1 ] ) for i in nex ] ) miss = np . any ( arr == "N" , axis = 0 ) nomiss = arr [ : , ~ miss ] nsnps = np . invert ( np . all ( nomiss == nomiss [ 0 , : ] , axis = 0 ) ) . sum ( ) return nomiss . shape [ 1 ] , nsnps
1234	def atomic_observe ( self , states , actions , internals , reward , terminal ) : self . current_terminal = terminal self . current_reward = reward if self . unique_state : states = dict ( state = states ) if self . unique_action : actions = dict ( action = actions ) self . episode = self . model . atomic_observe ( states = states , actions = actions , internals = internals , terminal = self . current_terminal , reward = self . current_reward )
8237	def right_complement ( clr ) : right = split_complementary ( clr ) [ 2 ] colors = complementary ( clr ) colors [ 3 ] . h = right . h colors [ 4 ] . h = right . h colors [ 5 ] . h = right . h colors = colorlist ( colors [ 0 ] , colors [ 2 ] , colors [ 1 ] , colors [ 5 ] , colors [ 4 ] , colors [ 3 ] ) return colors
6284	def start ( self ) : self . music . start ( ) if not self . start_paused : self . rocket . start ( )
9390	def get_aggregation_timestamp ( self , timestamp , granularity = 'second' ) : if granularity is None or granularity . lower ( ) == 'none' : return int ( timestamp ) , 1 elif granularity == 'hour' : return ( int ( timestamp ) / ( 3600 * 1000 ) ) * 3600 * 1000 , 3600 elif granularity == 'minute' : return ( int ( timestamp ) / ( 60 * 1000 ) ) * 60 * 1000 , 60 else : return ( int ( timestamp ) / 1000 ) * 1000 , 1
1062	def formatdate ( timeval = None ) : if timeval is None : timeval = time . time ( ) timeval = time . gmtime ( timeval ) return "%s, %02d %s %04d %02d:%02d:%02d GMT" % ( ( "Mon" , "Tue" , "Wed" , "Thu" , "Fri" , "Sat" , "Sun" ) [ timeval [ 6 ] ] , timeval [ 2 ] , ( "Jan" , "Feb" , "Mar" , "Apr" , "May" , "Jun" , "Jul" , "Aug" , "Sep" , "Oct" , "Nov" , "Dec" ) [ timeval [ 1 ] - 1 ] , timeval [ 0 ] , timeval [ 3 ] , timeval [ 4 ] , timeval [ 5 ] )
7441	def set_params ( self , param , newvalue ) : legacy_params = [ "edit_cutsites" , "trim_overhang" ] current_params = self . paramsdict . keys ( ) allowed_params = current_params + legacy_params if not param in allowed_params : raise IPyradParamsError ( "Parameter key not recognized: {}" . format ( param ) ) param = str ( param ) if len ( param ) < 3 : param = self . paramsdict . keys ( ) [ int ( param ) ] try : self = _paramschecker ( self , param , newvalue ) except Exception as inst : raise IPyradWarningExit ( BAD_PARAMETER . format ( param , inst , newvalue ) )
7866	def set_item ( self , key , value , timeout = None , timeout_callback = None ) : with self . _lock : logger . debug ( "expdict.__setitem__({0!r}, {1!r}, {2!r}, {3!r})" . format ( key , value , timeout , timeout_callback ) ) if not timeout : timeout = self . _default_timeout self . _timeouts [ key ] = ( time . time ( ) + timeout , timeout_callback ) return dict . __setitem__ ( self , key , value )
7805	def verify_jid_against_common_name ( self , jid ) : if not self . common_names : return False for name in self . common_names : try : cn_jid = JID ( name ) except ValueError : continue if jid == cn_jid : return True return False
2749	def get_droplet ( self , droplet_id ) : return Droplet . get_object ( api_token = self . token , droplet_id = droplet_id )
894	def mapCellsToColumns ( self , cells ) : cellsForColumns = defaultdict ( set ) for cell in cells : column = self . columnForCell ( cell ) cellsForColumns [ column ] . add ( cell ) return cellsForColumns
6287	def get ( self , name ) -> Track : name = name . lower ( ) track = self . track_map . get ( name ) if not track : track = Track ( name ) self . tacks . append ( track ) self . track_map [ name ] = track return track
10283	def count_sources ( edge_iter : EdgeIterator ) -> Counter : return Counter ( u for u , _ , _ in edge_iter )
13848	def set_time ( filename , mod_time ) : log . debug ( 'Setting modified time to %s' , mod_time ) mtime = calendar . timegm ( mod_time . utctimetuple ( ) ) mtime += mod_time . microsecond / 1000000 atime = os . stat ( filename ) . st_atime os . utime ( filename , ( atime , mtime ) )
13753	def prepare_path ( path ) : if type ( path ) == list : return os . path . join ( * path ) return path
10721	def get_command ( namespace ) : cmd = [ "pylint" , namespace . package ] + arg_map [ namespace . package ] if namespace . ignore : cmd . append ( "--ignore=%s" % namespace . ignore ) return cmd
6988	def serial_varfeatures ( lclist , outdir , maxobjects = None , timecols = None , magcols = None , errcols = None , mindet = 1000 , lcformat = 'hat-sql' , lcformatdir = None ) : if maxobjects : lclist = lclist [ : maxobjects ] tasks = [ ( x , outdir , timecols , magcols , errcols , mindet , lcformat , lcformatdir ) for x in lclist ] for task in tqdm ( tasks ) : result = _varfeatures_worker ( task ) return result
13178	def cache_func ( prefix , method = False ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : cache_args = args if method : cache_args = args [ 1 : ] cache_key = get_cache_key ( prefix , * cache_args , ** kwargs ) cached_value = cache . get ( cache_key ) if cached_value is None : cached_value = func ( * args , ** kwargs ) cache . set ( cache_key , cached_value ) return cached_value return wrapper return decorator
10406	def canonical_averages_dtype ( spanning_cluster = True ) : fields = list ( ) fields . extend ( [ ( 'number_of_runs' , 'uint32' ) , ] ) if spanning_cluster : fields . extend ( [ ( 'percolation_probability_mean' , 'float64' ) , ( 'percolation_probability_m2' , 'float64' ) , ] ) fields . extend ( [ ( 'max_cluster_size_mean' , 'float64' ) , ( 'max_cluster_size_m2' , 'float64' ) , ( 'moments_mean' , '(5,)float64' ) , ( 'moments_m2' , '(5,)float64' ) , ] ) return _ndarray_dtype ( fields )
855	def getBookmark ( self ) : if self . _write and self . _recordCount == 0 : return None rowDict = dict ( filepath = os . path . realpath ( self . _filename ) , currentRow = self . _recordCount ) return json . dumps ( rowDict )
5110	def simulate ( self , n = 1 , t = None , nA = None , nD = None ) : if t is None and nD is None and nA is None : for dummy in range ( n ) : self . next_event ( ) elif t is not None : then = self . _current_t + t while self . _current_t < then and self . _time < infty : self . next_event ( ) elif nD is not None : num_departures = self . num_departures + nD while self . num_departures < num_departures and self . _time < infty : self . next_event ( ) elif nA is not None : num_arrivals = self . _oArrivals + nA while self . _oArrivals < num_arrivals and self . _time < infty : self . next_event ( )
11175	def parse ( self , file ) : if isinstance ( file , basestring ) : file = open ( file ) line_number = 0 label = None block = self . untagged for line in file : line_number += 1 line = line . rstrip ( '\n' ) if self . tabsize > 0 : line = line . replace ( '\t' , ' ' * self . tabsize ) if self . decommenter : line = self . decommenter . decomment ( line ) if line is None : continue tag = line . split ( ':' , 1 ) [ 0 ] . strip ( ) if tag not in self . names : if block is None : if line and not line . isspace ( ) : raise ParseError ( file . name , line , "garbage before first block: %r" % line ) continue block . addline ( line ) continue name = self . names [ tag ] label = line . split ( ':' , 1 ) [ 1 ] . strip ( ) if name in self . labelled_classes : if not label : raise ParseError ( file . name , line , "missing label for %r block" % name ) block = self . blocks [ name ] . setdefault ( label , self . labelled_classes [ name ] ( ) ) else : if label : msg = "label %r present for unlabelled block %r" % ( label , name ) raise ParseError ( file . name , line_number , msg ) block = self . blocks [ name ] block . startblock ( )
3541	def do_apply ( mutation_pk , dict_synonyms , backup ) : filename , mutation_id = filename_and_mutation_id_from_pk ( int ( mutation_pk ) ) update_line_numbers ( filename ) context = Context ( mutation_id = mutation_id , filename = filename , dict_synonyms = dict_synonyms , ) mutate_file ( backup = backup , context = context , ) if context . number_of_performed_mutations == 0 : raise RuntimeError ( 'No mutations performed.' )
8745	def get_floatingips ( context , filters = None , fields = None , sorts = [ 'id' ] , limit = None , marker = None , page_reverse = False ) : LOG . info ( 'get_floatingips for tenant %s filters %s fields %s' % ( context . tenant_id , filters , fields ) ) floating_ips = _get_ips_by_type ( context , ip_types . FLOATING , filters = filters , fields = fields ) return [ v . _make_floating_ip_dict ( flip ) for flip in floating_ips ]
5154	def get_copy ( dict_ , key , default = None ) : value = dict_ . get ( key , default ) if value : return deepcopy ( value ) return value
335	def compute_consistency_score ( returns_test , preds ) : returns_test_cum = cum_returns ( returns_test , starting_value = 1. ) cum_preds = np . cumprod ( preds + 1 , 1 ) q = [ sp . stats . percentileofscore ( cum_preds [ : , i ] , returns_test_cum . iloc [ i ] , kind = 'weak' ) for i in range ( len ( returns_test_cum ) ) ] return 100 - np . abs ( 50 - np . mean ( q ) ) / .5
4013	def _ensure_managed_repos_dir_exists ( ) : if not os . path . exists ( constants . REPOS_DIR ) : os . makedirs ( constants . REPOS_DIR )
8581	def update_server ( self , datacenter_id , server_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : if attr == 'boot_volume' : boot_volume_properties = { "id" : value } boot_volume_entities = { "bootVolume" : boot_volume_properties } data . update ( boot_volume_entities ) else : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/servers/%s' % ( datacenter_id , server_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
8203	def set_size ( self , size ) : if self . size is None : self . size = size return size else : return self . size
582	def removeLabels ( self , start = None , end = None , labelFilter = None ) : if len ( self . saved_states ) == 0 : raise HTMPredictionModelInvalidRangeError ( "Invalid supplied range for " "'removeLabels'. Model has no saved records." ) startID = self . saved_states [ 0 ] . ROWID clippedStart = 0 if start is None else max ( 0 , start - startID ) clippedEnd = len ( self . saved_states ) if end is None else max ( 0 , min ( len ( self . saved_states ) , end - startID ) ) if clippedEnd <= clippedStart : raise HTMPredictionModelInvalidRangeError ( "Invalid supplied range for " "'removeLabels'." , debugInfo = { 'requestRange' : { 'startRecordID' : start , 'endRecordID' : end } , 'clippedRequestRange' : { 'startRecordID' : clippedStart , 'endRecordID' : clippedEnd } , 'validRange' : { 'startRecordID' : startID , 'endRecordID' : self . saved_states [ len ( self . saved_states ) - 1 ] . ROWID } , 'numRecordsStored' : len ( self . saved_states ) } ) recordsToDelete = [ ] for state in self . saved_states [ clippedStart : clippedEnd ] : if labelFilter is not None : if labelFilter in state . anomalyLabel : state . anomalyLabel . remove ( labelFilter ) else : state . anomalyLabel = [ ] state . setByUser = False recordsToDelete . append ( state ) self . _deleteRecordsFromKNN ( recordsToDelete ) self . _deleteRangeFromKNN ( start , end ) for state in self . saved_states [ clippedEnd : ] : self . _updateState ( state ) return { 'status' : 'success' }
164	def compute_pointwise_distances ( self , other , default = None ) : import shapely . geometry from . kps import Keypoint if isinstance ( other , Keypoint ) : other = shapely . geometry . Point ( ( other . x , other . y ) ) elif isinstance ( other , LineString ) : if len ( other . coords ) == 0 : return default elif len ( other . coords ) == 1 : other = shapely . geometry . Point ( other . coords [ 0 , : ] ) else : other = shapely . geometry . LineString ( other . coords ) elif isinstance ( other , tuple ) : assert len ( other ) == 2 other = shapely . geometry . Point ( other ) else : raise ValueError ( ( "Expected Keypoint or LineString or tuple (x,y), " + "got type %s." ) % ( type ( other ) , ) ) return [ shapely . geometry . Point ( point ) . distance ( other ) for point in self . coords ]
2917	def _eval_args ( args , my_task ) : results = [ ] for arg in args : if isinstance ( arg , Attrib ) or isinstance ( arg , PathAttrib ) : results . append ( valueof ( my_task , arg ) ) else : results . append ( arg ) return results
5700	def _distribution ( gtfs , table , column ) : cur = gtfs . conn . cursor ( ) cur . execute ( 'SELECT {column}, count(*) ' 'FROM {table} GROUP BY {column} ' 'ORDER BY {column}' . format ( column = column , table = table ) ) return ' ' . join ( '%s:%s' % ( t , c ) for t , c in cur )
9332	def empty_like ( array , dtype = None ) : array = numpy . asarray ( array ) if dtype is None : dtype = array . dtype return anonymousmemmap ( array . shape , dtype )
6513	def _most_popular_gender ( self , name , counter ) : if name not in self . names : return self . unknown_value max_count , max_tie = ( 0 , 0 ) best = self . names [ name ] . keys ( ) [ 0 ] for gender , country_values in self . names [ name ] . items ( ) : count , tie = counter ( country_values ) if count > max_count or ( count == max_count and tie > max_tie ) : max_count , max_tie , best = count , tie , gender return best if max_count > 0 else self . unknown_value
9873	def CherryPyWSGIServer ( bind_addr , wsgi_app , numthreads = 10 , server_name = None , max = - 1 , request_queue_size = 5 , timeout = 10 , shutdown_timeout = 5 ) : max_threads = max if max_threads < 0 : max_threads = 0 return Rocket ( bind_addr , 'wsgi' , { 'wsgi_app' : wsgi_app } , min_threads = numthreads , max_threads = max_threads , queue_size = request_queue_size , timeout = timeout )
13766	def parse ( self , string ) : var , eq , values = string . strip ( ) . partition ( '=' ) assert var == 'runtimepath' assert eq == '=' return values . split ( ',' )
10214	def summarize_subgraph_node_overlap ( graph : BELGraph , node_predicates = None , annotation : str = 'Subgraph' ) : r1 = group_nodes_by_annotation_filtered ( graph , node_predicates = node_predicates , annotation = annotation ) return calculate_tanimoto_set_distances ( r1 )
11613	def report_depths ( self , filename , tpm = True , grp_wise = False , reorder = 'as-is' , notes = None ) : if grp_wise : lname = self . probability . gname depths = self . allelic_expression * self . grp_conv_mat else : lname = self . probability . lname depths = self . allelic_expression if tpm : depths *= ( 1000000.0 / depths . sum ( ) ) total_depths = depths . sum ( axis = 0 ) if reorder == 'decreasing' : report_order = np . argsort ( total_depths . flatten ( ) ) report_order = report_order [ : : - 1 ] elif reorder == 'increasing' : report_order = np . argsort ( total_depths . flatten ( ) ) elif reorder == 'as-is' : report_order = np . arange ( len ( lname ) ) cntdata = np . vstack ( ( depths , total_depths ) ) fhout = open ( filename , 'w' ) fhout . write ( "locus\t" + "\t" . join ( self . probability . hname ) + "\ttotal" ) if notes is not None : fhout . write ( "\tnotes" ) fhout . write ( "\n" ) for locus_id in report_order : lname_cur = lname [ locus_id ] fhout . write ( "\t" . join ( [ lname_cur ] + map ( str , cntdata [ : , locus_id ] . ravel ( ) ) ) ) if notes is not None : fhout . write ( "\t%s" % notes [ lname_cur ] ) fhout . write ( "\n" ) fhout . close ( )
363	def natural_keys ( text ) : def atoi ( text ) : return int ( text ) if text . isdigit ( ) else text return [ atoi ( c ) for c in re . split ( '(\d+)' , text ) ]
7477	def count_seeds ( usort ) : with open ( usort , 'r' ) as insort : cmd1 = [ "cut" , "-f" , "2" ] cmd2 = [ "uniq" ] cmd3 = [ "wc" ] proc1 = sps . Popen ( cmd1 , stdin = insort , stdout = sps . PIPE , close_fds = True ) proc2 = sps . Popen ( cmd2 , stdin = proc1 . stdout , stdout = sps . PIPE , close_fds = True ) proc3 = sps . Popen ( cmd3 , stdin = proc2 . stdout , stdout = sps . PIPE , close_fds = True ) res = proc3 . communicate ( ) nseeds = int ( res [ 0 ] . split ( ) [ 0 ] ) proc1 . stdout . close ( ) proc2 . stdout . close ( ) proc3 . stdout . close ( ) return nseeds
5347	def compose_github ( projects , data ) : for p in [ project for project in data if len ( data [ project ] [ 'github_repos' ] ) > 0 ] : if 'github' not in projects [ p ] : projects [ p ] [ 'github' ] = [ ] urls = [ url [ 'url' ] for url in data [ p ] [ 'github_repos' ] if url [ 'url' ] not in projects [ p ] [ 'github' ] ] projects [ p ] [ 'github' ] += urls return projects
1742	def make_grid ( tensor , nrow = 8 , padding = 2 , pad_value = 0 ) : if not ( isinstance ( tensor , np . ndarray ) or ( isinstance ( tensor , list ) and all ( isinstance ( t , np . ndarray ) for t in tensor ) ) ) : raise TypeError ( 'tensor or list of tensors expected, got {}' . format ( type ( tensor ) ) ) if isinstance ( tensor , list ) : tensor = np . stack ( tensor , 0 ) if tensor . ndim == 2 : tensor = tensor . reshape ( ( 1 , tensor . shape [ 0 ] , tensor . shape [ 1 ] ) ) if tensor . ndim == 3 : if tensor . shape [ 0 ] == 1 : tensor = np . concatenate ( ( tensor , tensor , tensor ) , 0 ) tensor = tensor . reshape ( ( 1 , tensor . shape [ 0 ] , tensor . shape [ 1 ] , tensor . shape [ 2 ] ) ) if tensor . ndim == 4 and tensor . shape [ 1 ] == 1 : tensor = np . concatenate ( ( tensor , tensor , tensor ) , 1 ) if tensor . shape [ 0 ] == 1 : return np . squeeze ( tensor ) nmaps = tensor . shape [ 0 ] xmaps = min ( nrow , nmaps ) ymaps = int ( math . ceil ( float ( nmaps ) / xmaps ) ) height , width = int ( tensor . shape [ 2 ] + padding ) , int ( tensor . shape [ 3 ] + padding ) grid = np . ones ( ( 3 , height * ymaps + padding , width * xmaps + padding ) ) * pad_value k = 0 for y in range ( ymaps ) : for x in range ( xmaps ) : if k >= nmaps : break grid [ : , y * height + padding : ( y + 1 ) * height , x * width + padding : ( x + 1 ) * width ] = tensor [ k ] k = k + 1 return grid
12773	def _step_to_marker_frame ( self , frame_no , dt = None ) : self . markers . detach ( ) self . markers . reposition ( frame_no ) self . markers . attach ( frame_no ) self . ode_space . collide ( None , self . on_collision ) states = self . skeleton . get_body_states ( ) self . skeleton . set_body_states ( states ) yield states self . ode_world . step ( dt or self . dt ) self . ode_contactgroup . empty ( )
373	def brightness ( x , gamma = 1 , gain = 1 , is_random = False ) : if is_random : gamma = np . random . uniform ( 1 - gamma , 1 + gamma ) x = exposure . adjust_gamma ( x , gamma , gain ) return x
7799	def check_password ( self , username , password , properties ) : logger . debug ( "check_password{0!r}" . format ( ( username , password , properties ) ) ) pwd , pwd_format = self . get_password ( username , ( u"plain" , u"md5:user:realm:password" ) , properties ) if pwd_format == u"plain" : logger . debug ( "got plain password: {0!r}" . format ( pwd ) ) return pwd is not None and password == pwd elif pwd_format in ( u"md5:user:realm:password" ) : logger . debug ( "got md5:user:realm:password password: {0!r}" . format ( pwd ) ) realm = properties . get ( "realm" ) if realm is None : realm = "" else : realm = realm . encode ( "utf-8" ) username = username . encode ( "utf-8" ) password = password . encode ( "utf-8" ) urp_hash = hashlib . md5 ( b"%s:%s:%s" ) . hexdigest ( ) return urp_hash == pwd logger . debug ( "got password in unknown format: {0!r}" . format ( pwd_format ) ) return False
11631	def _readNamelist ( currentlyIncluding , cache , namFilename , unique_glyphs ) : filename = os . path . abspath ( os . path . normcase ( namFilename ) ) if filename in currentlyIncluding : raise NamelistRecursionError ( filename ) currentlyIncluding . add ( filename ) try : result = __readNamelist ( cache , filename , unique_glyphs ) finally : currentlyIncluding . remove ( filename ) return result
8494	def _error ( msg , * args ) : print ( msg % args , file = sys . stderr ) sys . exit ( 1 )
8347	def _getAttrMap ( self ) : if not getattr ( self , 'attrMap' ) : self . attrMap = { } for ( key , value ) in self . attrs : self . attrMap [ key ] = value return self . attrMap
10168	def get_components ( self , line , with_type = True ) : ret = { } line2 = reduce ( lambda x , y : x + y , split ( '\(.+\)' , line ) ) if with_type : splitted = split ( '\W+' , line2 ) [ 3 : ] else : splitted = split ( '\W+' , line2 ) [ 2 : ] ret = dict ( zip ( splitted [ 0 : : 2 ] , splitted [ 1 : : 2 ] ) ) return ret
10551	def update_result ( result ) : try : result_id = result . id result = _forbidden_attributes ( result ) res = _pybossa_req ( 'put' , 'result' , result_id , payload = result . data ) if res . get ( 'id' ) : return Result ( res ) else : return res except : raise
3072	def init_app ( self , app , scopes = None , client_secrets_file = None , client_id = None , client_secret = None , authorize_callback = None , storage = None , ** kwargs ) : self . app = app self . authorize_callback = authorize_callback self . flow_kwargs = kwargs if storage is None : storage = dictionary_storage . DictionaryStorage ( session , key = _CREDENTIALS_KEY ) self . storage = storage if scopes is None : scopes = app . config . get ( 'GOOGLE_OAUTH2_SCOPES' , _DEFAULT_SCOPES ) self . scopes = scopes self . _load_config ( client_secrets_file , client_id , client_secret ) app . register_blueprint ( self . _create_blueprint ( ) )
1393	def getTopologyByClusterRoleEnvironAndName ( self , cluster , role , environ , topologyName ) : topologies = list ( filter ( lambda t : t . name == topologyName and t . cluster == cluster and ( not role or t . execution_state . role == role ) and t . environ == environ , self . topologies ) ) if not topologies or len ( topologies ) > 1 : if role is not None : raise Exception ( "Topology not found for {0}, {1}, {2}, {3}" . format ( cluster , role , environ , topologyName ) ) else : raise Exception ( "Topology not found for {0}, {1}, {2}" . format ( cluster , environ , topologyName ) ) return topologies [ 0 ]
13401	def removeLogbook ( self , menu = None ) : if self . logMenuCount > 1 and menu is not None : menu . removeMenu ( ) self . logMenus . remove ( menu ) self . logMenuCount -= 1
258	def perf_attrib ( returns , positions , factor_returns , factor_loadings , transactions = None , pos_in_dollars = True ) : ( returns , positions , factor_returns , factor_loadings ) = _align_and_warn ( returns , positions , factor_returns , factor_loadings , transactions = transactions , pos_in_dollars = pos_in_dollars ) positions = _stack_positions ( positions , pos_in_dollars = pos_in_dollars ) return ep . perf_attrib ( returns , positions , factor_returns , factor_loadings )
5783	def read_exactly ( self , num_bytes ) : output = b'' remaining = num_bytes while remaining > 0 : output += self . read ( remaining ) remaining = num_bytes - len ( output ) return output
13048	def f_annotation_filter ( annotations , type_uri , number ) : filtered = [ annotation for annotation in annotations if annotation . type_uri == type_uri ] number = min ( [ len ( filtered ) , number ] ) if number == 0 : return None else : return filtered [ number - 1 ]
6873	def given_lc_get_out_of_transit_points ( time , flux , err_flux , blsfit_savpath = None , trapfit_savpath = None , in_out_transit_savpath = None , sigclip = None , magsarefluxes = True , nworkers = 1 , extra_maskfrac = 0.03 ) : tmids_obsd , t_starts , t_ends = ( given_lc_get_transit_tmids_tstarts_tends ( time , flux , err_flux , blsfit_savpath = blsfit_savpath , trapfit_savpath = trapfit_savpath , magsarefluxes = magsarefluxes , nworkers = nworkers , sigclip = sigclip , extra_maskfrac = extra_maskfrac ) ) in_transit = np . zeros_like ( time ) . astype ( bool ) for t_start , t_end in zip ( t_starts , t_ends ) : this_transit = ( ( time > t_start ) & ( time < t_end ) ) in_transit |= this_transit out_of_transit = ~ in_transit if in_out_transit_savpath : _in_out_transit_plot ( time , flux , in_transit , out_of_transit , in_out_transit_savpath ) return time [ out_of_transit ] , flux [ out_of_transit ] , err_flux [ out_of_transit ]
8829	def security_group_rule_update ( context , rule , ** kwargs ) : rule . update ( kwargs ) context . session . add ( rule ) return rule
6444	def _cond_s ( self , word , suffix_len ) : return word [ - suffix_len - 2 : - suffix_len ] == 'dr' or ( word [ - suffix_len - 1 ] == 't' and word [ - suffix_len - 2 : - suffix_len ] != 'tt' )
3790	def property_derivative_P ( self , T , P , zs , ws , order = 1 ) : r sorted_valid_methods = self . select_valid_methods ( T , P , zs , ws ) for method in sorted_valid_methods : try : return self . calculate_derivative_P ( P , T , zs , ws , method , order ) except : pass return None
919	def error ( self , msg , * args , ** kwargs ) : self . _baseLogger . error ( self , self . getExtendedMsg ( msg ) , * args , ** kwargs )
4069	def _validate ( self , conditions ) : allowed_keys = set ( self . searchkeys ) operators_set = set ( self . operators . keys ( ) ) for condition in conditions : if set ( condition . keys ( ) ) != allowed_keys : raise ze . ParamNotPassed ( "Keys must be all of: %s" % ", " . join ( self . searchkeys ) ) if condition . get ( "operator" ) not in operators_set : raise ze . ParamNotPassed ( "You have specified an unknown operator: %s" % condition . get ( "operator" ) ) permitted_operators = self . conditions_operators . get ( condition . get ( "condition" ) ) permitted_operators_list = set ( [ self . operators . get ( op ) for op in permitted_operators ] ) if condition . get ( "operator" ) not in permitted_operators_list : raise ze . ParamNotPassed ( "You may not use the '%s' operator when selecting the '%s' condition. \nAllowed operators: %s" % ( condition . get ( "operator" ) , condition . get ( "condition" ) , ", " . join ( list ( permitted_operators_list ) ) , ) )
11481	def _create_folder ( local_folder , parent_folder_id ) : new_folder = session . communicator . create_folder ( session . token , os . path . basename ( local_folder ) , parent_folder_id ) return new_folder [ 'folder_id' ]
5139	def process_token ( self , kind , string , start , end , line ) : if self . current_block . is_comment : if kind == tokenize . COMMENT : self . current_block . add ( string , start , end , line ) else : self . new_noncomment ( start [ 0 ] , end [ 0 ] ) else : if kind == tokenize . COMMENT : self . new_comment ( string , start , end , line ) else : self . current_block . add ( string , start , end , line )
13464	def add_memory ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) form = MemoryForm ( request . POST or None , request . FILES or None ) if form . is_valid ( ) : instance = form . save ( commit = False ) instance . user = request . user instance . event = event instance . save ( ) msg = "Your thoughts were added. " if request . FILES : photo_list = request . FILES . getlist ( 'photos' ) photo_count = len ( photo_list ) for upload_file in photo_list : process_upload ( upload_file , instance , form , event , request ) if photo_count > 1 : msg += "{} images were added and should appear soon." . format ( photo_count ) else : msg += "{} image was added and should appear soon." . format ( photo_count ) messages . success ( request , msg ) return HttpResponseRedirect ( '../' ) return render ( request , 'happenings/add_memories.html' , { 'form' : form , 'event' : event } )
1999	def _method ( self , expression , * args ) : assert expression . __class__ . __mro__ [ - 1 ] is object for cls in expression . __class__ . __mro__ : sort = cls . __name__ methodname = 'visit_%s' % sort method = getattr ( self , methodname , None ) if method is not None : method ( expression , * args ) return return
10254	def get_causal_out_edges ( graph : BELGraph , nbunch : Union [ BaseEntity , Iterable [ BaseEntity ] ] , ) -> Set [ Tuple [ BaseEntity , BaseEntity ] ] : return { ( u , v ) for u , v , k , d in graph . out_edges ( nbunch , keys = True , data = True ) if is_causal_relation ( graph , u , v , k , d ) }
12168	def _dispatch_function ( self , event , listener , * args , ** kwargs ) : try : return listener ( * args , ** kwargs ) except Exception as exc : if event == self . LISTENER_ERROR_EVENT : raise return self . emit ( self . LISTENER_ERROR_EVENT , event , listener , exc )
4175	def window_blackman ( N , alpha = 0.16 ) : r a0 = ( 1. - alpha ) / 2. a1 = 0.5 a2 = alpha / 2. if ( N == 1 ) : win = array ( [ 1. ] ) else : k = arange ( 0 , N ) / float ( N - 1. ) win = a0 - a1 * cos ( 2 * pi * k ) + a2 * cos ( 4 * pi * k ) return win
9949	def new_space ( self , name = None , bases = None , formula = None , * , refs = None , source = None , is_derived = False , prefix = "" ) : from modelx . core . space import StaticSpaceImpl if name is None : name = self . spacenamer . get_next ( self . namespace , prefix ) if name in self . namespace : raise ValueError ( "Name '%s' already exists." % name ) if not prefix and not is_valid_name ( name ) : raise ValueError ( "Invalid name '%s'." % name ) space = self . _new_space ( name = name , formula = formula , refs = refs , source = source , is_derived = is_derived , ) self . _set_space ( space ) self . model . spacegraph . add_space ( space ) if bases is not None : if isinstance ( bases , StaticSpaceImpl ) : bases = [ bases ] space . add_bases ( bases ) return space
8439	def _generate_files ( repo_dir , config , template , version ) : with unittest . mock . patch ( 'cookiecutter.generate.run_hook' , side_effect = _patched_run_hook ) : cc_generate . generate_files ( repo_dir = repo_dir , context = { 'cookiecutter' : config , 'template' : template , 'version' : version } , overwrite_if_exists = False , output_dir = '.' )
6997	def runcp_worker ( task ) : pfpickle , outdir , lcbasedir , kwargs = task try : return runcp ( pfpickle , outdir , lcbasedir , ** kwargs ) except Exception as e : LOGEXCEPTION ( ' could not make checkplots for %s: %s' % ( pfpickle , e ) ) return None
635	def computeActivity ( self , activePresynapticCells , connectedPermanence ) : numActiveConnectedSynapsesForSegment = [ 0 ] * self . _nextFlatIdx numActivePotentialSynapsesForSegment = [ 0 ] * self . _nextFlatIdx threshold = connectedPermanence - EPSILON for cell in activePresynapticCells : for synapse in self . _synapsesForPresynapticCell [ cell ] : flatIdx = synapse . segment . flatIdx numActivePotentialSynapsesForSegment [ flatIdx ] += 1 if synapse . permanence > threshold : numActiveConnectedSynapsesForSegment [ flatIdx ] += 1 return ( numActiveConnectedSynapsesForSegment , numActivePotentialSynapsesForSegment )
8873	def isA ( instance , typeList ) : return any ( map ( lambda iType : isinstance ( instance , iType ) , typeList ) )
5216	def fut_ticker ( gen_ticker : str , dt , freq : str , log = logs . LOG_LEVEL ) -> str : logger = logs . get_logger ( fut_ticker , level = log ) dt = pd . Timestamp ( dt ) t_info = gen_ticker . split ( ) asset = t_info [ - 1 ] if asset in [ 'Index' , 'Curncy' , 'Comdty' ] : ticker = ' ' . join ( t_info [ : - 1 ] ) prefix , idx , postfix = ticker [ : - 1 ] , int ( ticker [ - 1 ] ) - 1 , asset elif asset == 'Equity' : ticker = t_info [ 0 ] prefix , idx , postfix = ticker [ : - 1 ] , int ( ticker [ - 1 ] ) - 1 , ' ' . join ( t_info [ 1 : ] ) else : logger . error ( f'unkonwn asset type for ticker: {gen_ticker}' ) return '' month_ext = 4 if asset == 'Comdty' else 2 months = pd . date_range ( start = dt , periods = max ( idx + month_ext , 3 ) , freq = freq ) logger . debug ( f'pulling expiry dates for months: {months}' ) def to_fut ( month ) : return prefix + const . Futures [ month . strftime ( '%b' ) ] + month . strftime ( '%y' ) [ - 1 ] + ' ' + postfix fut = [ to_fut ( m ) for m in months ] logger . debug ( f'trying futures: {fut}' ) try : fut_matu = bdp ( tickers = fut , flds = 'last_tradeable_dt' , cache = True ) except Exception as e1 : logger . error ( f'error downloading futures contracts (1st trial) {e1}:\n{fut}' ) try : fut = fut [ : - 1 ] logger . debug ( f'trying futures (2nd trial): {fut}' ) fut_matu = bdp ( tickers = fut , flds = 'last_tradeable_dt' , cache = True ) except Exception as e2 : logger . error ( f'error downloading futures contracts (2nd trial) {e2}:\n{fut}' ) return '' sub_fut = fut_matu [ pd . DatetimeIndex ( fut_matu . last_tradeable_dt ) > dt ] logger . debug ( f'futures full chain:\n{fut_matu.to_string()}' ) logger . debug ( f'getting index {idx} from:\n{sub_fut.to_string()}' ) return sub_fut . index . values [ idx ]
2314	def anm_score ( self , x , y ) : gp = GaussianProcessRegressor ( ) . fit ( x , y ) y_predict = gp . predict ( x ) indepscore = normalized_hsic ( y_predict - y , x ) return indepscore
2181	def parse_authorization_response ( self , url ) : log . debug ( "Parsing token from query part of url %s" , url ) token = dict ( urldecode ( urlparse ( url ) . query ) ) log . debug ( "Updating internal client token attribute." ) self . _populate_attributes ( token ) self . token = token return token
3879	async def _handle_set_typing_notification ( self , set_typing_notification ) : conv_id = set_typing_notification . conversation_id . id res = parsers . parse_typing_status_message ( set_typing_notification ) await self . on_typing . fire ( res ) try : conv = await self . _get_or_fetch_conversation ( conv_id ) except exceptions . NetworkError : logger . warning ( 'Failed to fetch conversation for typing notification: %s' , conv_id ) else : await conv . on_typing . fire ( res )
1303	def PostMessage ( handle : int , msg : int , wParam : int , lParam : int ) -> bool : return bool ( ctypes . windll . user32 . PostMessageW ( ctypes . c_void_p ( handle ) , msg , wParam , lParam ) )
4632	def child ( self , offset256 ) : a = bytes ( self ) + offset256 s = hashlib . sha256 ( a ) . digest ( ) return self . add ( s )
11602	def save_formset ( self , request , form , formset , change ) : instances = formset . save ( commit = False ) for instance in instances : if isinstance ( instance , Photo ) : instance . author = request . user instance . save ( )
5242	def market_exact ( self , session , start_time : str , end_time : str ) -> Session : if session not in self . exch : return SessNA ss = self . exch [ session ] same_day = ss [ 0 ] < ss [ - 1 ] if not start_time : s_time = ss [ 0 ] else : s_time = param . to_hour ( start_time ) if same_day : s_time = max ( s_time , ss [ 0 ] ) if not end_time : e_time = ss [ - 1 ] else : e_time = param . to_hour ( end_time ) if same_day : e_time = min ( e_time , ss [ - 1 ] ) if same_day and ( s_time > e_time ) : return SessNA return Session ( start_time = s_time , end_time = e_time )
6399	def encode ( self , word ) : word = unicode_normalize ( 'NFC' , text_type ( word . upper ( ) ) ) for i , j in self . _substitutions : word = word . replace ( i , j ) word = word . translate ( self . _trans ) return '' . join ( c for c in self . _delete_consecutive_repeats ( word ) if c in self . _uc_set )
5771	def _advapi32_verify ( certificate_or_public_key , signature , data , hash_algorithm , rsa_pss_padding = False ) : algo = certificate_or_public_key . algorithm if algo == 'rsa' and rsa_pss_padding : hash_length = { 'sha1' : 20 , 'sha224' : 28 , 'sha256' : 32 , 'sha384' : 48 , 'sha512' : 64 } . get ( hash_algorithm , 0 ) decrypted_signature = raw_rsa_public_crypt ( certificate_or_public_key , signature ) key_size = certificate_or_public_key . bit_size if not verify_pss_padding ( hash_algorithm , hash_length , key_size , data , decrypted_signature ) : raise SignatureError ( 'Signature is invalid' ) return if algo == 'rsa' and hash_algorithm == 'raw' : padded_plaintext = raw_rsa_public_crypt ( certificate_or_public_key , signature ) try : plaintext = remove_pkcs1v15_signature_padding ( certificate_or_public_key . byte_size , padded_plaintext ) if not constant_compare ( plaintext , data ) : raise ValueError ( ) except ( ValueError ) : raise SignatureError ( 'Signature is invalid' ) return hash_handle = None try : alg_id = { 'md5' : Advapi32Const . CALG_MD5 , 'sha1' : Advapi32Const . CALG_SHA1 , 'sha256' : Advapi32Const . CALG_SHA_256 , 'sha384' : Advapi32Const . CALG_SHA_384 , 'sha512' : Advapi32Const . CALG_SHA_512 , } [ hash_algorithm ] hash_handle_pointer = new ( advapi32 , 'HCRYPTHASH *' ) res = advapi32 . CryptCreateHash ( certificate_or_public_key . context_handle , alg_id , null ( ) , 0 , hash_handle_pointer ) handle_error ( res ) hash_handle = unwrap ( hash_handle_pointer ) res = advapi32 . CryptHashData ( hash_handle , data , len ( data ) , 0 ) handle_error ( res ) if algo == 'dsa' : try : signature = algos . DSASignature . load ( signature ) . to_p1363 ( ) half_len = len ( signature ) // 2 signature = signature [ half_len : ] + signature [ : half_len ] except ( ValueError , OverflowError , TypeError ) : raise SignatureError ( 'Signature is invalid' ) reversed_signature = signature [ : : - 1 ] res = advapi32 . CryptVerifySignatureW ( hash_handle , reversed_signature , len ( signature ) , certificate_or_public_key . key_handle , null ( ) , 0 ) handle_error ( res ) finally : if hash_handle : advapi32 . CryptDestroyHash ( hash_handle )
1008	def _learnPhase2 ( self , readOnly = False ) : self . lrnPredictedState [ 't' ] . fill ( 0 ) for c in xrange ( self . numberOfCols ) : i , s , numActive = self . _getBestMatchingCell ( c , self . lrnActiveState [ 't' ] , minThreshold = self . activationThreshold ) if i is None : continue self . lrnPredictedState [ 't' ] [ c , i ] = 1 if readOnly : continue segUpdate = self . _getSegmentActiveSynapses ( c , i , s , activeState = self . lrnActiveState [ 't' ] , newSynapses = ( numActive < self . newSynapseCount ) ) s . totalActivations += 1 self . _addToSegmentUpdates ( c , i , segUpdate ) if self . doPooling : predSegment = self . _getBestMatchingSegment ( c , i , self . lrnActiveState [ 't-1' ] ) segUpdate = self . _getSegmentActiveSynapses ( c , i , predSegment , self . lrnActiveState [ 't-1' ] , newSynapses = True ) self . _addToSegmentUpdates ( c , i , segUpdate )
11262	def sub ( prev , pattern , repl , * args , ** kw ) : count = 0 if 'count' not in kw else kw . pop ( 'count' ) pattern_obj = re . compile ( pattern , * args , ** kw ) for s in prev : yield pattern_obj . sub ( repl , s , count = count )
11996	def _set_options ( self , options ) : if not options : return self . options . copy ( ) options = options . copy ( ) if 'magic' in options : self . set_magic ( options [ 'magic' ] ) del ( options [ 'magic' ] ) if 'flags' in options : flags = options [ 'flags' ] del ( options [ 'flags' ] ) for key , value in flags . iteritems ( ) : if not isinstance ( value , bool ) : raise TypeError ( 'Invalid flag type for: %s' % key ) else : flags = self . options [ 'flags' ] if 'info' in options : del ( options [ 'info' ] ) for key , value in options . iteritems ( ) : if not isinstance ( value , int ) : raise TypeError ( 'Invalid option type for: %s' % key ) if value < 0 or value > 255 : raise ValueError ( 'Option value out of range for: %s' % key ) new_options = self . options . copy ( ) new_options . update ( options ) new_options [ 'flags' ] . update ( flags ) return new_options
13086	def get ( self , section , key ) : try : return self . config . get ( section , key ) except configparser . NoSectionError : pass except configparser . NoOptionError : pass return self . defaults [ section ] [ key ]
12138	def _expand_pattern ( self , pattern ) : ( globpattern , regexp , fields , types ) = self . _decompose_pattern ( pattern ) filelist = glob . glob ( globpattern ) expansion = [ ] for fname in filelist : if fields == [ ] : expansion . append ( ( fname , { } ) ) continue match = re . match ( regexp , fname ) if match is None : continue match_items = match . groupdict ( ) . items ( ) tags = dict ( ( k , types . get ( k , str ) ( v ) ) for ( k , v ) in match_items ) expansion . append ( ( fname , tags ) ) return expansion
11439	def _get_children_by_tag_name ( node , name ) : try : return [ child for child in node . childNodes if child . nodeName == name ] except TypeError : return [ ]
11775	def EnsembleLearner ( learners ) : def train ( dataset ) : predictors = [ learner ( dataset ) for learner in learners ] def predict ( example ) : return mode ( predictor ( example ) for predictor in predictors ) return predict return train
8711	def __read_chunk ( self , buf ) : log . debug ( 'reading chunk' ) timeout_before = self . _port . timeout if SYSTEM != 'Windows' : if self . _port . timeout != MINIMAL_TIMEOUT : self . _port . timeout = MINIMAL_TIMEOUT end = time . time ( ) + timeout_before while len ( buf ) < 130 and time . time ( ) <= end : buf = buf + self . _port . read ( ) if buf [ 0 ] != BLOCK_START or len ( buf ) < 130 : log . debug ( 'buffer binary: %s ' , hexify ( buf ) ) raise Exception ( 'Bad blocksize or start byte' ) if SYSTEM != 'Windows' : self . _port . timeout = timeout_before chunk_size = ord ( buf [ 1 ] ) data = buf [ 2 : chunk_size + 2 ] buf = buf [ 130 : ] return ( data , buf )
12395	def register ( self , method , args , kwargs ) : invoc = self . dump_invoc ( * args , ** kwargs ) self . registry . append ( ( invoc , method . __name__ ) )
12093	def proto_04_01_MTmon70s2 ( abf = exampleABF ) : standard_inspect ( abf ) swhlab . memtest . memtest ( abf ) swhlab . memtest . checkSweep ( abf ) swhlab . plot . save ( abf , tag = 'check' , resize = False ) swhlab . memtest . plot_standard4 ( abf ) swhlab . plot . save ( abf , tag = 'memtests' )
8018	async def disconnect ( self , code ) : try : await asyncio . wait ( self . application_futures . values ( ) , return_when = asyncio . ALL_COMPLETED , timeout = self . application_close_timeout ) except asyncio . TimeoutError : pass
13612	def run ( self , options ) : self . set_signal ( ) self . check_exclusive_mode ( ) slot = self . Handle ( self ) i = 0 while i < options . threads : t = threading . Thread ( target = self . worker , args = [ slot ] ) if options . once is True or options . no_daemon is True : t . daemon = False else : t . daemon = True t . start ( ) i += 1 if options . once is False : while True : if threading . active_count ( ) > 1 : sleep ( 1 ) else : if threading . current_thread ( ) . name == "MainThread" : sys . exit ( 0 ) pass
1436	def update_count ( self , name , incr_by = 1 , key = None ) : if name not in self . metrics : Log . error ( "In update_count(): %s is not registered in the metric" , name ) if key is None and isinstance ( self . metrics [ name ] , CountMetric ) : self . metrics [ name ] . incr ( incr_by ) elif key is not None and isinstance ( self . metrics [ name ] , MultiCountMetric ) : self . metrics [ name ] . incr ( key , incr_by ) else : Log . error ( "In update_count(): %s is registered but not supported with this method" , name )
986	def mmPrettyPrintConnections ( self ) : text = "" text += ( "Segments: (format => " "(#) [(source cell=permanence ...), ...]\n" ) text += "------------------------------------\n" columns = range ( self . numberOfColumns ( ) ) for column in columns : cells = self . cellsForColumn ( column ) for cell in cells : segmentDict = dict ( ) for seg in self . connections . segmentsForCell ( cell ) : synapseList = [ ] for synapse in self . connections . synapsesForSegment ( seg ) : synapseData = self . connections . dataForSynapse ( synapse ) synapseList . append ( ( synapseData . presynapticCell , synapseData . permanence ) ) synapseList . sort ( ) synapseStringList = [ "{0:3}={1:.2f}" . format ( sourceCell , permanence ) for sourceCell , permanence in synapseList ] segmentDict [ seg ] = "({0})" . format ( " " . join ( synapseStringList ) ) text += ( "Column {0:3} / Cell {1:3}:\t({2}) {3}\n" . format ( column , cell , len ( segmentDict . values ( ) ) , "[{0}]" . format ( ", " . join ( segmentDict . values ( ) ) ) ) ) if column < len ( columns ) - 1 : text += "\n" text += "------------------------------------\n" return text
13269	def deparagraph ( element , doc ) : if isinstance ( element , Para ) : if element . next is not None : return element elif element . prev is not None : return element return Plain ( * element . content )
3182	def create ( self , data ) : if 'id' not in data : raise KeyError ( 'The store must have an id' ) if 'list_id' not in data : raise KeyError ( 'The store must have a list_id' ) if 'name' not in data : raise KeyError ( 'The store must have a name' ) if 'currency_code' not in data : raise KeyError ( 'The store must have a currency_code' ) if not re . match ( r"^[A-Z]{3}$" , data [ 'currency_code' ] ) : raise ValueError ( 'The currency_code must be a valid 3-letter ISO 4217 currency code' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . store_id = response [ 'id' ] else : self . store_id = None return response
9146	def sheet ( connection , skip , file : TextIO ) : from tabulate import tabulate header = [ '' , 'Name' , 'Description' , 'Terms' , 'Relations' ] rows = [ ] for i , ( idx , name , manager ) in enumerate ( _iterate_managers ( connection , skip ) , start = 1 ) : try : if not manager . is_populated ( ) : continue except AttributeError : click . secho ( f'{name} does not implement is_populated' , fg = 'red' ) continue terms , relations = None , None if isinstance ( manager , BELNamespaceManagerMixin ) : terms = manager . _count_model ( manager . namespace_model ) if isinstance ( manager , BELManagerMixin ) : try : relations = manager . count_relations ( ) except TypeError as e : relations = str ( e ) rows . append ( ( i , name , manager . __doc__ . split ( '\n' ) [ 0 ] . strip ( ) . strip ( '.' ) , terms , relations ) ) print ( tabulate ( rows , headers = header , ) )
5879	def store_image ( cls , http_client , link_hash , src , config ) : image = cls . read_localfile ( link_hash , src , config ) if image : return image if src . startswith ( 'data:image' ) : image = cls . write_localfile_base64 ( link_hash , src , config ) return image data = http_client . fetch ( src ) if data : image = cls . write_localfile ( data , link_hash , src , config ) if image : return image return None
8407	def expand_range ( range , mul = 0 , add = 0 , zero_width = 1 ) : x = range try : x [ 0 ] except TypeError : x = ( x , x ) if zero_range ( x ) : new = x [ 0 ] - zero_width / 2 , x [ 0 ] + zero_width / 2 else : dx = ( x [ 1 ] - x [ 0 ] ) * mul + add new = x [ 0 ] - dx , x [ 1 ] + dx return new
8624	def add_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_post_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise UserJobsNotAddedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
13007	def _from_parts ( cls , args , init = True ) : if args : args = list ( args ) if isinstance ( args [ 0 ] , WindowsPath2 ) : args [ 0 ] = args [ 0 ] . path elif args [ 0 ] . startswith ( "\\\\?\\" ) : args [ 0 ] = args [ 0 ] [ 4 : ] args = tuple ( args ) return super ( WindowsPath2 , cls ) . _from_parts ( args , init )
9983	def replace_funcname ( source : str , name : str ) : lines = source . splitlines ( ) atok = asttokens . ASTTokens ( source , parse = True ) for node in ast . walk ( atok . tree ) : if isinstance ( node , ast . FunctionDef ) : break i = node . first_token . index for i in range ( node . first_token . index , node . last_token . index ) : if ( atok . tokens [ i ] . type == token . NAME and atok . tokens [ i ] . string == "def" ) : break lineno , col_begin = atok . tokens [ i + 1 ] . start lineno_end , col_end = atok . tokens [ i + 1 ] . end assert lineno == lineno_end lines [ lineno - 1 ] = ( lines [ lineno - 1 ] [ : col_begin ] + name + lines [ lineno - 1 ] [ col_end : ] ) return "\n" . join ( lines ) + "\n"
940	def reapVarArgsCallback ( option , optStr , value , parser ) : newValues = [ ] gotDot = False for arg in parser . rargs : if arg . startswith ( "--" ) and len ( arg ) > 2 : break if arg . startswith ( "-" ) and len ( arg ) > 1 : break if arg == "." : gotDot = True break newValues . append ( arg ) if not newValues : raise optparse . OptionValueError ( ( "Empty arg list for option %r expecting one or more args " "(remaining tokens: %r)" ) % ( optStr , parser . rargs ) ) del parser . rargs [ : len ( newValues ) + int ( gotDot ) ] value = getattr ( parser . values , option . dest , [ ] ) if value is None : value = [ ] value . extend ( newValues ) setattr ( parser . values , option . dest , value )
8360	def draw ( self , widget , cr ) : if self . bot_size is None : self . draw_default_image ( cr ) return cr = driver . ensure_pycairo_context ( cr ) surface = self . backing_store . surface cr . set_source_surface ( surface ) cr . paint ( )
7876	def bind ( self , stream , resource ) : self . stream = stream stanza = Iq ( stanza_type = "set" ) payload = ResourceBindingPayload ( resource = resource ) stanza . set_payload ( payload ) self . stanza_processor . set_response_handlers ( stanza , self . _bind_success , self . _bind_error ) stream . send ( stanza ) stream . event ( BindingResourceEvent ( resource ) )
7869	def _decode_attributes ( self ) : try : from_jid = self . _element . get ( 'from' ) if from_jid : self . _from_jid = JID ( from_jid ) to_jid = self . _element . get ( 'to' ) if to_jid : self . _to_jid = JID ( to_jid ) except ValueError : raise JIDMalformedProtocolError self . _stanza_type = self . _element . get ( 'type' ) self . _stanza_id = self . _element . get ( 'id' ) lang = self . _element . get ( XML_LANG_QNAME ) if lang : self . _language = lang
12598	def read_xl ( xl_path : str ) : xl_path , choice = _check_xl_path ( xl_path ) reader = XL_READERS [ choice ] return reader ( xl_path )
9692	def send ( self , data ) : while len ( self . senders ) >= self . window : pass self . senders [ self . new_seq_no ] = self . Sender ( self . write , self . send_lock , data , self . new_seq_no , timeout = self . sending_timeout , callback = self . send_callback , ) self . senders [ self . new_seq_no ] . start ( ) self . new_seq_no = ( self . new_seq_no + 1 ) % HDLController . MAX_SEQ_NO
7044	def lightcurve_flux_measures ( ftimes , fmags , ferrs , magsarefluxes = False ) : ndet = len ( fmags ) if ndet > 9 : if magsarefluxes : series_fluxes = fmags else : series_fluxes = 10.0 ** ( - 0.4 * fmags ) series_flux_median = npmedian ( series_fluxes ) series_flux_percent_amplitude = ( npmax ( npabs ( series_fluxes ) ) / series_flux_median ) series_flux_percentiles = nppercentile ( series_fluxes , [ 5.0 , 10 , 17.5 , 25 , 32.5 , 40 , 60 , 67.5 , 75 , 82.5 , 90 , 95 ] ) series_frat_595 = ( series_flux_percentiles [ - 1 ] - series_flux_percentiles [ 0 ] ) series_frat_1090 = ( series_flux_percentiles [ - 2 ] - series_flux_percentiles [ 1 ] ) series_frat_175825 = ( series_flux_percentiles [ - 3 ] - series_flux_percentiles [ 2 ] ) series_frat_2575 = ( series_flux_percentiles [ - 4 ] - series_flux_percentiles [ 3 ] ) series_frat_325675 = ( series_flux_percentiles [ - 5 ] - series_flux_percentiles [ 4 ] ) series_frat_4060 = ( series_flux_percentiles [ - 6 ] - series_flux_percentiles [ 5 ] ) series_flux_percentile_ratio_mid20 = series_frat_4060 / series_frat_595 series_flux_percentile_ratio_mid35 = series_frat_325675 / series_frat_595 series_flux_percentile_ratio_mid50 = series_frat_2575 / series_frat_595 series_flux_percentile_ratio_mid65 = series_frat_175825 / series_frat_595 series_flux_percentile_ratio_mid80 = series_frat_1090 / series_frat_595 series_percent_difference_flux_percentile = ( series_frat_595 / series_flux_median ) series_percentile_magdiff = - 2.5 * nplog10 ( series_percent_difference_flux_percentile ) return { 'flux_median' : series_flux_median , 'flux_percent_amplitude' : series_flux_percent_amplitude , 'flux_percentiles' : series_flux_percentiles , 'flux_percentile_ratio_mid20' : series_flux_percentile_ratio_mid20 , 'flux_percentile_ratio_mid35' : series_flux_percentile_ratio_mid35 , 'flux_percentile_ratio_mid50' : series_flux_percentile_ratio_mid50 , 'flux_percentile_ratio_mid65' : series_flux_percentile_ratio_mid65 , 'flux_percentile_ratio_mid80' : series_flux_percentile_ratio_mid80 , 'percent_difference_flux_percentile' : series_percentile_magdiff , } else : LOGERROR ( 'not enough detections in this magseries ' 'to calculate flux measures' ) return None
332	def model_stoch_vol ( data , samples = 2000 , progressbar = True ) : from pymc3 . distributions . timeseries import GaussianRandomWalk with pm . Model ( ) as model : nu = pm . Exponential ( 'nu' , 1. / 10 , testval = 5. ) sigma = pm . Exponential ( 'sigma' , 1. / .02 , testval = .1 ) s = GaussianRandomWalk ( 's' , sigma ** - 2 , shape = len ( data ) ) volatility_process = pm . Deterministic ( 'volatility_process' , pm . math . exp ( - 2 * s ) ) pm . StudentT ( 'r' , nu , lam = volatility_process , observed = data ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace
9540	def number_range_exclusive ( min , max , type = float ) : def checker ( v ) : if type ( v ) <= min or type ( v ) >= max : raise ValueError ( v ) return checker
11415	def record_delete_subfield_from ( rec , tag , subfield_position , field_position_global = None , field_position_local = None ) : subfields = record_get_subfields ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) try : del subfields [ subfield_position ] except IndexError : raise InvenioBibRecordFieldError ( "The record does not contain the subfield " "'%(subfieldIndex)s' inside the field (local: " "'%(fieldIndexLocal)s, global: '%(fieldIndexGlobal)s' ) of tag " "'%(tag)s'." % { "subfieldIndex" : subfield_position , "fieldIndexLocal" : str ( field_position_local ) , "fieldIndexGlobal" : str ( field_position_global ) , "tag" : tag } ) if not subfields : if field_position_global is not None : for position , field in enumerate ( rec [ tag ] ) : if field [ 4 ] == field_position_global : del rec [ tag ] [ position ] else : del rec [ tag ] [ field_position_local ] if not rec [ tag ] : del rec [ tag ]
6533	def get_project_config ( project_path , use_cache = True ) : return get_local_config ( project_path , use_cache = use_cache ) or get_user_config ( project_path , use_cache = use_cache ) or get_default_config ( )
5538	def _extract ( self , in_tile = None , in_data = None , out_tile = None ) : return self . config . output . extract_subset ( input_data_tiles = [ ( in_tile , in_data ) ] , out_tile = out_tile )
4569	def dump ( data , file = sys . stdout , use_yaml = None , ** kwds ) : if use_yaml is None : use_yaml = ALWAYS_DUMP_YAML def dump ( fp ) : if use_yaml : yaml . safe_dump ( data , stream = fp , ** kwds ) else : json . dump ( data , fp , indent = 4 , sort_keys = True , ** kwds ) if not isinstance ( file , str ) : return dump ( file ) if os . path . isabs ( file ) : parent = os . path . dirname ( file ) if not os . path . exists ( parent ) : os . makedirs ( parent , exist_ok = True ) with open ( file , 'w' ) as fp : return dump ( fp )
1400	def extract_logical_plan ( self , topology ) : logicalPlan = { "spouts" : { } , "bolts" : { } , } for spout in topology . spouts ( ) : spoutName = spout . comp . name spoutType = "default" spoutSource = "NA" spoutVersion = "NA" spoutConfigs = spout . comp . config . kvs for kvs in spoutConfigs : if kvs . key == "spout.type" : spoutType = javaobj . loads ( kvs . serialized_value ) elif kvs . key == "spout.source" : spoutSource = javaobj . loads ( kvs . serialized_value ) elif kvs . key == "spout.version" : spoutVersion = javaobj . loads ( kvs . serialized_value ) spoutPlan = { "config" : convert_pb_kvs ( spoutConfigs , include_non_primitives = False ) , "type" : spoutType , "source" : spoutSource , "version" : spoutVersion , "outputs" : [ ] } for outputStream in list ( spout . outputs ) : spoutPlan [ "outputs" ] . append ( { "stream_name" : outputStream . stream . id } ) logicalPlan [ "spouts" ] [ spoutName ] = spoutPlan for bolt in topology . bolts ( ) : boltName = bolt . comp . name boltPlan = { "config" : convert_pb_kvs ( bolt . comp . config . kvs , include_non_primitives = False ) , "outputs" : [ ] , "inputs" : [ ] } for outputStream in list ( bolt . outputs ) : boltPlan [ "outputs" ] . append ( { "stream_name" : outputStream . stream . id } ) for inputStream in list ( bolt . inputs ) : boltPlan [ "inputs" ] . append ( { "stream_name" : inputStream . stream . id , "component_name" : inputStream . stream . component_name , "grouping" : topology_pb2 . Grouping . Name ( inputStream . gtype ) } ) logicalPlan [ "bolts" ] [ boltName ] = boltPlan return logicalPlan
12592	def query_reliabledictionary ( client , application_name , service_name , dictionary_name , query_string , partition_key = None , partition_id = None , output_file = None ) : cluster = Cluster . from_sfclient ( client ) dictionary = cluster . get_application ( application_name ) . get_service ( service_name ) . get_dictionary ( dictionary_name ) start = time . time ( ) if ( partition_id != None ) : result = dictionary . query ( query_string , PartitionLookup . ID , partition_id ) elif ( partition_key != None ) : result = dictionary . query ( query_string , PartitionLookup . KEY , partition_key ) else : result = dictionary . query ( query_string ) if type ( result ) is str : print ( result ) return else : result = json . dumps ( result . get ( "value" ) , indent = 4 ) print ( "Query took " + str ( time . time ( ) - start ) + " seconds" ) if ( output_file == None ) : output_file = "{}-{}-{}-query-output.json" . format ( application_name , service_name , dictionary_name ) with open ( output_file , "w" ) as output : output . write ( result ) print ( ) print ( 'Printed output to: ' + output_file ) print ( result )
7982	def auth_finish ( self , _unused ) : self . lock . acquire ( ) try : self . __logger . debug ( "Authenticated" ) self . authenticated = True self . state_change ( "authorized" , self . my_jid ) self . _post_auth ( ) finally : self . lock . release ( )
2739	def get_object ( cls , api_token , firewall_id ) : firewall = cls ( token = api_token , id = firewall_id ) firewall . load ( ) return firewall
7639	def get_comments ( jam , ann ) : jam_comments = jam . file_metadata . __json__ ann_comments = ann . annotation_metadata . __json__ return json . dumps ( { 'metadata' : jam_comments , 'annotation metadata' : ann_comments } , indent = 2 )
2201	def ensure_app_config_dir ( appname , * args ) : from ubelt import util_path dpath = get_app_config_dir ( appname , * args ) util_path . ensuredir ( dpath ) return dpath
10682	def H_mag ( self , T ) : tau = T / self . Tc_mag if tau <= 1.0 : h = ( - self . _A_mag / tau + self . _B_mag * ( tau ** 3 / 2 + tau ** 9 / 15 + tau ** 15 / 40 ) ) / self . _D_mag else : h = - ( tau ** - 5 / 2 + tau ** - 15 / 21 + tau ** - 25 / 60 ) / self . _D_mag return R * T * math . log ( self . beta0_mag + 1 ) * h
7744	def _timeout_cb ( self , method ) : self . _anything_done = True logger . debug ( "_timeout_cb() called for: {0!r}" . format ( method ) ) result = method ( ) rec = method . _pyxmpp_recurring if rec : self . _prepare_pending ( ) return True if rec is None and result is not None : logger . debug ( " auto-recurring, restarting in {0} s" . format ( result ) ) tag = glib . timeout_add ( int ( result * 1000 ) , self . _timeout_cb , method ) self . _timer_sources [ method ] = tag else : self . _timer_sources . pop ( method , None ) self . _prepare_pending ( ) return False
5283	def get_success_url ( self ) : if self . success_url : url = self . success_url else : url = self . request . get_full_path ( ) return url
3745	def calculate ( self , T , method ) : r if method == DUTT_PRASAD : A , B , C = self . DUTT_PRASAD_coeffs mu = ViswanathNatarajan3 ( T , A , B , C , ) elif method == VISWANATH_NATARAJAN_3 : A , B , C = self . VISWANATH_NATARAJAN_3_coeffs mu = ViswanathNatarajan3 ( T , A , B , C ) elif method == VISWANATH_NATARAJAN_2 : A , B = self . VISWANATH_NATARAJAN_2_coeffs mu = ViswanathNatarajan2 ( T , self . VISWANATH_NATARAJAN_2_coeffs [ 0 ] , self . VISWANATH_NATARAJAN_2_coeffs [ 1 ] ) elif method == VISWANATH_NATARAJAN_2E : C , D = self . VISWANATH_NATARAJAN_2E_coeffs mu = ViswanathNatarajan2Exponential ( T , C , D ) elif method == DIPPR_PERRY_8E : mu = EQ101 ( T , * self . Perrys2_313_coeffs ) elif method == COOLPROP : mu = CoolProp_T_dependent_property ( T , self . CASRN , 'V' , 'l' ) elif method == LETSOU_STIEL : mu = Letsou_Stiel ( T , self . MW , self . Tc , self . Pc , self . omega ) elif method == PRZEDZIECKI_SRIDHAR : Vml = self . Vml ( T ) if hasattr ( self . Vml , '__call__' ) else self . Vml mu = Przedziecki_Sridhar ( T , self . Tm , self . Tc , self . Pc , self . Vc , Vml , self . omega , self . MW ) elif method == VDI_PPDS : A , B , C , D , E = self . VDI_PPDS_coeffs term = ( C - T ) / ( T - D ) if term < 0 : term1 = - ( ( T - C ) / ( T - D ) ) ** ( 1 / 3. ) else : term1 = term ** ( 1 / 3. ) term2 = term * term1 mu = E * exp ( A * term1 + B * term2 ) elif method in self . tabular_data : mu = self . interpolate ( T , method ) return mu
12485	def get_dict_leaves ( data ) : result = [ ] if isinstance ( data , dict ) : for item in data . values ( ) : result . extend ( get_dict_leaves ( item ) ) elif isinstance ( data , list ) : result . extend ( data ) else : result . append ( data ) return result
8085	def nostroke ( self ) : c = self . _canvas . strokecolor self . _canvas . strokecolor = None return c
12403	def requirements_for_changes ( self , changes ) : requirements = [ ] reqs_set = set ( ) if isinstance ( changes , str ) : changes = changes . split ( '\n' ) if not changes or changes [ 0 ] . startswith ( '-' ) : return requirements for line in changes : line = line . strip ( ' -+*' ) if not line : continue match = IS_REQUIREMENTS_RE2 . search ( line ) if match : for match in REQUIREMENTS_RE . findall ( match . group ( 1 ) ) : if match [ 1 ] : version = '==' + match [ 2 ] if match [ 1 ] . startswith ( ' to ' ) else match [ 1 ] req_str = match [ 0 ] + version else : req_str = match [ 0 ] if req_str not in reqs_set : reqs_set . add ( req_str ) try : requirements . append ( pkg_resources . Requirement . parse ( req_str ) ) except Exception as e : log . warn ( 'Could not parse requirement "%s" from changes: %s' , req_str , e ) return requirements
10260	def remove_falsy_values ( counter : Mapping [ Any , int ] ) -> Mapping [ Any , int ] : return { label : count for label , count in counter . items ( ) if count }
12285	def add ( self , repo ) : key = self . key ( repo . username , repo . reponame ) repo . key = key self . repos [ key ] = repo return key
7379	def _get ( self , text ) : if self . strict : match = self . prog . match ( text ) if match : cmd = match . group ( ) if cmd in self : return cmd else : words = self . prog . findall ( text ) for word in words : if word in self : return word
3546	def _characteristics_discovered ( self , service ) : self . _discovered_services . add ( service ) if self . _discovered_services >= set ( self . _peripheral . services ( ) ) : self . _discovered . set ( )
2176	def request ( self , method , url , data = None , headers = None , withhold_token = False , client_id = None , client_secret = None , ** kwargs ) : if not is_secure_transport ( url ) : raise InsecureTransportError ( ) if self . token and not withhold_token : log . debug ( "Invoking %d protected resource request hooks." , len ( self . compliance_hook [ "protected_request" ] ) , ) for hook in self . compliance_hook [ "protected_request" ] : log . debug ( "Invoking hook %s." , hook ) url , headers , data = hook ( url , headers , data ) log . debug ( "Adding token %s to request." , self . token ) try : url , headers , data = self . _client . add_token ( url , http_method = method , body = data , headers = headers ) except TokenExpiredError : if self . auto_refresh_url : log . debug ( "Auto refresh is set, attempting to refresh at %s." , self . auto_refresh_url , ) auth = kwargs . pop ( "auth" , None ) if client_id and client_secret and ( auth is None ) : log . debug ( 'Encoding client_id "%s" with client_secret as Basic auth credentials.' , client_id , ) auth = requests . auth . HTTPBasicAuth ( client_id , client_secret ) token = self . refresh_token ( self . auto_refresh_url , auth = auth , ** kwargs ) if self . token_updater : log . debug ( "Updating token to %s using %s." , token , self . token_updater ) self . token_updater ( token ) url , headers , data = self . _client . add_token ( url , http_method = method , body = data , headers = headers ) else : raise TokenUpdated ( token ) else : raise log . debug ( "Requesting url %s using method %s." , url , method ) log . debug ( "Supplying headers %s and data %s" , headers , data ) log . debug ( "Passing through key word arguments %s." , kwargs ) return super ( OAuth2Session , self ) . request ( method , url , headers = headers , data = data , ** kwargs )
5814	def _try_decode ( byte_string ) : try : return str_cls ( byte_string , _encoding ) except ( UnicodeDecodeError ) : for encoding in _fallback_encodings : try : return str_cls ( byte_string , encoding , errors = 'strict' ) except ( UnicodeDecodeError ) : pass return str_cls ( byte_string , errors = 'replace' )
2968	def _sm_relieve_pain ( self , * args , ** kwargs ) : _logger . info ( "Ending the degradation for blockade %s" % self . _blockade_name ) self . _do_reset_all ( ) millisec = random . randint ( self . _start_min_delay , self . _start_max_delay ) self . _timer = threading . Timer ( millisec / 1000.0 , self . event_timeout ) self . _timer . start ( )
6505	def add_properties ( self ) : for property_name in [ p [ 0 ] for p in inspect . getmembers ( self . __class__ ) if isinstance ( p [ 1 ] , property ) ] : self . _results_fields [ property_name ] = getattr ( self , property_name , None )
6515	def output ( self , msg , newline = True ) : click . echo ( text_type ( msg ) , nl = newline , file = self . output_file )
11242	def indent_css ( f , output ) : line_count = get_line_count ( f ) f = open ( f , 'r+' ) output = open ( output , 'r+' ) for line in range ( line_count ) : string = f . readline ( ) . rstrip ( ) if len ( string ) > 0 : if string [ - 1 ] == ";" : output . write ( " " + string + "\n" ) else : output . write ( string + "\n" ) output . close ( ) f . close ( )
2782	def destroy ( self ) : return self . get_data ( "domains/%s/records/%s" % ( self . domain , self . id ) , type = DELETE , )
7152	def one ( prompt , * args , ** kwargs ) : indicator = '' if sys . version_info < ( 3 , 0 ) : indicator = '>' def go_back ( picker ) : return None , - 1 options , verbose_options = prepare_options ( args ) idx = kwargs . get ( 'idx' , 0 ) picker = Picker ( verbose_options , title = prompt , indicator = indicator , default_index = idx ) picker . register_custom_handler ( ord ( 'h' ) , go_back ) picker . register_custom_handler ( curses . KEY_LEFT , go_back ) with stdout_redirected ( sys . stderr ) : option , index = picker . start ( ) if index == - 1 : raise QuestionnaireGoBack if kwargs . get ( 'return_index' , False ) : return index return options [ index ]
2484	def create_checksum_node ( self , chksum ) : chksum_node = BNode ( ) type_triple = ( chksum_node , RDF . type , self . spdx_namespace . Checksum ) self . graph . add ( type_triple ) algorithm_triple = ( chksum_node , self . spdx_namespace . algorithm , Literal ( chksum . identifier ) ) self . graph . add ( algorithm_triple ) value_triple = ( chksum_node , self . spdx_namespace . checksumValue , Literal ( chksum . value ) ) self . graph . add ( value_triple ) return chksum_node
5741	def result ( self , timeout = None ) : start = time . time ( ) while True : task = self . get_task ( ) if not task or task . status not in ( FINISHED , FAILED ) : if not timeout : continue elif time . time ( ) - start < timeout : continue else : raise TimeoutError ( ) if task . status == FAILED : raise task . result return task . result
8837	def merge ( * args ) : ret = [ ] for arg in args : if isinstance ( arg , list ) or isinstance ( arg , tuple ) : ret += list ( arg ) else : ret . append ( arg ) return ret
1672	def ProcessFile ( filename , vlevel , extra_check_functions = None ) : _SetVerboseLevel ( vlevel ) _BackupFilters ( ) if not ProcessConfigOverrides ( filename ) : _RestoreFilters ( ) return lf_lines = [ ] crlf_lines = [ ] try : if filename == '-' : lines = codecs . StreamReaderWriter ( sys . stdin , codecs . getreader ( 'utf8' ) , codecs . getwriter ( 'utf8' ) , 'replace' ) . read ( ) . split ( '\n' ) else : lines = codecs . open ( filename , 'r' , 'utf8' , 'replace' ) . read ( ) . split ( '\n' ) for linenum in range ( len ( lines ) - 1 ) : if lines [ linenum ] . endswith ( '\r' ) : lines [ linenum ] = lines [ linenum ] . rstrip ( '\r' ) crlf_lines . append ( linenum + 1 ) else : lf_lines . append ( linenum + 1 ) except IOError : _cpplint_state . PrintError ( "Skipping input '%s': Can't open for reading\n" % filename ) _RestoreFilters ( ) return file_extension = filename [ filename . rfind ( '.' ) + 1 : ] if filename != '-' and file_extension not in GetAllExtensions ( ) : bazel_gen_files = set ( [ "external/local_config_cc/libtool" , "external/local_config_cc/make_hashed_objlist.py" , "external/local_config_cc/wrapped_ar" , "external/local_config_cc/wrapped_clang" , "external/local_config_cc/xcrunwrapper.sh" , ] ) if not filename in bazel_gen_files : _cpplint_state . PrintError ( 'Ignoring %s; not a valid file name ' '(%s)\n' % ( filename , ', ' . join ( GetAllExtensions ( ) ) ) ) else : ProcessFileData ( filename , file_extension , lines , Error , extra_check_functions ) if lf_lines and crlf_lines : for linenum in crlf_lines : Error ( filename , linenum , 'whitespace/newline' , 1 , 'Unexpected \\r (^M) found; better to use only \\n' ) _RestoreFilters ( )
10922	def do_levmarq_n_directions ( s , directions , max_iter = 2 , run_length = 2 , damping = 1e-3 , collect_stats = False , marquardt_damping = True , ** kwargs ) : normals = np . array ( [ d / np . sqrt ( np . dot ( d , d ) ) for d in directions ] ) if np . isnan ( normals ) . any ( ) : raise ValueError ( '`directions` must not be 0s or contain nan' ) obj = OptState ( s , normals ) lo = LMOptObj ( obj , max_iter = max_iter , run_length = run_length , damping = damping , marquardt_damping = marquardt_damping , ** kwargs ) lo . do_run_1 ( ) if collect_stats : return lo . get_termination_stats ( )
3164	def all ( self , workflow_id , email_id ) : self . workflow_id = workflow_id self . email_id = email_id self . subscriber_hash = None return self . _mc_client . _get ( url = self . _build_path ( workflow_id , 'emails' , email_id , 'queue' ) )
9012	def _start ( self ) : self . _instruction_library = self . _spec . new_default_instructions ( ) self . _as_instruction = self . _instruction_library . as_instruction self . _id_cache = { } self . _pattern_set = None self . _inheritance_todos = [ ] self . _instruction_todos = [ ]
4665	def id ( self ) : sigs = self . data [ "signatures" ] self . data . pop ( "signatures" , None ) h = hashlib . sha256 ( bytes ( self ) ) . digest ( ) self . data [ "signatures" ] = sigs return hexlify ( h [ : 20 ] ) . decode ( "ascii" )
1127	def Seq ( first_rule , * rest_of_rules , ** kwargs ) : @ llrule ( kwargs . get ( "loc" , None ) , first_rule . expected ) def rule ( parser ) : result = first_rule ( parser ) if result is unmatched : return result results = [ result ] for rule in rest_of_rules : result = rule ( parser ) if result is unmatched : return result results . append ( result ) return tuple ( results ) return rule
13475	def start ( self ) : assert not self . has_started ( ) , "called start() on an active GeventLoop" self . _stop_event = Event ( ) self . _greenlet = gevent . spawn ( self . _loop )
8955	def parse_glob ( pattern ) : if not pattern : return bits = pattern . split ( "/" ) dirs , filename = bits [ : - 1 ] , bits [ - 1 ] for dirname in dirs : if dirname == "**" : yield "(|.+/)" else : yield glob2re ( dirname ) + "/" yield glob2re ( filename )
2265	def dict_isect ( * args ) : if not args : return { } else : dictclass = OrderedDict if isinstance ( args [ 0 ] , OrderedDict ) else dict common_keys = set . intersection ( * map ( set , args ) ) first_dict = args [ 0 ] return dictclass ( ( k , first_dict [ k ] ) for k in common_keys )
8752	def is_isonet_vif ( vif ) : nicira_iface_id = vif . record . get ( 'other_config' ) . get ( 'nicira-iface-id' ) if nicira_iface_id : return True return False
8169	def run_context ( self ) : with LiveExecution . lock : if self . edited_source is None : yield True , self . known_good , self . ns return ns_snapshot = copy . copy ( self . ns ) try : yield False , self . edited_source , self . ns self . known_good = self . edited_source self . edited_source = None self . call_good_cb ( ) return except Exception as ex : tb = traceback . format_exc ( ) self . call_bad_cb ( tb ) self . edited_source = None self . ns . clear ( ) self . ns . update ( ns_snapshot )
10401	def get_final_score ( self ) -> float : if not self . done_chomping ( ) : raise ValueError ( 'algorithm has not yet completed' ) return self . graph . nodes [ self . target_node ] [ self . tag ]
10743	def declaration ( function ) : function , name = _strip_function ( function ) if not function . __code__ . co_code in [ empty_function . __code__ . co_code , doc_string_only_function . __code__ . co_code ] : raise ValueError ( 'Declaration requires empty function definition' ) def not_implemented_function ( * args , ** kwargs ) : raise ValueError ( 'Argument \'{}\' did not specify how \'{}\' should act on it' . format ( args [ 0 ] , name ) ) not_implemented_function . __qualname__ = not_implemented_function . __name__ return default ( not_implemented_function , name = name )
2060	def add ( self , constraint , check = False ) : if isinstance ( constraint , bool ) : constraint = BoolConstant ( constraint ) assert isinstance ( constraint , Bool ) constraint = simplify ( constraint ) if self . _child is not None : raise Exception ( 'ConstraintSet is frozen' ) if isinstance ( constraint , BoolConstant ) : if not constraint . value : logger . info ( "Adding an impossible constant constraint" ) self . _constraints = [ constraint ] else : return self . _constraints . append ( constraint ) if check : from . . . core . smtlib import solver if not solver . check ( self ) : raise ValueError ( "Added an impossible constraint" )
10139	def encrypt_files ( selected_host , only_link , file_name ) : if ENCRYPTION_DISABLED : print ( 'For encryption please install gpg' ) exit ( ) passphrase = '%030x' % random . randrange ( 16 ** 30 ) source_filename = file_name cmd = 'gpg --batch --symmetric --cipher-algo AES256 --passphrase-fd 0 ' '--output - {}' . format ( source_filename ) encrypted_output = Popen ( shlex . split ( cmd ) , stdout = PIPE , stdin = PIPE , stderr = PIPE ) encrypted_data = encrypted_output . communicate ( passphrase . encode ( ) ) [ 0 ] return upload_files ( encrypted_data , selected_host , only_link , file_name ) + '#' + passphrase
9344	def read ( self , n ) : while len ( self . pool ) < n : self . cur = self . files . next ( ) self . pool = numpy . append ( self . pool , self . fetch ( self . cur ) , axis = 0 ) rt = self . pool [ : n ] if n == len ( self . pool ) : self . pool = self . fetch ( None ) else : self . pool = self . pool [ n : ] return rt
13537	def _child_allowed ( self , child_rule ) : num_kids = self . node . children . count ( ) num_kids_allowed = len ( self . rule . children ) if not self . rule . multiple_paths : num_kids_allowed = 1 if num_kids >= num_kids_allowed : raise AttributeError ( 'Rule %s only allows %s children' % ( self . rule_name , self . num_kids_allowed ) ) for node in self . node . children . all ( ) : if node . data . rule_label == child_rule . class_label : raise AttributeError ( 'Child rule already exists' ) if child_rule not in self . rule . children : raise AttributeError ( 'Rule %s is not a valid child of Rule %s' % ( child_rule . __name__ , self . rule_name ) )
6243	def load ( self ) : self . _open_image ( ) width , height , depth = self . image . size [ 0 ] , self . image . size [ 1 ] // self . layers , self . layers components , data = image_data ( self . image ) texture = self . ctx . texture_array ( ( width , height , depth ) , components , data , ) texture . extra = { 'meta' : self . meta } if self . meta . mipmap : texture . build_mipmaps ( ) self . _close_image ( ) return texture
9677	def calculate_bin_boundary ( self , bb ) : return min ( enumerate ( OPC_LOOKUP ) , key = lambda x : abs ( x [ 1 ] - bb ) ) [ 0 ]
13199	def format_content ( self , format = 'plain' , mathjax = False , smart = True , extra_args = None ) : output_text = convert_lsstdoc_tex ( self . _tex , format , mathjax = mathjax , smart = smart , extra_args = extra_args ) return output_text
8855	def setup_mnu_style ( self , editor ) : menu = QtWidgets . QMenu ( 'Styles' , self . menuEdit ) group = QtWidgets . QActionGroup ( self ) self . styles_group = group current_style = editor . syntax_highlighter . color_scheme . name group . triggered . connect ( self . on_style_changed ) for s in sorted ( PYGMENTS_STYLES ) : a = QtWidgets . QAction ( menu ) a . setText ( s ) a . setCheckable ( True ) if s == current_style : a . setChecked ( True ) group . addAction ( a ) menu . addAction ( a ) self . menuEdit . addMenu ( menu )
3782	def set_user_methods_P ( self , user_methods_P , forced_P = False ) : r if isinstance ( user_methods_P , str ) : user_methods_P = [ user_methods_P ] self . user_methods_P = user_methods_P self . forced_P = forced_P if set ( self . user_methods_P ) . difference ( self . all_methods_P ) : raise Exception ( "One of the given methods is not available for this chemical" ) if not self . user_methods_P and self . forced : raise Exception ( 'Only user specified methods are considered when forced is True, but no methods were provided' ) self . method_P = None self . sorted_valid_methods_P = [ ] self . TP_cached = None
5755	def get_regressions ( package_descriptors , targets , building_repo_data , testing_repo_data , main_repo_data ) : regressions = { } for package_descriptor in package_descriptors . values ( ) : pkg_name = package_descriptor . pkg_name debian_pkg_name = package_descriptor . debian_pkg_name regressions [ pkg_name ] = { } for target in targets : regressions [ pkg_name ] [ target ] = False main_version = main_repo_data . get ( target , { } ) . get ( debian_pkg_name , None ) if main_version is not None : main_ver_loose = LooseVersion ( main_version ) for repo_data in [ building_repo_data , testing_repo_data ] : version = repo_data . get ( target , { } ) . get ( debian_pkg_name , None ) if not version or main_ver_loose > LooseVersion ( version ) : regressions [ pkg_name ] [ target ] = True return regressions
1962	def sys_rt_sigaction ( self , signum , act , oldact ) : return self . sys_sigaction ( signum , act , oldact )
5622	def path_is_remote ( path , s3 = True ) : prefixes = ( "http://" , "https://" , "/vsicurl/" ) if s3 : prefixes += ( "s3://" , "/vsis3/" ) return path . startswith ( prefixes )
8912	def includeme ( config ) : settings = config . registry . settings if asbool ( settings . get ( 'twitcher.rpcinterface' , True ) ) : LOGGER . debug ( 'Twitcher XML-RPC Interface enabled.' ) config . include ( 'twitcher.config' ) config . include ( 'twitcher.basicauth' ) config . include ( 'pyramid_rpc.xmlrpc' ) config . include ( 'twitcher.db' ) config . add_xmlrpc_endpoint ( 'api' , '/RPC2' ) config . add_xmlrpc_method ( RPCInterface , attr = 'generate_token' , endpoint = 'api' , method = 'generate_token' ) config . add_xmlrpc_method ( RPCInterface , attr = 'revoke_token' , endpoint = 'api' , method = 'revoke_token' ) config . add_xmlrpc_method ( RPCInterface , attr = 'revoke_all_tokens' , endpoint = 'api' , method = 'revoke_all_tokens' ) config . add_xmlrpc_method ( RPCInterface , attr = 'register_service' , endpoint = 'api' , method = 'register_service' ) config . add_xmlrpc_method ( RPCInterface , attr = 'unregister_service' , endpoint = 'api' , method = 'unregister_service' ) config . add_xmlrpc_method ( RPCInterface , attr = 'get_service_by_name' , endpoint = 'api' , method = 'get_service_by_name' ) config . add_xmlrpc_method ( RPCInterface , attr = 'get_service_by_url' , endpoint = 'api' , method = 'get_service_by_url' ) config . add_xmlrpc_method ( RPCInterface , attr = 'clear_services' , endpoint = 'api' , method = 'clear_services' ) config . add_xmlrpc_method ( RPCInterface , attr = 'list_services' , endpoint = 'api' , method = 'list_services' )
6821	def configure_modsecurity ( self ) : r = self . local_renderer if r . env . modsecurity_enabled and not self . last_manifest . modsecurity_enabled : self . install_packages ( ) fn = self . render_to_file ( 'apache/apache_modsecurity.template.conf' ) r . put ( local_path = fn , remote_path = '/etc/modsecurity/modsecurity.conf' , use_sudo = True ) r . env . modsecurity_download_filename = '/tmp/owasp-modsecurity-crs.tar.gz' r . sudo ( 'cd /tmp; wget --output-document={apache_modsecurity_download_filename} {apache_modsecurity_download_url}' ) r . env . modsecurity_download_top = r . sudo ( "cd /tmp; " "tar tzf %(apache_modsecurity_download_filename)s | sed -e 's@/.*@@' | uniq" % self . genv ) r . sudo ( 'cd /tmp; tar -zxvf %(apache_modsecurity_download_filename)s' % self . genv ) r . sudo ( 'cd /tmp; cp -R %(apache_modsecurity_download_top)s/* /etc/modsecurity/' % self . genv ) r . sudo ( 'mv /etc/modsecurity/modsecurity_crs_10_setup.conf.example /etc/modsecurity/modsecurity_crs_10_setup.conf' ) r . sudo ( 'rm -f /etc/modsecurity/activated_rules/*' ) r . sudo ( 'cd /etc/modsecurity/base_rules; ' 'for f in * ; do ln -s /etc/modsecurity/base_rules/$f /etc/modsecurity/activated_rules/$f ; done' ) r . sudo ( 'cd /etc/modsecurity/optional_rules; ' 'for f in * ; do ln -s /etc/modsecurity/optional_rules/$f /etc/modsecurity/activated_rules/$f ; done' ) r . env . httpd_conf_append . append ( 'Include "/etc/modsecurity/activated_rules/*.conf"' ) self . enable_mod ( 'evasive' ) self . enable_mod ( 'headers' ) elif not self . env . modsecurity_enabled and self . last_manifest . modsecurity_enabled : self . disable_mod ( 'modsecurity' )
8268	def color ( self , clr = None , d = 0.035 ) : if clr != None and not isinstance ( clr , Color ) : clr = color ( clr ) if clr != None and not self . grayscale : if clr . is_black : return self . black . color ( clr , d ) if clr . is_white : return self . white . color ( clr , d ) if clr . is_grey : return choice ( ( self . black . color ( clr , d ) , self . white . color ( clr , d ) ) ) h , s , b , a = self . h , self . s , self . b , self . a if clr != None : h , a = clr . h + d * ( random ( ) * 2 - 1 ) , clr . a hsba = [ ] for v in [ h , s , b , a ] : if isinstance ( v , _list ) : min , max = choice ( v ) elif isinstance ( v , tuple ) : min , max = v else : min , max = v , v hsba . append ( min + ( max - min ) * random ( ) ) h , s , b , a = hsba return color ( h , s , b , a , mode = "hsb" )
7579	def result_files ( self ) : reps = OPJ ( self . workdir , self . name + "-K-*-rep-*_f" ) repfiles = glob . glob ( reps ) return repfiles
2476	def set_lic_comment ( self , doc , comment ) : if self . has_extr_lic ( doc ) : if not self . extr_lic_comment_set : self . extr_lic_comment_set = True if validations . validate_is_free_form_text ( comment ) : self . extr_lic ( doc ) . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'ExtractedLicense::comment' ) else : raise CardinalityError ( 'ExtractedLicense::comment' ) else : raise OrderError ( 'ExtractedLicense::comment' )
13591	def n_p ( self ) : return 2 * _sltr . GeV2joule ( self . E ) * _spc . epsilon_0 / ( self . beta * _spc . elementary_charge ) ** 2
9150	def to_indra_statements ( self , * args , ** kwargs ) : graph = self . to_bel ( * args , ** kwargs ) return to_indra_statements ( graph )
13230	def get_macros ( tex_source ) : r macros = { } macros . update ( get_def_macros ( tex_source ) ) macros . update ( get_newcommand_macros ( tex_source ) ) return macros
4208	def lpc ( x , N = None ) : m = len ( x ) if N is None : N = m - 1 elif N > m - 1 : x . resize ( N + 1 ) X = fft ( x , 2 ** nextpow2 ( 2. * len ( x ) - 1 ) ) R = real ( ifft ( abs ( X ) ** 2 ) ) R = R / ( m - 1. ) a , e , ref = LEVINSON ( R , N ) return a , e
6000	def pix_to_regular ( self ) : pix_to_regular = [ [ ] for _ in range ( self . pixels ) ] for regular_pixel , pix_pixel in enumerate ( self . regular_to_pix ) : pix_to_regular [ pix_pixel ] . append ( regular_pixel ) return pix_to_regular
1914	def enqueue ( self , state ) : state_id = self . _workspace . save_state ( state ) self . put ( state_id ) self . _publish ( 'did_enqueue_state' , state_id , state ) return state_id
1521	def get_hostname ( ip_addr , cl_args ) : if is_self ( ip_addr ) : return get_self_hostname ( ) cmd = "hostname" ssh_cmd = ssh_remote_execute ( cmd , ip_addr , cl_args ) pid = subprocess . Popen ( ssh_cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) if return_code != 0 : Log . error ( "Failed to get hostname for remote host %s with output:\n%s" % ( ip_addr , output ) ) sys . exit ( - 1 ) return output [ 0 ] . strip ( "\n" )
7342	def set_debug ( ) : logging . basicConfig ( level = logging . WARNING ) peony . logger . setLevel ( logging . DEBUG )
2521	def p_file_type ( self , f_term , predicate ) : try : for _ , _ , ftype in self . graph . triples ( ( f_term , predicate , None ) ) : try : if ftype . endswith ( 'binary' ) : ftype = 'BINARY' elif ftype . endswith ( 'source' ) : ftype = 'SOURCE' elif ftype . endswith ( 'other' ) : ftype = 'OTHER' elif ftype . endswith ( 'archive' ) : ftype = 'ARCHIVE' self . builder . set_file_type ( self . doc , ftype ) except SPDXValueError : self . value_error ( 'FILE_TYPE' , ftype ) except CardinalityError : self . more_than_one_error ( 'file type' )
2565	def async_process ( fn ) : def run ( * args , ** kwargs ) : proc = mp . Process ( target = fn , args = args , kwargs = kwargs ) proc . start ( ) return proc return run
11756	def prop_symbols ( x ) : "Return a list of all propositional symbols in x." if not isinstance ( x , Expr ) : return [ ] elif is_prop_symbol ( x . op ) : return [ x ] else : return list ( set ( symbol for arg in x . args for symbol in prop_symbols ( arg ) ) )
5213	def intraday ( ticker , dt , session = '' , ** kwargs ) -> pd . DataFrame : from xbbg . core import intervals cur_data = bdib ( ticker = ticker , dt = dt , typ = kwargs . get ( 'typ' , 'TRADE' ) ) if cur_data . empty : return pd . DataFrame ( ) fmt = '%H:%M:%S' ss = intervals . SessNA ref = kwargs . get ( 'ref' , None ) exch = pd . Series ( ) if ref is None else const . exch_info ( ticker = ref ) if session : ss = intervals . get_interval ( ticker = kwargs . get ( 'ref' , ticker ) , session = session ) start_time = kwargs . get ( 'start_time' , None ) end_time = kwargs . get ( 'end_time' , None ) if ss != intervals . SessNA : start_time = pd . Timestamp ( ss . start_time ) . strftime ( fmt ) end_time = pd . Timestamp ( ss . end_time ) . strftime ( fmt ) if start_time and end_time : kw = dict ( start_time = start_time , end_time = end_time ) if not exch . empty : cur_tz = cur_data . index . tz res = cur_data . tz_convert ( exch . tz ) . between_time ( ** kw ) if kwargs . get ( 'keep_tz' , False ) : res = res . tz_convert ( cur_tz ) return pd . DataFrame ( res ) return pd . DataFrame ( cur_data . between_time ( ** kw ) ) return cur_data
2576	def _add_input_deps ( self , executor , args , kwargs ) : if executor == 'data_manager' : return args , kwargs inputs = kwargs . get ( 'inputs' , [ ] ) for idx , f in enumerate ( inputs ) : if isinstance ( f , File ) and f . is_remote ( ) : inputs [ idx ] = self . data_manager . stage_in ( f , executor ) for kwarg , f in kwargs . items ( ) : if isinstance ( f , File ) and f . is_remote ( ) : kwargs [ kwarg ] = self . data_manager . stage_in ( f , executor ) newargs = list ( args ) for idx , f in enumerate ( newargs ) : if isinstance ( f , File ) and f . is_remote ( ) : newargs [ idx ] = self . data_manager . stage_in ( f , executor ) return tuple ( newargs ) , kwargs
6710	def shell ( self , gui = 0 , command = '' , dryrun = None , shell_interactive_cmd_str = None ) : from burlap . common import get_hosts_for_site if dryrun is not None : self . dryrun = dryrun r = self . local_renderer if r . genv . SITE != r . genv . default_site : shell_hosts = get_hosts_for_site ( ) if shell_hosts : r . genv . host_string = shell_hosts [ 0 ] r . env . SITE = r . genv . SITE or r . genv . default_site if int ( gui ) : r . env . shell_default_options . append ( '-X' ) if 'host_string' not in self . genv or not self . genv . host_string : if 'available_sites' in self . genv and r . env . SITE not in r . genv . available_sites : raise Exception ( 'No host_string set. Unknown site %s.' % r . env . SITE ) else : raise Exception ( 'No host_string set.' ) if '@' in r . genv . host_string : r . env . shell_host_string = r . genv . host_string else : r . env . shell_host_string = '{user}@{host_string}' if command : r . env . shell_interactive_cmd_str = command else : r . env . shell_interactive_cmd_str = r . format ( shell_interactive_cmd_str or r . env . shell_interactive_cmd ) r . env . shell_default_options_str = ' ' . join ( r . env . shell_default_options ) if self . is_local : self . vprint ( 'Using direct local.' ) cmd = '{shell_interactive_cmd_str}' elif r . genv . key_filename : self . vprint ( 'Using key filename.' ) port = r . env . shell_host_string . split ( ':' ) [ - 1 ] if port . isdigit ( ) : r . env . shell_host_string = r . env . shell_host_string . split ( ':' ) [ 0 ] + ( ' -p %s' % port ) cmd = 'ssh -t {shell_default_options_str} -i {key_filename} {shell_host_string} "{shell_interactive_cmd_str}"' elif r . genv . password : self . vprint ( 'Using password.' ) cmd = 'ssh -t {shell_default_options_str} {shell_host_string} "{shell_interactive_cmd_str}"' else : self . vprint ( 'Using nothing.' ) cmd = 'ssh -t {shell_default_options_str} {shell_host_string} "{shell_interactive_cmd_str}"' r . local ( cmd )
10278	def get_neurommsig_score ( graph : BELGraph , genes : List [ Gene ] , ora_weight : Optional [ float ] = None , hub_weight : Optional [ float ] = None , top_percent : Optional [ float ] = None , topology_weight : Optional [ float ] = None ) -> float : ora_weight = ora_weight or 1.0 hub_weight = hub_weight or 1.0 topology_weight = topology_weight or 1.0 total_weight = ora_weight + hub_weight + topology_weight genes = list ( genes ) ora_score = neurommsig_gene_ora ( graph , genes ) hub_score = neurommsig_hubs ( graph , genes , top_percent = top_percent ) topology_score = neurommsig_topology ( graph , genes ) weighted_sum = ( ora_weight * ora_score + hub_weight * hub_score + topology_weight * topology_score ) return weighted_sum / total_weight
43	def make_sample_her_transitions ( replay_strategy , replay_k , reward_fun ) : if replay_strategy == 'future' : future_p = 1 - ( 1. / ( 1 + replay_k ) ) else : future_p = 0 def _sample_her_transitions ( episode_batch , batch_size_in_transitions ) : T = episode_batch [ 'u' ] . shape [ 1 ] rollout_batch_size = episode_batch [ 'u' ] . shape [ 0 ] batch_size = batch_size_in_transitions episode_idxs = np . random . randint ( 0 , rollout_batch_size , batch_size ) t_samples = np . random . randint ( T , size = batch_size ) transitions = { key : episode_batch [ key ] [ episode_idxs , t_samples ] . copy ( ) for key in episode_batch . keys ( ) } her_indexes = np . where ( np . random . uniform ( size = batch_size ) < future_p ) future_offset = np . random . uniform ( size = batch_size ) * ( T - t_samples ) future_offset = future_offset . astype ( int ) future_t = ( t_samples + 1 + future_offset ) [ her_indexes ] future_ag = episode_batch [ 'ag' ] [ episode_idxs [ her_indexes ] , future_t ] transitions [ 'g' ] [ her_indexes ] = future_ag info = { } for key , value in transitions . items ( ) : if key . startswith ( 'info_' ) : info [ key . replace ( 'info_' , '' ) ] = value reward_params = { k : transitions [ k ] for k in [ 'ag_2' , 'g' ] } reward_params [ 'info' ] = info transitions [ 'r' ] = reward_fun ( ** reward_params ) transitions = { k : transitions [ k ] . reshape ( batch_size , * transitions [ k ] . shape [ 1 : ] ) for k in transitions . keys ( ) } assert ( transitions [ 'u' ] . shape [ 0 ] == batch_size_in_transitions ) return transitions return _sample_her_transitions
9381	def set_sla ( obj , metric , sub_metric , rules ) : if not hasattr ( obj , 'sla_map' ) : return False rules_list = rules . split ( ) for rule in rules_list : if '<' in rule : stat , threshold = rule . split ( '<' ) sla = SLA ( metric , sub_metric , stat , threshold , 'lt' ) elif '>' in rule : stat , threshold = rule . split ( '>' ) sla = SLA ( metric , sub_metric , stat , threshold , 'gt' ) else : if hasattr ( obj , 'logger' ) : obj . logger . error ( 'Unsupported SLA type defined : ' + rule ) sla = None obj . sla_map [ metric ] [ sub_metric ] [ stat ] = sla if hasattr ( obj , 'sla_list' ) : obj . sla_list . append ( sla ) return True
1399	def extract_tmaster ( self , topology ) : tmasterLocation = { "name" : None , "id" : None , "host" : None , "controller_port" : None , "master_port" : None , "stats_port" : None , } if topology . tmaster : tmasterLocation [ "name" ] = topology . tmaster . topology_name tmasterLocation [ "id" ] = topology . tmaster . topology_id tmasterLocation [ "host" ] = topology . tmaster . host tmasterLocation [ "controller_port" ] = topology . tmaster . controller_port tmasterLocation [ "master_port" ] = topology . tmaster . master_port tmasterLocation [ "stats_port" ] = topology . tmaster . stats_port return tmasterLocation
12819	def _file_size ( self , field ) : size = 0 try : handle = open ( self . _files [ field ] , "r" ) size = os . fstat ( handle . fileno ( ) ) . st_size handle . close ( ) except : size = 0 self . _file_lengths [ field ] = size return self . _file_lengths [ field ]
996	def _updateStatsInferEnd ( self , stats , bottomUpNZ , predictedState , colConfidence ) : if not self . collectStats : return stats [ 'nInfersSinceReset' ] += 1 ( numExtra2 , numMissing2 , confidences2 ) = self . _checkPrediction ( patternNZs = [ bottomUpNZ ] , output = predictedState , colConfidence = colConfidence ) predictionScore , positivePredictionScore , negativePredictionScore = ( confidences2 [ 0 ] ) stats [ 'curPredictionScore2' ] = float ( predictionScore ) stats [ 'curFalseNegativeScore' ] = 1.0 - float ( positivePredictionScore ) stats [ 'curFalsePositiveScore' ] = float ( negativePredictionScore ) stats [ 'curMissing' ] = numMissing2 stats [ 'curExtra' ] = numExtra2 if stats [ 'nInfersSinceReset' ] <= self . burnIn : return stats [ 'nPredictions' ] += 1 numExpected = max ( 1.0 , float ( len ( bottomUpNZ ) ) ) stats [ 'totalMissing' ] += numMissing2 stats [ 'totalExtra' ] += numExtra2 stats [ 'pctExtraTotal' ] += 100.0 * numExtra2 / numExpected stats [ 'pctMissingTotal' ] += 100.0 * numMissing2 / numExpected stats [ 'predictionScoreTotal2' ] += float ( predictionScore ) stats [ 'falseNegativeScoreTotal' ] += 1.0 - float ( positivePredictionScore ) stats [ 'falsePositiveScoreTotal' ] += float ( negativePredictionScore ) if self . collectSequenceStats : cc = self . cellConfidence [ 't-1' ] * self . infActiveState [ 't' ] sconf = cc . sum ( axis = 1 ) for c in range ( self . numberOfCols ) : if sconf [ c ] > 0 : cc [ c , : ] /= sconf [ c ] self . _internalStats [ 'confHistogram' ] += cc
9514	def is_complete_orf ( self ) : if len ( self ) % 3 != 0 or len ( self ) < 6 : return False orfs = self . orfs ( ) complete_orf = intervals . Interval ( 0 , len ( self ) - 1 ) for orf in orfs : if orf == complete_orf : return True return False
8800	def run_migrations_offline ( ) : context . configure ( url = neutron_config . database . connection ) with context . begin_transaction ( ) : context . run_migrations ( )
1931	def update ( self , name : str , value = None , default = None , description : str = None ) : if name in self . _vars : description = description or self . _vars [ name ] . description default = default or self . _vars [ name ] . default elif name == 'name' : raise ConfigError ( "'name' is a reserved name for a group." ) v = _Var ( name , description = description , default = default , defined = False ) v . value = value self . _vars [ name ] = v
6082	def deflections_of_galaxies_from_grid ( grid , galaxies ) : if len ( galaxies ) > 0 : deflections = sum ( map ( lambda galaxy : galaxy . deflections_from_grid ( grid ) , galaxies ) ) else : deflections = np . full ( ( grid . shape [ 0 ] , 2 ) , 0.0 ) if isinstance ( grid , grids . SubGrid ) : return np . asarray ( [ grid . regular_data_1d_from_sub_data_1d ( deflections [ : , 0 ] ) , grid . regular_data_1d_from_sub_data_1d ( deflections [ : , 1 ] ) ] ) . T return deflections
12924	def safe_repr ( obj ) : try : obj_repr = repr ( obj ) except : obj_repr = "({0}<{1}> repr error)" . format ( type ( obj ) , id ( obj ) ) return obj_repr
6773	def uninstall_blacklisted ( self ) : from burlap . system import distrib_family blacklisted_packages = self . env . blacklisted_packages if not blacklisted_packages : print ( 'No blacklisted packages.' ) return else : family = distrib_family ( ) if family == DEBIAN : self . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq purge %s' % ' ' . join ( blacklisted_packages ) ) else : raise NotImplementedError ( 'Unknown family: %s' % family )
6663	def get_expiration_date ( self , fn ) : r = self . local_renderer r . env . crt_fn = fn with hide ( 'running' ) : ret = r . local ( 'openssl x509 -noout -in {ssl_crt_fn} -dates' , capture = True ) matches = re . findall ( 'notAfter=(.*?)$' , ret , flags = re . IGNORECASE ) if matches : return dateutil . parser . parse ( matches [ 0 ] )
12642	def get_config_bool ( name ) : cli_config = CLIConfig ( SF_CLI_CONFIG_DIR , SF_CLI_ENV_VAR_PREFIX ) return cli_config . getboolean ( 'servicefabric' , name , False )
4234	def _xml_get ( e , name ) : r = e . find ( name ) if r is not None : return r . text return None
9958	def restore_ipython ( self ) : if not self . is_ipysetup : return shell_class = type ( self . shell ) shell_class . showtraceback = shell_class . default_showtraceback del shell_class . default_showtraceback self . is_ipysetup = False
10429	def getrowcount ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) return len ( object_handle . AXRows )
12156	def list_move_to_back ( l , value = 'other' ) : l = list ( l ) if value in l : l . remove ( value ) l . append ( value ) return l
12226	def on_pref_update ( * args , ** kwargs ) : Preference . update_prefs ( * args , ** kwargs ) Preference . read_prefs ( get_prefs ( ) )
9359	def paragraphs ( quantity = 2 , separator = '\n\n' , wrap_start = '' , wrap_end = '' , html = False , sentences_quantity = 3 , as_list = False ) : if html : wrap_start = '<p>' wrap_end = '</p>' separator = '\n\n' result = [ ] try : for _ in xrange ( 0 , quantity ) : result . append ( wrap_start + sentences ( sentences_quantity ) + wrap_end ) except NameError : for _ in range ( 0 , quantity ) : result . append ( wrap_start + sentences ( sentences_quantity ) + wrap_end ) if as_list : return result else : return separator . join ( result )
5500	def get_tweets ( self , url , limit = None ) : try : tweets = self . cache [ url ] [ "tweets" ] self . mark_updated ( ) return sorted ( tweets , reverse = True ) [ : limit ] except KeyError : return [ ]
3505	def loopless_solution ( model , fluxes = None ) : if fluxes is None : sol = model . optimize ( objective_sense = None ) fluxes = sol . fluxes with model : prob = model . problem loopless_obj_constraint = prob . Constraint ( model . objective . expression , lb = - 1e32 , name = "loopless_obj_constraint" ) model . add_cons_vars ( [ loopless_obj_constraint ] ) _add_cycle_free ( model , fluxes ) solution = model . optimize ( objective_sense = None ) solution . objective_value = loopless_obj_constraint . primal return solution
9690	def start ( self ) : self . receiver = self . Receiver ( self . read , self . write , self . send_lock , self . senders , self . frames_received , callback = self . receive_callback , fcs_nack = self . fcs_nack , ) self . receiver . start ( )
1296	def demo_update ( self ) : fetches = self . demo_optimization_output self . monitored_session . run ( fetches = fetches )
5564	def input ( self ) : delimiters = dict ( zoom = self . init_zoom_levels , bounds = self . init_bounds , process_bounds = self . bounds , effective_bounds = self . effective_bounds ) raw_inputs = { get_hash ( v ) : v for zoom in self . init_zoom_levels if "input" in self . _params_at_zoom [ zoom ] for key , v in _flatten_tree ( self . _params_at_zoom [ zoom ] [ "input" ] ) if v is not None } initalized_inputs = { } for k , v in raw_inputs . items ( ) : if isinstance ( v , str ) : logger . debug ( "load input reader for simple input %s" , v ) try : reader = load_input_reader ( dict ( path = absolute_path ( path = v , base_dir = self . config_dir ) , pyramid = self . process_pyramid , pixelbuffer = self . process_pyramid . pixelbuffer , delimiters = delimiters ) , readonly = self . mode == "readonly" ) except Exception as e : logger . exception ( e ) raise MapcheteDriverError ( "error when loading input %s: %s" % ( v , e ) ) logger . debug ( "input reader for simple input %s is %s" , v , reader ) elif isinstance ( v , dict ) : logger . debug ( "load input reader for abstract input %s" , v ) try : reader = load_input_reader ( dict ( abstract = deepcopy ( v ) , pyramid = self . process_pyramid , pixelbuffer = self . process_pyramid . pixelbuffer , delimiters = delimiters , conf_dir = self . config_dir ) , readonly = self . mode == "readonly" ) except Exception as e : logger . exception ( e ) raise MapcheteDriverError ( "error when loading input %s: %s" % ( v , e ) ) logger . debug ( "input reader for abstract input %s is %s" , v , reader ) else : raise MapcheteConfigError ( "invalid input type %s" , type ( v ) ) reader . bbox ( out_crs = self . process_pyramid . crs ) initalized_inputs [ k ] = reader return initalized_inputs
1053	def extract_stack ( f = None , limit = None ) : if f is None : try : raise ZeroDivisionError except ZeroDivisionError : f = sys . exc_info ( ) [ 2 ] . tb_frame . f_back if limit is None : if hasattr ( sys , 'tracebacklimit' ) : limit = sys . tracebacklimit list = [ ] n = 0 while f is not None and ( limit is None or n < limit ) : lineno = f . f_lineno co = f . f_code filename = co . co_filename name = co . co_name linecache . checkcache ( filename ) line = linecache . getline ( filename , lineno , f . f_globals ) if line : line = line . strip ( ) else : line = None list . append ( ( filename , lineno , name , line ) ) f = f . f_back n = n + 1 list . reverse ( ) return list
10509	def stoplog ( self ) : if self . _file_logger : self . logger . removeHandler ( _file_logger ) self . _file_logger = None return 1
9865	def currency ( self ) : try : current_subscription = self . info [ "viewer" ] [ "home" ] [ "currentSubscription" ] return current_subscription [ "priceInfo" ] [ "current" ] [ "currency" ] except ( KeyError , TypeError , IndexError ) : _LOGGER . error ( "Could not find currency." ) return ""
9328	def get ( self , url , headers = None , params = None ) : merged_headers = self . _merge_headers ( headers ) if "Accept" not in merged_headers : merged_headers [ "Accept" ] = MEDIA_TYPE_TAXII_V20 accept = merged_headers [ "Accept" ] resp = self . session . get ( url , headers = merged_headers , params = params ) resp . raise_for_status ( ) content_type = resp . headers [ "Content-Type" ] if not self . valid_content_type ( content_type = content_type , accept = accept ) : msg = "Unexpected Response. Got Content-Type: '{}' for Accept: '{}'" raise TAXIIServiceException ( msg . format ( content_type , accept ) ) return _to_json ( resp )
1577	def make_shell_logfiles_url ( host , shell_port , _ , instance_id = None ) : if not shell_port : return None if not instance_id : return "http://%s:%d/browse/log-files" % ( host , shell_port ) else : return "http://%s:%d/file/log-files/%s.log.0" % ( host , shell_port , instance_id )
2755	def get_ssh_key ( self , ssh_key_id ) : return SSHKey . get_object ( api_token = self . token , ssh_key_id = ssh_key_id )
13018	def configure ( self , argv = None ) : self . _setupOptions ( ) self . _parseOptions ( argv ) self . _setupLogging ( ) self . _setupModel ( ) self . dbsession . commit ( ) return self
5335	def get_params ( ) : parser = get_params_parser ( ) args = parser . parse_args ( ) if not args . raw and not args . enrich and not args . identities and not args . panels : print ( "No tasks enabled" ) sys . exit ( 1 ) return args
11139	def get_stats ( self ) : if self . __path is None : return 0 , 0 nfiles = 0 ndirs = 0 for fdict in self . get_repository_state ( ) : fdname = list ( fdict ) [ 0 ] if fdname == '' : continue if fdict [ fdname ] . get ( 'pyrepfileinfo' , False ) : nfiles += 1 elif fdict [ fdname ] . get ( 'pyrepdirinfo' , False ) : ndirs += 1 else : raise Exception ( 'Not sure what to do next. Please report issue' ) return ndirs , nfiles
2709	def limit_sentences ( path , word_limit = 100 ) : word_count = 0 if isinstance ( path , str ) : path = json_iter ( path ) for meta in path : if not isinstance ( meta , SummarySent ) : p = SummarySent ( ** meta ) else : p = meta sent_text = p . text . strip ( ) . split ( " " ) sent_len = len ( sent_text ) if ( word_count + sent_len ) > word_limit : break else : word_count += sent_len yield sent_text , p . idx
4992	def transmit_content_metadata ( username , channel_code , channel_pk ) : start = time . time ( ) api_user = User . objects . get ( username = username ) integrated_channel = INTEGRATED_CHANNEL_CHOICES [ channel_code ] . objects . get ( pk = channel_pk ) LOGGER . info ( 'Transmitting content metadata to integrated channel using configuration: [%s]' , integrated_channel ) try : integrated_channel . transmit_content_metadata ( api_user ) except Exception : LOGGER . exception ( 'Transmission of content metadata failed for user [%s] and for integrated ' 'channel with code [%s] and id [%s].' , username , channel_code , channel_pk ) duration = time . time ( ) - start LOGGER . info ( 'Content metadata transmission task for integrated channel configuration [%s] took [%s] seconds' , integrated_channel , duration )
9364	def domain_name ( ) : result = random . choice ( get_dictionary ( 'company_names' ) ) . strip ( ) result += '.' + top_level_domain ( ) return result . lower ( )
2205	def userhome ( username = None ) : if username is None : if 'HOME' in os . environ : userhome_dpath = os . environ [ 'HOME' ] else : if sys . platform . startswith ( 'win32' ) : if 'USERPROFILE' in os . environ : userhome_dpath = os . environ [ 'USERPROFILE' ] elif 'HOMEPATH' in os . environ : drive = os . environ . get ( 'HOMEDRIVE' , '' ) userhome_dpath = join ( drive , os . environ [ 'HOMEPATH' ] ) else : raise OSError ( "Cannot determine the user's home directory" ) else : import pwd userhome_dpath = pwd . getpwuid ( os . getuid ( ) ) . pw_dir else : if sys . platform . startswith ( 'win32' ) : c_users = dirname ( userhome ( ) ) userhome_dpath = join ( c_users , username ) if not exists ( userhome_dpath ) : raise KeyError ( 'Unknown user: {}' . format ( username ) ) else : import pwd try : pwent = pwd . getpwnam ( username ) except KeyError : raise KeyError ( 'Unknown user: {}' . format ( username ) ) userhome_dpath = pwent . pw_dir return userhome_dpath
1035	def encodestring ( s ) : pieces = [ ] for i in range ( 0 , len ( s ) , MAXBINSIZE ) : chunk = s [ i : i + MAXBINSIZE ] pieces . append ( binascii . b2a_base64 ( chunk ) ) return "" . join ( pieces )
3199	def delete ( self , workflow_id , email_id ) : self . workflow_id = workflow_id self . email_id = email_id return self . _mc_client . _delete ( url = self . _build_path ( workflow_id , 'emails' , email_id ) )
7274	def play_pause ( self ) : self . _player_interface . PlayPause ( ) self . _is_playing = not self . _is_playing if self . _is_playing : self . playEvent ( self ) else : self . pauseEvent ( self )
10250	def is_node_highlighted ( graph : BELGraph , node : BaseEntity ) -> bool : return NODE_HIGHLIGHT in graph . node [ node ]
9776	def outputs ( ctx ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) try : PolyaxonClient ( ) . job . download_outputs ( user , project_name , _job ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not download outputs for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( 'Files downloaded.' )
8532	def can_diff ( msg_a , msg_b ) : if msg_a . method != msg_b . method : return False , 'method name of messages do not match' if len ( msg_a . args ) != len ( msg_b . args ) or not msg_a . args . is_isomorphic_to ( msg_b . args ) : return False , 'argument signature of methods do not match' return True , None
3196	def delete_permanent ( self , list_id , subscriber_hash ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash return self . _mc_client . _post ( url = self . _build_path ( list_id , 'members' , subscriber_hash , 'actions' , 'delete-permanent' ) )
11428	def record_strip_empty_fields ( rec , tag = None ) : if tag is None : tags = rec . keys ( ) for tag in tags : record_strip_empty_fields ( rec , tag ) elif tag in rec : if tag [ : 2 ] == '00' : if len ( rec [ tag ] ) == 0 or not rec [ tag ] [ 0 ] [ 3 ] : del rec [ tag ] else : fields = [ ] for field in rec [ tag ] : subfields = [ ] for subfield in field [ 0 ] : if subfield [ 1 ] : subfield = ( subfield [ 0 ] , subfield [ 1 ] . strip ( ) ) subfields . append ( subfield ) if len ( subfields ) > 0 : new_field = create_field ( subfields , field [ 1 ] , field [ 2 ] , field [ 3 ] ) fields . append ( new_field ) if len ( fields ) > 0 : rec [ tag ] = fields else : del rec [ tag ]
11022	def get_node ( self , string_key ) : pos = self . get_node_pos ( string_key ) if pos is None : return None return self . ring [ self . _sorted_keys [ pos ] ]
2253	def unique ( items , key = None ) : seen = set ( ) if key is None : for item in items : if item not in seen : seen . add ( item ) yield item else : for item in items : norm = key ( item ) if norm not in seen : seen . add ( norm ) yield item
9223	def away_from_zero_round ( value , ndigits = 0 ) : if sys . version_info [ 0 ] >= 3 : p = 10 ** ndigits return float ( math . floor ( ( value * p ) + math . copysign ( 0.5 , value ) ) ) / p else : return round ( value , ndigits )
11110	def walk_directory_directories_relative_path ( self , relativePath = "" ) : errorMessage = "" relativePath = os . path . normpath ( relativePath ) dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) assert dirInfoDict is not None , errorMessage for dname in dict . __getitem__ ( dirInfoDict , "directories" ) : yield os . path . join ( relativePath , dname )
10031	def execute ( helper , config , args ) : env = parse_env_config ( config , args . environment ) option_settings = env . get ( 'option_settings' , { } ) settings = parse_option_settings ( option_settings ) for setting in settings : out ( str ( setting ) )
1801	def LAHF ( cpu ) : used_regs = ( cpu . SF , cpu . ZF , cpu . AF , cpu . PF , cpu . CF ) is_expression = any ( issymbolic ( x ) for x in used_regs ) def make_flag ( val , offset ) : if is_expression : return Operators . ITEBV ( 8 , val , BitVecConstant ( 8 , 1 << offset ) , BitVecConstant ( 8 , 0 ) ) else : return val << offset cpu . AH = ( make_flag ( cpu . SF , 7 ) | make_flag ( cpu . ZF , 6 ) | make_flag ( 0 , 5 ) | make_flag ( cpu . AF , 4 ) | make_flag ( 0 , 3 ) | make_flag ( cpu . PF , 2 ) | make_flag ( 1 , 1 ) | make_flag ( cpu . CF , 0 ) )
7962	def _close ( self ) : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" ) if self . _socket is None : return try : self . _socket . shutdown ( socket . SHUT_RDWR ) except socket . error : pass self . _socket . close ( ) self . _socket = None self . _write_queue . clear ( ) self . _write_queue_cond . notify ( )
8546	def delete_datacenter ( self , datacenter_id ) : response = self . _perform_request ( url = '/datacenters/%s' % ( datacenter_id ) , method = 'DELETE' ) return response
2448	def set_pkg_file_name ( self , doc , name ) : self . assert_package_exists ( ) if not self . package_file_name_set : self . package_file_name_set = True doc . package . file_name = name return True else : raise CardinalityError ( 'Package::FileName' )
3071	def _get_flow_for_token ( csrf_token ) : flow_pickle = session . pop ( _FLOW_KEY . format ( csrf_token ) , None ) if flow_pickle is None : return None else : return pickle . loads ( flow_pickle )
11469	def rmdir ( self , foldername ) : current_folder = self . _ftp . pwd ( ) try : self . cd ( foldername ) except error_perm : print ( '550 Delete operation failed folder %s ' 'does not exist!' % ( foldername , ) ) else : self . cd ( current_folder ) try : self . _ftp . rmd ( foldername ) except error_perm : self . cd ( foldername ) contents = self . ls ( ) map ( self . _ftp . delete , contents [ 0 ] ) map ( self . rmdir , contents [ 1 ] ) self . cd ( current_folder ) self . _ftp . rmd ( foldername )
2972	def from_dict ( name , values ) : count = 1 count_value = values . get ( 'count' , 1 ) if isinstance ( count_value , int ) : count = max ( count_value , 1 ) def with_index ( name , idx ) : if name and idx : return '%s_%d' % ( name , idx ) return name def get_instance ( n , idx = None ) : return BlockadeContainerConfig ( with_index ( n , idx ) , values [ 'image' ] , command = values . get ( 'command' ) , links = values . get ( 'links' ) , volumes = values . get ( 'volumes' ) , publish_ports = values . get ( 'ports' ) , expose_ports = values . get ( 'expose' ) , environment = values . get ( 'environment' ) , hostname = values . get ( 'hostname' ) , dns = values . get ( 'dns' ) , start_delay = values . get ( 'start_delay' , 0 ) , neutral = values . get ( 'neutral' , False ) , holy = values . get ( 'holy' , False ) , container_name = with_index ( values . get ( 'container_name' ) , idx ) , cap_add = values . get ( 'cap_add' ) ) if count == 1 : yield get_instance ( name ) else : for idx in range ( 1 , count + 1 ) : yield get_instance ( name , idx )
338	def _GetNextLogCountPerToken ( token ) : global _log_counter_per_token _log_counter_per_token [ token ] = 1 + _log_counter_per_token . get ( token , - 1 ) return _log_counter_per_token [ token ]
5859	def __prune_search_template ( self , extract_as_keys , search_template ) : data = { "extract_as_keys" : extract_as_keys , "search_template" : search_template } failure_message = "Failed to prune a search template" return self . _get_success_json ( self . _post_json ( 'v1/search_templates/prune-to-extract-as' , data , failure_message = failure_message ) ) [ 'data' ]
12593	def execute_reliabledictionary ( client , application_name , service_name , input_file ) : cluster = Cluster . from_sfclient ( client ) service = cluster . get_application ( application_name ) . get_service ( service_name ) with open ( input_file ) as json_file : json_data = json . load ( json_file ) service . execute ( json_data ) return
12583	def _safe_cache ( memory , func , ** kwargs ) : cachedir = memory . cachedir if cachedir is None or cachedir in __CACHE_CHECKED : return memory . cache ( func , ** kwargs ) version_file = os . path . join ( cachedir , 'module_versions.json' ) versions = dict ( ) if os . path . exists ( version_file ) : with open ( version_file , 'r' ) as _version_file : versions = json . load ( _version_file ) modules = ( nibabel , ) my_versions = dict ( ( m . __name__ , LooseVersion ( m . __version__ ) . version [ : 2 ] ) for m in modules ) commons = set ( versions . keys ( ) ) . intersection ( set ( my_versions . keys ( ) ) ) collisions = [ m for m in commons if versions [ m ] != my_versions [ m ] ] if len ( collisions ) > 0 : if nilearn . CHECK_CACHE_VERSION : warnings . warn ( "Incompatible cache in %s: " "different version of nibabel. Deleting " "the cache. Put nilearn.CHECK_CACHE_VERSION " "to false to avoid this behavior." % cachedir ) try : tmp_dir = ( os . path . split ( cachedir ) [ : - 1 ] + ( 'old_%i' % os . getpid ( ) , ) ) tmp_dir = os . path . join ( * tmp_dir ) os . rename ( cachedir , tmp_dir ) shutil . rmtree ( tmp_dir ) except OSError : pass try : os . makedirs ( cachedir ) except OSError : pass else : warnings . warn ( "Incompatible cache in %s: " "old version of nibabel." % cachedir ) if versions != my_versions : with open ( version_file , 'w' ) as _version_file : json . dump ( my_versions , _version_file ) __CACHE_CHECKED [ cachedir ] = True return memory . cache ( func , ** kwargs )
11458	def keep_only_fields ( self ) : for tag in self . record . keys ( ) : if tag not in self . fields_list : record_delete_fields ( self . record , tag )
11322	def generate_dirlist_html ( FS , filepath ) : yield '<table class="dirlist">' if filepath == '/' : filepath = '' for name in FS . listdir ( filepath ) : full_path = pathjoin ( filepath , name ) if FS . isdir ( full_path ) : full_path = full_path + '/' yield u'<tr><td><a href="{0}">{0}</a></td></tr>' . format ( cgi . escape ( full_path ) ) yield '</table>'
1594	def choose_tasks ( self , stream_id , values ) : if stream_id not in self . targets : return [ ] ret = [ ] for target in self . targets [ stream_id ] : ret . extend ( target . choose_tasks ( values ) ) return ret
10200	def run ( self ) : return elasticsearch . helpers . bulk ( self . client , self . actionsiter ( ) , stats_only = True , chunk_size = 50 )
8902	def _multiple_self_ref_fk_check ( class_model ) : self_fk = [ ] for f in class_model . _meta . concrete_fields : if f . related_model in self_fk : return True if f . related_model == class_model : self_fk . append ( class_model ) return False
1335	def best_other_class ( logits , exclude ) : other_logits = logits - onehot_like ( logits , exclude , value = np . inf ) return np . argmax ( other_logits )
1490	def tail ( filename , n ) : size = os . path . getsize ( filename ) with open ( filename , "rb" ) as f : fm = mmap . mmap ( f . fileno ( ) , 0 , mmap . MAP_SHARED , mmap . PROT_READ ) try : for i in xrange ( size - 1 , - 1 , - 1 ) : if fm [ i ] == '\n' : n -= 1 if n == - 1 : break return fm [ i + 1 if i else 0 : ] . splitlines ( ) finally : fm . close ( )
7313	def process_request ( self , request ) : if not request : return if not db_loaded : load_db ( ) tz = request . session . get ( 'django_timezone' ) if not tz : tz = timezone . get_default_timezone ( ) client_ip = get_ip_address_from_request ( request ) ip_addrs = client_ip . split ( ',' ) for ip in ip_addrs : if is_valid_ip ( ip ) and not is_local_ip ( ip ) : if ':' in ip : tz = db_v6 . time_zone_by_addr ( ip ) break else : tz = db . time_zone_by_addr ( ip ) break if tz : timezone . activate ( tz ) request . session [ 'django_timezone' ] = str ( tz ) if getattr ( settings , 'AUTH_USER_MODEL' , None ) and getattr ( request , 'user' , None ) : detected_timezone . send ( sender = get_user_model ( ) , instance = request . user , timezone = tz ) else : timezone . deactivate ( )
8225	def _key_pressed ( self , key , keycode ) : self . _namespace [ 'key' ] = key self . _namespace [ 'keycode' ] = keycode self . _namespace [ 'keydown' ] = True
2488	def create_disjunction_node ( self , disjunction ) : node = BNode ( ) type_triple = ( node , RDF . type , self . spdx_namespace . DisjunctiveLicenseSet ) self . graph . add ( type_triple ) licenses = self . licenses_from_tree ( disjunction ) for lic in licenses : member_triple = ( node , self . spdx_namespace . member , lic ) self . graph . add ( member_triple ) return node
4007	def _compile_docker_commands ( app_name , assembled_specs , port_spec ) : app_spec = assembled_specs [ 'apps' ] [ app_name ] commands = [ 'set -e' ] commands += _lib_install_commands_for_app ( app_name , assembled_specs ) if app_spec [ 'mount' ] : commands . append ( "cd {}" . format ( container_code_path ( app_spec ) ) ) commands . append ( "export PATH=$PATH:{}" . format ( container_code_path ( app_spec ) ) ) commands += _copy_assets_commands_for_app ( app_spec , assembled_specs ) commands += _get_once_commands ( app_spec , port_spec ) commands += _get_always_commands ( app_spec ) return commands
5111	def _get_queues ( g , queues , edge , edge_type ) : INT = numbers . Integral if isinstance ( queues , INT ) : queues = [ queues ] elif queues is None : if edge is not None : if isinstance ( edge , tuple ) : if isinstance ( edge [ 0 ] , INT ) and isinstance ( edge [ 1 ] , INT ) : queues = [ g . edge_index [ edge ] ] elif isinstance ( edge [ 0 ] , collections . Iterable ) : if np . array ( [ len ( e ) == 2 for e in edge ] ) . all ( ) : queues = [ g . edge_index [ e ] for e in edge ] else : queues = [ g . edge_index [ edge ] ] elif edge_type is not None : if isinstance ( edge_type , collections . Iterable ) : edge_type = set ( edge_type ) else : edge_type = set ( [ edge_type ] ) tmp = [ ] for e in g . edges ( ) : if g . ep ( e , 'edge_type' ) in edge_type : tmp . append ( g . edge_index [ e ] ) queues = np . array ( tmp , int ) if queues is None : queues = range ( g . number_of_edges ( ) ) return queues
7741	def _configure_io_handler ( self , handler ) : if self . check_events ( ) : return if handler in self . _unprepared_handlers : old_fileno = self . _unprepared_handlers [ handler ] prepared = self . _prepare_io_handler ( handler ) else : old_fileno = None prepared = True fileno = handler . fileno ( ) if old_fileno is not None and fileno != old_fileno : tag = self . _io_sources . pop ( handler , None ) if tag is not None : glib . source_remove ( tag ) if not prepared : self . _unprepared_handlers [ handler ] = fileno if fileno is None : logger . debug ( " {0!r}.fileno() is None, not polling" . format ( handler ) ) return events = 0 if handler . is_readable ( ) : logger . debug ( " {0!r} readable" . format ( handler ) ) events |= glib . IO_IN | glib . IO_ERR if handler . is_writable ( ) : logger . debug ( " {0!r} writable" . format ( handler ) ) events |= glib . IO_OUT | glib . IO_HUP | glib . IO_ERR if events : logger . debug ( " registering {0!r} handler fileno {1} for" " events {2}" . format ( handler , fileno , events ) ) glib . io_add_watch ( fileno , events , self . _io_callback , handler )
819	def updateRow ( self , row , distribution ) : self . grow ( row + 1 , len ( distribution ) ) self . hist_ . axby ( row , 1 , 1 , distribution ) self . rowSums_ [ row ] += distribution . sum ( ) self . colSums_ += distribution self . hack_ = None
5320	def find_ports ( device ) : bus_id = device . bus dev_id = device . address for dirent in os . listdir ( USB_SYS_PREFIX ) : matches = re . match ( USB_PORTS_STR + '$' , dirent ) if matches : bus_str = readattr ( dirent , 'busnum' ) if bus_str : busnum = float ( bus_str ) else : busnum = None dev_str = readattr ( dirent , 'devnum' ) if dev_str : devnum = float ( dev_str ) else : devnum = None if busnum == bus_id and devnum == dev_id : return str ( matches . groups ( ) [ 1 ] )
7059	def s3_delete_file ( bucket , filename , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 's3' ) try : resp = client . delete_object ( Bucket = bucket , Key = filename ) if not resp : LOGERROR ( 'could not delete file %s from bucket %s' % ( filename , bucket ) ) else : return resp [ 'DeleteMarker' ] except Exception as e : LOGEXCEPTION ( 'could not delete file %s from bucket %s' % ( filename , bucket ) ) if raiseonfail : raise return None
5114	def clear_data ( self , queues = None , edge = None , edge_type = None ) : queues = _get_queues ( self . g , queues , edge , edge_type ) for k in queues : self . edge2queue [ k ] . data = { }
10385	def get_walks_exhaustive ( graph , node , length ) : if 0 == length : return ( node , ) , return tuple ( ( node , key ) + path for neighbor in graph . edge [ node ] for path in get_walks_exhaustive ( graph , neighbor , length - 1 ) if node not in path for key in graph . edge [ node ] [ neighbor ] )
7153	def many ( prompt , * args , ** kwargs ) : def get_options ( options , chosen ) : return [ options [ i ] for i , c in enumerate ( chosen ) if c ] def get_verbose_options ( verbose_options , chosen ) : no , yes = ' ' , '' if sys . version_info < ( 3 , 3 ) : no , yes = ' ' , '@' opts = [ '{} {}' . format ( yes if c else no , verbose_options [ i ] ) for i , c in enumerate ( chosen ) ] return opts + [ '{}{}' . format ( ' ' , kwargs . get ( 'done' , 'done...' ) ) ] options , verbose_options = prepare_options ( args ) chosen = [ False ] * len ( options ) index = kwargs . get ( 'idx' , 0 ) default = kwargs . get ( 'default' , None ) if isinstance ( default , list ) : for idx in default : chosen [ idx ] = True if isinstance ( default , int ) : chosen [ default ] = True while True : try : index = one ( prompt , * get_verbose_options ( verbose_options , chosen ) , return_index = True , idx = index ) except QuestionnaireGoBack : if any ( chosen ) : raise QuestionnaireGoBack ( 0 ) else : raise QuestionnaireGoBack if index == len ( options ) : return get_options ( options , chosen ) chosen [ index ] = not chosen [ index ]
6057	def resized_array_2d_from_array_2d_and_resized_shape ( array_2d , resized_shape , origin = ( - 1 , - 1 ) , pad_value = 0.0 ) : y_is_even = int ( array_2d . shape [ 0 ] ) % 2 == 0 x_is_even = int ( array_2d . shape [ 1 ] ) % 2 == 0 if origin is ( - 1 , - 1 ) : if y_is_even : y_centre = int ( array_2d . shape [ 0 ] / 2 ) elif not y_is_even : y_centre = int ( array_2d . shape [ 0 ] / 2 ) if x_is_even : x_centre = int ( array_2d . shape [ 1 ] / 2 ) elif not x_is_even : x_centre = int ( array_2d . shape [ 1 ] / 2 ) origin = ( y_centre , x_centre ) resized_array = np . zeros ( shape = resized_shape ) if y_is_even : y_min = origin [ 0 ] - int ( resized_shape [ 0 ] / 2 ) y_max = origin [ 0 ] + int ( ( resized_shape [ 0 ] / 2 ) ) + 1 elif not y_is_even : y_min = origin [ 0 ] - int ( resized_shape [ 0 ] / 2 ) y_max = origin [ 0 ] + int ( ( resized_shape [ 0 ] / 2 ) ) + 1 if x_is_even : x_min = origin [ 1 ] - int ( resized_shape [ 1 ] / 2 ) x_max = origin [ 1 ] + int ( ( resized_shape [ 1 ] / 2 ) ) + 1 elif not x_is_even : x_min = origin [ 1 ] - int ( resized_shape [ 1 ] / 2 ) x_max = origin [ 1 ] + int ( ( resized_shape [ 1 ] / 2 ) ) + 1 for y_resized , y in enumerate ( range ( y_min , y_max ) ) : for x_resized , x in enumerate ( range ( x_min , x_max ) ) : if y >= 0 and y < array_2d . shape [ 0 ] and x >= 0 and x < array_2d . shape [ 1 ] : if y_resized >= 0 and y_resized < resized_shape [ 0 ] and x_resized >= 0 and x_resized < resized_shape [ 1 ] : resized_array [ y_resized , x_resized ] = array_2d [ y , x ] else : if y_resized >= 0 and y_resized < resized_shape [ 0 ] and x_resized >= 0 and x_resized < resized_shape [ 1 ] : resized_array [ y_resized , x_resized ] = pad_value return resized_array
10874	def get_hsym_asym ( rho , z , get_hdet = False , include_K3_det = True , ** kwargs ) : K1 , Kprefactor = get_K ( rho , z , K = 1 , get_hdet = get_hdet , Kprefactor = None , return_Kprefactor = True , ** kwargs ) K2 = get_K ( rho , z , K = 2 , get_hdet = get_hdet , Kprefactor = Kprefactor , return_Kprefactor = False , ** kwargs ) if get_hdet and not include_K3_det : K3 = 0 * K1 else : K3 = get_K ( rho , z , K = 3 , get_hdet = get_hdet , Kprefactor = Kprefactor , return_Kprefactor = False , ** kwargs ) hsym = K1 * K1 . conj ( ) + K2 * K2 . conj ( ) + 0.5 * ( K3 * K3 . conj ( ) ) hasym = K1 * K2 . conj ( ) + K2 * K1 . conj ( ) + 0.5 * ( K3 * K3 . conj ( ) ) return hsym . real , hasym . real
5176	def fact ( self , name ) : facts = self . facts ( name = name ) return next ( fact for fact in facts )
6969	def _old_epd_diffmags ( coeff , fsv , fdv , fkv , xcc , ycc , bgv , bge , mag ) : return - ( coeff [ 0 ] * fsv ** 2. + coeff [ 1 ] * fsv + coeff [ 2 ] * fdv ** 2. + coeff [ 3 ] * fdv + coeff [ 4 ] * fkv ** 2. + coeff [ 5 ] * fkv + coeff [ 6 ] + coeff [ 7 ] * fsv * fdv + coeff [ 8 ] * fsv * fkv + coeff [ 9 ] * fdv * fkv + coeff [ 10 ] * np . sin ( 2 * np . pi * xcc ) + coeff [ 11 ] * np . cos ( 2 * np . pi * xcc ) + coeff [ 12 ] * np . sin ( 2 * np . pi * ycc ) + coeff [ 13 ] * np . cos ( 2 * np . pi * ycc ) + coeff [ 14 ] * np . sin ( 4 * np . pi * xcc ) + coeff [ 15 ] * np . cos ( 4 * np . pi * xcc ) + coeff [ 16 ] * np . sin ( 4 * np . pi * ycc ) + coeff [ 17 ] * np . cos ( 4 * np . pi * ycc ) + coeff [ 18 ] * bgv + coeff [ 19 ] * bge - mag )
9322	def refresh ( self , accept = MEDIA_TYPE_TAXII_V20 ) : self . refresh_information ( accept ) self . refresh_collections ( accept )
5674	def get_main_database_path ( self ) : cur = self . conn . cursor ( ) cur . execute ( "PRAGMA database_list" ) rows = cur . fetchall ( ) for row in rows : if row [ 1 ] == str ( "main" ) : return row [ 2 ]
1872	def RDTSC ( cpu ) : val = cpu . icount cpu . RAX = val & 0xffffffff cpu . RDX = ( val >> 32 ) & 0xffffffff
3788	def set_user_method ( self , user_methods , forced = False ) : r if isinstance ( user_methods , str ) : user_methods = [ user_methods ] self . user_methods = user_methods self . forced = forced if set ( self . user_methods ) . difference ( self . all_methods ) : raise Exception ( "One of the given methods is not available for this mixture" ) if not self . user_methods and self . forced : raise Exception ( 'Only user specified methods are considered when forced is True, but no methods were provided' ) self . method = None self . sorted_valid_methods = [ ] self . TP_zs_ws_cached = ( None , None , None , None )
9253	def get_string_for_issue ( self , issue ) : encapsulated_title = self . encapsulate_string ( issue [ 'title' ] ) try : title_with_number = u"{0} [\\#{1}]({2})" . format ( encapsulated_title , issue [ "number" ] , issue [ "html_url" ] ) except UnicodeEncodeError : title_with_number = "ERROR ERROR ERROR: #{0} {1}" . format ( issue [ "number" ] , issue [ 'title' ] ) print ( title_with_number , '\n' , issue [ "html_url" ] ) return self . issue_line_with_user ( title_with_number , issue )
7695	def timeout_handler ( interval , recurring = None ) : def decorator ( func ) : func . _pyxmpp_timeout = interval func . _pyxmpp_recurring = recurring return func return decorator
9374	def download_file ( url ) : try : ( local_file , headers ) = urllib . urlretrieve ( url ) except : sys . exit ( "ERROR: Problem downloading config file. Please check the URL (" + url + "). Exiting..." ) return local_file
4032	def _randone ( d , limit = 20 , grouprefs = None ) : if grouprefs is None : grouprefs = { } ret = '' for i in d : if i [ 0 ] == sre_parse . IN : ret += choice ( _in ( i [ 1 ] ) ) elif i [ 0 ] == sre_parse . LITERAL : ret += unichr ( i [ 1 ] ) elif i [ 0 ] == sre_parse . CATEGORY : ret += choice ( CATEGORIES . get ( i [ 1 ] , [ '' ] ) ) elif i [ 0 ] == sre_parse . ANY : ret += choice ( CATEGORIES [ 'category_any' ] ) elif i [ 0 ] == sre_parse . MAX_REPEAT or i [ 0 ] == sre_parse . MIN_REPEAT : if i [ 1 ] [ 1 ] + 1 - i [ 1 ] [ 0 ] >= limit : min , max = i [ 1 ] [ 0 ] , i [ 1 ] [ 0 ] + limit - 1 else : min , max = i [ 1 ] [ 0 ] , i [ 1 ] [ 1 ] for _ in range ( randint ( min , max ) ) : ret += _randone ( list ( i [ 1 ] [ 2 ] ) , limit , grouprefs ) elif i [ 0 ] == sre_parse . BRANCH : ret += _randone ( choice ( i [ 1 ] [ 1 ] ) , limit , grouprefs ) elif i [ 0 ] == sre_parse . SUBPATTERN or i [ 0 ] == sre_parse . ASSERT : subexpr = i [ 1 ] [ 1 ] if IS_PY36_OR_GREATER and i [ 0 ] == sre_parse . SUBPATTERN : subexpr = i [ 1 ] [ 3 ] subp = _randone ( subexpr , limit , grouprefs ) if i [ 1 ] [ 0 ] : grouprefs [ i [ 1 ] [ 0 ] ] = subp ret += subp elif i [ 0 ] == sre_parse . AT : continue elif i [ 0 ] == sre_parse . NOT_LITERAL : c = list ( CATEGORIES [ 'category_any' ] ) if unichr ( i [ 1 ] ) in c : c . remove ( unichr ( i [ 1 ] ) ) ret += choice ( c ) elif i [ 0 ] == sre_parse . GROUPREF : ret += grouprefs [ i [ 1 ] ] elif i [ 0 ] == sre_parse . ASSERT_NOT : pass else : print ( '[!] cannot handle expression "%s"' % str ( i ) ) return ret
11397	def add_cms_link ( self ) : intnote = record_get_field_values ( self . record , '690' , filter_subfield_code = "a" , filter_subfield_value = 'INTNOTE' ) if intnote : val_088 = record_get_field_values ( self . record , tag = '088' , filter_subfield_code = "a" ) for val in val_088 : if 'CMS' in val : url = ( 'http://weblib.cern.ch/abstract?CERN-CMS' + val . split ( 'CMS' , 1 ) [ - 1 ] ) record_add_field ( self . record , tag = '856' , ind1 = '4' , subfields = [ ( 'u' , url ) ] )
9467	def conference_play ( self , call_params ) : path = '/' + self . api_version + '/ConferencePlay/' method = 'POST' return self . request ( path , method , call_params )
9566	def pack_into ( fmt , buf , offset , * args , ** kwargs ) : return CompiledFormat ( fmt ) . pack_into ( buf , offset , * args , ** kwargs )
9929	def authenticate ( username , password , service = 'login' , encoding = 'utf-8' , resetcred = True ) : if sys . version_info >= ( 3 , ) : if isinstance ( username , str ) : username = username . encode ( encoding ) if isinstance ( password , str ) : password = password . encode ( encoding ) if isinstance ( service , str ) : service = service . encode ( encoding ) @ conv_func def my_conv ( n_messages , messages , p_response , app_data ) : addr = calloc ( n_messages , sizeof ( PamResponse ) ) p_response [ 0 ] = cast ( addr , POINTER ( PamResponse ) ) for i in range ( n_messages ) : if messages [ i ] . contents . msg_style == PAM_PROMPT_ECHO_OFF : pw_copy = strdup ( password ) p_response . contents [ i ] . resp = cast ( pw_copy , c_char_p ) p_response . contents [ i ] . resp_retcode = 0 return 0 handle = PamHandle ( ) conv = PamConv ( my_conv , 0 ) retval = pam_start ( service , username , byref ( conv ) , byref ( handle ) ) if retval != 0 : return False retval = pam_authenticate ( handle , 0 ) auth_success = ( retval == 0 ) if auth_success and resetcred : retval = pam_setcred ( handle , PAM_REINITIALIZE_CRED ) pam_end ( handle , retval ) return auth_success
7646	def note_hz_to_midi ( annotation ) : annotation . namespace = 'note_midi' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = 12 * ( np . log2 ( obs . value ) - np . log2 ( 440.0 ) ) + 69 ) return annotation
2948	def evaluate ( self , task , expression ) : if isinstance ( expression , Operator ) : return expression . _matches ( task ) else : return self . _eval ( task , expression , ** task . data )
4896	def get_enterprise_sso_uid ( self , obj ) : enterprise_learner = EnterpriseCustomerUser . objects . filter ( user_id = obj . id ) . first ( ) return enterprise_learner and enterprise_learner . get_remote_id ( )
5642	def compute_pseudo_connections ( transit_connections , start_time_dep , end_time_dep , transfer_margin , walk_network , walk_speed ) : pseudo_connection_set = set ( ) for c in transit_connections : if start_time_dep <= c . departure_time <= end_time_dep : walk_arr_stop = c . departure_stop walk_arr_time = c . departure_time - transfer_margin for _ , walk_dep_stop , data in walk_network . edges ( nbunch = [ walk_arr_stop ] , data = True ) : walk_dep_time = walk_arr_time - data [ 'd_walk' ] / float ( walk_speed ) if walk_dep_time > end_time_dep or walk_dep_time < start_time_dep : continue pseudo_connection = Connection ( walk_dep_stop , walk_arr_stop , walk_dep_time , walk_arr_time , Connection . WALK_TRIP_ID , Connection . WALK_SEQ , is_walk = True ) pseudo_connection_set . add ( pseudo_connection ) return pseudo_connection_set
2571	def send_message ( self ) : start = time . time ( ) message = None if not self . initialized : message = self . construct_start_message ( ) self . initialized = True else : message = self . construct_end_message ( ) self . send_UDP_message ( message ) end = time . time ( ) return end - start
2148	def create ( self , fail_on_found = False , force_on_exists = False , ** kwargs ) : config_item = self . _separate ( kwargs ) jt_id = kwargs . pop ( 'job_template' , None ) status = kwargs . pop ( 'status' , 'any' ) old_endpoint = self . endpoint if jt_id is not None : jt = get_resource ( 'job_template' ) jt . get ( pk = jt_id ) try : nt_id = self . get ( ** copy . deepcopy ( kwargs ) ) [ 'id' ] except exc . NotFound : pass else : if fail_on_found : raise exc . TowerCLIError ( 'Notification template already ' 'exists and fail-on-found is ' 'switched on. Please use' ' "associate_notification" method' ' of job_template instead.' ) else : debug . log ( 'Notification template already exists, ' 'associating with job template.' , header = 'details' ) return jt . associate_notification_template ( jt_id , nt_id , status = status ) self . endpoint = '/job_templates/%d/notification_templates_%s/' % ( jt_id , status ) self . _configuration ( kwargs , config_item ) result = super ( Resource , self ) . create ( ** kwargs ) self . endpoint = old_endpoint return result
12658	def append_dict_values ( list_of_dicts , keys = None ) : if keys is None : keys = list ( list_of_dicts [ 0 ] . keys ( ) ) dict_of_lists = DefaultOrderedDict ( list ) for d in list_of_dicts : for k in keys : dict_of_lists [ k ] . append ( d [ k ] ) return dict_of_lists
11303	def invalidate_stored_oembeds ( self , sender , instance , created , ** kwargs ) : ctype = ContentType . objects . get_for_model ( instance ) StoredOEmbed . objects . filter ( object_id = instance . pk , content_type = ctype ) . delete ( )
5057	def build_notification_message ( template_context , template_configuration = None ) : if ( template_configuration is not None and template_configuration . html_template and template_configuration . plaintext_template ) : plain_msg , html_msg = template_configuration . render_all_templates ( template_context ) else : plain_msg = render_to_string ( 'enterprise/emails/user_notification.txt' , template_context ) html_msg = render_to_string ( 'enterprise/emails/user_notification.html' , template_context ) return plain_msg , html_msg
1636	def CheckForFunctionLengths ( filename , clean_lines , linenum , function_state , error ) : lines = clean_lines . lines line = lines [ linenum ] joined_line = '' starting_func = False regexp = r'(\w(\w|::|\*|\&|\s)*)\(' match_result = Match ( regexp , line ) if match_result : function_name = match_result . group ( 1 ) . split ( ) [ - 1 ] if function_name == 'TEST' or function_name == 'TEST_F' or ( not Match ( r'[A-Z_]+$' , function_name ) ) : starting_func = True if starting_func : body_found = False for start_linenum in range ( linenum , clean_lines . NumLines ( ) ) : start_line = lines [ start_linenum ] joined_line += ' ' + start_line . lstrip ( ) if Search ( r'(;|})' , start_line ) : body_found = True break elif Search ( r'{' , start_line ) : body_found = True function = Search ( r'((\w|:)*)\(' , line ) . group ( 1 ) if Match ( r'TEST' , function ) : parameter_regexp = Search ( r'(\(.*\))' , joined_line ) if parameter_regexp : function += parameter_regexp . group ( 1 ) else : function += '()' function_state . Begin ( function ) break if not body_found : error ( filename , linenum , 'readability/fn_size' , 5 , 'Lint failed to find start of function body.' ) elif Match ( r'^\}\s*$' , line ) : function_state . Check ( error , filename , linenum ) function_state . End ( ) elif not Match ( r'^\s*$' , line ) : function_state . Count ( )
4273	def url ( self ) : url = self . name . encode ( 'utf-8' ) return url_quote ( url ) + '/' + self . url_ext
9504	def distance_to_point ( self , p ) : if self . start <= p <= self . end : return 0 else : return min ( abs ( self . start - p ) , abs ( self . end - p ) )
11901	def _run_server ( ) : port = _get_server_port ( ) SocketServer . TCPServer . allow_reuse_address = True server = SocketServer . TCPServer ( ( '' , port ) , SimpleHTTPServer . SimpleHTTPRequestHandler ) print ( 'Your images are at http://127.0.0.1:%d/%s' % ( port , INDEX_FILE_NAME ) ) try : server . serve_forever ( ) except KeyboardInterrupt : print ( 'User interrupted, stopping' ) except Exception as exptn : print ( exptn ) print ( 'Unhandled exception in server, stopping' )
12645	def set_aad_cache ( token , cache ) : set_config_value ( 'aad_token' , jsonpickle . encode ( token ) ) set_config_value ( 'aad_cache' , jsonpickle . encode ( cache ) )
6331	def encode ( self , word , terminator = '\0' ) : r if word : if terminator in word : raise ValueError ( 'Specified terminator, {}, already in word.' . format ( terminator if terminator != '\0' else '\\0' ) ) else : word += terminator wordlist = sorted ( word [ i : ] + word [ : i ] for i in range ( len ( word ) ) ) return '' . join ( [ w [ - 1 ] for w in wordlist ] ) else : return terminator
8907	def list_services ( self ) : my_services = [ ] for service in self . collection . find ( ) . sort ( 'name' , pymongo . ASCENDING ) : my_services . append ( Service ( service ) ) return my_services
7623	def melody ( ref , est , ** kwargs ) : r namespace = 'pitch_contour' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_times , ref_p = ref . to_event_values ( ) est_times , est_p = est . to_event_values ( ) ref_freq = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in ref_p ] ) est_freq = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in est_p ] ) return mir_eval . melody . evaluate ( ref_times , ref_freq , est_times , est_freq , ** kwargs )
7378	def process_keys ( func ) : @ wraps ( func ) def decorated ( self , k , * args ) : if not isinstance ( k , str ) : msg = "%s: key must be a string" % self . __class__ . __name__ raise ValueError ( msg ) if not k . startswith ( self . prefix ) : k = self . prefix + k return func ( self , k , * args ) return decorated
12630	def compose_err_msg ( msg , ** kwargs ) : updated_msg = msg for k , v in sorted ( kwargs . items ( ) ) : if isinstance ( v , _basestring ) : updated_msg += "\n" + k + ": " + v return updated_msg
133	def clip_out_of_image ( self , image ) : import shapely . geometry if self . is_out_of_image ( image , fully = True , partly = False ) : return [ ] h , w = image . shape [ 0 : 2 ] if ia . is_np_array ( image ) else image [ 0 : 2 ] poly_shapely = self . to_shapely_polygon ( ) poly_image = shapely . geometry . Polygon ( [ ( 0 , 0 ) , ( w , 0 ) , ( w , h ) , ( 0 , h ) ] ) multipoly_inter_shapely = poly_shapely . intersection ( poly_image ) if not isinstance ( multipoly_inter_shapely , shapely . geometry . MultiPolygon ) : ia . do_assert ( isinstance ( multipoly_inter_shapely , shapely . geometry . Polygon ) ) multipoly_inter_shapely = shapely . geometry . MultiPolygon ( [ multipoly_inter_shapely ] ) polygons = [ ] for poly_inter_shapely in multipoly_inter_shapely . geoms : polygons . append ( Polygon . from_shapely ( poly_inter_shapely , label = self . label ) ) polygons_reordered = [ ] for polygon in polygons : found = False for x , y in self . exterior : closest_idx , dist = polygon . find_closest_point_index ( x = x , y = y , return_distance = True ) if dist < 1e-6 : polygon_reordered = polygon . change_first_point_by_index ( closest_idx ) polygons_reordered . append ( polygon_reordered ) found = True break ia . do_assert ( found ) return polygons_reordered
9911	def is_expired ( self ) : expiration_time = self . created_at + datetime . timedelta ( days = 1 ) return timezone . now ( ) > expiration_time
6100	def intensities_from_grid_radii ( self , grid_radii ) : return np . multiply ( np . divide ( self . intensity , self . sigma * np . sqrt ( 2.0 * np . pi ) ) , np . exp ( - 0.5 * np . square ( np . divide ( grid_radii , self . sigma ) ) ) )
4491	def clone ( args ) : osf = _setup_osf ( args ) project = osf . project ( args . project ) output_dir = args . project if args . output is not None : output_dir = args . output with tqdm ( unit = 'files' ) as pbar : for store in project . storages : prefix = os . path . join ( output_dir , store . name ) for file_ in store . files : path = file_ . path if path . startswith ( '/' ) : path = path [ 1 : ] path = os . path . join ( prefix , path ) if os . path . exists ( path ) and args . update : if checksum ( path ) == file_ . hashes . get ( 'md5' ) : continue directory , _ = os . path . split ( path ) makedirs ( directory , exist_ok = True ) with open ( path , "wb" ) as f : file_ . write_to ( f ) pbar . update ( )
785	def jobCountCancellingJobs ( self , ) : with ConnectionFactory . get ( ) as conn : query = 'SELECT COUNT(job_id) ' 'FROM %s ' 'WHERE (status<>%%s AND cancel is TRUE)' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . STATUS_COMPLETED ] ) rows = conn . cursor . fetchall ( ) return rows [ 0 ] [ 0 ]
6544	def terminate ( self ) : if not self . is_terminated : log . debug ( "terminal client terminated" ) try : self . exec_command ( b"Quit" ) except BrokenPipeError : pass except socket . error as e : if e . errno != errno . ECONNRESET : raise self . app . close ( ) self . is_terminated = True
11346	def handle_endtag ( self , tag ) : if tag in self . mathml_elements : self . fed . append ( "</{0}>" . format ( tag ) )
9978	def find_funcdef ( source ) : try : module_node = compile ( source , "<string>" , mode = "exec" , flags = ast . PyCF_ONLY_AST ) except SyntaxError : return find_funcdef ( fix_lamdaline ( source ) ) for node in ast . walk ( module_node ) : if isinstance ( node , ast . FunctionDef ) or isinstance ( node , ast . Lambda ) : return node raise ValueError ( "function definition not found" )
61	def iou ( self , other ) : inters = self . intersection ( other ) if inters is None : return 0.0 else : area_union = self . area + other . area - inters . area return inters . area / area_union if area_union > 0 else 0.0
13148	def freeze ( self ) : data = super ( IndexBuilder , self ) . freeze ( ) try : base_file_names = data [ 'docnames' ] except KeyError : base_file_names = data [ 'filenames' ] store = { } c = itertools . count ( ) for prefix , items in iteritems ( data [ 'objects' ] ) : for name , ( index , typeindex , _ , shortanchor ) in iteritems ( items ) : objtype = data [ 'objtypes' ] [ typeindex ] if objtype . startswith ( 'cpp:' ) : split = name . rsplit ( '::' , 1 ) if len ( split ) != 2 : warnings . warn ( "What's up with %s?" % str ( ( prefix , name , objtype ) ) ) continue prefix , name = split last_prefix = prefix . split ( '::' ) [ - 1 ] else : last_prefix = prefix . split ( '.' ) [ - 1 ] store [ next ( c ) ] = { 'filename' : base_file_names [ index ] , 'objtype' : objtype , 'prefix' : prefix , 'last_prefix' : last_prefix , 'name' : name , 'shortanchor' : shortanchor , } data . update ( { 'store' : store } ) return data
8127	def search_images ( q , start = 1 , count = 10 , wait = 10 , asynchronous = False , cached = False ) : service = YAHOO_IMAGES return YahooSearch ( q , start , count , service , None , wait , asynchronous , cached )
12621	def repr_imgs ( imgs ) : if isinstance ( imgs , string_types ) : return imgs if isinstance ( imgs , collections . Iterable ) : return '[{}]' . format ( ', ' . join ( repr_imgs ( img ) for img in imgs ) ) try : filename = imgs . get_filename ( ) if filename is not None : img_str = "{}('{}')" . format ( imgs . __class__ . __name__ , filename ) else : img_str = "{}(shape={}, affine={})" . format ( imgs . __class__ . __name__ , repr ( get_shape ( imgs ) ) , repr ( imgs . get_affine ( ) ) ) except Exception as exc : log . error ( 'Error reading attributes from img.get_filename()' ) return repr ( imgs ) else : return img_str
10226	def get_correlation_triangles ( graph : BELGraph ) -> SetOfNodeTriples : return { tuple ( sorted ( [ n , u , v ] , key = str ) ) for n in graph for u , v in itt . combinations ( graph [ n ] , 2 ) if graph . has_edge ( u , v ) }
73	def Emboss ( alpha = 0 , strength = 1 , name = None , deterministic = False , random_state = None ) : alpha_param = iap . handle_continuous_param ( alpha , "alpha" , value_range = ( 0 , 1.0 ) , tuple_to_uniform = True , list_to_choice = True ) strength_param = iap . handle_continuous_param ( strength , "strength" , value_range = ( 0 , None ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( image , nb_channels , random_state_func ) : alpha_sample = alpha_param . draw_sample ( random_state = random_state_func ) ia . do_assert ( 0 <= alpha_sample <= 1.0 ) strength_sample = strength_param . draw_sample ( random_state = random_state_func ) matrix_nochange = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) matrix_effect = np . array ( [ [ - 1 - strength_sample , 0 - strength_sample , 0 ] , [ 0 - strength_sample , 1 , 0 + strength_sample ] , [ 0 , 0 + strength_sample , 1 + strength_sample ] ] , dtype = np . float32 ) matrix = ( 1 - alpha_sample ) * matrix_nochange + alpha_sample * matrix_effect return [ matrix ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
4102	def generate_gallery_rst ( app ) : try : plot_gallery = eval ( app . builder . config . plot_gallery ) except TypeError : plot_gallery = bool ( app . builder . config . plot_gallery ) gallery_conf . update ( app . config . sphinx_gallery_conf ) gallery_conf . update ( plot_gallery = plot_gallery ) gallery_conf . update ( abort_on_example_error = app . builder . config . abort_on_example_error ) app . config . sphinx_gallery_conf = gallery_conf app . config . html_static_path . append ( glr_path_static ( ) ) clean_gallery_out ( app . builder . outdir ) examples_dirs = gallery_conf [ 'examples_dirs' ] gallery_dirs = gallery_conf [ 'gallery_dirs' ] if not isinstance ( examples_dirs , list ) : examples_dirs = [ examples_dirs ] if not isinstance ( gallery_dirs , list ) : gallery_dirs = [ gallery_dirs ] mod_examples_dir = os . path . relpath ( gallery_conf [ 'mod_example_dir' ] , app . builder . srcdir ) seen_backrefs = set ( ) for examples_dir , gallery_dir in zip ( examples_dirs , gallery_dirs ) : examples_dir = os . path . relpath ( examples_dir , app . builder . srcdir ) gallery_dir = os . path . relpath ( gallery_dir , app . builder . srcdir ) for workdir in [ examples_dir , gallery_dir , mod_examples_dir ] : if not os . path . exists ( workdir ) : os . makedirs ( workdir ) fhindex = open ( os . path . join ( gallery_dir , 'index.rst' ) , 'w' ) fhindex . write ( generate_dir_rst ( examples_dir , gallery_dir , gallery_conf , seen_backrefs ) ) for directory in sorted ( os . listdir ( examples_dir ) ) : if os . path . isdir ( os . path . join ( examples_dir , directory ) ) : src_dir = os . path . join ( examples_dir , directory ) target_dir = os . path . join ( gallery_dir , directory ) fhindex . write ( generate_dir_rst ( src_dir , target_dir , gallery_conf , seen_backrefs ) ) fhindex . flush ( )
13440	def unlock_file ( filename ) : lockfile = "%s.lock" % filename if isfile ( lockfile ) : os . remove ( lockfile ) return True else : return False
12387	def parse_segment ( text ) : "we expect foo=bar" if not len ( text ) : return NoopQuerySegment ( ) q = QuerySegment ( ) equalities = zip ( constants . OPERATOR_EQUALITIES , itertools . repeat ( text ) ) equalities = map ( lambda x : ( x [ 0 ] , x [ 1 ] . split ( x [ 0 ] , 1 ) ) , equalities ) equalities = list ( filter ( lambda x : len ( x [ 1 ] ) > 1 , equalities ) ) key_len = len ( min ( ( x [ 1 ] [ 0 ] for x in equalities ) , key = len ) ) equalities = filter ( lambda x : len ( x [ 1 ] [ 0 ] ) == key_len , equalities ) op , ( key , value ) = min ( equalities , key = lambda x : len ( x [ 1 ] [ 1 ] ) ) key , directive = parse_directive ( key ) if directive : op = constants . OPERATOR_EQUALITY_FALLBACK q . directive = directive path = key . split ( constants . SEP_PATH ) last = path [ - 1 ] if last . endswith ( constants . OPERATOR_NEGATION ) : last = last [ : - 1 ] q . negated = not q . negated if last == constants . PATH_NEGATION : path . pop ( - 1 ) q . negated = not q . negated q . values = value . split ( constants . SEP_VALUE ) if path [ - 1 ] in constants . OPERATOR_SUFFIXES : if op not in constants . OPERATOR_FALLBACK : raise ValueError ( 'Both path-style operator and equality style operator ' 'provided. Please provide only a single style operator.' ) q . operator = constants . OPERATOR_SUFFIX_MAP [ path [ - 1 ] ] path . pop ( - 1 ) else : q . operator = constants . OPERATOR_EQUALITY_MAP [ op ] if not len ( path ) : raise ValueError ( 'No attribute navigation path provided.' ) q . path = path return q
10360	def is_edge_consistent ( graph , u , v ) : if not graph . has_edge ( u , v ) : raise ValueError ( '{} does not contain an edge ({}, {})' . format ( graph , u , v ) ) return 0 == len ( set ( d [ RELATION ] for d in graph . edge [ u ] [ v ] . values ( ) ) )
11329	def main ( ) : argparser = ArgumentParser ( ) subparsers = argparser . add_subparsers ( dest = 'selected_subparser' ) all_parser = subparsers . add_parser ( 'all' ) elsevier_parser = subparsers . add_parser ( 'elsevier' ) oxford_parser = subparsers . add_parser ( 'oxford' ) springer_parser = subparsers . add_parser ( 'springer' ) all_parser . add_argument ( '--update-credentials' , action = 'store_true' ) elsevier_parser . add_argument ( '--run-locally' , action = 'store_true' ) elsevier_parser . add_argument ( '--package-name' ) elsevier_parser . add_argument ( '--path' ) elsevier_parser . add_argument ( '--CONSYN' , action = 'store_true' ) elsevier_parser . add_argument ( '--update-credentials' , action = 'store_true' ) elsevier_parser . add_argument ( '--extract-nations' , action = 'store_true' ) oxford_parser . add_argument ( '--dont-empty-ftp' , action = 'store_true' ) oxford_parser . add_argument ( '--package-name' ) oxford_parser . add_argument ( '--path' ) oxford_parser . add_argument ( '--update-credentials' , action = 'store_true' ) oxford_parser . add_argument ( '--extract-nations' , action = 'store_true' ) springer_parser . add_argument ( '--package-name' ) springer_parser . add_argument ( '--path' ) springer_parser . add_argument ( '--update-credentials' , action = 'store_true' ) springer_parser . add_argument ( '--extract-nations' , action = 'store_true' ) settings = Bunch ( vars ( argparser . parse_args ( ) ) ) call_package ( settings )
4532	def clone ( self ) : args = { k : getattr ( self , k ) for k in self . CLONE_ATTRS } args [ 'color_list' ] = copy . copy ( self . color_list ) return self . __class__ ( [ ] , ** args )
1584	def yaml_config_reader ( config_path ) : if not config_path . endswith ( ".yaml" ) : raise ValueError ( "Config file not yaml" ) with open ( config_path , 'r' ) as f : config = yaml . load ( f ) return config
13631	def _adaptToResource ( self , result ) : if result is None : return NotFound ( ) spinneretResource = ISpinneretResource ( result , None ) if spinneretResource is not None : return SpinneretResource ( spinneretResource ) renderable = IRenderable ( result , None ) if renderable is not None : return _RenderableResource ( renderable ) resource = IResource ( result , None ) if resource is not None : return resource if isinstance ( result , URLPath ) : return Redirect ( str ( result ) ) return result
3212	def _update_cache_stats ( self , key , result ) : if result is None : self . _CACHE_STATS [ 'access_stats' ] . setdefault ( key , { 'hit' : 0 , 'miss' : 0 , 'expired' : 0 } ) else : self . _CACHE_STATS [ 'access_stats' ] [ key ] [ result ] += 1
2927	def write_file_to_package_zip ( self , filename , src_filename ) : f = open ( src_filename ) with f : data = f . read ( ) self . manifest [ filename ] = md5hash ( data ) self . package_zip . write ( src_filename , filename )
3811	def get_request_header ( self ) : if self . _client_id is not None : self . _request_header . client_identifier . resource = self . _client_id return self . _request_header
11135	def copyto ( self , new_abspath = None , new_dirpath = None , new_dirname = None , new_basename = None , new_fname = None , new_ext = None , overwrite = False , makedirs = False ) : self . assert_exists ( ) p = self . change ( new_abspath = new_abspath , new_dirpath = new_dirpath , new_dirname = new_dirname , new_basename = new_basename , new_fname = new_fname , new_ext = new_ext , ) if p . is_not_exist_or_allow_overwrite ( overwrite = overwrite ) : if self . abspath != p . abspath : try : shutil . copy ( self . abspath , p . abspath ) except IOError as e : if makedirs : os . makedirs ( p . parent . abspath ) shutil . copy ( self . abspath , p . abspath ) else : raise e return p
8941	def _to_webdav ( self , docs_base , release ) : try : git_path = subprocess . check_output ( 'git remote get-url origin 2>/dev/null' , shell = True ) except subprocess . CalledProcessError : git_path = '' else : git_path = git_path . decode ( 'ascii' ) . strip ( ) git_path = git_path . replace ( 'http://' , '' ) . replace ( 'https://' , '' ) . replace ( 'ssh://' , '' ) git_path = re . search ( r'[^:/]+?[:/](.+)' , git_path ) git_path = git_path . group ( 1 ) . replace ( '.git' , '' ) if git_path else '' url = None with self . _zipped ( docs_base ) as handle : url_ns = dict ( name = self . cfg . project . name , version = release , git_path = git_path ) reply = requests . put ( self . params [ 'url' ] . format ( ** url_ns ) , data = handle . read ( ) , headers = { 'Accept' : 'application/json' } ) if reply . status_code in range ( 200 , 300 ) : notify . info ( "{status_code} {reason}" . format ( ** vars ( reply ) ) ) try : data = reply . json ( ) except ValueError as exc : notify . warning ( "Didn't get a JSON response! ({})" . format ( exc ) ) else : if 'downloadUri' in data : url = data [ 'downloadUri' ] + '!/index.html' elif reply . status_code == 301 : url = reply . headers [ 'location' ] else : data = self . cfg . copy ( ) data . update ( self . params ) data . update ( vars ( reply ) ) notify . error ( "{status_code} {reason} for PUT to {url}" . format ( ** data ) ) if not url : notify . warning ( "Couldn't get URL from upload response!" ) return url
4793	def is_subset_of ( self , * supersets ) : if not isinstance ( self . val , Iterable ) : raise TypeError ( 'val is not iterable' ) if len ( supersets ) == 0 : raise ValueError ( 'one or more superset args must be given' ) missing = [ ] if hasattr ( self . val , 'keys' ) and callable ( getattr ( self . val , 'keys' ) ) and hasattr ( self . val , '__getitem__' ) : superdict = { } for l , j in enumerate ( supersets ) : self . _check_dict_like ( j , check_values = False , name = 'arg #%d' % ( l + 1 ) ) for k in j . keys ( ) : superdict . update ( { k : j [ k ] } ) for i in self . val . keys ( ) : if i not in superdict : missing . append ( { i : self . val [ i ] } ) elif self . val [ i ] != superdict [ i ] : missing . append ( { i : self . val [ i ] } ) if missing : self . _err ( 'Expected <%s> to be subset of %s, but %s %s missing.' % ( self . val , self . _fmt_items ( superdict ) , self . _fmt_items ( missing ) , 'was' if len ( missing ) == 1 else 'were' ) ) else : superset = set ( ) for j in supersets : try : for k in j : superset . add ( k ) except Exception : superset . add ( j ) for i in self . val : if i not in superset : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to be subset of %s, but %s %s missing.' % ( self . val , self . _fmt_items ( superset ) , self . _fmt_items ( missing ) , 'was' if len ( missing ) == 1 else 'were' ) ) return self
1973	def sys_openat ( self , dirfd , buf , flags , mode ) : if issymbolic ( dirfd ) : logger . debug ( "Ask to read from a symbolic directory file descriptor!!" ) self . constraints . add ( dirfd >= 0 ) self . constraints . add ( dirfd <= len ( self . files ) ) raise ConcretizeArgument ( self , 0 ) if issymbolic ( buf ) : logger . debug ( "Ask to read to a symbolic buffer" ) raise ConcretizeArgument ( self , 1 ) return super ( ) . sys_openat ( dirfd , buf , flags , mode )
2561	def recv_result_from_workers ( self ) : info = MPI . Status ( ) result = self . comm . recv ( source = MPI . ANY_SOURCE , tag = RESULT_TAG , status = info ) logger . debug ( "Received result from workers: {}" . format ( result ) ) return result
1459	def load_pex ( path_to_pex , include_deps = True ) : abs_path_to_pex = os . path . abspath ( path_to_pex ) Log . debug ( "Add a pex to the path: %s" % abs_path_to_pex ) if abs_path_to_pex not in sys . path : sys . path . insert ( 0 , os . path . dirname ( abs_path_to_pex ) ) if include_deps : for dep in _get_deps_list ( abs_path_to_pex ) : to_join = os . path . join ( os . path . dirname ( abs_path_to_pex ) , dep ) if to_join not in sys . path : Log . debug ( "Add a new dependency to the path: %s" % dep ) sys . path . insert ( 0 , to_join ) Log . debug ( "Python path: %s" % str ( sys . path ) )
4165	def _get_link ( self , cobj ) : fname_idx = None full_name = cobj [ 'module_short' ] + '.' + cobj [ 'name' ] if full_name in self . _searchindex [ 'objects' ] : value = self . _searchindex [ 'objects' ] [ full_name ] if isinstance ( value , dict ) : value = value [ next ( iter ( value . keys ( ) ) ) ] fname_idx = value [ 0 ] elif cobj [ 'module_short' ] in self . _searchindex [ 'objects' ] : value = self . _searchindex [ 'objects' ] [ cobj [ 'module_short' ] ] if cobj [ 'name' ] in value . keys ( ) : fname_idx = value [ cobj [ 'name' ] ] [ 0 ] if fname_idx is not None : fname = self . _searchindex [ 'filenames' ] [ fname_idx ] + '.html' if self . _is_windows : fname = fname . replace ( '/' , '\\' ) link = os . path . join ( self . doc_url , fname ) else : link = posixpath . join ( self . doc_url , fname ) if hasattr ( link , 'decode' ) : link = link . decode ( 'utf-8' , 'replace' ) if link in self . _page_cache : html = self . _page_cache [ link ] else : html = get_data ( link , self . gallery_dir ) self . _page_cache [ link ] = html comb_names = [ cobj [ 'module_short' ] + '.' + cobj [ 'name' ] ] if self . extra_modules_test is not None : for mod in self . extra_modules_test : comb_names . append ( mod + '.' + cobj [ 'name' ] ) url = False if hasattr ( html , 'decode' ) : html = html . decode ( 'utf-8' , 'replace' ) for comb_name in comb_names : if hasattr ( comb_name , 'decode' ) : comb_name = comb_name . decode ( 'utf-8' , 'replace' ) if comb_name in html : url = link + u'#' + comb_name link = url else : link = False return link
2509	def parse_only_extr_license ( self , extr_lic ) : ident = self . get_extr_license_ident ( extr_lic ) text = self . get_extr_license_text ( extr_lic ) comment = self . get_extr_lics_comment ( extr_lic ) xrefs = self . get_extr_lics_xref ( extr_lic ) name = self . get_extr_lic_name ( extr_lic ) if not ident : return lic = document . ExtractedLicense ( ident ) if text is not None : lic . text = text if name is not None : lic . full_name = name if comment is not None : lic . comment = comment lic . cross_ref = map ( lambda x : six . text_type ( x ) , xrefs ) return lic
3363	def save_yaml_model ( model , filename , sort = False , ** kwargs ) : obj = model_to_dict ( model , sort = sort ) obj [ "version" ] = YAML_SPEC if isinstance ( filename , string_types ) : with io . open ( filename , "w" ) as file_handle : yaml . dump ( obj , file_handle , ** kwargs ) else : yaml . dump ( obj , filename , ** kwargs )
11084	def save ( self , msg , args ) : self . send_message ( msg . channel , "Saving current state..." ) self . _bot . plugins . save_state ( ) self . send_message ( msg . channel , "Done." )
2503	def handle_lics ( self , lics ) : if ( lics , RDF . type , self . spdx_namespace [ 'ExtractedLicensingInfo' ] ) in self . graph : return self . parse_only_extr_license ( lics ) ident_start = lics . rfind ( '/' ) + 1 if ident_start == 0 : special = self . to_special_value ( lics ) if special == lics : if self . LICS_REF_REGEX . match ( lics ) : return document . License . from_identifier ( lics ) else : raise SPDXValueError ( 'License' ) else : return special else : return document . License . from_identifier ( lics [ ident_start : ] )
8352	def handle_charref ( self , ref ) : "Handle character references as data." if self . convertEntities : data = unichr ( int ( ref ) ) else : data = '&#%s;' % ref self . handle_data ( data )
7890	def get_user ( self , nick_or_jid , create = False ) : if isinstance ( nick_or_jid , JID ) : if not nick_or_jid . resource : return None for u in self . users . values ( ) : if nick_or_jid in ( u . room_jid , u . real_jid ) : return u if create : return MucRoomUser ( nick_or_jid ) else : return None return self . users . get ( nick_or_jid )
12391	def indexesOptional ( f ) : stack = inspect . stack ( ) _NO_INDEX_CHECK_NEEDED . add ( '%s.%s.%s' % ( f . __module__ , stack [ 1 ] [ 3 ] , f . __name__ ) ) del stack return f
7049	def massradius ( age , planetdist , coremass , mass = 'massjupiter' , radius = 'radiusjupiter' ) : MR = { 0.3 : MASSESRADII_0_3GYR , 1.0 : MASSESRADII_1_0GYR , 4.5 : MASSESRADII_4_5GYR } if age not in MR : print ( 'given age not in Fortney 2007, returning...' ) return massradius = MR [ age ] if ( planetdist in massradius ) and ( coremass in massradius [ planetdist ] ) : print ( 'getting % Gyr M-R for planet dist %s AU, ' 'core mass %s Mearth...' % ( age , planetdist , coremass ) ) massradrelation = massradius [ planetdist ] [ coremass ] outdict = { 'mass' : array ( massradrelation [ mass ] ) , 'radius' : array ( massradrelation [ radius ] ) } return outdict
13637	def maybe ( f , default = None ) : @ wraps ( f ) def _maybe ( x , * a , ** kw ) : if x is None : return default return f ( x , * a , ** kw ) return _maybe
8221	def do_toggle_fullscreen ( self , action ) : is_fullscreen = action . get_active ( ) if is_fullscreen : self . fullscreen ( ) else : self . unfullscreen ( )
1207	def setup ( app ) : global _is_sphinx _is_sphinx = True app . add_config_value ( 'no_underscore_emphasis' , False , 'env' ) app . add_source_parser ( '.md' , M2RParser ) app . add_directive ( 'mdinclude' , MdInclude )
12746	def as_flat_array ( iterables ) : arr = [ ] for x in iterables : arr . extend ( x ) return np . array ( arr )
12803	def get_room ( self , id ) : if id not in self . _rooms : self . _rooms [ id ] = Room ( self , id ) return self . _rooms [ id ]
3786	def TP_dependent_property_derivative_T ( self , T , P , order = 1 ) : r sorted_valid_methods_P = self . select_valid_methods_P ( T , P ) for method in sorted_valid_methods_P : try : return self . calculate_derivative_T ( T , P , method , order ) except : pass return None
3287	def _get_log ( self , limit = None ) : self . ui . pushbuffer ( ) commands . log ( self . ui , self . repo , limit = limit , date = None , rev = None , user = None ) res = self . ui . popbuffer ( ) . strip ( ) logList = [ ] for logentry in res . split ( "\n\n" ) : log = { } logList . append ( log ) for line in logentry . split ( "\n" ) : k , v = line . split ( ":" , 1 ) assert k in ( "changeset" , "tag" , "user" , "date" , "summary" ) log [ k . strip ( ) ] = v . strip ( ) log [ "parsed_date" ] = util . parse_time_string ( log [ "date" ] ) local_id , unid = log [ "changeset" ] . split ( ":" ) log [ "local_id" ] = int ( local_id ) log [ "unid" ] = unid return logList
12576	def set_mask ( self , mask_img ) : mask = load_mask ( mask_img , allow_empty = True ) check_img_compatibility ( self . img , mask , only_check_3d = True ) self . mask = mask
3919	async def _load ( self ) : try : conv_events = await self . _conversation . get_events ( self . _conversation . events [ 0 ] . id_ ) except ( IndexError , hangups . NetworkError ) : conv_events = [ ] if not conv_events : self . _first_loaded = True if self . _focus_position == self . POSITION_LOADING and conv_events : self . set_focus ( conv_events [ - 1 ] . id_ ) else : self . _modified ( ) self . _refresh_watermarked_events ( ) self . _is_loading = False
5525	def grab ( self , bbox = None ) : w = Gdk . get_default_root_window ( ) if bbox is not None : g = [ bbox [ 0 ] , bbox [ 1 ] , bbox [ 2 ] - bbox [ 0 ] , bbox [ 3 ] - bbox [ 1 ] ] else : g = w . get_geometry ( ) pb = Gdk . pixbuf_get_from_window ( w , * g ) if pb . get_bits_per_sample ( ) != 8 : raise ValueError ( 'Expected 8 bits per pixel.' ) elif pb . get_n_channels ( ) != 3 : raise ValueError ( 'Expected RGB image.' ) pixel_bytes = pb . read_pixel_bytes ( ) . get_data ( ) width , height = g [ 2 ] , g [ 3 ] return Image . frombytes ( 'RGB' , ( width , height ) , pixel_bytes , 'raw' , 'RGB' , pb . get_rowstride ( ) , 1 )
849	def getOutputElementCount ( self , name ) : if name == "resetOut" : print ( "WARNING: getOutputElementCount should not have been called with " "resetOut" ) return 1 elif name == "sequenceIdOut" : print ( "WARNING: getOutputElementCount should not have been called with " "sequenceIdOut" ) return 1 elif name == "dataOut" : if self . encoder is None : raise Exception ( "NuPIC requested output element count for 'dataOut' " "on a RecordSensor node, but the encoder has not " "been set" ) return self . encoder . getWidth ( ) elif name == "sourceOut" : if self . encoder is None : raise Exception ( "NuPIC requested output element count for 'sourceOut' " "on a RecordSensor node, " "but the encoder has not been set" ) return len ( self . encoder . getDescription ( ) ) elif name == "bucketIdxOut" : return 1 elif name == "actValueOut" : return 1 elif name == "categoryOut" : return self . numCategories elif name == 'spatialTopDownOut' or name == 'temporalTopDownOut' : if self . encoder is None : raise Exception ( "NuPIC requested output element count for 'sourceOut' " "on a RecordSensor node, " "but the encoder has not been set" ) return len ( self . encoder . getDescription ( ) ) else : raise Exception ( "Unknown output %s" % name )
13331	def add ( name , path , branch , type ) : if not name and not path : ctx = click . get_current_context ( ) click . echo ( ctx . get_help ( ) ) examples = ( '\nExamples:\n' ' cpenv module add my_module ./path/to/my_module\n' ' cpenv module add my_module git@github.com:user/my_module.git' ' cpenv module add my_module git@github.com:user/my_module.git --branch=master --type=shared' ) click . echo ( examples ) return if not name : click . echo ( 'Missing required argument: name' ) return if not path : click . echo ( 'Missing required argument: path' ) env = cpenv . get_active_env ( ) if type == 'local' : if not env : click . echo ( '\nActivate an environment to add a local module.\n' ) return if click . confirm ( '\nAdd {} to active env {}?' . format ( name , env . name ) ) : click . echo ( 'Adding module...' , nl = False ) try : env . add_module ( name , path , branch ) except : click . echo ( bold_red ( 'FAILED' ) ) raise else : click . echo ( bold_green ( 'OK!' ) ) return module_paths = cpenv . get_module_paths ( ) click . echo ( '\nAvailable module paths:\n' ) for i , mod_path in enumerate ( module_paths ) : click . echo ( ' {}. {}' . format ( i , mod_path ) ) choice = click . prompt ( 'Where do you want to add your module?' , type = int , default = 0 ) module_root = module_paths [ choice ] module_path = utils . unipath ( module_root , name ) click . echo ( 'Creating module {}...' . format ( module_path ) , nl = False ) try : cpenv . create_module ( module_path , path , branch ) except : click . echo ( bold_red ( 'FAILED' ) ) raise else : click . echo ( bold_green ( 'OK!' ) )
13277	def update_desc_lcin_path ( desc , pdesc_level ) : parent_breadth = desc [ 'parent_breadth_path' ] [ - 1 ] if ( desc [ 'sib_seq' ] == 0 ) : if ( parent_breadth == 0 ) : pass else : parent_lsib_breadth = parent_breadth - 1 plsib_desc = pdesc_level [ parent_lsib_breadth ] if ( plsib_desc [ 'leaf' ] ) : pass else : lcin_path = copy . deepcopy ( plsib_desc [ 'path' ] ) lcin_path . append ( plsib_desc [ 'sons_count' ] - 1 ) desc [ 'lcin_path' ] = lcin_path else : pass return ( desc )
6889	def _parallel_bls_worker ( task ) : try : times , mags , errs = task [ : 3 ] magsarefluxes = task [ 3 ] minfreq , nfreq , stepsize = task [ 4 : 7 ] ndurations , mintransitduration , maxtransitduration = task [ 7 : 10 ] blsobjective , blsmethod , blsoversample = task [ 10 : ] frequencies = minfreq + nparange ( nfreq ) * stepsize periods = 1.0 / frequencies durations = nplinspace ( mintransitduration * periods . min ( ) , maxtransitduration * periods . min ( ) , ndurations ) if magsarefluxes : blsmodel = BoxLeastSquares ( times * u . day , mags * u . dimensionless_unscaled , dy = errs * u . dimensionless_unscaled ) else : blsmodel = BoxLeastSquares ( times * u . day , mags * u . mag , dy = errs * u . mag ) blsresult = blsmodel . power ( periods * u . day , durations * u . day , objective = blsobjective , method = blsmethod , oversample = blsoversample ) return { 'blsresult' : blsresult , 'blsmodel' : blsmodel , 'durations' : durations , 'power' : nparray ( blsresult . power ) } except Exception as e : LOGEXCEPTION ( 'BLS for frequency chunk: (%.6f, %.6f) failed.' % ( frequencies [ 0 ] , frequencies [ - 1 ] ) ) return { 'blsresult' : None , 'blsmodel' : None , 'durations' : durations , 'power' : nparray ( [ npnan for x in range ( nfreq ) ] ) , }
9908	def send_duplicate_notification ( self ) : email_utils . send_email ( from_email = settings . DEFAULT_FROM_EMAIL , recipient_list = [ self . email ] , subject = _ ( "Registration Attempt" ) , template_name = "rest_email_auth/emails/duplicate-email" , ) logger . info ( "Sent duplicate email notification to: %s" , self . email )
10230	def flatten_list_abundance ( node : ListAbundance ) -> ListAbundance : return node . __class__ ( list ( chain . from_iterable ( ( flatten_list_abundance ( member ) . members if isinstance ( member , ListAbundance ) else [ member ] ) for member in node . members ) ) )
1938	def get_func_argument_types ( self , hsh : bytes ) : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) sig = self . _function_signatures_by_selector . get ( hsh ) return '()' if sig is None else sig [ sig . find ( '(' ) : ]
3822	async def easter_egg ( self , easter_egg_request ) : response = hangouts_pb2 . EasterEggResponse ( ) await self . _pb_request ( 'conversations/easteregg' , easter_egg_request , response ) return response
5802	def _convert_filetime_to_timestamp ( filetime ) : hundreds_nano_seconds = struct . unpack ( b'>Q' , struct . pack ( b'>LL' , filetime . dwHighDateTime , filetime . dwLowDateTime ) ) [ 0 ] seconds_since_1601 = hundreds_nano_seconds / 10000000 return seconds_since_1601 - 11644473600
3216	def get_classic_link ( vpc , ** conn ) : result = { } try : cl_result = describe_vpc_classic_link ( VpcIds = [ vpc [ "id" ] ] , ** conn ) [ 0 ] result [ "Enabled" ] = cl_result [ "ClassicLinkEnabled" ] dns_result = describe_vpc_classic_link_dns_support ( VpcIds = [ vpc [ "id" ] ] , ** conn ) [ 0 ] result [ "DnsEnabled" ] = dns_result [ "ClassicLinkDnsSupported" ] except ClientError as e : if 'UnsupportedOperation' not in str ( e ) : raise e return result
4177	def window_cosine ( N ) : r if N == 1 : return ones ( 1 ) n = arange ( 0 , N ) win = sin ( pi * n / ( N - 1. ) ) return win
5949	def filename ( self , filename = None , ext = None , set_default = False , use_my_ext = False ) : if filename is None : if not hasattr ( self , '_filename' ) : self . _filename = None if self . _filename : filename = self . _filename else : raise ValueError ( "A file name is required because no default file name was defined." ) my_ext = None else : filename , my_ext = os . path . splitext ( filename ) if set_default : self . _filename = filename if my_ext and use_my_ext : ext = my_ext if ext is not None : if ext . startswith ( os . extsep ) : ext = ext [ 1 : ] if ext != "" : filename = filename + os . extsep + ext return filename
8209	def coordinates ( self , x0 , y0 , distance , angle ) : x = x0 + cos ( radians ( angle ) ) * distance y = y0 + sin ( radians ( angle ) ) * distance return Point ( x , y )
5837	def tsne ( self , data_view_id ) : analysis = self . _data_analysis ( data_view_id ) projections = analysis [ 'projections' ] tsne = Tsne ( ) for k , v in projections . items ( ) : projection = Projection ( xs = v [ 'x' ] , ys = v [ 'y' ] , responses = v [ 'label' ] , tags = v [ 'inputs' ] , uids = v [ 'uid' ] ) tsne . add_projection ( k , projection ) return tsne
2152	def get ( self , pk = None , ** kwargs ) : self . _separate ( kwargs ) return super ( Resource , self ) . get ( pk = pk , ** kwargs )
8991	def rows_before ( self ) : rows_before = [ ] for mesh in self . consumed_meshes : if mesh . is_produced ( ) : row = mesh . producing_row if rows_before not in rows_before : rows_before . append ( row ) return rows_before
6866	def _pkl_periodogram ( lspinfo , plotdpi = 100 , override_pfmethod = None ) : pgramylabel = PLOTYLABELS [ lspinfo [ 'method' ] ] periods = lspinfo [ 'periods' ] lspvals = lspinfo [ 'lspvals' ] bestperiod = lspinfo [ 'bestperiod' ] nbestperiods = lspinfo [ 'nbestperiods' ] nbestlspvals = lspinfo [ 'nbestlspvals' ] pgramfig = plt . figure ( figsize = ( 7.5 , 4.8 ) , dpi = plotdpi ) plt . plot ( periods , lspvals ) plt . xscale ( 'log' , basex = 10 ) plt . xlabel ( 'Period [days]' ) plt . ylabel ( pgramylabel ) plottitle = '%s - %.6f d' % ( METHODLABELS [ lspinfo [ 'method' ] ] , bestperiod ) plt . title ( plottitle ) for xbestperiod , xbestpeak in zip ( nbestperiods , nbestlspvals ) : plt . annotate ( '%.6f' % xbestperiod , xy = ( xbestperiod , xbestpeak ) , xycoords = 'data' , xytext = ( 0.0 , 25.0 ) , textcoords = 'offset points' , arrowprops = dict ( arrowstyle = "->" ) , fontsize = '14.0' ) plt . grid ( color = '#a9a9a9' , alpha = 0.9 , zorder = 0 , linewidth = 1.0 , linestyle = ':' ) pgrampng = StrIO ( ) pgramfig . savefig ( pgrampng , pad_inches = 0.0 , format = 'png' ) plt . close ( ) pgrampng . seek ( 0 ) pgramb64 = base64 . b64encode ( pgrampng . read ( ) ) pgrampng . close ( ) if not override_pfmethod : checkplotdict = { lspinfo [ 'method' ] : { 'periods' : periods , 'lspvals' : lspvals , 'bestperiod' : bestperiod , 'nbestperiods' : nbestperiods , 'nbestlspvals' : nbestlspvals , 'periodogram' : pgramb64 , } } else : checkplotdict = { override_pfmethod : { 'periods' : periods , 'lspvals' : lspvals , 'bestperiod' : bestperiod , 'nbestperiods' : nbestperiods , 'nbestlspvals' : nbestlspvals , 'periodogram' : pgramb64 , } } return checkplotdict
13184	def row_to_dict ( cls , row ) : comment_code = row [ 3 ] if comment_code . lower ( ) == 'na' : comment_code = '' comp1 = row [ 4 ] if comp1 . lower ( ) == 'na' : comp1 = '' comp2 = row [ 5 ] if comp2 . lower ( ) == 'na' : comp2 = '' chart = row [ 6 ] if chart . lower ( ) == 'na' : chart = '' notes = row [ 7 ] if notes . lower ( ) == 'na' : notes = '' return { 'name' : row [ 0 ] , 'date' : row [ 1 ] , 'magnitude' : row [ 2 ] , 'comment_code' : comment_code , 'comp1' : comp1 , 'comp2' : comp2 , 'chart' : chart , 'notes' : notes , }
11949	def configure_custom ( self , config ) : c = config . pop ( '()' ) if not hasattr ( c , '__call__' ) and hasattr ( types , 'ClassType' ) and isinstance ( c , types . ClassType ) : c = self . resolve ( c ) props = config . pop ( '.' , None ) kwargs = dict ( ( k , config [ k ] ) for k in config if valid_ident ( k ) ) result = c ( ** kwargs ) if props : for name , value in props . items ( ) : setattr ( result , name , value ) return result
3765	def Joule_Thomson ( T , V , Cp , dV_dT = None , beta = None ) : r if dV_dT : return ( T * dV_dT - V ) / Cp elif beta : return V / Cp * ( beta * T - 1. ) else : raise Exception ( 'Either dV_dT or beta is needed' )
13655	def _matchRoute ( components , request , segments , partialMatching ) : if len ( components ) == 1 and isinstance ( components [ 0 ] , bytes ) : components = components [ 0 ] if components [ : 1 ] == '/' : components = components [ 1 : ] components = components . split ( '/' ) results = OrderedDict ( ) NO_MATCH = None , segments remaining = list ( segments ) if len ( segments ) == len ( components ) == 0 : return results , remaining for us , them in izip_longest ( components , segments ) : if us is None : if partialMatching : break else : return NO_MATCH elif them is None : return NO_MATCH if callable ( us ) : name , match = us ( request , them ) if match is None : return NO_MATCH results [ name ] = match elif us != them : return NO_MATCH remaining . pop ( 0 ) return results , remaining
6858	def database_exists ( name , ** kwargs ) : with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : res = query ( "SHOW DATABASES LIKE '%(name)s';" % { 'name' : name } , ** kwargs ) return res . succeeded and ( res == name )
12799	def _url ( self , url = None , parameters = None ) : uri = url or self . _settings [ "url" ] if url and self . _settings [ "base_url" ] : uri = "%s/%s" % ( self . _settings [ "base_url" ] , url ) uri += ".json" if parameters : uri += "?%s" % urllib . urlencode ( parameters ) return uri
6001	def pix_to_sub ( self ) : pix_to_sub = [ [ ] for _ in range ( self . pixels ) ] for regular_pixel , pix_pixel in enumerate ( self . sub_to_pix ) : pix_to_sub [ pix_pixel ] . append ( regular_pixel ) return pix_to_sub
7261	def search ( self , searchAreaWkt = None , filters = None , startDate = None , endDate = None , types = None ) : if not types : types = [ 'Acquisition' ] if startDate : startDateTime = datetime . datetime . strptime ( startDate , '%Y-%m-%dT%H:%M:%S.%fZ' ) if endDate : endDateTime = datetime . datetime . strptime ( endDate , '%Y-%m-%dT%H:%M:%S.%fZ' ) if startDate and endDate : diff = endDateTime - startDateTime if diff . days < 0 : raise Exception ( "startDate must come before endDate." ) postdata = { "searchAreaWkt" : searchAreaWkt , "types" : types , "startDate" : startDate , "endDate" : endDate , } if filters : postdata [ 'filters' ] = filters if searchAreaWkt : postdata [ 'searchAreaWkt' ] = searchAreaWkt url = '%(base_url)s/search' % { 'base_url' : self . base_url } headers = { 'Content-Type' : 'application/json' } r = self . gbdx_connection . post ( url , headers = headers , data = json . dumps ( postdata ) ) r . raise_for_status ( ) results = r . json ( ) [ 'results' ] return results
731	def _getW ( self ) : w = self . _w if type ( w ) is list : return w [ self . _random . getUInt32 ( len ( w ) ) ] else : return w
792	def jobCancel ( self , jobID ) : self . _logger . info ( 'Canceling jobID=%s' , jobID ) self . jobSetFields ( jobID , { "cancel" : True } , useConnectionID = False )
13344	def mean ( a , axis = None , dtype = None , out = None , keepdims = False ) : if ( isinstance ( a , np . ndarray ) or isinstance ( a , RemoteArray ) or isinstance ( a , DistArray ) ) : return a . mean ( axis = axis , dtype = dtype , out = out , keepdims = keepdims ) else : return np . mean ( a , axis = axis , dtype = dtype , out = out , keepdims = keepdims )
9892	def _uptime_minix ( ) : try : f = open ( '/proc/uptime' , 'r' ) up = float ( f . read ( ) ) f . close ( ) return up except ( IOError , ValueError ) : return None
2163	def list ( self , group = None , host_filter = None , ** kwargs ) : if group : kwargs [ 'query' ] = kwargs . get ( 'query' , ( ) ) + ( ( 'groups__in' , group ) , ) if host_filter : kwargs [ 'query' ] = kwargs . get ( 'query' , ( ) ) + ( ( 'host_filter' , host_filter ) , ) return super ( Resource , self ) . list ( ** kwargs )
1278	def markdown ( text , escape = True , ** kwargs ) : return Markdown ( escape = escape , ** kwargs ) ( text )
7661	def trim ( self , start_time , end_time , strict = False ) : if end_time <= start_time : raise ParameterError ( 'end_time must be greater than start_time.' ) if self . duration is None : orig_time = start_time orig_duration = end_time - start_time warnings . warn ( "Annotation.duration is not defined, cannot check " "for temporal intersection, assuming the annotation " "is valid between start_time and end_time." ) else : orig_time = self . time orig_duration = self . duration if start_time > ( orig_time + orig_duration ) or ( end_time < orig_time ) : warnings . warn ( 'Time range defined by [start_time,end_time] does not ' 'intersect with the time range spanned by this annotation, ' 'the trimmed annotation will be empty.' ) trim_start = self . time trim_end = trim_start else : trim_start = max ( orig_time , start_time ) trim_end = min ( orig_time + orig_duration , end_time ) ann_trimmed = Annotation ( self . namespace , data = None , annotation_metadata = self . annotation_metadata , sandbox = self . sandbox , time = trim_start , duration = trim_end - trim_start ) for obs in self . data : obs_start = obs . time obs_end = obs_start + obs . duration if obs_start < trim_end and obs_end > trim_start : new_start = max ( obs_start , trim_start ) new_end = min ( obs_end , trim_end ) new_duration = new_end - new_start if ( ( not strict ) or ( new_start == obs_start and new_end == obs_end ) ) : ann_trimmed . append ( time = new_start , duration = new_duration , value = obs . value , confidence = obs . confidence ) if 'trim' not in ann_trimmed . sandbox . keys ( ) : ann_trimmed . sandbox . update ( trim = [ { 'start_time' : start_time , 'end_time' : end_time , 'trim_start' : trim_start , 'trim_end' : trim_end } ] ) else : ann_trimmed . sandbox . trim . append ( { 'start_time' : start_time , 'end_time' : end_time , 'trim_start' : trim_start , 'trim_end' : trim_end } ) return ann_trimmed
11699	def serve ( self , sock , request_handler , error_handler , debug = False , request_timeout = 60 , ssl = None , request_max_size = None , reuse_port = False , loop = None , protocol = HttpProtocol , backlog = 100 , ** kwargs ) : if debug : loop . set_debug ( debug ) server = partial ( protocol , loop = loop , connections = self . connections , signal = self . signal , request_handler = request_handler , error_handler = error_handler , request_timeout = request_timeout , request_max_size = request_max_size , ) server_coroutine = loop . create_server ( server , host = None , port = None , ssl = ssl , reuse_port = reuse_port , sock = sock , backlog = backlog ) loop . call_soon ( partial ( update_current_time , loop ) ) return server_coroutine
11796	def min_conflicts_value ( csp , var , current ) : return argmin_random_tie ( csp . domains [ var ] , lambda val : csp . nconflicts ( var , val , current ) )
5772	def _bcrypt_verify ( certificate_or_public_key , signature , data , hash_algorithm , rsa_pss_padding = False ) : if hash_algorithm == 'raw' : digest = data else : hash_constant = { 'md5' : BcryptConst . BCRYPT_MD5_ALGORITHM , 'sha1' : BcryptConst . BCRYPT_SHA1_ALGORITHM , 'sha256' : BcryptConst . BCRYPT_SHA256_ALGORITHM , 'sha384' : BcryptConst . BCRYPT_SHA384_ALGORITHM , 'sha512' : BcryptConst . BCRYPT_SHA512_ALGORITHM } [ hash_algorithm ] digest = getattr ( hashlib , hash_algorithm ) ( data ) . digest ( ) padding_info = null ( ) flags = 0 if certificate_or_public_key . algorithm == 'rsa' : if rsa_pss_padding : flags = BcryptConst . BCRYPT_PAD_PSS padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_PSS_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) hash_buffer = buffer_from_unicode ( hash_constant ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info_struct . cbSalt = len ( digest ) else : flags = BcryptConst . BCRYPT_PAD_PKCS1 padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_PKCS1_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) if hash_algorithm == 'raw' : padding_info_struct . pszAlgId = null ( ) else : hash_buffer = buffer_from_unicode ( hash_constant ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info = cast ( bcrypt , 'void *' , padding_info_struct_pointer ) else : try : signature = algos . DSASignature . load ( signature ) . to_p1363 ( ) except ( ValueError , OverflowError , TypeError ) : raise SignatureError ( 'Signature is invalid' ) res = bcrypt . BCryptVerifySignature ( certificate_or_public_key . key_handle , padding_info , digest , len ( digest ) , signature , len ( signature ) , flags ) failure = res == BcryptConst . STATUS_INVALID_SIGNATURE failure = failure or res == BcryptConst . STATUS_INVALID_PARAMETER if failure : raise SignatureError ( 'Signature is invalid' ) handle_error ( res )
5494	def expand_mentions ( text , embed_names = True ) : if embed_names : mention_format = "@<{name} {url}>" else : mention_format = "@<{url}>" def handle_mention ( match ) : source = get_source_by_name ( match . group ( 1 ) ) if source is None : return "@{0}" . format ( match . group ( 1 ) ) return mention_format . format ( name = source . nick , url = source . url ) return short_mention_re . sub ( handle_mention , text )
4500	def _follow_next ( self , url ) : response = self . _json ( self . _get ( url ) , 200 ) data = response [ 'data' ] next_url = self . _get_attribute ( response , 'links' , 'next' ) while next_url is not None : response = self . _json ( self . _get ( next_url ) , 200 ) data . extend ( response [ 'data' ] ) next_url = self . _get_attribute ( response , 'links' , 'next' ) return data
5118	def get_queue_data ( self , queues = None , edge = None , edge_type = None , return_header = False ) : queues = _get_queues ( self . g , queues , edge , edge_type ) data = np . zeros ( ( 0 , 6 ) ) for q in queues : dat = self . edge2queue [ q ] . fetch_data ( ) if len ( dat ) > 0 : data = np . vstack ( ( data , dat ) ) if return_header : return data , 'arrival,service,departure,num_queued,num_total,q_id' return data
10236	def get_graphs_by_ids ( self , network_ids : Iterable [ int ] ) -> List [ BELGraph ] : return [ self . networks [ network_id ] for network_id in network_ids ]
12094	def proto_VC_50_MT_IV ( abf = exampleABF ) : swhlab . memtest . memtest ( abf ) swhlab . memtest . checkSweep ( abf ) swhlab . plot . save ( abf , tag = '02-check' , resize = False ) av1 , sd1 = swhlab . plot . IV ( abf , 1.2 , 1.4 , True , 'b' ) swhlab . plot . save ( abf , tag = 'iv' ) Xs = abf . clampValues ( 1.2 ) abf . saveThing ( [ Xs , av1 ] , '01_iv' )
11545	def set_pwm_frequency ( self , frequency , pin = None ) : if pin is None : self . _set_pwm_frequency ( frequency , None ) else : pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : self . _set_pwm_frequency ( frequency , pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
13232	def get_newcommand_macros ( tex_source ) : r macros = { } command = LatexCommand ( 'newcommand' , { 'name' : 'name' , 'required' : True , 'bracket' : '{' } , { 'name' : 'content' , 'required' : True , 'bracket' : '{' } ) for macro in command . parse ( tex_source ) : macros [ macro [ 'name' ] ] = macro [ 'content' ] return macros
5738	def enqueue ( self , f , * args , ** kwargs ) : task = Task ( uuid4 ( ) . hex , f , args , kwargs ) self . storage . put_task ( task ) return self . enqueue_task ( task )
10270	def get_unweighted_upstream_leaves ( graph : BELGraph , key : Optional [ str ] = None ) -> Iterable [ BaseEntity ] : if key is None : key = WEIGHT return filter_nodes ( graph , [ node_is_upstream_leaf , data_missing_key_builder ( key ) ] )
5672	def get_transfer_stop_pairs ( self ) : transfer_stop_pairs = [ ] previous_arrival_stop = None current_trip_id = None for leg in self . legs : if leg . trip_id is not None and leg . trip_id != current_trip_id and previous_arrival_stop is not None : transfer_stop_pair = ( previous_arrival_stop , leg . departure_stop ) transfer_stop_pairs . append ( transfer_stop_pair ) previous_arrival_stop = leg . arrival_stop current_trip_id = leg . trip_id return transfer_stop_pairs
11123	def remove_directory ( self , relativePath , removeFromSystem = False ) : relativePath = os . path . normpath ( relativePath ) parentDirInfoDict , errorMessage = self . get_parent_directory_info ( relativePath ) assert parentDirInfoDict is not None , errorMessage path , name = os . path . split ( relativePath ) if dict . __getitem__ ( parentDirInfoDict , 'directories' ) . get ( name , None ) is None : raise Exception ( "'%s' is not a registered directory in repository relative path '%s'" % ( name , path ) ) if removeFromSystem : for rp in self . walk_files_relative_path ( relativePath = relativePath ) : ap = os . path . join ( self . __path , relativePath , rp ) if not os . path . isfile ( ap ) : continue if not os . path . exists ( ap ) : continue if os . path . isfile ( ap ) : os . remove ( ap ) for rp in self . walk_directories_relative_path ( relativePath = relativePath ) : ap = os . path . join ( self . __path , relativePath , rp ) if not os . path . isdir ( ap ) : continue if not os . path . exists ( ap ) : continue if not len ( os . listdir ( ap ) ) : os . rmdir ( ap ) dict . __getitem__ ( parentDirInfoDict , 'directories' ) . pop ( name , None ) ap = os . path . join ( self . __path , relativePath ) if not os . path . isdir ( ap ) : if not len ( os . listdir ( ap ) ) : os . rmdir ( ap ) self . save ( )
12909	def from_json ( cls , fh ) : if isinstance ( fh , str ) : return cls ( json . loads ( fh ) ) else : return cls ( json . load ( fh ) )
3817	async def _pb_request ( self , endpoint , request_pb , response_pb ) : logger . debug ( 'Sending Protocol Buffer request %s:\n%s' , endpoint , request_pb ) res = await self . _base_request ( 'https://clients6.google.com/chat/v1/{}' . format ( endpoint ) , 'application/x-protobuf' , 'proto' , request_pb . SerializeToString ( ) ) try : response_pb . ParseFromString ( base64 . b64decode ( res . body ) ) except binascii . Error as e : raise exceptions . NetworkError ( 'Failed to decode base64 response: {}' . format ( e ) ) except google . protobuf . message . DecodeError as e : raise exceptions . NetworkError ( 'Failed to decode Protocol Buffer response: {}' . format ( e ) ) logger . debug ( 'Received Protocol Buffer response:\n%s' , response_pb ) status = response_pb . response_header . status if status != hangouts_pb2 . RESPONSE_STATUS_OK : description = response_pb . response_header . error_description raise exceptions . NetworkError ( 'Request failed with status {}: \'{}\'' . format ( status , description ) )
1456	def valid_path ( path ) : if path . endswith ( '*' ) : Log . debug ( 'Checking classpath entry suffix as directory: %s' , path [ : - 1 ] ) if os . path . isdir ( path [ : - 1 ] ) : return True return False Log . debug ( 'Checking classpath entry as directory: %s' , path ) if os . path . isdir ( path ) : return True else : Log . debug ( 'Checking classpath entry as file: %s' , path ) if os . path . isfile ( path ) : return True return False
6201	def simulate_timestamps_mix ( self , max_rates , populations , bg_rate , rs = None , seed = 1 , chunksize = 2 ** 16 , comp_filter = None , overwrite = False , skip_existing = False , scale = 10 , path = None , t_chunksize = None , timeslice = None ) : self . open_store_timestamp ( chunksize = chunksize , path = path ) rs = self . _get_group_randomstate ( rs , seed , self . ts_group ) if t_chunksize is None : t_chunksize = self . emission . chunkshape [ 1 ] timeslice_size = self . n_samples if timeslice is not None : timeslice_size = timeslice // self . t_step name = self . _get_ts_name_mix ( max_rates , populations , bg_rate , rs = rs ) kw = dict ( name = name , clk_p = self . t_step / scale , max_rates = max_rates , bg_rate = bg_rate , populations = populations , num_particles = self . num_particles , bg_particle = self . num_particles , overwrite = overwrite , chunksize = chunksize ) if comp_filter is not None : kw . update ( comp_filter = comp_filter ) try : self . _timestamps , self . _tparticles = ( self . ts_store . add_timestamps ( ** kw ) ) except ExistingArrayError as e : if skip_existing : print ( ' - Skipping already present timestamps array.' ) return else : raise e self . ts_group . _v_attrs [ 'init_random_state' ] = rs . get_state ( ) self . _timestamps . attrs [ 'init_random_state' ] = rs . get_state ( ) self . _timestamps . attrs [ 'PyBroMo' ] = __version__ ts_list , part_list = [ ] , [ ] bg_rates = [ None ] * ( len ( max_rates ) - 1 ) + [ bg_rate ] prev_time = 0 for i_start , i_end in iter_chunk_index ( timeslice_size , t_chunksize ) : curr_time = np . around ( i_start * self . t_step , decimals = 0 ) if curr_time > prev_time : print ( ' %.1fs' % curr_time , end = '' , flush = True ) prev_time = curr_time em_chunk = self . emission [ : , i_start : i_end ] times_chunk_s , par_index_chunk_s = self . _sim_timestamps_populations ( em_chunk , max_rates , populations , bg_rates , i_start , rs , scale ) ts_list . append ( times_chunk_s ) part_list . append ( par_index_chunk_s ) for ts , part in zip ( ts_list , part_list ) : self . _timestamps . append ( ts ) self . _tparticles . append ( part ) self . ts_group . _v_attrs [ 'last_random_state' ] = rs . get_state ( ) self . _timestamps . attrs [ 'last_random_state' ] = rs . get_state ( ) self . ts_store . h5file . flush ( )
11479	def _create_bitstream ( file_path , local_file , item_id , log_ind = None ) : checksum = _streaming_file_md5 ( file_path ) upload_token = session . communicator . generate_upload_token ( session . token , item_id , local_file , checksum ) if upload_token != '' : log_trace = 'Uploading bitstream from {0}' . format ( file_path ) session . communicator . perform_upload ( upload_token , local_file , filepath = file_path , itemid = item_id ) else : log_trace = 'Adding a bitstream link in this item to an existing ' 'bitstream from {0}' . format ( file_path ) if log_ind is not None : log_trace += log_ind print ( log_trace )
10630	def clone ( self ) : result = copy . copy ( self ) result . _compound_mfrs = copy . deepcopy ( self . _compound_mfrs ) return result
9027	def _width ( self ) : layout = self . _instruction . get ( GRID_LAYOUT ) if layout is not None : width = layout . get ( WIDTH ) if width is not None : return width return self . _instruction . number_of_consumed_meshes
11552	def analog_read ( self , pin ) : with self . data_lock : data = self . _command_handler . analog_response_table [ pin ] [ self . _command_handler . RESPONSE_TABLE_PIN_DATA_VALUE ] return data
7656	def validate ( self , strict = True ) : valid = True try : jsonschema . validate ( self . __json__ , self . __schema__ ) except jsonschema . ValidationError as invalid : if strict : raise SchemaError ( str ( invalid ) ) else : warnings . warn ( str ( invalid ) ) valid = False return valid
2583	def load ( cls , config : Optional [ Config ] = None ) : if cls . _dfk is not None : raise RuntimeError ( 'Config has already been loaded' ) if config is None : cls . _dfk = DataFlowKernel ( Config ( ) ) else : cls . _dfk = DataFlowKernel ( config ) return cls . _dfk
5635	def doc2md ( docstr , title , min_level = 1 , more_info = False , toc = True , maxdepth = 0 ) : text = doctrim ( docstr ) lines = text . split ( '\n' ) sections = find_sections ( lines ) if sections : level = min ( n for n , t in sections ) - 1 else : level = 1 shiftlevel = 0 if level < min_level : shiftlevel = min_level - level level = min_level sections = [ ( lev + shiftlevel , tit ) for lev , tit in sections ] head = next ( ( i for i , l in enumerate ( lines ) if is_heading ( l ) ) , 0 ) md = [ make_heading ( level , title ) , "" , ] + lines [ : head ] if toc : md += make_toc ( sections , maxdepth ) md += [ '' ] md += _doc2md ( lines [ head : ] , shiftlevel ) if more_info : return ( md , sections ) else : return "\n" . join ( md )
9448	def hangup_call ( self , call_params ) : path = '/' + self . api_version + '/HangupCall/' method = 'POST' return self . request ( path , method , call_params )
8937	def get_pypi_auth ( configfile = '~/.pypirc' ) : pypi_cfg = ConfigParser ( ) if pypi_cfg . read ( os . path . expanduser ( configfile ) ) : try : user = pypi_cfg . get ( 'pypi' , 'username' ) pwd = pypi_cfg . get ( 'pypi' , 'password' ) return user , pwd except ConfigError : notify . warning ( "No PyPI credentials in '{}'," " will fall back to '~/.netrc'..." . format ( configfile ) ) return None
6849	def needs_initrole ( self , stop_on_error = False ) : ret = False target_host_present = self . is_present ( ) if not target_host_present : default_host_present = self . is_present ( self . env . default_hostname ) if default_host_present : if self . verbose : print ( 'Target host missing and default host present so host init required.' ) ret = True else : if self . verbose : print ( 'Target host missing but default host also missing, ' 'so no host init required.' ) else : if self . verbose : print ( 'Target host is present so no host init required.' ) return ret
3111	def locked_get ( self ) : serialized = self . _dictionary . get ( self . _key ) if serialized is None : return None credentials = client . OAuth2Credentials . from_json ( serialized ) credentials . set_store ( self ) return credentials
9916	def validate_is_primary ( self , is_primary ) : if is_primary and not ( self . instance and self . instance . is_verified ) : raise serializers . ValidationError ( _ ( "Unverified email addresses may not be used as the " "primary address." ) ) return is_primary
12024	def adopt ( self , old_parent , new_parent ) : try : old_id = old_parent [ 'attributes' ] [ 'ID' ] except TypeError : try : old_id = self . lines [ old_parent ] [ 'attributes' ] [ 'ID' ] except TypeError : old_id = old_parent old_feature = self . features [ old_id ] old_indexes = [ ld [ 'line_index' ] for ld in old_feature ] try : new_id = new_parent [ 'attributes' ] [ 'ID' ] except TypeError : try : new_id = self . lines [ new_parent ] [ 'attributes' ] [ 'ID' ] except TypeError : new_id = new_parent new_feature = self . features [ new_id ] new_indexes = [ ld [ 'line_index' ] for ld in new_feature ] children = old_feature [ 0 ] [ 'children' ] new_parent_children_set = set ( [ ld [ 'line_index' ] for ld in new_feature [ 0 ] [ 'children' ] ] ) for child in children : if child [ 'line_index' ] not in new_parent_children_set : new_parent_children_set . add ( child [ 'line_index' ] ) for new_ld in new_feature : new_ld [ 'children' ] . append ( child ) child [ 'parents' ] . append ( new_feature ) child [ 'attributes' ] [ 'Parent' ] . append ( new_id ) child [ 'parents' ] = [ f for f in child [ 'parents' ] if f [ 0 ] [ 'attributes' ] [ 'ID' ] != old_id ] child [ 'attributes' ] [ 'Parent' ] = [ d for d in child [ 'attributes' ] [ 'Parent' ] if d != old_id ] for old_ld in old_feature : old_ld [ 'children' ] = [ ] return children
4995	def handle_user_post_save ( sender , ** kwargs ) : created = kwargs . get ( "created" , False ) user_instance = kwargs . get ( "instance" , None ) if user_instance is None : return try : pending_ecu = PendingEnterpriseCustomerUser . objects . get ( user_email = user_instance . email ) except PendingEnterpriseCustomerUser . DoesNotExist : return if not created : try : existing_record = EnterpriseCustomerUser . objects . get ( user_id = user_instance . id ) message_template = "User {user} have changed email to match pending Enterprise Customer link, " "but was already linked to Enterprise Customer {enterprise_customer} - " "deleting pending link record" logger . info ( message_template . format ( user = user_instance , enterprise_customer = existing_record . enterprise_customer ) ) pending_ecu . delete ( ) return except EnterpriseCustomerUser . DoesNotExist : pass enterprise_customer_user = EnterpriseCustomerUser . objects . create ( enterprise_customer = pending_ecu . enterprise_customer , user_id = user_instance . id ) pending_enrollments = list ( pending_ecu . pendingenrollment_set . all ( ) ) if pending_enrollments : def _complete_user_enrollment ( ) : for enrollment in pending_enrollments : enterprise_customer_user . enroll ( enrollment . course_id , enrollment . course_mode , cohort = enrollment . cohort_name ) track_enrollment ( 'pending-admin-enrollment' , user_instance . id , enrollment . course_id ) pending_ecu . delete ( ) transaction . on_commit ( _complete_user_enrollment ) else : pending_ecu . delete ( )
8644	def update_track ( session , track_id , latitude , longitude , stop_tracking = False ) : tracking_data = { 'track_point' : { 'latitude' : latitude , 'longitude' : longitude , } , 'stop_tracking' : stop_tracking } response = make_put_request ( session , 'tracks/{}' . format ( track_id ) , json_data = tracking_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise TrackNotUpdatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
11526	def create_small_thumbnail ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'itemId' ] = item_id response = self . request ( 'midas.thumbnailcreator.create.small.thumbnail' , parameters ) return response
5090	def export_as_csv_action ( description = "Export selected objects as CSV file" , fields = None , header = True ) : def export_as_csv ( modeladmin , request , queryset ) : opts = modeladmin . model . _meta if not fields : field_names = [ field . name for field in opts . fields ] else : field_names = fields response = HttpResponse ( content_type = "text/csv" ) response [ "Content-Disposition" ] = "attachment; filename={filename}.csv" . format ( filename = str ( opts ) . replace ( "." , "_" ) ) writer = unicodecsv . writer ( response , encoding = "utf-8" ) if header : writer . writerow ( field_names ) for obj in queryset : row = [ ] for field_name in field_names : field = getattr ( obj , field_name ) if callable ( field ) : value = field ( ) else : value = field if value is None : row . append ( "[Not Set]" ) elif not value and isinstance ( value , string_types ) : row . append ( "[Empty]" ) else : row . append ( value ) writer . writerow ( row ) return response export_as_csv . short_description = description return export_as_csv
1766	def decode_instruction ( self , pc ) : if pc in self . _instruction_cache : return self . _instruction_cache [ pc ] text = b'' for address in range ( pc , pc + self . max_instr_width ) : if not self . memory . access_ok ( address , 'x' ) : break c = self . memory [ address ] if issymbolic ( c ) : if isinstance ( self . memory , LazySMemory ) : try : vals = visitors . simplify_array_select ( c ) c = bytes ( [ vals [ 0 ] ] ) except visitors . ArraySelectSimplifier . ExpressionNotSimple : c = struct . pack ( 'B' , solver . get_value ( self . memory . constraints , c ) ) elif isinstance ( c , Constant ) : c = bytes ( [ c . value ] ) else : logger . error ( 'Concretize executable memory %r %r' , c , text ) raise ConcretizeMemory ( self . memory , address = pc , size = 8 * self . max_instr_width , policy = 'INSTRUCTION' ) text += c code = text . ljust ( self . max_instr_width , b'\x00' ) try : insn = self . disasm . disassemble_instruction ( code , pc ) except StopIteration as e : raise DecodeException ( pc , code ) if not self . memory . access_ok ( slice ( pc , pc + insn . size ) , 'x' ) : logger . info ( "Trying to execute instructions from non-executable memory" ) raise InvalidMemoryAccess ( pc , 'x' ) insn . operands = self . _wrap_operands ( insn . operands ) self . _instruction_cache [ pc ] = insn return insn
6328	def _add_to_ngcorpus ( self , corpus , words , count ) : if words [ 0 ] not in corpus : corpus [ words [ 0 ] ] = Counter ( ) if len ( words ) == 1 : corpus [ words [ 0 ] ] [ None ] += count else : self . _add_to_ngcorpus ( corpus [ words [ 0 ] ] , words [ 1 : ] , count )
11716	def edit ( self , config , etag ) : data = self . _json_encode ( config ) headers = self . _default_headers ( ) if etag is not None : headers [ "If-Match" ] = etag return self . _request ( self . name , ok_status = None , data = data , headers = headers , method = "PUT" )
8820	def delete_network ( context , id ) : LOG . info ( "delete_network %s for tenant %s" % ( id , context . tenant_id ) ) with context . session . begin ( ) : net = db_api . network_find ( context = context , limit = None , sorts = [ 'id' ] , marker = None , page_reverse = False , id = id , scope = db_api . ONE ) if not net : raise n_exc . NetworkNotFound ( net_id = id ) if not context . is_admin : if STRATEGY . is_provider_network ( net . id ) : raise n_exc . NotAuthorized ( net_id = id ) if net . ports : raise n_exc . NetworkInUse ( net_id = id ) net_driver = registry . DRIVER_REGISTRY . get_driver ( net [ "network_plugin" ] ) net_driver . delete_network ( context , id ) for subnet in net [ "subnets" ] : subnets . _delete_subnet ( context , subnet ) db_api . network_delete ( context , net )
3099	def _validate_clientsecrets ( clientsecrets_dict ) : _INVALID_FILE_FORMAT_MSG = ( 'Invalid file format. See ' 'https://developers.google.com/api-client-library/' 'python/guide/aaa_client_secrets' ) if clientsecrets_dict is None : raise InvalidClientSecretsError ( _INVALID_FILE_FORMAT_MSG ) try : ( client_type , client_info ) , = clientsecrets_dict . items ( ) except ( ValueError , AttributeError ) : raise InvalidClientSecretsError ( _INVALID_FILE_FORMAT_MSG + ' ' 'Expected a JSON object with a single property for a "web" or ' '"installed" application' ) if client_type not in VALID_CLIENT : raise InvalidClientSecretsError ( 'Unknown client type: {0}.' . format ( client_type ) ) for prop_name in VALID_CLIENT [ client_type ] [ 'required' ] : if prop_name not in client_info : raise InvalidClientSecretsError ( 'Missing property "{0}" in a client type of "{1}".' . format ( prop_name , client_type ) ) for prop_name in VALID_CLIENT [ client_type ] [ 'string' ] : if client_info [ prop_name ] . startswith ( '[[' ) : raise InvalidClientSecretsError ( 'Property "{0}" is not configured.' . format ( prop_name ) ) return client_type , client_info
12562	def get_unique_nonzeros ( arr ) : rois = np . unique ( arr ) rois = rois [ np . nonzero ( rois ) ] rois . sort ( ) return rois
9793	def _matches_patterns ( path , patterns ) : for glob in patterns : try : if PurePath ( path ) . match ( glob ) : return True except TypeError : pass return False
6468	def csi_wrap ( self , value , capname , * args ) : if isinstance ( value , str ) : value = value . encode ( 'utf-8' ) return b'' . join ( [ self . csi ( capname , * args ) , value , self . csi ( 'sgr0' ) , ] )
13415	def addlabel ( ax = None , toplabel = None , xlabel = None , ylabel = None , zlabel = None , clabel = None , cb = None , windowlabel = None , fig = None , axes = None ) : if ( axes is None ) and ( ax is not None ) : axes = ax if ( windowlabel is not None ) and ( fig is not None ) : fig . canvas . set_window_title ( windowlabel ) if fig is None : fig = _plt . gcf ( ) if fig is not None and axes is None : axes = fig . get_axes ( ) if axes == [ ] : logger . error ( 'No axes found!' ) if axes is not None : if toplabel is not None : axes . set_title ( toplabel ) if xlabel is not None : axes . set_xlabel ( xlabel ) if ylabel is not None : axes . set_ylabel ( ylabel ) if zlabel is not None : axes . set_zlabel ( zlabel ) if ( clabel is not None ) or ( cb is not None ) : if ( clabel is not None ) and ( cb is not None ) : cb . set_label ( clabel ) else : if clabel is None : logger . error ( 'Missing colorbar label' ) else : logger . error ( 'Missing colorbar instance' )
2302	def orient_undirected_graph ( self , data , graph ) : self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) self . arguments [ '{SCORE}' ] = self . scores [ self . score ] fe = DataFrame ( nx . adj_matrix ( graph , weight = None ) . todense ( ) ) fg = DataFrame ( 1 - fe . values ) results = self . _run_gies ( data , fixedGaps = fg , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
4269	def get_exif_data ( filename ) : logger = logging . getLogger ( __name__ ) img = _read_image ( filename ) try : exif = img . _getexif ( ) or { } except ZeroDivisionError : logger . warning ( 'Failed to read EXIF data.' ) return None data = { TAGS . get ( tag , tag ) : value for tag , value in exif . items ( ) } if 'GPSInfo' in data : try : data [ 'GPSInfo' ] = { GPSTAGS . get ( tag , tag ) : value for tag , value in data [ 'GPSInfo' ] . items ( ) } except AttributeError : logger = logging . getLogger ( __name__ ) logger . info ( 'Failed to get GPS Info' ) del data [ 'GPSInfo' ] return data
5279	def make_pmml_pipeline ( obj , active_fields = None , target_fields = None ) : steps = _filter_steps ( _get_steps ( obj ) ) pipeline = PMMLPipeline ( steps ) if active_fields is not None : pipeline . active_fields = numpy . asarray ( active_fields ) if target_fields is not None : pipeline . target_fields = numpy . asarray ( target_fields ) return pipeline
11218	def encode ( self ) -> str : payload = { } payload . update ( self . registered_claims ) payload . update ( self . payload ) return encode ( self . secret , payload , self . alg , self . header )
13871	def CanonicalPath ( path ) : path = os . path . normpath ( path ) path = os . path . abspath ( path ) path = os . path . normcase ( path ) return path
6029	def set_xy_labels ( units , kpc_per_arcsec , xlabelsize , ylabelsize , xyticksize ) : if units in 'arcsec' or kpc_per_arcsec is None : plt . xlabel ( 'x (arcsec)' , fontsize = xlabelsize ) plt . ylabel ( 'y (arcsec)' , fontsize = ylabelsize ) elif units in 'kpc' : plt . xlabel ( 'x (kpc)' , fontsize = xlabelsize ) plt . ylabel ( 'y (kpc)' , fontsize = ylabelsize ) else : raise exc . PlottingException ( 'The units supplied to the plotted are not a valid string (must be pixels | ' 'arcsec | kpc)' ) plt . tick_params ( labelsize = xyticksize )
9510	def subseq ( self , start , end ) : return Fasta ( self . id , self . seq [ start : end ] )
8121	def union ( self , b ) : mx , my = min ( self . x , b . x ) , min ( self . y , b . y ) return Bounds ( mx , my , max ( self . x + self . width , b . x + b . width ) - mx , max ( self . y + self . height , b . y + b . height ) - my )
6030	def grid_interpolate ( func ) : @ wraps ( func ) def wrapper ( profile , grid , grid_radial_minimum = None , * args , ** kwargs ) : if hasattr ( grid , "interpolator" ) : interpolator = grid . interpolator if grid . interpolator is not None : values = func ( profile , interpolator . interp_grid , grid_radial_minimum , * args , ** kwargs ) if values . ndim == 1 : return interpolator . interpolated_values_from_values ( values = values ) elif values . ndim == 2 : y_values = interpolator . interpolated_values_from_values ( values = values [ : , 0 ] ) x_values = interpolator . interpolated_values_from_values ( values = values [ : , 1 ] ) return np . asarray ( [ y_values , x_values ] ) . T return func ( profile , grid , grid_radial_minimum , * args , ** kwargs ) return wrapper
10762	def _wait_for_connection ( self , port ) : connected = False max_tries = 10 num_tries = 0 wait_time = 0.5 while not connected or num_tries >= max_tries : time . sleep ( wait_time ) try : af = socket . AF_INET addr = ( '127.0.0.1' , port ) sock = socket . socket ( af , socket . SOCK_STREAM ) sock . connect ( addr ) except socket . error : if sock : sock . close ( ) num_tries += 1 continue connected = True if not connected : print ( "Error connecting to sphinx searchd" , file = sys . stderr )
12215	def get_frame_locals ( stepback = 0 ) : with Frame ( stepback = stepback ) as frame : locals_dict = frame . f_locals return locals_dict
7791	def purge_items ( self ) : self . _lock . acquire ( ) try : il = self . _items_list num_items = len ( il ) need_remove = num_items - int ( 0.75 * self . max_items ) for _unused in range ( need_remove ) : item = il . pop ( 0 ) try : del self . _items [ item . address ] except KeyError : pass while il and il [ 0 ] . update_state ( ) == "purged" : item = il . pop ( 0 ) try : del self . _items [ item . address ] except KeyError : pass finally : self . _lock . release ( )
3512	def optimizely ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return OptimizelyNode ( )
1580	def send ( self , dispatcher ) : if self . sent_complete : return sent = dispatcher . send ( self . to_send ) self . to_send = self . to_send [ sent : ]
12595	def get_aad_token ( endpoint , no_verify ) : from azure . servicefabric . service_fabric_client_ap_is import ( ServiceFabricClientAPIs ) from sfctl . auth import ClientCertAuthentication from sfctl . config import set_aad_metadata auth = ClientCertAuthentication ( None , None , no_verify ) client = ServiceFabricClientAPIs ( auth , base_url = endpoint ) aad_metadata = client . get_aad_metadata ( ) if aad_metadata . type != "aad" : raise CLIError ( "Not AAD cluster" ) aad_resource = aad_metadata . metadata tenant_id = aad_resource . tenant authority_uri = aad_resource . login + '/' + tenant_id context = adal . AuthenticationContext ( authority_uri , api_version = None ) cluster_id = aad_resource . cluster client_id = aad_resource . client set_aad_metadata ( authority_uri , cluster_id , client_id ) code = context . acquire_user_code ( cluster_id , client_id ) print ( code [ 'message' ] ) token = context . acquire_token_with_device_code ( cluster_id , code , client_id ) print ( "Succeed!" ) return token , context . cache
7749	def process_iq ( self , stanza ) : typ = stanza . stanza_type if typ in ( "result" , "error" ) : return self . _process_iq_response ( stanza ) if typ not in ( "get" , "set" ) : raise BadRequestProtocolError ( "Bad <iq/> type" ) logger . debug ( "Handling <iq type='{0}'> stanza: {1!r}" . format ( stanza , typ ) ) payload = stanza . get_payload ( None ) logger . debug ( " payload: {0!r}" . format ( payload ) ) if not payload : raise BadRequestProtocolError ( "<iq/> stanza with no child element" ) handler = self . _get_iq_handler ( typ , payload ) if not handler : payload = stanza . get_payload ( None , specialize = True ) logger . debug ( " specialized payload: {0!r}" . format ( payload ) ) if not isinstance ( payload , XMLPayload ) : handler = self . _get_iq_handler ( typ , payload ) if handler : response = handler ( stanza ) self . _process_handler_result ( response ) return True else : raise ServiceUnavailableProtocolError ( "Not implemented" )
8055	def trusted_cmd ( f ) : def run_cmd ( self , line ) : if self . trusted : f ( self , line ) else : print ( "Sorry cannot do %s here." % f . __name__ [ 3 : ] ) global trusted_cmds trusted_cmds . add ( f . __name__ ) run_cmd . __doc__ = f . __doc__ return run_cmd
8708	def __got_ack ( self ) : log . debug ( 'waiting for ack' ) res = self . _port . read ( 1 ) log . debug ( 'ack read %s' , hexify ( res ) ) return res == ACK
2097	def cancel ( self , pk = None , fail_if_not_running = False , ** kwargs ) : if not pk : existing_data = self . get ( ** kwargs ) pk = existing_data [ 'id' ] cancel_endpoint = '%s%s/cancel/' % ( self . endpoint , pk ) try : client . post ( cancel_endpoint ) changed = True except exc . MethodNotAllowed : changed = False if fail_if_not_running : raise exc . TowerCLIError ( 'Job not running.' ) return { 'status' : 'canceled' , 'changed' : changed }
2616	def initialize_boto_client ( self ) : self . session = self . create_session ( ) self . client = self . session . client ( 'ec2' ) self . ec2 = self . session . resource ( 'ec2' ) self . instances = [ ] self . instance_states = { } self . vpc_id = 0 self . sg_id = 0 self . sn_ids = [ ]
12472	def get_extension ( filepath , check_if_exists = False , allowed_exts = ALLOWED_EXTS ) : if check_if_exists : if not op . exists ( filepath ) : raise IOError ( 'File not found: ' + filepath ) rest , ext = op . splitext ( filepath ) if ext in allowed_exts : alloweds = allowed_exts [ ext ] _ , ext2 = op . splitext ( rest ) if ext2 in alloweds : ext = ext2 + ext return ext
3209	def get_load_balancer ( load_balancer , flags = FLAGS . ALL ^ FLAGS . POLICY_TYPES , ** conn ) : try : basestring except NameError as _ : basestring = str if isinstance ( load_balancer , basestring ) : load_balancer = dict ( LoadBalancerName = load_balancer ) return registry . build_out ( flags , start_with = load_balancer , pass_datastructure = True , ** conn )
5036	def _handle_singular ( cls , enterprise_customer , manage_learners_form ) : form_field_value = manage_learners_form . cleaned_data [ ManageLearnersForm . Fields . EMAIL_OR_USERNAME ] email = email_or_username__to__email ( form_field_value ) try : validate_email_to_link ( email , form_field_value , ValidationMessages . INVALID_EMAIL_OR_USERNAME , True ) except ValidationError as exc : manage_learners_form . add_error ( ManageLearnersForm . Fields . EMAIL_OR_USERNAME , exc ) else : EnterpriseCustomerUser . objects . link_user ( enterprise_customer , email ) return [ email ]
1911	def GetNBits ( value , nbits ) : if isinstance ( value , int ) : return Operators . EXTRACT ( value , 0 , nbits ) elif isinstance ( value , BitVec ) : if value . size < nbits : return Operators . ZEXTEND ( value , nbits ) else : return Operators . EXTRACT ( value , 0 , nbits )
4044	def publications ( self ) : if self . library_type != "users" : raise ze . CallDoesNotExist ( "This API call does not exist for group libraries" ) query_string = "/{t}/{u}/publications/items" return self . _build_query ( query_string )
2601	def start ( self ) : if self . mode == "manual" : return if self . ipython_dir != '~/.ipython' : self . ipython_dir = os . path . abspath ( os . path . expanduser ( self . ipython_dir ) ) if self . log : stdout = open ( os . path . join ( self . ipython_dir , "{0}.controller.out" . format ( self . profile ) ) , 'w' ) stderr = open ( os . path . join ( self . ipython_dir , "{0}.controller.err" . format ( self . profile ) ) , 'w' ) else : stdout = open ( os . devnull , 'w' ) stderr = open ( os . devnull , 'w' ) try : opts = [ 'ipcontroller' , '' if self . ipython_dir == '~/.ipython' else '--ipython-dir={}' . format ( self . ipython_dir ) , self . interfaces if self . interfaces is not None else '--ip=*' , '' if self . profile == 'default' else '--profile={0}' . format ( self . profile ) , '--reuse' if self . reuse else '' , '--location={}' . format ( self . public_ip ) if self . public_ip else '' , '--port={}' . format ( self . port ) if self . port is not None else '' ] if self . port_range is not None : opts += [ '--HubFactory.hb={0},{1}' . format ( self . hb_ping , self . hb_pong ) , '--HubFactory.control={0},{1}' . format ( self . control_client , self . control_engine ) , '--HubFactory.mux={0},{1}' . format ( self . mux_client , self . mux_engine ) , '--HubFactory.task={0},{1}' . format ( self . task_client , self . task_engine ) ] logger . debug ( "Starting ipcontroller with '{}'" . format ( ' ' . join ( [ str ( x ) for x in opts ] ) ) ) self . proc = subprocess . Popen ( opts , stdout = stdout , stderr = stderr , preexec_fn = os . setsid ) except FileNotFoundError : msg = "Could not find ipcontroller. Please make sure that ipyparallel is installed and available in your env" logger . error ( msg ) raise ControllerError ( msg ) except Exception as e : msg = "IPPController failed to start: {0}" . format ( e ) logger . error ( msg ) raise ControllerError ( msg )
7010	def skyview_stamp ( ra , decl , survey = 'DSS2 Red' , scaling = 'Linear' , flip = True , convolvewith = None , forcefetch = False , cachedir = '~/.astrobase/stamp-cache' , timeout = 10.0 , retry_failed = False , savewcsheader = True , verbose = False ) : stampdict = get_stamp ( ra , decl , survey = survey , scaling = scaling , forcefetch = forcefetch , cachedir = cachedir , timeout = timeout , retry_failed = retry_failed , verbose = verbose ) if stampdict : stampfits = pyfits . open ( stampdict [ 'fitsfile' ] ) header = stampfits [ 0 ] . header frame = stampfits [ 0 ] . data stampfits . close ( ) if flip : frame = np . flipud ( frame ) if verbose : LOGINFO ( 'fetched stamp successfully for (%.3f, %.3f)' % ( ra , decl ) ) if convolvewith : convolved = aconv . convolve ( frame , convolvewith ) if savewcsheader : return convolved , header else : return convolved else : if savewcsheader : return frame , header else : return frame else : LOGERROR ( 'could not fetch the requested stamp for ' 'coords: (%.3f, %.3f) from survey: %s and scaling: %s' % ( ra , decl , survey , scaling ) ) return None
1890	def minmax ( self , constraints , x , iters = 10000 ) : if issymbolic ( x ) : m = self . min ( constraints , x , iters ) M = self . max ( constraints , x , iters ) return m , M else : return x , x
10814	def add_member ( self , user , state = MembershipState . ACTIVE ) : return Membership . create ( self , user , state )
13853	def run ( self ) : if not self . device : return try : data = "" while ( self . do_run ) : try : if ( self . device . inWaiting ( ) > 1 ) : l = self . device . readline ( ) [ : - 2 ] l = l . decode ( "UTF-8" ) if ( l == "[" ) : data = "[" elif ( l == "]" ) and ( len ( data ) > 4 ) and ( data [ 0 ] == "[" ) : data = data + "]" self . store . register_json ( data ) self . age ( ) elif ( l [ 0 : 3 ] == " {" ) : data = data + " " + l else : sleep ( 1 ) self . age ( ) except ( UnicodeDecodeError , ValueError ) : data = "" self . age ( ) except serial . serialutil . SerialException : print ( "Could not connect to the serial line at " + self . device_name )
1179	def split ( self , string , maxsplit = 0 ) : splitlist = [ ] state = _State ( string , 0 , sys . maxint , self . flags ) n = 0 last = state . start while not maxsplit or n < maxsplit : state . reset ( ) state . string_position = state . start if not state . search ( self . _code ) : break if state . start == state . string_position : if last == state . end : break state . start += 1 continue splitlist . append ( string [ last : state . start ] ) if self . groups : match = SRE_Match ( self , state ) splitlist += ( list ( match . groups ( None ) ) ) n += 1 last = state . start = state . string_position splitlist . append ( string [ last : state . end ] ) return splitlist
5882	def nodes_to_check ( self , docs ) : nodes_to_check = [ ] for doc in docs : for tag in [ 'p' , 'pre' , 'td' ] : items = self . parser . getElementsByTag ( doc , tag = tag ) nodes_to_check += items return nodes_to_check
12714	def connect_to ( self , joint , other_body , offset = ( 0 , 0 , 0 ) , other_offset = ( 0 , 0 , 0 ) , ** kwargs ) : anchor = self . world . move_next_to ( self , other_body , offset , other_offset ) self . world . join ( joint , self , other_body , anchor = anchor , ** kwargs )
11856	def extender ( self , edge ) : "See what edges can be extended by this edge." ( j , k , B , _ , _ ) = edge for ( i , j , A , alpha , B1b ) in self . chart [ j ] : if B1b and B == B1b [ 0 ] : self . add_edge ( [ i , k , A , alpha + [ edge ] , B1b [ 1 : ] ] )
7325	def cli ( sample , dry_run , limit , no_limit , database_filename , template_filename , config_filename ) : mailmerge . api . main ( sample = sample , dry_run = dry_run , limit = limit , no_limit = no_limit , database_filename = database_filename , template_filename = template_filename , config_filename = config_filename , )
3965	def restart_apps_or_services ( app_or_service_names = None ) : if app_or_service_names : log_to_client ( "Restarting the following apps or services: {}" . format ( ', ' . join ( app_or_service_names ) ) ) else : log_to_client ( "Restarting all active containers associated with Dusty" ) if app_or_service_names : specs = spec_assembler . get_assembled_specs ( ) specs_list = [ specs [ 'apps' ] [ app_name ] for app_name in app_or_service_names if app_name in specs [ 'apps' ] ] repos = set ( ) for spec in specs_list : if spec [ 'repo' ] : repos = repos . union ( spec_assembler . get_same_container_repos_from_spec ( spec ) ) nfs . update_nfs_with_repos ( repos ) else : nfs . update_nfs_with_repos ( spec_assembler . get_all_repos ( active_only = True , include_specs_repo = False ) ) compose . restart_running_services ( app_or_service_names )
6563	def load_cnf ( fp ) : fp = iter ( fp ) csp = ConstraintSatisfactionProblem ( dimod . BINARY ) num_clauses = num_variables = 0 problem_pattern = re . compile ( _PROBLEM_REGEX ) for line in fp : matches = problem_pattern . findall ( line ) if matches : if len ( matches ) > 1 : raise ValueError nv , nc = matches [ 0 ] num_variables , num_clauses = int ( nv ) , int ( nc ) break clause_pattern = re . compile ( _CLAUSE_REGEX ) for line in fp : if clause_pattern . match ( line ) is not None : clause = [ int ( v ) for v in line . split ( ' ' ) [ : - 1 ] ] variables = [ abs ( v ) for v in clause ] f = _cnf_or ( clause ) csp . add_constraint ( f , variables ) for v in range ( 1 , num_variables + 1 ) : csp . add_variable ( v ) for v in csp . variables : if v > num_variables : msg = ( "given .cnf file's header defines variables [1, {}] and {} clauses " "but constraints a reference to variable {}" ) . format ( num_variables , num_clauses , v ) raise ValueError ( msg ) if len ( csp ) != num_clauses : msg = ( "given .cnf file's header defines {} " "clauses but the file contains {}" ) . format ( num_clauses , len ( csp ) ) raise ValueError ( msg ) return csp
12351	def restore ( self , image , wait = True ) : return self . _action ( 'restore' , image = image , wait = wait )
5142	def make_index ( self ) : for prev , block in zip ( self . blocks [ : - 1 ] , self . blocks [ 1 : ] ) : if not block . is_comment : self . index [ block . start_lineno ] = prev
1011	def trimSegments ( self , minPermanence = None , minNumSyns = None ) : if minPermanence is None : minPermanence = self . connectedPerm if minNumSyns is None : minNumSyns = self . activationThreshold totalSegsRemoved , totalSynsRemoved = 0 , 0 for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : ( segsRemoved , synsRemoved ) = self . _trimSegmentsInCell ( colIdx = c , cellIdx = i , segList = self . cells [ c ] [ i ] , minPermanence = minPermanence , minNumSyns = minNumSyns ) totalSegsRemoved += segsRemoved totalSynsRemoved += synsRemoved if self . verbosity >= 5 : print "Cells, all segments:" self . printCells ( predictedOnly = False ) return totalSegsRemoved , totalSynsRemoved
2018	def MOD ( self , a , b ) : try : result = Operators . ITEBV ( 256 , b == 0 , 0 , a % b ) except ZeroDivisionError : result = 0 return result
4116	def lsf2poly ( lsf ) : lsf = numpy . array ( lsf ) if max ( lsf ) > numpy . pi or min ( lsf ) < 0 : raise ValueError ( 'Line spectral frequencies must be between 0 and pi.' ) p = len ( lsf ) z = numpy . exp ( 1.j * lsf ) rQ = z [ 0 : : 2 ] rP = z [ 1 : : 2 ] rQ = numpy . concatenate ( ( rQ , rQ . conjugate ( ) ) ) rP = numpy . concatenate ( ( rP , rP . conjugate ( ) ) ) Q = numpy . poly ( rQ ) P = numpy . poly ( rP ) if p % 2 : P1 = numpy . convolve ( P , [ 1 , 0 , - 1 ] ) Q1 = Q else : P1 = numpy . convolve ( P , [ 1 , - 1 ] ) Q1 = numpy . convolve ( Q , [ 1 , 1 ] ) a = .5 * ( P1 + Q1 ) return a [ 0 : - 1 : 1 ]
3489	def _sbase_notes_dict ( sbase , notes ) : if notes and len ( notes ) > 0 : tokens = [ '<html xmlns = "http://www.w3.org/1999/xhtml" >' ] + [ "<p>{}: {}</p>" . format ( k , v ) for ( k , v ) in notes . items ( ) ] + [ "</html>" ] _check ( sbase . setNotes ( "\n" . join ( tokens ) ) , "Setting notes on sbase: {}" . format ( sbase ) )
2026	def BYTE ( self , offset , value ) : offset = Operators . ITEBV ( 256 , offset < 32 , ( 31 - offset ) * 8 , 256 ) return Operators . ZEXTEND ( Operators . EXTRACT ( value , offset , 8 ) , 256 )
1511	def start_heron_tools ( masters , cl_args ) : single_master = list ( masters ) [ 0 ] wait_for_master_to_start ( single_master ) cmd = "%s run %s >> /tmp/heron_tools_start.log 2>&1 &" % ( get_nomad_path ( cl_args ) , get_heron_tools_job_file ( cl_args ) ) Log . info ( "Starting Heron Tools on %s" % single_master ) if not is_self ( single_master ) : cmd = ssh_remote_execute ( cmd , single_master , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) if return_code != 0 : Log . error ( "Failed to start Heron Tools on %s with error:\n%s" % ( single_master , output [ 1 ] ) ) sys . exit ( - 1 ) wait_for_job_to_start ( single_master , "heron-tools" ) Log . info ( "Done starting Heron Tools" )
11070	def proxy_factory ( BaseSchema , label , ProxiedClass , get_key ) : def local ( ) : key = get_key ( ) try : return proxies [ BaseSchema ] [ label ] [ key ] except KeyError : proxies [ BaseSchema ] [ label ] [ key ] = ProxiedClass ( ) return proxies [ BaseSchema ] [ label ] [ key ] return LocalProxy ( local )
7045	def all_nonperiodic_features ( times , mags , errs , magsarefluxes = False , stetson_weightbytimediff = True ) : finiteind = npisfinite ( times ) & npisfinite ( mags ) & npisfinite ( errs ) ftimes , fmags , ferrs = times [ finiteind ] , mags [ finiteind ] , errs [ finiteind ] nzind = npnonzero ( ferrs ) ftimes , fmags , ferrs = ftimes [ nzind ] , fmags [ nzind ] , ferrs [ nzind ] xfeatures = nonperiodic_lightcurve_features ( times , mags , errs , magsarefluxes = magsarefluxes ) stetj = stetson_jindex ( ftimes , fmags , ferrs , weightbytimediff = stetson_weightbytimediff ) stetk = stetson_kindex ( fmags , ferrs ) xfeatures . update ( { 'stetsonj' : stetj , 'stetsonk' : stetk } ) return xfeatures
11564	def servo_config ( self , pin , min_pulse = 544 , max_pulse = 2400 ) : self . set_pin_mode ( pin , self . SERVO , self . OUTPUT ) command = [ pin , min_pulse & 0x7f , ( min_pulse >> 7 ) & 0x7f , max_pulse & 0x7f , ( max_pulse >> 7 ) & 0x7f ] self . _command_handler . send_sysex ( self . _command_handler . SERVO_CONFIG , command )
6175	def init_app ( self , app ) : _state . _register_app = self . original_register_app if not hasattr ( app , 'extensions' ) : app . extensions = dict ( ) if 'celery' in app . extensions : raise ValueError ( 'Already registered extension CELERY.' ) app . extensions [ 'celery' ] = _CeleryState ( self , app ) super ( Celery , self ) . __init__ ( app . import_name , broker = app . config [ 'CELERY_BROKER_URL' ] ) if 'CELERY_RESULT_BACKEND' in app . config : self . _preconf [ 'CELERY_RESULT_BACKEND' ] = app . config [ 'CELERY_RESULT_BACKEND' ] self . conf . update ( app . config ) task_base = self . Task class ContextTask ( task_base ) : def __call__ ( self , * _args , ** _kwargs ) : with app . app_context ( ) : return task_base . __call__ ( self , * _args , ** _kwargs ) setattr ( ContextTask , 'abstract' , True ) setattr ( self , 'Task' , ContextTask )
6182	def hash ( self ) : hash_list = [ ] for key , value in sorted ( self . __dict__ . items ( ) ) : if not callable ( value ) : if isinstance ( value , np . ndarray ) : hash_list . append ( value . tostring ( ) ) else : hash_list . append ( str ( value ) ) return hashlib . md5 ( repr ( hash_list ) . encode ( ) ) . hexdigest ( )
10253	def remove_highlight_edges ( graph : BELGraph , edges = None ) : for u , v , k , _ in graph . edges ( keys = True , data = True ) if edges is None else edges : if is_edge_highlighted ( graph , u , v , k ) : del graph [ u ] [ v ] [ k ] [ EDGE_HIGHLIGHT ]
7977	def _post_connect ( self ) : if not self . initiator : if "plain" in self . auth_methods or "digest" in self . auth_methods : self . set_iq_get_handler ( "query" , "jabber:iq:auth" , self . auth_in_stage1 ) self . set_iq_set_handler ( "query" , "jabber:iq:auth" , self . auth_in_stage2 ) elif self . registration_callback : iq = Iq ( stanza_type = "get" ) iq . set_content ( Register ( ) ) self . set_response_handlers ( iq , self . registration_form_received , self . registration_error ) self . send ( iq ) return ClientStream . _post_connect ( self )
9193	def _insert_file ( cursor , file , media_type ) : resource_hash = _get_file_sha1 ( file ) cursor . execute ( "SELECT fileid FROM files WHERE sha1 = %s" , ( resource_hash , ) ) try : fileid = cursor . fetchone ( ) [ 0 ] except ( IndexError , TypeError ) : cursor . execute ( "INSERT INTO files (file, media_type) " "VALUES (%s, %s)" "RETURNING fileid" , ( psycopg2 . Binary ( file . read ( ) ) , media_type , ) ) fileid = cursor . fetchone ( ) [ 0 ] return fileid , resource_hash
6582	def play_station ( self , station ) : for song in iterate_forever ( station . get_playlist ) : try : self . play ( song ) except StopIteration : self . stop ( ) return
4495	def remove ( args ) : osf = _setup_osf ( args ) if osf . username is None or osf . password is None : sys . exit ( 'To remove a file you need to provide a username and' ' password.' ) project = osf . project ( args . project ) storage , remote_path = split_storage ( args . target ) store = project . storage ( storage ) for f in store . files : if norm_remote_path ( f . path ) == remote_path : f . remove ( )
5600	def for_web ( self , data ) : rgba = self . _prepare_array_for_png ( data ) data = ma . masked_where ( rgba == self . nodata , rgba ) return memory_file ( data , self . profile ( ) ) , 'image/png'
3291	def get_href ( self ) : safe = "/" + "!*'()," + "$-_|." return compat . quote ( self . provider . mount_path + self . provider . share_path + self . get_preferred_path ( ) , safe = safe , )
5902	def prehook ( self , ** kwargs ) : cmd = [ 'smpd' , '-s' ] logger . info ( "Starting smpd: " + " " . join ( cmd ) ) rc = subprocess . call ( cmd ) return rc
6760	def has_changes ( self ) : lm = self . last_manifest for tracker in self . get_trackers ( ) : last_thumbprint = lm [ '_tracker_%s' % tracker . get_natural_key_hash ( ) ] if tracker . is_changed ( last_thumbprint ) : return True return False
8580	def create_server ( self , datacenter_id , server ) : data = json . dumps ( self . _create_server_dict ( server ) ) response = self . _perform_request ( url = '/datacenters/%s/servers' % ( datacenter_id ) , method = 'POST' , data = data ) return response
5186	def aggregate_event_counts ( self , summarize_by , query = None , count_by = None , count_filter = None ) : return self . _query ( 'aggregate-event-counts' , query = query , summarize_by = summarize_by , count_by = count_by , count_filter = count_filter )
6782	def get_previous_thumbprint ( self , components = None ) : components = str_to_component_list ( components ) tp_fn = self . manifest_filename tp_text = None if self . file_exists ( tp_fn ) : fd = six . BytesIO ( ) get ( tp_fn , fd ) tp_text = fd . getvalue ( ) manifest_data = { } raw_data = yaml . load ( tp_text ) for k , v in raw_data . items ( ) : manifest_key = assert_valid_satchel ( k ) service_name = clean_service_name ( k ) if components and service_name not in components : continue manifest_data [ manifest_key ] = v return manifest_data
2761	def get_certificate ( self , id ) : return Certificate . get_object ( api_token = self . token , cert_id = id )
277	def plotting_context ( context = 'notebook' , font_scale = 1.5 , rc = None ) : if rc is None : rc = { } rc_default = { 'lines.linewidth' : 1.5 } for name , val in rc_default . items ( ) : rc . setdefault ( name , val ) return sns . plotting_context ( context = context , font_scale = font_scale , rc = rc )
12385	def parse ( text , encoding = 'utf8' ) : if isinstance ( text , six . binary_type ) : text = text . decode ( encoding ) return Query ( text , split_segments ( text ) )
5952	def start_logging ( logfile = "gromacs.log" ) : from . import log log . create ( "gromacs" , logfile = logfile ) logging . getLogger ( "gromacs" ) . info ( "GromacsWrapper %s STARTED logging to %r" , __version__ , logfile )
2267	def invert_dict ( dict_ , unique_vals = True ) : r if unique_vals : if isinstance ( dict_ , OrderedDict ) : inverted = OrderedDict ( ( val , key ) for key , val in dict_ . items ( ) ) else : inverted = { val : key for key , val in dict_ . items ( ) } else : inverted = defaultdict ( set ) for key , value in dict_ . items ( ) : inverted [ value ] . add ( key ) inverted = dict ( inverted ) return inverted
760	def appendInputWithNSimilarValues ( inputs , numNear = 10 ) : numInputs = len ( inputs ) skipOne = False for i in xrange ( numInputs ) : input = inputs [ i ] numChanged = 0 newInput = copy . deepcopy ( input ) for j in xrange ( len ( input ) - 1 ) : if skipOne : skipOne = False continue if input [ j ] == 1 and input [ j + 1 ] == 0 : newInput [ j ] = 0 newInput [ j + 1 ] = 1 inputs . append ( newInput ) newInput = copy . deepcopy ( newInput ) numChanged += 1 skipOne = True if numChanged == numNear : break
1565	def invoke_hook_spout_ack ( self , message_id , complete_latency_ns ) : if len ( self . task_hooks ) > 0 : spout_ack_info = SpoutAckInfo ( message_id = message_id , spout_task_id = self . get_task_id ( ) , complete_latency_ms = complete_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . spout_ack ( spout_ack_info )
12716	def position_rates ( self ) : return [ self . ode_obj . getPositionRate ( i ) for i in range ( self . LDOF ) ]
5443	def _validate_paths_or_fail ( uri , recursive ) : path , filename = os . path . split ( uri ) if '[' in uri or ']' in uri : raise ValueError ( 'Square bracket (character ranges) are not supported: %s' % uri ) if '?' in uri : raise ValueError ( 'Question mark wildcards are not supported: %s' % uri ) if '*' in path : raise ValueError ( 'Path wildcard (*) are only supported for files: %s' % uri ) if '**' in filename : raise ValueError ( 'Recursive wildcards ("**") not supported: %s' % uri ) if filename in ( '..' , '.' ) : raise ValueError ( 'Path characters ".." and "." not supported ' 'for file names: %s' % uri ) if not recursive and not filename : raise ValueError ( 'Input or output values that are not recursive must ' 'reference a filename or wildcard: %s' % uri )
7356	def create_input_peptides_files ( peptides , max_peptides_per_file = None , group_by_length = False ) : if group_by_length : peptide_lengths = { len ( p ) for p in peptides } peptide_groups = { l : [ ] for l in peptide_lengths } for p in peptides : peptide_groups [ len ( p ) ] . append ( p ) else : peptide_groups = { "" : peptides } file_names = [ ] for key , group in peptide_groups . items ( ) : n_peptides = len ( group ) if not max_peptides_per_file : max_peptides_per_file = n_peptides input_file = None for i , p in enumerate ( group ) : if i % max_peptides_per_file == 0 : if input_file is not None : file_names . append ( input_file . name ) input_file . close ( ) input_file = make_writable_tempfile ( prefix_number = i // max_peptides_per_file , prefix_name = key , suffix = ".txt" ) input_file . write ( "%s\n" % p ) if input_file is not None : file_names . append ( input_file . name ) input_file . close ( ) return file_names
8940	def _to_pypi ( self , docs_base , release ) : url = None with self . _zipped ( docs_base ) as handle : reply = requests . post ( self . params [ 'url' ] , auth = get_pypi_auth ( ) , allow_redirects = False , files = dict ( content = ( self . cfg . project . name + '.zip' , handle , 'application/zip' ) ) , data = { ':action' : 'doc_upload' , 'name' : self . cfg . project . name } ) if reply . status_code in range ( 200 , 300 ) : notify . info ( "{status_code} {reason}" . format ( ** vars ( reply ) ) ) elif reply . status_code == 301 : url = reply . headers [ 'location' ] else : data = self . cfg . copy ( ) data . update ( self . params ) data . update ( vars ( reply ) ) notify . error ( "{status_code} {reason} for POST to {url}" . format ( ** data ) ) return url
7576	def _call_structure ( mname , ename , sname , name , workdir , seed , ntaxa , nsites , kpop , rep ) : outname = os . path . join ( workdir , "{}-K-{}-rep-{}" . format ( name , kpop , rep ) ) cmd = [ "structure" , "-m" , mname , "-e" , ename , "-K" , str ( kpop ) , "-D" , str ( seed ) , "-N" , str ( ntaxa ) , "-L" , str ( nsites ) , "-i" , sname , "-o" , outname ] proc = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) comm = proc . communicate ( ) oldfiles = [ mname , ename , sname ] for oldfile in oldfiles : if os . path . exists ( oldfile ) : os . remove ( oldfile ) return comm
3400	def fill ( self , iterations = 1 ) : used_reactions = list ( ) for i in range ( iterations ) : self . model . slim_optimize ( error_value = None , message = 'gapfilling optimization failed' ) solution = [ self . model . reactions . get_by_id ( ind . rxn_id ) for ind in self . indicators if ind . _get_primal ( ) > self . integer_threshold ] if not self . validate ( solution ) : raise RuntimeError ( 'failed to validate gapfilled model, ' 'try lowering the integer_threshold' ) used_reactions . append ( solution ) self . update_costs ( ) return used_reactions
4605	def upgrade ( self ) : assert callable ( self . blockchain . upgrade_account ) return self . blockchain . upgrade_account ( account = self )
11222	def is_compressed_json_file ( abspath ) : abspath = abspath . lower ( ) fname , ext = os . path . splitext ( abspath ) if ext in [ ".json" , ".js" ] : is_compressed = False elif ext == ".gz" : is_compressed = True else : raise ValueError ( "'%s' is not a valid json file. " "extension has to be '.json' or '.js' for uncompressed, '.gz' " "for compressed." % abspath ) return is_compressed
7270	def use ( plugin ) : log . debug ( 'register new plugin: {}' . format ( plugin ) ) if inspect . isfunction ( plugin ) : return plugin ( Engine ) if plugin and hasattr ( plugin , 'register' ) : return plugin . register ( Engine ) raise ValueError ( 'invalid plugin: must be a function or ' 'implement register() method' )
4078	def write_py2k_header ( file_list ) : if not isinstance ( file_list , list ) : file_list = [ file_list ] python_re = re . compile ( br"^(#!.*\bpython)(.*)([\r\n]+)$" ) coding_re = re . compile ( br"coding[:=]\s*([-\w.]+)" ) new_line_re = re . compile ( br"([\r\n]+)$" ) version_3 = LooseVersion ( '3' ) for file in file_list : if not os . path . getsize ( file ) : continue rewrite_needed = False python_found = False coding_found = False lines = [ ] f = open ( file , 'rb' ) try : while len ( lines ) < 2 : line = f . readline ( ) match = python_re . match ( line ) if match : python_found = True version = LooseVersion ( match . group ( 2 ) . decode ( ) or '2' ) try : version_test = version >= version_3 except TypeError : version_test = True if version_test : line = python_re . sub ( br"\g<1>2\g<3>" , line ) rewrite_needed = True elif coding_re . search ( line ) : coding_found = True lines . append ( line ) if not coding_found : match = new_line_re . search ( lines [ 0 ] ) newline = match . group ( 1 ) if match else b"\n" line = b"# -*- coding: utf-8 -*-" + newline lines . insert ( 1 if python_found else 0 , line ) rewrite_needed = True if rewrite_needed : lines += f . readlines ( ) finally : f . close ( ) if rewrite_needed : f = open ( file , 'wb' ) try : f . writelines ( lines ) finally : f . close ( )
12244	def styblinski_tang ( theta ) : x , y = theta obj = 0.5 * ( x ** 4 - 16 * x ** 2 + 5 * x + y ** 4 - 16 * y ** 2 + 5 * y ) grad = np . array ( [ 2 * x ** 3 - 16 * x + 2.5 , 2 * y ** 3 - 16 * y + 2.5 , ] ) return obj , grad
3035	def flow_from_clientsecrets ( filename , scope , redirect_uri = None , message = None , cache = None , login_hint = None , device_uri = None , pkce = None , code_verifier = None , prompt = None ) : try : client_type , client_info = clientsecrets . loadfile ( filename , cache = cache ) if client_type in ( clientsecrets . TYPE_WEB , clientsecrets . TYPE_INSTALLED ) : constructor_kwargs = { 'redirect_uri' : redirect_uri , 'auth_uri' : client_info [ 'auth_uri' ] , 'token_uri' : client_info [ 'token_uri' ] , 'login_hint' : login_hint , } revoke_uri = client_info . get ( 'revoke_uri' ) optional = ( 'revoke_uri' , 'device_uri' , 'pkce' , 'code_verifier' , 'prompt' ) for param in optional : if locals ( ) [ param ] is not None : constructor_kwargs [ param ] = locals ( ) [ param ] return OAuth2WebServerFlow ( client_info [ 'client_id' ] , client_info [ 'client_secret' ] , scope , ** constructor_kwargs ) except clientsecrets . InvalidClientSecretsError as e : if message is not None : if e . args : message = ( 'The client secrets were invalid: ' '\n{0}\n{1}' . format ( e , message ) ) sys . exit ( message ) else : raise else : raise UnknownClientSecretsFlowError ( 'This OAuth 2.0 flow is unsupported: {0!r}' . format ( client_type ) )
8827	def update_ports_for_sg ( self , context , portid , jobid ) : port = db_api . port_find ( context , id = portid , scope = db_api . ONE ) if not port : LOG . warning ( "Port not found" ) return net_driver = port_api . _get_net_driver ( port . network , port = port ) base_net_driver = port_api . _get_net_driver ( port . network ) sg_list = [ sg for sg in port . security_groups ] success = False error = None retries = 3 retry_delay = 2 for retry in xrange ( retries ) : try : net_driver . update_port ( context , port_id = port [ "backend_key" ] , mac_address = port [ "mac_address" ] , device_id = port [ "device_id" ] , base_net_driver = base_net_driver , security_groups = sg_list ) success = True error = None break except Exception as error : LOG . warning ( "Could not connect to redis, but retrying soon" ) time . sleep ( retry_delay ) status_str = "" if not success : status_str = "Port %s update failed after %d tries. Error: %s" % ( portid , retries , error ) update_body = dict ( completed = True , status = status_str ) update_body = dict ( job = update_body ) job_api . update_job ( context . elevated ( ) , jobid , update_body )
4111	def rc2poly ( kr , r0 = None ) : from . levinson import levup p = len ( kr ) a = numpy . array ( [ 1 , kr [ 0 ] ] ) e = numpy . zeros ( len ( kr ) ) if r0 is None : e0 = 0 else : e0 = r0 e [ 0 ] = e0 * ( 1. - numpy . conj ( numpy . conjugate ( kr [ 0 ] ) * kr [ 0 ] ) ) for k in range ( 1 , p ) : [ a , e [ k ] ] = levup ( a , kr [ k ] , e [ k - 1 ] ) efinal = e [ - 1 ] return a , efinal
8073	def requirements ( debug = True , with_examples = True , with_pgi = None ) : reqs = list ( BASE_REQUIREMENTS ) if with_pgi is None : with_pgi = is_jython if debug : print ( "setup options: " ) print ( "with_pgi: " , "yes" if with_pgi else "no" ) print ( "with_examples: " , "yes" if with_examples else "no" ) if with_pgi : reqs . append ( "pgi" ) if debug : print ( "warning, as of April 2019 typography does not work with pgi" ) else : reqs . append ( PYGOBJECT ) if with_examples : reqs . extend ( EXAMPLE_REQUIREMENTS ) if debug : print ( "" ) print ( "" ) for req in reqs : print ( req ) return reqs
12412	def write ( self , chunk , serialize = False , format = None ) : self . require_not_closed ( ) if chunk is None : return if serialize or format is not None : self . serialize ( chunk , format = format ) return if type ( chunk ) is six . binary_type : self . _length += len ( chunk ) self . _stream . write ( chunk ) elif isinstance ( chunk , six . string_types ) : encoding = self . encoding if encoding is not None : chunk = chunk . encode ( encoding ) else : raise exceptions . InvalidOperation ( 'Attempting to write textual data without an encoding.' ) self . _length += len ( chunk ) self . _stream . write ( chunk ) elif isinstance ( chunk , collections . Iterable ) : for section in chunk : self . write ( section ) else : raise exceptions . InvalidOperation ( 'Attempting to write something not recognized.' )
6232	def calc_scene_bbox ( self ) : bbox_min , bbox_max = None , None for node in self . root_nodes : bbox_min , bbox_max = node . calc_global_bbox ( matrix44 . create_identity ( ) , bbox_min , bbox_max ) self . bbox_min = bbox_min self . bbox_max = bbox_max self . diagonal_size = vector3 . length ( self . bbox_max - self . bbox_min )
8138	def contrast ( self , value = 1.0 ) : c = ImageEnhance . Contrast ( self . img ) self . img = c . enhance ( value )
13245	async def _download_text ( url , session ) : logger = logging . getLogger ( __name__ ) async with session . get ( url ) as response : logger . info ( 'Downloading %r' , url ) return await response . text ( )
1970	def signal_receive ( self , fd ) : connections = self . connections if connections ( fd ) and self . twait [ connections ( fd ) ] : procid = random . sample ( self . twait [ connections ( fd ) ] , 1 ) [ 0 ] self . awake ( procid )
8558	def create_lan ( self , datacenter_id , lan ) : data = json . dumps ( self . _create_lan_dict ( lan ) ) response = self . _perform_request ( url = '/datacenters/%s/lans' % datacenter_id , method = 'POST' , data = data ) return response
5575	def available_input_formats ( ) : input_formats = [ ] for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : logger . debug ( "driver found: %s" , v ) driver_ = v . load ( ) if hasattr ( driver_ , "METADATA" ) and ( driver_ . METADATA [ "mode" ] in [ "r" , "rw" ] ) : input_formats . append ( driver_ . METADATA [ "driver_name" ] ) return input_formats
9471	def _xml ( self , root ) : element = root . createElement ( self . name ) keys = self . attrs . keys ( ) keys . sort ( ) for a in keys : element . setAttribute ( a , self . attrs [ a ] ) if self . body : text = root . createTextNode ( self . body ) element . appendChild ( text ) for c in self . elements : element . appendChild ( c . _xml ( root ) ) return element
7427	def refmap_init ( data , sample , force ) : sample . files . unmapped_reads = os . path . join ( data . dirs . edits , "{}-refmap_derep.fastq" . format ( sample . name ) ) sample . files . mapped_reads = os . path . join ( data . dirs . refmapping , "{}-mapped-sorted.bam" . format ( sample . name ) )
4338	def phaser ( self , gain_in = 0.8 , gain_out = 0.74 , delay = 3 , decay = 0.4 , speed = 0.5 , modulation_shape = 'sinusoidal' ) : if not is_number ( gain_in ) or gain_in <= 0 or gain_in > 1 : raise ValueError ( "gain_in must be a number between 0 and 1." ) if not is_number ( gain_out ) or gain_out <= 0 or gain_out > 1 : raise ValueError ( "gain_out must be a number between 0 and 1." ) if not is_number ( delay ) or delay <= 0 or delay > 5 : raise ValueError ( "delay must be a positive number." ) if not is_number ( decay ) or decay < 0.1 or decay > 0.5 : raise ValueError ( "decay must be a number between 0.1 and 0.5." ) if not is_number ( speed ) or speed < 0.1 or speed > 2 : raise ValueError ( "speed must be a positive number." ) if modulation_shape not in [ 'sinusoidal' , 'triangular' ] : raise ValueError ( "modulation_shape must be one of 'sinusoidal', 'triangular'." ) effect_args = [ 'phaser' , '{:f}' . format ( gain_in ) , '{:f}' . format ( gain_out ) , '{:f}' . format ( delay ) , '{:f}' . format ( decay ) , '{:f}' . format ( speed ) ] if modulation_shape == 'sinusoidal' : effect_args . append ( '-s' ) elif modulation_shape == 'triangular' : effect_args . append ( '-t' ) self . effects . extend ( effect_args ) self . effects_log . append ( 'phaser' ) return self
9972	def read_range ( filepath , range_expr , sheet = None , dict_generator = None ) : def default_generator ( cells ) : for row_ind , row in enumerate ( cells ) : for col_ind , cell in enumerate ( row ) : yield ( row_ind , col_ind ) , cell . value book = opxl . load_workbook ( filepath , data_only = True ) if _is_range_address ( range_expr ) : sheet_names = [ name . upper ( ) for name in book . sheetnames ] index = sheet_names . index ( sheet . upper ( ) ) cells = book . worksheets [ index ] [ range_expr ] else : cells = _get_namedrange ( book , range_expr , sheet ) if isinstance ( cells , opxl . cell . Cell ) : return cells . value if dict_generator is None : dict_generator = default_generator gen = dict_generator ( cells ) return { keyval [ 0 ] : keyval [ 1 ] for keyval in gen }
8793	def get_all ( self , model ) : tags = { } for name , tag in self . tags . items ( ) : for mtag in model . tags : if tag . is_tag ( mtag ) : tags [ name ] = tag . get ( model ) return tags
12185	def extract ( self , document , selector , debug_offset = '' ) : selected = self . select ( document , selector ) if selected is not None : if isinstance ( selected , ( list , tuple ) ) : if not len ( selected ) : return return [ self . _extract_single ( m ) for m in selected ] else : return self . _extract_single ( selected ) else : if self . DEBUG : print ( debug_offset , "selector did not match anything; return None" ) return None
6235	def get_time ( self ) -> float : if self . paused : return self . pause_time return mixer . music . get_pos ( ) / 1000.0
7818	def _update_handlers ( self ) : handler_map = defaultdict ( list ) for i , obj in enumerate ( self . handlers ) : for dummy , handler in inspect . getmembers ( obj , callable ) : if not hasattr ( handler , "_pyxmpp_event_handled" ) : continue event_class = handler . _pyxmpp_event_handled handler_map [ event_class ] . append ( ( i , handler ) ) self . _handler_map = handler_map
7760	def _call_timeout_handlers ( self ) : sources_handled = 0 now = time . time ( ) schedule = None while self . _timeout_handlers : schedule , handler = self . _timeout_handlers [ 0 ] if schedule <= now : logger . debug ( "About to call a timeout handler: {0!r}" . format ( handler ) ) self . _timeout_handlers = self . _timeout_handlers [ 1 : ] result = handler ( ) logger . debug ( " handler result: {0!r}" . format ( result ) ) rec = handler . _pyxmpp_recurring if rec : logger . debug ( " recurring, restarting in {0} s" . format ( handler . _pyxmpp_timeout ) ) self . _timeout_handlers . append ( ( now + handler . _pyxmpp_timeout , handler ) ) self . _timeout_handlers . sort ( key = lambda x : x [ 0 ] ) elif rec is None and result is not None : logger . debug ( " auto-recurring, restarting in {0} s" . format ( result ) ) self . _timeout_handlers . append ( ( now + result , handler ) ) self . _timeout_handlers . sort ( key = lambda x : x [ 0 ] ) sources_handled += 1 else : break if self . check_events ( ) : return 0 , sources_handled if self . _timeout_handlers and schedule : timeout = schedule - now else : timeout = None return timeout , sources_handled
10746	def get_default_fields ( self ) : field_names = self . _meta . get_all_field_names ( ) if 'id' in field_names : field_names . remove ( 'id' ) return field_names
11332	def err ( format_msg , * args , ** kwargs ) : exc_info = kwargs . pop ( "exc_info" , False ) stderr . warning ( str ( format_msg ) . format ( * args , ** kwargs ) , exc_info = exc_info )
4272	def create_output_directories ( self ) : check_or_create_dir ( self . dst_path ) if self . medias : check_or_create_dir ( join ( self . dst_path , self . settings [ 'thumb_dir' ] ) ) if self . medias and self . settings [ 'keep_orig' ] : self . orig_path = join ( self . dst_path , self . settings [ 'orig_dir' ] ) check_or_create_dir ( self . orig_path )
9294	def python_value ( self , value ) : value = coerce_to_bytes ( value ) obj = HashValue ( value ) obj . field = self return obj
7879	def add_prefix ( self , namespace , prefix ) : if prefix == "xml" and namespace != XML_NS : raise ValueError , "Cannot change 'xml' prefix meaning" self . _prefixes [ namespace ] = prefix
7271	def load ( ) : for operator in operators : module , symbols = operator [ 0 ] , operator [ 1 : ] path = 'grappa.operators.{}' . format ( module ) operator = __import__ ( path , None , None , symbols ) for symbol in symbols : Engine . register ( getattr ( operator , symbol ) )
2829	def convert_upsample_bilinear ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting upsample...' ) if names == 'short' : tf_name = 'UPSL' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) output_size = params [ 'output_size' ] align_corners = params [ 'align_corners' ] > 0 def target_layer ( x , size = output_size , align_corners = align_corners ) : import tensorflow as tf x = tf . transpose ( x , [ 0 , 2 , 3 , 1 ] ) x = tf . image . resize_images ( x , size , align_corners = align_corners ) x = tf . transpose ( x , [ 0 , 3 , 1 , 2 ] ) return x lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
10605	def remove_entity ( self , name ) : entity_to_remove = None for e in self . entities : if e . name == name : entity_to_remove = e if entity_to_remove is not None : self . entities . remove ( entity_to_remove )
5271	def lcs ( self , stringIdxs = - 1 ) : if stringIdxs == - 1 or not isinstance ( stringIdxs , list ) : stringIdxs = set ( range ( len ( self . word_starts ) ) ) else : stringIdxs = set ( stringIdxs ) deepestNode = self . _find_lcs ( self . root , stringIdxs ) start = deepestNode . idx end = deepestNode . idx + deepestNode . depth return self . word [ start : end ]
12644	def cert_info ( ) : sec_type = security_type ( ) if sec_type == 'pem' : return get_config_value ( 'pem_path' , fallback = None ) if sec_type == 'cert' : cert_path = get_config_value ( 'cert_path' , fallback = None ) key_path = get_config_value ( 'key_path' , fallback = None ) return cert_path , key_path return None
11621	def set_script ( self , i ) : if i in range ( 1 , 10 ) : n = i - 1 else : raise IllegalInput ( "Invalid Value for ATR %s" % ( hex ( i ) ) ) if n > - 1 : self . curr_script = n self . delta = n * DELTA return
10659	def masses ( amounts ) : return { compound : mass ( compound , amounts [ compound ] ) for compound in amounts . keys ( ) }
12109	def _review_all ( self , launchers ) : if self . launch_args is not None : proceed = self . review_args ( self . launch_args , show_repr = True , heading = 'Meta Arguments' ) if not proceed : return False reviewers = [ self . review_args , self . review_command , self . review_launcher ] for ( count , launcher ) in enumerate ( launchers ) : if not all ( reviewer ( launcher ) for reviewer in reviewers ) : print ( "\n == Aborting launch ==" ) return False if len ( launchers ) != 1 and count < len ( launchers ) - 1 : skip_remaining = self . input_options ( [ 'Y' , 'n' , 'quit' ] , '\nSkip remaining reviews?' , default = 'y' ) if skip_remaining == 'y' : break elif skip_remaining == 'quit' : return False if self . input_options ( [ 'y' , 'N' ] , 'Execute?' , default = 'n' ) != 'y' : return False else : return self . _launch_all ( launchers )
493	def close ( self ) : self . _logger . info ( "Closing" ) if self . _opened : self . _opened = False else : self . _logger . warning ( "close() called, but connection policy was alredy closed" ) return
8196	def click ( self , node ) : if not self . has_node ( node . id ) : return if node == self . root : return self . _dx , self . _dy = self . offset ( node ) self . previous = self . root . id self . load ( node . id )
1583	def generate ( ) : data_bytes = bytearray ( random . getrandbits ( 8 ) for i in range ( REQID . REQID_SIZE ) ) return REQID ( data_bytes )
2219	def register ( self , type ) : def _decorator ( func ) : if isinstance ( type , tuple ) : for t in type : self . func_registry [ t ] = func else : self . func_registry [ type ] = func return func return _decorator
8155	def create_table ( self , name , fields = [ ] , key = "id" ) : for f in fields : if f == key : fields . remove ( key ) sql = "create table " + name + " " sql += "(" + key + " integer primary key" for f in fields : sql += ", " + f + " varchar(255)" sql += ")" self . _cur . execute ( sql ) self . _con . commit ( ) self . index ( name , key , unique = True ) self . connect ( self . _name )
6596	def receive_one ( self ) : if self . nruns == 0 : return None ret = self . communicationChannel . receive_one ( ) if ret is not None : self . nruns -= 1 return ret
3772	def phase_select_property ( phase = None , s = None , l = None , g = None , V_over_F = None ) : r if phase == 's' : return s elif phase == 'l' : return l elif phase == 'g' : return g elif phase == 'two-phase' : return None elif phase is None : return None else : raise Exception ( 'Property not recognized' )
5483	def setup_service ( api_name , api_version , credentials = None ) : if not credentials : credentials = oauth2client . client . GoogleCredentials . get_application_default ( ) return apiclient . discovery . build ( api_name , api_version , credentials = credentials )
9984	def has_lambda ( src ) : module_node = ast . parse ( dedent ( src ) ) lambdaexp = [ node for node in ast . walk ( module_node ) if isinstance ( node , ast . Lambda ) ] return bool ( lambdaexp )
3721	def ion_balance_proportional ( anion_charges , cation_charges , zs , n_anions , n_cations , balance_error , method ) : anion_zs = zs [ 0 : n_anions ] cation_zs = zs [ n_anions : n_cations + n_anions ] anion_balance_error = sum ( [ zi * ci for zi , ci in zip ( anion_zs , anion_charges ) ] ) cation_balance_error = sum ( [ zi * ci for zi , ci in zip ( cation_zs , cation_charges ) ] ) if method == 'proportional insufficient ions increase' : if balance_error < 0 : multiplier = - anion_balance_error / cation_balance_error cation_zs = [ i * multiplier for i in cation_zs ] else : multiplier = - cation_balance_error / anion_balance_error anion_zs = [ i * multiplier for i in anion_zs ] elif method == 'proportional excess ions decrease' : if balance_error < 0 : multiplier = - cation_balance_error / anion_balance_error anion_zs = [ i * multiplier for i in anion_zs ] else : multiplier = - anion_balance_error / cation_balance_error cation_zs = [ i * multiplier for i in cation_zs ] elif method == 'proportional cation adjustment' : multiplier = - anion_balance_error / cation_balance_error cation_zs = [ i * multiplier for i in cation_zs ] elif method == 'proportional anion adjustment' : multiplier = - cation_balance_error / anion_balance_error anion_zs = [ i * multiplier for i in anion_zs ] else : raise Exception ( 'Allowable methods are %s' % charge_balance_methods ) z_water = 1. - sum ( anion_zs ) - sum ( cation_zs ) return anion_zs , cation_zs , z_water
781	def _startJobWithRetries ( self , jobID ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET status=%%s, ' ' _eng_cjm_conn_id=%%s, ' ' start_time=UTC_TIMESTAMP(), ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE (job_id=%%s AND status=%%s)' % ( self . jobsTableName , ) sqlParams = [ self . STATUS_RUNNING , self . _connectionID , jobID , self . STATUS_NOTSTARTED ] numRowsUpdated = conn . cursor . execute ( query , sqlParams ) if numRowsUpdated != 1 : self . _logger . warn ( 'jobStartNext: numRowsUpdated=%r instead of 1; ' 'likely side-effect of transient connection ' 'failure' , numRowsUpdated ) return
10232	def list_abundance_cartesian_expansion ( graph : BELGraph ) -> None : for u , v , k , d in list ( graph . edges ( keys = True , data = True ) ) : if CITATION not in d : continue if isinstance ( u , ListAbundance ) and isinstance ( v , ListAbundance ) : for u_member , v_member in itt . product ( u . members , v . members ) : graph . add_qualified_edge ( u_member , v_member , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( u , ListAbundance ) : for member in u . members : graph . add_qualified_edge ( member , v , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( v , ListAbundance ) : for member in v . members : graph . add_qualified_edge ( u , member , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) _remove_list_abundance_nodes ( graph )
8249	def nearest_hue ( self , primary = False ) : if self . is_black : return "black" elif self . is_white : return "white" elif self . is_grey : return "grey" if primary : hues = primary_hues else : hues = named_hues . keys ( ) nearest , d = "" , 1.0 for hue in hues : if abs ( self . hue - named_hues [ hue ] ) % 1 < d : nearest , d = hue , abs ( self . hue - named_hues [ hue ] ) % 1 return nearest
13897	def DumpDirHashToStringIO ( directory , stringio , base = '' , exclude = None , include = None ) : import fnmatch import os files = [ ( os . path . join ( directory , i ) , i ) for i in os . listdir ( directory ) ] files = [ i for i in files if os . path . isfile ( i [ 0 ] ) ] for fullname , filename in files : if include is not None : if not fnmatch . fnmatch ( fullname , include ) : continue if exclude is not None : if fnmatch . fnmatch ( fullname , exclude ) : continue md5 = Md5Hex ( fullname ) if base : stringio . write ( '%s/%s=%s\n' % ( base , filename , md5 ) ) else : stringio . write ( '%s=%s\n' % ( filename , md5 ) )
10229	def summarize_stability ( graph : BELGraph ) -> Mapping [ str , int ] : regulatory_pairs = get_regulatory_pairs ( graph ) chaotic_pairs = get_chaotic_pairs ( graph ) dampened_pairs = get_dampened_pairs ( graph ) contraditory_pairs = get_contradiction_summary ( graph ) separately_unstable_triples = get_separate_unstable_correlation_triples ( graph ) mutually_unstable_triples = get_mutually_unstable_correlation_triples ( graph ) jens_unstable_triples = get_jens_unstable ( graph ) increase_mismatch_triples = get_increase_mismatch_triplets ( graph ) decrease_mismatch_triples = get_decrease_mismatch_triplets ( graph ) chaotic_triples = get_chaotic_triplets ( graph ) dampened_triples = get_dampened_triplets ( graph ) return { 'Regulatory Pairs' : _count_or_len ( regulatory_pairs ) , 'Chaotic Pairs' : _count_or_len ( chaotic_pairs ) , 'Dampened Pairs' : _count_or_len ( dampened_pairs ) , 'Contradictory Pairs' : _count_or_len ( contraditory_pairs ) , 'Separately Unstable Triples' : _count_or_len ( separately_unstable_triples ) , 'Mutually Unstable Triples' : _count_or_len ( mutually_unstable_triples ) , 'Jens Unstable Triples' : _count_or_len ( jens_unstable_triples ) , 'Increase Mismatch Triples' : _count_or_len ( increase_mismatch_triples ) , 'Decrease Mismatch Triples' : _count_or_len ( decrease_mismatch_triples ) , 'Chaotic Triples' : _count_or_len ( chaotic_triples ) , 'Dampened Triples' : _count_or_len ( dampened_triples ) }
7990	def transport_connected ( self ) : with self . lock : if self . initiator : if self . _output_state is None : self . _initiate ( )
3926	def keypress ( self , size , key ) : key = super ( ) . keypress ( size , key ) num_tabs = len ( self . _widgets ) if key == self . _keys [ 'prev_tab' ] : self . _tab_index = ( self . _tab_index - 1 ) % num_tabs self . _update_tabs ( ) elif key == self . _keys [ 'next_tab' ] : self . _tab_index = ( self . _tab_index + 1 ) % num_tabs self . _update_tabs ( ) elif key == self . _keys [ 'close_tab' ] : if self . _tab_index > 0 : curr_tab = self . _widgets [ self . _tab_index ] self . _widgets . remove ( curr_tab ) del self . _widget_title [ curr_tab ] self . _tab_index -= 1 self . _update_tabs ( ) else : return key
426	def check_unfinished_task ( self , task_name = None , ** kwargs ) : if not isinstance ( task_name , str ) : raise Exception ( "task_name should be string" ) self . _fill_project_info ( kwargs ) kwargs . update ( { '$or' : [ { 'status' : 'pending' } , { 'status' : 'running' } ] } ) task = self . db . Task . find ( kwargs ) task_id_list = task . distinct ( '_id' ) n_task = len ( task_id_list ) if n_task == 0 : logging . info ( "[Database] No unfinished task - task_name: {}" . format ( task_name ) ) return False else : logging . info ( "[Database] Find {} unfinished task - task_name: {}" . format ( n_task , task_name ) ) return True
12120	def generate_colormap ( self , colormap = None , reverse = False ) : if colormap is None : colormap = pylab . cm . Dark2 self . cm = colormap self . colormap = [ ] for i in range ( self . sweeps ) : self . colormap . append ( colormap ( i / self . sweeps ) ) if reverse : self . colormap . reverse ( )
4097	def AKICc ( N , rho , k ) : r from numpy import log , array p = k res = log ( rho ) + p / N / ( N - p ) + ( 3. - ( p + 2. ) / N ) * ( p + 1. ) / ( N - p - 2. ) return res
777	def connect ( self , deleteOldVersions = False , recreate = False ) : with ConnectionFactory . get ( ) as conn : self . _initTables ( cursor = conn . cursor , deleteOldVersions = deleteOldVersions , recreate = recreate ) conn . cursor . execute ( 'SELECT CONNECTION_ID()' ) self . _connectionID = conn . cursor . fetchall ( ) [ 0 ] [ 0 ] self . _logger . info ( "clientJobsConnectionID=%r" , self . _connectionID ) return
8050	def load_source ( self ) : if self . filename in self . STDIN_NAMES : self . filename = "stdin" if sys . version_info [ 0 ] < 3 : self . source = sys . stdin . read ( ) else : self . source = TextIOWrapper ( sys . stdin . buffer , errors = "ignore" ) . read ( ) else : handle = tokenize_open ( self . filename ) self . source = handle . read ( ) handle . close ( )
7215	def list ( self ) : r = self . gbdx_connection . get ( self . _base_url ) raise_for_status ( r ) return r . json ( ) [ 'tasks' ]
6471	def update ( self , points , values = None ) : self . values = values or [ None ] * len ( points ) if np is None : if self . option . function : warnings . warn ( 'numpy not available, function ignored' ) self . points = points self . minimum = min ( self . points ) self . maximum = max ( self . points ) self . current = self . points [ - 1 ] else : self . points = self . apply_function ( points ) self . minimum = np . min ( self . points ) self . maximum = np . max ( self . points ) self . current = self . points [ - 1 ] if self . maximum == self . minimum : self . extents = 1 else : self . extents = ( self . maximum - self . minimum ) self . extents = ( self . maximum - self . minimum )
9407	def read_file ( path , session = None ) : try : data = loadmat ( path , struct_as_record = True ) except UnicodeDecodeError as e : raise Oct2PyError ( str ( e ) ) out = dict ( ) for ( key , value ) in data . items ( ) : out [ key ] = _extract ( value , session ) return out
3816	async def _add_channel_services ( self ) : logger . info ( 'Adding channel services...' ) services = [ "babel" , "babel_presence_last_seen" ] map_list = [ dict ( p = json . dumps ( { "3" : { "1" : { "1" : service } } } ) ) for service in services ] await self . _channel . send_maps ( map_list ) logger . info ( 'Channel services added' )
1876	def MOVQ ( cpu , dest , src ) : if dest . size == src . size and dest . size == 64 : dest . write ( src . read ( ) ) elif dest . size == src . size and dest . size == 128 : src_lo = Operators . EXTRACT ( src . read ( ) , 0 , 64 ) dest . write ( Operators . ZEXTEND ( src_lo , 128 ) ) elif dest . size == 128 and src . size == 64 : dest . write ( Operators . ZEXTEND ( src . read ( ) , dest . size ) ) elif dest . size == 64 and src . size == 128 : dest . write ( Operators . EXTRACT ( src . read ( ) , 0 , dest . size ) ) else : msg = 'Invalid size in MOVQ' logger . error ( msg ) raise Exception ( msg )
984	def mmGetMetricSequencesPredictedActiveCellsPerColumn ( self ) : self . _mmComputeTransitionTraces ( ) numCellsPerColumn = [ ] for predictedActiveCells in ( self . _mmData [ "predictedActiveCellsForSequence" ] . values ( ) ) : cellsForColumn = self . mapCellsToColumns ( predictedActiveCells ) numCellsPerColumn += [ len ( x ) for x in cellsForColumn . values ( ) ] return Metric ( self , "# predicted => active cells per column for each sequence" , numCellsPerColumn )
3332	def init_logging ( config ) : verbose = config . get ( "verbose" , 3 ) enable_loggers = config . get ( "enable_loggers" , [ ] ) if enable_loggers is None : enable_loggers = [ ] logger_date_format = config . get ( "logger_date_format" , "%Y-%m-%d %H:%M:%S" ) logger_format = config . get ( "logger_format" , "%(asctime)s.%(msecs)03d - <%(thread)d> %(name)-27s %(levelname)-8s: %(message)s" , ) formatter = logging . Formatter ( logger_format , logger_date_format ) consoleHandler = logging . StreamHandler ( sys . stdout ) consoleHandler . setFormatter ( formatter ) logger = logging . getLogger ( BASE_LOGGER_NAME ) if verbose >= 4 : logger . setLevel ( logging . DEBUG ) elif verbose == 3 : logger . setLevel ( logging . INFO ) elif verbose == 2 : logger . setLevel ( logging . WARN ) elif verbose == 1 : logger . setLevel ( logging . ERROR ) else : logger . setLevel ( logging . CRITICAL ) logger . propagate = False for hdlr in logger . handlers [ : ] : try : hdlr . flush ( ) hdlr . close ( ) except Exception : pass logger . removeHandler ( hdlr ) logger . addHandler ( consoleHandler ) if verbose >= 3 : for e in enable_loggers : if not e . startswith ( BASE_LOGGER_NAME + "." ) : e = BASE_LOGGER_NAME + "." + e lg = logging . getLogger ( e . strip ( ) ) lg . setLevel ( logging . DEBUG )
12416	def end ( self , * args , ** kwargs ) : self . send ( * args , ** kwargs ) self . close ( )
13868	def weekday ( when , weekday , start = mon ) : if isinstance ( when , datetime ) : when = when . date ( ) today = when . weekday ( ) delta = weekday - today if weekday < start and today >= start : delta += 7 elif weekday >= start and today < start : delta -= 7 return when + timedelta ( days = delta )
5616	def write_vector_window ( in_data = None , out_schema = None , out_tile = None , out_path = None , bucket_resource = None ) : try : os . remove ( out_path ) except OSError : pass out_features = [ ] for feature in in_data : try : for out_geom in multipart_to_singleparts ( clean_geometry_type ( to_shape ( feature [ "geometry" ] ) . intersection ( out_tile . bbox ) , out_schema [ "geometry" ] ) ) : out_features . append ( { "geometry" : mapping ( out_geom ) , "properties" : feature [ "properties" ] } ) except Exception as e : logger . warning ( "failed to prepare geometry for writing: %s" , e ) continue if out_features : try : if out_path . startswith ( "s3://" ) : with VectorWindowMemoryFile ( tile = out_tile , features = out_features , schema = out_schema , driver = "GeoJSON" ) as memfile : logger . debug ( ( out_tile . id , "upload tile" , out_path ) ) bucket_resource . put_object ( Key = "/" . join ( out_path . split ( "/" ) [ 3 : ] ) , Body = memfile ) else : with fiona . open ( out_path , 'w' , schema = out_schema , driver = "GeoJSON" , crs = out_tile . crs . to_dict ( ) ) as dst : logger . debug ( ( out_tile . id , "write tile" , out_path ) ) dst . writerecords ( out_features ) except Exception as e : logger . error ( "error while writing file %s: %s" , out_path , e ) raise else : logger . debug ( ( out_tile . id , "nothing to write" , out_path ) )
5393	def _make_environment ( self , inputs , outputs , mounts ) : env = { } env . update ( providers_util . get_file_environment_variables ( inputs ) ) env . update ( providers_util . get_file_environment_variables ( outputs ) ) env . update ( providers_util . get_file_environment_variables ( mounts ) ) return env
2334	def predict_proba ( self , a , b , idx = 0 , ** kwargs ) : return self . predict_dataset ( DataFrame ( [ [ a , b ] ] , columns = [ 'A' , 'B' ] ) )
9024	def write ( self , string ) : bytes_ = string . encode ( self . _encoding ) self . _file . write ( bytes_ )
3709	def calculate ( self , T , method ) : r if method == RACKETT : Vm = Rackett ( T , self . Tc , self . Pc , self . Zc ) elif method == YAMADA_GUNN : Vm = Yamada_Gunn ( T , self . Tc , self . Pc , self . omega ) elif method == BHIRUD_NORMAL : Vm = Bhirud_normal ( T , self . Tc , self . Pc , self . omega ) elif method == TOWNSEND_HALES : Vm = Townsend_Hales ( T , self . Tc , self . Vc , self . omega ) elif method == HTCOSTALD : Vm = COSTALD ( T , self . Tc , self . Vc , self . omega ) elif method == YEN_WOODS_SAT : Vm = Yen_Woods_saturation ( T , self . Tc , self . Vc , self . Zc ) elif method == MMSNM0 : Vm = SNM0 ( T , self . Tc , self . Vc , self . omega ) elif method == MMSNM0FIT : Vm = SNM0 ( T , self . Tc , self . Vc , self . omega , self . SNM0_delta_SRK ) elif method == CAMPBELL_THODOS : Vm = Campbell_Thodos ( T , self . Tb , self . Tc , self . Pc , self . MW , self . dipole ) elif method == HTCOSTALDFIT : Vm = COSTALD ( T , self . Tc , self . COSTALD_Vchar , self . COSTALD_omega_SRK ) elif method == RACKETTFIT : Vm = Rackett ( T , self . Tc , self . Pc , self . RACKETT_Z_RA ) elif method == PERRYDIPPR : A , B , C , D = self . DIPPR_coeffs Vm = 1. / EQ105 ( T , A , B , C , D ) elif method == CRC_INORG_L : rho = CRC_inorganic ( T , self . CRC_INORG_L_rho , self . CRC_INORG_L_k , self . CRC_INORG_L_Tm ) Vm = rho_to_Vm ( rho , self . CRC_INORG_L_MW ) elif method == VDI_PPDS : A , B , C , D = self . VDI_PPDS_coeffs tau = 1. - T / self . VDI_PPDS_Tc rho = self . VDI_PPDS_rhoc + A * tau ** 0.35 + B * tau ** ( 2 / 3. ) + C * tau + D * tau ** ( 4 / 3. ) Vm = rho_to_Vm ( rho , self . VDI_PPDS_MW ) elif method == CRC_INORG_L_CONST : Vm = self . CRC_INORG_L_CONST_Vm elif method == COOLPROP : Vm = 1. / CoolProp_T_dependent_property ( T , self . CASRN , 'DMOLAR' , 'l' ) elif method in self . tabular_data : Vm = self . interpolate ( T , method ) return Vm
11444	def parse ( self , path_to_xml = None ) : if not path_to_xml : if not self . path : self . logger . error ( "No path defined!" ) return path_to_xml = self . path root = self . _clean_xml ( path_to_xml ) if root . tag . lower ( ) == 'collection' : tree = ET . ElementTree ( root ) self . records = element_tree_collection_to_records ( tree ) elif root . tag . lower ( ) == 'record' : new_root = ET . Element ( 'collection' ) new_root . append ( root ) tree = ET . ElementTree ( new_root ) self . records = element_tree_collection_to_records ( tree ) else : header_subs = get_request_subfields ( root ) records = root . find ( 'ListRecords' ) if records is None : records = root . find ( 'GetRecord' ) if records is None : raise ValueError ( "Cannot find ListRecords or GetRecord!" ) tree = ET . ElementTree ( records ) for record , is_deleted in element_tree_oai_records ( tree , header_subs ) : if is_deleted : self . deleted_records . append ( self . create_deleted_record ( record ) ) else : self . records . append ( record )
9203	def render ( node , strict = False ) : if isinstance ( node , list ) : return render_list ( node ) elif isinstance ( node , dict ) : return render_node ( node , strict = strict ) else : raise NotImplementedError ( "You tried to render a %s. Only list and dicts can be rendered." % node . __class__ . __name__ )
4222	def disable ( ) : root = platform . config_root ( ) try : os . makedirs ( root ) except OSError : pass filename = os . path . join ( root , 'keyringrc.cfg' ) if os . path . exists ( filename ) : msg = "Refusing to overwrite {filename}" . format ( ** locals ( ) ) raise RuntimeError ( msg ) with open ( filename , 'w' ) as file : file . write ( '[backend]\ndefault-keyring=keyring.backends.null.Keyring' )
5740	def main ( path , pid , queue ) : setup_logging ( ) if pid : with open ( os . path . expanduser ( pid ) , "w" ) as f : f . write ( str ( os . getpid ( ) ) ) if not path : path = os . getcwd ( ) sys . path . insert ( 0 , path ) queue = import_queue ( queue ) import psq worker = psq . Worker ( queue = queue ) worker . listen ( )
4456	def limit ( self , offset , num ) : limit = Limit ( offset , num ) if self . _groups : self . _groups [ - 1 ] . limit = limit else : self . _limit = limit return self
3601	def create_token ( self , data , options = None ) : if not options : options = { } options . update ( { 'admin' : self . admin , 'debug' : self . debug } ) claims = self . _create_options_claims ( options ) claims [ 'v' ] = self . TOKEN_VERSION claims [ 'iat' ] = int ( time . mktime ( time . gmtime ( ) ) ) claims [ 'd' ] = data return self . _encode_token ( self . secret , claims )
13705	def iter_char_block ( self , text = None , width = 60 , fmtfunc = str ) : if width < 1 : width = 1 text = ( self . text if text is None else text ) or '' text = ' ' . join ( text . split ( '\n' ) ) escapecodes = get_codes ( text ) if not escapecodes : yield from ( fmtfunc ( text [ i : i + width ] ) for i in range ( 0 , len ( text ) , width ) ) else : blockwidth = 0 block = [ ] for i , s in enumerate ( get_indices_list ( text ) ) : block . append ( s ) if len ( s ) == 1 : blockwidth += 1 if blockwidth == width : yield '' . join ( block ) block = [ ] blockwidth = 0 if block : yield '' . join ( block )
12363	def get ( self , id , ** kwargs ) : return ( super ( MutableCollection , self ) . get ( ( id , ) , ** kwargs ) . get ( self . singular , None ) )
7723	def get_password ( self ) : for child in xml_element_iter ( self . xmlnode . children ) : if get_node_ns_uri ( child ) == MUC_NS and child . name == "password" : return from_utf8 ( child . getContent ( ) ) return None
2396	def confusion_matrix ( rater_a , rater_b , min_rating = None , max_rating = None ) : assert ( len ( rater_a ) == len ( rater_b ) ) rater_a = [ int ( a ) for a in rater_a ] rater_b = [ int ( b ) for b in rater_b ] min_rating = int ( min_rating ) max_rating = int ( max_rating ) if min_rating is None : min_rating = min ( rater_a ) if max_rating is None : max_rating = max ( rater_a ) num_ratings = int ( max_rating - min_rating + 1 ) conf_mat = [ [ 0 for i in range ( num_ratings ) ] for j in range ( num_ratings ) ] for a , b in zip ( rater_a , rater_b ) : conf_mat [ int ( a - min_rating ) ] [ int ( b - min_rating ) ] += 1 return conf_mat
2041	def SELFDESTRUCT ( self , recipient ) : recipient = Operators . EXTRACT ( recipient , 0 , 160 ) address = self . address if issymbolic ( recipient ) : logger . info ( "Symbolic recipient on self destruct" ) recipient = solver . get_value ( self . constraints , recipient ) if recipient not in self . world : self . world . create_account ( address = recipient ) self . world . send_funds ( address , recipient , self . world . get_balance ( address ) ) self . world . delete_account ( address ) raise EndTx ( 'SELFDESTRUCT' )
5095	def get_map_image ( url , dest_path = None ) : image = requests . get ( url , stream = True , timeout = 10 ) if dest_path : image_url = url . rsplit ( '/' , 2 ) [ 1 ] + '-' + url . rsplit ( '/' , 1 ) [ 1 ] image_filename = image_url . split ( '?' ) [ 0 ] dest = os . path . join ( dest_path , image_filename ) image . raise_for_status ( ) with open ( dest , 'wb' ) as data : image . raw . decode_content = True shutil . copyfileobj ( image . raw , data ) return image . raw
13078	def make_cache_keys ( self , endpoint , kwargs ) : keys = sorted ( kwargs . keys ( ) ) i18n_cache_key = endpoint + "|" + "|" . join ( [ kwargs [ k ] for k in keys ] ) if "lang" in keys : cache_key = endpoint + "|" + "|" . join ( [ kwargs [ k ] for k in keys if k != "lang" ] ) else : cache_key = i18n_cache_key return i18n_cache_key , cache_key
10198	def anonymize_user ( doc ) : ip = doc . pop ( 'ip_address' , None ) if ip : doc . update ( { 'country' : get_geoip ( ip ) } ) user_id = doc . pop ( 'user_id' , '' ) session_id = doc . pop ( 'session_id' , '' ) user_agent = doc . pop ( 'user_agent' , '' ) timestamp = arrow . get ( doc . get ( 'timestamp' ) ) timeslice = timestamp . strftime ( '%Y%m%d%H' ) salt = get_anonymization_salt ( timestamp ) visitor_id = hashlib . sha224 ( salt . encode ( 'utf-8' ) ) if user_id : visitor_id . update ( user_id . encode ( 'utf-8' ) ) elif session_id : visitor_id . update ( session_id . encode ( 'utf-8' ) ) elif ip and user_agent : vid = '{}|{}|{}' . format ( ip , user_agent , timeslice ) visitor_id . update ( vid . encode ( 'utf-8' ) ) else : pass unique_session_id = hashlib . sha224 ( salt . encode ( 'utf-8' ) ) if user_id : sid = '{}|{}' . format ( user_id , timeslice ) unique_session_id . update ( sid . encode ( 'utf-8' ) ) elif session_id : sid = '{}|{}' . format ( session_id , timeslice ) unique_session_id . update ( sid . encode ( 'utf-8' ) ) elif ip and user_agent : sid = '{}|{}|{}' . format ( ip , user_agent , timeslice ) unique_session_id . update ( sid . encode ( 'utf-8' ) ) doc . update ( dict ( visitor_id = visitor_id . hexdigest ( ) , unique_session_id = unique_session_id . hexdigest ( ) ) ) return doc
4974	def verify_edx_resources ( ) : required_methods = { 'ProgramDataExtender' : ProgramDataExtender , } for method in required_methods : if required_methods [ method ] is None : raise NotConnectedToOpenEdX ( _ ( "The following method from the Open edX platform is necessary for this view but isn't available." ) + "\nUnavailable: {method}" . format ( method = method ) )
6792	def manage ( self , cmd , * args , ** kwargs ) : r = self . local_renderer environs = kwargs . pop ( 'environs' , '' ) . strip ( ) if environs : environs = ' ' . join ( 'export %s=%s;' % tuple ( _ . split ( '=' ) ) for _ in environs . split ( ',' ) ) environs = ' ' + environs + ' ' r . env . cmd = cmd r . env . SITE = r . genv . SITE or r . genv . default_site r . env . args = ' ' . join ( map ( str , args ) ) r . env . kwargs = ' ' . join ( ( '--%s' % _k if _v in ( True , 'True' ) else '--%s=%s' % ( _k , _v ) ) for _k , _v in kwargs . items ( ) ) r . env . environs = environs if self . is_local : r . env . project_dir = r . env . local_project_dir r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE};{environs} cd {project_dir}; {manage_cmd} {cmd} {args} {kwargs}' )
1567	def invoke_hook_bolt_execute ( self , heron_tuple , execute_latency_ns ) : if len ( self . task_hooks ) > 0 : bolt_execute_info = BoltExecuteInfo ( heron_tuple = heron_tuple , executing_task_id = self . get_task_id ( ) , execute_latency_ms = execute_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . bolt_execute ( bolt_execute_info )
3931	def _auth_with_refresh_token ( session , refresh_token ) : token_request_data = { 'client_id' : OAUTH2_CLIENT_ID , 'client_secret' : OAUTH2_CLIENT_SECRET , 'grant_type' : 'refresh_token' , 'refresh_token' : refresh_token , } res = _make_token_request ( session , token_request_data ) return res [ 'access_token' ]
11177	def parsestr ( self , argstr ) : argv = shlex . split ( argstr , comments = True ) if len ( argv ) != 1 : raise BadNumberOfArguments ( 1 , len ( argv ) ) arg = argv [ 0 ] lower = arg . lower ( ) if lower in self . true : return True if lower in self . false : return False raise BadArgument ( arg , "Allowed values are " + self . allowed + '.' )
5060	def get_enterprise_customer ( uuid ) : EnterpriseCustomer = apps . get_model ( 'enterprise' , 'EnterpriseCustomer' ) try : return EnterpriseCustomer . objects . get ( uuid = uuid ) except EnterpriseCustomer . DoesNotExist : return None
3105	def code_verifier ( n_bytes = 64 ) : verifier = base64 . urlsafe_b64encode ( os . urandom ( n_bytes ) ) . rstrip ( b'=' ) if len ( verifier ) < 43 : raise ValueError ( "Verifier too short. n_bytes must be > 30." ) elif len ( verifier ) > 128 : raise ValueError ( "Verifier too long. n_bytes must be < 97." ) else : return verifier
1614	def Match ( pattern , s ) : if pattern not in _regexp_compile_cache : _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) return _regexp_compile_cache [ pattern ] . match ( s )
4661	def broadcast ( self , tx = None ) : if tx : return self . transactionbuilder_class ( tx , blockchain_instance = self ) . broadcast ( ) else : return self . txbuffer . broadcast ( )
4113	def rc2is ( k ) : assert numpy . isrealobj ( k ) , 'Inverse sine parameters not defined for complex reflection coefficients.' if max ( numpy . abs ( k ) ) >= 1 : raise ValueError ( 'All reflection coefficients should have magnitude less than unity.' ) return ( 2 / numpy . pi ) * numpy . arcsin ( k )
954	def getCallerInfo ( depth = 2 ) : f = sys . _getframe ( depth ) method_name = f . f_code . co_name filename = f . f_code . co_filename arg_class = None args = inspect . getargvalues ( f ) if len ( args [ 0 ] ) > 0 : arg_name = args [ 0 ] [ 0 ] arg_class = args [ 3 ] [ arg_name ] . __class__ . __name__ return ( method_name , filename , arg_class )
1263	def states ( self ) : screen = self . env . getScreenRGB ( ) return dict ( shape = screen . shape , type = 'int' )
9650	def check_integrity ( sakefile , settings ) : sprint = settings [ "sprint" ] error = settings [ "error" ] sprint ( "Call to check_integrity issued" , level = "verbose" ) if not sakefile : error ( "Sakefile is empty" ) return False if len ( sakefile . keys ( ) ) != len ( set ( sakefile . keys ( ) ) ) : error ( "Sakefile contains duplicate targets" ) return False for target in sakefile : if target == "all" : if not check_target_integrity ( target , sakefile [ "all" ] , all = True ) : error ( "Failed to accept target 'all'" ) return False continue if "formula" not in sakefile [ target ] : if not check_target_integrity ( target , sakefile [ target ] , meta = True ) : errmes = "Failed to accept meta-target '{}'" . format ( target ) error ( errmes ) return False for atom_target in sakefile [ target ] : if atom_target == "help" : continue if not check_target_integrity ( atom_target , sakefile [ target ] [ atom_target ] , parent = target ) : errmes = "Failed to accept target '{}'\n" . format ( atom_target ) error ( errmes ) return False continue if not check_target_integrity ( target , sakefile [ target ] ) : errmes = "Failed to accept target '{}'\n" . format ( target ) error ( errmes ) return False return True
343	def validation_metrics ( self ) : if ( self . _validation_iterator is None ) or ( self . _validation_metrics is None ) : raise AttributeError ( 'Validation is not setup.' ) n = 0.0 metric_sums = [ 0.0 ] * len ( self . _validation_metrics ) self . _sess . run ( self . _validation_iterator . initializer ) while True : try : metrics = self . _sess . run ( self . _validation_metrics ) for i , m in enumerate ( metrics ) : metric_sums [ i ] += m n += 1.0 except tf . errors . OutOfRangeError : break for i , m in enumerate ( metric_sums ) : metric_sums [ i ] = metric_sums [ i ] / n return zip ( self . _validation_metrics , metric_sums )
13553	def _delete_resource ( self , url ) : headers = { "Content-Type" : "application/json" , "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = WhenIWork_DAO ( ) . deleteURL ( url , headers ) if not ( response . status == 200 or response . status == 201 or response . status == 204 ) : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
12305	def get_module_class ( class_path ) : mod_name , cls_name = class_path . rsplit ( '.' , 1 ) try : mod = import_module ( mod_name ) except ImportError as ex : raise EvoStreamException ( 'Error importing module %s: ' '"%s"' % ( mod_name , ex ) ) return getattr ( mod , cls_name )
857	def _updateSequenceInfo ( self , r ) : newSequence = False sequenceId = ( r [ self . _sequenceIdIdx ] if self . _sequenceIdIdx is not None else None ) if sequenceId != self . _currSequence : if sequenceId in self . _sequences : raise Exception ( 'Broken sequence: %s, record: %s' % ( sequenceId , r ) ) self . _sequences . add ( self . _currSequence ) self . _currSequence = sequenceId if self . _resetIdx : assert r [ self . _resetIdx ] == 1 newSequence = True else : reset = False if self . _resetIdx : reset = r [ self . _resetIdx ] if reset == 1 : newSequence = True if not newSequence : if self . _timeStampIdx and self . _currTime is not None : t = r [ self . _timeStampIdx ] if t < self . _currTime : raise Exception ( 'No time travel. Early timestamp for record: %s' % r ) if self . _timeStampIdx : self . _currTime = r [ self . _timeStampIdx ]
12283	def lookup ( self , username = None , reponame = None , key = None ) : if key is None : key = self . key ( username , reponame ) if key not in self . repos : raise UnknownRepository ( ) return self . repos [ key ]
12191	async def _get_socket_url ( self ) : data = await self . api . execute_method ( self . RTM_START_ENDPOINT , simple_latest = True , no_unreads = True , ) return data [ 'url' ]
6589	def open ( self ) : self . workingArea . open ( ) self . runid_pkgidx_map = { } self . runid_to_return = deque ( )
4204	def rlevinson ( a , efinal ) : a = numpy . array ( a ) realdata = numpy . isrealobj ( a ) assert a [ 0 ] == 1 , 'First coefficient of the prediction polynomial must be unity' p = len ( a ) if p < 2 : raise ValueError ( 'Polynomial should have at least two coefficients' ) if realdata == True : U = numpy . zeros ( ( p , p ) ) else : U = numpy . zeros ( ( p , p ) , dtype = complex ) U [ : , p - 1 ] = numpy . conj ( a [ - 1 : : - 1 ] ) p = p - 1 e = numpy . zeros ( p ) e [ - 1 ] = efinal for k in range ( p - 1 , 0 , - 1 ) : [ a , e [ k - 1 ] ] = levdown ( a , e [ k ] ) U [ : , k ] = numpy . concatenate ( ( numpy . conj ( a [ - 1 : : - 1 ] . transpose ( ) ) , [ 0 ] * ( p - k ) ) ) e0 = e [ 0 ] / ( 1. - abs ( a [ 1 ] ** 2 ) ) U [ 0 , 0 ] = 1 kr = numpy . conj ( U [ 0 , 1 : ] ) kr = kr . transpose ( ) R = numpy . zeros ( 1 , dtype = complex ) k = 1 R0 = e0 R [ 0 ] = - numpy . conj ( U [ 0 , 1 ] ) * R0 for k in range ( 1 , p ) : r = - sum ( numpy . conj ( U [ k - 1 : : - 1 , k ] ) * R [ - 1 : : - 1 ] ) - kr [ k ] * e [ k - 1 ] R = numpy . insert ( R , len ( R ) , r ) R = numpy . insert ( R , 0 , e0 ) return R , U , kr , e
4511	def crop ( image , top_offset = 0 , left_offset = 0 , bottom_offset = 0 , right_offset = 0 ) : if bottom_offset or top_offset or left_offset or right_offset : width , height = image . size box = ( left_offset , top_offset , width - right_offset , height - bottom_offset ) image = image . crop ( box = box ) return image
13515	def residual_resistance_coef ( slenderness , prismatic_coef , froude_number ) : Cr = cr ( slenderness , prismatic_coef , froude_number ) if math . isnan ( Cr ) : Cr = cr_nearest ( slenderness , prismatic_coef , froude_number ) return Cr
9113	def message ( self ) : try : with open ( join ( self . fs_path , u'message' ) ) as message_file : return u'' . join ( [ line . decode ( 'utf-8' ) for line in message_file . readlines ( ) ] ) except IOError : return u''
6836	def base_boxes ( self ) : return sorted ( list ( set ( [ name for name , provider in self . _box_list ( ) ] ) ) )
13076	def view_maker ( self , name , instance = None ) : if instance is None : instance = self sig = "lang" in [ parameter . name for parameter in inspect . signature ( getattr ( instance , name ) ) . parameters . values ( ) ] def route ( ** kwargs ) : if sig and "lang" not in kwargs : kwargs [ "lang" ] = self . get_locale ( ) if "semantic" in kwargs : del kwargs [ "semantic" ] return self . route ( getattr ( instance , name ) , ** kwargs ) return route
1067	def getheaders ( self , name ) : result = [ ] current = '' have_header = 0 for s in self . getallmatchingheaders ( name ) : if s [ 0 ] . isspace ( ) : if current : current = "%s\n %s" % ( current , s . strip ( ) ) else : current = s . strip ( ) else : if have_header : result . append ( current ) current = s [ s . find ( ":" ) + 1 : ] . strip ( ) have_header = 1 if have_header : result . append ( current ) return result
8886	def finalize ( self ) : if self . __head_less : warn ( f'{self.__class__.__name__} configured to head less mode. finalize unusable' ) elif not self . __head_generate : warn ( f'{self.__class__.__name__} already finalized or fitted' ) elif not self . __head_dict : raise NotFittedError ( f'{self.__class__.__name__} instance is not fitted yet' ) else : if self . remove_rare_ratio : self . __clean_head ( * self . __head_rare ) self . __prepare_header ( ) self . __head_rare = None self . __head_generate = False
3347	def add_members ( self , new_members ) : if isinstance ( new_members , string_types ) or hasattr ( new_members , "id" ) : warn ( "need to pass in a list" ) new_members = [ new_members ] self . _members . update ( new_members )
1277	def tf_step ( self , x , iteration , deltas , improvement , last_improvement , estimated_improvement ) : x , next_iteration , deltas , improvement , last_improvement , estimated_improvement = super ( LineSearch , self ) . tf_step ( x , iteration , deltas , improvement , last_improvement , estimated_improvement ) next_x = [ t + delta for t , delta in zip ( x , deltas ) ] if self . mode == 'linear' : next_deltas = deltas next_estimated_improvement = estimated_improvement + self . estimated_incr elif self . mode == 'exponential' : next_deltas = [ delta * self . parameter for delta in deltas ] next_estimated_improvement = estimated_improvement * self . parameter target_value = self . fn_x ( next_deltas ) next_improvement = tf . divide ( x = ( target_value - self . base_value ) , y = tf . maximum ( x = next_estimated_improvement , y = util . epsilon ) ) return next_x , next_iteration , next_deltas , next_improvement , improvement , next_estimated_improvement
7668	def trim ( self , start_time , end_time , strict = False ) : trimmed_array = AnnotationArray ( ) for ann in self : trimmed_array . append ( ann . trim ( start_time , end_time , strict = strict ) ) return trimmed_array
528	def _getInputNeighborhood ( self , centerInput ) : if self . _wrapAround : return topology . wrappingNeighborhood ( centerInput , self . _potentialRadius , self . _inputDimensions ) else : return topology . neighborhood ( centerInput , self . _potentialRadius , self . _inputDimensions )
13659	def route ( self , * components ) : def _factory ( f ) : self . _addRoute ( f , route ( * components ) ) return f return _factory
8253	def context_to_rgb ( self , str ) : matches = [ ] for clr in context : tags = context [ clr ] for tag in tags : if tag . startswith ( str ) or str . startswith ( tag ) : matches . append ( clr ) break matches = [ color ( name ) for name in matches ] return matches
9177	def _dissect_roles ( metadata ) : for role_key in cnxepub . ATTRIBUTED_ROLE_KEYS : for user in metadata . get ( role_key , [ ] ) : if user [ 'type' ] != 'cnx-id' : raise ValueError ( "Archive only accepts Connexions users." ) uid = parse_user_uri ( user [ 'id' ] ) yield uid , role_key raise StopIteration ( )
9563	def _as_dict ( self , r ) : d = dict ( ) for i , f in enumerate ( self . _field_names ) : d [ f ] = r [ i ] if i < len ( r ) else None return d
12940	def getRedisPool ( params ) : global RedisPools global _defaultRedisConnectionParams global _redisManagedConnectionParams if not params : params = _defaultRedisConnectionParams isDefaultParams = True else : isDefaultParams = bool ( params is _defaultRedisConnectionParams ) if 'connection_pool' in params : return params [ 'connection_pool' ] hashValue = hashDictOneLevel ( params ) if hashValue in RedisPools : params [ 'connection_pool' ] = RedisPools [ hashValue ] return RedisPools [ hashValue ] if not isDefaultParams : origParams = params params = copy . copy ( params ) else : origParams = params checkAgain = False if 'host' not in params : if not isDefaultParams and 'host' in _defaultRedisConnectionParams : params [ 'host' ] = _defaultRedisConnectionParams [ 'host' ] else : params [ 'host' ] = '127.0.0.1' checkAgain = True if 'port' not in params : if not isDefaultParams and 'port' in _defaultRedisConnectionParams : params [ 'port' ] = _defaultRedisConnectionParams [ 'port' ] else : params [ 'port' ] = 6379 checkAgain = True if 'db' not in params : if not isDefaultParams and 'db' in _defaultRedisConnectionParams : params [ 'db' ] = _defaultRedisConnectionParams [ 'db' ] else : params [ 'db' ] = 0 checkAgain = True if not isDefaultParams : otherGlobalKeys = set ( _defaultRedisConnectionParams . keys ( ) ) - set ( params . keys ( ) ) for otherKey in otherGlobalKeys : if otherKey == 'connection_pool' : continue params [ otherKey ] = _defaultRedisConnectionParams [ otherKey ] checkAgain = True if checkAgain : hashValue = hashDictOneLevel ( params ) if hashValue in RedisPools : params [ 'connection_pool' ] = RedisPools [ hashValue ] return RedisPools [ hashValue ] connectionPool = redis . ConnectionPool ( ** params ) origParams [ 'connection_pool' ] = params [ 'connection_pool' ] = connectionPool RedisPools [ hashValue ] = connectionPool origParamsHash = hashDictOneLevel ( origParams ) if origParamsHash not in _redisManagedConnectionParams : _redisManagedConnectionParams [ origParamsHash ] = [ origParams ] elif origParams not in _redisManagedConnectionParams [ origParamsHash ] : _redisManagedConnectionParams [ origParamsHash ] . append ( origParams ) return connectionPool
3503	def add_loopless ( model , zero_cutoff = None ) : zero_cutoff = normalize_cutoff ( model , zero_cutoff ) internal = [ i for i , r in enumerate ( model . reactions ) if not r . boundary ] s_int = create_stoichiometric_matrix ( model ) [ : , numpy . array ( internal ) ] n_int = nullspace ( s_int ) . T max_bound = max ( max ( abs ( b ) for b in r . bounds ) for r in model . reactions ) prob = model . problem to_add = [ ] for i in internal : rxn = model . reactions [ i ] indicator = prob . Variable ( "indicator_" + rxn . id , type = "binary" ) on_off_constraint = prob . Constraint ( rxn . flux_expression - max_bound * indicator , lb = - max_bound , ub = 0 , name = "on_off_" + rxn . id ) delta_g = prob . Variable ( "delta_g_" + rxn . id ) delta_g_range = prob . Constraint ( delta_g + ( max_bound + 1 ) * indicator , lb = 1 , ub = max_bound , name = "delta_g_range_" + rxn . id ) to_add . extend ( [ indicator , on_off_constraint , delta_g , delta_g_range ] ) model . add_cons_vars ( to_add ) for i , row in enumerate ( n_int ) : name = "nullspace_constraint_" + str ( i ) nullspace_constraint = prob . Constraint ( Zero , lb = 0 , ub = 0 , name = name ) model . add_cons_vars ( [ nullspace_constraint ] ) coefs = { model . variables [ "delta_g_" + model . reactions [ ridx ] . id ] : row [ i ] for i , ridx in enumerate ( internal ) if abs ( row [ i ] ) > zero_cutoff } model . constraints [ name ] . set_linear_coefficients ( coefs )
4457	def get_args ( self ) : args = [ self . _query_string ] if self . _no_content : args . append ( 'NOCONTENT' ) if self . _fields : args . append ( 'INFIELDS' ) args . append ( len ( self . _fields ) ) args += self . _fields if self . _verbatim : args . append ( 'VERBATIM' ) if self . _no_stopwords : args . append ( 'NOSTOPWORDS' ) if self . _filters : for flt in self . _filters : assert isinstance ( flt , Filter ) args += flt . args if self . _with_payloads : args . append ( 'WITHPAYLOADS' ) if self . _ids : args . append ( 'INKEYS' ) args . append ( len ( self . _ids ) ) args += self . _ids if self . _slop >= 0 : args += [ 'SLOP' , self . _slop ] if self . _in_order : args . append ( 'INORDER' ) if self . _return_fields : args . append ( 'RETURN' ) args . append ( len ( self . _return_fields ) ) args += self . _return_fields if self . _sortby : assert isinstance ( self . _sortby , SortbyField ) args . append ( 'SORTBY' ) args += self . _sortby . args if self . _language : args += [ 'LANGUAGE' , self . _language ] args += self . _summarize_fields + self . _highlight_fields args += [ "LIMIT" , self . _offset , self . _num ] return args
3627	def normalize_cols ( table ) : longest_row_len = max ( [ len ( row ) for row in table ] ) for row in table : while len ( row ) < longest_row_len : row . append ( '' ) return table
12800	def is_text ( self ) : return self . type in [ self . _TYPE_PASTE , self . _TYPE_TEXT , self . _TYPE_TWEET ]
8021	async def websocket_close ( self , message , stream_name ) : if stream_name in self . applications_accepting_frames : self . applications_accepting_frames . remove ( stream_name ) if self . closing : return if not self . applications_accepting_frames : await self . close ( message . get ( "code" ) )
5167	def __intermediate_dns_search ( self , uci , address ) : if 'dns_search' in uci : return uci [ 'dns_search' ] if address [ 'proto' ] == 'none' : return None dns_search = self . netjson . get ( 'dns_search' , None ) if dns_search : return ' ' . join ( dns_search )
385	def parse_darknet_ann_list_to_cls_box ( annotations ) : class_list = [ ] bbox_list = [ ] for ann in annotations : class_list . append ( ann [ 0 ] ) bbox_list . append ( ann [ 1 : ] ) return class_list , bbox_list
4707	def power_on ( self , interval = 200 ) : if self . __power_on_port is None : cij . err ( "cij.usb.relay: Invalid USB_RELAY_POWER_ON" ) return 1 return self . __press ( self . __power_on_port , interval = interval )
6324	def train ( self , text ) : r text = text_type ( text ) if '\x00' in text : text = text . replace ( '\x00' , ' ' ) counts = Counter ( text ) counts [ '\x00' ] = 1 tot_letters = sum ( counts . values ( ) ) tot = 0 self . _probs = { } prev = Fraction ( 0 ) for char , count in sorted ( counts . items ( ) , key = lambda x : ( x [ 1 ] , x [ 0 ] ) , reverse = True ) : follow = Fraction ( tot + count , tot_letters ) self . _probs [ char ] = ( prev , follow ) prev = follow tot = tot + count
13037	def overview ( ) : doc = Host ( ) search = doc . search ( ) search . aggs . bucket ( 'tag_count' , 'terms' , field = 'tags' , order = { '_count' : 'desc' } , size = 100 ) response = search . execute ( ) print_line ( "{0:<25} {1}" . format ( 'Tag' , 'Count' ) ) print_line ( "-" * 30 ) for entry in response . aggregations . tag_count . buckets : print_line ( "{0:<25} {1}" . format ( entry . key , entry . doc_count ) )
10767	def submit_poll ( self , poll , * , request_policy = None ) : if poll . id is not None : raise ExistingPoll ( ) options = poll . options data = { 'title' : poll . title , 'options' : options , 'multi' : poll . multi , 'dupcheck' : poll . dupcheck , 'captcha' : poll . captcha } return self . _http_client . post ( self . _POLLS , data = data , request_policy = request_policy , cls = strawpoll . Poll )
2533	def validate ( self , messages ) : messages = self . validate_checksum ( messages ) messages = self . validate_optional_str_fields ( messages ) messages = self . validate_mandatory_str_fields ( messages ) messages = self . validate_files ( messages ) messages = self . validate_mandatory_fields ( messages ) messages = self . validate_optional_fields ( messages ) return messages
6172	def _select_manager ( backend_name ) : if backend_name == 'RedisBackend' : lock_manager = _LockManagerRedis elif backend_name == 'DatabaseBackend' : lock_manager = _LockManagerDB else : raise NotImplementedError return lock_manager
1350	def write_json_response ( self , response ) : self . write ( tornado . escape . json_encode ( response ) ) self . set_header ( "Content-Type" , "application/json" )
12860	def add_period ( self , p , holiday_obj = None ) : if isinstance ( p , ( list , tuple ) ) : return [ BusinessDate . add_period ( self , pd ) for pd in p ] elif isinstance ( p , str ) : period = BusinessPeriod ( p ) else : period = p res = self res = BusinessDate . add_months ( res , period . months ) res = BusinessDate . add_years ( res , period . years ) res = BusinessDate . add_days ( res , period . days ) if period . businessdays : if holiday_obj : res = BusinessDate . add_business_days ( res , period . businessdays , holiday_obj ) else : res = BusinessDate . add_business_days ( res , period . businessdays , period . holiday ) return res
10658	def amount_fractions ( masses ) : n = amounts ( masses ) n_total = sum ( n . values ( ) ) return { compound : n [ compound ] / n_total for compound in n . keys ( ) }
8935	def provider ( workdir , commit = True , ** kwargs ) : return SCM_PROVIDER [ auto_detect ( workdir ) ] ( workdir , commit = commit , ** kwargs )
1884	def concretize ( self , symbolic , policy , maxcount = 7 ) : assert self . constraints == self . platform . constraints symbolic = self . migrate_expression ( symbolic ) vals = [ ] if policy == 'MINMAX' : vals = self . _solver . minmax ( self . _constraints , symbolic ) elif policy == 'MAX' : vals = self . _solver . max ( self . _constraints , symbolic ) elif policy == 'MIN' : vals = self . _solver . min ( self . _constraints , symbolic ) elif policy == 'SAMPLED' : m , M = self . _solver . minmax ( self . _constraints , symbolic ) vals += [ m , M ] if M - m > 3 : if self . _solver . can_be_true ( self . _constraints , symbolic == ( m + M ) // 2 ) : vals . append ( ( m + M ) // 2 ) if M - m > 100 : for i in ( 0 , 1 , 2 , 5 , 32 , 64 , 128 , 320 ) : if self . _solver . can_be_true ( self . _constraints , symbolic == m + i ) : vals . append ( m + i ) if maxcount <= len ( vals ) : break if M - m > 1000 and maxcount > len ( vals ) : vals += self . _solver . get_all_values ( self . _constraints , symbolic , maxcnt = maxcount - len ( vals ) , silent = True ) elif policy == 'ONE' : vals = [ self . _solver . get_value ( self . _constraints , symbolic ) ] else : assert policy == 'ALL' vals = solver . get_all_values ( self . _constraints , symbolic , maxcnt = maxcount , silent = True ) return tuple ( set ( vals ) )
3776	def T_dependent_property_derivative ( self , T , order = 1 ) : r if self . method : if self . test_method_validity ( T , self . method ) : try : return self . calculate_derivative ( T , self . method , order ) except : pass sorted_valid_methods = self . select_valid_methods ( T ) for method in sorted_valid_methods : try : return self . calculate_derivative ( T , method , order ) except : pass return None
246	def apply_slippage_penalty ( returns , txn_daily , simulate_starting_capital , backtest_starting_capital , impact = 0.1 ) : mult = simulate_starting_capital / backtest_starting_capital simulate_traded_shares = abs ( mult * txn_daily . amount ) simulate_traded_dollars = txn_daily . price * simulate_traded_shares simulate_pct_volume_used = simulate_traded_shares / txn_daily . volume penalties = simulate_pct_volume_used ** 2 * impact * simulate_traded_dollars daily_penalty = penalties . resample ( 'D' ) . sum ( ) daily_penalty = daily_penalty . reindex ( returns . index ) . fillna ( 0 ) portfolio_value = ep . cum_returns ( returns , starting_value = backtest_starting_capital ) * mult adj_returns = returns - ( daily_penalty / portfolio_value ) return adj_returns
13347	def cmd ( ) : if platform == 'win' : return [ 'cmd.exe' , '/K' ] elif platform == 'linux' : ppid = os . getppid ( ) ppid_cmdline_file = '/proc/{0}/cmdline' . format ( ppid ) try : with open ( ppid_cmdline_file ) as f : cmd = f . read ( ) if cmd . endswith ( '\x00' ) : cmd = cmd [ : - 1 ] cmd = cmd . split ( '\x00' ) return cmd + [ binpath ( 'subshell.sh' ) ] except : cmd = 'bash' else : cmd = 'bash' return [ cmd , binpath ( 'subshell.sh' ) ]
5181	def _url ( self , endpoint , path = None ) : log . debug ( '_url called with endpoint: {0} and path: {1}' . format ( endpoint , path ) ) try : endpoint = ENDPOINTS [ endpoint ] except KeyError : raise APIError url = '{base_url}/{endpoint}' . format ( base_url = self . base_url , endpoint = endpoint , ) if path is not None : url = '{0}/{1}' . format ( url , quote ( path ) ) return url
691	def _setTypes ( self , encoderSpec ) : if self . encoderType is None : if self . dataType in [ 'int' , 'float' ] : self . encoderType = 'adaptiveScalar' elif self . dataType == 'string' : self . encoderType = 'category' elif self . dataType in [ 'date' , 'datetime' ] : self . encoderType = 'date' if self . dataType is None : if self . encoderType in [ 'scalar' , 'adaptiveScalar' ] : self . dataType = 'float' elif self . encoderType in [ 'category' , 'enumeration' ] : self . dataType = 'string' elif self . encoderType in [ 'date' , 'datetime' ] : self . dataType = 'datetime'
3425	def get_metabolite_compartments ( self ) : warn ( 'use Model.compartments instead' , DeprecationWarning ) return { met . compartment for met in self . metabolites if met . compartment is not None }
1457	def valid_java_classpath ( classpath ) : paths = classpath . split ( ':' ) for path_entry in paths : if not valid_path ( path_entry . strip ( ) ) : return False return True
11190	def show ( dataset_uri ) : try : dataset = dtoolcore . ProtoDataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) except dtoolcore . DtoolCoreTypeError : dataset = dtoolcore . DataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) readme_content = dataset . get_readme_content ( ) click . secho ( readme_content )
278	def axes_style ( style = 'darkgrid' , rc = None ) : if rc is None : rc = { } rc_default = { } for name , val in rc_default . items ( ) : rc . setdefault ( name , val ) return sns . axes_style ( style = style , rc = rc )
8698	def __expect ( self , exp = '> ' , timeout = None ) : timeout_before = self . _port . timeout timeout = timeout or self . _timeout if SYSTEM != 'Windows' : if self . _port . timeout != MINIMAL_TIMEOUT : self . _port . timeout = MINIMAL_TIMEOUT end = time . time ( ) + timeout data = '' while not data . endswith ( exp ) and time . time ( ) <= end : data += self . _port . read ( ) log . debug ( 'expect returned: `{0}`' . format ( data ) ) if time . time ( ) > end : raise CommunicationTimeout ( 'Timeout waiting for data' , data ) if not data . endswith ( exp ) and len ( exp ) > 0 : raise BadResponseException ( 'Bad response.' , exp , data ) if SYSTEM != 'Windows' : self . _port . timeout = timeout_before return data
2354	def find_element ( self , strategy , locator ) : return self . driver_adapter . find_element ( strategy , locator , root = self . root )
9428	def printdir ( self ) : print ( "%-46s %19s %12s" % ( "File Name" , "Modified " , "Size" ) ) for rarinfo in self . filelist : date = "%d-%02d-%02d %02d:%02d:%02d" % rarinfo . date_time [ : 6 ] print ( "%-46s %s %12d" % ( rarinfo . filename , date , rarinfo . file_size ) )
13082	def register_plugins ( self ) : if len ( [ plugin for plugin in self . __plugins__ . values ( ) if plugin . clear_routes ] ) > 0 : self . _urls = list ( ) self . cached = list ( ) clear_assets = [ plugin for plugin in self . __plugins__ . values ( ) if plugin . clear_assets ] if len ( clear_assets ) > 0 and not self . prevent_plugin_clearing_assets : self . __assets__ = copy ( type ( self ) . ASSETS ) static_path = [ plugin . static_folder for plugin in clear_assets if plugin . static_folder ] if len ( static_path ) > 0 : self . static_folder = static_path [ - 1 ] for plugin in self . __plugins__ . values ( ) : self . _urls . extend ( [ ( url , function , methods , plugin ) for url , function , methods in plugin . routes ] ) self . _filters . extend ( [ ( filt , plugin ) for filt in plugin . filters ] ) self . __templates_namespaces__ . extend ( [ ( namespace , directory ) for namespace , directory in plugin . templates . items ( ) ] ) for asset_type in self . __assets__ : for key , value in plugin . assets [ asset_type ] . items ( ) : self . __assets__ [ asset_type ] [ key ] = value if plugin . augment : self . __plugins_render_views__ . append ( plugin ) if hasattr ( plugin , "CACHED" ) : for func in plugin . CACHED : self . cached . append ( ( getattr ( plugin , func ) , plugin ) ) plugin . register_nemo ( self )
12503	def _smooth_array ( arr , affine , fwhm = None , ensure_finite = True , copy = True , ** kwargs ) : if arr . dtype . kind == 'i' : if arr . dtype == np . int64 : arr = arr . astype ( np . float64 ) else : arr = arr . astype ( np . float32 ) if copy : arr = arr . copy ( ) if ensure_finite : arr [ np . logical_not ( np . isfinite ( arr ) ) ] = 0 if fwhm == 'fast' : arr = _fast_smooth_array ( arr ) elif fwhm is not None : affine = affine [ : 3 , : 3 ] fwhm_over_sigma_ratio = np . sqrt ( 8 * np . log ( 2 ) ) vox_size = np . sqrt ( np . sum ( affine ** 2 , axis = 0 ) ) sigma = fwhm / ( fwhm_over_sigma_ratio * vox_size ) for n , s in enumerate ( sigma ) : ndimage . gaussian_filter1d ( arr , s , output = arr , axis = n , ** kwargs ) return arr
199	def draw ( self , size = None , background_threshold = 0.01 , background_class_id = None , colors = None , return_foreground_mask = False ) : arr = self . get_arr_int ( background_threshold = background_threshold , background_class_id = background_class_id ) nb_classes = 1 + np . max ( arr ) segmap_drawn = np . zeros ( ( arr . shape [ 0 ] , arr . shape [ 1 ] , 3 ) , dtype = np . uint8 ) if colors is None : colors = SegmentationMapOnImage . DEFAULT_SEGMENT_COLORS ia . do_assert ( nb_classes <= len ( colors ) , "Can't draw all %d classes as it would exceed the maximum number of %d available colors." % ( nb_classes , len ( colors ) , ) ) ids_in_map = np . unique ( arr ) for c , color in zip ( sm . xrange ( nb_classes ) , colors ) : if c in ids_in_map : class_mask = ( arr == c ) segmap_drawn [ class_mask ] = color if return_foreground_mask : background_class_id = 0 if background_class_id is None else background_class_id foreground_mask = ( arr != background_class_id ) else : foreground_mask = None if size is not None : segmap_drawn = ia . imresize_single_image ( segmap_drawn , size , interpolation = "nearest" ) if foreground_mask is not None : foreground_mask = ia . imresize_single_image ( foreground_mask . astype ( np . uint8 ) , size , interpolation = "nearest" ) > 0 if foreground_mask is not None : return segmap_drawn , foreground_mask return segmap_drawn
4071	def split_multiline ( value ) : return [ element for element in ( line . strip ( ) for line in value . split ( '\n' ) ) if element ]
5513	def bytes_per_second ( ftp , retr = True ) : tot_bytes = 0 if retr : def request_file ( ) : ftp . voidcmd ( 'TYPE I' ) conn = ftp . transfercmd ( "retr " + TESTFN ) return conn with contextlib . closing ( request_file ( ) ) as conn : register_memory ( ) stop_at = time . time ( ) + 1.0 while stop_at > time . time ( ) : chunk = conn . recv ( BUFFER_LEN ) if not chunk : a = time . time ( ) ftp . voidresp ( ) conn . close ( ) conn = request_file ( ) stop_at += time . time ( ) - a tot_bytes += len ( chunk ) try : while chunk : chunk = conn . recv ( BUFFER_LEN ) ftp . voidresp ( ) conn . close ( ) except ( ftplib . error_temp , ftplib . error_perm ) : pass else : ftp . voidcmd ( 'TYPE I' ) with contextlib . closing ( ftp . transfercmd ( "STOR " + TESTFN ) ) as conn : register_memory ( ) chunk = b'x' * BUFFER_LEN stop_at = time . time ( ) + 1 while stop_at > time . time ( ) : tot_bytes += conn . send ( chunk ) ftp . voidresp ( ) return tot_bytes
6688	def groupinstall ( group , options = None ) : manager = MANAGER if options is None : options = [ ] elif isinstance ( options , str ) : options = [ options ] options = " " . join ( options ) run_as_root ( '%(manager)s %(options)s groupinstall "%(group)s"' % locals ( ) , pty = False )
10865	def _tile ( self , n ) : zsc = np . array ( [ 1.0 / self . zscale , 1 , 1 ] ) pos , rad = self . pos [ n ] , self . rad [ n ] pos = self . _trans ( pos ) return Tile ( pos - zsc * rad , pos + zsc * rad ) . pad ( self . support_pad )
3294	def is_locked ( self ) : if self . provider . lock_manager is None : return False return self . provider . lock_manager . is_url_locked ( self . get_ref_url ( ) )
5265	def backslashcase ( string ) : str1 = re . sub ( r"_" , r"\\" , snakecase ( string ) ) return str1
8956	def compile_glob ( spec ) : parsed = "" . join ( parse_glob ( spec ) ) regex = "^{0}$" . format ( parsed ) return re . compile ( regex )
13182	def writerow ( self , observation_data ) : if isinstance ( observation_data , ( list , tuple ) ) : row = observation_data else : row = self . dict_to_row ( observation_data ) self . writer . writerow ( row )
8863	def run_pep8 ( request_data ) : import pycodestyle from pyqode . python . backend . pep8utils import CustomChecker WARNING = 1 code = request_data [ 'code' ] path = request_data [ 'path' ] max_line_length = request_data [ 'max_line_length' ] ignore_rules = request_data [ 'ignore_rules' ] ignore_rules += [ 'W291' , 'W292' , 'W293' , 'W391' ] pycodestyle . MAX_LINE_LENGTH = max_line_length pep8style = pycodestyle . StyleGuide ( parse_argv = False , config_file = '' , checker_class = CustomChecker ) try : results = pep8style . input_file ( path , lines = code . splitlines ( True ) ) except Exception : _logger ( ) . exception ( 'Failed to run PEP8 analysis with data=%r' % request_data ) return [ ] else : messages = [ ] for line_number , offset , code , text , doc in results : if code in ignore_rules : continue messages . append ( ( '[PEP8] %s: %s' % ( code , text ) , WARNING , line_number - 1 ) ) return messages
12655	def remove_dcm2nii_underprocessed ( filepaths ) : cln_flist = [ ] len_sorted = sorted ( filepaths , key = len ) for idx , fpath in enumerate ( len_sorted ) : remove = False fname = op . basename ( fpath ) rest = len_sorted [ idx + 1 : ] for rest_fpath in rest : rest_file = op . basename ( rest_fpath ) if rest_file . endswith ( fname ) : remove = True break if not remove : cln_flist . append ( fpath ) return cln_flist
13297	def default ( self , obj ) : if isinstance ( obj , datetime . datetime ) : return self . _encode_datetime ( obj ) return json . JSONEncoder . default ( self , obj )
12388	def set ( self , target , value ) : if not self . _set : return if self . path is None : self . set = lambda * a : None return None if self . _segments [ target . __class__ ] : self . get ( target ) if self . _segments [ target . __class__ ] : return parent_getter = compose ( * self . _getters [ target . __class__ ] [ : - 1 ] ) target = parent_getter ( target ) func = self . _make_setter ( self . path . split ( '.' ) [ - 1 ] , target . __class__ ) func ( target , value ) def setter ( target , value ) : func ( parent_getter ( target ) , value ) self . set = setter
5407	def _operation_status ( self ) : if not google_v2_operations . is_done ( self . _op ) : return 'RUNNING' if google_v2_operations . is_success ( self . _op ) : return 'SUCCESS' if google_v2_operations . is_canceled ( self . _op ) : return 'CANCELED' if google_v2_operations . is_failed ( self . _op ) : return 'FAILURE' raise ValueError ( 'Status for operation {} could not be determined' . format ( self . _op [ 'name' ] ) )
5919	def center_fit ( self , ** kwargs ) : kwargs . setdefault ( 's' , self . tpr ) kwargs . setdefault ( 'n' , self . ndx ) kwargs [ 'f' ] = self . xtc kwargs . setdefault ( 'o' , self . outfile ( self . infix_filename ( None , self . xtc , '_centfit' , 'xtc' ) ) ) force = kwargs . pop ( 'force' , self . force ) logger . info ( "Centering and fitting trajectory {f!r}..." . format ( ** kwargs ) ) with utilities . in_dir ( self . dirname ) : if not self . check_file_exists ( kwargs [ 'o' ] , resolve = "indicate" , force = force ) : trj_fitandcenter ( ** kwargs ) logger . info ( "Centered and fit trajectory: {o!r}." . format ( ** kwargs ) ) return { 'tpr' : self . rp ( kwargs [ 's' ] ) , 'xtc' : self . rp ( kwargs [ 'o' ] ) }
11049	def _handle_field_value ( self , field , value ) : if field == 'event' : self . _event = value elif field == 'data' : self . _data_lines . append ( value ) elif field == 'id' : pass elif field == 'retry' : pass
7007	def _fourier_func ( fourierparams , phase , mags ) : order = int ( len ( fourierparams ) / 2 ) f_amp = fourierparams [ : order ] f_pha = fourierparams [ order : ] f_orders = [ f_amp [ x ] * npcos ( 2.0 * pi_value * x * phase + f_pha [ x ] ) for x in range ( order ) ] total_f = npmedian ( mags ) for fo in f_orders : total_f += fo return total_f
9797	def delete ( ctx ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) if not click . confirm ( "Are sure you want to delete experiment group `{}`" . format ( _group ) ) : click . echo ( 'Existing without deleting experiment group.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . experiment_group . delete_experiment_group ( user , project_name , _group ) GroupManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete experiment group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Experiment group `{}` was delete successfully" . format ( _group ) )
13097	def terminate_processes ( self ) : if self . relay : self . relay . terminate ( ) if self . responder : self . responder . terminate ( )
9218	def _smixins ( self , name ) : return ( self . _mixins [ name ] if name in self . _mixins else False )
4039	def _cache ( self , response , key ) : thetime = datetime . datetime . utcnow ( ) . replace ( tzinfo = pytz . timezone ( "GMT" ) ) self . templates [ key ] = { "tmplt" : response . json ( ) , "updated" : thetime } return copy . deepcopy ( response . json ( ) )
196	def Clouds ( name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return meta . SomeOf ( ( 1 , 2 ) , children = [ CloudLayer ( intensity_mean = ( 196 , 255 ) , intensity_freq_exponent = ( - 2.5 , - 2.0 ) , intensity_coarse_scale = 10 , alpha_min = 0 , alpha_multiplier = ( 0.25 , 0.75 ) , alpha_size_px_max = ( 2 , 8 ) , alpha_freq_exponent = ( - 2.5 , - 2.0 ) , sparsity = ( 0.8 , 1.0 ) , density_multiplier = ( 0.5 , 1.0 ) ) , CloudLayer ( intensity_mean = ( 196 , 255 ) , intensity_freq_exponent = ( - 2.0 , - 1.0 ) , intensity_coarse_scale = 10 , alpha_min = 0 , alpha_multiplier = ( 0.5 , 1.0 ) , alpha_size_px_max = ( 64 , 128 ) , alpha_freq_exponent = ( - 2.0 , - 1.0 ) , sparsity = ( 1.0 , 1.4 ) , density_multiplier = ( 0.8 , 1.5 ) ) ] , random_order = False , name = name , deterministic = deterministic , random_state = random_state )
8423	def hls_palette ( n_colors = 6 , h = .01 , l = .6 , s = .65 ) : hues = np . linspace ( 0 , 1 , n_colors + 1 ) [ : - 1 ] hues += h hues %= 1 hues -= hues . astype ( int ) palette = [ colorsys . hls_to_rgb ( h_i , l , s ) for h_i in hues ] return palette
3648	def applyConsumable ( self , item_id , resource_id ) : method = 'POST' url = 'item/resource/%s' % resource_id data = { 'apply' : [ { 'id' : item_id } ] } self . __request__ ( method , url , data = json . dumps ( data ) )
315	def gross_lev ( positions ) : exposure = positions . drop ( 'cash' , axis = 1 ) . abs ( ) . sum ( axis = 1 ) return exposure / positions . sum ( axis = 1 )
2166	def list_commands ( self , ctx ) : commands = set ( self . list_resource_commands ( ) ) commands . union ( set ( self . list_misc_commands ( ) ) ) return sorted ( commands )
5414	def get_provider ( args , resources ) : provider = getattr ( args , 'provider' , 'google' ) if provider == 'google' : return google . GoogleJobProvider ( getattr ( args , 'verbose' , False ) , getattr ( args , 'dry_run' , False ) , args . project ) elif provider == 'google-v2' : return google_v2 . GoogleV2JobProvider ( getattr ( args , 'verbose' , False ) , getattr ( args , 'dry_run' , False ) , args . project ) elif provider == 'local' : return local . LocalJobProvider ( resources ) elif provider == 'test-fails' : return test_fails . FailsJobProvider ( ) else : raise ValueError ( 'Unknown provider: ' + provider )
4719	def tsuite_setup ( trun , declr , enum ) : suite = copy . deepcopy ( TESTSUITE ) suite [ "name" ] = declr . get ( "name" ) if suite [ "name" ] is None : cij . err ( "rnr:tsuite_setup: no testsuite is given" ) return None suite [ "alias" ] = declr . get ( "alias" ) suite [ "ident" ] = "%s_%d" % ( suite [ "name" ] , enum ) suite [ "res_root" ] = os . sep . join ( [ trun [ "conf" ] [ "OUTPUT" ] , suite [ "ident" ] ] ) suite [ "aux_root" ] = os . sep . join ( [ suite [ "res_root" ] , "_aux" ] ) suite [ "evars" ] . update ( copy . deepcopy ( trun [ "evars" ] ) ) suite [ "evars" ] . update ( copy . deepcopy ( declr . get ( "evars" , { } ) ) ) os . makedirs ( suite [ "res_root" ] ) os . makedirs ( suite [ "aux_root" ] ) suite [ "hooks" ] = hooks_setup ( trun , suite , declr . get ( "hooks" ) ) suite [ "hooks_pr_tcase" ] = declr . get ( "hooks_pr_tcase" , [ ] ) suite [ "fname" ] = "%s.suite" % suite [ "name" ] suite [ "fpath" ] = os . sep . join ( [ trun [ "conf" ] [ "TESTSUITES" ] , suite [ "fname" ] ] ) tcase_fpaths = [ ] if os . path . exists ( suite [ "fpath" ] ) : suite_lines = ( l . strip ( ) for l in open ( suite [ "fpath" ] ) . read ( ) . splitlines ( ) ) tcase_fpaths . extend ( ( l for l in suite_lines if len ( l ) > 1 and l [ 0 ] != "#" ) ) else : tcase_fpaths . extend ( declr . get ( "testcases" , [ ] ) ) if len ( set ( tcase_fpaths ) ) != len ( tcase_fpaths ) : cij . err ( "rnr:suite: failed: duplicate tcase in suite not supported" ) return None for tcase_fname in tcase_fpaths : tcase = tcase_setup ( trun , suite , tcase_fname ) if not tcase : cij . err ( "rnr:suite: failed: tcase_setup" ) return None suite [ "testcases" ] . append ( tcase ) return suite
12702	def _get_params ( target , param , dof ) : return [ target . getParam ( getattr ( ode , 'Param{}{}' . format ( param , s ) ) ) for s in [ '' , '2' , '3' ] [ : dof ] ]
8210	def contains_point ( self , x , y , d = 2 ) : if self . path != None and len ( self . path ) > 1 and self . path . contains ( x , y ) : if not self . path . contains ( x + d , y ) or not self . path . contains ( x , y + d ) or not self . path . contains ( x - d , y ) or not self . path . contains ( x , y - d ) or not self . path . contains ( x + d , y + d ) or not self . path . contains ( x - d , y - d ) or not self . path . contains ( x + d , y - d ) or not self . path . contains ( x - d , y + d ) : return True return False
4671	def unlock ( self , pwd ) : if self . store . is_encrypted ( ) : return self . store . unlock ( pwd )
2568	def construct_start_message ( self ) : uname = getpass . getuser ( ) . encode ( 'latin1' ) hashed_username = hashlib . sha256 ( uname ) . hexdigest ( ) [ 0 : 10 ] hname = socket . gethostname ( ) . encode ( 'latin1' ) hashed_hostname = hashlib . sha256 ( hname ) . hexdigest ( ) [ 0 : 10 ] message = { 'uuid' : self . uuid , 'uname' : hashed_username , 'hname' : hashed_hostname , 'test' : self . test_mode , 'parsl_v' : self . parsl_version , 'python_v' : self . python_version , 'os' : platform . system ( ) , 'os_v' : platform . release ( ) , 'start' : time . time ( ) } return json . dumps ( message )
2653	def push_file ( self , local_source , remote_dir ) : remote_dest = remote_dir + '/' + os . path . basename ( local_source ) try : self . makedirs ( remote_dir , exist_ok = True ) except IOError as e : logger . exception ( "Pushing {0} to {1} failed" . format ( local_source , remote_dir ) ) if e . errno == 2 : raise BadScriptPath ( e , self . hostname ) elif e . errno == 13 : raise BadPermsScriptPath ( e , self . hostname ) else : logger . exception ( "File push failed due to SFTP client failure" ) raise FileCopyException ( e , self . hostname ) try : self . sftp_client . put ( local_source , remote_dest , confirm = True ) self . sftp_client . chmod ( remote_dest , 0o777 ) except Exception as e : logger . exception ( "File push from local source {} to remote destination {} failed" . format ( local_source , remote_dest ) ) raise FileCopyException ( e , self . hostname ) return remote_dest
13079	def render ( self , template , ** kwargs ) : kwargs [ "cache_key" ] = "%s" % kwargs [ "url" ] . values ( ) kwargs [ "lang" ] = self . get_locale ( ) kwargs [ "assets" ] = self . assets kwargs [ "main_collections" ] = self . main_collections ( kwargs [ "lang" ] ) kwargs [ "cache_active" ] = self . cache is not None kwargs [ "cache_time" ] = 0 kwargs [ "cache_key" ] , kwargs [ "cache_key_i18n" ] = self . make_cache_keys ( request . endpoint , kwargs [ "url" ] ) kwargs [ "template" ] = template for plugin in self . __plugins_render_views__ : kwargs . update ( plugin . render ( ** kwargs ) ) return render_template ( kwargs [ "template" ] , ** kwargs )
4312	def _validate_volumes ( input_volumes ) : if not ( input_volumes is None or isinstance ( input_volumes , list ) ) : raise TypeError ( "input_volumes must be None or a list." ) if isinstance ( input_volumes , list ) : for vol in input_volumes : if not core . is_number ( vol ) : raise ValueError ( "Elements of input_volumes must be numbers: found {}" . format ( vol ) )
11257	def values ( prev , * keys , ** kw ) : d = next ( prev ) if isinstance ( d , dict ) : yield [ d [ k ] for k in keys if k in d ] for d in prev : yield [ d [ k ] for k in keys if k in d ] else : yield [ d [ i ] for i in keys if 0 <= i < len ( d ) ] for d in prev : yield [ d [ i ] for i in keys if 0 <= i < len ( d ) ]
1972	def _interp_total_size ( interp ) : load_segs = [ x for x in interp . iter_segments ( ) if x . header . p_type == 'PT_LOAD' ] last = load_segs [ - 1 ] return last . header . p_vaddr + last . header . p_memsz
4796	def contains_entry ( self , * args , ** kwargs ) : self . _check_dict_like ( self . val , check_values = False ) entries = list ( args ) + [ { k : v } for k , v in kwargs . items ( ) ] if len ( entries ) == 0 : raise ValueError ( 'one or more entry args must be given' ) missing = [ ] for e in entries : if type ( e ) is not dict : raise TypeError ( 'given entry arg must be a dict' ) if len ( e ) != 1 : raise ValueError ( 'given entry args must contain exactly one key-value pair' ) k = next ( iter ( e ) ) if k not in self . val : missing . append ( e ) elif self . val [ k ] != e [ k ] : missing . append ( e ) if missing : self . _err ( 'Expected <%s> to contain entries %s, but did not contain %s.' % ( self . val , self . _fmt_items ( entries ) , self . _fmt_items ( missing ) ) ) return self
371	def flip_axis_multi ( x , axis , is_random = False ) : if is_random : factor = np . random . uniform ( - 1 , 1 ) if factor > 0 : results = [ ] for data in x : data = np . asarray ( data ) . swapaxes ( axis , 0 ) data = data [ : : - 1 , ... ] data = data . swapaxes ( 0 , axis ) results . append ( data ) return np . asarray ( results ) else : return np . asarray ( x ) else : results = [ ] for data in x : data = np . asarray ( data ) . swapaxes ( axis , 0 ) data = data [ : : - 1 , ... ] data = data . swapaxes ( 0 , axis ) results . append ( data ) return np . asarray ( results )
3391	def prune_unused_metabolites ( cobra_model ) : output_model = cobra_model . copy ( ) inactive_metabolites = [ m for m in output_model . metabolites if len ( m . reactions ) == 0 ] output_model . remove_metabolites ( inactive_metabolites ) return output_model , inactive_metabolites
7601	def get_popular_decks ( self , ** params : keys ) : url = self . api . POPULAR + '/decks' return self . _get_model ( url , ** params )
324	def rolling_volatility ( returns , rolling_vol_window ) : return returns . rolling ( rolling_vol_window ) . std ( ) * np . sqrt ( APPROX_BDAYS_PER_YEAR )
8700	def __exchange ( self , output , timeout = None ) : self . __writeln ( output ) self . _port . flush ( ) return self . __expect ( timeout = timeout or self . _timeout )
2353	def wait_for_region_to_load ( self ) : self . wait . until ( lambda _ : self . loaded ) self . pm . hook . pypom_after_wait_for_region_to_load ( region = self ) return self
4369	def send ( self , message , json = False , callback = None ) : pkt = dict ( type = "message" , data = message , endpoint = self . ns_name ) if json : pkt [ 'type' ] = "json" if callback : pkt [ 'ack' ] = True pkt [ 'id' ] = msgid = self . socket . _get_next_msgid ( ) self . socket . _save_ack_callback ( msgid , callback ) self . socket . send_packet ( pkt )
4688	def get_shared_secret ( priv , pub ) : pub_point = pub . point ( ) priv_point = int ( repr ( priv ) , 16 ) res = pub_point * priv_point res_hex = "%032x" % res . x ( ) res_hex = "0" * ( 64 - len ( res_hex ) ) + res_hex return res_hex
9182	def validate_model ( cursor , model ) : _validate_license ( model ) _validate_roles ( model ) required_metadata = ( 'title' , 'summary' , ) for metadata_key in required_metadata : if model . metadata . get ( metadata_key ) in [ None , '' , [ ] ] : raise exceptions . MissingRequiredMetadata ( metadata_key ) _validate_derived_from ( cursor , model ) _validate_subjects ( cursor , model )
10274	def prune_mechanism_by_data ( graph , key : Optional [ str ] = None ) -> None : remove_unweighted_leaves ( graph , key = key ) remove_unweighted_sources ( graph , key = key )
2012	def instruction ( self ) : try : _decoding_cache = getattr ( self , '_decoding_cache' ) except Exception : _decoding_cache = self . _decoding_cache = { } pc = self . pc if isinstance ( pc , Constant ) : pc = pc . value if pc in _decoding_cache : return _decoding_cache [ pc ] def getcode ( ) : bytecode = self . bytecode for pc_i in range ( pc , len ( bytecode ) ) : yield simplify ( bytecode [ pc_i ] ) . value while True : yield 0 instruction = EVMAsm . disassemble_one ( getcode ( ) , pc = pc , fork = DEFAULT_FORK ) _decoding_cache [ pc ] = instruction return instruction
4907	def _sync_content_metadata ( self , serialized_data , http_method ) : try : status_code , response_body = getattr ( self , '_' + http_method ) ( urljoin ( self . enterprise_configuration . degreed_base_url , self . global_degreed_config . course_api_path ) , serialized_data , self . CONTENT_PROVIDER_SCOPE ) except requests . exceptions . RequestException as exc : raise ClientError ( 'DegreedAPIClient request failed: {error} {message}' . format ( error = exc . __class__ . __name__ , message = str ( exc ) ) ) if status_code >= 400 : raise ClientError ( 'DegreedAPIClient request failed with status {status_code}: {message}' . format ( status_code = status_code , message = response_body ) )
105	def pool ( arr , block_size , func , cval = 0 , preserve_dtype = True ) : from . import dtypes as iadt iadt . gate_dtypes ( arr , allowed = [ "bool" , "uint8" , "uint16" , "uint32" , "int8" , "int16" , "int32" , "float16" , "float32" , "float64" , "float128" ] , disallowed = [ "uint64" , "uint128" , "uint256" , "int64" , "int128" , "int256" , "float256" ] , augmenter = None ) do_assert ( arr . ndim in [ 2 , 3 ] ) is_valid_int = is_single_integer ( block_size ) and block_size >= 1 is_valid_tuple = is_iterable ( block_size ) and len ( block_size ) in [ 2 , 3 ] and [ is_single_integer ( val ) and val >= 1 for val in block_size ] do_assert ( is_valid_int or is_valid_tuple ) if is_single_integer ( block_size ) : block_size = [ block_size , block_size ] if len ( block_size ) < arr . ndim : block_size = list ( block_size ) + [ 1 ] input_dtype = arr . dtype arr_reduced = skimage . measure . block_reduce ( arr , tuple ( block_size ) , func , cval = cval ) if preserve_dtype and arr_reduced . dtype . type != input_dtype : arr_reduced = arr_reduced . astype ( input_dtype ) return arr_reduced
9116	def reset_cleansers ( confirm = True ) : if value_asbool ( confirm ) and not yesno ( ) : exit ( "Glad I asked..." ) get_vars ( ) cleanser_count = AV [ 'ploy_cleanser_count' ] fab . run ( 'ezjail-admin stop worker' ) for cleanser_index in range ( cleanser_count ) : cindex = '{:02d}' . format ( cleanser_index + 1 ) fab . run ( 'ezjail-admin stop cleanser_{cindex}' . format ( cindex = cindex ) ) with fab . warn_only ( ) : fab . run ( 'zfs destroy tank/jails/cleanser_{cindex}@jdispatch_rollback' . format ( cindex = cindex ) ) fab . run ( 'ezjail-admin delete -fw cleanser_{cindex}' . format ( cindex = cindex ) ) fab . run ( 'umount -f /usr/jails/cleanser_{cindex}' . format ( cindex = cindex ) ) fab . run ( 'rm -rf /usr/jails/cleanser_{cindex}' . format ( cindex = cindex ) ) with fab . warn_only ( ) : fab . run ( 'zfs destroy -R tank/jails/cleanser@clonesource' ) fab . run ( 'ezjail-admin start worker' ) fab . run ( 'ezjail-admin stop cleanser' ) fab . run ( 'ezjail-admin start cleanser' )
13836	def _MergeMessageField ( self , tokenizer , message , field ) : is_map_entry = _IsMapEntry ( field ) if tokenizer . TryConsume ( '<' ) : end_token = '>' else : tokenizer . Consume ( '{' ) end_token = '}' if field . label == descriptor . FieldDescriptor . LABEL_REPEATED : if field . is_extension : sub_message = message . Extensions [ field ] . add ( ) elif is_map_entry : sub_message = field . message_type . _concrete_class ( ) else : sub_message = getattr ( message , field . name ) . add ( ) else : if field . is_extension : sub_message = message . Extensions [ field ] else : sub_message = getattr ( message , field . name ) sub_message . SetInParent ( ) while not tokenizer . TryConsume ( end_token ) : if tokenizer . AtEnd ( ) : raise tokenizer . ParseErrorPreviousToken ( 'Expected "%s".' % ( end_token , ) ) self . _MergeField ( tokenizer , sub_message ) if is_map_entry : value_cpptype = field . message_type . fields_by_name [ 'value' ] . cpp_type if value_cpptype == descriptor . FieldDescriptor . CPPTYPE_MESSAGE : value = getattr ( message , field . name ) [ sub_message . key ] value . MergeFrom ( sub_message . value ) else : getattr ( message , field . name ) [ sub_message . key ] = sub_message . value
2592	def get_all_checkpoints ( rundir = "runinfo" ) : if ( not os . path . isdir ( rundir ) ) : return [ ] dirs = sorted ( os . listdir ( rundir ) ) checkpoints = [ ] for runid in dirs : checkpoint = os . path . abspath ( '{}/{}/checkpoint' . format ( rundir , runid ) ) if os . path . isdir ( checkpoint ) : checkpoints . append ( checkpoint ) return checkpoints
6794	def shell ( self ) : r = self . local_renderer if '@' in self . genv . host_string : r . env . shell_host_string = self . genv . host_string else : r . env . shell_host_string = '{user}@{host_string}' r . env . shell_default_dir = self . genv . shell_default_dir_template r . env . shell_interactive_djshell_str = self . genv . interactive_shell_template r . run_or_local ( 'ssh -t -i {key_filename} {shell_host_string} "{shell_interactive_djshell_str}"' )
2776	def remove_droplets ( self , droplet_ids ) : return self . get_data ( "load_balancers/%s/droplets/" % self . id , type = DELETE , params = { "droplet_ids" : droplet_ids } )
3185	def get ( self , store_id , product_id , image_id , ** queryparams ) : self . store_id = store_id self . product_id = product_id self . image_id = image_id return self . _mc_client . _post ( url = self . _build_path ( store_id , 'products' , product_id , 'images' , image_id ) , ** queryparams )
7291	def set_fields ( self ) : if self . is_initialized : self . model_map_dict = self . create_document_dictionary ( self . model_instance ) else : self . model_map_dict = self . create_document_dictionary ( self . model ) form_field_dict = self . get_form_field_dict ( self . model_map_dict ) self . set_form_fields ( form_field_dict )
10785	def add_missing_particles ( st , rad = 'calc' , tries = 50 , ** kwargs ) : if rad == 'calc' : rad = guess_add_radii ( st ) guess , npart = feature_guess ( st , rad , ** kwargs ) tries = np . min ( [ tries , npart ] ) accepts , new_poses = check_add_particles ( st , guess [ : tries ] , rad = rad , ** kwargs ) return accepts , new_poses
7525	def draw ( self , show_tip_labels = True , show_node_support = False , use_edge_lengths = False , orient = "right" , print_args = False , * args , ** kwargs ) : self . _decompose_tree ( orient = orient , use_edge_lengths = use_edge_lengths ) dwargs = { } dwargs [ "show_tip_labels" ] = show_tip_labels dwargs [ "show_node_support" ] = show_node_support dwargs . update ( kwargs ) canvas , axes , panel = tree_panel_plot ( self , print_args , ** dwargs ) return canvas , axes , panel
13301	def df_quantile ( df , nb = 100 ) : quantiles = np . linspace ( 0 , 1. , nb ) res = pd . DataFrame ( ) for q in quantiles : res = res . append ( df . quantile ( q ) , ignore_index = True ) return res
300	def plot_slippage_sensitivity ( returns , positions , transactions , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) avg_returns_given_slippage = pd . Series ( ) for bps in range ( 1 , 100 ) : adj_returns = txn . adjust_returns_for_slippage ( returns , positions , transactions , bps ) avg_returns = ep . annual_return ( adj_returns ) avg_returns_given_slippage . loc [ bps ] = avg_returns avg_returns_given_slippage . plot ( alpha = 1.0 , lw = 2 , ax = ax ) ax . set_title ( 'Average annual returns given additional per-dollar slippage' ) ax . set_xticks ( np . arange ( 0 , 100 , 10 ) ) ax . set_ylabel ( 'Average annual return' ) ax . set_xlabel ( 'Per-dollar slippage (bps)' ) return ax
11178	def get_separator ( self , i ) : return i and self . separator [ min ( i - 1 , len ( self . separator ) - 1 ) ] or ''
5078	def get_current_course_run ( course , users_active_course_runs ) : current_course_run = None filtered_course_runs = [ ] all_course_runs = course [ 'course_runs' ] if users_active_course_runs : current_course_run = get_closest_course_run ( users_active_course_runs ) else : for course_run in all_course_runs : if is_course_run_enrollable ( course_run ) and is_course_run_upgradeable ( course_run ) : filtered_course_runs . append ( course_run ) if not filtered_course_runs : filtered_course_runs = all_course_runs if filtered_course_runs : current_course_run = get_closest_course_run ( filtered_course_runs ) return current_course_run
5248	def custom_req ( session , request ) : while ( session . tryNextEvent ( ) ) : pass print ( "Sending Request:\n %s" % request ) session . sendRequest ( request ) messages = [ ] while ( True ) : ev = session . nextEvent ( 500 ) for msg in ev : print ( "Message Received:\n %s" % msg ) messages . append ( msg ) if ev . eventType ( ) == blpapi . Event . RESPONSE : break return messages
9449	def schedule_hangup ( self , call_params ) : path = '/' + self . api_version + '/ScheduleHangup/' method = 'POST' return self . request ( path , method , call_params )
5873	def deserialize_organization ( organization_dict ) : return models . Organization ( id = organization_dict . get ( 'id' ) , name = organization_dict . get ( 'name' , '' ) , short_name = organization_dict . get ( 'short_name' , '' ) , description = organization_dict . get ( 'description' , '' ) , logo = organization_dict . get ( 'logo' , '' ) )
10059	def location ( ) : d = current_app . config [ 'DATADIR' ] with db . session . begin_nested ( ) : Location . query . delete ( ) loc = Location ( name = 'local' , uri = d , default = True ) db . session . add ( loc ) db . session . commit ( )
2435	def reset_creation_info ( self ) : self . created_date_set = False self . creation_comment_set = False self . lics_list_ver_set = False
4640	def find_next ( self ) : if int ( self . num_retries ) < 0 : self . _cnt_retries += 1 sleeptime = ( self . _cnt_retries - 1 ) * 2 if self . _cnt_retries < 10 else 10 if sleeptime : log . warning ( "Lost connection to node during rpcexec(): %s (%d/%d) " % ( self . url , self . _cnt_retries , self . num_retries ) + "Retrying in %d seconds" % sleeptime ) sleep ( sleeptime ) return next ( self . urls ) urls = [ k for k , v in self . _url_counter . items ( ) if ( int ( self . num_retries ) >= 0 and v <= self . num_retries and ( k != self . url or len ( self . _url_counter ) == 1 ) ) ] if not len ( urls ) : raise NumRetriesReached url = urls [ 0 ] return url
12534	def update ( self , dicomset ) : if not isinstance ( dicomset , DicomFileSet ) : raise ValueError ( 'Given dicomset is not a DicomFileSet.' ) self . items = list ( set ( self . items ) . update ( dicomset ) )
1934	def get_constructor_arguments ( self ) -> str : item = self . _constructor_abi_item return '()' if item is None else self . tuple_signature_for_components ( item [ 'inputs' ] )
12713	def join_to ( self , joint , other_body = None , ** kwargs ) : self . world . join ( joint , self , other_body , ** kwargs )
2882	def get_timer_event_definition ( self , timerEventDefinition ) : timeDate = first ( self . xpath ( './/bpmn:timeDate' ) ) return TimerEventDefinition ( self . node . get ( 'name' , timeDate . text ) , self . parser . parse_condition ( timeDate . text , None , None , None , None , self ) )
404	def swish ( x , name = 'swish' ) : with tf . name_scope ( name ) : x = tf . nn . sigmoid ( x ) * x return x
7717	def _roster_set ( self , item , callback , error_callback ) : stanza = Iq ( to_jid = self . server , stanza_type = "set" ) payload = RosterPayload ( [ item ] ) stanza . set_payload ( payload ) def success_cb ( result_stanza ) : if callback : callback ( item ) def error_cb ( error_stanza ) : if error_callback : error_callback ( error_stanza ) else : logger . error ( "Roster change of '{0}' failed" . format ( item . jid ) ) processor = self . stanza_processor processor . set_response_handlers ( stanza , success_cb , error_cb ) processor . send ( stanza )
6116	def unmasked_for_shape_and_pixel_scale ( cls , shape , pixel_scale , invert = False ) : mask = np . full ( tuple ( map ( lambda d : int ( d ) , shape ) ) , False ) if invert : mask = np . invert ( mask ) return cls ( array = mask , pixel_scale = pixel_scale )
4176	def window_gaussian ( N , alpha = 2.5 ) : r t = linspace ( - ( N - 1 ) / 2. , ( N - 1 ) / 2. , N ) w = exp ( - 0.5 * ( alpha * t / ( N / 2. ) ) ** 2. ) return w
6193	def add ( self , num_particles , D ) : self . _plist += self . _generate ( num_particles , D , box = self . box , rs = self . rs )
10890	def contains ( self , items , pad = 0 ) : o = ( ( items >= self . l - pad ) & ( items < self . r + pad ) ) if len ( o . shape ) == 2 : o = o . all ( axis = - 1 ) elif len ( o . shape ) == 1 : o = o . all ( ) return o
2760	def get_load_balancer ( self , id ) : return LoadBalancer . get_object ( api_token = self . token , id = id )
6189	def get_bromo_fnames_da ( d_em_kHz , d_bg_kHz , a_em_kHz , a_bg_kHz , ID = '1+2+3+4+5+6' , t_tot = '480' , num_p = '30' , pM = '64' , t_step = 0.5e-6 , D = 1.2e-11 , dir_ = '' ) : clk_p = t_step / 32. E_sim = 1. * a_em_kHz / ( a_em_kHz + d_em_kHz ) FRET_val = 100. * E_sim print ( "Simulated FRET value: %.1f%%" % FRET_val ) d_em_kHz_str = "%04d" % d_em_kHz a_em_kHz_str = "%04d" % a_em_kHz d_bg_kHz_str = "%04.1f" % d_bg_kHz a_bg_kHz_str = "%04.1f" % a_bg_kHz print ( "D: EM %s BG %s " % ( d_em_kHz_str , d_bg_kHz_str ) ) print ( "A: EM %s BG %s " % ( a_em_kHz_str , a_bg_kHz_str ) ) fname_d = ( 'ph_times_{t_tot}s_D{D}_{np}P_{pM}pM_' 'step{ts_us}us_ID{ID}_EM{em}kHz_BG{bg}kHz.npy' ) . format ( em = d_em_kHz_str , bg = d_bg_kHz_str , t_tot = t_tot , pM = pM , np = num_p , ID = ID , ts_us = t_step * 1e6 , D = D ) fname_a = ( 'ph_times_{t_tot}s_D{D}_{np}P_{pM}pM_' 'step{ts_us}us_ID{ID}_EM{em}kHz_BG{bg}kHz.npy' ) . format ( em = a_em_kHz_str , bg = a_bg_kHz_str , t_tot = t_tot , pM = pM , np = num_p , ID = ID , ts_us = t_step * 1e6 , D = D ) print ( fname_d ) print ( fname_a ) name = ( 'BroSim_E{:.1f}_dBG{:.1f}k_aBG{:.1f}k_' 'dEM{:.0f}k' ) . format ( FRET_val , d_bg_kHz , a_bg_kHz , d_em_kHz ) return dir_ + fname_d , dir_ + fname_a , name , clk_p , E_sim
1371	def get_heron_dir ( ) : go_above_dirs = 9 path = "/" . join ( os . path . realpath ( __file__ ) . split ( '/' ) [ : - go_above_dirs ] ) return normalized_class_path ( path )
12743	def get_urls ( self ) : urls = self . get_subfields ( "856" , "u" , i1 = "4" , i2 = "2" ) return map ( lambda x : x . replace ( "&amp;" , "&" ) , urls )
4404	def lock ( self , name , timeout = None , sleep = 0.1 ) : return Lock ( self , name , timeout = timeout , sleep = sleep )
9178	def obtain_licenses ( ) : with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( ) licenses = { r [ 0 ] : r [ 1 ] for r in cursor . fetchall ( ) } return licenses
2652	def execute_no_wait ( self , cmd , walltime = 2 , envs = { } ) : stdin , stdout , stderr = self . ssh_client . exec_command ( self . prepend_envs ( cmd , envs ) , bufsize = - 1 , timeout = walltime ) return None , stdout , stderr
12074	def _update_state ( self , vals ) : self . _steps_complete += 1 if self . _steps_complete == self . max_steps : self . _termination_info = ( False , self . _best_val , self . _arg ) return StopIteration arg_inc , arg_dec = vals best_val = min ( arg_inc , arg_dec , self . _best_val ) if best_val == self . _best_val : self . _termination_info = ( True , best_val , self . _arg ) return StopIteration self . _arg += self . stepsize if ( arg_dec > arg_inc ) else - self . stepsize self . _best_val = best_val return [ { self . key : self . _arg + self . stepsize } , { self . key : self . _arg - self . stepsize } ]
3234	def list_targets_by_rule ( client = None , ** kwargs ) : result = client . list_targets_by_rule ( ** kwargs ) if not result . get ( "Targets" ) : result . update ( { "Targets" : [ ] } ) return result
4725	def get_chunk_meta ( self , meta_file ) : chunks = self . envs [ "CHUNKS" ] if cij . nvme . get_meta ( 0 , chunks * self . envs [ "CHUNK_META_SIZEOF" ] , meta_file ) : raise RuntimeError ( "cij.liblight.get_chunk_meta: fail" ) chunk_meta = cij . bin . Buffer ( types = self . envs [ "CHUNK_META_STRUCT" ] , length = chunks ) chunk_meta . read ( meta_file ) return chunk_meta
8102	def open_socket ( self ) : self . socket = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) self . socket . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) self . socket . setblocking ( 0 ) self . socket . bind ( ( self . host , self . port ) )
11878	def getProcessOwner ( pid ) : try : ownerUid = os . stat ( '/proc/' + str ( pid ) ) . st_uid except : return None try : ownerName = pwd . getpwuid ( ownerUid ) . pw_name except : ownerName = None return { 'uid' : ownerUid , 'name' : ownerName }
640	def set ( cls , prop , value ) : if cls . _properties is None : cls . _readStdConfigFiles ( ) cls . _properties [ prop ] = str ( value )
13296	def decode_jsonld ( jsonld_text ) : decoder = json . JSONDecoder ( object_pairs_hook = _decode_object_pairs ) return decoder . decode ( jsonld_text )
12182	async def execute_method ( self , method , ** params ) : url = self . url_builder ( method , url_params = params ) logger . info ( 'Executing method %r' , method ) response = await aiohttp . get ( url ) logger . info ( 'Status: %r' , response . status ) if response . status == 200 : json = await response . json ( ) logger . debug ( '...with JSON %r' , json ) if json . get ( 'ok' ) : return json raise SlackApiError ( json [ 'error' ] ) else : raise_for_status ( response )
1167	def _dump_registry ( cls , file = None ) : print >> file , "Class: %s.%s" % ( cls . __module__ , cls . __name__ ) print >> file , "Inv.counter: %s" % ABCMeta . _abc_invalidation_counter for name in sorted ( cls . __dict__ . keys ( ) ) : if name . startswith ( "_abc_" ) : value = getattr ( cls , name ) print >> file , "%s: %r" % ( name , value )
7950	def send_element ( self , element ) : with self . lock : if self . _eof or self . _socket is None or not self . _serializer : logger . debug ( "Dropping element: {0}" . format ( element_to_unicode ( element ) ) ) return data = self . _serializer . emit_stanza ( element ) self . _write ( data . encode ( "utf-8" ) )
10742	def print_profile ( function ) : import memory_profiler def wrapper ( * args , ** kwargs ) : m = StringIO ( ) pr = cProfile . Profile ( ) pr . enable ( ) temp_func = memory_profiler . profile ( func = function , stream = m , precision = 4 ) output = temp_func ( * args , ** kwargs ) print ( m . getvalue ( ) ) pr . disable ( ) ps = pstats . Stats ( pr ) ps . sort_stats ( 'cumulative' ) . print_stats ( '(?!.*memory_profiler.*)(^.*$)' , 20 ) m . close ( ) return output return wrapper
8591	def create_snapshot ( self , datacenter_id , volume_id , name = None , description = None ) : data = { 'name' : name , 'description' : description } response = self . _perform_request ( '/datacenters/%s/volumes/%s/create-snapshot' % ( datacenter_id , volume_id ) , method = 'POST-ACTION-JSON' , data = urlencode ( data ) ) return response
10164	def get_personalities ( self , line ) : return [ split ( '\W+' , i ) [ 1 ] for i in line . split ( ':' ) [ 1 ] . split ( ' ' ) if i . startswith ( '[' ) ]
3484	def _create_bound ( model , reaction , bound_type , f_replace , units = None , flux_udef = None ) : value = getattr ( reaction , bound_type ) if value == config . lower_bound : return LOWER_BOUND_ID elif value == 0 : return ZERO_BOUND_ID elif value == config . upper_bound : return UPPER_BOUND_ID elif value == - float ( "Inf" ) : return BOUND_MINUS_INF elif value == float ( "Inf" ) : return BOUND_PLUS_INF else : rid = reaction . id if f_replace and F_REACTION_REV in f_replace : rid = f_replace [ F_REACTION_REV ] ( rid ) pid = rid + "_" + bound_type _create_parameter ( model , pid = pid , value = value , sbo = SBO_FLUX_BOUND , units = units , flux_udef = flux_udef ) return pid
4537	def wheel_helper ( pos , length , cycle_step ) : return wheel_color ( ( pos * len ( _WHEEL ) / length ) + cycle_step )
3992	def get_nginx_configuration_spec ( port_spec_dict , docker_bridge_ip ) : nginx_http_config , nginx_stream_config = "" , "" for port_spec in port_spec_dict [ 'nginx' ] : if port_spec [ 'type' ] == 'http' : nginx_http_config += _nginx_http_spec ( port_spec , docker_bridge_ip ) elif port_spec [ 'type' ] == 'stream' : nginx_stream_config += _nginx_stream_spec ( port_spec , docker_bridge_ip ) return { 'http' : nginx_http_config , 'stream' : nginx_stream_config }
5065	def filter_audit_course_modes ( enterprise_customer , course_modes ) : audit_modes = getattr ( settings , 'ENTERPRISE_COURSE_ENROLLMENT_AUDIT_MODES' , [ 'audit' ] ) if not enterprise_customer . enable_audit_enrollment : return [ course_mode for course_mode in course_modes if course_mode [ 'mode' ] not in audit_modes ] return course_modes
13874	def _CopyFileLocal ( source_filename , target_filename , copy_symlink = True ) : import shutil try : dir_name = os . path . dirname ( target_filename ) if dir_name and not os . path . isdir ( dir_name ) : os . makedirs ( dir_name ) if copy_symlink and IsLink ( source_filename ) : if os . path . isfile ( target_filename ) or IsLink ( target_filename ) : DeleteFile ( target_filename ) source_filename = ReadLink ( source_filename ) CreateLink ( source_filename , target_filename ) else : if sys . platform == 'win32' : while IsLink ( source_filename ) : link = ReadLink ( source_filename ) if os . path . isabs ( link ) : source_filename = link else : source_filename = os . path . join ( os . path . dirname ( source_filename ) , link ) shutil . copyfile ( source_filename , target_filename ) shutil . copymode ( source_filename , target_filename ) except Exception as e : reraise ( e , 'While executiong _filesystem._CopyFileLocal(%s, %s)' % ( source_filename , target_filename ) )
4250	def country_name_by_addr ( self , addr ) : VALID_EDITIONS = ( const . COUNTRY_EDITION , const . COUNTRY_EDITION_V6 ) if self . _databaseType in VALID_EDITIONS : country_id = self . id_by_addr ( addr ) return const . COUNTRY_NAMES [ country_id ] elif self . _databaseType in const . CITY_EDITIONS : return self . record_by_addr ( addr ) . get ( 'country_name' ) else : message = 'Invalid database type, expected Country or City' raise GeoIPError ( message )
8184	def update ( self , iterations = 10 ) : self . alpha += 0.05 self . alpha = min ( self . alpha , 1.0 ) if self . layout . i == 0 : self . layout . prepare ( ) self . layout . i += 1 elif self . layout . i == 1 : self . layout . iterate ( ) elif self . layout . i < self . layout . n : n = min ( iterations , self . layout . i / 10 + 1 ) for i in range ( n ) : self . layout . iterate ( ) min_ , max = self . layout . bounds self . x = _ctx . WIDTH - max . x * self . d - min_ . x * self . d self . y = _ctx . HEIGHT - max . y * self . d - min_ . y * self . d self . x /= 2 self . y /= 2 return not self . layout . done
5050	def from_children ( cls , program_uuid , * children ) : if not children or any ( child is None for child in children ) : return None granted = all ( ( child . granted for child in children ) ) exists = any ( ( child . exists for child in children ) ) usernames = set ( [ child . username for child in children ] ) enterprises = set ( [ child . enterprise_customer for child in children ] ) if not len ( usernames ) == len ( enterprises ) == 1 : raise InvalidProxyConsent ( 'Children used to create a bulk proxy consent object must ' 'share a single common username and EnterpriseCustomer.' ) username = children [ 0 ] . username enterprise_customer = children [ 0 ] . enterprise_customer return cls ( enterprise_customer = enterprise_customer , username = username , program_uuid = program_uuid , exists = exists , granted = granted , child_consents = children )
10760	def from_rectilinear ( cls , x , y , z , formatter = numpy_formatter ) : x = np . asarray ( x , dtype = np . float64 ) y = np . asarray ( y , dtype = np . float64 ) z = np . ma . asarray ( z , dtype = np . float64 ) if x . ndim != 1 : raise TypeError ( "'x' must be a 1D array but is a {:d}D array" . format ( x . ndim ) ) if y . ndim != 1 : raise TypeError ( "'y' must be a 1D array but is a {:d}D array" . format ( y . ndim ) ) if z . ndim != 2 : raise TypeError ( "'z' must be a 2D array but it a {:d}D array" . format ( z . ndim ) ) if x . size != z . shape [ 1 ] : raise TypeError ( ( "the length of 'x' must be equal to the number of columns in " "'z' but the length of 'x' is {:d} and 'z' has {:d} " "columns" ) . format ( x . size , z . shape [ 1 ] ) ) if y . size != z . shape [ 0 ] : raise TypeError ( ( "the length of 'y' must be equal to the number of rows in " "'z' but the length of 'y' is {:d} and 'z' has {:d} " "rows" ) . format ( y . size , z . shape [ 0 ] ) ) y , x = np . meshgrid ( y , x , indexing = 'ij' ) return cls ( x , y , z , formatter )
1846	def JS ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . SF , target . read ( ) , cpu . PC )
10804	def resolve_admin_type ( admin ) : if admin is current_user or isinstance ( admin , UserMixin ) : return 'User' else : return admin . __class__ . __name__
11286	def flush ( self , line ) : sys . stdout . write ( line ) sys . stdout . flush ( )
9439	def heartbeat ( ) : print "We got a call heartbeat notification\n" if request . method == 'POST' : print request . form else : print request . args return "OK"
6455	def dist_mlipns ( src , tar , threshold = 0.25 , max_mismatches = 2 ) : return MLIPNS ( ) . dist ( src , tar , threshold , max_mismatches )
8834	def less ( a , b , * args ) : types = set ( [ type ( a ) , type ( b ) ] ) if float in types or int in types : try : a , b = float ( a ) , float ( b ) except TypeError : return False return a < b and ( not args or less ( b , * args ) )
4828	def is_enrolled ( self , username , course_run_id ) : enrollment = self . get_course_enrollment ( username , course_run_id ) return enrollment is not None and enrollment . get ( 'is_active' , False )
1536	def get_heron_options_from_env ( ) : heron_options_raw = os . environ . get ( "HERON_OPTIONS" ) if heron_options_raw is None : raise RuntimeError ( "HERON_OPTIONS environment variable not found" ) options = { } for option_line in heron_options_raw . replace ( "%%%%" , " " ) . split ( ',' ) : key , sep , value = option_line . partition ( "=" ) if sep : options [ key ] = value else : raise ValueError ( "Invalid HERON_OPTIONS part %r" % option_line ) return options
8405	def _censor_with ( x , range , value = None ) : return [ val if range [ 0 ] <= val <= range [ 1 ] else value for val in x ]
13541	def create ( self , server ) : if len ( self . geometries ) == 0 : raise Exception ( 'no geometries' ) return server . post ( 'task_admin' , self . as_payload ( ) , replacements = { 'slug' : self . __challenge__ . slug , 'identifier' : self . identifier } )
10784	def should_particle_exist ( absent_err , present_err , absent_d , present_d , im_change_frac = 0.2 , min_derr = 0.1 ) : delta_im = np . ravel ( present_d - absent_d ) im_change = np . dot ( delta_im , delta_im ) err_cutoff = max ( [ im_change_frac * im_change , min_derr ] ) return ( absent_err - present_err ) >= err_cutoff
242	def create_perf_attrib_tear_sheet ( returns , positions , factor_returns , factor_loadings , transactions = None , pos_in_dollars = True , return_fig = False , factor_partitions = FACTOR_PARTITIONS ) : portfolio_exposures , perf_attrib_data = perf_attrib . perf_attrib ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars = pos_in_dollars ) display ( Markdown ( "## Performance Relative to Common Risk Factors" ) ) perf_attrib . show_perf_attrib_stats ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars ) vertical_sections = 1 + 2 * max ( len ( factor_partitions ) , 1 ) current_section = 0 fig = plt . figure ( figsize = [ 14 , vertical_sections * 6 ] ) gs = gridspec . GridSpec ( vertical_sections , 1 , wspace = 0.5 , hspace = 0.5 ) perf_attrib . plot_returns ( perf_attrib_data , ax = plt . subplot ( gs [ current_section ] ) ) current_section += 1 if factor_partitions is not None : for factor_type , partitions in factor_partitions . iteritems ( ) : columns_to_select = perf_attrib_data . columns . intersection ( partitions ) perf_attrib . plot_factor_contribution_to_perf ( perf_attrib_data [ columns_to_select ] , ax = plt . subplot ( gs [ current_section ] ) , title = ( 'Cumulative common {} returns attribution' ) . format ( factor_type ) ) current_section += 1 for factor_type , partitions in factor_partitions . iteritems ( ) : perf_attrib . plot_risk_exposures ( portfolio_exposures [ portfolio_exposures . columns . intersection ( partitions ) ] , ax = plt . subplot ( gs [ current_section ] ) , title = 'Daily {} factor exposures' . format ( factor_type ) ) current_section += 1 else : perf_attrib . plot_factor_contribution_to_perf ( perf_attrib_data , ax = plt . subplot ( gs [ current_section ] ) ) current_section += 1 perf_attrib . plot_risk_exposures ( portfolio_exposures , ax = plt . subplot ( gs [ current_section ] ) ) gs . tight_layout ( fig ) if return_fig : return fig
1478	def _get_heron_support_processes ( self ) : retval = { } retval [ self . heron_shell_ids [ self . shard ] ] = Command ( [ '%s' % self . heron_shell_binary , '--port=%s' % self . shell_port , '--log_file_prefix=%s/heron-shell-%s.log' % ( self . log_dir , self . shard ) , '--secret=%s' % self . topology_id ] , self . shell_env ) return retval
12297	def plugins_show ( what = None , name = None , version = None , details = False ) : global pluginmgr return pluginmgr . show ( what , name , version , details )
6588	def get_integer ( prompt ) : while True : try : return int ( input ( prompt ) . strip ( ) ) except ValueError : print ( Colors . red ( "Invalid Input!" ) )
11606	def condense_ranges ( cls , ranges ) : result = [ ] if ranges : ranges . sort ( key = lambda tup : tup [ 0 ] ) result . append ( ranges [ 0 ] ) for i in range ( 1 , len ( ranges ) ) : if result [ - 1 ] [ 1 ] + 1 >= ranges [ i ] [ 0 ] : result [ - 1 ] = ( result [ - 1 ] [ 0 ] , max ( result [ - 1 ] [ 1 ] , ranges [ i ] [ 1 ] ) ) else : result . append ( ranges [ i ] ) return result
5707	def get_lockdown_form ( form_path ) : if not form_path : raise ImproperlyConfigured ( 'No LOCKDOWN_FORM specified.' ) form_path_list = form_path . split ( "." ) new_module = "." . join ( form_path_list [ : - 1 ] ) attr = form_path_list [ - 1 ] try : mod = import_module ( new_module ) except ( ImportError , ValueError ) : raise ImproperlyConfigured ( 'Module configured in LOCKDOWN_FORM (%s) to' ' contain the form class couldn\'t be ' 'found.' % new_module ) try : form = getattr ( mod , attr ) except AttributeError : raise ImproperlyConfigured ( 'The module configured in LOCKDOWN_FORM ' ' (%s) doesn\'t define a "%s" form.' % ( new_module , attr ) ) return form
2273	def _win32_read_junction ( path ) : if not jwfs . is_reparse_point ( path ) : raise ValueError ( 'not a junction' ) handle = jwfs . api . CreateFile ( path , 0 , 0 , None , jwfs . api . OPEN_EXISTING , jwfs . api . FILE_FLAG_OPEN_REPARSE_POINT | jwfs . api . FILE_FLAG_BACKUP_SEMANTICS , None ) if handle == jwfs . api . INVALID_HANDLE_VALUE : raise WindowsError ( ) res = jwfs . reparse . DeviceIoControl ( handle , jwfs . api . FSCTL_GET_REPARSE_POINT , None , 10240 ) bytes = jwfs . create_string_buffer ( res ) p_rdb = jwfs . cast ( bytes , jwfs . POINTER ( jwfs . api . REPARSE_DATA_BUFFER ) ) rdb = p_rdb . contents if rdb . tag not in [ 2684354563 , jwfs . api . IO_REPARSE_TAG_SYMLINK ] : raise RuntimeError ( "Expected <2684354563 or 2684354572>, but got %d" % rdb . tag ) jwfs . handle_nonzero_success ( jwfs . api . CloseHandle ( handle ) ) subname = rdb . get_substitute_name ( ) if subname . startswith ( '?\\' ) : subname = subname [ 2 : ] return subname
2578	def submit ( self , func , * args , executors = 'all' , fn_hash = None , cache = False , ** kwargs ) : if self . cleanup_called : raise ValueError ( "Cannot submit to a DFK that has been cleaned up" ) task_id = self . task_count self . task_count += 1 if isinstance ( executors , str ) and executors . lower ( ) == 'all' : choices = list ( e for e in self . executors if e != 'data_manager' ) elif isinstance ( executors , list ) : choices = executors executor = random . choice ( choices ) args , kwargs = self . _add_input_deps ( executor , args , kwargs ) task_def = { 'depends' : None , 'executor' : executor , 'func' : func , 'func_name' : func . __name__ , 'args' : args , 'kwargs' : kwargs , 'fn_hash' : fn_hash , 'memoize' : cache , 'callback' : None , 'exec_fu' : None , 'checkpoint' : None , 'fail_count' : 0 , 'fail_history' : [ ] , 'env' : None , 'status' : States . unsched , 'id' : task_id , 'time_submitted' : None , 'time_returned' : None , 'app_fu' : None } if task_id in self . tasks : raise DuplicateTaskError ( "internal consistency error: Task {0} already exists in task list" . format ( task_id ) ) else : self . tasks [ task_id ] = task_def dep_cnt , depends = self . _gather_all_deps ( args , kwargs ) self . tasks [ task_id ] [ 'depends' ] = depends task_stdout = kwargs . get ( 'stdout' ) task_stderr = kwargs . get ( 'stderr' ) logger . info ( "Task {} submitted for App {}, waiting on tasks {}" . format ( task_id , task_def [ 'func_name' ] , [ fu . tid for fu in depends ] ) ) self . tasks [ task_id ] [ 'task_launch_lock' ] = threading . Lock ( ) app_fu = AppFuture ( tid = task_id , stdout = task_stdout , stderr = task_stderr ) self . tasks [ task_id ] [ 'app_fu' ] = app_fu app_fu . add_done_callback ( partial ( self . handle_app_update , task_id ) ) self . tasks [ task_id ] [ 'status' ] = States . pending logger . debug ( "Task {} set to pending state with AppFuture: {}" . format ( task_id , task_def [ 'app_fu' ] ) ) for d in depends : def callback_adapter ( dep_fut ) : self . launch_if_ready ( task_id ) try : d . add_done_callback ( callback_adapter ) except Exception as e : logger . error ( "add_done_callback got an exception {} which will be ignored" . format ( e ) ) self . launch_if_ready ( task_id ) return task_def [ 'app_fu' ]
13757	def get_path_extension ( path ) : file_path , file_ext = os . path . splitext ( path ) return file_ext . lstrip ( '.' )
11496	def get_user_by_email ( self , email ) : parameters = dict ( ) parameters [ 'email' ] = email response = self . request ( 'midas.user.get' , parameters ) return response
8533	def is_isomorphic_to ( self , other ) : return ( isinstance ( other , self . __class__ ) and len ( self . fields ) == len ( other . fields ) and all ( a . is_isomorphic_to ( b ) for a , b in zip ( self . fields , other . fields ) ) )
3760	def mass_fractions ( self ) : r things = dict ( ) for zi , atoms in zip ( self . zs , self . atomss ) : for atom , count in atoms . iteritems ( ) : if atom in things : things [ atom ] += zi * count else : things [ atom ] = zi * count return mass_fractions ( things )
2264	def dict_union ( * args ) : if not args : return { } else : dictclass = OrderedDict if isinstance ( args [ 0 ] , OrderedDict ) else dict return dictclass ( it . chain . from_iterable ( d . items ( ) for d in args ) )
9914	def create ( self , validated_data ) : email_query = models . EmailAddress . objects . filter ( email = self . validated_data [ "email" ] ) if email_query . exists ( ) : email = email_query . get ( ) email . send_duplicate_notification ( ) else : email = super ( EmailSerializer , self ) . create ( validated_data ) email . send_confirmation ( ) user = validated_data . get ( "user" ) query = models . EmailAddress . objects . filter ( is_primary = True , user = user ) if not query . exists ( ) : email . set_primary ( ) return email
5088	def ecommerce_coupon_url ( self , instance ) : if not instance . entitlement_id : return "N/A" return format_html ( '<a href="{base_url}/coupons/{id}" target="_blank">View coupon "{id}" details</a>' , base_url = settings . ECOMMERCE_PUBLIC_URL_ROOT , id = instance . entitlement_id )
9580	def read_struct_array ( fd , endian , header ) : field_name_length = read_elements ( fd , endian , [ 'miINT32' ] ) if field_name_length > 32 : raise ParseError ( 'Unexpected field name length: {}' . format ( field_name_length ) ) fields = read_elements ( fd , endian , [ 'miINT8' ] , is_name = True ) if isinstance ( fields , basestring ) : fields = [ fields ] empty = lambda : [ list ( ) for i in range ( header [ 'dims' ] [ 0 ] ) ] array = { } for row in range ( header [ 'dims' ] [ 0 ] ) : for col in range ( header [ 'dims' ] [ 1 ] ) : for field in fields : vheader , next_pos , fd_var = read_var_header ( fd , endian ) data = read_var_array ( fd_var , endian , vheader ) if field not in array : array [ field ] = empty ( ) array [ field ] [ row ] . append ( data ) fd . seek ( next_pos ) for field in fields : rows = array [ field ] for i in range ( header [ 'dims' ] [ 0 ] ) : rows [ i ] = squeeze ( rows [ i ] ) array [ field ] = squeeze ( array [ field ] ) return array
4567	def _write ( self , filename , frames , fps , loop = 0 , palette = 256 ) : from PIL import Image images = [ ] for f in frames : data = open ( f , 'rb' ) . read ( ) images . append ( Image . open ( io . BytesIO ( data ) ) ) duration = round ( 1 / fps , 2 ) im = images . pop ( 0 ) im . save ( filename , save_all = True , append_images = images , duration = duration , loop = loop , palette = palette )
6586	def retries ( max_tries , exceptions = ( Exception , ) ) : def decorator ( func ) : def function ( * args , ** kwargs ) : retries_left = max_tries while retries_left > 0 : try : retries_left -= 1 return func ( * args , ** kwargs ) except exceptions as exc : if isinstance ( exc , PandoraException ) : raise if retries_left > 0 : time . sleep ( delay_exponential ( 0.5 , 2 , max_tries - retries_left ) ) else : raise return function return decorator
12275	def iso_reference_isvalid ( ref ) : ref = str ( ref ) cs_source = ref [ 4 : ] + ref [ : 4 ] return ( iso_reference_str2int ( cs_source ) % 97 ) == 1
2630	def scale_in ( self , blocks = 0 , machines = 0 , strategy = None ) : count = 0 instances = self . client . servers . list ( ) for instance in instances [ 0 : machines ] : print ( "Deleting : " , instance ) instance . delete ( ) count += 1 return count
726	def addNoise ( self , bits , amount ) : newBits = set ( ) for bit in bits : if self . _random . getReal64 ( ) < amount : newBits . add ( self . _random . getUInt32 ( self . _n ) ) else : newBits . add ( bit ) return newBits
13711	def invalidate_ip ( self , ip ) : if self . _use_cache : key = self . _make_cache_key ( ip ) self . _cache . delete ( key , version = self . _cache_version )
1347	def clone ( git_uri ) : hash_digest = sha256_hash ( git_uri ) local_path = home_directory_path ( FOLDER , hash_digest ) exists_locally = path_exists ( local_path ) if not exists_locally : _clone_repo ( git_uri , local_path ) else : logging . info ( "Git repository already exists locally." ) return local_path
8627	def get_users ( session , query ) : response = make_get_request ( session , 'users' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise UsersNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
3476	def _associate_gene ( self , cobra_gene ) : self . _genes . add ( cobra_gene ) cobra_gene . _reaction . add ( self ) cobra_gene . _model = self . _model
8486	def get ( self , name , default , allow_default = True ) : if not self . settings . get ( 'pyconfig.case_sensitive' , False ) : name = name . lower ( ) if name not in self . settings : if not allow_default : raise LookupError ( 'No setting "{name}"' . format ( name = name ) ) self . settings [ name ] = default return self . settings [ name ]
3304	def _run_paste ( app , config , mode ) : from paste import httpserver version = "WsgiDAV/{} {} Python {}" . format ( __version__ , httpserver . WSGIHandler . server_version , util . PYTHON_VERSION ) _logger . info ( "Running {}..." . format ( version ) ) server = httpserver . serve ( app , host = config [ "host" ] , port = config [ "port" ] , server_version = version , protocol_version = "HTTP/1.1" , start_loop = False , ) if config [ "verbose" ] >= 5 : __handle_one_request = server . RequestHandlerClass . handle_one_request def handle_one_request ( self ) : __handle_one_request ( self ) if self . close_connection == 1 : _logger . debug ( "HTTP Connection : close" ) else : _logger . debug ( "HTTP Connection : continue" ) server . RequestHandlerClass . handle_one_request = handle_one_request server . RequestHandlerClass . handle_one_request = handle_one_request host , port = server . server_address if host == "0.0.0.0" : _logger . info ( "Serving on 0.0.0.0:{} view at {}://127.0.0.1:{}" . format ( port , "http" , port ) ) else : _logger . info ( "Serving on {}://{}:{}" . format ( "http" , host , port ) ) try : server . serve_forever ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
1879	def VEXTRACTF128 ( cpu , dest , src , offset ) : offset = offset . read ( ) dest . write ( Operators . EXTRACT ( src . read ( ) , offset * 128 , ( offset + 1 ) * 128 ) )
11548	def guess_array_memory_usage ( bam_readers , dtype , use_strand = False ) : ARRAY_COUNT = 5 if not isinstance ( bam_readers , list ) : bam_readers = [ bam_readers ] if isinstance ( dtype , basestring ) : dtype = NUMPY_DTYPES . get ( dtype , None ) use_strand = use_strand + 1 dtypes = guess_numpy_dtypes_from_idxstats ( bam_readers , default = None , force_dtype = False ) if not [ dt for dt in dtypes if dt is not None ] : dtypes = guess_numpy_dtypes_from_idxstats ( bam_readers , default = dtype or numpy . uint64 , force_dtype = True ) elif dtype : dtypes = [ dtype if dt else None for dt in dtypes ] read_groups = [ ] no_read_group = False for bam in bam_readers : rgs = bam . get_read_groups ( ) if rgs : for rg in rgs : if rg not in read_groups : read_groups . append ( rg ) else : no_read_group = True read_groups = len ( read_groups ) + no_read_group max_ref_size = 0 array_byte_overhead = sys . getsizeof ( numpy . zeros ( ( 0 ) , dtype = numpy . uint64 ) ) array_count = ARRAY_COUNT * use_strand * read_groups for bam in bam_readers : for i , ( name , length ) in enumerate ( bam . get_references ( ) ) : if dtypes [ i ] is not None : max_ref_size = max ( max_ref_size , ( length + length * dtypes [ i ] ( ) . nbytes * array_count + ( array_byte_overhead * ( array_count + 1 ) ) ) ) return max_ref_size
3770	def mixing_simple ( fracs , props ) : r if not none_and_length_check ( [ fracs , props ] ) : return None result = sum ( frac * prop for frac , prop in zip ( fracs , props ) ) return result
7176	def retype_path ( src , pyi_dir , targets , * , src_explicitly_given = False , quiet = False , hg = False ) : if src . is_dir ( ) : for child in src . iterdir ( ) : if child == pyi_dir or child == targets : continue yield from retype_path ( child , pyi_dir / src . name , targets / src . name , quiet = quiet , hg = hg , ) elif src . suffix == '.py' or src_explicitly_given : try : retype_file ( src , pyi_dir , targets , quiet = quiet , hg = hg ) except Exception as e : yield ( src , str ( e ) , type ( e ) , traceback . format_tb ( e . __traceback__ ) , )
5453	def _remove_empty_items ( d , required ) : new_dict = { } for k , v in d . items ( ) : if k in required : new_dict [ k ] = v elif isinstance ( v , int ) or v : new_dict [ k ] = v return new_dict
6178	def map_chunk ( func , array , out_array ) : for slice in iter_chunk_slice ( array . shape [ - 1 ] , array . chunkshape [ - 1 ] ) : out_array . append ( func ( array [ ... , slice ] ) ) return out_array
3655	def stop_image_acquisition ( self ) : if self . is_acquiring_images : self . _is_acquiring_images = False if self . thread_image_acquisition . is_running : self . thread_image_acquisition . stop ( ) with MutexLocker ( self . thread_image_acquisition ) : self . device . node_map . AcquisitionStop . execute ( ) try : self . device . node_map . TLParamsLocked . value = 0 except LogicalErrorException : pass for data_stream in self . _data_streams : try : data_stream . stop_acquisition ( ACQ_STOP_FLAGS_LIST . ACQ_STOP_FLAGS_KILL ) except ( ResourceInUseException , TimeoutException ) as e : self . _logger . error ( e , exc_info = True ) data_stream . flush_buffer_queue ( ACQ_QUEUE_TYPE_LIST . ACQ_QUEUE_ALL_DISCARD ) for event_manager in self . _event_new_buffer_managers : event_manager . flush_event_queue ( ) if self . _create_ds_at_connection : self . _release_buffers ( ) else : self . _release_data_streams ( ) self . _has_acquired_1st_image = False self . _chunk_adapter . detach_buffer ( ) self . _logger . info ( '{0} stopped image acquisition.' . format ( self . _device . id_ ) ) if self . _profiler : self . _profiler . print_diff ( )
12324	def save ( self ) : if self . code : raise HolviError ( "Orders cannot be updated" ) send_json = self . to_holvi_dict ( ) send_json . update ( { 'pool' : self . api . connection . pool } ) url = six . u ( self . api . base_url + "order/" ) stat = self . api . connection . make_post ( url , send_json ) code = stat [ "details_uri" ] . split ( "/" ) [ - 2 ] return ( stat [ "checkout_uri" ] , self . api . get_order ( code ) )
7149	def encode ( cls , hex ) : out = [ ] for i in range ( len ( hex ) // 8 ) : word = endian_swap ( hex [ 8 * i : 8 * i + 8 ] ) x = int ( word , 16 ) w1 = x % cls . n w2 = ( x // cls . n + w1 ) % cls . n w3 = ( x // cls . n // cls . n + w2 ) % cls . n out += [ cls . word_list [ w1 ] , cls . word_list [ w2 ] , cls . word_list [ w3 ] ] checksum = cls . get_checksum ( " " . join ( out ) ) out . append ( checksum ) return " " . join ( out )
9425	def open ( self , member , pwd = None ) : if isinstance ( member , RarInfo ) : member = member . filename archive = unrarlib . RAROpenArchiveDataEx ( self . filename , mode = constants . RAR_OM_EXTRACT ) handle = self . _open ( archive ) password = pwd or self . pwd if password is not None : unrarlib . RARSetPassword ( handle , b ( password ) ) data = _ReadIntoMemory ( ) c_callback = unrarlib . UNRARCALLBACK ( data . _callback ) unrarlib . RARSetCallback ( handle , c_callback , 0 ) try : rarinfo = self . _read_header ( handle ) while rarinfo is not None : if rarinfo . filename == member : self . _process_current ( handle , constants . RAR_TEST ) break else : self . _process_current ( handle , constants . RAR_SKIP ) rarinfo = self . _read_header ( handle ) if rarinfo is None : data = None except unrarlib . MissingPassword : raise RuntimeError ( "File is encrypted, password required" ) except unrarlib . BadPassword : raise RuntimeError ( "Bad password for File" ) except unrarlib . BadDataError : if password is not None : raise RuntimeError ( "File CRC error or incorrect password" ) else : raise RuntimeError ( "File CRC error" ) except unrarlib . UnrarException as e : raise BadRarFile ( "Bad RAR archive data: %s" % str ( e ) ) finally : self . _close ( handle ) if data is None : raise KeyError ( 'There is no item named %r in the archive' % member ) return data . get_bytes ( )
9124	def belanno ( keyword : str , file : TextIO ) : directory = get_data_dir ( keyword ) obo_url = f'http://purl.obolibrary.org/obo/{keyword}.obo' obo_path = os . path . join ( directory , f'{keyword}.obo' ) obo_cache_path = os . path . join ( directory , f'{keyword}.obo.pickle' ) obo_getter = make_obo_getter ( obo_url , obo_path , preparsed_path = obo_cache_path ) graph = obo_getter ( ) convert_obo_graph_to_belanno ( graph , file = file , )
6840	def distrib_family ( ) : distrib = ( distrib_id ( ) or '' ) . lower ( ) if distrib in [ 'debian' , 'ubuntu' , 'linuxmint' , 'elementary os' ] : return DEBIAN elif distrib in [ 'redhat' , 'rhel' , 'centos' , 'sles' , 'fedora' ] : return REDHAT elif distrib in [ 'sunos' ] : return SUN elif distrib in [ 'gentoo' ] : return GENTOO elif distrib in [ 'arch' , 'manjarolinux' ] : return ARCH return 'other'
8004	def submit_form ( self , form ) : result = Register ( ) if self . form : result . form = form . make_submit ( ) return result if "FORM_TYPE" not in form or "jabber:iq:register" not in form [ "FORM_TYPE" ] . values : raise ValueError ( "FORM_TYPE is not jabber:iq:register" ) for field in legacy_fields : self . __logger . debug ( u"submitted field %r" % ( field , ) ) value = getattr ( self , field ) try : form_value = form [ field ] . value except KeyError : if value : raise ValueError ( "Required field with no value!" ) continue setattr ( result , field , form_value ) return result
1477	def _get_instance_plans ( self , packing_plan , container_id ) : this_container_plan = None for container_plan in packing_plan . container_plans : if container_plan . id == container_id : this_container_plan = container_plan if this_container_plan is None : return None return this_container_plan . instance_plans
1398	def extract_scheduler_location ( self , topology ) : schedulerLocation = { "name" : None , "http_endpoint" : None , "job_page_link" : None , } if topology . scheduler_location : schedulerLocation [ "name" ] = topology . scheduler_location . topology_name schedulerLocation [ "http_endpoint" ] = topology . scheduler_location . http_endpoint schedulerLocation [ "job_page_link" ] = topology . scheduler_location . job_page_link [ 0 ] if len ( topology . scheduler_location . job_page_link ) > 0 else "" return schedulerLocation
6282	def cursor_event ( self , x , y , dx , dy ) : self . sys_camera . rot_state ( x , y )
5810	def _parse_hello_extensions ( data ) : if data == b'' : return extentions_length = int_from_bytes ( data [ 0 : 2 ] ) extensions_start = 2 extensions_end = 2 + extentions_length pointer = extensions_start while pointer < extensions_end : extension_type = int_from_bytes ( data [ pointer : pointer + 2 ] ) extension_length = int_from_bytes ( data [ pointer + 2 : pointer + 4 ] ) yield ( extension_type , data [ pointer + 4 : pointer + 4 + extension_length ] ) pointer += 4 + extension_length
13735	def get_api_error ( response ) : error_class = _status_code_to_class . get ( response . status_code , APIError ) return error_class ( response )
9034	def instruction_in_grid ( self , instruction ) : row_position = self . _rows_in_grid [ instruction . row ] . xy x = instruction . index_of_first_consumed_mesh_in_row position = Point ( row_position . x + x , row_position . y ) return InstructionInGrid ( instruction , position )
5336	def __kibiter_version ( self ) : version = None es_url = self . conf [ 'es_enrichment' ] [ 'url' ] config_url = '.kibana/config/_search' url = urijoin ( es_url , config_url ) version = None try : res = self . grimoire_con . get ( url ) res . raise_for_status ( ) version = res . json ( ) [ 'hits' ] [ 'hits' ] [ 0 ] [ '_id' ] logger . debug ( "Kibiter version: %s" , version ) except requests . exceptions . HTTPError : logger . warning ( "Can not find Kibiter version" ) return version
2180	def fetch_access_token ( self , url , verifier = None , ** request_kwargs ) : if verifier : self . _client . client . verifier = verifier if not getattr ( self . _client . client , "verifier" , None ) : raise VerifierMissing ( "No client verifier has been set." ) token = self . _fetch_token ( url , ** request_kwargs ) log . debug ( "Resetting verifier attribute, should not be used anymore." ) self . _client . client . verifier = None return token
12393	def use ( ** kwargs ) : config = dict ( use . config ) use . config . update ( kwargs ) return config
2763	def get_snapshot ( self , snapshot_id ) : return Snapshot . get_object ( api_token = self . token , snapshot_id = snapshot_id )
429	def read_image ( image , path = '' ) : return imageio . imread ( os . path . join ( path , image ) )
12812	def rawDataReceived ( self , data ) : if self . _len_expected is not None : data , extra = data [ : self . _len_expected ] , data [ self . _len_expected : ] self . _len_expected -= len ( data ) else : extra = "" self . _buffer += data if self . _len_expected == 0 : data = self . _buffer . strip ( ) if data : lines = data . split ( "\r" ) for line in lines : try : message = self . factory . get_stream ( ) . get_connection ( ) . parse ( line ) if message : self . factory . get_stream ( ) . received ( [ message ] ) except ValueError : pass self . _buffer = "" self . _len_expected = None self . setLineMode ( extra )
9743	async def reboot ( ip_address ) : _ , protocol = await asyncio . get_event_loop ( ) . create_datagram_endpoint ( QRebootProtocol , local_addr = ( ip_address , 0 ) , allow_broadcast = True , reuse_address = True , ) LOG . info ( "Sending reboot on %s" , ip_address ) protocol . send_reboot ( )
9808	def create_tarfile ( files , project_name ) : fd , filename = tempfile . mkstemp ( prefix = "polyaxon_{}" . format ( project_name ) , suffix = '.tar.gz' ) with tarfile . open ( filename , "w:gz" ) as tar : for f in files : tar . add ( f ) yield filename os . close ( fd ) os . remove ( filename )
11568	def open ( self , verbose ) : if verbose : print ( '\nOpening Arduino Serial port %s ' % self . port_id ) try : self . arduino . close ( ) time . sleep ( 1 ) self . arduino . open ( ) time . sleep ( 1 ) return self . arduino except Exception : raise
5604	def _get_warped_array ( input_file = None , indexes = None , dst_bounds = None , dst_shape = None , dst_crs = None , resampling = None , src_nodata = None , dst_nodata = None ) : try : return _rasterio_read ( input_file = input_file , indexes = indexes , dst_bounds = dst_bounds , dst_shape = dst_shape , dst_crs = dst_crs , resampling = resampling , src_nodata = src_nodata , dst_nodata = dst_nodata ) except Exception as e : logger . exception ( "error while reading file %s: %s" , input_file , e ) raise
11026	def sort_pem_objects ( pem_objects ) : keys , certs , ca_certs = [ ] , [ ] , [ ] for pem_object in pem_objects : if isinstance ( pem_object , pem . Key ) : keys . append ( pem_object ) else : if _is_ca ( pem_object ) : ca_certs . append ( pem_object ) else : certs . append ( pem_object ) [ key ] , [ cert ] = keys , certs return key , cert , ca_certs
13891	def ReadLink ( path ) : _AssertIsLocal ( path ) if sys . platform != 'win32' : return os . readlink ( path ) if not IsLink ( path ) : from . _exceptions import FileNotFoundError raise FileNotFoundError ( path ) import jaraco . windows . filesystem result = jaraco . windows . filesystem . readlink ( path ) if '\\??\\' in result : result = result . split ( '\\??\\' ) [ 1 ] return result
13666	def command_handle ( self ) : self . __results = self . execute ( self . args . command ) self . close ( ) self . logger . debug ( "results: {}" . format ( self . __results ) ) if not self . __results : self . unknown ( "{} return nothing." . format ( self . args . command ) ) if len ( self . __results ) != 1 : self . unknown ( "{} return more than one number." . format ( self . args . command ) ) self . __result = int ( self . __results [ 0 ] ) self . logger . debug ( "result: {}" . format ( self . __result ) ) if not isinstance ( self . __result , ( int , long ) ) : self . unknown ( "{} didn't return single number." . format ( self . args . command ) ) status = self . ok if self . __result > self . args . warning : status = self . warning if self . __result > self . args . critical : status = self . critical self . shortoutput = "{0} return {1}." . format ( self . args . command , self . __result ) [ self . longoutput . append ( line ) for line in self . __results if self . __results ] self . perfdata . append ( "{command}={result};{warn};{crit};0;" . format ( crit = self . args . critical , warn = self . args . warning , result = self . __result , command = self . args . command ) ) status ( self . output ( long_output_limit = None ) ) self . logger . debug ( "Return status and exit to Nagios." )
1097	def decode ( in_file , out_file = None , mode = None , quiet = 0 ) : opened_files = [ ] if in_file == '-' : in_file = sys . stdin elif isinstance ( in_file , basestring ) : in_file = open ( in_file ) opened_files . append ( in_file ) try : while True : hdr = in_file . readline ( ) if not hdr : raise Error ( 'No valid begin line found in input file' ) if not hdr . startswith ( 'begin' ) : continue hdrfields = hdr . split ( ' ' , 2 ) if len ( hdrfields ) == 3 and hdrfields [ 0 ] == 'begin' : try : int ( hdrfields [ 1 ] , 8 ) break except ValueError : pass if out_file is None : out_file = hdrfields [ 2 ] . rstrip ( ) if os . path . exists ( out_file ) : raise Error ( 'Cannot overwrite existing file: %s' % out_file ) if mode is None : mode = int ( hdrfields [ 1 ] , 8 ) if out_file == '-' : out_file = sys . stdout elif isinstance ( out_file , basestring ) : fp = open ( out_file , 'wb' ) try : os . path . chmod ( out_file , mode ) except AttributeError : pass out_file = fp opened_files . append ( out_file ) s = in_file . readline ( ) while s and s . strip ( ) != 'end' : try : data = binascii . a2b_uu ( s ) except binascii . Error , v : nbytes = ( ( ( ord ( s [ 0 ] ) - 32 ) & 63 ) * 4 + 5 ) // 3 data = binascii . a2b_uu ( s [ : nbytes ] ) if not quiet : sys . stderr . write ( "Warning: %s\n" % v ) out_file . write ( data ) s = in_file . readline ( ) if not s : raise Error ( 'Truncated input file' ) finally : for f in opened_files : f . close ( )
3881	async def _sync ( self ) : logger . info ( 'Syncing events since {}' . format ( self . _sync_timestamp ) ) try : res = await self . _client . sync_all_new_events ( hangouts_pb2 . SyncAllNewEventsRequest ( request_header = self . _client . get_request_header ( ) , last_sync_timestamp = parsers . to_timestamp ( self . _sync_timestamp ) , max_response_size_bytes = 1048576 , ) ) except exceptions . NetworkError as e : logger . warning ( 'Failed to sync events, some events may be lost: {}' . format ( e ) ) else : for conv_state in res . conversation_state : conv_id = conv_state . conversation_id . id conv = self . _conv_dict . get ( conv_id , None ) if conv is not None : conv . update_conversation ( conv_state . conversation ) for event_ in conv_state . event : timestamp = parsers . from_timestamp ( event_ . timestamp ) if timestamp > self . _sync_timestamp : await self . _on_event ( event_ ) else : self . _add_conversation ( conv_state . conversation , conv_state . event , conv_state . event_continuation_token )
5841	def submit_design_run ( self , data_view_id , num_candidates , effort , target = None , constraints = [ ] , sampler = "Default" ) : if effort > 30 : raise CitrinationClientError ( "Parameter effort must be less than 30 to trigger a design run" ) if target is not None : target = target . to_dict ( ) constraint_dicts = [ c . to_dict ( ) for c in constraints ] body = { "num_candidates" : num_candidates , "target" : target , "effort" : effort , "constraints" : constraint_dicts , "sampler" : sampler } url = routes . submit_data_view_design ( data_view_id ) response = self . _post_json ( url , body ) . json ( ) return DesignRun ( response [ "data" ] [ "design_run" ] [ "uid" ] )
6482	def _load_class ( class_path , default ) : if class_path is None : return default component = class_path . rsplit ( '.' , 1 ) result_processor = getattr ( importlib . import_module ( component [ 0 ] ) , component [ 1 ] , default ) if len ( component ) > 1 else default return result_processor
1989	def save_stream ( self , key , binary = False ) : mode = 'wb' if binary else 'w' with open ( os . path . join ( self . uri , key ) , mode ) as f : yield f
6169	def filter ( self , x ) : y = signal . lfilter ( self . b , [ 1 ] , x ) return y
5072	def get_configuration_value ( val_name , default = None , ** kwargs ) : if kwargs . get ( 'type' ) == 'url' : return get_url ( val_name ) or default if callable ( get_url ) else default return configuration_helpers . get_value ( val_name , default , ** kwargs ) if configuration_helpers else default
13085	def set ( self , section , key , value ) : if not section in self . config : self . config . add_section ( section ) self . config . set ( section , key , value )
4850	def _transmit_create ( self , channel_metadata_item_map ) : for chunk in chunks ( channel_metadata_item_map , self . enterprise_configuration . transmission_chunk_size ) : serialized_chunk = self . _serialize_items ( list ( chunk . values ( ) ) ) try : self . client . create_content_metadata ( serialized_chunk ) except ClientError as exc : LOGGER . error ( 'Failed to update [%s] content metadata items for integrated channel [%s] [%s]' , len ( chunk ) , self . enterprise_configuration . enterprise_customer . name , self . enterprise_configuration . channel_code , ) LOGGER . error ( exc ) else : self . _create_transmissions ( chunk )
12097	def delete ( self , force = False , ** kwargs ) : if force : return super ( BaseActivatableModel , self ) . delete ( ** kwargs ) else : setattr ( self , self . ACTIVATABLE_FIELD_NAME , False ) return self . save ( update_fields = [ self . ACTIVATABLE_FIELD_NAME ] )
6044	def padded_grid_from_shape_psf_shape_and_pixel_scale ( cls , shape , psf_shape , pixel_scale ) : padded_shape = ( shape [ 0 ] + psf_shape [ 0 ] - 1 , shape [ 1 ] + psf_shape [ 1 ] - 1 ) padded_regular_grid = grid_util . regular_grid_1d_masked_from_mask_pixel_scales_and_origin ( mask = np . full ( padded_shape , False ) , pixel_scales = ( pixel_scale , pixel_scale ) ) padded_mask = msk . Mask . unmasked_for_shape_and_pixel_scale ( shape = padded_shape , pixel_scale = pixel_scale ) return PaddedRegularGrid ( arr = padded_regular_grid , mask = padded_mask , image_shape = shape )
666	def sample ( self , rgen ) : rf = rgen . uniform ( 0 , self . sum ) index = bisect . bisect ( self . cdf , rf ) return self . keys [ index ] , numpy . log ( self . pmf [ index ] )
10652	def prepare_to_run ( self , clock , period_count ) : for c in self . components : c . prepare_to_run ( clock , period_count ) for a in self . activities : a . prepare_to_run ( clock , period_count )
3762	def Tt ( CASRN , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in Staveley_data . index : methods . append ( STAVELEY ) if Tm ( CASRN ) : methods . append ( MELTING ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == STAVELEY : Tt = Staveley_data . at [ CASRN , "Tt68" ] elif Method == MELTING : Tt = Tm ( CASRN ) elif Method == NONE : Tt = None else : raise Exception ( 'Failure in in function' ) return Tt
13428	def get_site ( self , site_id ) : url = "/2/sites/%s" % site_id return self . site_from_json ( self . _get_resource ( url ) [ "site" ] )
2631	def _status ( self ) : job_id_list = ' ' . join ( self . resources . keys ( ) ) cmd = "condor_q {0} -af:jr JobStatus" . format ( job_id_list ) retcode , stdout , stderr = super ( ) . execute_wait ( cmd ) for line in stdout . strip ( ) . split ( '\n' ) : parts = line . split ( ) job_id = parts [ 0 ] status = translate_table . get ( parts [ 1 ] , 'UNKNOWN' ) self . resources [ job_id ] [ 'status' ] = status
305	def plot_monthly_returns_timeseries ( returns , ax = None , ** kwargs ) : def cumulate_returns ( x ) : return ep . cum_returns ( x ) [ - 1 ] if ax is None : ax = plt . gca ( ) monthly_rets = returns . resample ( 'M' ) . apply ( lambda x : cumulate_returns ( x ) ) monthly_rets = monthly_rets . to_period ( ) sns . barplot ( x = monthly_rets . index , y = monthly_rets . values , color = 'steelblue' ) locs , labels = plt . xticks ( ) plt . setp ( labels , rotation = 90 ) xticks_coord = [ ] xticks_label = [ ] count = 0 for i in monthly_rets . index : if i . month == 1 : xticks_label . append ( i ) xticks_coord . append ( count ) ax . axvline ( count , color = 'gray' , ls = '--' , alpha = 0.3 ) count += 1 ax . axhline ( 0.0 , color = 'darkgray' , ls = '-' ) ax . set_xticks ( xticks_coord ) ax . set_xticklabels ( xticks_label ) return ax
2959	def _state_delete ( self ) : try : os . remove ( self . _state_file ) except OSError as err : if err . errno not in ( errno . EPERM , errno . ENOENT ) : raise try : os . rmdir ( self . _state_dir ) except OSError as err : if err . errno not in ( errno . ENOTEMPTY , errno . ENOENT ) : raise
3300	def make_sub_element ( parent , tag , nsmap = None ) : if use_lxml : return etree . SubElement ( parent , tag , nsmap = nsmap ) return etree . SubElement ( parent , tag )
3434	def _populate_solver ( self , reaction_list , metabolite_list = None ) : constraint_terms = AutoVivification ( ) to_add = [ ] if metabolite_list is not None : for met in metabolite_list : to_add += [ self . problem . Constraint ( Zero , name = met . id , lb = 0 , ub = 0 ) ] self . add_cons_vars ( to_add ) for reaction in reaction_list : if reaction . id not in self . variables : forward_variable = self . problem . Variable ( reaction . id ) reverse_variable = self . problem . Variable ( reaction . reverse_id ) self . add_cons_vars ( [ forward_variable , reverse_variable ] ) else : reaction = self . reactions . get_by_id ( reaction . id ) forward_variable = reaction . forward_variable reverse_variable = reaction . reverse_variable for metabolite , coeff in six . iteritems ( reaction . metabolites ) : if metabolite . id in self . constraints : constraint = self . constraints [ metabolite . id ] else : constraint = self . problem . Constraint ( Zero , name = metabolite . id , lb = 0 , ub = 0 ) self . add_cons_vars ( constraint , sloppy = True ) constraint_terms [ constraint ] [ forward_variable ] = coeff constraint_terms [ constraint ] [ reverse_variable ] = - coeff self . solver . update ( ) for reaction in reaction_list : reaction = self . reactions . get_by_id ( reaction . id ) reaction . update_variable_bounds ( ) for constraint , terms in six . iteritems ( constraint_terms ) : constraint . set_linear_coefficients ( terms )
11676	def fit ( self , X , y = None ) : self . features_ = as_features ( X , stack = True , bare = True ) return self
4287	def get_thumb ( settings , filename ) : path , filen = os . path . split ( filename ) name , ext = os . path . splitext ( filen ) if ext . lower ( ) in settings [ 'video_extensions' ] : ext = '.jpg' return join ( path , settings [ 'thumb_dir' ] , settings [ 'thumb_prefix' ] + name + settings [ 'thumb_suffix' ] + ext )
9642	def set_trace ( context ) : try : import ipdb as pdb except ImportError : import pdb print ( "For best results, pip install ipdb." ) print ( "Variables that are available in the current context:" ) render = lambda s : template . Template ( s ) . render ( context ) availables = get_variables ( context ) pprint ( availables ) print ( 'Type `availables` to show this list.' ) print ( 'Type <variable_name> to access one.' ) print ( 'Use render("template string") to test template rendering' ) for var in availables : locals ( ) [ var ] = context [ var ] pdb . set_trace ( ) return ''
11758	def dpll ( clauses , symbols , model ) : "See if the clauses are true in a partial model." unknown_clauses = [ ] for c in clauses : val = pl_true ( c , model ) if val == False : return False if val != True : unknown_clauses . append ( c ) if not unknown_clauses : return model P , value = find_pure_symbol ( symbols , unknown_clauses ) if P : return dpll ( clauses , removeall ( P , symbols ) , extend ( model , P , value ) ) P , value = find_unit_clause ( clauses , model ) if P : return dpll ( clauses , removeall ( P , symbols ) , extend ( model , P , value ) ) P , symbols = symbols [ 0 ] , symbols [ 1 : ] return ( dpll ( clauses , symbols , extend ( model , P , True ) ) or dpll ( clauses , symbols , extend ( model , P , False ) ) )
3500	def assess_component ( model , reaction , side , flux_coefficient_cutoff = 0.001 , solver = None ) : reaction = model . reactions . get_by_any ( reaction ) [ 0 ] result_key = dict ( reactants = 'produced' , products = 'capacity' ) [ side ] get_components = attrgetter ( side ) with model as m : m . objective = reaction if _optimize_or_value ( m , solver = solver ) >= flux_coefficient_cutoff : return True simulation_results = { } demand_reactions = { } for component in get_components ( reaction ) : coeff = reaction . metabolites [ component ] demand = m . add_boundary ( component , type = 'demand' ) demand . metabolites [ component ] = coeff demand_reactions [ demand ] = ( component , coeff ) joint_demand = Reaction ( "joint_demand" ) for demand_reaction in demand_reactions : joint_demand += demand_reaction m . add_reactions ( [ joint_demand ] ) m . objective = joint_demand if _optimize_or_value ( m , solver = solver ) >= flux_coefficient_cutoff : return True for demand_reaction , ( component , coeff ) in iteritems ( demand_reactions ) : with m : m . objective = demand_reaction flux = _optimize_or_value ( m , solver = solver ) if flux_coefficient_cutoff > flux : simulation_results . update ( { component : { 'required' : flux_coefficient_cutoff / abs ( coeff ) , result_key : flux / abs ( coeff ) } } ) if len ( simulation_results ) == 0 : simulation_results = False return simulation_results
132	def is_out_of_image ( self , image , fully = True , partly = False ) : if len ( self . exterior ) == 0 : raise Exception ( "Cannot determine whether the polygon is inside the image, because it contains no points." ) ls = self . to_line_string ( ) return ls . is_out_of_image ( image , fully = fully , partly = partly )
9	def encode_observation ( ob_space , placeholder ) : if isinstance ( ob_space , Discrete ) : return tf . to_float ( tf . one_hot ( placeholder , ob_space . n ) ) elif isinstance ( ob_space , Box ) : return tf . to_float ( placeholder ) elif isinstance ( ob_space , MultiDiscrete ) : placeholder = tf . cast ( placeholder , tf . int32 ) one_hots = [ tf . to_float ( tf . one_hot ( placeholder [ ... , i ] , ob_space . nvec [ i ] ) ) for i in range ( placeholder . shape [ - 1 ] ) ] return tf . concat ( one_hots , axis = - 1 ) else : raise NotImplementedError
8265	def _interpolate ( self , colors , n = 100 ) : gradient = [ ] for i in _range ( n ) : l = len ( colors ) - 1 x = int ( 1.0 * i / n * l ) x = min ( x + 0 , l ) y = min ( x + 1 , l ) base = 1.0 * n / l * x d = ( i - base ) / ( 1.0 * n / l ) r = colors [ x ] . r * ( 1 - d ) + colors [ y ] . r * d g = colors [ x ] . g * ( 1 - d ) + colors [ y ] . g * d b = colors [ x ] . b * ( 1 - d ) + colors [ y ] . b * d a = colors [ x ] . a * ( 1 - d ) + colors [ y ] . a * d gradient . append ( color ( r , g , b , a , mode = "rgb" ) ) gradient . append ( colors [ - 1 ] ) return gradient
5506	def screenshot ( url , * args , ** kwargs ) : phantomscript = os . path . join ( os . path . dirname ( __file__ ) , 'take_screenshot.js' ) directory = kwargs . get ( 'save_dir' , '/tmp' ) image_name = kwargs . get ( 'image_name' , None ) or _image_name_from_url ( url ) ext = kwargs . get ( 'format' , 'png' ) . lower ( ) save_path = os . path . join ( directory , image_name ) + '.' + ext crop_to_visible = kwargs . get ( 'crop_to_visible' , False ) cmd_args = [ 'phantomjs' , '--ssl-protocol=any' , phantomscript , url , '--width' , str ( kwargs [ 'width' ] ) , '--height' , str ( kwargs [ 'height' ] ) , '--useragent' , str ( kwargs [ 'user_agent' ] ) , '--dir' , directory , '--ext' , ext , '--name' , str ( image_name ) , ] if crop_to_visible : cmd_args . append ( '--croptovisible' ) output = subprocess . Popen ( cmd_args , stdout = subprocess . PIPE ) . communicate ( ) [ 0 ] return Screenshot ( save_path , directory , image_name + '.' + ext , ext )
13102	def get_template_uuid ( self ) : response = requests . get ( self . url + 'editor/scan/templates' , headers = self . headers , verify = False ) templates = json . loads ( response . text ) for template in templates [ 'templates' ] : if template [ 'name' ] == self . template_name : return template [ 'uuid' ]
4742	def env_export ( prefix , exported , env ) : for exp in exported : ENV [ "_" . join ( [ prefix , exp ] ) ] = env [ exp ]
7510	def _insert_to_array ( self , start , results ) : qrts , wgts , qsts = results with h5py . File ( self . database . output , 'r+' ) as out : chunk = self . _chunksize out [ 'quartets' ] [ start : start + chunk ] = qrts if self . checkpoint . boots : key = "qboots/b{}" . format ( self . checkpoint . boots - 1 ) out [ key ] [ start : start + chunk ] = qsts else : out [ "qstats" ] [ start : start + chunk ] = qsts
10510	def imagecapture ( self , window_name = None , out_file = None , x = 0 , y = 0 , width = None , height = None ) : if not out_file : out_file = tempfile . mktemp ( '.png' , 'ldtp_' ) else : out_file = os . path . expanduser ( out_file ) if _ldtp_windows_env : if width == None : width = - 1 if height == None : height = - 1 if window_name == None : window_name = '' data = self . _remote_imagecapture ( window_name , x , y , width , height ) f = open ( out_file , 'wb' ) f . write ( b64decode ( data ) ) f . close ( ) return out_file
9593	def switch_to_frame ( self , frame_reference = None ) : if frame_reference is not None and type ( frame_reference ) not in [ int , WebElement ] : raise TypeError ( 'Type of frame_reference must be None or int or WebElement' ) self . _execute ( Command . SWITCH_TO_FRAME , { 'id' : frame_reference } )
11869	def color_from_hls ( hue , light , sat ) : if light > 0.95 : return 256 elif light < 0.05 : return - 1 else : hue = ( - hue + 1 + 2.0 / 3.0 ) % 1 return int ( floor ( hue * 256 ) )
5182	def nodes ( self , unreported = 2 , with_status = False , ** kwargs ) : nodes = self . _query ( 'nodes' , ** kwargs ) now = datetime . datetime . utcnow ( ) if type ( nodes ) == dict : nodes = [ nodes , ] if with_status : latest_events = self . event_counts ( query = EqualsOperator ( "latest_report?" , True ) , summarize_by = 'certname' ) for node in nodes : node [ 'status_report' ] = None node [ 'events' ] = None if with_status : status = [ s for s in latest_events if s [ 'subject' ] [ 'title' ] == node [ 'certname' ] ] try : node [ 'status_report' ] = node [ 'latest_report_status' ] if status : node [ 'events' ] = status [ 0 ] except KeyError : if status : node [ 'events' ] = status = status [ 0 ] if status [ 'successes' ] > 0 : node [ 'status_report' ] = 'changed' if status [ 'noops' ] > 0 : node [ 'status_report' ] = 'noop' if status [ 'failures' ] > 0 : node [ 'status_report' ] = 'failed' else : node [ 'status_report' ] = 'unchanged' if node [ 'report_timestamp' ] is not None : try : last_report = json_to_datetime ( node [ 'report_timestamp' ] ) last_report = last_report . replace ( tzinfo = None ) unreported_border = now - timedelta ( hours = unreported ) if last_report < unreported_border : delta = ( now - last_report ) node [ 'unreported' ] = True node [ 'unreported_time' ] = '{0}d {1}h {2}m' . format ( delta . days , int ( delta . seconds / 3600 ) , int ( ( delta . seconds % 3600 ) / 60 ) ) except AttributeError : node [ 'unreported' ] = True if not node [ 'report_timestamp' ] : node [ 'unreported' ] = True yield Node ( self , name = node [ 'certname' ] , deactivated = node [ 'deactivated' ] , expired = node [ 'expired' ] , report_timestamp = node [ 'report_timestamp' ] , catalog_timestamp = node [ 'catalog_timestamp' ] , facts_timestamp = node [ 'facts_timestamp' ] , status_report = node [ 'status_report' ] , noop = node . get ( 'latest_report_noop' ) , noop_pending = node . get ( 'latest_report_noop_pending' ) , events = node [ 'events' ] , unreported = node . get ( 'unreported' ) , unreported_time = node . get ( 'unreported_time' ) , report_environment = node [ 'report_environment' ] , catalog_environment = node [ 'catalog_environment' ] , facts_environment = node [ 'facts_environment' ] , latest_report_hash = node . get ( 'latest_report_hash' ) , cached_catalog_status = node . get ( 'cached_catalog_status' ) )
10529	def get_projects ( limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : print ( OFFSET_WARNING ) params = dict ( limit = limit , offset = offset ) try : res = _pybossa_req ( 'get' , 'project' , params = params ) if type ( res ) . __name__ == 'list' : return [ Project ( project ) for project in res ] else : raise TypeError except : raise
8325	def buildTagMap ( default , * args ) : built = { } for portion in args : if hasattr ( portion , 'items' ) : for k , v in portion . items ( ) : built [ k ] = v elif isList ( portion ) : for k in portion : built [ k ] = default else : built [ portion ] = default return built
10153	def _extract_operation_from_view ( self , view , args ) : op = { 'responses' : { 'default' : { 'description' : 'UNDOCUMENTED RESPONSE' } } , } renderer = args . get ( 'renderer' , '' ) if "json" in renderer : produces = [ 'application/json' ] elif renderer == 'xml' : produces = [ 'text/xml' ] else : produces = None if produces : op . setdefault ( 'produces' , produces ) consumes = args . get ( 'content_type' ) if consumes is not None : consumes = to_list ( consumes ) consumes = [ x for x in consumes if not callable ( x ) ] op [ 'consumes' ] = consumes is_colander = self . _is_colander_schema ( args ) if is_colander : schema = self . _extract_transform_colander_schema ( args ) parameters = self . parameters . from_schema ( schema ) else : parameters = None if parameters : op [ 'parameters' ] = parameters if isinstance ( view , six . string_types ) : if 'klass' in args : ob = args [ 'klass' ] view_ = getattr ( ob , view . lower ( ) ) docstring = trim ( view_ . __doc__ ) else : docstring = str ( trim ( view . __doc__ ) ) if docstring and self . summary_docstrings : op [ 'summary' ] = docstring if 'response_schemas' in args : op [ 'responses' ] = self . responses . from_schema_mapping ( args [ 'response_schemas' ] ) if 'tags' in args : op [ 'tags' ] = args [ 'tags' ] if 'operation_id' in args : op [ 'operationId' ] = args [ 'operation_id' ] if 'api_security' in args : op [ 'security' ] = args [ 'api_security' ] return op
590	def _allocateSpatialFDR ( self , rfInput ) : if self . _sfdr : return autoArgs = dict ( ( name , getattr ( self , name ) ) for name in self . _spatialArgNames ) if ( ( self . SpatialClass == CPPSpatialPooler ) or ( self . SpatialClass == PYSpatialPooler ) ) : autoArgs [ 'columnDimensions' ] = [ self . columnCount ] autoArgs [ 'inputDimensions' ] = [ self . inputWidth ] autoArgs [ 'potentialRadius' ] = self . inputWidth self . _sfdr = self . SpatialClass ( ** autoArgs )
13367	def register_proxy_type ( cls , real_type , proxy_type ) : if distob . engine is None : cls . _initial_proxy_types [ real_type ] = proxy_type elif isinstance ( distob . engine , ObjectHub ) : distob . engine . _runtime_reg_proxy_type ( real_type , proxy_type ) else : distob . engine . _singleeng_reg_proxy_type ( real_type , proxy_type ) pass
1021	def buildSequencePool ( numSequences = 10 , seqLen = [ 2 , 3 , 4 ] , numPatterns = 5 , numOnBitsPerPattern = 3 , patternOverlap = 0 , ** kwargs ) : patterns = getSimplePatterns ( numOnBitsPerPattern , numPatterns , patternOverlap ) numCols = len ( patterns [ 0 ] ) trainingSequences = [ ] for _ in xrange ( numSequences ) : sequence = [ ] length = random . choice ( seqLen ) for _ in xrange ( length ) : patIdx = random . choice ( xrange ( numPatterns ) ) sequence . append ( patterns [ patIdx ] ) trainingSequences . append ( sequence ) if VERBOSITY >= 3 : print "\nTraining sequences" printAllTrainingSequences ( trainingSequences ) return ( numCols , trainingSequences )
3499	def assess ( model , reaction , flux_coefficient_cutoff = 0.001 , solver = None ) : reaction = model . reactions . get_by_any ( reaction ) [ 0 ] with model as m : m . objective = reaction if _optimize_or_value ( m , solver = solver ) >= flux_coefficient_cutoff : return True else : results = dict ( ) results [ 'precursors' ] = assess_component ( model , reaction , 'reactants' , flux_coefficient_cutoff ) results [ 'products' ] = assess_component ( model , reaction , 'products' , flux_coefficient_cutoff ) return results
6138	def set_default_sim_param ( self , * args , ** kwargs ) : if len ( args ) is 1 and isinstance ( args [ 0 ] , SimulationParameter ) : self . __default_param = args [ 0 ] else : self . __default_param = SimulationParameter ( * args , ** kwargs ) return
12158	def abfSort ( IDs ) : IDs = list ( IDs ) monO = [ ] monN = [ ] monD = [ ] good = [ ] for ID in IDs : if ID is None : continue if 'o' in ID : monO . append ( ID ) elif 'n' in ID : monN . append ( ID ) elif 'd' in ID : monD . append ( ID ) else : good . append ( ID ) return sorted ( good ) + sorted ( monO ) + sorted ( monN ) + sorted ( monD )
7012	def lcdict_to_pickle ( lcdict , outfile = None ) : if not outfile and lcdict [ 'objectid' ] : outfile = '%s-hplc.pkl' % lcdict [ 'objectid' ] elif not outfile and not lcdict [ 'objectid' ] : outfile = 'hplc.pkl' with open ( outfile , 'wb' ) as outfd : pickle . dump ( lcdict , outfd , protocol = pickle . HIGHEST_PROTOCOL ) if os . path . exists ( outfile ) : LOGINFO ( 'lcdict for object: %s -> %s OK' % ( lcdict [ 'objectid' ] , outfile ) ) return outfile else : LOGERROR ( 'could not make a pickle for this lcdict!' ) return None
13703	def expand_words ( self , line , width = 60 ) : if not line . strip ( ) : return line wordi = 1 while len ( strip_codes ( line ) ) < width : wordendi = self . find_word_end ( line , wordi ) if wordendi < 0 : wordi = 1 wordendi = self . find_word_end ( line , wordi ) if wordendi < 0 : line = '' . join ( ( ' ' , line ) ) else : line = ' ' . join ( ( line [ : wordendi ] , line [ wordendi : ] ) ) wordi += 1 if ' ' not in strip_codes ( line ) . strip ( ) : return line . replace ( ' ' , '' ) return line
1533	def get_scheduler_location ( self , topologyName , callback = None ) : if callback : self . scheduler_location_watchers [ topologyName ] . append ( callback ) else : scheduler_location_path = self . get_scheduler_location_path ( topologyName ) with open ( scheduler_location_path ) as f : data = f . read ( ) scheduler_location = SchedulerLocation ( ) scheduler_location . ParseFromString ( data ) return scheduler_location
5109	def set_num_servers ( self , n ) : if not isinstance ( n , numbers . Integral ) and n is not infty : the_str = "n must be an integer or infinity.\n{0}" raise TypeError ( the_str . format ( str ( self ) ) ) elif n <= 0 : the_str = "n must be a positive integer or infinity.\n{0}" raise ValueError ( the_str . format ( str ( self ) ) ) else : self . num_servers = n
1312	def HardwareInput ( uMsg : int , param : int = 0 ) -> INPUT : return _CreateInput ( HARDWAREINPUT ( uMsg , param & 0xFFFF , param >> 16 & 0xFFFF ) )
9229	def fetch_closed_pull_requests ( self ) : pull_requests = [ ] verbose = self . options . verbose gh = self . github user = self . options . user repo = self . options . project if verbose : print ( "Fetching closed pull requests..." ) page = 1 while page > 0 : if verbose > 2 : print ( "." , end = "" ) if self . options . release_branch : rc , data = gh . repos [ user ] [ repo ] . pulls . get ( page = page , per_page = PER_PAGE_NUMBER , state = 'closed' , base = self . options . release_branch ) else : rc , data = gh . repos [ user ] [ repo ] . pulls . get ( page = page , per_page = PER_PAGE_NUMBER , state = 'closed' , ) if rc == 200 : pull_requests . extend ( data ) else : self . raise_GitHubError ( rc , data , gh . getheaders ( ) ) page = NextPage ( gh ) if verbose > 2 : print ( "." ) if verbose > 1 : print ( "\tfetched {} closed pull requests." . format ( len ( pull_requests ) ) ) return pull_requests
3595	def details ( self , packageName ) : path = DETAILS_URL + "?doc={}" . format ( requests . utils . quote ( packageName ) ) data = self . executeRequestApi2 ( path ) return utils . parseProtobufObj ( data . payload . detailsResponse . docV2 )
8502	def as_live ( self ) : key = self . get_key ( ) default = pyconfig . get ( key ) if default : default = repr ( default ) else : default = self . _default ( ) or NotSet ( ) return "%s = %s" % ( key , default )
5832	def get ( self , data_view_id ) : failure_message = "Dataview get failed" return self . _get_success_json ( self . _get ( 'v1/data_views/' + data_view_id , None , failure_message = failure_message ) ) [ 'data' ] [ 'data_view' ]
12029	def setsweeps ( self ) : for sweep in range ( self . sweeps ) : self . setsweep ( sweep ) yield self . sweep
4362	def _heartbeat ( self ) : interval = self . config [ 'heartbeat_interval' ] while self . connected : gevent . sleep ( interval ) self . put_client_msg ( "2::" )
12887	def create_session ( self ) : req_url = '%s/%s' % ( self . __webfsapi , 'CREATE_SESSION' ) sid = yield from self . __session . get ( req_url , params = dict ( pin = self . pin ) , timeout = self . timeout ) text = yield from sid . text ( encoding = 'utf-8' ) doc = objectify . fromstring ( text ) return doc . sessionId . text
4291	def cleanup_directory ( config_data ) : if os . path . exists ( config_data . project_directory ) : choice = False if config_data . noinput is False and not config_data . verbose : choice = query_yes_no ( 'The installation failed.\n' 'Do you want to clean up by removing {0}?\n' '\tWarning: this will delete all files in:\n' '\t\t{0}\n' 'Do you want to cleanup?' . format ( os . path . abspath ( config_data . project_directory ) ) , 'no' ) else : sys . stdout . write ( 'The installation has failed.\n' ) if config_data . skip_project_dir_check is False and ( choice or ( config_data . noinput and config_data . delete_project_dir ) ) : sys . stdout . write ( 'Removing everything under {0}\n' . format ( os . path . abspath ( config_data . project_directory ) ) ) shutil . rmtree ( config_data . project_directory , True )
12935	def _parse_allele_data ( self ) : pref_freq , frequencies = self . _parse_frequencies ( ) info_clnvar_single_tags = [ 'ALLELEID' , 'CLNSIG' , 'CLNHGVS' ] cln_data = { x . lower ( ) : self . info [ x ] if x in self . info else None for x in info_clnvar_single_tags } cln_data . update ( { 'clndisdb' : [ x . split ( ',' ) for x in self . info [ 'CLNDISDB' ] . split ( '|' ) ] if 'CLNDISDB' in self . info else [ ] } ) cln_data . update ( { 'clndn' : self . info [ 'CLNDN' ] . split ( '|' ) if 'CLNDN' in self . info else [ ] } ) cln_data . update ( { 'clnvi' : self . info [ 'CLNVI' ] . split ( ',' ) if 'CLNVI' in self . info else [ ] } ) try : sequence = self . alt_alleles [ 0 ] except IndexError : sequence = self . ref_allele allele = ClinVarAllele ( frequency = pref_freq , sequence = sequence , ** cln_data ) if not cln_data [ 'clnsig' ] : return [ ] return [ allele ]
521	def _initPermanence ( self , potential , connectedPct ) : perm = numpy . zeros ( self . _numInputs , dtype = realDType ) for i in xrange ( self . _numInputs ) : if ( potential [ i ] < 1 ) : continue if ( self . _random . getReal64 ( ) <= connectedPct ) : perm [ i ] = self . _initPermConnected ( ) else : perm [ i ] = self . _initPermNonConnected ( ) perm [ perm < self . _synPermTrimThreshold ] = 0 return perm
5773	def dsa_sign ( private_key , data , hash_algorithm ) : if private_key . algorithm != 'dsa' : raise ValueError ( 'The key specified is not a DSA private key' ) return _sign ( private_key , data , hash_algorithm )
11659	def inverse_transform ( self , X ) : X = check_array ( X , copy = self . copy ) X -= self . min_ X /= self . scale_ return X
11252	def get_datetime_string ( datetime_obj ) : if isinstance ( datetime_obj , datetime ) : dft = DTFormat ( ) return datetime_obj . strftime ( dft . datetime_format ) return None
12900	def get_equalisers ( self ) : if not self . __equalisers : self . __equalisers = yield from self . handle_list ( self . API . get ( 'equalisers' ) ) return self . __equalisers
10533	def update_project ( project ) : try : project_id = project . id project = _forbidden_attributes ( project ) res = _pybossa_req ( 'put' , 'project' , project_id , payload = project . data ) if res . get ( 'id' ) : return Project ( res ) else : return res except : raise
5356	def set_param ( self , section , param , value ) : if section not in self . conf or param not in self . conf [ section ] : logger . error ( 'Config section %s and param %s not exists' , section , param ) else : self . conf [ section ] [ param ] = value
7056	def ec2_ssh ( ip_address , keypem_file , username = 'ec2-user' , raiseonfail = False ) : c = paramiko . client . SSHClient ( ) c . load_system_host_keys ( ) c . set_missing_host_key_policy ( paramiko . client . AutoAddPolicy ) privatekey = paramiko . RSAKey . from_private_key_file ( keypem_file ) try : c . connect ( ip_address , pkey = privatekey , username = 'ec2-user' ) return c except Exception as e : LOGEXCEPTION ( 'could not connect to EC2 instance at %s ' 'using keyfile: %s and user: %s' % ( ip_address , keypem_file , username ) ) if raiseonfail : raise return None
2744	def load_by_pub_key ( self , public_key ) : data = self . get_data ( "account/keys/" ) for jsoned in data [ 'ssh_keys' ] : if jsoned . get ( 'public_key' , "" ) == public_key : self . id = jsoned [ 'id' ] self . load ( ) return self return None
7038	def get_dataset ( lcc_server , dataset_id , strformat = False , page = 1 ) : urlparams = { 'strformat' : 1 if strformat else 0 , 'page' : page , 'json' : 1 } urlqs = urlencode ( urlparams ) dataset_url = '%s/set/%s?%s' % ( lcc_server , dataset_id , urlqs ) LOGINFO ( 'retrieving dataset %s from %s, using URL: %s ...' % ( lcc_server , dataset_id , dataset_url ) ) try : have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } req = Request ( dataset_url , data = None , headers = headers ) resp = urlopen ( req ) dataset = json . loads ( resp . read ( ) ) return dataset except Exception as e : LOGEXCEPTION ( 'could not retrieve the dataset JSON!' ) return None
7500	def get_spans ( maparr , spans ) : bidx = 1 spans = np . zeros ( ( maparr [ - 1 , 0 ] , 2 ) , np . uint64 ) for idx in xrange ( 1 , maparr . shape [ 0 ] ) : cur = maparr [ idx , 0 ] if cur != bidx : idy = idx + 1 spans [ cur - 2 , 1 ] = idx spans [ cur - 1 , 0 ] = idx bidx = cur spans [ - 1 , 1 ] = maparr [ - 1 , - 1 ] return spans
9282	def set_filter ( self , filter_text ) : self . filter = filter_text self . logger . info ( "Setting filter to: %s" , self . filter ) if self . _connected : self . _sendall ( "#filter %s\r\n" % self . filter )
10199	def hash_id ( iso_timestamp , msg ) : return '{0}-{1}' . format ( iso_timestamp , hashlib . sha1 ( msg . get ( 'unique_id' ) . encode ( 'utf-8' ) + str ( msg . get ( 'visitor_id' ) ) . encode ( 'utf-8' ) ) . hexdigest ( ) )
1326	def from_keras ( cls , model , bounds , input_shape = None , channel_axis = 3 , preprocessing = ( 0 , 1 ) ) : import tensorflow as tf if input_shape is None : try : input_shape = model . input_shape [ 1 : ] except AttributeError : raise ValueError ( 'Please specify input_shape manually or ' 'provide a model with an input_shape attribute' ) with tf . keras . backend . get_session ( ) . as_default ( ) : inputs = tf . placeholder ( tf . float32 , ( None , ) + input_shape ) logits = model ( inputs ) return cls ( inputs , logits , bounds = bounds , channel_axis = channel_axis , preprocessing = preprocessing )
13560	def decorate ( msg = "" , waitmsg = "Please wait" ) : def decorator ( func ) : @ functools . wraps ( func ) def wrapper ( * args , ** kwargs ) : spin = Spinner ( msg = msg , waitmsg = waitmsg ) spin . start ( ) a = None try : a = func ( * args , ** kwargs ) except Exception as e : spin . msg = "Something went wrong: " spin . stop_spinning ( ) spin . join ( ) raise e spin . stop_spinning ( ) spin . join ( ) return a return wrapper return decorator
11211	def normalized ( self ) : days = int ( self . days ) hours_f = round ( self . hours + 24 * ( self . days - days ) , 11 ) hours = int ( hours_f ) minutes_f = round ( self . minutes + 60 * ( hours_f - hours ) , 10 ) minutes = int ( minutes_f ) seconds_f = round ( self . seconds + 60 * ( minutes_f - minutes ) , 8 ) seconds = int ( seconds_f ) microseconds = round ( self . microseconds + 1e6 * ( seconds_f - seconds ) ) return self . __class__ ( years = self . years , months = self . months , days = days , hours = hours , minutes = minutes , seconds = seconds , microseconds = microseconds , leapdays = self . leapdays , year = self . year , month = self . month , day = self . day , weekday = self . weekday , hour = self . hour , minute = self . minute , second = self . second , microsecond = self . microsecond )
5786	def _raw_write ( self ) : data_available = libssl . BIO_ctrl_pending ( self . _wbio ) if data_available == 0 : return b'' to_read = min ( self . _buffer_size , data_available ) read = libssl . BIO_read ( self . _wbio , self . _bio_write_buffer , to_read ) to_write = bytes_from_buffer ( self . _bio_write_buffer , read ) output = to_write while len ( to_write ) : raise_disconnect = False try : sent = self . _socket . send ( to_write ) except ( socket_ . error ) as e : if e . errno == 104 or e . errno == 32 : raise_disconnect = True else : raise if raise_disconnect : raise_disconnection ( ) to_write = to_write [ sent : ] if len ( to_write ) : self . select_write ( ) return output
8319	def parse_tables ( self , markup ) : tables = [ ] m = re . findall ( self . re [ "table" ] , markup ) for chunk in m : table = WikipediaTable ( ) table . properties = chunk . split ( "\n" ) [ 0 ] . strip ( "{|" ) . strip ( ) self . connect_table ( table , chunk , markup ) row = None for chunk in chunk . split ( "\n" ) : chunk = chunk . strip ( ) if chunk . startswith ( "|+" ) : title = self . plain ( chunk . strip ( "|+" ) ) table . title = title elif chunk . startswith ( "|-" ) : if row : row . properties = chunk . strip ( "|-" ) . strip ( ) table . append ( row ) row = None elif chunk . startswith ( "|}" ) : pass elif chunk . startswith ( "|" ) or chunk . startswith ( "!" ) : row = self . parse_table_row ( chunk , row ) if row : table . append ( row ) if len ( table ) > 0 : tables . append ( table ) return tables
7841	def get_category ( self ) : var = self . xmlnode . prop ( "category" ) if not var : var = "?" return var . decode ( "utf-8" )
5993	def get_normalization_min_max ( array , norm_min , norm_max ) : if norm_min is None : norm_min = array . min ( ) if norm_max is None : norm_max = array . max ( ) return norm_min , norm_max
13613	def combine_filenames ( filenames , max_length = 40 ) : path = None names = [ ] extension = None timestamps = [ ] shas = [ ] filenames . sort ( ) concat_names = "_" . join ( filenames ) if concat_names in COMBINED_FILENAMES_GENERATED : return COMBINED_FILENAMES_GENERATED [ concat_names ] for filename in filenames : name = os . path . basename ( filename ) if not extension : extension = os . path . splitext ( name ) [ 1 ] elif os . path . splitext ( name ) [ 1 ] != extension : raise ValueError ( "Can't combine multiple file extensions" ) for base in MEDIA_ROOTS : try : shas . append ( md5 ( os . path . join ( base , filename ) ) ) break except IOError : pass if path is None : path = os . path . dirname ( filename ) else : if len ( os . path . dirname ( filename ) ) < len ( path ) : path = os . path . dirname ( filename ) m = hashlib . md5 ( ) m . update ( "," . join ( shas ) ) new_filename = "%s-inkmd" % m . hexdigest ( ) new_filename = new_filename [ : max_length ] new_filename += extension COMBINED_FILENAMES_GENERATED [ concat_names ] = new_filename return os . path . join ( path , new_filename )
13089	def ensure_remote_branch_is_tracked ( branch ) : if branch == MASTER_BRANCH : return output = subprocess . check_output ( [ 'git' , 'branch' , '--list' ] ) for line in output . split ( '\n' ) : if line . strip ( ) == branch : break else : try : sys . stdout . write ( subprocess . check_output ( [ 'git' , 'checkout' , '--track' , 'origin/%s' % branch ] ) ) except subprocess . CalledProcessError : raise SystemExit ( 1 )
8383	def update ( self ) : if self . delay > 0 : self . delay -= 1 return if self . fi == 0 : if len ( self . q ) == 1 : self . fn = float ( "inf" ) else : self . fn = len ( self . q [ self . i ] ) / self . speed self . fn = max ( self . fn , self . mf ) self . fi += 1 if self . fi > self . fn : self . fi = 0 self . i = ( self . i + 1 ) % len ( self . q )
1072	def getdomain ( self ) : sdlist = [ ] while self . pos < len ( self . field ) : if self . field [ self . pos ] in self . LWS : self . pos += 1 elif self . field [ self . pos ] == '(' : self . commentlist . append ( self . getcomment ( ) ) elif self . field [ self . pos ] == '[' : sdlist . append ( self . getdomainliteral ( ) ) elif self . field [ self . pos ] == '.' : self . pos += 1 sdlist . append ( '.' ) elif self . field [ self . pos ] in self . atomends : break else : sdlist . append ( self . getatom ( ) ) return '' . join ( sdlist )
1653	def CheckGlobalStatic ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] if linenum + 1 < clean_lines . NumLines ( ) and not Search ( r'[;({]' , line ) : line += clean_lines . elided [ linenum + 1 ] . strip ( ) match = Match ( r'((?:|static +)(?:|const +))(?::*std::)?string( +const)? +' r'([a-zA-Z0-9_:]+)\b(.*)' , line ) if ( match and not Search ( r'\bstring\b(\s+const)?\s*[\*\&]\s*(const\s+)?\w' , line ) and not Search ( r'\boperator\W' , line ) and not Match ( r'\s*(<.*>)?(::[a-zA-Z0-9_]+)*\s*\(([^"]|$)' , match . group ( 4 ) ) ) : if Search ( r'\bconst\b' , line ) : error ( filename , linenum , 'runtime/string' , 4 , 'For a static/global string constant, use a C style string ' 'instead: "%schar%s %s[]".' % ( match . group ( 1 ) , match . group ( 2 ) or '' , match . group ( 3 ) ) ) else : error ( filename , linenum , 'runtime/string' , 4 , 'Static/global string variables are not permitted.' ) if ( Search ( r'\b([A-Za-z0-9_]*_)\(\1\)' , line ) or Search ( r'\b([A-Za-z0-9_]*_)\(CHECK_NOTNULL\(\1\)\)' , line ) ) : error ( filename , linenum , 'runtime/init' , 4 , 'You seem to be initializing a member variable with itself.' )
8690	def put ( self , key ) : self . _consul_request ( 'PUT' , self . _key_url ( key [ 'name' ] ) , json = key ) return key [ 'name' ]
11336	def get_records ( self , url ) : page = urllib2 . urlopen ( url ) pages = [ BeautifulSoup ( page ) ] numpag = pages [ 0 ] . body . findAll ( 'span' , attrs = { 'class' : 'number-of-pages' } ) if len ( numpag ) > 0 : if re . search ( '^\d+$' , numpag [ 0 ] . string ) : for i in range ( int ( numpag [ 0 ] . string ) - 1 ) : page = urllib2 . urlopen ( '%s/page/%i' % ( url , i + 2 ) ) pages . append ( BeautifulSoup ( page ) ) else : print ( "number of pages %s not an integer" % ( numpag [ 0 ] . string ) ) impl = getDOMImplementation ( ) doc = impl . createDocument ( None , "collection" , None ) links = [ ] for page in pages : links += page . body . findAll ( 'p' , attrs = { 'class' : 'title' } ) links += page . body . findAll ( 'h3' , attrs = { 'class' : 'title' } ) for link in links : record = self . _get_record ( link ) doc . firstChild . appendChild ( record ) return doc . toprettyxml ( )
9791	def find_matching ( cls , path , patterns ) : for pattern in patterns : if pattern . match ( path ) : yield pattern
10161	def py_hash ( key , num_buckets ) : b , j = - 1 , 0 if num_buckets < 1 : raise ValueError ( 'num_buckets must be a positive number' ) while j < num_buckets : b = int ( j ) key = ( ( key * long ( 2862933555777941757 ) ) + 1 ) & 0xffffffffffffffff j = float ( b + 1 ) * ( float ( 1 << 31 ) / float ( ( key >> 33 ) + 1 ) ) return int ( b )
9033	def _walk ( self ) : while self . _todo : args = self . _todo . pop ( 0 ) self . _step ( * args )
1522	def is_self ( addr ) : ips = [ ] for i in netifaces . interfaces ( ) : entry = netifaces . ifaddresses ( i ) if netifaces . AF_INET in entry : for ipv4 in entry [ netifaces . AF_INET ] : if "addr" in ipv4 : ips . append ( ipv4 [ "addr" ] ) return addr in ips or addr == get_self_hostname ( )
5468	def get_event_of_type ( op , event_type ) : events = get_events ( op ) if not events : return None return [ e for e in events if e . get ( 'details' , { } ) . get ( '@type' ) == event_type ]
6532	def get_local_config ( project_path , use_cache = True ) : pyproject_path = os . path . join ( project_path , 'pyproject.toml' ) if os . path . exists ( pyproject_path ) : with open ( pyproject_path , 'r' ) as config_file : config = pytoml . load ( config_file ) config = config . get ( 'tool' , { } ) . get ( 'tidypy' , { } ) config = merge_dict ( get_default_config ( ) , config ) config = process_extensions ( config , project_path , use_cache = use_cache ) return config return None
1814	def SETNG ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . OR ( cpu . ZF , cpu . SF != cpu . OF ) , 1 , 0 ) )
5694	def create_table ( self , conn ) : cur = conn . cursor ( ) if self . tabledef is None : return if not self . tabledef . startswith ( 'CREATE' ) : cur . execute ( 'CREATE TABLE IF NOT EXISTS %s %s' % ( self . table , self . tabledef ) ) else : cur . execute ( self . tabledef ) conn . commit ( )
1416	def get_execution_state ( self , topologyName , callback = None ) : isWatching = False ret = { "result" : None } if callback : isWatching = True else : def callback ( data ) : ret [ "result" ] = data self . _get_execution_state_with_watch ( topologyName , callback , isWatching ) return ret [ "result" ]
2246	def symlink ( real_path , link_path , overwrite = False , verbose = 0 ) : path = normpath ( real_path ) link = normpath ( link_path ) if not os . path . isabs ( path ) : if _can_symlink ( ) : path = os . path . relpath ( path , os . path . dirname ( link ) ) else : path = os . path . abspath ( path ) if verbose : print ( 'Symlink: {path} -> {link}' . format ( path = path , link = link ) ) if islink ( link ) : if verbose : print ( '... already exists' ) pointed = _readlink ( link ) if pointed == path : if verbose > 1 : print ( '... and points to the right place' ) return link if verbose > 1 : if not exists ( link ) : print ( '... but it is broken and points somewhere else: {}' . format ( pointed ) ) else : print ( '... but it points somewhere else: {}' . format ( pointed ) ) if overwrite : util_io . delete ( link , verbose = verbose > 1 ) elif exists ( link ) : if _win32_links is None : if verbose : print ( '... already exists, but its a file. This will error.' ) raise FileExistsError ( 'cannot overwrite a physical path: "{}"' . format ( path ) ) else : if verbose : print ( '... already exists, and is either a file or hard link. ' 'Assuming it is a hard link. ' 'On non-win32 systems this would error.' ) if _win32_links is None : os . symlink ( path , link ) else : _win32_links . _symlink ( path , link , overwrite = overwrite , verbose = verbose ) return link
13629	def put ( self , metrics ) : if type ( metrics ) == list : for metric in metrics : self . c . put_metric_data ( ** metric ) else : self . c . put_metric_data ( ** metrics )
1419	def get_scheduler_location ( self , topologyName , callback = None ) : isWatching = False ret = { "result" : None } if callback : isWatching = True else : def callback ( data ) : ret [ "result" ] = data self . _get_scheduler_location_with_watch ( topologyName , callback , isWatching ) return ret [ "result" ]
4563	def to_type_constructor ( value , python_path = None ) : if not value : return value if callable ( value ) : return { 'datatype' : value } value = to_type ( value ) typename = value . get ( 'typename' ) if typename : r = aliases . resolve ( typename ) try : value [ 'datatype' ] = importer . import_symbol ( r , python_path = python_path ) del value [ 'typename' ] except Exception as e : value [ '_exception' ] = e return value
10272	def get_unweighted_sources ( graph : BELGraph , key : Optional [ str ] = None ) -> Iterable [ BaseEntity ] : if key is None : key = WEIGHT for node in graph : if is_unweighted_source ( graph , node , key ) : yield node
10632	def get_compound_mfr ( self , compound ) : if compound in self . material . compounds : return self . _compound_mfrs [ self . material . get_compound_index ( compound ) ] else : return 0.0
11777	def replicated_dataset ( dataset , weights , n = None ) : "Copy dataset, replicating each example in proportion to its weight." n = n or len ( dataset . examples ) result = copy . copy ( dataset ) result . examples = weighted_replicate ( dataset . examples , weights , n ) return result
7210	def stdout ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get stdout.' ) if self . batch_values : raise NotImplementedError ( "Query Each Workflow Id within the Batch Workflow for stdout." ) wf = self . workflow . get ( self . id ) stdout_list = [ ] for task in wf [ 'tasks' ] : stdout_list . append ( { 'id' : task [ 'id' ] , 'taskType' : task [ 'taskType' ] , 'name' : task [ 'name' ] , 'stdout' : self . workflow . get_stdout ( self . id , task [ 'id' ] ) } ) return stdout_list
6754	def all_other_enabled_satchels ( self ) : return dict ( ( name , satchel ) for name , satchel in self . all_satchels . items ( ) if name != self . name . upper ( ) and name . lower ( ) in map ( str . lower , self . genv . services ) )
8410	def best_units ( self , sequence ) : ts_range = self . value ( max ( sequence ) ) - self . value ( min ( sequence ) ) package = self . determine_package ( sequence [ 0 ] ) if package == 'pandas' : cuts = [ ( 0.9 , 'us' ) , ( 0.9 , 'ms' ) , ( 0.9 , 's' ) , ( 9 , 'm' ) , ( 6 , 'h' ) , ( 4 , 'd' ) , ( 4 , 'w' ) , ( 4 , 'M' ) , ( 3 , 'y' ) ] denomination = NANOSECONDS base_units = 'ns' else : cuts = [ ( 0.9 , 's' ) , ( 9 , 'm' ) , ( 6 , 'h' ) , ( 4 , 'd' ) , ( 4 , 'w' ) , ( 4 , 'M' ) , ( 3 , 'y' ) ] denomination = SECONDS base_units = 'ms' for size , units in reversed ( cuts ) : if ts_range >= size * denomination [ units ] : return units return base_units
7832	def _new_from_xml ( cls , xmlnode ) : child = xmlnode . children fields = [ ] while child : if child . type != "element" or child . ns ( ) . content != DATAFORM_NS : pass elif child . name == "field" : fields . append ( Field . _new_from_xml ( child ) ) child = child . next return cls ( fields )
11241	def get_line_count ( fname ) : i = 0 with open ( fname ) as f : for i , l in enumerate ( f ) : pass return i + 1
6684	def check_for_change ( self ) : r = self . local_renderer lm = self . last_manifest last_fingerprint = lm . fingerprint current_fingerprint = self . get_target_geckodriver_version_number ( ) self . vprint ( 'last_fingerprint:' , last_fingerprint ) self . vprint ( 'current_fingerprint:' , current_fingerprint ) if last_fingerprint != current_fingerprint : print ( 'A new release is available. %s' % self . get_most_recent_version ( ) ) return True print ( 'No updates found.' ) return False
8479	def potential ( self , value ) : if value : self . _potential = True else : self . _potential = False
7306	def process_document ( self , document , form_key , passed_key ) : if passed_key is not None : current_key , remaining_key_array = trim_field_key ( document , passed_key ) else : current_key , remaining_key_array = trim_field_key ( document , form_key ) key_array_digit = remaining_key_array [ - 1 ] if remaining_key_array and has_digit ( remaining_key_array ) else None remaining_key = make_key ( remaining_key_array ) if current_key . lower ( ) == 'id' : raise KeyError ( u"Mongonaut does not work with models which have fields beginning with id_" ) is_embedded_doc = ( isinstance ( document . _fields . get ( current_key , None ) , EmbeddedDocumentField ) if hasattr ( document , '_fields' ) else False ) is_list = not key_array_digit is None key_in_fields = current_key in document . _fields . keys ( ) if hasattr ( document , '_fields' ) else False if key_in_fields : if is_embedded_doc : self . set_embedded_doc ( document , form_key , current_key , remaining_key ) elif is_list : self . set_list_field ( document , form_key , current_key , remaining_key , key_array_digit ) else : value = translate_value ( document . _fields [ current_key ] , self . form . cleaned_data [ form_key ] ) setattr ( document , current_key , value )
8998	def string ( self , string ) : object_ = json . loads ( string ) return self . object ( object_ )
9591	def set_window_size ( self , width , height , window_handle = 'current' ) : self . _execute ( Command . SET_WINDOW_SIZE , { 'width' : int ( width ) , 'height' : int ( height ) , 'window_handle' : window_handle } )
69	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , alpha = 1.0 , size = 1 , copy = True , raise_if_out_of_image = False , thickness = None ) : image = np . copy ( image ) if copy else image for bb in self . bounding_boxes : image = bb . draw_on_image ( image , color = color , alpha = alpha , size = size , copy = False , raise_if_out_of_image = raise_if_out_of_image , thickness = thickness ) return image
7491	def compute_tree_stats ( self , ipyclient ) : names = self . samples if self . params . nboots : fulltre = ete3 . Tree ( self . trees . tree , format = 0 ) fulltre . unroot ( ) with open ( self . trees . boots , 'r' ) as inboots : bb = [ ete3 . Tree ( i . strip ( ) , format = 0 ) for i in inboots . readlines ( ) ] wboots = [ fulltre ] + bb [ - self . params . nboots : ] wctre , wcounts = consensus_tree ( wboots , names = names ) self . trees . cons = os . path . join ( self . dirs , self . name + ".cons" ) with open ( self . trees . cons , 'w' ) as ocons : ocons . write ( wctre . write ( format = 0 ) ) else : wctre = ete3 . Tree ( self . trees . tree , format = 0 ) wctre . unroot ( ) self . trees . nhx = os . path . join ( self . dirs , self . name + ".nhx" ) with open ( self . files . stats , 'w' ) as ostats : if self . params . nboots : ostats . write ( "## splits observed in {} trees\n" . format ( len ( wboots ) ) ) for i , j in enumerate ( self . samples ) : ostats . write ( "{:<3} {}\n" . format ( i , j ) ) ostats . write ( "\n" ) for split , freq in wcounts : if split . count ( '1' ) > 1 : ostats . write ( "{} {:.2f}\n" . format ( split , round ( freq , 2 ) ) ) ostats . write ( "\n" ) lbview = ipyclient . load_balanced_view ( ) qtots = { } qsamp = { } tots = sum ( 1 for i in wctre . iter_leaves ( ) ) totn = set ( wctre . get_leaf_names ( ) ) for node in wctre . traverse ( ) : qtots [ node ] = lbview . apply ( _get_total , * ( tots , node ) ) qsamp [ node ] = lbview . apply ( _get_sampled , * ( self , totn , node ) ) ipyclient . wait ( ) for node in wctre . traverse ( ) : total = qtots [ node ] . result ( ) sampled = qsamp [ node ] . result ( ) node . add_feature ( "quartets_total" , total ) node . add_feature ( "quartets_sampled" , sampled ) features = [ "quartets_total" , "quartets_sampled" ] with open ( self . trees . nhx , 'w' ) as outtre : outtre . write ( wctre . write ( format = 0 , features = features ) )
11491	def download ( server_path , local_path = '.' ) : session . token = verify_credentials ( ) is_item , resource_id = _find_resource_id_from_path ( server_path ) if resource_id == - 1 : print ( 'Unable to locate {0}' . format ( server_path ) ) else : if is_item : _download_item ( resource_id , local_path ) else : _download_folder_recursive ( resource_id , local_path )
13027	def detect_os ( self , ip ) : process = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'checker.py' ) , str ( ip ) ] , stdout = subprocess . PIPE ) out = process . stdout . decode ( 'utf-8' ) . split ( '\n' ) system_os = '' for line in out : if line . startswith ( 'Target OS:' ) : system_os = line . replace ( 'Target OS: ' , '' ) break return system_os
7888	def error ( self , stanza ) : err = stanza . get_error ( ) self . __logger . debug ( "Error from: %r Condition: %r" % ( stanza . get_from ( ) , err . get_condition ) )
19	def sf01 ( arr ) : s = arr . shape return arr . swapaxes ( 0 , 1 ) . reshape ( s [ 0 ] * s [ 1 ] , * s [ 2 : ] )
1966	def syscall ( self ) : index = self . _syscall_abi . syscall_number ( ) try : table = getattr ( linux_syscalls , self . current . machine ) name = table . get ( index , None ) implementation = getattr ( self , name ) except ( AttributeError , KeyError ) : if name is not None : raise SyscallNotImplemented ( index , name ) else : raise Exception ( f"Bad syscall index, {index}" ) return self . _syscall_abi . invoke ( implementation )
8239	def triad ( clr , angle = 120 ) : clr = color ( clr ) colors = colorlist ( clr ) colors . append ( clr . rotate_ryb ( angle ) . lighten ( 0.1 ) ) colors . append ( clr . rotate_ryb ( - angle ) . lighten ( 0.1 ) ) return colors
4314	def validate_input_file ( input_filepath ) : if not os . path . exists ( input_filepath ) : raise IOError ( "input_filepath {} does not exist." . format ( input_filepath ) ) ext = file_extension ( input_filepath ) if ext not in VALID_FORMATS : logger . info ( "Valid formats: %s" , " " . join ( VALID_FORMATS ) ) logger . warning ( "This install of SoX cannot process .{} files." . format ( ext ) )
11240	def copy_web_file_to_local ( file_path , target_path ) : response = urllib . request . urlopen ( file_path ) f = open ( target_path , 'w' ) f . write ( response . read ( ) ) f . close ( )
2900	def get_tasks ( self , state = Task . ANY_MASK ) : return [ t for t in Task . Iterator ( self . task_tree , state ) ]
4264	def build ( source , destination , debug , verbose , force , config , theme , title , ncpu ) : level = ( ( debug and logging . DEBUG ) or ( verbose and logging . INFO ) or logging . WARNING ) init_logging ( __name__ , level = level ) logger = logging . getLogger ( __name__ ) if not os . path . isfile ( config ) : logger . error ( "Settings file not found: %s" , config ) sys . exit ( 1 ) start_time = time . time ( ) settings = read_settings ( config ) for key in ( 'source' , 'destination' , 'theme' ) : arg = locals ( ) [ key ] if arg is not None : settings [ key ] = os . path . abspath ( arg ) logger . info ( "%12s : %s" , key . capitalize ( ) , settings [ key ] ) if not settings [ 'source' ] or not os . path . isdir ( settings [ 'source' ] ) : logger . error ( "Input directory not found: %s" , settings [ 'source' ] ) sys . exit ( 1 ) relative_check = True try : relative_check = os . path . relpath ( settings [ 'destination' ] , settings [ 'source' ] ) . startswith ( '..' ) except ValueError : pass if not relative_check : logger . error ( "Output directory should be outside of the input " "directory." ) sys . exit ( 1 ) if title : settings [ 'title' ] = title locale . setlocale ( locale . LC_ALL , settings [ 'locale' ] ) init_plugins ( settings ) gal = Gallery ( settings , ncpu = ncpu ) gal . build ( force = force ) for src , dst in settings [ 'files_to_copy' ] : src = os . path . join ( settings [ 'source' ] , src ) dst = os . path . join ( settings [ 'destination' ] , dst ) logger . debug ( 'Copy %s to %s' , src , dst ) copy ( src , dst , symlink = settings [ 'orig_link' ] , rellink = settings [ 'rel_link' ] ) stats = gal . stats def format_stats ( _type ) : opt = [ "{} {}" . format ( stats [ _type + '_' + subtype ] , subtype ) for subtype in ( 'skipped' , 'failed' ) if stats [ _type + '_' + subtype ] > 0 ] opt = ' ({})' . format ( ', ' . join ( opt ) ) if opt else '' return '{} {}s{}' . format ( stats [ _type ] , _type , opt ) print ( 'Done.\nProcessed {} and {} in {:.2f} seconds.' . format ( format_stats ( 'image' ) , format_stats ( 'video' ) , time . time ( ) - start_time ) )
10328	def rank_causalr_hypothesis ( graph , node_to_regulation , regulator_node ) : upregulation_hypothesis = { 'correct' : 0 , 'incorrect' : 0 , 'ambiguous' : 0 } downregulation_hypothesis = { 'correct' : 0 , 'incorrect' : 0 , 'ambiguous' : 0 } targets = [ node for node in node_to_regulation if node != regulator_node ] predicted_regulations = run_cna ( graph , regulator_node , targets ) for _ , target_node , predicted_regulation in predicted_regulations : if ( predicted_regulation is Effect . inhibition or predicted_regulation is Effect . activation ) and ( predicted_regulation . value == node_to_regulation [ target_node ] ) : upregulation_hypothesis [ 'correct' ] += 1 downregulation_hypothesis [ 'incorrect' ] += 1 elif predicted_regulation is Effect . ambiguous : upregulation_hypothesis [ 'ambiguous' ] += 1 downregulation_hypothesis [ 'ambiguous' ] += 1 elif predicted_regulation is Effect . no_effect : continue else : downregulation_hypothesis [ 'correct' ] += 1 upregulation_hypothesis [ 'incorrect' ] += 1 upregulation_hypothesis [ 'score' ] = upregulation_hypothesis [ 'correct' ] - upregulation_hypothesis [ 'incorrect' ] downregulation_hypothesis [ 'score' ] = downregulation_hypothesis [ 'correct' ] - downregulation_hypothesis [ 'incorrect' ] return upregulation_hypothesis , downregulation_hypothesis
3436	def optimize ( self , objective_sense = None , raise_error = False ) : original_direction = self . objective . direction self . objective . direction = { "maximize" : "max" , "minimize" : "min" } . get ( objective_sense , original_direction ) self . slim_optimize ( ) solution = get_solution ( self , raise_error = raise_error ) self . objective . direction = original_direction return solution
6686	def is_installed ( pkg_name ) : manager = MANAGER with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : res = run ( "rpm --query %(pkg_name)s" % locals ( ) ) if res . succeeded : return True return False
340	def log_if ( level , msg , condition , * args ) : if condition : vlog ( level , msg , * args )
2077	def secho ( message , ** kwargs ) : if not settings . color : for key in ( 'fg' , 'bg' , 'bold' , 'blink' ) : kwargs . pop ( key , None ) return click . secho ( message , ** kwargs )
9528	def pbkdf2 ( password , salt , iterations , dklen = 0 , digest = None ) : if digest is None : digest = settings . CRYPTOGRAPHY_DIGEST if not dklen : dklen = digest . digest_size password = force_bytes ( password ) salt = force_bytes ( salt ) kdf = PBKDF2HMAC ( algorithm = digest , length = dklen , salt = salt , iterations = iterations , backend = settings . CRYPTOGRAPHY_BACKEND ) return kdf . derive ( password )
4461	def transform ( self , jam ) : yield jam for jam_out in self . transformer . transform ( jam ) : yield jam_out
1662	def FilesBelongToSameModule ( filename_cc , filename_h ) : fileinfo_cc = FileInfo ( filename_cc ) if not fileinfo_cc . Extension ( ) . lstrip ( '.' ) in GetNonHeaderExtensions ( ) : return ( False , '' ) fileinfo_h = FileInfo ( filename_h ) if not fileinfo_h . Extension ( ) . lstrip ( '.' ) in GetHeaderExtensions ( ) : return ( False , '' ) filename_cc = filename_cc [ : - ( len ( fileinfo_cc . Extension ( ) ) ) ] matched_test_suffix = Search ( _TEST_FILE_SUFFIX , fileinfo_cc . BaseName ( ) ) if matched_test_suffix : filename_cc = filename_cc [ : - len ( matched_test_suffix . group ( 1 ) ) ] filename_cc = filename_cc . replace ( '/public/' , '/' ) filename_cc = filename_cc . replace ( '/internal/' , '/' ) filename_h = filename_h [ : - ( len ( fileinfo_h . Extension ( ) ) ) ] if filename_h . endswith ( '-inl' ) : filename_h = filename_h [ : - len ( '-inl' ) ] filename_h = filename_h . replace ( '/public/' , '/' ) filename_h = filename_h . replace ( '/internal/' , '/' ) files_belong_to_same_module = filename_cc . endswith ( filename_h ) common_path = '' if files_belong_to_same_module : common_path = filename_cc [ : - len ( filename_h ) ] return files_belong_to_same_module , common_path
11807	def encode ( plaintext , code ) : "Encodes text, using a code which is a permutation of the alphabet." from string import maketrans trans = maketrans ( alphabet + alphabet . upper ( ) , code + code . upper ( ) ) return plaintext . translate ( trans )
7014	def concatenate_textlcs ( lclist , sortby = 'rjd' , normalize = True ) : lcdict = read_hatpi_textlc ( lclist [ 0 ] ) lccounter = 0 lcdict [ 'concatenated' ] = { lccounter : os . path . abspath ( lclist [ 0 ] ) } lcdict [ 'lcn' ] = np . full_like ( lcdict [ 'rjd' ] , lccounter ) if normalize : for col in MAGCOLS : if col in lcdict : thismedval = np . nanmedian ( lcdict [ col ] ) if col in ( 'ifl1' , 'ifl2' , 'ifl3' ) : lcdict [ col ] = lcdict [ col ] / thismedval else : lcdict [ col ] = lcdict [ col ] - thismedval for lcf in lclist [ 1 : ] : thislcd = read_hatpi_textlc ( lcf ) if thislcd [ 'columns' ] != lcdict [ 'columns' ] : LOGERROR ( 'file %s does not have the ' 'same columns as first file %s, skipping...' % ( lcf , lclist [ 0 ] ) ) continue else : LOGINFO ( 'adding %s (ndet: %s) to %s (ndet: %s)' % ( lcf , thislcd [ 'objectinfo' ] [ 'ndet' ] , lclist [ 0 ] , lcdict [ lcdict [ 'columns' ] [ 0 ] ] . size ) ) lccounter = lccounter + 1 lcdict [ 'concatenated' ] [ lccounter ] = os . path . abspath ( lcf ) lcdict [ 'lcn' ] = np . concatenate ( ( lcdict [ 'lcn' ] , np . full_like ( thislcd [ 'rjd' ] , lccounter ) ) ) for col in lcdict [ 'columns' ] : if normalize and col in MAGCOLS : thismedval = np . nanmedian ( thislcd [ col ] ) if col in ( 'ifl1' , 'ifl2' , 'ifl3' ) : thislcd [ col ] = thislcd [ col ] / thismedval else : thislcd [ col ] = thislcd [ col ] - thismedval lcdict [ col ] = np . concatenate ( ( lcdict [ col ] , thislcd [ col ] ) ) lcdict [ 'objectinfo' ] [ 'ndet' ] = lcdict [ lcdict [ 'columns' ] [ 0 ] ] . size lcdict [ 'objectinfo' ] [ 'stations' ] = [ 'HP%s' % x for x in np . unique ( lcdict [ 'stf' ] ) . tolist ( ) ] lcdict [ 'nconcatenated' ] = lccounter + 1 if sortby and sortby in [ x [ 0 ] for x in COLDEFS ] : LOGINFO ( 'sorting concatenated light curve by %s...' % sortby ) sortind = np . argsort ( lcdict [ sortby ] ) for col in lcdict [ 'columns' ] : lcdict [ col ] = lcdict [ col ] [ sortind ] lcdict [ 'lcn' ] = lcdict [ 'lcn' ] [ sortind ] LOGINFO ( 'done. concatenated light curve has %s detections' % lcdict [ 'objectinfo' ] [ 'ndet' ] ) return lcdict
11973	def convert ( ip , notation = IP_DOT , inotation = IP_UNKNOWN , check = True ) : return _convert ( ip , notation , inotation , _check = check , _isnm = False )
10152	def _extract_path_from_service ( self , service ) : path_obj = { } path = service . path route_name = getattr ( service , 'pyramid_route' , None ) if route_name : registry = self . pyramid_registry or get_current_registry ( ) route_intr = registry . introspector . get ( 'routes' , route_name ) if route_intr : path = route_intr [ 'pattern' ] else : msg = 'Route `{}` is not found by ' 'pyramid introspector' . format ( route_name ) raise ValueError ( msg ) for subpath_marker in ( '*subpath' , '*traverse' ) : path = path . replace ( subpath_marker , '{subpath}' ) parameters = self . parameters . from_path ( path ) if parameters : path_obj [ 'parameters' ] = parameters return path , path_obj
1849	def LOOP ( cpu , dest ) : counter_name = { 16 : 'CX' , 32 : 'ECX' , 64 : 'RCX' } [ cpu . address_bit_size ] counter = cpu . write_register ( counter_name , cpu . read_register ( counter_name ) - 1 ) cpu . PC = Operators . ITEBV ( cpu . address_bit_size , counter == 0 , ( cpu . PC + dest . read ( ) ) & ( ( 1 << dest . size ) - 1 ) , cpu . PC + cpu . instruction . size )
13203	def format_authors ( self , format = 'html5' , deparagraph = True , mathjax = False , smart = True , extra_args = None ) : formatted_authors = [ ] for latex_author in self . authors : formatted_author = convert_lsstdoc_tex ( latex_author , format , deparagraph = deparagraph , mathjax = mathjax , smart = smart , extra_args = extra_args ) formatted_author = formatted_author . strip ( ) formatted_authors . append ( formatted_author ) return formatted_authors
6410	def lehmer_mean ( nums , exp = 2 ) : r return sum ( x ** exp for x in nums ) / sum ( x ** ( exp - 1 ) for x in nums )
11076	def load_user_rights ( self , user ) : if user . username in self . admins : user . is_admin = True elif not hasattr ( user , 'is_admin' ) : user . is_admin = False
2471	def set_file_atrificat_of_project ( self , doc , symbol , value ) : if self . has_package ( doc ) and self . has_file ( doc ) : self . file ( doc ) . add_artifact ( symbol , value ) else : raise OrderError ( 'File::Artificat' )
11209	def datetime_exists ( dt , tz = None ) : if tz is None : if dt . tzinfo is None : raise ValueError ( 'Datetime is naive and no time zone provided.' ) tz = dt . tzinfo dt = dt . replace ( tzinfo = None ) dt_rt = dt . replace ( tzinfo = tz ) . astimezone ( tzutc ( ) ) . astimezone ( tz ) dt_rt = dt_rt . replace ( tzinfo = None ) return dt == dt_rt
1885	def solve_buffer ( self , addr , nbytes , constrain = False ) : buffer = self . cpu . read_bytes ( addr , nbytes ) result = [ ] with self . _constraints as temp_cs : cs_to_use = self . constraints if constrain else temp_cs for c in buffer : result . append ( self . _solver . get_value ( cs_to_use , c ) ) cs_to_use . add ( c == result [ - 1 ] ) return result
10136	def _detect_or_validate ( self , val ) : if isinstance ( val , list ) or isinstance ( val , dict ) or isinstance ( val , SortableDict ) or isinstance ( val , Grid ) : self . _assert_version ( VER_3_0 )
5179	def reports ( self , ** kwargs ) : return self . __api . reports ( query = EqualsOperator ( "certname" , self . name ) , ** kwargs )
8488	def load ( self , prefix = None , depth = None ) : prefix = prefix or self . prefix prefix = '/' + prefix . strip ( '/' ) + '/' if depth is None : depth = self . inherit_depth if not self . configured : log . debug ( "etcd not available" ) return if self . watching : log . info ( "Starting watcher for %r" , prefix ) self . start_watching ( ) log . info ( "Loading from etcd %r" , prefix ) try : result = self . client . get ( prefix ) except self . module . EtcdKeyNotFound : result = None if not result : log . info ( "No configuration found" ) return { } update = { } for item in result . children : key = item . key value = item . value try : value = pytool . json . from_json ( value ) except : pass if not self . case_sensitive : key = key . lower ( ) if key . startswith ( prefix ) : key = key [ len ( prefix ) : ] update [ key ] = value inherited = Config ( ) . settings . get ( self . inherit_key , update . get ( self . inherit_key , None ) ) if depth > 0 and inherited : log . info ( " ... inheriting ..." ) inherited = self . load ( inherited , depth - 1 ) or { } inherited . update ( update ) update = inherited return update
8019	async def websocket_send ( self , message , stream_name ) : text = message . get ( "text" ) json = await self . decode_json ( text ) data = { "stream" : stream_name , "payload" : json } await self . send_json ( data )
11256	def flatten ( prev , depth = sys . maxsize ) : def inner_flatten ( iterable , curr_level , max_levels ) : for i in iterable : if hasattr ( i , '__iter__' ) and curr_level < max_levels : for j in inner_flatten ( i , curr_level + 1 , max_levels ) : yield j else : yield i for d in prev : if hasattr ( d , '__iter__' ) and depth > 0 : for inner_d in inner_flatten ( d , 1 , depth ) : yield inner_d else : yield d
6786	def get_component_funcs ( self , components = None ) : current_tp = self . get_current_thumbprint ( components = components ) or { } previous_tp = self . get_previous_thumbprint ( components = components ) or { } if self . verbose : print ( 'Current thumbprint:' ) pprint ( current_tp , indent = 4 ) print ( 'Previous thumbprint:' ) pprint ( previous_tp , indent = 4 ) differences = list ( iter_dict_differences ( current_tp , previous_tp ) ) if self . verbose : print ( 'Differences:' ) pprint ( differences , indent = 4 ) component_order = get_component_order ( [ k for k , ( _ , _ ) in differences ] ) if self . verbose : print ( 'component_order:' ) pprint ( component_order , indent = 4 ) plan_funcs = list ( get_deploy_funcs ( component_order , current_tp , previous_tp ) ) return component_order , plan_funcs
4001	def get_port_spec_document ( expanded_active_specs , docker_vm_ip ) : forwarding_port = 65000 port_spec = { 'docker_compose' : { } , 'nginx' : [ ] , 'hosts_file' : [ ] } host_full_addresses , host_names , stream_host_ports = set ( ) , set ( ) , set ( ) for app_name in sorted ( expanded_active_specs [ 'apps' ] . keys ( ) ) : app_spec = expanded_active_specs [ 'apps' ] [ app_name ] if 'host_forwarding' not in app_spec : continue port_spec [ 'docker_compose' ] [ app_name ] = [ ] for host_forwarding_spec in app_spec [ 'host_forwarding' ] : _add_full_addresses ( host_forwarding_spec , host_full_addresses ) if host_forwarding_spec [ 'type' ] == 'stream' : _add_stream_host_port ( host_forwarding_spec , stream_host_ports ) port_spec [ 'docker_compose' ] [ app_name ] . append ( _docker_compose_port_spec ( host_forwarding_spec , forwarding_port ) ) port_spec [ 'nginx' ] . append ( _nginx_port_spec ( host_forwarding_spec , forwarding_port , docker_vm_ip ) ) _add_host_names ( host_forwarding_spec , docker_vm_ip , port_spec , host_names ) forwarding_port += 1 return port_spec
8910	def owsproxy_delegate ( request ) : twitcher_url = request . registry . settings . get ( 'twitcher.url' ) protected_path = request . registry . settings . get ( 'twitcher.ows_proxy_protected_path' , '/ows' ) url = twitcher_url + protected_path + '/proxy' if request . matchdict . get ( 'service_name' ) : url += '/' + request . matchdict . get ( 'service_name' ) if request . matchdict . get ( 'access_token' ) : url += '/' + request . matchdict . get ( 'service_name' ) url += '?' + urlparse . urlencode ( request . params ) LOGGER . debug ( "delegate to owsproxy: %s" , url ) resp = requests . request ( method = request . method . upper ( ) , url = url , data = request . body , headers = request . headers , verify = False ) return Response ( resp . content , status = resp . status_code , headers = resp . headers )
843	def _addPartitionId ( self , index , partitionId = None ) : if partitionId is None : self . _partitionIdList . append ( numpy . inf ) else : self . _partitionIdList . append ( partitionId ) indices = self . _partitionIdMap . get ( partitionId , [ ] ) indices . append ( index ) self . _partitionIdMap [ partitionId ] = indices
6677	def md5sum ( self , filename , use_sudo = False ) : func = use_sudo and run_as_root or self . run with self . settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : if exists ( u'/usr/bin/md5sum' ) : res = func ( u'/usr/bin/md5sum %(filename)s' % locals ( ) ) elif exists ( u'/sbin/md5' ) : res = func ( u'/sbin/md5 -r %(filename)s' % locals ( ) ) elif exists ( u'/opt/local/gnu/bin/md5sum' ) : res = func ( u'/opt/local/gnu/bin/md5sum %(filename)s' % locals ( ) ) elif exists ( u'/opt/local/bin/md5sum' ) : res = func ( u'/opt/local/bin/md5sum %(filename)s' % locals ( ) ) else : md5sum = func ( u'which md5sum' ) md5 = func ( u'which md5' ) if exists ( md5sum ) : res = func ( '%(md5sum)s %(filename)s' % locals ( ) ) elif exists ( md5 ) : res = func ( '%(md5)s %(filename)s' % locals ( ) ) else : abort ( 'No MD5 utility was found on this system.' ) if res . succeeded : _md5sum = res else : warn ( res ) _md5sum = None if isinstance ( _md5sum , six . string_types ) : _md5sum = _md5sum . strip ( ) . split ( '\n' ) [ - 1 ] . split ( ) [ 0 ] return _md5sum
13720	def main ( ) : ep = requests . get ( TRELLO_API_DOC ) . content root = html . fromstring ( ep ) links = root . xpath ( '//a[contains(@class, "reference internal")]/@href' ) pages = [ requests . get ( TRELLO_API_DOC + u ) for u in links if u . endswith ( 'index.html' ) ] endpoints = [ ] for page in pages : root = html . fromstring ( page . content ) sections = root . xpath ( '//div[@class="section"]/h2/..' ) for sec in sections : ep_html = etree . tostring ( sec ) . decode ( 'utf-8' ) ep_text = html2text ( ep_html ) . splitlines ( ) match = EP_DESC_REGEX . match ( ep_text [ 0 ] ) if not match : continue ep_method , ep_url = match . groups ( ) ep_text [ 0 ] = ' ' . join ( [ ep_method , ep_url ] ) ep_doc = b64encode ( gzip . compress ( '\n' . join ( ep_text ) . encode ( 'utf-8' ) ) ) endpoints . append ( ( ep_method , ep_url , ep_doc ) ) print ( yaml . dump ( create_tree ( endpoints ) ) )
13714	def upload ( self ) : success = False batch = self . next ( ) if len ( batch ) == 0 : return False try : self . request ( batch ) success = True except Exception as e : self . log . error ( 'error uploading: %s' , e ) success = False if self . on_error : self . on_error ( e , batch ) finally : for item in batch : self . queue . task_done ( ) return success
10820	def _filter ( cls , query , state = MembershipState . ACTIVE , eager = None ) : query = query . filter_by ( state = state ) eager = eager or [ ] for field in eager : query = query . options ( joinedload ( field ) ) return query
7649	def _open ( name_or_fdesc , mode = 'r' , fmt = 'auto' ) : open_map = { 'jams' : open , 'json' : open , 'jamz' : gzip . open , 'gz' : gzip . open } if hasattr ( name_or_fdesc , 'read' ) or hasattr ( name_or_fdesc , 'write' ) : yield name_or_fdesc elif isinstance ( name_or_fdesc , six . string_types ) : if fmt == 'auto' : _ , ext = os . path . splitext ( name_or_fdesc ) ext = ext [ 1 : ] else : ext = fmt try : ext = ext . lower ( ) if ext in [ 'jamz' , 'gz' ] and 't' not in mode : mode = '{:s}t' . format ( mode ) with open_map [ ext ] ( name_or_fdesc , mode = mode ) as fdesc : yield fdesc except KeyError : raise ParameterError ( 'Unknown JAMS extension ' 'format: "{:s}"' . format ( ext ) ) else : raise ParameterError ( 'Invalid filename or ' 'descriptor: {}' . format ( name_or_fdesc ) )
4716	def tcase_setup ( trun , parent , tcase_fname ) : case = copy . deepcopy ( TESTCASE ) case [ "fname" ] = tcase_fname case [ "fpath_orig" ] = os . sep . join ( [ trun [ "conf" ] [ "TESTCASES" ] , case [ "fname" ] ] ) if not os . path . exists ( case [ "fpath_orig" ] ) : cij . err ( 'rnr:tcase_setup: !case["fpath_orig"]: %r' % case [ "fpath_orig" ] ) return None case [ "name" ] = os . path . splitext ( case [ "fname" ] ) [ 0 ] case [ "ident" ] = "/" . join ( [ parent [ "ident" ] , case [ "fname" ] ] ) case [ "res_root" ] = os . sep . join ( [ parent [ "res_root" ] , case [ "fname" ] ] ) case [ "aux_root" ] = os . sep . join ( [ case [ "res_root" ] , "_aux" ] ) case [ "log_fpath" ] = os . sep . join ( [ case [ "res_root" ] , "run.log" ] ) case [ "fpath" ] = os . sep . join ( [ case [ "res_root" ] , case [ "fname" ] ] ) case [ "evars" ] . update ( copy . deepcopy ( parent [ "evars" ] ) ) os . makedirs ( case [ "res_root" ] ) os . makedirs ( case [ "aux_root" ] ) shutil . copyfile ( case [ "fpath_orig" ] , case [ "fpath" ] ) case [ "hooks" ] = hooks_setup ( trun , case , parent . get ( "hooks_pr_tcase" ) ) return case
6322	def resolve_loader ( self , meta : ProgramDescription ) : if not meta . loader : meta . loader = 'single' if meta . path else 'separate' for loader_cls in self . _loaders : if loader_cls . name == meta . loader : meta . loader_cls = loader_cls break else : raise ImproperlyConfigured ( ( "Program {} has no loader class registered." "Check PROGRAM_LOADERS or PROGRAM_DIRS" ) . format ( meta . path ) )
12104	def _qsub_collate_and_launch ( self , output_dir , error_dir , job_names ) : job_name = "%s_%s_collate_%d" % ( self . batch_name , self . job_timestamp , self . collate_count ) overrides = [ ( "-e" , error_dir ) , ( '-N' , job_name ) , ( "-o" , output_dir ) , ( '-hold_jid' , ',' . join ( job_names ) ) ] resume_cmds = [ "import os, pickle, lancet" , ( "pickle_path = os.path.join(%r, 'qlauncher.pkl')" % self . root_directory ) , "launcher = pickle.load(open(pickle_path,'rb'))" , "launcher.collate_and_launch()" ] cmd_args = [ self . command . executable , '-c' , ';' . join ( resume_cmds ) ] popen_args = self . _qsub_args ( overrides , cmd_args ) p = subprocess . Popen ( popen_args , stdout = subprocess . PIPE ) ( stdout , stderr ) = p . communicate ( ) self . debug ( stdout ) if p . poll ( ) != 0 : raise EnvironmentError ( "qsub command exit with code: %d" % p . poll ( ) ) self . collate_count += 1 self . message ( "Invoked qsub for next batch." ) return job_name
298	def plot_turnover ( returns , transactions , positions , legend_loc = 'best' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) df_turnover = txn . get_turnover ( positions , transactions ) df_turnover_by_month = df_turnover . resample ( "M" ) . mean ( ) df_turnover . plot ( color = 'steelblue' , alpha = 1.0 , lw = 0.5 , ax = ax , ** kwargs ) df_turnover_by_month . plot ( color = 'orangered' , alpha = 0.5 , lw = 2 , ax = ax , ** kwargs ) ax . axhline ( df_turnover . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 , alpha = 1.0 ) ax . legend ( [ 'Daily turnover' , 'Average daily turnover, by month' , 'Average daily turnover, net' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) ax . set_title ( 'Daily turnover' ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) ax . set_ylim ( ( 0 , 2 ) ) ax . set_ylabel ( 'Turnover' ) ax . set_xlabel ( '' ) return ax
6313	def print ( self ) : print ( "---[ START {} ]---" . format ( self . name ) ) for i , line in enumerate ( self . lines ) : print ( "{}: {}" . format ( str ( i ) . zfill ( 3 ) , line ) ) print ( "---[ END {} ]---" . format ( self . name ) )
7821	def challenge ( self , challenge ) : if not challenge : logger . debug ( "Empty challenge" ) return Failure ( "bad-challenge" ) if self . _server_first_message : return self . _final_challenge ( challenge ) match = SERVER_FIRST_MESSAGE_RE . match ( challenge ) if not match : logger . debug ( "Bad challenge syntax: {0!r}" . format ( challenge ) ) return Failure ( "bad-challenge" ) self . _server_first_message = challenge mext = match . group ( "mext" ) if mext : logger . debug ( "Unsupported extension received: {0!r}" . format ( mext ) ) return Failure ( "bad-challenge" ) nonce = match . group ( "nonce" ) if not nonce . startswith ( self . _c_nonce ) : logger . debug ( "Nonce does not start with our nonce" ) return Failure ( "bad-challenge" ) salt = match . group ( "salt" ) try : salt = a2b_base64 ( salt ) except ValueError : logger . debug ( "Bad base64 encoding for salt: {0!r}" . format ( salt ) ) return Failure ( "bad-challenge" ) iteration_count = match . group ( "iteration_count" ) try : iteration_count = int ( iteration_count ) except ValueError : logger . debug ( "Bad iteration_count: {0!r}" . format ( iteration_count ) ) return Failure ( "bad-challenge" ) return self . _make_response ( nonce , salt , iteration_count )
5801	def extract_from_system ( cert_callback = None , callback_only_on_failure = False ) : all_purposes = '2.5.29.37.0' ca_path = system_path ( ) output = [ ] with open ( ca_path , 'rb' ) as f : for armor_type , _ , cert_bytes in unarmor ( f . read ( ) , multiple = True ) : if armor_type == 'CERTIFICATE' : if cert_callback : cert_callback ( Certificate . load ( cert_bytes ) , None ) output . append ( ( cert_bytes , set ( ) , set ( ) ) ) elif armor_type == 'TRUSTED CERTIFICATE' : cert , aux = TrustedCertificate . load ( cert_bytes ) reject_all = False trust_oids = set ( ) reject_oids = set ( ) for purpose in aux [ 'trust' ] : if purpose . dotted == all_purposes : trust_oids = set ( [ purpose . dotted ] ) break trust_oids . add ( purpose . dotted ) for purpose in aux [ 'reject' ] : if purpose . dotted == all_purposes : reject_all = True break reject_oids . add ( purpose . dotted ) if reject_all : if cert_callback : cert_callback ( cert , 'explicitly distrusted' ) continue if cert_callback and not callback_only_on_failure : cert_callback ( cert , None ) output . append ( ( cert . dump ( ) , trust_oids , reject_oids ) ) return output
4345	def stats ( self , input_filepath ) : effect_args = [ 'channels' , '1' , 'stats' ] _ , _ , stats_output = self . build ( input_filepath , None , extra_args = effect_args , return_output = True ) stats_dict = { } lines = stats_output . split ( '\n' ) for line in lines : split_line = line . split ( ) if len ( split_line ) == 0 : continue value = split_line [ - 1 ] key = ' ' . join ( split_line [ : - 1 ] ) stats_dict [ key ] = value return stats_dict
4745	def dev_get_chunk ( dev_name , state , pugrp = None , punit = None ) : rprt = dev_get_rprt ( dev_name , pugrp , punit ) if not rprt : return None return next ( ( d for d in rprt if d [ "cs" ] == state ) , None )
12270	def read_file ( self , filename ) : try : fh = open ( filename , 'rb' ) table_set = any_tableset ( fh ) except : table_set = None return table_set
4682	def getKeyType ( self , account , pub ) : for authority in [ "owner" , "active" ] : for key in account [ authority ] [ "key_auths" ] : if str ( pub ) == key [ 0 ] : return authority if str ( pub ) == account [ "options" ] [ "memo_key" ] : return "memo" return None
12466	def run_cmd ( cmd , echo = False , fail_silently = False , ** kwargs ) : r out , err = None , None if echo : cmd_str = cmd if isinstance ( cmd , string_types ) else ' ' . join ( cmd ) kwargs [ 'stdout' ] , kwargs [ 'stderr' ] = sys . stdout , sys . stderr print_message ( '$ {0}' . format ( cmd_str ) ) else : out , err = get_temp_streams ( ) kwargs [ 'stdout' ] , kwargs [ 'stderr' ] = out , err try : retcode = subprocess . call ( cmd , ** kwargs ) except subprocess . CalledProcessError as err : if fail_silently : return False print_error ( str ( err ) if IS_PY3 else unicode ( err ) ) finally : if out : out . close ( ) if err : err . close ( ) if retcode and echo and not fail_silently : print_error ( 'Command {0!r} returned non-zero exit status {1}' . format ( cmd_str , retcode ) ) return retcode
2513	def get_file_name ( self , f_term ) : for _ , _ , name in self . graph . triples ( ( f_term , self . spdx_namespace [ 'fileName' ] , None ) ) : return name return
5559	def _unflatten_tree ( flat ) : tree = { } for key , value in flat . items ( ) : path = key . split ( "/" ) if len ( path ) == 1 : tree [ key ] = value else : if not path [ 0 ] in tree : tree [ path [ 0 ] ] = _unflatten_tree ( { "/" . join ( path [ 1 : ] ) : value } ) else : branch = _unflatten_tree ( { "/" . join ( path [ 1 : ] ) : value } ) if not path [ 1 ] in tree [ path [ 0 ] ] : tree [ path [ 0 ] ] [ path [ 1 ] ] = branch [ path [ 1 ] ] else : tree [ path [ 0 ] ] [ path [ 1 ] ] . update ( branch [ path [ 1 ] ] ) return tree
280	def plot_annual_returns ( returns , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) x_axis_formatter = FuncFormatter ( utils . percentage ) ax . xaxis . set_major_formatter ( FuncFormatter ( x_axis_formatter ) ) ax . tick_params ( axis = 'x' , which = 'major' ) ann_ret_df = pd . DataFrame ( ep . aggregate_returns ( returns , 'yearly' ) ) ax . axvline ( 100 * ann_ret_df . values . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 4 , alpha = 0.7 ) ( 100 * ann_ret_df . sort_index ( ascending = False ) ) . plot ( ax = ax , kind = 'barh' , alpha = 0.70 , ** kwargs ) ax . axvline ( 0.0 , color = 'black' , linestyle = '-' , lw = 3 ) ax . set_ylabel ( 'Year' ) ax . set_xlabel ( 'Returns' ) ax . set_title ( "Annual returns" ) ax . legend ( [ 'Mean' ] , frameon = True , framealpha = 0.5 ) return ax
12918	def delete ( self ) : if len ( self ) == 0 : return 0 mdl = self . getModel ( ) return mdl . deleter . deleteMultiple ( self )
3676	def rdkitmol ( self ) : r if self . __rdkitmol : return self . __rdkitmol else : try : self . __rdkitmol = Chem . MolFromSmiles ( self . smiles ) return self . __rdkitmol except : return None
1898	def _getvalue ( self , expression ) : if not issymbolic ( expression ) : return expression assert isinstance ( expression , Variable ) if isinstance ( expression , Array ) : result = bytearray ( ) for c in expression : expression_str = translate_to_smtlib ( c ) self . _send ( '(get-value (%s))' % expression_str ) response = self . _recv ( ) result . append ( int ( '0x{:s}' . format ( response . split ( expression_str ) [ 1 ] [ 3 : - 2 ] ) , 16 ) ) return bytes ( result ) else : self . _send ( '(get-value (%s))' % expression . name ) ret = self . _recv ( ) assert ret . startswith ( '((' ) and ret . endswith ( '))' ) , ret if isinstance ( expression , Bool ) : return { 'true' : True , 'false' : False } [ ret [ 2 : - 2 ] . split ( ' ' ) [ 1 ] ] elif isinstance ( expression , BitVec ) : pattern , base = self . _get_value_fmt m = pattern . match ( ret ) expr , value = m . group ( 'expr' ) , m . group ( 'value' ) return int ( value , base ) raise NotImplementedError ( "_getvalue only implemented for Bool and BitVec" )
2136	def disassociate_notification_template ( self , workflow , notification_template , status ) : return self . _disassoc ( 'notification_templates_%s' % status , workflow , notification_template )
46	def project ( self , from_shape , to_shape ) : xy_proj = project_coords ( [ ( self . x , self . y ) ] , from_shape , to_shape ) return self . deepcopy ( x = xy_proj [ 0 ] [ 0 ] , y = xy_proj [ 0 ] [ 1 ] )
719	def queryModelIDs ( self ) : jobID = self . getJobID ( ) modelCounterPairs = _clientJobsDB ( ) . modelsGetUpdateCounters ( jobID ) modelIDs = tuple ( x [ 0 ] for x in modelCounterPairs ) return modelIDs
584	def _deleteRecordsFromKNN ( self , recordsToDelete ) : classifier = self . htm_prediction_model . _getAnomalyClassifier ( ) knn = classifier . getSelf ( ) . _knn prototype_idx = classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) idsToDelete = [ r . ROWID for r in recordsToDelete if not r . setByUser and r . ROWID in prototype_idx ] nProtos = knn . _numPatterns knn . removeIds ( idsToDelete ) assert knn . _numPatterns == nProtos - len ( idsToDelete )
11061	def send_message ( self , channel , text , thread = None , reply_broadcast = None ) : if isinstance ( channel , SlackRoomIMBase ) : channel = channel . id self . log . debug ( "Trying to send to %s: %s" , channel , text ) self . sc . rtm_send_message ( channel , text , thread = thread , reply_broadcast = reply_broadcast )
13530	def add_child ( self , ** kwargs ) : data_class = self . graph . data_content_type . model_class ( ) node = Node . objects . create ( graph = self . graph ) data_class . objects . create ( node = node , ** kwargs ) node . parents . add ( self ) self . children . add ( node ) return node
3282	def read ( self , size = 0 ) : res = self . unread self . unread = "" while res == "" or size < 0 or ( size > 0 and len ( res ) < size ) : try : res += compat . to_native ( self . queue . get ( True , 0.1 ) ) except compat . queue . Empty : if self . is_closed : break if size > 0 and len ( res ) > size : self . unread = res [ size : ] res = res [ : size ] return res
6461	def _ends_in_cvc ( self , term ) : return len ( term ) > 2 and ( term [ - 1 ] not in self . _vowels and term [ - 2 ] in self . _vowels and term [ - 3 ] not in self . _vowels and term [ - 1 ] not in tuple ( 'wxY' ) )
2832	def get_platform_pwm ( ** keywords ) : plat = Platform . platform_detect ( ) if plat == Platform . RASPBERRY_PI : import RPi . GPIO return RPi_PWM_Adapter ( RPi . GPIO , ** keywords ) elif plat == Platform . BEAGLEBONE_BLACK : import Adafruit_BBIO . PWM return BBIO_PWM_Adapter ( Adafruit_BBIO . PWM , ** keywords ) elif plat == Platform . UNKNOWN : raise RuntimeError ( 'Could not determine platform.' )
13906	def apply_defaults ( self , commands ) : for command in commands : if 'action' in command and "()" in command [ 'action' ] : command [ 'action' ] = eval ( "self.{}" . format ( command [ 'action' ] ) ) if command [ 'keys' ] [ 0 ] . startswith ( '-' ) : if 'required' not in command : command [ 'required' ] = False
6292	def add_data_dir ( self , directory ) : dirs = list ( self . DATA_DIRS ) dirs . append ( directory ) self . DATA_DIRS = dirs
8803	def notify ( context , event_type , ipaddress , send_usage = False , * args , ** kwargs ) : if ( event_type == IP_ADD and not CONF . QUARK . notify_ip_add ) or ( event_type == IP_DEL and not CONF . QUARK . notify_ip_delete ) or ( event_type == IP_ASSOC and not CONF . QUARK . notify_flip_associate ) or ( event_type == IP_DISASSOC and not CONF . QUARK . notify_flip_disassociate ) or ( event_type == IP_EXISTS and not CONF . QUARK . notify_ip_exists ) : LOG . debug ( 'IP_BILL: notification {} is disabled by config' . format ( event_type ) ) return if 'rollback' in kwargs and kwargs [ 'rollback' ] : LOG . debug ( 'IP_BILL: not sending notification because we are in undo' ) return ts = ipaddress . allocated_at if event_type == IP_ADD else _now ( ) payload = build_payload ( ipaddress , event_type , event_time = ts ) do_notify ( context , event_type , payload ) if send_usage : if ipaddress . allocated_at is not None and ipaddress . allocated_at >= _midnight_today ( ) : start_time = ipaddress . allocated_at else : start_time = _midnight_today ( ) payload = build_payload ( ipaddress , IP_EXISTS , start_time = start_time , end_time = ts ) do_notify ( context , IP_EXISTS , payload )
8923	def baseurl ( url ) : parsed_url = urlparse . urlparse ( url ) if not parsed_url . netloc or parsed_url . scheme not in ( "http" , "https" ) : raise ValueError ( 'bad url' ) service_url = "%s://%s%s" % ( parsed_url . scheme , parsed_url . netloc , parsed_url . path . strip ( ) ) return service_url
342	def create_distributed_session ( task_spec = None , checkpoint_dir = None , scaffold = None , hooks = None , chief_only_hooks = None , save_checkpoint_secs = 600 , save_summaries_steps = object ( ) , save_summaries_secs = object ( ) , config = None , stop_grace_period_secs = 120 , log_step_count_steps = 100 ) : target = task_spec . target ( ) if task_spec is not None else None is_chief = task_spec . is_master ( ) if task_spec is not None else True return tf . train . MonitoredTrainingSession ( master = target , is_chief = is_chief , checkpoint_dir = checkpoint_dir , scaffold = scaffold , save_checkpoint_secs = save_checkpoint_secs , save_summaries_steps = save_summaries_steps , save_summaries_secs = save_summaries_secs , log_step_count_steps = log_step_count_steps , stop_grace_period_secs = stop_grace_period_secs , config = config , hooks = hooks , chief_only_hooks = chief_only_hooks )
4120	def onesided_2_twosided ( data ) : psd = np . concatenate ( ( data [ 0 : - 1 ] , cshift ( data [ - 1 : 0 : - 1 ] , - 1 ) ) ) / 2. psd [ 0 ] *= 2. psd [ - 1 ] *= 2. return psd
12869	async def manage ( self ) : cm = _ContextManager ( self . database ) if isinstance ( self . database . obj , AIODatabase ) : cm . connection = await self . database . async_connect ( ) else : cm . connection = self . database . connect ( ) return cm
2298	def predict_proba ( self , x , y = None , ** kwargs ) : if self . clf is None : raise ValueError ( "Model has to be trained before making predictions." ) if x is pandas . Series : input_ = self . featurize_row ( x . iloc [ 0 ] , x . iloc [ 1 ] ) . reshape ( ( 1 , - 1 ) ) elif x is pandas . DataFrame : input_ = np . array ( [ self . featurize_row ( x . iloc [ 0 ] , x . iloc [ 1 ] ) for row in x ] ) elif y is not None : input_ = self . featurize_row ( x , y ) . reshape ( ( 1 , - 1 ) ) else : raise TypeError ( "DataType not understood." ) return self . clf . predict ( input_ )
937	def save ( self , saveModelDir ) : logger = self . _getLogger ( ) logger . debug ( "(%s) Creating local checkpoint in %r..." , self , saveModelDir ) modelPickleFilePath = self . _getModelPickleFilePath ( saveModelDir ) if os . path . exists ( saveModelDir ) : if not os . path . isdir ( saveModelDir ) : raise Exception ( ( "Existing filesystem entry <%s> is not a model" " checkpoint -- refusing to delete (not a directory)" ) % saveModelDir ) if not os . path . isfile ( modelPickleFilePath ) : raise Exception ( ( "Existing filesystem entry <%s> is not a model" " checkpoint -- refusing to delete" " (%s missing or not a file)" ) % ( saveModelDir , modelPickleFilePath ) ) shutil . rmtree ( saveModelDir ) self . __makeDirectoryFromAbsolutePath ( saveModelDir ) with open ( modelPickleFilePath , 'wb' ) as modelPickleFile : logger . debug ( "(%s) Pickling Model instance..." , self ) pickle . dump ( self , modelPickleFile , protocol = pickle . HIGHEST_PROTOCOL ) logger . debug ( "(%s) Finished pickling Model instance" , self ) self . _serializeExtraData ( extraDataDir = self . _getModelExtraDataDir ( saveModelDir ) ) logger . debug ( "(%s) Finished creating local checkpoint" , self ) return
9538	def search_pattern ( regex ) : prog = re . compile ( regex ) def checker ( v ) : result = prog . search ( v ) if result is None : raise ValueError ( v ) return checker
8880	def predict_proba ( self , X ) : check_is_fitted ( self , [ 'tree' ] ) X = check_array ( X ) return self . tree . query ( X ) [ 0 ] . flatten ( )
2139	def associate ( self , group , parent , ** kwargs ) : parent_id = self . lookup_with_inventory ( parent , kwargs . get ( 'inventory' , None ) ) [ 'id' ] group_id = self . lookup_with_inventory ( group , kwargs . get ( 'inventory' , None ) ) [ 'id' ] return self . _assoc ( 'children' , parent_id , group_id )
11590	def _rc_smove ( self , src , dst , value ) : if self . type ( src ) != b ( "set" ) : return self . smove ( src + "{" + src + "}" , dst , value ) if self . type ( dst ) != b ( "set" ) : return self . smove ( dst + "{" + dst + "}" , src , value ) if self . srem ( src , value ) : return 1 if self . sadd ( dst , value ) else 0 return 0
12986	def toBytes ( self , value ) : if type ( value ) == bytes : return value return value . encode ( self . getEncoding ( ) )
12763	def load_attachments ( self , source , skeleton ) : self . targets = { } self . offsets = { } filename = source if isinstance ( source , str ) : source = open ( source ) else : filename = '(file-{})' . format ( id ( source ) ) for i , line in enumerate ( source ) : tokens = line . split ( '#' ) [ 0 ] . strip ( ) . split ( ) if not tokens : continue label = tokens . pop ( 0 ) if label not in self . channels : logging . info ( '%s:%d: unknown marker %s' , filename , i , label ) continue if not tokens : continue name = tokens . pop ( 0 ) bodies = [ b for b in skeleton . bodies if b . name == name ] if len ( bodies ) != 1 : logging . info ( '%s:%d: %d skeleton bodies match %s' , filename , i , len ( bodies ) , name ) continue b = self . targets [ label ] = bodies [ 0 ] o = self . offsets [ label ] = np . array ( list ( map ( float , tokens ) ) ) * b . dimensions / 2 logging . info ( '%s < , label , b . name , o )
12519	def mask ( self , image ) : if image is None : self . _mask = None try : mask = load_mask ( image ) except Exception as exc : raise Exception ( 'Could not load mask image {}.' . format ( image ) ) from exc else : self . _mask = mask
10038	def pick_coda_from_letter ( letter ) : try : __ , __ , coda = split_phonemes ( letter , onset = False , nucleus = False , coda = True ) except ValueError : return None else : return coda
4087	def asyncClose ( fn ) : @ functools . wraps ( fn ) def wrapper ( * args , ** kwargs ) : f = asyncio . ensure_future ( fn ( * args , ** kwargs ) ) while not f . done ( ) : QApplication . instance ( ) . processEvents ( ) return wrapper
1060	def cmp_to_key ( mycmp ) : class K ( object ) : __slots__ = [ 'obj' ] def __init__ ( self , obj , * args ) : self . obj = obj def __lt__ ( self , other ) : return mycmp ( self . obj , other . obj ) < 0 def __gt__ ( self , other ) : return mycmp ( self . obj , other . obj ) > 0 def __eq__ ( self , other ) : return mycmp ( self . obj , other . obj ) == 0 def __le__ ( self , other ) : return mycmp ( self . obj , other . obj ) <= 0 def __ge__ ( self , other ) : return mycmp ( self . obj , other . obj ) >= 0 def __ne__ ( self , other ) : return mycmp ( self . obj , other . obj ) != 0 def __hash__ ( self ) : raise TypeError ( 'hash not implemented' ) return K
647	def generateSimpleSequences ( nCoinc = 10 , seqLength = [ 5 , 6 , 7 ] , nSeq = 100 ) : coincList = range ( nCoinc ) seqList = [ ] for i in xrange ( nSeq ) : if max ( seqLength ) <= nCoinc : seqList . append ( random . sample ( coincList , random . choice ( seqLength ) ) ) else : len = random . choice ( seqLength ) seq = [ ] for x in xrange ( len ) : seq . append ( random . choice ( coincList ) ) seqList . append ( seq ) return seqList
1544	def get_logical_plan ( cluster , env , topology , role ) : instance = tornado . ioloop . IOLoop . instance ( ) try : return instance . run_sync ( lambda : API . get_logical_plan ( cluster , env , topology , role ) ) except Exception : Log . debug ( traceback . format_exc ( ) ) raise
4292	def validate_project ( project_name ) : if '-' in project_name : return None if keyword . iskeyword ( project_name ) : return None if project_name in dir ( __builtins__ ) : return None try : __import__ ( project_name ) return None except ImportError : return project_name
8340	def toEncoding ( self , s , encoding = None ) : if isinstance ( s , unicode ) : if encoding : s = s . encode ( encoding ) elif isinstance ( s , str ) : if encoding : s = s . encode ( encoding ) else : s = unicode ( s ) else : if encoding : s = self . toEncoding ( str ( s ) , encoding ) else : s = unicode ( s ) return s
9493	def _simulate_stack ( code : list ) -> int : max_stack = 0 curr_stack = 0 def _check_stack ( ins ) : if curr_stack < 0 : raise CompileError ( "Stack turned negative on instruction: {}" . format ( ins ) ) if curr_stack > max_stack : return curr_stack for instruction in code : assert isinstance ( instruction , dis . Instruction ) if instruction . arg is not None : try : effect = dis . stack_effect ( instruction . opcode , instruction . arg ) except ValueError as e : raise CompileError ( "Invalid opcode `{}` when compiling" . format ( instruction . opcode ) ) from e else : try : effect = dis . stack_effect ( instruction . opcode ) except ValueError as e : raise CompileError ( "Invalid opcode `{}` when compiling" . format ( instruction . opcode ) ) from e curr_stack += effect _should_new_stack = _check_stack ( instruction ) if _should_new_stack : max_stack = _should_new_stack return max_stack
1730	def match ( self , string , pos ) : return self . pat . match ( string , int ( pos ) )
3183	def update ( self , store_id , data ) : self . store_id = store_id return self . _mc_client . _patch ( url = self . _build_path ( store_id ) , data = data )
607	def findRequirements ( ) : requirementsPath = os . path . join ( REPO_DIR , "requirements.txt" ) requirements = parse_file ( requirementsPath ) if nupicBindingsPrereleaseInstalled ( ) : requirements = [ req for req in requirements if "nupic.bindings" not in req ] return requirements
7846	def add_item ( self , jid , node = None , name = None , action = None ) : return DiscoItem ( self , jid , node , name , action )
5485	def _eval_arg_type ( arg_type , T = Any , arg = None , sig = None ) : try : T = eval ( arg_type ) except Exception as e : raise ValueError ( 'The type of {0} could not be evaluated in {1} for {2}: {3}' . format ( arg_type , arg , sig , text_type ( e ) ) ) else : if type ( T ) not in ( type , Type ) : raise TypeError ( '{0} is not a valid type in {1} for {2}' . format ( repr ( T ) , arg , sig ) ) return T
11101	def select_by_ctime ( self , min_time = 0 , max_time = ts_2100 , recursive = True ) : def filters ( p ) : return min_time <= p . ctime <= max_time return self . select_file ( filters , recursive )
6153	def fir_remez_bsf ( f_pass1 , f_stop1 , f_stop2 , f_pass2 , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : n , ff , aa , wts = bandstop_order ( f_pass1 , f_stop1 , f_stop2 , f_pass2 , d_pass , d_stop , fsamp = fs ) if np . mod ( n , 2 ) != 0 : n += 1 N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 , maxiter = 25 , grid_density = 16 ) print ( 'N_bump must be odd to maintain odd filter length' ) print ( 'Remez filter taps = %d.' % N_taps ) return b
75	def DirectedEdgeDetect ( alpha = 0 , direction = ( 0.0 , 1.0 ) , name = None , deterministic = False , random_state = None ) : alpha_param = iap . handle_continuous_param ( alpha , "alpha" , value_range = ( 0 , 1.0 ) , tuple_to_uniform = True , list_to_choice = True ) direction_param = iap . handle_continuous_param ( direction , "direction" , value_range = None , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( _image , nb_channels , random_state_func ) : alpha_sample = alpha_param . draw_sample ( random_state = random_state_func ) ia . do_assert ( 0 <= alpha_sample <= 1.0 ) direction_sample = direction_param . draw_sample ( random_state = random_state_func ) deg = int ( direction_sample * 360 ) % 360 rad = np . deg2rad ( deg ) x = np . cos ( rad - 0.5 * np . pi ) y = np . sin ( rad - 0.5 * np . pi ) direction_vector = np . array ( [ x , y ] ) matrix_effect = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 0 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) for x in [ - 1 , 0 , 1 ] : for y in [ - 1 , 0 , 1 ] : if ( x , y ) != ( 0 , 0 ) : cell_vector = np . array ( [ x , y ] ) distance_deg = np . rad2deg ( ia . angle_between_vectors ( cell_vector , direction_vector ) ) distance = distance_deg / 180 similarity = ( 1 - distance ) ** 4 matrix_effect [ y + 1 , x + 1 ] = similarity matrix_effect = matrix_effect / np . sum ( matrix_effect ) matrix_effect = matrix_effect * ( - 1 ) matrix_effect [ 1 , 1 ] = 1 matrix_nochange = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) matrix = ( 1 - alpha_sample ) * matrix_nochange + alpha_sample * matrix_effect return [ matrix ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
12194	def _validate_first_message ( cls , msg ) : data = cls . _unpack_message ( msg ) logger . debug ( data ) if data != cls . RTM_HANDSHAKE : raise SlackApiError ( 'Unexpected response: {!r}' . format ( data ) ) logger . info ( 'Joined real-time messaging.' )
11747	def routes_simple ( self ) : routes = [ ] for bundle in self . _registered_bundles : bundle_path = bundle [ 'path' ] for blueprint in bundle [ 'blueprints' ] : bp_path = blueprint [ 'path' ] for child in blueprint [ 'routes' ] : routes . append ( ( child [ 'endpoint' ] , bundle_path + bp_path + child [ 'path' ] , child [ 'methods' ] ) ) return routes
11167	def keys ( self ) : return self . options . keys ( ) + [ p . name for p in self . positional_args ]
6598	def end ( self ) : results = self . communicationChannel . receive ( ) if self . nruns != len ( results ) : import logging logger = logging . getLogger ( __name__ ) logger . warning ( 'too few results received: {} results received, {} expected' . format ( len ( results ) , self . nruns ) ) return results
5518	def clone ( self ) : return StreamThrottle ( read = self . read . clone ( ) , write = self . write . clone ( ) )
5000	def require_at_least_one_query_parameter ( * query_parameter_names ) : def outer_wrapper ( view ) : @ wraps ( view ) def wrapper ( request , * args , ** kwargs ) : requirement_satisfied = False for query_parameter_name in query_parameter_names : query_parameter_values = request . query_params . getlist ( query_parameter_name ) kwargs [ query_parameter_name ] = query_parameter_values if query_parameter_values : requirement_satisfied = True if not requirement_satisfied : raise ValidationError ( detail = 'You must provide at least one of the following query parameters: {params}.' . format ( params = ', ' . join ( query_parameter_names ) ) ) return view ( request , * args , ** kwargs ) return wrapper return outer_wrapper
591	def compute ( self , inputs , outputs ) : if False and self . learningMode and self . _iterations > 0 and self . _iterations <= 10 : import hotshot if self . _iterations == 10 : print "\n Collecting and sorting internal node profiling stats generated by hotshot..." stats = hotshot . stats . load ( "hotshot.stats" ) stats . strip_dirs ( ) stats . sort_stats ( 'time' , 'calls' ) stats . print_stats ( ) if self . _profileObj is None : print "\n Preparing to capture profile using hotshot..." if os . path . exists ( 'hotshot.stats' ) : os . remove ( 'hotshot.stats' ) self . _profileObj = hotshot . Profile ( "hotshot.stats" , 1 , 1 ) self . _profileObj . runcall ( self . _compute , * [ inputs , outputs ] ) else : self . _compute ( inputs , outputs )
973	def _slowIsSegmentActive ( self , seg , timeStep ) : numSyn = seg . size ( ) numActiveSyns = 0 for synIdx in xrange ( numSyn ) : if seg . getPermanence ( synIdx ) < self . connectedPerm : continue sc , si = self . getColCellIdx ( seg . getSrcCellIdx ( synIdx ) ) if self . infActiveState [ timeStep ] [ sc , si ] : numActiveSyns += 1 if numActiveSyns >= self . activationThreshold : return True return numActiveSyns >= self . activationThreshold
2200	def ensure_app_data_dir ( appname , * args ) : from ubelt import util_path dpath = get_app_data_dir ( appname , * args ) util_path . ensuredir ( dpath ) return dpath
6251	def create_transformation ( self , rotation = None , translation = None ) : mat = None if rotation is not None : mat = Matrix44 . from_eulers ( Vector3 ( rotation ) ) if translation is not None : trans = matrix44 . create_from_translation ( Vector3 ( translation ) ) if mat is None : mat = trans else : mat = matrix44 . multiply ( mat , trans ) return mat
12921	def refetch ( self ) : if len ( self ) == 0 : return IRQueryableList ( ) mdl = self . getModel ( ) pks = [ item . _id for item in self if item . _id ] return mdl . objects . getMultiple ( pks )
4143	def _numpy_solver ( A , B ) : x = numpy . linalg . solve ( A , B ) return x
178	def subdivide ( self , points_per_edge ) : if len ( self . coords ) <= 1 or points_per_edge < 1 : return self . deepcopy ( ) coords = interpolate_points ( self . coords , nb_steps = points_per_edge , closed = False ) return self . deepcopy ( coords = coords )
6031	def unmasked_blurred_image_from_psf_and_unmasked_image ( self , psf , unmasked_image_1d ) : blurred_image_1d = self . regular . convolve_array_1d_with_psf ( padded_array_1d = unmasked_image_1d , psf = psf ) return self . regular . scaled_array_2d_from_array_1d ( array_1d = blurred_image_1d )
7494	def count_snps ( mat ) : snps = np . zeros ( 4 , dtype = np . uint32 ) snps [ 0 ] = np . uint32 ( mat [ 0 , 5 ] + mat [ 0 , 10 ] + mat [ 0 , 15 ] + mat [ 5 , 0 ] + mat [ 5 , 10 ] + mat [ 5 , 15 ] + mat [ 10 , 0 ] + mat [ 10 , 5 ] + mat [ 10 , 15 ] + mat [ 15 , 0 ] + mat [ 15 , 5 ] + mat [ 15 , 10 ] ) for i in range ( 16 ) : if i % 5 : snps [ 1 ] += mat [ i , i ] snps [ 2 ] = mat [ 1 , 4 ] + mat [ 2 , 8 ] + mat [ 3 , 12 ] + mat [ 4 , 1 ] + mat [ 6 , 9 ] + mat [ 7 , 13 ] + mat [ 8 , 2 ] + mat [ 9 , 6 ] + mat [ 11 , 14 ] + mat [ 12 , 3 ] + mat [ 13 , 7 ] + mat [ 14 , 11 ] snps [ 3 ] = ( mat . sum ( ) - np . diag ( mat ) . sum ( ) ) - snps [ 2 ] return snps
3702	def Tm_depression_eutectic ( Tm , Hm , x = None , M = None , MW = None ) : r if x : dTm = R * Tm ** 2 * x / Hm elif M and MW : MW = MW / 1000. dTm = R * Tm ** 2 * MW * M / Hm else : raise Exception ( 'Either molality or mole fraction of the solute must be specified; MW of the solvent is required also if molality is provided' ) return dTm
750	def _getClassifierRegion ( self ) : if ( self . _netInfo . net is not None and "Classifier" in self . _netInfo . net . regions ) : return self . _netInfo . net . regions [ "Classifier" ] else : return None
10333	def group_nodes_by_annotation_filtered ( graph : BELGraph , node_predicates : NodePredicates = None , annotation : str = 'Subgraph' , ) -> Mapping [ str , Set [ BaseEntity ] ] : node_filter = concatenate_node_predicates ( node_predicates ) return { key : { node for node in nodes if node_filter ( graph , node ) } for key , nodes in group_nodes_by_annotation ( graph , annotation ) . items ( ) }
576	def tick ( self ) : for act in self . __activities : if not act . iteratorHolder [ 0 ] : continue try : next ( act . iteratorHolder [ 0 ] ) except StopIteration : act . cb ( ) if act . repeating : act . iteratorHolder [ 0 ] = iter ( xrange ( act . period ) ) else : act . iteratorHolder [ 0 ] = None return True
10115	def parse ( grid_str , mode = MODE_ZINC , charset = 'utf-8' ) : if isinstance ( grid_str , six . binary_type ) : grid_str = grid_str . decode ( encoding = charset ) _parse = functools . partial ( parse_grid , mode = mode , charset = charset ) if mode == MODE_JSON : if isinstance ( grid_str , six . string_types ) : grid_data = json . loads ( grid_str ) else : grid_data = grid_str if isinstance ( grid_data , dict ) : return _parse ( grid_data ) else : return list ( map ( _parse , grid_data ) ) else : return list ( map ( _parse , GRID_SEP . split ( grid_str . rstrip ( ) ) ) )
12889	def handle_set ( self , item , value ) : doc = yield from self . call ( 'SET/{}' . format ( item ) , dict ( value = value ) ) if doc is None : return None return doc . status == 'FS_OK'
1376	def get_java_path ( ) : java_home = os . environ . get ( "JAVA_HOME" ) return os . path . join ( java_home , BIN_DIR , "java" )
9770	def update ( ctx , name , description , tags ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the job.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . job . update_job ( user , project_name , _job , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Job updated." ) get_job_details ( response )
824	def mostLikely ( self , pred ) : if len ( pred ) == 1 : return pred . keys ( ) [ 0 ] mostLikelyOutcome = None maxProbability = 0 for prediction , probability in pred . items ( ) : if probability > maxProbability : mostLikelyOutcome = prediction maxProbability = probability return mostLikelyOutcome
821	def compute ( slidingWindow , total , newVal , windowSize ) : if len ( slidingWindow ) == windowSize : total -= slidingWindow . pop ( 0 ) slidingWindow . append ( newVal ) total += newVal return float ( total ) / len ( slidingWindow ) , slidingWindow , total
11474	def login ( email = None , password = None , api_key = None , application = 'Default' , url = None , verify_ssl_certificate = True ) : try : input_ = raw_input except NameError : input_ = input if url is None : url = input_ ( 'Server URL: ' ) url = url . rstrip ( '/' ) if session . communicator is None : session . communicator = Communicator ( url ) else : session . communicator . url = url session . communicator . verify_ssl_certificate = verify_ssl_certificate if email is None : email = input_ ( 'Email: ' ) session . email = email if api_key is None : if password is None : password = getpass . getpass ( ) session . api_key = session . communicator . get_default_api_key ( session . email , password ) session . application = 'Default' else : session . api_key = api_key session . application = application return renew_token ( )
7762	def make_error_response ( self , cond ) : if self . stanza_type == "error" : raise ValueError ( "Errors may not be generated in response" " to errors" ) msg = Message ( stanza_type = "error" , from_jid = self . to_jid , to_jid = self . from_jid , stanza_id = self . stanza_id , error_cond = cond , subject = self . _subject , body = self . _body , thread = self . _thread ) if self . _payload is None : self . decode_payload ( ) for payload in self . _payload : msg . add_payload ( payload . copy ( ) ) return msg
1661	def ExpectingFunctionArgs ( clean_lines , linenum ) : line = clean_lines . elided [ linenum ] return ( Match ( r'^\s*MOCK_(CONST_)?METHOD\d+(_T)?\(' , line ) or ( linenum >= 2 and ( Match ( r'^\s*MOCK_(?:CONST_)?METHOD\d+(?:_T)?\((?:\S+,)?\s*$' , clean_lines . elided [ linenum - 1 ] ) or Match ( r'^\s*MOCK_(?:CONST_)?METHOD\d+(?:_T)?\(\s*$' , clean_lines . elided [ linenum - 2 ] ) or Search ( r'\bstd::m?function\s*\<\s*$' , clean_lines . elided [ linenum - 1 ] ) ) ) )
8566	def get_loadbalancer_members ( self , datacenter_id , loadbalancer_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers/%s/balancednics?depth=%s' % ( datacenter_id , loadbalancer_id , str ( depth ) ) ) return response
12041	def matrixToHTML ( data , names = None , units = None , bookName = None , sheetName = None , xCol = None ) : if not names : names = [ "" ] * len ( data [ 0 ] ) if data . dtype . names : names = list ( data . dtype . names ) if not units : units = [ "" ] * len ( data [ 0 ] ) for i in range ( len ( units ) ) : if names [ i ] in UNITS . keys ( ) : units [ i ] = UNITS [ names [ i ] ] if 'recarray' in str ( type ( data ) ) : data = data . view ( float ) . reshape ( data . shape + ( - 1 , ) ) if xCol and xCol in names : xCol = names . index ( xCol ) names . insert ( 0 , names [ xCol ] ) units . insert ( 0 , units [ xCol ] ) data = np . insert ( data , 0 , data [ : , xCol ] , 1 ) htmlFname = tempfile . gettempdir ( ) + "/swhlab/WKS-%s.%s.html" % ( bookName , sheetName ) html = html += "<h1>FauxRigin</h1>" if bookName or sheetName : html += '<code><b>%s / %s</b></code><br><br>' % ( bookName , sheetName ) html += "<table>" colNames = [ '' ] for i in range ( len ( units ) ) : label = "%s (%d)" % ( chr ( i + ord ( 'A' ) ) , i ) colNames . append ( label ) html += htmlListToTR ( colNames , 'labelCol' , 'labelCol' ) html += htmlListToTR ( [ 'Long Name' ] + list ( names ) , 'name' , td1Class = 'labelRow' ) html += htmlListToTR ( [ 'Units' ] + list ( units ) , 'units' , td1Class = 'labelRow' ) cutOff = False for y in range ( len ( data ) ) : html += htmlListToTR ( [ y + 1 ] + list ( data [ y ] ) , trClass = 'data%d' % ( y % 2 ) , td1Class = 'labelRow' ) if y >= 200 : cutOff = True break html += "</table>" html = html . replace ( ">nan<" , ">--<" ) html = html . replace ( ">None<" , "><" ) if cutOff : html += "<h3>... showing only %d of %d rows ...</h3>" % ( y , len ( data ) ) html += "</body></html>" with open ( htmlFname , 'w' ) as f : f . write ( html ) webbrowser . open ( htmlFname ) return
10516	def verifyscrollbarhorizontal ( self , window_name , object_name ) : try : object_handle = self . _get_object_handle ( window_name , object_name ) if object_handle . AXOrientation == "AXHorizontalOrientation" : return 1 except : pass return 0
7534	def cluster ( data , sample , nthreads , force ) : if "reference" in data . paramsdict [ "assembly_method" ] : derephandle = os . path . join ( data . dirs . edits , sample . name + "-refmap_derep.fastq" ) if not os . stat ( derephandle ) . st_size : uhandle = os . path . join ( data . dirs . clusts , sample . name + ".utemp" ) usort = os . path . join ( data . dirs . clusts , sample . name + ".utemp.sort" ) hhandle = os . path . join ( data . dirs . clusts , sample . name + ".htemp" ) for f in [ uhandle , usort , hhandle ] : open ( f , 'a' ) . close ( ) return else : derephandle = os . path . join ( data . dirs . edits , sample . name + "_derep.fastq" ) uhandle = os . path . join ( data . dirs . clusts , sample . name + ".utemp" ) temphandle = os . path . join ( data . dirs . clusts , sample . name + ".htemp" ) if not os . path . isfile ( derephandle ) : LOGGER . warn ( "Bad derephandle - {}" . format ( derephandle ) ) raise IPyradError ( "Input file for clustering doesn't exist - {}" . format ( derephandle ) ) strand = "plus" cov = 0.75 minsl = 0.5 if data . paramsdict [ "datatype" ] in [ "gbs" , "2brad" ] : strand = "both" cov = 0.5 minsl = 0.5 elif data . paramsdict [ "datatype" ] == 'pairgbs' : strand = "both" cov = 0.75 minsl = 0.75 if data . _hackersonly [ "query_cov" ] : cov = str ( data . _hackersonly [ "query_cov" ] ) assert float ( cov ) <= 1 , "query_cov must be <= 1.0" cmd = [ ipyrad . bins . vsearch , "-cluster_smallmem" , derephandle , "-strand" , strand , "-query_cov" , str ( cov ) , "-id" , str ( data . paramsdict [ "clust_threshold" ] ) , "-minsl" , str ( minsl ) , "-userout" , uhandle , "-userfields" , "query+target+id+gaps+qstrand+qcov" , "-maxaccepts" , "1" , "-maxrejects" , "0" , "-threads" , str ( nthreads ) , "-notmatched" , temphandle , "-fasta_width" , "0" , "-fastq_qmax" , "100" , "-fulldp" , "-usersort" ] LOGGER . debug ( "%s" , cmd ) proc = sps . Popen ( cmd , stderr = sps . STDOUT , stdout = sps . PIPE , close_fds = True ) try : res = proc . communicate ( ) [ 0 ] except KeyboardInterrupt : proc . kill ( ) raise KeyboardInterrupt if proc . returncode : LOGGER . error ( "error %s: %s" , cmd , res ) raise IPyradWarningExit ( "cmd {}: {}" . format ( cmd , res ) )
11449	def send ( self , recipient , message ) : if self . _logindata [ 'login_rufnummer' ] is None or self . _logindata [ 'login_passwort' ] is None : err_mess = "YesssSMS: Login data required" raise self . LoginError ( err_mess ) if not recipient : raise self . NoRecipientError ( "YesssSMS: recipient number missing" ) if not isinstance ( recipient , str ) : raise ValueError ( "YesssSMS: str expected as recipient number" ) if not message : raise self . EmptyMessageError ( "YesssSMS: message is empty" ) with self . _login ( requests . Session ( ) ) as sess : sms_data = { 'to_nummer' : recipient , 'nachricht' : message } req = sess . post ( self . _websms_url , data = sms_data ) if not ( req . status_code == 200 or req . status_code == 302 ) : raise self . SMSSendingError ( "YesssSMS: error sending SMS" ) if _UNSUPPORTED_CHARS_STRING in req . text : raise self . UnsupportedCharsError ( "YesssSMS: message contains unsupported character(s)" ) if _SMS_SENDING_SUCCESSFUL_STRING not in req . text : raise self . SMSSendingError ( "YesssSMS: error sending SMS" ) sess . get ( self . _logout_url )
8868	def _unique ( self , seq ) : checked = [ ] for e in seq : present = False for c in checked : if str ( c ) == str ( e ) : present = True break if not present : checked . append ( e ) return checked
1745	def _set_perms ( self , perms ) : assert isinstance ( perms , str ) and len ( perms ) <= 3 and perms . strip ( ) in [ '' , 'r' , 'w' , 'x' , 'rw' , 'r x' , 'rx' , 'rwx' , 'wx' , ] self . _perms = perms
7652	def match_query ( string , query ) : if six . callable ( query ) : return query ( string ) elif ( isinstance ( query , six . string_types ) and isinstance ( string , six . string_types ) ) : return re . match ( query , string ) is not None else : return query == string
10111	def iterrows ( lines_or_file , namedtuples = False , dicts = False , encoding = 'utf-8' , ** kw ) : if namedtuples and dicts : raise ValueError ( 'either namedtuples or dicts can be chosen as output format' ) elif namedtuples : _reader = NamedTupleReader elif dicts : _reader = UnicodeDictReader else : _reader = UnicodeReader with _reader ( lines_or_file , encoding = encoding , ** fix_kw ( kw ) ) as r : for item in r : yield item
3098	def validate_token ( key , token , user_id , action_id = "" , current_time = None ) : if not token : return False try : decoded = base64 . urlsafe_b64decode ( token ) token_time = int ( decoded . split ( DELIMITER ) [ - 1 ] ) except ( TypeError , ValueError , binascii . Error ) : return False if current_time is None : current_time = time . time ( ) if current_time - token_time > DEFAULT_TIMEOUT_SECS : return False expected_token = generate_token ( key , user_id , action_id = action_id , when = token_time ) if len ( token ) != len ( expected_token ) : return False different = 0 for x , y in zip ( bytearray ( token ) , bytearray ( expected_token ) ) : different |= x ^ y return not different
3090	def locked_put ( self , credentials ) : entity = self . _model . get_or_insert ( self . _key_name ) setattr ( entity , self . _property_name , credentials ) entity . put ( ) if self . _cache : self . _cache . set ( self . _key_name , credentials . to_json ( ) )
6618	def _expand_tuple ( path_cfg , alias_dict , overriding_kargs ) : new_path_cfg = path_cfg [ 0 ] new_overriding_kargs = path_cfg [ 1 ] . copy ( ) new_overriding_kargs . update ( overriding_kargs ) return expand_path_cfg ( new_path_cfg , overriding_kargs = new_overriding_kargs , alias_dict = alias_dict )
9520	def make_random_contigs ( contigs , length , outfile , name_by_letters = False , prefix = '' , seed = None , first_number = 1 ) : random . seed ( a = seed ) fout = utils . open_file_write ( outfile ) letters = list ( 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' ) letters_index = 0 for i in range ( contigs ) : if name_by_letters : name = letters [ letters_index ] letters_index += 1 if letters_index == len ( letters ) : letters_index = 0 else : name = str ( i + first_number ) fa = sequences . Fasta ( prefix + name , '' . join ( [ random . choice ( 'ACGT' ) for x in range ( length ) ] ) ) print ( fa , file = fout ) utils . close ( fout )
828	def getFieldDescription ( self , fieldName ) : description = self . getDescription ( ) + [ ( "end" , self . getWidth ( ) ) ] for i in xrange ( len ( description ) ) : ( name , offset ) = description [ i ] if ( name == fieldName ) : break if i >= len ( description ) - 1 : raise RuntimeError ( "Field name %s not found in this encoder" % fieldName ) return ( offset , description [ i + 1 ] [ 1 ] - offset )
5770	def rsa_pkcs1v15_verify ( certificate_or_public_key , signature , data , hash_algorithm ) : if certificate_or_public_key . algorithm != 'rsa' : raise ValueError ( 'The key specified is not an RSA public key' ) return _verify ( certificate_or_public_key , signature , data , hash_algorithm )
3483	def write_sbml_model ( cobra_model , filename , f_replace = F_REPLACE , ** kwargs ) : doc = _model_to_sbml ( cobra_model , f_replace = f_replace , ** kwargs ) if isinstance ( filename , string_types ) : libsbml . writeSBMLToFile ( doc , filename ) elif hasattr ( filename , "write" ) : sbml_str = libsbml . writeSBMLToString ( doc ) filename . write ( sbml_str )
13	def copy_obs_dict ( obs ) : return { k : np . copy ( v ) for k , v in obs . items ( ) }
8761	def delete_subnet ( context , id ) : LOG . info ( "delete_subnet %s for tenant %s" % ( id , context . tenant_id ) ) with context . session . begin ( ) : subnet = db_api . subnet_find ( context , id = id , scope = db_api . ONE ) if not subnet : raise n_exc . SubnetNotFound ( subnet_id = id ) if not context . is_admin : if STRATEGY . is_provider_network ( subnet . network_id ) : if subnet . tenant_id == context . tenant_id : raise n_exc . NotAuthorized ( subnet_id = id ) else : raise n_exc . SubnetNotFound ( subnet_id = id ) _delete_subnet ( context , subnet )
4985	def get ( self , request , enterprise_uuid , program_uuid ) : verify_edx_resources ( ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) context_data = get_global_context ( request , enterprise_customer ) program_details , error_code = self . get_program_details ( request , program_uuid , enterprise_customer ) if error_code : return render ( request , ENTERPRISE_GENERAL_ERROR_PAGE , context = context_data , status = 404 , ) if program_details [ 'certificate_eligible_for_program' ] : return redirect ( LMS_PROGRAMS_DASHBOARD_URL . format ( uuid = program_uuid ) ) course_run_ids = [ ] for course in program_details [ 'courses' ] : for course_run in course [ 'course_runs' ] : course_run_ids . append ( course_run [ 'key' ] ) embargo_url = EmbargoApiClient . redirect_if_blocked ( course_run_ids , request . user , get_ip ( request ) , request . path ) if embargo_url : return redirect ( embargo_url ) return self . get_enterprise_program_enrollment_page ( request , enterprise_customer , program_details )
4017	def get_lib_volume_mounts ( base_lib_name , assembled_specs ) : volumes = [ _get_lib_repo_volume_mount ( assembled_specs [ 'libs' ] [ base_lib_name ] ) ] volumes . append ( get_command_files_volume_mount ( base_lib_name , test = True ) ) for lib_name in assembled_specs [ 'libs' ] [ base_lib_name ] [ 'depends' ] [ 'libs' ] : lib_spec = assembled_specs [ 'libs' ] [ lib_name ] volumes . append ( _get_lib_repo_volume_mount ( lib_spec ) ) return volumes
12101	def _record_info ( self , setup_info = None ) : info_path = os . path . join ( self . root_directory , ( '%s.info' % self . batch_name ) ) if setup_info is None : try : with open ( info_path , 'r' ) as info_file : setup_info = json . load ( info_file ) except : setup_info = { } setup_info . update ( { 'end_time' : tuple ( time . localtime ( ) ) } ) else : setup_info . update ( { 'end_time' : None , 'metadata' : self . metadata } ) with open ( info_path , 'w' ) as info_file : json . dump ( setup_info , info_file , sort_keys = True , indent = 4 )
11911	def get_version ( filename , pattern ) : with open ( filename ) as f : match = re . search ( r"^(\s*%s\s*=\s*')(.+?)(')(?sm)" % pattern , f . read ( ) ) if match : before , version , after = match . groups ( ) return version fail ( 'Could not find {} in {}' . format ( pattern , filename ) )
4302	def create_user ( config_data ) : with chdir ( os . path . abspath ( config_data . project_directory ) ) : env = deepcopy ( dict ( os . environ ) ) env [ str ( 'DJANGO_SETTINGS_MODULE' ) ] = str ( '{0}.settings' . format ( config_data . project_name ) ) env [ str ( 'PYTHONPATH' ) ] = str ( os . pathsep . join ( map ( shlex_quote , sys . path ) ) ) subprocess . check_call ( [ sys . executable , 'create_user.py' ] , env = env , stderr = subprocess . STDOUT ) for ext in [ 'py' , 'pyc' ] : try : os . remove ( 'create_user.{0}' . format ( ext ) ) except OSError : pass
9719	async def take_control ( self , password ) : cmd = "takecontrol %s" % password return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
15	def q_retrace ( R , D , q_i , v , rho_i , nenvs , nsteps , gamma ) : rho_bar = batch_to_seq ( tf . minimum ( 1.0 , rho_i ) , nenvs , nsteps , True ) rs = batch_to_seq ( R , nenvs , nsteps , True ) ds = batch_to_seq ( D , nenvs , nsteps , True ) q_is = batch_to_seq ( q_i , nenvs , nsteps , True ) vs = batch_to_seq ( v , nenvs , nsteps + 1 , True ) v_final = vs [ - 1 ] qret = v_final qrets = [ ] for i in range ( nsteps - 1 , - 1 , - 1 ) : check_shape ( [ qret , ds [ i ] , rs [ i ] , rho_bar [ i ] , q_is [ i ] , vs [ i ] ] , [ [ nenvs ] ] * 6 ) qret = rs [ i ] + gamma * qret * ( 1.0 - ds [ i ] ) qrets . append ( qret ) qret = ( rho_bar [ i ] * ( qret - q_is [ i ] ) ) + vs [ i ] qrets = qrets [ : : - 1 ] qret = seq_to_batch ( qrets , flat = True ) return qret
9058	def gradient ( self ) : r self . _update_approx ( ) g = self . _ep . lml_derivatives ( self . _X ) ed = exp ( - self . logitdelta ) es = exp ( self . logscale ) grad = dict ( ) grad [ "logitdelta" ] = g [ "delta" ] * ( ed / ( 1 + ed ) ) / ( 1 + ed ) grad [ "logscale" ] = g [ "scale" ] * es grad [ "beta" ] = g [ "mean" ] return grad
4056	def _json_processor ( self , retrieved ) : json_kwargs = { } if self . preserve_json_order : json_kwargs [ "object_pairs_hook" ] = OrderedDict try : items = [ json . loads ( e [ "content" ] [ 0 ] [ "value" ] , ** json_kwargs ) for e in retrieved . entries ] except KeyError : return self . _tags_data ( retrieved ) return items
1858	def BTC ( cpu , dest , src ) : if dest . type == 'register' : value = dest . read ( ) pos = src . read ( ) % dest . size cpu . CF = value & ( 1 << pos ) == 1 << pos dest . write ( value ^ ( 1 << pos ) ) elif dest . type == 'memory' : addr , pos = cpu . _getMemoryBit ( dest , src ) base , size , ty = cpu . get_descriptor ( cpu . DS ) addr += base value = cpu . read_int ( addr , 8 ) cpu . CF = value & ( 1 << pos ) == 1 << pos value = value ^ ( 1 << pos ) cpu . write_int ( addr , value , 8 ) else : raise NotImplementedError ( f"Unknown operand for BTC: {dest.type}" )
1429	def convert_args_dict_to_list ( dict_extra_args ) : list_extra_args = [ ] if 'component_parallelism' in dict_extra_args : list_extra_args += [ "--component_parallelism" , ',' . join ( dict_extra_args [ 'component_parallelism' ] ) ] if 'runtime_config' in dict_extra_args : list_extra_args += [ "--runtime_config" , ',' . join ( dict_extra_args [ 'runtime_config' ] ) ] if 'container_number' in dict_extra_args : list_extra_args += [ "--container_number" , ',' . join ( dict_extra_args [ 'container_number' ] ) ] if 'dry_run' in dict_extra_args and dict_extra_args [ 'dry_run' ] : list_extra_args += [ '--dry_run' ] if 'dry_run_format' in dict_extra_args : list_extra_args += [ '--dry_run_format' , dict_extra_args [ 'dry_run_format' ] ] return list_extra_args
6388	def _sb_ends_in_short_syllable ( self , term ) : if not term : return False if len ( term ) == 2 : if term [ - 2 ] in self . _vowels and term [ - 1 ] not in self . _vowels : return True elif len ( term ) >= 3 : if ( term [ - 3 ] not in self . _vowels and term [ - 2 ] in self . _vowels and term [ - 1 ] in self . _codanonvowels ) : return True return False
8554	def reserve_ipblock ( self , ipblock ) : properties = { "name" : ipblock . name } if ipblock . location : properties [ 'location' ] = ipblock . location if ipblock . size : properties [ 'size' ] = str ( ipblock . size ) raw = { "properties" : properties , } response = self . _perform_request ( url = '/ipblocks' , method = 'POST' , data = json . dumps ( raw ) ) return response
8746	def get_floatingips_count ( context , filters = None ) : LOG . info ( 'get_floatingips_count for tenant %s filters %s' % ( context . tenant_id , filters ) ) if filters is None : filters = { } filters [ '_deallocated' ] = False filters [ 'address_type' ] = ip_types . FLOATING count = db_api . ip_address_count_all ( context , filters ) LOG . info ( 'Found %s floating ips for tenant %s' % ( count , context . tenant_id ) ) return count
12745	def pid ( kp = 0. , ki = 0. , kd = 0. , smooth = 0.1 ) : r state = dict ( p = 0 , i = 0 , d = 0 ) def control ( error , dt = 1 ) : state [ 'd' ] = smooth * state [ 'd' ] + ( 1 - smooth ) * ( error - state [ 'p' ] ) / dt state [ 'i' ] += error * dt state [ 'p' ] = error return kp * state [ 'p' ] + ki * state [ 'i' ] + kd * state [ 'd' ] return control
639	def getBool ( cls , prop ) : value = cls . getInt ( prop ) if value not in ( 0 , 1 ) : raise ValueError ( "Expected 0 or 1, but got %r in config property %s" % ( value , prop ) ) return bool ( value )
2849	def _poll_read ( self , expected , timeout_s = 5.0 ) : start = time . time ( ) response = bytearray ( expected ) index = 0 while time . time ( ) - start <= timeout_s : ret , data = ftdi . read_data ( self . _ctx , expected - index ) if ret < 0 : raise RuntimeError ( 'ftdi_read_data failed with error code {0}.' . format ( ret ) ) response [ index : index + ret ] = data [ : ret ] index += ret if index >= expected : return str ( response ) time . sleep ( 0.01 ) raise RuntimeError ( 'Timeout while polling ftdi_read_data for {0} bytes!' . format ( expected ) )
6121	def elliptical_annular ( cls , shape , pixel_scale , inner_major_axis_radius_arcsec , inner_axis_ratio , inner_phi , outer_major_axis_radius_arcsec , outer_axis_ratio , outer_phi , centre = ( 0.0 , 0.0 ) , invert = False ) : mask = mask_util . mask_elliptical_annular_from_shape_pixel_scale_and_radius ( shape , pixel_scale , inner_major_axis_radius_arcsec , inner_axis_ratio , inner_phi , outer_major_axis_radius_arcsec , outer_axis_ratio , outer_phi , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
10818	def can_invite_others ( self , user ) : if self . is_managed : return False elif self . is_admin ( user ) : return True elif self . subscription_policy != SubscriptionPolicy . CLOSED : return True else : return False
11114	def remove_repository ( self , path = None , relatedFiles = False , relatedFolders = False , verbose = True ) : if path is not None : realPath = os . path . realpath ( os . path . expanduser ( path ) ) else : realPath = self . __path if realPath is None : if verbose : warnings . warn ( 'path is None and current Repository is not initialized!' ) return if not self . is_repository ( realPath ) : if verbose : warnings . warn ( "No repository found in '%s'!" % realPath ) return if realPath == os . path . realpath ( '/..' ) : if verbose : warnings . warn ( 'You are about to wipe out your system !!! action aboarded' ) return if path is not None : repo = Repository ( ) repo . load_repository ( realPath ) else : repo = self if relatedFiles : for relativePath in repo . walk_files_relative_path ( ) : realPath = os . path . join ( repo . path , relativePath ) if not os . path . isfile ( realPath ) : continue if not os . path . exists ( realPath ) : continue os . remove ( realPath ) if relatedFolders : for relativePath in reversed ( list ( repo . walk_directories_relative_path ( ) ) ) : realPath = os . path . join ( repo . path , relativePath ) if not os . path . isdir ( realPath ) : continue if not os . path . exists ( realPath ) : continue if not len ( os . listdir ( realPath ) ) : os . rmdir ( realPath ) os . remove ( os . path . join ( repo . path , ".pyrepinfo" ) ) for fname in ( ".pyrepstate" , ".pyreplock" ) : p = os . path . join ( repo . path , fname ) if os . path . exists ( p ) : os . remove ( p ) if os . path . isdir ( repo . path ) : if not len ( os . listdir ( repo . path ) ) : os . rmdir ( repo . path ) repo . __reset_repository ( )
7395	def get_publication ( context , id ) : pbl = Publication . objects . filter ( pk = int ( id ) ) if len ( pbl ) < 1 : return '' pbl [ 0 ] . links = pbl [ 0 ] . customlink_set . all ( ) pbl [ 0 ] . files = pbl [ 0 ] . customfile_set . all ( ) return render_template ( 'publications/publication.html' , context [ 'request' ] , { 'publication' : pbl [ 0 ] } )
10411	def compare ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Mapping [ str , Mapping [ str , float ] ] : canonical_mechanisms = get_subgraphs_by_annotation ( graph , annotation ) canonical_nodes = _transform_graph_dict_to_node_dict ( canonical_mechanisms ) candidate_mechanisms = generate_bioprocess_mechanisms ( graph ) candidate_nodes = _transform_graph_dict_to_node_dict ( candidate_mechanisms ) results : Dict [ str , Dict [ str , float ] ] = defaultdict ( dict ) it = itt . product ( canonical_nodes . items ( ) , candidate_nodes . items ( ) ) for ( canonical_name , canonical_graph ) , ( candidate_bp , candidate_graph ) in it : tanimoto = tanimoto_set_similarity ( candidate_nodes , canonical_nodes ) results [ canonical_name ] [ candidate_bp ] = tanimoto return dict ( results )
3342	def send_status_response ( environ , start_response , e , add_headers = None , is_head = False ) : status = get_http_status_string ( e ) headers = [ ] if add_headers : headers . extend ( add_headers ) if e in ( HTTP_NOT_MODIFIED , HTTP_NO_CONTENT ) : start_response ( status , [ ( "Content-Length" , "0" ) , ( "Date" , get_rfc1123_time ( ) ) ] + headers ) return [ b"" ] if e in ( HTTP_OK , HTTP_CREATED ) : e = DAVError ( e ) assert isinstance ( e , DAVError ) content_type , body = e . get_response_page ( ) if is_head : body = compat . b_empty assert compat . is_bytes ( body ) , body start_response ( status , [ ( "Content-Type" , content_type ) , ( "Date" , get_rfc1123_time ( ) ) , ( "Content-Length" , str ( len ( body ) ) ) , ] + headers , ) return [ body ]
5395	def _get_input_target_path ( self , local_file_path ) : path , filename = os . path . split ( local_file_path ) if '*' in filename : return path + '/' else : return local_file_path
5646	def createcolorbar ( cmap , norm ) : cax , kw = matplotlib . colorbar . make_axes ( matplotlib . pyplot . gca ( ) ) c = matplotlib . colorbar . ColorbarBase ( cax , cmap = cmap , norm = norm ) return c
8457	def _needs_new_cc_config_for_update ( old_template , old_version , new_template , new_version ) : if old_template != new_template : return True else : return _cookiecutter_configs_have_changed ( new_template , old_version , new_version )
12046	def pickle_save ( thing , fname ) : pickle . dump ( thing , open ( fname , "wb" ) , pickle . HIGHEST_PROTOCOL ) return thing
10563	def compare_song_collections ( src_songs , dst_songs ) : def gather_field_values ( song ) : return tuple ( ( _normalize_metadata ( song [ field ] ) for field in _filter_comparison_fields ( song ) ) ) dst_songs_criteria = { gather_field_values ( _normalize_song ( dst_song ) ) for dst_song in dst_songs } return [ src_song for src_song in src_songs if gather_field_values ( _normalize_song ( src_song ) ) not in dst_songs_criteria ]
938	def _getModelPickleFilePath ( saveModelDir ) : path = os . path . join ( saveModelDir , "model.pkl" ) path = os . path . abspath ( path ) return path
7487	def concat_multiple_inputs ( data , sample ) : if len ( sample . files . fastqs ) > 1 : cmd1 = [ "cat" ] + [ i [ 0 ] for i in sample . files . fastqs ] isgzip = ".gz" if not sample . files . fastqs [ 0 ] [ 0 ] . endswith ( ".gz" ) : isgzip = "" conc1 = os . path . join ( data . dirs . edits , sample . name + "_R1_concat.fq{}" . format ( isgzip ) ) with open ( conc1 , 'w' ) as cout1 : proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = cout1 , close_fds = True ) res1 = proc1 . communicate ( ) [ 0 ] if proc1 . returncode : raise IPyradWarningExit ( "error in: {}, {}" . format ( cmd1 , res1 ) ) conc2 = 0 if "pair" in data . paramsdict [ "datatype" ] : cmd2 = [ "cat" ] + [ i [ 1 ] for i in sample . files . fastqs ] conc2 = os . path . join ( data . dirs . edits , sample . name + "_R2_concat.fq{}" . format ( isgzip ) ) with open ( conc2 , 'w' ) as cout2 : proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = cout2 , close_fds = True ) res2 = proc2 . communicate ( ) [ 0 ] if proc2 . returncode : raise IPyradWarningExit ( "Error concatenating fastq files. Make sure all " + "these files exist: {}\nError message: {}" . format ( cmd2 , proc2 . returncode ) ) sample . files . concat = [ ( conc1 , conc2 ) ] return sample . files . concat
12882	def main ( world_cls , referee_cls , gui_cls , gui_actor_cls , ai_actor_cls , theater_cls = PygletTheater , default_host = DEFAULT_HOST , default_port = DEFAULT_PORT , argv = None ) : import sys , os , docopt , nonstdlib exe_name = os . path . basename ( sys . argv [ 0 ] ) usage = main . __doc__ . format ( ** locals ( ) ) . strip ( ) args = docopt . docopt ( usage , argv or sys . argv [ 1 : ] ) num_guis = int ( args [ '<num_guis>' ] or 1 ) num_ais = int ( args [ '<num_ais>' ] or 0 ) host , port = args [ '--host' ] , int ( args [ '--port' ] ) logging . basicConfig ( format = '%(levelname)s: %(name)s: %(message)s' , level = nonstdlib . verbosity ( args [ '--verbose' ] ) , ) if args [ 'debug' ] : print ( ) game = MultiplayerDebugger ( world_cls , referee_cls , gui_cls , gui_actor_cls , num_guis , ai_actor_cls , num_ais , theater_cls , host , port ) else : game = theater_cls ( ) ai_actors = [ ai_actor_cls ( ) for i in range ( num_ais ) ] if args [ 'sandbox' ] : game . gui = gui_cls ( ) game . initial_stage = UniplayerGameStage ( world_cls ( ) , referee_cls ( ) , gui_actor_cls ( ) , ai_actors ) game . initial_stage . successor = PostgameSplashStage ( ) if args [ 'client' ] : game . gui = gui_cls ( ) game . initial_stage = ClientConnectionStage ( world_cls ( ) , gui_actor_cls ( ) , host , port ) if args [ 'server' ] : game . initial_stage = ServerConnectionStage ( world_cls ( ) , referee_cls ( ) , num_guis , ai_actors , host , port ) game . play ( )
6265	def translate_buffer_format ( vertex_format ) : buffer_format = [ ] attributes = [ ] mesh_attributes = [ ] if "T2F" in vertex_format : buffer_format . append ( "2f" ) attributes . append ( "in_uv" ) mesh_attributes . append ( ( "TEXCOORD_0" , "in_uv" , 2 ) ) if "C3F" in vertex_format : buffer_format . append ( "3f" ) attributes . append ( "in_color" ) mesh_attributes . append ( ( "NORMAL" , "in_color" , 3 ) ) if "N3F" in vertex_format : buffer_format . append ( "3f" ) attributes . append ( "in_normal" ) mesh_attributes . append ( ( "NORMAL" , "in_normal" , 3 ) ) buffer_format . append ( "3f" ) attributes . append ( "in_position" ) mesh_attributes . append ( ( "POSITION" , "in_position" , 3 ) ) return " " . join ( buffer_format ) , attributes , mesh_attributes
11958	def is_dec ( ip ) : try : dec = int ( str ( ip ) ) except ValueError : return False if dec > 4294967295 or dec < 0 : return False return True
6400	def stem ( self , word ) : wlen = len ( word ) - 2 if wlen > 2 and word [ - 1 ] == 's' : word = word [ : - 1 ] wlen -= 1 _endings = { 5 : { 'elser' , 'heten' } , 4 : { 'arne' , 'erna' , 'ande' , 'else' , 'aste' , 'orna' , 'aren' } , 3 : { 'are' , 'ast' , 'het' } , 2 : { 'ar' , 'er' , 'or' , 'en' , 'at' , 'te' , 'et' } , 1 : { 'a' , 'e' , 'n' , 't' } , } for end_len in range ( 5 , 0 , - 1 ) : if wlen > end_len and word [ - end_len : ] in _endings [ end_len ] : return word [ : - end_len ] return word
2525	def get_reviewer ( self , r_term ) : reviewer_list = list ( self . graph . triples ( ( r_term , self . spdx_namespace [ 'reviewer' ] , None ) ) ) if len ( reviewer_list ) != 1 : self . error = True msg = 'Review must have exactly one reviewer' self . logger . log ( msg ) return try : return self . builder . create_entity ( self . doc , six . text_type ( reviewer_list [ 0 ] [ 2 ] ) ) except SPDXValueError : self . value_error ( 'REVIEWER_VALUE' , reviewer_list [ 0 ] [ 2 ] )
3205	def all ( self , get_all = False , ** queryparams ) : self . batch_id = None self . operation_status = None if get_all : return self . _iterate ( url = self . _build_path ( ) , ** queryparams ) else : return self . _mc_client . _get ( url = self . _build_path ( ) , ** queryparams )
6823	def maint_up ( self ) : r = self . local_renderer fn = self . render_to_file ( r . env . maintenance_template , extra = { 'current_hostname' : self . current_hostname } ) r . put ( local_path = fn , remote_path = r . env . maintenance_path , use_sudo = True ) r . sudo ( 'chown -R {apache_web_user}:{apache_web_group} {maintenance_path}' )
12601	def _check_cols ( df , col_names ) : for col in col_names : if not hasattr ( df , col ) : raise AttributeError ( "DataFrame does not have a '{}' column, got {}." . format ( col , df . columns ) )
8104	def update ( self ) : try : self . manager . handle ( self . socket . recv ( 1024 ) ) except socket . error : pass
5171	def auto_client ( cls , host , server , ca_path = None , ca_contents = None , cert_path = None , cert_contents = None , key_path = None , key_contents = None ) : client = { "mode" : "p2p" , "nobind" : True , "resolv_retry" : "infinite" , "tls_client" : True } port = server . get ( 'port' ) or 1195 client [ 'remote' ] = [ { 'host' : host , 'port' : port } ] if server . get ( 'proto' ) == 'tcp-server' : client [ 'proto' ] = 'tcp-client' else : client [ 'proto' ] = 'udp' if 'server' in server or 'server_bridge' in server : client [ 'pull' ] = True if 'tls_server' not in server or not server [ 'tls_server' ] : client [ 'tls_client' ] = False ns_cert_type = { None : '' , '' : '' , 'client' : 'server' } client [ 'ns_cert_type' ] = ns_cert_type [ server . get ( 'ns_cert_type' ) ] remote_cert_tls = { None : '' , '' : '' , 'client' : 'server' } client [ 'remote_cert_tls' ] = remote_cert_tls [ server . get ( 'remote_cert_tls' ) ] copy_keys = [ 'name' , 'dev_type' , 'dev' , 'comp_lzo' , 'auth' , 'cipher' , 'ca' , 'cert' , 'key' , 'pkcs12' , 'mtu_disc' , 'mtu_test' , 'fragment' , 'mssfix' , 'keepalive' , 'persist_tun' , 'mute' , 'persist_key' , 'script_security' , 'user' , 'group' , 'log' , 'mute_replay_warnings' , 'secret' , 'reneg_sec' , 'tls_timeout' , 'tls_cipher' , 'float' , 'fast_io' , 'verb' ] for key in copy_keys : if key in server : client [ key ] = server [ key ] files = cls . _auto_client_files ( client , ca_path , ca_contents , cert_path , cert_contents , key_path , key_contents ) return { 'openvpn' : [ client ] , 'files' : files }
3172	def get ( self , store_id , customer_id , ** queryparams ) : self . store_id = store_id self . customer_id = customer_id return self . _mc_client . _get ( url = self . _build_path ( store_id , 'customers' , customer_id ) , ** queryparams )
10377	def one_sided ( value : float , distribution : List [ float ] ) -> float : assert distribution return sum ( value < element for element in distribution ) / len ( distribution )
7922	def __prepare_resource ( data ) : if not data : return None data = unicode ( data ) try : resource = RESOURCEPREP . prepare ( data ) except StringprepError , err : raise JIDError ( u"Local part invalid: {0}" . format ( err ) ) if len ( resource . encode ( "utf-8" ) ) > 1023 : raise JIDError ( "Resource name too long" ) return resource
6141	def in_out_check ( self ) : devices = available_devices ( ) if not self . in_idx in devices : raise OSError ( "Input device is unavailable" ) in_check = devices [ self . in_idx ] if not self . out_idx in devices : raise OSError ( "Output device is unavailable" ) out_check = devices [ self . out_idx ] if ( ( in_check [ 'inputs' ] == 0 ) and ( out_check [ 'outputs' ] == 0 ) ) : raise StandardError ( 'Invalid input and output devices' ) elif ( in_check [ 'inputs' ] == 0 ) : raise ValueError ( 'Selected input device has no inputs' ) elif ( out_check [ 'outputs' ] == 0 ) : raise ValueError ( 'Selected output device has no outputs' ) return True
11825	def random_boggle ( n = 4 ) : cubes = [ cubes16 [ i % 16 ] for i in range ( n * n ) ] random . shuffle ( cubes ) return map ( random . choice , cubes )
8334	def findAllPrevious ( self , name = None , attrs = { } , text = None , limit = None , ** kwargs ) : return self . _findAll ( name , attrs , text , limit , self . previousGenerator , ** kwargs )
11202	def strip_comments ( string , comment_symbols = frozenset ( ( '#' , '//' ) ) ) : lines = string . splitlines ( ) for k in range ( len ( lines ) ) : for symbol in comment_symbols : lines [ k ] = strip_comment_line_with_symbol ( lines [ k ] , start = symbol ) return '\n' . join ( lines )
205	def deepcopy ( self ) : segmap = SegmentationMapOnImage ( self . arr , shape = self . shape , nb_classes = self . nb_classes ) segmap . input_was = self . input_was return segmap
5703	def get_vehicle_hours_by_type ( gtfs , route_type ) : day = gtfs . get_suitable_date_for_daily_extract ( ) query = ( " SELECT * , SUM(end_time_ds - start_time_ds)/3600 as vehicle_hours_type" " FROM" " (SELECT * FROM day_trips as q1" " INNER JOIN" " (SELECT route_I, type FROM routes) as q2" " ON q1.route_I = q2.route_I" " WHERE type = {route_type}" " AND date = '{day}')" . format ( day = day , route_type = route_type ) ) df = gtfs . execute_custom_query_pandas ( query ) return df [ 'vehicle_hours_type' ] . item ( )
1256	def setup_hooks ( self ) : hooks = list ( ) if self . saver_spec is not None and ( self . execution_type == 'single' or self . distributed_spec [ 'task_index' ] == 0 ) : self . saver_directory = self . saver_spec [ 'directory' ] hooks . append ( tf . train . CheckpointSaverHook ( checkpoint_dir = self . saver_directory , save_secs = self . saver_spec . get ( 'seconds' , None if 'steps' in self . saver_spec else 600 ) , save_steps = self . saver_spec . get ( 'steps' ) , saver = None , checkpoint_basename = self . saver_spec . get ( 'basename' , 'model.ckpt' ) , scaffold = self . scaffold , listeners = None ) ) else : self . saver_directory = None return hooks
9564	def create_validator ( ) : field_names = ( 'study_id' , 'patient_id' , 'gender' , 'age_years' , 'age_months' , 'date_inclusion' ) validator = CSVValidator ( field_names ) validator . add_header_check ( 'EX1' , 'bad header' ) validator . add_record_length_check ( 'EX2' , 'unexpected record length' ) validator . add_value_check ( 'study_id' , int , 'EX3' , 'study id must be an integer' ) validator . add_value_check ( 'patient_id' , int , 'EX4' , 'patient id must be an integer' ) validator . add_value_check ( 'gender' , enumeration ( 'M' , 'F' ) , 'EX5' , 'invalid gender' ) validator . add_value_check ( 'age_years' , number_range_inclusive ( 0 , 120 , int ) , 'EX6' , 'invalid age in years' ) validator . add_value_check ( 'date_inclusion' , datetime_string ( '%Y-%m-%d' ) , 'EX7' , 'invalid date' ) def check_age_variables ( r ) : age_years = int ( r [ 'age_years' ] ) age_months = int ( r [ 'age_months' ] ) valid = ( age_months >= age_years * 12 and age_months % age_years < 12 ) if not valid : raise RecordError ( 'EX8' , 'invalid age variables' ) validator . add_record_check ( check_age_variables ) return validator
13406	def submitEntry ( self ) : mcclogs , physlogs = self . selectedLogs ( ) success = True if mcclogs != [ ] : if not self . acceptedUser ( "MCC" ) : QMessageBox ( ) . warning ( self , "Invalid User" , "Please enter a valid user name!" ) return fileName = self . xmlSetup ( "MCC" , mcclogs ) if fileName is None : return if not self . imagePixmap . isNull ( ) : self . prepareImages ( fileName , "MCC" ) success = self . sendToLogbook ( fileName , "MCC" ) if physlogs != [ ] : for i in range ( len ( physlogs ) ) : fileName = self . xmlSetup ( "Physics" , physlogs [ i ] ) if fileName is None : return if not self . imagePixmap . isNull ( ) : self . prepareImages ( fileName , "Physics" ) success_phys = self . sendToLogbook ( fileName , "Physics" , physlogs [ i ] ) success = success and success_phys self . done ( success )
2132	def _compare_node_lists ( old , new ) : to_expand = [ ] to_delete = [ ] to_recurse = [ ] old_records = { } new_records = { } for tree_node in old : old_records . setdefault ( tree_node . unified_job_template , [ ] ) old_records [ tree_node . unified_job_template ] . append ( tree_node ) for tree_node in new : new_records . setdefault ( tree_node . unified_job_template , [ ] ) new_records [ tree_node . unified_job_template ] . append ( tree_node ) for ujt_id in old_records : if ujt_id not in new_records : to_delete . extend ( old_records [ ujt_id ] ) continue old_list = old_records [ ujt_id ] new_list = new_records . pop ( ujt_id ) if len ( old_list ) == 1 and len ( new_list ) == 1 : to_recurse . append ( ( old_list [ 0 ] , new_list [ 0 ] ) ) else : to_delete . extend ( old_list ) to_expand . extend ( new_list ) for nodes in new_records . values ( ) : to_expand . extend ( nodes ) return to_expand , to_delete , to_recurse
10544	def update_task ( task ) : try : task_id = task . id task = _forbidden_attributes ( task ) res = _pybossa_req ( 'put' , 'task' , task_id , payload = task . data ) if res . get ( 'id' ) : return Task ( res ) else : return res except : raise
2372	def variables ( self ) : for table in self . tables : if isinstance ( table , VariableTable ) : for statement in table . rows : if statement [ 0 ] != "" : yield statement
10596	def h_x ( self , L , theta , Ts , ** statef ) : Nu_x = self . Nu_x ( L , theta , Ts , ** statef ) k = self . _fluid . k ( T = self . Tr ) return Nu_x * k / L
3872	def get_all ( self , include_archived = False ) : return [ conv for conv in self . _conv_dict . values ( ) if not conv . is_archived or include_archived ]
6090	def transform_grid ( func ) : @ wraps ( func ) def wrapper ( profile , grid , * args , ** kwargs ) : if not isinstance ( grid , TransformedGrid ) : return func ( profile , profile . transform_grid_to_reference_frame ( grid ) , * args , ** kwargs ) else : return func ( profile , grid , * args , ** kwargs ) return wrapper
11475	def renew_token ( ) : session . token = session . communicator . login_with_api_key ( session . email , session . api_key , application = session . application ) if len ( session . token ) < 10 : one_time_pass = getpass . getpass ( 'One-Time Password: ' ) session . token = session . communicator . mfa_otp_login ( session . token , one_time_pass ) return session . token
6724	def exists ( name = None , group = None , release = None , except_release = None , verbose = 1 ) : verbose = int ( verbose ) instances = list_instances ( name = name , group = group , release = release , except_release = except_release , verbose = verbose , show = verbose ) ret = bool ( instances ) if verbose : print ( '\ninstance %s exist' % ( 'DOES' if ret else 'does NOT' ) ) return instances
6339	def lcsseq ( self , src , tar ) : lengths = np_zeros ( ( len ( src ) + 1 , len ( tar ) + 1 ) , dtype = np_int ) for i , src_char in enumerate ( src ) : for j , tar_char in enumerate ( tar ) : if src_char == tar_char : lengths [ i + 1 , j + 1 ] = lengths [ i , j ] + 1 else : lengths [ i + 1 , j + 1 ] = max ( lengths [ i + 1 , j ] , lengths [ i , j + 1 ] ) result = '' i , j = len ( src ) , len ( tar ) while i != 0 and j != 0 : if lengths [ i , j ] == lengths [ i - 1 , j ] : i -= 1 elif lengths [ i , j ] == lengths [ i , j - 1 ] : j -= 1 else : result = src [ i - 1 ] + result i -= 1 j -= 1 return result
3609	def post_async ( self , url , data , callback = None , params = None , headers = None ) : params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , None ) self . _authenticate ( params , headers ) data = json . dumps ( data , cls = JSONEncoder ) process_pool . apply_async ( make_post_request , args = ( endpoint , data , params , headers ) , callback = callback )
4886	def update_throttle_scope ( self ) : self . scope = SERVICE_USER_SCOPE self . rate = self . get_rate ( ) self . num_requests , self . duration = self . parse_rate ( self . rate )
12568	def create_dataset ( self , ds_name , data , attrs = None , dtype = None ) : if ds_name in self . _datasets : ds = self . _datasets [ ds_name ] if ds . dtype != data . dtype : warnings . warn ( 'Dataset and data dtype are different!' ) else : if dtype is None : dtype = data . dtype ds = self . _group . create_dataset ( ds_name , data . shape , dtype = dtype ) if attrs is not None : for key in attrs : setattr ( ds . attrs , key , attrs [ key ] ) ds . read_direct ( data ) self . _datasets [ ds_name ] = ds return ds
1634	def CheckPosixThreading ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] for single_thread_func , multithread_safe_func , pattern in _THREADING_LIST : if Search ( pattern , line ) : error ( filename , linenum , 'runtime/threadsafe_fn' , 2 , 'Consider using ' + multithread_safe_func + '...) instead of ' + single_thread_func + '...) for improved thread safety.' )
7993	def _restart_stream ( self ) : self . _input_state = "restart" self . _output_state = "restart" self . features = None self . transport . restart ( ) if self . initiator : self . _send_stream_start ( self . stream_id )
12656	def dictify ( a_named_tuple ) : return dict ( ( s , getattr ( a_named_tuple , s ) ) for s in a_named_tuple . _fields )
4235	def _convert ( value , to_type , default = None ) : try : return default if value is None else to_type ( value ) except ValueError : return default
11592	def _rc_sunionstore ( self , dst , src , * args ) : args = list_or_args ( src , args ) result = self . sunion ( * args ) if result is not set ( [ ] ) : return self . sadd ( dst , * list ( result ) ) return 0
4263	def filter_nomedia ( album , settings = None ) : nomediapath = os . path . join ( album . src_path , ".nomedia" ) if os . path . isfile ( nomediapath ) : if os . path . getsize ( nomediapath ) == 0 : logger . info ( "Ignoring album '%s' because of present 0-byte " ".nomedia file" , album . name ) _remove_albums_with_subdirs ( album . gallery . albums , [ album . path ] ) try : os . rmdir ( album . dst_path ) except OSError as e : pass album . subdirs = [ ] album . medias = [ ] else : with open ( nomediapath , "r" ) as nomediaFile : logger . info ( "Found a .nomedia file in %s, ignoring its " "entries" , album . name ) ignored = nomediaFile . read ( ) . split ( "\n" ) album . medias = [ media for media in album . medias if media . src_filename not in ignored ] album . subdirs = [ dirname for dirname in album . subdirs if dirname not in ignored ] _remove_albums_with_subdirs ( album . gallery . albums , ignored , album . path + os . path . sep )
306	def plot_round_trip_lifetimes ( round_trips , disp_amount = 16 , lsize = 18 , ax = None ) : if ax is None : ax = plt . subplot ( ) symbols_sample = round_trips . symbol . unique ( ) np . random . seed ( 1 ) sample = np . random . choice ( round_trips . symbol . unique ( ) , replace = False , size = min ( disp_amount , len ( symbols_sample ) ) ) sample_round_trips = round_trips [ round_trips . symbol . isin ( sample ) ] symbol_idx = pd . Series ( np . arange ( len ( sample ) ) , index = sample ) for symbol , sym_round_trips in sample_round_trips . groupby ( 'symbol' ) : for _ , row in sym_round_trips . iterrows ( ) : c = 'b' if row . long else 'r' y_ix = symbol_idx [ symbol ] + 0.05 ax . plot ( [ row [ 'open_dt' ] , row [ 'close_dt' ] ] , [ y_ix , y_ix ] , color = c , linewidth = lsize , solid_capstyle = 'butt' ) ax . set_yticks ( range ( disp_amount ) ) ax . set_yticklabels ( [ utils . format_asset ( s ) for s in sample ] ) ax . set_ylim ( ( - 0.5 , min ( len ( sample ) , disp_amount ) - 0.5 ) ) blue = patches . Rectangle ( [ 0 , 0 ] , 1 , 1 , color = 'b' , label = 'Long' ) red = patches . Rectangle ( [ 0 , 0 ] , 1 , 1 , color = 'r' , label = 'Short' ) leg = ax . legend ( handles = [ blue , red ] , loc = 'lower left' , frameon = True , framealpha = 0.5 ) leg . get_frame ( ) . set_edgecolor ( 'black' ) ax . grid ( False ) return ax
6334	def dist ( self , src , tar ) : if src == tar : return 0.0 return self . dist_abs ( src , tar ) / ( len ( src ) + len ( tar ) )
7284	def has_edit_permission ( self , request ) : return request . user . is_authenticated and request . user . is_active and request . user . is_staff
8742	def update_floatingip ( context , id , content ) : LOG . info ( 'update_floatingip %s for tenant %s and body %s' % ( id , context . tenant_id , content ) ) if 'port_id' not in content : raise n_exc . BadRequest ( resource = 'floating_ip' , msg = 'port_id is required.' ) requested_ports = [ ] if content . get ( 'port_id' ) : requested_ports = [ { 'port_id' : content . get ( 'port_id' ) } ] flip = _update_flip ( context , id , ip_types . FLOATING , requested_ports ) return v . _make_floating_ip_dict ( flip )
7535	def muscle_chunker ( data , sample ) : LOGGER . info ( "inside muscle_chunker" ) if data . paramsdict [ "assembly_method" ] != "reference" : clustfile = os . path . join ( data . dirs . clusts , sample . name + ".clust.gz" ) with iter ( gzip . open ( clustfile , 'rb' ) ) as clustio : nloci = sum ( 1 for i in clustio if "//" in i ) // 2 optim = ( nloci // 20 ) + ( nloci % 20 ) LOGGER . info ( "optim for align chunks: %s" , optim ) clustio = gzip . open ( clustfile , 'rb' ) inclusts = iter ( clustio . read ( ) . strip ( ) . split ( "//\n//\n" ) ) inc = optim // 10 for idx in range ( 10 ) : this = optim + ( idx * inc ) left = nloci - this if idx == 9 : grabchunk = list ( itertools . islice ( inclusts , int ( 1e9 ) ) ) else : grabchunk = list ( itertools . islice ( inclusts , this ) ) nloci = left tmpfile = os . path . join ( data . tmpdir , sample . name + "_chunk_{}.ali" . format ( idx ) ) with open ( tmpfile , 'wb' ) as out : out . write ( "//\n//\n" . join ( grabchunk ) ) clustio . close ( )
13544	def formatter ( color , s ) : if no_coloring : return s return "{begin}{s}{reset}" . format ( begin = color , s = s , reset = Colors . RESET )
2531	def parse_doc_fields ( self , doc_term ) : try : self . builder . set_doc_spdx_id ( self . doc , doc_term ) except SPDXValueError : self . value_error ( 'DOC_SPDX_ID_VALUE' , doc_term ) try : if doc_term . count ( '#' , 0 , len ( doc_term ) ) <= 1 : doc_namespace = doc_term . split ( '#' ) [ 0 ] self . builder . set_doc_namespace ( self . doc , doc_namespace ) else : self . value_error ( 'DOC_NAMESPACE_VALUE' , doc_term ) except SPDXValueError : self . value_error ( 'DOC_NAMESPACE_VALUE' , doc_term ) for _s , _p , o in self . graph . triples ( ( doc_term , self . spdx_namespace [ 'specVersion' ] , None ) ) : try : self . builder . set_doc_version ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'DOC_VERS_VALUE' , o ) except CardinalityError : self . more_than_one_error ( 'specVersion' ) break for _s , _p , o in self . graph . triples ( ( doc_term , self . spdx_namespace [ 'dataLicense' ] , None ) ) : try : self . builder . set_doc_data_lic ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'DOC_D_LICS' , o ) except CardinalityError : self . more_than_one_error ( 'dataLicense' ) break for _s , _p , o in self . graph . triples ( ( doc_term , self . spdx_namespace [ 'name' ] , None ) ) : try : self . builder . set_doc_name ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'name' ) break for _s , _p , o in self . graph . triples ( ( doc_term , RDFS . comment , None ) ) : try : self . builder . set_doc_comment ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'Document comment' ) break
11803	def nconflicts ( self , var , val , assignment ) : n = len ( self . vars ) c = self . rows [ val ] + self . downs [ var + val ] + self . ups [ var - val + n - 1 ] if assignment . get ( var , None ) == val : c -= 3 return c
702	def getResultsPerChoice ( self , swarmId , maxGenIdx , varName ) : results = dict ( ) ( allParticles , _ , resultErrs , _ , _ ) = self . getParticleInfos ( swarmId , genIdx = None , matured = True ) for particleState , resultErr in itertools . izip ( allParticles , resultErrs ) : if maxGenIdx is not None : if particleState [ 'genIdx' ] > maxGenIdx : continue if resultErr == numpy . inf : continue position = Particle . getPositionFromState ( particleState ) varPosition = position [ varName ] varPositionStr = str ( varPosition ) if varPositionStr in results : results [ varPositionStr ] [ 1 ] . append ( resultErr ) else : results [ varPositionStr ] = ( varPosition , [ resultErr ] ) return results
5477	def parse_rfc3339_utc_string ( rfc3339_utc_string ) : m = re . match ( r'(\d{4})-(\d{2})-(\d{2})T(\d{2}):(\d{2}):(\d{2}).?(\d*)Z' , rfc3339_utc_string ) if not m : return None groups = m . groups ( ) if len ( groups [ 6 ] ) not in ( 0 , 3 , 6 , 9 ) : return None g = [ int ( val ) for val in groups [ : 6 ] ] fraction = groups [ 6 ] if not fraction : micros = 0 elif len ( fraction ) == 3 : micros = int ( fraction ) * 1000 elif len ( fraction ) == 6 : micros = int ( fraction ) elif len ( fraction ) == 9 : micros = int ( round ( int ( fraction ) / 1000 ) ) else : assert False , 'Fraction length not 0, 6, or 9: {}' . len ( fraction ) try : return datetime ( g [ 0 ] , g [ 1 ] , g [ 2 ] , g [ 3 ] , g [ 4 ] , g [ 5 ] , micros , tzinfo = pytz . utc ) except ValueError as e : assert False , 'Could not parse RFC3339 datestring: {} exception: {}' . format ( rfc3339_utc_string , e )
7397	def tex_parse ( string ) : string = string . replace ( '{' , '' ) . replace ( '}' , '' ) def tex_replace ( match ) : return sub ( r'\^(\w)' , r'<sup>\1</sup>' , sub ( r'\^\{(.*?)\}' , r'<sup>\1</sup>' , sub ( r'\_(\w)' , r'<sub>\1</sub>' , sub ( r'\_\{(.*?)\}' , r'<sub>\1</sub>' , sub ( r'\\(' + GREEK_LETTERS + ')' , r'&\1;' , match . group ( 1 ) ) ) ) ) ) return mark_safe ( sub ( r'\$([^\$]*)\$' , tex_replace , escape ( string ) ) )
11433	def _shift_field_positions_global ( record , start , delta = 1 ) : if not delta : return for tag , fields in record . items ( ) : newfields = [ ] for field in fields : if field [ 4 ] < start : newfields . append ( field ) else : newfields . append ( tuple ( list ( field [ : 4 ] ) + [ field [ 4 ] + delta ] ) ) record [ tag ] = newfields
8686	def _decrypt ( self , hexified_value ) : encrypted_value = binascii . unhexlify ( hexified_value ) with warnings . catch_warnings ( ) : warnings . simplefilter ( "ignore" ) jsonified_value = self . cipher . decrypt ( encrypted_value ) . decode ( 'ascii' ) value = json . loads ( jsonified_value ) return value
2345	def forward ( self , x ) : features = self . conv ( x ) . mean ( dim = 2 ) return self . dense ( features )
8277	def fseq ( self , client , message ) : client . last_frame = client . current_frame client . current_frame = message [ 3 ]
8132	def export ( self , filename ) : self . flatten ( ) self . layers [ 1 ] . img . save ( filename ) return filename
54	def shift ( self , x = 0 , y = 0 ) : keypoints = [ keypoint . shift ( x = x , y = y ) for keypoint in self . keypoints ] return self . deepcopy ( keypoints )
9406	def _cleanup ( self ) : self . exit ( ) workspace = osp . join ( os . getcwd ( ) , 'octave-workspace' ) if osp . exists ( workspace ) : os . remove ( workspace )
9967	def shareable_parameters ( cells ) : result = [ ] for c in cells . values ( ) : params = c . formula . parameters for i in range ( min ( len ( result ) , len ( params ) ) ) : if params [ i ] != result [ i ] : return None for i in range ( len ( result ) , len ( params ) ) : result . append ( params [ i ] ) return result
8681	def delete ( self , key_name ) : self . _assert_valid_stash ( ) if key_name == 'stored_passphrase' : raise GhostError ( '`stored_passphrase` is a reserved ghost key name ' 'which cannot be deleted' ) if not self . get ( key_name ) : raise GhostError ( 'Key `{0}` not found' . format ( key_name ) ) key = self . _storage . get ( key_name ) if key . get ( 'lock' ) : raise GhostError ( 'Key `{0}` is locked and therefore cannot be deleted ' 'Please unlock the key and try again' . format ( key_name ) ) deleted = self . _storage . delete ( key_name ) audit ( storage = self . _storage . db_path , action = 'DELETE' , message = json . dumps ( dict ( key_name = key_name ) ) ) if not deleted : raise GhostError ( 'Failed to delete {0}' . format ( key_name ) )
2286	def graph_evaluation ( data , adj_matrix , gpu = None , gpu_id = 0 , ** kwargs ) : gpu = SETTINGS . get_default ( gpu = gpu ) device = 'cuda:{}' . format ( gpu_id ) if gpu else 'cpu' obs = th . FloatTensor ( data ) . to ( device ) cgnn = CGNN_model ( adj_matrix , data . shape [ 0 ] , gpu_id = gpu_id , ** kwargs ) . to ( device ) cgnn . reset_parameters ( ) return cgnn . run ( obs , ** kwargs )
1527	def is_host_port_reachable ( self ) : for hostport in self . hostportlist : try : socket . create_connection ( hostport , StateManager . TIMEOUT_SECONDS ) return True except : LOG . info ( "StateManager %s Unable to connect to host: %s port %i" % ( self . name , hostport [ 0 ] , hostport [ 1 ] ) ) continue return False
6584	def station_selection_menu ( self , error = None ) : self . screen . clear ( ) if error : self . screen . print_error ( "{}\n" . format ( error ) ) for i , station in enumerate ( self . stations ) : i = "{:>3}" . format ( i ) print ( "{}: {}" . format ( Colors . yellow ( i ) , station . name ) ) return self . stations [ self . screen . get_integer ( "Station: " ) ]
9158	def version ( ) : with io . open ( 'pgmagick/_version.py' ) as input_file : for line in input_file : if line . startswith ( '__version__' ) : return ast . parse ( line ) . body [ 0 ] . value . s
1508	def add_additional_args ( parsers ) : for parser in parsers : cli_args . add_verbose ( parser ) cli_args . add_config ( parser ) parser . add_argument ( '--heron-dir' , default = config . get_heron_dir ( ) , help = 'Path to Heron home directory' )
3475	def compartments ( self ) : if self . _compartments is None : self . _compartments = { met . compartment for met in self . _metabolites if met . compartment is not None } return self . _compartments
4628	def get_private ( self ) : encoded = "%s %d" % ( self . brainkey , self . sequence ) a = _bytes ( encoded ) s = hashlib . sha256 ( hashlib . sha512 ( a ) . digest ( ) ) . digest ( ) return PrivateKey ( hexlify ( s ) . decode ( "ascii" ) , prefix = self . prefix )
10289	def enrich_reactions ( graph : BELGraph ) : nodes = list ( get_nodes_by_function ( graph , REACTION ) ) for u in nodes : for v in u . reactants : graph . add_has_reactant ( u , v ) for v in u . products : graph . add_has_product ( u , v )
11118	def get_parent_directory_info ( self , relativePath ) : relativePath = os . path . normpath ( relativePath ) if relativePath in ( '' , '.' ) : return self , "relativePath is empty pointing to the repostitory itself." parentDirPath , _ = os . path . split ( relativePath ) return self . get_directory_info ( parentDirPath )
1996	def cmp_regs ( cpu , should_print = False ) : differing = False gdb_regs = gdb . getCanonicalRegisters ( ) for name in sorted ( gdb_regs ) : vg = gdb_regs [ name ] if name . endswith ( 'psr' ) : name = 'apsr' v = cpu . read_register ( name . upper ( ) ) if should_print : logger . debug ( f'{name} gdb:{vg:x} mcore:{v:x}' ) if vg != v : if should_print : logger . warning ( '^^ unequal' ) differing = True if differing : logger . debug ( qemu . correspond ( None ) ) return differing
7997	def set_peer_authenticated ( self , peer , restart_stream = False ) : with self . lock : self . peer_authenticated = True self . peer = peer if restart_stream : self . _restart_stream ( ) self . event ( AuthenticatedEvent ( self . peer ) )
9708	def get_sanitizer ( self ) : sanitizer = self . sanitizer if not sanitizer : default_sanitizer = settings . CONFIG . get ( self . SANITIZER_KEY ) field_settings = getattr ( self , 'field_settings' , None ) if isinstance ( field_settings , six . string_types ) : profiles = settings . CONFIG . get ( self . SANITIZER_PROFILES_KEY , { } ) sanitizer = profiles . get ( field_settings , default_sanitizer ) else : sanitizer = default_sanitizer if isinstance ( sanitizer , six . string_types ) : sanitizer = import_string ( sanitizer ) return sanitizer or noop
5777	def _advapi32_encrypt ( certificate_or_public_key , data , rsa_oaep_padding = False ) : flags = 0 if rsa_oaep_padding : flags = Advapi32Const . CRYPT_OAEP out_len = new ( advapi32 , 'DWORD *' , len ( data ) ) res = advapi32 . CryptEncrypt ( certificate_or_public_key . ex_key_handle , null ( ) , True , flags , null ( ) , out_len , 0 ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) write_to_buffer ( buffer , data ) pointer_set ( out_len , len ( data ) ) res = advapi32 . CryptEncrypt ( certificate_or_public_key . ex_key_handle , null ( ) , True , flags , buffer , out_len , buffer_len ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) ) [ : : - 1 ]
9923	def save ( self ) : try : email = models . EmailAddress . objects . get ( email = self . validated_data [ "email" ] , is_verified = False ) logger . debug ( "Resending verification email to %s" , self . validated_data [ "email" ] , ) email . send_confirmation ( ) except models . EmailAddress . DoesNotExist : logger . debug ( "Not resending verification email to %s because the address " "doesn't exist in the database." , self . validated_data [ "email" ] , )
13135	def autocomplete ( query , country = None , hurricanes = False , cities = True , timeout = 5 ) : data = { } data [ 'query' ] = quote ( query ) data [ 'country' ] = country or '' data [ 'hurricanes' ] = 1 if hurricanes else 0 data [ 'cities' ] = 1 if cities else 0 data [ 'format' ] = 'JSON' r = requests . get ( AUTOCOMPLETE_URL . format ( ** data ) , timeout = timeout ) results = json . loads ( r . content ) [ 'RESULTS' ] return results
8801	def run_migrations_online ( ) : engine = create_engine ( neutron_config . database . connection , poolclass = pool . NullPool ) connection = engine . connect ( ) context . configure ( connection = connection , target_metadata = target_metadata ) try : with context . begin_transaction ( ) : context . run_migrations ( ) finally : connection . close ( )
7715	def update_item ( self , jid , name = NO_CHANGE , groups = NO_CHANGE , callback = None , error_callback = None ) : item = self . roster [ jid ] if name is NO_CHANGE and groups is NO_CHANGE : return if name is NO_CHANGE : name = item . name if groups is NO_CHANGE : groups = item . groups item = RosterItem ( jid , name , groups ) self . _roster_set ( item , callback , error_callback )
13391	def format_uuid ( uuid , max_length = 10 ) : if max_length <= 3 : raise ValueError ( "max length must be larger than 3" ) if len ( uuid ) > max_length : uuid = "{}..." . format ( uuid [ 0 : max_length - 3 ] ) return uuid
149	def clip_out_of_image ( self ) : polys_cut = [ poly . clip_out_of_image ( self . shape ) for poly in self . polygons if poly . is_partly_within_image ( self . shape ) ] polys_cut_flat = [ poly for poly_lst in polys_cut for poly in poly_lst ] return PolygonsOnImage ( polys_cut_flat , shape = self . shape )
3344	def read_timeout_value_header ( timeoutvalue ) : timeoutsecs = 0 timeoutvaluelist = timeoutvalue . split ( "," ) for timeoutspec in timeoutvaluelist : timeoutspec = timeoutspec . strip ( ) if timeoutspec . lower ( ) == "infinite" : return - 1 else : listSR = reSecondsReader . findall ( timeoutspec ) for secs in listSR : timeoutsecs = int ( secs ) if timeoutsecs > MAX_FINITE_TIMEOUT_LIMIT : return - 1 if timeoutsecs != 0 : return timeoutsecs return None
9746	def datagram_received ( self , datagram , address ) : size , _ = RTheader . unpack_from ( datagram , 0 ) info , = struct . unpack_from ( "{0}s" . format ( size - 3 - 8 ) , datagram , RTheader . size ) base_port , = QRTDiscoveryBasePort . unpack_from ( datagram , size - 2 ) if self . receiver is not None : self . receiver ( QRTDiscoveryResponse ( info , address [ 0 ] , base_port ) )
936	def writeBaseToProto ( self , proto ) : inferenceType = self . getInferenceType ( ) inferenceType = inferenceType [ : 1 ] . lower ( ) + inferenceType [ 1 : ] proto . inferenceType = inferenceType proto . numPredictions = self . _numPredictions proto . learningEnabled = self . __learningEnabled proto . inferenceEnabled = self . __inferenceEnabled proto . inferenceArgs = json . dumps ( self . __inferenceArgs )
463	def open_tensorboard ( log_dir = '/tmp/tensorflow' , port = 6006 ) : text = "[TL] Open tensorboard, go to localhost:" + str ( port ) + " to access" text2 = " not yet supported by this function (tl.ops.open_tb)" if not tl . files . exists_or_mkdir ( log_dir , verbose = False ) : tl . logging . info ( "[TL] Log reportory was created at %s" % log_dir ) if _platform == "linux" or _platform == "linux2" : raise NotImplementedError ( ) elif _platform == "darwin" : tl . logging . info ( 'OS X: %s' % text ) subprocess . Popen ( sys . prefix + " | python -m tensorflow.tensorboard --logdir=" + log_dir + " --port=" + str ( port ) , shell = True ) elif _platform == "win32" : raise NotImplementedError ( "this function is not supported on the Windows platform" ) else : tl . logging . info ( _platform + text2 )
5237	def get_interval ( ticker , session ) -> Session : if '_' not in session : session = f'{session}_normal_0_0' interval = Intervals ( ticker = ticker ) ss_info = session . split ( '_' ) return getattr ( interval , f'market_{ss_info.pop(1)}' ) ( * ss_info )
3960	def update_local_repo_async ( self , task_queue , force = False ) : self . ensure_local_repo ( ) task_queue . enqueue_task ( self . update_local_repo , force = force )
7022	def pklc_fovcatalog_objectinfo ( pklcdir , fovcatalog , fovcatalog_columns = [ 0 , 1 , 2 , 6 , 7 , 8 , 9 , 10 , 11 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 ] , fovcatalog_colnames = [ 'objectid' , 'ra' , 'decl' , 'jmag' , 'jmag_err' , 'hmag' , 'hmag_err' , 'kmag' , 'kmag_err' , 'bmag' , 'vmag' , 'rmag' , 'imag' , 'sdssu' , 'sdssg' , 'sdssr' , 'sdssi' , 'sdssz' ] , fovcatalog_colformats = ( 'U20,f8,f8,' 'f8,f8,' 'f8,f8,' 'f8,f8,' 'f8,f8,f8,f8,' 'f8,f8,f8,' 'f8,f8' ) ) : if fovcatalog . endswith ( '.gz' ) : catfd = gzip . open ( fovcatalog ) else : catfd = open ( fovcatalog ) fovcat = np . genfromtxt ( catfd , usecols = fovcatalog_columns , names = fovcatalog_colnames , dtype = fovcatalog_colformats ) catfd . close ( ) pklclist = sorted ( glob . glob ( os . path . join ( pklcdir , '*HAT*-pklc.pkl' ) ) ) updatedpklcs , failedpklcs = [ ] , [ ] for pklc in pklclist : lcdict = read_hatpi_pklc ( pklc ) objectid = lcdict [ 'objectid' ] catind = np . where ( fovcat [ 'objectid' ] == objectid ) if len ( catind ) > 0 and catind [ 0 ] : lcdict [ 'objectinfo' ] . update ( { x : y for x , y in zip ( fovcatalog_colnames , [ np . asscalar ( fovcat [ z ] [ catind ] ) for z in fovcatalog_colnames ] ) } ) with open ( pklc + '-tmp' , 'wb' ) as outfd : pickle . dump ( lcdict , outfd , pickle . HIGHEST_PROTOCOL ) if os . path . exists ( pklc + '-tmp' ) : shutil . move ( pklc + '-tmp' , pklc ) LOGINFO ( 'updated %s with catalog info for %s at %.3f, %.3f OK' % ( pklc , objectid , lcdict [ 'objectinfo' ] [ 'ra' ] , lcdict [ 'objectinfo' ] [ 'decl' ] ) ) updatedpklcs . append ( pklc ) else : failedpklcs . append ( pklc ) return updatedpklcs , failedpklcs
7020	def generate_hatpi_binnedlc_pkl ( binnedpklf , textlcf , timebinsec , outfile = None ) : binlcdict = read_hatpi_binnedlc ( binnedpklf , textlcf , timebinsec ) if binlcdict : if outfile is None : outfile = os . path . join ( os . path . dirname ( binnedpklf ) , '%s-hplc.pkl' % ( os . path . basename ( binnedpklf ) . replace ( 'sec-lc.pkl.gz' , '' ) ) ) return lcdict_to_pickle ( binlcdict , outfile = outfile ) else : LOGERROR ( 'could not read binned HATPI LC: %s' % binnedpklf ) return None
1239	def move ( self , external_index , new_priority ) : index = external_index + ( self . _capacity - 1 ) return self . _move ( index , new_priority )
11808	def samples ( self , nwords ) : n = self . n nminus1gram = ( '' , ) * ( n - 1 ) output = [ ] for i in range ( nwords ) : if nminus1gram not in self . cond_prob : nminus1gram = ( '' , ) * ( n - 1 ) wn = self . cond_prob [ nminus1gram ] . sample ( ) output . append ( wn ) nminus1gram = nminus1gram [ 1 : ] + ( wn , ) return ' ' . join ( output )
1341	def imagenet_example ( shape = ( 224 , 224 ) , data_format = 'channels_last' ) : assert len ( shape ) == 2 assert data_format in [ 'channels_first' , 'channels_last' ] from PIL import Image path = os . path . join ( os . path . dirname ( __file__ ) , 'example.png' ) image = Image . open ( path ) image = image . resize ( shape ) image = np . asarray ( image , dtype = np . float32 ) image = image [ : , : , : 3 ] assert image . shape == shape + ( 3 , ) if data_format == 'channels_first' : image = np . transpose ( image , ( 2 , 0 , 1 ) ) return image , 282
10854	def sphere_triangle_cdf ( dr , a , alpha ) : p0 = ( dr + alpha ) ** 2 / ( 2 * alpha ** 2 ) * ( 0 > dr ) * ( dr > - alpha ) p1 = 1 * ( dr > 0 ) - ( alpha - dr ) ** 2 / ( 2 * alpha ** 2 ) * ( 0 < dr ) * ( dr < alpha ) return ( 1 - np . clip ( p0 + p1 , 0 , 1 ) )
11824	def genetic_search ( problem , fitness_fn , ngen = 1000 , pmut = 0.1 , n = 20 ) : s = problem . initial_state states = [ problem . result ( s , a ) for a in problem . actions ( s ) ] random . shuffle ( states ) return genetic_algorithm ( states [ : n ] , problem . value , ngen , pmut )
501	def _recomputeRecordFromKNN ( self , record ) : inputs = { "categoryIn" : [ None ] , "bottomUpIn" : self . _getStateAnomalyVector ( record ) , } outputs = { "categoriesOut" : numpy . zeros ( ( 1 , ) ) , "bestPrototypeIndices" : numpy . zeros ( ( 1 , ) ) , "categoryProbabilitiesOut" : numpy . zeros ( ( 1 , ) ) } classifier_indexes = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) valid_idx = numpy . where ( ( classifier_indexes >= self . getParameter ( 'trainRecords' ) ) & ( classifier_indexes < record . ROWID ) ) [ 0 ] . tolist ( ) if len ( valid_idx ) == 0 : return None self . _knnclassifier . setParameter ( 'inferenceMode' , None , True ) self . _knnclassifier . setParameter ( 'learningMode' , None , False ) self . _knnclassifier . compute ( inputs , outputs ) self . _knnclassifier . setParameter ( 'learningMode' , None , True ) classifier_distances = self . _knnclassifier . getLatestDistances ( ) valid_distances = classifier_distances [ valid_idx ] if valid_distances . min ( ) <= self . _classificationMaxDist : classifier_indexes_prev = classifier_indexes [ valid_idx ] rowID = classifier_indexes_prev [ valid_distances . argmin ( ) ] indexID = numpy . where ( classifier_indexes == rowID ) [ 0 ] [ 0 ] category = self . _knnclassifier . getCategoryList ( ) [ indexID ] return category return None
7626	def transcription ( ref , est , ** kwargs ) : r namespace = 'pitch_contour' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_intervals , ref_p = ref . to_interval_values ( ) est_intervals , est_p = est . to_interval_values ( ) ref_pitches = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in ref_p ] ) est_pitches = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in est_p ] ) return mir_eval . transcription . evaluate ( ref_intervals , ref_pitches , est_intervals , est_pitches , ** kwargs )
12407	def cons ( collection , value ) : if isinstance ( value , collections . Mapping ) : if collection is None : collection = { } collection . update ( ** value ) elif isinstance ( value , six . string_types ) : if collection is None : collection = [ ] collection . append ( value ) elif isinstance ( value , collections . Iterable ) : if collection is None : collection = [ ] collection . extend ( value ) else : if collection is None : collection = [ ] collection . append ( value ) return collection
4270	def get_iptc_data ( filename ) : logger = logging . getLogger ( __name__ ) iptc_data = { } raw_iptc = { } try : img = _read_image ( filename ) raw_iptc = IptcImagePlugin . getiptcinfo ( img ) except SyntaxError : logger . info ( 'IPTC Error in %s' , filename ) if raw_iptc and ( 2 , 5 ) in raw_iptc : iptc_data [ "title" ] = raw_iptc [ ( 2 , 5 ) ] . decode ( 'utf-8' , errors = 'replace' ) if raw_iptc and ( 2 , 120 ) in raw_iptc : iptc_data [ "description" ] = raw_iptc [ ( 2 , 120 ) ] . decode ( 'utf-8' , errors = 'replace' ) if raw_iptc and ( 2 , 105 ) in raw_iptc : iptc_data [ "headline" ] = raw_iptc [ ( 2 , 105 ) ] . decode ( 'utf-8' , errors = 'replace' ) return iptc_data
612	def _getExperimentDescriptionSchema ( ) : installPath = os . path . dirname ( os . path . abspath ( __file__ ) ) schemaFilePath = os . path . join ( installPath , "experimentDescriptionSchema.json" ) return json . loads ( open ( schemaFilePath , 'r' ) . read ( ) )
13872	def StandardizePath ( path , strip = False ) : path = path . replace ( SEPARATOR_WINDOWS , SEPARATOR_UNIX ) if strip : path = path . rstrip ( SEPARATOR_UNIX ) return path
5286	def post ( self , request , * args , ** kwargs ) : formset = self . construct_formset ( ) if formset . is_valid ( ) : return self . formset_valid ( formset ) else : return self . formset_invalid ( formset )
13725	def register_credentials ( self , credentials = None , user = None , user_file = None , password = None , password_file = None ) : if credentials is not None : self . credentials = credentials else : self . credentials = { } if user : self . credentials [ "user" ] = user elif user_file : with open ( user_file , "r" ) as of : pattern = re . compile ( "^user: " ) for l in of : if re . match ( pattern , l ) : l = l [ 0 : - 1 ] self . credentials [ "user" ] = re . sub ( pattern , "" , l ) if self . credentials [ "user" ] [ 0 : 1 ] == '"' and self . credentials [ "user" ] [ - 1 : ] == '"' : self . credentials [ "user" ] = self . credentials [ "user" ] [ 1 : - 1 ] if password : self . credentials [ "password" ] = password elif password_file : with open ( password_file , "r" ) as of : pattern = re . compile ( "^password: " ) for l in of : if re . match ( pattern , l ) : l = l [ 0 : - 1 ] self . credentials [ "password" ] = re . sub ( pattern , "" , l ) if self . credentials [ "password" ] [ 0 : 1 ] == '"' and self . credentials [ "password" ] [ - 1 : ] == '"' : self . credentials [ "password" ] = self . credentials [ "password" ] [ 1 : - 1 ] if "user" in self . credentials and "password" in self . credentials : c = self . credentials [ "user" ] + ":" + self . credentials [ "password" ] self . credentials [ "base64" ] = b64encode ( c . encode ( ) ) . decode ( "ascii" )
1154	def remove ( self , value ) : if value not in self : raise KeyError ( value ) self . discard ( value )
8529	def report ( self ) : self . _output . write ( '\r' ) sort_by = 'avg' results = { } for key , latencies in self . _latencies_by_method . items ( ) : result = { } result [ 'count' ] = len ( latencies ) result [ 'avg' ] = sum ( latencies ) / len ( latencies ) result [ 'min' ] = min ( latencies ) result [ 'max' ] = max ( latencies ) latencies = sorted ( latencies ) result [ 'p90' ] = percentile ( latencies , 0.90 ) result [ 'p95' ] = percentile ( latencies , 0.95 ) result [ 'p99' ] = percentile ( latencies , 0.99 ) result [ 'p999' ] = percentile ( latencies , 0.999 ) results [ key ] = result headers = [ 'method' , 'count' , 'avg' , 'min' , 'max' , 'p90' , 'p95' , 'p99' , 'p999' ] data = [ ] results = sorted ( results . items ( ) , key = lambda it : it [ 1 ] [ sort_by ] , reverse = True ) def row ( key , res ) : data = [ key ] + [ res [ header ] for header in headers [ 1 : ] ] return tuple ( data ) data = [ row ( key , result ) for key , result in results ] self . _output . write ( '%s\n' % tabulate ( data , headers = headers ) ) self . _output . flush ( )
2564	def start ( self ) : self . comm . Barrier ( ) logger . debug ( "Manager synced with workers" ) self . _kill_event = threading . Event ( ) self . _task_puller_thread = threading . Thread ( target = self . pull_tasks , args = ( self . _kill_event , ) ) self . _result_pusher_thread = threading . Thread ( target = self . push_results , args = ( self . _kill_event , ) ) self . _task_puller_thread . start ( ) self . _result_pusher_thread . start ( ) start = None result_counter = 0 task_recv_counter = 0 task_sent_counter = 0 logger . info ( "Loop start" ) while not self . _kill_event . is_set ( ) : time . sleep ( LOOP_SLOWDOWN ) timer = time . time ( ) + 0.05 counter = min ( 10 , comm . size ) while time . time ( ) < timer : info = MPI . Status ( ) if counter > 10 : logger . debug ( "Hit max mpi events per round" ) break if not self . comm . Iprobe ( status = info ) : logger . debug ( "Timer expired, processed {} mpi events" . format ( counter ) ) break else : tag = info . Get_tag ( ) logger . info ( "Message with tag {} received" . format ( tag ) ) counter += 1 if tag == RESULT_TAG : result = self . recv_result_from_workers ( ) self . pending_result_queue . put ( result ) result_counter += 1 elif tag == TASK_REQUEST_TAG : worker_rank = self . recv_task_request_from_workers ( ) self . ready_worker_queue . put ( worker_rank ) else : logger . error ( "Unknown tag {} - ignoring this message and continuing" . format ( tag ) ) available_worker_cnt = self . ready_worker_queue . qsize ( ) available_task_cnt = self . pending_task_queue . qsize ( ) logger . debug ( "[MAIN] Ready workers: {} Ready tasks: {}" . format ( available_worker_cnt , available_task_cnt ) ) this_round = min ( available_worker_cnt , available_task_cnt ) for i in range ( this_round ) : worker_rank = self . ready_worker_queue . get ( ) task = self . pending_task_queue . get ( ) comm . send ( task , dest = worker_rank , tag = worker_rank ) task_sent_counter += 1 logger . debug ( "Assigning worker:{} task:{}" . format ( worker_rank , task [ 'task_id' ] ) ) if not start : start = time . time ( ) logger . debug ( "Tasks recvd:{} Tasks dispatched:{} Results recvd:{}" . format ( task_recv_counter , task_sent_counter , result_counter ) ) self . _task_puller_thread . join ( ) self . _result_pusher_thread . join ( ) self . task_incoming . close ( ) self . result_outgoing . close ( ) self . context . term ( ) delta = time . time ( ) - start logger . info ( "mpi_worker_pool ran for {} seconds" . format ( delta ) )
2508	def get_extr_lics_comment ( self , extr_lics ) : comment_list = list ( self . graph . triples ( ( extr_lics , RDFS . comment , None ) ) ) if len ( comment_list ) > 1 : self . more_than_one_error ( 'extracted license comment' ) return elif len ( comment_list ) == 1 : return comment_list [ 0 ] [ 2 ] else : return
3761	def draw_2d ( self , Hs = False ) : r try : from rdkit . Chem import Draw from rdkit . Chem . Draw import IPythonConsole if Hs : mols = [ i . rdkitmol_Hs for i in self . Chemicals ] else : mols = [ i . rdkitmol for i in self . Chemicals ] return Draw . MolsToImage ( mols ) except : return 'Rdkit is required for this feature.'
7826	def payload_element_name ( element_name ) : def decorator ( klass ) : from . stanzapayload import STANZA_PAYLOAD_CLASSES from . stanzapayload import STANZA_PAYLOAD_ELEMENTS if hasattr ( klass , "_pyxmpp_payload_element_name" ) : klass . _pyxmpp_payload_element_name . append ( element_name ) else : klass . _pyxmpp_payload_element_name = [ element_name ] if element_name in STANZA_PAYLOAD_CLASSES : logger = logging . getLogger ( 'pyxmpp.payload_element_name' ) logger . warning ( "Overriding payload class for {0!r}" . format ( element_name ) ) STANZA_PAYLOAD_CLASSES [ element_name ] = klass STANZA_PAYLOAD_ELEMENTS [ klass ] . append ( element_name ) return klass return decorator
13577	def paste ( tid = None , review = False ) : submit ( pastebin = True , tid = tid , review = False )
2070	def basen_to_integer ( self , X , cols , base ) : out_cols = X . columns . values . tolist ( ) for col in cols : col_list = [ col0 for col0 in out_cols if str ( col0 ) . startswith ( str ( col ) ) ] insert_at = out_cols . index ( col_list [ 0 ] ) if base == 1 : value_array = np . array ( [ int ( col0 . split ( '_' ) [ - 1 ] ) for col0 in col_list ] ) else : len0 = len ( col_list ) value_array = np . array ( [ base ** ( len0 - 1 - i ) for i in range ( len0 ) ] ) X . insert ( insert_at , col , np . dot ( X [ col_list ] . values , value_array . T ) ) X . drop ( col_list , axis = 1 , inplace = True ) out_cols = X . columns . values . tolist ( ) return X
8049	def run ( self ) : if self . err is not None : assert self . source is None msg = "%s%03i %s" % ( rst_prefix , rst_fail_load , "Failed to load file: %s" % self . err , ) yield 0 , 0 , msg , type ( self ) module = [ ] try : module = parse ( StringIO ( self . source ) , self . filename ) except SyntaxError as err : msg = "%s%03i %s" % ( rst_prefix , rst_fail_parse , "Failed to parse file: %s" % err , ) yield 0 , 0 , msg , type ( self ) module = [ ] except AllError : msg = "%s%03i %s" % ( rst_prefix , rst_fail_all , "Failed to parse __all__ entry." , ) yield 0 , 0 , msg , type ( self ) module = [ ] for definition in module : if not definition . docstring : continue try : unindented = trim ( dequote_docstring ( definition . docstring ) ) rst_errors = list ( rst_lint . lint ( unindented ) ) except Exception as err : msg = "%s%03i %s" % ( rst_prefix , rst_fail_lint , "Failed to lint docstring: %s - %s" % ( definition . name , err ) , ) yield definition . start , 0 , msg , type ( self ) continue for rst_error in rst_errors : if rst_error . level <= 1 : continue msg = rst_error . message . split ( "\n" , 1 ) [ 0 ] code = code_mapping ( rst_error . level , msg ) assert code < 100 , code code += 100 * rst_error . level msg = "%s%03i %s" % ( rst_prefix , code , msg ) yield definition . start + rst_error . line , 0 , msg , type ( self )
11772	def NaiveBayesLearner ( dataset ) : targetvals = dataset . values [ dataset . target ] target_dist = CountingProbDist ( targetvals ) attr_dists = dict ( ( ( gv , attr ) , CountingProbDist ( dataset . values [ attr ] ) ) for gv in targetvals for attr in dataset . inputs ) for example in dataset . examples : targetval = example [ dataset . target ] target_dist . add ( targetval ) for attr in dataset . inputs : attr_dists [ targetval , attr ] . add ( example [ attr ] ) def predict ( example ) : def class_probability ( targetval ) : return ( target_dist [ targetval ] * product ( attr_dists [ targetval , attr ] [ example [ attr ] ] for attr in dataset . inputs ) ) return argmax ( targetvals , class_probability ) return predict
5105	def poisson_random_measure ( t , rate , rate_max ) : scale = 1.0 / rate_max t = t + exponential ( scale ) while rate_max * uniform ( ) > rate ( t ) : t = t + exponential ( scale ) return t
799	def modelsInfo ( self , modelIDs ) : assert isinstance ( modelIDs , self . _SEQUENCE_TYPES ) , ( "wrong modelIDs type: %s" ) % ( type ( modelIDs ) , ) assert modelIDs , "modelIDs is empty" rows = self . _getMatchingRowsWithRetries ( self . _models , dict ( model_id = modelIDs ) , [ self . _models . pubToDBNameDict [ f ] for f in self . _models . modelInfoNamedTuple . _fields ] ) results = [ self . _models . modelInfoNamedTuple . _make ( r ) for r in rows ] assert len ( results ) == len ( modelIDs ) , "modelIDs not found: %s" % ( set ( modelIDs ) - set ( r . modelId for r in results ) ) return results
5990	def constant_regularization_matrix_from_pixel_neighbors ( coefficients , pixel_neighbors , pixel_neighbors_size ) : pixels = len ( pixel_neighbors ) regularization_matrix = np . zeros ( shape = ( pixels , pixels ) ) regularization_coefficient = coefficients [ 0 ] ** 2.0 for i in range ( pixels ) : regularization_matrix [ i , i ] += 1e-8 for j in range ( pixel_neighbors_size [ i ] ) : neighbor_index = pixel_neighbors [ i , j ] regularization_matrix [ i , i ] += regularization_coefficient regularization_matrix [ i , neighbor_index ] -= regularization_coefficient return regularization_matrix
292	def plot_rolling_sharpe ( returns , factor_returns = None , rolling_window = APPROX_BDAYS_PER_MONTH * 6 , legend_loc = 'best' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) rolling_sharpe_ts = timeseries . rolling_sharpe ( returns , rolling_window ) rolling_sharpe_ts . plot ( alpha = .7 , lw = 3 , color = 'orangered' , ax = ax , ** kwargs ) if factor_returns is not None : rolling_sharpe_ts_factor = timeseries . rolling_sharpe ( factor_returns , rolling_window ) rolling_sharpe_ts_factor . plot ( alpha = .7 , lw = 3 , color = 'grey' , ax = ax , ** kwargs ) ax . set_title ( 'Rolling Sharpe ratio (6-month)' ) ax . axhline ( rolling_sharpe_ts . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 ) ax . axhline ( 0.0 , color = 'black' , linestyle = '-' , lw = 3 ) ax . set_ylabel ( 'Sharpe ratio' ) ax . set_xlabel ( '' ) if factor_returns is None : ax . legend ( [ 'Sharpe' , 'Average' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) else : ax . legend ( [ 'Sharpe' , 'Benchmark Sharpe' , 'Average' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) return ax
4361	def _watcher ( self ) : while True : gevent . sleep ( 1.0 ) if not self . connected : for ns_name , ns in list ( six . iteritems ( self . active_ns ) ) : ns . recv_disconnect ( ) gevent . killall ( self . jobs ) break
9464	def conference_undeaf ( self , call_params ) : path = '/' + self . api_version + '/ConferenceUndeaf/' method = 'POST' return self . request ( path , method , call_params )
10177	def run ( self , start_date = None , end_date = None , update_bookmark = True ) : if not Index ( self . event_index , using = self . client ) . exists ( ) : return lower_limit = start_date or self . get_bookmark ( ) if lower_limit is None : return upper_limit = min ( end_date or datetime . datetime . max , datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) , datetime . datetime . combine ( lower_limit + datetime . timedelta ( self . batch_size ) , datetime . datetime . min . time ( ) ) ) while upper_limit <= datetime . datetime . utcnow ( ) : self . indices = set ( ) self . new_bookmark = upper_limit . strftime ( self . doc_id_suffix ) bulk ( self . client , self . agg_iter ( lower_limit , upper_limit ) , stats_only = True , chunk_size = 50 ) current_search_client . indices . flush ( index = ',' . join ( self . indices ) , wait_if_ongoing = True ) if update_bookmark : self . set_bookmark ( ) self . indices = set ( ) lower_limit = lower_limit + datetime . timedelta ( self . batch_size ) upper_limit = min ( end_date or datetime . datetime . max , datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) , lower_limit + datetime . timedelta ( self . batch_size ) ) if lower_limit > upper_limit : break
11732	def registerGoodClass ( self , class_ ) : self . _valid_classes . append ( class_ ) for name , cls in class_members ( class_ ) : if self . isValidClass ( cls ) : self . registerGoodClass ( cls )
95	def quokka_segmentation_map ( size = None , extract = None ) : from imgaug . augmentables . segmaps import SegmentationMapOnImage with open ( QUOKKA_ANNOTATIONS_FP , "r" ) as f : json_dict = json . load ( f ) xx = [ ] yy = [ ] for kp_dict in json_dict [ "polygons" ] [ 0 ] [ "keypoints" ] : x = kp_dict [ "x" ] y = kp_dict [ "y" ] xx . append ( x ) yy . append ( y ) img_seg = np . zeros ( ( 643 , 960 , 1 ) , dtype = np . float32 ) rr , cc = skimage . draw . polygon ( np . array ( yy ) , np . array ( xx ) , shape = img_seg . shape ) img_seg [ rr , cc ] = 1.0 if extract is not None : bb = _quokka_normalize_extract ( extract ) img_seg = bb . extract_from_image ( img_seg ) segmap = SegmentationMapOnImage ( img_seg , shape = img_seg . shape [ 0 : 2 ] + ( 3 , ) ) if size is not None : shape_resized = _compute_resized_shape ( img_seg . shape , size ) segmap = segmap . resize ( shape_resized [ 0 : 2 ] ) segmap . shape = tuple ( shape_resized [ 0 : 2 ] ) + ( 3 , ) return segmap
7192	def histogram_equalize ( self , use_bands , ** kwargs ) : data = self . _read ( self [ use_bands , ... ] , ** kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) flattened = data . flatten ( ) if 0 in data : masked = np . ma . masked_values ( data , 0 ) . compressed ( ) image_histogram , bin_edges = np . histogram ( masked , 256 ) else : image_histogram , bin_edges = np . histogram ( flattened , 256 ) bins = ( bin_edges [ : - 1 ] + bin_edges [ 1 : ] ) / 2.0 cdf = image_histogram . cumsum ( ) cdf = cdf / float ( cdf [ - 1 ] ) image_equalized = np . interp ( flattened , bins , cdf ) . reshape ( data . shape ) if 'stretch' in kwargs or 'gamma' in kwargs : return self . _histogram_stretch ( image_equalized , ** kwargs ) else : return image_equalized
3187	def create ( self , store_id , data ) : self . store_id = store_id if 'id' not in data : raise KeyError ( 'The order must have an id' ) if 'customer' not in data : raise KeyError ( 'The order must have a customer' ) if 'id' not in data [ 'customer' ] : raise KeyError ( 'The order customer must have an id' ) if 'currency_code' not in data : raise KeyError ( 'The order must have a currency_code' ) if not re . match ( r"^[A-Z]{3}$" , data [ 'currency_code' ] ) : raise ValueError ( 'The currency_code must be a valid 3-letter ISO 4217 currency code' ) if 'order_total' not in data : raise KeyError ( 'The order must have an order_total' ) if 'lines' not in data : raise KeyError ( 'The order must have at least one order line' ) for line in data [ 'lines' ] : if 'id' not in line : raise KeyError ( 'Each order line must have an id' ) if 'product_id' not in line : raise KeyError ( 'Each order line must have a product_id' ) if 'product_variant_id' not in line : raise KeyError ( 'Each order line must have a product_variant_id' ) if 'quantity' not in line : raise KeyError ( 'Each order line must have a quantity' ) if 'price' not in line : raise KeyError ( 'Each order line must have a price' ) response = self . _mc_client . _post ( url = self . _build_path ( store_id , 'orders' ) , data = data ) if response is not None : self . order_id = response [ 'id' ] else : self . order_id = None return response
13687	def assert_equal_files ( self , obtained_fn , expected_fn , fix_callback = lambda x : x , binary = False , encoding = None ) : import os from zerotk . easyfs import GetFileContents , GetFileLines __tracebackhide__ = True import io def FindFile ( filename ) : data_filename = self . get_filename ( filename ) if os . path . isfile ( data_filename ) : return data_filename if os . path . isfile ( filename ) : return filename from . _exceptions import MultipleFilesNotFound raise MultipleFilesNotFound ( [ filename , data_filename ] ) obtained_fn = FindFile ( obtained_fn ) expected_fn = FindFile ( expected_fn ) if binary : obtained_lines = GetFileContents ( obtained_fn , binary = True ) expected_lines = GetFileContents ( expected_fn , binary = True ) assert obtained_lines == expected_lines else : obtained_lines = fix_callback ( GetFileLines ( obtained_fn , encoding = encoding ) ) expected_lines = GetFileLines ( expected_fn , encoding = encoding ) if obtained_lines != expected_lines : html_fn = os . path . splitext ( obtained_fn ) [ 0 ] + '.diff.html' html_diff = self . _generate_html_diff ( expected_fn , expected_lines , obtained_fn , obtained_lines ) with io . open ( html_fn , 'w' ) as f : f . write ( html_diff ) import difflib diff = [ 'FILES DIFFER:' , obtained_fn , expected_fn ] diff += [ 'HTML DIFF: %s' % html_fn ] diff += difflib . context_diff ( obtained_lines , expected_lines ) raise AssertionError ( '\n' . join ( diff ) + '\n' )
6009	def load_background_noise_map ( background_noise_map_path , background_noise_map_hdu , pixel_scale , convert_background_noise_map_from_weight_map , convert_background_noise_map_from_inverse_noise_map ) : background_noise_map_options = sum ( [ convert_background_noise_map_from_weight_map , convert_background_noise_map_from_inverse_noise_map ] ) if background_noise_map_options == 0 and background_noise_map_path is not None : return NoiseMap . from_fits_with_pixel_scale ( file_path = background_noise_map_path , hdu = background_noise_map_hdu , pixel_scale = pixel_scale ) elif convert_background_noise_map_from_weight_map and background_noise_map_path is not None : weight_map = Array . from_fits ( file_path = background_noise_map_path , hdu = background_noise_map_hdu ) return NoiseMap . from_weight_map ( weight_map = weight_map , pixel_scale = pixel_scale ) elif convert_background_noise_map_from_inverse_noise_map and background_noise_map_path is not None : inverse_noise_map = Array . from_fits ( file_path = background_noise_map_path , hdu = background_noise_map_hdu ) return NoiseMap . from_inverse_noise_map ( inverse_noise_map = inverse_noise_map , pixel_scale = pixel_scale ) else : return None
4015	def consume ( consumer_id ) : global _consumers consumer = _consumers [ consumer_id ] client = get_docker_client ( ) try : status = client . inspect_container ( consumer . container_id ) [ 'State' ] [ 'Status' ] except Exception as e : status = 'unknown' new_logs = client . logs ( consumer . container_id , stdout = True , stderr = True , stream = False , timestamps = False , since = calendar . timegm ( consumer . offset . timetuple ( ) ) ) updated_consumer = Consumer ( consumer . container_id , datetime . utcnow ( ) ) _consumers [ str ( consumer_id ) ] = updated_consumer response = jsonify ( { 'logs' : new_logs , 'status' : status } ) response . headers [ 'Access-Control-Allow-Origin' ] = '*' response . headers [ 'Access-Control-Allow-Methods' ] = 'GET, POST' return response
4704	def memcopy ( self , stream , offset = 0 , length = float ( "inf" ) ) : data = [ ord ( i ) for i in list ( stream ) ] size = min ( length , len ( data ) , self . m_size ) buff = cast ( self . m_buf , POINTER ( c_uint8 ) ) for i in range ( size ) : buff [ offset + i ] = data [ i ]
13100	def render ( self , ** kwargs ) : breadcrumbs = [ ] breadcrumbs = [ ] if "collections" in kwargs : breadcrumbs = [ { "title" : "Text Collections" , "link" : ".r_collections" , "args" : { } } ] if "parents" in kwargs [ "collections" ] : breadcrumbs += [ { "title" : parent [ "label" ] , "link" : ".r_collection_semantic" , "args" : { "objectId" : parent [ "id" ] , "semantic" : f_slugify ( parent [ "label" ] ) , } , } for parent in kwargs [ "collections" ] [ "parents" ] ] [ : : - 1 ] if "current" in kwargs [ "collections" ] : breadcrumbs . append ( { "title" : kwargs [ "collections" ] [ "current" ] [ "label" ] , "link" : None , "args" : { } } ) if len ( breadcrumbs ) > 0 : breadcrumbs [ - 1 ] [ "link" ] = None return { "breadcrumbs" : breadcrumbs }
11607	def social_widget_render ( parser , token ) : bits = token . split_contents ( ) tag_name = bits [ 0 ] if len ( bits ) < 2 : raise TemplateSyntaxError ( "'%s' takes at least one argument" % tag_name ) args = [ ] kwargs = { } bits = bits [ 1 : ] if len ( bits ) : for bit in bits : match = kwarg_re . match ( bit ) if not match : raise TemplateSyntaxError ( "Malformed arguments to %s tag" % tag_name ) name , value = match . groups ( ) if name : name = name . replace ( '-' , '_' ) kwargs [ name ] = parser . compile_filter ( value ) else : args . append ( parser . compile_filter ( value ) ) return SocialWidgetNode ( args , kwargs )
6970	def _old_epd_magseries ( times , mags , errs , fsv , fdv , fkv , xcc , ycc , bgv , bge , epdsmooth_windowsize = 21 , epdsmooth_sigclip = 3.0 , epdsmooth_func = smooth_magseries_signal_medfilt , epdsmooth_extraparams = None ) : finiteind = np . isfinite ( mags ) mags_median = np . median ( mags [ finiteind ] ) mags_stdev = np . nanstd ( mags ) if epdsmooth_sigclip : excludeind = abs ( mags - mags_median ) < epdsmooth_sigclip * mags_stdev finalind = finiteind & excludeind else : finalind = finiteind final_mags = mags [ finalind ] final_len = len ( final_mags ) if isinstance ( epdsmooth_extraparams , dict ) : smoothedmags = epdsmooth_func ( final_mags , epdsmooth_windowsize , ** epdsmooth_extraparams ) else : smoothedmags = epdsmooth_func ( final_mags , epdsmooth_windowsize ) epdmatrix = np . c_ [ fsv [ finalind ] ** 2.0 , fsv [ finalind ] , fdv [ finalind ] ** 2.0 , fdv [ finalind ] , fkv [ finalind ] ** 2.0 , fkv [ finalind ] , np . ones ( final_len ) , fsv [ finalind ] * fdv [ finalind ] , fsv [ finalind ] * fkv [ finalind ] , fdv [ finalind ] * fkv [ finalind ] , np . sin ( 2 * np . pi * xcc [ finalind ] ) , np . cos ( 2 * np . pi * xcc [ finalind ] ) , np . sin ( 2 * np . pi * ycc [ finalind ] ) , np . cos ( 2 * np . pi * ycc [ finalind ] ) , np . sin ( 4 * np . pi * xcc [ finalind ] ) , np . cos ( 4 * np . pi * xcc [ finalind ] ) , np . sin ( 4 * np . pi * ycc [ finalind ] ) , np . cos ( 4 * np . pi * ycc [ finalind ] ) , bgv [ finalind ] , bge [ finalind ] ] try : coeffs , residuals , rank , singulars = lstsq ( epdmatrix , smoothedmags , rcond = None ) if DEBUG : print ( 'coeffs = %s, residuals = %s' % ( coeffs , residuals ) ) retdict = { 'times' : times , 'mags' : ( mags_median + _old_epd_diffmags ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , mags ) ) , 'errs' : errs , 'fitcoeffs' : coeffs , 'residuals' : residuals } return retdict except Exception as e : LOGEXCEPTION ( 'EPD solution did not converge' ) retdict = { 'times' : times , 'mags' : np . full_like ( mags , np . nan ) , 'errs' : errs , 'fitcoeffs' : coeffs , 'residuals' : residuals } return retdict
1836	def JG ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . AND ( cpu . ZF == False , cpu . SF == cpu . OF ) , target . read ( ) , cpu . PC )
11645	def fit ( self , X , y = None ) : n = X . shape [ 0 ] if X . shape != ( n , n ) : raise TypeError ( "Input must be a square matrix." ) memory = get_memory ( self . memory ) vals , vecs = memory . cache ( scipy . linalg . eigh , ignore = [ 'overwrite_a' ] ) ( X , overwrite_a = not self . copy ) vals = vals [ : , None ] self . flip_ = np . dot ( vecs , np . sign ( vals ) * vecs . T ) return self
3645	def tradepileDelete ( self , trade_id ) : method = 'DELETE' url = 'trade/%s' % trade_id self . __request__ ( method , url ) return True
960	def validateOpfJsonValue ( value , opfJsonSchemaFilename ) : jsonSchemaPath = os . path . join ( os . path . dirname ( __file__ ) , "jsonschema" , opfJsonSchemaFilename ) jsonhelpers . validate ( value , schemaPath = jsonSchemaPath ) return
6892	def serial_starfeatures ( lclist , outdir , lc_catalog_pickle , neighbor_radius_arcsec , maxobjects = None , deredden = True , custom_bandpasses = None , lcformat = 'hat-sql' , lcformatdir = None ) : if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if maxobjects : lclist = lclist [ : maxobjects ] with open ( lc_catalog_pickle , 'rb' ) as infd : kdt_dict = pickle . load ( infd ) kdt = kdt_dict [ 'kdtree' ] objlist = kdt_dict [ 'objects' ] [ 'objectid' ] objlcfl = kdt_dict [ 'objects' ] [ 'lcfname' ] tasks = [ ( x , outdir , kdt , objlist , objlcfl , neighbor_radius_arcsec , deredden , custom_bandpasses , lcformat , lcformatdir ) for x in lclist ] for task in tqdm ( tasks ) : result = _starfeatures_worker ( task ) return result
4987	def get_course_run_id ( user , enterprise_customer , course_key ) : try : course = CourseCatalogApiServiceClient ( enterprise_customer . site ) . get_course_details ( course_key ) except ImproperlyConfigured : raise Http404 users_all_enrolled_courses = EnrollmentApiClient ( ) . get_enrolled_courses ( user . username ) users_active_course_runs = get_active_course_runs ( course , users_all_enrolled_courses ) if users_all_enrolled_courses else [ ] course_run = get_current_course_run ( course , users_active_course_runs ) if course_run : course_run_id = course_run [ 'key' ] return course_run_id else : raise Http404
8911	def ows_security_tween_factory ( handler , registry ) : security = owssecurity_factory ( registry ) def ows_security_tween ( request ) : try : security . check_request ( request ) return handler ( request ) except OWSException as err : logger . exception ( "security check failed." ) return err except Exception as err : logger . exception ( "unknown error" ) return OWSNoApplicableCode ( "{}" . format ( err ) ) return ows_security_tween
7859	def make_result_response ( self ) : if self . stanza_type not in ( "set" , "get" ) : raise ValueError ( "Results may only be generated for" " 'set' or 'get' iq" ) stanza = Iq ( stanza_type = "result" , from_jid = self . to_jid , to_jid = self . from_jid , stanza_id = self . stanza_id ) return stanza
2993	def otcSymbolsDF ( token = '' , version = '' ) : df = pd . DataFrame ( otcSymbols ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
1982	def sync ( f ) : def new_function ( self , * args , ** kw ) : self . _lock . acquire ( ) try : return f ( self , * args , ** kw ) finally : self . _lock . release ( ) return new_function
489	def close ( self ) : self . _logger . info ( "Closing" ) if self . _conn is not None : self . _conn . close ( ) self . _conn = None else : self . _logger . warning ( "close() called, but connection policy was alredy closed" ) return
841	def getPattern ( self , idx , sparseBinaryForm = False , cat = None ) : if cat is not None : assert idx is None idx = self . _categoryList . index ( cat ) if not self . useSparseMemory : pattern = self . _Memory [ idx ] if sparseBinaryForm : pattern = pattern . nonzero ( ) [ 0 ] else : ( nz , values ) = self . _Memory . rowNonZeros ( idx ) if not sparseBinaryForm : pattern = numpy . zeros ( self . _Memory . nCols ( ) ) numpy . put ( pattern , nz , 1 ) else : pattern = nz return pattern
12150	def htmlFor ( self , fname ) : if os . path . splitext ( fname ) [ 1 ] . lower ( ) in [ '.jpg' , '.png' ] : html = '<a href="%s"><img src="%s"></a>' % ( fname , fname ) if "_tif_" in fname : html = html . replace ( '<img ' , '<img class="datapic micrograph"' ) if "_plot_" in fname : html = html . replace ( '<img ' , '<img class="datapic intrinsic" ' ) if "_experiment_" in fname : html = html . replace ( '<img ' , '<img class="datapic experiment" ' ) elif os . path . splitext ( fname ) [ 1 ] . lower ( ) in [ '.html' , '.htm' ] : html = 'LINK: %s' % fname else : html = '<br>Not sure how to show: [%s]</br>' % fname return html
9781	def get ( ctx ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) try : response = PolyaxonClient ( ) . build_job . get_build ( user , project_name , _build ) cache . cache ( config_manager = BuildJobManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get build job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_build_details ( response )
13468	def normalize_slice ( slice_obj , length ) : if isinstance ( slice_obj , slice ) : start , stop , step = slice_obj . start , slice_obj . stop , slice_obj . step if start is None : start = 0 if stop is None : stop = length if step is None : step = 1 if start < 0 : start += length if stop < 0 : stop += length elif isinstance ( slice_obj , int ) : start = slice_obj if start < 0 : start += length stop = start + 1 step = 1 else : raise TypeError if ( 0 <= start <= length ) and ( 0 <= stop <= length ) : return start , stop , step raise IndexError
4795	def does_not_contain_value ( self , * values ) : self . _check_dict_like ( self . val , check_getitem = False ) if len ( values ) == 0 : raise ValueError ( 'one or more value args must be given' ) else : found = [ ] for v in values : if v in self . val . values ( ) : found . append ( v ) if found : self . _err ( 'Expected <%s> to not contain values %s, but did contain %s.' % ( self . val , self . _fmt_items ( values ) , self . _fmt_items ( found ) ) ) return self
7495	def chunk_to_matrices ( narr , mapcol , nmask ) : mats = np . zeros ( ( 3 , 16 , 16 ) , dtype = np . uint32 ) last_loc = - 1 for idx in xrange ( mapcol . shape [ 0 ] ) : if not nmask [ idx ] : if not mapcol [ idx ] == last_loc : i = narr [ : , idx ] mats [ 0 , ( 4 * i [ 0 ] ) + i [ 1 ] , ( 4 * i [ 2 ] ) + i [ 3 ] ] += 1 last_loc = mapcol [ idx ] x = np . uint8 ( 0 ) for y in np . array ( [ 0 , 4 , 8 , 12 ] , dtype = np . uint8 ) : for z in np . array ( [ 0 , 4 , 8 , 12 ] , dtype = np . uint8 ) : mats [ 1 , y : y + np . uint8 ( 4 ) , z : z + np . uint8 ( 4 ) ] = mats [ 0 , x ] . reshape ( 4 , 4 ) mats [ 2 , y : y + np . uint8 ( 4 ) , z : z + np . uint8 ( 4 ) ] = mats [ 0 , x ] . reshape ( 4 , 4 ) . T x += np . uint8 ( 1 ) return mats
5192	def send_select_and_operate_command ( self , command , index , callback = asiodnp3 . PrintingCommandCallback . Get ( ) , config = opendnp3 . TaskConfig ( ) . Default ( ) ) : self . master . SelectAndOperate ( command , index , callback , config )
3719	def conductivity ( CASRN = None , AvailableMethods = False , Method = None , full_info = True ) : r def list_methods ( ) : methods = [ ] if CASRN in Lange_cond_pure . index : methods . append ( LANGE_COND ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == LANGE_COND : kappa = float ( Lange_cond_pure . at [ CASRN , 'Conductivity' ] ) if full_info : T = float ( Lange_cond_pure . at [ CASRN , 'T' ] ) elif Method == NONE : kappa , T = None , None else : raise Exception ( 'Failure in in function' ) if full_info : return kappa , T else : return kappa
6268	def resolve_loader ( self , meta : SceneDescription ) : for loader_cls in self . _loaders : if loader_cls . supports_file ( meta ) : meta . loader_cls = loader_cls break else : raise ImproperlyConfigured ( "Scene {} has no loader class registered. Check settings.SCENE_LOADERS" . format ( meta . path ) )
2552	def attr ( * args , ** kwargs ) : ctx = dom_tag . _with_contexts [ _get_thread_context ( ) ] if ctx and ctx [ - 1 ] : dicts = args + ( kwargs , ) for d in dicts : for attr , value in d . items ( ) : ctx [ - 1 ] . tag . set_attribute ( * dom_tag . clean_pair ( attr , value ) ) else : raise ValueError ( 'not in a tag context' )
4912	def contains_content_items ( self , request , pk , course_run_ids , program_uuids ) : enterprise_customer = self . get_object ( ) course_run_ids = [ unquote ( quote_plus ( course_run_id ) ) for course_run_id in course_run_ids ] contains_content_items = False for catalog in enterprise_customer . enterprise_customer_catalogs . all ( ) : contains_course_runs = not course_run_ids or catalog . contains_courses ( course_run_ids ) contains_program_uuids = not program_uuids or catalog . contains_programs ( program_uuids ) if contains_course_runs and contains_program_uuids : contains_content_items = True break return Response ( { 'contains_content_items' : contains_content_items } )
7914	def get_int_range_validator ( start , stop ) : def validate_int_range ( value ) : value = int ( value ) if value >= start and value < stop : return value raise ValueError ( "Not in <{0},{1}) range" . format ( start , stop ) ) return validate_int_range
2899	def get_tasks_from_spec_name ( self , name ) : return [ task for task in self . get_tasks ( ) if task . task_spec . name == name ]
2915	def cancel ( self ) : if self . _is_finished ( ) : for child in self . children : child . cancel ( ) return self . _set_state ( self . CANCELLED ) self . _drop_children ( ) self . task_spec . _on_cancel ( self )
7776	def __from_xml ( self , value ) : n = value . children vns = get_node_ns ( value ) while n : if n . type != 'element' : n = n . next continue ns = get_node_ns ( n ) if ( ns and vns and ns . getContent ( ) != vns . getContent ( ) ) : n = n . next continue if n . name == 'POBOX' : self . pobox = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name in ( 'EXTADR' , 'EXTADD' ) : self . extadr = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'STREET' : self . street = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'LOCALITY' : self . locality = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'REGION' : self . region = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'PCODE' : self . pcode = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'CTRY' : self . ctry = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name in ( "HOME" , "WORK" , "POSTAL" , "PARCEL" , "DOM" , "INTL" , "PREF" ) : self . type . append ( n . name . lower ( ) ) n = n . next if self . type == [ ] : self . type = [ "intl" , "postal" , "parcel" , "work" ] elif "dom" in self . type and "intl" in self . type : raise ValueError ( "Both 'dom' and 'intl' specified in vcard ADR" )
47	def shift ( self , x = 0 , y = 0 ) : return self . deepcopy ( self . x + x , self . y + y )
5565	def baselevels ( self ) : if "baselevels" not in self . _raw : return { } baselevels = self . _raw [ "baselevels" ] minmax = { k : v for k , v in baselevels . items ( ) if k in [ "min" , "max" ] } if not minmax : raise MapcheteConfigError ( "no min and max values given for baselevels" ) for v in minmax . values ( ) : if not isinstance ( v , int ) or v < 0 : raise MapcheteConfigError ( "invalid baselevel zoom parameter given: %s" % minmax . values ( ) ) zooms = list ( range ( minmax . get ( "min" , min ( self . zoom_levels ) ) , minmax . get ( "max" , max ( self . zoom_levels ) ) + 1 ) ) if not set ( self . zoom_levels ) . difference ( set ( zooms ) ) : raise MapcheteConfigError ( "baselevels zooms fully cover process zooms" ) return dict ( zooms = zooms , lower = baselevels . get ( "lower" , "nearest" ) , higher = baselevels . get ( "higher" , "nearest" ) , tile_pyramid = BufferedTilePyramid ( self . output_pyramid . grid , pixelbuffer = self . output_pyramid . pixelbuffer , metatiling = self . process_pyramid . metatiling ) )
13665	def update_item ( filename , item , uuid ) : with atomic_write ( os . fsencode ( str ( filename ) ) ) as temp_file : with open ( os . fsencode ( str ( filename ) ) ) as products_file : products_data = json . load ( products_file ) if 'products' in products_data [ - 1 ] : [ products_data [ i ] [ "products" ] [ 0 ] . update ( item ) for ( i , j ) in enumerate ( products_data ) if j [ "uuid" ] == str ( uuid ) ] else : [ products_data [ i ] . update ( item ) for ( i , j ) in enumerate ( products_data ) if j [ "uuid" ] == str ( uuid ) ] json . dump ( products_data , temp_file ) return True
11095	def select_by_ext ( self , ext , recursive = True ) : ext = [ ext . strip ( ) . lower ( ) for ext in ensure_list ( ext ) ] def filters ( p ) : return p . suffix . lower ( ) in ext return self . select_file ( filters , recursive )
7578	def _get_evanno_table ( self , kpops , max_var_multiple , quiet ) : kpops = sorted ( kpops ) replnliks = [ ] for kpop in kpops : reps , excluded = _concat_reps ( self , kpop , max_var_multiple , quiet ) if excluded : if not quiet : sys . stderr . write ( "[K{}] {} reps excluded (not converged) see 'max_var_multiple'.\n" . format ( kpop , excluded ) ) if reps : ninds = reps [ 0 ] . inds nreps = len ( reps ) else : ninds = nreps = 0 if not reps : print "no result files found" replnliks . append ( [ i . est_lnlik for i in reps ] ) if len ( replnliks ) > 1 : lnmean = [ np . mean ( i ) for i in replnliks ] lnstds = [ np . std ( i , ddof = 1 ) for i in replnliks ] else : lnmean = replnliks lnstds = np . nan tab = pd . DataFrame ( index = kpops , data = { "Nreps" : [ len ( i ) for i in replnliks ] , "lnPK" : [ 0 ] * len ( kpops ) , "lnPPK" : [ 0 ] * len ( kpops ) , "deltaK" : [ 0 ] * len ( kpops ) , "estLnProbMean" : lnmean , "estLnProbStdev" : lnstds , } ) for kpop in kpops [ 1 : ] : tab . loc [ kpop , "lnPK" ] = tab . loc [ kpop , "estLnProbMean" ] - tab . loc [ kpop - 1 , "estLnProbMean" ] for kpop in kpops [ 1 : - 1 ] : tab . loc [ kpop , "lnPPK" ] = abs ( tab . loc [ kpop + 1 , "lnPK" ] - tab . loc [ kpop , "lnPK" ] ) tab . loc [ kpop , "deltaK" ] = ( abs ( tab . loc [ kpop + 1 , "estLnProbMean" ] - 2.0 * tab . loc [ kpop , "estLnProbMean" ] + tab . loc [ kpop - 1 , "estLnProbMean" ] ) / tab . loc [ kpop , "estLnProbStdev" ] ) return tab
9277	def parse ( packet ) : if not isinstance ( packet , string_type_parse ) : raise TypeError ( "Expected packet to be str/unicode/bytes, got %s" , type ( packet ) ) if len ( packet ) == 0 : raise ParseError ( "packet is empty" , packet ) if isinstance ( packet , bytes ) : packet = _unicode_packet ( packet ) packet = packet . rstrip ( "\r\n" ) logger . debug ( "Parsing: %s" , packet ) try : ( head , body ) = packet . split ( ':' , 1 ) except : raise ParseError ( "packet has no body" , packet ) if len ( body ) == 0 : raise ParseError ( "packet body is empty" , packet ) parsed = { 'raw' : packet , } try : parsed . update ( parse_header ( head ) ) except ParseError as msg : raise ParseError ( str ( msg ) , packet ) packet_type = body [ 0 ] body = body [ 1 : ] if len ( body ) == 0 and packet_type != '>' : raise ParseError ( "packet body is empty after packet type character" , packet ) try : _try_toparse_body ( packet_type , body , parsed ) except ( UnknownFormat , ParseError ) as exp : exp . packet = packet raise if 'format' not in parsed : if not re . match ( r"^(AIR.*|ALL.*|AP.*|BEACON|CQ.*|GPS.*|DF.*|DGPS.*|" "DRILL.*|DX.*|ID.*|JAVA.*|MAIL.*|MICE.*|QST.*|QTH.*|" "RTCM.*|SKY.*|SPACE.*|SPC.*|SYM.*|TEL.*|TEST.*|TLM.*|" "WX.*|ZIP.*|UIDIGI)$" , parsed [ 'to' ] ) : raise UnknownFormat ( "format is not supported" , packet ) parsed . update ( { 'format' : 'beacon' , 'text' : packet_type + body , } ) logger . debug ( "Parsed ok." ) return parsed
8328	def _lastRecursiveChild ( self ) : "Finds the last element beneath this object to be parsed." lastChild = self while hasattr ( lastChild , 'contents' ) and lastChild . contents : lastChild = lastChild . contents [ - 1 ] return lastChild
1995	def _named_stream ( self , name , binary = False ) : with self . _store . save_stream ( self . _named_key ( name ) , binary = binary ) as s : yield s
9947	def new_space ( self , name = None , bases = None , formula = None , refs = None ) : space = self . _impl . model . currentspace = self . _impl . new_space ( name = name , bases = get_impls ( bases ) , formula = formula , refs = refs ) return space . interface
3005	def _get_storage_model ( ) : storage_model_settings = getattr ( django . conf . settings , 'GOOGLE_OAUTH2_STORAGE_MODEL' , None ) if storage_model_settings is not None : return ( storage_model_settings [ 'model' ] , storage_model_settings [ 'user_property' ] , storage_model_settings [ 'credentials_property' ] ) else : return None , None , None
8927	def dist ( ctx , devpi = False , egg = False , wheel = False , auto = True ) : config . load ( ) cmd = [ "python" , "setup.py" , "sdist" ] if auto : egg = sys . version_info . major == 2 try : import wheel as _ wheel = True except ImportError : wheel = False if egg : cmd . append ( "bdist_egg" ) if wheel : cmd . append ( "bdist_wheel" ) ctx . run ( "invoke clean --all build --docs test check" ) ctx . run ( ' ' . join ( cmd ) ) if devpi : ctx . run ( "devpi upload dist/*" )
4744	def dev_get_rprt ( dev_name , pugrp = None , punit = None ) : cmd = [ "nvm_cmd" , "rprt_all" , dev_name ] if not ( pugrp is None and punit is None ) : cmd = [ "nvm_cmd" , "rprt_lun" , dev_name , str ( pugrp ) , str ( punit ) ] _ , _ , _ , struct = cij . test . command_to_struct ( cmd ) if not struct : return None return struct [ "rprt_descr" ]
11827	def boggle_neighbors ( n2 , cache = { } ) : if cache . get ( n2 ) : return cache . get ( n2 ) n = exact_sqrt ( n2 ) neighbors = [ None ] * n2 for i in range ( n2 ) : neighbors [ i ] = [ ] on_top = i < n on_bottom = i >= n2 - n on_left = i % n == 0 on_right = ( i + 1 ) % n == 0 if not on_top : neighbors [ i ] . append ( i - n ) if not on_left : neighbors [ i ] . append ( i - n - 1 ) if not on_right : neighbors [ i ] . append ( i - n + 1 ) if not on_bottom : neighbors [ i ] . append ( i + n ) if not on_left : neighbors [ i ] . append ( i + n - 1 ) if not on_right : neighbors [ i ] . append ( i + n + 1 ) if not on_left : neighbors [ i ] . append ( i - 1 ) if not on_right : neighbors [ i ] . append ( i + 1 ) cache [ n2 ] = neighbors return neighbors
4239	def config_finish ( self ) : _LOGGER . info ( "Config finish" ) if not self . config_started : return True success , _ = self . _make_request ( SERVICE_DEVICE_CONFIG , "ConfigurationFinished" , { "NewStatus" : "ChangesApplied" } ) self . config_started = not success return success
3747	def calculate ( self , T , P , zs , ws , method ) : r if method == MIXING_LOG_MOLAR : mus = [ i ( T , P ) for i in self . ViscosityLiquids ] return mixing_logarithmic ( zs , mus ) elif method == MIXING_LOG_MASS : mus = [ i ( T , P ) for i in self . ViscosityLiquids ] return mixing_logarithmic ( ws , mus ) elif method == LALIBERTE_MU : ws = list ( ws ) ws . pop ( self . index_w ) return Laliberte_viscosity ( T , ws , self . wCASs ) else : raise Exception ( 'Method not valid' )
13717	def request ( self , batch , attempt = 0 ) : try : q = self . api . new_queue ( ) for msg in batch : q . add ( msg [ 'event' ] , msg [ 'value' ] , source = msg [ 'source' ] ) q . submit ( ) except : if attempt > self . retries : raise self . request ( batch , attempt + 1 )
1087	def concat ( a , b ) : "Same as a + b, for a and b sequences." if not hasattr ( a , '__getitem__' ) : msg = "'%s' object can't be concatenated" % type ( a ) . __name__ raise TypeError ( msg ) return a + b
2209	def parse_requirements_alt ( fname = 'requirements.txt' ) : import requirements from os . path import dirname , join , exists require_fpath = join ( dirname ( __file__ ) , fname ) if exists ( require_fpath ) : with open ( require_fpath , 'r' ) as file : requires = list ( requirements . parse ( file ) ) packages = [ r . name for r in requires ] return packages return [ ]
13133	def parse_domain_users ( domain_users_file , domain_groups_file ) : with open ( domain_users_file ) as f : users = json . loads ( f . read ( ) ) domain_groups = { } if domain_groups_file : with open ( domain_groups_file ) as f : groups = json . loads ( f . read ( ) ) for group in groups : sid = get_field ( group , 'objectSid' ) domain_groups [ int ( sid . split ( '-' ) [ - 1 ] ) ] = get_field ( group , 'cn' ) user_search = UserSearch ( ) count = 0 total = len ( users ) print_notification ( "Importing {} users" . format ( total ) ) for entry in users : result = parse_user ( entry , domain_groups ) user = user_search . id_to_object ( result [ 'username' ] ) user . name = result [ 'name' ] user . domain . append ( result [ 'domain' ] ) user . description = result [ 'description' ] user . groups . extend ( result [ 'groups' ] ) user . flags . extend ( result [ 'flags' ] ) user . sid = result [ 'sid' ] user . add_tag ( "domaindump" ) user . save ( ) count += 1 sys . stdout . write ( '\r' ) sys . stdout . write ( "[{}/{}]" . format ( count , total ) ) sys . stdout . flush ( ) sys . stdout . write ( '\r' ) return count
42	def wrap_deepmind_retro ( env , scale = True , frame_stack = 4 ) : env = WarpFrame ( env ) env = ClipRewardEnv ( env ) if frame_stack > 1 : env = FrameStack ( env , frame_stack ) if scale : env = ScaledFloatFrame ( env ) return env
9015	def _row ( self , values ) : row_id = self . _to_id ( values [ ID ] ) row = self . _spec . new_row ( row_id , values , self ) if SAME_AS in values : self . _delay_inheritance ( row , self . _to_id ( values [ SAME_AS ] ) ) self . _delay_instructions ( row ) self . _id_cache [ row_id ] = row return row
1532	def get_execution_state ( self , topologyName , callback = None ) : if callback : self . execution_state_watchers [ topologyName ] . append ( callback ) else : execution_state_path = self . get_execution_state_path ( topologyName ) with open ( execution_state_path ) as f : data = f . read ( ) executionState = ExecutionState ( ) executionState . ParseFromString ( data ) return executionState
8872	def match ( self , subsetLines , offsetOfSubset , fileName ) : for ( offset , l ) in enumerate ( subsetLines ) : for t in self . regex : m = t . Regex . search ( l ) if m != None : truePosition = offset + offsetOfSubset _logger . debug ( 'Found match on line {}' . format ( str ( truePosition + 1 ) ) ) _logger . debug ( 'Line is {}' . format ( l ) ) self . failed = True self . matchLocation = CheckFileParser . FileLocation ( fileName , truePosition + 1 ) raise DirectiveException ( self )
13864	def tsms ( when , tz = None ) : if not when : return None when = totz ( when , tz ) return calendar . timegm ( when . timetuple ( ) ) * 1000 + int ( round ( when . microsecond / 1000.0 ) )
3914	def _on_typing ( self , typing_message ) : self . _typing_statuses [ typing_message . user_id ] = typing_message . status self . _update ( )
9656	def get_the_node_dict ( G , name ) : for node in G . nodes ( data = True ) : if node [ 0 ] == name : return node [ 1 ]
7915	def list_all ( cls , basic = None ) : if basic is None : return [ s for s in cls . _defs ] else : return [ s . name for s in cls . _defs . values ( ) if s . basic == basic ]
1139	def wrap ( text , width = 70 , ** kwargs ) : w = TextWrapper ( width = width , ** kwargs ) return w . wrap ( text )
8296	def render ( self , size , frame , drawqueue ) : r_context = self . create_rcontext ( size , frame ) drawqueue . render ( r_context ) self . rendering_finished ( size , frame , r_context ) return r_context
8635	def get_milestones ( session , project_ids = [ ] , milestone_ids = [ ] , user_details = None , limit = 10 , offset = 0 ) : get_milestones_data = { } if milestone_ids : get_milestones_data [ 'milestones[]' ] = milestone_ids if project_ids : get_milestones_data [ 'projects[]' ] = project_ids get_milestones_data [ 'limit' ] = limit get_milestones_data [ 'offset' ] = offset if user_details : get_milestones_data . update ( user_details ) response = make_get_request ( session , 'milestones' , params_data = get_milestones_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MilestonesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
6273	def load ( self , meta : ResourceDescription ) -> Any : self . _check_meta ( meta ) self . resolve_loader ( meta ) return meta . loader_cls ( meta ) . load ( )
703	def _getStreamDef ( self , modelDescription ) : aggregationPeriod = { 'days' : 0 , 'hours' : 0 , 'microseconds' : 0 , 'milliseconds' : 0 , 'minutes' : 0 , 'months' : 0 , 'seconds' : 0 , 'weeks' : 0 , 'years' : 0 , } aggFunctionsDict = { } if 'aggregation' in modelDescription [ 'streamDef' ] : for key in aggregationPeriod . keys ( ) : if key in modelDescription [ 'streamDef' ] [ 'aggregation' ] : aggregationPeriod [ key ] = modelDescription [ 'streamDef' ] [ 'aggregation' ] [ key ] if 'fields' in modelDescription [ 'streamDef' ] [ 'aggregation' ] : for ( fieldName , func ) in modelDescription [ 'streamDef' ] [ 'aggregation' ] [ 'fields' ] : aggFunctionsDict [ fieldName ] = str ( func ) hasAggregation = False for v in aggregationPeriod . values ( ) : if v != 0 : hasAggregation = True break aggFunctionList = aggFunctionsDict . items ( ) aggregationInfo = dict ( aggregationPeriod ) aggregationInfo [ 'fields' ] = aggFunctionList streamDef = copy . deepcopy ( modelDescription [ 'streamDef' ] ) streamDef [ 'aggregation' ] = copy . deepcopy ( aggregationInfo ) return streamDef
3585	def get_all ( self , cbobjects ) : try : with self . _lock : return [ self . _metadata [ x ] for x in cbobjects ] except KeyError : raise RuntimeError ( 'Failed to find expected metadata for CoreBluetooth object!' )
11950	def _set_global_verbosity_level ( is_verbose_output = False ) : global verbose_output verbose_output = is_verbose_output if verbose_output : jocker_lgr . setLevel ( logging . DEBUG ) else : jocker_lgr . setLevel ( logging . INFO )
10470	def terminateAppByBundleId ( bundleID ) : ra = AppKit . NSRunningApplication if getattr ( ra , "runningApplicationsWithBundleIdentifier_" ) : appList = ra . runningApplicationsWithBundleIdentifier_ ( bundleID ) if appList and len ( appList ) > 0 : app = appList [ 0 ] return app and app . terminate ( ) and True or False return False
12392	def read ( self , deserialize = False , format = None ) : if deserialize : data , _ = self . deserialize ( format = format ) return data content = self . _read ( ) if not content : return '' if type ( content ) is six . binary_type : content = content . decode ( self . encoding ) return content
2382	def from_resolver ( cls , spec_resolver ) : spec_validators = cls . _get_spec_validators ( spec_resolver ) return validators . extend ( Draft4Validator , spec_validators )
4403	def mget ( self , keys , * args ) : args = list_or_args ( keys , args ) server_keys = { } ret_dict = { } for key in args : server_name = self . get_server_name ( key ) server_keys [ server_name ] = server_keys . get ( server_name , [ ] ) server_keys [ server_name ] . append ( key ) for server_name , sub_keys in iteritems ( server_keys ) : values = self . connections [ server_name ] . mget ( sub_keys ) ret_dict . update ( dict ( zip ( sub_keys , values ) ) ) result = [ ] for key in args : result . append ( ret_dict . get ( key , None ) ) return result
7856	def __response ( self , stanza ) : try : d = self . disco_class ( stanza . get_query ( ) ) self . got_it ( d ) except ValueError , e : self . error ( e )
11576	def sonar_data ( self , data ) : val = int ( ( data [ self . MSB ] << 7 ) + data [ self . LSB ] ) pin_number = data [ 0 ] with self . pymata . data_lock : sonar_pin_entry = self . active_sonar_map [ pin_number ] self . digital_response_table [ data [ self . RESPONSE_TABLE_MODE ] ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] = val if sonar_pin_entry [ 0 ] is not None : if sonar_pin_entry [ 1 ] != val : self . active_sonar_map [ pin_number ] [ 0 ] ( [ self . pymata . SONAR , pin_number , val ] ) sonar_pin_entry [ 1 ] = val self . active_sonar_map [ pin_number ] = sonar_pin_entry
11913	def git_tag ( tag ) : print ( 'Tagging "{}"' . format ( tag ) ) msg = '"Released version {}"' . format ( tag ) Popen ( [ 'git' , 'tag' , '-s' , '-m' , msg , tag ] ) . wait ( )
13038	def main ( ) : cred_search = CredentialSearch ( ) arg = argparse . ArgumentParser ( parents = [ cred_search . argparser ] , conflict_handler = 'resolve' ) arg . add_argument ( '-c' , '--count' , help = "Only show the number of results" , action = "store_true" ) arguments = arg . parse_args ( ) if arguments . count : print_line ( "Number of credentials: {}" . format ( cred_search . argument_count ( ) ) ) else : response = cred_search . get_credentials ( ) for hit in response : print_json ( hit . to_dict ( include_meta = True ) )
313	def sharpe_ratio ( returns , risk_free = 0 , period = DAILY ) : return ep . sharpe_ratio ( returns , risk_free = risk_free , period = period )
4589	def serpentine_x ( x , y , matrix ) : if y % 2 : return matrix . columns - 1 - x , y return x , y
7041	def list_lc_collections ( lcc_server ) : url = '%s/api/collections' % lcc_server try : LOGINFO ( 'getting list of recent publicly visible ' 'and owned LC collections from %s' % ( lcc_server , ) ) have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } req = Request ( url , data = None , headers = headers ) resp = urlopen ( req ) lcc_list = json . loads ( resp . read ( ) ) [ 'result' ] [ 'collections' ] return lcc_list except HTTPError as e : LOGERROR ( 'could not retrieve list of collections, ' 'URL used: %s, error code: %s, reason: %s' % ( url , e . code , e . reason ) ) return None
6958	def list_trilegal_filtersystems ( ) : print ( '%-40s %s' % ( 'FILTER SYSTEM NAME' , 'DESCRIPTION' ) ) print ( '%-40s %s' % ( '------------------' , '-----------' ) ) for key in sorted ( TRILEGAL_FILTER_SYSTEMS . keys ( ) ) : print ( '%-40s %s' % ( key , TRILEGAL_FILTER_SYSTEMS [ key ] [ 'desc' ] ) )
7987	def request_software_version ( stanza_processor , target_jid , callback , error_callback = None ) : stanza = Iq ( to_jid = target_jid , stanza_type = "get" ) payload = VersionPayload ( ) stanza . set_payload ( payload ) def wrapper ( stanza ) : payload = stanza . get_payload ( VersionPayload ) if payload is None : if error_callback : error_callback ( stanza ) else : logger . warning ( "Invalid version query response." ) else : callback ( payload ) stanza_processor . set_response_handlers ( stanza , wrapper , error_callback ) stanza_processor . send ( stanza )
498	def _addRecordToKNN ( self , record ) : knn = self . _knnclassifier . _knn prototype_idx = self . _knnclassifier . getParameter ( 'categoryRecencyList' ) category = self . _labelListToCategoryNumber ( record . anomalyLabel ) if record . ROWID in prototype_idx : knn . prototypeSetCategory ( record . ROWID , category ) return pattern = self . _getStateAnomalyVector ( record ) rowID = record . ROWID knn . learn ( pattern , category , rowID = rowID )
7485	def concat_reads ( data , subsamples , ipyclient ) : if any ( [ len ( i . files . fastqs ) > 1 for i in subsamples ] ) : start = time . time ( ) printstr = " concatenating inputs | {} | s2 |" finished = 0 catjobs = { } for sample in subsamples : if len ( sample . files . fastqs ) > 1 : catjobs [ sample . name ] = ipyclient [ 0 ] . apply ( concat_multiple_inputs , * ( data , sample ) ) else : sample . files . concat = sample . files . fastqs while 1 : finished = sum ( [ i . ready ( ) for i in catjobs . values ( ) ] ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( catjobs ) , finished , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if finished == len ( catjobs ) : print ( "" ) break for async in catjobs : if catjobs [ async ] . successful ( ) : data . samples [ async ] . files . concat = catjobs [ async ] . result ( ) else : error = catjobs [ async ] . result ( ) LOGGER . error ( "error in step2 concat %s" , error ) raise IPyradWarningExit ( "error in step2 concat: {}" . format ( error ) ) else : for sample in subsamples : sample . files . concat = sample . files . fastqs return subsamples
9928	def authenticate ( self , request , email = None , password = None , username = None ) : email = email or username try : email_instance = models . EmailAddress . objects . get ( is_verified = True , email = email ) except models . EmailAddress . DoesNotExist : return None user = email_instance . user if user . check_password ( password ) : return user return None
13073	def r_assets ( self , filetype , asset ) : if filetype in self . assets and asset in self . assets [ filetype ] and self . assets [ filetype ] [ asset ] : return send_from_directory ( directory = self . assets [ filetype ] [ asset ] , filename = asset ) abort ( 404 )
790	def jobSetStatus ( self , jobID , status , useConnectionID = True , ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET status=%%s, ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE job_id=%%s' % ( self . jobsTableName , ) sqlParams = [ status , jobID ] if useConnectionID : query += ' AND _eng_cjm_conn_id=%s' sqlParams . append ( self . _connectionID ) result = conn . cursor . execute ( query , sqlParams ) if result != 1 : raise RuntimeError ( "Tried to change the status of job %d to %s, but " "this job belongs to some other CJM" % ( jobID , status ) )
6943	def jhk_to_bmag ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , BJHK , BJH , BJK , BHK , BJ , BH , BK )
8326	def setup ( self , parent = None , previous = None ) : self . parent = parent self . previous = previous self . next = None self . previousSibling = None self . nextSibling = None if self . parent and self . parent . contents : self . previousSibling = self . parent . contents [ - 1 ] self . previousSibling . nextSibling = self
8545	def get_datacenter_by_name ( self , name , depth = 1 ) : all_data_centers = self . list_datacenters ( depth = depth ) [ 'items' ] data_center = find_item_by_name ( all_data_centers , lambda i : i [ 'properties' ] [ 'name' ] , name ) if not data_center : raise NameError ( "No data center found with name " "containing '{name}'." . format ( name = name ) ) if len ( data_center ) > 1 : raise NameError ( "Found {n} data centers with the name '{name}': {names}" . format ( n = len ( data_center ) , name = name , names = ", " . join ( d [ 'properties' ] [ 'name' ] for d in data_center ) ) ) return data_center [ 0 ]
2196	def flush ( self ) : if self . redirect is not None : self . redirect . flush ( ) super ( TeeStringIO , self ) . flush ( )
5558	def _flatten_tree ( tree , old_path = None ) : flat_tree = [ ] for key , value in tree . items ( ) : new_path = "/" . join ( [ old_path , key ] ) if old_path else key if isinstance ( value , dict ) and "format" not in value : flat_tree . extend ( _flatten_tree ( value , old_path = new_path ) ) else : flat_tree . append ( ( new_path , value ) ) return flat_tree
892	def columnForCell ( self , cell ) : self . _validateCell ( cell ) return int ( cell / self . cellsPerColumn )
4400	def _generate_ranges ( start_date , end_date ) : range_start = start_date while range_start < end_date : range_end = range_start + timedelta ( days = 60 ) yield ( range_start . strftime ( "%d/%m/%Y" ) , range_end . strftime ( "%d/%m/%Y" ) ) range_start += timedelta ( days = 30 )
10425	def infer_missing_backwards_edge ( graph , u , v , k ) : if u in graph [ v ] : for attr_dict in graph [ v ] [ u ] . values ( ) : if attr_dict == graph [ u ] [ v ] [ k ] : return graph . add_edge ( v , u , key = k , ** graph [ u ] [ v ] [ k ] )
8602	def get_user ( self , user_id , depth = 1 ) : response = self . _perform_request ( '/um/users/%s?depth=%s' % ( user_id , str ( depth ) ) ) return response
2715	def create ( self , ** kwargs ) : for attr in kwargs . keys ( ) : setattr ( self , attr , kwargs [ attr ] ) params = { "name" : self . name } output = self . get_data ( "tags" , type = "POST" , params = params ) if output : self . name = output [ 'tag' ] [ 'name' ] self . resources = output [ 'tag' ] [ 'resources' ]
1651	def _ClassifyInclude ( fileinfo , include , is_system ) : is_cpp_h = include in _CPP_HEADERS if is_system and os . path . splitext ( include ) [ 1 ] in [ '.hpp' , '.hxx' , '.h++' ] : is_system = False if is_system : if is_cpp_h : return _CPP_SYS_HEADER else : return _C_SYS_HEADER target_dir , target_base = ( os . path . split ( _DropCommonSuffixes ( fileinfo . RepositoryName ( ) ) ) ) include_dir , include_base = os . path . split ( _DropCommonSuffixes ( include ) ) target_dir_pub = os . path . normpath ( target_dir + '/../public' ) target_dir_pub = target_dir_pub . replace ( '\\' , '/' ) if target_base == include_base and ( include_dir == target_dir or include_dir == target_dir_pub ) : return _LIKELY_MY_HEADER target_first_component = _RE_FIRST_COMPONENT . match ( target_base ) include_first_component = _RE_FIRST_COMPONENT . match ( include_base ) if ( target_first_component and include_first_component and target_first_component . group ( 0 ) == include_first_component . group ( 0 ) ) : return _POSSIBLE_MY_HEADER return _OTHER_HEADER
11783	def add_example ( self , example ) : "Add an example to the list of examples, checking it first." self . check_example ( example ) self . examples . append ( example )
9264	def filter_merged_pull_requests ( self , pull_requests ) : if self . options . verbose : print ( "Fetching merge date for pull requests..." ) closed_pull_requests = self . fetcher . fetch_closed_pull_requests ( ) if not pull_requests : return [ ] pulls = copy . deepcopy ( pull_requests ) for pr in pulls : fetched_pr = None for fpr in closed_pull_requests : if fpr [ 'number' ] == pr [ 'number' ] : fetched_pr = fpr if fetched_pr : pr [ 'merged_at' ] = fetched_pr [ 'merged_at' ] closed_pull_requests . remove ( fetched_pr ) for pr in pulls : if not pr . get ( 'merged_at' ) : pulls . remove ( pr ) return pulls
2155	def set_or_reset_runtime_param ( self , key , value ) : if self . _runtime . has_option ( 'general' , key ) : self . _runtime = self . _new_parser ( ) if value is None : return settings . _runtime . set ( 'general' , key . replace ( 'tower_' , '' ) , six . text_type ( value ) )
13198	def read ( cls , root_tex_path ) : root_dir = os . path . dirname ( root_tex_path ) tex_source = read_tex_file ( root_tex_path ) tex_macros = get_macros ( tex_source ) tex_source = replace_macros ( tex_source , tex_macros ) return cls ( tex_source , root_dir = root_dir )
5210	def bdp ( tickers , flds , ** kwargs ) : logger = logs . get_logger ( bdp , level = kwargs . pop ( 'log' , logs . LOG_LEVEL ) ) con , _ = create_connection ( ) ovrds = assist . proc_ovrds ( ** kwargs ) logger . info ( f'loading reference data from Bloomberg:\n' f'{assist.info_qry(tickers=tickers, flds=flds)}' ) data = con . ref ( tickers = tickers , flds = flds , ovrds = ovrds ) if not kwargs . get ( 'cache' , False ) : return [ data ] qry_data = [ ] for r , snap in data . iterrows ( ) : subset = [ r ] data_file = storage . ref_file ( ticker = snap . ticker , fld = snap . field , ext = 'pkl' , ** kwargs ) if data_file : if not files . exists ( data_file ) : qry_data . append ( data . iloc [ subset ] ) files . create_folder ( data_file , is_file = True ) data . iloc [ subset ] . to_pickle ( data_file ) return qry_data
10989	def link_zscale ( st ) : psf = st . get ( 'psf' ) psf . param_dict [ 'zscale' ] = psf . param_dict [ 'psf-zscale' ] psf . params [ psf . params . index ( 'psf-zscale' ) ] = 'zscale' psf . global_zscale = True psf . param_dict . pop ( 'psf-zscale' ) st . trigger_parameter_change ( ) st . reset ( )
10522	def oneleft ( self , window_name , object_name , iterations ) : if not self . verifyscrollbarhorizontal ( window_name , object_name ) : raise LdtpServerException ( 'Object not horizontal scrollbar' ) object_handle = self . _get_object_handle ( window_name , object_name ) i = 0 minValue = 1.0 / 8 flag = False while i < iterations : if object_handle . AXValue <= 0 : raise LdtpServerException ( 'Minimum limit reached' ) object_handle . AXValue -= minValue time . sleep ( 1.0 / 100 ) flag = True i += 1 if flag : return 1 else : raise LdtpServerException ( 'Unable to decrease scrollbar' )
7273	def set_rate ( self , rate ) : self . _rate = self . _player_interface_property ( 'Rate' , dbus . Double ( rate ) ) return self . _rate
6920	def _autocorr_func2 ( mags , lag , maglen , magmed , magstd ) : lagindex = nparange ( 0 , maglen - lag ) products = ( mags [ lagindex ] - magmed ) * ( mags [ lagindex + lag ] - magmed ) autocovarfunc = npsum ( products ) / lagindex . size varfunc = npsum ( ( mags [ lagindex ] - magmed ) * ( mags [ lagindex ] - magmed ) ) / mags . size acorr = autocovarfunc / varfunc return acorr
8660	def is_valid_identifier ( name ) : if not isinstance ( name , str ) : return False if '\n' in name : return False if name . strip ( ) != name : return False try : code = compile ( '\n{0}=None' . format ( name ) , filename = '<string>' , mode = 'single' ) exec ( code ) return True except SyntaxError : return False
8578	def list_servers ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
6024	def convolve ( self , array ) : if self . shape [ 0 ] % 2 == 0 or self . shape [ 1 ] % 2 == 0 : raise exc . KernelException ( "PSF Kernel must be odd" ) return scipy . signal . convolve2d ( array , self , mode = 'same' )
11706	def reproduce_sexually ( self , egg_donor , sperm_donor ) : egg_word = random . choice ( egg_donor . genome ) egg = self . generate_gamete ( egg_word ) sperm_word = random . choice ( sperm_donor . genome ) sperm = self . generate_gamete ( sperm_word ) self . genome = list ( set ( egg + sperm ) ) self . parents = [ egg_donor . name , sperm_donor . name ] self . generation = max ( egg_donor . generation , sperm_donor . generation ) + 1 sum_ = egg_donor . divinity + sperm_donor . divinity self . divinity = int ( npchoice ( divinities , 1 , p = p_divinity [ sum_ ] ) [ 0 ] )
1708	def connect ( command , data = None , env = None , cwd = None ) : command_str = expand_args ( command ) . pop ( ) environ = dict ( os . environ ) environ . update ( env or { } ) process = subprocess . Popen ( command_str , universal_newlines = True , shell = False , env = environ , stdin = subprocess . PIPE , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = 0 , cwd = cwd , ) return ConnectedCommand ( process = process )
13026	def combine_files ( self , f1 , f2 , f3 ) : with open ( os . path . join ( self . datadir , f3 ) , 'wb' ) as new_file : with open ( os . path . join ( self . datadir , f1 ) , 'rb' ) as file_1 : new_file . write ( file_1 . read ( ) ) with open ( os . path . join ( self . datadir , f2 ) , 'rb' ) as file_2 : new_file . write ( file_2 . read ( ) )
2687	def get_library_config ( name ) : try : proc = Popen ( [ 'pkg-config' , '--cflags' , '--libs' , name ] , stdout = PIPE , stderr = PIPE ) except OSError : print ( 'pkg-config is required for building PyAV' ) exit ( 1 ) raw_cflags , err = proc . communicate ( ) if proc . wait ( ) : return known , unknown = parse_cflags ( raw_cflags . decode ( 'utf8' ) ) if unknown : print ( "pkg-config returned flags we don't understand: {}" . format ( unknown ) ) exit ( 1 ) return known
1246	def disconnect ( self ) : if not self . socket : logging . warning ( "No active socket to close!" ) return self . socket . close ( ) self . socket = None
6685	def update ( kernel = False ) : manager = MANAGER cmds = { 'yum -y --color=never' : { False : '--exclude=kernel* update' , True : 'update' } } cmd = cmds [ manager ] [ kernel ] run_as_root ( "%(manager)s %(cmd)s" % locals ( ) )
