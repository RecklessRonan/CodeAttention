12749	def load_asf ( self , source , ** kwargs ) : if hasattr ( source , 'read' ) : p = parser . parse_asf ( source , self . world , self . jointgroup , ** kwargs ) else : with open ( source ) as handle : p = parser . parse_asf ( handle , self . world , self . jointgroup , ** kwargs ) self . bodies = p . bodies self . joints = p . joints self . set_pid_params ( kp = 0.999 / self . world . dt )
7220	def to_geotiff ( arr , path = './output.tif' , proj = None , spec = None , bands = None , ** kwargs ) : assert has_rasterio , "To create geotiff images please install rasterio" try : img_md = arr . rda . metadata [ "image" ] x_size = img_md [ "tileXSize" ] y_size = img_md [ "tileYSize" ] except ( AttributeError , KeyError ) : x_size = kwargs . get ( "chunk_size" , 256 ) y_size = kwargs . get ( "chunk_size" , 256 ) try : tfm = kwargs [ 'transform' ] if 'transform' in kwargs else arr . affine except : tfm = None dtype = arr . dtype . name if arr . dtype . name != 'int8' else 'uint8' if spec is not None and spec . lower ( ) == 'rgb' : if bands is None : bands = arr . _rgb_bands if not arr . options . get ( 'dra' ) : from gbdxtools . rda . interface import RDA rda = RDA ( ) dra = rda . HistogramDRA ( arr ) arr = dra . aoi ( bbox = arr . bounds ) arr = arr [ bands , ... ] . astype ( np . uint8 ) dtype = 'uint8' else : if bands is not None : arr = arr [ bands , ... ] meta = { 'width' : arr . shape [ 2 ] , 'height' : arr . shape [ 1 ] , 'count' : arr . shape [ 0 ] , 'dtype' : dtype , 'driver' : 'GTiff' , 'transform' : tfm } if proj is not None : meta [ "crs" ] = { 'init' : proj } if "tiled" in kwargs and kwargs [ "tiled" ] : meta . update ( blockxsize = x_size , blockysize = y_size , tiled = "yes" ) with rasterio . open ( path , "w" , ** meta ) as dst : writer = rio_writer ( dst ) result = store ( arr , writer , compute = False ) result . compute ( scheduler = threaded_get ) return path
1914	def enqueue ( self , state ) : state_id = self . _workspace . save_state ( state ) self . put ( state_id ) self . _publish ( 'did_enqueue_state' , state_id , state ) return state_id
122	def _augment_images_worker ( self , augseq , queue_source , queue_result , seedval ) : np . random . seed ( seedval ) random . seed ( seedval ) augseq . reseed ( seedval ) ia . seed ( seedval ) loader_finished = False while not loader_finished : try : batch_str = queue_source . get ( timeout = 0.1 ) batch = pickle . loads ( batch_str ) if batch is None : loader_finished = True queue_source . put ( pickle . dumps ( None , protocol = - 1 ) ) else : batch_aug = augseq . augment_batch ( batch ) batch_str = pickle . dumps ( batch_aug , protocol = - 1 ) queue_result . put ( batch_str ) except QueueEmpty : time . sleep ( 0.01 ) queue_result . put ( pickle . dumps ( None , protocol = - 1 ) ) time . sleep ( 0.01 )
1485	def run ( self , name , config , builder ) : if not isinstance ( name , str ) : raise RuntimeError ( "Name has to be a string type" ) if not isinstance ( config , Config ) : raise RuntimeError ( "config has to be a Config type" ) if not isinstance ( builder , Builder ) : raise RuntimeError ( "builder has to be a Builder type" ) bldr = TopologyBuilder ( name = name ) builder . build ( bldr ) bldr . set_config ( config . _api_config ) bldr . build_and_submit ( )
13221	def breakfast ( self , message = "Breakfast is ready" , shout : bool = False ) : return self . helper . output ( message , shout )
9539	def number_range_inclusive ( min , max , type = float ) : def checker ( v ) : if type ( v ) < min or type ( v ) > max : raise ValueError ( v ) return checker
572	def rCopy ( d , f = identityConversion , discardNoneKeys = True , deepCopy = True ) : if deepCopy : d = copy . deepcopy ( d ) newDict = { } toCopy = [ ( k , v , newDict , ( ) ) for k , v in d . iteritems ( ) ] while len ( toCopy ) > 0 : k , v , d , prevKeys = toCopy . pop ( ) prevKeys = prevKeys + ( k , ) if isinstance ( v , dict ) : d [ k ] = dict ( ) toCopy [ 0 : 0 ] = [ ( innerK , innerV , d [ k ] , prevKeys ) for innerK , innerV in v . iteritems ( ) ] else : newV = f ( v , prevKeys ) if not discardNoneKeys or newV is not None : d [ k ] = newV return newDict
11002	def pack_args ( self ) : mapper = { 'psf-kfki' : 'kfki' , 'psf-alpha' : 'alpha' , 'psf-n2n1' : 'n2n1' , 'psf-sigkf' : 'sigkf' , 'psf-sph6-ab' : 'sph6_ab' , 'psf-laser-wavelength' : 'laser_wavelength' , 'psf-pinhole-width' : 'pinhole_width' } bads = [ self . zscale , 'psf-zslab' ] d = { } for k , v in iteritems ( mapper ) : if k in self . param_dict : d [ v ] = self . param_dict [ k ] d . update ( { 'polar_angle' : self . polar_angle , 'normalize' : self . normalize , 'include_K3_det' : self . use_J1 } ) if self . polychromatic : d . update ( { 'nkpts' : self . nkpts } ) d . update ( { 'k_dist' : self . k_dist } ) if self . do_pinhole : d . update ( { 'nlpts' : self . num_line_pts } ) d . update ( { 'use_laggauss' : True } ) return d
11355	def record_xml_output ( rec , pretty = True ) : from . html_utils import MathMLParser ret = etree . tostring ( rec , xml_declaration = False ) ret = re . sub ( "(&lt;)(([\/]?{0}))" . format ( "|[\/]?" . join ( MathMLParser . mathml_elements ) ) , '<\g<2>' , ret ) ret = re . sub ( "&gt;" , '>' , ret ) if pretty : ret = ret . replace ( '</datafield>' , ' </datafield>\n' ) ret = re . sub ( r'<datafield(.*?)>' , r' <datafield\1>\n' , ret ) ret = ret . replace ( '</subfield>' , '</subfield>\n' ) ret = ret . replace ( '<subfield' , ' <subfield' ) ret = ret . replace ( 'record>' , 'record>\n' ) return ret
1375	def parse_override_config ( namespace ) : overrides = dict ( ) for config in namespace : kv = config . split ( "=" ) if len ( kv ) != 2 : raise Exception ( "Invalid config property format (%s) expected key=value" % config ) if kv [ 1 ] in [ 'true' , 'True' , 'TRUE' ] : overrides [ kv [ 0 ] ] = True elif kv [ 1 ] in [ 'false' , 'False' , 'FALSE' ] : overrides [ kv [ 0 ] ] = False else : overrides [ kv [ 0 ] ] = kv [ 1 ] return overrides
1612	def ProcessGlobalSuppresions ( lines ) : for line in lines : if _SEARCH_C_FILE . search ( line ) : for category in _DEFAULT_C_SUPPRESSED_CATEGORIES : _global_error_suppressions [ category ] = True if _SEARCH_KERNEL_FILE . search ( line ) : for category in _DEFAULT_KERNEL_SUPPRESSED_CATEGORIES : _global_error_suppressions [ category ] = True
12558	def drain_rois ( img ) : img_data = get_img_data ( img ) out = np . zeros ( img_data . shape , dtype = img_data . dtype ) krn_dim = [ 3 ] * img_data . ndim kernel = np . ones ( krn_dim , dtype = int ) vals = np . unique ( img_data ) vals = vals [ vals != 0 ] for i in vals : roi = img_data == i hits = scn . binary_hit_or_miss ( roi , kernel ) roi [ hits ] = 0 out [ roi > 0 ] = i return out
5810	def _parse_hello_extensions ( data ) : if data == b'' : return extentions_length = int_from_bytes ( data [ 0 : 2 ] ) extensions_start = 2 extensions_end = 2 + extentions_length pointer = extensions_start while pointer < extensions_end : extension_type = int_from_bytes ( data [ pointer : pointer + 2 ] ) extension_length = int_from_bytes ( data [ pointer + 2 : pointer + 4 ] ) yield ( extension_type , data [ pointer + 4 : pointer + 4 + extension_length ] ) pointer += 4 + extension_length
3879	async def _handle_set_typing_notification ( self , set_typing_notification ) : conv_id = set_typing_notification . conversation_id . id res = parsers . parse_typing_status_message ( set_typing_notification ) await self . on_typing . fire ( res ) try : conv = await self . _get_or_fetch_conversation ( conv_id ) except exceptions . NetworkError : logger . warning ( 'Failed to fetch conversation for typing notification: %s' , conv_id ) else : await conv . on_typing . fire ( res )
285	def plot_drawdown_underwater ( returns , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . percentage ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) df_cum_rets = ep . cum_returns ( returns , starting_value = 1.0 ) running_max = np . maximum . accumulate ( df_cum_rets ) underwater = - 100 * ( ( running_max - df_cum_rets ) / running_max ) ( underwater ) . plot ( ax = ax , kind = 'area' , color = 'coral' , alpha = 0.7 , ** kwargs ) ax . set_ylabel ( 'Drawdown' ) ax . set_title ( 'Underwater plot' ) ax . set_xlabel ( '' ) return ax
504	def _categoryToLabelList ( self , category ) : if category is None : return [ ] labelList = [ ] labelNum = 0 while category > 0 : if category % 2 == 1 : labelList . append ( self . saved_categories [ labelNum ] ) labelNum += 1 category = category >> 1 return labelList
12923	def start_tag ( self ) : direct_attributes = ( attribute . render ( self ) for attribute in self . render_attributes ) attributes = ( ) if hasattr ( self , '_attributes' ) : attributes = ( '{0}="{1}"' . format ( key , value ) for key , value in self . attributes . items ( ) if value ) rendered_attributes = " " . join ( filter ( bool , chain ( direct_attributes , attributes ) ) ) return '<{0}{1}{2}{3}>' . format ( self . tag , ' ' if rendered_attributes else '' , rendered_attributes , ' /' if self . tag_self_closes else "" )
259	def compute_exposures ( positions , factor_loadings , stack_positions = True , pos_in_dollars = True ) : if stack_positions : positions = _stack_positions ( positions , pos_in_dollars = pos_in_dollars ) return ep . compute_exposures ( positions , factor_loadings )
5670	def temporal_network ( gtfs , start_time_ut = None , end_time_ut = None , route_type = None ) : events_df = gtfs . get_transit_events ( start_time_ut = start_time_ut , end_time_ut = end_time_ut , route_type = route_type ) events_df . drop ( 'to_seq' , 1 , inplace = True ) events_df . drop ( 'shape_id' , 1 , inplace = True ) events_df . drop ( 'duration' , 1 , inplace = True ) events_df . drop ( 'route_id' , 1 , inplace = True ) events_df . rename ( columns = { 'from_seq' : "seq" } , inplace = True ) return events_df
10622	def get_element_mass ( self , element ) : result = numpy . zeros ( 1 ) for compound in self . material . compounds : result += self . get_compound_mass ( compound ) * numpy . array ( stoich . element_mass_fractions ( compound , [ element ] ) ) return result [ 0 ]
10211	def cmdclass ( path , enable = None , user = None ) : import warnings from setuptools . command . install import install from setuptools . command . develop import develop from os . path import dirname , join , exists , realpath from traceback import extract_stack try : from notebook . nbextensions import install_nbextension from notebook . services . config import ConfigManager except ImportError : try : from IPython . html . nbextensions import install_nbextension from IPython . html . services . config import ConfigManager except ImportError : warnings . warn ( "No jupyter notebook found in your environment. " "Hence jupyter nbextensions were not installed. " "If you would like to have them," "please issue 'pip install jupyter'." ) return { } if user is None : user = not _is_root ( ) calling_file = extract_stack ( ) [ - 2 ] [ 0 ] fullpath = realpath ( calling_file ) if not exists ( fullpath ) : raise Exception ( 'Could not find path of setup file.' ) extension_dir = join ( dirname ( fullpath ) , path ) def run_nbextension_install ( develop ) : import sys sysprefix = hasattr ( sys , 'real_prefix' ) if sysprefix : install_nbextension ( extension_dir , symlink = develop , sys_prefix = sysprefix ) else : install_nbextension ( extension_dir , symlink = develop , user = user ) if enable is not None : print ( "Enabling the extension ..." ) cm = ConfigManager ( ) cm . update ( 'notebook' , { "load_extensions" : { enable : True } } ) class InstallCommand ( install ) : def run ( self ) : print ( "Installing Python module..." ) install . run ( self ) print ( "Installing nbextension ..." ) run_nbextension_install ( False ) class DevelopCommand ( develop ) : def run ( self ) : print ( "Installing Python module..." ) develop . run ( self ) print ( "Installing nbextension ..." ) run_nbextension_install ( True ) return { 'install' : InstallCommand , 'develop' : DevelopCommand , }
7933	def _connect ( self , server = None , port = None ) : if self . me . node or self . me . resource : raise Value ( "Component JID may have only domain defined" ) if not server : server = self . server if not port : port = self . port if not server or not port : raise ValueError ( "Server or port not given" ) Stream . _connect ( self , server , port , None , self . me )
10089	def files ( self ) : files_ = super ( Deposit , self ) . files if files_ : sort_by_ = files_ . sort_by def sort_by ( * args , ** kwargs ) : if 'draft' != self . status : raise PIDInvalidAction ( ) return sort_by_ ( * args , ** kwargs ) files_ . sort_by = sort_by return files_
7594	def get_player ( self , * tags : crtag , ** params : keys ) : url = self . api . PLAYER + '/' + ',' . join ( tags ) return self . _get_model ( url , FullPlayer , ** params )
2433	def set_created_date ( self , doc , created ) : if not self . created_date_set : self . created_date_set = True date = utils . datetime_from_iso_format ( created ) if date is not None : doc . creation_info . created = date return True else : raise SPDXValueError ( 'CreationInfo::Date' ) else : raise CardinalityError ( 'CreationInfo::Created' )
1495	def find_closing_braces ( self , query ) : if query [ 0 ] != '(' : raise Exception ( "Trying to find closing braces for no opening braces" ) num_open_braces = 0 for i in range ( len ( query ) ) : c = query [ i ] if c == '(' : num_open_braces += 1 elif c == ')' : num_open_braces -= 1 if num_open_braces == 0 : return i raise Exception ( "No closing braces found" )
2999	def sectorPerformanceDF ( token = '' , version = '' ) : df = pd . DataFrame ( sectorPerformance ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'name' ) return df
11688	def get_changeset ( changeset ) : url = 'https://www.openstreetmap.org/api/0.6/changeset/{}/download' . format ( changeset ) return ET . fromstring ( requests . get ( url ) . content )
1874	def MOVHPD ( cpu , dest , src ) : if src . size == 128 : assert dest . size == 64 dest . write ( Operators . EXTRACT ( src . read ( ) , 64 , 64 ) ) else : assert src . size == 64 and dest . size == 128 value = Operators . EXTRACT ( dest . read ( ) , 0 , 64 ) dest . write ( Operators . CONCAT ( 128 , src . read ( ) , value ) )
13850	def ensure_dir_exists ( func ) : "wrap a function that returns a dir, making sure it exists" @ functools . wraps ( func ) def make_if_not_present ( ) : dir = func ( ) if not os . path . isdir ( dir ) : os . makedirs ( dir ) return dir return make_if_not_present
239	def create_position_tear_sheet ( returns , positions , show_and_plot_top_pos = 2 , hide_positions = False , return_fig = False , sector_mappings = None , transactions = None , estimate_intraday = 'infer' ) : positions = utils . check_intraday ( estimate_intraday , returns , positions , transactions ) if hide_positions : show_and_plot_top_pos = 0 vertical_sections = 7 if sector_mappings is not None else 6 fig = plt . figure ( figsize = ( 14 , vertical_sections * 6 ) ) gs = gridspec . GridSpec ( vertical_sections , 3 , wspace = 0.5 , hspace = 0.5 ) ax_exposures = plt . subplot ( gs [ 0 , : ] ) ax_top_positions = plt . subplot ( gs [ 1 , : ] , sharex = ax_exposures ) ax_max_median_pos = plt . subplot ( gs [ 2 , : ] , sharex = ax_exposures ) ax_holdings = plt . subplot ( gs [ 3 , : ] , sharex = ax_exposures ) ax_long_short_holdings = plt . subplot ( gs [ 4 , : ] ) ax_gross_leverage = plt . subplot ( gs [ 5 , : ] , sharex = ax_exposures ) positions_alloc = pos . get_percent_alloc ( positions ) plotting . plot_exposures ( returns , positions , ax = ax_exposures ) plotting . show_and_plot_top_positions ( returns , positions_alloc , show_and_plot = show_and_plot_top_pos , hide_positions = hide_positions , ax = ax_top_positions ) plotting . plot_max_median_position_concentration ( positions , ax = ax_max_median_pos ) plotting . plot_holdings ( returns , positions_alloc , ax = ax_holdings ) plotting . plot_long_short_holdings ( returns , positions_alloc , ax = ax_long_short_holdings ) plotting . plot_gross_leverage ( returns , positions , ax = ax_gross_leverage ) if sector_mappings is not None : sector_exposures = pos . get_sector_exposures ( positions , sector_mappings ) if len ( sector_exposures . columns ) > 1 : sector_alloc = pos . get_percent_alloc ( sector_exposures ) sector_alloc = sector_alloc . drop ( 'cash' , axis = 'columns' ) ax_sector_alloc = plt . subplot ( gs [ 6 , : ] , sharex = ax_exposures ) plotting . plot_sector_allocations ( returns , sector_alloc , ax = ax_sector_alloc ) for ax in fig . axes : plt . setp ( ax . get_xticklabels ( ) , visible = True ) if return_fig : return fig
8242	def outline ( path , colors , precision = 0.4 , continuous = True ) : def _point_count ( path , precision ) : return max ( int ( path . length * precision * 0.5 ) , 10 ) n = sum ( [ _point_count ( contour , precision ) for contour in path . contours ] ) contour_i = 0 contour_n = len ( path . contours ) - 1 if contour_n == 0 : continuous = False i = 0 for contour in path . contours : if not continuous : i = 0 j = _point_count ( contour , precision ) first = True for pt in contour . points ( j ) : if first : first = False else : if not continuous : clr = float ( i ) / j * len ( colors ) else : clr = float ( i ) / n * len ( colors ) - 1 * contour_i / contour_n _ctx . stroke ( colors [ int ( clr ) ] ) _ctx . line ( x0 , y0 , pt . x , pt . y ) x0 = pt . x y0 = pt . y i += 1 pt = contour . point ( 0.9999999 ) _ctx . line ( x0 , y0 , pt . x , pt . y ) contour_i += 1
7974	def stop ( self , join = False , timeout = None ) : logger . debug ( "Closing the io handlers..." ) for handler in self . io_handlers : handler . close ( ) if self . event_thread and self . event_thread . is_alive ( ) : logger . debug ( "Sending the QUIT signal" ) self . event_queue . put ( QUIT ) logger . debug ( " sent" ) threads = self . io_threads + self . timeout_threads for thread in threads : logger . debug ( "Stopping thread: {0!r}" . format ( thread ) ) thread . stop ( ) if not join : return if self . event_thread : threads . append ( self . event_thread ) if timeout is None : for thread in threads : thread . join ( ) else : timeout1 = ( timeout * 0.01 ) / len ( threads ) threads_left = [ ] for thread in threads : logger . debug ( "Quick-joining thread {0!r}..." . format ( thread ) ) thread . join ( timeout1 ) if thread . is_alive ( ) : logger . debug ( " thread still alive" . format ( thread ) ) threads_left . append ( thread ) if threads_left : timeout2 = ( timeout * 0.99 ) / len ( threads_left ) for thread in threads_left : logger . debug ( "Joining thread {0!r}..." . format ( thread ) ) thread . join ( timeout2 ) self . io_threads = [ ] self . event_thread = None
4093	def addBorrowers ( self , * borrowers ) : self . _borrowers . extend ( borrowers ) debug . logger & debug . flagCompiler and debug . logger ( 'current MIB borrower(s): %s' % ', ' . join ( [ str ( x ) for x in self . _borrowers ] ) ) return self
1057	def add_extension ( module , name , code ) : code = int ( code ) if not 1 <= code <= 0x7fffffff : raise ValueError , "code out of range" key = ( module , name ) if ( _extension_registry . get ( key ) == code and _inverted_registry . get ( code ) == key ) : return if key in _extension_registry : raise ValueError ( "key %s is already registered with code %s" % ( key , _extension_registry [ key ] ) ) if code in _inverted_registry : raise ValueError ( "code %s is already in use for key %s" % ( code , _inverted_registry [ code ] ) ) _extension_registry [ key ] = code _inverted_registry [ code ] = key
10932	def get_termination_stats ( self , get_cos = True ) : delta_vals = self . _last_vals - self . param_vals delta_err = self . _last_error - self . error frac_err = delta_err / self . error to_return = { 'delta_vals' : delta_vals , 'delta_err' : delta_err , 'num_iter' : 1 * self . _num_iter , 'frac_err' : frac_err , 'error' : self . error , 'exp_err' : self . _exp_err } if get_cos : model_cosine = self . calc_model_cosine ( ) to_return . update ( { 'model_cosine' : model_cosine } ) return to_return
8865	def complete ( code , line , column , path , encoding , prefix ) : ret_val = [ ] try : script = jedi . Script ( code , line + 1 , column , path , encoding ) completions = script . completions ( ) print ( 'completions: %r' % completions ) except jedi . NotFoundError : completions = [ ] for completion in completions : ret_val . append ( { 'name' : completion . name , 'icon' : icon_from_typename ( completion . name , completion . type ) , 'tooltip' : completion . description } ) return ret_val
7633	def __get_dtype ( typespec ) : if 'type' in typespec : return __TYPE_MAP__ . get ( typespec [ 'type' ] , np . object_ ) elif 'enum' in typespec : return np . object_ elif 'oneOf' in typespec : types = [ __get_dtype ( v ) for v in typespec [ 'oneOf' ] ] if all ( [ t == types [ 0 ] for t in types ] ) : return types [ 0 ] return np . object_
1123	def Tok ( kind , loc = None ) : @ llrule ( loc , lambda parser : [ kind ] ) def rule ( parser ) : return parser . _accept ( kind ) return rule
9854	def _detect_byteorder ( ccp4file ) : bsaflag = None ccp4file . seek ( 52 * 4 ) mapbin = ccp4file . read ( 4 ) for flag in '@=<>' : mapstr = struct . unpack ( flag + '4s' , mapbin ) [ 0 ] . decode ( 'utf-8' ) if mapstr . upper ( ) == 'MAP ' : bsaflag = flag break else : raise TypeError ( "Cannot decode header --- corrupted or wrong format?" ) ccp4file . seek ( 0 ) return bsaflag
11061	def send_message ( self , channel , text , thread = None , reply_broadcast = None ) : if isinstance ( channel , SlackRoomIMBase ) : channel = channel . id self . log . debug ( "Trying to send to %s: %s" , channel , text ) self . sc . rtm_send_message ( channel , text , thread = thread , reply_broadcast = reply_broadcast )
1457	def valid_java_classpath ( classpath ) : paths = classpath . split ( ':' ) for path_entry in paths : if not valid_path ( path_entry . strip ( ) ) : return False return True
8438	def _patched_run_hook ( hook_name , project_dir , context ) : if hook_name == 'post_gen_project' : with temple . utils . cd ( project_dir ) : temple . utils . write_temple_config ( context [ 'cookiecutter' ] , context [ 'template' ] , context [ 'version' ] ) return cc_hooks . run_hook ( hook_name , project_dir , context )
9486	def ensure_instruction ( instruction : int ) -> bytes : if PY36 : return instruction . to_bytes ( 2 , byteorder = "little" ) else : return instruction . to_bytes ( 1 , byteorder = "little" )
753	def _translateMetricsToJSON ( self , metrics , label ) : metricsDict = metrics def _mapNumpyValues ( obj ) : import numpy if isinstance ( obj , numpy . float32 ) : return float ( obj ) elif isinstance ( obj , numpy . bool_ ) : return bool ( obj ) elif isinstance ( obj , numpy . ndarray ) : return obj . tolist ( ) else : raise TypeError ( "UNEXPECTED OBJ: %s; class=%s" % ( obj , obj . __class__ ) ) jsonString = json . dumps ( metricsDict , indent = 4 , default = _mapNumpyValues ) return jsonString
4416	async def play ( self , track_index : int = 0 , ignore_shuffle : bool = False ) : if self . repeat and self . current : self . queue . append ( self . current ) self . previous = self . current self . current = None self . position = 0 self . paused = False if not self . queue : await self . stop ( ) await self . _lavalink . dispatch_event ( QueueEndEvent ( self ) ) else : if self . shuffle and not ignore_shuffle : track = self . queue . pop ( randrange ( len ( self . queue ) ) ) else : track = self . queue . pop ( min ( track_index , len ( self . queue ) - 1 ) ) self . current = track await self . _lavalink . ws . send ( op = 'play' , guildId = self . guild_id , track = track . track ) await self . _lavalink . dispatch_event ( TrackStartEvent ( self , track ) )
13506	def get_position ( self , position_id ) : url = "/2/positions/%s" % position_id return self . position_from_json ( self . _get_resource ( url ) [ "position" ] )
9116	def reset_cleansers ( confirm = True ) : if value_asbool ( confirm ) and not yesno ( ) : exit ( "Glad I asked..." ) get_vars ( ) cleanser_count = AV [ 'ploy_cleanser_count' ] fab . run ( 'ezjail-admin stop worker' ) for cleanser_index in range ( cleanser_count ) : cindex = '{:02d}' . format ( cleanser_index + 1 ) fab . run ( 'ezjail-admin stop cleanser_{cindex}' . format ( cindex = cindex ) ) with fab . warn_only ( ) : fab . run ( 'zfs destroy tank/jails/cleanser_{cindex}@jdispatch_rollback' . format ( cindex = cindex ) ) fab . run ( 'ezjail-admin delete -fw cleanser_{cindex}' . format ( cindex = cindex ) ) fab . run ( 'umount -f /usr/jails/cleanser_{cindex}' . format ( cindex = cindex ) ) fab . run ( 'rm -rf /usr/jails/cleanser_{cindex}' . format ( cindex = cindex ) ) with fab . warn_only ( ) : fab . run ( 'zfs destroy -R tank/jails/cleanser@clonesource' ) fab . run ( 'ezjail-admin start worker' ) fab . run ( 'ezjail-admin stop cleanser' ) fab . run ( 'ezjail-admin start cleanser' )
1033	def encode ( input , output ) : while True : s = input . read ( MAXBINSIZE ) if not s : break while len ( s ) < MAXBINSIZE : ns = input . read ( MAXBINSIZE - len ( s ) ) if not ns : break s += ns line = binascii . b2a_base64 ( s ) output . write ( line )
10561	def _mutagen_fields_to_single_value ( metadata ) : return dict ( ( k , v [ 0 ] ) for k , v in metadata . items ( ) if v )
4954	def _disconnect_user_post_save_for_migrations ( self , sender , ** kwargs ) : from django . db . models . signals import post_save post_save . disconnect ( sender = self . auth_user_model , dispatch_uid = USER_POST_SAVE_DISPATCH_UID )
7881	def _split_qname ( self , name , is_element ) : if name . startswith ( u"{" ) : namespace , name = name [ 1 : ] . split ( u"}" , 1 ) if namespace in STANZA_NAMESPACES : namespace = self . stanza_namespace elif is_element : raise ValueError ( u"Element with no namespace: {0!r}" . format ( name ) ) else : namespace = None return namespace , name
5820	def _cached_path_needs_update ( ca_path , cache_length ) : exists = os . path . exists ( ca_path ) if not exists : return True stats = os . stat ( ca_path ) if stats . st_mtime < time . time ( ) - cache_length * 60 * 60 : return True if stats . st_size == 0 : return True return False
4365	def decode ( rawstr , json_loads = default_json_loads ) : decoded_msg = { } try : rawstr = rawstr . decode ( 'utf-8' ) except AttributeError : pass split_data = rawstr . split ( ":" , 3 ) msg_type = split_data [ 0 ] msg_id = split_data [ 1 ] endpoint = split_data [ 2 ] data = '' if msg_id != '' : if "+" in msg_id : msg_id = msg_id . split ( '+' ) [ 0 ] decoded_msg [ 'id' ] = int ( msg_id ) decoded_msg [ 'ack' ] = 'data' else : decoded_msg [ 'id' ] = int ( msg_id ) decoded_msg [ 'ack' ] = True msg_type_id = int ( msg_type ) if msg_type_id in MSG_VALUES : decoded_msg [ 'type' ] = MSG_VALUES [ int ( msg_type ) ] else : raise Exception ( "Unknown message type: %s" % msg_type ) decoded_msg [ 'endpoint' ] = endpoint if len ( split_data ) > 3 : data = split_data [ 3 ] if msg_type == "0" : pass elif msg_type == "1" : decoded_msg [ 'qs' ] = data elif msg_type == "2" : pass elif msg_type == "3" : decoded_msg [ 'data' ] = data elif msg_type == "4" : decoded_msg [ 'data' ] = json_loads ( data ) elif msg_type == "5" : try : data = json_loads ( data ) except ValueError : print ( "Invalid JSON event message" , data ) decoded_msg [ 'args' ] = [ ] else : decoded_msg [ 'name' ] = data . pop ( 'name' ) if 'args' in data : decoded_msg [ 'args' ] = data [ 'args' ] else : decoded_msg [ 'args' ] = [ ] elif msg_type == "6" : if '+' in data : ackId , data = data . split ( '+' ) decoded_msg [ 'ackId' ] = int ( ackId ) decoded_msg [ 'args' ] = json_loads ( data ) else : decoded_msg [ 'ackId' ] = int ( data ) decoded_msg [ 'args' ] = [ ] elif msg_type == "7" : if '+' in data : reason , advice = data . split ( '+' ) decoded_msg [ 'reason' ] = REASONS_VALUES [ int ( reason ) ] decoded_msg [ 'advice' ] = ADVICES_VALUES [ int ( advice ) ] else : decoded_msg [ 'advice' ] = '' if data != '' : decoded_msg [ 'reason' ] = REASONS_VALUES [ int ( data ) ] else : decoded_msg [ 'reason' ] = '' elif msg_type == "8" : pass return decoded_msg
4082	def set_directory ( path = None ) : old_path = get_directory ( ) terminate_server ( ) cache . clear ( ) if path : cache [ 'language_check_dir' ] = path try : get_jar_info ( ) except Error : cache [ 'language_check_dir' ] = old_path raise
10739	def log_calls ( function ) : def wrapper ( self , * args , ** kwargs ) : self . log . log ( group = function . __name__ , message = 'Enter' ) function ( self , * args , ** kwargs ) self . log . log ( group = function . __name__ , message = 'Exit' ) return wrapper
3277	def handle_move ( self , dest_path ) : if "/by_tag/" not in self . path : raise DAVError ( HTTP_FORBIDDEN ) if "/by_tag/" not in dest_path : raise DAVError ( HTTP_FORBIDDEN ) catType , tag , _rest = util . save_split ( self . path . strip ( "/" ) , "/" , 2 ) assert catType == "by_tag" assert tag in self . data [ "tags" ] self . data [ "tags" ] . remove ( tag ) catType , tag , _rest = util . save_split ( dest_path . strip ( "/" ) , "/" , 2 ) assert catType == "by_tag" if tag not in self . data [ "tags" ] : self . data [ "tags" ] . append ( tag ) return True
7428	def _subsample ( self ) : spans = self . maparr samp = np . zeros ( spans . shape [ 0 ] , dtype = np . uint64 ) for i in xrange ( spans . shape [ 0 ] ) : samp [ i ] = np . random . randint ( spans [ i , 0 ] , spans [ i , 1 ] , 1 ) return samp
8686	def _decrypt ( self , hexified_value ) : encrypted_value = binascii . unhexlify ( hexified_value ) with warnings . catch_warnings ( ) : warnings . simplefilter ( "ignore" ) jsonified_value = self . cipher . decrypt ( encrypted_value ) . decode ( 'ascii' ) value = json . loads ( jsonified_value ) return value
10902	def lbl ( axis , label , size = 22 ) : at = AnchoredText ( label , loc = 2 , prop = dict ( size = size ) , frameon = True ) at . patch . set_boxstyle ( "round,pad=0.,rounding_size=0.0" ) axis . add_artist ( at )
316	def perf_stats ( returns , factor_returns = None , positions = None , transactions = None , turnover_denom = 'AGB' ) : stats = pd . Series ( ) for stat_func in SIMPLE_STAT_FUNCS : stats [ STAT_FUNC_NAMES [ stat_func . __name__ ] ] = stat_func ( returns ) if positions is not None : stats [ 'Gross leverage' ] = gross_lev ( positions ) . mean ( ) if transactions is not None : stats [ 'Daily turnover' ] = get_turnover ( positions , transactions , turnover_denom ) . mean ( ) if factor_returns is not None : for stat_func in FACTOR_STAT_FUNCS : res = stat_func ( returns , factor_returns ) stats [ STAT_FUNC_NAMES [ stat_func . __name__ ] ] = res return stats
8324	def isString ( s ) : try : return isinstance ( s , unicode ) or isinstance ( s , basestring ) except NameError : return isinstance ( s , str )
1156	def go_str ( value ) : io = StringIO . StringIO ( ) io . write ( '"' ) for c in value : if c in _ESCAPES : io . write ( _ESCAPES [ c ] ) elif c in _SIMPLE_CHARS : io . write ( c ) else : io . write ( r'\x{:02x}' . format ( ord ( c ) ) ) io . write ( '"' ) return io . getvalue ( )
94	def quokka ( size = None , extract = None ) : img = imageio . imread ( QUOKKA_FP , pilmode = "RGB" ) if extract is not None : bb = _quokka_normalize_extract ( extract ) img = bb . extract_from_image ( img ) if size is not None : shape_resized = _compute_resized_shape ( img . shape , size ) img = imresize_single_image ( img , shape_resized [ 0 : 2 ] ) return img
5464	def get_action_by_id ( op , action_id ) : actions = get_actions ( op ) if actions and 1 <= action_id < len ( actions ) : return actions [ action_id - 1 ]
2355	def find_elements ( self , strategy , locator ) : return self . driver_adapter . find_elements ( strategy , locator , root = self . root )
11632	def codepointsInNamelist ( namFilename , unique_glyphs = False , cache = None ) : key = 'charset' if not unique_glyphs else 'ownCharset' internals_dir = os . path . dirname ( os . path . abspath ( __file__ ) ) target = os . path . join ( internals_dir , namFilename ) result = readNamelist ( target , unique_glyphs , cache ) return result [ key ]
7746	def stanza_factory ( element , return_path = None , language = None ) : tag = element . tag if tag . endswith ( "}iq" ) or tag == "iq" : return Iq ( element , return_path = return_path , language = language ) if tag . endswith ( "}message" ) or tag == "message" : return Message ( element , return_path = return_path , language = language ) if tag . endswith ( "}presence" ) or tag == "presence" : return Presence ( element , return_path = return_path , language = language ) else : return Stanza ( element , return_path = return_path , language = language )
10770	def shapely_formatter ( _ , vertices , codes = None ) : elements = [ ] if codes is None : for vertices_ in vertices : if np . all ( vertices_ [ 0 , : ] == vertices_ [ - 1 , : ] ) : if len ( vertices ) < 3 : elements . append ( Point ( vertices_ [ 0 , : ] ) ) else : elements . append ( LinearRing ( vertices_ ) ) else : elements . append ( LineString ( vertices_ ) ) else : for vertices_ , codes_ in zip ( vertices , codes ) : starts = np . nonzero ( codes_ == MPLPATHCODE . MOVETO ) [ 0 ] stops = np . nonzero ( codes_ == MPLPATHCODE . CLOSEPOLY ) [ 0 ] try : rings = [ LinearRing ( vertices_ [ start : stop + 1 , : ] ) for start , stop in zip ( starts , stops ) ] elements . append ( Polygon ( rings [ 0 ] , rings [ 1 : ] ) ) except ValueError as err : if np . any ( stop - start - 1 == 0 ) : if stops [ 0 ] < starts [ 0 ] + 2 : pass else : rings = [ LinearRing ( vertices_ [ start : stop + 1 , : ] ) for start , stop in zip ( starts , stops ) if stop >= start + 2 ] elements . append ( Polygon ( rings [ 0 ] , rings [ 1 : ] ) ) else : raise ( err ) return elements
6182	def hash ( self ) : hash_list = [ ] for key , value in sorted ( self . __dict__ . items ( ) ) : if not callable ( value ) : if isinstance ( value , np . ndarray ) : hash_list . append ( value . tostring ( ) ) else : hash_list . append ( str ( value ) ) return hashlib . md5 ( repr ( hash_list ) . encode ( ) ) . hexdigest ( )
2737	def reserve ( self , * args , ** kwargs ) : data = self . get_data ( 'floating_ips/' , type = POST , params = { 'region' : self . region_slug } ) if data : self . ip = data [ 'floating_ip' ] [ 'ip' ] self . region = data [ 'floating_ip' ] [ 'region' ] return self
887	def _createSegment ( cls , connections , lastUsedIterationForSegment , cell , iteration , maxSegmentsPerCell ) : while connections . numSegments ( cell ) >= maxSegmentsPerCell : leastRecentlyUsedSegment = min ( connections . segmentsForCell ( cell ) , key = lambda segment : lastUsedIterationForSegment [ segment . flatIdx ] ) connections . destroySegment ( leastRecentlyUsedSegment ) segment = connections . createSegment ( cell ) if segment . flatIdx == len ( lastUsedIterationForSegment ) : lastUsedIterationForSegment . append ( iteration ) elif segment . flatIdx < len ( lastUsedIterationForSegment ) : lastUsedIterationForSegment [ segment . flatIdx ] = iteration else : raise AssertionError ( "All segments should be created with the TM createSegment method." ) return segment
1249	def _is_action_available_left ( self , state ) : for row in range ( 4 ) : has_empty = False for col in range ( 4 ) : has_empty |= state [ row , col ] == 0 if state [ row , col ] != 0 and has_empty : return True if ( state [ row , col ] != 0 and col > 0 and state [ row , col ] == state [ row , col - 1 ] ) : return True return False
9626	def detail_view ( self , request , module , preview ) : try : preview = self . __previews [ module ] [ preview ] except KeyError : raise Http404 return preview . detail_view ( request )
8135	def down ( self ) : i = self . index ( ) if i != None : del self . canvas . layers [ i ] i = max ( 0 , i - 1 ) self . canvas . layers . insert ( i , self )
2687	def get_library_config ( name ) : try : proc = Popen ( [ 'pkg-config' , '--cflags' , '--libs' , name ] , stdout = PIPE , stderr = PIPE ) except OSError : print ( 'pkg-config is required for building PyAV' ) exit ( 1 ) raw_cflags , err = proc . communicate ( ) if proc . wait ( ) : return known , unknown = parse_cflags ( raw_cflags . decode ( 'utf8' ) ) if unknown : print ( "pkg-config returned flags we don't understand: {}" . format ( unknown ) ) exit ( 1 ) return known
1234	def atomic_observe ( self , states , actions , internals , reward , terminal ) : self . current_terminal = terminal self . current_reward = reward if self . unique_state : states = dict ( state = states ) if self . unique_action : actions = dict ( action = actions ) self . episode = self . model . atomic_observe ( states = states , actions = actions , internals = internals , terminal = self . current_terminal , reward = self . current_reward )
9131	def ls ( cls , session : Optional [ Session ] = None ) -> List [ 'Action' ] : if session is None : session = _make_session ( ) actions = session . query ( cls ) . order_by ( cls . created . desc ( ) ) . all ( ) session . close ( ) return actions
3524	def intercom ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return IntercomNode ( )
2363	def _pre_install ( ) : dat = join ( setup_dir , 'src' , 'hcl' , 'parsetab.dat' ) if exists ( dat ) : os . unlink ( dat ) sys . path . insert ( 0 , join ( setup_dir , 'src' ) ) import hcl from hcl . parser import HclParser parser = HclParser ( )
444	def roi_pooling ( input , rois , pool_height , pool_width ) : out = roi_pooling_module . roi_pooling ( input , rois , pool_height = pool_height , pool_width = pool_width ) output , argmax_output = out [ 0 ] , out [ 1 ] return output
5672	def get_transfer_stop_pairs ( self ) : transfer_stop_pairs = [ ] previous_arrival_stop = None current_trip_id = None for leg in self . legs : if leg . trip_id is not None and leg . trip_id != current_trip_id and previous_arrival_stop is not None : transfer_stop_pair = ( previous_arrival_stop , leg . departure_stop ) transfer_stop_pairs . append ( transfer_stop_pair ) previous_arrival_stop = leg . arrival_stop current_trip_id = leg . trip_id return transfer_stop_pairs
12631	def group_dicom_files ( dicom_file_paths , header_fields ) : dist = SimpleDicomFileDistance ( field_weights = header_fields ) path_list = dicom_file_paths . copy ( ) path_groups = DefaultOrderedDict ( DicomFileSet ) while len ( path_list ) > 0 : file_path1 = path_list . pop ( ) file_subgroup = [ file_path1 ] dist . set_dicom_file1 ( file_path1 ) j = len ( path_list ) - 1 while j >= 0 : file_path2 = path_list [ j ] dist . set_dicom_file2 ( file_path2 ) if dist . transform ( ) : file_subgroup . append ( file_path2 ) path_list . pop ( j ) j -= 1 path_groups [ file_path1 ] . from_set ( file_subgroup , check_if_dicoms = False ) return path_groups
1816	def SETNO ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . OF == False , 1 , 0 ) )
8678	def put ( self , name , value = None , modify = False , metadata = None , description = '' , encrypt = True , lock = False , key_type = 'secret' , add = False ) : def assert_key_is_unlocked ( existing_key ) : if existing_key and existing_key . get ( 'lock' ) : raise GhostError ( 'Key `{0}` is locked and therefore cannot be modified. ' 'Unlock the key and try again' . format ( name ) ) def assert_value_provided_for_new_key ( value , existing_key ) : if not value and not existing_key . get ( 'value' ) : raise GhostError ( 'You must provide a value for new keys' ) self . _assert_valid_stash ( ) self . _validate_key_schema ( value , key_type ) if value and encrypt and not isinstance ( value , dict ) : raise GhostError ( 'Value must be of type dict' ) key = self . _handle_existing_key ( name , modify or add ) assert_key_is_unlocked ( key ) assert_value_provided_for_new_key ( value , key ) new_key = dict ( name = name , lock = lock ) if value : if add : value = self . _update_existing_key ( key , value ) new_key [ 'value' ] = self . _encrypt ( value ) if encrypt else value else : new_key [ 'value' ] = key . get ( 'value' ) new_key [ 'description' ] = description or key . get ( 'description' ) new_key [ 'created_at' ] = key . get ( 'created_at' ) or _get_current_time ( ) new_key [ 'modified_at' ] = _get_current_time ( ) new_key [ 'metadata' ] = metadata or key . get ( 'metadata' ) new_key [ 'uid' ] = key . get ( 'uid' ) or str ( uuid . uuid4 ( ) ) new_key [ 'type' ] = key . get ( 'type' ) or key_type key_id = self . _storage . put ( new_key ) audit ( storage = self . _storage . db_path , action = 'MODIFY' if ( modify or add ) else 'PUT' , message = json . dumps ( dict ( key_name = new_key [ 'name' ] , value = 'HIDDEN' , description = new_key [ 'description' ] , uid = new_key [ 'uid' ] , metadata = json . dumps ( new_key [ 'metadata' ] ) , lock = new_key [ 'lock' ] , type = new_key [ 'type' ] ) ) ) return key_id
2313	def predict_proba ( self , a , b , ** kwargs ) : a = scale ( a ) . reshape ( ( - 1 , 1 ) ) b = scale ( b ) . reshape ( ( - 1 , 1 ) ) return self . anm_score ( b , a ) - self . anm_score ( a , b )
10985	def optimize_from_initial ( s , max_mem = 1e9 , invert = 'guess' , desc = '' , rz_order = 3 , min_rad = None , max_rad = None ) : RLOG . info ( 'Initial burn:' ) if desc is not None : desc_burn = desc + 'initial-burn' desc_polish = desc + 'addsub-polish' else : desc_burn , desc_polish = [ None ] * 2 opt . burn ( s , mode = 'burn' , n_loop = 3 , fractol = 0.1 , desc = desc_burn , max_mem = max_mem , include_rad = False , dowarn = False ) opt . burn ( s , mode = 'burn' , n_loop = 3 , fractol = 0.1 , desc = desc_burn , max_mem = max_mem , include_rad = True , dowarn = False ) RLOG . info ( 'Start add-subtract' ) rad = s . obj_get_radii ( ) if min_rad is None : min_rad = 0.5 * np . median ( rad ) if max_rad is None : max_rad = 1.5 * np . median ( rad ) addsub . add_subtract ( s , tries = 30 , min_rad = min_rad , max_rad = max_rad , invert = invert ) if desc is not None : states . save ( s , desc = desc + 'initial-addsub' ) RLOG . info ( 'Final polish:' ) d = opt . burn ( s , mode = 'polish' , n_loop = 8 , fractol = 3e-4 , desc = desc_polish , max_mem = max_mem , rz_order = rz_order , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' ) return s
5003	def handle ( self , * args , ** options ) : LOGGER . info ( 'Starting assigning enterprise roles to users!' ) role = options [ 'role' ] if role == ENTERPRISE_ADMIN_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_admin_users_batch , options ) elif role == ENTERPRISE_OPERATOR_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_operator_users_batch , options ) elif role == ENTERPRISE_LEARNER_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_customer_users_batch , options ) elif role == ENTERPRISE_ENROLLMENT_API_ADMIN_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_enrollment_api_admin_users_batch , options , True ) elif role == ENTERPRISE_CATALOG_ADMIN_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_catalog_admin_users_batch , options , True ) else : raise CommandError ( 'Please provide a valid role name. Supported roles are {admin} and {learner}' . format ( admin = ENTERPRISE_ADMIN_ROLE , learner = ENTERPRISE_LEARNER_ROLE ) ) LOGGER . info ( 'Successfully finished assigning enterprise roles to users!' )
7440	def get_params ( self , param = "" ) : fullcurdir = os . path . realpath ( os . path . curdir ) if not param : for index , ( key , value ) in enumerate ( self . paramsdict . items ( ) ) : if isinstance ( value , str ) : value = value . replace ( fullcurdir + "/" , "./" ) sys . stdout . write ( "{}{:<4}{:<28}{:<45}\n" . format ( self . _spacer , index , key , value ) ) else : try : if int ( param ) : return self . paramsdict . values ( ) [ int ( param ) ] except ( ValueError , TypeError , NameError , IndexError ) : try : return self . paramsdict [ param ] except KeyError : return 'key not recognized'
648	def generateHubSequences ( nCoinc = 10 , hubs = [ 2 , 6 ] , seqLength = [ 5 , 6 , 7 ] , nSeq = 100 ) : coincList = range ( nCoinc ) for hub in hubs : coincList . remove ( hub ) seqList = [ ] for i in xrange ( nSeq ) : length = random . choice ( seqLength ) - 1 seq = random . sample ( coincList , length ) seq . insert ( length // 2 , random . choice ( hubs ) ) seqList . append ( seq ) return seqList
6819	def sync_media ( self , sync_set = None , clean = 0 , iter_local_paths = 0 ) : self . genv . SITE = self . genv . SITE or self . genv . default_site r = self . local_renderer clean = int ( clean ) self . vprint ( 'Getting site data for %s...' % self . genv . SITE ) self . set_site_specifics ( self . genv . SITE ) sync_sets = r . env . sync_sets if sync_set : sync_sets = [ sync_set ] ret_paths = [ ] for _sync_set in sync_sets : for paths in r . env . sync_sets [ _sync_set ] : r . env . sync_local_path = os . path . abspath ( paths [ 'local_path' ] % self . genv ) if paths [ 'local_path' ] . endswith ( '/' ) and not r . env . sync_local_path . endswith ( '/' ) : r . env . sync_local_path += '/' if iter_local_paths : ret_paths . append ( r . env . sync_local_path ) continue r . env . sync_remote_path = paths [ 'remote_path' ] % self . genv if clean : r . sudo ( 'rm -Rf {apache_sync_remote_path}' ) print ( 'Syncing %s to %s...' % ( r . env . sync_local_path , r . env . sync_remote_path ) ) r . env . tmp_chmod = paths . get ( 'chmod' , r . env . chmod ) r . sudo ( 'mkdir -p {apache_sync_remote_path}' ) r . sudo ( 'chmod -R {apache_tmp_chmod} {apache_sync_remote_path}' ) r . local ( 'rsync -rvz --progress --recursive --no-p --no-g ' '--rsh "ssh -o StrictHostKeyChecking=no -i {key_filename}" {apache_sync_local_path} {user}@{host_string}:{apache_sync_remote_path}' ) r . sudo ( 'chown -R {apache_web_user}:{apache_web_group} {apache_sync_remote_path}' ) if iter_local_paths : return ret_paths
4534	def fill ( self , color , start = 0 , end = - 1 ) : start = max ( start , 0 ) if end < 0 or end >= self . numLEDs : end = self . numLEDs - 1 for led in range ( start , end + 1 ) : self . _set_base ( led , color )
10852	def otsu_threshold ( data , bins = 255 ) : h0 , x0 = np . histogram ( data . ravel ( ) , bins = bins ) h = h0 . astype ( 'float' ) / h0 . sum ( ) x = 0.5 * ( x0 [ 1 : ] + x0 [ : - 1 ] ) wk = np . array ( [ h [ : i + 1 ] . sum ( ) for i in range ( h . size ) ] ) mk = np . array ( [ sum ( x [ : i + 1 ] * h [ : i + 1 ] ) for i in range ( h . size ) ] ) mt = mk [ - 1 ] sb = ( mt * wk - mk ) ** 2 / ( wk * ( 1 - wk ) + 1e-15 ) ind = sb . argmax ( ) return 0.5 * ( x0 [ ind ] + x0 [ ind + 1 ] )
13213	def rename ( self , from_name , to_name ) : log . info ( 'renaming database from %s to %s' % ( from_name , to_name ) ) self . _run_stmt ( 'alter database %s rename to %s' % ( from_name , to_name ) )
658	def plotHistogram ( freqCounts , title = 'On-Times Histogram' , xLabel = 'On-Time' ) : import pylab pylab . ion ( ) pylab . figure ( ) pylab . bar ( numpy . arange ( len ( freqCounts ) ) - 0.5 , freqCounts ) pylab . title ( title ) pylab . xlabel ( xLabel )
2905	def _add_child ( self , task_spec , state = MAYBE ) : if task_spec is None : raise ValueError ( self , '_add_child() requires a TaskSpec' ) if self . _is_predicted ( ) and state & self . PREDICTED_MASK == 0 : msg = 'Attempt to add non-predicted child to predicted task' raise WorkflowException ( self . task_spec , msg ) task = Task ( self . workflow , task_spec , self , state = state ) task . thread_id = self . thread_id if state == self . READY : task . _ready ( ) return task
4451	def search ( self , query ) : args , query = self . _mk_query_args ( query ) st = time . time ( ) res = self . redis . execute_command ( self . SEARCH_CMD , * args ) return Result ( res , not query . _no_content , duration = ( time . time ( ) - st ) * 1000.0 , has_payload = query . _with_payloads )
2532	def parse_ext_doc_ref ( self , ext_doc_ref_term ) : for _s , _p , o in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'externalDocumentId' ] , None ) ) : try : self . builder . set_ext_doc_id ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'External Document ID' ) break for _s , _p , o in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'spdxDocument' ] , None ) ) : try : self . builder . set_spdx_doc_uri ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'SPDX Document URI' ) break for _s , _p , checksum in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'checksum' ] , None ) ) : for _ , _ , value in self . graph . triples ( ( checksum , self . spdx_namespace [ 'checksumValue' ] , None ) ) : try : self . builder . set_chksum ( self . doc , six . text_type ( value ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'Checksum' ) break
4919	def course_run_detail ( self , request , pk , course_id ) : enterprise_customer_catalog = self . get_object ( ) course_run = enterprise_customer_catalog . get_course_run ( course_id ) if not course_run : raise Http404 context = self . get_serializer_context ( ) context [ 'enterprise_customer_catalog' ] = enterprise_customer_catalog serializer = serializers . CourseRunDetailSerializer ( course_run , context = context ) return Response ( serializer . data )
13815	def _StructMessageToJsonObject ( message , unused_including_default = False ) : fields = message . fields ret = { } for key in fields : ret [ key ] = _ValueMessageToJsonObject ( fields [ key ] ) return ret
12765	def reposition ( self , frame_no ) : for label , j in self . channels . items ( ) : body = self . bodies [ label ] body . position = self . positions [ frame_no , j ] body . linear_velocity = self . velocities [ frame_no , j ]
13662	def atomic_write ( filename ) : f = _tempfile ( os . fsencode ( filename ) ) try : yield f finally : f . close ( ) os . replace ( f . name , filename )
7610	def get_location ( self , location_id : int , timeout : int = None ) : url = self . api . LOCATIONS + '/' + str ( location_id ) return self . _get_model ( url , timeout = timeout )
672	def getPredictionResults ( network , clRegionName ) : classifierRegion = network . regions [ clRegionName ] actualValues = classifierRegion . getOutputData ( "actualValues" ) probabilities = classifierRegion . getOutputData ( "probabilities" ) steps = classifierRegion . getSelf ( ) . stepsList N = classifierRegion . getSelf ( ) . maxCategoryCount results = { step : { } for step in steps } for i in range ( len ( steps ) ) : stepProbabilities = probabilities [ i * N : ( i + 1 ) * N - 1 ] mostLikelyCategoryIdx = stepProbabilities . argmax ( ) predictedValue = actualValues [ mostLikelyCategoryIdx ] predictionConfidence = stepProbabilities [ mostLikelyCategoryIdx ] results [ steps [ i ] ] [ "predictedValue" ] = predictedValue results [ steps [ i ] ] [ "predictionConfidence" ] = predictionConfidence return results
9225	def permutations_with_replacement ( iterable , r = None ) : pool = tuple ( iterable ) n = len ( pool ) r = n if r is None else r for indices in itertools . product ( range ( n ) , repeat = r ) : yield list ( pool [ i ] for i in indices )
6327	def get_count ( self , ngram , corpus = None ) : r if not corpus : corpus = self . ngcorpus if not ngram : return corpus [ None ] if isinstance ( ngram , ( text_type , str ) ) : ngram = text_type ( ngram ) . split ( ) if ngram [ 0 ] in corpus : return self . get_count ( ngram [ 1 : ] , corpus [ ngram [ 0 ] ] ) return 0
12521	def to_file ( self , output_file , smooth_fwhm = 0 , outdtype = None ) : outmat , mask_indices , mask_shape = self . to_matrix ( smooth_fwhm , outdtype ) exporter = ExportData ( ) content = { 'data' : outmat , 'labels' : self . labels , 'mask_indices' : mask_indices , 'mask_shape' : mask_shape , } if self . others : content . update ( self . others ) log . debug ( 'Creating content in file {}.' . format ( output_file ) ) try : exporter . save_variables ( output_file , content ) except Exception as exc : raise Exception ( 'Error saving variables to file {}.' . format ( output_file ) ) from exc
5727	def _get_responses_windows ( self , timeout_sec ) : timeout_time_sec = time . time ( ) + timeout_sec responses = [ ] while True : try : self . gdb_process . stdout . flush ( ) if PYTHON3 : raw_output = self . gdb_process . stdout . readline ( ) . replace ( b"\r" , b"\n" ) else : raw_output = self . gdb_process . stdout . read ( ) . replace ( b"\r" , b"\n" ) responses += self . _get_responses_list ( raw_output , "stdout" ) except IOError : pass try : self . gdb_process . stderr . flush ( ) if PYTHON3 : raw_output = self . gdb_process . stderr . readline ( ) . replace ( b"\r" , b"\n" ) else : raw_output = self . gdb_process . stderr . read ( ) . replace ( b"\r" , b"\n" ) responses += self . _get_responses_list ( raw_output , "stderr" ) except IOError : pass if time . time ( ) > timeout_time_sec : break return responses
8124	def draw_cornu_flat ( x0 , y0 , t0 , t1 , s0 , c0 , flip , cs , ss , cmd ) : for j in range ( 0 , 100 ) : t = j * .01 s , c = eval_cornu ( t0 + t * ( t1 - t0 ) ) s *= flip s -= s0 c -= c0 x = c * cs - s * ss y = s * cs + c * ss print_pt ( x0 + x , y0 + y , cmd ) cmd = 'lineto' return cmd
5598	def execute ( mp , resampling = "nearest" , scale_method = None , scales_minmax = None ) : with mp . open ( "raster" , resampling = resampling ) as raster_file : if raster_file . is_empty ( ) : return "empty" scaled = ( ) mask = ( ) raster_data = raster_file . read ( ) if raster_data . ndim == 2 : raster_data = ma . expand_dims ( raster_data , axis = 0 ) if not scale_method : scales_minmax = [ ( i , i ) for i in range ( len ( raster_data ) ) ] for band , ( scale_min , scale_max ) in zip ( raster_data , scales_minmax ) : if scale_method in [ "dtype_scale" , "minmax_scale" ] : scaled += ( _stretch_array ( band , scale_min , scale_max ) , ) elif scale_method == "crop" : scaled += ( np . clip ( band , scale_min , scale_max ) , ) else : scaled += ( band , ) mask += ( band . mask , ) return ma . masked_array ( np . stack ( scaled ) , np . stack ( mask ) )
9549	def validate ( self , data , expect_header_row = True , ignore_lines = 0 , summarize = False , limit = 0 , context = None , report_unexpected_exceptions = True ) : problems = list ( ) problem_generator = self . ivalidate ( data , expect_header_row , ignore_lines , summarize , context , report_unexpected_exceptions ) for i , p in enumerate ( problem_generator ) : if not limit or i < limit : problems . append ( p ) return problems
13376	def walk_dn ( start_dir , depth = 10 ) : start_depth = len ( os . path . split ( start_dir ) ) end_depth = start_depth + depth for root , subdirs , files in os . walk ( start_dir ) : yield root , subdirs , files if len ( os . path . split ( root ) ) >= end_depth : break
7669	def slice ( self , start_time , end_time , strict = False ) : sliced_array = AnnotationArray ( ) for ann in self : sliced_array . append ( ann . slice ( start_time , end_time , strict = strict ) ) return sliced_array
7797	def _register_server_authenticator ( klass , name ) : SERVER_MECHANISMS_D [ name ] = klass items = sorted ( SERVER_MECHANISMS_D . items ( ) , key = _key_func , reverse = True ) SERVER_MECHANISMS [ : ] = [ k for ( k , v ) in items ] SECURE_SERVER_MECHANISMS [ : ] = [ k for ( k , v ) in items if v . _pyxmpp_sasl_secure ]
742	def readFromFile ( cls , f , packed = True ) : schema = cls . getSchema ( ) if packed : proto = schema . read_packed ( f ) else : proto = schema . read ( f ) return cls . read ( proto )
12826	def flush_buffer ( self ) : self . code_builder . add_line ( '{0}.extend([{1}])' , self . result_var , ',' . join ( self . buffered ) ) self . buffered = [ ]
7714	def add_item ( self , jid , name = None , groups = None , callback = None , error_callback = None ) : if jid in self . roster : raise ValueError ( "{0!r} already in the roster" . format ( jid ) ) item = RosterItem ( jid , name , groups ) self . _roster_set ( item , callback , error_callback )
13391	def format_uuid ( uuid , max_length = 10 ) : if max_length <= 3 : raise ValueError ( "max length must be larger than 3" ) if len ( uuid ) > max_length : uuid = "{}..." . format ( uuid [ 0 : max_length - 3 ] ) return uuid
9747	def send_discovery_packet ( self ) : if self . port is None : return self . transport . sendto ( QRTDiscoveryP1 . pack ( QRTDiscoveryPacketSize , QRTPacketType . PacketDiscover . value ) + QRTDiscoveryP2 . pack ( self . port ) , ( "<broadcast>" , 22226 ) , )
2784	def get_timeout ( self ) : timeout_str = os . environ . get ( REQUEST_TIMEOUT_ENV_VAR ) if timeout_str : try : return float ( timeout_str ) except : self . _log . error ( 'Failed parsing the request read timeout of ' '"%s". Please use a valid float number!' % timeout_str ) return None
7202	def deprecate_module_attr ( mod , deprecated ) : deprecated = set ( deprecated ) class Wrapper ( object ) : def __getattr__ ( self , attr ) : if attr in deprecated : warnings . warn ( "Property {} is deprecated" . format ( attr ) , GBDXDeprecation ) return getattr ( mod , attr ) def __setattr__ ( self , attr , value ) : if attr in deprecated : warnings . warn ( "Property {} is deprecated" . format ( attr ) , GBDXDeprecation ) return setattr ( mod , attr , value ) return Wrapper ( )
12018	def disassemble ( self ) : ser_pb = open ( self . input_file , 'rb' ) . read ( ) fd = FileDescriptorProto ( ) fd . ParseFromString ( ser_pb ) self . name = fd . name self . _print ( '// Reversed by pbd (https://github.com/rsc-dev/pbd)' ) self . _print ( 'syntax = "proto2";' ) self . _print ( '' ) if len ( fd . package ) > 0 : self . _print ( 'package {};' . format ( fd . package ) ) self . package = fd . package else : self . _print ( '// Package not defined' ) self . _walk ( fd )
12796	def parse ( self , text , key = None ) : try : data = json . loads ( text ) except ValueError as e : raise ValueError ( "%s: Value: [%s]" % ( e , text ) ) if data and key : if key not in data : raise ValueError ( "Invalid response (key %s not found): %s" % ( key , data ) ) data = data [ key ] return data
8506	def _default ( self ) : try : iter ( self . default ) except TypeError : return repr ( self . default ) for v in self . default : if isinstance ( v , Unparseable ) : default = self . _default_value_only ( ) if default : return default return ', ' . join ( str ( v ) for v in self . default )
13348	def prompt ( prefix = None , colored = True ) : if platform == 'win' : return '[{0}] $P$G' . format ( prefix ) else : if colored : return ( '[{0}] ' '\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\] ' '\\[\\033[01;34m\\]\\w $ \\[\\033[00m\\]' ) . format ( prefix ) return '[{0}] \\u@\\h \\w $ ' . format ( prefix )
7174	def _train_and_save ( obj , cache , data , print_updates ) : obj . train ( data ) if print_updates : print ( 'Regenerated ' + obj . name + '.' ) obj . save ( cache )
4844	def is_course_in_catalog ( self , catalog_id , course_id ) : try : course_run_id = str ( CourseKey . from_string ( course_id ) ) except InvalidKeyError : course_run_id = None endpoint = self . client . catalogs ( catalog_id ) . contains if course_run_id : resp = endpoint . get ( course_run_id = course_run_id ) else : resp = endpoint . get ( course_id = course_id ) return resp . get ( 'courses' , { } ) . get ( course_id , False )
4506	def find_serial_devices ( self ) : if self . devices is not None : return self . devices self . devices = { } hardware_id = "(?i)" + self . hardware_id for ports in serial . tools . list_ports . grep ( hardware_id ) : port = ports [ 0 ] try : id = self . get_device_id ( port ) ver = self . _get_device_version ( port ) except : log . debug ( 'Error getting device_id for %s, %s' , port , self . baudrate ) if True : raise continue if getattr ( ports , '__len__' , lambda : 0 ) ( ) : log . debug ( 'Multi-port device %s:%s:%s with %s ports found' , self . hardware_id , id , ver , len ( ports ) ) if id < 0 : log . debug ( 'Serial device %s:%s:%s with id %s < 0' , self . hardware_id , id , ver ) else : self . devices [ id ] = port , ver return self . devices
2029	def CALLDATACOPY ( self , mem_offset , data_offset , size ) : if issymbolic ( size ) : if solver . can_be_true ( self . _constraints , size <= len ( self . data ) + 32 ) : self . constraints . add ( size <= len ( self . data ) + 32 ) raise ConcretizeArgument ( 3 , policy = 'SAMPLED' ) if issymbolic ( data_offset ) : if solver . can_be_true ( self . _constraints , data_offset == self . _used_calldata_size ) : self . constraints . add ( data_offset == self . _used_calldata_size ) raise ConcretizeArgument ( 2 , policy = 'SAMPLED' ) self . _use_calldata ( data_offset , size ) self . _allocate ( mem_offset , size ) for i in range ( size ) : try : c = Operators . ITEBV ( 8 , data_offset + i < len ( self . data ) , Operators . ORD ( self . data [ data_offset + i ] ) , 0 ) except IndexError : c = 0 self . _store ( mem_offset + i , c )
12741	def _parse_persons ( self , datafield , subfield , roles = [ "aut" ] ) : parsed_persons = [ ] raw_persons = self . get_subfields ( datafield , subfield ) for person in raw_persons : other_subfields = person . other_subfields if "4" in other_subfields and roles != [ "any" ] : person_roles = other_subfields [ "4" ] relevant = any ( map ( lambda role : role in roles , person_roles ) ) if not relevant : continue ind1 = person . i1 ind2 = person . i2 person = person . strip ( ) name = "" second_name = "" surname = "" title = "" if ind1 == "1" and ind2 == " " : if "," in person : surname , name = person . split ( "," , 1 ) elif " " in person : surname , name = person . split ( " " , 1 ) else : surname = person if "c" in other_subfields : title = "," . join ( other_subfields [ "c" ] ) elif ind1 == "0" and ind2 == " " : name = person . strip ( ) if "b" in other_subfields : second_name = "," . join ( other_subfields [ "b" ] ) if "c" in other_subfields : surname = "," . join ( other_subfields [ "c" ] ) elif ind1 == "1" and ind2 == "0" or ind1 == "0" and ind2 == "0" : name = person . strip ( ) if "c" in other_subfields : title = "," . join ( other_subfields [ "c" ] ) parsed_persons . append ( Person ( name . strip ( ) , second_name . strip ( ) , surname . strip ( ) , title . strip ( ) ) ) return parsed_persons
3041	def access_token_expired ( self ) : if self . invalid : return True if not self . token_expiry : return False now = _UTCNOW ( ) if now >= self . token_expiry : logger . info ( 'access_token is expired. Now: %s, token_expiry: %s' , now , self . token_expiry ) return True return False
6821	def configure_modsecurity ( self ) : r = self . local_renderer if r . env . modsecurity_enabled and not self . last_manifest . modsecurity_enabled : self . install_packages ( ) fn = self . render_to_file ( 'apache/apache_modsecurity.template.conf' ) r . put ( local_path = fn , remote_path = '/etc/modsecurity/modsecurity.conf' , use_sudo = True ) r . env . modsecurity_download_filename = '/tmp/owasp-modsecurity-crs.tar.gz' r . sudo ( 'cd /tmp; wget --output-document={apache_modsecurity_download_filename} {apache_modsecurity_download_url}' ) r . env . modsecurity_download_top = r . sudo ( "cd /tmp; " "tar tzf %(apache_modsecurity_download_filename)s | sed -e 's@/.*@@' | uniq" % self . genv ) r . sudo ( 'cd /tmp; tar -zxvf %(apache_modsecurity_download_filename)s' % self . genv ) r . sudo ( 'cd /tmp; cp -R %(apache_modsecurity_download_top)s/* /etc/modsecurity/' % self . genv ) r . sudo ( 'mv /etc/modsecurity/modsecurity_crs_10_setup.conf.example /etc/modsecurity/modsecurity_crs_10_setup.conf' ) r . sudo ( 'rm -f /etc/modsecurity/activated_rules/*' ) r . sudo ( 'cd /etc/modsecurity/base_rules; ' 'for f in * ; do ln -s /etc/modsecurity/base_rules/$f /etc/modsecurity/activated_rules/$f ; done' ) r . sudo ( 'cd /etc/modsecurity/optional_rules; ' 'for f in * ; do ln -s /etc/modsecurity/optional_rules/$f /etc/modsecurity/activated_rules/$f ; done' ) r . env . httpd_conf_append . append ( 'Include "/etc/modsecurity/activated_rules/*.conf"' ) self . enable_mod ( 'evasive' ) self . enable_mod ( 'headers' ) elif not self . env . modsecurity_enabled and self . last_manifest . modsecurity_enabled : self . disable_mod ( 'modsecurity' )
7144	def transfer ( self , address , amount , priority = prio . NORMAL , payment_id = None , unlock_time = 0 , relay = True ) : return self . _backend . transfer ( [ ( address , amount ) ] , priority , payment_id , unlock_time , account = self . index , relay = relay )
12600	def concat_sheets ( xl_path : str , sheetnames = None , add_tab_names = False ) : xl_path , choice = _check_xl_path ( xl_path ) if sheetnames is None : sheetnames = get_sheet_list ( xl_path ) sheets = pd . read_excel ( xl_path , sheetname = sheetnames ) if add_tab_names : for tab in sheets : sheets [ tab ] [ 'Tab' ] = [ tab ] * len ( sheets [ tab ] ) return pd . concat ( [ sheets [ tab ] for tab in sheets ] )
1957	def _init_arm_kernel_helpers ( self ) : page_data = bytearray ( b'\xf1\xde\xfd\xe7' * 1024 ) preamble = binascii . unhexlify ( 'ff0300ea' + '650400ea' + 'f0ff9fe5' + '430400ea' + '220400ea' + '810400ea' + '000400ea' + '870400ea' ) __kuser_cmpxchg64 = binascii . unhexlify ( '30002de9' + '08c09de5' + '30009ce8' + '010055e1' + '00005401' + '0100a013' + '0000a003' + '0c008c08' + '3000bde8' + '1eff2fe1' ) __kuser_dmb = binascii . unhexlify ( '5bf07ff5' + '1eff2fe1' ) __kuser_cmpxchg = binascii . unhexlify ( '003092e5' + '000053e1' + '0000a003' + '00108205' + '0100a013' + '1eff2fe1' ) self . _arm_tls_memory = self . current . memory . mmap ( None , 4 , 'rw ' ) __kuser_get_tls = binascii . unhexlify ( '04009FE5' + '010090e8' + '1eff2fe1' ) + struct . pack ( '<I' , self . _arm_tls_memory ) tls_area = b'\x00' * 12 version = struct . pack ( '<I' , 5 ) def update ( address , code ) : page_data [ address : address + len ( code ) ] = code update ( 0x000 , preamble ) update ( 0xf60 , __kuser_cmpxchg64 ) update ( 0xfa0 , __kuser_dmb ) update ( 0xfc0 , __kuser_cmpxchg ) update ( 0xfe0 , __kuser_get_tls ) update ( 0xff0 , tls_area ) update ( 0xffc , version ) self . current . memory . mmap ( 0xffff0000 , len ( page_data ) , 'r x' , page_data )
8680	def list ( self , key_name = None , max_suggestions = 100 , cutoff = 0.5 , locked_only = False , key_type = None ) : self . _assert_valid_stash ( ) key_list = [ k for k in self . _storage . list ( ) if k [ 'name' ] != 'stored_passphrase' and ( k . get ( 'lock' ) if locked_only else True ) ] if key_type : types = ( 'secret' , None ) if key_type == 'secret' else [ key_type ] key_list = [ k for k in key_list if k . get ( 'type' ) in types ] key_list = [ k [ 'name' ] for k in key_list ] if key_name : if key_name . startswith ( '~' ) : key_list = difflib . get_close_matches ( key_name . lstrip ( '~' ) , key_list , max_suggestions , cutoff ) else : key_list = [ k for k in key_list if key_name in k ] audit ( storage = self . _storage . db_path , action = 'LIST' + ( '[LOCKED]' if locked_only else '' ) , message = json . dumps ( dict ( ) ) ) return key_list
1689	def InTemplateArgumentList ( self , clean_lines , linenum , pos ) : while linenum < clean_lines . NumLines ( ) : line = clean_lines . elided [ linenum ] match = Match ( r'^[^{};=\[\]\.<>]*(.)' , line [ pos : ] ) if not match : linenum += 1 pos = 0 continue token = match . group ( 1 ) pos += len ( match . group ( 0 ) ) if token in ( '{' , '}' , ';' ) : return False if token in ( '>' , '=' , '[' , ']' , '.' ) : return True if token != '<' : pos += 1 if pos >= len ( line ) : linenum += 1 pos = 0 continue ( _ , end_line , end_pos ) = CloseExpression ( clean_lines , linenum , pos - 1 ) if end_pos < 0 : return False linenum = end_line pos = end_pos return False
13589	def json_post_required ( * decorator_args ) : def decorator ( method ) : @ wraps ( method ) def wrapper ( * args , ** kwargs ) : field = decorator_args [ 0 ] if len ( decorator_args ) == 2 : request_name = decorator_args [ 1 ] else : request_name = field request = args [ 0 ] if request . method != 'POST' : logger . error ( 'POST required for this url' ) raise Http404 ( 'only POST allowed for this url' ) if field not in request . POST : s = 'Expected field named %s in POST' % field logger . error ( s ) raise Http404 ( s ) setattr ( request , request_name , json . loads ( request . POST [ field ] ) ) return method ( * args , ** kwargs ) return wrapper return decorator
13771	def collect_links ( self , env = None ) : for asset in self . assets . values ( ) : if asset . has_bundles ( ) : asset . collect_files ( ) if env is None : env = self . config . env if env == static_bundle . ENV_PRODUCTION : self . _minify ( emulate = True ) self . _add_url_prefix ( )
7949	def send_stream_tail ( self ) : with self . lock : if not self . _socket or self . _hup : logger . debug ( u"Cannot send stream closing tag: already closed" ) return data = self . _serializer . emit_tail ( ) try : self . _write ( data . encode ( "utf-8" ) ) except ( IOError , SystemError , socket . error ) , err : logger . debug ( u"Sending stream closing tag failed: {0}" . format ( err ) ) self . _serializer = None self . _hup = True if self . _tls_state is None : try : self . _socket . shutdown ( socket . SHUT_WR ) except socket . error : pass self . _set_state ( "closing" ) self . _write_queue . clear ( ) self . _write_queue_cond . notify ( )
6173	def single_instance ( func = None , lock_timeout = None , include_args = False ) : if func is None : return partial ( single_instance , lock_timeout = lock_timeout , include_args = include_args ) @ wraps ( func ) def wrapped ( celery_self , * args , ** kwargs ) : timeout = ( lock_timeout or celery_self . soft_time_limit or celery_self . time_limit or celery_self . app . conf . get ( 'CELERYD_TASK_SOFT_TIME_LIMIT' ) or celery_self . app . conf . get ( 'CELERYD_TASK_TIME_LIMIT' ) or ( 60 * 5 ) ) manager_class = _select_manager ( celery_self . backend . __class__ . __name__ ) lock_manager = manager_class ( celery_self , timeout , include_args , args , kwargs ) with lock_manager : ret_value = func ( * args , ** kwargs ) return ret_value return wrapped
13397	def get_reference_to_class ( cls , class_or_class_name ) : if isinstance ( class_or_class_name , type ) : return class_or_class_name elif isinstance ( class_or_class_name , string_types ) : if ":" in class_or_class_name : mod_name , class_name = class_or_class_name . split ( ":" ) if not mod_name in sys . modules : __import__ ( mod_name ) mod = sys . modules [ mod_name ] return mod . __dict__ [ class_name ] else : return cls . load_class_from_locals ( class_or_class_name ) else : msg = "Unexpected Type '%s'" % type ( class_or_class_name ) raise InternalCashewException ( msg )
13381	def env_to_dict ( env , pathsep = os . pathsep ) : out_dict = { } for k , v in env . iteritems ( ) : if pathsep in v : out_dict [ k ] = v . split ( pathsep ) else : out_dict [ k ] = v return out_dict
10754	def iso_name_slugify ( name ) : name = name . encode ( 'ascii' , 'replace' ) . replace ( b'?' , b'_' ) return name . decode ( 'ascii' )
4185	def window_flattop ( N , mode = 'symmetric' , precision = None ) : r assert mode in [ 'periodic' , 'symmetric' ] t = arange ( 0 , N ) if mode == 'periodic' : x = 2 * pi * t / float ( N ) else : if N == 1 : return ones ( 1 ) x = 2 * pi * t / float ( N - 1 ) a0 = 0.21557895 a1 = 0.41663158 a2 = 0.277263158 a3 = 0.083578947 a4 = 0.006947368 if precision == 'octave' : d = 4.6402 a0 = 1. / d a1 = 1.93 / d a2 = 1.29 / d a3 = 0.388 / d a4 = 0.0322 / d w = a0 - a1 * cos ( x ) + a2 * cos ( 2 * x ) - a3 * cos ( 3 * x ) + a4 * cos ( 4 * x ) return w
10502	def waitForValueToChange ( self , timeout = 10 ) : callback = AXCallbacks . returnElemCallback retelem = None return self . waitFor ( timeout , 'AXValueChanged' , callback = callback , args = ( retelem , ) )
3136	def get ( self , ** queryparams ) : return self . _mc_client . _get ( url = self . _build_path ( ) , ** queryparams )
5658	def _validate_no_null_values ( self ) : for table in DB_TABLE_NAMES : null_not_ok_warning = "Null values in must-have columns in table {table}" . format ( table = table ) null_warn_warning = "Null values in good-to-have columns in table {table}" . format ( table = table ) null_not_ok_fields = DB_TABLE_NAME_TO_FIELDS_WHERE_NULL_NOT_OK [ table ] null_warn_fields = DB_TABLE_NAME_TO_FIELDS_WHERE_NULL_OK_BUT_WARN [ table ] df = self . gtfs . get_table ( table ) for warning , fields in zip ( [ null_not_ok_warning , null_warn_warning ] , [ null_not_ok_fields , null_warn_fields ] ) : null_unwanted_df = df [ fields ] rows_having_null = null_unwanted_df . isnull ( ) . any ( 1 ) if sum ( rows_having_null ) > 0 : rows_having_unwanted_null = df [ rows_having_null . values ] self . warnings_container . add_warning ( warning , rows_having_unwanted_null , len ( rows_having_unwanted_null ) )
1772	def push ( cpu , value , size ) : assert size in ( 8 , 16 , cpu . address_bit_size ) cpu . STACK = cpu . STACK - size // 8 base , _ , _ = cpu . get_descriptor ( cpu . read_register ( 'SS' ) ) address = cpu . STACK + base cpu . write_int ( address , value , size )
11512	def share_item ( self , token , item_id , dest_folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = item_id parameters [ 'dstfolderid' ] = dest_folder_id response = self . request ( 'midas.item.share' , parameters ) return response
11470	def get_filesize ( self , filename ) : result = [ ] def dir_callback ( val ) : result . append ( val . split ( ) [ 4 ] ) self . _ftp . dir ( filename , dir_callback ) return result [ 0 ]
13270	def all_subclasses ( cls ) : for subclass in cls . __subclasses__ ( ) : yield subclass for subc in all_subclasses ( subclass ) : yield subc
2575	def launch_task ( self , task_id , executable , * args , ** kwargs ) : self . tasks [ task_id ] [ 'time_submitted' ] = datetime . datetime . now ( ) hit , memo_fu = self . memoizer . check_memo ( task_id , self . tasks [ task_id ] ) if hit : logger . info ( "Reusing cached result for task {}" . format ( task_id ) ) return memo_fu executor_label = self . tasks [ task_id ] [ "executor" ] try : executor = self . executors [ executor_label ] except Exception : logger . exception ( "Task {} requested invalid executor {}: config is\n{}" . format ( task_id , executor_label , self . _config ) ) if self . monitoring is not None and self . monitoring . resource_monitoring_enabled : executable = self . monitoring . monitor_wrapper ( executable , task_id , self . monitoring . monitoring_hub_url , self . run_id , self . monitoring . resource_monitoring_interval ) with self . submitter_lock : exec_fu = executor . submit ( executable , * args , ** kwargs ) self . tasks [ task_id ] [ 'status' ] = States . launched if self . monitoring is not None : task_log_info = self . _create_task_log_info ( task_id , 'lazy' ) self . monitoring . send ( MessageType . TASK_INFO , task_log_info ) exec_fu . retries_left = self . _config . retries - self . tasks [ task_id ] [ 'fail_count' ] logger . info ( "Task {} launched on executor {}" . format ( task_id , executor . label ) ) return exec_fu
9509	def intersection ( self , i ) : if self . intersects ( i ) : return Interval ( max ( self . start , i . start ) , min ( self . end , i . end ) ) else : return None
4412	def fetch ( self , key : object , default = None ) : return self . _user_data . get ( key , default )
2537	def set_pkg_source_info ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_source_info_set : self . package_source_info_set = True doc . package . source_info = text return True else : raise CardinalityError ( 'Package::SourceInfo' )
11798	def suppose ( self , var , value ) : "Start accumulating inferences from assuming var=value." self . support_pruning ( ) removals = [ ( var , a ) for a in self . curr_domains [ var ] if a != value ] self . curr_domains [ var ] = [ value ] return removals
5300	def with_setup ( self , colormode = None , colorpalette = None , extend_colors = False ) : colorful = Colorful ( colormode = self . colorful . colormode , colorpalette = copy . copy ( self . colorful . colorpalette ) ) colorful . setup ( colormode = colormode , colorpalette = colorpalette , extend_colors = extend_colors ) yield colorful
13001	def hr_diagram_from_data ( data , x_range , y_range ) : _ , color_mapper = hr_diagram_color_helper ( [ ] ) data_dict = { 'x' : list ( data [ 'temperature' ] ) , 'y' : list ( data [ 'luminosity' ] ) , 'color' : list ( data [ 'color' ] ) } source = ColumnDataSource ( data = data_dict ) pf = figure ( y_axis_type = 'log' , x_range = x_range , y_range = y_range ) _diagram ( source = source , plot_figure = pf , color = { 'field' : 'color' , 'transform' : color_mapper } , xaxis_label = 'Temperature (Kelvin)' , yaxis_label = 'Luminosity (solar units)' ) show_with_bokeh_server ( pf )
12894	def set_power ( self , value = False ) : power = ( yield from self . handle_set ( self . API . get ( 'power' ) , int ( value ) ) ) return bool ( power )
7318	def parsemail ( raw_message ) : message = email . parser . Parser ( ) . parsestr ( raw_message ) detected = chardet . detect ( bytearray ( raw_message , "utf-8" ) ) encoding = detected [ "encoding" ] print ( ">>> encoding {}" . format ( encoding ) ) for part in message . walk ( ) : if part . get_content_maintype ( ) == 'multipart' : continue part . set_charset ( encoding ) addrs = email . utils . getaddresses ( message . get_all ( "TO" , [ ] ) ) + email . utils . getaddresses ( message . get_all ( "CC" , [ ] ) ) + email . utils . getaddresses ( message . get_all ( "BCC" , [ ] ) ) recipients = [ x [ 1 ] for x in addrs ] message . __delitem__ ( "bcc" ) message . __setitem__ ( 'Date' , email . utils . formatdate ( ) ) sender = message [ "from" ] return ( message , sender , recipients )
11807	def encode ( plaintext , code ) : "Encodes text, using a code which is a permutation of the alphabet." from string import maketrans trans = maketrans ( alphabet + alphabet . upper ( ) , code + code . upper ( ) ) return plaintext . translate ( trans )
1159	def wait ( self , timeout = None ) : if not self . _is_owned ( ) : raise RuntimeError ( "cannot wait on un-acquired lock" ) waiter = _allocate_lock ( ) waiter . acquire ( ) self . __waiters . append ( waiter ) saved_state = self . _release_save ( ) try : if timeout is None : waiter . acquire ( ) if __debug__ : self . _note ( "%s.wait(): got it" , self ) else : endtime = _time ( ) + timeout delay = 0.0005 while True : gotit = waiter . acquire ( 0 ) if gotit : break remaining = endtime - _time ( ) if remaining <= 0 : break delay = min ( delay * 2 , remaining , .05 ) _sleep ( delay ) if not gotit : if __debug__ : self . _note ( "%s.wait(%s): timed out" , self , timeout ) try : self . __waiters . remove ( waiter ) except ValueError : pass else : if __debug__ : self . _note ( "%s.wait(%s): got it" , self , timeout ) finally : self . _acquire_restore ( saved_state )
1286	def build_metagraph_list ( self ) : ops = [ ] self . ignore_unknown_dtypes = True for key in sorted ( self . meta_params ) : value = self . convert_data_to_string ( self . meta_params [ key ] ) if len ( value ) == 0 : continue if isinstance ( value , str ) : ops . append ( tf . contrib . summary . generic ( name = key , tensor = tf . convert_to_tensor ( str ( value ) ) ) ) else : ops . append ( tf . contrib . summary . generic ( name = key , tensor = tf . as_string ( tf . convert_to_tensor ( value ) ) ) ) return ops
6475	def line ( self , p1 , p2 , resolution = 1 ) : xdiff = max ( p1 . x , p2 . x ) - min ( p1 . x , p2 . x ) ydiff = max ( p1 . y , p2 . y ) - min ( p1 . y , p2 . y ) xdir = [ - 1 , 1 ] [ int ( p1 . x <= p2 . x ) ] ydir = [ - 1 , 1 ] [ int ( p1 . y <= p2 . y ) ] r = int ( round ( max ( xdiff , ydiff ) ) ) if r == 0 : return for i in range ( ( r + 1 ) * resolution ) : x = p1 . x y = p1 . y if xdiff : x += ( float ( i ) * xdiff ) / r * xdir / resolution if ydiff : y += ( float ( i ) * ydiff ) / r * ydir / resolution yield Point ( ( x , y ) )
3508	def nullspace ( A , atol = 1e-13 , rtol = 0 ) : A = np . atleast_2d ( A ) u , s , vh = np . linalg . svd ( A ) tol = max ( atol , rtol * s [ 0 ] ) nnz = ( s >= tol ) . sum ( ) ns = vh [ nnz : ] . conj ( ) . T return ns
782	def jobReactivateRunningJobs ( self ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET _eng_cjm_conn_id=%%s, ' ' _eng_allocate_new_workers=TRUE ' ' WHERE status=%%s ' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . _connectionID , self . STATUS_RUNNING ] ) return
4688	def get_shared_secret ( priv , pub ) : pub_point = pub . point ( ) priv_point = int ( repr ( priv ) , 16 ) res = pub_point * priv_point res_hex = "%032x" % res . x ( ) res_hex = "0" * ( 64 - len ( res_hex ) ) + res_hex return res_hex
13162	def raw_sql ( cls , cur , query : str , values : tuple ) : yield from cur . execute ( query , values ) return ( yield from cur . fetchall ( ) )
622	def coordinatesFromIndex ( index , dimensions ) : coordinates = [ 0 ] * len ( dimensions ) shifted = index for i in xrange ( len ( dimensions ) - 1 , 0 , - 1 ) : coordinates [ i ] = shifted % dimensions [ i ] shifted = shifted / dimensions [ i ] coordinates [ 0 ] = shifted return coordinates
3594	def search ( self , query ) : if self . authSubToken is None : raise LoginError ( "You need to login before executing any request" ) path = SEARCH_URL + "?c=3&q={}" . format ( requests . utils . quote ( query ) ) self . toc ( ) data = self . executeRequestApi2 ( path ) if utils . hasPrefetch ( data ) : response = data . preFetch [ 0 ] . response else : response = data resIterator = response . payload . listResponse . doc return list ( map ( utils . parseProtobufObj , resIterator ) )
8302	def add ( self , callback , name ) : if callback == None : del self . callbacks [ name ] else : self . callbacks [ name ] = callback
13592	def main ( target , label ) : check_environment ( target , label ) click . secho ( 'Fetching tags from the upstream ...' ) handler = TagHandler ( git . list_tags ( ) ) print_information ( handler , label ) tag = handler . yield_tag ( target , label ) confirm ( tag )
1436	def update_count ( self , name , incr_by = 1 , key = None ) : if name not in self . metrics : Log . error ( "In update_count(): %s is not registered in the metric" , name ) if key is None and isinstance ( self . metrics [ name ] , CountMetric ) : self . metrics [ name ] . incr ( incr_by ) elif key is not None and isinstance ( self . metrics [ name ] , MultiCountMetric ) : self . metrics [ name ] . incr ( key , incr_by ) else : Log . error ( "In update_count(): %s is registered but not supported with this method" , name )
12989	def setup_notebook ( debug = False ) : output_notebook ( INLINE , hide_banner = True ) if debug : _setup_logging ( logging . DEBUG ) logging . debug ( 'Running notebook in debug mode.' ) else : _setup_logging ( logging . WARNING ) if 'JUPYTERHUB_SERVICE_PREFIX' not in os . environ : global jupyter_proxy_url jupyter_proxy_url = 'localhost:8888' logging . info ( 'Setting jupyter proxy to local mode.' )
11458	def keep_only_fields ( self ) : for tag in self . record . keys ( ) : if tag not in self . fields_list : record_delete_fields ( self . record , tag )
5193	def send_select_and_operate_command_set ( self , command_set , callback = asiodnp3 . PrintingCommandCallback . Get ( ) , config = opendnp3 . TaskConfig ( ) . Default ( ) ) : self . master . SelectAndOperate ( command_set , callback , config )
5007	def get_user_from_social_auth ( tpa_provider , tpa_username ) : user_social_auth = UserSocialAuth . objects . select_related ( 'user' ) . filter ( user__username = tpa_username , provider = tpa_provider . backend_name ) . first ( ) return user_social_auth . user if user_social_auth else None
7550	def _debug_off ( ) : if _os . path . exists ( __debugflag__ ) : _os . remove ( __debugflag__ ) __loglevel__ = "ERROR" _LOGGER . info ( "debugging turned off" ) _set_debug_dict ( __loglevel__ )
1085	def timetz ( self ) : "Return the time part, with same tzinfo." return time ( self . hour , self . minute , self . second , self . microsecond , self . _tzinfo )
9584	def write_var_data ( fd , data ) : fd . write ( struct . pack ( 'b3xI' , etypes [ 'miMATRIX' ] [ 'n' ] , len ( data ) ) ) fd . write ( data )
4217	def delete_password ( self , service , username ) : if not self . connected ( service ) : raise PasswordDeleteError ( "Cancelled by user" ) if not self . iface . hasEntry ( self . handle , service , username , self . appid ) : raise PasswordDeleteError ( "Password not found" ) self . iface . removeEntry ( self . handle , service , username , self . appid )
1196	def _make_future_features ( node ) : assert isinstance ( node , ast . ImportFrom ) assert node . module == '__future__' features = FutureFeatures ( ) for alias in node . names : name = alias . name if name in _FUTURE_FEATURES : if name not in _IMPLEMENTED_FUTURE_FEATURES : msg = 'future feature {} not yet implemented by grumpy' . format ( name ) raise util . ParseError ( node , msg ) setattr ( features , name , True ) elif name == 'braces' : raise util . ParseError ( node , 'not a chance' ) elif name not in _REDUNDANT_FUTURE_FEATURES : msg = 'future feature {} is not defined' . format ( name ) raise util . ParseError ( node , msg ) return features
11285	def _list_networks ( ) : output = core . run ( "virsh net-list --all" ) networks = { } net_lines = [ n . strip ( ) for n in output . splitlines ( ) [ 2 : ] ] for line in net_lines : if not line : continue name , state , auto = line . split ( ) networks [ name ] = state == "active" return networks
6984	def timebinlc ( lcfile , binsizesec , outdir = None , lcformat = 'hat-sql' , lcformatdir = None , timecols = None , magcols = None , errcols = None , minbinelems = 7 ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if timecols is None : timecols = dtimecols if magcols is None : magcols = dmagcols if errcols is None : errcols = derrcols lcdict = readerfunc ( lcfile ) if ( ( isinstance ( lcdict , ( list , tuple ) ) ) and ( isinstance ( lcdict [ 0 ] , dict ) ) ) : lcdict = lcdict [ 0 ] if 'binned' in lcdict : LOGERROR ( 'this light curve appears to be binned already, skipping...' ) return None lcdict [ 'binned' ] = { } for tcol , mcol , ecol in zip ( timecols , magcols , errcols ) : if '.' in tcol : tcolget = tcol . split ( '.' ) else : tcolget = [ tcol ] times = _dict_get ( lcdict , tcolget ) if '.' in mcol : mcolget = mcol . split ( '.' ) else : mcolget = [ mcol ] mags = _dict_get ( lcdict , mcolget ) if '.' in ecol : ecolget = ecol . split ( '.' ) else : ecolget = [ ecol ] errs = _dict_get ( lcdict , ecolget ) if normfunc is None : ntimes , nmags = normalize_magseries ( times , mags , magsarefluxes = magsarefluxes ) times , mags , errs = ntimes , nmags , errs binned = time_bin_magseries_with_errs ( times , mags , errs , binsize = binsizesec , minbinelems = minbinelems ) lcdict [ 'binned' ] [ mcol ] = { 'times' : binned [ 'binnedtimes' ] , 'mags' : binned [ 'binnedmags' ] , 'errs' : binned [ 'binnederrs' ] , 'nbins' : binned [ 'nbins' ] , 'timebins' : binned [ 'jdbins' ] , 'binsizesec' : binsizesec } if outdir is None : outdir = os . path . dirname ( lcfile ) outfile = os . path . join ( outdir , '%s-binned%.1fsec-%s.pkl' % ( squeeze ( lcdict [ 'objectid' ] ) . replace ( ' ' , '-' ) , binsizesec , lcformat ) ) with open ( outfile , 'wb' ) as outfd : pickle . dump ( lcdict , outfd , protocol = pickle . HIGHEST_PROTOCOL ) return outfile
3307	def _run_cheroot ( app , config , mode ) : assert mode == "cheroot" try : from cheroot import server , wsgi except ImportError : _logger . error ( "*" * 78 ) _logger . error ( "ERROR: Could not import Cheroot." ) _logger . error ( "Try `pip install cheroot` or specify another server using the --server option." ) _logger . error ( "*" * 78 ) raise server_name = "WsgiDAV/{} {} Python/{}" . format ( __version__ , wsgi . Server . version , util . PYTHON_VERSION ) wsgi . Server . version = server_name ssl_certificate = _get_checked_path ( config . get ( "ssl_certificate" ) , config ) ssl_private_key = _get_checked_path ( config . get ( "ssl_private_key" ) , config ) ssl_certificate_chain = _get_checked_path ( config . get ( "ssl_certificate_chain" ) , config ) ssl_adapter = config . get ( "ssl_adapter" , "builtin" ) protocol = "http" if ssl_certificate and ssl_private_key : ssl_adapter = server . get_ssl_adapter_class ( ssl_adapter ) wsgi . Server . ssl_adapter = ssl_adapter ( ssl_certificate , ssl_private_key , ssl_certificate_chain ) protocol = "https" _logger . info ( "SSL / HTTPS enabled. Adapter: {}" . format ( ssl_adapter ) ) elif ssl_certificate or ssl_private_key : raise RuntimeError ( "Option 'ssl_certificate' and 'ssl_private_key' must be used together." ) _logger . info ( "Running {}" . format ( server_name ) ) _logger . info ( "Serving on {}://{}:{} ..." . format ( protocol , config [ "host" ] , config [ "port" ] ) ) server_args = { "bind_addr" : ( config [ "host" ] , config [ "port" ] ) , "wsgi_app" : app , "server_name" : server_name , } server_args . update ( config . get ( "server_args" , { } ) ) server = wsgi . Server ( ** server_args ) startup_event = config . get ( "startup_event" ) if startup_event : def _patched_tick ( ) : server . tick = org_tick _logger . info ( "wsgi.Server is ready" ) startup_event . set ( ) org_tick ( ) org_tick = server . tick server . tick = _patched_tick try : server . start ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) finally : server . stop ( ) return
1659	def CheckCasts ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] match = Search ( r'(\bnew\s+(?:const\s+)?|\S<\s*(?:const\s+)?)?\b' r'(int|float|double|bool|char|int32|uint32|int64|uint64)' r'(\([^)].*)' , line ) expecting_function = ExpectingFunctionArgs ( clean_lines , linenum ) if match and not expecting_function : matched_type = match . group ( 2 ) matched_new_or_template = match . group ( 1 ) if Match ( r'\([^()]+\)\s*\[' , match . group ( 3 ) ) : return matched_funcptr = match . group ( 3 ) if ( matched_new_or_template is None and not ( matched_funcptr and ( Match ( r'\((?:[^() ]+::\s*\*\s*)?[^() ]+\)\s*\(' , matched_funcptr ) or matched_funcptr . startswith ( '(*)' ) ) ) and not Match ( r'\s*using\s+\S+\s*=\s*' + matched_type , line ) and not Search ( r'new\(\S+\)\s*' + matched_type , line ) ) : error ( filename , linenum , 'readability/casting' , 4 , 'Using deprecated casting style. ' 'Use static_cast<%s>(...) instead' % matched_type ) if not expecting_function : CheckCStyleCast ( filename , clean_lines , linenum , 'static_cast' , r'\((int|float|double|bool|char|u?int(16|32|64))\)' , error ) if CheckCStyleCast ( filename , clean_lines , linenum , 'const_cast' , r'\((char\s?\*+\s?)\)\s*"' , error ) : pass else : CheckCStyleCast ( filename , clean_lines , linenum , 'reinterpret_cast' , r'\((\w+\s?\*+\s?)\)' , error ) match = Search ( r'(?:[^\w]&\(([^)*][^)]*)\)[\w(])|' r'(?:[^\w]&(static|dynamic|down|reinterpret)_cast\b)' , line ) if match : parenthesis_error = False match = Match ( r'^(.*&(?:static|dynamic|down|reinterpret)_cast\b)<' , line ) if match : _ , y1 , x1 = CloseExpression ( clean_lines , linenum , len ( match . group ( 1 ) ) ) if x1 >= 0 and clean_lines . elided [ y1 ] [ x1 ] == '(' : _ , y2 , x2 = CloseExpression ( clean_lines , y1 , x1 ) if x2 >= 0 : extended_line = clean_lines . elided [ y2 ] [ x2 : ] if y2 < clean_lines . NumLines ( ) - 1 : extended_line += clean_lines . elided [ y2 + 1 ] if Match ( r'\s*(?:->|\[)' , extended_line ) : parenthesis_error = True if parenthesis_error : error ( filename , linenum , 'readability/casting' , 4 , ( 'Are you taking an address of something dereferenced ' 'from a cast? Wrapping the dereferenced expression in ' 'parentheses will make the binding more obvious' ) ) else : error ( filename , linenum , 'runtime/casting' , 4 , ( 'Are you taking an address of a cast? ' 'This is dangerous: could be a temp var. ' 'Take the address before doing the cast, rather than after' ) )
6498	def remove ( self , doc_type , doc_ids , ** kwargs ) : try : actions = [ ] for doc_id in doc_ids : log . debug ( "Removing document of type %s and index %s" , doc_type , doc_id ) action = { '_op_type' : 'delete' , "_index" : self . index_name , "_type" : doc_type , "_id" : doc_id } actions . append ( action ) bulk ( self . _es , actions , ** kwargs ) except BulkIndexError as ex : valid_errors = [ error for error in ex . errors if error [ 'delete' ] [ 'status' ] != 404 ] if valid_errors : log . exception ( "An error occurred while removing documents from the index." ) raise
3188	def create ( self , list_id , subscriber_hash , data ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash if 'note' not in data : raise KeyError ( 'The list member note must have a note' ) response = self . _mc_client . _post ( url = self . _build_path ( list_id , 'members' , subscriber_hash , 'notes' ) , data = data ) if response is not None : self . note_id = response [ 'id' ] else : self . note_id = None return response
11548	def guess_array_memory_usage ( bam_readers , dtype , use_strand = False ) : ARRAY_COUNT = 5 if not isinstance ( bam_readers , list ) : bam_readers = [ bam_readers ] if isinstance ( dtype , basestring ) : dtype = NUMPY_DTYPES . get ( dtype , None ) use_strand = use_strand + 1 dtypes = guess_numpy_dtypes_from_idxstats ( bam_readers , default = None , force_dtype = False ) if not [ dt for dt in dtypes if dt is not None ] : dtypes = guess_numpy_dtypes_from_idxstats ( bam_readers , default = dtype or numpy . uint64 , force_dtype = True ) elif dtype : dtypes = [ dtype if dt else None for dt in dtypes ] read_groups = [ ] no_read_group = False for bam in bam_readers : rgs = bam . get_read_groups ( ) if rgs : for rg in rgs : if rg not in read_groups : read_groups . append ( rg ) else : no_read_group = True read_groups = len ( read_groups ) + no_read_group max_ref_size = 0 array_byte_overhead = sys . getsizeof ( numpy . zeros ( ( 0 ) , dtype = numpy . uint64 ) ) array_count = ARRAY_COUNT * use_strand * read_groups for bam in bam_readers : for i , ( name , length ) in enumerate ( bam . get_references ( ) ) : if dtypes [ i ] is not None : max_ref_size = max ( max_ref_size , ( length + length * dtypes [ i ] ( ) . nbytes * array_count + ( array_byte_overhead * ( array_count + 1 ) ) ) ) return max_ref_size
7529	def setup_dirs ( data ) : pdir = os . path . realpath ( data . paramsdict [ "project_dir" ] ) data . dirs . clusts = os . path . join ( pdir , "{}_clust_{}" . format ( data . name , data . paramsdict [ "clust_threshold" ] ) ) if not os . path . exists ( data . dirs . clusts ) : os . mkdir ( data . dirs . clusts ) data . tmpdir = os . path . abspath ( os . path . expanduser ( os . path . join ( pdir , data . name + '-tmpalign' ) ) ) if not os . path . exists ( data . tmpdir ) : os . mkdir ( data . tmpdir ) if not data . paramsdict [ "assembly_method" ] == "denovo" : data . dirs . refmapping = os . path . join ( pdir , "{}_refmapping" . format ( data . name ) ) if not os . path . exists ( data . dirs . refmapping ) : os . mkdir ( data . dirs . refmapping )
8200	def set_bot ( self , bot ) : self . bot = bot self . sink . set_bot ( bot )
3871	def next_event ( self , event_id , prev = False ) : i = self . events . index ( self . _events_dict [ event_id ] ) if prev and i > 0 : return self . events [ i - 1 ] elif not prev and i + 1 < len ( self . events ) : return self . events [ i + 1 ] else : return None
13410	def addLogbooks ( self , type = None , logs = [ ] , default = "" ) : if type is not None and len ( logs ) != 0 : if type in self . logList : for logbook in logs : if logbook not in self . logList . get ( type ) [ 0 ] : self . logList . get ( type ) [ 0 ] . append ( logbook ) else : self . logList [ type ] = [ ] self . logList [ type ] . append ( logs ) if len ( self . logList [ type ] ) > 1 and default != "" : self . logList . get ( type ) [ 1 ] == default else : self . logList . get ( type ) . append ( default ) self . logType . clear ( ) self . logType . addItems ( list ( self . logList . keys ( ) ) ) self . changeLogType ( )
13067	def make_parents ( self , collection , lang = None ) : return [ { "id" : member . id , "label" : str ( member . get_label ( lang ) ) , "model" : str ( member . model ) , "type" : str ( member . type ) , "size" : member . size } for member in collection . parents if member . get_label ( ) ]
5918	def outfile ( self , p ) : if self . outdir is not None : return os . path . join ( self . outdir , os . path . basename ( p ) ) else : return p
3360	def elements ( self ) : tmp_formula = self . formula if tmp_formula is None : return { } tmp_formula = str ( self . formula ) if "*" in tmp_formula : warn ( "invalid character '*' found in formula '%s'" % self . formula ) tmp_formula = tmp_formula . replace ( "*" , "" ) if "(" in tmp_formula or ")" in tmp_formula : warn ( "invalid formula (has parenthesis) in '%s'" % self . formula ) return None composition = { } parsed = element_re . findall ( tmp_formula ) for ( element , count ) in parsed : if count == '' : count = 1 else : try : count = float ( count ) int_count = int ( count ) if count == int_count : count = int_count else : warn ( "%s is not an integer (in formula %s)" % ( count , self . formula ) ) except ValueError : warn ( "failed to parse %s (in formula %s)" % ( count , self . formula ) ) return None if element in composition : composition [ element ] += count else : composition [ element ] = count return composition
13870	def NormalizePath ( path ) : if path . endswith ( '/' ) or path . endswith ( '\\' ) : slash = os . path . sep else : slash = '' return os . path . normpath ( path ) + slash
11508	def download_item ( self , item_id , token = None , revision = None ) : parameters = dict ( ) parameters [ 'id' ] = item_id if token : parameters [ 'token' ] = token if revision : parameters [ 'revision' ] = revision method_url = self . full_url + 'midas.item.download' request = requests . get ( method_url , params = parameters , stream = True , verify = self . _verify_ssl_certificate ) filename = request . headers [ 'content-disposition' ] [ 21 : ] . strip ( '"' ) return filename , request . iter_content ( chunk_size = 10 * 1024 )
6900	def parallel_periodicfeatures_lcdir ( pfpkl_dir , lcbasedir , outdir , pfpkl_glob = 'periodfinding-*.pkl*' , starfeaturesdir = None , fourierorder = 5 , transitparams = ( - 0.01 , 0.1 , 0.1 ) , ebparams = ( - 0.2 , 0.3 , 0.7 , 0.5 ) , pdiff_threshold = 1.0e-4 , sidereal_threshold = 1.0e-4 , sampling_peak_multiplier = 5.0 , sampling_startp = None , sampling_endp = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , sigclip = 10.0 , verbose = False , maxobjects = None , nworkers = NCPUS , recursive = True , ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None fileglob = pfpkl_glob LOGINFO ( 'searching for periodfinding pickles in %s ...' % pfpkl_dir ) if recursive is False : matching = glob . glob ( os . path . join ( pfpkl_dir , fileglob ) ) else : if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( pfpkl_dir , '**' , fileglob ) , recursive = True ) else : walker = os . walk ( pfpkl_dir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , fileglob ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) if matching and len ( matching ) > 0 : LOGINFO ( 'found %s periodfinding pickles, getting periodicfeatures...' % len ( matching ) ) return parallel_periodicfeatures ( matching , lcbasedir , outdir , starfeaturesdir = starfeaturesdir , fourierorder = fourierorder , transitparams = transitparams , ebparams = ebparams , pdiff_threshold = pdiff_threshold , sidereal_threshold = sidereal_threshold , sampling_peak_multiplier = sampling_peak_multiplier , sampling_startp = sampling_startp , sampling_endp = sampling_endp , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = lcformatdir , sigclip = sigclip , verbose = verbose , maxobjects = maxobjects , nworkers = nworkers , ) else : LOGERROR ( 'no periodfinding pickles found in %s' % ( pfpkl_dir ) ) return None
2862	def _i2c_write_bytes ( self , data ) : for byte in data : self . _command . append ( str ( bytearray ( ( 0x11 , 0x00 , 0x00 , byte ) ) ) ) self . _ft232h . output_pins ( { 0 : GPIO . LOW , 1 : GPIO . HIGH } , write = False ) self . _command . append ( self . _ft232h . mpsse_gpio ( ) * _REPEAT_DELAY ) self . _command . append ( '\x22\x00' ) self . _expected += len ( data )
9576	def read_header ( fd , endian ) : flag_class , nzmax = read_elements ( fd , endian , [ 'miUINT32' ] ) header = { 'mclass' : flag_class & 0x0FF , 'is_logical' : ( flag_class >> 9 & 1 ) == 1 , 'is_global' : ( flag_class >> 10 & 1 ) == 1 , 'is_complex' : ( flag_class >> 11 & 1 ) == 1 , 'nzmax' : nzmax } header [ 'dims' ] = read_elements ( fd , endian , [ 'miINT32' ] ) header [ 'n_dims' ] = len ( header [ 'dims' ] ) if header [ 'n_dims' ] != 2 : raise ParseError ( 'Only matrices with dimension 2 are supported.' ) header [ 'name' ] = read_elements ( fd , endian , [ 'miINT8' ] , is_name = True ) return header
3330	def acquire_write ( self , timeout = None ) : if timeout is not None : endtime = time ( ) + timeout me , upgradewriter = currentThread ( ) , False self . __condition . acquire ( ) try : if self . __writer is me : self . __writercount += 1 return elif me in self . __readers : if self . __upgradewritercount : raise ValueError ( "Inevitable dead lock, denying write lock" ) upgradewriter = True self . __upgradewritercount = self . __readers . pop ( me ) else : self . __pendingwriters . append ( me ) while True : if not self . __readers and self . __writer is None : if self . __upgradewritercount : if upgradewriter : self . __writer = me self . __writercount = self . __upgradewritercount + 1 self . __upgradewritercount = 0 return elif self . __pendingwriters [ 0 ] is me : self . __writer = me self . __writercount = 1 self . __pendingwriters = self . __pendingwriters [ 1 : ] return if timeout is not None : remaining = endtime - time ( ) if remaining <= 0 : if upgradewriter : self . __readers [ me ] = self . __upgradewritercount self . __upgradewritercount = 0 else : self . __pendingwriters . remove ( me ) raise RuntimeError ( "Acquiring write lock timed out" ) self . __condition . wait ( remaining ) else : self . __condition . wait ( ) finally : self . __condition . release ( )
4944	def get_course_data_sharing_consent ( username , course_id , enterprise_customer_uuid ) : DataSharingConsent = apps . get_model ( 'consent' , 'DataSharingConsent' ) return DataSharingConsent . objects . proxied_get ( username = username , course_id = course_id , enterprise_customer__uuid = enterprise_customer_uuid )
988	def createTemporalAnomaly ( recordParams , spatialParams = _SP_PARAMS , temporalParams = _TM_PARAMS , verbosity = _VERBOSITY ) : inputFilePath = recordParams [ "inputFilePath" ] scalarEncoderArgs = recordParams [ "scalarEncoderArgs" ] dateEncoderArgs = recordParams [ "dateEncoderArgs" ] scalarEncoder = ScalarEncoder ( ** scalarEncoderArgs ) dateEncoder = DateEncoder ( ** dateEncoderArgs ) encoder = MultiEncoder ( ) encoder . addEncoder ( scalarEncoderArgs [ "name" ] , scalarEncoder ) encoder . addEncoder ( dateEncoderArgs [ "name" ] , dateEncoder ) network = Network ( ) network . addRegion ( "sensor" , "py.RecordSensor" , json . dumps ( { "verbosity" : verbosity } ) ) sensor = network . regions [ "sensor" ] . getSelf ( ) sensor . encoder = encoder sensor . dataSource = FileRecordStream ( streamID = inputFilePath ) spatialParams [ "inputWidth" ] = sensor . encoder . getWidth ( ) network . addRegion ( "spatialPoolerRegion" , "py.SPRegion" , json . dumps ( spatialParams ) ) network . link ( "sensor" , "spatialPoolerRegion" , "UniformLink" , "" ) network . link ( "sensor" , "spatialPoolerRegion" , "UniformLink" , "" , srcOutput = "resetOut" , destInput = "resetIn" ) network . link ( "spatialPoolerRegion" , "sensor" , "UniformLink" , "" , srcOutput = "spatialTopDownOut" , destInput = "spatialTopDownIn" ) network . link ( "spatialPoolerRegion" , "sensor" , "UniformLink" , "" , srcOutput = "temporalTopDownOut" , destInput = "temporalTopDownIn" ) network . addRegion ( "temporalPoolerRegion" , "py.TMRegion" , json . dumps ( temporalParams ) ) network . link ( "spatialPoolerRegion" , "temporalPoolerRegion" , "UniformLink" , "" ) network . link ( "temporalPoolerRegion" , "spatialPoolerRegion" , "UniformLink" , "" , srcOutput = "topDownOut" , destInput = "topDownIn" ) spatialPoolerRegion = network . regions [ "spatialPoolerRegion" ] spatialPoolerRegion . setParameter ( "learningMode" , True ) spatialPoolerRegion . setParameter ( "anomalyMode" , False ) temporalPoolerRegion = network . regions [ "temporalPoolerRegion" ] temporalPoolerRegion . setParameter ( "topDownMode" , True ) temporalPoolerRegion . setParameter ( "learningMode" , True ) temporalPoolerRegion . setParameter ( "inferenceMode" , True ) temporalPoolerRegion . setParameter ( "anomalyMode" , True ) return network
13317	def remove ( name_or_path ) : r = resolve ( name_or_path ) r . resolved [ 0 ] . remove ( ) EnvironmentCache . discard ( r . resolved [ 0 ] ) EnvironmentCache . save ( )
5775	def _advapi32_sign ( private_key , data , hash_algorithm , rsa_pss_padding = False ) : algo = private_key . algorithm if algo == 'rsa' and hash_algorithm == 'raw' : padded_data = add_pkcs1v15_signature_padding ( private_key . byte_size , data ) return raw_rsa_private_crypt ( private_key , padded_data ) if algo == 'rsa' and rsa_pss_padding : hash_length = { 'sha1' : 20 , 'sha224' : 28 , 'sha256' : 32 , 'sha384' : 48 , 'sha512' : 64 } . get ( hash_algorithm , 0 ) padded_data = add_pss_padding ( hash_algorithm , hash_length , private_key . bit_size , data ) return raw_rsa_private_crypt ( private_key , padded_data ) if private_key . algorithm == 'dsa' and hash_algorithm == 'md5' : raise ValueError ( pretty_message ( ) ) hash_handle = None try : alg_id = { 'md5' : Advapi32Const . CALG_MD5 , 'sha1' : Advapi32Const . CALG_SHA1 , 'sha256' : Advapi32Const . CALG_SHA_256 , 'sha384' : Advapi32Const . CALG_SHA_384 , 'sha512' : Advapi32Const . CALG_SHA_512 , } [ hash_algorithm ] hash_handle_pointer = new ( advapi32 , 'HCRYPTHASH *' ) res = advapi32 . CryptCreateHash ( private_key . context_handle , alg_id , null ( ) , 0 , hash_handle_pointer ) handle_error ( res ) hash_handle = unwrap ( hash_handle_pointer ) res = advapi32 . CryptHashData ( hash_handle , data , len ( data ) , 0 ) handle_error ( res ) out_len = new ( advapi32 , 'DWORD *' ) res = advapi32 . CryptSignHashW ( hash_handle , Advapi32Const . AT_SIGNATURE , null ( ) , 0 , null ( ) , out_len ) handle_error ( res ) buffer_length = deref ( out_len ) buffer_ = buffer_from_bytes ( buffer_length ) res = advapi32 . CryptSignHashW ( hash_handle , Advapi32Const . AT_SIGNATURE , null ( ) , 0 , buffer_ , out_len ) handle_error ( res ) output = bytes_from_buffer ( buffer_ , deref ( out_len ) ) output = output [ : : - 1 ] if algo == 'dsa' : half_len = len ( output ) // 2 output = output [ half_len : ] + output [ : half_len ] output = algos . DSASignature . from_p1363 ( output ) . dump ( ) return output finally : if hash_handle : advapi32 . CryptDestroyHash ( hash_handle )
8754	def get_groups_to_ack ( groups_to_ack , init_sg_states , curr_sg_states ) : security_groups_changed = [ ] for vif in groups_to_ack : initial_state = init_sg_states [ vif ] [ sg_cli . SECURITY_GROUP_HASH_ATTR ] current_state = curr_sg_states [ vif ] [ sg_cli . SECURITY_GROUP_HASH_ATTR ] bad_match_msg = ( 'security group rules were changed for vif "%s" while' ' executing xapi_client.update_interfaces.' ' Will not ack rule.' % vif ) if len ( initial_state ) != len ( current_state ) : security_groups_changed . append ( vif ) LOG . info ( bad_match_msg ) elif len ( initial_state ) > 0 : for rule in current_state : if rule not in initial_state : security_groups_changed . append ( vif ) LOG . info ( bad_match_msg ) break ret = [ group for group in groups_to_ack if group not in security_groups_changed ] return ret
2507	def get_extr_lics_xref ( self , extr_lic ) : xrefs = list ( self . graph . triples ( ( extr_lic , RDFS . seeAlso , None ) ) ) return map ( lambda xref_triple : xref_triple [ 2 ] , xrefs )
8185	def offset ( self , node ) : x = self . x + node . x - _ctx . WIDTH / 2 y = self . y + node . y - _ctx . HEIGHT / 2 return x , y
12670	def create ( _ ) : endpoint = client_endpoint ( ) if not endpoint : raise CLIError ( "Connection endpoint not found. " "Before running sfctl commands, connect to a cluster using " "the 'sfctl cluster select' command." ) no_verify = no_verify_setting ( ) if security_type ( ) == 'aad' : auth = AdalAuthentication ( no_verify ) else : cert = cert_info ( ) ca_cert = ca_cert_info ( ) auth = ClientCertAuthentication ( cert , ca_cert , no_verify ) return ServiceFabricClientAPIs ( auth , base_url = endpoint )
8995	def relative_file ( self , module , file ) : path = self . _relative_to_absolute ( module , file ) return self . path ( path )
7740	def hold_exception ( method ) : @ functools . wraps ( method ) def wrapper ( self , * args , ** kwargs ) : try : return method ( self , * args , ** kwargs ) except Exception : if self . exc_info : raise if not self . _stack : logger . debug ( '@hold_exception wrapped method {0!r} called' ' from outside of the main loop' . format ( method ) ) raise self . exc_info = sys . exc_info ( ) logger . debug ( u"exception in glib main loop callback:" , exc_info = self . exc_info ) main_loop = self . _stack [ - 1 ] if main_loop is not None : main_loop . quit ( ) return False return wrapper
5468	def get_event_of_type ( op , event_type ) : events = get_events ( op ) if not events : return None return [ e for e in events if e . get ( 'details' , { } ) . get ( '@type' ) == event_type ]
2272	def _win32_is_junction ( path ) : if not exists ( path ) : if os . path . isdir ( path ) : if not os . path . islink ( path ) : return True return False return jwfs . is_reparse_point ( path ) and not os . path . islink ( path )
2145	def request ( self , method , url , * args , ** kwargs ) : import re url = re . sub ( "^/?api/v[0-9]+/" , "" , url ) use_version = not url . startswith ( '/o/' ) url = '%s%s' % ( self . get_prefix ( use_version ) , url . lstrip ( '/' ) ) kwargs . setdefault ( 'auth' , BasicTowerAuth ( settings . username , settings . password , self ) ) headers = kwargs . get ( 'headers' , { } ) if method . upper ( ) in ( 'PATCH' , 'POST' , 'PUT' ) : headers . setdefault ( 'Content-Type' , 'application/json' ) kwargs [ 'headers' ] = headers debug . log ( '%s %s' % ( method , url ) , fg = 'blue' , bold = True ) if method in ( 'POST' , 'PUT' , 'PATCH' ) : debug . log ( 'Data: %s' % kwargs . get ( 'data' , { } ) , fg = 'blue' , bold = True ) if method == 'GET' or kwargs . get ( 'params' , None ) : debug . log ( 'Params: %s' % kwargs . get ( 'params' , { } ) , fg = 'blue' , bold = True ) debug . log ( '' ) if headers . get ( 'Content-Type' , '' ) == 'application/json' : kwargs [ 'data' ] = json . dumps ( kwargs . get ( 'data' , { } ) ) r = self . _make_request ( method , url , args , kwargs ) if r . status_code >= 500 : raise exc . ServerError ( 'The Tower server sent back a server error. ' 'Please try again later.' ) if r . status_code == 401 : raise exc . AuthError ( 'Invalid Tower authentication credentials (HTTP 401).' ) if r . status_code == 403 : raise exc . Forbidden ( "You don't have permission to do that (HTTP 403)." ) if r . status_code == 404 : raise exc . NotFound ( 'The requested object could not be found.' ) if r . status_code == 405 : raise exc . MethodNotAllowed ( "The Tower server says you can't make a request with the " "%s method to that URL (%s)." % ( method , url ) , ) if r . status_code >= 400 : raise exc . BadRequest ( 'The Tower server claims it was sent a bad request.\n\n' '%s %s\nParams: %s\nData: %s\n\nResponse: %s' % ( method , url , kwargs . get ( 'params' , None ) , kwargs . get ( 'data' , None ) , r . content . decode ( 'utf8' ) ) ) r . __class__ = APIResponse return r
787	def partitionAtIntervals ( data , intervals ) : assert sum ( intervals ) <= len ( data ) start = 0 for interval in intervals : end = start + interval yield data [ start : end ] start = end raise StopIteration
10231	def list_abundance_expansion ( graph : BELGraph ) -> None : mapping = { node : flatten_list_abundance ( node ) for node in graph if isinstance ( node , ListAbundance ) } relabel_nodes ( graph , mapping , copy = False )
2704	def collect_entities ( sent , ranks , stopwords , spacy_nlp ) : global DEBUG sent_text = " " . join ( [ w . raw for w in sent ] ) if DEBUG : print ( "sent:" , sent_text ) for ent in spacy_nlp ( sent_text ) . ents : if DEBUG : print ( "NER:" , ent . label_ , ent . text ) if ( ent . label_ not in [ "CARDINAL" ] ) and ( ent . text . lower ( ) not in stopwords ) : w_ranks , w_ids = find_entity ( sent , ranks , ent . text . split ( " " ) , 0 ) if w_ranks and w_ids : rl = RankedLexeme ( text = ent . text . lower ( ) , rank = w_ranks , ids = w_ids , pos = "np" , count = 1 ) if DEBUG : print ( rl ) yield rl
2374	def append ( self , row ) : if len ( row ) == 0 : return if ( row [ 0 ] != "" and ( not row [ 0 ] . lstrip ( ) . startswith ( "#" ) ) ) : self . _children . append ( self . _childClass ( self . parent , row . linenumber , row [ 0 ] ) ) if len ( row . cells ) > 1 : row [ 0 ] = "" self . _children [ - 1 ] . append ( row . linenumber , row . raw_text , row . cells ) elif len ( self . _children ) == 0 : self . comments . append ( row ) else : if len ( row . cells ) > 0 : self . _children [ - 1 ] . append ( row . linenumber , row . raw_text , row . cells )
8946	def run ( self , cmd , * args , ** kwargs ) : runner = self . ctx . run if self . ctx else None return run ( cmd , runner = runner , * args , ** kwargs )
1142	def _long2bytesBigEndian ( n , blocksize = 0 ) : s = b'' pack = struct . pack while n > 0 : s = pack ( '>I' , n & 0xffffffff ) + s n = n >> 32 for i in range ( len ( s ) ) : if s [ i ] != '\000' : break else : s = '\000' i = 0 s = s [ i : ] if blocksize > 0 and len ( s ) % blocksize : s = ( blocksize - len ( s ) % blocksize ) * '\000' + s return s
11694	def full_analysis ( self ) : self . count ( ) self . verify_words ( ) self . verify_user ( ) if self . review_requested == 'yes' : self . label_suspicious ( 'Review requested' )
5329	def get_raw ( config , backend_section , arthur ) : if arthur : task = TaskRawDataArthurCollection ( config , backend_section = backend_section ) else : task = TaskRawDataCollection ( config , backend_section = backend_section ) TaskProjects ( config ) . execute ( ) try : task . execute ( ) logging . info ( "Loading raw data finished!" ) except Exception as e : logging . error ( str ( e ) ) sys . exit ( - 1 )
2802	def convert_concat ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting concat ...' ) concat_nodes = [ layers [ i ] for i in inputs ] if len ( concat_nodes ) == 1 : layers [ scope_name ] = concat_nodes [ 0 ] return if names == 'short' : tf_name = 'CAT' + random_string ( 5 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) cat = keras . layers . Concatenate ( name = tf_name , axis = params [ 'axis' ] ) layers [ scope_name ] = cat ( concat_nodes )
27	def nn ( input , layers_sizes , reuse = None , flatten = False , name = "" ) : for i , size in enumerate ( layers_sizes ) : activation = tf . nn . relu if i < len ( layers_sizes ) - 1 else None input = tf . layers . dense ( inputs = input , units = size , kernel_initializer = tf . contrib . layers . xavier_initializer ( ) , reuse = reuse , name = name + '_' + str ( i ) ) if activation : input = activation ( input ) if flatten : assert layers_sizes [ - 1 ] == 1 input = tf . reshape ( input , [ - 1 ] ) return input
10948	def reset ( self , ** kwargs ) : self . aug_state . reset ( ) super ( LMAugmentedState , self ) . reset ( ** kwargs )
11054	def rm_fwd_refs ( obj ) : for stack , key in obj . _backrefs_flat : backref_key , parent_schema_name , parent_field_name = stack parent_schema = obj . _collections [ parent_schema_name ] parent_key_store = parent_schema . _pk_to_storage ( key ) parent_object = parent_schema . load ( parent_key_store ) if parent_object is None : continue if parent_object . _fields [ parent_field_name ] . _list : getattr ( parent_object , parent_field_name ) . remove ( obj ) else : parent_field_object = parent_object . _fields [ parent_field_name ] setattr ( parent_object , parent_field_name , parent_field_object . _gen_default ( ) ) parent_object . save ( )
996	def _updateStatsInferEnd ( self , stats , bottomUpNZ , predictedState , colConfidence ) : if not self . collectStats : return stats [ 'nInfersSinceReset' ] += 1 ( numExtra2 , numMissing2 , confidences2 ) = self . _checkPrediction ( patternNZs = [ bottomUpNZ ] , output = predictedState , colConfidence = colConfidence ) predictionScore , positivePredictionScore , negativePredictionScore = ( confidences2 [ 0 ] ) stats [ 'curPredictionScore2' ] = float ( predictionScore ) stats [ 'curFalseNegativeScore' ] = 1.0 - float ( positivePredictionScore ) stats [ 'curFalsePositiveScore' ] = float ( negativePredictionScore ) stats [ 'curMissing' ] = numMissing2 stats [ 'curExtra' ] = numExtra2 if stats [ 'nInfersSinceReset' ] <= self . burnIn : return stats [ 'nPredictions' ] += 1 numExpected = max ( 1.0 , float ( len ( bottomUpNZ ) ) ) stats [ 'totalMissing' ] += numMissing2 stats [ 'totalExtra' ] += numExtra2 stats [ 'pctExtraTotal' ] += 100.0 * numExtra2 / numExpected stats [ 'pctMissingTotal' ] += 100.0 * numMissing2 / numExpected stats [ 'predictionScoreTotal2' ] += float ( predictionScore ) stats [ 'falseNegativeScoreTotal' ] += 1.0 - float ( positivePredictionScore ) stats [ 'falsePositiveScoreTotal' ] += float ( negativePredictionScore ) if self . collectSequenceStats : cc = self . cellConfidence [ 't-1' ] * self . infActiveState [ 't' ] sconf = cc . sum ( axis = 1 ) for c in range ( self . numberOfCols ) : if sconf [ c ] > 0 : cc [ c , : ] /= sconf [ c ] self . _internalStats [ 'confHistogram' ] += cc
10457	def getaccesskey ( self , window_name , object_name ) : menu_handle = self . _get_menu_handle ( window_name , object_name ) key = menu_handle . AXMenuItemCmdChar modifiers = menu_handle . AXMenuItemCmdModifiers glpyh = menu_handle . AXMenuItemCmdGlyph virtual_key = menu_handle . AXMenuItemCmdVirtualKey modifiers_type = "" if modifiers == 0 : modifiers_type = "<command>" elif modifiers == 1 : modifiers_type = "<shift><command>" elif modifiers == 2 : modifiers_type = "<option><command>" elif modifiers == 3 : modifiers_type = "<option><shift><command>" elif modifiers == 4 : modifiers_type = "<ctrl><command>" elif modifiers == 6 : modifiers_type = "<ctrl><option><command>" if virtual_key == 115 and glpyh == 102 : modifiers = "<option>" key = "<cursor_left>" elif virtual_key == 119 and glpyh == 105 : modifiers = "<option>" key = "<right>" elif virtual_key == 116 and glpyh == 98 : modifiers = "<option>" key = "<up>" elif virtual_key == 121 and glpyh == 107 : modifiers = "<option>" key = "<down>" elif virtual_key == 126 and glpyh == 104 : key = "<up>" elif virtual_key == 125 and glpyh == 106 : key = "<down>" elif virtual_key == 124 and glpyh == 101 : key = "<right>" elif virtual_key == 123 and glpyh == 100 : key = "<left>" elif virtual_key == 53 and glpyh == 27 : key = "<escape>" if not key : raise LdtpServerException ( "No access key associated" ) return modifiers_type + key
1435	def register_metrics ( self , metrics_collector , interval ) : for field , metrics in self . metrics . items ( ) : metrics_collector . register_metric ( field , metrics , interval )
12417	def replaced_directory ( dirname ) : if dirname [ - 1 ] == '/' : dirname = dirname [ : - 1 ] full_path = os . path . abspath ( dirname ) if not os . path . isdir ( full_path ) : raise AttributeError ( 'dir_name must be a directory' ) base , name = os . path . split ( full_path ) tempdir = tempfile . mkdtemp ( ) shutil . move ( full_path , tempdir ) os . mkdir ( full_path ) try : yield tempdir finally : shutil . rmtree ( full_path ) moved = os . path . join ( tempdir , name ) shutil . move ( moved , base ) shutil . rmtree ( tempdir )
12977	def deleteOne ( self , obj , conn = None ) : if not getattr ( obj , '_id' , None ) : return 0 if conn is None : conn = self . _get_connection ( ) pipeline = conn . pipeline ( ) executeAfter = True else : pipeline = conn executeAfter = False pipeline . delete ( self . _get_key_for_id ( obj . _id ) ) self . _rem_id_from_keys ( obj . _id , pipeline ) for indexedFieldName in self . indexedFields : self . _rem_id_from_index ( indexedFieldName , obj . _id , obj . _origData [ indexedFieldName ] , pipeline ) obj . _id = None if executeAfter is True : pipeline . execute ( ) return 1
2429	def reset_document ( self ) : self . doc_version_set = False self . doc_comment_set = False self . doc_namespace_set = False self . doc_data_lics_set = False self . doc_name_set = False self . doc_spdx_id_set = False
3555	def power_on ( self , timeout_sec = TIMEOUT_SEC ) : self . _powered_on . clear ( ) IOBluetoothPreferenceSetControllerPowerState ( 1 ) if not self . _powered_on . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to power on!' )
653	def spDiff ( SP1 , SP2 ) : if ( len ( SP1 . _masterConnectedM ) != len ( SP2 . _masterConnectedM ) ) : print "Connected synapse matrices are different sizes" return False if ( len ( SP1 . _masterPotentialM ) != len ( SP2 . _masterPotentialM ) ) : print "Potential synapse matrices are different sizes" return False if ( len ( SP1 . _masterPermanenceM ) != len ( SP2 . _masterPermanenceM ) ) : print "Permanence matrices are different sizes" return False for i in range ( 0 , len ( SP1 . _masterConnectedM ) ) : connected1 = SP1 . _masterConnectedM [ i ] connected2 = SP2 . _masterConnectedM [ i ] if ( connected1 != connected2 ) : print "Connected Matrices for cell %d different" % ( i ) return False permanences1 = SP1 . _masterPermanenceM [ i ] permanences2 = SP2 . _masterPermanenceM [ i ] if ( permanences1 != permanences2 ) : print "Permanence Matrices for cell %d different" % ( i ) return False potential1 = SP1 . _masterPotentialM [ i ] potential2 = SP2 . _masterPotentialM [ i ] if ( potential1 != potential2 ) : print "Potential Matrices for cell %d different" % ( i ) return False if ( not numpy . array_equal ( SP1 . _firingBoostFactors , SP2 . _firingBoostFactors ) ) : print "Firing boost factors are different between spatial poolers" return False if ( not numpy . array_equal ( SP1 . _dutyCycleAfterInh , SP2 . _dutyCycleAfterInh ) ) : print "Duty cycles after inhibition are different between spatial poolers" return False if ( not numpy . array_equal ( SP1 . _dutyCycleBeforeInh , SP2 . _dutyCycleBeforeInh ) ) : print "Duty cycles before inhibition are different between spatial poolers" return False print ( "Spatial Poolers are equivalent" ) return True
7090	def _lclist_parallel_worker ( task ) : lcf , columns , lcformat , lcformatdir , lcndetkey = task try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None lcobjdict = { 'lcfname' : os . path . abspath ( lcf ) } try : lcdict = readerfunc ( lcf ) if ( ( isinstance ( lcdict , ( list , tuple ) ) ) and ( isinstance ( lcdict [ 0 ] , dict ) ) ) : lcdict = lcdict [ 0 ] for colkey in columns : if '.' in colkey : getkey = colkey . split ( '.' ) else : getkey = [ colkey ] try : thiscolval = _dict_get ( lcdict , getkey ) except Exception as e : LOGWARNING ( 'column %s does not exist for %s' % ( colkey , lcf ) ) thiscolval = np . nan lcobjdict [ getkey [ - 1 ] ] = thiscolval except Exception as e : LOGEXCEPTION ( 'could not figure out columns for %s' % lcf ) for colkey in columns : if '.' in colkey : getkey = colkey . split ( '.' ) else : getkey = [ colkey ] thiscolval = np . nan lcobjdict [ getkey [ - 1 ] ] = thiscolval for dk in lcndetkey : try : if '.' in dk : getdk = dk . split ( '.' ) else : getdk = [ dk ] ndetcol = _dict_get ( lcdict , getdk ) actualndets = ndetcol [ np . isfinite ( ndetcol ) ] . size lcobjdict [ '%s.ndet' % getdk [ - 1 ] ] = actualndets except Exception as e : lcobjdict [ '%s.ndet' % getdk [ - 1 ] ] = np . nan return lcobjdict
6413	def ghmean ( nums ) : m_g = gmean ( nums ) m_h = hmean ( nums ) if math . isnan ( m_g ) or math . isnan ( m_h ) : return float ( 'nan' ) while round ( m_h , 12 ) != round ( m_g , 12 ) : m_g , m_h = ( m_g * m_h ) ** ( 1 / 2 ) , ( 2 * m_g * m_h ) / ( m_g + m_h ) return m_g
12070	def tryLoadingFrom ( tryPath , moduleName = 'swhlab' ) : if not 'site-packages' in swhlab . __file__ : print ( "loaded custom swhlab module from" , os . path . dirname ( swhlab . __file__ ) ) return while len ( tryPath ) > 5 : sp = tryPath + "/swhlab/" if os . path . isdir ( sp ) and os . path . exists ( sp + "/__init__.py" ) : if not os . path . dirname ( tryPath ) in sys . path : sys . path . insert ( 0 , os . path . dirname ( tryPath ) ) print ( "#" * 80 ) print ( "# WARNING: using site-packages swhlab module" ) print ( "#" * 80 ) tryPath = os . path . dirname ( tryPath ) return
5315	def colorpalette ( self , colorpalette ) : if isinstance ( colorpalette , str ) : colorpalette = colors . parse_colors ( colorpalette ) self . _colorpalette = colors . sanitize_color_palette ( colorpalette )
3393	def undelete_model_genes ( cobra_model ) : if cobra_model . _trimmed_genes is not None : for x in cobra_model . _trimmed_genes : x . functional = True if cobra_model . _trimmed_reactions is not None : for the_reaction , ( lower_bound , upper_bound ) in cobra_model . _trimmed_reactions . items ( ) : the_reaction . lower_bound = lower_bound the_reaction . upper_bound = upper_bound cobra_model . _trimmed_genes = [ ] cobra_model . _trimmed_reactions = { } cobra_model . _trimmed = False
13481	def _regex_replacement ( self , target , replacement ) : match = re . compile ( target ) self . data = match . sub ( replacement , self . data )
13685	def load_values ( self ) : for config_name , evar in self . evar_defs . items ( ) : if evar . is_required and evar . name not in os . environ : raise RuntimeError ( ( "Missing required environment variable: {evar_name}\n" "{help_txt}" ) . format ( evar_name = evar . name , help_txt = evar . help_txt ) ) if evar . name in os . environ : self [ config_name ] = os . environ . get ( evar . name ) else : self [ config_name ] = evar . default_val for filter in evar . filters : current_val = self . get ( config_name ) new_val = filter ( current_val , evar ) self [ config_name ] = new_val self . _filter_all ( )
6245	def set_time ( self , value : float ) : if value < 0 : value = 0 self . controller . row = self . rps * value
4588	def show_image ( setter , width , height , image_path = '' , image_obj = None , offset = ( 0 , 0 ) , bgcolor = COLORS . Off , brightness = 255 ) : bgcolor = color_scale ( bgcolor , brightness ) img = image_obj if image_path and not img : from PIL import Image img = Image . open ( image_path ) elif not img : raise ValueError ( 'Must provide either image_path or image_obj' ) w = min ( width - offset [ 0 ] , img . size [ 0 ] ) h = min ( height - offset [ 1 ] , img . size [ 1 ] ) ox = offset [ 0 ] oy = offset [ 1 ] for x in range ( ox , w + ox ) : for y in range ( oy , h + oy ) : r , g , b , a = ( 0 , 0 , 0 , 255 ) rgba = img . getpixel ( ( x - ox , y - oy ) ) if isinstance ( rgba , int ) : raise ValueError ( 'Image must be in RGB or RGBA format!' ) if len ( rgba ) == 3 : r , g , b = rgba elif len ( rgba ) == 4 : r , g , b , a = rgba else : raise ValueError ( 'Image must be in RGB or RGBA format!' ) if a == 0 : r , g , b = bgcolor else : r , g , b = color_scale ( ( r , g , b ) , a ) if brightness != 255 : r , g , b = color_scale ( ( r , g , b ) , brightness ) setter ( x , y , ( r , g , b ) )
8806	def calc_periods ( hour = 0 , minute = 0 ) : period_end = datetime . datetime . utcnow ( ) . replace ( hour = hour , minute = minute , second = 0 , microsecond = 0 ) period_start = period_end - datetime . timedelta ( days = 1 ) period_end -= datetime . timedelta ( seconds = 1 ) return ( period_start , period_end )
10352	def make_pubmed_gene_group ( entrez_ids : Iterable [ Union [ str , int ] ] ) -> Iterable [ str ] : url = PUBMED_GENE_QUERY_URL . format ( ',' . join ( str ( x ) . strip ( ) for x in entrez_ids ) ) response = requests . get ( url ) tree = ElementTree . fromstring ( response . content ) for x in tree . findall ( './DocumentSummarySet/DocumentSummary' ) : yield '\n# {}' . format ( x . find ( 'Description' ) . text ) yield 'SET Citation = {{"Other", "PubMed Gene", "{}"}}' . format ( x . attrib [ 'uid' ] ) yield 'SET Evidence = "{}"' . format ( x . find ( 'Summary' ) . text . strip ( ) . replace ( '\n' , '' ) ) yield '\nUNSET Evidence\nUNSET Citation'
4989	def redirect ( self , request , * args , ** kwargs ) : enterprise_customer_uuid , course_run_id , course_key , program_uuid = RouterView . get_path_variables ( ** kwargs ) resource_id = course_key or course_run_id or program_uuid path = re . sub ( '{}|{}' . format ( enterprise_customer_uuid , re . escape ( resource_id ) ) , '{}' , request . path ) kwargs . pop ( 'course_key' , None ) return self . VIEWS [ path ] . as_view ( ) ( request , * args , ** kwargs )
6914	def generate_rrab_lightcurve ( times , mags = None , errs = None , paramdists = { 'period' : sps . uniform ( loc = 0.45 , scale = 0.35 ) , 'fourierorder' : [ 8 , 11 ] , 'amplitude' : sps . uniform ( loc = 0.4 , scale = 0.5 ) , 'phioffset' : np . pi , } , magsarefluxes = False ) : modeldict = generate_sinusoidal_lightcurve ( times , mags = mags , errs = errs , paramdists = paramdists , magsarefluxes = magsarefluxes ) modeldict [ 'vartype' ] = 'RRab' return modeldict
5225	def flatten ( iterable , maps = None , unique = False ) -> list : if iterable is None : return [ ] if maps is None : maps = dict ( ) if isinstance ( iterable , ( str , int , float ) ) : return [ maps . get ( iterable , iterable ) ] else : x = [ maps . get ( item , item ) for item in _to_gen_ ( iterable ) ] return list ( set ( x ) ) if unique else x
11227	def _invalidates_cache ( f ) : def inner_func ( self , * args , ** kwargs ) : rv = f ( self , * args , ** kwargs ) self . _invalidate_cache ( ) return rv return inner_func
9465	def conference_record_start ( self , call_params ) : path = '/' + self . api_version + '/ConferenceRecordStart/' method = 'POST' return self . request ( path , method , call_params )
2480	def datetime_from_iso_format ( string ) : match = DATE_ISO_REGEX . match ( string ) if match : date = datetime . datetime ( year = int ( match . group ( DATE_ISO_YEAR_GRP ) ) , month = int ( match . group ( DATE_ISO_MONTH_GRP ) ) , day = int ( match . group ( DATE_ISO_DAY_GRP ) ) , hour = int ( match . group ( DATE_ISO_HOUR_GRP ) ) , second = int ( match . group ( DATE_ISO_SEC_GRP ) ) , minute = int ( match . group ( DATE_ISO_MIN_GRP ) ) ) return date else : return None
8726	def _localize ( dt ) : try : tz = dt . tzinfo return tz . localize ( dt . replace ( tzinfo = None ) ) except AttributeError : return dt
1521	def get_hostname ( ip_addr , cl_args ) : if is_self ( ip_addr ) : return get_self_hostname ( ) cmd = "hostname" ssh_cmd = ssh_remote_execute ( cmd , ip_addr , cl_args ) pid = subprocess . Popen ( ssh_cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) if return_code != 0 : Log . error ( "Failed to get hostname for remote host %s with output:\n%s" % ( ip_addr , output ) ) sys . exit ( - 1 ) return output [ 0 ] . strip ( "\n" )
12523	def check_call ( cmd_args ) : p = subprocess . Popen ( cmd_args , stdout = subprocess . PIPE ) ( output , err ) = p . communicate ( ) return output
4265	def serve ( destination , port , config ) : if os . path . exists ( destination ) : pass elif os . path . exists ( config ) : settings = read_settings ( config ) destination = settings . get ( 'destination' ) if not os . path . exists ( destination ) : sys . stderr . write ( "The '{}' directory doesn't exist, maybe try " "building first?\n" . format ( destination ) ) sys . exit ( 1 ) else : sys . stderr . write ( "The {destination} directory doesn't exist " "and the config file ({config}) could not be read.\n" . format ( destination = destination , config = config ) ) sys . exit ( 2 ) print ( 'DESTINATION : {}' . format ( destination ) ) os . chdir ( destination ) Handler = server . SimpleHTTPRequestHandler httpd = socketserver . TCPServer ( ( "" , port ) , Handler , False ) print ( " * Running on http://127.0.0.1:{}/" . format ( port ) ) try : httpd . allow_reuse_address = True httpd . server_bind ( ) httpd . server_activate ( ) httpd . serve_forever ( ) except KeyboardInterrupt : print ( '\nAll done!' )
4555	def genVector ( width , height , x_mult = 1 , y_mult = 1 ) : center_x = ( width - 1 ) / 2 center_y = ( height - 1 ) / 2 def length ( x , y ) : dx = math . pow ( x - center_x , 2 * x_mult ) dy = math . pow ( y - center_y , 2 * y_mult ) return int ( math . sqrt ( dx + dy ) ) return [ [ length ( x , y ) for x in range ( width ) ] for y in range ( height ) ]
6721	def get_combined_requirements ( self , requirements = None ) : requirements = requirements or self . env . requirements def iter_lines ( fn ) : with open ( fn , 'r' ) as fin : for line in fin . readlines ( ) : line = line . strip ( ) if not line or line . startswith ( '#' ) : continue yield line content = [ ] if isinstance ( requirements , ( tuple , list ) ) : for f in requirements : f = self . find_template ( f ) content . extend ( list ( iter_lines ( f ) ) ) else : assert isinstance ( requirements , six . string_types ) f = self . find_template ( requirements ) content . extend ( list ( iter_lines ( f ) ) ) return '\n' . join ( content )
6554	def flip_variable ( self , v ) : try : idx = self . variables . index ( v ) except ValueError : raise ValueError ( "variable {} is not a variable in constraint {}" . format ( v , self . name ) ) if self . vartype is dimod . BINARY : original_func = self . func def func ( * args ) : new_args = list ( args ) new_args [ idx ] = 1 - new_args [ idx ] return original_func ( * new_args ) self . func = func self . configurations = frozenset ( config [ : idx ] + ( 1 - config [ idx ] , ) + config [ idx + 1 : ] for config in self . configurations ) else : original_func = self . func def func ( * args ) : new_args = list ( args ) new_args [ idx ] = - new_args [ idx ] return original_func ( * new_args ) self . func = func self . configurations = frozenset ( config [ : idx ] + ( - config [ idx ] , ) + config [ idx + 1 : ] for config in self . configurations ) self . name = '{} ({} flipped)' . format ( self . name , v )
9256	def issues_to_log ( self , issues , pull_requests ) : log = "" sections_a , issues_a = self . parse_by_sections ( issues , pull_requests ) for section , s_issues in sections_a . items ( ) : log += self . generate_sub_section ( s_issues , section ) log += self . generate_sub_section ( issues_a , self . options . issue_prefix ) return log
13821	def _ConvertStructMessage ( value , message ) : if not isinstance ( value , dict ) : raise ParseError ( 'Struct must be in a dict which is {0}.' . format ( value ) ) for key in value : _ConvertValueMessage ( value [ key ] , message . fields [ key ] ) return
8614	def create_volume ( self , datacenter_id , volume ) : data = ( json . dumps ( self . _create_volume_dict ( volume ) ) ) response = self . _perform_request ( url = '/datacenters/%s/volumes' % datacenter_id , method = 'POST' , data = data ) return response
1863	def STOS ( cpu , dest , src ) : size = src . size dest . write ( src . read ( ) ) dest_reg = dest . mem . base increment = Operators . ITEBV ( { 'RDI' : 64 , 'EDI' : 32 , 'DI' : 16 } [ dest_reg ] , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( dest_reg , cpu . read_register ( dest_reg ) + increment )
11495	def list_users ( self , limit = 20 ) : parameters = dict ( ) parameters [ 'limit' ] = limit response = self . request ( 'midas.user.list' , parameters ) return response
6149	def firwin_bpf ( N_taps , f1 , f2 , fs = 1.0 , pass_zero = False ) : return signal . firwin ( N_taps , 2 * ( f1 , f2 ) / fs , pass_zero = pass_zero )
4492	def fetch ( args ) : storage , remote_path = split_storage ( args . remote ) local_path = args . local if local_path is None : _ , local_path = os . path . split ( remote_path ) local_path_exists = os . path . exists ( local_path ) if local_path_exists and not args . force and not args . update : sys . exit ( "Local file %s already exists, not overwriting." % local_path ) directory , _ = os . path . split ( local_path ) if directory : makedirs ( directory , exist_ok = True ) osf = _setup_osf ( args ) project = osf . project ( args . project ) store = project . storage ( storage ) for file_ in store . files : if norm_remote_path ( file_ . path ) == remote_path : if local_path_exists and not args . force and args . update : if file_ . hashes . get ( 'md5' ) == checksum ( local_path ) : print ( "Local file %s already matches remote." % local_path ) break with open ( local_path , 'wb' ) as fp : file_ . write_to ( fp ) break
3504	def _add_cycle_free ( model , fluxes ) : model . objective = model . solver . interface . Objective ( Zero , direction = "min" , sloppy = True ) objective_vars = [ ] for rxn in model . reactions : flux = fluxes [ rxn . id ] if rxn . boundary : rxn . bounds = ( flux , flux ) continue if flux >= 0 : rxn . bounds = max ( 0 , rxn . lower_bound ) , max ( flux , rxn . upper_bound ) objective_vars . append ( rxn . forward_variable ) else : rxn . bounds = min ( flux , rxn . lower_bound ) , min ( 0 , rxn . upper_bound ) objective_vars . append ( rxn . reverse_variable ) model . objective . set_linear_coefficients ( { v : 1.0 for v in objective_vars } )
4873	def create ( self , validated_data ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) lms_user = validated_data . get ( 'lms_user_id' ) tpa_user = validated_data . get ( 'tpa_user_id' ) user_email = validated_data . get ( 'user_email' ) course_run_id = validated_data . get ( 'course_run_id' ) course_mode = validated_data . get ( 'course_mode' ) cohort = validated_data . get ( 'cohort' ) email_students = validated_data . get ( 'email_students' ) is_active = validated_data . get ( 'is_active' ) enterprise_customer_user = lms_user or tpa_user or user_email if isinstance ( enterprise_customer_user , models . EnterpriseCustomerUser ) : validated_data [ 'enterprise_customer_user' ] = enterprise_customer_user try : if is_active : enterprise_customer_user . enroll ( course_run_id , course_mode , cohort = cohort ) else : enterprise_customer_user . unenroll ( course_run_id ) except ( CourseEnrollmentDowngradeError , CourseEnrollmentPermissionError , HttpClientError ) as exc : validated_data [ 'detail' ] = str ( exc ) return validated_data if is_active : track_enrollment ( 'enterprise-customer-enrollment-api' , enterprise_customer_user . user_id , course_run_id ) else : if is_active : enterprise_customer_user = enterprise_customer . enroll_user_pending_registration ( user_email , course_mode , course_run_id , cohort = cohort ) else : enterprise_customer . clear_pending_registration ( user_email , course_run_id ) if email_students : enterprise_customer . notify_enrolled_learners ( self . context . get ( 'request_user' ) , course_run_id , [ enterprise_customer_user ] ) validated_data [ 'detail' ] = 'success' return validated_data
11734	def get_resample_data ( self ) : if self . data is not None : if self . _pvpc_mean_daily is None : self . _pvpc_mean_daily = self . data [ 'data' ] . resample ( 'D' ) . mean ( ) if self . _pvpc_mean_monthly is None : self . _pvpc_mean_monthly = self . data [ 'data' ] . resample ( 'MS' ) . mean ( ) return self . _pvpc_mean_daily , self . _pvpc_mean_monthly
9625	def register ( self , cls ) : preview = cls ( site = self ) logger . debug ( 'Registering %r with %r' , preview , self ) index = self . __previews . setdefault ( preview . module , { } ) index [ cls . __name__ ] = preview
6526	def parse ( cls , content , is_pyproject = False ) : parsed = pytoml . loads ( content ) if is_pyproject : parsed = parsed . get ( 'tool' , { } ) parsed = parsed . get ( 'tidypy' , { } ) return parsed
860	def getTemporalDelay ( inferenceElement , key = None ) : if inferenceElement in ( InferenceElement . prediction , InferenceElement . encodings ) : return 1 if inferenceElement in ( InferenceElement . anomalyScore , InferenceElement . anomalyLabel , InferenceElement . classification , InferenceElement . classConfidences ) : return 0 if inferenceElement in ( InferenceElement . multiStepPredictions , InferenceElement . multiStepBestPredictions ) : return int ( key ) return 0
5726	def get_gdb_response ( self , timeout_sec = DEFAULT_GDB_TIMEOUT_SEC , raise_error_on_timeout = True ) : self . verify_valid_gdb_subprocess ( ) if timeout_sec < 0 : self . logger . warning ( "timeout_sec was negative, replacing with 0" ) timeout_sec = 0 if USING_WINDOWS : retval = self . _get_responses_windows ( timeout_sec ) else : retval = self . _get_responses_unix ( timeout_sec ) if not retval and raise_error_on_timeout : raise GdbTimeoutError ( "Did not get response from gdb after %s seconds" % timeout_sec ) else : return retval
7218	def delete ( self , task_name ) : r = self . gbdx_connection . delete ( self . _base_url + '/' + task_name ) raise_for_status ( r ) return r . text
3809	def serialize_formula ( formula ) : r charge = charge_from_formula ( formula ) element_dict = nested_formula_parser ( formula ) base = atoms_to_Hill ( element_dict ) if charge == 0 : pass elif charge > 0 : if charge == 1 : base += '+' else : base += '+' + str ( charge ) elif charge < 0 : if charge == - 1 : base += '-' else : base += str ( charge ) return base
12728	def axes ( self , axes ) : self . lmotor . axes = [ axes [ 0 ] ] self . ode_obj . setAxis ( tuple ( axes [ 0 ] ) )
8036	def lexrank ( sentences , continuous = False , sim_threshold = 0.1 , alpha = 0.9 , use_divrank = False , divrank_alpha = 0.25 ) : ranker_params = { 'max_iter' : 1000 } if use_divrank : ranker = divrank_scipy ranker_params [ 'alpha' ] = divrank_alpha ranker_params [ 'd' ] = alpha else : ranker = networkx . pagerank_scipy ranker_params [ 'alpha' ] = alpha graph = networkx . DiGraph ( ) sent_tf_list = [ ] for sent in sentences : words = tools . word_segmenter_ja ( sent ) tf = collections . Counter ( words ) sent_tf_list . append ( tf ) sent_vectorizer = DictVectorizer ( sparse = True ) sent_vecs = sent_vectorizer . fit_transform ( sent_tf_list ) sim_mat = 1 - pairwise_distances ( sent_vecs , sent_vecs , metric = 'cosine' ) if continuous : linked_rows , linked_cols = numpy . where ( sim_mat > 0 ) else : linked_rows , linked_cols = numpy . where ( sim_mat >= sim_threshold ) graph . add_nodes_from ( range ( sent_vecs . shape [ 0 ] ) ) for i , j in zip ( linked_rows , linked_cols ) : if i == j : continue weight = sim_mat [ i , j ] if continuous else 1.0 graph . add_edge ( i , j , { 'weight' : weight } ) scores = ranker ( graph , ** ranker_params ) return scores , sim_mat
2708	def limit_keyphrases ( path , phrase_limit = 20 ) : rank_thresh = None if isinstance ( path , str ) : lex = [ ] for meta in json_iter ( path ) : rl = RankedLexeme ( ** meta ) lex . append ( rl ) else : lex = path if len ( lex ) > 0 : rank_thresh = statistics . mean ( [ rl . rank for rl in lex ] ) else : rank_thresh = 0 used = 0 for rl in lex : if rl . pos [ 0 ] != "v" : if ( used > phrase_limit ) or ( rl . rank < rank_thresh ) : return used += 1 yield rl . text . replace ( " - " , "-" )
6234	def start ( self ) : if self . initialized : mixer . music . unpause ( ) else : mixer . music . play ( ) mixer . music . play ( ) self . initialized = True self . paused = False
5044	def notify_program_learners ( cls , enterprise_customer , program_details , users ) : program_name = program_details . get ( 'title' ) program_branding = program_details . get ( 'type' ) program_uuid = program_details . get ( 'uuid' ) lms_root_url = get_configuration_value_for_site ( enterprise_customer . site , 'LMS_ROOT_URL' , settings . LMS_ROOT_URL ) program_path = urlquote ( '/dashboard/programs/{program_uuid}/?tpa_hint={tpa_hint}' . format ( program_uuid = program_uuid , tpa_hint = enterprise_customer . identity_provider , ) ) destination_url = '{site}/{login_or_register}?next={program_path}' . format ( site = lms_root_url , login_or_register = '{login_or_register}' , program_path = program_path ) program_type = 'program' program_start = get_earliest_start_date_from_program ( program_details ) with mail . get_connection ( ) as email_conn : for user in users : login_or_register = 'register' if isinstance ( user , PendingEnterpriseCustomerUser ) else 'login' destination_url = destination_url . format ( login_or_register = login_or_register ) send_email_notification_message ( user = user , enrolled_in = { 'name' : program_name , 'url' : destination_url , 'type' : program_type , 'start' : program_start , 'branding' : program_branding , } , enterprise_customer = enterprise_customer , email_connection = email_conn )
40	def add ( self , * args , ** kwargs ) : idx = self . _next_idx super ( ) . add ( * args , ** kwargs ) self . _it_sum [ idx ] = self . _max_priority ** self . _alpha self . _it_min [ idx ] = self . _max_priority ** self . _alpha
1201	def reset ( self ) : self . level . reset ( ) return self . level . observations ( ) [ self . state_attribute ]
7867	def expire ( self ) : with self . _lock : logger . debug ( "expdict.expire. timeouts: {0!r}" . format ( self . _timeouts ) ) next_timeout = None for k in self . _timeouts . keys ( ) : ret = self . _expire_item ( k ) if ret is not None : if next_timeout is None : next_timeout = ret else : next_timeout = min ( next_timeout , ret ) return next_timeout
6471	def update ( self , points , values = None ) : self . values = values or [ None ] * len ( points ) if np is None : if self . option . function : warnings . warn ( 'numpy not available, function ignored' ) self . points = points self . minimum = min ( self . points ) self . maximum = max ( self . points ) self . current = self . points [ - 1 ] else : self . points = self . apply_function ( points ) self . minimum = np . min ( self . points ) self . maximum = np . max ( self . points ) self . current = self . points [ - 1 ] if self . maximum == self . minimum : self . extents = 1 else : self . extents = ( self . maximum - self . minimum ) self . extents = ( self . maximum - self . minimum )
4889	def update_course ( self , course , enterprise_customer , enterprise_context ) : course [ 'course_runs' ] = self . update_course_runs ( course_runs = course . get ( 'course_runs' ) or [ ] , enterprise_customer = enterprise_customer , enterprise_context = enterprise_context , ) marketing_url = course . get ( 'marketing_url' ) if marketing_url : query_parameters = dict ( enterprise_context , ** utils . get_enterprise_utm_context ( enterprise_customer ) ) course . update ( { 'marketing_url' : utils . update_query_parameters ( marketing_url , query_parameters ) } ) course . update ( enterprise_context ) return course
7027	def objectlist_radeclbox ( radeclbox , gaia_mirror = None , columns = ( 'source_id' , 'ra' , 'dec' , 'phot_g_mean_mag' , 'l' , 'b' , 'parallax, parallax_error' , 'pmra' , 'pmra_error' , 'pmdec' , 'pmdec_error' ) , extra_filter = None , returnformat = 'csv' , forcefetch = False , cachedir = '~/.astrobase/gaia-cache' , verbose = True , timeout = 15.0 , refresh = 2.0 , maxtimeout = 300.0 , maxtries = 3 , complete_query_later = True ) : query = ( "select {columns} from {{table}} where " "CONTAINS(POINT('ICRS',{{table}}.ra, {{table}}.dec)," "BOX('ICRS',{ra_center:.5f},{decl_center:.5f}," "{ra_width:.5f},{decl_height:.5f}))=1" "{extra_filter_str}" ) ra_min , ra_max , decl_min , decl_max = radeclbox ra_center = ( ra_max + ra_min ) / 2.0 decl_center = ( decl_max + decl_min ) / 2.0 ra_width = ra_max - ra_min decl_height = decl_max - decl_min if extra_filter is not None : extra_filter_str = ' and %s ' % extra_filter else : extra_filter_str = '' formatted_query = query . format ( columns = ', ' . join ( columns ) , extra_filter_str = extra_filter_str , ra_center = ra_center , decl_center = decl_center , ra_width = ra_width , decl_height = decl_height ) return tap_query ( formatted_query , gaia_mirror = gaia_mirror , returnformat = returnformat , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , complete_query_later = complete_query_later )
11186	def _prompt_for_values ( d ) : for key , value in d . items ( ) : if isinstance ( value , CommentedMap ) : _prompt_for_values ( value ) elif isinstance ( value , list ) : for item in value : _prompt_for_values ( item ) else : typ = type ( value ) if isinstance ( value , ScalarFloat ) : typ = float new_value = click . prompt ( key , type = typ , default = value ) d [ key ] = new_value return d
13560	def decorate ( msg = "" , waitmsg = "Please wait" ) : def decorator ( func ) : @ functools . wraps ( func ) def wrapper ( * args , ** kwargs ) : spin = Spinner ( msg = msg , waitmsg = waitmsg ) spin . start ( ) a = None try : a = func ( * args , ** kwargs ) except Exception as e : spin . msg = "Something went wrong: " spin . stop_spinning ( ) spin . join ( ) raise e spin . stop_spinning ( ) spin . join ( ) return a return wrapper return decorator
7857	def __error ( self , stanza ) : try : self . error ( stanza . get_error ( ) ) except ProtocolError : from . . error import StanzaErrorNode self . error ( StanzaErrorNode ( "undefined-condition" ) )
3040	def from_json ( cls , json_data ) : data = json . loads ( _helpers . _from_bytes ( json_data ) ) if ( data . get ( 'token_expiry' ) and not isinstance ( data [ 'token_expiry' ] , datetime . datetime ) ) : try : data [ 'token_expiry' ] = datetime . datetime . strptime ( data [ 'token_expiry' ] , EXPIRY_FORMAT ) except ValueError : data [ 'token_expiry' ] = None retval = cls ( data [ 'access_token' ] , data [ 'client_id' ] , data [ 'client_secret' ] , data [ 'refresh_token' ] , data [ 'token_expiry' ] , data [ 'token_uri' ] , data [ 'user_agent' ] , revoke_uri = data . get ( 'revoke_uri' , None ) , id_token = data . get ( 'id_token' , None ) , id_token_jwt = data . get ( 'id_token_jwt' , None ) , token_response = data . get ( 'token_response' , None ) , scopes = data . get ( 'scopes' , None ) , token_info_uri = data . get ( 'token_info_uri' , None ) ) retval . invalid = data [ 'invalid' ] return retval
6561	def _bqm_from_2sat ( constraint ) : configurations = constraint . configurations variables = constraint . variables vartype = constraint . vartype u , v = constraint . variables if len ( configurations ) == 4 : return dimod . BinaryQuadraticModel . empty ( constraint . vartype ) components = irreducible_components ( constraint ) if len ( components ) > 1 : const0 = Constraint . from_configurations ( ( ( config [ 0 ] , ) for config in configurations ) , ( u , ) , vartype ) const1 = Constraint . from_configurations ( ( ( config [ 1 ] , ) for config in configurations ) , ( v , ) , vartype ) bqm = _bqm_from_1sat ( const0 ) bqm . update ( _bqm_from_1sat ( const1 ) ) return bqm assert len ( configurations ) > 1 , "single configurations should be irreducible" bqm = dimod . BinaryQuadraticModel . empty ( vartype ) if all ( operator . eq ( * config ) for config in configurations ) : bqm . add_interaction ( u , v , - 1 , vartype = dimod . SPIN ) elif all ( operator . ne ( * config ) for config in configurations ) : bqm . add_interaction ( u , v , + 1 , vartype = dimod . SPIN ) elif ( 1 , 1 ) not in configurations : bqm . add_interaction ( u , v , 2 , vartype = dimod . BINARY ) elif ( - 1 , + 1 ) not in configurations and ( 0 , 1 ) not in configurations : bqm . add_interaction ( u , v , - 2 , vartype = dimod . BINARY ) bqm . add_variable ( v , 2 , vartype = dimod . BINARY ) elif ( + 1 , - 1 ) not in configurations and ( 1 , 0 ) not in configurations : bqm . add_interaction ( u , v , - 2 , vartype = dimod . BINARY ) bqm . add_variable ( u , 2 , vartype = dimod . BINARY ) else : bqm . add_interaction ( u , v , 2 , vartype = dimod . BINARY ) bqm . add_variable ( u , - 2 , vartype = dimod . BINARY ) bqm . add_variable ( v , - 2 , vartype = dimod . BINARY ) return bqm
10295	def get_undefined_namespaces ( graph : BELGraph ) -> Set [ str ] : return { exc . namespace for _ , exc , _ in graph . warnings if isinstance ( exc , UndefinedNamespaceWarning ) }
7830	def _new_from_xml ( cls , xmlnode ) : field_type = xmlnode . prop ( "type" ) label = from_utf8 ( xmlnode . prop ( "label" ) ) name = from_utf8 ( xmlnode . prop ( "var" ) ) child = xmlnode . children values = [ ] options = [ ] required = False desc = None while child : if child . type != "element" or child . ns ( ) . content != DATAFORM_NS : pass elif child . name == "required" : required = True elif child . name == "desc" : desc = from_utf8 ( child . getContent ( ) ) elif child . name == "value" : values . append ( from_utf8 ( child . getContent ( ) ) ) elif child . name == "option" : options . append ( Option . _new_from_xml ( child ) ) child = child . next if field_type and not field_type . endswith ( "-multi" ) and len ( values ) > 1 : raise BadRequestProtocolError ( "Multiple values for a single-value field" ) return cls ( name , values , field_type , label , options , required , desc )
13760	def _format_iso_time ( self , time ) : if isinstance ( time , str ) : return time elif isinstance ( time , datetime ) : return time . strftime ( '%Y-%m-%dT%H:%M:%S.%fZ' ) else : return None
4608	def nolist ( self , account ) : assert callable ( self . blockchain . account_whitelist ) return self . blockchain . account_whitelist ( account , lists = [ ] , account = self )
9691	def stop ( self ) : if self . receiver != None : self . receiver . join ( ) for s in self . senders . values ( ) : s . join ( )
6054	def regular_to_sparse_from_sparse_mappings ( regular_to_unmasked_sparse , unmasked_sparse_to_sparse ) : total_regular_pixels = regular_to_unmasked_sparse . shape [ 0 ] regular_to_sparse = np . zeros ( total_regular_pixels ) for regular_index in range ( total_regular_pixels ) : regular_to_sparse [ regular_index ] = unmasked_sparse_to_sparse [ regular_to_unmasked_sparse [ regular_index ] ] return regular_to_sparse
10735	def ld_to_dl ( ld ) : if ld : keys = list ( ld [ 0 ] ) dl = { key : [ d [ key ] for d in ld ] for key in keys } return dl else : return { }
9838	def __gridpositions ( self ) : try : tok = self . __consume ( ) except DXParserNoTokens : return if tok . equals ( 'counts' ) : shape = [ ] try : while True : self . __peek ( ) . value ( 'INTEGER' ) tok = self . __consume ( ) shape . append ( tok . value ( 'INTEGER' ) ) except ( DXParserNoTokens , ValueError ) : pass if len ( shape ) == 0 : raise DXParseError ( 'gridpositions: no shape parameters' ) self . currentobject [ 'shape' ] = shape elif tok . equals ( 'origin' ) : origin = [ ] try : while ( self . __peek ( ) . iscode ( 'INTEGER' ) or self . __peek ( ) . iscode ( 'REAL' ) ) : tok = self . __consume ( ) origin . append ( tok . value ( ) ) except DXParserNoTokens : pass if len ( origin ) == 0 : raise DXParseError ( 'gridpositions: no origin parameters' ) self . currentobject [ 'origin' ] = origin elif tok . equals ( 'delta' ) : d = [ ] try : while ( self . __peek ( ) . iscode ( 'INTEGER' ) or self . __peek ( ) . iscode ( 'REAL' ) ) : tok = self . __consume ( ) d . append ( tok . value ( ) ) except DXParserNoTokens : pass if len ( d ) == 0 : raise DXParseError ( 'gridpositions: missing delta parameters' ) try : self . currentobject [ 'delta' ] . append ( d ) except KeyError : self . currentobject [ 'delta' ] = [ d ] else : raise DXParseError ( 'gridpositions: ' + str ( tok ) + ' not recognized.' )
5676	def get_timezone_name ( self ) : tz_name = self . conn . execute ( 'SELECT timezone FROM agencies LIMIT 1' ) . fetchone ( ) if tz_name is None : raise ValueError ( "This database does not have a timezone defined." ) return tz_name [ 0 ]
3376	def fix_objective_as_constraint ( model , fraction = 1 , bound = None , name = 'fixed_objective_{}' ) : fix_objective_name = name . format ( model . objective . name ) if fix_objective_name in model . constraints : model . solver . remove ( fix_objective_name ) if bound is None : bound = model . slim_optimize ( error_value = None ) * fraction if model . objective . direction == 'max' : ub , lb = None , bound else : ub , lb = bound , None constraint = model . problem . Constraint ( model . objective . expression , name = fix_objective_name , ub = ub , lb = lb ) add_cons_vars_to_problem ( model , constraint , sloppy = True ) return bound
2768	def get_volume ( self , volume_id ) : return Volume . get_object ( api_token = self . token , volume_id = volume_id )
8501	def as_namespace ( self , namespace = None ) : key = self . key if namespace and key . startswith ( namespace ) : key = key [ len ( namespace ) + 1 : ] return "%s = %s" % ( self . get_key ( ) , self . _default ( ) or NotSet ( ) )
9306	def encode_body ( req ) : if isinstance ( req . body , text_type ) : split = req . headers . get ( 'content-type' , 'text/plain' ) . split ( ';' ) if len ( split ) == 2 : ct , cs = split cs = cs . split ( '=' ) [ 1 ] req . body = req . body . encode ( cs ) else : ct = split [ 0 ] if ( ct == 'application/x-www-form-urlencoded' or 'x-amz-' in ct ) : req . body = req . body . encode ( ) else : req . body = req . body . encode ( 'utf-8' ) req . headers [ 'content-type' ] = ct + '; charset=utf-8'
7494	def count_snps ( mat ) : snps = np . zeros ( 4 , dtype = np . uint32 ) snps [ 0 ] = np . uint32 ( mat [ 0 , 5 ] + mat [ 0 , 10 ] + mat [ 0 , 15 ] + mat [ 5 , 0 ] + mat [ 5 , 10 ] + mat [ 5 , 15 ] + mat [ 10 , 0 ] + mat [ 10 , 5 ] + mat [ 10 , 15 ] + mat [ 15 , 0 ] + mat [ 15 , 5 ] + mat [ 15 , 10 ] ) for i in range ( 16 ) : if i % 5 : snps [ 1 ] += mat [ i , i ] snps [ 2 ] = mat [ 1 , 4 ] + mat [ 2 , 8 ] + mat [ 3 , 12 ] + mat [ 4 , 1 ] + mat [ 6 , 9 ] + mat [ 7 , 13 ] + mat [ 8 , 2 ] + mat [ 9 , 6 ] + mat [ 11 , 14 ] + mat [ 12 , 3 ] + mat [ 13 , 7 ] + mat [ 14 , 11 ] snps [ 3 ] = ( mat . sum ( ) - np . diag ( mat ) . sum ( ) ) - snps [ 2 ] return snps
13169	def _match ( self , pred ) : if not pred : return True pred = pred [ 1 : - 1 ] if pred . startswith ( '@' ) : pred = pred [ 1 : ] if '=' in pred : attr , value = pred . split ( '=' , 1 ) if value [ 0 ] in ( '"' , "'" ) : value = value [ 1 : ] if value [ - 1 ] in ( '"' , "'" ) : value = value [ : - 1 ] return self . attrs . get ( attr ) == value else : return pred in self . attrs elif num_re . match ( pred ) : index = int ( pred ) if index < 0 : if self . parent : return self . index == ( len ( self . parent . _children ) + index ) else : return index == 0 else : return index == self . index else : if '=' in pred : tag , value = pred . split ( '=' , 1 ) if value [ 0 ] in ( '"' , "'" ) : value = value [ 1 : ] if value [ - 1 ] in ( '"' , "'" ) : value = value [ : - 1 ] for c in self . _children : if c . tagname == tag and c . data == value : return True else : for c in self . _children : if c . tagname == pred : return True return False
10949	def get_shares ( self ) : self . shares = self . api . get ( url = PATHS [ 'GET_SHARES' ] % self . url ) [ 'shares' ] return self . shares
7773	def _quote ( data ) : data = data . replace ( b'\\' , b'\\\\' ) data = data . replace ( b'"' , b'\\"' ) return data
10810	def get_by_name ( cls , name ) : try : return cls . query . filter_by ( name = name ) . one ( ) except NoResultFound : return None
463	def open_tensorboard ( log_dir = '/tmp/tensorflow' , port = 6006 ) : text = "[TL] Open tensorboard, go to localhost:" + str ( port ) + " to access" text2 = " not yet supported by this function (tl.ops.open_tb)" if not tl . files . exists_or_mkdir ( log_dir , verbose = False ) : tl . logging . info ( "[TL] Log reportory was created at %s" % log_dir ) if _platform == "linux" or _platform == "linux2" : raise NotImplementedError ( ) elif _platform == "darwin" : tl . logging . info ( 'OS X: %s' % text ) subprocess . Popen ( sys . prefix + " | python -m tensorflow.tensorboard --logdir=" + log_dir + " --port=" + str ( port ) , shell = True ) elif _platform == "win32" : raise NotImplementedError ( "this function is not supported on the Windows platform" ) else : tl . logging . info ( _platform + text2 )
13129	def initialize_indices ( ) : Host . init ( ) Range . init ( ) Service . init ( ) User . init ( ) Credential . init ( ) Log . init ( )
13035	def read_translation ( filename ) : translation = triple_pb . Translation ( ) with open ( filename , "rb" ) as f : translation . ParseFromString ( f . read ( ) ) def unwrap_translation_units ( units ) : for u in units : yield u . element , u . index return ( list ( unwrap_translation_units ( translation . entities ) ) , list ( unwrap_translation_units ( translation . relations ) ) )
11887	def receive ( self ) : try : buffer = self . _socket . recv ( BUFFER_SIZE ) except socket . timeout as error : _LOGGER . error ( "Error receiving: %s" , error ) return "" buffering = True response = '' while buffering : if '\n' in buffer . decode ( "utf8" ) : response = buffer . decode ( "utf8" ) . split ( '\n' ) [ 0 ] buffering = False else : try : more = self . _socket . recv ( BUFFER_SIZE ) except socket . timeout : more = None if not more : buffering = False response = buffer . decode ( "utf8" ) else : buffer += more return response
7665	def to_html ( self , max_rows = None ) : n = len ( self . data ) div_id = _get_divid ( self ) out = r . format ( div_id , self . namespace , n ) out += r . format ( div_id ) out += r . format ( self . annotation_metadata . _repr_html_ ( ) ) out += r . format ( self . sandbox . _repr_html_ ( ) ) out += r . format ( self . namespace , n ) out += r if max_rows is None or n <= max_rows : out += self . _fmt_rows ( 0 , n ) else : out += self . _fmt_rows ( 0 , max_rows // 2 ) out += r out += self . _fmt_rows ( n - max_rows // 2 , n ) out += r out += r out += r return out
10825	def search ( cls , query , q ) : query = query . join ( User ) . filter ( User . email . like ( '%{0}%' . format ( q ) ) , ) return query
13096	def watch ( self ) : wm = pyinotify . WatchManager ( ) self . notifier = pyinotify . Notifier ( wm , default_proc_fun = self . callback ) wm . add_watch ( self . directory , pyinotify . ALL_EVENTS ) try : self . notifier . loop ( ) except ( KeyboardInterrupt , AttributeError ) : print_notification ( "Stopping" ) finally : self . notifier . stop ( ) self . terminate_processes ( )
1699	def log ( self ) : from heronpy . streamlet . impl . logbolt import LogStreamlet log_streamlet = LogStreamlet ( self ) self . _add_child ( log_streamlet ) return
440	def print_layers ( self ) : for i , layer in enumerate ( self . all_layers ) : logging . info ( " layer {:3}: {:20} {:15} {}" . format ( i , layer . name , str ( layer . get_shape ( ) ) , layer . dtype . name ) )
6006	def load_ccd_data_from_fits ( image_path , pixel_scale , image_hdu = 0 , resized_ccd_shape = None , resized_ccd_origin_pixels = None , resized_ccd_origin_arcsec = None , psf_path = None , psf_hdu = 0 , resized_psf_shape = None , renormalize_psf = True , noise_map_path = None , noise_map_hdu = 0 , noise_map_from_image_and_background_noise_map = False , convert_noise_map_from_weight_map = False , convert_noise_map_from_inverse_noise_map = False , background_noise_map_path = None , background_noise_map_hdu = 0 , convert_background_noise_map_from_weight_map = False , convert_background_noise_map_from_inverse_noise_map = False , poisson_noise_map_path = None , poisson_noise_map_hdu = 0 , poisson_noise_map_from_image = False , convert_poisson_noise_map_from_weight_map = False , convert_poisson_noise_map_from_inverse_noise_map = False , exposure_time_map_path = None , exposure_time_map_hdu = 0 , exposure_time_map_from_single_value = None , exposure_time_map_from_inverse_noise_map = False , background_sky_map_path = None , background_sky_map_hdu = 0 , convert_from_electrons = False , gain = None , convert_from_adus = False , lens_name = None ) : image = load_image ( image_path = image_path , image_hdu = image_hdu , pixel_scale = pixel_scale ) background_noise_map = load_background_noise_map ( background_noise_map_path = background_noise_map_path , background_noise_map_hdu = background_noise_map_hdu , pixel_scale = pixel_scale , convert_background_noise_map_from_weight_map = convert_background_noise_map_from_weight_map , convert_background_noise_map_from_inverse_noise_map = convert_background_noise_map_from_inverse_noise_map ) if background_noise_map is not None : inverse_noise_map = 1.0 / background_noise_map else : inverse_noise_map = None exposure_time_map = load_exposure_time_map ( exposure_time_map_path = exposure_time_map_path , exposure_time_map_hdu = exposure_time_map_hdu , pixel_scale = pixel_scale , shape = image . shape , exposure_time = exposure_time_map_from_single_value , exposure_time_map_from_inverse_noise_map = exposure_time_map_from_inverse_noise_map , inverse_noise_map = inverse_noise_map ) poisson_noise_map = load_poisson_noise_map ( poisson_noise_map_path = poisson_noise_map_path , poisson_noise_map_hdu = poisson_noise_map_hdu , pixel_scale = pixel_scale , convert_poisson_noise_map_from_weight_map = convert_poisson_noise_map_from_weight_map , convert_poisson_noise_map_from_inverse_noise_map = convert_poisson_noise_map_from_inverse_noise_map , image = image , exposure_time_map = exposure_time_map , poisson_noise_map_from_image = poisson_noise_map_from_image , convert_from_electrons = convert_from_electrons , gain = gain , convert_from_adus = convert_from_adus ) noise_map = load_noise_map ( noise_map_path = noise_map_path , noise_map_hdu = noise_map_hdu , pixel_scale = pixel_scale , image = image , background_noise_map = background_noise_map , exposure_time_map = exposure_time_map , convert_noise_map_from_weight_map = convert_noise_map_from_weight_map , convert_noise_map_from_inverse_noise_map = convert_noise_map_from_inverse_noise_map , noise_map_from_image_and_background_noise_map = noise_map_from_image_and_background_noise_map , convert_from_electrons = convert_from_electrons , gain = gain , convert_from_adus = convert_from_adus ) psf = load_psf ( psf_path = psf_path , psf_hdu = psf_hdu , pixel_scale = pixel_scale , renormalize = renormalize_psf ) background_sky_map = load_background_sky_map ( background_sky_map_path = background_sky_map_path , background_sky_map_hdu = background_sky_map_hdu , pixel_scale = pixel_scale ) image = CCDData ( image = image , pixel_scale = pixel_scale , psf = psf , noise_map = noise_map , background_noise_map = background_noise_map , poisson_noise_map = poisson_noise_map , exposure_time_map = exposure_time_map , background_sky_map = background_sky_map , gain = gain , name = lens_name ) if resized_ccd_shape is not None : image = image . new_ccd_data_with_resized_arrays ( new_shape = resized_ccd_shape , new_centre_pixels = resized_ccd_origin_pixels , new_centre_arcsec = resized_ccd_origin_arcsec ) if resized_psf_shape is not None : image = image . new_ccd_data_with_resized_psf ( new_shape = resized_psf_shape ) if convert_from_electrons : image = image . new_ccd_data_converted_from_electrons ( ) elif convert_from_adus : image = image . new_ccd_data_converted_from_adus ( gain = gain ) return image
13009	def format ( ) : argparser = argparse . ArgumentParser ( description = 'Formats a json object in a certain way. Use with pipes.' ) argparser . add_argument ( 'format' , metavar = 'format' , help = 'How to format the json for example "{address}:{port}".' , nargs = '?' ) arguments = argparser . parse_args ( ) service_style = "{address:15} {port:7} {protocol:5} {service:15} {state:10} {banner} {tags}" host_style = "{address:15} {tags}" ranges_style = "{range:18} {tags}" users_style = "{username}" if arguments . format : format_input ( arguments . format ) else : doc_mapper = DocMapper ( ) if doc_mapper . is_pipe : for obj in doc_mapper . get_pipe ( ) : style = '' if isinstance ( obj , Range ) : style = ranges_style elif isinstance ( obj , Host ) : style = host_style elif isinstance ( obj , Service ) : style = service_style elif isinstance ( obj , User ) : style = users_style print_line ( fmt . format ( style , ** obj . to_dict ( include_meta = True ) ) ) else : print_error ( "Please use this script with pipes" )
7795	def unregister_fetcher ( self , object_class ) : self . _lock . acquire ( ) try : cache = self . _caches . get ( object_class ) if not cache : return cache . set_fetcher ( None ) finally : self . _lock . release ( )
1068	def getaddrlist ( self , name ) : raw = [ ] for h in self . getallmatchingheaders ( name ) : if h [ 0 ] in ' \t' : raw . append ( h ) else : if raw : raw . append ( ', ' ) i = h . find ( ':' ) if i > 0 : addr = h [ i + 1 : ] raw . append ( addr ) alladdrs = '' . join ( raw ) a = AddressList ( alladdrs ) return a . addresslist
2294	def integral_approx_estimator ( x , y ) : a , b = ( 0. , 0. ) x = np . array ( x ) y = np . array ( y ) idx , idy = ( np . argsort ( x ) , np . argsort ( y ) ) for x1 , x2 , y1 , y2 in zip ( x [ [ idx ] ] [ : - 1 ] , x [ [ idx ] ] [ 1 : ] , y [ [ idx ] ] [ : - 1 ] , y [ [ idx ] ] [ 1 : ] ) : if x1 != x2 and y1 != y2 : a = a + np . log ( np . abs ( ( y2 - y1 ) / ( x2 - x1 ) ) ) for x1 , x2 , y1 , y2 in zip ( x [ [ idy ] ] [ : - 1 ] , x [ [ idy ] ] [ 1 : ] , y [ [ idy ] ] [ : - 1 ] , y [ [ idy ] ] [ 1 : ] ) : if x1 != x2 and y1 != y2 : b = b + np . log ( np . abs ( ( x2 - x1 ) / ( y2 - y1 ) ) ) return ( a - b ) / len ( x )
9634	def numeric ( _ , n ) : try : nt = n . as_tuple ( ) except AttributeError : raise TypeError ( 'numeric field requires Decimal value (got %r)' % n ) digits = [ ] if isinstance ( nt . exponent , str ) : ndigits = 0 weight = 0 sign = 0xC000 dscale = 0 else : decdigits = list ( reversed ( nt . digits + ( nt . exponent % 4 ) * ( 0 , ) ) ) weight = 0 while decdigits : if any ( decdigits [ : 4 ] ) : break weight += 1 del decdigits [ : 4 ] while decdigits : digits . insert ( 0 , ndig ( decdigits [ : 4 ] ) ) del decdigits [ : 4 ] ndigits = len ( digits ) weight += nt . exponent // 4 + ndigits - 1 sign = nt . sign * 0x4000 dscale = - min ( 0 , nt . exponent ) data = [ ndigits , weight , sign , dscale ] + digits return ( 'ihhHH%dH' % ndigits , [ 2 * len ( data ) ] + data )
5435	def tasks_file_to_task_descriptors ( tasks , retries , input_file_param_util , output_file_param_util ) : task_descriptors = [ ] path = tasks [ 'path' ] task_min = tasks . get ( 'min' ) task_max = tasks . get ( 'max' ) param_file = dsub_util . load_file ( path ) reader = csv . reader ( param_file , delimiter = '\t' ) header = six . advance_iterator ( reader ) job_params = parse_tasks_file_header ( header , input_file_param_util , output_file_param_util ) for row in reader : task_id = reader . line_num - 1 if task_min and task_id < task_min : continue if task_max and task_id > task_max : continue if len ( row ) != len ( job_params ) : dsub_util . print_error ( 'Unexpected number of fields %s vs %s: line %s' % ( len ( row ) , len ( job_params ) , reader . line_num ) ) envs = set ( ) inputs = set ( ) outputs = set ( ) labels = set ( ) for i in range ( 0 , len ( job_params ) ) : param = job_params [ i ] name = param . name if isinstance ( param , job_model . EnvParam ) : envs . add ( job_model . EnvParam ( name , row [ i ] ) ) elif isinstance ( param , job_model . LabelParam ) : labels . add ( job_model . LabelParam ( name , row [ i ] ) ) elif isinstance ( param , job_model . InputFileParam ) : inputs . add ( input_file_param_util . make_param ( name , row [ i ] , param . recursive ) ) elif isinstance ( param , job_model . OutputFileParam ) : outputs . add ( output_file_param_util . make_param ( name , row [ i ] , param . recursive ) ) task_descriptors . append ( job_model . TaskDescriptor ( { 'task-id' : task_id , 'task-attempt' : 1 if retries else None } , { 'labels' : labels , 'envs' : envs , 'inputs' : inputs , 'outputs' : outputs } , job_model . Resources ( ) ) ) if not task_descriptors : raise ValueError ( 'No tasks added from %s' % path ) return task_descriptors
5716	def validate ( self ) : warnings . warn ( 'Property "package.validate" is deprecated.' , UserWarning ) descriptor = self . to_dict ( ) self . profile . validate ( descriptor )
12346	def compress ( self , delete_tif = False , folder = None ) : return compress ( self . images , delete_tif , folder )
12336	def pip ( self , package_names , raise_on_error = True ) : if isinstance ( package_names , basestring ) : package_names = [ package_names ] cmd = "pip install -U %s" % ( ' ' . join ( package_names ) ) return self . wait ( cmd , raise_on_error = raise_on_error )
4745	def dev_get_chunk ( dev_name , state , pugrp = None , punit = None ) : rprt = dev_get_rprt ( dev_name , pugrp , punit ) if not rprt : return None return next ( ( d for d in rprt if d [ "cs" ] == state ) , None )
4892	def get_learner_data_records ( self , enterprise_enrollment , completed_date = None , grade = None , is_passing = False ) : LearnerDataTransmissionAudit = apps . get_model ( 'integrated_channel' , 'LearnerDataTransmissionAudit' ) completed_timestamp = None course_completed = False if completed_date is not None : completed_timestamp = parse_datetime_to_epoch_millis ( completed_date ) course_completed = is_passing return [ LearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , course_id = enterprise_enrollment . course_id , course_completed = course_completed , completed_timestamp = completed_timestamp , grade = grade , ) ]
11166	def unusedoptions ( self , sections ) : unused = set ( [ ] ) for section in _list ( sections ) : if not self . has_section ( section ) : continue options = self . options ( section ) raw_values = [ self . get ( section , option , raw = True ) for option in options ] for option in options : formatter = "%(" + option + ")s" for raw_value in raw_values : if formatter in raw_value : break else : unused . add ( option ) return list ( unused )
11240	def copy_web_file_to_local ( file_path , target_path ) : response = urllib . request . urlopen ( file_path ) f = open ( target_path , 'w' ) f . write ( response . read ( ) ) f . close ( )
9552	def _apply_value_checks ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for field_name , check , code , message , modulus in self . _value_checks : if i % modulus == 0 : fi = self . _field_names . index ( field_name ) if fi < len ( r ) : value = r [ fi ] try : check ( value ) except ValueError : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field_name p [ 'value' ] = value p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field_name p [ 'value' ] = value p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( check . __name__ , check . __doc__ ) if context is not None : p [ 'context' ] = context yield p
5416	def get_dstat_provider_args ( provider , project ) : provider_name = get_provider_name ( provider ) args = [ ] if provider_name == 'google' : args . append ( '--project %s' % project ) elif provider_name == 'google-v2' : args . append ( '--project %s' % project ) elif provider_name == 'local' : pass elif provider_name == 'test-fails' : pass else : assert False , 'Provider %s needs get_dstat_provider_args support' % provider args . insert ( 0 , '--provider %s' % provider_name ) return ' ' . join ( args )
9221	def mix ( self , color1 , color2 , weight = 50 , * args ) : if color1 and color2 : if isinstance ( weight , string_types ) : weight = float ( weight . strip ( '%' ) ) weight = ( ( weight / 100.0 ) * 2 ) - 1 rgb1 = self . _hextorgb ( color1 ) rgb2 = self . _hextorgb ( color2 ) alpha = 0 w1 = ( ( ( weight if weight * alpha == - 1 else weight + alpha ) / ( 1 + weight * alpha ) ) + 1 ) w1 = w1 / 2.0 w2 = 1 - w1 rgb = [ rgb1 [ 0 ] * w1 + rgb2 [ 0 ] * w2 , rgb1 [ 1 ] * w1 + rgb2 [ 1 ] * w2 , rgb1 [ 2 ] * w1 + rgb2 [ 2 ] * w2 , ] return self . _rgbatohex ( rgb ) raise ValueError ( 'Illegal color values' )
4813	def _document_frequency ( X ) : if sp . isspmatrix_csr ( X ) : return np . bincount ( X . indices , minlength = X . shape [ 1 ] ) return np . diff ( sp . csc_matrix ( X , copy = False ) . indptr )
9216	def file ( self , filename ) : with open ( filename ) as f : self . lexer . input ( f . read ( ) ) return self
3187	def create ( self , store_id , data ) : self . store_id = store_id if 'id' not in data : raise KeyError ( 'The order must have an id' ) if 'customer' not in data : raise KeyError ( 'The order must have a customer' ) if 'id' not in data [ 'customer' ] : raise KeyError ( 'The order customer must have an id' ) if 'currency_code' not in data : raise KeyError ( 'The order must have a currency_code' ) if not re . match ( r"^[A-Z]{3}$" , data [ 'currency_code' ] ) : raise ValueError ( 'The currency_code must be a valid 3-letter ISO 4217 currency code' ) if 'order_total' not in data : raise KeyError ( 'The order must have an order_total' ) if 'lines' not in data : raise KeyError ( 'The order must have at least one order line' ) for line in data [ 'lines' ] : if 'id' not in line : raise KeyError ( 'Each order line must have an id' ) if 'product_id' not in line : raise KeyError ( 'Each order line must have a product_id' ) if 'product_variant_id' not in line : raise KeyError ( 'Each order line must have a product_variant_id' ) if 'quantity' not in line : raise KeyError ( 'Each order line must have a quantity' ) if 'price' not in line : raise KeyError ( 'Each order line must have a price' ) response = self . _mc_client . _post ( url = self . _build_path ( store_id , 'orders' ) , data = data ) if response is not None : self . order_id = response [ 'id' ] else : self . order_id = None return response
8342	def _convertEntities ( self , match ) : x = match . group ( 1 ) if self . convertHTMLEntities and x in name2codepoint : return unichr ( name2codepoint [ x ] ) elif x in self . XML_ENTITIES_TO_SPECIAL_CHARS : if self . convertXMLEntities : return self . XML_ENTITIES_TO_SPECIAL_CHARS [ x ] else : return u'&%s;' % x elif len ( x ) > 0 and x [ 0 ] == '#' : if len ( x ) > 1 and x [ 1 ] == 'x' : return unichr ( int ( x [ 2 : ] , 16 ) ) else : return unichr ( int ( x [ 1 : ] ) ) elif self . escapeUnrecognizedEntities : return u'&amp;%s;' % x else : return u'&%s;' % x
13341	def expand_dims ( a , axis ) : if hasattr ( a , 'expand_dims' ) and hasattr ( type ( a ) , '__array_interface__' ) : return a . expand_dims ( axis ) else : return np . expand_dims ( a , axis )
12280	def find_matching_files ( self , includes ) : if len ( includes ) == 0 : return [ ] files = [ f [ 'relativepath' ] for f in self . package [ 'resources' ] ] includes = r'|' . join ( [ fnmatch . translate ( x ) for x in includes ] ) files = [ f for f in files if re . match ( includes , os . path . basename ( f ) ) ] + [ f for f in files if re . match ( includes , f ) ] files = list ( set ( files ) ) return files
4571	def adapt_animation_layout ( animation ) : layout = animation . layout required = getattr ( animation , 'LAYOUT_CLASS' , None ) if not required or isinstance ( layout , required ) : return msg = LAYOUT_WARNING % ( type ( animation ) . __name__ , required . __name__ , type ( layout ) . __name__ ) setter = layout . set adaptor = None if required is strip . Strip : if isinstance ( layout , matrix . Matrix ) : width = layout . width def adaptor ( pixel , color = None ) : y , x = divmod ( pixel , width ) setter ( x , y , color or BLACK ) elif isinstance ( layout , cube . Cube ) : lx , ly = layout . x , layout . y def adaptor ( pixel , color = None ) : yz , x = divmod ( pixel , lx ) z , y = divmod ( yz , ly ) setter ( x , y , z , color or BLACK ) elif isinstance ( layout , circle . Circle ) : def adaptor ( pixel , color = None ) : layout . _set_base ( pixel , color or BLACK ) elif required is matrix . Matrix : if isinstance ( layout , strip . Strip ) : width = animation . width def adaptor ( x , y , color = None ) : setter ( x + y * width , color or BLACK ) if not adaptor : raise ValueError ( msg ) log . warning ( msg ) animation . layout . set = adaptor
1187	def unexpo ( intpart , fraction , expo ) : if expo > 0 : f = len ( fraction ) intpart , fraction = intpart + fraction [ : expo ] , fraction [ expo : ] if expo > f : intpart = intpart + '0' * ( expo - f ) elif expo < 0 : i = len ( intpart ) intpart , fraction = intpart [ : expo ] , intpart [ expo : ] + fraction if expo < - i : fraction = '0' * ( - expo - i ) + fraction return intpart , fraction
8749	def delete_scalingip ( context , id ) : LOG . info ( 'delete_scalingip %s for tenant %s' % ( id , context . tenant_id ) ) _delete_flip ( context , id , ip_types . SCALING )
11363	def download_file ( from_url , to_filename = None , chunk_size = 1024 * 8 , retry_count = 3 ) : if not to_filename : to_filename = get_temporary_file ( ) session = requests . Session ( ) adapter = requests . adapters . HTTPAdapter ( max_retries = retry_count ) session . mount ( from_url , adapter ) response = session . get ( from_url , stream = True ) with open ( to_filename , 'wb' ) as fd : for chunk in response . iter_content ( chunk_size ) : fd . write ( chunk ) return to_filename
9866	def price_unit ( self ) : currency = self . currency consumption_unit = self . consumption_unit if not currency or not consumption_unit : _LOGGER . error ( "Could not find price_unit." ) return " " return currency + "/" + consumption_unit
3342	def send_status_response ( environ , start_response , e , add_headers = None , is_head = False ) : status = get_http_status_string ( e ) headers = [ ] if add_headers : headers . extend ( add_headers ) if e in ( HTTP_NOT_MODIFIED , HTTP_NO_CONTENT ) : start_response ( status , [ ( "Content-Length" , "0" ) , ( "Date" , get_rfc1123_time ( ) ) ] + headers ) return [ b"" ] if e in ( HTTP_OK , HTTP_CREATED ) : e = DAVError ( e ) assert isinstance ( e , DAVError ) content_type , body = e . get_response_page ( ) if is_head : body = compat . b_empty assert compat . is_bytes ( body ) , body start_response ( status , [ ( "Content-Type" , content_type ) , ( "Date" , get_rfc1123_time ( ) ) , ( "Content-Length" , str ( len ( body ) ) ) , ] + headers , ) return [ body ]
5564	def input ( self ) : delimiters = dict ( zoom = self . init_zoom_levels , bounds = self . init_bounds , process_bounds = self . bounds , effective_bounds = self . effective_bounds ) raw_inputs = { get_hash ( v ) : v for zoom in self . init_zoom_levels if "input" in self . _params_at_zoom [ zoom ] for key , v in _flatten_tree ( self . _params_at_zoom [ zoom ] [ "input" ] ) if v is not None } initalized_inputs = { } for k , v in raw_inputs . items ( ) : if isinstance ( v , str ) : logger . debug ( "load input reader for simple input %s" , v ) try : reader = load_input_reader ( dict ( path = absolute_path ( path = v , base_dir = self . config_dir ) , pyramid = self . process_pyramid , pixelbuffer = self . process_pyramid . pixelbuffer , delimiters = delimiters ) , readonly = self . mode == "readonly" ) except Exception as e : logger . exception ( e ) raise MapcheteDriverError ( "error when loading input %s: %s" % ( v , e ) ) logger . debug ( "input reader for simple input %s is %s" , v , reader ) elif isinstance ( v , dict ) : logger . debug ( "load input reader for abstract input %s" , v ) try : reader = load_input_reader ( dict ( abstract = deepcopy ( v ) , pyramid = self . process_pyramid , pixelbuffer = self . process_pyramid . pixelbuffer , delimiters = delimiters , conf_dir = self . config_dir ) , readonly = self . mode == "readonly" ) except Exception as e : logger . exception ( e ) raise MapcheteDriverError ( "error when loading input %s: %s" % ( v , e ) ) logger . debug ( "input reader for abstract input %s is %s" , v , reader ) else : raise MapcheteConfigError ( "invalid input type %s" , type ( v ) ) reader . bbox ( out_crs = self . process_pyramid . crs ) initalized_inputs [ k ] = reader return initalized_inputs
8360	def draw ( self , widget , cr ) : if self . bot_size is None : self . draw_default_image ( cr ) return cr = driver . ensure_pycairo_context ( cr ) surface = self . backing_store . surface cr . set_source_surface ( surface ) cr . paint ( )
5476	def get_zones ( input_list ) : if not input_list : return [ ] output_list = [ ] for zone in input_list : if zone . endswith ( '*' ) : prefix = zone [ : - 1 ] output_list . extend ( [ z for z in _ZONES if z . startswith ( prefix ) ] ) else : output_list . append ( zone ) return output_list
5260	def memberness ( context ) : if context : texts = context . xpath ( './/*[local-name()="explicitMember"]/text()' ) . extract ( ) text = str ( texts ) . lower ( ) if len ( texts ) > 1 : return 2 elif 'country' in text : return 2 elif 'member' not in text : return 0 elif 'successor' in text : return 1 elif 'parent' in text : return 2 return 3
7504	def _parse_names ( self ) : self . samples = [ ] with iter ( open ( self . files . data , 'r' ) ) as infile : infile . next ( ) . strip ( ) . split ( ) while 1 : try : self . samples . append ( infile . next ( ) . split ( ) [ 0 ] ) except StopIteration : break
4382	def is_allowed ( self , role , method , resource ) : return ( role , method , resource ) in self . _allowed
1391	def convert_pb_kvs ( kvs , include_non_primitives = True ) : config = { } for kv in kvs : if kv . value : config [ kv . key ] = kv . value elif kv . serialized_value : if topology_pb2 . JAVA_SERIALIZED_VALUE == kv . type : jv = _convert_java_value ( kv , include_non_primitives = include_non_primitives ) if jv is not None : config [ kv . key ] = jv else : config [ kv . key ] = _raw_value ( kv ) return config
12446	def resource ( ** kwargs ) : def inner ( function ) : name = kwargs . pop ( 'name' , None ) if name is None : name = utils . dasherize ( function . __name__ ) methods = kwargs . pop ( 'methods' , None ) if isinstance ( methods , six . string_types ) : methods = methods , handler = ( function , methods ) if name not in _resources : _handlers [ name ] = [ ] from armet import resources kwargs [ 'name' ] = name class LightweightResource ( resources . Resource ) : Meta = type ( str ( 'Meta' ) , ( ) , kwargs ) def route ( self , request , response ) : for handler , methods in _handlers [ name ] : if methods is None or request . method in methods : return handler ( request , response ) resources . Resource . route ( self ) _resources [ name ] = LightweightResource _handlers [ name ] . append ( handler ) return _resources [ name ] return inner
8638	def revoke_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'revoke' } endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotRevokedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
9653	def write_shas_to_shastore ( sha_dict ) : if sys . version_info [ 0 ] < 3 : fn_open = open else : fn_open = io . open with fn_open ( ".shastore" , "w" ) as fh : fh . write ( "---\n" ) fh . write ( 'sake version: {}\n' . format ( constants . VERSION ) ) if sha_dict : fh . write ( yaml . dump ( sha_dict ) ) fh . write ( "..." )
9544	def add_value_check ( self , field_name , value_check , code = VALUE_CHECK_FAILED , message = MESSAGES [ VALUE_CHECK_FAILED ] , modulus = 1 ) : assert field_name in self . _field_names , 'unexpected field name: %s' % field_name assert callable ( value_check ) , 'value check must be a callable function' t = field_name , value_check , code , message , modulus self . _value_checks . append ( t )
8982	def _set_pixel ( self , x , y , color ) : if not self . is_in_bounds ( x , y ) : return rgb = self . _convert_rrggbb_to_image_color ( color ) x -= self . _min_x y -= self . _min_y self . _image . putpixel ( ( x , y ) , rgb )
1959	def sys_openat ( self , dirfd , buf , flags , mode ) : filename = self . current . read_string ( buf ) dirfd = ctypes . c_int32 ( dirfd ) . value if os . path . isabs ( filename ) or dirfd == self . FCNTL_FDCWD : return self . sys_open ( buf , flags , mode ) try : dir_entry = self . _get_fd ( dirfd ) except FdError as e : logger . info ( "openat: Not valid file descriptor. Returning EBADF" ) return - e . err if not isinstance ( dir_entry , Directory ) : logger . info ( "openat: Not directory descriptor. Returning ENOTDIR" ) return - errno . ENOTDIR dir_path = dir_entry . name filename = os . path . join ( dir_path , filename ) try : f = self . _sys_open_get_file ( filename , flags ) logger . debug ( f"Opening file {filename} for real fd {f.fileno()}" ) except IOError as e : logger . info ( f"Could not open file {filename}. Reason: {e!s}" ) return - e . errno if e . errno is not None else - errno . EINVAL return self . _open ( f )
13502	def extra_context ( request ) : host = os . environ . get ( 'DJANGO_LIVE_TEST_SERVER_ADDRESS' , None ) or request . get_host ( ) d = { 'request' : request , 'HOST' : host , 'IN_ADMIN' : request . path . startswith ( '/admin/' ) , } return d
4018	def _get_app_libs_volume_mounts ( app_name , assembled_specs ) : volumes = [ ] for lib_name in assembled_specs [ 'apps' ] [ app_name ] [ 'depends' ] [ 'libs' ] : lib_spec = assembled_specs [ 'libs' ] [ lib_name ] volumes . append ( "{}:{}" . format ( Repo ( lib_spec [ 'repo' ] ) . vm_path , container_code_path ( lib_spec ) ) ) return volumes
4213	def pass_from_pipe ( cls ) : is_pipe = not sys . stdin . isatty ( ) return is_pipe and cls . strip_last_newline ( sys . stdin . read ( ) )
6092	def cos_and_sin_from_x_axis ( self ) : phi_radians = np . radians ( self . phi ) return np . cos ( phi_radians ) , np . sin ( phi_radians )
2245	def hzcat ( args , sep = '' ) : import unicodedata if '\n' in sep or '\r' in sep : raise ValueError ( '`sep` cannot contain newline characters' ) args = [ unicodedata . normalize ( 'NFC' , ensure_unicode ( val ) ) for val in args ] arglines = [ a . split ( '\n' ) for a in args ] height = max ( map ( len , arglines ) ) arglines = [ lines + [ '' ] * ( height - len ( lines ) ) for lines in arglines ] all_lines = [ '' for _ in range ( height ) ] width = 0 n_args = len ( args ) for sx , lines in enumerate ( arglines ) : for lx , line in enumerate ( lines ) : all_lines [ lx ] += line width = max ( width , max ( map ( len , all_lines ) ) ) if sx < n_args - 1 : for lx , line in list ( enumerate ( all_lines ) ) : residual = width - len ( line ) all_lines [ lx ] = line + ( ' ' * residual ) + sep width += len ( sep ) all_lines = [ line . rstrip ( ' ' ) for line in all_lines ] ret = '\n' . join ( all_lines ) return ret
464	def clear_all_placeholder_variables ( printable = True ) : tl . logging . info ( 'clear all .....................................' ) gl = globals ( ) . copy ( ) for var in gl : if var [ 0 ] == '_' : continue if 'func' in str ( globals ( ) [ var ] ) : continue if 'module' in str ( globals ( ) [ var ] ) : continue if 'class' in str ( globals ( ) [ var ] ) : continue if printable : tl . logging . info ( " clear_all ------- %s" % str ( globals ( ) [ var ] ) ) del globals ( ) [ var ]
6464	def usage_palette ( parser ) : parser . print_usage ( ) print ( '' ) print ( 'available palettes:' ) for palette in sorted ( PALETTE ) : print ( ' %-12s' % ( palette , ) ) return 0
612	def _getExperimentDescriptionSchema ( ) : installPath = os . path . dirname ( os . path . abspath ( __file__ ) ) schemaFilePath = os . path . join ( installPath , "experimentDescriptionSchema.json" ) return json . loads ( open ( schemaFilePath , 'r' ) . read ( ) )
10889	def kvectors ( self , norm = False , form = 'broadcast' , real = False , shift = False ) : if norm is False : norm = 1 if norm is True : norm = np . array ( self . shape ) norm = aN ( norm , self . dim , dtype = 'float' ) v = list ( np . fft . fftfreq ( self . shape [ i ] ) / norm [ i ] for i in range ( self . dim ) ) if shift : v = list ( np . fft . fftshift ( t ) for t in v ) if real : v [ - 1 ] = v [ - 1 ] [ : ( self . shape [ - 1 ] + 1 ) // 2 ] return self . _format_vector ( v , form = form )
722	def getData ( self , n ) : records = [ self . getNext ( ) for x in range ( n ) ] return records
3419	def save_matlab_model ( model , file_name , varname = None ) : if not scipy_io : raise ImportError ( 'load_matlab_model requires scipy' ) if varname is None : varname = str ( model . id ) if model . id is not None and len ( model . id ) > 0 else "exported_model" mat = create_mat_dict ( model ) scipy_io . savemat ( file_name , { varname : mat } , appendmat = True , oned_as = "column" )
10921	def do_levmarq_all_particle_groups ( s , region_size = 40 , max_iter = 2 , damping = 1.0 , decrease_damp_factor = 10. , run_length = 4 , collect_stats = False , ** kwargs ) : lp = LMParticleGroupCollection ( s , region_size = region_size , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , get_cos = collect_stats , max_iter = max_iter , ** kwargs ) lp . do_run_2 ( ) if collect_stats : return lp . stats
2536	def set_chksum ( self , doc , chk_sum ) : if chk_sum : doc . ext_document_references [ - 1 ] . check_sum = checksum . Algorithm ( 'SHA1' , chk_sum ) else : raise SPDXValueError ( 'ExternalDocumentRef::Checksum' )
5128	def transitions ( self , return_matrix = True ) : if return_matrix : mat = np . zeros ( ( self . nV , self . nV ) ) for v in self . g . nodes ( ) : ind = [ e [ 1 ] for e in sorted ( self . g . out_edges ( v ) ) ] mat [ v , ind ] = self . _route_probs [ v ] else : mat = { k : { e [ 1 ] : p for e , p in zip ( sorted ( self . g . out_edges ( k ) ) , value ) } for k , value in enumerate ( self . _route_probs ) } return mat
8448	def _has_branch ( branch ) : ret = temple . utils . shell ( 'git rev-parse --verify {}' . format ( branch ) , stderr = subprocess . DEVNULL , stdout = subprocess . DEVNULL , check = False ) return ret . returncode == 0
5823	def to_dict ( self ) : return { "type" : self . type , "name" : self . name , "group_by_key" : self . group_by_key , "role" : self . role , "units" : self . units , "options" : self . build_options ( ) }
7119	def filter_dict ( unfiltered , filter_keys ) : filtered = DotDict ( ) for k in filter_keys : filtered [ k ] = unfiltered [ k ] return filtered
8442	def _code_search ( query , github_user = None ) : github_client = temple . utils . GithubClient ( ) headers = { 'Accept' : 'application/vnd.github.v3.text-match+json' } resp = github_client . get ( '/search/code' , params = { 'q' : query , 'per_page' : 100 } , headers = headers ) if resp . status_code == requests . codes . unprocessable_entity and github_user : raise temple . exceptions . InvalidGithubUserError ( 'Invalid Github user or org - "{}"' . format ( github_user ) ) resp . raise_for_status ( ) resp_data = resp . json ( ) repositories = collections . defaultdict ( dict ) while True : repositories . update ( { 'git@github.com:{}.git' . format ( repo [ 'repository' ] [ 'full_name' ] ) : repo [ 'repository' ] for repo in resp_data [ 'items' ] } ) next_url = _parse_link_header ( resp . headers ) . get ( 'next' ) if next_url : resp = requests . get ( next_url , headers = headers ) resp . raise_for_status ( ) resp_data = resp . json ( ) else : break return repositories
1130	def urljoin ( base , url , allow_fragments = True ) : if not base : return url if not url : return base bscheme , bnetloc , bpath , bparams , bquery , bfragment = urlparse ( base , '' , allow_fragments ) scheme , netloc , path , params , query , fragment = urlparse ( url , bscheme , allow_fragments ) if scheme != bscheme or scheme not in uses_relative : return url if scheme in uses_netloc : if netloc : return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) netloc = bnetloc if path [ : 1 ] == '/' : return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) if not path and not params : path = bpath params = bparams if not query : query = bquery return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) segments = bpath . split ( '/' ) [ : - 1 ] + path . split ( '/' ) if segments [ - 1 ] == '.' : segments [ - 1 ] = '' while '.' in segments : segments . remove ( '.' ) while 1 : i = 1 n = len ( segments ) - 1 while i < n : if ( segments [ i ] == '..' and segments [ i - 1 ] not in ( '' , '..' ) ) : del segments [ i - 1 : i + 1 ] break i = i + 1 else : break if segments == [ '' , '..' ] : segments [ - 1 ] = '' elif len ( segments ) >= 2 and segments [ - 1 ] == '..' : segments [ - 2 : ] = [ '' ] return urlunparse ( ( scheme , netloc , '/' . join ( segments ) , params , query , fragment ) )
12444	def route ( self , request , response ) : self . require_http_allowed_method ( request ) function = getattr ( self , request . method . lower ( ) , None ) if function is None : raise http . exceptions . NotImplemented ( ) return function ( request , response )
4999	def enterprise_customer_required ( view ) : @ wraps ( view ) def wrapper ( request , * args , ** kwargs ) : user = request . user enterprise_customer = get_enterprise_customer_for_user ( user ) if enterprise_customer : args = args + ( enterprise_customer , ) return view ( request , * args , ** kwargs ) else : raise PermissionDenied ( 'User {username} is not associated with an EnterpriseCustomer.' . format ( username = user . username ) ) return wrapper
4283	def generate_video ( source , outname , settings , options = None ) : logger = logging . getLogger ( __name__ ) converter = settings [ 'video_converter' ] w_src , h_src = video_size ( source , converter = converter ) w_dst , h_dst = settings [ 'video_size' ] logger . debug ( 'Video size: %i, %i -> %i, %i' , w_src , h_src , w_dst , h_dst ) base , src_ext = splitext ( source ) base , dst_ext = splitext ( outname ) if dst_ext == src_ext and w_src <= w_dst and h_src <= h_dst : logger . debug ( 'Video is smaller than the max size, copying it instead' ) shutil . copy ( source , outname ) return if h_dst * w_src < h_src * w_dst : resize_opt = [ '-vf' , "scale=trunc(oh*a/2)*2:%i" % h_dst ] else : resize_opt = [ '-vf' , "scale=%i:trunc(ow/a/2)*2" % w_dst ] if w_src <= w_dst and h_src <= h_dst : resize_opt = [ ] cmd = [ converter , '-i' , source , '-y' ] if options is not None : cmd += options cmd += resize_opt + [ outname ] logger . debug ( 'Processing video: %s' , ' ' . join ( cmd ) ) check_subprocess ( cmd , source , outname )
11437	def _fields_sort_by_indicators ( fields ) : field_dict = { } field_positions_global = [ ] for field in fields : field_dict . setdefault ( field [ 1 : 3 ] , [ ] ) . append ( field ) field_positions_global . append ( field [ 4 ] ) indicators = field_dict . keys ( ) indicators . sort ( ) field_list = [ ] for indicator in indicators : for field in field_dict [ indicator ] : field_list . append ( field [ : 4 ] + ( field_positions_global . pop ( 0 ) , ) ) return field_list
7955	def getpeercert ( self ) : with self . lock : if not self . _socket or self . _tls_state != "connected" : raise ValueError ( "Not TLS-connected" ) return get_certificate_from_ssl_socket ( self . _socket )
9499	def convert_completezip ( path ) : for filepath in path . glob ( '**/index_auto_generated.cnxml' ) : filepath . rename ( filepath . parent / 'index.cnxml' ) logger . debug ( 'removed {}' . format ( filepath ) ) for filepath in path . glob ( '**/index.cnxml.html' ) : filepath . unlink ( ) return parse_litezip ( path )
3597	def list ( self , cat , ctr = None , nb_results = None , offset = None ) : path = LIST_URL + "?c=3&cat={}" . format ( requests . utils . quote ( cat ) ) if ctr is not None : path += "&ctr={}" . format ( requests . utils . quote ( ctr ) ) if nb_results is not None : path += "&n={}" . format ( requests . utils . quote ( str ( nb_results ) ) ) if offset is not None : path += "&o={}" . format ( requests . utils . quote ( str ( offset ) ) ) data = self . executeRequestApi2 ( path ) clusters = [ ] docs = [ ] if ctr is None : for pf in data . preFetch : for cluster in pf . response . payload . listResponse . doc : clusters . extend ( cluster . child ) return [ c . docid for c in clusters ] else : apps = [ ] for d in data . payload . listResponse . doc : for c in d . child : for a in c . child : apps . append ( utils . parseProtobufObj ( a ) ) return apps
5697	def get_median_lat_lon_of_stops ( gtfs ) : stops = gtfs . get_table ( "stops" ) median_lat = numpy . percentile ( stops [ 'lat' ] . values , 50 ) median_lon = numpy . percentile ( stops [ 'lon' ] . values , 50 ) return median_lat , median_lon
8263	def swatch ( self , x , y , w = 35 , h = 35 , padding = 0 , roundness = 0 ) : for clr in self : clr . swatch ( x , y , w , h , roundness ) y += h + padding
1357	def get_argument_topology ( self ) : try : topology = self . get_argument ( constants . PARAM_TOPOLOGY ) return topology except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
10701	def set_state ( _id , body ) : url = DEVICE_URL % _id if "mode" in body : url = MODES_URL % _id arequest = requests . put ( url , headers = HEADERS , data = json . dumps ( body ) ) status_code = str ( arequest . status_code ) if status_code != '202' : _LOGGER . error ( "State not accepted. " + status_code ) return False
13286	def get_dataframe_from_variable ( nc , data_var ) : time_var = nc . get_variables_by_attributes ( standard_name = 'time' ) [ 0 ] depth_vars = nc . get_variables_by_attributes ( axis = lambda v : v is not None and v . lower ( ) == 'z' ) depth_vars += nc . get_variables_by_attributes ( standard_name = lambda v : v in [ 'height' , 'depth' 'surface_altitude' ] , positive = lambda x : x is not None ) depth_var = None for d in depth_vars : try : if d . _name in data_var . coordinates . split ( " " ) or d . _name in data_var . dimensions : depth_var = d break except AttributeError : continue times = netCDF4 . num2date ( time_var [ : ] , units = time_var . units , calendar = getattr ( time_var , 'calendar' , 'standard' ) ) original_times_size = times . size if depth_var is None and hasattr ( data_var , 'sensor_depth' ) : depth_type = get_type ( data_var . sensor_depth ) depths = np . asarray ( [ data_var . sensor_depth ] * len ( times ) ) . flatten ( ) values = data_var [ : ] . flatten ( ) elif depth_var is None : depths = np . asarray ( [ np . nan ] * len ( times ) ) . flatten ( ) depth_type = get_type ( depths ) values = data_var [ : ] . flatten ( ) else : depths = depth_var [ : ] depth_type = get_type ( depths ) if len ( data_var . shape ) > 1 : times = np . repeat ( times , depths . size ) depths = np . tile ( depths , original_times_size ) values = data_var [ : , : ] . flatten ( ) else : values = data_var [ : ] . flatten ( ) if getattr ( depth_var , 'positive' , 'down' ) . lower ( ) == 'up' : logger . warning ( "Converting depths to positive down before returning the DataFrame" ) depths = depths * - 1 if ( isinstance ( depths , np . ma . core . MaskedConstant ) or ( hasattr ( depths , 'mask' ) and depths . mask . all ( ) ) ) : depths = np . asarray ( [ np . nan ] * len ( times ) ) . flatten ( ) df = pd . DataFrame ( { 'time' : times , 'value' : values . astype ( data_var . dtype ) , 'unit' : data_var . units if hasattr ( data_var , 'units' ) else np . nan , 'depth' : depths . astype ( depth_type ) } ) df . set_index ( [ pd . DatetimeIndex ( df [ 'time' ] ) , pd . Float64Index ( df [ 'depth' ] ) ] , inplace = True ) return df
297	def plot_return_quantiles ( returns , live_start_date = None , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) is_returns = returns if live_start_date is None else returns . loc [ returns . index < live_start_date ] is_weekly = ep . aggregate_returns ( is_returns , 'weekly' ) is_monthly = ep . aggregate_returns ( is_returns , 'monthly' ) sns . boxplot ( data = [ is_returns , is_weekly , is_monthly ] , palette = [ "#4c72B0" , "#55A868" , "#CCB974" ] , ax = ax , ** kwargs ) if live_start_date is not None : oos_returns = returns . loc [ returns . index >= live_start_date ] oos_weekly = ep . aggregate_returns ( oos_returns , 'weekly' ) oos_monthly = ep . aggregate_returns ( oos_returns , 'monthly' ) sns . swarmplot ( data = [ oos_returns , oos_weekly , oos_monthly ] , ax = ax , color = "red" , marker = "d" , ** kwargs ) red_dots = matplotlib . lines . Line2D ( [ ] , [ ] , color = "red" , marker = "d" , label = "Out-of-sample data" , linestyle = '' ) ax . legend ( handles = [ red_dots ] , frameon = True , framealpha = 0.5 ) ax . set_xticklabels ( [ 'Daily' , 'Weekly' , 'Monthly' ] ) ax . set_title ( 'Return quantiles' ) return ax
13625	def Boolean ( value , true = ( u'yes' , u'1' , u'true' ) , false = ( u'no' , u'0' , u'false' ) , encoding = None ) : value = Text ( value , encoding ) if value is not None : value = value . lower ( ) . strip ( ) if value in true : return True elif value in false : return False return None
9585	def write_compressed_var_array ( fd , array , name ) : bd = BytesIO ( ) write_var_array ( bd , array , name ) data = zlib . compress ( bd . getvalue ( ) ) bd . close ( ) fd . write ( struct . pack ( 'b3xI' , etypes [ 'miCOMPRESSED' ] [ 'n' ] , len ( data ) ) ) fd . write ( data )
5424	def _wait_and_retry ( provider , job_id , poll_interval , retries , job_descriptor ) : while True : tasks = provider . lookup_job_tasks ( { '*' } , job_ids = [ job_id ] ) running_tasks = set ( ) completed_tasks = set ( ) canceled_tasks = set ( ) fully_failed_tasks = set ( ) task_fail_count = dict ( ) message_task = None task_dict = dict ( ) for t in tasks : task_id = job_model . numeric_task_id ( t . get_field ( 'task-id' ) ) task_dict [ task_id ] = t status = t . get_field ( 'task-status' ) if status == 'FAILURE' : task_fail_count [ task_id ] = task_fail_count . get ( task_id , 0 ) + 1 if task_fail_count [ task_id ] > retries : fully_failed_tasks . add ( task_id ) message_task = t elif status == 'CANCELED' : canceled_tasks . add ( task_id ) if not message_task : message_task = t elif status == 'SUCCESS' : completed_tasks . add ( task_id ) elif status == 'RUNNING' : running_tasks . add ( task_id ) retry_tasks = ( set ( task_fail_count ) . difference ( fully_failed_tasks ) . difference ( running_tasks ) . difference ( completed_tasks ) . difference ( canceled_tasks ) ) if not retry_tasks and not running_tasks : if message_task : return [ provider . get_tasks_completion_messages ( [ message_task ] ) ] return [ ] for task_id in retry_tasks : identifier = '{}.{}' . format ( job_id , task_id ) if task_id else job_id print ( ' {} (attempt {}) failed. Retrying.' . format ( identifier , task_fail_count [ task_id ] ) ) msg = task_dict [ task_id ] . get_field ( 'status-message' ) print ( ' Failure message: {}' . format ( msg ) ) _retry_task ( provider , job_descriptor , task_id , task_fail_count [ task_id ] + 1 ) SLEEP_FUNCTION ( poll_interval )
9243	def find_closed_date_by_commit ( self , issue ) : if not issue . get ( 'events' ) : return compare_string = "merged" if 'merged_at' in issue else "closed" issue [ 'events' ] . reverse ( ) found_date = False for event in issue [ 'events' ] : if event [ "event" ] == compare_string : self . set_date_from_event ( event , issue ) found_date = True break if not found_date : print ( "\nWARNING: Issue without 'actual_date':" " #{0} {1}" . format ( issue [ "number" ] , issue [ "title" ] ) )
3761	def draw_2d ( self , Hs = False ) : r try : from rdkit . Chem import Draw from rdkit . Chem . Draw import IPythonConsole if Hs : mols = [ i . rdkitmol_Hs for i in self . Chemicals ] else : mols = [ i . rdkitmol for i in self . Chemicals ] return Draw . MolsToImage ( mols ) except : return 'Rdkit is required for this feature.'
2662	def _hold_block ( self , block_id ) : managers = self . connected_managers for manager in managers : if manager [ 'block_id' ] == block_id : logger . debug ( "[HOLD_BLOCK]: Sending hold to manager:{}" . format ( manager [ 'manager' ] ) ) self . hold_worker ( manager [ 'manager' ] )
7831	def add_field ( self , name = None , values = None , field_type = None , label = None , options = None , required = False , desc = None , value = None ) : field = Field ( name , values , field_type , label , options , required , desc , value ) self . fields . append ( field ) return field
8182	def remove_edge ( self , id1 , id2 ) : for e in list ( self . edges ) : if id1 in ( e . node1 . id , e . node2 . id ) and id2 in ( e . node1 . id , e . node2 . id ) : e . node1 . links . remove ( e . node2 ) e . node2 . links . remove ( e . node1 ) self . edges . remove ( e )
11222	def is_compressed_json_file ( abspath ) : abspath = abspath . lower ( ) fname , ext = os . path . splitext ( abspath ) if ext in [ ".json" , ".js" ] : is_compressed = False elif ext == ".gz" : is_compressed = True else : raise ValueError ( "'%s' is not a valid json file. " "extension has to be '.json' or '.js' for uncompressed, '.gz' " "for compressed." % abspath ) return is_compressed
11071	def with_proxies ( proxy_map , get_key ) : def wrapper ( cls ) : for label , ProxiedClass in six . iteritems ( proxy_map ) : proxy = proxy_factory ( cls , label , ProxiedClass , get_key ) setattr ( cls , label , proxy ) return cls return wrapper
2292	def orient_undirected_graph ( self , data , umg , alg = 'HC' ) : warnings . warn ( "The pairwise GNN model is computed on each edge of the UMG " "to initialize the model and start CGNN with a DAG" ) gnn = GNN ( nh = self . nh , lr = self . lr ) og = gnn . orient_graph ( data , umg , nb_runs = self . nb_runs , nb_max_runs = self . nb_runs , nb_jobs = self . nb_jobs , train_epochs = self . train_epochs , test_epochs = self . test_epochs , verbose = self . verbose , gpu = self . gpu ) dag = dagify_min_edge ( og ) return self . orient_directed_graph ( data , dag , alg = alg )
8721	def operation_download ( uploader , sources ) : sources , destinations = destination_from_source ( sources , False ) print ( 'sources' , sources ) print ( 'destinations' , destinations ) if len ( destinations ) == len ( sources ) : if uploader . prepare ( ) : for filename , dst in zip ( sources , destinations ) : uploader . read_file ( filename , dst ) else : raise Exception ( 'You must specify a destination filename for each file you want to download.' ) log . info ( 'All done!' )
10201	def register_events ( ) : return [ dict ( event_type = 'file-download' , templates = 'invenio_stats.contrib.file_download' , processor_class = EventsIndexer , processor_config = dict ( preprocessors = [ flag_robots , anonymize_user , build_file_unique_id ] ) ) , dict ( event_type = 'record-view' , templates = 'invenio_stats.contrib.record_view' , processor_class = EventsIndexer , processor_config = dict ( preprocessors = [ flag_robots , anonymize_user , build_record_unique_id ] ) ) ]
10155	def convert ( self , schema_node , definition_handler ) : converted = { 'name' : schema_node . name , 'in' : self . _in , 'required' : schema_node . required } if schema_node . description : converted [ 'description' ] = schema_node . description if schema_node . default : converted [ 'default' ] = schema_node . default schema = definition_handler ( schema_node ) schema . pop ( 'title' , None ) converted . update ( schema ) if schema . get ( 'type' ) == 'array' : converted [ 'items' ] = { 'type' : schema [ 'items' ] [ 'type' ] } return converted
1117	def make_table ( self , fromlines , tolines , fromdesc = '' , todesc = '' , context = False , numlines = 5 ) : self . _make_prefix ( ) fromlines , tolines = self . _tab_newline_replace ( fromlines , tolines ) if context : context_lines = numlines else : context_lines = None diffs = _mdiff ( fromlines , tolines , context_lines , linejunk = self . _linejunk , charjunk = self . _charjunk ) if self . _wrapcolumn : diffs = self . _line_wrapper ( diffs ) fromlist , tolist , flaglist = self . _collect_lines ( diffs ) fromlist , tolist , flaglist , next_href , next_id = self . _convert_flags ( fromlist , tolist , flaglist , context , numlines ) s = [ ] fmt = ' <tr><td class="diff_next"%s>%s</td>%s' + '<td class="diff_next">%s</td>%s</tr>\n' for i in range ( len ( flaglist ) ) : if flaglist [ i ] is None : if i > 0 : s . append ( ' </tbody> \n <tbody>\n' ) else : s . append ( fmt % ( next_id [ i ] , next_href [ i ] , fromlist [ i ] , next_href [ i ] , tolist [ i ] ) ) if fromdesc or todesc : header_row = '<thead><tr>%s%s%s%s</tr></thead>' % ( '<th class="diff_next"><br /></th>' , '<th colspan="2" class="diff_header">%s</th>' % fromdesc , '<th class="diff_next"><br /></th>' , '<th colspan="2" class="diff_header">%s</th>' % todesc ) else : header_row = '' table = self . _table_template % dict ( data_rows = '' . join ( s ) , header_row = header_row , prefix = self . _prefix [ 1 ] ) return table . replace ( '\0+' , '<span class="diff_add">' ) . replace ( '\0-' , '<span class="diff_sub">' ) . replace ( '\0^' , '<span class="diff_chg">' ) . replace ( '\1' , '</span>' ) . replace ( '\t' , '&nbsp;' )
2384	def read_yaml_file ( path , loader = ExtendedSafeLoader ) : with open ( path ) as fh : return load ( fh , loader )
8706	def verify_file ( self , path , destination , verify = 'none' ) : content = from_file ( path ) log . info ( 'Verifying using %s...' % verify ) if verify == 'raw' : data = self . download_file ( destination ) if content != data : log . error ( 'Raw verification failed.' ) raise VerificationError ( 'Verification failed.' ) else : log . info ( 'Verification successful. Contents are identical.' ) elif verify == 'sha1' : data = self . __exchange ( 'shafile("' + destination + '")' ) . splitlines ( ) [ 1 ] log . info ( 'Remote SHA1: %s' , data ) filehashhex = hashlib . sha1 ( content . encode ( ENCODING ) ) . hexdigest ( ) log . info ( 'Local SHA1: %s' , filehashhex ) if data != filehashhex : log . error ( 'SHA1 verification failed.' ) raise VerificationError ( 'SHA1 Verification failed.' ) else : log . info ( 'Verification successful. Checksums match' ) elif verify != 'none' : raise Exception ( verify + ' is not a valid verification method.' )
3985	def _dusty_hosts_config ( hosts_specs ) : rules = '' . join ( [ '{} {}\n' . format ( spec [ 'forwarded_ip' ] , spec [ 'host_address' ] ) for spec in hosts_specs ] ) return config_file . create_config_section ( rules )
11282	def append ( self , next ) : next . chained = True if self . next : self . next . append ( next ) else : self . next = next
2691	def iter_cython ( path ) : for dir_path , dir_names , file_names in os . walk ( path ) : for file_name in file_names : if file_name . startswith ( '.' ) : continue if os . path . splitext ( file_name ) [ 1 ] not in ( '.pyx' , '.pxd' ) : continue yield os . path . join ( dir_path , file_name )
8297	def hexDump ( bytes ) : for i in range ( len ( bytes ) ) : sys . stdout . write ( "%2x " % ( ord ( bytes [ i ] ) ) ) if ( i + 1 ) % 8 == 0 : print repr ( bytes [ i - 7 : i + 1 ] ) if ( len ( bytes ) % 8 != 0 ) : print string . rjust ( "" , 11 ) , repr ( bytes [ i - len ( bytes ) % 8 : i + 1 ] )
12360	def format_request_url ( self , resource , * args ) : return '/' . join ( ( self . api_url , self . api_version , resource ) + tuple ( str ( x ) for x in args ) )
8273	def color ( self , d = 0.035 ) : s = sum ( [ w for clr , rng , w in self . ranges ] ) r = random ( ) for clr , rng , weight in self . ranges : if weight / s >= r : break r -= weight / s return rng ( clr , d )
11327	def autodiscover ( ) : import imp from django . conf import settings for app in settings . INSTALLED_APPS : try : app_path = __import__ ( app , { } , { } , [ app . split ( '.' ) [ - 1 ] ] ) . __path__ except AttributeError : continue try : imp . find_module ( 'oembed_providers' , app_path ) except ImportError : continue __import__ ( "%s.oembed_providers" % app )
12672	def group ( * args ) : if args and isinstance ( args [ 0 ] , dataframe . DataFrame ) : return args [ 0 ] . group ( * args [ 1 : ] ) elif not args : raise ValueError ( "No arguments provided" ) else : return pipeable . Pipeable ( pipeable . PipingMethod . GROUP , * args )
6441	def sim_euclidean ( src , tar , qval = 2 , alphabet = None ) : return Euclidean ( ) . sim ( src , tar , qval , alphabet )
5349	def compose_title ( projects , data ) : for project in data : projects [ project ] = { 'meta' : { 'title' : data [ project ] [ 'title' ] } } return projects
2112	def parse_requirements ( filename ) : reqs = [ ] version_spec_in_play = None for line in open ( filename , 'r' ) . read ( ) . strip ( ) . split ( '\n' ) : if not line . strip ( ) : continue if not line . startswith ( '#' ) : reqs . append ( line ) continue match = re . search ( r'^# === [Pp]ython (?P<op>[<>=]{1,2}) ' r'(?P<major>[\d])\.(?P<minor>[\d]+) ===[\s]*$' , line ) if match : version_spec_in_play = match . groupdict ( ) for key in ( 'major' , 'minor' ) : version_spec_in_play [ key ] = int ( version_spec_in_play [ key ] ) continue if ' ' not in line [ 1 : ] . strip ( ) and version_spec_in_play : package = line [ 1 : ] . strip ( ) op = version_spec_in_play [ 'op' ] vspec = ( version_spec_in_play [ 'major' ] , version_spec_in_play [ 'minor' ] ) if '=' in op and sys . version_info [ 0 : 2 ] == vspec : reqs . append ( package ) elif '>' in op and sys . version_info [ 0 : 2 ] > vspec : reqs . append ( package ) elif '<' in op and sys . version_info [ 0 : 2 ] < vspec : reqs . append ( package ) return reqs
11603	def parse_byteranges ( cls , environ ) : r = [ ] s = environ . get ( cls . header_range , '' ) . replace ( ' ' , '' ) . lower ( ) if s : l = s . split ( '=' ) if len ( l ) == 2 : unit , vals = tuple ( l ) if unit == 'bytes' and vals : gen_rng = ( tuple ( rng . split ( '-' ) ) for rng in vals . split ( ',' ) if '-' in rng ) for start , end in gen_rng : if start or end : r . append ( ( int ( start ) if start else None , int ( end ) if end else None ) ) return r
9203	def render ( node , strict = False ) : if isinstance ( node , list ) : return render_list ( node ) elif isinstance ( node , dict ) : return render_node ( node , strict = strict ) else : raise NotImplementedError ( "You tried to render a %s. Only list and dicts can be rendered." % node . __class__ . __name__ )
9276	def apply_exclude_tags ( self , all_tags ) : filtered = copy . deepcopy ( all_tags ) for tag in all_tags : if tag [ "name" ] not in self . options . exclude_tags : self . warn_if_tag_not_found ( tag , "exclude-tags" ) else : filtered . remove ( tag ) return filtered
8766	def _fix_missing_tenant_id ( self , context , body , key ) : if not body : raise n_exc . BadRequest ( resource = key , msg = "Body malformed" ) resource = body . get ( key ) if not resource : raise n_exc . BadRequest ( resource = key , msg = "Body malformed" ) if context . tenant_id is None : context . tenant_id = resource . get ( "tenant_id" ) if context . tenant_id is None : msg = _ ( "Running without keystone AuthN requires " "that tenant_id is specified" ) raise n_exc . BadRequest ( resource = key , msg = msg )
9997	def del_cells ( self , name ) : if name in self . cells : cells = self . cells [ name ] self . cells . del_item ( name ) self . inherit ( ) self . model . spacegraph . update_subspaces ( self ) elif name in self . dynamic_spaces : cells = self . dynamic_spaces . pop ( name ) self . dynamic_spaces . set_update ( ) else : raise KeyError ( "Cells '%s' does not exist" % name ) NullImpl ( cells )
9817	def install ( self ) : if not self . is_valid : raise PolyaxonDeploymentConfigError ( 'Deployment type `{}` not supported' . format ( self . deployment_type ) ) if self . is_kubernetes : self . install_on_kubernetes ( ) elif self . is_docker_compose : self . install_on_docker_compose ( ) elif self . is_docker : self . install_on_docker ( ) elif self . is_heroku : self . install_on_heroku ( )
7312	def is_local_ip ( ip_address ) : try : ip = ipaddress . ip_address ( u'' + ip_address ) return ip . is_loopback except ValueError as e : return None
669	def createDataOutLink ( network , sensorRegionName , regionName ) : network . link ( sensorRegionName , regionName , "UniformLink" , "" , srcOutput = "dataOut" , destInput = "bottomUpIn" )
6089	def for_data_and_tracer ( cls , lens_data , tracer , padded_tracer = None ) : if tracer . has_light_profile and not tracer . has_pixelization : return LensProfileFit ( lens_data = lens_data , tracer = tracer , padded_tracer = padded_tracer ) elif not tracer . has_light_profile and tracer . has_pixelization : return LensInversionFit ( lens_data = lens_data , tracer = tracer , padded_tracer = None ) elif tracer . has_light_profile and tracer . has_pixelization : return LensProfileInversionFit ( lens_data = lens_data , tracer = tracer , padded_tracer = None ) else : raise exc . FittingException ( 'The fit routine did not call a Fit class - check the ' 'properties of the tracer' )
1125	def Rule ( name , loc = None ) : @ llrule ( loc , lambda parser : getattr ( parser , name ) . expected ( parser ) ) def rule ( parser ) : return getattr ( parser , name ) ( ) return rule
6047	def map_to_2d_keep_padded ( self , padded_array_1d ) : return mapping_util . map_unmasked_1d_array_to_2d_array_from_array_1d_and_shape ( array_1d = padded_array_1d , shape = self . mask . shape )
12499	def fwhm2sigma ( fwhm ) : fwhm = np . asarray ( fwhm ) return fwhm / np . sqrt ( 8 * np . log ( 2 ) )
11073	def set_nested ( data , value , * keys ) : if len ( keys ) == 1 : data [ keys [ 0 ] ] = value else : if keys [ 0 ] not in data : data [ keys [ 0 ] ] = { } set_nested ( data [ keys [ 0 ] ] , value , * keys [ 1 : ] )
6691	def sync ( self , sync_set , force = 0 , site = None , role = None ) : from burlap . dj import dj force = int ( force ) r = self . local_renderer r . env . sync_force_flag = ' --force ' if force else '' _settings = dj . get_settings ( site = site , role = role ) assert _settings , 'Unable to import settings.' for k in _settings . __dict__ . iterkeys ( ) : if k . startswith ( 'AWS_' ) : r . genv [ k ] = _settings . __dict__ [ k ] site_data = r . genv . sites [ r . genv . SITE ] r . env . update ( site_data ) r . env . virtualenv_bin_dir = os . path . split ( sys . executable ) [ 0 ] rets = [ ] for paths in r . env . sync_sets [ sync_set ] : is_local = paths . get ( 'is_local' , True ) local_path = paths [ 'local_path' ] % r . genv remote_path = paths [ 'remote_path' ] remote_path = remote_path . replace ( ':/' , '/' ) if not remote_path . startswith ( 's3://' ) : remote_path = 's3://' + remote_path local_path = local_path % r . genv if is_local : r . env . local_path = os . path . abspath ( local_path ) else : r . env . local_path = local_path if local_path . endswith ( '/' ) and not r . env . local_path . endswith ( '/' ) : r . env . local_path = r . env . local_path + '/' r . env . remote_path = remote_path % r . genv print ( 'Syncing %s to %s...' % ( r . env . local_path , r . env . remote_path ) ) if force : r . env . sync_cmd = 'put' else : r . env . sync_cmd = 'sync' r . local ( 'export AWS_ACCESS_KEY_ID={aws_access_key_id}; ' 'export AWS_SECRET_ACCESS_KEY={aws_secret_access_key}; ' '{s3cmd_path} {sync_cmd} --progress --acl-public --guess-mime-type --no-mime-magic ' '--delete-removed --cf-invalidate --recursive {sync_force_flag} ' '{local_path} {remote_path}' )
731	def _getW ( self ) : w = self . _w if type ( w ) is list : return w [ self . _random . getUInt32 ( len ( w ) ) ] else : return w
9896	def uptime ( ) : if __boottime is not None : return time . time ( ) - __boottime return { 'amiga' : _uptime_amiga , 'aros12' : _uptime_amiga , 'beos5' : _uptime_beos , 'cygwin' : _uptime_linux , 'darwin' : _uptime_osx , 'haiku1' : _uptime_beos , 'linux' : _uptime_linux , 'linux-armv71' : _uptime_linux , 'linux2' : _uptime_linux , 'mac' : _uptime_mac , 'minix3' : _uptime_minix , 'riscos' : _uptime_riscos , 'sunos5' : _uptime_solaris , 'syllable' : _uptime_syllable , 'win32' : _uptime_windows , 'wince' : _uptime_windows } . get ( sys . platform , _uptime_bsd ) ( ) or _uptime_bsd ( ) or _uptime_plan9 ( ) or _uptime_linux ( ) or _uptime_windows ( ) or _uptime_solaris ( ) or _uptime_beos ( ) or _uptime_amiga ( ) or _uptime_riscos ( ) or _uptime_posix ( ) or _uptime_syllable ( ) or _uptime_mac ( ) or _uptime_osx ( )
10026	def create_application_version ( self , version_label , key ) : out ( "Creating application version " + str ( version_label ) + " for " + str ( key ) ) self . ebs . create_application_version ( self . app_name , version_label , s3_bucket = self . aws . bucket , s3_key = self . aws . bucket_path + key )
1161	def acquire ( self , blocking = 1 ) : rc = False with self . __cond : while self . __value == 0 : if not blocking : break if __debug__ : self . _note ( "%s.acquire(%s): blocked waiting, value=%s" , self , blocking , self . __value ) self . __cond . wait ( ) else : self . __value = self . __value - 1 if __debug__ : self . _note ( "%s.acquire: success, value=%s" , self , self . __value ) rc = True return rc
5147	def generate ( self ) : tar_bytes = BytesIO ( ) tar = tarfile . open ( fileobj = tar_bytes , mode = 'w' ) self . _generate_contents ( tar ) self . _process_files ( tar ) tar . close ( ) tar_bytes . seek ( 0 ) gzip_bytes = BytesIO ( ) gz = gzip . GzipFile ( fileobj = gzip_bytes , mode = 'wb' , mtime = 0 ) gz . write ( tar_bytes . getvalue ( ) ) gz . close ( ) gzip_bytes . seek ( 0 ) return gzip_bytes
3734	def load_included_indentifiers ( self , file_name ) : self . restrict_identifiers = True included_identifiers = set ( ) with open ( file_name ) as f : [ included_identifiers . add ( int ( line ) ) for line in f ] self . included_identifiers = included_identifiers
13892	def _AssertIsLocal ( path ) : from six . moves . urllib . parse import urlparse if not _UrlIsLocal ( urlparse ( path ) ) : from . _exceptions import NotImplementedForRemotePathError raise NotImplementedForRemotePathError
13156	def transaction ( func ) : @ wraps ( func ) def wrapper ( cls , * args , ** kwargs ) : with ( yield from cls . get_cursor ( _CursorType . NAMEDTUPLE ) ) as c : try : yield from c . execute ( 'BEGIN' ) result = ( yield from func ( cls , c , * args , ** kwargs ) ) except Exception : yield from c . execute ( 'ROLLBACK' ) else : yield from c . execute ( 'COMMIT' ) return result return wrapper
3247	def get_users ( group , ** conn ) : group_details = get_group_api ( group [ 'GroupName' ] , ** conn ) user_list = [ ] for user in group_details . get ( 'Users' , [ ] ) : user_list . append ( user [ 'UserName' ] ) return user_list
5671	def plot_temporal_distance_cdf ( self ) : xvalues , cdf = self . profile_block_analyzer . _temporal_distance_cdf ( ) fig = plt . figure ( ) ax = fig . add_subplot ( 111 ) xvalues = numpy . array ( xvalues ) / 60.0 ax . plot ( xvalues , cdf , "-k" ) ax . fill_between ( xvalues , cdf , color = "red" , alpha = 0.2 ) ax . set_ylabel ( "CDF(t)" ) ax . set_xlabel ( "Temporal distance t (min)" ) return fig
8540	def _read_config ( self , filename = None ) : if filename : self . _config_filename = filename else : try : import appdirs except ImportError : raise Exception ( "Missing dependency for determining config path. Please install " "the 'appdirs' Python module." ) self . _config_filename = appdirs . user_config_dir ( _LIBRARY_NAME , "ProfitBricks" ) + ".ini" if not self . _config : self . _config = configparser . ConfigParser ( ) self . _config . optionxform = str self . _config . read ( self . _config_filename )
11289	def strip_xml_namespace ( root ) : try : root . tag = root . tag . split ( '}' ) [ 1 ] except IndexError : pass for element in root . getchildren ( ) : strip_xml_namespace ( element )
12601	def _check_cols ( df , col_names ) : for col in col_names : if not hasattr ( df , col ) : raise AttributeError ( "DataFrame does not have a '{}' column, got {}." . format ( col , df . columns ) )
6222	def _update_yaw_and_pitch ( self ) : front = Vector3 ( [ 0.0 , 0.0 , 0.0 ] ) front . x = cos ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) front . y = sin ( radians ( self . pitch ) ) front . z = sin ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) self . dir = vector . normalise ( front ) self . right = vector . normalise ( vector3 . cross ( self . dir , self . _up ) ) self . up = vector . normalise ( vector3 . cross ( self . right , self . dir ) )
9663	def get_tied_targets ( original_targets , the_ties ) : my_ties = [ ] for original_target in original_targets : for item in the_ties : if original_target in item : for thing in item : my_ties . append ( thing ) my_ties = list ( set ( my_ties ) ) if my_ties : ties_message = "" ties_message += "The following targets share dependencies and must be run together:" for item in sorted ( my_ties ) : ties_message += "\n - {}" . format ( item ) return list ( set ( my_ties + original_targets ) ) , ties_message return original_targets , ""
8073	def requirements ( debug = True , with_examples = True , with_pgi = None ) : reqs = list ( BASE_REQUIREMENTS ) if with_pgi is None : with_pgi = is_jython if debug : print ( "setup options: " ) print ( "with_pgi: " , "yes" if with_pgi else "no" ) print ( "with_examples: " , "yes" if with_examples else "no" ) if with_pgi : reqs . append ( "pgi" ) if debug : print ( "warning, as of April 2019 typography does not work with pgi" ) else : reqs . append ( PYGOBJECT ) if with_examples : reqs . extend ( EXAMPLE_REQUIREMENTS ) if debug : print ( "" ) print ( "" ) for req in reqs : print ( req ) return reqs
859	def isTemporal ( inferenceElement ) : if InferenceElement . __temporalInferenceElements is None : InferenceElement . __temporalInferenceElements = set ( [ InferenceElement . prediction ] ) return inferenceElement in InferenceElement . __temporalInferenceElements
488	def _trackInstanceAndCheckForConcurrencyViolation ( self ) : global g_max_concurrency , g_max_concurrency_raise_exception assert g_max_concurrency is not None assert self not in self . _clsOutstandingInstances , repr ( self ) self . _creationTracebackString = traceback . format_stack ( ) if self . _clsNumOutstanding >= g_max_concurrency : errorMsg = ( "With numOutstanding=%r, exceeded concurrency limit=%r " "when requesting %r. OTHER TRACKED UNRELEASED " "INSTANCES (%s): %r" ) % ( self . _clsNumOutstanding , g_max_concurrency , self , len ( self . _clsOutstandingInstances ) , self . _clsOutstandingInstances , ) self . _logger . error ( errorMsg ) if g_max_concurrency_raise_exception : raise ConcurrencyExceededError ( errorMsg ) self . _clsOutstandingInstances . add ( self ) self . _addedToInstanceSet = True return
7341	async def get_size ( media ) : if hasattr ( media , 'seek' ) : await execute ( media . seek ( 0 , os . SEEK_END ) ) size = await execute ( media . tell ( ) ) await execute ( media . seek ( 0 ) ) elif hasattr ( media , 'headers' ) : size = int ( media . headers [ 'Content-Length' ] ) elif isinstance ( media , bytes ) : size = len ( media ) else : raise TypeError ( "Can't get size of media of type:" , type ( media ) . __name__ ) _logger . info ( "media size: %dB" % size ) return size
13205	def _parse_title ( self ) : command = LatexCommand ( 'title' , { 'name' : 'short_title' , 'required' : False , 'bracket' : '[' } , { 'name' : 'long_title' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no title' ) self . _title = None self . _short_title = None self . _title = parsed [ 'long_title' ] try : self . _short_title = parsed [ 'short_title' ] except KeyError : self . _logger . warning ( 'lsstdoc has no short title' ) self . _short_title = None
1313	def ControlFromPoint ( x : int , y : int ) -> Control : element = _AutomationClient . instance ( ) . IUIAutomation . ElementFromPoint ( ctypes . wintypes . POINT ( x , y ) ) return Control . CreateControlFromElement ( element )
4803	def raises ( self , ex ) : if not callable ( self . val ) : raise TypeError ( 'val must be callable' ) if not issubclass ( ex , BaseException ) : raise TypeError ( 'given arg must be exception' ) return AssertionBuilder ( self . val , self . description , self . kind , ex )
6986	def parallel_timebin_lcdir ( lcdir , binsizesec , maxobjects = None , outdir = None , lcformat = 'hat-sql' , lcformatdir = None , timecols = None , magcols = None , errcols = None , minbinelems = 7 , nworkers = NCPUS , maxworkertasks = 1000 ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( fileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None lclist = sorted ( glob . glob ( os . path . join ( lcdir , fileglob ) ) ) return parallel_timebin ( lclist , binsizesec , maxobjects = maxobjects , outdir = outdir , lcformat = lcformat , timecols = timecols , magcols = magcols , errcols = errcols , minbinelems = minbinelems , nworkers = nworkers , maxworkertasks = maxworkertasks )
5575	def available_input_formats ( ) : input_formats = [ ] for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : logger . debug ( "driver found: %s" , v ) driver_ = v . load ( ) if hasattr ( driver_ , "METADATA" ) and ( driver_ . METADATA [ "mode" ] in [ "r" , "rw" ] ) : input_formats . append ( driver_ . METADATA [ "driver_name" ] ) return input_formats
2423	def set_doc_version ( self , doc , value ) : if not self . doc_version_set : self . doc_version_set = True m = self . VERS_STR_REGEX . match ( value ) if m is None : raise SPDXValueError ( 'Document::Version' ) else : doc . version = version . Version ( major = int ( m . group ( 1 ) ) , minor = int ( m . group ( 2 ) ) ) return True else : raise CardinalityError ( 'Document::Version' )
7965	def start ( self , tag , attrs ) : if self . _level == 0 : self . _root = ElementTree . Element ( tag , attrs ) self . _handler . stream_start ( self . _root ) if self . _level < 2 : self . _builder = ElementTree . TreeBuilder ( ) self . _level += 1 return self . _builder . start ( tag , attrs )
11090	def select ( self , filters = all_true , recursive = True ) : self . assert_is_dir_and_exists ( ) if recursive : for p in self . glob ( "**/*" ) : if filters ( p ) : yield p else : for p in self . iterdir ( ) : if filters ( p ) : yield p
12371	def logon ( self , username , password ) : if self . _token : self . logoff ( ) try : response = self . __makerequest ( 'logon' , email = username , password = password ) except FogBugzAPIError : e = sys . exc_info ( ) [ 1 ] raise FogBugzLogonError ( e ) self . _token = response . token . string if type ( self . _token ) == CData : self . _token = self . _token . encode ( 'utf-8' )
10662	def element_mass_fraction ( compound , element ) : coeff = stoichiometry_coefficient ( compound , element ) if coeff == 0.0 : return 0.0 formula_mass = molar_mass ( compound ) element_mass = molar_mass ( element ) return coeff * element_mass / formula_mass
3965	def restart_apps_or_services ( app_or_service_names = None ) : if app_or_service_names : log_to_client ( "Restarting the following apps or services: {}" . format ( ', ' . join ( app_or_service_names ) ) ) else : log_to_client ( "Restarting all active containers associated with Dusty" ) if app_or_service_names : specs = spec_assembler . get_assembled_specs ( ) specs_list = [ specs [ 'apps' ] [ app_name ] for app_name in app_or_service_names if app_name in specs [ 'apps' ] ] repos = set ( ) for spec in specs_list : if spec [ 'repo' ] : repos = repos . union ( spec_assembler . get_same_container_repos_from_spec ( spec ) ) nfs . update_nfs_with_repos ( repos ) else : nfs . update_nfs_with_repos ( spec_assembler . get_all_repos ( active_only = True , include_specs_repo = False ) ) compose . restart_running_services ( app_or_service_names )
13904	def parse_hub_key ( key ) : if key is None : raise ValueError ( 'Not a valid key' ) match = re . match ( PATTERN , key ) if not match : match = re . match ( PATTERN_S0 , key ) if not match : raise ValueError ( 'Not a valid key' ) return dict ( map ( normalise_part , zip ( [ p for p in PARTS_S0 . keys ( ) ] , match . groups ( ) ) ) ) return dict ( zip ( PARTS . keys ( ) , match . groups ( ) ) )
13016	def hook ( name ) : def hookTarget ( wrapped ) : if not hasattr ( wrapped , '__hook__' ) : wrapped . __hook__ = [ name ] else : wrapped . __hook__ . append ( name ) return wrapped return hookTarget
13460	def event_all_comments_list ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) comments = event . all_comments page = int ( request . GET . get ( 'page' , 99999 ) ) is_paginated = False if comments : paginator = Paginator ( comments , 50 ) try : comments = paginator . page ( page ) except EmptyPage : comments = paginator . page ( paginator . num_pages ) is_paginated = comments . has_other_pages ( ) return render ( request , 'happenings/event_comments.html' , { "event" : event , "comment_list" : comments , "object_list" : comments , "page_obj" : comments , "page" : page , "is_paginated" : is_paginated , "key" : key } )
12085	def folderScan ( self , abfFolder = None ) : if abfFolder is None and 'abfFolder' in dir ( self ) : abfFolder = self . abfFolder else : self . abfFolder = abfFolder self . abfFolder = os . path . abspath ( self . abfFolder ) self . log . info ( "scanning [%s]" , self . abfFolder ) if not os . path . exists ( self . abfFolder ) : self . log . error ( "path doesn't exist: [%s]" , abfFolder ) return self . abfFolder2 = os . path . abspath ( self . abfFolder + "/swhlab/" ) if not os . path . exists ( self . abfFolder2 ) : self . log . error ( "./swhlab/ doesn't exist. creating it..." ) os . mkdir ( self . abfFolder2 ) self . fnames = os . listdir ( self . abfFolder ) self . fnames2 = os . listdir ( self . abfFolder2 ) self . log . debug ( "./ has %d files" , len ( self . fnames ) ) self . log . debug ( "./swhlab/ has %d files" , len ( self . fnames2 ) ) self . fnamesByExt = filesByExtension ( self . fnames ) if not "abf" in self . fnamesByExt . keys ( ) : self . log . error ( "no ABF files found" ) self . log . debug ( "found %d ABFs" , len ( self . fnamesByExt [ "abf" ] ) ) self . cells = findCells ( self . fnames ) self . log . debug ( "found %d cells" % len ( self . cells ) ) self . fnamesByCell = filesByCell ( self . fnames , self . cells ) self . log . debug ( "grouped cells by number of source files: %s" % str ( [ len ( self . fnamesByCell [ elem ] ) for elem in self . fnamesByCell ] ) )
5842	def get_design_run_status ( self , data_view_id , run_uuid ) : url = routes . get_data_view_design_status ( data_view_id , run_uuid ) response = self . _get ( url ) . json ( ) status = response [ "data" ] return ProcessStatus ( result = status . get ( "result" ) , progress = status . get ( "progress" ) , status = status . get ( "status" ) , messages = status . get ( "messages" ) )
9690	def start ( self ) : self . receiver = self . Receiver ( self . read , self . write , self . send_lock , self . senders , self . frames_received , callback = self . receive_callback , fcs_nack = self . fcs_nack , ) self . receiver . start ( )
6085	def unmasked_blurred_image_of_planes_from_padded_grid_stack_and_psf ( planes , padded_grid_stack , psf ) : unmasked_blurred_image_of_planes = [ ] for plane in planes : if plane . has_pixelization : unmasked_blurred_image_of_plane = None else : unmasked_blurred_image_of_plane = padded_grid_stack . unmasked_blurred_image_from_psf_and_unmasked_image ( psf = psf , unmasked_image_1d = plane . image_plane_image_1d ) unmasked_blurred_image_of_planes . append ( unmasked_blurred_image_of_plane ) return unmasked_blurred_image_of_planes
6799	def database_renderer ( self , name = None , site = None , role = None ) : name = name or self . env . default_db_name site = site or self . genv . SITE role = role or self . genv . ROLE key = ( name , site , role ) self . vprint ( 'checking key:' , key ) if key not in self . _database_renderers : self . vprint ( 'No cached db renderer, generating...' ) if self . verbose : print ( 'db.name:' , name ) print ( 'db.databases:' , self . env . databases ) print ( 'db.databases[%s]:' % name , self . env . databases . get ( name ) ) d = type ( self . genv ) ( self . lenv ) d . update ( self . get_database_defaults ( ) ) d . update ( self . env . databases . get ( name , { } ) ) d [ 'db_name' ] = name if self . verbose : print ( 'db.d:' ) pprint ( d , indent = 4 ) print ( 'db.connection_handler:' , d . connection_handler ) if d . connection_handler == CONNECTION_HANDLER_DJANGO : self . vprint ( 'Using django handler...' ) dj = self . get_satchel ( 'dj' ) if self . verbose : print ( 'Loading Django DB settings for site {} and role {}.' . format ( site , role ) , file = sys . stderr ) dj . set_db ( name = name , site = site , role = role ) _d = dj . local_renderer . collect_genv ( include_local = True , include_global = False ) for k , v in _d . items ( ) : if k . startswith ( 'dj_db_' ) : _d [ k [ 3 : ] ] = v del _d [ k ] if self . verbose : print ( 'Loaded:' ) pprint ( _d ) d . update ( _d ) elif d . connection_handler and d . connection_handler . startswith ( CONNECTION_HANDLER_CUSTOM + ':' ) : _callable_str = d . connection_handler [ len ( CONNECTION_HANDLER_CUSTOM + ':' ) : ] self . vprint ( 'Using custom handler %s...' % _callable_str ) _d = str_to_callable ( _callable_str ) ( role = self . genv . ROLE ) if self . verbose : print ( 'Loaded:' ) pprint ( _d ) d . update ( _d ) r = LocalRenderer ( self , lenv = d ) self . set_root_login ( r ) self . _database_renderers [ key ] = r else : self . vprint ( 'Cached db renderer found.' ) return self . _database_renderers [ key ]
2851	def _mpsse_sync ( self , max_retries = 10 ) : self . _write ( '\xAB' ) tries = 0 sync = False while not sync : data = self . _poll_read ( 2 ) if data == '\xFA\xAB' : sync = True tries += 1 if tries >= max_retries : raise RuntimeError ( 'Could not synchronize with FT232H!' )
1223	def from_spec ( spec , kwargs = None ) : if isinstance ( spec , dict ) : spec = [ spec ] stack = PreprocessorStack ( ) for preprocessor_spec in spec : preprocessor_kwargs = copy . deepcopy ( kwargs ) preprocessor = util . get_object ( obj = preprocessor_spec , predefined_objects = tensorforce . core . preprocessors . preprocessors , kwargs = preprocessor_kwargs ) assert isinstance ( preprocessor , Preprocessor ) stack . preprocessors . append ( preprocessor ) return stack
10476	def _queueMouseButton ( self , coord , mouseButton , modFlags , clickCount = 1 , dest_coord = None ) : mouseButtons = { Quartz . kCGMouseButtonLeft : 'LeftMouse' , Quartz . kCGMouseButtonRight : 'RightMouse' , } if mouseButton not in mouseButtons : raise ValueError ( 'Mouse button given not recognized' ) eventButtonDown = getattr ( Quartz , 'kCGEvent%sDown' % mouseButtons [ mouseButton ] ) eventButtonUp = getattr ( Quartz , 'kCGEvent%sUp' % mouseButtons [ mouseButton ] ) eventButtonDragged = getattr ( Quartz , 'kCGEvent%sDragged' % mouseButtons [ mouseButton ] ) buttonDown = Quartz . CGEventCreateMouseEvent ( None , eventButtonDown , coord , mouseButton ) Quartz . CGEventSetFlags ( buttonDown , modFlags ) Quartz . CGEventSetIntegerValueField ( buttonDown , Quartz . kCGMouseEventClickState , int ( clickCount ) ) if dest_coord : buttonDragged = Quartz . CGEventCreateMouseEvent ( None , eventButtonDragged , dest_coord , mouseButton ) Quartz . CGEventSetFlags ( buttonDragged , modFlags ) buttonUp = Quartz . CGEventCreateMouseEvent ( None , eventButtonUp , dest_coord , mouseButton ) else : buttonUp = Quartz . CGEventCreateMouseEvent ( None , eventButtonUp , coord , mouseButton ) Quartz . CGEventSetFlags ( buttonUp , modFlags ) Quartz . CGEventSetIntegerValueField ( buttonUp , Quartz . kCGMouseEventClickState , int ( clickCount ) ) self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGSessionEventTap , buttonDown ) ) if dest_coord : self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGHIDEventTap , buttonDragged ) ) self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGSessionEventTap , buttonUp ) )
291	def plot_rolling_volatility ( returns , factor_returns = None , rolling_window = APPROX_BDAYS_PER_MONTH * 6 , legend_loc = 'best' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) rolling_vol_ts = timeseries . rolling_volatility ( returns , rolling_window ) rolling_vol_ts . plot ( alpha = .7 , lw = 3 , color = 'orangered' , ax = ax , ** kwargs ) if factor_returns is not None : rolling_vol_ts_factor = timeseries . rolling_volatility ( factor_returns , rolling_window ) rolling_vol_ts_factor . plot ( alpha = .7 , lw = 3 , color = 'grey' , ax = ax , ** kwargs ) ax . set_title ( 'Rolling volatility (6-month)' ) ax . axhline ( rolling_vol_ts . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 ) ax . axhline ( 0.0 , color = 'black' , linestyle = '-' , lw = 2 ) ax . set_ylabel ( 'Volatility' ) ax . set_xlabel ( '' ) if factor_returns is None : ax . legend ( [ 'Volatility' , 'Average volatility' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) else : ax . legend ( [ 'Volatility' , 'Benchmark volatility' , 'Average volatility' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) return ax
3477	def _dissociate_gene ( self , cobra_gene ) : self . _genes . discard ( cobra_gene ) cobra_gene . _reaction . discard ( self )
12703	def _set_params ( target , param , values , dof ) : if not isinstance ( values , ( list , tuple , np . ndarray ) ) : values = [ values ] * dof assert dof == len ( values ) for s , value in zip ( [ '' , '2' , '3' ] [ : dof ] , values ) : target . setParam ( getattr ( ode , 'Param{}{}' . format ( param , s ) ) , value )
3218	def get_route_tables ( vpc , ** conn ) : route_tables = describe_route_tables ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , ** conn ) rt_ids = [ ] for r in route_tables : rt_ids . append ( r [ "RouteTableId" ] ) return rt_ids
9914	def create ( self , validated_data ) : email_query = models . EmailAddress . objects . filter ( email = self . validated_data [ "email" ] ) if email_query . exists ( ) : email = email_query . get ( ) email . send_duplicate_notification ( ) else : email = super ( EmailSerializer , self ) . create ( validated_data ) email . send_confirmation ( ) user = validated_data . get ( "user" ) query = models . EmailAddress . objects . filter ( is_primary = True , user = user ) if not query . exists ( ) : email . set_primary ( ) return email
10164	def get_personalities ( self , line ) : return [ split ( '\W+' , i ) [ 1 ] for i in line . split ( ':' ) [ 1 ] . split ( ' ' ) if i . startswith ( '[' ) ]
8971	def next ( self , data ) : self . __length += 1 result = self . __kdf . calculate ( self . __key , data , 64 ) self . __key = result [ : 32 ] return result [ 32 : ]
11970	def _wildcard_to_dec ( nm , check = False ) : if check and not is_wildcard_nm ( nm ) : raise ValueError ( '_wildcard_to_dec: invalid netmask: "%s"' % nm ) return 0xFFFFFFFF - _dot_to_dec ( nm , check = False )
2317	def _run_pc ( self , data , fixedEdges = None , fixedGaps = None , verbose = True ) : if ( self . arguments [ '{CITEST}' ] == self . dir_CI_test [ 'hsic' ] and self . arguments [ '{METHOD_INDEP}' ] == self . dir_method_indep [ 'corr' ] ) : warnings . warn ( 'Selected method for indep is unfit for the hsic test,' ' setting the hsic.gamma method.' ) self . arguments [ '{METHOD_INDEP}' ] = self . dir_method_indep [ 'hsic_gamma' ] elif ( self . arguments [ '{CITEST}' ] == self . dir_CI_test [ 'gaussian' ] and self . arguments [ '{METHOD_INDEP}' ] != self . dir_method_indep [ 'corr' ] ) : warnings . warn ( 'Selected method for indep is unfit for the selected test,' ' setting the classic correlation-based method.' ) self . arguments [ '{METHOD_INDEP}' ] = self . dir_method_indep [ 'corr' ] id = str ( uuid . uuid4 ( ) ) os . makedirs ( '/tmp/cdt_pc' + id + '/' ) self . arguments [ '{FOLDER}' ] = '/tmp/cdt_pc' + id + '/' def retrieve_result ( ) : return read_csv ( '/tmp/cdt_pc' + id + '/result.csv' , delimiter = ',' ) . values try : data . to_csv ( '/tmp/cdt_pc' + id + '/data.csv' , header = False , index = False ) if fixedGaps is not None and fixedEdges is not None : fixedGaps . to_csv ( '/tmp/cdt_pc' + id + '/fixedgaps.csv' , index = False , header = False ) fixedEdges . to_csv ( '/tmp/cdt_pc' + id + '/fixededges.csv' , index = False , header = False ) self . arguments [ '{SKELETON}' ] = 'TRUE' else : self . arguments [ '{SKELETON}' ] = 'FALSE' pc_result = launch_R_script ( "{}/R_templates/pc.R" . format ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) , self . arguments , output_function = retrieve_result , verbose = verbose ) except Exception as e : rmtree ( '/tmp/cdt_pc' + id + '' ) raise e except KeyboardInterrupt : rmtree ( '/tmp/cdt_pc' + id + '/' ) raise KeyboardInterrupt rmtree ( '/tmp/cdt_pc' + id + '' ) return pc_result
8222	def do_toggle_variables ( self , action ) : self . show_vars = action . get_active ( ) if self . show_vars : self . show_variables_window ( ) else : self . hide_variables_window ( )
2170	def command ( method = None , ** kwargs ) : def actual_decorator ( method ) : method . _cli_command = True method . _cli_command_attrs = kwargs return method if method and isinstance ( method , types . FunctionType ) : return actual_decorator ( method ) else : return actual_decorator
11047	def _parse_field_value ( line ) : if line . startswith ( ':' ) : return None , None if ':' not in line : return line , '' field , value = line . split ( ':' , 1 ) value = value [ 1 : ] if value . startswith ( ' ' ) else value return field , value
4173	def window_visu ( N = 51 , name = 'hamming' , ** kargs ) : mindB = kargs . pop ( 'mindB' , - 100 ) maxdB = kargs . pop ( 'maxdB' , None ) norm = kargs . pop ( 'norm' , True ) w = Window ( N , name , ** kargs ) w . plot_time_freq ( mindB = mindB , maxdB = maxdB , norm = norm )
10670	def _split_compound_string_ ( compound_string ) : formula = compound_string . replace ( ']' , '' ) . split ( '[' ) [ 0 ] phase = compound_string . replace ( ']' , '' ) . split ( '[' ) [ 1 ] return formula , phase
12176	def plot_shaded_data ( X , Y , variances , varianceX ) : plt . plot ( X , Y , color = 'k' , lw = 2 ) nChunks = int ( len ( Y ) / CHUNK_POINTS ) for i in range ( 0 , 100 , PERCENT_STEP ) : varLimitLow = np . percentile ( variances , i ) varLimitHigh = np . percentile ( variances , i + PERCENT_STEP ) varianceIsAboveMin = np . where ( variances >= varLimitLow ) [ 0 ] varianceIsBelowMax = np . where ( variances <= varLimitHigh ) [ 0 ] varianceIsRange = [ chunkNumber for chunkNumber in range ( nChunks ) if chunkNumber in varianceIsAboveMin and chunkNumber in varianceIsBelowMax ] for chunkNumber in varianceIsRange : t1 = chunkNumber * CHUNK_POINTS / POINTS_PER_SEC t2 = t1 + CHUNK_POINTS / POINTS_PER_SEC plt . axvspan ( t1 , t2 , alpha = .3 , color = COLORMAP ( i / 100 ) , lw = 0 )
811	def generateStats ( filename , statsInfo , maxSamples = None , filters = [ ] , cache = True ) : if not isinstance ( statsInfo , dict ) : raise RuntimeError ( "statsInfo must be a dict -- " "found '%s' instead" % type ( statsInfo ) ) filename = resource_filename ( "nupic.datafiles" , filename ) if cache : statsFilename = getStatsFilename ( filename , statsInfo , filters ) if os . path . exists ( statsFilename ) : try : r = pickle . load ( open ( statsFilename , "rb" ) ) except : print "Warning: unable to load stats for %s -- " "will regenerate" % filename r = dict ( ) requestedKeys = set ( [ s for s in statsInfo ] ) availableKeys = set ( r . keys ( ) ) unavailableKeys = requestedKeys . difference ( availableKeys ) if len ( unavailableKeys ) == 0 : return r else : print "generateStats: re-generating stats file %s because " "keys %s are not available" % ( filename , str ( unavailableKeys ) ) os . remove ( filename ) print "Generating statistics for file '%s' with filters '%s'" % ( filename , filters ) sensor = RecordSensor ( ) sensor . dataSource = FileRecordStream ( filename ) sensor . preEncodingFilters = filters stats = [ ] for field in statsInfo : if statsInfo [ field ] == "number" : statsInfo [ field ] = NumberStatsCollector ( ) elif statsInfo [ field ] == "category" : statsInfo [ field ] = CategoryStatsCollector ( ) else : raise RuntimeError ( "Unknown stats type '%s' for field '%s'" % ( statsInfo [ field ] , field ) ) if maxSamples is None : maxSamples = 500000 for i in xrange ( maxSamples ) : try : record = sensor . getNextRecord ( ) except StopIteration : break for ( name , collector ) in statsInfo . items ( ) : collector . add ( record [ name ] ) del sensor r = dict ( ) for ( field , collector ) in statsInfo . items ( ) : stats = collector . getStats ( ) if field not in r : r [ field ] = stats else : r [ field ] . update ( stats ) if cache : f = open ( statsFilename , "wb" ) pickle . dump ( r , f ) f . close ( ) r [ "_filename" ] = statsFilename return r
9828	def write ( self , file , optstring = "" , quote = False ) : classid = str ( self . id ) if quote : classid = '"' + classid + '"' file . write ( 'object ' + classid + ' class ' + str ( self . name ) + ' ' + optstring + '\n' )
7668	def trim ( self , start_time , end_time , strict = False ) : trimmed_array = AnnotationArray ( ) for ann in self : trimmed_array . append ( ann . trim ( start_time , end_time , strict = strict ) ) return trimmed_array
5923	def get_configuration ( filename = CONFIGNAME ) : global cfg , configuration cfg = GMXConfigParser ( filename = filename ) globals ( ) . update ( cfg . configuration ) configuration = cfg . configuration return cfg
9793	def _matches_patterns ( path , patterns ) : for glob in patterns : try : if PurePath ( path ) . match ( glob ) : return True except TypeError : pass return False
11944	def add ( self , level , message , extra_tags = '' ) : if not message : return level = int ( level ) if level < self . level : return if level not in stored_messages_settings . STORE_LEVELS or self . user . is_anonymous ( ) : return super ( StorageMixin , self ) . add ( level , message , extra_tags ) self . added_new = True m = self . backend . create_message ( level , message , extra_tags ) self . backend . archive_store ( [ self . user ] , m ) self . _queued_messages . append ( m )
6091	def cache ( func ) : def wrapper ( instance : GeometryProfile , grid : np . ndarray , * args , ** kwargs ) : if not hasattr ( instance , "cache" ) : instance . cache = { } key = ( func . __name__ , grid . tobytes ( ) ) if key not in instance . cache : instance . cache [ key ] = func ( instance , grid ) return instance . cache [ key ] return wrapper
4725	def get_chunk_meta ( self , meta_file ) : chunks = self . envs [ "CHUNKS" ] if cij . nvme . get_meta ( 0 , chunks * self . envs [ "CHUNK_META_SIZEOF" ] , meta_file ) : raise RuntimeError ( "cij.liblight.get_chunk_meta: fail" ) chunk_meta = cij . bin . Buffer ( types = self . envs [ "CHUNK_META_STRUCT" ] , length = chunks ) chunk_meta . read ( meta_file ) return chunk_meta
13725	def register_credentials ( self , credentials = None , user = None , user_file = None , password = None , password_file = None ) : if credentials is not None : self . credentials = credentials else : self . credentials = { } if user : self . credentials [ "user" ] = user elif user_file : with open ( user_file , "r" ) as of : pattern = re . compile ( "^user: " ) for l in of : if re . match ( pattern , l ) : l = l [ 0 : - 1 ] self . credentials [ "user" ] = re . sub ( pattern , "" , l ) if self . credentials [ "user" ] [ 0 : 1 ] == '"' and self . credentials [ "user" ] [ - 1 : ] == '"' : self . credentials [ "user" ] = self . credentials [ "user" ] [ 1 : - 1 ] if password : self . credentials [ "password" ] = password elif password_file : with open ( password_file , "r" ) as of : pattern = re . compile ( "^password: " ) for l in of : if re . match ( pattern , l ) : l = l [ 0 : - 1 ] self . credentials [ "password" ] = re . sub ( pattern , "" , l ) if self . credentials [ "password" ] [ 0 : 1 ] == '"' and self . credentials [ "password" ] [ - 1 : ] == '"' : self . credentials [ "password" ] = self . credentials [ "password" ] [ 1 : - 1 ] if "user" in self . credentials and "password" in self . credentials : c = self . credentials [ "user" ] + ":" + self . credentials [ "password" ] self . credentials [ "base64" ] = b64encode ( c . encode ( ) ) . decode ( "ascii" )
6687	def install ( packages , repos = None , yes = None , options = None ) : manager = MANAGER if options is None : options = [ ] elif isinstance ( options , six . string_types ) : options = [ options ] if not isinstance ( packages , six . string_types ) : packages = " " . join ( packages ) if repos : for repo in repos : options . append ( '--enablerepo=%(repo)s' % locals ( ) ) options = " " . join ( options ) if isinstance ( yes , str ) : run_as_root ( 'yes %(yes)s | %(manager)s %(options)s install %(packages)s' % locals ( ) ) else : run_as_root ( '%(manager)s %(options)s install %(packages)s' % locals ( ) )
12753	def indices_for_body ( self , name , step = 3 ) : for j , body in enumerate ( self . bodies ) : if body . name == name : return list ( range ( j * step , ( j + 1 ) * step ) ) return [ ]
6027	def voronoi_from_pixel_centers ( pixel_centers ) : return scipy . spatial . Voronoi ( np . asarray ( [ pixel_centers [ : , 1 ] , pixel_centers [ : , 0 ] ] ) . T , qhull_options = 'Qbb Qc Qx Qm' )
3836	async def set_conversation_notification_level ( self , set_conversation_notification_level_request ) : response = hangouts_pb2 . SetConversationNotificationLevelResponse ( ) await self . _pb_request ( 'conversations/setconversationnotificationlevel' , set_conversation_notification_level_request , response ) return response
4673	def addPrivateKey ( self , wif ) : try : pub = self . publickey_from_wif ( wif ) except Exception : raise InvalidWifError ( "Invalid Key format!" ) if str ( pub ) in self . store : raise KeyAlreadyInStoreException ( "Key already in the store" ) self . store . add ( str ( wif ) , str ( pub ) )
143	def exterior_almost_equals ( self , other , max_distance = 1e-6 , points_per_edge = 8 ) : if isinstance ( other , list ) : other = Polygon ( np . float32 ( other ) ) elif ia . is_np_array ( other ) : other = Polygon ( other ) else : assert isinstance ( other , Polygon ) other = other return self . to_line_string ( closed = True ) . coords_almost_equals ( other . to_line_string ( closed = True ) , max_distance = max_distance , points_per_edge = points_per_edge )
5124	def show_type ( self , edge_type , ** kwargs ) : for v in self . g . nodes ( ) : e = ( v , v ) if self . g . is_edge ( e ) and self . g . ep ( e , 'edge_type' ) == edge_type : ei = self . g . edge_index [ e ] self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_highlight' ] ) self . g . set_vp ( v , 'vertex_color' , self . edge2queue [ ei ] . colors [ 'vertex_color' ] ) else : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_inactive' ] ) self . g . set_vp ( v , 'vertex_color' , [ 0 , 0 , 0 , 0.9 ] ) for e in self . g . edges ( ) : if self . g . ep ( e , 'edge_type' ) == edge_type : self . g . set_ep ( e , 'edge_color' , self . colors [ 'edge_active' ] ) else : self . g . set_ep ( e , 'edge_color' , self . colors [ 'edge_inactive' ] ) self . draw ( update_colors = False , ** kwargs ) self . _update_all_colors ( )
13579	def determine_type ( x ) : types = ( int , float , str ) _type = filter ( lambda a : is_type ( a , x ) , types ) [ 0 ] return _type ( x )
4358	def send_packet ( self , pkt ) : self . put_client_msg ( packet . encode ( pkt , self . json_dumps ) )
1973	def sys_openat ( self , dirfd , buf , flags , mode ) : if issymbolic ( dirfd ) : logger . debug ( "Ask to read from a symbolic directory file descriptor!!" ) self . constraints . add ( dirfd >= 0 ) self . constraints . add ( dirfd <= len ( self . files ) ) raise ConcretizeArgument ( self , 0 ) if issymbolic ( buf ) : logger . debug ( "Ask to read to a symbolic buffer" ) raise ConcretizeArgument ( self , 1 ) return super ( ) . sys_openat ( dirfd , buf , flags , mode )
8916	def _retrieve_certificate ( self , access_token , timeout = 3 ) : logger . debug ( "Retrieve certificate with token." ) key_pair = crypto . PKey ( ) key_pair . generate_key ( crypto . TYPE_RSA , 2048 ) private_key = crypto . dump_privatekey ( crypto . FILETYPE_PEM , key_pair ) . decode ( "utf-8" ) cert_request = crypto . X509Req ( ) cert_request . set_pubkey ( key_pair ) cert_request . sign ( key_pair , 'md5' ) der_cert_req = crypto . dump_certificate_request ( crypto . FILETYPE_ASN1 , cert_request ) encoded_cert_req = base64 . b64encode ( der_cert_req ) token = { 'access_token' : access_token , 'token_type' : 'Bearer' } client = OAuth2Session ( token = token ) response = client . post ( self . certificate_url , data = { 'certificate_request' : encoded_cert_req } , verify = False , timeout = timeout , ) if response . ok : content = "{} {}" . format ( response . text , private_key ) with open ( self . esgf_credentials , 'w' ) as fh : fh . write ( content ) logger . debug ( 'Fetched certificate successfully.' ) else : msg = "Could not get certificate: {} {}" . format ( response . status_code , response . reason ) raise Exception ( msg ) return True
13420	def validate ( cls , definition ) : schema_path = os . path . join ( os . path . dirname ( __file__ ) , '../../schema/mapper_definition_schema.json' ) with open ( schema_path , 'r' ) as jsonfp : schema = json . load ( jsonfp ) jsonschema . validate ( definition , schema ) assert definition [ 'main_key' ] in definition [ 'supported_keys' ] , '\'main_key\' must be contained in \'supported_keys\'' assert set ( definition . get ( 'list_valued_keys' , [ ] ) ) <= set ( definition [ 'supported_keys' ] ) , '\'list_valued_keys\' must be a subset of \'supported_keys\'' assert set ( definition . get ( 'disjoint' , [ ] ) ) <= set ( definition . get ( 'list_valued_keys' , [ ] ) ) , '\'disjoint\' must be a subset of \'list_valued_keys\'' assert set ( definition . get ( 'key_synonyms' , { } ) . values ( ) ) <= set ( definition [ 'supported_keys' ] ) , '\'The values of the \'key_synonyms\' mapping must be in \'supported_keys\''
580	def getSpec ( cls ) : spec = { "description" : IdentityRegion . __doc__ , "singleNodeOnly" : True , "inputs" : { "in" : { "description" : "The input vector." , "dataType" : "Real32" , "count" : 0 , "required" : True , "regionLevel" : False , "isDefaultInput" : True , "requireSplitterMap" : False } , } , "outputs" : { "out" : { "description" : "A copy of the input vector." , "dataType" : "Real32" , "count" : 0 , "regionLevel" : True , "isDefaultOutput" : True } , } , "parameters" : { "dataWidth" : { "description" : "Size of inputs" , "accessMode" : "Read" , "dataType" : "UInt32" , "count" : 1 , "constraints" : "" } , } , } return spec
3580	def disconnect_devices ( self , service_uuids = [ ] ) : service_uuids = set ( service_uuids ) for device in self . list_devices ( ) : if not device . is_connected : continue device_uuids = set ( map ( lambda x : x . uuid , device . list_services ( ) ) ) if device_uuids >= service_uuids : device . disconnect ( )
8748	def update_scalingip ( context , id , content ) : LOG . info ( 'update_scalingip %s for tenant %s and body %s' % ( id , context . tenant_id , content ) ) requested_ports = content . get ( 'ports' , [ ] ) flip = _update_flip ( context , id , ip_types . SCALING , requested_ports ) return v . _make_scaling_ip_dict ( flip )
9356	def words ( quantity = 10 , as_list = False ) : global _words if not _words : _words = ' ' . join ( get_dictionary ( 'lorem_ipsum' ) ) . lower ( ) . replace ( '\n' , '' ) _words = re . sub ( r'\.|,|;/' , '' , _words ) _words = _words . split ( ' ' ) result = random . sample ( _words , quantity ) if as_list : return result else : return ' ' . join ( result )
9724	async def load_project ( self , project_path ) : cmd = "loadproject %s" % project_path return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
6052	def sparse_to_unmasked_sparse_from_mask_and_pixel_centres ( total_sparse_pixels , mask , unmasked_sparse_grid_pixel_centres ) : pix_to_full_pix = np . zeros ( total_sparse_pixels ) pixel_index = 0 for full_pixel_index in range ( unmasked_sparse_grid_pixel_centres . shape [ 0 ] ) : y = unmasked_sparse_grid_pixel_centres [ full_pixel_index , 0 ] x = unmasked_sparse_grid_pixel_centres [ full_pixel_index , 1 ] if not mask [ y , x ] : pix_to_full_pix [ pixel_index ] = full_pixel_index pixel_index += 1 return pix_to_full_pix
8801	def run_migrations_online ( ) : engine = create_engine ( neutron_config . database . connection , poolclass = pool . NullPool ) connection = engine . connect ( ) context . configure ( connection = connection , target_metadata = target_metadata ) try : with context . begin_transaction ( ) : context . run_migrations ( ) finally : connection . close ( )
3013	def locked_delete ( self ) : filters = { self . key_name : self . key_value } self . session . query ( self . model_class ) . filter_by ( ** filters ) . delete ( )
5279	def make_pmml_pipeline ( obj , active_fields = None , target_fields = None ) : steps = _filter_steps ( _get_steps ( obj ) ) pipeline = PMMLPipeline ( steps ) if active_fields is not None : pipeline . active_fields = numpy . asarray ( active_fields ) if target_fields is not None : pipeline . target_fields = numpy . asarray ( target_fields ) return pipeline
4799	def is_file ( self ) : self . exists ( ) if not os . path . isfile ( self . val ) : self . _err ( 'Expected <%s> to be a file, but was not.' % self . val ) return self
5945	def convert_aa_code ( x ) : if len ( x ) == 1 : return amino_acid_codes [ x . upper ( ) ] elif len ( x ) == 3 : return inverse_aa_codes [ x . upper ( ) ] else : raise ValueError ( "Can only convert 1-letter or 3-letter amino acid codes, " "not %r" % x )
4471	def _transform ( self , jam , state ) : if not hasattr ( jam . sandbox , 'muda' ) : raise RuntimeError ( 'No muda state found in jams sandbox.' ) jam_w = copy . deepcopy ( jam ) jam_w . sandbox . muda [ 'history' ] . append ( { 'transformer' : self . __serialize__ , 'state' : state } ) if hasattr ( self , 'audio' ) : self . audio ( jam_w . sandbox . muda , state ) if hasattr ( self , 'metadata' ) : self . metadata ( jam_w . file_metadata , state ) for query , function_name in six . iteritems ( self . dispatch ) : function = getattr ( self , function_name ) for matched_annotation in jam_w . search ( namespace = query ) : function ( matched_annotation , state ) return jam_w
11667	def linear ( Ks , dim , num_q , rhos , nus ) : r return _get_linear ( Ks , dim ) ( num_q , rhos , nus )
11384	def module ( self ) : if not hasattr ( self , '_module' ) : if "__main__" in sys . modules : mod = sys . modules [ "__main__" ] path = self . normalize_path ( mod . __file__ ) if os . path . splitext ( path ) == os . path . splitext ( self . path ) : self . _module = mod else : self . _module = imp . load_source ( 'captain_script' , self . path ) return self . _module
10529	def get_projects ( limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : print ( OFFSET_WARNING ) params = dict ( limit = limit , offset = offset ) try : res = _pybossa_req ( 'get' , 'project' , params = params ) if type ( res ) . __name__ == 'list' : return [ Project ( project ) for project in res ] else : raise TypeError except : raise
2888	def disconnect ( self , callback ) : if self . weak_subscribers is not None : with self . lock : index = self . _weakly_connected_index ( callback ) if index is not None : self . weak_subscribers . pop ( index ) [ 0 ] if self . hard_subscribers is not None : try : index = self . _hard_callbacks ( ) . index ( callback ) except ValueError : pass else : self . hard_subscribers . pop ( index )
7485	def concat_reads ( data , subsamples , ipyclient ) : if any ( [ len ( i . files . fastqs ) > 1 for i in subsamples ] ) : start = time . time ( ) printstr = " concatenating inputs | {} | s2 |" finished = 0 catjobs = { } for sample in subsamples : if len ( sample . files . fastqs ) > 1 : catjobs [ sample . name ] = ipyclient [ 0 ] . apply ( concat_multiple_inputs , * ( data , sample ) ) else : sample . files . concat = sample . files . fastqs while 1 : finished = sum ( [ i . ready ( ) for i in catjobs . values ( ) ] ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( catjobs ) , finished , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if finished == len ( catjobs ) : print ( "" ) break for async in catjobs : if catjobs [ async ] . successful ( ) : data . samples [ async ] . files . concat = catjobs [ async ] . result ( ) else : error = catjobs [ async ] . result ( ) LOGGER . error ( "error in step2 concat %s" , error ) raise IPyradWarningExit ( "error in step2 concat: {}" . format ( error ) ) else : for sample in subsamples : sample . files . concat = sample . files . fastqs return subsamples
2772	def load ( self ) : data = self . get_data ( 'load_balancers/%s' % self . id , type = GET ) load_balancer = data [ 'load_balancer' ] for attr in load_balancer . keys ( ) : if attr == 'health_check' : health_check = HealthCheck ( ** load_balancer [ 'health_check' ] ) setattr ( self , attr , health_check ) elif attr == 'sticky_sessions' : sticky_ses = StickySesions ( ** load_balancer [ 'sticky_sessions' ] ) setattr ( self , attr , sticky_ses ) elif attr == 'forwarding_rules' : rules = list ( ) for rule in load_balancer [ 'forwarding_rules' ] : rules . append ( ForwardingRule ( ** rule ) ) setattr ( self , attr , rules ) else : setattr ( self , attr , load_balancer [ attr ] ) return self
52	def on ( self , image ) : shape = normalize_shape ( image ) if shape [ 0 : 2 ] == self . shape [ 0 : 2 ] : return self . deepcopy ( ) else : keypoints = [ kp . project ( self . shape , shape ) for kp in self . keypoints ] return self . deepcopy ( keypoints , shape )
8905	def create_access_token ( self , valid_in_hours = 1 , data = None ) : data = data or { } token = AccessToken ( token = self . generate ( ) , expires_at = expires_at ( hours = valid_in_hours ) , data = data ) return token
11397	def add_cms_link ( self ) : intnote = record_get_field_values ( self . record , '690' , filter_subfield_code = "a" , filter_subfield_value = 'INTNOTE' ) if intnote : val_088 = record_get_field_values ( self . record , tag = '088' , filter_subfield_code = "a" ) for val in val_088 : if 'CMS' in val : url = ( 'http://weblib.cern.ch/abstract?CERN-CMS' + val . split ( 'CMS' , 1 ) [ - 1 ] ) record_add_field ( self . record , tag = '856' , ind1 = '4' , subfields = [ ( 'u' , url ) ] )
6694	def static ( self ) : fn = self . render_to_file ( 'ip/ip_interfaces_static.template' ) r = self . local_renderer r . put ( local_path = fn , remote_path = r . env . interfaces_fn , use_sudo = True )
3916	def _get_date_str ( timestamp , datetimefmt , show_date = False ) : fmt = '' if show_date : fmt += '\n' + datetimefmt . get ( 'date' , '' ) + '\n' fmt += datetimefmt . get ( 'time' , '' ) return timestamp . astimezone ( tz = None ) . strftime ( fmt )
3141	def get ( self , workflow_id , email_id ) : self . workflow_id = workflow_id self . email_id = email_id return self . _mc_client . _get ( url = self . _build_path ( workflow_id , 'emails' , email_id ) )
7270	def use ( plugin ) : log . debug ( 'register new plugin: {}' . format ( plugin ) ) if inspect . isfunction ( plugin ) : return plugin ( Engine ) if plugin and hasattr ( plugin , 'register' ) : return plugin . register ( Engine ) raise ValueError ( 'invalid plugin: must be a function or ' 'implement register() method' )
4479	def file_empty ( fp ) : if six . PY2 : contents = fp . read ( ) fp . seek ( 0 ) return not bool ( contents ) else : return not fp . peek ( )
11910	def bump_version ( version , which = None ) : try : parts = [ int ( n ) for n in version . split ( '.' ) ] except ValueError : fail ( 'Current version is not numeric' ) if len ( parts ) != 3 : fail ( 'Current version is not semantic versioning' ) PARTS = { 'major' : 0 , 'minor' : 1 , 'patch' : 2 } index = PARTS [ which ] if which in PARTS else 2 before , middle , after = parts [ : index ] , parts [ index ] , parts [ index + 1 : ] middle += 1 return '.' . join ( str ( n ) for n in before + [ middle ] + after )
2854	def mpsse_gpio ( self ) : level_low = chr ( self . _level & 0xFF ) level_high = chr ( ( self . _level >> 8 ) & 0xFF ) dir_low = chr ( self . _direction & 0xFF ) dir_high = chr ( ( self . _direction >> 8 ) & 0xFF ) return str ( bytearray ( ( 0x80 , level_low , dir_low , 0x82 , level_high , dir_high ) ) )
5797	def _get_func_info ( docstring , def_lineno , code_lines , prefix ) : def_index = def_lineno - 1 definition = code_lines [ def_index ] definition = definition . rstrip ( ) while not definition . endswith ( ':' ) : def_index += 1 definition += '\n' + code_lines [ def_index ] . rstrip ( ) definition = textwrap . dedent ( definition ) . rstrip ( ':' ) definition = definition . replace ( '\n' , '\n' + prefix ) description = '' found_colon = False params = '' for line in docstring . splitlines ( ) : if line and line [ 0 ] == ':' : found_colon = True if not found_colon : if description : description += '\n' description += line else : if params : params += '\n' params += line description = description . strip ( ) description_md = '' if description : description_md = "%s%s" % ( prefix , description . replace ( '\n' , '\n' + prefix ) ) description_md = re . sub ( '\n>(\\s+)\n' , '\n>\n' , description_md ) params = params . strip ( ) if params : definition += ( ':\n%s ' % prefix ) definition = re . sub ( '\n>(\\s+)\n' , '\n>\n' , definition ) for search , replace in definition_replacements . items ( ) : definition = definition . replace ( search , replace ) return ( definition , description_md )
12153	def convolve ( signal , kernel ) : pad = np . ones ( len ( kernel ) / 2 ) signal = np . concatenate ( ( pad * signal [ 0 ] , signal , pad * signal [ - 1 ] ) ) signal = np . convolve ( signal , kernel , mode = 'same' ) signal = signal [ len ( pad ) : - len ( pad ) ] return signal
9606	def check_unused_args ( self , used_args , args , kwargs ) : for k , v in kwargs . items ( ) : if k in used_args : self . _used_kwargs . update ( { k : v } ) else : self . _unused_kwargs . update ( { k : v } )
13328	def add ( path ) : click . echo ( '\nAdding {} to cache......' . format ( path ) , nl = False ) try : r = cpenv . resolve ( path ) except Exception as e : click . echo ( bold_red ( 'FAILED' ) ) click . echo ( e ) return if isinstance ( r . resolved [ 0 ] , cpenv . VirtualEnvironment ) : EnvironmentCache . add ( r . resolved [ 0 ] ) EnvironmentCache . save ( ) click . echo ( bold_green ( 'OK!' ) )
6707	def run_as_root ( command , * args , ** kwargs ) : from burlap . common import run_or_dryrun , sudo_or_dryrun if env . user == 'root' : func = run_or_dryrun else : func = sudo_or_dryrun return func ( command , * args , ** kwargs )
8685	def _encrypt ( self , value ) : value = json . dumps ( value ) with warnings . catch_warnings ( ) : warnings . simplefilter ( "ignore" ) encrypted_value = self . cipher . encrypt ( value . encode ( 'utf8' ) ) hexified_value = binascii . hexlify ( encrypted_value ) . decode ( 'ascii' ) return hexified_value
13452	def imgmin ( self ) : if not hasattr ( self , '_imgmin' ) : imgmin = _np . min ( self . images [ 0 ] ) for img in self . images : imin = _np . min ( img ) if imin > imgmin : imgmin = imin self . _imgmin = imgmin return _np . min ( self . image )
12366	def update ( self , id , name ) : return super ( Keys , self ) . update ( id , name = name )
9810	def dashboard ( yes , url ) : dashboard_url = "{}/app" . format ( PolyaxonClient ( ) . api_config . http_host ) if url : click . echo ( dashboard_url ) sys . exit ( 0 ) if not yes : click . confirm ( 'Dashboard page will now open in your browser. Continue?' , abort = True , default = True ) click . launch ( dashboard_url )
9620	def main ( ) : import time print ( 'Testing controller in position 1:' ) print ( 'Running 3 x 3 seconds tests' ) con = rController ( 1 ) for i in range ( 3 ) : print ( 'Waiting...' ) time . sleep ( 2.5 ) print ( 'State: ' , con . gamepad ) print ( 'Buttons: ' , con . buttons ) time . sleep ( 0.5 ) print ( 'Done!' )
7882	def _make_prefix ( self , declared_prefixes ) : used_prefixes = set ( self . _prefixes . values ( ) ) used_prefixes |= set ( declared_prefixes . values ( ) ) while True : prefix = u"ns{0}" . format ( self . _next_id ) self . _next_id += 1 if prefix not in used_prefixes : break return prefix
13629	def put ( self , metrics ) : if type ( metrics ) == list : for metric in metrics : self . c . put_metric_data ( ** metric ) else : self . c . put_metric_data ( ** metrics )
8426	def hue_pal ( h = .01 , l = .6 , s = .65 , color_space = 'hls' ) : if not all ( [ 0 <= val <= 1 for val in ( h , l , s ) ] ) : msg = ( "hue_pal expects values to be between 0 and 1. " " I got h={}, l={}, s={}" . format ( h , l , s ) ) raise ValueError ( msg ) if color_space not in ( 'hls' , 'husl' ) : msg = "color_space should be one of ['hls', 'husl']" raise ValueError ( msg ) name = '{}_palette' . format ( color_space ) palette = globals ( ) [ name ] def _hue_pal ( n ) : colors = palette ( n , h = h , l = l , s = s ) return [ mcolors . rgb2hex ( c ) for c in colors ] return _hue_pal
7295	def create_document_dictionary ( self , document , document_key = None , owner_document = None ) : doc_dict = self . create_doc_dict ( document , document_key , owner_document ) for doc_key , doc_field in doc_dict . items ( ) : if doc_key . startswith ( "_" ) : continue if isinstance ( doc_field , ListField ) : doc_dict [ doc_key ] = self . create_list_dict ( document , doc_field , doc_key ) elif isinstance ( doc_field , EmbeddedDocumentField ) : doc_dict [ doc_key ] = self . create_document_dictionary ( doc_dict [ doc_key ] . document_type_obj , doc_key ) else : doc_dict [ doc_key ] = { "_document" : document , "_key" : doc_key , "_document_field" : doc_field , "_widget" : get_widget ( doc_dict [ doc_key ] , getattr ( doc_field , 'disabled' , False ) ) } return doc_dict
11865	def pointwise_product ( self , other , bn ) : "Multiply two factors, combining their variables." vars = list ( set ( self . vars ) | set ( other . vars ) ) cpt = dict ( ( event_values ( e , vars ) , self . p ( e ) * other . p ( e ) ) for e in all_events ( vars , bn , { } ) ) return Factor ( vars , cpt )
11967	def _BYTES_TO_BITS ( ) : the_table = 256 * [ None ] bits_per_byte = list ( range ( 7 , - 1 , - 1 ) ) for n in range ( 256 ) : l = n bits = 8 * [ None ] for i in bits_per_byte : bits [ i ] = '01' [ n & 1 ] n >>= 1 the_table [ l ] = '' . join ( bits ) return the_table
13164	def format_value ( value ) : value_id = id ( value ) if value_id in recursion_breaker . processed : return u'<recursion>' recursion_breaker . processed . add ( value_id ) try : if isinstance ( value , six . binary_type ) : return u"'{0}'" . format ( value . decode ( 'utf-8' ) ) elif isinstance ( value , six . text_type ) : return u"u'{0}'" . format ( value ) elif isinstance ( value , ( list , tuple ) ) : values = list ( map ( format_value , value ) ) result = serialize_list ( u'[' , values , delimiter = u',' ) + u']' return force_unicode ( result ) elif isinstance ( value , dict ) : items = six . iteritems ( value ) items = ( tuple ( map ( format_value , item ) ) for item in items ) items = list ( items ) items . sort ( ) items = [ serialize_text ( u'{0}: ' . format ( key ) , item_value ) for key , item_value in items ] result = serialize_list ( u'{' , items , delimiter = u',' ) + u'}' return force_unicode ( result ) return force_unicode ( repr ( value ) ) finally : recursion_breaker . processed . remove ( value_id )
12059	def TK_message ( title , msg ) : root = tkinter . Tk ( ) root . withdraw ( ) root . attributes ( "-topmost" , True ) root . lift ( ) tkinter . messagebox . showwarning ( title , msg ) root . destroy ( )
6111	def trace_to_next_plane ( self ) : return list ( map ( lambda positions , deflections : np . subtract ( positions , deflections ) , self . positions , self . deflections ) )
10129	def _map_timezones ( ) : tz_map = { } todo = HAYSTACK_TIMEZONES_SET . copy ( ) for full_tz in pytz . all_timezones : if not bool ( todo ) : break if full_tz in todo : tz_map [ full_tz ] = full_tz todo . discard ( full_tz ) continue if '/' not in full_tz : continue ( prefix , suffix ) = full_tz . split ( '/' , 1 ) if '/' in suffix : continue if suffix in todo : tz_map [ suffix ] = full_tz todo . discard ( suffix ) continue return tz_map
1680	def SetVerboseLevel ( self , level ) : last_verbose_level = self . verbose_level self . verbose_level = level return last_verbose_level
11690	def get_area ( self , geojson ) : geojson = json . load ( open ( geojson , 'r' ) ) self . area = Polygon ( geojson [ 'features' ] [ 0 ] [ 'geometry' ] [ 'coordinates' ] [ 0 ] )
11059	def run ( self , start = True ) : if not self . is_setup : raise NotSetupError self . webserver . start ( ) first_connect = True try : while self . runnable : if self . reconnect_needed : if not self . sc . rtm_connect ( with_team_state = start ) : return False self . reconnect_needed = False if first_connect : first_connect = False self . plugins . connect ( ) try : events = self . sc . rtm_read ( ) except AttributeError : self . log . exception ( 'Something has failed in the slack rtm library. This is fatal.' ) self . runnable = False events = [ ] except : self . log . exception ( 'Unhandled exception in rtm_read()' ) self . reconnect_needed = True events = [ ] for e in events : try : self . _handle_event ( e ) except KeyboardInterrupt : self . runnable = False except : self . log . exception ( 'Unhandled exception in event handler' ) sleep ( 0.1 ) except KeyboardInterrupt : pass except : self . log . exception ( 'Unhandled exception' )
8668	def lock_key ( key_name , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Locking key...' ) stash . lock ( key_name = key_name ) click . echo ( 'Key locked successfully' ) except GhostError as ex : sys . exit ( ex )
6578	def _base_repr ( self , and_also = None ) : items = [ "=" . join ( ( key , repr ( getattr ( self , key ) ) ) ) for key in sorted ( self . _fields . keys ( ) ) ] if items : output = ", " . join ( items ) else : output = None if and_also : return "{}({}, {})" . format ( self . __class__ . __name__ , output , and_also ) else : return "{}({})" . format ( self . __class__ . __name__ , output )
9503	def remove_contained_in_list ( l ) : i = 0 l . sort ( ) while i < len ( l ) - 1 : if l [ i + 1 ] . contains ( l [ i ] ) : l . pop ( i ) elif l [ i ] . contains ( l [ i + 1 ] ) : l . pop ( i + 1 ) else : i += 1
2081	def jt_aggregate ( func , is_create = False , has_pk = False ) : def helper ( kwargs , obj ) : unified_job_template = None for item in UNIFIED_JT : if kwargs . get ( item , None ) is not None : jt_id = kwargs . pop ( item ) if unified_job_template is None : unified_job_template = ( item , jt_id ) else : raise exc . UsageError ( 'More than one unified job template fields provided, ' 'please tighten your criteria.' ) if unified_job_template is not None : kwargs [ 'unified_job_template' ] = unified_job_template [ 1 ] obj . identity = tuple ( list ( obj . identity ) + [ 'unified_job_template' ] ) return '/' . join ( [ UNIFIED_JT [ unified_job_template [ 0 ] ] , str ( unified_job_template [ 1 ] ) , 'schedules/' ] ) elif is_create : raise exc . UsageError ( 'You must provide exactly one unified job' ' template field during creation.' ) def decorator_without_pk ( obj , * args , ** kwargs ) : old_endpoint = obj . endpoint new_endpoint = helper ( kwargs , obj ) if is_create : obj . endpoint = new_endpoint result = func ( obj , * args , ** kwargs ) obj . endpoint = old_endpoint return result def decorator_with_pk ( obj , pk = None , * args , ** kwargs ) : old_endpoint = obj . endpoint new_endpoint = helper ( kwargs , obj ) if is_create : obj . endpoint = new_endpoint result = func ( obj , pk = pk , * args , ** kwargs ) obj . endpoint = old_endpoint return result decorator = decorator_with_pk if has_pk else decorator_without_pk for item in CLICK_ATTRS : setattr ( decorator , item , getattr ( func , item , [ ] ) ) decorator . __doc__ = func . __doc__ return decorator
1738	def unify_string_literals ( js_string ) : n = 0 res = '' limit = len ( js_string ) while n < limit : char = js_string [ n ] if char == '\\' : new , n = do_escape ( js_string , n ) res += new else : res += char n += 1 return res
12851	def _unlock_temporarily ( self ) : if not self . _is_locked : yield else : try : self . _is_locked = False yield finally : self . _is_locked = True
7689	def piano_roll ( annotation , sr = 22050 , length = None , ** kwargs ) : intervals , pitches = annotation . to_interval_values ( ) pitch_map = { f : idx for idx , f in enumerate ( np . unique ( pitches ) ) } gram = np . zeros ( ( len ( pitch_map ) , len ( intervals ) ) ) for col , f in enumerate ( pitches ) : gram [ pitch_map [ f ] , col ] = 1 return filter_kwargs ( mir_eval . sonify . time_frequency , gram , pitches , intervals , sr , length = length , ** kwargs )
7997	def set_peer_authenticated ( self , peer , restart_stream = False ) : with self . lock : self . peer_authenticated = True self . peer = peer if restart_stream : self . _restart_stream ( ) self . event ( AuthenticatedEvent ( self . peer ) )
1585	def send_buffered_messages ( self ) : while not self . out_stream . is_empty ( ) and self . _stmgr_client . is_registered : tuple_set = self . out_stream . poll ( ) if isinstance ( tuple_set , tuple_pb2 . HeronTupleSet ) : tuple_set . src_task_id = self . my_pplan_helper . my_task_id self . gateway_metrics . update_sent_packet ( tuple_set . ByteSize ( ) ) self . _stmgr_client . send_message ( tuple_set )
8951	def get_devpi_url ( ctx ) : cmd = 'devpi use --urls' lines = ctx . run ( cmd , hide = 'out' , echo = False ) . stdout . splitlines ( ) for line in lines : try : line , base_url = line . split ( ':' , 1 ) except ValueError : notify . warning ( 'Ignoring "{}"!' . format ( line ) ) else : if line . split ( ) [ - 1 ] . strip ( ) == 'simpleindex' : return base_url . split ( '\x1b' ) [ 0 ] . strip ( ) . rstrip ( '/' ) raise LookupError ( "Cannot find simpleindex URL in '{}' output:\n {}" . format ( cmd , '\n ' . join ( lines ) , ) )
9137	def clear_cache ( module_name : str , keep_database : bool = True ) -> None : data_dir = get_data_dir ( module_name ) if not os . path . exists ( data_dir ) : return for name in os . listdir ( data_dir ) : if name in { 'config.ini' , 'cfg.ini' } : continue if name == 'cache.db' and keep_database : continue path = os . path . join ( data_dir , name ) if os . path . isdir ( path ) : shutil . rmtree ( path ) else : os . remove ( path ) os . rmdir ( data_dir )
2160	def _format_id ( self , payload ) : if 'id' in payload : return str ( payload [ 'id' ] ) if 'results' in payload : return ' ' . join ( [ six . text_type ( item [ 'id' ] ) for item in payload [ 'results' ] ] ) raise MultipleRelatedError ( 'Could not serialize output with id format.' )
13233	def load ( directory_name , module_name ) : directory_name = os . path . expanduser ( directory_name ) if os . path . isdir ( directory_name ) and directory_name not in sys . path : sys . path . append ( directory_name ) try : return importlib . import_module ( module_name ) except ImportError : pass
12609	def _query_data ( data , field_names = None , operators = '__eq__' ) : if field_names is None : field_names = list ( data . keys ( ) ) if isinstance ( field_names , str ) : field_names = [ field_names ] sample = OrderedDict ( [ ( fn , data [ fn ] ) for fn in field_names ] ) return _query_sample ( sample , operators = operators )
4192	def compute_response ( self , ** kargs ) : from numpy . fft import fft , fftshift norm = kargs . get ( 'norm' , self . norm ) NFFT = kargs . get ( 'NFFT' , 2048 ) if NFFT < len ( self . data ) : NFFT = self . data . size * 2 A = fft ( self . data , NFFT ) mag = abs ( fftshift ( A ) ) if norm is True : mag = mag / max ( mag ) response = 20. * stools . log10 ( mag ) self . __response = response
3915	def _update ( self ) : typing_users = [ self . _conversation . get_user ( user_id ) for user_id , status in self . _typing_statuses . items ( ) if status == hangups . TYPING_TYPE_STARTED ] displayed_names = [ user . first_name for user in typing_users if not user . is_self ] if displayed_names : typing_message = '{} {} typing...' . format ( ', ' . join ( sorted ( displayed_names ) ) , 'is' if len ( displayed_names ) == 1 else 'are' ) else : typing_message = '' if not self . _is_connected : self . _widget . set_text ( "RECONNECTING..." ) elif self . _message is not None : self . _widget . set_text ( self . _message ) else : self . _widget . set_text ( typing_message )
11444	def parse ( self , path_to_xml = None ) : if not path_to_xml : if not self . path : self . logger . error ( "No path defined!" ) return path_to_xml = self . path root = self . _clean_xml ( path_to_xml ) if root . tag . lower ( ) == 'collection' : tree = ET . ElementTree ( root ) self . records = element_tree_collection_to_records ( tree ) elif root . tag . lower ( ) == 'record' : new_root = ET . Element ( 'collection' ) new_root . append ( root ) tree = ET . ElementTree ( new_root ) self . records = element_tree_collection_to_records ( tree ) else : header_subs = get_request_subfields ( root ) records = root . find ( 'ListRecords' ) if records is None : records = root . find ( 'GetRecord' ) if records is None : raise ValueError ( "Cannot find ListRecords or GetRecord!" ) tree = ET . ElementTree ( records ) for record , is_deleted in element_tree_oai_records ( tree , header_subs ) : if is_deleted : self . deleted_records . append ( self . create_deleted_record ( record ) ) else : self . records . append ( record )
13474	def _loop ( self ) : while True : try : with uncaught_greenlet_exception_context ( ) : self . _loop_callback ( ) except gevent . GreenletExit : break if self . _stop_event . wait ( self . _interval ) : break self . _clear ( )
5493	def validate_config_key ( ctx , param , value ) : if not value : return value try : section , item = value . split ( "." , 1 ) except ValueError : raise click . BadArgumentUsage ( "Given key does not contain a section name." ) else : return section , item
6425	def tanimoto_coeff ( self , src , tar , qval = 2 ) : coeff = self . sim ( src , tar , qval ) if coeff != 0 : return log ( coeff , 2 ) return float ( '-inf' )
4810	def train_model ( best_processed_path , weight_path = '../weight/model_weight.h5' , verbose = 2 ) : x_train_char , x_train_type , y_train = prepare_feature ( best_processed_path , option = 'train' ) x_test_char , x_test_type , y_test = prepare_feature ( best_processed_path , option = 'test' ) validation_set = False if os . path . isdir ( os . path . join ( best_processed_path , 'val' ) ) : validation_set = True x_val_char , x_val_type , y_val = prepare_feature ( best_processed_path , option = 'val' ) if not os . path . isdir ( os . path . dirname ( weight_path ) ) : os . makedirs ( os . path . dirname ( weight_path ) ) callbacks_list = [ ReduceLROnPlateau ( ) , ModelCheckpoint ( weight_path , save_best_only = True , save_weights_only = True , monitor = 'val_loss' , mode = 'min' , verbose = 1 ) ] model = get_convo_nn2 ( ) train_params = [ ( 10 , 256 ) , ( 3 , 512 ) , ( 3 , 2048 ) , ( 3 , 4096 ) , ( 3 , 8192 ) ] for ( epochs , batch_size ) in train_params : print ( "train with {} epochs and {} batch size" . format ( epochs , batch_size ) ) if validation_set : model . fit ( [ x_train_char , x_train_type ] , y_train , epochs = epochs , batch_size = batch_size , verbose = verbose , callbacks = callbacks_list , validation_data = ( [ x_val_char , x_val_type ] , y_val ) ) else : model . fit ( [ x_train_char , x_train_type ] , y_train , epochs = epochs , batch_size = batch_size , verbose = verbose , callbacks = callbacks_list ) return model
11657	def fit ( self , X , y = None ) : X = check_array ( X , copy = self . copy , dtype = [ np . float64 , np . float32 , np . float16 , np . float128 ] ) feature_range = self . feature_range if feature_range [ 0 ] >= feature_range [ 1 ] : raise ValueError ( "Minimum of desired feature range must be smaller" " than maximum. Got %s." % str ( feature_range ) ) if self . fit_feature_range is not None : fit_feature_range = self . fit_feature_range if fit_feature_range [ 0 ] >= fit_feature_range [ 1 ] : raise ValueError ( "Minimum of desired (fit) feature range must " "be smaller than maximum. Got %s." % str ( feature_range ) ) if ( fit_feature_range [ 0 ] < feature_range [ 0 ] or fit_feature_range [ 1 ] > feature_range [ 1 ] ) : raise ValueError ( "fit_feature_range must be a subset of " "feature_range. Got %s, fit %s." % ( str ( feature_range ) , str ( fit_feature_range ) ) ) feature_range = fit_feature_range data_min = np . min ( X , axis = 0 ) data_range = np . max ( X , axis = 0 ) - data_min data_range [ data_range == 0.0 ] = 1.0 self . scale_ = ( feature_range [ 1 ] - feature_range [ 0 ] ) / data_range self . min_ = feature_range [ 0 ] - data_min * self . scale_ self . data_range = data_range self . data_min = data_min return self
8555	def get_lan ( self , datacenter_id , lan_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/lans/%s?depth=%s' % ( datacenter_id , lan_id , str ( depth ) ) ) return response
5815	def _read_callback ( connection_id , data_buffer , data_length_pointer ) : self = None try : self = _connection_refs . get ( connection_id ) if not self : socket = _socket_refs . get ( connection_id ) else : socket = self . _socket if not self and not socket : return 0 bytes_requested = deref ( data_length_pointer ) timeout = socket . gettimeout ( ) error = None data = b'' try : while len ( data ) < bytes_requested : if timeout is not None and timeout > 0.0 : read_ready , _ , _ = select . select ( [ socket ] , [ ] , [ ] , timeout ) if len ( read_ready ) == 0 : raise socket_ . error ( errno . EAGAIN , 'timed out' ) chunk = socket . recv ( bytes_requested - len ( data ) ) data += chunk if chunk == b'' : if len ( data ) == 0 : if timeout is None : return SecurityConst . errSSLClosedNoNotify return SecurityConst . errSSLClosedAbort break except ( socket_ . error ) as e : error = e . errno if error is not None and error != errno . EAGAIN : if error == errno . ECONNRESET or error == errno . EPIPE : return SecurityConst . errSSLClosedNoNotify return SecurityConst . errSSLClosedAbort if self and not self . _done_handshake : if len ( data ) >= 3 and len ( self . _server_hello ) == 0 : valid_record_type = data [ 0 : 1 ] in set ( [ b'\x15' , b'\x16' ] ) valid_protocol_version = data [ 1 : 3 ] in set ( [ b'\x03\x00' , b'\x03\x01' , b'\x03\x02' , b'\x03\x03' , b'\x03\x04' ] ) if not valid_record_type or not valid_protocol_version : self . _server_hello += data + _read_remaining ( socket ) return SecurityConst . errSSLProtocol self . _server_hello += data write_to_buffer ( data_buffer , data ) pointer_set ( data_length_pointer , len ( data ) ) if len ( data ) != bytes_requested : return SecurityConst . errSSLWouldBlock return 0 except ( KeyboardInterrupt ) as e : if self : self . _exception = e return SecurityConst . errSSLClosedAbort
6643	def getExtraIncludes ( self ) : if 'extraIncludes' in self . description : return [ os . path . normpath ( x ) for x in self . description [ 'extraIncludes' ] ] else : return [ ]
8979	def _path ( self , path ) : mode , encoding = self . _mode_and_encoding_for_open ( ) with open ( path , mode , encoding = encoding ) as file : self . __dump_to_file ( file )
3085	def service_account_email ( self ) : if self . _service_account_email is None : self . _service_account_email = ( app_identity . get_service_account_name ( ) ) return self . _service_account_email
9920	def save ( self ) : token = models . PasswordResetToken . objects . get ( key = self . validated_data [ "key" ] ) token . email . user . set_password ( self . validated_data [ "password" ] ) token . email . user . save ( ) logger . info ( "Reset password for %s" , token . email . user ) token . delete ( )
7748	def _process_iq_response ( self , stanza ) : stanza_id = stanza . stanza_id from_jid = stanza . from_jid if from_jid : ufrom = from_jid . as_unicode ( ) else : ufrom = None res_handler = err_handler = None try : res_handler , err_handler = self . _iq_response_handlers . pop ( ( stanza_id , ufrom ) ) except KeyError : logger . debug ( "No response handler for id={0!r} from={1!r}" . format ( stanza_id , ufrom ) ) logger . debug ( " from_jid: {0!r} peer: {1!r} me: {2!r}" . format ( from_jid , self . peer , self . me ) ) if ( ( from_jid == self . peer or from_jid == self . me or self . me and from_jid == self . me . bare ( ) ) ) : try : logger . debug ( " trying id={0!r} from=None" . format ( stanza_id ) ) res_handler , err_handler = self . _iq_response_handlers . pop ( ( stanza_id , None ) ) except KeyError : pass if stanza . stanza_type == "result" : if res_handler : response = res_handler ( stanza ) else : return False else : if err_handler : response = err_handler ( stanza ) else : return False self . _process_handler_result ( response ) return True
13510	def pyflakes ( ) : packages = [ x for x in options . setup . packages if '.' not in x ] sh ( 'pyflakes {param} {files}' . format ( param = options . paved . pycheck . pyflakes . param , files = ' ' . join ( packages ) ) )
11055	def rm_back_refs ( obj ) : for ref in _collect_refs ( obj ) : ref [ 'value' ] . _remove_backref ( ref [ 'field_instance' ] . _backref_field_name , obj , ref [ 'field_name' ] , strict = False )
7629	def namespace_array ( ns_key ) : obs_sch = namespace ( ns_key ) obs_sch [ 'title' ] = 'Observation' sch = copy . deepcopy ( JAMS_SCHEMA [ 'definitions' ] [ 'SparseObservationList' ] ) sch [ 'items' ] = obs_sch return sch
5235	def filter_by_dates ( files_or_folders : list , date_fmt = DATE_FMT ) -> list : r = re . compile ( f'.*{date_fmt}.*' ) return list ( filter ( lambda vv : r . match ( vv . replace ( '\\' , '/' ) . split ( '/' ) [ - 1 ] ) is not None , files_or_folders , ) )
7234	def tilemap ( self , query , styles = { } , bbox = [ - 180 , - 90 , 180 , 90 ] , zoom = 16 , api_key = os . environ . get ( 'MAPBOX_API_KEY' , None ) , image = None , image_bounds = None , index = "vector-user-provided" , name = "GBDX_Task_Output" , ** kwargs ) : try : from IPython . display import display except : print ( "IPython is required to produce maps." ) return assert api_key is not None , "No Mapbox API Key found. You can either pass in a token or set the MAPBOX_API_KEY environment variable." wkt = box ( * bbox ) . wkt features = self . query ( wkt , query , index = index ) union = cascaded_union ( [ shape ( f [ 'geometry' ] ) for f in features ] ) lon , lat = union . centroid . coords [ 0 ] url = 'https://vector.geobigdata.io/insight-vector/api/mvt/{z}/{x}/{y}?' url += 'q={}&index={}' . format ( query , index ) if styles is not None and not isinstance ( styles , list ) : styles = [ styles ] map_id = "map_{}" . format ( str ( int ( time . time ( ) ) ) ) map_data = VectorTileLayer ( url , source_name = name , styles = styles , ** kwargs ) image_layer = self . _build_image_layer ( image , image_bounds ) template = BaseTemplate ( map_id , ** { "lat" : lat , "lon" : lon , "zoom" : zoom , "datasource" : json . dumps ( map_data . datasource ) , "layers" : json . dumps ( map_data . layers ) , "image_layer" : image_layer , "mbkey" : api_key , "token" : self . gbdx_connection . access_token } ) template . inject ( )
8599	def get_share ( self , group_id , resource_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s/shares/%s?depth=%s' % ( group_id , resource_id , str ( depth ) ) ) return response
1767	def execute ( self ) : if issymbolic ( self . PC ) : raise ConcretizeRegister ( self , 'PC' , policy = 'ALL' ) if not self . memory . access_ok ( self . PC , 'x' ) : raise InvalidMemoryAccess ( self . PC , 'x' ) self . _publish ( 'will_decode_instruction' , self . PC ) insn = self . decode_instruction ( self . PC ) self . _last_pc = self . PC self . _publish ( 'will_execute_instruction' , self . PC , insn ) if insn . address != self . PC : return name = self . canonicalize_instruction_name ( insn ) if logger . level == logging . DEBUG : logger . debug ( self . render_instruction ( insn ) ) for l in self . render_registers ( ) : register_logger . debug ( l ) try : if self . _concrete and 'SYSCALL' in name : self . emu . sync_unicorn_to_manticore ( ) if self . _concrete and 'SYSCALL' not in name : self . emulate ( insn ) if self . PC == self . _break_unicorn_at : logger . debug ( "Switching from Unicorn to Manticore" ) self . _break_unicorn_at = None self . _concrete = False else : implementation = getattr ( self , name , None ) if implementation is not None : implementation ( * insn . operands ) else : text_bytes = ' ' . join ( '%02x' % x for x in insn . bytes ) logger . warning ( "Unimplemented instruction: 0x%016x:\t%s\t%s\t%s" , insn . address , text_bytes , insn . mnemonic , insn . op_str ) self . backup_emulate ( insn ) except ( Interruption , Syscall ) as e : e . on_handled = lambda : self . _publish_instruction_as_executed ( insn ) raise e else : self . _publish_instruction_as_executed ( insn )
8833	def soft_equals ( a , b ) : if isinstance ( a , str ) or isinstance ( b , str ) : return str ( a ) == str ( b ) if isinstance ( a , bool ) or isinstance ( b , bool ) : return bool ( a ) is bool ( b ) return a == b
13311	def site_path ( self ) : if platform == 'win' : return unipath ( self . path , 'Lib' , 'site-packages' ) py_ver = 'python{0}' . format ( sys . version [ : 3 ] ) return unipath ( self . path , 'lib' , py_ver , 'site-packages' )
7589	def call_fastq_dump_on_SRRs ( self , srr , outname , paired ) : fd_cmd = [ "fastq-dump" , srr , "--accession" , outname , "--outdir" , self . workdir , "--gzip" , ] if paired : fd_cmd += [ "--split-files" ] proc = sps . Popen ( fd_cmd , stderr = sps . STDOUT , stdout = sps . PIPE ) o , e = proc . communicate ( ) srafile = os . path . join ( self . workdir , "sra" , srr + ".sra" ) if os . path . exists ( srafile ) : os . remove ( srafile )
1406	def validated_formatter ( self , url_format ) : valid_parameters = { "${CLUSTER}" : "cluster" , "${ENVIRON}" : "environ" , "${TOPOLOGY}" : "topology" , "${ROLE}" : "role" , "${USER}" : "user" , } dummy_formatted_url = url_format for key , value in valid_parameters . items ( ) : dummy_formatted_url = dummy_formatted_url . replace ( key , value ) if '$' in dummy_formatted_url : raise Exception ( "Invalid viz.url.format: %s" % ( url_format ) ) return url_format
1246	def disconnect ( self ) : if not self . socket : logging . warning ( "No active socket to close!" ) return self . socket . close ( ) self . socket = None
3235	def list_buckets ( client = None , ** kwargs ) : buckets = client . list_buckets ( ** kwargs ) return [ b . __dict__ for b in buckets ]
2880	def get_event_definition ( self ) : messageEventDefinition = first ( self . xpath ( './/bpmn:messageEventDefinition' ) ) if messageEventDefinition is not None : return self . get_message_event_definition ( messageEventDefinition ) timerEventDefinition = first ( self . xpath ( './/bpmn:timerEventDefinition' ) ) if timerEventDefinition is not None : return self . get_timer_event_definition ( timerEventDefinition ) raise NotImplementedError ( 'Unsupported Intermediate Catch Event: %r' , ET . tostring ( self . node ) )
9956	def tracemessage ( self , maxlen = 6 ) : result = "" for i , value in enumerate ( self ) : result += "{0}: {1}\n" . format ( i , get_node_repr ( value ) ) result = result . strip ( "\n" ) lines = result . split ( "\n" ) if maxlen and len ( lines ) > maxlen : i = int ( maxlen / 2 ) lines = lines [ : i ] + [ "..." ] + lines [ - ( maxlen - i ) : ] result = "\n" . join ( lines ) return result
5503	def config ( ctx , key , value , remove , edit ) : conf = ctx . obj [ "conf" ] if not edit and not key : raise click . BadArgumentUsage ( "You have to specify either a key or use --edit." ) if edit : return click . edit ( filename = conf . config_file ) if remove : try : conf . cfg . remove_option ( key [ 0 ] , key [ 1 ] ) except Exception as e : logger . debug ( e ) else : conf . write_config ( ) return if not value : try : click . echo ( conf . cfg . get ( key [ 0 ] , key [ 1 ] ) ) except Exception as e : logger . debug ( e ) return if not conf . cfg . has_section ( key [ 0 ] ) : conf . cfg . add_section ( key [ 0 ] ) conf . cfg . set ( key [ 0 ] , key [ 1 ] , value ) conf . write_config ( )
2828	def convert_selu ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting selu ...' ) if names == 'short' : tf_name = 'SELU' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) selu = keras . layers . Activation ( 'selu' , name = tf_name ) layers [ scope_name ] = selu ( layers [ inputs [ 0 ] ] )
6189	def get_bromo_fnames_da ( d_em_kHz , d_bg_kHz , a_em_kHz , a_bg_kHz , ID = '1+2+3+4+5+6' , t_tot = '480' , num_p = '30' , pM = '64' , t_step = 0.5e-6 , D = 1.2e-11 , dir_ = '' ) : clk_p = t_step / 32. E_sim = 1. * a_em_kHz / ( a_em_kHz + d_em_kHz ) FRET_val = 100. * E_sim print ( "Simulated FRET value: %.1f%%" % FRET_val ) d_em_kHz_str = "%04d" % d_em_kHz a_em_kHz_str = "%04d" % a_em_kHz d_bg_kHz_str = "%04.1f" % d_bg_kHz a_bg_kHz_str = "%04.1f" % a_bg_kHz print ( "D: EM %s BG %s " % ( d_em_kHz_str , d_bg_kHz_str ) ) print ( "A: EM %s BG %s " % ( a_em_kHz_str , a_bg_kHz_str ) ) fname_d = ( 'ph_times_{t_tot}s_D{D}_{np}P_{pM}pM_' 'step{ts_us}us_ID{ID}_EM{em}kHz_BG{bg}kHz.npy' ) . format ( em = d_em_kHz_str , bg = d_bg_kHz_str , t_tot = t_tot , pM = pM , np = num_p , ID = ID , ts_us = t_step * 1e6 , D = D ) fname_a = ( 'ph_times_{t_tot}s_D{D}_{np}P_{pM}pM_' 'step{ts_us}us_ID{ID}_EM{em}kHz_BG{bg}kHz.npy' ) . format ( em = a_em_kHz_str , bg = a_bg_kHz_str , t_tot = t_tot , pM = pM , np = num_p , ID = ID , ts_us = t_step * 1e6 , D = D ) print ( fname_d ) print ( fname_a ) name = ( 'BroSim_E{:.1f}_dBG{:.1f}k_aBG{:.1f}k_' 'dEM{:.0f}k' ) . format ( FRET_val , d_bg_kHz , a_bg_kHz , d_em_kHz ) return dir_ + fname_d , dir_ + fname_a , name , clk_p , E_sim
4448	def delete_document ( self , doc_id , conn = None ) : if conn is None : conn = self . redis return conn . execute_command ( self . DEL_CMD , self . index_name , doc_id )
11385	def body ( self ) : if not hasattr ( self , '_body' ) : self . _body = inspect . getsource ( self . module ) return self . _body
3270	def md_entry ( node ) : key = None value = None if 'key' in node . attrib : key = node . attrib [ 'key' ] else : key = None if key in [ 'time' , 'elevation' ] or key . startswith ( 'custom_dimension' ) : value = md_dimension_info ( key , node . find ( "dimensionInfo" ) ) elif key == 'DynamicDefaultValues' : value = md_dynamic_default_values_info ( key , node . find ( "DynamicDefaultValues" ) ) elif key == 'JDBC_VIRTUAL_TABLE' : value = md_jdbc_virtual_table ( key , node . find ( "virtualTable" ) ) else : value = node . text if None in [ key , value ] : return None else : return ( key , value )
4334	def norm ( self , db_level = - 3.0 ) : if not is_number ( db_level ) : raise ValueError ( 'db_level must be a number.' ) effect_args = [ 'norm' , '{:f}' . format ( db_level ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'norm' ) return self
481	def main_restore_embedding_layer ( ) : vocabulary_size = 50000 embedding_size = 128 model_file_name = "model_word2vec_50k_128" batch_size = None print ( "Load existing embedding matrix and dictionaries" ) all_var = tl . files . load_npy_to_any ( name = model_file_name + '.npy' ) data = all_var [ 'data' ] count = all_var [ 'count' ] dictionary = all_var [ 'dictionary' ] reverse_dictionary = all_var [ 'reverse_dictionary' ] tl . nlp . save_vocab ( count , name = 'vocab_' + model_file_name + '.txt' ) del all_var , data , count load_params = tl . files . load_npz ( name = model_file_name + '.npz' ) x = tf . placeholder ( tf . int32 , shape = [ batch_size ] ) emb_net = tl . layers . EmbeddingInputlayer ( x , vocabulary_size , embedding_size , name = 'emb' ) sess . run ( tf . global_variables_initializer ( ) ) tl . files . assign_params ( sess , [ load_params [ 0 ] ] , emb_net ) emb_net . print_params ( ) emb_net . print_layers ( ) word = b'hello' word_id = dictionary [ word ] print ( 'word_id:' , word_id ) words = [ b'i' , b'am' , b'tensor' , b'layer' ] word_ids = tl . nlp . words_to_word_ids ( words , dictionary , _UNK ) context = tl . nlp . word_ids_to_words ( word_ids , reverse_dictionary ) print ( 'word_ids:' , word_ids ) print ( 'context:' , context ) vector = sess . run ( emb_net . outputs , feed_dict = { x : [ word_id ] } ) print ( 'vector:' , vector . shape ) vectors = sess . run ( emb_net . outputs , feed_dict = { x : word_ids } ) print ( 'vectors:' , vectors . shape )
2591	def stage_out ( self , file , executor ) : if file . scheme == 'http' or file . scheme == 'https' : raise Exception ( 'HTTP/HTTPS file staging out is not supported' ) elif file . scheme == 'ftp' : raise Exception ( 'FTP file staging out is not supported' ) elif file . scheme == 'globus' : globus_ep = self . _get_globus_endpoint ( executor ) stage_out_app = self . _globus_stage_out_app ( ) return stage_out_app ( globus_ep , inputs = [ file ] ) else : raise Exception ( 'Staging out with unknown file scheme {} is not supported' . format ( file . scheme ) )
10841	def delete ( self ) : url = PATHS [ 'DELETE' ] % self . id return self . api . post ( url = url )
5177	def resources ( self , type_ = None , title = None , ** kwargs ) : if type_ is None : resources = self . __api . resources ( query = EqualsOperator ( "certname" , self . name ) , ** kwargs ) elif type_ is not None and title is None : resources = self . __api . resources ( type_ = type_ , query = EqualsOperator ( "certname" , self . name ) , ** kwargs ) else : resources = self . __api . resources ( type_ = type_ , title = title , query = EqualsOperator ( "certname" , self . name ) , ** kwargs ) return resources
9865	def currency ( self ) : try : current_subscription = self . info [ "viewer" ] [ "home" ] [ "currentSubscription" ] return current_subscription [ "priceInfo" ] [ "current" ] [ "currency" ] except ( KeyError , TypeError , IndexError ) : _LOGGER . error ( "Could not find currency." ) return ""
1634	def CheckPosixThreading ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] for single_thread_func , multithread_safe_func , pattern in _THREADING_LIST : if Search ( pattern , line ) : error ( filename , linenum , 'runtime/threadsafe_fn' , 2 , 'Consider using ' + multithread_safe_func + '...) instead of ' + single_thread_func + '...) for improved thread safety.' )
741	def radiusForSpeed ( self , speed ) : overlap = 1.5 coordinatesPerTimestep = speed * self . timestep / self . scale radius = int ( round ( float ( coordinatesPerTimestep ) / 2 * overlap ) ) minRadius = int ( math . ceil ( ( math . sqrt ( self . w ) - 1 ) / 2 ) ) return max ( radius , minRadius )
912	def write ( self , proto ) : super ( PreviousValueModel , self ) . writeBaseToProto ( proto . modelBase ) proto . fieldNames = self . _fieldNames proto . fieldTypes = self . _fieldTypes if self . _predictedField : proto . predictedField = self . _predictedField proto . predictionSteps = self . _predictionSteps
13217	def connection_dsn ( self , name = None ) : return ' ' . join ( "%s=%s" % ( param , value ) for param , value in self . _connect_options ( name ) )
11135	def copyto ( self , new_abspath = None , new_dirpath = None , new_dirname = None , new_basename = None , new_fname = None , new_ext = None , overwrite = False , makedirs = False ) : self . assert_exists ( ) p = self . change ( new_abspath = new_abspath , new_dirpath = new_dirpath , new_dirname = new_dirname , new_basename = new_basename , new_fname = new_fname , new_ext = new_ext , ) if p . is_not_exist_or_allow_overwrite ( overwrite = overwrite ) : if self . abspath != p . abspath : try : shutil . copy ( self . abspath , p . abspath ) except IOError as e : if makedirs : os . makedirs ( p . parent . abspath ) shutil . copy ( self . abspath , p . abspath ) else : raise e return p
13656	def routedResource ( f , routerAttribute = 'router' ) : return wraps ( f ) ( lambda * a , ** kw : getattr ( f ( * a , ** kw ) , routerAttribute ) . resource ( ) )
6946	def jhk_to_imag ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , IJHK , IJH , IJK , IHK , IJ , IH , IK )
5295	def get_start_date ( self , obj ) : obj_date = getattr ( obj , self . get_date_field ( ) ) try : obj_date = obj_date . date ( ) except AttributeError : pass return obj_date
5569	def zoom_index_gen ( mp = None , out_dir = None , zoom = None , geojson = False , gpkg = False , shapefile = False , txt = False , vrt = False , fieldname = "location" , basepath = None , for_gdal = True , threading = False , ) : for zoom in get_zoom_levels ( process_zoom_levels = zoom ) : with ExitStack ( ) as es : index_writers = [ ] if geojson : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "GeoJSON" , out_path = _index_file_path ( out_dir , zoom , "geojson" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if gpkg : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "GPKG" , out_path = _index_file_path ( out_dir , zoom , "gpkg" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if shapefile : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "ESRI Shapefile" , out_path = _index_file_path ( out_dir , zoom , "shp" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if txt : index_writers . append ( es . enter_context ( TextFileWriter ( out_path = _index_file_path ( out_dir , zoom , "txt" ) ) ) ) if vrt : index_writers . append ( es . enter_context ( VRTFileWriter ( out_path = _index_file_path ( out_dir , zoom , "vrt" ) , output = mp . config . output , out_pyramid = mp . config . output_pyramid ) ) ) logger . debug ( "use the following index writers: %s" , index_writers ) def _worker ( tile ) : tile_path = _tile_path ( orig_path = mp . config . output . get_path ( tile ) , basepath = basepath , for_gdal = for_gdal ) indexes = [ i for i in index_writers if not i . entry_exists ( tile = tile , path = tile_path ) ] if indexes : output_exists = mp . config . output . tiles_exist ( output_tile = tile ) else : output_exists = None return tile , tile_path , indexes , output_exists with concurrent . futures . ThreadPoolExecutor ( ) as executor : for task in concurrent . futures . as_completed ( ( executor . submit ( _worker , i ) for i in mp . config . output_pyramid . tiles_from_geom ( mp . config . area_at_zoom ( zoom ) , zoom ) ) ) : tile , tile_path , indexes , output_exists = task . result ( ) if indexes and output_exists : logger . debug ( "%s exists" , tile_path ) logger . debug ( "write to %s indexes" % len ( indexes ) ) for index in indexes : index . write ( tile , tile_path ) yield tile
5045	def get_failed_enrollment_message ( cls , users , enrolled_in ) : failed_emails = [ user . email for user in users ] return ( 'error' , _ ( 'The following learners could not be enrolled in {enrolled_in}: {user_list}' ) . format ( enrolled_in = enrolled_in , user_list = ', ' . join ( failed_emails ) , ) )
9705	def run ( self ) : while self . isRunning . is_set ( ) : try : try : self . monitorTUN ( ) except timeout_decorator . TimeoutError as error : pass self . checkSerial ( ) except KeyboardInterrupt : break
7106	def export ( self , model_name , export_folder ) : for transformer in self . transformers : if isinstance ( transformer , MultiLabelBinarizer ) : joblib . dump ( transformer , join ( export_folder , "label.transformer.bin" ) , protocol = 2 ) if isinstance ( transformer , TfidfVectorizer ) : joblib . dump ( transformer , join ( export_folder , "tfidf.transformer.bin" ) , protocol = 2 ) if isinstance ( transformer , CountVectorizer ) : joblib . dump ( transformer , join ( export_folder , "count.transformer.bin" ) , protocol = 2 ) if isinstance ( transformer , NumberRemover ) : joblib . dump ( transformer , join ( export_folder , "number.transformer.bin" ) , protocol = 2 ) model = [ model for model in self . models if model . name == model_name ] [ 0 ] e = Experiment ( self . X , self . y , model . estimator , None ) model_filename = join ( export_folder , "model.bin" ) e . export ( model_filename )
11710	def request ( self , path , data = None , headers = None , method = None ) : if isinstance ( data , str ) : data = data . encode ( 'utf-8' ) response = urlopen ( self . _request ( path , data = data , headers = headers , method = method ) ) self . _set_session_cookie ( response ) return response
9449	def schedule_hangup ( self , call_params ) : path = '/' + self . api_version + '/ScheduleHangup/' method = 'POST' return self . request ( path , method , call_params )
11145	def get_repository_state ( self , relaPath = None ) : state = [ ] def _walk_dir ( relaPath , dirList ) : dirDict = { 'type' : 'dir' , 'exists' : os . path . isdir ( os . path . join ( self . __path , relaPath ) ) , 'pyrepdirinfo' : os . path . isfile ( os . path . join ( self . __path , relaPath , self . __dirInfo ) ) , } state . append ( { relaPath : dirDict } ) for fname in sorted ( [ f for f in dirList if isinstance ( f , basestring ) ] ) : relaFilePath = os . path . join ( relaPath , fname ) realFilePath = os . path . join ( self . __path , relaFilePath ) fileDict = { 'type' : 'file' , 'exists' : os . path . isfile ( realFilePath ) , 'pyrepfileinfo' : os . path . isfile ( os . path . join ( self . __path , relaPath , self . __fileInfo % fname ) ) , } state . append ( { relaFilePath : fileDict } ) for ddict in sorted ( [ d for d in dirList if isinstance ( d , dict ) ] , key = lambda k : list ( k ) [ 0 ] ) : dirname = list ( ddict ) [ 0 ] _walk_dir ( relaPath = os . path . join ( relaPath , dirname ) , dirList = ddict [ dirname ] ) if relaPath is None : _walk_dir ( relaPath = '' , dirList = self . __repo [ 'walk_repo' ] ) else : assert isinstance ( relaPath , basestring ) , "relaPath must be None or a str" relaPath = self . to_repo_relative_path ( path = relaPath , split = False ) spath = relaPath . split ( os . sep ) dirList = self . __repo [ 'walk_repo' ] while len ( spath ) : dirname = spath . pop ( 0 ) dList = [ d for d in dirList if isinstance ( d , dict ) ] if not len ( dList ) : dirList = None break cDict = [ d for d in dList if dirname in d ] if not len ( cDict ) : dirList = None break dirList = cDict [ 0 ] [ dirname ] if dirList is not None : _walk_dir ( relaPath = relaPath , dirList = dirList ) return state
2724	def change_kernel ( self , kernel , return_dict = True ) : if type ( kernel ) != Kernel : raise BadKernelObject ( "Use Kernel object" ) return self . _perform_action ( { 'type' : 'change_kernel' , 'kernel' : kernel . id } , return_dict )
6156	def stereo_FM ( x , fs = 2.4e6 , file_name = 'test.wav' ) : N1 = 10 b = signal . firwin ( 64 , 2 * 200e3 / float ( fs ) ) y = signal . lfilter ( b , 1 , x ) z = ss . downsample ( y , N1 ) z_bb = discrim ( z ) b12 = signal . firwin ( 128 , 2 * 12e3 / ( float ( fs ) / N1 ) ) y_lpr = signal . lfilter ( b12 , 1 , z_bb ) b19 = signal . firwin ( 128 , 2 * 1e3 * np . array ( [ 19 - 5 , 19 + 5 ] ) / ( float ( fs ) / N1 ) , pass_zero = False ) z_bb19 = signal . lfilter ( b19 , 1 , z_bb ) theta , phi_error = pilot_PLL ( z_bb19 , 19000 , fs / N1 , 2 , 10 , 0.707 ) b38 = signal . firwin ( 128 , 2 * 1e3 * np . array ( [ 38 - 5 , 38 + 5 ] ) / ( float ( fs ) / N1 ) , pass_zero = False ) x_lmr = signal . lfilter ( b38 , 1 , z_bb ) x_lmr = 2 * np . sqrt ( 2 ) * np . cos ( 2 * theta ) * x_lmr y_lmr = signal . lfilter ( b12 , 1 , x_lmr ) y_left = y_lpr + y_lmr y_right = y_lpr - y_lmr N2 = 5 fs2 = float ( fs ) / ( N1 * N2 ) y_left_DN2 = ss . downsample ( y_left , N2 ) y_right_DN2 = ss . downsample ( y_right , N2 ) a_de = np . exp ( - 2.1 * 1e3 * 2 * np . pi / fs2 ) z_left = signal . lfilter ( [ 1 - a_de ] , [ 1 , - a_de ] , y_left_DN2 ) z_right = signal . lfilter ( [ 1 - a_de ] , [ 1 , - a_de ] , y_right_DN2 ) z_out = np . hstack ( ( np . array ( [ z_left ] ) . T , ( np . array ( [ z_right ] ) . T ) ) ) ss . to_wav ( file_name , 48000 , z_out / 2 ) print ( 'Done!' ) return z_bb , theta , y_lpr , y_lmr , z_out
13545	def get_user ( self , user_id ) : url = "/2/users/%s" % user_id return self . user_from_json ( self . _get_resource ( url ) [ "user" ] )
145	def deepcopy ( self , exterior = None , label = None ) : return Polygon ( exterior = np . copy ( self . exterior ) if exterior is None else exterior , label = self . label if label is None else label )
4736	def info ( txt ) : print ( "%s# %s%s%s" % ( PR_EMPH_CC , get_time_stamp ( ) , txt , PR_NC ) ) sys . stdout . flush ( )
9774	def resources ( ctx , gpu ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . job . resources ( user , project_name , _job , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
9737	def get_3d_markers_no_label ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPositionNoLabel , component_info , data , component_position )
4648	def get_raw ( self ) : if not self . ops : return ops = [ self . operations . Op_wrapper ( op = o ) for o in list ( self . ops ) ] proposer = self . account_class ( self . proposer , blockchain_instance = self . blockchain ) data = { "fee" : { "amount" : 0 , "asset_id" : "1.3.0" } , "fee_paying_account" : proposer [ "id" ] , "expiration_time" : formatTimeFromNow ( self . proposal_expiration ) , "proposed_ops" : [ o . json ( ) for o in ops ] , "extensions" : [ ] , } if self . proposal_review : data . update ( { "review_period_seconds" : self . proposal_review } ) ops = self . operations . Proposal_create ( ** data ) return self . operation_class ( ops )
11531	def _hashkey ( self , method , url , ** kwa ) : to_hash = '' . join ( [ str ( method ) , str ( url ) , str ( kwa . get ( 'data' , '' ) ) , str ( kwa . get ( 'params' , '' ) ) ] ) return hashlib . md5 ( to_hash . encode ( ) ) . hexdigest ( )
5187	def inventory ( self , ** kwargs ) : inventory = self . _query ( 'inventory' , ** kwargs ) for inv in inventory : yield Inventory ( node = inv [ 'certname' ] , time = inv [ 'timestamp' ] , environment = inv [ 'environment' ] , facts = inv [ 'facts' ] , trusted = inv [ 'trusted' ] )
9984	def has_lambda ( src ) : module_node = ast . parse ( dedent ( src ) ) lambdaexp = [ node for node in ast . walk ( module_node ) if isinstance ( node , ast . Lambda ) ] return bool ( lambdaexp )
4077	def run_3to2 ( args = None ) : args = BASE_ARGS_3TO2 if args is None else BASE_ARGS_3TO2 + args try : proc = subprocess . Popen ( [ '3to2' ] + args , stderr = subprocess . PIPE ) except OSError : for path in glob . glob ( '*.egg' ) : if os . path . isdir ( path ) and path not in sys . path : sys . path . append ( path ) try : from lib3to2 . main import main as lib3to2_main except ImportError : raise OSError ( '3to2 script is unavailable.' ) else : if lib3to2_main ( 'lib3to2.fixes' , args ) : raise Exception ( 'lib3to2 parsing error' ) else : num_errors = 0 while proc . poll ( ) is None : line = proc . stderr . readline ( ) sys . stderr . write ( line ) num_errors += line . count ( ': ParseError: ' ) if proc . returncode or num_errors : raise Exception ( 'lib3to2 parsing error' )
2557	def clean_attribute ( attribute ) : attribute = { 'cls' : 'class' , 'className' : 'class' , 'class_name' : 'class' , 'fr' : 'for' , 'html_for' : 'for' , 'htmlFor' : 'for' , } . get ( attribute , attribute ) if attribute [ 0 ] == '_' : attribute = attribute [ 1 : ] if attribute in set ( [ 'http_equiv' ] ) or attribute . startswith ( 'data_' ) : attribute = attribute . replace ( '_' , '-' ) . lower ( ) if attribute . split ( '_' ) [ 0 ] in ( 'xlink' , 'xml' , 'xmlns' ) : attribute = attribute . replace ( '_' , ':' , 1 ) . lower ( ) return attribute
3183	def update ( self , store_id , data ) : self . store_id = store_id return self . _mc_client . _patch ( url = self . _build_path ( store_id ) , data = data )
9995	def del_attr ( self , name ) : if name in self . namespace : if name in self . cells : self . del_cells ( name ) elif name in self . spaces : self . del_space ( name ) elif name in self . refs : self . del_ref ( name ) else : raise RuntimeError ( "Must not happen" ) else : raise KeyError ( "'%s' not found in Space '%s'" % ( name , self . name ) )
7421	def fetch_cluster_se ( data , samfile , chrom , rstart , rend ) : overlap_buffer = data . _hackersonly [ "min_SE_refmap_overlap" ] rstart_buff = rstart + overlap_buffer rend_buff = rend - overlap_buffer if rstart_buff > rend_buff : tmp = rstart_buff rstart_buff = rend_buff rend_buff = tmp if rstart_buff == rend_buff : rend_buff += 1 rdict = { } clust = [ ] iterreg = [ ] iterreg = samfile . fetch ( chrom , rstart_buff , rend_buff ) for read in iterreg : if read . qname not in rdict : rdict [ read . qname ] = read sfunc = lambda x : int ( x . split ( ";size=" ) [ 1 ] . split ( ";" ) [ 0 ] ) rkeys = sorted ( rdict . keys ( ) , key = sfunc , reverse = True ) try : read1 = rdict [ rkeys [ 0 ] ] except ValueError : LOGGER . error ( "Found bad cluster, skipping - key:{} rdict:{}" . format ( rkeys [ 0 ] , rdict ) ) return "" poss = read1 . get_reference_positions ( full_length = True ) seed_r1start = min ( poss ) seed_r1end = max ( poss ) if read1 . is_reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq size = sfunc ( rkeys [ 0 ] ) clust . append ( ">{}:{}:{};size={};*\n{}" . format ( chrom , seed_r1start , seed_r1end , size , seq ) ) if len ( rkeys ) > 1 : for key in rkeys [ 1 : ] : skip = False try : read1 = rdict [ key ] except ValueError : read1 = rdict [ key ] [ 0 ] skip = True if not skip : poss = read1 . get_reference_positions ( full_length = True ) minpos = min ( poss ) maxpos = max ( poss ) if read1 . is_reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq size = sfunc ( key ) clust . append ( ">{}:{}:{};size={};+\n{}" . format ( chrom , minpos , maxpos , size , seq ) ) else : pass return clust
9941	def collect ( self ) : if self . symlink and not self . local : raise CommandError ( "Can't symlink to a remote destination." ) if self . clear : self . clear_dir ( '' ) if self . symlink : handler = self . link_file else : handler = self . copy_file found_files = OrderedDict ( ) for finder in get_finders ( ) : for path , storage in finder . list ( self . ignore_patterns ) : if getattr ( storage , 'prefix' , None ) : prefixed_path = os . path . join ( storage . prefix , path ) else : prefixed_path = path if prefixed_path not in found_files : found_files [ prefixed_path ] = ( storage , path ) handler ( path , prefixed_path , storage ) if self . post_process and hasattr ( self . storage , 'post_process' ) : processor = self . storage . post_process ( found_files , dry_run = self . dry_run ) for original_path , processed_path , processed in processor : if isinstance ( processed , Exception ) : self . stderr . write ( "Post-processing '%s' failed!" % original_path ) self . stderr . write ( "" ) raise processed if processed : self . log ( "Post-processed '%s' as '%s'" % ( original_path , processed_path ) , level = 1 ) self . post_processed_files . append ( original_path ) else : self . log ( "Skipped post-processing '%s'" % original_path ) return { 'modified' : self . copied_files + self . symlinked_files , 'unmodified' : self . unmodified_files , 'post_processed' : self . post_processed_files , }
7291	def set_fields ( self ) : if self . is_initialized : self . model_map_dict = self . create_document_dictionary ( self . model_instance ) else : self . model_map_dict = self . create_document_dictionary ( self . model ) form_field_dict = self . get_form_field_dict ( self . model_map_dict ) self . set_form_fields ( form_field_dict )
6840	def distrib_family ( ) : distrib = ( distrib_id ( ) or '' ) . lower ( ) if distrib in [ 'debian' , 'ubuntu' , 'linuxmint' , 'elementary os' ] : return DEBIAN elif distrib in [ 'redhat' , 'rhel' , 'centos' , 'sles' , 'fedora' ] : return REDHAT elif distrib in [ 'sunos' ] : return SUN elif distrib in [ 'gentoo' ] : return GENTOO elif distrib in [ 'arch' , 'manjarolinux' ] : return ARCH return 'other'
13427	def delete_messages ( self , messages ) : url = "/2/messages/?%s" % urlencode ( [ ( 'ids' , "," . join ( messages ) ) ] ) data = self . _delete_resource ( url ) return data
5511	def register_memory ( ) : def get_mem ( proc ) : if os . name == 'posix' : mem = proc . memory_info_ex ( ) counter = mem . rss if 'shared' in mem . _fields : counter -= mem . shared return counter else : return proc . get_memory_info ( ) . rss if SERVER_PROC is not None : mem = get_mem ( SERVER_PROC ) for child in SERVER_PROC . children ( ) : mem += get_mem ( child ) server_memory . append ( bytes2human ( mem ) )
8570	def get_location ( self , location_id , depth = 0 ) : response = self . _perform_request ( '/locations/%s?depth=%s' % ( location_id , depth ) ) return response
4403	def mget ( self , keys , * args ) : args = list_or_args ( keys , args ) server_keys = { } ret_dict = { } for key in args : server_name = self . get_server_name ( key ) server_keys [ server_name ] = server_keys . get ( server_name , [ ] ) server_keys [ server_name ] . append ( key ) for server_name , sub_keys in iteritems ( server_keys ) : values = self . connections [ server_name ] . mget ( sub_keys ) ret_dict . update ( dict ( zip ( sub_keys , values ) ) ) result = [ ] for key in args : result . append ( ret_dict . get ( key , None ) ) return result
7128	def inv_entry_to_path ( data ) : path_tuple = data [ 2 ] . split ( "#" ) if len ( path_tuple ) > 1 : path_str = "#" . join ( ( path_tuple [ 0 ] , path_tuple [ - 1 ] ) ) else : path_str = data [ 2 ] return path_str
6418	def sim_editex ( src , tar , cost = ( 0 , 1 , 2 ) , local = False ) : return Editex ( ) . sim ( src , tar , cost , local )
4254	def time_zone_by_country_and_region ( country_code , region_code = None ) : timezone = country_dict . get ( country_code ) if not timezone : return None if isinstance ( timezone , str ) : return timezone return timezone . get ( region_code )
11232	def run_excel_to_html ( ) : parser = argparse . ArgumentParser ( prog = 'excel_to_html' ) parser . add_argument ( '-p' , nargs = '?' , help = 'Path to an excel file for conversion.' ) parser . add_argument ( '-s' , nargs = '?' , help = 'The name of a sheet in our excel file. Defaults to "Sheet1".' , ) parser . add_argument ( '-css' , nargs = '?' , help = 'Space separated css classes to append to the table.' ) parser . add_argument ( '-m' , action = 'store_true' , help = 'Merge, attempt to combine merged cells.' ) parser . add_argument ( '-c' , nargs = '?' , help = 'Caption for creating an accessible table.' ) parser . add_argument ( '-d' , nargs = '?' , help = 'Two strings separated by a | character. The first string \ is for the html "summary" attribute and the second string is for the html "details" attribute. \ both values must be provided and nothing more.' , ) parser . add_argument ( '-r' , action = 'store_true' , help = 'Row headers. Does the table have row headers?' ) args = parser . parse_args ( ) inputs = { 'p' : args . p , 's' : args . s , 'css' : args . css , 'm' : args . m , 'c' : args . c , 'd' : args . d , 'r' : args . r , } p = inputs [ 'p' ] s = inputs [ 's' ] if inputs [ 's' ] else 'Sheet1' css = inputs [ 'css' ] if inputs [ 'css' ] else '' m = inputs [ 'm' ] if inputs [ 'm' ] else False c = inputs [ 'c' ] if inputs [ 'c' ] else '' d = inputs [ 'd' ] . split ( '|' ) if inputs [ 'd' ] else [ ] r = inputs [ 'r' ] if inputs [ 'r' ] else False html = fp . excel_to_html ( p , sheetname = s , css_classes = css , caption = c , details = d , row_headers = r , merge = m ) print ( html )
9411	def _encode ( data , convert_to_float ) : ctf = convert_to_float if isinstance ( data , ( OctaveVariablePtr ) ) : return _encode ( data . value , ctf ) if isinstance ( data , OctaveUserClass ) : return _encode ( OctaveUserClass . to_value ( data ) , ctf ) if isinstance ( data , ( OctaveFunctionPtr , MatlabFunction ) ) : raise Oct2PyError ( 'Cannot write Octave functions' ) if isinstance ( data , MatlabObject ) : view = data . view ( np . ndarray ) out = MatlabObject ( data , data . classname ) for name in out . dtype . names : out [ name ] = _encode ( view [ name ] , ctf ) return out if isinstance ( data , ( DataFrame , Series ) ) : return _encode ( data . values , ctf ) if isinstance ( data , dict ) : out = dict ( ) for ( key , value ) in data . items ( ) : out [ key ] = _encode ( value , ctf ) return out if data is None : return np . NaN if isinstance ( data , set ) : return _encode ( list ( data ) , ctf ) if isinstance ( data , list ) : if _is_simple_numeric ( data ) : return _encode ( np . array ( data ) , ctf ) return _encode ( tuple ( data ) , ctf ) if isinstance ( data , tuple ) : obj = np . empty ( len ( data ) , dtype = object ) for ( i , item ) in enumerate ( data ) : obj [ i ] = _encode ( item , ctf ) return obj if isinstance ( data , spmatrix ) : return data . astype ( np . float64 ) if not isinstance ( data , np . ndarray ) : return data if data . dtype . kind in 'OV' : out = np . empty ( data . size , dtype = data . dtype ) for ( i , item ) in enumerate ( data . ravel ( ) ) : if data . dtype . names : for name in data . dtype . names : out [ i ] [ name ] = _encode ( item [ name ] , ctf ) else : out [ i ] = _encode ( item , ctf ) return out . reshape ( data . shape ) if data . dtype . name == 'complex256' : return data . astype ( np . complex128 ) if ctf and data . dtype . kind in 'ui' : return data . astype ( np . float64 ) return data
8086	def strokewidth ( self , w = None ) : if w is not None : self . _canvas . strokewidth = w else : return self . _canvas . strokewidth
5163	def __intermediate_address ( self , address ) : for key in self . _address_keys : if key in address : del address [ key ] return address
4058	def _bib_processor ( self , retrieved ) : items = [ ] for bib in retrieved . entries : items . append ( bib [ "content" ] [ 0 ] [ "value" ] ) self . url_params = None return items
4689	def init_aes ( shared_secret , nonce ) : " Shared Secret " ss = hashlib . sha512 ( unhexlify ( shared_secret ) ) . digest ( ) " Seed " seed = bytes ( str ( nonce ) , "ascii" ) + hexlify ( ss ) seed_digest = hexlify ( hashlib . sha512 ( seed ) . digest ( ) ) . decode ( "ascii" ) " AES " key = unhexlify ( seed_digest [ 0 : 64 ] ) iv = unhexlify ( seed_digest [ 64 : 96 ] ) return AES . new ( key , AES . MODE_CBC , iv )
2585	def migrate_tasks_to_internal ( self , kill_event ) : logger . info ( "[TASK_PULL_THREAD] Starting" ) task_counter = 0 poller = zmq . Poller ( ) poller . register ( self . task_incoming , zmq . POLLIN ) while not kill_event . is_set ( ) : try : msg = self . task_incoming . recv_pyobj ( ) except zmq . Again : logger . debug ( "[TASK_PULL_THREAD] {} tasks in internal queue" . format ( self . pending_task_queue . qsize ( ) ) ) continue if msg == 'STOP' : kill_event . set ( ) break else : self . pending_task_queue . put ( msg ) task_counter += 1 logger . debug ( "[TASK_PULL_THREAD] Fetched task:{}" . format ( task_counter ) )
2685	def fate ( name ) : return cached_download ( 'http://fate.ffmpeg.org/fate-suite/' + name , os . path . join ( 'fate-suite' , name . replace ( '/' , os . path . sep ) ) )
6731	def create_module ( name , code = None ) : if name not in sys . modules : sys . modules [ name ] = imp . new_module ( name ) module = sys . modules [ name ] if code : print ( 'executing code for %s: %s' % ( name , code ) ) exec ( code in module . __dict__ ) exec ( "from %s import %s" % ( name , '*' ) ) return module
2582	def load_checkpoints ( self , checkpointDirs ) : self . memo_lookup_table = None if not checkpointDirs : return { } if type ( checkpointDirs ) is not list : raise BadCheckpoint ( "checkpointDirs expects a list of checkpoints" ) return self . _load_checkpoints ( checkpointDirs )
564	def toDict ( self ) : def items2dict ( items ) : d = { } for k , v in items . items ( ) : d [ k ] = v . __dict__ return d self . invariant ( ) return dict ( description = self . description , singleNodeOnly = self . singleNodeOnly , inputs = items2dict ( self . inputs ) , outputs = items2dict ( self . outputs ) , parameters = items2dict ( self . parameters ) , commands = items2dict ( self . commands ) )
3810	async def connect ( self ) : proxy = os . environ . get ( 'HTTP_PROXY' ) self . _session = http_utils . Session ( self . _cookies , proxy = proxy ) try : self . _channel = channel . Channel ( self . _session , self . _max_retries , self . _retry_backoff_base ) self . _channel . on_connect . add_observer ( self . on_connect . fire ) self . _channel . on_reconnect . add_observer ( self . on_reconnect . fire ) self . _channel . on_disconnect . add_observer ( self . on_disconnect . fire ) self . _channel . on_receive_array . add_observer ( self . _on_receive_array ) self . _listen_future = asyncio . ensure_future ( self . _channel . listen ( ) ) try : await self . _listen_future except asyncio . CancelledError : self . _listen_future . cancel ( ) logger . info ( 'Client.connect returning because Channel.listen returned' ) finally : await self . _session . close ( )
9442	def reload_cache_config ( self , call_params ) : path = '/' + self . api_version + '/ReloadCacheConfig/' method = 'POST' return self . request ( path , method , call_params )
8932	def capture ( cmd , ** kw ) : kw = kw . copy ( ) kw [ 'hide' ] = 'out' if not kw . get ( 'echo' , False ) : kw [ 'echo' ] = False ignore_failures = kw . pop ( 'ignore_failures' , False ) try : return invoke_run ( cmd , ** kw ) . stdout . strip ( ) except exceptions . Failure as exc : if not ignore_failures : notify . error ( "Command `{}` failed with RC={}!" . format ( cmd , exc . result . return_code , ) ) raise
13372	def expandpath ( path ) : return os . path . abspath ( os . path . expandvars ( os . path . expanduser ( path ) ) )
10292	def expand_internal ( universe : BELGraph , graph : BELGraph , edge_predicates : EdgePredicates = None ) -> None : edge_filter = and_edge_predicates ( edge_predicates ) for u , v in itt . product ( graph , repeat = 2 ) : if graph . has_edge ( u , v ) or not universe . has_edge ( u , v ) : continue rs = defaultdict ( list ) for key , data in universe [ u ] [ v ] . items ( ) : if not edge_filter ( universe , u , v , key ) : continue rs [ data [ RELATION ] ] . append ( ( key , data ) ) if 1 == len ( rs ) : relation = list ( rs ) [ 0 ] for key , data in rs [ relation ] : graph . add_edge ( u , v , key = key , ** data ) else : log . debug ( 'Multiple relationship types found between %s and %s' , u , v )
8910	def owsproxy_delegate ( request ) : twitcher_url = request . registry . settings . get ( 'twitcher.url' ) protected_path = request . registry . settings . get ( 'twitcher.ows_proxy_protected_path' , '/ows' ) url = twitcher_url + protected_path + '/proxy' if request . matchdict . get ( 'service_name' ) : url += '/' + request . matchdict . get ( 'service_name' ) if request . matchdict . get ( 'access_token' ) : url += '/' + request . matchdict . get ( 'service_name' ) url += '?' + urlparse . urlencode ( request . params ) LOGGER . debug ( "delegate to owsproxy: %s" , url ) resp = requests . request ( method = request . method . upper ( ) , url = url , data = request . body , headers = request . headers , verify = False ) return Response ( resp . content , status = resp . status_code , headers = resp . headers )
3379	def add_lp_feasibility ( model ) : obj_vars = [ ] prob = model . problem for met in model . metabolites : s_plus = prob . Variable ( "s_plus_" + met . id , lb = 0 ) s_minus = prob . Variable ( "s_minus_" + met . id , lb = 0 ) model . add_cons_vars ( [ s_plus , s_minus ] ) model . constraints [ met . id ] . set_linear_coefficients ( { s_plus : 1.0 , s_minus : - 1.0 } ) obj_vars . append ( s_plus ) obj_vars . append ( s_minus ) model . objective = prob . Objective ( Zero , sloppy = True , direction = "min" ) model . objective . set_linear_coefficients ( { v : 1.0 for v in obj_vars } )
7046	def _bls_runner ( times , mags , nfreq , freqmin , stepsize , nbins , minduration , maxduration ) : workarr_u = npones ( times . size ) workarr_v = npones ( times . size ) blsresult = eebls ( times , mags , workarr_u , workarr_v , nfreq , freqmin , stepsize , nbins , minduration , maxduration ) return { 'power' : blsresult [ 0 ] , 'bestperiod' : blsresult [ 1 ] , 'bestpower' : blsresult [ 2 ] , 'transdepth' : blsresult [ 3 ] , 'transduration' : blsresult [ 4 ] , 'transingressbin' : blsresult [ 5 ] , 'transegressbin' : blsresult [ 6 ] }
7278	def play_sync ( self ) : self . play ( ) logger . info ( "Playing synchronously" ) try : time . sleep ( 0.05 ) logger . debug ( "Wait for playing to start" ) while self . is_playing ( ) : time . sleep ( 0.05 ) except DBusException : logger . error ( "Cannot play synchronously any longer as DBus calls timed out." )
396	def choice_action_by_probs ( probs = ( 0.5 , 0.5 ) , action_list = None ) : if action_list is None : n_action = len ( probs ) action_list = np . arange ( n_action ) else : if len ( action_list ) != len ( probs ) : raise Exception ( "number of actions should equal to number of probabilities." ) return np . random . choice ( action_list , p = probs )
9009	def next_instruction_in_row ( self ) : index = self . index_in_row + 1 if index >= len ( self . row_instructions ) : return None return self . row_instructions [ index ]
6625	def availableTags ( self ) : return [ GithubComponentVersion ( '' , t [ 0 ] , t [ 1 ] , self . name , cache_key = _createCacheKey ( 'tag' , t [ 0 ] , t [ 1 ] , self . name ) ) for t in self . _getTags ( ) ]
12777	def resorted ( values ) : if not values : return values values = sorted ( values ) first_word = next ( ( cnt for cnt , val in enumerate ( values ) if val and not val [ 0 ] . isdigit ( ) ) , None ) if first_word is None : return values words = values [ first_word : ] numbers = values [ : first_word ] return words + numbers
1133	def getlines ( filename , module_globals = None ) : if filename in cache : return cache [ filename ] [ 2 ] try : return updatecache ( filename , module_globals ) except MemoryError : clearcache ( ) return [ ]
11767	def weighted_sample_with_replacement ( seq , weights , n ) : sample = weighted_sampler ( seq , weights ) return [ sample ( ) for s in range ( n ) ]
11754	def parse_definite_clause ( s ) : "Return the antecedents and the consequent of a definite clause." assert is_definite_clause ( s ) if is_symbol ( s . op ) : return [ ] , s else : antecedent , consequent = s . args return conjuncts ( antecedent ) , consequent
13864	def tsms ( when , tz = None ) : if not when : return None when = totz ( when , tz ) return calendar . timegm ( when . timetuple ( ) ) * 1000 + int ( round ( when . microsecond / 1000.0 ) )
1160	def notify ( self , n = 1 ) : if not self . _is_owned ( ) : raise RuntimeError ( "cannot notify on un-acquired lock" ) __waiters = self . __waiters waiters = __waiters [ : n ] if not waiters : if __debug__ : self . _note ( "%s.notify(): no waiters" , self ) return self . _note ( "%s.notify(): notifying %d waiter%s" , self , n , n != 1 and "s" or "" ) for waiter in waiters : waiter . release ( ) try : __waiters . remove ( waiter ) except ValueError : pass
4004	def get_authed_registries ( ) : result = set ( ) if not os . path . exists ( constants . DOCKER_CONFIG_PATH ) : return result config = json . load ( open ( constants . DOCKER_CONFIG_PATH , 'r' ) ) for registry in config . get ( 'auths' , { } ) . iterkeys ( ) : try : parsed = urlparse ( registry ) except Exception : log_to_client ( 'Error parsing registry {} from Docker config, will skip this registry' ) . format ( registry ) result . add ( parsed . netloc ) if parsed . netloc else result . add ( parsed . path ) return result
10591	def create_transaction ( self , name , description = None , tx_date = datetime . min . date ( ) , dt_account = None , cr_account = None , source = None , amount = 0.00 ) : new_tx = Transaction ( name , description , tx_date , dt_account , cr_account , source , amount ) self . transactions . append ( new_tx ) return new_tx
6244	def draw ( self , projection_matrix = None , view_matrix = None , camera_matrix = None , time = 0 ) : if self . mesh_program : self . mesh_program . draw ( self , projection_matrix = projection_matrix , view_matrix = view_matrix , camera_matrix = camera_matrix , time = time )
11693	def label_suspicious ( self , reason ) : self . suspicion_reasons . append ( reason ) self . is_suspect = True
7953	def handle_write ( self ) : with self . lock : logger . debug ( "handle_write: queue: {0!r}" . format ( self . _write_queue ) ) try : job = self . _write_queue . popleft ( ) except IndexError : return if isinstance ( job , WriteData ) : self . _do_write ( job . data ) elif isinstance ( job , ContinueConnect ) : self . _continue_connect ( ) elif isinstance ( job , StartTLS ) : self . _initiate_starttls ( ** job . kwargs ) elif isinstance ( job , TLSHandshake ) : self . _continue_tls_handshake ( ) else : raise ValueError ( "Unrecognized job in the write queue: " "{0!r}" . format ( job ) )
6221	def set_position ( self , x , y , z ) : self . position = Vector3 ( [ x , y , z ] )
12771	def settle_to_markers ( self , frame_no = 0 , max_distance = 0.05 , max_iters = 300 , states = None ) : if states is not None : self . skeleton . set_body_states ( states ) dist = None for _ in range ( max_iters ) : for _ in self . _step_to_marker_frame ( frame_no ) : pass dist = np . nanmean ( abs ( self . markers . distances ( ) ) ) logging . info ( 'settling to frame %d: marker distance %.3f' , frame_no , dist ) if dist < max_distance : return self . skeleton . get_body_states ( ) for b in self . skeleton . bodies : b . linear_velocity = 0 , 0 , 0 b . angular_velocity = 0 , 0 , 0 return states
12823	def fspaths ( draw , allow_pathlike = None ) : has_pathlike = hasattr ( os , 'PathLike' ) if allow_pathlike is None : allow_pathlike = has_pathlike if allow_pathlike and not has_pathlike : raise InvalidArgument ( 'allow_pathlike: os.PathLike not supported, use None instead ' 'to enable it only when available' ) result_type = draw ( sampled_from ( [ bytes , text_type ] ) ) def tp ( s = '' ) : return _str_to_path ( s , result_type ) special_component = sampled_from ( [ tp ( os . curdir ) , tp ( os . pardir ) ] ) normal_component = _filename ( result_type ) path_component = one_of ( normal_component , special_component ) extension = normal_component . map ( lambda f : tp ( os . extsep ) + f ) root = _path_root ( result_type ) def optional ( st ) : return one_of ( st , just ( result_type ( ) ) ) sep = sampled_from ( [ os . sep , os . altsep or os . sep ] ) . map ( tp ) path_part = builds ( lambda s , l : s . join ( l ) , sep , lists ( path_component ) ) main_strategy = builds ( lambda * x : tp ( ) . join ( x ) , optional ( root ) , path_part , optional ( extension ) ) if allow_pathlike and hasattr ( os , 'fspath' ) : pathlike_strategy = main_strategy . map ( lambda p : _PathLike ( p ) ) main_strategy = one_of ( main_strategy , pathlike_strategy ) return draw ( main_strategy )
3891	def html ( tag ) : return ( HTML_START . format ( tag = tag ) , HTML_END . format ( tag = tag ) )
11345	def handle_starttag ( self , tag , attrs ) : if tag in self . mathml_elements : final_attr = "" for key , value in attrs : final_attr += ' {0}="{1}"' . format ( key , value ) self . fed . append ( "<{0}{1}>" . format ( tag , final_attr ) )
4959	def get_course_runs_from_program ( program ) : course_runs = set ( ) for course in program . get ( "courses" , [ ] ) : for run in course . get ( "course_runs" , [ ] ) : if "key" in run and run [ "key" ] : course_runs . add ( run [ "key" ] ) return course_runs
8407	def expand_range ( range , mul = 0 , add = 0 , zero_width = 1 ) : x = range try : x [ 0 ] except TypeError : x = ( x , x ) if zero_range ( x ) : new = x [ 0 ] - zero_width / 2 , x [ 0 ] + zero_width / 2 else : dx = ( x [ 1 ] - x [ 0 ] ) * mul + add new = x [ 0 ] - dx , x [ 1 ] + dx return new
5362	def validate_config ( self ) : for key , key_config in self . params_map . items ( ) : if key_config [ 'required' ] : if key not in self . config : raise ValueError ( "Invalid Configuration! Required parameter '%s' was not provided to Sultan." ) for key in self . config . keys ( ) : if key not in self . params_map : raise ValueError ( "Invalid Configuration! The parameter '%s' provided is not used by Sultan!" % key )
8509	def fit ( self , X , y = None ) : from pylearn2 . config import yaml_parse from pylearn2 . train import Train params = self . get_params ( ) yaml_string = Template ( self . yaml_string ) . substitute ( params ) self . trainer = yaml_parse . load ( yaml_string ) assert isinstance ( self . trainer , Train ) if self . trainer . dataset is not None : raise ValueError ( 'Train YAML database must evaluate to None.' ) self . trainer . dataset = self . _get_dataset ( X , y ) if ( hasattr ( self . trainer . algorithm , 'monitoring_dataset' ) and self . trainer . algorithm . monitoring_dataset is not None ) : monitoring_dataset = self . trainer . algorithm . monitoring_dataset if len ( monitoring_dataset ) == 1 and '' in monitoring_dataset : monitoring_dataset [ '' ] = self . trainer . dataset else : monitoring_dataset [ 'train' ] = self . trainer . dataset self . trainer . algorithm . _set_monitoring_dataset ( monitoring_dataset ) else : self . trainer . algorithm . _set_monitoring_dataset ( self . trainer . dataset ) self . trainer . main_loop ( )
8493	def _handle_module ( args ) : module = _get_module_filename ( args . module ) if not module : _error ( "Could not load module or package: %r" , args . module ) elif isinstance ( module , Unparseable ) : _error ( "Could not determine module source: %r" , args . module ) _parse_and_output ( module , args )
7404	def below ( self , ref ) : if not self . _valid_ordering_reference ( ref ) : raise ValueError ( "%r can only be moved below instances of %r which %s equals %r." % ( self , self . __class__ , self . order_with_respect_to , self . _get_order_with_respect_to ( ) ) ) if self . order == ref . order : return if self . order > ref . order : o = self . get_ordering_queryset ( ) . filter ( order__gt = ref . order ) . aggregate ( Min ( 'order' ) ) . get ( 'order__min' ) or 0 else : o = ref . order self . to ( o )
13199	def format_content ( self , format = 'plain' , mathjax = False , smart = True , extra_args = None ) : output_text = convert_lsstdoc_tex ( self . _tex , format , mathjax = mathjax , smart = smart , extra_args = extra_args ) return output_text
10804	def resolve_admin_type ( admin ) : if admin is current_user or isinstance ( admin , UserMixin ) : return 'User' else : return admin . __class__ . __name__
11178	def get_separator ( self , i ) : return i and self . separator [ min ( i - 1 , len ( self . separator ) - 1 ) ] or ''
1052	def print_stack ( f = None , limit = None , file = None ) : if f is None : try : raise ZeroDivisionError except ZeroDivisionError : f = sys . exc_info ( ) [ 2 ] . tb_frame . f_back print_list ( extract_stack ( f , limit ) , file )
4960	def get_earliest_start_date_from_program ( program ) : start_dates = [ ] for course in program . get ( 'courses' , [ ] ) : for run in course . get ( 'course_runs' , [ ] ) : if run . get ( 'start' ) : start_dates . append ( parse_lms_api_datetime ( run [ 'start' ] ) ) if not start_dates : return None return min ( start_dates )
12617	def get_data ( img ) : if hasattr ( img , '_data_cache' ) and img . _data_cache is None : img = copy . deepcopy ( img ) gc . collect ( ) return img . get_data ( )
427	def augment_with_ngrams ( unigrams , unigram_vocab_size , n_buckets , n = 2 ) : def get_ngrams ( n ) : return list ( zip ( * [ unigrams [ i : ] for i in range ( n ) ] ) ) def hash_ngram ( ngram ) : bytes_ = array . array ( 'L' , ngram ) . tobytes ( ) hash_ = int ( hashlib . sha256 ( bytes_ ) . hexdigest ( ) , 16 ) return unigram_vocab_size + hash_ % n_buckets return unigrams + [ hash_ngram ( ngram ) for i in range ( 2 , n + 1 ) for ngram in get_ngrams ( i ) ]
13101	def main ( ) : config = Config ( ) core = HostSearch ( ) hosts = core . get_hosts ( tags = [ '!nessus' ] , up = True ) hosts = [ host for host in hosts ] host_ips = "," . join ( [ str ( host . address ) for host in hosts ] ) url = config . get ( 'nessus' , 'host' ) access = config . get ( 'nessus' , 'access_key' ) secret = config . get ( 'nessus' , 'secret_key' ) template_name = config . get ( 'nessus' , 'template_name' ) nessus = Nessus ( access , secret , url , template_name ) scan_id = nessus . create_scan ( host_ips ) nessus . start_scan ( scan_id ) for host in hosts : host . add_tag ( 'nessus' ) host . save ( ) Logger ( ) . log ( "nessus" , "Nessus scan started on {} hosts" . format ( len ( hosts ) ) , { 'scanned_hosts' : len ( hosts ) } )
8209	def coordinates ( self , x0 , y0 , distance , angle ) : x = x0 + cos ( radians ( angle ) ) * distance y = y0 + sin ( radians ( angle ) ) * distance return Point ( x , y )
2958	def _assure_dir ( self ) : try : os . makedirs ( self . _state_dir ) except OSError as err : if err . errno != errno . EEXIST : raise
2339	def weighted_mean_and_std ( values , weights ) : average = np . average ( values , weights = weights , axis = 0 ) variance = np . dot ( weights , ( values - average ) ** 2 ) / weights . sum ( ) return ( average , np . sqrt ( variance ) )
2406	def get_algorithms ( algorithm ) : if algorithm == util_functions . AlgorithmTypes . classification : clf = sklearn . ensemble . GradientBoostingClassifier ( n_estimators = 100 , learn_rate = .05 , max_depth = 4 , random_state = 1 , min_samples_leaf = 3 ) clf2 = sklearn . ensemble . GradientBoostingClassifier ( n_estimators = 100 , learn_rate = .05 , max_depth = 4 , random_state = 1 , min_samples_leaf = 3 ) else : clf = sklearn . ensemble . GradientBoostingRegressor ( n_estimators = 100 , learn_rate = .05 , max_depth = 4 , random_state = 1 , min_samples_leaf = 3 ) clf2 = sklearn . ensemble . GradientBoostingRegressor ( n_estimators = 100 , learn_rate = .05 , max_depth = 4 , random_state = 1 , min_samples_leaf = 3 ) return clf , clf2
6151	def fir_remez_hpf ( f_stop , f_pass , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : f_pass_eq = fs / 2. - f_pass f_stop_eq = fs / 2. - f_stop n , ff , aa , wts = lowpass_order ( f_pass_eq , f_stop_eq , d_pass , d_stop , fsamp = fs ) N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 ) n = np . arange ( len ( b ) ) b *= ( - 1 ) ** n print ( 'Remez filter taps = %d.' % N_taps ) return b
7016	def concat_write_pklc ( lcbasedir , objectid , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , outdir = None , recursive = True ) : concatlcd = concatenate_textlcs_for_objectid ( lcbasedir , objectid , aperture = aperture , sortby = sortby , normalize = normalize , recursive = recursive ) if not outdir : outdir = 'pklcs' if not os . path . exists ( outdir ) : os . mkdir ( outdir ) outfpath = os . path . join ( outdir , '%s-%s-pklc.pkl' % ( concatlcd [ 'objectid' ] , aperture ) ) pklc = lcdict_to_pickle ( concatlcd , outfile = outfpath ) return pklc
2901	def complete_task_from_id ( self , task_id ) : if task_id is None : raise WorkflowException ( self . spec , 'task_id is None' ) for task in self . task_tree : if task . id == task_id : return task . complete ( ) msg = 'A task with the given task_id (%s) was not found' % task_id raise WorkflowException ( self . spec , msg )
8643	def post_track ( session , user_id , project_id , latitude , longitude ) : tracking_data = { 'user_id' : user_id , 'project_id' : project_id , 'track_point' : { 'latitude' : latitude , 'longitude' : longitude } } response = make_post_request ( session , 'tracks' , json_data = tracking_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise TrackNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
2243	def argval ( key , default = util_const . NoParam , argv = None ) : if argv is None : argv = sys . argv keys = [ key ] if isinstance ( key , six . string_types ) else key n_max = len ( argv ) - 1 for argx , item in enumerate ( argv ) : for key_ in keys : if item == key_ : if argx < n_max : value = argv [ argx + 1 ] return value elif item . startswith ( key_ + '=' ) : value = '=' . join ( item . split ( '=' ) [ 1 : ] ) return value value = default return value
4937	def transform_description ( self , content_metadata_item ) : full_description = content_metadata_item . get ( 'full_description' ) or '' if 0 < len ( full_description ) <= self . LONG_STRING_LIMIT : return full_description return content_metadata_item . get ( 'short_description' ) or content_metadata_item . get ( 'title' ) or ''
1536	def get_heron_options_from_env ( ) : heron_options_raw = os . environ . get ( "HERON_OPTIONS" ) if heron_options_raw is None : raise RuntimeError ( "HERON_OPTIONS environment variable not found" ) options = { } for option_line in heron_options_raw . replace ( "%%%%" , " " ) . split ( ',' ) : key , sep , value = option_line . partition ( "=" ) if sep : options [ key ] = value else : raise ValueError ( "Invalid HERON_OPTIONS part %r" % option_line ) return options
8003	def get_form ( self , form_type = "form" ) : if self . form : if self . form . type != form_type : raise ValueError ( "Bad form type in the jabber:iq:register element" ) return self . form form = Form ( form_type , instructions = self . instructions ) form . add_field ( "FORM_TYPE" , [ u"jabber:iq:register" ] , "hidden" ) for field in legacy_fields : field_type , field_label = legacy_fields [ field ] value = getattr ( self , field ) if value is None : continue if form_type == "form" : if not value : value = None form . add_field ( name = field , field_type = field_type , label = field_label , value = value , required = True ) else : form . add_field ( name = field , value = value ) return form
8857	def on_run ( self ) : filename = self . tabWidget . current_widget ( ) . file . path wd = os . path . dirname ( filename ) args = Settings ( ) . get_run_config_for_file ( filename ) self . interactiveConsole . start_process ( Settings ( ) . interpreter , args = [ filename ] + args , cwd = wd ) self . dockWidget . show ( ) self . actionRun . setEnabled ( False ) self . actionConfigure_run . setEnabled ( False )
13127	def get_pipe ( self ) : lines = [ ] for line in sys . stdin : try : lines . append ( self . line_to_object ( line . strip ( ) ) ) except ValueError : pass except KeyError : pass return lines
5502	def timeline ( ctx , pager , limit , twtfile , sorting , timeout , porcelain , source , cache , force_update ) : if source : source_obj = ctx . obj [ "conf" ] . get_source_by_nick ( source ) if not source_obj : logger . debug ( "Not following {0}, trying as URL" . format ( source ) ) source_obj = Source ( source , source ) sources = [ source_obj ] else : sources = ctx . obj [ "conf" ] . following tweets = [ ] if cache : try : with Cache . discover ( update_interval = ctx . obj [ "conf" ] . timeline_update_interval ) as cache : force_update = force_update or not cache . is_valid if force_update : tweets = get_remote_tweets ( sources , limit , timeout , cache ) else : logger . debug ( "Multiple calls to 'timeline' within {0} seconds. Skipping update" . format ( cache . update_interval ) ) tweets = list ( chain . from_iterable ( [ cache . get_tweets ( source . url ) for source in sources ] ) ) except OSError as e : logger . debug ( e ) tweets = get_remote_tweets ( sources , limit , timeout ) else : tweets = get_remote_tweets ( sources , limit , timeout ) if twtfile and not source : source = Source ( ctx . obj [ "conf" ] . nick , ctx . obj [ "conf" ] . twturl , file = twtfile ) tweets . extend ( get_local_tweets ( source , limit ) ) if not tweets : return tweets = sort_and_truncate_tweets ( tweets , sorting , limit ) if pager : click . echo_via_pager ( style_timeline ( tweets , porcelain ) ) else : click . echo ( style_timeline ( tweets , porcelain ) )
9848	def _load_dx ( self , filename ) : dx = OpenDX . field ( 0 ) dx . read ( filename ) grid , edges = dx . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
2188	def _get_certificate ( self , cfgstr = None ) : certificate = self . cacher . tryload ( cfgstr = cfgstr ) return certificate
4374	def get_messages_payload ( self , socket , timeout = None ) : try : msgs = socket . get_multiple_client_msgs ( timeout = timeout ) data = self . encode_payload ( msgs ) except Empty : data = "" return data
12781	def set_name ( self , name ) : if not self . _campfire . get_user ( ) . admin : return False result = self . _connection . put ( "room/%s" % self . id , { "room" : { "name" : name } } ) if result [ "success" ] : self . _load ( ) return result [ "success" ]
7060	def sqs_create_queue ( queue_name , options = None , client = None ) : if not client : client = boto3 . client ( 'sqs' ) try : if isinstance ( options , dict ) : resp = client . create_queue ( QueueName = queue_name , Attributes = options ) else : resp = client . create_queue ( QueueName = queue_name ) if resp is not None : return { 'url' : resp [ 'QueueUrl' ] , 'name' : queue_name } else : LOGERROR ( 'could not create the specified queue: %s with options: %s' % ( queue_name , options ) ) return None except Exception as e : LOGEXCEPTION ( 'could not create the specified queue: %s with options: %s' % ( queue_name , options ) ) return None
12169	def _dispatch ( self , event , listener , * args , ** kwargs ) : if ( asyncio . iscoroutinefunction ( listener ) or isinstance ( listener , functools . partial ) and asyncio . iscoroutinefunction ( listener . func ) ) : return self . _dispatch_coroutine ( event , listener , * args , ** kwargs ) return self . _dispatch_function ( event , listener , * args , ** kwargs )
7733	def make_kick_request ( self , nick , reason ) : self . clear_muc_child ( ) self . muc_child = MucAdminQuery ( parent = self . xmlnode ) item = MucItem ( "none" , "none" , nick = nick , reason = reason ) self . muc_child . add_item ( item ) return self . muc_child
4567	def _write ( self , filename , frames , fps , loop = 0 , palette = 256 ) : from PIL import Image images = [ ] for f in frames : data = open ( f , 'rb' ) . read ( ) images . append ( Image . open ( io . BytesIO ( data ) ) ) duration = round ( 1 / fps , 2 ) im = images . pop ( 0 ) im . save ( filename , save_all = True , append_images = images , duration = duration , loop = loop , palette = palette )
5168	def __intermediate_htmode ( self , radio ) : protocol = radio . pop ( 'protocol' ) channel_width = radio . pop ( 'channel_width' ) if 'htmode' in radio : return radio [ 'htmode' ] if protocol == '802.11n' : return 'HT{0}' . format ( channel_width ) elif protocol == '802.11ac' : return 'VHT{0}' . format ( channel_width ) return 'NONE'
2744	def load_by_pub_key ( self , public_key ) : data = self . get_data ( "account/keys/" ) for jsoned in data [ 'ssh_keys' ] : if jsoned . get ( 'public_key' , "" ) == public_key : self . id = jsoned [ 'id' ] self . load ( ) return self return None
1080	def isocalendar ( self ) : year = self . _year week1monday = _isoweek1monday ( year ) today = _ymd2ord ( self . _year , self . _month , self . _day ) week , day = divmod ( today - week1monday , 7 ) if week < 0 : year -= 1 week1monday = _isoweek1monday ( year ) week , day = divmod ( today - week1monday , 7 ) elif week >= 52 : if today >= _isoweek1monday ( year + 1 ) : year += 1 week = 0 return year , week + 1 , day + 1
12975	def _doSave ( self , obj , isInsert , conn , pipeline = None ) : if pipeline is None : pipeline = conn newDict = obj . asDict ( forStorage = True ) key = self . _get_key_for_id ( obj . _id ) if isInsert is True : for thisField in self . fields : fieldValue = newDict . get ( thisField , thisField . getDefaultValue ( ) ) pipeline . hset ( key , thisField , fieldValue ) if fieldValue == IR_NULL_STR : obj . _origData [ thisField ] = irNull else : obj . _origData [ thisField ] = object . __getattribute__ ( obj , str ( thisField ) ) self . _add_id_to_keys ( obj . _id , pipeline ) for indexedField in self . indexedFields : self . _add_id_to_index ( indexedField , obj . _id , obj . _origData [ indexedField ] , pipeline ) else : updatedFields = obj . getUpdatedFields ( ) for thisField , fieldValue in updatedFields . items ( ) : ( oldValue , newValue ) = fieldValue oldValueForStorage = thisField . toStorage ( oldValue ) newValueForStorage = thisField . toStorage ( newValue ) pipeline . hset ( key , thisField , newValueForStorage ) if thisField in self . indexedFields : self . _rem_id_from_index ( thisField , obj . _id , oldValueForStorage , pipeline ) self . _add_id_to_index ( thisField , obj . _id , newValueForStorage , pipeline ) obj . _origData [ thisField ] = newValue
7196	def ndwi ( self ) : data = self . _read ( self [ self . _ndwi_bands , ... ] ) . astype ( np . float32 ) return ( data [ 1 , : , : ] - data [ 0 , : , : ] ) / ( data [ 0 , : , : ] + data [ 1 , : , : ] )
3772	def phase_select_property ( phase = None , s = None , l = None , g = None , V_over_F = None ) : r if phase == 's' : return s elif phase == 'l' : return l elif phase == 'g' : return g elif phase == 'two-phase' : return None elif phase is None : return None else : raise Exception ( 'Property not recognized' )
7247	def status ( self , workflow_id ) : self . logger . debug ( 'Get status of workflow: ' + workflow_id ) url = '%(wf_url)s/%(wf_id)s' % { 'wf_url' : self . workflows_url , 'wf_id' : workflow_id } r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( ) [ 'state' ]
4091	def addSources ( self , * sources ) : self . _sources . extend ( sources ) debug . logger & debug . flagCompiler and debug . logger ( 'current MIB source(s): %s' % ', ' . join ( [ str ( x ) for x in self . _sources ] ) ) return self
12352	def rebuild ( self , image , wait = True ) : return self . _action ( 'rebuild' , image = image , wait = wait )
1094	def findall ( pattern , string , flags = 0 ) : return _compile ( pattern , flags ) . findall ( string ) def finditer ( pattern , string , flags = 0 ) : return _compile ( pattern , flags ) . finditer ( string )
4368	def error ( self , error_name , error_message , msg_id = None , quiet = False ) : self . socket . error ( error_name , error_message , endpoint = self . ns_name , msg_id = msg_id , quiet = quiet )
2558	def clean_pair ( cls , attribute , value ) : attribute = cls . clean_attribute ( attribute ) if value is True : value = attribute if value is False : value = "false" return ( attribute , value )
10314	def canonical_circulation ( elements : T , key : Optional [ Callable [ [ T ] , bool ] ] = None ) -> T : return min ( get_circulations ( elements ) , key = key )
9197	def get ( self , key , default = _sentinel ) : tup = self . _data . get ( key . lower ( ) ) if tup is not None : return tup [ 1 ] elif default is not _sentinel : return default else : return None
13646	def hump_to_underscore ( name ) : new_name = '' pos = 0 for c in name : if pos == 0 : new_name = c . lower ( ) elif 65 <= ord ( c ) <= 90 : new_name += '_' + c . lower ( ) pass else : new_name += c pos += 1 pass return new_name
2373	def statements ( self ) : if len ( self . rows ) == 0 : return [ ] current_statement = Statement ( self . rows [ 0 ] ) current_statement . startline = self . rows [ 0 ] . linenumber current_statement . endline = self . rows [ 0 ] . linenumber statements = [ ] for row in self . rows [ 1 : ] : if len ( row ) > 0 and row [ 0 ] == "..." : current_statement += row [ 1 : ] current_statement . endline = row . linenumber else : if len ( current_statement ) > 0 : statements . append ( current_statement ) current_statement = Statement ( row ) current_statement . startline = row . linenumber current_statement . endline = row . linenumber if len ( current_statement ) > 0 : statements . append ( current_statement ) while ( len ( statements [ - 1 ] ) == 0 or ( ( len ( statements [ - 1 ] ) == 1 ) and len ( statements [ - 1 ] [ 0 ] ) == 0 ) ) : statements . pop ( ) return statements
6456	def sim ( src , tar , method = sim_levenshtein ) : if callable ( method ) : return method ( src , tar ) else : raise AttributeError ( 'Unknown similarity function: ' + str ( method ) )
3569	def centralManager_didDiscoverPeripheral_advertisementData_RSSI_ ( self , manager , peripheral , data , rssi ) : logger . debug ( 'centralManager_didDiscoverPeripheral_advertisementData_RSSI called' ) device = device_list ( ) . get ( peripheral ) if device is None : device = device_list ( ) . add ( peripheral , CoreBluetoothDevice ( peripheral ) ) device . _update_advertised ( data )
3294	def is_locked ( self ) : if self . provider . lock_manager is None : return False return self . provider . lock_manager . is_url_locked ( self . get_ref_url ( ) )
9182	def validate_model ( cursor , model ) : _validate_license ( model ) _validate_roles ( model ) required_metadata = ( 'title' , 'summary' , ) for metadata_key in required_metadata : if model . metadata . get ( metadata_key ) in [ None , '' , [ ] ] : raise exceptions . MissingRequiredMetadata ( metadata_key ) _validate_derived_from ( cursor , model ) _validate_subjects ( cursor , model )
8559	def update_lan ( self , datacenter_id , lan_id , name = None , public = None , ip_failover = None ) : data = { } if name : data [ 'name' ] = name if public is not None : data [ 'public' ] = public if ip_failover : data [ 'ipFailover' ] = ip_failover response = self . _perform_request ( url = '/datacenters/%s/lans/%s' % ( datacenter_id , lan_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
2394	def calc_list_average ( l ) : total = 0.0 for value in l : total += value return total / len ( l )
3474	def check_mass_balance ( self ) : reaction_element_dict = defaultdict ( int ) for metabolite , coefficient in iteritems ( self . _metabolites ) : if metabolite . charge is not None : reaction_element_dict [ "charge" ] += coefficient * metabolite . charge if metabolite . elements is None : raise ValueError ( "No elements found in metabolite %s" % metabolite . id ) for element , amount in iteritems ( metabolite . elements ) : reaction_element_dict [ element ] += coefficient * amount return { k : v for k , v in iteritems ( reaction_element_dict ) if v != 0 }
5613	def reproject_geometry ( geometry , src_crs = None , dst_crs = None , error_on_clip = False , validity_check = True , antimeridian_cutting = False ) : src_crs = _validated_crs ( src_crs ) dst_crs = _validated_crs ( dst_crs ) def _reproject_geom ( geometry , src_crs , dst_crs ) : if geometry . is_empty : return geometry else : out_geom = to_shape ( transform_geom ( src_crs . to_dict ( ) , dst_crs . to_dict ( ) , mapping ( geometry ) , antimeridian_cutting = antimeridian_cutting ) ) return _repair ( out_geom ) if validity_check else out_geom if src_crs == dst_crs or geometry . is_empty : return _repair ( geometry ) elif ( dst_crs . is_epsg_code and dst_crs . get ( "init" ) in CRS_BOUNDS and dst_crs . get ( "init" ) != "epsg:4326" ) : wgs84_crs = CRS ( ) . from_epsg ( 4326 ) crs_bbox = box ( * CRS_BOUNDS [ dst_crs . get ( "init" ) ] ) geometry_4326 = _reproject_geom ( geometry , src_crs , wgs84_crs ) if error_on_clip and not geometry_4326 . within ( crs_bbox ) : raise RuntimeError ( "geometry outside target CRS bounds" ) return _reproject_geom ( crs_bbox . intersection ( geometry_4326 ) , wgs84_crs , dst_crs ) else : return _reproject_geom ( geometry , src_crs , dst_crs )
11867	def normalize ( self ) : "Return my probabilities; must be down to one variable." assert len ( self . vars ) == 1 return ProbDist ( self . vars [ 0 ] , dict ( ( k , v ) for ( ( k , ) , v ) in self . cpt . items ( ) ) )
11113	def get_repository ( self , path , info = None , verbose = True ) : if path . strip ( ) in ( '' , '.' ) : path = os . getcwd ( ) realPath = os . path . realpath ( os . path . expanduser ( path ) ) if not os . path . isdir ( realPath ) : os . makedirs ( realPath ) if not self . is_repository ( realPath ) : self . create_repository ( realPath , info = info , verbose = verbose ) else : self . load_repository ( realPath )
10127	def draw ( self ) : if self . enabled : self . _vertex_list . colors = self . _gl_colors self . _vertex_list . vertices = self . _gl_vertices self . _vertex_list . draw ( pyglet . gl . GL_TRIANGLES )
6769	def install_apt ( self , fn = None , package_name = None , update = 0 , list_only = 0 ) : r = self . local_renderer assert self . genv [ ROLE ] apt_req_fqfn = fn or ( self . env . apt_requirments_fn and self . find_template ( self . env . apt_requirments_fn ) ) if not apt_req_fqfn : return [ ] assert os . path . isfile ( apt_req_fqfn ) lines = list ( self . env . apt_packages or [ ] ) for _ in open ( apt_req_fqfn ) . readlines ( ) : if _ . strip ( ) and not _ . strip ( ) . startswith ( '#' ) and ( not package_name or _ . strip ( ) == package_name ) : lines . extend ( _pkg . strip ( ) for _pkg in _ . split ( ' ' ) if _pkg . strip ( ) ) if list_only : return lines tmp_fn = r . write_temp_file ( '\n' . join ( lines ) ) apt_req_fqfn = tmp_fn if not self . genv . is_local : r . put ( local_path = tmp_fn , remote_path = tmp_fn ) apt_req_fqfn = self . genv . put_remote_path r . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq update --fix-missing' ) r . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq install `cat "%s" | tr "\\n" " "`' % apt_req_fqfn )
11134	def is_not_exist_or_allow_overwrite ( self , overwrite = False ) : if self . exists ( ) and overwrite is False : return False else : return True
30	def initialize ( ) : new_variables = set ( tf . global_variables ( ) ) - ALREADY_INITIALIZED get_session ( ) . run ( tf . variables_initializer ( new_variables ) ) ALREADY_INITIALIZED . update ( new_variables )
8494	def _error ( msg , * args ) : print ( msg % args , file = sys . stderr ) sys . exit ( 1 )
5673	def from_directory_as_inmemory_db ( cls , gtfs_directory ) : from gtfspy . import_gtfs import import_gtfs conn = sqlite3 . connect ( ":memory:" ) import_gtfs ( gtfs_directory , conn , preserve_connection = True , print_progress = False ) return cls ( conn )
3134	def update ( self , list_id , data ) : self . list_id = list_id if 'name' not in data : raise KeyError ( 'The list must have a name' ) if 'contact' not in data : raise KeyError ( 'The list must have a contact' ) if 'company' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a company' ) if 'address1' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a address1' ) if 'city' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a city' ) if 'state' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a state' ) if 'zip' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a zip' ) if 'country' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a country' ) if 'permission_reminder' not in data : raise KeyError ( 'The list must have a permission_reminder' ) if 'campaign_defaults' not in data : raise KeyError ( 'The list must have a campaign_defaults' ) if 'from_name' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a from_name' ) if 'from_email' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a from_email' ) check_email ( data [ 'campaign_defaults' ] [ 'from_email' ] ) if 'subject' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a subject' ) if 'language' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a language' ) if 'email_type_option' not in data : raise KeyError ( 'The list must have an email_type_option' ) if data [ 'email_type_option' ] not in [ True , False ] : raise TypeError ( 'The list email_type_option must be True or False' ) return self . _mc_client . _patch ( url = self . _build_path ( list_id ) , data = data )
10504	def removecallback ( window_name ) : if window_name in _pollEvents . _callback : del _pollEvents . _callback [ window_name ] return _remote_removecallback ( window_name )
3664	def calculate ( self , T , method ) : r if method == PERRY151 : Cp = ( self . PERRY151_const + self . PERRY151_lin * T + self . PERRY151_quadinv / T ** 2 + self . PERRY151_quad * T ** 2 ) * calorie elif method == CRCSTD : Cp = self . CRCSTD_Cp elif method == LASTOVKA_S : Cp = Lastovka_solid ( T , self . similarity_variable ) Cp = property_mass_to_molar ( Cp , self . MW ) elif method in self . tabular_data : Cp = self . interpolate ( T , method ) return Cp
9066	def delta ( self ) : v = float ( self . _logistic . value ) if v > 0.0 : v = 1 / ( 1 + exp ( - v ) ) else : v = exp ( v ) v = v / ( v + 1.0 ) return min ( max ( v , epsilon . tiny ) , 1 - epsilon . tiny )
271	def estimate_intraday ( returns , positions , transactions , EOD_hour = 23 ) : txn_val = transactions . copy ( ) txn_val . index . names = [ 'date' ] txn_val [ 'value' ] = txn_val . amount * txn_val . price txn_val = txn_val . reset_index ( ) . pivot_table ( index = 'date' , values = 'value' , columns = 'symbol' ) . replace ( np . nan , 0 ) txn_val [ 'date' ] = txn_val . index . date txn_val = txn_val . groupby ( 'date' ) . cumsum ( ) txn_val [ 'exposure' ] = txn_val . abs ( ) . sum ( axis = 1 ) condition = ( txn_val [ 'exposure' ] == txn_val . groupby ( pd . TimeGrouper ( '24H' ) ) [ 'exposure' ] . transform ( max ) ) txn_val = txn_val [ condition ] . drop ( 'exposure' , axis = 1 ) txn_val [ 'cash' ] = - txn_val . sum ( axis = 1 ) positions_shifted = positions . copy ( ) . shift ( 1 ) . fillna ( 0 ) starting_capital = positions . iloc [ 0 ] . sum ( ) / ( 1 + returns [ 0 ] ) positions_shifted . cash [ 0 ] = starting_capital txn_val . index = txn_val . index . normalize ( ) corrected_positions = positions_shifted . add ( txn_val , fill_value = 0 ) corrected_positions . index . name = 'period_close' corrected_positions . columns . name = 'sid' return corrected_positions
289	def plot_rolling_returns ( returns , factor_returns = None , live_start_date = None , logy = False , cone_std = None , legend_loc = 'best' , volatility_match = False , cone_function = timeseries . forecast_cone_bootstrap , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Cumulative returns' ) ax . set_yscale ( 'log' if logy else 'linear' ) if volatility_match and factor_returns is None : raise ValueError ( 'volatility_match requires passing of ' 'factor_returns.' ) elif volatility_match and factor_returns is not None : bmark_vol = factor_returns . loc [ returns . index ] . std ( ) returns = ( returns / returns . std ( ) ) * bmark_vol cum_rets = ep . cum_returns ( returns , 1.0 ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) if factor_returns is not None : cum_factor_returns = ep . cum_returns ( factor_returns [ cum_rets . index ] , 1.0 ) cum_factor_returns . plot ( lw = 2 , color = 'gray' , label = factor_returns . name , alpha = 0.60 , ax = ax , ** kwargs ) if live_start_date is not None : live_start_date = ep . utils . get_utc_timestamp ( live_start_date ) is_cum_returns = cum_rets . loc [ cum_rets . index < live_start_date ] oos_cum_returns = cum_rets . loc [ cum_rets . index >= live_start_date ] else : is_cum_returns = cum_rets oos_cum_returns = pd . Series ( [ ] ) is_cum_returns . plot ( lw = 3 , color = 'forestgreen' , alpha = 0.6 , label = 'Backtest' , ax = ax , ** kwargs ) if len ( oos_cum_returns ) > 0 : oos_cum_returns . plot ( lw = 4 , color = 'red' , alpha = 0.6 , label = 'Live' , ax = ax , ** kwargs ) if cone_std is not None : if isinstance ( cone_std , ( float , int ) ) : cone_std = [ cone_std ] is_returns = returns . loc [ returns . index < live_start_date ] cone_bounds = cone_function ( is_returns , len ( oos_cum_returns ) , cone_std = cone_std , starting_value = is_cum_returns [ - 1 ] ) cone_bounds = cone_bounds . set_index ( oos_cum_returns . index ) for std in cone_std : ax . fill_between ( cone_bounds . index , cone_bounds [ float ( std ) ] , cone_bounds [ float ( - std ) ] , color = 'steelblue' , alpha = 0.5 ) if legend_loc is not None : ax . legend ( loc = legend_loc , frameon = True , framealpha = 0.5 ) ax . axhline ( 1.0 , linestyle = '--' , color = 'black' , lw = 2 ) return ax
5950	def check_file_exists ( self , filename , resolve = 'exception' , force = None ) : def _warn ( x ) : msg = "File {0!r} already exists." . format ( x ) logger . warn ( msg ) warnings . warn ( msg ) return True def _raise ( x ) : msg = "File {0!r} already exists." . format ( x ) logger . error ( msg ) raise IOError ( errno . EEXIST , x , msg ) solutions = { 'ignore' : lambda x : False , 'indicate' : lambda x : True , 'warn' : _warn , 'warning' : _warn , 'exception' : _raise , 'raise' : _raise , } if force is True : resolve = 'ignore' elif force is False : resolve = 'exception' if not os . path . isfile ( filename ) : return False else : return solutions [ resolve ] ( filename )
7213	def get_proj ( prj_code ) : if prj_code in CUSTOM_PRJ : proj = pyproj . Proj ( CUSTOM_PRJ [ prj_code ] ) else : proj = pyproj . Proj ( init = prj_code ) return proj
3873	async def leave_conversation ( self , conv_id ) : logger . info ( 'Leaving conversation: {}' . format ( conv_id ) ) await self . _conv_dict [ conv_id ] . leave ( ) del self . _conv_dict [ conv_id ]
11182	def release ( self ) : self . _lock . release ( ) with self . _stat_lock : self . _locked = False self . _last_released = datetime . now ( )
12505	def signed_session ( self , session = None ) : if session : session = super ( ClientCertAuthentication , self ) . signed_session ( session ) else : session = super ( ClientCertAuthentication , self ) . signed_session ( ) if self . cert is not None : session . cert = self . cert if self . ca_cert is not None : session . verify = self . ca_cert if self . no_verify : session . verify = False return session
181	def to_polygon ( self ) : from . polys import Polygon return Polygon ( self . coords , label = self . label )
6788	def push ( self , components = None , yes = 0 ) : from burlap import notifier service = self . get_satchel ( 'service' ) self . lock ( ) try : yes = int ( yes ) if not yes : if self . genv . host_string == self . genv . hosts [ 0 ] : execute ( partial ( self . preview , components = components , ask = 1 ) ) notifier . notify_pre_deployment ( ) component_order , plan_funcs = self . get_component_funcs ( components = components ) service . pre_deploy ( ) for func_name , plan_func in plan_funcs : print ( 'Executing %s...' % func_name ) plan_func ( ) self . fake ( components = components ) service . post_deploy ( ) notifier . notify_post_deployment ( ) finally : self . unlock ( )
6919	def _autocorr_func1 ( mags , lag , maglen , magmed , magstd ) : lagindex = nparange ( 1 , maglen - lag ) products = ( mags [ lagindex ] - magmed ) * ( mags [ lagindex + lag ] - magmed ) acorr = ( 1.0 / ( ( maglen - lag ) * magstd ) ) * npsum ( products ) return acorr
8139	def desaturate ( self ) : alpha = self . img . split ( ) [ 3 ] self . img = self . img . convert ( "L" ) self . img = self . img . convert ( "RGBA" ) self . img . putalpha ( alpha )
6909	def xieta_from_radecl ( inra , indecl , incenterra , incenterdecl , deg = True ) : if deg : ra = np . radians ( inra ) decl = np . radians ( indecl ) centerra = np . radians ( incenterra ) centerdecl = np . radians ( incenterdecl ) else : ra = inra decl = indecl centerra = incenterra centerdecl = incenterdecl cdecc = np . cos ( centerdecl ) sdecc = np . sin ( centerdecl ) crac = np . cos ( centerra ) srac = np . sin ( centerra ) uu = np . cos ( decl ) * np . cos ( ra ) vv = np . cos ( decl ) * np . sin ( ra ) ww = np . sin ( decl ) uun = uu * cdecc * crac + vv * cdecc * srac + ww * sdecc vvn = - uu * srac + vv * crac wwn = - uu * sdecc * crac - vv * sdecc * srac + ww * cdecc denom = vvn * vvn + wwn * wwn aunn = np . zeros_like ( uun ) aunn [ uun >= 1.0 ] = 0.0 aunn [ uun < 1.0 ] = np . arccos ( uun ) xi , eta = np . zeros_like ( aunn ) , np . zeros_like ( aunn ) xi [ ( aunn <= 0.0 ) | ( denom <= 0.0 ) ] = 0.0 eta [ ( aunn <= 0.0 ) | ( denom <= 0.0 ) ] = 0.0 sdenom = np . sqrt ( denom ) xi [ ( aunn > 0.0 ) | ( denom > 0.0 ) ] = aunn * vvn / sdenom eta [ ( aunn > 0.0 ) | ( denom > 0.0 ) ] = aunn * wwn / sdenom if deg : return np . degrees ( xi ) , np . degrees ( eta ) else : return xi , eta
5433	def split_pair ( pair_string , separator , nullable_idx = 1 ) : pair = pair_string . split ( separator , 1 ) if len ( pair ) == 1 : if nullable_idx == 0 : return [ None , pair [ 0 ] ] elif nullable_idx == 1 : return [ pair [ 0 ] , None ] else : raise IndexError ( 'nullable_idx should be either 0 or 1.' ) else : return pair
10624	def _calculate_Hfr ( self , T ) : if self . isCoal : return self . _calculate_Hfr_coal ( T ) Hfr = 0.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) dHfr = thermo . H ( compound , T , self . _compound_mfrs [ index ] ) Hfr = Hfr + dHfr return Hfr
9550	def ivalidate ( self , data , expect_header_row = True , ignore_lines = 0 , summarize = False , context = None , report_unexpected_exceptions = True ) : unique_sets = self . _init_unique_sets ( ) for i , r in enumerate ( data ) : if expect_header_row and i == ignore_lines : for p in self . _apply_header_checks ( i , r , summarize , context ) : yield p elif i >= ignore_lines : skip = False for p in self . _apply_skips ( i , r , summarize , report_unexpected_exceptions , context ) : if p is True : skip = True else : yield p if not skip : for p in self . _apply_each_methods ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_value_checks ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_record_length_checks ( i , r , summarize , context ) : yield p for p in self . _apply_value_predicates ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_record_checks ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_record_predicates ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_unique_checks ( i , r , unique_sets , summarize ) : yield p for p in self . _apply_check_methods ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_assert_methods ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_finally_assert_methods ( summarize , report_unexpected_exceptions , context ) : yield p
8495	def _get_module_filename ( module ) : module = module . split ( '.' ) package = '.' . join ( module [ : - 1 ] ) module = module [ - 1 ] try : if not package : module = __import__ ( module ) else : package = __import__ ( package , fromlist = [ module ] ) module = getattr ( package , module , None ) filename = getattr ( module , '__file__' , None ) if not filename : return Unparseable ( ) if filename . endswith ( '.pyc' ) : filename = filename [ : - 1 ] if not os . path . exists ( filename ) and os . path . isfile ( filename ) : return Unparseable ( ) if filename . endswith ( '__init__.py' ) : filename = filename [ : - 11 ] return filename except ImportError : return
4084	def get_newest_possible_languagetool_version ( ) : java_path = find_executable ( 'java' ) if not java_path : return JAVA_6_COMPATIBLE_VERSION output = subprocess . check_output ( [ java_path , '-version' ] , stderr = subprocess . STDOUT , universal_newlines = True ) java_version = parse_java_version ( output ) if java_version >= ( 1 , 8 ) : return LATEST_VERSION elif java_version >= ( 1 , 7 ) : return JAVA_7_COMPATIBLE_VERSION elif java_version >= ( 1 , 6 ) : warn ( 'language-check would be able to use a newer version of ' 'LanguageTool if you had Java 7 or newer installed' ) return JAVA_6_COMPATIBLE_VERSION else : raise SystemExit ( 'You need at least Java 6 to use language-check' )
12403	def requirements_for_changes ( self , changes ) : requirements = [ ] reqs_set = set ( ) if isinstance ( changes , str ) : changes = changes . split ( '\n' ) if not changes or changes [ 0 ] . startswith ( '-' ) : return requirements for line in changes : line = line . strip ( ' -+*' ) if not line : continue match = IS_REQUIREMENTS_RE2 . search ( line ) if match : for match in REQUIREMENTS_RE . findall ( match . group ( 1 ) ) : if match [ 1 ] : version = '==' + match [ 2 ] if match [ 1 ] . startswith ( ' to ' ) else match [ 1 ] req_str = match [ 0 ] + version else : req_str = match [ 0 ] if req_str not in reqs_set : reqs_set . add ( req_str ) try : requirements . append ( pkg_resources . Requirement . parse ( req_str ) ) except Exception as e : log . warn ( 'Could not parse requirement "%s" from changes: %s' , req_str , e ) return requirements
9615	def elements ( self , using , value ) : return self . _execute ( Command . FIND_CHILD_ELEMENTS , { 'using' : using , 'value' : value } )
12643	def set_config_value ( name , value ) : cli_config = CLIConfig ( SF_CLI_CONFIG_DIR , SF_CLI_ENV_VAR_PREFIX ) cli_config . set_value ( 'servicefabric' , name , value )
9440	def request ( self , path , method = None , data = { } ) : if not path : raise ValueError ( 'Invalid path parameter' ) if method and method not in [ 'GET' , 'POST' , 'DELETE' , 'PUT' ] : raise NotImplementedError ( 'HTTP %s method not implemented' % method ) if path [ 0 ] == '/' : uri = self . url + path else : uri = self . url + '/' + path if APPENGINE : return json . loads ( self . _appengine_fetch ( uri , data , method ) ) return json . loads ( self . _urllib2_fetch ( uri , data , method ) )
12785	def get_xdg_home ( self ) : config_home = getenv ( 'XDG_CONFIG_HOME' , '' ) if config_home : self . _log . debug ( 'XDG_CONFIG_HOME is set to %r' , config_home ) return expanduser ( join ( config_home , self . group_name , self . app_name ) ) return expanduser ( '~/.config/%s/%s' % ( self . group_name , self . app_name ) )
11795	def min_conflicts ( csp , max_steps = 100000 ) : csp . current = current = { } for var in csp . vars : val = min_conflicts_value ( csp , var , current ) csp . assign ( var , val , current ) for i in range ( max_steps ) : conflicted = csp . conflicted_vars ( current ) if not conflicted : return current var = random . choice ( conflicted ) val = min_conflicts_value ( csp , var , current ) csp . assign ( var , val , current ) return None
9234	def run ( self ) : if not self . options . project or not self . options . user : print ( "Project and/or user missing. " "For help run:\n pygcgen --help" ) return if not self . options . quiet : print ( "Generating changelog..." ) log = None try : log = self . generator . compound_changelog ( ) except ChangelogGeneratorError as err : print ( "\n\033[91m\033[1m{}\x1b[0m" . format ( err . args [ 0 ] ) ) exit ( 1 ) if not log : if not self . options . quiet : print ( "Empty changelog generated. {} not written." . format ( self . options . output ) ) return if self . options . no_overwrite : out = checkname ( self . options . output ) else : out = self . options . output with codecs . open ( out , "w" , "utf-8" ) as fh : fh . write ( log ) if not self . options . quiet : print ( "Done!" ) print ( "Generated changelog written to {}" . format ( out ) )
7943	def _start_connect ( self ) : family , addr = self . _dst_addrs . pop ( 0 ) self . _socket = socket . socket ( family , socket . SOCK_STREAM ) self . _socket . setblocking ( False ) self . _dst_addr = addr self . _family = family try : self . _socket . connect ( addr ) except socket . error , err : logger . debug ( "Connect error: {0}" . format ( err ) ) if err . args [ 0 ] in BLOCKING_ERRORS : self . _set_state ( "connecting" ) self . _write_queue . append ( ContinueConnect ( ) ) self . _write_queue_cond . notify ( ) self . event ( ConnectingEvent ( addr ) ) return elif self . _dst_addrs : self . _set_state ( "connect" ) return elif self . _dst_nameports : self . _set_state ( "resolve-hostname" ) return else : self . _socket . close ( ) self . _socket = None self . _set_state ( "aborted" ) self . _write_queue . clear ( ) self . _write_queue_cond . notify ( ) raise self . _connected ( )
5004	def transmit ( self , payload , ** kwargs ) : kwargs [ 'app_label' ] = 'degreed' kwargs [ 'model_name' ] = 'DegreedLearnerDataTransmissionAudit' kwargs [ 'remote_user_id' ] = 'degreed_user_email' super ( DegreedLearnerTransmitter , self ) . transmit ( payload , ** kwargs )
7681	def piano_roll ( annotation , ** kwargs ) : times , midi = annotation . to_interval_values ( ) return mir_eval . display . piano_roll ( times , midi = midi , ** kwargs )
9679	def ping ( self ) : b = self . cnxn . xfer ( [ 0xCF ] ) [ 0 ] sleep ( 0.1 ) return True if b == 0xF3 else False
279	def plot_monthly_returns_heatmap ( returns , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) monthly_ret_table = ep . aggregate_returns ( returns , 'monthly' ) monthly_ret_table = monthly_ret_table . unstack ( ) . round ( 3 ) sns . heatmap ( monthly_ret_table . fillna ( 0 ) * 100.0 , annot = True , annot_kws = { "size" : 9 } , alpha = 1.0 , center = 0.0 , cbar = False , cmap = matplotlib . cm . RdYlGn , ax = ax , ** kwargs ) ax . set_ylabel ( 'Year' ) ax . set_xlabel ( 'Month' ) ax . set_title ( "Monthly returns (%)" ) return ax
330	def model_returns_normal ( data , samples = 500 , progressbar = True ) : with pm . Model ( ) as model : mu = pm . Normal ( 'mean returns' , mu = 0 , sd = .01 , testval = data . mean ( ) ) sigma = pm . HalfCauchy ( 'volatility' , beta = 1 , testval = data . std ( ) ) returns = pm . Normal ( 'returns' , mu = mu , sd = sigma , observed = data ) pm . Deterministic ( 'annual volatility' , returns . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'sharpe' , returns . distribution . mean / returns . distribution . variance ** .5 * np . sqrt ( 252 ) ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace
4785	def starts_with ( self , prefix ) : if prefix is None : raise TypeError ( 'given prefix arg must not be none' ) if isinstance ( self . val , str_types ) : if not isinstance ( prefix , str_types ) : raise TypeError ( 'given prefix arg must be a string' ) if len ( prefix ) == 0 : raise ValueError ( 'given prefix arg must not be empty' ) if not self . val . startswith ( prefix ) : self . _err ( 'Expected <%s> to start with <%s>, but did not.' % ( self . val , prefix ) ) elif isinstance ( self . val , Iterable ) : if len ( self . val ) == 0 : raise ValueError ( 'val must not be empty' ) first = next ( iter ( self . val ) ) if first != prefix : self . _err ( 'Expected %s to start with <%s>, but did not.' % ( self . val , prefix ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
11439	def _get_children_by_tag_name ( node , name ) : try : return [ child for child in node . childNodes if child . nodeName == name ] except TypeError : return [ ]
9286	def sendall ( self , line ) : if isinstance ( line , APRSPacket ) : line = str ( line ) elif not isinstance ( line , string_type ) : raise TypeError ( "Expected line to be str or APRSPacket, got %s" , type ( line ) ) if not self . _connected : raise ConnectionError ( "not connected" ) if line == "" : return line = line . rstrip ( "\r\n" ) + "\r\n" try : self . sock . setblocking ( 1 ) self . sock . settimeout ( 5 ) self . _sendall ( line ) except socket . error as exp : self . close ( ) raise ConnectionError ( str ( exp ) )
2022	def SIGNEXTEND ( self , size , value ) : testbit = Operators . ITEBV ( 256 , size <= 31 , size * 8 + 7 , 257 ) result1 = ( value | ( TT256 - ( 1 << testbit ) ) ) result2 = ( value & ( ( 1 << testbit ) - 1 ) ) result = Operators . ITEBV ( 256 , ( value & ( 1 << testbit ) ) != 0 , result1 , result2 ) return Operators . ITEBV ( 256 , size <= 31 , result , value )
7410	def count_var ( nex ) : arr = np . array ( [ list ( i . split ( ) [ - 1 ] ) for i in nex ] ) miss = np . any ( arr == "N" , axis = 0 ) nomiss = arr [ : , ~ miss ] nsnps = np . invert ( np . all ( nomiss == nomiss [ 0 , : ] , axis = 0 ) ) . sum ( ) return nomiss . shape [ 1 ] , nsnps
9371	def password ( at_least = 6 , at_most = 12 , lowercase = True , uppercase = True , digits = True , spaces = False , punctuation = False ) : return text ( at_least = at_least , at_most = at_most , lowercase = lowercase , uppercase = uppercase , digits = digits , spaces = spaces , punctuation = punctuation )
12439	def serialize ( self , data , response = None , request = None , format = None ) : if isinstance ( self , Resource ) : if not request : request = self . _request Serializer = None if format : Serializer = self . meta . serializers [ format ] if not Serializer : media_ranges = ( request . get ( 'Accept' ) or '*/*' ) . strip ( ) if not media_ranges : media_ranges = '*/*' if media_ranges != '*/*' : media_types = six . iterkeys ( self . _serializer_map ) media_type = mimeparse . best_match ( media_types , media_ranges ) if media_type : format = self . _serializer_map [ media_type ] Serializer = self . meta . serializers [ format ] else : default = self . meta . default_serializer Serializer = self . meta . serializers [ default ] if Serializer : try : serializer = Serializer ( request , response ) return serializer . serialize ( data ) , serializer except ValueError : pass available = { } for name in self . meta . allowed_serializers : Serializer = self . meta . serializers [ name ] instance = Serializer ( request , None ) if instance . can_serialize ( data ) : available [ name ] = Serializer . media_types [ 0 ] raise http . exceptions . NotAcceptable ( available )
4012	def configure_nfs_server ( ) : repos_for_export = get_all_repos ( active_only = True , include_specs_repo = False ) current_exports = _get_current_exports ( ) needed_exports = _get_exports_for_repos ( repos_for_export ) _ensure_managed_repos_dir_exists ( ) if not needed_exports . difference ( current_exports ) : if not _server_is_running ( ) : _restart_server ( ) return _write_exports_config ( needed_exports ) _restart_server ( )
13748	def get_counter ( self , name , start = 0 ) : item = self . get_item ( hash_key = name , start = start ) counter = Counter ( dynamo_item = item , pool = self ) return counter
6867	def _pkl_magseries_plot ( stimes , smags , serrs , plotdpi = 100 , magsarefluxes = False ) : scaledplottime = stimes - npmin ( stimes ) magseriesfig = plt . figure ( figsize = ( 7.5 , 4.8 ) , dpi = plotdpi ) plt . plot ( scaledplottime , smags , marker = 'o' , ms = 2.0 , ls = 'None' , mew = 0 , color = 'green' , rasterized = True ) if not magsarefluxes : plot_ylim = plt . ylim ( ) plt . ylim ( ( plot_ylim [ 1 ] , plot_ylim [ 0 ] ) ) plt . xlim ( ( npmin ( scaledplottime ) - 2.0 , npmax ( scaledplottime ) + 2.0 ) ) plt . grid ( color = '#a9a9a9' , alpha = 0.9 , zorder = 0 , linewidth = 1.0 , linestyle = ':' ) plot_xlabel = 'JD - %.3f' % npmin ( stimes ) if magsarefluxes : plot_ylabel = 'flux' else : plot_ylabel = 'magnitude' plt . xlabel ( plot_xlabel ) plt . ylabel ( plot_ylabel ) plt . gca ( ) . get_yaxis ( ) . get_major_formatter ( ) . set_useOffset ( False ) plt . gca ( ) . get_xaxis ( ) . get_major_formatter ( ) . set_useOffset ( False ) magseriespng = StrIO ( ) magseriesfig . savefig ( magseriespng , pad_inches = 0.05 , format = 'png' ) plt . close ( ) magseriespng . seek ( 0 ) magseriesb64 = base64 . b64encode ( magseriespng . read ( ) ) magseriespng . close ( ) checkplotdict = { 'magseries' : { 'plot' : magseriesb64 , 'times' : stimes , 'mags' : smags , 'errs' : serrs } } return checkplotdict
3430	def add_reactions ( self , reaction_list ) : def existing_filter ( rxn ) : if rxn . id in self . reactions : LOGGER . warning ( "Ignoring reaction '%s' since it already exists." , rxn . id ) return False return True pruned = DictList ( filter ( existing_filter , reaction_list ) ) context = get_context ( self ) for reaction in pruned : reaction . _model = self for metabolite in list ( reaction . metabolites ) : if metabolite not in self . metabolites : self . add_metabolites ( metabolite ) else : stoichiometry = reaction . _metabolites . pop ( metabolite ) model_metabolite = self . metabolites . get_by_id ( metabolite . id ) reaction . _metabolites [ model_metabolite ] = stoichiometry model_metabolite . _reaction . add ( reaction ) if context : context ( partial ( model_metabolite . _reaction . remove , reaction ) ) for gene in list ( reaction . _genes ) : if not self . genes . has_id ( gene . id ) : self . genes += [ gene ] gene . _model = self if context : context ( partial ( self . genes . __isub__ , [ gene ] ) ) context ( partial ( setattr , gene , '_model' , None ) ) else : model_gene = self . genes . get_by_id ( gene . id ) if model_gene is not gene : reaction . _dissociate_gene ( gene ) reaction . _associate_gene ( model_gene ) self . reactions += pruned if context : context ( partial ( self . reactions . __isub__ , pruned ) ) self . _populate_solver ( pruned )
689	def removeAllRecords ( self ) : for field in self . fields : field . encodings , field . values = [ ] , [ ] field . numRecords , field . numEncodings = ( 0 , 0 )
7066	def delete_spot_fleet_cluster ( spot_fleet_reqid , client = None , ) : if not client : client = boto3 . client ( 'ec2' ) resp = client . cancel_spot_fleet_requests ( SpotFleetRequestIds = [ spot_fleet_reqid ] , TerminateInstances = True ) return resp
6416	def var ( nums , mean_func = amean , ddof = 0 ) : r x_bar = mean_func ( nums ) return sum ( ( x - x_bar ) ** 2 for x in nums ) / ( len ( nums ) - ddof )
1827	def CALL ( cpu , op0 ) : proc = op0 . read ( ) cpu . push ( cpu . PC , cpu . address_bit_size ) cpu . PC = proc
6760	def has_changes ( self ) : lm = self . last_manifest for tracker in self . get_trackers ( ) : last_thumbprint = lm [ '_tracker_%s' % tracker . get_natural_key_hash ( ) ] if tracker . is_changed ( last_thumbprint ) : return True return False
2916	def get_dump ( self , indent = 0 , recursive = True ) : dbg = ( ' ' * indent * 2 ) dbg += '%s/' % self . id dbg += '%s:' % self . thread_id dbg += ' Task of %s' % self . get_name ( ) if self . task_spec . description : dbg += ' (%s)' % self . get_description ( ) dbg += ' State: %s' % self . get_state_name ( ) dbg += ' Children: %s' % len ( self . children ) if recursive : for child in self . children : dbg += '\n' + child . get_dump ( indent + 1 ) return dbg
2852	def mpsse_set_clock ( self , clock_hz , adaptive = False , three_phase = False ) : self . _write ( '\x8A' ) if adaptive : self . _write ( '\x96' ) else : self . _write ( '\x97' ) if three_phase : self . _write ( '\x8C' ) else : self . _write ( '\x8D' ) divisor = int ( math . ceil ( ( 30000000.0 - float ( clock_hz ) ) / float ( clock_hz ) ) ) & 0xFFFF if three_phase : divisor = int ( divisor * ( 2.0 / 3.0 ) ) logger . debug ( 'Setting clockspeed with divisor value {0}' . format ( divisor ) ) self . _write ( str ( bytearray ( ( 0x86 , divisor & 0xFF , ( divisor >> 8 ) & 0xFF ) ) ) )
9987	def to_frame ( self , * args ) : if sys . version_info < ( 3 , 6 , 0 ) : from collections import OrderedDict impls = OrderedDict ( ) for name , obj in self . items ( ) : impls [ name ] = obj . _impl else : impls = get_impls ( self ) return _to_frame_inner ( impls , args )
1185	def dispatch ( self , opcode , context ) : if id ( context ) in self . executing_contexts : generator = self . executing_contexts [ id ( context ) ] del self . executing_contexts [ id ( context ) ] has_finished = generator . next ( ) else : method = self . DISPATCH_TABLE . get ( opcode , _OpcodeDispatcher . unknown ) has_finished = method ( self , context ) if hasattr ( has_finished , "next" ) : generator = has_finished has_finished = generator . next ( ) if not has_finished : self . executing_contexts [ id ( context ) ] = generator return has_finished
12874	def satisfies ( guard ) : i = peek ( ) if ( i is EndOfFile ) or ( not guard ( i ) ) : fail ( [ "<satisfies predicate " + _fun_to_str ( guard ) + ">" ] ) next ( ) return i
11811	def score ( self , word , docid ) : "Compute a score for this word on this docid." return ( math . log ( 1 + self . index [ word ] [ docid ] ) / math . log ( 1 + self . documents [ docid ] . nwords ) )
6036	def array_2d_from_array_1d ( self , array_1d ) : return mapping_util . map_masked_1d_array_to_2d_array_from_array_1d_shape_and_one_to_two ( array_1d = array_1d , shape = self . mask . shape , one_to_two = self . mask . masked_grid_index_to_pixel )
12891	def handle_int ( self , item ) : doc = yield from self . handle_get ( item ) if doc is None : return None return int ( doc . value . u8 . text ) or None
2794	def get_object ( cls , api_token , image_id_or_slug ) : if cls . _is_string ( image_id_or_slug ) : image = cls ( token = api_token , slug = image_id_or_slug ) image . load ( use_slug = True ) else : image = cls ( token = api_token , id = image_id_or_slug ) image . load ( ) return image
12250	def _get_key_internal ( self , * args , ** kwargs ) : if args [ 1 ] is not None and 'force' in args [ 1 ] : key , res = super ( Bucket , self ) . _get_key_internal ( * args , ** kwargs ) if key : mimicdb . backend . sadd ( tpl . bucket % self . name , key . name ) mimicdb . backend . hmset ( tpl . key % ( self . name , key . name ) , dict ( size = key . size , md5 = key . etag . strip ( '"' ) ) ) return key , res key = None if mimicdb . backend . sismember ( tpl . bucket % self . name , args [ 0 ] ) : key = Key ( self ) key . name = args [ 0 ] return key , None
12418	def capture_stdout ( ) : stdout = sys . stdout try : capture_out = StringIO ( ) sys . stdout = capture_out yield capture_out finally : sys . stdout = stdout
13515	def residual_resistance_coef ( slenderness , prismatic_coef , froude_number ) : Cr = cr ( slenderness , prismatic_coef , froude_number ) if math . isnan ( Cr ) : Cr = cr_nearest ( slenderness , prismatic_coef , froude_number ) return Cr
1036	def chain ( self , expanded_from ) : return Range ( self . source_buffer , self . begin_pos , self . begin_pos , expanded_from = expanded_from )
8399	def transform ( x ) : try : x = date2num ( x ) except AttributeError : x = [ pd . Timestamp ( item ) for item in x ] x = date2num ( x ) return x
6224	def _gl_look_at ( self , pos , target , up ) : z = vector . normalise ( pos - target ) x = vector . normalise ( vector3 . cross ( vector . normalise ( up ) , z ) ) y = vector3 . cross ( z , x ) translate = matrix44 . create_identity ( ) translate [ 3 ] [ 0 ] = - pos . x translate [ 3 ] [ 1 ] = - pos . y translate [ 3 ] [ 2 ] = - pos . z rotate = matrix44 . create_identity ( ) rotate [ 0 ] [ 0 ] = x [ 0 ] rotate [ 1 ] [ 0 ] = x [ 1 ] rotate [ 2 ] [ 0 ] = x [ 2 ] rotate [ 0 ] [ 1 ] = y [ 0 ] rotate [ 1 ] [ 1 ] = y [ 1 ] rotate [ 2 ] [ 1 ] = y [ 2 ] rotate [ 0 ] [ 2 ] = z [ 0 ] rotate [ 1 ] [ 2 ] = z [ 1 ] rotate [ 2 ] [ 2 ] = z [ 2 ] return matrix44 . multiply ( translate , rotate )
541	def __deleteModelCheckpoint ( self , modelID ) : checkpointID = self . _jobsDAO . modelsGetFields ( modelID , [ 'modelCheckpointId' ] ) [ 0 ] if checkpointID is None : return try : shutil . rmtree ( os . path . join ( self . _experimentDir , str ( self . _modelCheckpointGUID ) ) ) except : self . _logger . warn ( "Failed to delete model checkpoint %s. " "Assuming that another worker has already deleted it" , checkpointID ) return self . _jobsDAO . modelSetFields ( modelID , { 'modelCheckpointId' : None } , ignoreUnchanged = True ) return
2080	def callback ( self , pk = None , host_config_key = '' , extra_vars = None ) : url = self . endpoint + '%s/callback/' % pk if not host_config_key : host_config_key = client . get ( url ) . json ( ) [ 'host_config_key' ] post_data = { 'host_config_key' : host_config_key } if extra_vars : post_data [ 'extra_vars' ] = parser . process_extra_vars ( list ( extra_vars ) , force_json = True ) r = client . post ( url , data = post_data , auth = None ) if r . status_code == 201 : return { 'changed' : True }
4394	def adsSyncReadReqEx2 ( port , address , index_group , index_offset , data_type , return_ctypes = False ) : sync_read_request = _adsDLL . AdsSyncReadReqEx2 ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) index_group_c = ctypes . c_ulong ( index_group ) index_offset_c = ctypes . c_ulong ( index_offset ) if data_type == PLCTYPE_STRING : data = ( STRING_BUFFER * PLCTYPE_STRING ) ( ) else : data = data_type ( ) data_pointer = ctypes . pointer ( data ) data_length = ctypes . c_ulong ( ctypes . sizeof ( data ) ) bytes_read = ctypes . c_ulong ( ) bytes_read_pointer = ctypes . pointer ( bytes_read ) error_code = sync_read_request ( port , ams_address_pointer , index_group_c , index_offset_c , data_length , data_pointer , bytes_read_pointer , ) if error_code : raise ADSError ( error_code ) if data_type != PLCTYPE_STRING and bytes_read . value != data_length . value : raise RuntimeError ( "Insufficient data (expected {0} bytes, {1} were read)." . format ( data_length . value , bytes_read . value ) ) if return_ctypes : return data if data_type == PLCTYPE_STRING : return data . value . decode ( "utf-8" ) if type ( data_type ) . __name__ == "PyCArrayType" : return [ i for i in data ] if hasattr ( data , "value" ) : return data . value return data
11056	def ensure_backrefs ( obj , fields = None ) : for ref in _collect_refs ( obj , fields ) : updated = ref [ 'value' ] . _update_backref ( ref [ 'field_instance' ] . _backref_field_name , obj , ref [ 'field_name' ] , ) if updated : logging . debug ( 'Updated reference {}:{}:{}:{}:{}' . format ( obj . _name , obj . _primary_key , ref [ 'field_name' ] , ref [ 'value' ] . _name , ref [ 'value' ] . _primary_key , ) )
8221	def do_toggle_fullscreen ( self , action ) : is_fullscreen = action . get_active ( ) if is_fullscreen : self . fullscreen ( ) else : self . unfullscreen ( )
471	def read_analogies_file ( eval_file = 'questions-words.txt' , word2id = None ) : if word2id is None : word2id = { } questions = [ ] questions_skipped = 0 with open ( eval_file , "rb" ) as analogy_f : for line in analogy_f : if line . startswith ( b":" ) : continue words = line . strip ( ) . lower ( ) . split ( b" " ) ids = [ word2id . get ( w . strip ( ) ) for w in words ] if None in ids or len ( ids ) != 4 : questions_skipped += 1 else : questions . append ( np . array ( ids ) ) tl . logging . info ( "Eval analogy file: %s" % eval_file ) tl . logging . info ( "Questions: %d" , len ( questions ) ) tl . logging . info ( "Skipped: %d" , questions_skipped ) analogy_questions = np . array ( questions , dtype = np . int32 ) return analogy_questions
5653	def execute ( cur , * args ) : stmt = args [ 0 ] if len ( args ) > 1 : stmt = stmt . replace ( '%' , '%%' ) . replace ( '?' , '%r' ) print ( stmt % ( args [ 1 ] ) ) return cur . execute ( * args )
8329	def findNext ( self , name = None , attrs = { } , text = None , ** kwargs ) : return self . _findOne ( self . findAllNext , name , attrs , text , ** kwargs )
982	def create ( * args , ** kwargs ) : impl = kwargs . pop ( 'implementation' , None ) if impl is None : impl = Configuration . get ( 'nupic.opf.sdrClassifier.implementation' ) if impl == 'py' : return SDRClassifier ( * args , ** kwargs ) elif impl == 'cpp' : return FastSDRClassifier ( * args , ** kwargs ) elif impl == 'diff' : return SDRClassifierDiff ( * args , ** kwargs ) else : raise ValueError ( 'Invalid classifier implementation (%r). Value must be ' '"py", "cpp" or "diff".' % impl )
4500	def _follow_next ( self , url ) : response = self . _json ( self . _get ( url ) , 200 ) data = response [ 'data' ] next_url = self . _get_attribute ( response , 'links' , 'next' ) while next_url is not None : response = self . _json ( self . _get ( next_url ) , 200 ) data . extend ( response [ 'data' ] ) next_url = self . _get_attribute ( response , 'links' , 'next' ) return data
11960	def is_bits_nm ( nm ) : try : bits = int ( str ( nm ) ) except ValueError : return False if bits > 32 or bits < 0 : return False return True
8482	def env_key ( key , default ) : env = key . upper ( ) . replace ( '.' , '_' ) return os . environ . get ( env , default )
13139	def to_json ( self ) : if self . subreference is not None : return { "source" : self . objectId , "selector" : { "type" : "FragmentSelector" , "conformsTo" : "http://ontology-dts.org/terms/subreference" , "value" : self . subreference } } else : return { "source" : self . objectId }
740	def coordinateForPosition ( self , longitude , latitude , altitude = None ) : coords = PROJ ( longitude , latitude ) if altitude is not None : coords = transform ( PROJ , geocentric , coords [ 0 ] , coords [ 1 ] , altitude ) coordinate = numpy . array ( coords ) coordinate = coordinate / self . scale return coordinate . astype ( int )
6743	def render_to_file ( template , fn = None , extra = None , ** kwargs ) : import tempfile dryrun = get_dryrun ( kwargs . get ( 'dryrun' ) ) append_newline = kwargs . pop ( 'append_newline' , True ) style = kwargs . pop ( 'style' , 'cat' ) formatter = kwargs . pop ( 'formatter' , None ) content = render_to_string ( template , extra = extra ) if append_newline and not content . endswith ( '\n' ) : content += '\n' if formatter and callable ( formatter ) : content = formatter ( content ) if dryrun : if not fn : fd , fn = tempfile . mkstemp ( ) fout = os . fdopen ( fd , 'wt' ) fout . close ( ) else : if fn : fout = open ( fn , 'w' ) else : fd , fn = tempfile . mkstemp ( ) fout = os . fdopen ( fd , 'wt' ) fout . write ( content ) fout . close ( ) assert fn if style == 'cat' : cmd = 'cat <<EOF > %s\n%s\nEOF' % ( fn , content ) elif style == 'echo' : cmd = 'echo -e %s > %s' % ( shellquote ( content ) , fn ) else : raise NotImplementedError if BURLAP_COMMAND_PREFIX : print ( '%s run: %s' % ( render_command_prefix ( ) , cmd ) ) else : print ( cmd ) return fn
7884	def _make_ns_declarations ( declarations , declared_prefixes ) : result = [ ] for namespace , prefix in declarations . items ( ) : if prefix : result . append ( u' xmlns:{0}={1}' . format ( prefix , quoteattr ( namespace ) ) ) else : result . append ( u' xmlns={1}' . format ( prefix , quoteattr ( namespace ) ) ) for d_namespace , d_prefix in declared_prefixes . items ( ) : if ( not prefix and not d_prefix ) or d_prefix == prefix : if namespace != d_namespace : del declared_prefixes [ d_namespace ] return u" " . join ( result )
498	def _addRecordToKNN ( self , record ) : knn = self . _knnclassifier . _knn prototype_idx = self . _knnclassifier . getParameter ( 'categoryRecencyList' ) category = self . _labelListToCategoryNumber ( record . anomalyLabel ) if record . ROWID in prototype_idx : knn . prototypeSetCategory ( record . ROWID , category ) return pattern = self . _getStateAnomalyVector ( record ) rowID = record . ROWID knn . learn ( pattern , category , rowID = rowID )
11591	def _rc_sunion ( self , src , * args ) : args = list_or_args ( src , args ) src_set = self . smembers ( args . pop ( 0 ) ) if src_set is not set ( [ ] ) : for key in args : src_set . update ( self . smembers ( key ) ) return src_set
12356	def wait ( self ) : interval_seconds = 5 while True : actions = self . actions ( ) slept = False for a in actions : if a [ 'status' ] == 'in-progress' : time . sleep ( interval_seconds ) slept = True break if not slept : break
4724	def main ( conf ) : fpath = yml_fpath ( conf [ "OUTPUT" ] ) if os . path . exists ( fpath ) : cij . err ( "main:FAILED { fpath: %r }, exists" % fpath ) return 1 trun = trun_setup ( conf ) if not trun : return 1 trun_to_file ( trun ) trun_emph ( trun ) tr_err = 0 tr_ent_err = trun_enter ( trun ) for tsuite in ( ts for ts in trun [ "testsuites" ] if not tr_ent_err ) : ts_err = 0 ts_ent_err = tsuite_enter ( trun , tsuite ) for tcase in ( tc for tc in tsuite [ "testcases" ] if not ts_ent_err ) : tc_err = tcase_enter ( trun , tsuite , tcase ) if not tc_err : tc_err += script_run ( trun , tcase ) tc_err += tcase_exit ( trun , tsuite , tcase ) tcase [ "status" ] = "FAIL" if tc_err else "PASS" trun [ "progress" ] [ tcase [ "status" ] ] += 1 trun [ "progress" ] [ "UNKN" ] -= 1 ts_err += tc_err trun_to_file ( trun ) if not ts_ent_err : ts_err += tsuite_exit ( trun , tsuite ) ts_err += ts_ent_err tr_err += ts_err tsuite [ "status" ] = "FAIL" if ts_err else "PASS" cij . emph ( "rnr:tsuite %r" % tsuite [ "status" ] , tsuite [ "status" ] != "PASS" ) if not tr_ent_err : trun_exit ( trun ) tr_err += tr_ent_err trun [ "status" ] = "FAIL" if tr_err else "PASS" trun [ "stamp" ] [ "end" ] = int ( time . time ( ) ) + 1 trun_to_file ( trun ) cij . emph ( "rnr:main:progress %r" % trun [ "progress" ] ) cij . emph ( "rnr:main:trun %r" % trun [ "status" ] , trun [ "status" ] != "PASS" ) return trun [ "progress" ] [ "UNKN" ] + trun [ "progress" ] [ "FAIL" ]
2991	def symbolsDF ( token = '' , version = '' ) : df = pd . DataFrame ( symbols ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
3779	def calculate_integral_over_T ( self , T1 , T2 , method ) : r return float ( quad ( lambda T : self . calculate ( T , method ) / T , T1 , T2 ) [ 0 ] )
13507	def get_positions ( self ) : url = "/2/positions" data = self . _get_resource ( url ) positions = [ ] for entry in data [ 'positions' ] : positions . append ( self . position_from_json ( entry ) ) return positions
8027	def groupBy ( groups_in , classifier , fun_desc = '?' , keep_uniques = False , * args , ** kwargs ) : groups , count , group_count = { } , 0 , len ( groups_in ) for pos , paths in enumerate ( groups_in . values ( ) ) : out . write ( "Subdividing group %d of %d by %s... (%d files examined, %d " "in current group)" % ( pos + 1 , group_count , fun_desc , count , len ( paths ) ) ) for key , group in classifier ( paths , * args , ** kwargs ) . items ( ) : groups . setdefault ( key , set ( ) ) . update ( group ) count += len ( group ) if not keep_uniques : groups = dict ( [ ( x , groups [ x ] ) for x in groups if len ( groups [ x ] ) > 1 ] ) out . write ( "Found %s sets of files with identical %s. (%d files examined)" % ( len ( groups ) , fun_desc , count ) , newline = True ) return groups
9438	def load_network ( self , layers = 1 ) : if layers : ctor = payload_type ( self . type ) [ 0 ] if ctor : ctor = ctor payload = self . payload self . payload = ctor ( payload , layers - 1 ) else : pass
1012	def _cleanUpdatesList ( self , col , cellIdx , seg ) : for key , updateList in self . segmentUpdates . iteritems ( ) : c , i = key [ 0 ] , key [ 1 ] if c == col and i == cellIdx : for update in updateList : if update [ 1 ] . segment == seg : self . _removeSegmentUpdate ( update )
4826	def enroll_user_in_course ( self , username , course_id , mode , cohort = None ) : return self . client . enrollment . post ( { 'user' : username , 'course_details' : { 'course_id' : course_id } , 'mode' : mode , 'cohort' : cohort , } )
3987	def _move_temp_binary_to_path ( tmp_binary_path ) : binary_path = _get_binary_location ( ) if not binary_path . endswith ( constants . DUSTY_BINARY_NAME ) : raise RuntimeError ( 'Refusing to overwrite binary {}' . format ( binary_path ) ) st = os . stat ( binary_path ) permissions = st . st_mode owner = st . st_uid group = st . st_gid shutil . move ( tmp_binary_path , binary_path ) os . chown ( binary_path , owner , group ) os . chmod ( binary_path , permissions ) return binary_path
7301	def post ( self , request , * args , ** kwargs ) : form_class = self . get_form_class ( ) form = self . get_form ( form_class ) mongo_ids = self . get_initial ( ) [ 'mongo_id' ] for form_mongo_id in form . data . getlist ( 'mongo_id' ) : for mongo_id in mongo_ids : if form_mongo_id == mongo_id : self . document . objects . get ( pk = mongo_id ) . delete ( ) return self . form_invalid ( form )
7519	def write_snps_map ( data ) : start = time . time ( ) tmparrs = os . path . join ( data . dirs . outfiles , "tmp-{}.h5" . format ( data . name ) ) with h5py . File ( tmparrs , 'r' ) as io5 : maparr = io5 [ "maparr" ] [ : ] end = np . where ( np . all ( maparr [ : ] == 0 , axis = 1 ) ) [ 0 ] if np . any ( end ) : end = end . min ( ) else : end = maparr . shape [ 0 ] outchunk = [ ] with open ( data . outfiles . snpsmap , 'w' ) as out : for idx in xrange ( end ) : line = maparr [ idx , : ] outchunk . append ( "{}\trad{}_snp{}\t{}\t{}\n" . format ( line [ 0 ] , line [ 1 ] , line [ 2 ] , 0 , line [ 3 ] ) ) if not idx % 10000 : out . write ( "" . join ( outchunk ) ) outchunk = [ ] out . write ( "" . join ( outchunk ) ) LOGGER . debug ( "finished writing snps_map in: %s" , time . time ( ) - start )
2868	def get_platform_gpio ( ** keywords ) : plat = Platform . platform_detect ( ) if plat == Platform . RASPBERRY_PI : import RPi . GPIO return RPiGPIOAdapter ( RPi . GPIO , ** keywords ) elif plat == Platform . BEAGLEBONE_BLACK : import Adafruit_BBIO . GPIO return AdafruitBBIOAdapter ( Adafruit_BBIO . GPIO , ** keywords ) elif plat == Platform . MINNOWBOARD : import mraa return AdafruitMinnowAdapter ( mraa , ** keywords ) elif plat == Platform . JETSON_NANO : import Jetson . GPIO return RPiGPIOAdapter ( Jetson . GPIO , ** keywords ) elif plat == Platform . UNKNOWN : raise RuntimeError ( 'Could not determine platform.' )
4314	def validate_input_file ( input_filepath ) : if not os . path . exists ( input_filepath ) : raise IOError ( "input_filepath {} does not exist." . format ( input_filepath ) ) ext = file_extension ( input_filepath ) if ext not in VALID_FORMATS : logger . info ( "Valid formats: %s" , " " . join ( VALID_FORMATS ) ) logger . warning ( "This install of SoX cannot process .{} files." . format ( ext ) )
2641	def shutdown ( self ) : self . is_alive = False logging . debug ( "Waking management thread" ) self . incoming_q . put ( None ) self . _queue_management_thread . join ( ) logging . debug ( "Exiting thread" ) self . worker . join ( ) return True
6839	def distrib_release ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : kernel = ( run ( 'uname -s' ) or '' ) . strip ( ) . lower ( ) if kernel == LINUX : return run ( 'lsb_release -r --short' ) elif kernel == SUNOS : return run ( 'uname -v' )
10546	def get_taskruns ( project_id , limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) params [ 'project_id' ] = project_id try : res = _pybossa_req ( 'get' , 'taskrun' , params = params ) if type ( res ) . __name__ == 'list' : return [ TaskRun ( taskrun ) for taskrun in res ] else : raise TypeError except : raise
12149	def analyzeAll ( self ) : searchableData = str ( self . files2 ) self . log . debug ( "considering analysis for %d ABFs" , len ( self . IDs ) ) for ID in self . IDs : if not ID + "_" in searchableData : self . log . debug ( "%s needs analysis" , ID ) try : self . analyzeABF ( ID ) except : print ( "EXCEPTION! " * 100 ) else : self . log . debug ( "%s has existing analysis, not overwriting" , ID ) self . log . debug ( "verified analysis of %d ABFs" , len ( self . IDs ) )
5107	def fetch_data ( self , return_header = False ) : qdata = [ ] for d in self . data . values ( ) : qdata . extend ( d ) dat = np . zeros ( ( len ( qdata ) , 6 ) ) if len ( qdata ) > 0 : dat [ : , : 5 ] = np . array ( qdata ) dat [ : , 5 ] = self . edge [ 2 ] dType = [ ( 'a' , float ) , ( 's' , float ) , ( 'd' , float ) , ( 'q' , float ) , ( 'n' , float ) , ( 'id' , float ) ] dat = np . array ( [ tuple ( d ) for d in dat ] , dtype = dType ) dat = np . sort ( dat , order = 'a' ) dat = np . array ( [ tuple ( d ) for d in dat ] ) if return_header : return dat , 'arrival,service,departure,num_queued,num_total,q_id' return dat
8556	def list_lans ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/lans?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
156	def succ_key ( self , key , default = _sentinel ) : item = self . succ_item ( key , default ) return default if item is default else item [ 0 ]
4786	def ends_with ( self , suffix ) : if suffix is None : raise TypeError ( 'given suffix arg must not be none' ) if isinstance ( self . val , str_types ) : if not isinstance ( suffix , str_types ) : raise TypeError ( 'given suffix arg must be a string' ) if len ( suffix ) == 0 : raise ValueError ( 'given suffix arg must not be empty' ) if not self . val . endswith ( suffix ) : self . _err ( 'Expected <%s> to end with <%s>, but did not.' % ( self . val , suffix ) ) elif isinstance ( self . val , Iterable ) : if len ( self . val ) == 0 : raise ValueError ( 'val must not be empty' ) last = None for last in self . val : pass if last != suffix : self . _err ( 'Expected %s to end with <%s>, but did not.' % ( self . val , suffix ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
10723	def xformers ( sig ) : return [ ( _wrapper ( f ) , l ) for ( f , l ) in _XFORMER . PARSER . parseString ( sig , parseAll = True ) ]
5342	def __get_dash_menu ( self , kibiter_major ) : omenu = [ ] omenu . append ( self . menu_panels_common [ 'Overview' ] ) ds_menu = self . __get_menu_entries ( kibiter_major ) kafka_menu = None community_menu = None found_kafka = [ pos for pos , menu in enumerate ( ds_menu ) if menu [ 'name' ] == KAFKA_NAME ] if found_kafka : kafka_menu = ds_menu . pop ( found_kafka [ 0 ] ) found_community = [ pos for pos , menu in enumerate ( ds_menu ) if menu [ 'name' ] == COMMUNITY_NAME ] if found_community : community_menu = ds_menu . pop ( found_community [ 0 ] ) ds_menu . sort ( key = operator . itemgetter ( 'name' ) ) omenu += ds_menu if kafka_menu : omenu . append ( kafka_menu ) if community_menu : omenu . append ( community_menu ) omenu . append ( self . menu_panels_common [ 'Data Status' ] ) omenu . append ( self . menu_panels_common [ 'About' ] ) logger . debug ( "Menu for panels: %s" , json . dumps ( ds_menu , indent = 4 ) ) return omenu
10296	def get_incorrect_names_by_namespace ( graph : BELGraph , namespace : str ) -> Set [ str ] : return { exc . name for _ , exc , _ in graph . warnings if isinstance ( exc , ( MissingNamespaceNameWarning , MissingNamespaceRegexWarning ) ) and exc . namespace == namespace }
9744	def on_packet ( packet ) : print ( "Framenumber: {}" . format ( packet . framenumber ) ) header , markers = packet . get_3d_markers ( ) print ( "Component info: {}" . format ( header ) ) for marker in markers : print ( "\t" , marker )
13716	def next_item ( self ) : queue = self . queue try : item = queue . get ( block = True , timeout = 5 ) return item except Exception : return None
3039	def has_scopes ( self , scopes ) : scopes = _helpers . string_to_scopes ( scopes ) return set ( scopes ) . issubset ( self . scopes )
11477	def _create_or_reuse_folder ( local_folder , parent_folder_id , reuse_existing = False ) : local_folder_name = os . path . basename ( local_folder ) folder_id = None if reuse_existing : children = session . communicator . folder_children ( session . token , parent_folder_id ) folders = children [ 'folders' ] for folder in folders : if folder [ 'name' ] == local_folder_name : folder_id = folder [ 'folder_id' ] break if folder_id is None : new_folder = session . communicator . create_folder ( session . token , local_folder_name , parent_folder_id ) folder_id = new_folder [ 'folder_id' ] return folder_id
532	def getParameter ( self , paramName ) : ( setter , getter ) = self . _getParameterMethods ( paramName ) if getter is None : import exceptions raise exceptions . Exception ( "getParameter -- parameter name '%s' does not exist in region %s of type %s" % ( paramName , self . name , self . type ) ) return getter ( paramName )
10690	def render ( self , format = ReportFormat . printout ) : table = self . _generate_table_ ( ) if format == ReportFormat . printout : print ( tabulate ( table , headers = "firstrow" , tablefmt = "simple" ) ) elif format == ReportFormat . latex : self . _render_latex_ ( table ) elif format == ReportFormat . txt : self . _render_txt_ ( table ) elif format == ReportFormat . csv : self . _render_csv_ ( table ) elif format == ReportFormat . string : return str ( tabulate ( table , headers = "firstrow" , tablefmt = "simple" ) ) elif format == ReportFormat . matplotlib : self . _render_matplotlib_ ( ) elif format == ReportFormat . png : if self . output_path is None : self . _render_matplotlib_ ( ) else : self . _render_matplotlib_ ( True )
5035	def get_pending_users_queryset ( self , search_keyword , customer_uuid ) : queryset = PendingEnterpriseCustomerUser . objects . filter ( enterprise_customer__uuid = customer_uuid ) if search_keyword is not None : queryset = queryset . filter ( user_email__icontains = search_keyword ) return queryset
10304	def min_tanimoto_set_similarity ( x : Iterable [ X ] , y : Iterable [ X ] ) -> float : a , b = set ( x ) , set ( y ) if not a or not b : return 0.0 return len ( a & b ) / min ( len ( a ) , len ( b ) )
12102	def _launch_process_group ( self , process_commands , streams_path ) : processes = { } def check_complete_processes ( wait = False ) : result = False for proc in list ( processes ) : if wait : proc . wait ( ) if proc . poll ( ) is not None : self . debug ( "Process %d exited with code %d." % ( processes [ proc ] [ 'tid' ] , proc . poll ( ) ) ) processes [ proc ] [ 'stdout' ] . close ( ) processes [ proc ] [ 'stderr' ] . close ( ) del processes [ proc ] result = True return result for cmd , tid in process_commands : self . debug ( "Starting process %d..." % tid ) job_timestamp = time . strftime ( '%H%M%S' ) basename = "%s_%s_tid_%d" % ( self . batch_name , job_timestamp , tid ) stdout_handle = open ( os . path . join ( streams_path , "%s.o.%d" % ( basename , tid ) ) , "wb" ) stderr_handle = open ( os . path . join ( streams_path , "%s.e.%d" % ( basename , tid ) ) , "wb" ) proc = subprocess . Popen ( cmd , stdout = stdout_handle , stderr = stderr_handle ) processes [ proc ] = { 'tid' : tid , 'stdout' : stdout_handle , 'stderr' : stderr_handle } if self . max_concurrency : while len ( processes ) >= self . max_concurrency : if not check_complete_processes ( len ( processes ) == 1 ) : time . sleep ( 0.1 ) while len ( processes ) > 0 : if not check_complete_processes ( True ) : time . sleep ( 0.1 )
9930	def post ( self , request ) : serializer = self . get_serializer ( data = request . data ) if serializer . is_valid ( ) : serializer . save ( ) return Response ( serializer . data ) return Response ( serializer . errors , status = status . HTTP_400_BAD_REQUEST )
8187	def prune ( self , depth = 0 ) : for n in list ( self . nodes ) : if len ( n . links ) <= depth : self . remove_node ( n . id )
295	def plot_max_median_position_concentration ( positions , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) alloc_summary = pos . get_max_median_position_concentration ( positions ) colors = [ 'mediumblue' , 'steelblue' , 'tomato' , 'firebrick' ] alloc_summary . plot ( linewidth = 1 , color = colors , alpha = 0.6 , ax = ax ) ax . legend ( loc = 'center left' , frameon = True , framealpha = 0.5 ) ax . set_ylabel ( 'Exposure' ) ax . set_title ( 'Long/short max and median position concentration' ) return ax
6959	def query_radecl ( ra , decl , filtersystem = 'sloan_2mass' , field_deg2 = 1.0 , usebinaries = True , extinction_sigma = 0.1 , magnitude_limit = 26.0 , maglim_filtercol = 4 , trilegal_version = 1.6 , extraparams = None , forcefetch = False , cachedir = '~/.astrobase/trilegal-cache' , verbose = True , timeout = 60.0 , refresh = 150.0 , maxtimeout = 700.0 ) : radecl = SkyCoord ( ra = ra * u . degree , dec = decl * u . degree ) gl = radecl . galactic . l . degree gb = radecl . galactic . b . degree return query_galcoords ( gl , gb , filtersystem = filtersystem , field_deg2 = field_deg2 , usebinaries = usebinaries , extinction_sigma = extinction_sigma , magnitude_limit = magnitude_limit , maglim_filtercol = maglim_filtercol , trilegal_version = trilegal_version , extraparams = extraparams , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout )
2661	def hold_worker ( self , worker_id ) : c = self . command_client . run ( "HOLD_WORKER;{}" . format ( worker_id ) ) logger . debug ( "Sent hold request to worker: {}" . format ( worker_id ) ) return c
7122	def seeded_auth_token ( client , service , seed ) : hash_func = hashlib . md5 ( ) token = ',' . join ( ( client , service , seed ) ) . encode ( 'utf-8' ) hash_func . update ( token ) return hash_func . hexdigest ( )
10596	def h_x ( self , L , theta , Ts , ** statef ) : Nu_x = self . Nu_x ( L , theta , Ts , ** statef ) k = self . _fluid . k ( T = self . Tr ) return Nu_x * k / L
5555	def _element_at_zoom ( name , element , zoom ) : if isinstance ( element , dict ) : if "format" in element : return element out_elements = { } for sub_name , sub_element in element . items ( ) : out_element = _element_at_zoom ( sub_name , sub_element , zoom ) if name == "input" : out_elements [ sub_name ] = out_element elif out_element is not None : out_elements [ sub_name ] = out_element if len ( out_elements ) == 1 and name != "input" : return next ( iter ( out_elements . values ( ) ) ) if len ( out_elements ) == 0 : return None return out_elements elif isinstance ( name , str ) : if name . startswith ( "zoom" ) : return _filter_by_zoom ( conf_string = name . strip ( "zoom" ) . strip ( ) , zoom = zoom , element = element ) else : return element else : return element
10152	def _extract_path_from_service ( self , service ) : path_obj = { } path = service . path route_name = getattr ( service , 'pyramid_route' , None ) if route_name : registry = self . pyramid_registry or get_current_registry ( ) route_intr = registry . introspector . get ( 'routes' , route_name ) if route_intr : path = route_intr [ 'pattern' ] else : msg = 'Route `{}` is not found by ' 'pyramid introspector' . format ( route_name ) raise ValueError ( msg ) for subpath_marker in ( '*subpath' , '*traverse' ) : path = path . replace ( subpath_marker , '{subpath}' ) parameters = self . parameters . from_path ( path ) if parameters : path_obj [ 'parameters' ] = parameters return path , path_obj
4699	def get_meta ( offset , length , output ) : if env ( ) : cij . err ( "cij.nvme.meta: Invalid NVMe ENV." ) return 1 nvme = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) max_size = 0x40000 with open ( output , "wb" ) as fout : for off in range ( offset , length , max_size ) : size = min ( length - off , max_size ) cmd = [ "nvme get-log" , nvme [ "DEV_PATH" ] , "-i 0xca" , "-o 0x%x" % off , "-l 0x%x" % size , "-b" ] status , stdout , _ = cij . ssh . command ( cmd , shell = True ) if status : cij . err ( "cij.nvme.meta: Error get chunk meta" ) return 1 fout . write ( stdout ) return 0
1540	def set_config ( self , config ) : if not isinstance ( config , dict ) : raise TypeError ( "Argument to set_config needs to be dict, given: %s" % str ( config ) ) self . _topology_config = config
1310	def MouseInput ( dx : int , dy : int , mouseData : int = 0 , dwFlags : int = MouseEventFlag . LeftDown , time_ : int = 0 ) -> INPUT : return _CreateInput ( MOUSEINPUT ( dx , dy , mouseData , dwFlags , time_ , None ) )
9453	def play ( self , call_params ) : path = '/' + self . api_version + '/Play/' method = 'POST' return self . request ( path , method , call_params )
12027	def abfProtocol ( fname ) : f = open ( fname , 'rb' ) raw = f . read ( 30 * 1000 ) f . close ( ) raw = raw . decode ( "utf-8" , "ignore" ) raw = raw . split ( "Clampex" ) [ 1 ] . split ( ".pro" ) [ 0 ] protocol = os . path . basename ( raw ) protocolID = protocol . split ( " " ) [ 0 ] return protocolID
6864	def normalized_flux_to_mag ( lcdict , columns = ( 'sap.sap_flux' , 'sap.sap_flux_err' , 'sap.sap_bkg' , 'sap.sap_bkg_err' , 'pdc.pdcsap_flux' , 'pdc.pdcsap_flux_err' ) ) : tess_mag = lcdict [ 'objectinfo' ] [ 'tessmag' ] for key in columns : k1 , k2 = key . split ( '.' ) if 'err' not in k2 : lcdict [ k1 ] [ k2 . replace ( 'flux' , 'mag' ) ] = ( tess_mag - 2.5 * np . log10 ( lcdict [ k1 ] [ k2 ] ) ) else : lcdict [ k1 ] [ k2 . replace ( 'flux' , 'mag' ) ] = ( - 2.5 * np . log10 ( 1.0 - lcdict [ k1 ] [ k2 ] ) ) return lcdict
9336	def get ( self , Q ) : while self . Errors . empty ( ) : try : return Q . get ( timeout = 1 ) except queue . Empty : if not self . is_alive ( ) : try : return Q . get ( timeout = 0 ) except queue . Empty : raise StopProcessGroup else : continue else : raise StopProcessGroup
5495	def make_aware ( dt ) : return dt if dt . tzinfo else dt . replace ( tzinfo = timezone . utc )
8642	def create_milestone_payment ( session , project_id , bidder_id , amount , reason , description ) : milestone_data = { 'project_id' : project_id , 'bidder_id' : bidder_id , 'amount' : amount , 'reason' : reason , 'description' : description } response = make_post_request ( session , 'milestones' , json_data = milestone_data ) json_data = response . json ( ) if response . status_code == 200 : milestone_data = json_data [ 'result' ] return Milestone ( milestone_data ) else : raise MilestoneNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
11601	def save_model ( self , request , obj , form , change ) : obj . author = request . user obj . save ( )
9856	def get_data ( self , ** kwargs ) : limit = int ( kwargs . get ( 'limit' , 288 ) ) end_date = kwargs . get ( 'end_date' , False ) if end_date and isinstance ( end_date , datetime . datetime ) : end_date = self . convert_datetime ( end_date ) if self . mac_address is not None : service_address = 'devices/%s' % self . mac_address self . api_instance . log ( 'SERVICE ADDRESS: %s' % service_address ) data = dict ( limit = limit ) if end_date : data . update ( { 'endDate' : end_date } ) self . api_instance . log ( 'DATA:' ) self . api_instance . log ( data ) return self . api_instance . api_call ( service_address , ** data )
13743	def get_schema ( self ) : if not self . schema : raise NotImplementedError ( 'You must provide a schema value or override the get_schema method' ) return self . conn . create_schema ( ** self . schema )
10787	def add_subtract_misfeatured_tile ( st , tile , rad = 'calc' , max_iter = 3 , invert = 'guess' , max_allowed_remove = 20 , minmass = None , use_tp = False , ** kwargs ) : if rad == 'calc' : rad = guess_add_radii ( st ) if invert == 'guess' : invert = guess_invert ( st ) initial_error = np . copy ( st . error ) rinds = np . nonzero ( tile . contains ( st . obj_get_positions ( ) ) ) [ 0 ] if rinds . size >= max_allowed_remove : CLOG . fatal ( 'Misfeatured region too large!' ) raise RuntimeError elif rinds . size >= max_allowed_remove / 2 : CLOG . warn ( 'Large misfeatured regions.' ) elif rinds . size > 0 : rpos , rrad = st . obj_remove_particle ( rinds ) n_added = - rinds . size added_poses = [ ] for _ in range ( max_iter ) : if invert : im = 1 - st . residuals [ tile . slicer ] else : im = st . residuals [ tile . slicer ] guess , _ = _feature_guess ( im , rad , minmass = minmass , use_tp = use_tp ) accepts , poses = check_add_particles ( st , guess + tile . l , rad = rad , do_opt = True , ** kwargs ) added_poses . extend ( poses ) n_added += accepts if accepts == 0 : break else : CLOG . warn ( 'Runaway adds or insufficient max_iter' ) ainds = [ ] for p in added_poses : ainds . append ( st . obj_closest_particle ( p ) ) if len ( ainds ) > max_allowed_remove : for i in range ( 0 , len ( ainds ) , max_allowed_remove ) : opt . do_levmarq_particles ( st , np . array ( ainds [ i : i + max_allowed_remove ] ) , include_rad = True , max_iter = 3 ) elif len ( ainds ) > 0 : opt . do_levmarq_particles ( st , ainds , include_rad = True , max_iter = 3 ) did_something = ( rinds . size > 0 ) or ( len ( ainds ) > 0 ) if did_something & ( st . error > initial_error ) : CLOG . info ( 'Failed addsub, Tile {} -> {}' . format ( tile . l . tolist ( ) , tile . r . tolist ( ) ) ) if len ( ainds ) > 0 : _ = st . obj_remove_particle ( ainds ) if rinds . size > 0 : for p , r in zip ( rpos . reshape ( - 1 , 3 ) , rrad . reshape ( - 1 ) ) : _ = st . obj_add_particle ( p , r ) n_added = 0 ainds = [ ] return n_added , ainds
6689	def groupuninstall ( group , options = None ) : manager = MANAGER if options is None : options = [ ] elif isinstance ( options , str ) : options = [ options ] options = " " . join ( options ) run_as_root ( '%(manager)s %(options)s groupremove "%(group)s"' % locals ( ) )
7245	def _tile_coords ( self , bounds ) : tfm = partial ( pyproj . transform , pyproj . Proj ( init = "epsg:3857" ) , pyproj . Proj ( init = "epsg:4326" ) ) bounds = ops . transform ( tfm , box ( * bounds ) ) . bounds west , south , east , north = bounds epsilon = 1.0e-10 if east != west and north != south : west += epsilon south += epsilon east -= epsilon north -= epsilon params = [ west , south , east , north , [ self . zoom_level ] ] tile_coords = [ ( tile . x , tile . y ) for tile in mercantile . tiles ( * params ) ] xtiles , ytiles = zip ( * tile_coords ) minx = min ( xtiles ) miny = min ( ytiles ) maxx = max ( xtiles ) maxy = max ( ytiles ) return minx , miny , maxx , maxy
8590	def reboot_server ( self , datacenter_id , server_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/reboot' % ( datacenter_id , server_id ) , method = 'POST-ACTION' ) return response
2993	def otcSymbolsDF ( token = '' , version = '' ) : df = pd . DataFrame ( otcSymbols ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
6649	def inheritsFrom ( self , target_name ) : for t in self . hierarchy : if t and t . getName ( ) == target_name or target_name in t . description . get ( 'inherits' , { } ) : return True return False
1747	def _in_range ( self , index ) : if isinstance ( index , slice ) : in_range = index . start < index . stop and index . start >= self . start and index . stop <= self . end else : in_range = index >= self . start and index <= self . end return in_range
9100	def write_bel_namespace_mappings ( self , file : TextIO , ** kwargs ) -> None : json . dump ( self . _get_namespace_identifier_to_name ( ** kwargs ) , file , indent = 2 , sort_keys = True )
8479	def potential ( self , value ) : if value : self . _potential = True else : self . _potential = False
5426	def _group_tasks_by_jobid ( tasks ) : ret = collections . defaultdict ( list ) for t in tasks : ret [ t . get_field ( 'job-id' ) ] . append ( t ) return ret
10678	def H ( self , T ) : result = 0.0 if T < self . Tmax : lT = T else : lT = self . Tmax Tref = self . Tmin for c , e in zip ( self . _coefficients , self . _exponents ) : if e == - 1.0 : result += c * math . log ( lT / Tref ) else : result += c * ( lT ** ( e + 1.0 ) - Tref ** ( e + 1.0 ) ) / ( e + 1.0 ) return result
9627	def url ( self ) : return reverse ( '%s:detail' % URL_NAMESPACE , kwargs = { 'module' : self . module , 'preview' : type ( self ) . __name__ , } )
8489	def get_watcher ( self ) : if not self . watching : raise StopIteration ( ) return self . client . eternal_watch ( self . prefix , recursive = True )
8783	def _get_base_network_info ( self , context , network_id , base_net_driver ) : driver_name = base_net_driver . get_name ( ) net_info = { "network_type" : driver_name } LOG . debug ( '_get_base_network_info: %s %s' % ( driver_name , network_id ) ) if driver_name == 'NVP' : LOG . debug ( 'looking up lswitch ids for network %s' % ( network_id ) ) lswitch_ids = base_net_driver . get_lswitch_ids_for_network ( context , network_id ) if not lswitch_ids or len ( lswitch_ids ) > 1 : msg = ( 'lswitch id lookup failed, %s ids found.' % ( len ( lswitch_ids ) ) ) LOG . error ( msg ) raise IronicException ( msg ) lswitch_id = lswitch_ids . pop ( ) LOG . info ( 'found lswitch for network %s: %s' % ( network_id , lswitch_id ) ) net_info [ 'lswitch_id' ] = lswitch_id LOG . debug ( '_get_base_network_info finished: %s %s %s' % ( driver_name , network_id , net_info ) ) return net_info
3114	def _from_base_type ( self , value ) : if not value : return None try : credentials = client . Credentials . new_from_json ( value ) except ValueError : credentials = None return credentials
10736	def split_list ( l , N ) : npmode = isinstance ( l , np . ndarray ) if npmode : l = list ( l ) g = np . concatenate ( ( np . array ( [ 0 ] ) , np . cumsum ( split_integer ( len ( l ) , length = N ) ) ) ) s = [ l [ g [ i ] : g [ i + 1 ] ] for i in range ( N ) ] if npmode : s = [ np . array ( sl ) for sl in s ] return s
13068	def r_collections ( self , lang = None ) : collection = self . resolver . getMetadata ( ) return { "template" : "main::collection.html" , "current_label" : collection . get_label ( lang ) , "collections" : { "members" : self . make_members ( collection , lang = lang ) } }
1642	def CheckBracesSpacing ( filename , clean_lines , linenum , nesting_state , error ) : line = clean_lines . elided [ linenum ] match = Match ( r'^(.*[^ ({>]){' , line ) if match : leading_text = match . group ( 1 ) ( endline , endlinenum , endpos ) = CloseExpression ( clean_lines , linenum , len ( match . group ( 1 ) ) ) trailing_text = '' if endpos > - 1 : trailing_text = endline [ endpos : ] for offset in xrange ( endlinenum + 1 , min ( endlinenum + 3 , clean_lines . NumLines ( ) - 1 ) ) : trailing_text += clean_lines . elided [ offset ] if ( not Match ( r'^[\s}]*[{.;,)<>\]:]' , trailing_text ) and not _IsType ( clean_lines , nesting_state , leading_text ) ) : error ( filename , linenum , 'whitespace/braces' , 5 , 'Missing space before {' ) if Search ( r'}else' , line ) : error ( filename , linenum , 'whitespace/braces' , 5 , 'Missing space before else' ) if Search ( r':\s*;\s*$' , line ) : error ( filename , linenum , 'whitespace/semicolon' , 5 , 'Semicolon defining empty statement. Use {} instead.' ) elif Search ( r'^\s*;\s*$' , line ) : error ( filename , linenum , 'whitespace/semicolon' , 5 , 'Line contains only semicolon. If this should be an empty statement, ' 'use {} instead.' ) elif ( Search ( r'\s+;\s*$' , line ) and not Search ( r'\bfor\b' , line ) ) : error ( filename , linenum , 'whitespace/semicolon' , 5 , 'Extra space before last semicolon. If this should be an empty ' 'statement, use {} instead.' )
6494	def set_mappings ( cls , index_name , doc_type , mappings ) : cache . set ( cls . get_cache_item_name ( index_name , doc_type ) , mappings )
1470	def getStmgrsRegSummary ( self , tmaster , callback = None ) : if not tmaster or not tmaster . host or not tmaster . stats_port : return reg_request = tmaster_pb2 . StmgrsRegistrationSummaryRequest ( ) request_str = reg_request . SerializeToString ( ) port = str ( tmaster . stats_port ) host = tmaster . host url = "http://{0}:{1}/stmgrsregistrationsummary" . format ( host , port ) request = tornado . httpclient . HTTPRequest ( url , body = request_str , method = 'POST' , request_timeout = 5 ) Log . debug ( 'Making HTTP call to fetch stmgrsregistrationsummary url: %s' , url ) try : client = tornado . httpclient . AsyncHTTPClient ( ) result = yield client . fetch ( request ) Log . debug ( "HTTP call complete." ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) ) responseCode = result . code if responseCode >= 400 : message = "Error in getting exceptions from Tmaster, code: " + responseCode Log . error ( message ) raise tornado . gen . Return ( { "message" : message } ) reg_response = tmaster_pb2 . StmgrsRegistrationSummaryResponse ( ) reg_response . ParseFromString ( result . body ) ret = { } for stmgr in reg_response . registered_stmgrs : ret [ stmgr ] = True for stmgr in reg_response . absent_stmgrs : ret [ stmgr ] = False raise tornado . gen . Return ( ret )
5209	def info_qry ( tickers , flds ) -> str : full_list = '\n' . join ( [ f'tickers: {tickers[:8]}' ] + [ f' {tickers[n:(n + 8)]}' for n in range ( 8 , len ( tickers ) , 8 ) ] ) return f'{full_list}\nfields: {flds}'
11398	def update_reportnumbers ( self ) : rep_088_fields = record_get_field_instances ( self . record , '088' ) for field in rep_088_fields : subs = field_get_subfields ( field ) if '9' in subs : for val in subs [ '9' ] : if val . startswith ( 'P0' ) or val . startswith ( 'CM-P0' ) : sf = [ ( '9' , 'CERN' ) , ( 'b' , val ) ] record_add_field ( self . record , '595' , subfields = sf ) for key , val in field [ 0 ] : if key in [ 'a' , '9' ] and not val . startswith ( 'SIS-' ) : record_add_field ( self . record , '037' , subfields = [ ( 'a' , val ) ] ) record_delete_fields ( self . record , "088" ) rep_037_fields = record_get_field_instances ( self . record , '037' ) for field in rep_037_fields : subs = field_get_subfields ( field ) if 'a' in subs : for value in subs [ 'a' ] : if 'arXiv' in value : new_subs = [ ( 'a' , value ) , ( '9' , 'arXiv' ) ] for fld in record_get_field_instances ( self . record , '695' ) : for key , val in field_get_subfield_instances ( fld ) : if key == 'a' : new_subs . append ( ( 'c' , val ) ) break nf = create_field ( subfields = new_subs ) record_replace_field ( self . record , '037' , nf , field [ 4 ] ) for key , val in field [ 0 ] : if key in [ 'a' , '9' ] and val . startswith ( 'SIS-' ) : record_delete_field ( self . record , '037' , field_position_global = field [ 4 ] )
8402	def rescale_max ( x , to = ( 0 , 1 ) , _from = None ) : array_like = True try : len ( x ) except TypeError : array_like = False x = [ x ] if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) if _from is None : _from = np . array ( [ np . min ( x ) , np . max ( x ) ] ) out = x / _from [ 1 ] * to [ 1 ] if not array_like : out = out [ 0 ] return out
2054	def ADR ( cpu , dest , src ) : aligned_pc = ( cpu . instruction . address + 4 ) & 0xfffffffc dest . write ( aligned_pc + src . read ( ) )
10445	def getchild ( self , window_name , child_name = '' , role = '' , parent = '' ) : matches = [ ] if role : role = re . sub ( ' ' , '_' , role ) self . _windows = { } if parent and ( child_name or role ) : _window_handle , _window_name = self . _get_window_handle ( window_name ) [ 0 : 2 ] if not _window_handle : raise LdtpServerException ( 'Unable to find window "%s"' % window_name ) appmap = self . _get_appmap ( _window_handle , _window_name ) obj = self . _get_object_map ( window_name , parent ) def _get_all_children_under_obj ( obj , child_list ) : if role and obj [ 'class' ] == role : child_list . append ( obj [ 'label' ] ) elif child_name and self . _match_name_to_appmap ( child_name , obj ) : child_list . append ( obj [ 'label' ] ) if obj : children = obj [ 'children' ] if not children : return child_list for child in children . split ( ) : return _get_all_children_under_obj ( appmap [ child ] , child_list ) matches = _get_all_children_under_obj ( obj , [ ] ) if not matches : if child_name : _name = 'name "%s" ' % child_name if role : _role = 'role "%s" ' % role if parent : _parent = 'parent "%s"' % parent exception = 'Could not find a child %s%s%s' % ( _name , _role , _parent ) raise LdtpServerException ( exception ) return matches _window_handle , _window_name = self . _get_window_handle ( window_name ) [ 0 : 2 ] if not _window_handle : raise LdtpServerException ( 'Unable to find window "%s"' % window_name ) appmap = self . _get_appmap ( _window_handle , _window_name ) for name in appmap . keys ( ) : obj = appmap [ name ] if role and not child_name and obj [ 'class' ] == role : matches . append ( name ) if parent and child_name and not role and self . _match_name_to_appmap ( parent , obj ) : matches . append ( name ) if child_name and not role and self . _match_name_to_appmap ( child_name , obj ) : return name matches . append ( name ) if role and child_name and obj [ 'class' ] == role and self . _match_name_to_appmap ( child_name , obj ) : matches . append ( name ) if not matches : _name = '' _role = '' _parent = '' if child_name : _name = 'name "%s" ' % child_name if role : _role = 'role "%s" ' % role if parent : _parent = 'parent "%s"' % parent exception = 'Could not find a child %s%s%s' % ( _name , _role , _parent ) raise LdtpServerException ( exception ) return matches
13569	def selected_exercise ( func ) : @ wraps ( func ) def inner ( * args , ** kwargs ) : exercise = Exercise . get_selected ( ) return func ( exercise , * args , ** kwargs ) return inner
10511	def onwindowcreate ( self , window_name , fn_name , * args ) : self . _pollEvents . _callback [ window_name ] = [ "onwindowcreate" , fn_name , args ] return self . _remote_onwindowcreate ( window_name )
10555	def find_helping_materials ( project_id , ** kwargs ) : try : kwargs [ 'project_id' ] = project_id res = _pybossa_req ( 'get' , 'helpingmaterial' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ HelpingMaterial ( helping ) for helping in res ] else : return res except : raise
5412	def build_action ( name = None , image_uri = None , commands = None , entrypoint = None , environment = None , pid_namespace = None , flags = None , port_mappings = None , mounts = None , labels = None ) : return { 'name' : name , 'imageUri' : image_uri , 'commands' : commands , 'entrypoint' : entrypoint , 'environment' : environment , 'pidNamespace' : pid_namespace , 'flags' : flags , 'portMappings' : port_mappings , 'mounts' : mounts , 'labels' : labels , }
11896	def _create_index_file ( root_dir , location , image_files , dirs , force_no_processing = False ) : header_text = 'imageMe: ' + location + ' [' + str ( len ( image_files ) ) + ' image(s)]' html = [ '<!DOCTYPE html>' , '<html>' , ' <head>' , ' <title>imageMe</title>' ' <style>' , ' html, body {margin: 0;padding: 0;}' , ' .header {text-align: right;}' , ' .content {' , ' padding: 3em;' , ' padding-left: 4em;' , ' padding-right: 4em;' , ' }' , ' .image {max-width: 100%; border-radius: 0.3em;}' , ' td {width: ' + str ( 100.0 / IMAGES_PER_ROW ) + '%;}' , ' </style>' , ' </head>' , ' <body>' , ' <div class="content">' , ' <h2 class="header">' + header_text + '</h2>' ] directories = [ ] if root_dir != location : directories = [ '..' ] directories += dirs if len ( directories ) > 0 : html . append ( '<hr>' ) for directory in directories : link = directory + '/' + INDEX_FILE_NAME html += [ ' <h3 class="header">' , ' <a href="' + link + '">' + directory + '</a>' , ' </h3>' ] table_row_count = 1 html += [ '<hr>' , '<table>' ] for image_file in image_files : if table_row_count == 1 : html . append ( '<tr>' ) img_src = _get_thumbnail_src_from_file ( location , image_file , force_no_processing ) link_target = _get_image_link_target_from_file ( location , image_file , force_no_processing ) html += [ ' <td>' , ' <a href="' + link_target + '">' , ' <img class="image" src="' + img_src + '">' , ' </a>' , ' </td>' ] if table_row_count == IMAGES_PER_ROW : table_row_count = 0 html . append ( '</tr>' ) table_row_count += 1 html += [ '</tr>' , '</table>' ] html += [ ' </div>' , ' </body>' , '</html>' ] index_file_path = _get_index_file_path ( location ) print ( 'Creating index file %s' % index_file_path ) index_file = open ( index_file_path , 'w' ) index_file . write ( '\n' . join ( html ) ) index_file . close ( ) return index_file_path
10058	def records ( ) : import pkg_resources from dojson . contrib . marc21 import marc21 from dojson . contrib . marc21 . utils import create_record , split_blob from flask_login import login_user , logout_user from invenio_accounts . models import User from invenio_deposit . api import Deposit users = User . query . all ( ) data_path = pkg_resources . resource_filename ( 'invenio_records' , 'data/marc21/bibliographic.xml' ) with open ( data_path ) as source : with current_app . test_request_context ( ) : indexer = RecordIndexer ( ) with db . session . begin_nested ( ) : for index , data in enumerate ( split_blob ( source . read ( ) ) , start = 1 ) : login_user ( users [ index % len ( users ) ] ) record = marc21 . do ( create_record ( data ) ) indexer . index ( Deposit . create ( record ) ) logout_user ( ) db . session . commit ( )
7895	def set_subject ( self , subject ) : m = Message ( to_jid = self . room_jid . bare ( ) , stanza_type = "groupchat" , subject = subject ) self . manager . stream . send ( m )
4293	def _manage_args ( parser , args ) : for item in data . CONFIGURABLE_OPTIONS : action = parser . _option_string_actions [ item ] choices = default = '' input_value = getattr ( args , action . dest ) new_val = None if not args . noinput : if action . choices : choices = ' (choices: {0})' . format ( ', ' . join ( action . choices ) ) if input_value : if type ( input_value ) == list : default = ' [default {0}]' . format ( ', ' . join ( input_value ) ) else : default = ' [default {0}]' . format ( input_value ) while not new_val : prompt = '{0}{1}{2}: ' . format ( action . help , choices , default ) if action . choices in ( 'yes' , 'no' ) : new_val = utils . query_yes_no ( prompt ) else : new_val = compat . input ( prompt ) new_val = compat . clean ( new_val ) if not new_val and input_value : new_val = input_value if new_val and action . dest == 'templates' : if new_val != 'no' and not os . path . isdir ( new_val ) : sys . stdout . write ( 'Given directory does not exists, retry\n' ) new_val = False if new_val and action . dest == 'db' : action ( parser , args , new_val , action . option_strings ) new_val = getattr ( args , action . dest ) else : if not input_value and action . required : raise ValueError ( 'Option {0} is required when in no-input mode' . format ( action . dest ) ) new_val = input_value if action . dest == 'db' : action ( parser , args , new_val , action . option_strings ) new_val = getattr ( args , action . dest ) if action . dest == 'templates' and ( new_val == 'no' or not os . path . isdir ( new_val ) ) : new_val = False if action . dest in ( 'bootstrap' , 'starting_page' ) : new_val = ( new_val == 'yes' ) setattr ( args , action . dest , new_val ) return args
12618	def get_shape ( img ) : if hasattr ( img , 'shape' ) : shape = img . shape else : shape = img . get_data ( ) . shape return shape
1250	def do_action ( self , action ) : temp_state = np . rot90 ( self . _state , action ) reward = self . _do_action_left ( temp_state ) self . _state = np . rot90 ( temp_state , - action ) self . _score += reward self . add_random_tile ( ) return reward
1044	def float_unpack ( Q , size , le ) : if size == 8 : MIN_EXP = - 1021 MAX_EXP = 1024 MANT_DIG = 53 BITS = 64 elif size == 4 : MIN_EXP = - 125 MAX_EXP = 128 MANT_DIG = 24 BITS = 32 else : raise ValueError ( "invalid size value" ) if Q >> BITS : raise ValueError ( "input out of range" ) sign = Q >> BITS - 1 exp = ( Q & ( ( 1 << BITS - 1 ) - ( 1 << MANT_DIG - 1 ) ) ) >> MANT_DIG - 1 mant = Q & ( ( 1 << MANT_DIG - 1 ) - 1 ) if exp == MAX_EXP - MIN_EXP + 2 : result = float ( 'nan' ) if mant else float ( 'inf' ) elif exp == 0 : result = math . ldexp ( float ( mant ) , MIN_EXP - MANT_DIG ) else : mant += 1 << MANT_DIG - 1 result = math . ldexp ( float ( mant ) , exp + MIN_EXP - MANT_DIG - 1 ) return - result if sign else result
9874	def aggregate ( l ) : tree = radix . Radix ( ) for item in l : try : tree . add ( item ) except ( ValueError ) as err : raise Exception ( "ERROR: invalid IP prefix: {}" . format ( item ) ) return aggregate_tree ( tree ) . prefixes ( )
7383	def plot_axis ( self , rs , theta ) : xs , ys = get_cartesian ( rs , theta ) self . ax . plot ( xs , ys , 'black' , alpha = 0.3 )
12784	def get_xdg_dirs ( self ) : config_dirs = getenv ( 'XDG_CONFIG_DIRS' , '' ) if config_dirs : self . _log . debug ( 'XDG_CONFIG_DIRS is set to %r' , config_dirs ) output = [ ] for path in reversed ( config_dirs . split ( ':' ) ) : output . append ( join ( path , self . group_name , self . app_name ) ) return output return [ '/etc/xdg/%s/%s' % ( self . group_name , self . app_name ) ]
10014	def create_archive ( directory , filename , config = { } , ignore_predicate = None , ignored_files = [ '.git' , '.svn' ] ) : with zipfile . ZipFile ( filename , 'w' , compression = zipfile . ZIP_DEFLATED ) as zip_file : root_len = len ( os . path . abspath ( directory ) ) out ( "Creating archive: " + str ( filename ) ) for root , dirs , files in os . walk ( directory , followlinks = True ) : archive_root = os . path . abspath ( root ) [ root_len + 1 : ] for f in files : fullpath = os . path . join ( root , f ) archive_name = os . path . join ( archive_root , f ) if filename in fullpath : continue if ignored_files is not None : for name in ignored_files : if fullpath . endswith ( name ) : out ( "Skipping: " + str ( name ) ) continue if ignore_predicate is not None : if not ignore_predicate ( archive_name ) : out ( "Skipping: " + str ( archive_name ) ) continue out ( "Adding: " + str ( archive_name ) ) zip_file . write ( fullpath , archive_name , zipfile . ZIP_DEFLATED ) return filename
172	def draw_points_heatmap_array ( self , image_shape , alpha = 1.0 , size = 1 , raise_if_out_of_image = False ) : assert len ( image_shape ) == 2 or ( len ( image_shape ) == 3 and image_shape [ - 1 ] == 1 ) , ( "Expected (H,W) or (H,W,1) as image_shape, got %s." % ( image_shape , ) ) arr = self . draw_points_on_image ( np . zeros ( image_shape , dtype = np . uint8 ) , color = 255 , alpha = alpha , size = size , raise_if_out_of_image = raise_if_out_of_image ) return arr . astype ( np . float32 ) / 255.0
5782	def select_read ( self , timeout = None ) : if len ( self . _decrypted_bytes ) > 0 : return True read_ready , _ , _ = select . select ( [ self . _socket ] , [ ] , [ ] , timeout ) return len ( read_ready ) > 0
595	def _getTPClass ( temporalImp ) : if temporalImp == 'py' : return backtracking_tm . BacktrackingTM elif temporalImp == 'cpp' : return backtracking_tm_cpp . BacktrackingTMCPP elif temporalImp == 'tm_py' : return backtracking_tm_shim . TMShim elif temporalImp == 'tm_cpp' : return backtracking_tm_shim . TMCPPShim elif temporalImp == 'monitored_tm_py' : return backtracking_tm_shim . MonitoredTMShim else : raise RuntimeError ( "Invalid temporalImp '%s'. Legal values are: 'py', " "'cpp', 'tm_py', 'monitored_tm_py'" % ( temporalImp ) )
9710	def heapreplace_max ( heap , item ) : returnitem = heap [ 0 ] heap [ 0 ] = item _siftup_max ( heap , 0 ) return returnitem
13487	def get_or_create_index ( self , index_ratio , index_width ) : if not self . index_path . exists ( ) or not self . filepath . stat ( ) . st_mtime == self . index_path . stat ( ) . st_mtime : create_index ( self . filepath , self . index_path , index_ratio = index_ratio , index_width = index_width ) return IndexFile ( str ( self . index_path ) )
3543	def python_source_files ( path , tests_dirs ) : if isdir ( path ) : for root , dirs , files in os . walk ( path ) : dirs [ : ] = [ d for d in dirs if os . path . join ( root , d ) not in tests_dirs ] for filename in files : if filename . endswith ( '.py' ) : yield os . path . join ( root , filename ) else : yield path
1768	def _publish_instruction_as_executed ( self , insn ) : self . _icount += 1 self . _publish ( 'did_execute_instruction' , self . _last_pc , self . PC , insn )
10881	def delistify ( a , b = None ) : if isinstance ( b , ( tuple , list , np . ndarray ) ) : if isinstance ( a , ( tuple , list , np . ndarray ) ) : return type ( b ) ( a ) return type ( b ) ( [ a ] ) else : if isinstance ( a , ( tuple , list , np . ndarray ) ) and len ( a ) == 1 : return a [ 0 ] return a return a
7305	def process_post_form ( self , success_message = None ) : if not hasattr ( self , 'document' ) or self . document is None : self . document = self . document_type ( ) self . form = MongoModelForm ( model = self . document_type , instance = self . document , form_post_data = self . request . POST ) . get_form ( ) self . form . is_bound = True if self . form . is_valid ( ) : self . document_map_dict = MongoModelForm ( model = self . document_type ) . create_document_dictionary ( self . document_type ) self . new_document = self . document_type self . embedded_list_docs = { } if self . new_document is None : messages . error ( self . request , u"Failed to save document" ) else : self . new_document = self . new_document ( ) for form_key in self . form . cleaned_data . keys ( ) : if form_key == 'id' and hasattr ( self , 'document' ) : self . new_document . id = self . document . id continue self . process_document ( self . new_document , form_key , None ) self . new_document . save ( ) if success_message : messages . success ( self . request , success_message ) return self . form
11484	def _upload_folder_as_item ( local_folder , parent_folder_id , reuse_existing = False ) : item_id = _create_or_reuse_item ( local_folder , parent_folder_id , reuse_existing ) subdir_contents = sorted ( os . listdir ( local_folder ) ) filecount = len ( subdir_contents ) for ( ind , current_file ) in enumerate ( subdir_contents ) : file_path = os . path . join ( local_folder , current_file ) log_ind = '({0} of {1})' . format ( ind + 1 , filecount ) _create_bitstream ( file_path , current_file , item_id , log_ind ) for callback in session . item_upload_callbacks : callback ( session . communicator , session . token , item_id )
9036	def walk_rows ( self , mapping = identity ) : row_in_grid = self . _walk . row_in_grid return map ( lambda row : mapping ( row_in_grid ( row ) ) , self . _rows )
2934	def merge_option_and_config_str ( cls , option_name , config , options ) : opt = getattr ( options , option_name , None ) if opt : config . set ( CONFIG_SECTION_NAME , option_name , opt ) elif config . has_option ( CONFIG_SECTION_NAME , option_name ) : setattr ( options , option_name , config . get ( CONFIG_SECTION_NAME , option_name ) )
9394	def plot_cdf ( self , graphing_library = 'matplotlib' ) : graphed = False for percentile_csv in self . percentiles_files : csv_filename = os . path . basename ( percentile_csv ) column = self . csv_column_map [ percentile_csv . replace ( ".percentiles." , "." ) ] if not self . check_important_sub_metrics ( column ) : continue column = naarad . utils . sanitize_string ( column ) graph_title = '.' . join ( csv_filename . split ( '.' ) [ 0 : - 1 ] ) if self . sub_metric_description and column in self . sub_metric_description . keys ( ) : graph_title += ' (' + self . sub_metric_description [ column ] + ')' if self . sub_metric_unit and column in self . sub_metric_unit . keys ( ) : plot_data = [ PD ( input_csv = percentile_csv , csv_column = 1 , series_name = graph_title , x_label = 'Percentiles' , y_label = column + ' (' + self . sub_metric_unit [ column ] + ')' , precision = None , graph_height = 600 , graph_width = 1200 , graph_type = 'line' ) ] else : plot_data = [ PD ( input_csv = percentile_csv , csv_column = 1 , series_name = graph_title , x_label = 'Percentiles' , y_label = column , precision = None , graph_height = 600 , graph_width = 1200 , graph_type = 'line' ) ] graphed , div_file = Metric . graphing_modules [ graphing_library ] . graph_data_on_the_same_graph ( plot_data , self . resource_directory , self . resource_path , graph_title ) if graphed : self . plot_files . append ( div_file ) return True
679	def getRecord ( self , n = None ) : if n is None : assert len ( self . fields ) > 0 n = self . fields [ 0 ] . numRecords - 1 assert ( all ( field . numRecords > n for field in self . fields ) ) record = [ field . values [ n ] for field in self . fields ] return record
4371	def spawn ( self , fn , * args , ** kwargs ) : if hasattr ( self , 'exception_handler_decorator' ) : fn = self . exception_handler_decorator ( fn ) new = gevent . spawn ( fn , * args , ** kwargs ) self . jobs . append ( new ) return new
8228	def show ( self , format = 'png' , as_data = False ) : from io import BytesIO b = BytesIO ( ) if format == 'png' : from IPython . display import Image surface = cairo . ImageSurface ( cairo . FORMAT_ARGB32 , self . WIDTH , self . HEIGHT ) self . snapshot ( surface ) surface . write_to_png ( b ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return Image ( data ) elif format == 'svg' : from IPython . display import SVG surface = cairo . SVGSurface ( b , self . WIDTH , self . HEIGHT ) surface . finish ( ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return SVG ( data )
11033	def parse ( self , value : str , type_ : typing . Type [ typing . Any ] = str , subtype : typing . Type [ typing . Any ] = str , ) -> typing . Any : if type_ is bool : return type_ ( value . lower ( ) in self . TRUE_STRINGS ) try : if isinstance ( type_ , type ) and issubclass ( type_ , ( list , tuple , set , frozenset ) ) : return type_ ( self . parse ( v . strip ( " " ) , subtype ) for v in value . split ( "," ) if value . strip ( " " ) ) return type_ ( value ) except ValueError as e : raise ConfigError ( * e . args )
12357	def connect ( self , interactive = False ) : from poseidon . ssh import SSHClient rs = SSHClient ( self . ip_address , interactive = interactive ) return rs
9495	def _parse_document_id ( elm_tree ) : xpath = '//md:content-id/text()' return [ x for x in elm_tree . xpath ( xpath , namespaces = COLLECTION_NSMAP ) ] [ 0 ]
13653	def Text ( name , encoding = None ) : def _match ( request , value ) : return name , query . Text ( value , encoding = contentEncoding ( request . requestHeaders , encoding ) ) return _match
5086	def has_implicit_access_to_catalog ( user , obj ) : request = get_request_or_stub ( ) decoded_jwt = get_decoded_jwt_from_request ( request ) return request_user_has_implicit_access_via_jwt ( decoded_jwt , ENTERPRISE_CATALOG_ADMIN_ROLE , obj )
9472	def validateRequest ( self , uri , postVars , expectedSignature ) : s = uri for k , v in sorted ( postVars . items ( ) ) : s += k + v return ( base64 . encodestring ( hmac . new ( self . auth_token , s , sha1 ) . digest ( ) ) . strip ( ) == expectedSignature )
10174	def set_bookmark ( self ) : def _success_date ( ) : bookmark = { 'date' : self . new_bookmark or datetime . datetime . utcnow ( ) . strftime ( self . doc_id_suffix ) } yield dict ( _index = self . last_index_written , _type = self . bookmark_doc_type , _source = bookmark ) if self . last_index_written : bulk ( self . client , _success_date ( ) , stats_only = True )
529	def Array ( dtype , size = None , ref = False ) : def getArrayType ( self ) : return self . _dtype if ref : assert size is None index = basicTypes . index ( dtype ) if index == - 1 : raise Exception ( 'Invalid data type: ' + dtype ) if size and size <= 0 : raise Exception ( 'Array size must be positive' ) suffix = 'ArrayRef' if ref else 'Array' arrayFactory = getattr ( engine_internal , dtype + suffix ) arrayFactory . getType = getArrayType if size : a = arrayFactory ( size ) else : a = arrayFactory ( ) a . _dtype = basicTypes [ index ] return a
10853	def harris_feature ( im , region_size = 5 , to_return = 'harris' , scale = 0.05 ) : ndim = im . ndim grads = [ nd . sobel ( im , axis = i ) for i in range ( ndim ) ] matrix = np . zeros ( ( ndim , ndim ) + im . shape ) for a in range ( ndim ) : for b in range ( ndim ) : matrix [ a , b ] = nd . filters . gaussian_filter ( grads [ a ] * grads [ b ] , region_size ) if to_return == 'matrix' : return matrix trc = np . trace ( matrix , axis1 = 0 , axis2 = 1 ) det = np . linalg . det ( matrix . T ) . T if to_return == 'trace-determinant' : return trc , det else : harris = det - scale * trc * trc return harris
9385	def convert_to_G ( self , word ) : value = 0.0 if word [ - 1 ] == 'G' or word [ - 1 ] == 'g' : value = float ( word [ : - 1 ] ) elif word [ - 1 ] == 'M' or word [ - 1 ] == 'm' : value = float ( word [ : - 1 ] ) / 1000.0 elif word [ - 1 ] == 'K' or word [ - 1 ] == 'k' : value = float ( word [ : - 1 ] ) / 1000.0 / 1000.0 else : value = float ( word ) / 1000.0 / 1000.0 / 1000.0 return str ( value )
10548	def delete_taskrun ( taskrun_id ) : try : res = _pybossa_req ( 'delete' , 'taskrun' , taskrun_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : raise
5528	def open ( config , mode = "continue" , zoom = None , bounds = None , single_input_file = None , with_cache = False , debug = False ) : return Mapchete ( MapcheteConfig ( config , mode = mode , zoom = zoom , bounds = bounds , single_input_file = single_input_file , debug = debug ) , with_cache = with_cache )
8888	def fit ( self , X ) : X = iter2array ( X , dtype = ReactionContainer ) self . _train_signatures = { self . __get_signature ( x ) for x in X } return self
5615	def read_vector_window ( input_files , tile , validity_check = True ) : if not isinstance ( input_files , list ) : input_files = [ input_files ] return [ feature for feature in chain . from_iterable ( [ _read_vector_window ( path , tile , validity_check = validity_check ) for path in input_files ] ) ]
12105	def _qsub_block ( self , output_dir , error_dir , tid_specs ) : processes = [ ] job_names = [ ] for ( tid , spec ) in tid_specs : job_name = "%s_%s_tid_%d" % ( self . batch_name , self . job_timestamp , tid ) job_names . append ( job_name ) cmd_args = self . command ( self . command . _formatter ( spec ) , tid , self . _launchinfo ) popen_args = self . _qsub_args ( [ ( "-e" , error_dir ) , ( '-N' , job_name ) , ( "-o" , output_dir ) ] , cmd_args ) p = subprocess . Popen ( popen_args , stdout = subprocess . PIPE ) ( stdout , stderr ) = p . communicate ( ) self . debug ( stdout ) if p . poll ( ) != 0 : raise EnvironmentError ( "qsub command exit with code: %d" % p . poll ( ) ) processes . append ( p ) self . message ( "Invoked qsub for %d commands" % len ( processes ) ) if ( self . reduction_fn is not None ) or self . dynamic : self . _qsub_collate_and_launch ( output_dir , error_dir , job_names )
3481	def read_sbml_model ( filename , number = float , f_replace = F_REPLACE , set_missing_bounds = False , ** kwargs ) : try : doc = _get_doc_from_filename ( filename ) return _sbml_to_model ( doc , number = number , f_replace = f_replace , set_missing_bounds = set_missing_bounds , ** kwargs ) except IOError as e : raise e except Exception : LOGGER . error ( traceback . print_exc ( ) ) raise CobraSBMLError ( "Something went wrong reading the SBML model. Most likely the SBML" " model is not valid. Please check that your model is valid using " "the `cobra.io.sbml.validate_sbml_model` function or via the " "online validator at http://sbml.org/validator .\n" "\t`(model, errors) = validate_sbml_model(filename)`" "\nIf the model is valid and cannot be read please open an issue " "at https://github.com/opencobra/cobrapy/issues ." )
1439	def update_sent_packet ( self , sent_pkt_size_bytes ) : self . update_count ( self . SENT_PKT_COUNT ) self . update_count ( self . SENT_PKT_SIZE , incr_by = sent_pkt_size_bytes )
8496	def _parse_and_output ( filename , args ) : relpath = os . path . dirname ( filename ) if os . path . isfile ( filename ) : calls = _parse_file ( filename , relpath ) elif os . path . isdir ( filename ) : calls = _parse_dir ( filename , relpath ) else : _error ( "Could not determine file type: %r" , filename ) if not calls : _error ( "No pyconfig calls." ) if args . load_configs : keys = set ( ) for call in calls : keys . add ( call . key ) conf = pyconfig . Config ( ) for key , value in conf . settings . items ( ) : if key in keys : continue calls . append ( _PyconfigCall ( 'set' , key , value , [ None ] * 4 ) ) _output ( calls , args )
12624	def recursive_dir_match ( folder_path , regex = '' ) : outlist = [ ] for root , dirs , files in os . walk ( folder_path ) : outlist . extend ( [ op . join ( root , f ) for f in dirs if re . match ( regex , f ) ] ) return outlist
3994	def _load_ssh_auth_pre_yosemite ( ) : for process in psutil . process_iter ( ) : if process . name ( ) == 'ssh-agent' : ssh_auth_sock = subprocess . check_output ( [ 'launchctl' , 'bsexec' , str ( process . pid ) , 'launchctl' , 'getenv' , 'SSH_AUTH_SOCK' ] ) . rstrip ( ) if ssh_auth_sock : _set_ssh_auth_sock ( ssh_auth_sock ) break else : daemon_warnings . warn ( 'ssh' , 'No running ssh-agent found linked to SSH_AUTH_SOCK' )
13902	def ensure_specifier_exists ( db_spec ) : local_match = LOCAL_RE . match ( db_spec ) remote_match = REMOTE_RE . match ( db_spec ) plain_match = PLAIN_RE . match ( db_spec ) if local_match : db_name = local_match . groupdict ( ) . get ( 'database' ) server = shortcuts . get_server ( ) if db_name not in server : server . create ( db_name ) return True elif remote_match : hostname , portnum , database = map ( remote_match . groupdict ( ) . get , ( 'hostname' , 'portnum' , 'database' ) ) server = shortcuts . get_server ( server_url = ( 'http://%s:%s' % ( hostname , portnum ) ) ) if database not in server : server . create ( database ) return True elif plain_match : db_name = plain_match . groupdict ( ) . get ( 'database' ) server = shortcuts . get_server ( ) if db_name not in server : server . create ( db_name ) return True return False
12713	def join_to ( self , joint , other_body = None , ** kwargs ) : self . world . join ( joint , self , other_body , ** kwargs )
4911	def ensure_data_exists ( self , request , data , error_message = None ) : if not data : error_message = ( error_message or "Unable to fetch API response from endpoint '{}'." . format ( request . get_full_path ( ) ) ) LOGGER . error ( error_message ) raise NotFound ( error_message )
275	def sample_colormap ( cmap_name , n_samples ) : colors = [ ] colormap = cm . cmap_d [ cmap_name ] for i in np . linspace ( 0 , 1 , n_samples ) : colors . append ( colormap ( i ) ) return colors
1119	def listdir ( path ) : try : cached_mtime , list = cache [ path ] del cache [ path ] except KeyError : cached_mtime , list = - 1 , [ ] mtime = os . stat ( path ) . st_mtime if mtime != cached_mtime : list = os . listdir ( path ) list . sort ( ) cache [ path ] = mtime , list return list
1378	def check_release_file_exists ( ) : release_file = get_heron_release_file ( ) if not os . path . isfile ( release_file ) : Log . error ( "Required file not found: %s" % release_file ) return False return True
457	def _add_notice_to_docstring ( doc , no_doc_str , notice ) : if not doc : lines = [ no_doc_str ] else : lines = _normalize_docstring ( doc ) . splitlines ( ) notice = [ '' ] + notice if len ( lines ) > 1 : if lines [ 1 ] . strip ( ) : notice . append ( '' ) lines [ 1 : 1 ] = notice else : lines += notice return '\n' . join ( lines )
4875	def validate_tpa_user_id ( self , value ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) try : tpa_client = ThirdPartyAuthApiClient ( ) username = tpa_client . get_username_from_remote_id ( enterprise_customer . identity_provider , value ) user = User . objects . get ( username = username ) return models . EnterpriseCustomerUser . objects . get ( user_id = user . id , enterprise_customer = enterprise_customer ) except ( models . EnterpriseCustomerUser . DoesNotExist , User . DoesNotExist ) : pass return None
9135	def get_connection ( module_name : str , connection : Optional [ str ] = None ) -> str : if connection is not None : return connection module_name = module_name . lower ( ) module_config_cls = get_module_config_cls ( module_name ) module_config = module_config_cls . load ( ) return module_config . connection or config . connection
10008	def get_dynamic_base ( self , bases : tuple ) : try : return self . _dynamic_bases_inverse [ bases ] except KeyError : name = self . _dynamic_base_namer . get_next ( self . _dynamic_bases ) base = self . _new_space ( name = name ) self . spacegraph . add_space ( base ) self . _dynamic_bases [ name ] = base self . _dynamic_bases_inverse [ bases ] = base base . add_bases ( bases ) return base
5659	def _validate_danglers ( self ) : for query , warning in zip ( DANGLER_QUERIES , DANGLER_WARNINGS ) : dangler_count = self . gtfs . execute_custom_query ( query ) . fetchone ( ) [ 0 ] if dangler_count > 0 : if self . verbose : print ( str ( dangler_count ) + " " + warning ) self . warnings_container . add_warning ( warning , self . location , count = dangler_count )
10142	def upload_files ( selected_file , selected_host , only_link , file_name ) : try : answer = requests . post ( url = selected_host [ 0 ] + "upload.php" , files = { 'files[]' : selected_file } ) file_name_1 = re . findall ( r'"url": *"((h.+\/){0,1}(.+?))"[,\}]' , answer . text . replace ( "\\" , "" ) ) [ 0 ] [ 2 ] if only_link : return [ selected_host [ 1 ] + file_name_1 , "{}: {}{}" . format ( file_name , selected_host [ 1 ] , file_name_1 ) ] else : return "{}: {}{}" . format ( file_name , selected_host [ 1 ] , file_name_1 ) except requests . exceptions . ConnectionError : print ( file_name + ' couldn\'t be uploaded to ' + selected_host [ 0 ] )
11150	def get_text_fingerprint ( text , hash_meth , encoding = "utf-8" ) : m = hash_meth ( ) m . update ( text . encode ( encoding ) ) return m . hexdigest ( )
10322	def spanning_1d_chain ( length ) : ret = nx . grid_graph ( dim = [ int ( length + 2 ) ] ) ret . node [ 0 ] [ 'span' ] = 0 ret [ 0 ] [ 1 ] [ 'span' ] = 0 ret . node [ length + 1 ] [ 'span' ] = 1 ret [ length ] [ length + 1 ] [ 'span' ] = 1 return ret
7064	def sqs_delete_item ( queue_url , receipt_handle , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 'sqs' ) try : client . delete_message ( QueueUrl = queue_url , ReceiptHandle = receipt_handle ) except Exception as e : LOGEXCEPTION ( 'could not delete message with receipt handle: ' '%s from queue: %s' % ( receipt_handle , queue_url ) ) if raiseonfail : raise
9989	def import_funcs ( self , module ) : newcells = self . _impl . new_cells_from_module ( module ) return get_interfaces ( newcells )
12686	def find ( self , * args ) : curr_node = self . __root return self . __traverse ( curr_node , 0 , * args )
13025	def create_payload ( self , x86_file , x64_file , payload_file ) : sc_x86 = open ( os . path . join ( self . datadir , x86_file ) , 'rb' ) . read ( ) sc_x64 = open ( os . path . join ( self . datadir , x64_file ) , 'rb' ) . read ( ) fp = open ( os . path . join ( self . datadir , payload_file ) , 'wb' ) fp . write ( b'\x31\xc0\x40\x0f\x84' + pack ( '<I' , len ( sc_x86 ) ) ) fp . write ( sc_x86 ) fp . write ( sc_x64 ) fp . close ( )
6404	def get_feature ( vector , feature ) : if feature not in _FEATURE_MASK : raise AttributeError ( "feature must be one of: '" + "', '" . join ( ( 'consonantal' , 'sonorant' , 'syllabic' , 'labial' , 'round' , 'coronal' , 'anterior' , 'distributed' , 'dorsal' , 'high' , 'low' , 'back' , 'tense' , 'pharyngeal' , 'ATR' , 'voice' , 'spread_glottis' , 'constricted_glottis' , 'continuant' , 'strident' , 'lateral' , 'delayed_release' , 'nasal' , ) ) + "'" ) mask = _FEATURE_MASK [ feature ] pos_mask = mask >> 1 retvec = [ ] for char in vector : if char < 0 : retvec . append ( float ( 'NaN' ) ) else : masked = char & mask if masked == 0 : retvec . append ( 0 ) elif masked == mask : retvec . append ( 2 ) elif masked & pos_mask : retvec . append ( 1 ) else : retvec . append ( - 1 ) return retvec
7466	def _load_existing_results ( self , name , workdir ) : path = os . path . realpath ( os . path . join ( self . workdir , self . name ) ) mcmcs = glob . glob ( path + "_r*.mcmc.txt" ) outs = glob . glob ( path + "_r*.out.txt" ) trees = glob . glob ( path + "_r*.tre" ) for mcmcfile in mcmcs : if mcmcfile not in self . files . mcmcfiles : self . files . mcmcfiles . append ( mcmcfile ) for outfile in outs : if outfile not in self . files . outfiles : self . files . outfiles . append ( outfile ) for tree in trees : if tree not in self . files . treefiles : self . files . treefiles . append ( tree )
1434	def custom_serialized ( cls , serialized , is_java = True ) : if not isinstance ( serialized , bytes ) : raise TypeError ( "Argument to custom_serialized() must be " "a serialized Python class as bytes, given: %s" % str ( serialized ) ) if not is_java : return cls . CUSTOM ( gtype = topology_pb2 . Grouping . Value ( "CUSTOM" ) , python_serialized = serialized ) else : raise NotImplementedError ( "Custom grouping implemented in Java for Python topology" "is not yet supported." )
3573	def peripheral_didUpdateValueForCharacteristic_error_ ( self , peripheral , characteristic , error ) : logger . debug ( 'peripheral_didUpdateValueForCharacteristic_error called' ) if error is not None : return device = device_list ( ) . get ( peripheral ) if device is not None : device . _characteristic_changed ( characteristic )
1289	def baseline_optimizer_arguments ( self , states , internals , reward ) : arguments = dict ( time = self . global_timestep , variables = self . baseline . get_variables ( ) , arguments = dict ( states = states , internals = internals , reward = reward , update = tf . constant ( value = True ) , ) , fn_reference = self . baseline . reference , fn_loss = self . fn_baseline_loss , ) if self . global_model is not None : arguments [ 'global_variables' ] = self . global_model . baseline . get_variables ( ) return arguments
5708	def process_request ( self , request ) : try : session = request . session except AttributeError : raise ImproperlyConfigured ( 'django-lockdown requires the Django ' 'sessions framework' ) if settings . ENABLED is False : return None if self . remote_addr_exceptions : remote_addr_exceptions = self . remote_addr_exceptions else : remote_addr_exceptions = settings . REMOTE_ADDR_EXCEPTIONS if remote_addr_exceptions : trusted_proxies = self . trusted_proxies or settings . TRUSTED_PROXIES remote_addr = request . META . get ( 'REMOTE_ADDR' ) if remote_addr in remote_addr_exceptions : return None if remote_addr in trusted_proxies : x_forwarded_for = request . META . get ( 'HTTP_X_FORWARDED_FOR' ) if x_forwarded_for : remote_addr = x_forwarded_for . split ( ',' ) [ - 1 ] . strip ( ) if remote_addr in remote_addr_exceptions : return None if self . url_exceptions : url_exceptions = compile_url_exceptions ( self . url_exceptions ) else : url_exceptions = compile_url_exceptions ( settings . URL_EXCEPTIONS ) for pattern in url_exceptions : if pattern . search ( request . path ) : return None try : resolved_path = resolve ( request . path ) except Resolver404 : pass else : if resolved_path . func in settings . VIEW_EXCEPTIONS : return None if self . until_date : until_date = self . until_date else : until_date = settings . UNTIL_DATE if self . after_date : after_date = self . after_date else : after_date = settings . AFTER_DATE if until_date or after_date : locked_date = False if until_date and datetime . datetime . now ( ) < until_date : locked_date = True if after_date and datetime . datetime . now ( ) > after_date : locked_date = True if not locked_date : return None form_data = request . POST if request . method == 'POST' else None if self . form : form_class = self . form else : form_class = get_lockdown_form ( settings . FORM ) form = form_class ( data = form_data , ** self . form_kwargs ) authorized = False token = session . get ( self . session_key ) if hasattr ( form , 'authenticate' ) : if form . authenticate ( token ) : authorized = True elif token is True : authorized = True if authorized and self . logout_key and self . logout_key in request . GET : if self . session_key in session : del session [ self . session_key ] querystring = request . GET . copy ( ) del querystring [ self . logout_key ] return self . redirect ( request ) if authorized : return None if form . is_valid ( ) : if hasattr ( form , 'generate_token' ) : token = form . generate_token ( ) else : token = True session [ self . session_key ] = token return self . redirect ( request ) page_data = { 'until_date' : until_date , 'after_date' : after_date } if not hasattr ( form , 'show_form' ) or form . show_form ( ) : page_data [ 'form' ] = form if self . extra_context : page_data . update ( self . extra_context ) return render ( request , 'lockdown/form.html' , page_data )
10612	def _calculate_T ( self , H ) : x = list ( ) x . append ( self . _T ) x . append ( self . _T + 10.0 ) y = list ( ) y . append ( self . _calculate_H ( x [ 0 ] ) - H ) y . append ( self . _calculate_H ( x [ 1 ] ) - H ) for i in range ( 2 , 50 ) : x . append ( x [ i - 1 ] - y [ i - 1 ] * ( ( x [ i - 1 ] - x [ i - 2 ] ) / ( y [ i - 1 ] - y [ i - 2 ] ) ) ) y . append ( self . _calculate_H ( x [ i ] ) - H ) if abs ( y [ i - 1 ] ) < 1.0e-5 : break return x [ len ( x ) - 1 ]
5483	def setup_service ( api_name , api_version , credentials = None ) : if not credentials : credentials = oauth2client . client . GoogleCredentials . get_application_default ( ) return apiclient . discovery . build ( api_name , api_version , credentials = credentials )
5779	def _advapi32_decrypt ( private_key , ciphertext , rsa_oaep_padding = False ) : flags = 0 if rsa_oaep_padding : flags = Advapi32Const . CRYPT_OAEP ciphertext = ciphertext [ : : - 1 ] buffer = buffer_from_bytes ( ciphertext ) out_len = new ( advapi32 , 'DWORD *' , len ( ciphertext ) ) res = advapi32 . CryptDecrypt ( private_key . ex_key_handle , null ( ) , True , flags , buffer , out_len ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) )
6361	def sim_matrix ( src , tar , mat = None , mismatch_cost = 0 , match_cost = 1 , symmetric = True , alphabet = None , ) : if alphabet : alphabet = tuple ( alphabet ) for i in src : if i not in alphabet : raise ValueError ( 'src value not in alphabet' ) for i in tar : if i not in alphabet : raise ValueError ( 'tar value not in alphabet' ) if src == tar : if mat and ( src , src ) in mat : return mat [ ( src , src ) ] return match_cost if mat and ( src , tar ) in mat : return mat [ ( src , tar ) ] elif symmetric and mat and ( tar , src ) in mat : return mat [ ( tar , src ) ] return mismatch_cost
5817	def _write_callback ( connection_id , data_buffer , data_length_pointer ) : try : self = _connection_refs . get ( connection_id ) if not self : socket = _socket_refs . get ( connection_id ) else : socket = self . _socket if not self and not socket : return 0 data_length = deref ( data_length_pointer ) data = bytes_from_buffer ( data_buffer , data_length ) if self and not self . _done_handshake : self . _client_hello += data error = None try : sent = socket . send ( data ) except ( socket_ . error ) as e : error = e . errno if error is not None and error != errno . EAGAIN : if error == errno . ECONNRESET or error == errno . EPIPE : return SecurityConst . errSSLClosedNoNotify return SecurityConst . errSSLClosedAbort if sent != data_length : pointer_set ( data_length_pointer , sent ) return SecurityConst . errSSLWouldBlock return 0 except ( KeyboardInterrupt ) as e : self . _exception = e return SecurityConst . errSSLPeerUserCancelled
11394	def mock_request ( ) : current_site = Site . objects . get_current ( ) request = HttpRequest ( ) request . META [ 'SERVER_NAME' ] = current_site . domain return request
1557	def _get_stream_id ( comp_name , stream_id ) : proto_stream_id = topology_pb2 . StreamId ( ) proto_stream_id . id = stream_id proto_stream_id . component_name = comp_name return proto_stream_id
511	def _updateMinDutyCyclesGlobal ( self ) : self . _minOverlapDutyCycles . fill ( self . _minPctOverlapDutyCycles * self . _overlapDutyCycles . max ( ) )
2117	def convert ( self , value , param , ctx ) : choice = super ( MappedChoice , self ) . convert ( value , param , ctx ) ix = self . choices . index ( choice ) return self . actual_choices [ ix ]
2189	def _rectify_products ( self , product = None ) : products = self . product if product is None else product if products is None : return None if not isinstance ( products , ( list , tuple ) ) : products = [ products ] return products
7391	def draw ( self ) : self . ax . set_xlim ( - self . plot_radius ( ) , self . plot_radius ( ) ) self . ax . set_ylim ( - self . plot_radius ( ) , self . plot_radius ( ) ) self . add_axes_and_nodes ( ) self . add_edges ( ) self . ax . axis ( 'off' )
5870	def fetch_organization_courses ( organization ) : organization_obj = serializers . deserialize_organization ( organization ) queryset = internal . OrganizationCourse . objects . filter ( organization = organization_obj , active = True ) . select_related ( 'organization' ) return [ serializers . serialize_organization_with_course ( organization ) for organization in queryset ]
11558	def get_stepper_version ( self , timeout = 20 ) : start_time = time . time ( ) while self . _command_handler . stepper_library_version <= 0 : if time . time ( ) - start_time > timeout : if self . verbose is True : print ( "Stepper Library Version Request timed-out. " "Did you send a stepper_request_library_version command?" ) return else : pass return self . _command_handler . stepper_library_version
12200	def from_jsonfile ( cls , fp , selector_handler = None , strict = False , debug = False ) : return cls . _from_jsonlines ( fp , selector_handler = selector_handler , strict = strict , debug = debug )
2063	def is_declared ( self , expression_var ) : if not isinstance ( expression_var , Variable ) : raise ValueError ( f'Expression must be a Variable (not a {type(expression_var)})' ) return any ( expression_var is x for x in self . get_declared_variables ( ) )
1579	def create_packet ( reqid , message ) : assert message . IsInitialized ( ) packet = '' typename = message . DESCRIPTOR . full_name datasize = HeronProtocol . get_size_to_pack_string ( typename ) + REQID . REQID_SIZE + HeronProtocol . get_size_to_pack_message ( message ) packet += HeronProtocol . pack_int ( datasize ) packet += HeronProtocol . pack_int ( len ( typename ) ) packet += typename packet += reqid . pack ( ) packet += HeronProtocol . pack_int ( message . ByteSize ( ) ) packet += message . SerializeToString ( ) return OutgoingPacket ( packet )
1011	def trimSegments ( self , minPermanence = None , minNumSyns = None ) : if minPermanence is None : minPermanence = self . connectedPerm if minNumSyns is None : minNumSyns = self . activationThreshold totalSegsRemoved , totalSynsRemoved = 0 , 0 for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : ( segsRemoved , synsRemoved ) = self . _trimSegmentsInCell ( colIdx = c , cellIdx = i , segList = self . cells [ c ] [ i ] , minPermanence = minPermanence , minNumSyns = minNumSyns ) totalSegsRemoved += segsRemoved totalSynsRemoved += synsRemoved if self . verbosity >= 5 : print "Cells, all segments:" self . printCells ( predictedOnly = False ) return totalSegsRemoved , totalSynsRemoved
4362	def _heartbeat ( self ) : interval = self . config [ 'heartbeat_interval' ] while self . connected : gevent . sleep ( interval ) self . put_client_msg ( "2::" )
9317	def refresh ( self , accept = MEDIA_TYPE_TAXII_V20 ) : response = self . __raw = self . _conn . get ( self . url , headers = { "Accept" : accept } ) self . _populate_fields ( ** response )
2597	def can ( obj ) : import_needed = False for cls , canner in iteritems ( can_map ) : if isinstance ( cls , string_types ) : import_needed = True break elif istype ( obj , cls ) : return canner ( obj ) if import_needed : _import_mapping ( can_map , _original_can_map ) return can ( obj ) return obj
517	def _bumpUpWeakColumns ( self ) : weakColumns = numpy . where ( self . _overlapDutyCycles < self . _minOverlapDutyCycles ) [ 0 ] for columnIndex in weakColumns : perm = self . _permanences [ columnIndex ] . astype ( realDType ) maskPotential = numpy . where ( self . _potentialPools [ columnIndex ] > 0 ) [ 0 ] perm [ maskPotential ] += self . _synPermBelowStimulusInc self . _updatePermanencesForColumn ( perm , columnIndex , raisePerm = False )
10134	def add_item ( self , key , value , after = False , index = None , pos_key = None , replace = True ) : if self . _validate_fn : self . _validate_fn ( value ) if ( index is not None ) and ( pos_key is not None ) : raise ValueError ( 'Either specify index or pos_key, not both.' ) elif pos_key is not None : try : index = self . index ( pos_key ) except ValueError : raise KeyError ( '%r not found' % pos_key ) if after and ( index is not None ) : index += 1 if key in self . _values : if not replace : raise KeyError ( '%r is duplicate' % key ) if index is not None : del self [ key ] else : self . _values [ key ] = value return if index is not None : self . _order . insert ( index , key ) else : self . _order . append ( key ) self . _values [ key ] = value
5566	def params_at_zoom ( self , zoom ) : if zoom not in self . init_zoom_levels : raise ValueError ( "zoom level not available with current configuration" ) out = dict ( self . _params_at_zoom [ zoom ] , input = { } , output = self . output ) if "input" in self . _params_at_zoom [ zoom ] : flat_inputs = { } for k , v in _flatten_tree ( self . _params_at_zoom [ zoom ] [ "input" ] ) : if v is None : flat_inputs [ k ] = None else : flat_inputs [ k ] = self . input [ get_hash ( v ) ] out [ "input" ] = _unflatten_tree ( flat_inputs ) else : out [ "input" ] = { } return out
9178	def obtain_licenses ( ) : with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( ) licenses = { r [ 0 ] : r [ 1 ] for r in cursor . fetchall ( ) } return licenses
10056	def put ( self , pid , record , key ) : try : data = json . loads ( request . data . decode ( 'utf-8' ) ) new_key = data [ 'filename' ] except KeyError : raise WrongFile ( ) new_key_secure = secure_filename ( new_key ) if not new_key_secure or new_key != new_key_secure : raise WrongFile ( ) try : obj = record . files . rename ( str ( key ) , new_key_secure ) except KeyError : abort ( 404 ) record . commit ( ) db . session . commit ( ) return self . make_response ( obj = obj , pid = pid , record = record )
8089	def text ( self , txt , x , y , width = None , height = 1000000 , outline = False , draw = True , ** kwargs ) : txt = self . Text ( txt , x , y , width , height , outline = outline , ctx = None , ** kwargs ) if outline : path = txt . path if draw : path . draw ( ) return path else : return txt
9801	def config ( list ) : if list : _config = GlobalConfigManager . get_config_or_default ( ) Printer . print_header ( 'Current config:' ) dict_tabulate ( _config . to_dict ( ) )
1347	def clone ( git_uri ) : hash_digest = sha256_hash ( git_uri ) local_path = home_directory_path ( FOLDER , hash_digest ) exists_locally = path_exists ( local_path ) if not exists_locally : _clone_repo ( git_uri , local_path ) else : logging . info ( "Git repository already exists locally." ) return local_path
8243	def guess_name ( clr ) : clr = Color ( clr ) if clr . is_transparent : return "transparent" if clr . is_black : return "black" if clr . is_white : return "white" if clr . is_black : return "black" for name in named_colors : try : r , g , b = named_colors [ name ] except : continue if r == clr . r and g == clr . g and b == clr . b : return name for shade in shades : if clr in shade : return shade . name + " " + clr . nearest_hue ( ) break return clr . nearest_hue ( )
12303	def url_is_valid ( self , url ) : if url . startswith ( "file://" ) : url = url . replace ( "file://" , "" ) return os . path . exists ( url )
7133	def add_icon ( icon_data , dest ) : with open ( os . path . join ( dest , "icon.png" ) , "wb" ) as f : f . write ( icon_data )
11045	def init_storage_dir ( storage_dir ) : storage_path = FilePath ( storage_dir ) default_cert_path = storage_path . child ( 'default.pem' ) if not default_cert_path . exists ( ) : default_cert_path . setContent ( generate_wildcard_pem_bytes ( ) ) unmanaged_certs_path = storage_path . child ( 'unmanaged-certs' ) if not unmanaged_certs_path . exists ( ) : unmanaged_certs_path . createDirectory ( ) certs_path = storage_path . child ( 'certs' ) if not certs_path . exists ( ) : certs_path . createDirectory ( ) return storage_path , certs_path
5899	def get_double_or_single_prec_mdrun ( ) : try : gromacs . mdrun_d ( h = True , stdout = False , stderr = False ) logger . debug ( "using double precision gromacs.mdrun_d" ) return gromacs . mdrun_d except ( AttributeError , GromacsError , OSError ) : wmsg = "No 'mdrun_d' binary found so trying 'mdrun' instead.\n" "(Note that energy minimization runs better with mdrun_d.)" logger . warn ( wmsg ) warnings . warn ( wmsg , category = AutoCorrectionWarning ) return gromacs . mdrun
293	def plot_gross_leverage ( returns , positions , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) gl = timeseries . gross_lev ( positions ) gl . plot ( lw = 0.5 , color = 'limegreen' , legend = False , ax = ax , ** kwargs ) ax . axhline ( gl . mean ( ) , color = 'g' , linestyle = '--' , lw = 3 ) ax . set_title ( 'Gross leverage' ) ax . set_ylabel ( 'Gross leverage' ) ax . set_xlabel ( '' ) return ax
6889	def _parallel_bls_worker ( task ) : try : times , mags , errs = task [ : 3 ] magsarefluxes = task [ 3 ] minfreq , nfreq , stepsize = task [ 4 : 7 ] ndurations , mintransitduration , maxtransitduration = task [ 7 : 10 ] blsobjective , blsmethod , blsoversample = task [ 10 : ] frequencies = minfreq + nparange ( nfreq ) * stepsize periods = 1.0 / frequencies durations = nplinspace ( mintransitduration * periods . min ( ) , maxtransitduration * periods . min ( ) , ndurations ) if magsarefluxes : blsmodel = BoxLeastSquares ( times * u . day , mags * u . dimensionless_unscaled , dy = errs * u . dimensionless_unscaled ) else : blsmodel = BoxLeastSquares ( times * u . day , mags * u . mag , dy = errs * u . mag ) blsresult = blsmodel . power ( periods * u . day , durations * u . day , objective = blsobjective , method = blsmethod , oversample = blsoversample ) return { 'blsresult' : blsresult , 'blsmodel' : blsmodel , 'durations' : durations , 'power' : nparray ( blsresult . power ) } except Exception as e : LOGEXCEPTION ( 'BLS for frequency chunk: (%.6f, %.6f) failed.' % ( frequencies [ 0 ] , frequencies [ - 1 ] ) ) return { 'blsresult' : None , 'blsmodel' : None , 'durations' : durations , 'power' : nparray ( [ npnan for x in range ( nfreq ) ] ) , }
5158	def _add_uninstall ( self , context ) : contents = self . _render_template ( 'uninstall.sh' , context ) self . config . setdefault ( 'files' , [ ] ) self . _add_unique_file ( { "path" : "/uninstall.sh" , "contents" : contents , "mode" : "755" } )
3145	def delete ( self , file_id ) : self . file_id = file_id return self . _mc_client . _delete ( url = self . _build_path ( file_id ) )
8875	def allele_frequency ( expec ) : r expec = asarray ( expec , float ) if expec . ndim != 2 : raise ValueError ( "Expectation matrix must be bi-dimensional." ) ploidy = expec . shape [ - 1 ] return expec . sum ( - 2 ) / ploidy
2718	def remove_droplets ( self , droplet ) : droplets = droplet if not isinstance ( droplets , list ) : droplets = [ droplet ] resources = self . __extract_resources_from_droplets ( droplets ) if len ( resources ) > 0 : return self . __remove_resources ( resources ) return False
13695	def debug ( * args , ** kwargs ) : if not ( DEBUG and args ) : return None parent = kwargs . get ( 'parent' , None ) with suppress ( KeyError ) : kwargs . pop ( 'parent' ) backlevel = kwargs . get ( 'back' , 1 ) with suppress ( KeyError ) : kwargs . pop ( 'back' ) frame = inspect . currentframe ( ) while backlevel > 0 : frame = frame . f_back backlevel -= 1 fname = os . path . split ( frame . f_code . co_filename ) [ - 1 ] lineno = frame . f_lineno if parent : func = '{}.{}' . format ( parent . __class__ . __name__ , frame . f_code . co_name ) else : func = frame . f_code . co_name lineinfo = '{}:{} {}: ' . format ( C ( fname , 'yellow' ) , C ( str ( lineno ) . ljust ( 4 ) , 'blue' ) , C ( ) . join ( C ( func , 'magenta' ) , '()' ) . ljust ( 20 ) ) pargs = list ( C ( a , 'green' ) . str ( ) for a in args ) pargs [ 0 ] = '' . join ( ( lineinfo , pargs [ 0 ] ) ) print_err ( * pargs , ** kwargs )
9412	def _is_simple_numeric ( data ) : for item in data : if isinstance ( item , set ) : item = list ( item ) if isinstance ( item , list ) : if not _is_simple_numeric ( item ) : return False elif not isinstance ( item , ( int , float , complex ) ) : return False return True
3756	def Tautoignition ( CASRN , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in IEC_2010 . index and not np . isnan ( IEC_2010 . at [ CASRN , 'Tautoignition' ] ) : methods . append ( IEC ) if CASRN in NFPA_2008 . index and not np . isnan ( NFPA_2008 . at [ CASRN , 'Tautoignition' ] ) : methods . append ( NFPA ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IEC : return float ( IEC_2010 . at [ CASRN , 'Tautoignition' ] ) elif Method == NFPA : return float ( NFPA_2008 . at [ CASRN , 'Tautoignition' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
11610	def update_probability_at_read_level ( self , model = 3 ) : self . probability . reset ( ) if model == 1 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . HAPLOGROUP , grouping_mat = self . t2t_mat ) haplogroup_sum_mat = self . allelic_expression * self . t2t_mat self . probability . multiply ( haplogroup_sum_mat , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . GROUP , grouping_mat = self . t2t_mat ) self . probability . multiply ( haplogroup_sum_mat . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . READ ) elif model == 2 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . LOCUS ) self . probability . multiply ( self . allelic_expression . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . GROUP , grouping_mat = self . t2t_mat ) self . probability . multiply ( ( self . allelic_expression * self . t2t_mat ) . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . READ ) elif model == 3 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . GROUP , grouping_mat = self . t2t_mat ) self . probability . multiply ( ( self . allelic_expression * self . t2t_mat ) . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . READ ) elif model == 4 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . READ ) else : raise RuntimeError ( 'The read normalization model should be 1, 2, 3, or 4.' )
6936	def cp_objectinfo_worker ( task ) : cpf , cpkwargs = task try : newcpf = update_checkplot_objectinfo ( cpf , ** cpkwargs ) return newcpf except Exception as e : LOGEXCEPTION ( 'failed to update objectinfo for %s' % cpf ) return None
2805	def convert_elementwise_add ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting elementwise_add ...' ) if 'broadcast' in params : model0 = layers [ inputs [ 0 ] ] model1 = layers [ inputs [ 1 ] ] if names == 'short' : tf_name = 'A' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) def target_layer ( x ) : layer = tf . add ( x [ 0 ] , x [ 1 ] ) return layer lambda_layer = keras . layers . Lambda ( target_layer , name = tf_name ) layers [ scope_name ] = lambda_layer ( [ layers [ inputs [ 0 ] ] , layers [ inputs [ 1 ] ] ] ) else : model0 = layers [ inputs [ 0 ] ] model1 = layers [ inputs [ 1 ] ] if names == 'short' : tf_name = 'A' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) add = keras . layers . Add ( name = tf_name ) layers [ scope_name ] = add ( [ model0 , model1 ] )
10240	def count_author_publications ( graph : BELGraph ) -> typing . Counter [ str ] : authors = group_as_dict ( _iter_author_publiations ( graph ) ) return Counter ( count_dict_values ( count_defaultdict ( authors ) ) )
7086	def _single_true ( iterable ) : iterator = iter ( iterable ) has_true = any ( iterator ) has_another_true = any ( iterator ) return has_true and not has_another_true
11485	def upload ( file_pattern , destination = 'Private' , leaf_folders_as_items = False , reuse_existing = False ) : session . token = verify_credentials ( ) parent_folder_id = None user_folders = session . communicator . list_user_folders ( session . token ) if destination . startswith ( '/' ) : parent_folder_id = _find_resource_id_from_path ( destination ) else : for cur_folder in user_folders : if cur_folder [ 'name' ] == destination : parent_folder_id = cur_folder [ 'folder_id' ] if parent_folder_id is None : print ( 'Unable to locate specified destination. Defaulting to {0}.' . format ( user_folders [ 0 ] [ 'name' ] ) ) parent_folder_id = user_folders [ 0 ] [ 'folder_id' ] for current_file in glob . iglob ( file_pattern ) : current_file = os . path . normpath ( current_file ) if os . path . isfile ( current_file ) : print ( 'Uploading item from {0}' . format ( current_file ) ) _upload_as_item ( os . path . basename ( current_file ) , parent_folder_id , current_file , reuse_existing ) else : _upload_folder_recursive ( current_file , parent_folder_id , leaf_folders_as_items , reuse_existing )
8184	def update ( self , iterations = 10 ) : self . alpha += 0.05 self . alpha = min ( self . alpha , 1.0 ) if self . layout . i == 0 : self . layout . prepare ( ) self . layout . i += 1 elif self . layout . i == 1 : self . layout . iterate ( ) elif self . layout . i < self . layout . n : n = min ( iterations , self . layout . i / 10 + 1 ) for i in range ( n ) : self . layout . iterate ( ) min_ , max = self . layout . bounds self . x = _ctx . WIDTH - max . x * self . d - min_ . x * self . d self . y = _ctx . HEIGHT - max . y * self . d - min_ . y * self . d self . x /= 2 self . y /= 2 return not self . layout . done
10734	def fork ( self , name ) : fork = deepcopy ( self ) self [ name ] = fork return fork
2288	def forward ( self ) : self . noise . data . normal_ ( ) if not self . confounding : for i in self . topological_order : self . generated [ i ] = self . blocks [ i ] ( th . cat ( [ v for c in [ [ self . generated [ j ] for j in np . nonzero ( self . adjacency_matrix [ : , i ] ) [ 0 ] ] , [ self . noise [ : , [ i ] ] ] ] for v in c ] , 1 ) ) else : for i in self . topological_order : self . generated [ i ] = self . blocks [ i ] ( th . cat ( [ v for c in [ [ self . generated [ j ] for j in np . nonzero ( self . adjacency_matrix [ : , i ] ) [ 0 ] ] , [ self . corr_noise [ min ( i , j ) , max ( i , j ) ] for j in np . nonzero ( self . i_adj_matrix [ : , i ] ) [ 0 ] ] [ self . noise [ : , [ i ] ] ] ] for v in c ] , 1 ) ) return th . cat ( self . generated , 1 )
6215	def load_glb ( self ) : with open ( self . path , 'rb' ) as fd : magic = fd . read ( 4 ) if magic != GLTF_MAGIC_HEADER : raise ValueError ( "{} has incorrect header {} != {}" . format ( self . path , magic , GLTF_MAGIC_HEADER ) ) version = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] if version != 2 : raise ValueError ( "{} has unsupported version {}" . format ( self . path , version ) ) _ = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] chunk_0_length = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] chunk_0_type = fd . read ( 4 ) if chunk_0_type != b'JSON' : raise ValueError ( "Expected JSON chunk, not {} in file {}" . format ( chunk_0_type , self . path ) ) json_meta = fd . read ( chunk_0_length ) . decode ( ) chunk_1_length = struct . unpack ( '<I' , fd . read ( 4 ) ) [ 0 ] chunk_1_type = fd . read ( 4 ) if chunk_1_type != b'BIN\x00' : raise ValueError ( "Expected BIN chunk, not {} in file {}" . format ( chunk_1_type , self . path ) ) self . meta = GLTFMeta ( self . path , json . loads ( json_meta ) , binary_buffer = fd . read ( chunk_1_length ) )
8206	def overlap ( self , x1 , y1 , x2 , y2 , r = 5 ) : if abs ( x2 - x1 ) < r and abs ( y2 - y1 ) < r : return True else : return False
10915	def _check_groups ( s , groups ) : ans = [ ] for g in groups : ans . extend ( g ) if np . unique ( ans ) . size != np . size ( ans ) : return False elif np . unique ( ans ) . size != s . obj_get_positions ( ) . shape [ 0 ] : return False else : return ( np . arange ( s . obj_get_radii ( ) . size ) == np . sort ( ans ) ) . all ( )
7799	def check_password ( self , username , password , properties ) : logger . debug ( "check_password{0!r}" . format ( ( username , password , properties ) ) ) pwd , pwd_format = self . get_password ( username , ( u"plain" , u"md5:user:realm:password" ) , properties ) if pwd_format == u"plain" : logger . debug ( "got plain password: {0!r}" . format ( pwd ) ) return pwd is not None and password == pwd elif pwd_format in ( u"md5:user:realm:password" ) : logger . debug ( "got md5:user:realm:password password: {0!r}" . format ( pwd ) ) realm = properties . get ( "realm" ) if realm is None : realm = "" else : realm = realm . encode ( "utf-8" ) username = username . encode ( "utf-8" ) password = password . encode ( "utf-8" ) urp_hash = hashlib . md5 ( b"%s:%s:%s" ) . hexdigest ( ) return urp_hash == pwd logger . debug ( "got password in unknown format: {0!r}" . format ( pwd_format ) ) return False
11739	def move_dot ( self ) : return self . __class__ ( self . production , self . pos + 1 , self . lookahead )
9432	def _c_func ( func , restype , argtypes , errcheck = None ) : func . restype = restype func . argtypes = argtypes if errcheck is not None : func . errcheck = errcheck return func
6317	def image_data ( image ) : data = image . tobytes ( ) components = len ( data ) // ( image . size [ 0 ] * image . size [ 1 ] ) return components , data
5000	def require_at_least_one_query_parameter ( * query_parameter_names ) : def outer_wrapper ( view ) : @ wraps ( view ) def wrapper ( request , * args , ** kwargs ) : requirement_satisfied = False for query_parameter_name in query_parameter_names : query_parameter_values = request . query_params . getlist ( query_parameter_name ) kwargs [ query_parameter_name ] = query_parameter_values if query_parameter_values : requirement_satisfied = True if not requirement_satisfied : raise ValidationError ( detail = 'You must provide at least one of the following query parameters: {params}.' . format ( params = ', ' . join ( query_parameter_names ) ) ) return view ( request , * args , ** kwargs ) return wrapper return outer_wrapper
9338	def map ( self , func , sequence , reduce = None , star = False , minlength = 0 ) : def realreduce ( r ) : if reduce : if isinstance ( r , tuple ) : return reduce ( * r ) else : return reduce ( r ) return r def realfunc ( i ) : if star : return func ( * i ) else : return func ( i ) if len ( sequence ) <= 0 or self . np == 0 or get_debug ( ) : self . local = lambda : None self . local . rank = 0 rt = [ realreduce ( realfunc ( i ) ) for i in sequence ] self . local = None return rt np = min ( [ self . np , len ( sequence ) ] ) Q = self . backend . QueueFactory ( 64 ) R = self . backend . QueueFactory ( 64 ) self . ordered . reset ( ) pg = ProcessGroup ( main = self . _main , np = np , backend = self . backend , args = ( Q , R , sequence , realfunc ) ) pg . start ( ) L = [ ] N = [ ] def feeder ( pg , Q , N ) : j = 0 try : for i , work in enumerate ( sequence ) : if not hasattr ( sequence , '__getitem__' ) : pg . put ( Q , ( i , work ) ) else : pg . put ( Q , ( i , ) ) j = j + 1 N . append ( j ) for i in range ( np ) : pg . put ( Q , None ) except StopProcessGroup : return finally : pass feeder = threading . Thread ( None , feeder , args = ( pg , Q , N ) ) feeder . start ( ) count = 0 try : while True : try : capsule = pg . get ( R ) except queue . Empty : continue except StopProcessGroup : raise pg . get_exception ( ) capsule = capsule [ 0 ] , realreduce ( capsule [ 1 ] ) heapq . heappush ( L , capsule ) count = count + 1 if len ( N ) > 0 and count == N [ 0 ] : break rt = [ ] while len ( L ) > 0 : rt . append ( heapq . heappop ( L ) [ 1 ] ) pg . join ( ) feeder . join ( ) assert N [ 0 ] == len ( rt ) return rt except BaseException as e : pg . killall ( ) pg . join ( ) feeder . join ( ) raise
4536	def fillHSV ( self , hsv , start = 0 , end = - 1 ) : self . fill ( conversions . hsv2rgb ( hsv ) , start , end )
4255	def compress ( self , filename ) : compressed_filename = self . get_compressed_filename ( filename ) if not compressed_filename : return self . do_compress ( filename , compressed_filename )
2341	def forward ( self , x ) : self . noise . normal_ ( ) return self . layers ( th . cat ( [ x , self . noise ] , 1 ) )
5861	def _keys_to_camel_case ( self , obj ) : return dict ( ( to_camel_case ( key ) , value ) for ( key , value ) in obj . items ( ) )
2153	def config_from_environment ( ) : kwargs = { } for k in CONFIG_OPTIONS : env = 'TOWER_' + k . upper ( ) v = os . getenv ( env , None ) if v is not None : kwargs [ k ] = v return kwargs
12985	def getCompressMod ( self ) : if self . compressMode == COMPRESS_MODE_ZLIB : return zlib if self . compressMode == COMPRESS_MODE_BZ2 : return bz2 if self . compressMode == COMPRESS_MODE_LZMA : global _lzmaMod if _lzmaMod is not None : return _lzmaMod try : import lzma _lzmaMod = lzma return _lzmaMod except : try : from backports import lzma _lzmaMod = lzma return _lzmaMod except : pass try : import lzmaffi as lzma _lzmaMod = lzma return _lzmaMod except : pass raise ImportError ( "Requested compress mode is lzma and could not find a module providing lzma support. Tried: 'lzma', 'backports.lzma', 'lzmaffi' and none of these were available. Please install one of these, or to use an unlisted implementation, set IndexedRedis.fields.compressed._lzmaMod to the module (must implement standard python compression interface)" )
10864	def _update_type ( self , params ) : dozscale = False particles = [ ] for p in listify ( params ) : typ , ind = self . _p2i ( p ) particles . append ( ind ) dozscale = dozscale or typ == 'zscale' particles = set ( particles ) return dozscale , particles
4279	def reduce_opacity ( im , opacity ) : assert opacity >= 0 and opacity <= 1 if im . mode != 'RGBA' : im = im . convert ( 'RGBA' ) else : im = im . copy ( ) alpha = im . split ( ) [ 3 ] alpha = ImageEnhance . Brightness ( alpha ) . enhance ( opacity ) im . putalpha ( alpha ) return im
10103	def send ( self , email_id , recipient , email_data = None , sender = None , cc = None , bcc = None , tags = [ ] , headers = { } , esp_account = None , locale = None , email_version_name = None , inline = None , files = [ ] , timeout = None ) : if not email_data : email_data = { } if isinstance ( recipient , string_types ) : warnings . warn ( "Passing email directly for recipient is deprecated" , DeprecationWarning ) recipient = { 'address' : recipient } payload = { 'email_id' : email_id , 'recipient' : recipient , 'email_data' : email_data } if sender : payload [ 'sender' ] = sender if cc : if not type ( cc ) == list : logger . error ( 'kwarg cc must be type(list), got %s' % type ( cc ) ) payload [ 'cc' ] = cc if bcc : if not type ( bcc ) == list : logger . error ( 'kwarg bcc must be type(list), got %s' % type ( bcc ) ) payload [ 'bcc' ] = bcc if tags : if not type ( tags ) == list : logger . error ( 'kwarg tags must be type(list), got %s' % ( type ( tags ) ) ) payload [ 'tags' ] = tags if headers : if not type ( headers ) == dict : logger . error ( 'kwarg headers must be type(dict), got %s' % ( type ( headers ) ) ) payload [ 'headers' ] = headers if esp_account : if not isinstance ( esp_account , string_types ) : logger . error ( 'kwarg esp_account must be a string, got %s' % ( type ( esp_account ) ) ) payload [ 'esp_account' ] = esp_account if locale : if not isinstance ( locale , string_types ) : logger . error ( 'kwarg locale must be a string, got %s' % ( type ( locale ) ) ) payload [ 'locale' ] = locale if email_version_name : if not isinstance ( email_version_name , string_types ) : logger . error ( 'kwarg email_version_name must be a string, got %s' % ( type ( email_version_name ) ) ) payload [ 'version_name' ] = email_version_name if inline : payload [ 'inline' ] = self . _make_file_dict ( inline ) if files : payload [ 'files' ] = [ self . _make_file_dict ( f ) for f in files ] return self . _api_request ( self . SEND_ENDPOINT , self . HTTP_POST , payload = payload , timeout = timeout )
5103	def draw_graph ( self , line_kwargs = None , scatter_kwargs = None , ** kwargs ) : if not HAS_MATPLOTLIB : raise ImportError ( "Matplotlib is required to draw the graph." ) fig = plt . figure ( figsize = kwargs . get ( 'figsize' , ( 7 , 7 ) ) ) ax = fig . gca ( ) mpl_kwargs = { 'line_kwargs' : line_kwargs , 'scatter_kwargs' : scatter_kwargs , 'pos' : kwargs . get ( 'pos' ) } line_kwargs , scatter_kwargs = self . lines_scatter_args ( ** mpl_kwargs ) edge_collection = LineCollection ( ** line_kwargs ) ax . add_collection ( edge_collection ) ax . scatter ( ** scatter_kwargs ) if hasattr ( ax , 'set_facecolor' ) : ax . set_facecolor ( kwargs . get ( 'bgcolor' , [ 1 , 1 , 1 , 1 ] ) ) else : ax . set_axis_bgcolor ( kwargs . get ( 'bgcolor' , [ 1 , 1 , 1 , 1 ] ) ) ax . get_xaxis ( ) . set_visible ( False ) ax . get_yaxis ( ) . set_visible ( False ) if 'fname' in kwargs : new_kwargs = { k : v for k , v in kwargs . items ( ) if k in SAVEFIG_KWARGS } fig . savefig ( kwargs [ 'fname' ] , ** new_kwargs ) else : plt . ion ( ) plt . show ( )
13267	def gml_to_geojson ( el ) : if el . get ( 'srsName' ) not in ( 'urn:ogc:def:crs:EPSG::4326' , None ) : if el . get ( 'srsName' ) == 'EPSG:4326' : return _gmlv2_to_geojson ( el ) else : raise NotImplementedError ( "Unrecognized srsName %s" % el . get ( 'srsName' ) ) tag = el . tag . replace ( '{%s}' % NS_GML , '' ) if tag == 'Point' : coordinates = _reverse_gml_coords ( el . findtext ( '{%s}pos' % NS_GML ) ) [ 0 ] elif tag == 'LineString' : coordinates = _reverse_gml_coords ( el . findtext ( '{%s}posList' % NS_GML ) ) elif tag == 'Polygon' : coordinates = [ ] for ring in el . xpath ( 'gml:exterior/gml:LinearRing/gml:posList' , namespaces = NSMAP ) + el . xpath ( 'gml:interior/gml:LinearRing/gml:posList' , namespaces = NSMAP ) : coordinates . append ( _reverse_gml_coords ( ring . text ) ) elif tag in ( 'MultiPoint' , 'MultiLineString' , 'MultiPolygon' ) : single_type = tag [ 5 : ] member_tag = single_type [ 0 ] . lower ( ) + single_type [ 1 : ] + 'Member' coordinates = [ gml_to_geojson ( member ) [ 'coordinates' ] for member in el . xpath ( 'gml:%s/gml:%s' % ( member_tag , single_type ) , namespaces = NSMAP ) ] else : raise NotImplementedError return { 'type' : tag , 'coordinates' : coordinates }
4838	def get_course_and_course_run ( self , course_run_id ) : course_id = parse_course_key ( course_run_id ) course = self . get_course_details ( course_id ) course_run = None if course : course_run = None course_runs = [ course_run for course_run in course [ 'course_runs' ] if course_run [ 'key' ] == course_run_id ] if course_runs : course_run = course_runs [ 0 ] return course , course_run
2747	def get_all_regions ( self ) : data = self . get_data ( "regions/" ) regions = list ( ) for jsoned in data [ 'regions' ] : region = Region ( ** jsoned ) region . token = self . token regions . append ( region ) return regions
2539	def set_pkg_excl_file ( self , doc , filename ) : self . assert_package_exists ( ) doc . package . add_exc_file ( filename )
9887	def _read_all_attribute_info ( self ) : num = copy . deepcopy ( self . _num_attrs ) fname = copy . deepcopy ( self . fname ) out = fortran_cdf . inquire_all_attr ( fname , num , len ( fname ) ) status = out [ 0 ] names = out [ 1 ] . astype ( 'U' ) scopes = out [ 2 ] max_gentries = out [ 3 ] max_rentries = out [ 4 ] max_zentries = out [ 5 ] attr_nums = out [ 6 ] global_attrs_info = { } var_attrs_info = { } if status == 0 : for name , scope , gentry , rentry , zentry , num in zip ( names , scopes , max_gentries , max_rentries , max_zentries , attr_nums ) : name = '' . join ( name ) name = name . rstrip ( ) nug = { } nug [ 'scope' ] = scope nug [ 'max_gentry' ] = gentry nug [ 'max_rentry' ] = rentry nug [ 'max_zentry' ] = zentry nug [ 'attr_num' ] = num flag = ( gentry == 0 ) & ( rentry == 0 ) & ( zentry == 0 ) if not flag : if scope == 1 : global_attrs_info [ name ] = nug elif scope == 2 : var_attrs_info [ name ] = nug self . global_attrs_info = global_attrs_info self . var_attrs_info = var_attrs_info else : raise IOError ( fortran_cdf . statusreporter ( status ) )
10364	def has_degradation_increases_activity ( data : Dict ) -> bool : return part_has_modifier ( data , SUBJECT , DEGRADATION ) and part_has_modifier ( data , OBJECT , ACTIVITY )
9517	def trim_Ns ( self ) : i = 0 while i < len ( self ) and self . seq [ i ] in 'nN' : i += 1 self . seq = self . seq [ i : ] self . qual = self . qual [ i : ] self . seq = self . seq . rstrip ( 'Nn' ) self . qual = self . qual [ : len ( self . seq ) ]
12126	def spec_formatter ( cls , spec ) : " Formats the elements of an argument set appropriately" return type ( spec ) ( ( k , str ( v ) ) for ( k , v ) in spec . items ( ) )
7462	def encode ( self , obj ) : def hint_tuples ( item ) : if isinstance ( item , tuple ) : return { '__tuple__' : True , 'items' : item } if isinstance ( item , list ) : return [ hint_tuples ( e ) for e in item ] if isinstance ( item , dict ) : return { key : hint_tuples ( val ) for key , val in item . iteritems ( ) } else : return item return super ( Encoder , self ) . encode ( hint_tuples ( obj ) )
959	def aggregationDivide ( dividend , divisor ) : dividendMonthSec = aggregationToMonthsSeconds ( dividend ) divisorMonthSec = aggregationToMonthsSeconds ( divisor ) if ( dividendMonthSec [ 'months' ] != 0 and divisorMonthSec [ 'seconds' ] != 0 ) or ( dividendMonthSec [ 'seconds' ] != 0 and divisorMonthSec [ 'months' ] != 0 ) : raise RuntimeError ( "Aggregation dicts with months/years can only be " "inter-operated with other aggregation dicts that contain " "months/years" ) if dividendMonthSec [ 'months' ] > 0 : return float ( dividendMonthSec [ 'months' ] ) / divisor [ 'months' ] else : return float ( dividendMonthSec [ 'seconds' ] ) / divisorMonthSec [ 'seconds' ]
4282	def video_size ( source , converter = 'ffmpeg' ) : res = subprocess . run ( [ converter , '-i' , source ] , stderr = subprocess . PIPE ) stderr = res . stderr . decode ( 'utf8' ) pattern = re . compile ( r'Stream.*Video.* ([0-9]+)x([0-9]+)' ) match = pattern . search ( stderr ) rot_pattern = re . compile ( r'rotate\s*:\s*-?(90|270)' ) rot_match = rot_pattern . search ( stderr ) if match : x , y = int ( match . groups ( ) [ 0 ] ) , int ( match . groups ( ) [ 1 ] ) else : x = y = 0 if rot_match : x , y = y , x return x , y
6873	def given_lc_get_out_of_transit_points ( time , flux , err_flux , blsfit_savpath = None , trapfit_savpath = None , in_out_transit_savpath = None , sigclip = None , magsarefluxes = True , nworkers = 1 , extra_maskfrac = 0.03 ) : tmids_obsd , t_starts , t_ends = ( given_lc_get_transit_tmids_tstarts_tends ( time , flux , err_flux , blsfit_savpath = blsfit_savpath , trapfit_savpath = trapfit_savpath , magsarefluxes = magsarefluxes , nworkers = nworkers , sigclip = sigclip , extra_maskfrac = extra_maskfrac ) ) in_transit = np . zeros_like ( time ) . astype ( bool ) for t_start , t_end in zip ( t_starts , t_ends ) : this_transit = ( ( time > t_start ) & ( time < t_end ) ) in_transit |= this_transit out_of_transit = ~ in_transit if in_out_transit_savpath : _in_out_transit_plot ( time , flux , in_transit , out_of_transit , in_out_transit_savpath ) return time [ out_of_transit ] , flux [ out_of_transit ] , err_flux [ out_of_transit ]
6040	def regular_data_1d_from_sub_data_1d ( self , sub_array_1d ) : return np . multiply ( self . sub_grid_fraction , sub_array_1d . reshape ( - 1 , self . sub_grid_length ) . sum ( axis = 1 ) )
815	def Indicator ( pos , size , dtype ) : x = numpy . zeros ( size , dtype = dtype ) x [ pos ] = 1 return x
5698	def get_centroid_of_stops ( gtfs ) : stops = gtfs . get_table ( "stops" ) mean_lat = numpy . mean ( stops [ 'lat' ] . values ) mean_lon = numpy . mean ( stops [ 'lon' ] . values ) return mean_lat , mean_lon
10444	def getobjectproperty ( self , window_name , object_name , prop ) : try : obj_info = self . _get_object_map ( window_name , object_name , wait_for_object = False ) except atomac . _a11y . ErrorInvalidUIElement : self . _windows = { } obj_info = self . _get_object_map ( window_name , object_name , wait_for_object = False ) if obj_info and prop != "obj" and prop in obj_info : if prop == "class" : return ldtp_class_type . get ( obj_info [ prop ] , obj_info [ prop ] ) else : return obj_info [ prop ] raise LdtpServerException ( 'Unknown property "%s" in %s' % ( prop , object_name ) )
12789	def get ( self , q = None , page = None ) : etag = generate_etag ( current_ext . content_version . encode ( 'utf8' ) ) self . check_etag ( etag , weak = True ) res = jsonify ( current_ext . styles ) res . set_etag ( etag ) return res
13761	def _handle_response ( self , response ) : if not str ( response . status_code ) . startswith ( '2' ) : raise get_api_error ( response ) return response
13407	def sendToLogbook ( self , fileName , logType , location = None ) : import subprocess success = True if logType == "MCC" : fileString = "" if not self . imagePixmap . isNull ( ) : fileString = fileName + "." + self . imageType logcmd = "xml2elog " + fileName + ".xml " + fileString process = subprocess . Popen ( logcmd , shell = True ) process . wait ( ) if process . returncode != 0 : success = False else : from shutil import copy path = "/u1/" + location . lower ( ) + "/physics/logbook/data/" try : if not self . imagePixmap . isNull ( ) : copy ( fileName + ".png" , path ) if self . imageType == "png" : copy ( fileName + ".ps" , path ) else : copy ( fileName + "." + self . imageType , path ) copy ( fileName + ".xml" , path ) except IOError as error : print ( error ) success = False return success
8304	def eof ( self ) : return ( not self . is_alive ( ) ) and self . _queue . empty ( ) or self . _fd . closed
613	def _generateExtraMetricSpecs ( options ) : _metricSpecSchema = { 'properties' : { } } results = [ ] for metric in options [ 'metrics' ] : for propertyName in _metricSpecSchema [ 'properties' ] . keys ( ) : _getPropertyValue ( _metricSpecSchema , propertyName , metric ) specString , label = _generateMetricSpecString ( field = metric [ 'field' ] , metric = metric [ 'metric' ] , params = metric [ 'params' ] , inferenceElement = metric [ 'inferenceElement' ] , returnLabel = True ) if metric [ 'logged' ] : options [ 'loggedMetrics' ] . append ( label ) results . append ( specString ) return results
3232	def get_user_agent_default ( pkg_name = 'cloudaux' ) : version = '0.0.1' try : import pkg_resources version = pkg_resources . get_distribution ( pkg_name ) . version except pkg_resources . DistributionNotFound : pass except ImportError : pass return 'cloudaux/%s' % ( version )
10057	def delete ( self , pid , record , key ) : try : del record . files [ str ( key ) ] record . commit ( ) db . session . commit ( ) return make_response ( '' , 204 ) except KeyError : abort ( 404 , 'The specified object does not exist or has already ' 'been deleted.' )
5022	def on_init ( app ) : docs_path = os . path . abspath ( os . path . dirname ( __file__ ) ) root_path = os . path . abspath ( os . path . join ( docs_path , '..' ) ) apidoc_path = 'sphinx-apidoc' if hasattr ( sys , 'real_prefix' ) : bin_path = os . path . abspath ( os . path . join ( sys . prefix , 'bin' ) ) apidoc_path = os . path . join ( bin_path , apidoc_path ) check_call ( [ apidoc_path , '-o' , docs_path , os . path . join ( root_path , 'enterprise' ) , os . path . join ( root_path , 'enterprise/migrations' ) ] )
3104	def oauth_enabled ( decorated_function = None , scopes = None , ** decorator_kwargs ) : def curry_wrapper ( wrapped_function ) : @ wraps ( wrapped_function ) def enabled_wrapper ( request , * args , ** kwargs ) : return_url = decorator_kwargs . pop ( 'return_url' , request . get_full_path ( ) ) user_oauth = django_util . UserOAuth2 ( request , scopes , return_url ) setattr ( request , django_util . oauth2_settings . request_prefix , user_oauth ) return wrapped_function ( request , * args , ** kwargs ) return enabled_wrapper if decorated_function : return curry_wrapper ( decorated_function ) else : return curry_wrapper
13799	def log ( self , string ) : self . wfile . write ( json . dumps ( { 'log' : string } ) + NEWLINE )
11108	def walk_directories_relative_path ( self , relativePath = "" ) : def walk_directories ( directory , relativePath ) : directories = dict . __getitem__ ( directory , 'directories' ) dirNames = dict . keys ( directories ) for d in sorted ( dirNames ) : yield os . path . join ( relativePath , d ) for k in sorted ( dict . keys ( directories ) ) : path = os . path . join ( relativePath , k ) dir = dict . __getitem__ ( directories , k ) for e in walk_directories ( dir , path ) : yield e dir , errorMessage = self . get_directory_info ( relativePath ) assert dir is not None , errorMessage return walk_directories ( dir , relativePath = '' )
6354	def _apply_rule_if_compat ( self , phonetic , target , language_arg ) : candidate = phonetic + target if '[' not in candidate : return candidate candidate = self . _expand_alternates ( candidate ) candidate_array = candidate . split ( '|' ) candidate = '' found = False for i in range ( len ( candidate_array ) ) : this_candidate = candidate_array [ i ] if language_arg != 1 : this_candidate = self . _normalize_lang_attrs ( this_candidate + '[' + str ( language_arg ) + ']' , False ) if this_candidate != '[0]' : found = True if candidate : candidate += '|' candidate += this_candidate if not found : return None if '|' in candidate : candidate = '(' + candidate + ')' return candidate
10685	def G_mag ( self , T ) : tau = T / self . Tc_mag if tau <= 1.0 : g = 1 - ( self . _A_mag / tau + self . _B_mag * ( tau ** 3 / 6 + tau ** 9 / 135 + tau ** 15 / 600 ) ) / self . _D_mag else : g = - ( tau ** - 5 / 10 + tau ** - 15 / 315 + tau ** - 25 / 1500 ) / self . _D_mag return R * T * math . log ( self . beta0_mag + 1 ) * g
11081	def webhook ( * args , ** kwargs ) : def wrapper ( func ) : func . is_webhook = True func . route = args [ 0 ] func . form_params = kwargs . get ( 'form_params' , [ ] ) func . method = kwargs . get ( 'method' , 'POST' ) return func return wrapper
11125	def rename_file ( self , relativePath , name , newName , replace = False , verbose = True ) : relativePath = os . path . normpath ( relativePath ) if relativePath == '.' : relativePath = '' dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) assert dirInfoDict is not None , errorMessage assert name in dict . __getitem__ ( dirInfoDict , "files" ) , "file '%s' is not found in repository relative path '%s'" % ( name , relativePath ) realPath = os . path . join ( self . __path , relativePath , name ) assert os . path . isfile ( realPath ) , "file '%s' is not found in system" % realPath assert newName not in dict . __getitem__ ( dirInfoDict , "files" ) , "file '%s' already exists in repository relative path '%s'" % ( newName , relativePath ) newRealPath = os . path . join ( self . __path , relativePath , newName ) if os . path . isfile ( newRealPath ) : if replace : os . remove ( newRealPath ) if verbose : warnings . warn ( "file '%s' already exists found in system, it is now replaced by '%s' because 'replace' flag is True." % ( newRealPath , realPath ) ) else : raise Exception ( "file '%s' already exists in system but not registered in repository." % newRealPath ) os . rename ( realPath , newRealPath ) dict . __setitem__ ( dict . __getitem__ ( dirInfoDict , "files" ) , newName , dict . __getitem__ ( dirInfoDict , "files" ) . pop ( name ) ) self . save ( )
4624	def _get_encrypted_masterpassword ( self ) : if not self . unlocked ( ) : raise WalletLocked aes = AESCipher ( self . password ) return "{}${}" . format ( self . _derive_checksum ( self . masterkey ) , aes . encrypt ( self . masterkey ) )
5181	def _url ( self , endpoint , path = None ) : log . debug ( '_url called with endpoint: {0} and path: {1}' . format ( endpoint , path ) ) try : endpoint = ENDPOINTS [ endpoint ] except KeyError : raise APIError url = '{base_url}/{endpoint}' . format ( base_url = self . base_url , endpoint = endpoint , ) if path is not None : url = '{0}/{1}' . format ( url , quote ( path ) ) return url
9192	def _get_file_sha1 ( file ) : bits = file . read ( ) file . seek ( 0 ) h = hashlib . new ( 'sha1' , bits ) . hexdigest ( ) return h
6298	def draw ( self , mesh , projection_matrix = None , view_matrix = None , camera_matrix = None , time = 0 ) : self . program [ "m_proj" ] . write ( projection_matrix ) self . program [ "m_mv" ] . write ( view_matrix ) mesh . vao . render ( self . program )
6810	def fix_lsmod_for_pi3 ( self ) : r = self . local_renderer r . env . rpi2_conf = '/etc/modules-load.d/rpi2.conf' r . sudo ( "sed '/bcm2808_rng/d' {rpi2_conf}" ) r . sudo ( "echo bcm2835_rng >> {rpi2_conf}" )
4469	def _pprint ( params , offset = 0 , printer = repr ) : options = np . get_printoptions ( ) np . set_printoptions ( precision = 5 , threshold = 64 , edgeitems = 2 ) params_list = list ( ) this_line_length = offset line_sep = ',\n' + ( 1 + offset // 2 ) * ' ' for i , ( k , v ) in enumerate ( sorted ( six . iteritems ( params ) ) ) : if type ( v ) is float : this_repr = '%s=%s' % ( k , str ( v ) ) else : this_repr = '%s=%s' % ( k , printer ( v ) ) if len ( this_repr ) > 500 : this_repr = this_repr [ : 300 ] + '...' + this_repr [ - 100 : ] if i > 0 : if ( this_line_length + len ( this_repr ) >= 75 or '\n' in this_repr ) : params_list . append ( line_sep ) this_line_length = len ( line_sep ) else : params_list . append ( ', ' ) this_line_length += 2 params_list . append ( this_repr ) this_line_length += len ( this_repr ) np . set_printoptions ( ** options ) lines = '' . join ( params_list ) lines = '\n' . join ( l . rstrip ( ' ' ) for l in lines . split ( '\n' ) ) return lines
7300	def get_context_data ( self , ** kwargs ) : context = super ( DocumentListView , self ) . get_context_data ( ** kwargs ) context = self . set_permissions_in_context ( context ) if not context [ 'has_view_permission' ] : return HttpResponseForbidden ( "You do not have permissions to view this content." ) context [ 'object_list' ] = self . get_queryset ( ) context [ 'document' ] = self . document context [ 'app_label' ] = self . app_label context [ 'document_name' ] = self . document_name context [ 'request' ] = self . request context [ 'page' ] = self . page context [ 'documents_per_page' ] = self . documents_per_page if self . page > 1 : previous_page_number = self . page - 1 else : previous_page_number = None if self . page < self . total_pages : next_page_number = self . page + 1 else : next_page_number = None context [ 'previous_page_number' ] = previous_page_number context [ 'has_previous_page' ] = previous_page_number is not None context [ 'next_page_number' ] = next_page_number context [ 'has_next_page' ] = next_page_number is not None context [ 'total_pages' ] = self . total_pages if self . queryset . count ( ) : context [ 'keys' ] = [ 'id' , ] for key in [ x for x in self . mongoadmin . list_fields if x != 'id' and x in self . document . _fields . keys ( ) ] : if isinstance ( self . document . _fields [ key ] , EmbeddedDocumentField ) : continue if isinstance ( self . document . _fields [ key ] , ListField ) : continue context [ 'keys' ] . append ( key ) if self . mongoadmin . search_fields : context [ 'search_field' ] = True return context
10641	def Re ( L : float , v : float , nu : float ) -> float : return v * L / nu
8503	def as_call ( self ) : default = self . _default ( ) default = ', ' + default if default else '' return "pyconfig.%s(%r%s)" % ( self . method , self . get_key ( ) , default )
920	def critical ( self , msg , * args , ** kwargs ) : self . _baseLogger . critical ( self , self . getExtendedMsg ( msg ) , * args , ** kwargs )
9891	def _uptime_amiga ( ) : global __boottime try : __boottime = os . stat ( 'RAM:' ) . st_ctime return time . time ( ) - __boottime except ( NameError , OSError ) : return None
10350	def lint_directory ( source , target ) : for path in os . listdir ( source ) : if not path . endswith ( '.bel' ) : continue log . info ( 'linting: %s' , path ) with open ( os . path . join ( source , path ) ) as i , open ( os . path . join ( target , path ) , 'w' ) as o : lint_file ( i , o )
10097	def create_new_version ( self , name , subject , text = '' , template_id = None , html = None , locale = None , timeout = None ) : if ( html ) : payload = { 'name' : name , 'subject' : subject , 'html' : html , 'text' : text } else : payload = { 'name' : name , 'subject' : subject , 'text' : text } if locale : url = self . TEMPLATES_SPECIFIC_LOCALE_VERSIONS_ENDPOINT % ( template_id , locale ) else : url = self . TEMPLATES_NEW_VERSION_ENDPOINT % template_id return self . _api_request ( url , self . HTTP_POST , payload = payload , timeout = timeout )
5480	def cancel ( batch_fn , cancel_fn , ops ) : canceled_ops = [ ] error_messages = [ ] max_batch = 256 total_ops = len ( ops ) for first_op in range ( 0 , total_ops , max_batch ) : batch_canceled , batch_messages = _cancel_batch ( batch_fn , cancel_fn , ops [ first_op : first_op + max_batch ] ) canceled_ops . extend ( batch_canceled ) error_messages . extend ( batch_messages ) return canceled_ops , error_messages
4210	def csvd ( A ) : U , S , V = numpy . linalg . svd ( A ) return U , S , V
3194	def create_or_update ( self , list_id , subscriber_hash , data ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash if 'email_address' not in data : raise KeyError ( 'The list member must have an email_address' ) check_email ( data [ 'email_address' ] ) if 'status_if_new' not in data : raise KeyError ( 'The list member must have a status_if_new' ) if data [ 'status_if_new' ] not in [ 'subscribed' , 'unsubscribed' , 'cleaned' , 'pending' , 'transactional' ] : raise ValueError ( 'The list member status_if_new must be one of "subscribed", "unsubscribed", "cleaned", ' '"pending", or "transactional"' ) return self . _mc_client . _put ( url = self . _build_path ( list_id , 'members' , subscriber_hash ) , data = data )
5166	def __intermediate_dns_servers ( self , uci , address ) : if 'dns' in uci : return uci [ 'dns' ] if address [ 'proto' ] in [ 'dhcp' , 'dhcpv6' , 'none' ] : return None dns = self . netjson . get ( 'dns_servers' , None ) if dns : return ' ' . join ( dns )
1553	def _get_comp_config ( self ) : proto_config = topology_pb2 . Config ( ) key = proto_config . kvs . add ( ) key . key = TOPOLOGY_COMPONENT_PARALLELISM key . value = str ( self . parallelism ) key . type = topology_pb2 . ConfigValueType . Value ( "STRING_VALUE" ) if self . custom_config is not None : sanitized = self . _sanitize_config ( self . custom_config ) for key , value in sanitized . items ( ) : if isinstance ( value , str ) : kvs = proto_config . kvs . add ( ) kvs . key = key kvs . value = value kvs . type = topology_pb2 . ConfigValueType . Value ( "STRING_VALUE" ) else : kvs = proto_config . kvs . add ( ) kvs . key = key kvs . serialized_value = default_serializer . serialize ( value ) kvs . type = topology_pb2 . ConfigValueType . Value ( "PYTHON_SERIALIZED_VALUE" ) return proto_config
6602	def collect_result ( self , package_index ) : result_fullpath = self . result_fullpath ( package_index ) try : with gzip . open ( result_fullpath , 'rb' ) as f : result = pickle . load ( f ) except Exception as e : logger = logging . getLogger ( __name__ ) logger . warning ( e ) return None return result
10200	def run ( self ) : return elasticsearch . helpers . bulk ( self . client , self . actionsiter ( ) , stats_only = True , chunk_size = 50 )
5981	def setup_figure ( figsize , as_subplot ) : if not as_subplot : fig = plt . figure ( figsize = figsize ) return fig
5973	def generate_submit_array ( templates , directories , ** kwargs ) : dirname = kwargs . setdefault ( 'dirname' , os . path . curdir ) reldirs = [ relpath ( p , start = dirname ) for p in asiterable ( directories ) ] missing = [ p for p in ( os . path . join ( dirname , subdir ) for subdir in reldirs ) if not os . path . exists ( p ) ] if len ( missing ) > 0 : logger . debug ( "template=%(template)r: dirname=%(dirname)r reldirs=%(reldirs)r" , vars ( ) ) logger . error ( "Some directories are not accessible from the array script: " "%(missing)r" , vars ( ) ) def write_script ( template ) : qsystem = detect_queuing_system ( template ) if qsystem is None or not qsystem . has_arrays ( ) : logger . warning ( "Not known how to make a job array for %(template)r; skipping..." , vars ( ) ) return None kwargs [ 'jobarray_string' ] = qsystem . array ( reldirs ) return generate_submit_scripts ( template , ** kwargs ) [ 0 ] return [ write_script ( template ) for template in config . get_templates ( templates ) ]
6997	def runcp_worker ( task ) : pfpickle , outdir , lcbasedir , kwargs = task try : return runcp ( pfpickle , outdir , lcbasedir , ** kwargs ) except Exception as e : LOGEXCEPTION ( ' could not make checkplots for %s: %s' % ( pfpickle , e ) ) return None
4857	def deprecated ( extra ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : message = 'You called the deprecated function `{function}`. {extra}' . format ( function = func . __name__ , extra = extra ) frame = inspect . currentframe ( ) . f_back warnings . warn_explicit ( message , category = DeprecationWarning , filename = inspect . getfile ( frame . f_code ) , lineno = frame . f_lineno ) return func ( * args , ** kwargs ) return wrapper return decorator
6370	def specificity ( self ) : r if self . _tn + self . _fp == 0 : return float ( 'NaN' ) return self . _tn / ( self . _tn + self . _fp )
13379	def _join_seq ( d , k , v ) : if k not in d : d [ k ] = list ( v ) elif isinstance ( d [ k ] , list ) : for item in v : if item not in d [ k ] : d [ k ] . insert ( 0 , item ) elif isinstance ( d [ k ] , string_types ) : v . append ( d [ k ] ) d [ k ] = v
13076	def view_maker ( self , name , instance = None ) : if instance is None : instance = self sig = "lang" in [ parameter . name for parameter in inspect . signature ( getattr ( instance , name ) ) . parameters . values ( ) ] def route ( ** kwargs ) : if sig and "lang" not in kwargs : kwargs [ "lang" ] = self . get_locale ( ) if "semantic" in kwargs : del kwargs [ "semantic" ] return self . route ( getattr ( instance , name ) , ** kwargs ) return route
4617	def formatTime ( t ) : if isinstance ( t , float ) : return datetime . utcfromtimestamp ( t ) . strftime ( timeFormat ) if isinstance ( t , datetime ) : return t . strftime ( timeFormat )
4209	def pascal ( n ) : errors . is_positive_integer ( n ) result = numpy . zeros ( ( n , n ) ) for i in range ( 0 , n ) : result [ i , 0 ] = 1 result [ 0 , i ] = 1 if n > 1 : for i in range ( 1 , n ) : for j in range ( 1 , n ) : result [ i , j ] = result [ i - 1 , j ] + result [ i , j - 1 ] return result
11680	def disconnect ( self ) : logger . info ( u'Disconnecting' ) self . sock . shutdown ( socket . SHUT_RDWR ) self . sock . close ( ) self . state = DISCONNECTED
9954	def custom_showwarning ( message , category , filename = "" , lineno = - 1 , file = None , line = None ) : if file is None : file = sys . stderr if file is None : return text = "%s: %s\n" % ( category . __name__ , message ) try : file . write ( text ) except OSError : pass
2777	def add_forwarding_rules ( self , forwarding_rules ) : rules_dict = [ rule . __dict__ for rule in forwarding_rules ] return self . get_data ( "load_balancers/%s/forwarding_rules/" % self . id , type = POST , params = { "forwarding_rules" : rules_dict } )
7443	def _step1func ( self , force , ipyclient ) : sfiles = self . paramsdict [ "sorted_fastq_path" ] rfiles = self . paramsdict [ "raw_fastq_path" ] if sfiles and rfiles : raise IPyradWarningExit ( NOT_TWO_PATHS ) if not ( sfiles or rfiles ) : raise IPyradWarningExit ( NO_SEQ_PATH_FOUND ) if self . _headers : if sfiles : print ( "\n{}Step 1: Loading sorted fastq data to Samples" . format ( self . _spacer ) ) else : print ( "\n{}Step 1: Demultiplexing fastq data to Samples" . format ( self . _spacer ) ) if self . samples : if not force : print ( SAMPLES_EXIST . format ( len ( self . samples ) , self . name ) ) else : if glob . glob ( sfiles ) : self . _link_fastqs ( ipyclient = ipyclient , force = force ) else : assemble . demultiplex . run2 ( self , ipyclient , force ) else : if glob . glob ( sfiles ) : self . _link_fastqs ( ipyclient = ipyclient ) else : assemble . demultiplex . run2 ( self , ipyclient , force )
9311	def amz_cano_querystring ( qs ) : safe_qs_amz_chars = '&=+' safe_qs_unresvd = '-_.~' if PY2 : qs = qs . encode ( 'utf-8' ) safe_qs_amz_chars = safe_qs_amz_chars . encode ( ) safe_qs_unresvd = safe_qs_unresvd . encode ( ) qs = unquote ( qs ) space = b' ' if PY2 else ' ' qs = qs . split ( space ) [ 0 ] qs = quote ( qs , safe = safe_qs_amz_chars ) qs_items = { } for name , vals in parse_qs ( qs , keep_blank_values = True ) . items ( ) : name = quote ( name , safe = safe_qs_unresvd ) vals = [ quote ( val , safe = safe_qs_unresvd ) for val in vals ] qs_items [ name ] = vals qs_strings = [ ] for name , vals in qs_items . items ( ) : for val in vals : qs_strings . append ( '=' . join ( [ name , val ] ) ) qs = '&' . join ( sorted ( qs_strings ) ) if PY2 : qs = unicode ( qs ) return qs
5881	def is_highlink_density ( self , element ) : links = self . parser . getElementsByTag ( element , tag = 'a' ) if not links : return False text = self . parser . getText ( element ) words = text . split ( ' ' ) words_number = float ( len ( words ) ) link_text_parts = [ ] for link in links : link_text_parts . append ( self . parser . getText ( link ) ) link_text = '' . join ( link_text_parts ) link_words = link_text . split ( ' ' ) number_of_link_words = float ( len ( link_words ) ) number_of_links = float ( len ( links ) ) link_divisor = float ( number_of_link_words / words_number ) score = float ( link_divisor * number_of_links ) if score >= 1.0 : return True return False
3291	def get_href ( self ) : safe = "/" + "!*'()," + "$-_|." return compat . quote ( self . provider . mount_path + self . provider . share_path + self . get_preferred_path ( ) , safe = safe , )
8330	def findAllNext ( self , name = None , attrs = { } , text = None , limit = None , ** kwargs ) : return self . _findAll ( name , attrs , text , limit , self . nextGenerator , ** kwargs )
11271	def to_str ( prev , encoding = None ) : first = next ( prev ) if isinstance ( first , str ) : if encoding is None : yield first for s in prev : yield s else : yield first . encode ( encoding ) for s in prev : yield s . encode ( encoding ) else : if encoding is None : encoding = sys . stdout . encoding or 'utf-8' yield first . decode ( encoding ) for s in prev : yield s . decode ( encoding )
13886	def ReplaceInFile ( filename , old , new , encoding = None ) : contents = GetFileContents ( filename , encoding = encoding ) contents = contents . replace ( old , new ) CreateFile ( filename , contents , encoding = encoding ) return contents
1019	def getSimplePatterns ( numOnes , numPatterns , patternOverlap = 0 ) : assert ( patternOverlap < numOnes ) numNewBitsInEachPattern = numOnes - patternOverlap numCols = numNewBitsInEachPattern * numPatterns + patternOverlap p = [ ] for i in xrange ( numPatterns ) : x = numpy . zeros ( numCols , dtype = 'float32' ) startBit = i * numNewBitsInEachPattern nextStartBit = startBit + numOnes x [ startBit : nextStartBit ] = 1 p . append ( x ) return p
6177	def reduce_chunk ( func , array ) : res = [ ] for slice in iter_chunk_slice ( array . shape [ - 1 ] , array . chunkshape [ - 1 ] ) : res . append ( func ( array [ ... , slice ] ) ) return func ( res )
9305	def handle_date_mismatch ( self , req ) : req_datetime = self . get_request_date ( req ) new_key_date = req_datetime . strftime ( '%Y%m%d' ) self . regenerate_signing_key ( date = new_key_date )
7442	def branch ( self , newname , subsamples = None , infile = None ) : remove = 0 if ( newname == self . name or os . path . exists ( os . path . join ( self . paramsdict [ "project_dir" ] , newname + ".assembly" ) ) ) : print ( "{}Assembly object named {} already exists" . format ( self . _spacer , newname ) ) else : self . _check_name ( newname ) if newname . startswith ( "params-" ) : newname = newname . split ( "params-" ) [ 1 ] newobj = copy . deepcopy ( self ) newobj . name = newname newobj . paramsdict [ "assembly_name" ] = newname if subsamples and infile : print ( BRANCH_NAMES_AND_INPUT ) if infile : if infile [ 0 ] == "-" : remove = 1 infile = infile [ 1 : ] if os . path . exists ( infile ) : subsamples = _read_sample_names ( infile ) if remove : subsamples = list ( set ( self . samples . keys ( ) ) - set ( subsamples ) ) if subsamples : for sname in subsamples : if sname in self . samples : newobj . samples [ sname ] = copy . deepcopy ( self . samples [ sname ] ) else : print ( "Sample name not found: {}" . format ( sname ) ) newobj . samples = { name : sample for name , sample in newobj . samples . items ( ) if name in subsamples } else : for sample in self . samples : newobj . samples [ sample ] = copy . deepcopy ( self . samples [ sample ] ) newobj . save ( ) return newobj
9598	def element ( self , using , value ) : return self . _execute ( Command . FIND_ELEMENT , { 'using' : using , 'value' : value } )
10360	def is_edge_consistent ( graph , u , v ) : if not graph . has_edge ( u , v ) : raise ValueError ( '{} does not contain an edge ({}, {})' . format ( graph , u , v ) ) return 0 == len ( set ( d [ RELATION ] for d in graph . edge [ u ] [ v ] . values ( ) ) )
2172	def new_state ( self ) : try : self . _state = self . state ( ) log . debug ( "Generated new state %s." , self . _state ) except TypeError : self . _state = self . state log . debug ( "Re-using previously supplied state %s." , self . _state ) return self . _state
3883	def from_entity ( entity , self_user_id ) : user_id = UserID ( chat_id = entity . id . chat_id , gaia_id = entity . id . gaia_id ) return User ( user_id , entity . properties . display_name , entity . properties . first_name , entity . properties . photo_url , entity . properties . email , ( self_user_id == user_id ) or ( self_user_id is None ) )
6153	def fir_remez_bsf ( f_pass1 , f_stop1 , f_stop2 , f_pass2 , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : n , ff , aa , wts = bandstop_order ( f_pass1 , f_stop1 , f_stop2 , f_pass2 , d_pass , d_stop , fsamp = fs ) if np . mod ( n , 2 ) != 0 : n += 1 N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 , maxiter = 25 , grid_density = 16 ) print ( 'N_bump must be odd to maintain odd filter length' ) print ( 'Remez filter taps = %d.' % N_taps ) return b
4988	def eligible_for_direct_audit_enrollment ( self , request , enterprise_customer , resource_id , course_key = None ) : course_identifier = course_key if course_key else resource_id return request . GET . get ( 'audit' ) and request . path == self . COURSE_ENROLLMENT_VIEW_URL . format ( enterprise_customer . uuid , course_identifier ) and enterprise_customer . catalog_contains_course ( resource_id ) and EnrollmentApiClient ( ) . has_course_mode ( resource_id , 'audit' )
6095	def voronoi_regular_to_pix_from_grids_and_geometry ( regular_grid , regular_to_nearest_pix , pixel_centres , pixel_neighbors , pixel_neighbors_size ) : regular_to_pix = np . zeros ( ( regular_grid . shape [ 0 ] ) ) for regular_index in range ( regular_grid . shape [ 0 ] ) : nearest_pix_pixel_index = regular_to_nearest_pix [ regular_index ] while True : nearest_pix_pixel_center = pixel_centres [ nearest_pix_pixel_index ] sub_to_nearest_pix_distance = ( regular_grid [ regular_index , 0 ] - nearest_pix_pixel_center [ 0 ] ) ** 2 + ( regular_grid [ regular_index , 1 ] - nearest_pix_pixel_center [ 1 ] ) ** 2 closest_separation_from_pix_neighbor = 1.0e8 for neighbor_index in range ( pixel_neighbors_size [ nearest_pix_pixel_index ] ) : neighbor = pixel_neighbors [ nearest_pix_pixel_index , neighbor_index ] separation_from_neighbor = ( regular_grid [ regular_index , 0 ] - pixel_centres [ neighbor , 0 ] ) ** 2 + ( regular_grid [ regular_index , 1 ] - pixel_centres [ neighbor , 1 ] ) ** 2 if separation_from_neighbor < closest_separation_from_pix_neighbor : closest_separation_from_pix_neighbor = separation_from_neighbor closest_neighbor_index = neighbor_index neighboring_pix_pixel_index = pixel_neighbors [ nearest_pix_pixel_index , closest_neighbor_index ] sub_to_neighboring_pix_distance = closest_separation_from_pix_neighbor if sub_to_nearest_pix_distance <= sub_to_neighboring_pix_distance : regular_to_pix [ regular_index ] = nearest_pix_pixel_index break else : nearest_pix_pixel_index = neighboring_pix_pixel_index return regular_to_pix
6734	def get_hosts_retriever ( s = None ) : s = s or env . hosts_retriever if not s : return env_hosts_retriever return str_to_callable ( s ) or env_hosts_retriever
9979	def extract_params ( source ) : funcdef = find_funcdef ( source ) params = [ ] for node in ast . walk ( funcdef . args ) : if isinstance ( node , ast . arg ) : if node . arg not in params : params . append ( node . arg ) return params
4162	def _parse_dict_recursive ( dict_str ) : dict_out = dict ( ) pos_last = 0 pos = dict_str . find ( ':' ) while pos >= 0 : key = dict_str [ pos_last : pos ] if dict_str [ pos + 1 ] == '[' : pos_tmp = dict_str . find ( ']' , pos + 1 ) if pos_tmp < 0 : raise RuntimeError ( 'error when parsing dict' ) value = dict_str [ pos + 2 : pos_tmp ] . split ( ',' ) for i in range ( len ( value ) ) : try : value [ i ] = int ( value [ i ] ) except ValueError : pass elif dict_str [ pos + 1 ] == '{' : subdict_str = _select_block ( dict_str [ pos : ] , '{' , '}' ) value = _parse_dict_recursive ( subdict_str ) pos_tmp = pos + len ( subdict_str ) else : raise ValueError ( 'error when parsing dict: unknown elem' ) key = key . strip ( '"' ) if len ( key ) > 0 : dict_out [ key ] = value pos_last = dict_str . find ( ',' , pos_tmp ) if pos_last < 0 : break pos_last += 1 pos = dict_str . find ( ':' , pos_last ) return dict_out
3659	def add_coeffs ( self , Tmin , Tmax , coeffs ) : self . n += 1 if not self . Ts : self . Ts = [ Tmin , Tmax ] self . coeff_sets = [ coeffs ] else : for ind , T in enumerate ( self . Ts ) : if Tmin < T : self . Ts . insert ( ind , Tmin ) self . coeff_sets . insert ( ind , coeffs ) return self . Ts . append ( Tmax ) self . coeff_sets . append ( coeffs )
13493	def read ( args ) : if args . config_file is None or not isfile ( args . config_file ) : return logging . info ( "Reading configure file: %s" % args . config_file ) config = cparser . ConfigParser ( ) config . read ( args . config_file ) if not config . has_section ( 'lrcloud' ) : raise RuntimeError ( "Configure file has no [lrcloud] section!" ) for ( name , value ) in config . items ( 'lrcloud' ) : if value == "True" : value = True elif value == "False" : value = False if getattr ( args , name ) is None : setattr ( args , name , value )
9097	def drop_bel_namespace ( self ) -> Optional [ Namespace ] : namespace = self . _get_default_namespace ( ) if namespace is not None : for entry in tqdm ( namespace . entries , desc = f'deleting entries in {self._get_namespace_name()}' ) : self . session . delete ( entry ) self . session . delete ( namespace ) log . info ( 'committing deletions' ) self . session . commit ( ) return namespace
7241	def aoi ( self , ** kwargs ) : g = self . _parse_geoms ( ** kwargs ) if g is None : return self else : return self [ g ]
5140	def new_noncomment ( self , start_lineno , end_lineno ) : block = NonComment ( start_lineno , end_lineno ) self . blocks . append ( block ) self . current_block = block
10855	def sphere_analytical_gaussian_trim ( dr , a , alpha = 0.2765 , cut = 1.6 ) : m = np . abs ( dr ) <= cut rr = dr [ m ] t = - rr / ( alpha * np . sqrt ( 2 ) ) q = 0.5 * ( 1 + erf ( t ) ) - np . sqrt ( 0.5 / np . pi ) * ( alpha / ( rr + a + 1e-10 ) ) * np . exp ( - t * t ) ans = 0 * dr ans [ m ] = q ans [ dr > cut ] = 0 ans [ dr < - cut ] = 1 return ans
13784	def get_tm_session ( session_factory , transaction_manager ) : dbsession = session_factory ( ) zope . sqlalchemy . register ( dbsession , transaction_manager = transaction_manager ) return dbsession
10246	def count_confidences ( graph : BELGraph ) -> typing . Counter [ str ] : return Counter ( ( 'None' if ANNOTATIONS not in data or 'Confidence' not in data [ ANNOTATIONS ] else list ( data [ ANNOTATIONS ] [ 'Confidence' ] ) [ 0 ] ) for _ , _ , data in graph . edges ( data = True ) if CITATION in data )
12216	def traverse_local_prefs ( stepback = 0 ) : locals_dict = get_frame_locals ( stepback + 1 ) for k in locals_dict : if not k . startswith ( '_' ) and k . upper ( ) == k : yield k , locals_dict
2716	def __extract_resources_from_droplets ( self , data ) : resources = [ ] if not isinstance ( data , list ) : return data for a_droplet in data : res = { } try : if isinstance ( a_droplet , unicode ) : res = { "resource_id" : a_droplet , "resource_type" : "droplet" } except NameError : pass if isinstance ( a_droplet , str ) or isinstance ( a_droplet , int ) : res = { "resource_id" : str ( a_droplet ) , "resource_type" : "droplet" } elif isinstance ( a_droplet , Droplet ) : res = { "resource_id" : str ( a_droplet . id ) , "resource_type" : "droplet" } if len ( res ) > 0 : resources . append ( res ) return resources
11802	def conflicted_vars ( self , current ) : "Return a list of variables in current assignment that are in conflict" return [ var for var in self . vars if self . nconflicts ( var , current [ var ] , current ) > 0 ]
9177	def _dissect_roles ( metadata ) : for role_key in cnxepub . ATTRIBUTED_ROLE_KEYS : for user in metadata . get ( role_key , [ ] ) : if user [ 'type' ] != 'cnx-id' : raise ValueError ( "Archive only accepts Connexions users." ) uid = parse_user_uri ( user [ 'id' ] ) yield uid , role_key raise StopIteration ( )
4110	def ac2poly ( data ) : a , e , _c = LEVINSON ( data ) a = numpy . insert ( a , 0 , 1 ) return a , e
13385	def store_env ( path = None ) : path = path or get_store_env_tmp ( ) env_dict = yaml . safe_dump ( os . environ . data , default_flow_style = False ) with open ( path , 'w' ) as f : f . write ( env_dict ) return path
13230	def get_macros ( tex_source ) : r macros = { } macros . update ( get_def_macros ( tex_source ) ) macros . update ( get_newcommand_macros ( tex_source ) ) return macros
7365	def run_command ( args , ** kwargs ) : assert len ( args ) > 0 start_time = time . time ( ) process = AsyncProcess ( args , ** kwargs ) process . wait ( ) elapsed_time = time . time ( ) - start_time logger . info ( "%s took %0.4f seconds" , args [ 0 ] , elapsed_time )
4266	def set_meta ( target , keys , overwrite = False ) : if not os . path . exists ( target ) : sys . stderr . write ( "The target {} does not exist.\n" . format ( target ) ) sys . exit ( 1 ) if len ( keys ) < 2 or len ( keys ) % 2 > 0 : sys . stderr . write ( "Need an even number of arguments.\n" ) sys . exit ( 1 ) if os . path . isdir ( target ) : descfile = os . path . join ( target , 'index.md' ) else : descfile = os . path . splitext ( target ) [ 0 ] + '.md' if os . path . exists ( descfile ) and not overwrite : sys . stderr . write ( "Description file '{}' already exists. " "Use --overwrite to overwrite it.\n" . format ( descfile ) ) sys . exit ( 2 ) with open ( descfile , "w" ) as fp : for i in range ( len ( keys ) // 2 ) : k , v = keys [ i * 2 : ( i + 1 ) * 2 ] fp . write ( "{}: {}\n" . format ( k . capitalize ( ) , v ) ) print ( "{} metadata key(s) written to {}" . format ( len ( keys ) // 2 , descfile ) )
12445	def options ( self , request , response ) : response [ 'Allowed' ] = ', ' . join ( self . meta . http_allowed_methods ) response . status = http . client . OK
4533	def set_color_list ( self , color_list , offset = 0 ) : if not len ( color_list ) : return color_list = make . colors ( color_list ) size = len ( self . _colors ) - offset if len ( color_list ) > size : color_list = color_list [ : size ] self . _colors [ offset : offset + len ( color_list ) ] = color_list
12969	def get ( self , pk , cascadeFetch = False ) : conn = self . _get_connection ( ) key = self . _get_key_for_id ( pk ) res = conn . hgetall ( key ) if type ( res ) != dict or not len ( res . keys ( ) ) : return None res [ '_id' ] = pk ret = self . _redisResultToObj ( res ) if cascadeFetch is True : self . _doCascadeFetch ( ret ) return ret
1489	def save_file ( self , obj ) : try : import StringIO as pystringIO except ImportError : import io as pystringIO if not hasattr ( obj , 'name' ) or not hasattr ( obj , 'mode' ) : raise pickle . PicklingError ( "Cannot pickle files that do not map to an actual file" ) if obj is sys . stdout : return self . save_reduce ( getattr , ( sys , 'stdout' ) , obj = obj ) if obj is sys . stderr : return self . save_reduce ( getattr , ( sys , 'stderr' ) , obj = obj ) if obj is sys . stdin : raise pickle . PicklingError ( "Cannot pickle standard input" ) if hasattr ( obj , 'isatty' ) and obj . isatty ( ) : raise pickle . PicklingError ( "Cannot pickle files that map to tty objects" ) if 'r' not in obj . mode : raise pickle . PicklingError ( "Cannot pickle files that are not opened for reading" ) name = obj . name try : fsize = os . stat ( name ) . st_size except OSError : raise pickle . PicklingError ( "Cannot pickle file %s as it cannot be stat" % name ) if obj . closed : retval = pystringIO . StringIO ( "" ) retval . close ( ) elif not fsize : retval = pystringIO . StringIO ( "" ) try : tmpfile = file ( name ) tst = tmpfile . read ( 1 ) except IOError : raise pickle . PicklingError ( "Cannot pickle file %s as it cannot be read" % name ) tmpfile . close ( ) if tst != '' : raise pickle . PicklingError ( "Cannot pickle file %s as it does not appear to map to a physical, real file" % name ) else : try : tmpfile = file ( name ) contents = tmpfile . read ( ) tmpfile . close ( ) except IOError : raise pickle . PicklingError ( "Cannot pickle file %s as it cannot be read" % name ) retval = pystringIO . StringIO ( contents ) curloc = obj . tell ( ) retval . seek ( curloc ) retval . name = name self . save ( retval ) self . memoize ( obj )
2688	def update_extend ( dst , src ) : for k , v in src . items ( ) : existing = dst . setdefault ( k , [ ] ) for x in v : if x not in existing : existing . append ( x )
8565	def update_loadbalancer ( self , datacenter_id , loadbalancer_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
12911	def union ( self , other , recursive = True , overwrite = False ) : if not isinstance ( other , composite ) : raise AssertionError ( 'Cannot union composite and {} types' . format ( type ( other ) ) ) if self . meta_type != other . meta_type : return composite ( [ self , other ] ) if self . meta_type == 'list' : keep = [ ] for item in self . _list : keep . append ( item ) for item in other . _list : if item not in self . _list : keep . append ( item ) return composite ( keep ) elif self . meta_type == 'dict' : keep = { } for key in list ( set ( list ( self . _dict . keys ( ) ) + list ( other . _dict . keys ( ) ) ) ) : left = self . _dict . get ( key ) right = other . _dict . get ( key ) if recursive and isinstance ( left , composite ) and isinstance ( right , composite ) : keep [ key ] = left . union ( right , recursive = recursive , overwrite = overwrite ) elif left == right : keep [ key ] = left elif left is None : keep [ key ] = right elif right is None : keep [ key ] = left elif overwrite : keep [ key ] = right else : keep [ key ] = composite ( [ left , right ] ) return composite ( keep ) return
8032	def pruneUI ( dupeList , mainPos = 1 , mainLen = 1 ) : dupeList = sorted ( dupeList ) print for pos , val in enumerate ( dupeList ) : print "%d) %s" % ( pos + 1 , val ) while True : choice = raw_input ( "[%s/%s] Keepers: " % ( mainPos , mainLen ) ) . strip ( ) if not choice : print ( "Please enter a space/comma-separated list of numbers or " "'all'." ) continue elif choice . lower ( ) == 'all' : return [ ] try : out = [ int ( x ) - 1 for x in choice . replace ( ',' , ' ' ) . split ( ) ] return [ val for pos , val in enumerate ( dupeList ) if pos not in out ] except ValueError : print ( "Invalid choice. Please enter a space/comma-separated list" "of numbers or 'all'." )
10900	def check_consistency ( self ) : error = False regex = re . compile ( '([a-zA-Z_][a-zA-Z0-9_]*)' ) if 'full' not in self . modelstr : raise ModelError ( 'Model must contain a `full` key describing ' 'the entire image formation' ) for name , eq in iteritems ( self . modelstr ) : var = regex . findall ( eq ) for v in var : v = re . sub ( r"^d" , '' , v ) if v not in self . varmap : log . error ( "Variable '%s' (eq. '%s': '%s') not found in category map %r" % ( v , name , eq , self . varmap ) ) error = True if error : raise ModelError ( 'Inconsistent varmap and modelstr descriptions' )
11325	def extract_oembeds ( self , text , maxwidth = None , maxheight = None , resource_type = None ) : parser = text_parser ( ) urls = parser . extract_urls ( text ) return self . handle_extracted_urls ( urls , maxwidth , maxheight , resource_type )
783	def jobGetDemand ( self , ) : rows = self . _getMatchingRowsWithRetries ( self . _jobs , dict ( status = self . STATUS_RUNNING ) , [ self . _jobs . pubToDBNameDict [ f ] for f in self . _jobs . jobDemandNamedTuple . _fields ] ) return [ self . _jobs . jobDemandNamedTuple . _make ( r ) for r in rows ]
950	def corruptVector ( v1 , noiseLevel , numActiveCols ) : size = len ( v1 ) v2 = np . zeros ( size , dtype = "uint32" ) bitsToSwap = int ( noiseLevel * numActiveCols ) for i in range ( size ) : v2 [ i ] = v1 [ i ] for _ in range ( bitsToSwap ) : i = random . randrange ( size ) if v2 [ i ] == 1 : v2 [ i ] = 0 else : v2 [ i ] = 1 return v2
10477	def _leftMouseDragged ( self , stopCoord , strCoord , speed ) : appPid = self . _getPid ( ) if strCoord == ( 0 , 0 ) : loc = AppKit . NSEvent . mouseLocation ( ) strCoord = ( loc . x , Quartz . CGDisplayPixelsHigh ( 0 ) - loc . y ) appPid = self . _getPid ( ) pressLeftButton = Quartz . CGEventCreateMouseEvent ( None , Quartz . kCGEventLeftMouseDown , strCoord , Quartz . kCGMouseButtonLeft ) Quartz . CGEventPost ( Quartz . CoreGraphics . kCGHIDEventTap , pressLeftButton ) time . sleep ( 5 ) speed = round ( 1 / float ( speed ) , 2 ) xmoved = stopCoord [ 0 ] - strCoord [ 0 ] ymoved = stopCoord [ 1 ] - strCoord [ 1 ] if ymoved == 0 : raise ValueError ( 'Not support horizontal moving' ) else : k = abs ( ymoved / xmoved ) if xmoved != 0 : for xpos in range ( int ( abs ( xmoved ) ) ) : if xmoved > 0 and ymoved > 0 : currcoord = ( strCoord [ 0 ] + xpos , strCoord [ 1 ] + xpos * k ) elif xmoved > 0 and ymoved < 0 : currcoord = ( strCoord [ 0 ] + xpos , strCoord [ 1 ] - xpos * k ) elif xmoved < 0 and ymoved < 0 : currcoord = ( strCoord [ 0 ] - xpos , strCoord [ 1 ] - xpos * k ) elif xmoved < 0 and ymoved > 0 : currcoord = ( strCoord [ 0 ] - xpos , strCoord [ 1 ] + xpos * k ) dragLeftButton = Quartz . CGEventCreateMouseEvent ( None , Quartz . kCGEventLeftMouseDragged , currcoord , Quartz . kCGMouseButtonLeft ) Quartz . CGEventPost ( Quartz . CoreGraphics . kCGHIDEventTap , dragLeftButton ) time . sleep ( speed ) else : raise ValueError ( 'Not support vertical moving' ) upLeftButton = Quartz . CGEventCreateMouseEvent ( None , Quartz . kCGEventLeftMouseUp , stopCoord , Quartz . kCGMouseButtonLeft ) time . sleep ( 5 ) Quartz . CGEventPost ( Quartz . CoreGraphics . kCGHIDEventTap , upLeftButton )
4931	def transform_courserun_description ( self , content_metadata_item ) : description_with_locales = [ ] content_metadata_language_code = transform_language_code ( content_metadata_item . get ( 'content_language' , '' ) ) for locale in self . enterprise_configuration . get_locales ( default_locale = content_metadata_language_code ) : description_with_locales . append ( { 'locale' : locale , 'value' : ( content_metadata_item [ 'full_description' ] or content_metadata_item [ 'short_description' ] or content_metadata_item [ 'title' ] or '' ) } ) return description_with_locales
12306	def find_executable_files ( ) : files = glob . glob ( "*" ) + glob . glob ( "*/*" ) + glob . glob ( '*/*/*' ) files = filter ( lambda f : os . path . isfile ( f ) , files ) executable = stat . S_IEXEC | stat . S_IXGRP | stat . S_IXOTH final = [ ] for filename in files : if os . path . isfile ( filename ) : st = os . stat ( filename ) mode = st . st_mode if mode & executable : final . append ( filename ) if len ( final ) > 5 : break return final
3471	def add_metabolites ( self , metabolites_to_add , combine = True , reversibly = True ) : old_coefficients = self . metabolites new_metabolites = [ ] _id_to_metabolites = dict ( [ ( x . id , x ) for x in self . _metabolites ] ) for metabolite , coefficient in iteritems ( metabolites_to_add ) : if isinstance ( metabolite , Metabolite ) : if ( ( metabolite . model is not None ) and ( metabolite . model is not self . _model ) ) : metabolite = metabolite . copy ( ) met_id = str ( metabolite ) if met_id in _id_to_metabolites : reaction_metabolite = _id_to_metabolites [ met_id ] if combine : self . _metabolites [ reaction_metabolite ] += coefficient else : self . _metabolites [ reaction_metabolite ] = coefficient else : if self . _model : try : metabolite = self . _model . metabolites . get_by_id ( met_id ) except KeyError as e : if isinstance ( metabolite , Metabolite ) : new_metabolites . append ( metabolite ) else : raise e elif isinstance ( metabolite , string_types ) : raise ValueError ( "Reaction '%s' does not belong to a " "model. Either add the reaction to a " "model or use Metabolite objects instead " "of strings as keys." % self . id ) self . _metabolites [ metabolite ] = coefficient metabolite . _reaction . add ( self ) model = self . model if model is not None : model . add_metabolites ( new_metabolites ) for metabolite , coefficient in self . _metabolites . items ( ) : model . constraints [ metabolite . id ] . set_linear_coefficients ( { self . forward_variable : coefficient , self . reverse_variable : - coefficient } ) for metabolite , the_coefficient in list ( self . _metabolites . items ( ) ) : if the_coefficient == 0 : metabolite . _reaction . remove ( self ) self . _metabolites . pop ( metabolite ) context = get_context ( self ) if context and reversibly : if combine : context ( partial ( self . subtract_metabolites , metabolites_to_add , combine = True , reversibly = False ) ) else : mets_to_reset = { key : old_coefficients [ model . metabolites . get_by_any ( key ) [ 0 ] ] for key in iterkeys ( metabolites_to_add ) } context ( partial ( self . add_metabolites , mets_to_reset , combine = False , reversibly = False ) )
6209	def print_attrs ( data_file , node_name = '/' , which = 'user' , compress = False ) : node = data_file . get_node ( node_name ) print ( 'List of attributes for:\n %s\n' % node ) for attr in node . _v_attrs . _f_list ( ) : print ( '\t%s' % attr ) attr_content = repr ( node . _v_attrs [ attr ] ) if compress : attr_content = attr_content . split ( '\n' ) [ 0 ] print ( "\t %s" % attr_content )
2173	def authorization_url ( self , url , state = None , ** kwargs ) : state = state or self . new_state ( ) return ( self . _client . prepare_request_uri ( url , redirect_uri = self . redirect_uri , scope = self . scope , state = state , ** kwargs ) , state , )
6737	def get_component_settings ( prefixes = None ) : prefixes = prefixes or [ ] assert isinstance ( prefixes , ( tuple , list ) ) , 'Prefixes must be a sequence type, not %s.' % type ( prefixes ) data = { } for name in prefixes : name = name . lower ( ) . strip ( ) for k in sorted ( env ) : if k . startswith ( '%s_' % name ) : new_k = k [ len ( name ) + 1 : ] data [ new_k ] = env [ k ] return data
10141	def parse_arguments ( args , clone_list ) : returned_string = "" host_number = args . host if args . show_list : print ( generate_host_string ( clone_list , "Available hosts: " ) ) exit ( ) if args . decrypt : for i in args . files : print ( decrypt_files ( i ) ) exit ( ) if args . files : for i in args . files : if args . limit_size : if args . host == host_number and host_number is not None : if not check_max_filesize ( i , clone_list [ host_number ] [ 3 ] ) : host_number = None for n , host in enumerate ( clone_list ) : if not check_max_filesize ( i , host [ 3 ] ) : clone_list [ n ] = None if not clone_list : print ( 'None of the clones is able to support so big file.' ) if args . no_cloudflare : if args . host == host_number and host_number is not None and not clone_list [ host_number ] [ 4 ] : print ( "This host uses Cloudflare, please choose different host." ) exit ( 1 ) else : for n , host in enumerate ( clone_list ) : if not host [ 4 ] : clone_list [ n ] = None clone_list = list ( filter ( None , clone_list ) ) if host_number is None or args . host != host_number : host_number = random . randrange ( 0 , len ( clone_list ) ) while True : try : if args . encrypt : returned_string = encrypt_files ( clone_list [ host_number ] , args . only_link , i ) else : returned_string = upload_files ( open ( i , 'rb' ) , clone_list [ host_number ] , args . only_link , i ) if args . only_link : print ( returned_string [ 0 ] ) else : print ( returned_string ) except IndexError : host_number = random . randrange ( 0 , len ( clone_list ) ) continue except IsADirectoryError : print ( 'limf does not support directory upload, if you want to upload ' 'every file in directory use limf {}/*.' . format ( i . replace ( '/' , '' ) ) ) if args . log : with open ( os . path . expanduser ( args . logfile ) , "a+" ) as logfile : if args . only_link : logfile . write ( returned_string [ 1 ] ) else : logfile . write ( returned_string ) logfile . write ( "\n" ) break else : print ( "limf: try 'limf -h' for more information" )
9490	def _get_const_info ( const_index , const_list ) : argval = const_index if const_list is not None : try : argval = const_list [ const_index ] except IndexError : raise ValidationError ( "Consts value out of range: {}" . format ( const_index ) ) from None return argval , repr ( argval )
1166	def join ( self , timeout = None ) : if not self . __initialized : raise RuntimeError ( "Thread.__init__() not called" ) if not self . __started . is_set ( ) : raise RuntimeError ( "cannot join thread before it is started" ) if self is current_thread ( ) : raise RuntimeError ( "cannot join current thread" ) if __debug__ : if not self . __stopped : self . _note ( "%s.join(): waiting until thread stops" , self ) self . __block . acquire ( ) try : if timeout is None : while not self . __stopped : self . __block . wait ( ) if __debug__ : self . _note ( "%s.join(): thread stopped" , self ) else : deadline = _time ( ) + timeout while not self . __stopped : delay = deadline - _time ( ) if delay <= 0 : if __debug__ : self . _note ( "%s.join(): timed out" , self ) break self . __block . wait ( delay ) else : if __debug__ : self . _note ( "%s.join(): thread stopped" , self ) finally : self . __block . release ( )
3102	def run_flow ( flow , storage , flags = None , http = None ) : if flags is None : flags = argparser . parse_args ( ) logging . getLogger ( ) . setLevel ( getattr ( logging , flags . logging_level ) ) if not flags . noauth_local_webserver : success = False port_number = 0 for port in flags . auth_host_port : port_number = port try : httpd = ClientRedirectServer ( ( flags . auth_host_name , port ) , ClientRedirectHandler ) except socket . error : pass else : success = True break flags . noauth_local_webserver = not success if not success : print ( _FAILED_START_MESSAGE ) if not flags . noauth_local_webserver : oauth_callback = 'http://{host}:{port}/' . format ( host = flags . auth_host_name , port = port_number ) else : oauth_callback = client . OOB_CALLBACK_URN flow . redirect_uri = oauth_callback authorize_url = flow . step1_get_authorize_url ( ) if not flags . noauth_local_webserver : import webbrowser webbrowser . open ( authorize_url , new = 1 , autoraise = True ) print ( _BROWSER_OPENED_MESSAGE . format ( address = authorize_url ) ) else : print ( _GO_TO_LINK_MESSAGE . format ( address = authorize_url ) ) code = None if not flags . noauth_local_webserver : httpd . handle_request ( ) if 'error' in httpd . query_params : sys . exit ( 'Authentication request was rejected.' ) if 'code' in httpd . query_params : code = httpd . query_params [ 'code' ] else : print ( 'Failed to find "code" in the query parameters ' 'of the redirect.' ) sys . exit ( 'Try running with --noauth_local_webserver.' ) else : code = input ( 'Enter verification code: ' ) . strip ( ) try : credential = flow . step2_exchange ( code , http = http ) except client . FlowExchangeError as e : sys . exit ( 'Authentication has failed: {0}' . format ( e ) ) storage . put ( credential ) credential . set_store ( storage ) print ( 'Authentication successful.' ) return credential
2501	def value_error ( self , key , bad_value ) : msg = ERROR_MESSAGES [ key ] . format ( bad_value ) self . logger . log ( msg ) self . error = True
4744	def dev_get_rprt ( dev_name , pugrp = None , punit = None ) : cmd = [ "nvm_cmd" , "rprt_all" , dev_name ] if not ( pugrp is None and punit is None ) : cmd = [ "nvm_cmd" , "rprt_lun" , dev_name , str ( pugrp ) , str ( punit ) ] _ , _ , _ , struct = cij . test . command_to_struct ( cmd ) if not struct : return None return struct [ "rprt_descr" ]
6109	def xticks ( self ) : return np . linspace ( np . amin ( self . grid_stack . regular [ : , 1 ] ) , np . amax ( self . grid_stack . regular [ : , 1 ] ) , 4 )
4775	def contains_duplicates ( self ) : try : if len ( self . val ) != len ( set ( self . val ) ) : return self except TypeError : raise TypeError ( 'val is not iterable' ) self . _err ( 'Expected <%s> to contain duplicates, but did not.' % self . val )
2960	def __base_state ( self , containers ) : return dict ( blockade_id = self . _blockade_id , containers = containers , version = self . _state_version )
9492	def compile_bytecode ( code : list ) -> bytes : bc = b"" for i , op in enumerate ( code ) : try : if isinstance ( op , _PyteOp ) or isinstance ( op , _PyteAugmentedComparator ) : bc_op = op . to_bytes ( bc ) elif isinstance ( op , int ) : bc_op = op . to_bytes ( 1 , byteorder = "little" ) elif isinstance ( op , bytes ) : bc_op = op else : raise CompileError ( "Could not compile code of type {}" . format ( type ( op ) ) ) bc += bc_op except Exception as e : print ( "Fatal compiliation error on operator {i} ({op})." . format ( i = i , op = op ) ) raise e return bc
8824	def context ( self ) : if not self . _context : self . _context = context . get_admin_context ( ) return self . _context
7139	def spend_key ( self ) : key = self . _backend . spend_key ( ) if key == numbers . EMPTY_KEY : return None return key
12688	def send_now ( users , label , extra_context = None , sender = None ) : sent = False if extra_context is None : extra_context = { } notice_type = NoticeType . objects . get ( label = label ) current_language = get_language ( ) for user in users : try : language = get_notification_language ( user ) except LanguageStoreNotAvailable : language = None if language is not None : activate ( language ) for backend in NOTIFICATION_BACKENDS . values ( ) : if backend . can_send ( user , notice_type ) : backend . deliver ( user , sender , notice_type , extra_context ) sent = True activate ( current_language ) return sent
7694	def _sasl_authenticate ( self , stream , username , authzid ) : if not stream . initiator : raise SASLAuthenticationFailed ( "Only initiating entity start" " SASL authentication" ) if stream . features is None or not self . peer_sasl_mechanisms : raise SASLNotAvailable ( "Peer doesn't support SASL" ) props = dict ( stream . auth_properties ) if not props . get ( "service-domain" ) and ( stream . peer and stream . peer . domain ) : props [ "service-domain" ] = stream . peer . domain if username is not None : props [ "username" ] = username if authzid is not None : props [ "authzid" ] = authzid if "password" in self . settings : props [ "password" ] = self . settings [ "password" ] props [ "available_mechanisms" ] = self . peer_sasl_mechanisms enabled = sasl . filter_mechanism_list ( self . settings [ 'sasl_mechanisms' ] , props , self . settings [ 'insecure_auth' ] ) if not enabled : raise SASLNotAvailable ( "None of SASL mechanism selected can be used" ) props [ "enabled_mechanisms" ] = enabled mechanism = None for mech in enabled : if mech in self . peer_sasl_mechanisms : mechanism = mech break if not mechanism : raise SASLMechanismNotAvailable ( "Peer doesn't support any of" " our SASL mechanisms" ) logger . debug ( "Our mechanism: {0!r}" . format ( mechanism ) ) stream . auth_method_used = mechanism self . authenticator = sasl . client_authenticator_factory ( mechanism ) initial_response = self . authenticator . start ( props ) if not isinstance ( initial_response , sasl . Response ) : raise SASLAuthenticationFailed ( "SASL initiation failed" ) element = ElementTree . Element ( AUTH_TAG ) element . set ( "mechanism" , mechanism ) if initial_response . data : if initial_response . encode : element . text = initial_response . encode ( ) else : element . text = initial_response . data stream . write_element ( element )
857	def _updateSequenceInfo ( self , r ) : newSequence = False sequenceId = ( r [ self . _sequenceIdIdx ] if self . _sequenceIdIdx is not None else None ) if sequenceId != self . _currSequence : if sequenceId in self . _sequences : raise Exception ( 'Broken sequence: %s, record: %s' % ( sequenceId , r ) ) self . _sequences . add ( self . _currSequence ) self . _currSequence = sequenceId if self . _resetIdx : assert r [ self . _resetIdx ] == 1 newSequence = True else : reset = False if self . _resetIdx : reset = r [ self . _resetIdx ] if reset == 1 : newSequence = True if not newSequence : if self . _timeStampIdx and self . _currTime is not None : t = r [ self . _timeStampIdx ] if t < self . _currTime : raise Exception ( 'No time travel. Early timestamp for record: %s' % r ) if self . _timeStampIdx : self . _currTime = r [ self . _timeStampIdx ]
3960	def update_local_repo_async ( self , task_queue , force = False ) : self . ensure_local_repo ( ) task_queue . enqueue_task ( self . update_local_repo , force = force )
9433	def _load_savefile_header ( file_h ) : try : raw_savefile_header = file_h . read ( 24 ) except UnicodeDecodeError : print ( "\nMake sure the input file is opened in read binary, 'rb'\n" ) raise InvalidEncoding ( "Could not read file; it might not be opened in binary mode." ) if raw_savefile_header [ : 4 ] in [ struct . pack ( ">I" , _MAGIC_NUMBER ) , struct . pack ( ">I" , _MAGIC_NUMBER_NS ) ] : byte_order = b'big' unpacked = struct . unpack ( '>IhhIIII' , raw_savefile_header ) elif raw_savefile_header [ : 4 ] in [ struct . pack ( "<I" , _MAGIC_NUMBER ) , struct . pack ( "<I" , _MAGIC_NUMBER_NS ) ] : byte_order = b'little' unpacked = struct . unpack ( '<IhhIIII' , raw_savefile_header ) else : raise UnknownMagicNumber ( "No supported Magic Number found" ) ( magic , major , minor , tz_off , ts_acc , snaplen , ll_type ) = unpacked header = __pcap_header__ ( magic , major , minor , tz_off , ts_acc , snaplen , ll_type , ctypes . c_char_p ( byte_order ) , magic == _MAGIC_NUMBER_NS ) if not __validate_header__ ( header ) : raise InvalidHeader ( "Invalid Header" ) else : return header
9481	def to_bytes ( self , previous : bytes ) : if len ( self . conditions ) != len ( self . body ) : raise exc . CompileError ( "Conditions and body length mismatch!" ) bc = b"" prev_len = len ( previous ) for condition , body in zip ( self . conditions , self . body ) : cond_bytecode = condition . to_bytecode ( previous ) bc += cond_bytecode body_bc = compiler . compile_bytecode ( body ) bdyl = len ( body_bc ) gen_len = prev_len + len ( cond_bytecode ) + bdyl + 1 bc += generate_simple_call ( tokens . POP_JUMP_IF_FALSE , gen_len ) bc += body_bc prev_len = len ( previous ) + len ( bc ) return bc
12547	def abs_img ( img ) : bool_img = np . abs ( read_img ( img ) . get_data ( ) ) return bool_img . astype ( int )
7822	def _make_response ( self , nonce , salt , iteration_count ) : self . _salted_password = self . Hi ( self . Normalize ( self . password ) , salt , iteration_count ) self . password = None if self . channel_binding : channel_binding = b"c=" + standard_b64encode ( self . _gs2_header + self . _cb_data ) else : channel_binding = b"c=" + standard_b64encode ( self . _gs2_header ) client_final_message_without_proof = ( channel_binding + b",r=" + nonce ) client_key = self . HMAC ( self . _salted_password , b"Client Key" ) stored_key = self . H ( client_key ) auth_message = ( self . _client_first_message_bare + b"," + self . _server_first_message + b"," + client_final_message_without_proof ) self . _auth_message = auth_message client_signature = self . HMAC ( stored_key , auth_message ) client_proof = self . XOR ( client_key , client_signature ) proof = b"p=" + standard_b64encode ( client_proof ) client_final_message = ( client_final_message_without_proof + b"," + proof ) return Response ( client_final_message )
6405	def cmp_features ( feat1 , feat2 ) : if feat1 < 0 or feat2 < 0 : return - 1.0 if feat1 == feat2 : return 1.0 magnitude = len ( _FEATURE_MASK ) featxor = feat1 ^ feat2 diffbits = 0 while featxor : if featxor & 0b1 : diffbits += 1 featxor >>= 1 return 1 - ( diffbits / ( 2 * magnitude ) )
1572	def setup ( self , context ) : myindex = context . get_partition_index ( ) self . _files_to_consume = self . _files [ myindex : : context . get_num_partitions ( ) ] self . logger . info ( "TextFileSpout files to consume %s" % self . _files_to_consume ) self . _lines_to_consume = self . _get_next_lines ( ) self . _emit_count = 0
2603	def client_file ( self ) : return os . path . join ( self . ipython_dir , 'profile_{0}' . format ( self . profile ) , 'security/ipcontroller-client.json' )
12775	def inverse_dynamics ( self , angles , start = 0 , end = 1e100 , states = None , max_force = 100 ) : if states is not None : self . skeleton . set_body_states ( states ) for frame_no , frame in enumerate ( angles ) : if frame_no < start : continue if frame_no >= end : break self . ode_space . collide ( None , self . on_collision ) states = self . skeleton . get_body_states ( ) self . skeleton . set_body_states ( states ) self . skeleton . enable_motors ( max_force ) self . skeleton . set_target_angles ( angles [ frame_no ] ) self . ode_world . step ( self . dt ) torques = self . skeleton . joint_torques self . skeleton . disable_motors ( ) self . skeleton . set_body_states ( states ) self . skeleton . add_torques ( torques ) yield torques self . ode_world . step ( self . dt ) self . ode_contactgroup . empty ( )
886	def punishPredictedColumn ( self , column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells ) : self . _punishPredictedColumn ( self . connections , columnMatchingSegments , prevActiveCells , self . predictedSegmentDecrement )
1882	def new_symbolic_buffer ( self , nbytes , ** options ) : label = options . get ( 'label' ) avoid_collisions = False if label is None : label = 'buffer' avoid_collisions = True taint = options . get ( 'taint' , frozenset ( ) ) expr = self . _constraints . new_array ( name = label , index_max = nbytes , value_bits = 8 , taint = taint , avoid_collisions = avoid_collisions ) self . _input_symbols . append ( expr ) if options . get ( 'cstring' , False ) : for i in range ( nbytes - 1 ) : self . _constraints . add ( expr [ i ] != 0 ) return expr
1596	def format_mtime ( mtime ) : now = datetime . now ( ) dt = datetime . fromtimestamp ( mtime ) return '%s %2d %5s' % ( dt . strftime ( '%b' ) , dt . day , dt . year if dt . year != now . year else dt . strftime ( '%H:%M' ) )
2401	def gen_length_feats ( self , e_set ) : text = e_set . _text lengths = [ len ( e ) for e in text ] word_counts = [ max ( len ( t ) , 1 ) for t in e_set . _tokens ] comma_count = [ e . count ( "," ) for e in text ] ap_count = [ e . count ( "'" ) for e in text ] punc_count = [ e . count ( "." ) + e . count ( "?" ) + e . count ( "!" ) for e in text ] chars_per_word = [ lengths [ m ] / float ( word_counts [ m ] ) for m in xrange ( 0 , len ( text ) ) ] good_pos_tags , bad_pos_positions = self . _get_grammar_errors ( e_set . _pos , e_set . _text , e_set . _tokens ) good_pos_tag_prop = [ good_pos_tags [ m ] / float ( word_counts [ m ] ) for m in xrange ( 0 , len ( text ) ) ] length_arr = numpy . array ( ( lengths , word_counts , comma_count , ap_count , punc_count , chars_per_word , good_pos_tags , good_pos_tag_prop ) ) . transpose ( ) return length_arr . copy ( )
2124	def associate_always_node ( self , parent , child = None , ** kwargs ) : return self . _assoc_or_create ( 'always' , parent , child , ** kwargs )
2032	def MLOAD ( self , address ) : self . _allocate ( address , 32 ) value = self . _load ( address , 32 ) return value
7719	def xpath_eval ( self , expr ) : ctxt = common_doc . xpathNewContext ( ) ctxt . setContextNode ( self . xmlnode ) ctxt . xpathRegisterNs ( "muc" , self . ns . getContent ( ) ) ret = ctxt . xpathEval ( to_utf8 ( expr ) ) ctxt . xpathFreeContext ( ) return ret
6068	def tabulate_integral ( self , grid , tabulate_bins ) : eta_min = 1.0e-4 eta_max = 1.05 * np . max ( self . grid_to_elliptical_radii ( grid ) ) minimum_log_eta = np . log10 ( eta_min ) maximum_log_eta = np . log10 ( eta_max ) bin_size = ( maximum_log_eta - minimum_log_eta ) / ( tabulate_bins - 1 ) return eta_min , eta_max , minimum_log_eta , maximum_log_eta , bin_size
1725	def eval ( self , expression , use_compilation_plan = False ) : code = 'PyJsEvalResult = eval(%s)' % json . dumps ( expression ) self . execute ( code , use_compilation_plan = use_compilation_plan ) return self [ 'PyJsEvalResult' ]
9213	def t_t_eopen ( self , t ) : r'~"|~\'' if t . value [ 1 ] == '"' : t . lexer . push_state ( 'escapequotes' ) elif t . value [ 1 ] == '\'' : t . lexer . push_state ( 'escapeapostrophe' ) return t
344	def train_and_validate_to_end ( self , validate_step_size = 50 ) : while not self . _sess . should_stop ( ) : self . train_on_batch ( ) if self . global_step % validate_step_size == 0 : log_str = 'step: %d, ' % self . global_step for n , m in self . validation_metrics : log_str += '%s: %f, ' % ( n . name , m ) logging . info ( log_str )
1331	def batch_predictions ( self , images , greedy = False , strict = True , return_details = False ) : if strict : in_bounds = self . in_bounds ( images ) assert in_bounds self . _total_prediction_calls += len ( images ) predictions = self . __model . batch_predictions ( images ) assert predictions . ndim == 2 assert predictions . shape [ 0 ] == images . shape [ 0 ] if return_details : assert greedy adversarials = [ ] for i in range ( len ( predictions ) ) : if strict : in_bounds_i = True else : in_bounds_i = self . in_bounds ( images [ i ] ) is_adversarial , is_best , distance = self . __is_adversarial ( images [ i ] , predictions [ i ] , in_bounds_i ) if is_adversarial and greedy : if return_details : return predictions , is_adversarial , i , is_best , distance else : return predictions , is_adversarial , i adversarials . append ( is_adversarial ) if greedy : if return_details : return predictions , False , None , False , None else : return predictions , False , None is_adversarial = np . array ( adversarials ) assert is_adversarial . ndim == 1 assert is_adversarial . shape [ 0 ] == images . shape [ 0 ] return predictions , is_adversarial
3375	def add_absolute_expression ( model , expression , name = "abs_var" , ub = None , difference = 0 , add = True ) : Components = namedtuple ( 'Components' , [ 'variable' , 'upper_constraint' , 'lower_constraint' ] ) variable = model . problem . Variable ( name , lb = 0 , ub = ub ) upper_constraint = model . problem . Constraint ( expression - variable , ub = difference , name = "abs_pos_" + name ) , lower_constraint = model . problem . Constraint ( expression + variable , lb = difference , name = "abs_neg_" + name ) to_add = Components ( variable , upper_constraint , lower_constraint ) if add : add_cons_vars_to_problem ( model , to_add ) return to_add
2209	def parse_requirements_alt ( fname = 'requirements.txt' ) : import requirements from os . path import dirname , join , exists require_fpath = join ( dirname ( __file__ ) , fname ) if exists ( require_fpath ) : with open ( require_fpath , 'r' ) as file : requires = list ( requirements . parse ( file ) ) packages = [ r . name for r in requires ] return packages return [ ]
5138	def process_file ( self , file ) : if sys . version_info [ 0 ] >= 3 : nxt = file . __next__ else : nxt = file . next for token in tokenize . generate_tokens ( nxt ) : self . process_token ( * token ) self . make_index ( )
6204	def populations_diff_coeff ( particles , populations ) : D_counts = particles . diffusion_coeff_counts if len ( D_counts ) == 1 : pop_sizes = [ pop . stop - pop . start for pop in populations ] assert D_counts [ 0 ] [ 1 ] >= sum ( pop_sizes ) D_counts = [ ( D_counts [ 0 ] [ 0 ] , ps ) for ps in pop_sizes ] D_list = [ ] D_pop_start = 0 for pop , ( D , counts ) in zip ( populations , D_counts ) : D_list . append ( D ) assert pop . start >= D_pop_start assert pop . stop <= D_pop_start + counts D_pop_start += counts return D_list
4200	def _thumbnail_div ( full_dir , fname , snippet , is_backref = False ) : thumb = os . path . join ( full_dir , 'images' , 'thumb' , 'sphx_glr_%s_thumb.png' % fname [ : - 3 ] ) ref_name = os . path . join ( full_dir , fname ) . replace ( os . path . sep , '_' ) template = BACKREF_THUMBNAIL_TEMPLATE if is_backref else THUMBNAIL_TEMPLATE return template . format ( snippet = snippet , thumbnail = thumb , ref_name = ref_name )
10042	def admin_permission_factory ( ) : try : pkg_resources . get_distribution ( 'invenio-access' ) from invenio_access . permissions import DynamicPermission as Permission except pkg_resources . DistributionNotFound : from flask_principal import Permission return Permission ( action_admin_access )
11947	def jocker ( test_options = None ) : version = ver_check ( ) options = test_options or docopt ( __doc__ , version = version ) _set_global_verbosity_level ( options . get ( '--verbose' ) ) jocker_lgr . debug ( options ) jocker_run ( options )
6943	def jhk_to_bmag ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , BJHK , BJH , BJK , BHK , BJ , BH , BK )
11238	def imap_send ( func , gen ) : gen = iter ( gen ) assert _is_just_started ( gen ) yielder = yield_from ( gen ) for item in yielder : with yielder : yielder . send ( func ( ( yield item ) ) ) return_ ( yielder . result )
13053	def nmap_scan ( ) : hs = HostSearch ( ) config = Config ( ) nmap_types = [ 'top10' , 'top100' , 'custom' , 'top1000' , 'all' ] options = { 'top10' : '--top-ports 10' , 'top100' : '--top-ports 100' , 'custom' : config . get ( 'nmap' , 'options' ) , 'top1000' : '--top-ports 1000' , 'all' : '-p-' } hs_parser = hs . argparser argparser = argparse . ArgumentParser ( parents = [ hs_parser ] , conflict_handler = 'resolve' , description = "Scans hosts from the database using nmap, any arguments that are not in the help are passed to nmap" ) argparser . add_argument ( 'type' , metavar = 'type' , help = 'The number of ports to scan: top10, top100, custom, top1000 (default) or all' , type = str , choices = nmap_types , default = 'top1000' , const = 'top1000' , nargs = '?' ) arguments , extra_nmap_args = argparser . parse_known_args ( ) tags = nmap_types [ nmap_types . index ( arguments . type ) : ] tags = [ "!nmap_" + tag for tag in tags ] hosts = hs . get_hosts ( tags = tags ) hosts = [ host for host in hosts ] nmap_args = [ ] nmap_args . extend ( extra_nmap_args ) nmap_args . extend ( options [ arguments . type ] . split ( ' ' ) ) print_notification ( "Running nmap with args: {} on {} hosts(s)" . format ( nmap_args , len ( hosts ) ) ) if len ( hosts ) : result = nmap ( nmap_args , [ str ( h . address ) for h in hosts ] ) for host in hosts : host . add_tag ( "nmap_{}" . format ( arguments . type ) ) host . save ( ) print_notification ( "Nmap done, importing results" ) stats = import_nmap ( result , "nmap_{}" . format ( arguments . type ) , check_function = all_hosts , import_services = True ) stats [ 'scanned_hosts' ] = len ( hosts ) stats [ 'type' ] = arguments . type Logger ( ) . log ( 'nmap_scan' , "Performed nmap {} scan on {} hosts" . format ( arguments . type , len ( hosts ) ) , stats ) else : print_notification ( "No hosts found" )
10387	def build_database ( manager : pybel . Manager , annotation_url : Optional [ str ] = None ) -> None : annotation_url = annotation_url or NEUROMMSIG_DEFAULT_URL annotation = manager . get_namespace_by_url ( annotation_url ) if annotation is None : raise RuntimeError ( 'no graphs in database with given annotation' ) networks = get_networks_using_annotation ( manager , annotation ) dtis = ... for network in networks : graph = network . as_bel ( ) scores = epicom_on_graph ( graph , dtis ) for ( drug_name , subgraph_name ) , score in scores . items ( ) : drug_model = get_drug_model ( manager , drug_name ) subgraph_model = manager . get_annotation_entry ( annotation_url , subgraph_name ) score_model = Score ( network = network , annotation = subgraph_model , drug = drug_model , score = score ) manager . session . add ( score_model ) t = time . time ( ) logger . info ( 'committing scores' ) manager . session . commit ( ) logger . info ( 'committed scores in %.2f seconds' , time . time ( ) - t )
5892	def get_urls ( self ) : urls = patterns ( '' , url ( r'^upload/$' , self . admin_site . admin_view ( self . handle_upload ) , name = 'quill-file-upload' ) , ) return urls + super ( QuillAdmin , self ) . get_urls ( )
7358	def _check_peptide_inputs ( self , peptides ) : require_iterable_of ( peptides , string_types ) check_X = not self . allow_X_in_peptides check_lower = not self . allow_lowercase_in_peptides check_min_length = self . min_peptide_length is not None min_length = self . min_peptide_length check_max_length = self . max_peptide_length is not None max_length = self . max_peptide_length for p in peptides : if not p . isalpha ( ) : raise ValueError ( "Invalid characters in peptide '%s'" % p ) elif check_X and "X" in p : raise ValueError ( "Invalid character 'X' in peptide '%s'" % p ) elif check_lower and not p . isupper ( ) : raise ValueError ( "Invalid lowercase letters in peptide '%s'" % p ) elif check_min_length and len ( p ) < min_length : raise ValueError ( "Peptide '%s' too short (%d chars), must be at least %d" % ( p , len ( p ) , min_length ) ) elif check_max_length and len ( p ) > max_length : raise ValueError ( "Peptide '%s' too long (%d chars), must be at least %d" % ( p , len ( p ) , max_length ) )
3774	def select_valid_methods ( self , T ) : r if self . forced : considered_methods = list ( self . user_methods ) else : considered_methods = list ( self . all_methods ) if self . user_methods : [ considered_methods . remove ( i ) for i in self . user_methods ] preferences = sorted ( [ self . ranked_methods . index ( i ) for i in considered_methods ] ) sorted_methods = [ self . ranked_methods [ i ] for i in preferences ] if self . user_methods : [ sorted_methods . insert ( 0 , i ) for i in reversed ( self . user_methods ) ] sorted_valid_methods = [ ] for method in sorted_methods : if self . test_method_validity ( T , method ) : sorted_valid_methods . append ( method ) return sorted_valid_methods
1045	def float_pack ( x , size ) : if size == 8 : MIN_EXP = - 1021 MAX_EXP = 1024 MANT_DIG = 53 BITS = 64 elif size == 4 : MIN_EXP = - 125 MAX_EXP = 128 MANT_DIG = 24 BITS = 32 else : raise ValueError ( "invalid size value" ) sign = math . copysign ( 1.0 , x ) < 0.0 if math . isinf ( x ) : mant = 0 exp = MAX_EXP - MIN_EXP + 2 elif math . isnan ( x ) : mant = 1 << ( MANT_DIG - 2 ) exp = MAX_EXP - MIN_EXP + 2 elif x == 0.0 : mant = 0 exp = 0 else : m , e = math . frexp ( abs ( x ) ) exp = e - ( MIN_EXP - 1 ) if exp > 0 : mant = round_to_nearest ( m * ( 1 << MANT_DIG ) ) mant -= 1 << MANT_DIG - 1 else : if exp + MANT_DIG - 1 >= 0 : mant = round_to_nearest ( m * ( 1 << exp + MANT_DIG - 1 ) ) else : mant = 0 exp = 0 assert 0 <= mant <= 1 << MANT_DIG - 1 if mant == 1 << MANT_DIG - 1 : mant = 0 exp += 1 if exp >= MAX_EXP - MIN_EXP + 2 : raise OverflowError ( "float too large to pack in this format" ) assert 0 <= mant < 1 << MANT_DIG - 1 assert 0 <= exp <= MAX_EXP - MIN_EXP + 2 assert 0 <= sign <= 1 return ( ( sign << BITS - 1 ) | ( exp << MANT_DIG - 1 ) ) | mant
5139	def process_token ( self , kind , string , start , end , line ) : if self . current_block . is_comment : if kind == tokenize . COMMENT : self . current_block . add ( string , start , end , line ) else : self . new_noncomment ( start [ 0 ] , end [ 0 ] ) else : if kind == tokenize . COMMENT : self . new_comment ( string , start , end , line ) else : self . current_block . add ( string , start , end , line )
786	def jobGetCancellingJobs ( self , ) : with ConnectionFactory . get ( ) as conn : query = 'SELECT job_id ' 'FROM %s ' 'WHERE (status<>%%s AND cancel is TRUE)' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . STATUS_COMPLETED ] ) rows = conn . cursor . fetchall ( ) return tuple ( r [ 0 ] for r in rows )
3359	def insert ( self , index , object ) : self . _check ( object . id ) list . insert ( self , index , object ) _dict = self . _dict for i , j in iteritems ( _dict ) : if j >= index : _dict [ i ] = j + 1 _dict [ object . id ] = index
11364	def run_shell_command ( commands , ** kwargs ) : p = subprocess . Popen ( commands , stdout = subprocess . PIPE , stderr = subprocess . PIPE , ** kwargs ) output , error = p . communicate ( ) return p . returncode , output , error
4926	def transform_title ( self , content_metadata_item ) : title_with_locales = [ ] for locale in self . enterprise_configuration . get_locales ( ) : title_with_locales . append ( { 'locale' : locale , 'value' : content_metadata_item . get ( 'title' , '' ) } ) return title_with_locales
10333	def group_nodes_by_annotation_filtered ( graph : BELGraph , node_predicates : NodePredicates = None , annotation : str = 'Subgraph' , ) -> Mapping [ str , Set [ BaseEntity ] ] : node_filter = concatenate_node_predicates ( node_predicates ) return { key : { node for node in nodes if node_filter ( graph , node ) } for key , nodes in group_nodes_by_annotation ( graph , annotation ) . items ( ) }
12155	def list_move_to_front ( l , value = 'other' ) : l = list ( l ) if value in l : l . remove ( value ) l . insert ( 0 , value ) return l
6579	def _send_cmd ( self , cmd ) : self . _process . stdin . write ( "{}\n" . format ( cmd ) . encode ( "utf-8" ) ) self . _process . stdin . flush ( )
8660	def is_valid_identifier ( name ) : if not isinstance ( name , str ) : return False if '\n' in name : return False if name . strip ( ) != name : return False try : code = compile ( '\n{0}=None' . format ( name ) , filename = '<string>' , mode = 'single' ) exec ( code ) return True except SyntaxError : return False
6143	def DSP_capture_add_samples_stereo ( self , new_data_left , new_data_right ) : self . capture_sample_count = self . capture_sample_count + len ( new_data_left ) + len ( new_data_right ) if self . Tcapture > 0 : self . data_capture_left = np . hstack ( ( self . data_capture_left , new_data_left ) ) self . data_capture_right = np . hstack ( ( self . data_capture_right , new_data_right ) ) if ( len ( self . data_capture_left ) > self . Ncapture ) : self . data_capture_left = self . data_capture_left [ - self . Ncapture : ] if ( len ( self . data_capture_right ) > self . Ncapture ) : self . data_capture_right = self . data_capture_right [ - self . Ncapture : ]
13543	def from_server ( cls , server , slug , identifier ) : task = server . get ( 'task' , replacements = { 'slug' : slug , 'identifier' : identifier } ) return cls ( ** task )
8613	def delete_volume ( self , datacenter_id , volume_id ) : response = self . _perform_request ( url = '/datacenters/%s/volumes/%s' % ( datacenter_id , volume_id ) , method = 'DELETE' ) return response
8090	def textheight ( self , txt , width = None ) : w = width return self . textmetrics ( txt , width = w ) [ 1 ]
12002	def _remove_magic ( self , data ) : if not self . magic : return data magic_size = len ( self . magic ) magic = data [ : magic_size ] if magic != self . magic : raise Exception ( 'Invalid magic' ) data = data [ magic_size : ] return data
4998	def delete_enterprise_learner_role_assignment ( sender , instance , ** kwargs ) : if instance . user : enterprise_learner_role , __ = SystemWideEnterpriseRole . objects . get_or_create ( name = ENTERPRISE_LEARNER_ROLE ) try : SystemWideEnterpriseUserRoleAssignment . objects . get ( user = instance . user , role = enterprise_learner_role ) . delete ( ) except SystemWideEnterpriseUserRoleAssignment . DoesNotExist : pass
4206	def levup ( acur , knxt , ecur = None ) : if acur [ 0 ] != 1 : raise ValueError ( 'At least one of the reflection coefficients is equal to one.' ) acur = acur [ 1 : ] anxt = numpy . concatenate ( ( acur , [ 0 ] ) ) + knxt * numpy . concatenate ( ( numpy . conj ( acur [ - 1 : : - 1 ] ) , [ 1 ] ) ) enxt = None if ecur is not None : enxt = ( 1. - numpy . dot ( numpy . conj ( knxt ) , knxt ) ) * ecur anxt = numpy . insert ( anxt , 0 , 1 ) return anxt , enxt
11217	def _pop_claims_from_payload ( self ) : claims_in_payload = [ k for k in self . payload . keys ( ) if k in registered_claims . values ( ) ] for name in claims_in_payload : self . registered_claims [ name ] = self . payload . pop ( name )
13278	def update_desc_rcin_path ( desc , sibs_len , pdesc_level ) : psibs_len = pdesc_level . __len__ ( ) parent_breadth = desc [ 'parent_breadth_path' ] [ - 1 ] if ( desc [ 'sib_seq' ] == ( sibs_len - 1 ) ) : if ( parent_breadth == ( psibs_len - 1 ) ) : pass else : parent_rsib_breadth = parent_breadth + 1 prsib_desc = pdesc_level [ parent_rsib_breadth ] if ( prsib_desc [ 'leaf' ] ) : pass else : rcin_path = copy . deepcopy ( prsib_desc [ 'path' ] ) rcin_path . append ( 0 ) desc [ 'rcin_path' ] = rcin_path else : pass return ( desc )
3032	def credentials_from_code ( client_id , client_secret , scope , code , redirect_uri = 'postmessage' , http = None , user_agent = None , token_uri = oauth2client . GOOGLE_TOKEN_URI , auth_uri = oauth2client . GOOGLE_AUTH_URI , revoke_uri = oauth2client . GOOGLE_REVOKE_URI , device_uri = oauth2client . GOOGLE_DEVICE_URI , token_info_uri = oauth2client . GOOGLE_TOKEN_INFO_URI , pkce = False , code_verifier = None ) : flow = OAuth2WebServerFlow ( client_id , client_secret , scope , redirect_uri = redirect_uri , user_agent = user_agent , auth_uri = auth_uri , token_uri = token_uri , revoke_uri = revoke_uri , device_uri = device_uri , token_info_uri = token_info_uri , pkce = pkce , code_verifier = code_verifier ) credentials = flow . step2_exchange ( code , http = http ) return credentials
5198	def GetApplicationIIN ( self ) : application_iin = opendnp3 . ApplicationIIN ( ) application_iin . configCorrupt = False application_iin . deviceTrouble = False application_iin . localControl = False application_iin . needTime = False iin_field = application_iin . ToIIN ( ) _log . debug ( 'OutstationApplication.GetApplicationIIN: IINField LSB={}, MSB={}' . format ( iin_field . LSB , iin_field . MSB ) ) return application_iin
9970	def _get_col_index ( name ) : index = string . ascii_uppercase . index col = 0 for c in name . upper ( ) : col = col * 26 + index ( c ) + 1 return col
341	def google2_log_prefix ( level , timestamp = None , file_and_line = None ) : global _level_names now = timestamp or _time . time ( ) now_tuple = _time . localtime ( now ) now_microsecond = int ( 1e6 * ( now % 1.0 ) ) ( filename , line ) = file_and_line or _GetFileAndLine ( ) basename = _os . path . basename ( filename ) severity = 'I' if level in _level_names : severity = _level_names [ level ] [ 0 ] s = '%c%02d%02d %02d: %02d: %02d.%06d %5d %s: %d] ' % ( severity , now_tuple [ 1 ] , now_tuple [ 2 ] , now_tuple [ 3 ] , now_tuple [ 4 ] , now_tuple [ 5 ] , now_microsecond , _get_thread_id ( ) , basename , line ) return s
646	def generateVectors ( numVectors = 100 , length = 500 , activity = 50 ) : vectors = [ ] coinc = numpy . zeros ( length , dtype = 'int32' ) indexList = range ( length ) for i in xrange ( numVectors ) : coinc [ : ] = 0 coinc [ random . sample ( indexList , activity ) ] = 1 vectors . append ( coinc . copy ( ) ) return vectors
5617	def clean_geometry_type ( geometry , target_type , allow_multipart = True ) : multipart_geoms = { "Point" : MultiPoint , "LineString" : MultiLineString , "Polygon" : MultiPolygon , "MultiPoint" : MultiPoint , "MultiLineString" : MultiLineString , "MultiPolygon" : MultiPolygon } if target_type not in multipart_geoms . keys ( ) : raise TypeError ( "target type is not supported: %s" % target_type ) if geometry . geom_type == target_type : return geometry elif allow_multipart : target_multipart_type = multipart_geoms [ target_type ] if geometry . geom_type == "GeometryCollection" : return target_multipart_type ( [ clean_geometry_type ( g , target_type , allow_multipart ) for g in geometry ] ) elif any ( [ isinstance ( geometry , target_multipart_type ) , multipart_geoms [ geometry . geom_type ] == target_multipart_type ] ) : return geometry raise GeometryTypeError ( "geometry type does not match: %s, %s" % ( geometry . geom_type , target_type ) )
13212	def build_jsonld ( self , url = None , code_url = None , ci_url = None , readme_url = None , license_id = None ) : jsonld = { '@context' : [ "https://raw.githubusercontent.com/codemeta/codemeta/2.0-rc/" "codemeta.jsonld" , "http://schema.org" ] , '@type' : [ 'Report' , 'SoftwareSourceCode' ] , 'language' : 'TeX' , 'reportNumber' : self . handle , 'name' : self . plain_title , 'description' : self . plain_abstract , 'author' : [ { '@type' : 'Person' , 'name' : author_name } for author_name in self . plain_authors ] , 'dateModified' : self . revision_datetime } try : jsonld [ 'articleBody' ] = self . plain_content jsonld [ 'fileFormat' ] = 'text/plain' except RuntimeError : self . _logger . exception ( 'Could not convert latex body to plain ' 'text for articleBody.' ) self . _logger . warning ( 'Falling back to tex source for articleBody' ) jsonld [ 'articleBody' ] = self . _tex jsonld [ 'fileFormat' ] = 'text/plain' if url is not None : jsonld [ '@id' ] = url jsonld [ 'url' ] = url else : jsonld [ '@id' ] = self . handle if code_url is not None : jsonld [ 'codeRepository' ] = code_url if ci_url is not None : jsonld [ 'contIntegration' ] = ci_url if readme_url is not None : jsonld [ 'readme' ] = readme_url if license_id is not None : jsonld [ 'license_id' ] = None return jsonld
5383	def delete_jobs ( self , user_ids , job_ids , task_ids , labels , create_time_min = None , create_time_max = None ) : tasks = list ( self . lookup_job_tasks ( { 'RUNNING' } , user_ids = user_ids , job_ids = job_ids , task_ids = task_ids , labels = labels , create_time_min = create_time_min , create_time_max = create_time_max ) ) print ( 'Found %d tasks to delete.' % len ( tasks ) ) return google_base . cancel ( self . _service . new_batch_http_request , self . _service . operations ( ) . cancel , tasks )
10013	def parse_env_config ( config , env_name ) : all_env = get ( config , 'app.all_environments' , { } ) env = get ( config , 'app.environments.' + str ( env_name ) , { } ) return merge_dict ( all_env , env )
5914	def _process_command ( self , command , name = None ) : self . _command_counter += 1 if name is None : name = "CMD{0:03d}" . format ( self . _command_counter ) try : fd , tmp_ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = 'tmp_' + name + '__' ) cmd = [ command , '' , 'q' ] rc , out , err = self . make_ndx ( o = tmp_ndx , input = cmd ) self . check_output ( out , "No atoms found for selection {command!r}." . format ( ** vars ( ) ) , err = err ) groups = parse_ndxlist ( out ) last = groups [ - 1 ] fd , ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = name + '__' ) name_cmd = [ "keep {0:d}" . format ( last [ 'nr' ] ) , "name 0 {0!s}" . format ( name ) , 'q' ] rc , out , err = self . make_ndx ( n = tmp_ndx , o = ndx , input = name_cmd ) finally : utilities . unlink_gmx ( tmp_ndx ) return name , ndx
7449	def combinefiles ( filepath ) : fastqs = glob . glob ( filepath ) firsts = [ i for i in fastqs if "_R1_" in i ] if not firsts : raise IPyradWarningExit ( "First read files names must contain '_R1_'." ) seconds = [ ff . replace ( "_R1_" , "_R2_" ) for ff in firsts ] return zip ( firsts , seconds )
12454	def create_env ( env , args , recreate = False , ignore_activated = False , quiet = False ) : cmd = None result = True inside_env = hasattr ( sys , 'real_prefix' ) or os . environ . get ( 'VIRTUAL_ENV' ) env_exists = os . path . isdir ( env ) if not quiet : print_message ( '== Step 1. Create virtual environment ==' ) if ( recreate or ( not inside_env and not env_exists ) ) or ( ignore_activated and not env_exists ) : cmd = ( 'virtualenv' , ) + args + ( env , ) if not cmd and not quiet : if inside_env : message = 'Working inside of virtual environment, done...' else : message = 'Virtual environment {0!r} already created, done...' print_message ( message . format ( env ) ) if cmd : with disable_error_handler ( ) : result = not run_cmd ( cmd , echo = not quiet ) if not quiet : print_message ( ) return result
2015	def _rollback ( self ) : last_pc , last_gas , last_instruction , last_arguments , fee , allocated = self . _checkpoint_data self . _push_arguments ( last_arguments ) self . _gas = last_gas self . _pc = last_pc self . _allocated = allocated self . _checkpoint_data = None
7165	def add_entity ( self , name , lines , reload_cache = False ) : Entity . verify_name ( name ) self . entities . add ( Entity . wrap_name ( name ) , lines , reload_cache ) self . padaos . add_entity ( name , lines ) self . must_train = True
1327	def normalized_distance ( self , image ) : return self . __distance ( self . __original_image_for_distance , image , bounds = self . bounds ( ) )
12416	def end ( self , * args , ** kwargs ) : self . send ( * args , ** kwargs ) self . close ( )
10838	def interactions ( self ) : interactions = [ ] url = PATHS [ 'GET_INTERACTIONS' ] % self . id response = self . api . get ( url = url ) for interaction in response [ 'interactions' ] : interactions . append ( ResponseObject ( interaction ) ) self . __interactions = interactions return self . __interactions
4830	def get_course_grade ( self , course_id , username ) : results = self . client . courses ( course_id ) . get ( username = username ) for row in results : if row . get ( 'username' ) == username : return row raise HttpNotFoundError ( 'No grade record found for course={}, username={}' . format ( course_id , username ) )
10407	def bond_initialize_canonical_averages ( canonical_statistics , ** kwargs ) : spanning_cluster = ( 'percolation_probability' in canonical_statistics . dtype . names ) ret = np . empty_like ( canonical_statistics , dtype = canonical_averages_dtype ( spanning_cluster = spanning_cluster ) , ) ret [ 'number_of_runs' ] = 1 if spanning_cluster : ret [ 'percolation_probability_mean' ] = ( canonical_statistics [ 'percolation_probability' ] ) ret [ 'percolation_probability_m2' ] = 0.0 ret [ 'max_cluster_size_mean' ] = ( canonical_statistics [ 'max_cluster_size' ] ) ret [ 'max_cluster_size_m2' ] = 0.0 ret [ 'moments_mean' ] = canonical_statistics [ 'moments' ] ret [ 'moments_m2' ] = 0.0 return ret
13046	def f_hierarchical_passages ( reffs , citation ) : d = OrderedDict ( ) levels = [ x for x in citation ] for cit , name in reffs : ref = cit . split ( '-' ) [ 0 ] levs = [ '%{}|{}%' . format ( levels [ i ] . name , v ) for i , v in enumerate ( ref . split ( '.' ) ) ] getFromDict ( d , levs [ : - 1 ] ) [ name ] = cit return d
8510	def _predict ( self , X , method = 'fprop' ) : import theano X_sym = self . trainer . model . get_input_space ( ) . make_theano_batch ( ) y_sym = getattr ( self . trainer . model , method ) ( X_sym ) f = theano . function ( [ X_sym ] , y_sym , allow_input_downcast = True ) return f ( X )
1732	def is_empty_object ( n , last ) : if n . strip ( ) : return False last = last . strip ( ) markers = { ')' , ';' , } if not last or last [ - 1 ] in markers : return False return True
11642	def yaml_write_data ( yaml_data , filename ) : with open ( filename , 'w' ) as fd : yaml . dump ( yaml_data , fd , default_flow_style = False ) return True return False
5303	def parse_json_color_file ( path ) : with open ( path , "r" ) as color_file : color_list = json . load ( color_file ) color_dict = { c [ "name" ] : c [ "hex" ] for c in color_list } return color_dict
2234	def _proc_async_iter_stream ( proc , stream , buffersize = 1 ) : from six . moves import queue from threading import Thread def enqueue_output ( proc , stream , stream_queue ) : while proc . poll ( ) is None : line = stream . readline ( ) stream_queue . put ( line ) for line in _textio_iterlines ( stream ) : stream_queue . put ( line ) stream_queue . put ( None ) stream_queue = queue . Queue ( maxsize = buffersize ) _thread = Thread ( target = enqueue_output , args = ( proc , stream , stream_queue ) ) _thread . daemon = True _thread . start ( ) return stream_queue
6444	def _cond_s ( self , word , suffix_len ) : return word [ - suffix_len - 2 : - suffix_len ] == 'dr' or ( word [ - suffix_len - 1 ] == 't' and word [ - suffix_len - 2 : - suffix_len ] != 'tt' )
4397	def adsSyncAddDeviceNotificationReqEx ( port , adr , data_name , pNoteAttrib , callback , user_handle = None ) : global callback_store if NOTEFUNC is None : raise TypeError ( "Callback function type can't be None" ) adsSyncAddDeviceNotificationReqFct = _adsDLL . AdsSyncAddDeviceNotificationReqEx pAmsAddr = ctypes . pointer ( adr . amsAddrStruct ( ) ) hnl = adsSyncReadWriteReqEx2 ( port , adr , ADSIGRP_SYM_HNDBYNAME , 0x0 , PLCTYPE_UDINT , data_name , PLCTYPE_STRING ) nIndexGroup = ctypes . c_ulong ( ADSIGRP_SYM_VALBYHND ) nIndexOffset = ctypes . c_ulong ( hnl ) attrib = pNoteAttrib . notificationAttribStruct ( ) pNotification = ctypes . c_ulong ( ) nHUser = ctypes . c_ulong ( hnl ) if user_handle is not None : nHUser = ctypes . c_ulong ( user_handle ) adsSyncAddDeviceNotificationReqFct . argtypes = [ ctypes . c_ulong , ctypes . POINTER ( SAmsAddr ) , ctypes . c_ulong , ctypes . c_ulong , ctypes . POINTER ( SAdsNotificationAttrib ) , NOTEFUNC , ctypes . c_ulong , ctypes . POINTER ( ctypes . c_ulong ) , ] adsSyncAddDeviceNotificationReqFct . restype = ctypes . c_long def wrapper ( addr , notification , user ) : return callback ( notification , data_name ) c_callback = NOTEFUNC ( wrapper ) err_code = adsSyncAddDeviceNotificationReqFct ( port , pAmsAddr , nIndexGroup , nIndexOffset , ctypes . byref ( attrib ) , c_callback , nHUser , ctypes . byref ( pNotification ) , ) if err_code : raise ADSError ( err_code ) callback_store [ pNotification . value ] = c_callback return ( pNotification . value , hnl )
13173	def parents ( self , name = None ) : p = self . parent while p is not None : if name is None or p . tagname == name : yield p p = p . parent
702	def getResultsPerChoice ( self , swarmId , maxGenIdx , varName ) : results = dict ( ) ( allParticles , _ , resultErrs , _ , _ ) = self . getParticleInfos ( swarmId , genIdx = None , matured = True ) for particleState , resultErr in itertools . izip ( allParticles , resultErrs ) : if maxGenIdx is not None : if particleState [ 'genIdx' ] > maxGenIdx : continue if resultErr == numpy . inf : continue position = Particle . getPositionFromState ( particleState ) varPosition = position [ varName ] varPositionStr = str ( varPosition ) if varPositionStr in results : results [ varPositionStr ] [ 1 ] . append ( resultErr ) else : results [ varPositionStr ] = ( varPosition , [ resultErr ] ) return results
6170	def filter ( self , x ) : y = signal . sosfilt ( self . sos , x ) return y
109	def show_grid ( images , rows = None , cols = None ) : grid = draw_grid ( images , rows = rows , cols = cols ) imshow ( grid )
6659	def _bias_correction ( V_IJ , inbag , pred_centered , n_trees ) : n_train_samples = inbag . shape [ 0 ] n_var = np . mean ( np . square ( inbag [ 0 : n_trees ] ) . mean ( axis = 1 ) . T . view ( ) - np . square ( inbag [ 0 : n_trees ] . mean ( axis = 1 ) ) . T . view ( ) ) boot_var = np . square ( pred_centered ) . sum ( axis = 1 ) / n_trees bias_correction = n_train_samples * n_var * boot_var / n_trees V_IJ_unbiased = V_IJ - bias_correction return V_IJ_unbiased
448	def _bias_add ( x , b , data_format ) : if data_format == 'NHWC' : return tf . add ( x , b ) elif data_format == 'NCHW' : return tf . add ( x , _to_channel_first_bias ( b ) ) else : raise ValueError ( 'invalid data_format: %s' % data_format )
10257	def get_causal_sink_nodes ( graph : BELGraph , func ) -> Set [ BaseEntity ] : return { node for node in graph if node . function == func and is_causal_sink ( graph , node ) }
3636	def club ( self , sort = 'desc' , ctype = 'player' , defId = '' , start = 0 , count = None , page_size = itemsPerPage [ 'club' ] , level = None , category = None , assetId = None , league = None , club = None , position = None , zone = None , nationality = None , rare = False , playStyle = None ) : method = 'GET' url = 'club' if count : page_size = count params = { 'sort' : sort , 'type' : ctype , 'defId' : defId , 'start' : start , 'count' : page_size } if level : params [ 'level' ] = level if category : params [ 'cat' ] = category if assetId : params [ 'maskedDefId' ] = assetId if league : params [ 'leag' ] = league if club : params [ 'team' ] = club if position : params [ 'pos' ] = position if zone : params [ 'zone' ] = zone if nationality : params [ 'nat' ] = nationality if rare : params [ 'rare' ] = 'SP' if playStyle : params [ 'playStyle' ] = playStyle rc = self . __request__ ( method , url , params = params ) if start == 0 : if ctype == 'player' : pgid = 'Club - Players - List View' elif ctype == 'staff' : pgid = 'Club - Staff - List View' elif ctype in ( 'item' , 'kit' , 'ball' , 'badge' , 'stadium' ) : pgid = 'Club - Club Items - List View' events = [ self . pin . event ( 'page_view' , 'Hub - Club' ) , self . pin . event ( 'page_view' , pgid ) ] if rc [ 'itemData' ] : events . append ( self . pin . event ( 'page_view' , 'Item - Detail View' ) ) self . pin . send ( events ) return [ itemParse ( { 'itemData' : i } ) for i in rc [ 'itemData' ] ]
11347	def html_to_text ( cls , html ) : s = cls ( ) s . feed ( html ) unescaped_data = s . unescape ( s . get_data ( ) ) return escape_for_xml ( unescaped_data , tags_to_keep = s . mathml_elements )
10608	def _create_element_list ( self ) : element_set = stoich . elements ( self . compounds ) return sorted ( list ( element_set ) )
8679	def get ( self , key_name , decrypt = True ) : self . _assert_valid_stash ( ) key = self . _storage . get ( key_name ) . copy ( ) if not key . get ( 'value' ) : return None if decrypt : key [ 'value' ] = self . _decrypt ( key [ 'value' ] ) audit ( storage = self . _storage . db_path , action = 'GET' , message = json . dumps ( dict ( key_name = key_name ) ) ) return key
13516	def dimension ( self , length , draught , beam , speed , slenderness_coefficient , prismatic_coefficient ) : self . length = length self . draught = draught self . beam = beam self . speed = speed self . slenderness_coefficient = slenderness_coefficient self . prismatic_coefficient = prismatic_coefficient self . displacement = ( self . length / self . slenderness_coefficient ) ** 3 self . surface_area = 1.025 * ( 1.7 * self . length * self . draught + self . displacement / self . draught )
9101	def write_directory ( self , directory : str ) -> bool : current_md5_hash = self . get_namespace_hash ( ) md5_hash_path = os . path . join ( directory , f'{self.module_name}.belns.md5' ) if not os . path . exists ( md5_hash_path ) : old_md5_hash = None else : with open ( md5_hash_path ) as file : old_md5_hash = file . read ( ) . strip ( ) if old_md5_hash == current_md5_hash : return False with open ( os . path . join ( directory , f'{self.module_name}.belns' ) , 'w' ) as file : self . write_bel_namespace ( file , use_names = False ) with open ( md5_hash_path , 'w' ) as file : print ( current_md5_hash , file = file ) if self . has_names : with open ( os . path . join ( directory , f'{self.module_name}-names.belns' ) , 'w' ) as file : self . write_bel_namespace ( file , use_names = True ) with open ( os . path . join ( directory , f'{self.module_name}.belns.mapping' ) , 'w' ) as file : self . write_bel_namespace_mappings ( file , desc = 'writing mapping' ) return True
1040	def line ( self ) : line , column = self . source_buffer . decompose_position ( self . begin_pos ) return line
13554	def create_shift ( self , params = { } ) : url = "/2/shifts/" body = params data = self . _post_resource ( url , body ) shift = self . shift_from_json ( data [ "shift" ] ) return shift
1035	def encodestring ( s ) : pieces = [ ] for i in range ( 0 , len ( s ) , MAXBINSIZE ) : chunk = s [ i : i + MAXBINSIZE ] pieces . append ( binascii . b2a_base64 ( chunk ) ) return "" . join ( pieces )
8504	def get_key ( self ) : if not isinstance ( self . key , Unparseable ) : return self . key line = self . source [ self . col_offset : ] regex = re . compile ( ) match = regex . match ( line ) if not match : return Unparseable ( ) return "<%s>" % match . group ( 1 )
6100	def intensities_from_grid_radii ( self , grid_radii ) : return np . multiply ( np . divide ( self . intensity , self . sigma * np . sqrt ( 2.0 * np . pi ) ) , np . exp ( - 0.5 * np . square ( np . divide ( grid_radii , self . sigma ) ) ) )
5962	def parse ( self , stride = None ) : if stride is None : stride = self . stride self . corrupted_lineno = [ ] irow = 0 with utilities . openany ( self . real_filename ) as xvg : rows = [ ] ncol = None for lineno , line in enumerate ( xvg ) : line = line . strip ( ) if len ( line ) == 0 : continue if "label" in line and "xaxis" in line : self . xaxis = line . split ( '"' ) [ - 2 ] if "label" in line and "yaxis" in line : self . yaxis = line . split ( '"' ) [ - 2 ] if line . startswith ( "@ legend" ) : if not "legend" in self . metadata : self . metadata [ "legend" ] = [ ] self . metadata [ "legend" ] . append ( line . split ( "legend " ) [ - 1 ] ) if line . startswith ( "@ s" ) and "subtitle" not in line : name = line . split ( "legend " ) [ - 1 ] . replace ( '"' , '' ) . strip ( ) self . names . append ( name ) if line . startswith ( ( '#' , '@' ) ) : continue if line . startswith ( '&' ) : raise NotImplementedError ( '{0!s}: Multi-data not supported, only simple NXY format.' . format ( self . real_filename ) ) try : row = [ float ( el ) for el in line . split ( ) ] except : if self . permissive : self . logger . warn ( "%s: SKIPPING unparsable line %d: %r" , self . real_filename , lineno + 1 , line ) self . corrupted_lineno . append ( lineno + 1 ) continue self . logger . error ( "%s: Cannot parse line %d: %r" , self . real_filename , lineno + 1 , line ) raise if ncol is not None and len ( row ) != ncol : if self . permissive : self . logger . warn ( "%s: SKIPPING line %d with wrong number of columns: %r" , self . real_filename , lineno + 1 , line ) self . corrupted_lineno . append ( lineno + 1 ) continue errmsg = "{0!s}: Wrong number of columns in line {1:d}: {2!r}" . format ( self . real_filename , lineno + 1 , line ) self . logger . error ( errmsg ) raise IOError ( errno . ENODATA , errmsg , self . real_filename ) if irow % stride == 0 : ncol = len ( row ) rows . append ( row ) irow += 1 try : self . __array = numpy . array ( rows ) . transpose ( ) except : self . logger . error ( "%s: Failed reading XVG file, possibly data corrupted. " "Check the last line of the file..." , self . real_filename ) raise finally : del rows
11750	def _register_blueprint ( self , app , bp , bundle_path , child_path , description ) : base_path = sanitize_path ( self . _journey_path + bundle_path + child_path ) app . register_blueprint ( bp , url_prefix = base_path ) return { 'name' : bp . name , 'path' : child_path , 'import_name' : bp . import_name , 'description' : description , 'routes' : self . get_blueprint_routes ( app , base_path ) }
8017	async def websocket_disconnect ( self , message ) : self . closing = True await self . send_upstream ( message ) await super ( ) . websocket_disconnect ( message )
13484	def ghpages ( ) : opts = options docroot = path ( opts . get ( 'docroot' , 'docs' ) ) if not docroot . exists ( ) : raise BuildFailure ( "Sphinx documentation root (%s) does not exist." % docroot ) builddir = docroot / opts . get ( "builddir" , ".build" ) builddir = builddir / 'html' if not builddir . exists ( ) : raise BuildFailure ( "Sphinx build directory (%s) does not exist." % builddir ) nojekyll = path ( builddir ) / '.nojekyll' nojekyll . touch ( ) sh ( 'ghp-import -p %s' % ( builddir ) )
10635	def afr ( self ) : result = 0.0 for compound in self . material . compounds : result += self . get_compound_afr ( compound ) return result
981	def _initializeBucketMap ( self , maxBuckets , offset ) : self . _maxBuckets = maxBuckets self . minIndex = self . _maxBuckets / 2 self . maxIndex = self . _maxBuckets / 2 self . _offset = offset self . bucketMap = { } def _permutation ( n ) : r = numpy . arange ( n , dtype = numpy . uint32 ) self . random . shuffle ( r ) return r self . bucketMap [ self . minIndex ] = _permutation ( self . n ) [ 0 : self . w ] self . numTries = 0
3	def make_vec_env ( env_id , env_type , num_env , seed , wrapper_kwargs = None , start_index = 0 , reward_scale = 1.0 , flatten_dict_observations = True , gamestate = None ) : wrapper_kwargs = wrapper_kwargs or { } mpi_rank = MPI . COMM_WORLD . Get_rank ( ) if MPI else 0 seed = seed + 10000 * mpi_rank if seed is not None else None logger_dir = logger . get_dir ( ) def make_thunk ( rank ) : return lambda : make_env ( env_id = env_id , env_type = env_type , mpi_rank = mpi_rank , subrank = rank , seed = seed , reward_scale = reward_scale , gamestate = gamestate , flatten_dict_observations = flatten_dict_observations , wrapper_kwargs = wrapper_kwargs , logger_dir = logger_dir ) set_global_seeds ( seed ) if num_env > 1 : return SubprocVecEnv ( [ make_thunk ( i + start_index ) for i in range ( num_env ) ] ) else : return DummyVecEnv ( [ make_thunk ( start_index ) ] )
11192	def item ( proto_dataset_uri , input_file , relpath_in_dataset ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( proto_dataset_uri , config_path = CONFIG_PATH ) if relpath_in_dataset == "" : relpath_in_dataset = os . path . basename ( input_file ) proto_dataset . put_item ( input_file , relpath_in_dataset )
13852	def age ( self ) : if self . rounds == 1 : self . do_run = False elif self . rounds > 1 : self . rounds -= 1
10146	def from_path ( self , path ) : path_components = path . split ( '/' ) param_names = [ comp [ 1 : - 1 ] for comp in path_components if comp . startswith ( '{' ) and comp . endswith ( '}' ) ] params = [ ] for name in param_names : param_schema = colander . SchemaNode ( colander . String ( ) , name = name ) param = self . parameter_converter ( 'path' , param_schema ) if self . ref : param = self . _ref ( param ) params . append ( param ) return params
8789	def _pop ( self , model ) : tags = [ ] for tag in model . tags : if self . is_tag ( tag ) : tags . append ( tag ) if tags : for tag in tags : model . tags . remove ( tag ) return tags
11035	def _request ( self , endpoint , * args , ** kwargs ) : kwargs [ 'url' ] = endpoint return ( super ( MarathonLbClient , self ) . request ( * args , ** kwargs ) . addCallback ( raise_for_status ) )
3606	def get_async ( self , url , name , callback = None , params = None , headers = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) process_pool . apply_async ( make_get_request , args = ( endpoint , params , headers ) , callback = callback )
9656	def get_the_node_dict ( G , name ) : for node in G . nodes ( data = True ) : if node [ 0 ] == name : return node [ 1 ]
12428	def check_directories ( self ) : self . log . debug ( 'Checking directories' ) if not os . path . exists ( self . _ve_dir ) : os . makedirs ( self . _ve_dir ) if not os . path . exists ( self . _app_dir ) : os . makedirs ( self . _app_dir ) if not os . path . exists ( self . _conf_dir ) : os . makedirs ( self . _conf_dir ) if not os . path . exists ( self . _var_dir ) : os . makedirs ( self . _var_dir ) if not os . path . exists ( self . _log_dir ) : os . makedirs ( self . _log_dir ) if not os . path . exists ( self . _script_dir ) : os . makedirs ( self . _script_dir ) uwsgi_params = '/etc/nginx/uwsgi_params' if os . path . exists ( uwsgi_params ) : shutil . copy ( uwsgi_params , self . _conf_dir ) else : logging . warning ( 'Unable to find Nginx uwsgi_params. You must manually copy this to {0}.' . format ( self . _conf_dir ) ) mime_types = '/etc/nginx/mime.types' if os . path . exists ( mime_types ) : shutil . copy ( mime_types , self . _conf_dir ) self . _include_mimetypes = True else : logging . warn ( 'Unable to find mime.types for Nginx. You must manually copy this to {0}.' . format ( self . _conf_dir ) )
1972	def _interp_total_size ( interp ) : load_segs = [ x for x in interp . iter_segments ( ) if x . header . p_type == 'PT_LOAD' ] last = load_segs [ - 1 ] return last . header . p_vaddr + last . header . p_memsz
6305	def runnable_effects ( self ) -> List [ Type [ Effect ] ] : return [ cls for cls in self . effect_classes if cls . runnable ]
4552	def fill_triangle ( setter , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : a = b = y = last = 0 if y0 > y1 : y0 , y1 = y1 , y0 x0 , x1 = x1 , x0 if y1 > y2 : y2 , y1 = y1 , y2 x2 , x1 = x1 , x2 if y0 > y1 : y0 , y1 = y1 , y0 x0 , x1 = x1 , x0 if y0 == y2 : a = b = x0 if x1 < a : a = x1 elif x1 > b : b = x1 if x2 < a : a = x2 elif x2 > b : b = x2 _draw_fast_hline ( setter , a , y0 , b - a + 1 , color , aa ) dx01 = x1 - x0 dy01 = y1 - y0 dx02 = x2 - x0 dy02 = y2 - y0 dx12 = x2 - x1 dy12 = y2 - y1 sa = 0 sb = 0 if y1 == y2 : last = y1 else : last = y1 - 1 for y in range ( y , last + 1 ) : a = x0 + sa / dy01 b = x0 + sb / dy02 sa += dx01 sb += dx02 if a > b : a , b = b , a _draw_fast_hline ( setter , a , y , b - a + 1 , color , aa ) sa = dx12 * ( y - y1 ) sb = dx02 * ( y - y0 ) for y in range ( y , y2 + 1 ) : a = x1 + sa / dy12 b = x0 + sb / dy02 sa += dx12 sb += dx02 if a > b : a , b = b , a _draw_fast_hline ( setter , a , y , b - a + 1 , color , aa )
8295	def cliques ( graph , threshold = 3 ) : cliques = [ ] for n in graph . nodes : c = clique ( graph , n . id ) if len ( c ) >= threshold : c . sort ( ) if c not in cliques : cliques . append ( c ) return cliques
9099	def write_bel_annotation ( self , file : TextIO ) -> None : if not self . is_populated ( ) : self . populate ( ) values = self . _get_namespace_name_to_encoding ( desc = 'writing names' ) write_annotation ( keyword = self . _get_namespace_keyword ( ) , citation_name = self . _get_namespace_name ( ) , description = '' , values = values , file = file , )
12482	def find_in_sections ( var_name , app_name ) : sections = get_sections ( app_name ) if not sections : raise ValueError ( 'No sections found in {} rcfiles.' . format ( app_name ) ) for s in sections : try : var_value = get_rcfile_variable_value ( var_name , section_name = s , app_name = app_name ) except : pass else : return s , var_value raise KeyError ( 'No variable {} has been found in {} ' 'rcfiles.' . format ( var_name , app_name ) )
3801	def calculate ( self , T , method ) : r if method == SHEFFY_JOHNSON : kl = Sheffy_Johnson ( T , self . MW , self . Tm ) elif method == SATO_RIEDEL : kl = Sato_Riedel ( T , self . MW , self . Tb , self . Tc ) elif method == GHARAGHEIZI_L : kl = Gharagheizi_liquid ( T , self . MW , self . Tb , self . Pc , self . omega ) elif method == NICOLA : kl = Nicola ( T , self . MW , self . Tc , self . Pc , self . omega ) elif method == NICOLA_ORIGINAL : kl = Nicola_original ( T , self . MW , self . Tc , self . omega , self . Hfus ) elif method == LAKSHMI_PRASAD : kl = Lakshmi_Prasad ( T , self . MW ) elif method == BAHADORI_L : kl = Bahadori_liquid ( T , self . MW ) elif method == DIPPR_PERRY_8E : kl = EQ100 ( T , * self . Perrys2_315_coeffs ) elif method == VDI_PPDS : kl = horner ( self . VDI_PPDS_coeffs , T ) elif method == COOLPROP : kl = CoolProp_T_dependent_property ( T , self . CASRN , 'L' , 'l' ) elif method in self . tabular_data : kl = self . interpolate ( T , method ) return kl
4179	def window_bartlett_hann ( N ) : r if N == 1 : return ones ( 1 ) n = arange ( 0 , N ) a0 = 0.62 a1 = 0.48 a2 = 0.38 win = a0 - a1 * abs ( n / ( N - 1. ) - 0.5 ) - a2 * cos ( 2 * pi * n / ( N - 1. ) ) return win
4884	def handle_transmission_error ( self , learner_data , request_exception ) : try : sys_msg = request_exception . response . content except AttributeError : pass else : if 'user account is inactive' in sys_msg : ecu = EnterpriseCustomerUser . objects . get ( enterprise_enrollments__id = learner_data . enterprise_course_enrollment_id ) ecu . active = False ecu . save ( ) LOGGER . warning ( 'User %s with ID %s and email %s is a former employee of %s ' 'and has been marked inactive in SAPSF. Now marking inactive internally.' , ecu . username , ecu . user_id , ecu . user_email , ecu . enterprise_customer ) return super ( SapSuccessFactorsLearnerTransmitter , self ) . handle_transmission_error ( learner_data , request_exception )
10799	def _eval_firstorder ( self , rvecs , data , sigma ) : if not self . blocksize : dist_between_points = self . _distance_matrix ( rvecs , self . x ) gaussian_weights = self . _weight ( dist_between_points , sigma = sigma ) return gaussian_weights . dot ( data ) / gaussian_weights . sum ( axis = 1 ) else : ans = np . zeros ( rvecs . shape [ 0 ] , dtype = 'float' ) bs = self . blocksize for a in range ( 0 , rvecs . shape [ 0 ] , bs ) : dist = self . _distance_matrix ( rvecs [ a : a + bs ] , self . x ) weights = self . _weight ( dist , sigma = sigma ) ans [ a : a + bs ] += weights . dot ( data ) / weights . sum ( axis = 1 ) return ans
10416	def data_contains_key_builder ( key : str ) -> NodePredicate : def data_contains_key ( _ : BELGraph , node : BaseEntity ) -> bool : return key in node return data_contains_key
1034	def decode ( input , output ) : while True : line = input . readline ( ) if not line : break s = binascii . a2b_base64 ( line ) output . write ( s )
1239	def move ( self , external_index , new_priority ) : index = external_index + ( self . _capacity - 1 ) return self . _move ( index , new_priority )
12201	def from_yamlfile ( cls , fp , selector_handler = None , strict = False , debug = False ) : return cls . from_yamlstring ( fp . read ( ) , selector_handler = selector_handler , strict = strict , debug = debug )
10021	def get_environments ( self ) : response = self . ebs . describe_environments ( application_name = self . app_name , include_deleted = False ) return response [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ]
4769	def is_instance_of ( self , some_class ) : try : if not isinstance ( self . val , some_class ) : if hasattr ( self . val , '__name__' ) : t = self . val . __name__ elif hasattr ( self . val , '__class__' ) : t = self . val . __class__ . __name__ else : t = 'unknown' self . _err ( 'Expected <%s:%s> to be instance of class <%s>, but was not.' % ( self . val , t , some_class . __name__ ) ) except TypeError : raise TypeError ( 'given arg must be a class' ) return self
5454	def task_view_generator ( job_descriptor ) : for task_descriptor in job_descriptor . task_descriptors : jd = JobDescriptor ( job_descriptor . job_metadata , job_descriptor . job_params , job_descriptor . job_resources , [ task_descriptor ] ) yield jd
10995	def _barnes ( self , pos ) : b_in = self . b_in dist = lambda x : np . sqrt ( np . dot ( x , x ) ) sz = self . npts [ 1 ] coeffs = self . get_values ( self . barnes_params ) b = BarnesInterpolationND ( b_in , coeffs , filter_size = self . filtsize , damp = 0.9 , iterations = 3 , clip = self . local_updates , clipsize = self . barnes_clip_size , blocksize = 100 ) return b ( pos )
5601	def serve ( mapchete_file , port = None , internal_cache = None , zoom = None , bounds = None , overwrite = False , readonly = False , memory = False , input_file = None , debug = False , logfile = None ) : app = create_app ( mapchete_files = [ mapchete_file ] , zoom = zoom , bounds = bounds , single_input_file = input_file , mode = _get_mode ( memory , readonly , overwrite ) , debug = debug ) if os . environ . get ( "MAPCHETE_TEST" ) == "TRUE" : logger . debug ( "don't run flask app, MAPCHETE_TEST environment detected" ) else : app . run ( threaded = True , debug = True , port = port , host = '0.0.0.0' , extra_files = [ mapchete_file ] )
6949	def jhk_to_sdssr ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSR_JHK , SDSSR_JH , SDSSR_JK , SDSSR_HK , SDSSR_J , SDSSR_H , SDSSR_K )
7785	def error ( self , error_data ) : if not self . active : return if not self . _try_backup_item ( ) : self . _error_handler ( self . address , error_data ) self . cache . invalidate_object ( self . address ) self . _deactivate ( )
13683	def get ( self , url , params = { } ) : params . update ( { 'api_key' : self . api_key } ) try : response = requests . get ( self . host + url , params = params ) except RequestException as e : response = e . args return self . json_parse ( response . content )
12816	def _send_to_consumer ( self , block ) : self . _consumer . write ( block ) self . _sent += len ( block ) if self . _callback : self . _callback ( self . _sent , self . length )
7476	def inserted_indels ( indels , ocatg ) : newcatg = np . zeros ( ocatg . shape , dtype = np . uint32 ) for iloc in xrange ( ocatg . shape [ 0 ] ) : indidx = np . where ( indels [ iloc , : ] ) [ 0 ] if np . any ( indidx ) : allrows = np . arange ( ocatg . shape [ 1 ] ) mask = np . ones ( allrows . shape [ 0 ] , dtype = np . bool_ ) for idx in indidx : mask [ idx ] = False not_idx = allrows [ mask == 1 ] newcatg [ iloc ] [ not_idx ] = ocatg [ iloc , : not_idx . shape [ 0 ] ] else : newcatg [ iloc ] = ocatg [ iloc ] return newcatg
6828	def fetch ( self , path , use_sudo = False , user = None , remote = None ) : if path is None : raise ValueError ( "Path to the working copy is needed to fetch from a remote repository." ) if remote is not None : cmd = 'git fetch %s' % remote else : cmd = 'git fetch' with cd ( path ) : if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
8855	def setup_mnu_style ( self , editor ) : menu = QtWidgets . QMenu ( 'Styles' , self . menuEdit ) group = QtWidgets . QActionGroup ( self ) self . styles_group = group current_style = editor . syntax_highlighter . color_scheme . name group . triggered . connect ( self . on_style_changed ) for s in sorted ( PYGMENTS_STYLES ) : a = QtWidgets . QAction ( menu ) a . setText ( s ) a . setCheckable ( True ) if s == current_style : a . setChecked ( True ) group . addAction ( a ) menu . addAction ( a ) self . menuEdit . addMenu ( menu )
5335	def get_params ( ) : parser = get_params_parser ( ) args = parser . parse_args ( ) if not args . raw and not args . enrich and not args . identities and not args . panels : print ( "No tasks enabled" ) sys . exit ( 1 ) return args
12213	def update_field_from_proxy ( field_obj , pref_proxy ) : attr_names = ( 'verbose_name' , 'help_text' , 'default' ) for attr_name in attr_names : setattr ( field_obj , attr_name , getattr ( pref_proxy , attr_name ) )
3469	def copy ( self ) : model = self . _model self . _model = None for i in self . _metabolites : i . _model = None for i in self . _genes : i . _model = None new_reaction = deepcopy ( self ) self . _model = model for i in self . _metabolites : i . _model = model for i in self . _genes : i . _model = model return new_reaction
3409	def knock_out ( self ) : self . functional = False for reaction in self . reactions : if not reaction . functional : reaction . bounds = ( 0 , 0 )
11891	def set_all ( self , red , green , blue , brightness ) : command = "C {},{},{},{},{},\r\n" . format ( self . _zid , red , green , blue , brightness ) response = self . _hub . send_command ( command ) _LOGGER . debug ( "Set all %s: %s" , repr ( command ) , response ) return response
3274	def get_user_info ( self ) : if self . value in ERROR_DESCRIPTIONS : s = "{}" . format ( ERROR_DESCRIPTIONS [ self . value ] ) else : s = "{}" . format ( self . value ) if self . context_info : s += ": {}" . format ( self . context_info ) elif self . value in ERROR_RESPONSES : s += ": {}" . format ( ERROR_RESPONSES [ self . value ] ) if self . src_exception : s += "\n Source exception: '{}'" . format ( self . src_exception ) if self . err_condition : s += "\n Error condition: '{}'" . format ( self . err_condition ) return s
4464	def load_jam_audio ( jam_in , audio_file , validate = True , strict = True , fmt = 'auto' , ** kwargs ) : if isinstance ( jam_in , jams . JAMS ) : jam = jam_in else : jam = jams . load ( jam_in , validate = validate , strict = strict , fmt = fmt ) y , sr = librosa . load ( audio_file , ** kwargs ) if jam . file_metadata . duration is None : jam . file_metadata . duration = librosa . get_duration ( y = y , sr = sr ) return jam_pack ( jam , _audio = dict ( y = y , sr = sr ) )
200	def draw_on_image ( self , image , alpha = 0.75 , resize = "segmentation_map" , background_threshold = 0.01 , background_class_id = None , colors = None , draw_background = False ) : ia . do_assert ( image . ndim == 3 ) ia . do_assert ( image . shape [ 2 ] == 3 ) ia . do_assert ( image . dtype . type == np . uint8 ) ia . do_assert ( 0 - 1e-8 <= alpha <= 1.0 + 1e-8 ) ia . do_assert ( resize in [ "segmentation_map" , "image" ] ) if resize == "image" : image = ia . imresize_single_image ( image , self . arr . shape [ 0 : 2 ] , interpolation = "cubic" ) segmap_drawn , foreground_mask = self . draw ( background_threshold = background_threshold , background_class_id = background_class_id , size = image . shape [ 0 : 2 ] if resize == "segmentation_map" else None , colors = colors , return_foreground_mask = True ) if draw_background : mix = np . clip ( ( 1 - alpha ) * image + alpha * segmap_drawn , 0 , 255 ) . astype ( np . uint8 ) else : foreground_mask = foreground_mask [ ... , np . newaxis ] mix = np . zeros_like ( image ) mix += ( ~ foreground_mask ) . astype ( np . uint8 ) * image mix += foreground_mask . astype ( np . uint8 ) * np . clip ( ( 1 - alpha ) * image + alpha * segmap_drawn , 0 , 255 ) . astype ( np . uint8 ) return mix
10707	def create_vacation ( body ) : arequest = requests . post ( VACATIONS_URL , headers = HEADERS , data = json . dumps ( body ) ) status_code = str ( arequest . status_code ) if status_code != '200' : _LOGGER . error ( "Failed to create vacation. " + status_code ) _LOGGER . error ( arequest . json ( ) ) return False return arequest . json ( )
6739	def check_settings_for_differences ( old , new , as_bool = False , as_tri = False ) : assert not as_bool or not as_tri old = old or { } new = new or { } changes = set ( k for k in set ( new . iterkeys ( ) ) . intersection ( old . iterkeys ( ) ) if new [ k ] != old [ k ] ) if changes and as_bool : return True added_keys = set ( new . iterkeys ( ) ) . difference ( old . iterkeys ( ) ) if added_keys and as_bool : return True if not as_tri : changes . update ( added_keys ) deled_keys = set ( old . iterkeys ( ) ) . difference ( new . iterkeys ( ) ) if deled_keys and as_bool : return True if as_bool : return False if not as_tri : changes . update ( deled_keys ) if as_tri : return added_keys , changes , deled_keys return changes
8693	def init ( self ) : try : self . client . create_bucket ( Bucket = self . db_path , CreateBucketConfiguration = self . bucket_configuration ) except botocore . exceptions . ClientError as e : if 'BucketAlreadyOwnedByYou' not in str ( e . response [ 'Error' ] [ 'Code' ] ) : raise e
10496	def leftMouseDragged ( self , stopCoord , strCoord = ( 0 , 0 ) , speed = 1 ) : self . _leftMouseDragged ( stopCoord , strCoord , speed )
500	def _deleteRangeFromKNN ( self , start = 0 , end = None ) : prototype_idx = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) if end is None : end = prototype_idx . max ( ) + 1 idsIdxToDelete = numpy . logical_and ( prototype_idx >= start , prototype_idx < end ) idsToDelete = prototype_idx [ idsIdxToDelete ] nProtos = self . _knnclassifier . _knn . _numPatterns self . _knnclassifier . _knn . removeIds ( idsToDelete . tolist ( ) ) assert self . _knnclassifier . _knn . _numPatterns == nProtos - len ( idsToDelete )
7679	def event ( annotation , ** kwargs ) : times , values = annotation . to_interval_values ( ) if any ( values ) : labels = values else : labels = None return mir_eval . display . events ( times , labels = labels , ** kwargs )
126	def Negative ( other_param , mode = "invert" , reroll_count_max = 2 ) : return ForceSign ( other_param = other_param , positive = False , mode = mode , reroll_count_max = reroll_count_max )
12555	def sav_to_pandas_savreader ( input_file ) : from savReaderWriter import SavReader lines = [ ] with SavReader ( input_file , returnHeader = True ) as reader : header = next ( reader ) for line in reader : lines . append ( line ) return pd . DataFrame ( data = lines , columns = header )
7097	def on_map_fragment_created ( self , obj_id ) : self . fragment = MapFragment ( __id__ = obj_id ) self . map . onMapReady . connect ( self . on_map_ready ) self . fragment . getMapAsync ( self . map . getId ( ) ) context = self . get_context ( ) def on_transaction ( id ) : trans = FragmentTransaction ( __id__ = id ) trans . add ( self . widget . getId ( ) , self . fragment ) trans . commit ( ) def on_fragment_manager ( id ) : fm = FragmentManager ( __id__ = id ) fm . beginTransaction ( ) . then ( on_transaction ) context . widget . getSupportFragmentManager ( ) . then ( on_fragment_manager )
12019	def find_imports ( self , pbds ) : imports = list ( set ( self . uses ) . difference ( set ( self . defines ) ) ) for imp in imports : for p in pbds : if imp in p . defines : self . imports . append ( p . name ) break self . imports = list ( set ( self . imports ) ) for import_file in self . imports : self . lines . insert ( 2 , 'import "{}";' . format ( import_file ) )
243	def daily_txns_with_bar_data ( transactions , market_data ) : transactions . index . name = 'date' txn_daily = pd . DataFrame ( transactions . assign ( amount = abs ( transactions . amount ) ) . groupby ( [ 'symbol' , pd . TimeGrouper ( 'D' ) ] ) . sum ( ) [ 'amount' ] ) txn_daily [ 'price' ] = market_data [ 'price' ] . unstack ( ) txn_daily [ 'volume' ] = market_data [ 'volume' ] . unstack ( ) txn_daily = txn_daily . reset_index ( ) . set_index ( 'date' ) return txn_daily
9588	def _execute ( self , command , data = None , unpack = True ) : if not data : data = { } if self . session_id is not None : data . setdefault ( 'session_id' , self . session_id ) data = self . _wrap_el ( data ) res = self . remote_invoker . execute ( command , data ) ret = WebDriverResult . from_object ( res ) ret . raise_for_status ( ) ret . value = self . _unwrap_el ( ret . value ) if not unpack : return ret return ret . value
888	def _destroyMinPermanenceSynapses ( cls , connections , random , segment , nDestroy , excludeCells ) : destroyCandidates = sorted ( ( synapse for synapse in connections . synapsesForSegment ( segment ) if synapse . presynapticCell not in excludeCells ) , key = lambda s : s . _ordinal ) for _ in xrange ( nDestroy ) : if len ( destroyCandidates ) == 0 : break minSynapse = None minPermanence = float ( "inf" ) for synapse in destroyCandidates : if synapse . permanence < minPermanence - EPSILON : minSynapse = synapse minPermanence = synapse . permanence connections . destroySynapse ( minSynapse ) destroyCandidates . remove ( minSynapse )
9772	def restart ( ctx , copy , file , u ) : config = None update_code = None if file : config = rhea . read ( file ) if u : ctx . invoke ( upload , sync = False ) update_code = True user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) try : if copy : response = PolyaxonClient ( ) . job . copy ( user , project_name , _job , config = config , update_code = update_code ) else : response = PolyaxonClient ( ) . job . restart ( user , project_name , _job , config = config , update_code = update_code ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not restart job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_job_details ( response )
9886	def _call_multi_fortran_z ( self , names , data_types , rec_nums , dim_sizes , input_type_code , func , epoch = False , data_offset = None , epoch16 = False ) : idx , = np . where ( data_types == input_type_code ) if len ( idx ) > 0 : max_rec = rec_nums [ idx ] . max ( ) sub_names = np . array ( names ) [ idx ] sub_sizes = dim_sizes [ idx ] status , data = func ( self . fname , sub_names . tolist ( ) , sub_sizes , sub_sizes . sum ( ) , max_rec , len ( sub_names ) ) if status == 0 : if data_offset is not None : data = data . astype ( int ) idx , idy , = np . where ( data < 0 ) data [ idx , idy ] += data_offset if epoch : data -= 62167219200000 data = data . astype ( '<M8[ms]' ) if epoch16 : data [ 0 : : 2 , : ] -= 62167219200 data = data [ 0 : : 2 , : ] * 1E9 + data [ 1 : : 2 , : ] / 1.E3 data = data . astype ( 'datetime64[ns]' ) sub_sizes /= 2 self . _process_return_multi_z ( data , sub_names , sub_sizes ) else : raise IOError ( fortran_cdf . statusreporter ( status ) )
6672	def is_dir ( self , path , use_sudo = False ) : if self . is_local and not use_sudo : return os . path . isdir ( path ) else : func = use_sudo and _sudo or _run with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : return func ( '[ -d "%(path)s" ]' % locals ( ) ) . succeeded
6916	def add_variability_to_fakelc_collection ( simbasedir , override_paramdists = None , overwrite_existingvar = False ) : infof = os . path . join ( simbasedir , 'fakelcs-info.pkl' ) with open ( infof , 'rb' ) as infd : lcinfo = pickle . load ( infd ) lclist = lcinfo [ 'lcfpath' ] varflag = lcinfo [ 'isvariable' ] vartypes = lcinfo [ 'vartype' ] vartind = 0 varinfo = { } for lc , varf , _lcind in zip ( lclist , varflag , range ( len ( lclist ) ) ) : if varf : thisvartype = vartypes [ vartind ] if ( override_paramdists and isinstance ( override_paramdists , dict ) and thisvartype in override_paramdists and isinstance ( override_paramdists [ thisvartype ] , dict ) ) : thisoverride_paramdists = override_paramdists [ thisvartype ] else : thisoverride_paramdists = None varlc = add_fakelc_variability ( lc , thisvartype , override_paramdists = thisoverride_paramdists , overwrite = overwrite_existingvar ) varinfo [ varlc [ 'objectid' ] ] = { 'params' : varlc [ 'actual_varparams' ] , 'vartype' : varlc [ 'actual_vartype' ] } vartind = vartind + 1 else : varlc = add_fakelc_variability ( lc , None , overwrite = overwrite_existingvar ) varinfo [ varlc [ 'objectid' ] ] = { 'params' : varlc [ 'actual_varparams' ] , 'vartype' : varlc [ 'actual_vartype' ] } lcinfo [ 'varinfo' ] = varinfo tempoutf = '%s.%s' % ( infof , md5 ( npr . bytes ( 4 ) ) . hexdigest ( ) [ - 8 : ] ) with open ( tempoutf , 'wb' ) as outfd : pickle . dump ( lcinfo , outfd , pickle . HIGHEST_PROTOCOL ) if os . path . exists ( tempoutf ) : shutil . copy ( tempoutf , infof ) os . remove ( tempoutf ) else : LOGEXCEPTION ( 'could not write output light curve file to dir: %s' % os . path . dirname ( tempoutf ) ) raise return lcinfo
2749	def get_droplet ( self , droplet_id ) : return Droplet . get_object ( api_token = self . token , droplet_id = droplet_id )
1447	def offer ( self , item ) : try : self . _buffer . put ( item , block = False ) if self . _consumer_callback is not None : self . _consumer_callback ( ) return True except Queue . Full : Log . debug ( "%s: Full in offer()" % str ( self ) ) raise Queue . Full
7325	def cli ( sample , dry_run , limit , no_limit , database_filename , template_filename , config_filename ) : mailmerge . api . main ( sample = sample , dry_run = dry_run , limit = limit , no_limit = no_limit , database_filename = database_filename , template_filename = template_filename , config_filename = config_filename , )
3854	def add_color_to_scheme ( scheme , name , foreground , background , palette_colors ) : if foreground is None and background is None : return scheme new_scheme = [ ] for item in scheme : if item [ 0 ] == name : if foreground is None : foreground = item [ 1 ] if background is None : background = item [ 2 ] if palette_colors > 16 : new_scheme . append ( ( name , '' , '' , '' , foreground , background ) ) else : new_scheme . append ( ( name , foreground , background ) ) else : new_scheme . append ( item ) return new_scheme
9528	def pbkdf2 ( password , salt , iterations , dklen = 0 , digest = None ) : if digest is None : digest = settings . CRYPTOGRAPHY_DIGEST if not dklen : dklen = digest . digest_size password = force_bytes ( password ) salt = force_bytes ( salt ) kdf = PBKDF2HMAC ( algorithm = digest , length = dklen , salt = salt , iterations = iterations , backend = settings . CRYPTOGRAPHY_BACKEND ) return kdf . derive ( password )
635	def computeActivity ( self , activePresynapticCells , connectedPermanence ) : numActiveConnectedSynapsesForSegment = [ 0 ] * self . _nextFlatIdx numActivePotentialSynapsesForSegment = [ 0 ] * self . _nextFlatIdx threshold = connectedPermanence - EPSILON for cell in activePresynapticCells : for synapse in self . _synapsesForPresynapticCell [ cell ] : flatIdx = synapse . segment . flatIdx numActivePotentialSynapsesForSegment [ flatIdx ] += 1 if synapse . permanence > threshold : numActiveConnectedSynapsesForSegment [ flatIdx ] += 1 return ( numActiveConnectedSynapsesForSegment , numActivePotentialSynapsesForSegment )
3712	def calculate_P ( self , T , P , method ) : r if method == EOS : self . eos [ 0 ] = self . eos [ 0 ] . to_TP ( T = T , P = P ) Vm = self . eos [ 0 ] . V_g elif method == TSONOPOULOS_EXTENDED : B = BVirial_Tsonopoulos_extended ( T , self . Tc , self . Pc , self . omega , dipole = self . dipole ) Vm = ideal_gas ( T , P ) + B elif method == TSONOPOULOS : B = BVirial_Tsonopoulos ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == ABBOTT : B = BVirial_Abbott ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == PITZER_CURL : B = BVirial_Pitzer_Curl ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == CRC_VIRIAL : a1 , a2 , a3 , a4 , a5 = self . CRC_VIRIAL_coeffs t = 298.15 / T - 1. B = ( a1 + a2 * t + a3 * t ** 2 + a4 * t ** 3 + a5 * t ** 4 ) / 1E6 Vm = ideal_gas ( T , P ) + B elif method == IDEAL : Vm = ideal_gas ( T , P ) elif method == COOLPROP : Vm = 1. / PropsSI ( 'DMOLAR' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : Vm = self . interpolate_P ( T , P , method ) return Vm
10280	def bond_task ( perc_graph_result , seeds , ps , convolution_factors_tasks_iterator ) : convolution_factors_tasks = list ( convolution_factors_tasks_iterator ) return reduce ( percolate . hpc . bond_reduce , map ( bond_run , itertools . repeat ( perc_graph_result ) , seeds , itertools . repeat ( ps ) , itertools . repeat ( convolution_factors_tasks ) , ) )
1561	def get_sources ( self , component_id ) : StreamId = namedtuple ( 'StreamId' , 'id, component_name' ) if component_id in self . inputs : ret = { } for istream in self . inputs . get ( component_id ) : key = StreamId ( id = istream . stream . id , component_name = istream . stream . component_name ) ret [ key ] = istream . gtype return ret else : return None
4299	def create_project ( config_data ) : env = deepcopy ( dict ( os . environ ) ) env [ str ( 'DJANGO_SETTINGS_MODULE' ) ] = str ( '{0}.settings' . format ( config_data . project_name ) ) env [ str ( 'PYTHONPATH' ) ] = str ( os . pathsep . join ( map ( shlex_quote , sys . path ) ) ) kwargs = { } args = [ ] if config_data . template : kwargs [ 'template' ] = config_data . template args . append ( config_data . project_name ) if config_data . project_directory : args . append ( config_data . project_directory ) if not os . path . exists ( config_data . project_directory ) : os . makedirs ( config_data . project_directory ) base_cmd = 'django-admin.py' start_cmds = [ os . path . join ( os . path . dirname ( sys . executable ) , base_cmd ) ] start_cmd_pnodes = [ 'Scripts' ] start_cmds . extend ( [ os . path . join ( os . path . dirname ( sys . executable ) , pnode , base_cmd ) for pnode in start_cmd_pnodes ] ) start_cmd = [ base_cmd ] for p in start_cmds : if os . path . exists ( p ) : start_cmd = [ sys . executable , p ] break cmd_args = start_cmd + [ 'startproject' ] + args if config_data . verbose : sys . stdout . write ( 'Project creation command: {0}\n' . format ( ' ' . join ( cmd_args ) ) ) try : output = subprocess . check_output ( cmd_args , stderr = subprocess . STDOUT ) sys . stdout . write ( output . decode ( 'utf-8' ) ) except subprocess . CalledProcessError as e : if config_data . verbose : sys . stdout . write ( e . output . decode ( 'utf-8' ) ) raise
139	def to_bounding_box ( self ) : from imgaug . augmentables . bbs import BoundingBox xx = self . xx yy = self . yy return BoundingBox ( x1 = min ( xx ) , x2 = max ( xx ) , y1 = min ( yy ) , y2 = max ( yy ) , label = self . label )
3436	def optimize ( self , objective_sense = None , raise_error = False ) : original_direction = self . objective . direction self . objective . direction = { "maximize" : "max" , "minimize" : "min" } . get ( objective_sense , original_direction ) self . slim_optimize ( ) solution = get_solution ( self , raise_error = raise_error ) self . objective . direction = original_direction return solution
10439	def stopprocessmonitor ( self , process_name ) : if process_name in self . _process_stats : self . _process_stats [ process_name ] . stop ( ) return 1
9932	def get_refkey ( self , obj , referent ) : if isinstance ( obj , dict ) : for k , v in obj . items ( ) : if v is referent : return " (via its %r key)" % k for k in dir ( obj ) + [ '__dict__' ] : if getattr ( obj , k , None ) is referent : return " (via its %r attribute)" % k return ""
6548	def move_to ( self , ypos , xpos ) : xpos -= 1 ypos -= 1 self . exec_command ( "MoveCursor({0}, {1})" . format ( ypos , xpos ) . encode ( "ascii" ) )
6288	def find_commands ( command_dir : str ) -> List [ str ] : if not command_dir : return [ ] return [ name for _ , name , is_pkg in pkgutil . iter_modules ( [ command_dir ] ) if not is_pkg and not name . startswith ( '_' ) ]
8967	def step ( self , key , chain ) : if chain == "sending" : self . __previous_sending_chain_length = self . sending_chain_length self . __sending_chain = self . __SendingChain ( key ) if chain == "receiving" : self . __receiving_chain = self . __ReceivingChain ( key )
424	def run_top_task ( self , task_name = None , sort = None , ** kwargs ) : if not isinstance ( task_name , str ) : raise Exception ( "task_name should be string" ) self . _fill_project_info ( kwargs ) kwargs . update ( { 'status' : 'pending' } ) task = self . db . Task . find_one_and_update ( kwargs , { '$set' : { 'status' : 'running' } } , sort = sort ) try : if task is None : logging . info ( "[Database] Find Task FAIL: key: {} sort: {}" . format ( task_name , sort ) ) return False else : logging . info ( "[Database] Find Task SUCCESS: key: {} sort: {}" . format ( task_name , sort ) ) _datetime = task [ 'time' ] _script = task [ 'script' ] _id = task [ '_id' ] _hyper_parameters = task [ 'hyper_parameters' ] _saved_result_keys = task [ 'saved_result_keys' ] logging . info ( " hyper parameters:" ) for key in _hyper_parameters : globals ( ) [ key ] = _hyper_parameters [ key ] logging . info ( " {}: {}" . format ( key , _hyper_parameters [ key ] ) ) s = time . time ( ) logging . info ( "[Database] Start Task: key: {} sort: {} push time: {}" . format ( task_name , sort , _datetime ) ) _script = _script . decode ( 'utf-8' ) with tf . Graph ( ) . as_default ( ) : exec ( _script , globals ( ) ) _ = self . db . Task . find_one_and_update ( { '_id' : _id } , { '$set' : { 'status' : 'finished' } } ) __result = { } for _key in _saved_result_keys : logging . info ( " result: {}={} {}" . format ( _key , globals ( ) [ _key ] , type ( globals ( ) [ _key ] ) ) ) __result . update ( { "%s" % _key : globals ( ) [ _key ] } ) _ = self . db . Task . find_one_and_update ( { '_id' : _id } , { '$set' : { 'result' : __result } } , return_document = pymongo . ReturnDocument . AFTER ) logging . info ( "[Database] Finished Task: task_name - {} sort: {} push time: {} took: {}s" . format ( task_name , sort , _datetime , time . time ( ) - s ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) logging . info ( "[Database] Fail to run task" ) _ = self . db . Task . find_one_and_update ( { '_id' : _id } , { '$set' : { 'status' : 'pending' } } ) return False
3088	def _delete_entity ( self ) : if self . _is_ndb ( ) : _NDB_KEY ( self . _model , self . _key_name ) . delete ( ) else : entity_key = db . Key . from_path ( self . _model . kind ( ) , self . _key_name ) db . delete ( entity_key )
10610	def _calculate_H ( self , T ) : if self . isCoal : return self . _calculate_Hfr_coal ( T ) H = 0.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) dH = thermo . H ( compound , T , self . _compound_masses [ index ] ) H = H + dH return H
2340	def GNN_instance ( x , idx = 0 , device = None , nh = 20 , ** kwargs ) : device = SETTINGS . get_default ( device = device ) xy = scale ( x ) . astype ( 'float32' ) inputx = th . FloatTensor ( xy [ : , [ 0 ] ] ) . to ( device ) target = th . FloatTensor ( xy [ : , [ 1 ] ] ) . to ( device ) GNNXY = GNN_model ( x . shape [ 0 ] , device = device , nh = nh ) . to ( device ) GNNYX = GNN_model ( x . shape [ 0 ] , device = device , nh = nh ) . to ( device ) GNNXY . reset_parameters ( ) GNNYX . reset_parameters ( ) XY = GNNXY . run ( inputx , target , ** kwargs ) YX = GNNYX . run ( target , inputx , ** kwargs ) return [ XY , YX ]
12001	def _unsign_data ( self , data , options ) : if options [ 'signature_algorithm_id' ] not in self . signature_algorithms : raise Exception ( 'Unknown signature algorithm id: %d' % options [ 'signature_algorithm_id' ] ) signature_algorithm = self . signature_algorithms [ options [ 'signature_algorithm_id' ] ] algorithm = self . _get_algorithm_info ( signature_algorithm ) key_salt = '' if algorithm [ 'salt_size' ] : key_salt = data [ - algorithm [ 'salt_size' ] : ] data = data [ : - algorithm [ 'salt_size' ] ] key = self . _generate_key ( options [ 'signature_passphrase_id' ] , self . signature_passphrases , key_salt , algorithm ) data = self . _decode ( data , algorithm , key ) return data
1590	def _get_dict_from_config ( topology_config ) : config = { } for kv in topology_config . kvs : if kv . HasField ( "value" ) : assert kv . type == topology_pb2 . ConfigValueType . Value ( "STRING_VALUE" ) if PhysicalPlanHelper . _is_number ( kv . value ) : config [ kv . key ] = PhysicalPlanHelper . _get_number ( kv . value ) elif kv . value . lower ( ) in ( "true" , "false" ) : config [ kv . key ] = True if kv . value . lower ( ) == "true" else False else : config [ kv . key ] = kv . value elif kv . HasField ( "serialized_value" ) and kv . type == topology_pb2 . ConfigValueType . Value ( "PYTHON_SERIALIZED_VALUE" ) : config [ kv . key ] = default_serializer . deserialize ( kv . serialized_value ) else : assert kv . HasField ( "type" ) Log . error ( "Unsupported config <key:value> found: %s, with type: %s" % ( str ( kv ) , str ( kv . type ) ) ) continue return config
5539	def read ( self , ** kwargs ) : if self . tile . pixelbuffer > self . config . output . pixelbuffer : output_tiles = list ( self . config . output_pyramid . tiles_from_bounds ( self . tile . bounds , self . tile . zoom ) ) else : output_tiles = self . config . output_pyramid . intersecting ( self . tile ) return self . config . output . extract_subset ( input_data_tiles = [ ( output_tile , self . config . output . read ( output_tile ) ) for output_tile in output_tiles ] , out_tile = self . tile , )
1666	def CheckRedundantOverrideOrFinal ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] declarator_end = line . rfind ( ')' ) if declarator_end >= 0 : fragment = line [ declarator_end : ] else : if linenum > 1 and clean_lines . elided [ linenum - 1 ] . rfind ( ')' ) >= 0 : fragment = line else : return if Search ( r'\boverride\b' , fragment ) and Search ( r'\bfinal\b' , fragment ) : error ( filename , linenum , 'readability/inheritance' , 4 , ( '"override" is redundant since function is ' 'already declared as "final"' ) )
13488	def create ( self , server ) : for chunk in self . __cut_to_size ( ) : server . post ( 'tasks_admin' , chunk . as_payload ( ) , replacements = { 'slug' : chunk . challenge . slug } )
1318	def SetWindowText ( self , text : str ) -> bool : handle = self . NativeWindowHandle if handle : return SetWindowText ( handle , text ) return False
1511	def start_heron_tools ( masters , cl_args ) : single_master = list ( masters ) [ 0 ] wait_for_master_to_start ( single_master ) cmd = "%s run %s >> /tmp/heron_tools_start.log 2>&1 &" % ( get_nomad_path ( cl_args ) , get_heron_tools_job_file ( cl_args ) ) Log . info ( "Starting Heron Tools on %s" % single_master ) if not is_self ( single_master ) : cmd = ssh_remote_execute ( cmd , single_master , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) if return_code != 0 : Log . error ( "Failed to start Heron Tools on %s with error:\n%s" % ( single_master , output [ 1 ] ) ) sys . exit ( - 1 ) wait_for_job_to_start ( single_master , "heron-tools" ) Log . info ( "Done starting Heron Tools" )
1906	def strlen ( state , s ) : cpu = state . cpu if issymbolic ( s ) : raise ConcretizeArgument ( state . cpu , 1 ) zero_idx = _find_zero ( cpu , state . constraints , s ) ret = zero_idx for offset in range ( zero_idx - 1 , - 1 , - 1 ) : byt = cpu . read_int ( s + offset , 8 ) if issymbolic ( byt ) : ret = ITEBV ( cpu . address_bit_size , byt == 0 , offset , ret ) return ret
7406	def bottom ( self ) : o = self . get_ordering_queryset ( ) . aggregate ( Max ( 'order' ) ) . get ( 'order__max' ) self . to ( o )
518	def _raisePermanenceToThreshold ( self , perm , mask ) : if len ( mask ) < self . _stimulusThreshold : raise Exception ( "This is likely due to a " + "value of stimulusThreshold that is too large relative " + "to the input size. [len(mask) < self._stimulusThreshold]" ) numpy . clip ( perm , self . _synPermMin , self . _synPermMax , out = perm ) while True : numConnected = numpy . nonzero ( perm > self . _synPermConnected - PERMANENCE_EPSILON ) [ 0 ] . size if numConnected >= self . _stimulusThreshold : return perm [ mask ] += self . _synPermBelowStimulusInc
13727	def set_delegate ( address = None , pubkey = None , secret = None ) : c . DELEGATE [ 'ADDRESS' ] = address c . DELEGATE [ 'PUBKEY' ] = pubkey c . DELEGATE [ 'PASSPHRASE' ] = secret
6607	def poll ( self ) : clusterids = clusterprocids2clusterids ( self . clusterprocids_outstanding ) clusterprocid_status_list = query_status_for ( clusterids ) if clusterprocid_status_list : clusterprocids , statuses = zip ( * clusterprocid_status_list ) else : clusterprocids , statuses = ( ) , ( ) clusterprocids_finished = [ i for i in self . clusterprocids_outstanding if i not in clusterprocids ] self . clusterprocids_finished . extend ( clusterprocids_finished ) self . clusterprocids_outstanding [ : ] = clusterprocids counter = collections . Counter ( statuses ) messages = [ ] if counter : messages . append ( ', ' . join ( [ '{}: {}' . format ( HTCONDOR_JOBSTATUS [ k ] , counter [ k ] ) for k in counter . keys ( ) ] ) ) if self . clusterprocids_finished : messages . append ( 'Finished {}' . format ( len ( self . clusterprocids_finished ) ) ) logger = logging . getLogger ( __name__ ) logger . info ( ', ' . join ( messages ) ) return clusterprocids_finished
10563	def compare_song_collections ( src_songs , dst_songs ) : def gather_field_values ( song ) : return tuple ( ( _normalize_metadata ( song [ field ] ) for field in _filter_comparison_fields ( song ) ) ) dst_songs_criteria = { gather_field_values ( _normalize_song ( dst_song ) ) for dst_song in dst_songs } return [ src_song for src_song in src_songs if gather_field_values ( _normalize_song ( src_song ) ) not in dst_songs_criteria ]
6505	def add_properties ( self ) : for property_name in [ p [ 0 ] for p in inspect . getmembers ( self . __class__ ) if isinstance ( p [ 1 ] , property ) ] : self . _results_fields [ property_name ] = getattr ( self , property_name , None )
12361	def send_request ( self , kind , url_components , ** kwargs ) : return self . api . send_request ( kind , self . resource_path , url_components , ** kwargs )
12338	def stitch_macro ( path , output_folder = None ) : output_folder = output_folder or path debug ( 'stitching ' + path + ' to ' + output_folder ) fields = glob ( _pattern ( path , _field ) ) xs = [ attribute ( field , 'X' ) for field in fields ] ys = [ attribute ( field , 'Y' ) for field in fields ] x_min , x_max = min ( xs ) , max ( xs ) y_min , y_max = min ( ys ) , max ( ys ) fields_column = len ( set ( xs ) ) fields_row = len ( set ( ys ) ) images = glob ( _pattern ( fields [ 0 ] , _image ) ) attr = attributes ( images [ 0 ] ) channels = [ ] z_stacks = [ ] for image in images : channel = attribute_as_str ( image , 'C' ) if channel not in channels : channels . append ( channel ) z = attribute_as_str ( image , 'Z' ) if z not in z_stacks : z_stacks . append ( z ) debug ( 'channels ' + str ( channels ) ) debug ( 'z-stacks ' + str ( z_stacks ) ) _ , extension = os . path . splitext ( images [ - 1 ] ) if extension == '.tif' : extension = '.ome.tif' macros = [ ] output_files = [ ] for Z in z_stacks : for C in channels : filenames = os . path . join ( _field + '--X{xx}--Y{yy}' , _image + '--L' + attr . L + '--S' + attr . S + '--U' + attr . U + '--V' + attr . V + '--J' + attr . J + '--E' + attr . E + '--O' + attr . O + '--X{xx}--Y{yy}' + '--T' + attr . T + '--Z' + Z + '--C' + C + extension ) debug ( 'filenames ' + filenames ) cur_attr = attributes ( filenames ) . _asdict ( ) f = 'stitched--U{U}--V{V}--C{C}--Z{Z}.png' . format ( ** cur_attr ) output = os . path . join ( output_folder , f ) debug ( 'output ' + output ) output_files . append ( output ) if os . path . isfile ( output ) : print ( 'leicaexperiment stitched file already' ' exists {}' . format ( output ) ) continue macros . append ( fijibin . macro . stitch ( path , filenames , fields_column , fields_row , output_filename = output , x_start = x_min , y_start = y_min ) ) return ( output_files , macros )
3285	def end_write ( self , with_errors ) : if not with_errors : commands . add ( self . provider . ui , self . provider . repo , self . localHgPath )
12031	def get_protocol_sequence ( self , sweep ) : self . setsweep ( sweep ) return list ( self . protoSeqX ) , list ( self . protoSeqY )
5391	def _delocalize_logging_command ( self , logging_path , user_project ) : logging_prefix = os . path . splitext ( logging_path . uri ) [ 0 ] if logging_path . file_provider == job_model . P_LOCAL : mkdir_cmd = 'mkdir -p "%s"\n' % os . path . dirname ( logging_prefix ) cp_cmd = 'cp' elif logging_path . file_provider == job_model . P_GCS : mkdir_cmd = '' if user_project : cp_cmd = 'gsutil -u {} -mq cp' . format ( user_project ) else : cp_cmd = 'gsutil -mq cp' else : assert False copy_logs_cmd = textwrap . dedent ( ) . format ( cp_cmd = cp_cmd , prefix = logging_prefix ) body = textwrap . dedent ( ) . format ( mkdir_cmd = mkdir_cmd , copy_logs_cmd = copy_logs_cmd ) return body
10850	def generate_sphere ( radius ) : rint = np . ceil ( radius ) . astype ( 'int' ) t = np . arange ( - rint , rint + 1 , 1 ) x , y , z = np . meshgrid ( t , t , t , indexing = 'ij' ) r = np . sqrt ( x * x + y * y + z * z ) sphere = r < radius return sphere
9633	def render_to_message ( self , extra_context = None , * args , ** kwargs ) : message = super ( TemplatedHTMLEmailMessageView , self ) . render_to_message ( extra_context , * args , ** kwargs ) if extra_context is None : extra_context = { } context = self . get_context_data ( ** extra_context ) content = self . render_html_body ( context ) message . attach_alternative ( content , mimetype = 'text/html' ) return message
5531	def get_process_tiles ( self , zoom = None ) : if zoom or zoom == 0 : for tile in self . config . process_pyramid . tiles_from_geom ( self . config . area_at_zoom ( zoom ) , zoom ) : yield tile else : for zoom in reversed ( self . config . zoom_levels ) : for tile in self . config . process_pyramid . tiles_from_geom ( self . config . area_at_zoom ( zoom ) , zoom ) : yield tile
947	def _printAvailableCheckpoints ( experimentDir ) : checkpointParentDir = getCheckpointParentDir ( experimentDir ) if not os . path . exists ( checkpointParentDir ) : print "No available checkpoints." return checkpointDirs = [ x for x in os . listdir ( checkpointParentDir ) if _isCheckpointDir ( os . path . join ( checkpointParentDir , x ) ) ] if not checkpointDirs : print "No available checkpoints." return print "Available checkpoints:" checkpointList = [ _checkpointLabelFromCheckpointDir ( x ) for x in checkpointDirs ] for checkpoint in sorted ( checkpointList ) : print "\t" , checkpoint print print "To start from a checkpoint:" print " python run_opf_experiment.py experiment --load <CHECKPOINT>" print "For example, to start from the checkpoint \"MyCheckpoint\":" print " python run_opf_experiment.py experiment --load MyCheckpoint"
1008	def _learnPhase2 ( self , readOnly = False ) : self . lrnPredictedState [ 't' ] . fill ( 0 ) for c in xrange ( self . numberOfCols ) : i , s , numActive = self . _getBestMatchingCell ( c , self . lrnActiveState [ 't' ] , minThreshold = self . activationThreshold ) if i is None : continue self . lrnPredictedState [ 't' ] [ c , i ] = 1 if readOnly : continue segUpdate = self . _getSegmentActiveSynapses ( c , i , s , activeState = self . lrnActiveState [ 't' ] , newSynapses = ( numActive < self . newSynapseCount ) ) s . totalActivations += 1 self . _addToSegmentUpdates ( c , i , segUpdate ) if self . doPooling : predSegment = self . _getBestMatchingSegment ( c , i , self . lrnActiveState [ 't-1' ] ) segUpdate = self . _getSegmentActiveSynapses ( c , i , predSegment , self . lrnActiveState [ 't-1' ] , newSynapses = True ) self . _addToSegmentUpdates ( c , i , segUpdate )
6851	def get_public_ip ( self ) : r = self . local_renderer ret = r . run ( r . env . get_public_ip_command ) or '' ret = ret . strip ( ) print ( 'ip:' , ret ) return ret
8913	def save_service ( self , service , overwrite = True ) : name = namesgenerator . get_sane_name ( service . name ) if not name : name = namesgenerator . get_random_name ( ) if name in self . name_index : name = namesgenerator . get_random_name ( retry = True ) if name in self . name_index : if overwrite : self . _delete ( name = name ) else : raise Exception ( "service name already registered." ) self . _insert ( Service ( name = name , url = baseurl ( service . url ) , type = service . type , purl = service . purl , public = service . public , auth = service . auth , verify = service . verify ) ) return self . fetch_by_name ( name = name )
6706	def expire_password ( self , username ) : r = self . local_renderer r . env . username = username r . sudo ( 'chage -d 0 {username}' )
7267	def run ( self , * args , ** kw ) : log . debug ( '[operator] run "{}" with arguments: {}' . format ( self . __class__ . __name__ , args ) ) if self . kind == OperatorTypes . ATTRIBUTE : return self . match ( self . ctx ) else : return self . run_matcher ( * args , ** kw )
12496	def warn_if_not_float ( X , estimator = 'This algorithm' ) : if not isinstance ( estimator , str ) : estimator = estimator . __class__ . __name__ if X . dtype . kind != 'f' : warnings . warn ( "%s assumes floating point values as input, " "got %s" % ( estimator , X . dtype ) ) return True return False
3675	def charge ( self ) : r try : if not self . rdkitmol : return charge_from_formula ( self . formula ) else : return Chem . GetFormalCharge ( self . rdkitmol ) except : return charge_from_formula ( self . formula )
7284	def has_edit_permission ( self , request ) : return request . user . is_authenticated and request . user . is_active and request . user . is_staff
9477	def parse_dom ( dom ) : root = dom . getElementsByTagName ( "graphml" ) [ 0 ] graph = root . getElementsByTagName ( "graph" ) [ 0 ] name = graph . getAttribute ( 'id' ) g = Graph ( name ) for node in graph . getElementsByTagName ( "node" ) : n = g . add_node ( id = node . getAttribute ( 'id' ) ) for attr in node . getElementsByTagName ( "data" ) : if attr . firstChild : n [ attr . getAttribute ( "key" ) ] = attr . firstChild . data else : n [ attr . getAttribute ( "key" ) ] = "" for edge in graph . getElementsByTagName ( "edge" ) : source = edge . getAttribute ( 'source' ) dest = edge . getAttribute ( 'target' ) e = g . add_edge_by_id ( source , dest ) for attr in edge . getElementsByTagName ( "data" ) : if attr . firstChild : e [ attr . getAttribute ( "key" ) ] = attr . firstChild . data else : e [ attr . getAttribute ( "key" ) ] = "" return g
11009	def subscribe ( self , event , bet_ids ) : if not self . _subscriptions . get ( event ) : self . _subscriptions [ event ] = set ( ) self . _subscriptions [ event ] = self . _subscriptions [ event ] . union ( bet_ids )
7407	def populate ( publications ) : customlinks = CustomLink . objects . filter ( publication__in = publications ) customfiles = CustomFile . objects . filter ( publication__in = publications ) publications_ = { } for publication in publications : publication . links = [ ] publication . files = [ ] publications_ [ publication . id ] = publication for link in customlinks : publications_ [ link . publication_id ] . links . append ( link ) for file in customfiles : publications_ [ file . publication_id ] . files . append ( file )
8375	def var_deleted ( self , v ) : widget = self . widgets [ v . name ] parent = widget . get_parent ( ) self . container . remove ( parent ) del self . widgets [ v . name ] self . window . set_size_request ( 400 , 35 * len ( self . widgets . keys ( ) ) ) self . window . show_all ( )
11780	def ContinuousXor ( n ) : "2 inputs are chosen uniformly from (0.0 .. 2.0]; output is xor of ints." examples = [ ] for i in range ( n ) : x , y = [ random . uniform ( 0.0 , 2.0 ) for i in '12' ] examples . append ( [ x , y , int ( x ) != int ( y ) ] ) return DataSet ( name = "continuous xor" , examples = examples )
5946	def in_dir ( directory , create = True ) : startdir = os . getcwd ( ) try : try : os . chdir ( directory ) logger . debug ( "Working in {directory!r}..." . format ( ** vars ( ) ) ) except OSError as err : if create and err . errno == errno . ENOENT : os . makedirs ( directory ) os . chdir ( directory ) logger . info ( "Working in {directory!r} (newly created)..." . format ( ** vars ( ) ) ) else : logger . exception ( "Failed to start working in {directory!r}." . format ( ** vars ( ) ) ) raise yield os . getcwd ( ) finally : os . chdir ( startdir )
8289	def sbot_executable ( ) : gsettings = load_gsettings ( ) venv = gsettings . get_string ( 'current-virtualenv' ) if venv == 'Default' : sbot = which ( 'sbot' ) elif venv == 'System' : env_venv = os . environ . get ( 'VIRTUAL_ENV' ) if not env_venv : return which ( 'sbot' ) for p in os . environ [ 'PATH' ] . split ( os . path . pathsep ) : sbot = '%s/sbot' % p if not p . startswith ( env_venv ) and os . path . isfile ( sbot ) : return sbot else : sbot = os . path . join ( venv , 'bin/sbot' ) if not os . path . isfile ( sbot ) : print ( 'Shoebot not found, reverting to System shoebot' ) sbot = which ( 'sbot' ) return os . path . realpath ( sbot )
10371	def build_pmid_exclusion_filter ( pmids : Strings ) -> EdgePredicate : if isinstance ( pmids , str ) : @ edge_predicate def pmid_exclusion_filter ( data : EdgeData ) -> bool : return has_pubmed ( data ) and data [ CITATION ] [ CITATION_REFERENCE ] != pmids elif isinstance ( pmids , Iterable ) : pmids = set ( pmids ) @ edge_predicate def pmid_exclusion_filter ( data : EdgeData ) -> bool : return has_pubmed ( data ) and data [ CITATION ] [ CITATION_REFERENCE ] not in pmids else : raise TypeError return pmid_exclusion_filter
9391	def aggregate_count_over_time ( self , metric_store , groupby_name , aggregate_timestamp ) : all_qps = metric_store [ 'qps' ] qps = all_qps [ groupby_name ] if aggregate_timestamp in qps : qps [ aggregate_timestamp ] += 1 else : qps [ aggregate_timestamp ] = 1 return None
10858	def get_update_tile ( self , params , values ) : doglobal , particles = self . _update_type ( params ) if doglobal : return self . shape . copy ( ) values0 = self . get_values ( params ) tiles0 = [ self . _tile ( n ) for n in particles ] self . set_values ( params , values ) tiles1 = [ self . _tile ( n ) for n in particles ] self . set_values ( params , values0 ) return Tile . boundingtile ( tiles0 + tiles1 )
5355	def convert_from_eclipse ( self , eclipse_projects ) : projects = { } projects [ 'unknown' ] = { "gerrit" : [ "git.eclipse.org" ] , "bugzilla" : [ "https://bugs.eclipse.org/bugs/" ] } projects = compose_title ( projects , eclipse_projects ) projects = compose_projects_json ( projects , eclipse_projects ) return projects
11279	def _encode_ids ( * args ) : ids = [ ] for v in args : if isinstance ( v , basestring ) : qv = v . encode ( 'utf-8' ) if isinstance ( v , unicode ) else v ids . append ( urllib . quote ( qv ) ) else : qv = str ( v ) ids . append ( urllib . quote ( qv ) ) return ';' . join ( ids )
9275	def apply_exclude_tags_regex ( self , all_tags ) : filtered = [ ] for tag in all_tags : if not re . match ( self . options . exclude_tags_regex , tag [ "name" ] ) : filtered . append ( tag ) if len ( all_tags ) == len ( filtered ) : self . warn_if_nonmatching_regex ( ) return filtered
4692	def env ( ) : ipmi = cij . env_to_dict ( PREFIX , REQUIRED ) if ipmi is None : ipmi [ "USER" ] = "admin" ipmi [ "PASS" ] = "admin" ipmi [ "HOST" ] = "localhost" ipmi [ "PORT" ] = "623" cij . info ( "ipmi.env: USER: %s, PASS: %s, HOST: %s, PORT: %s" % ( ipmi [ "USER" ] , ipmi [ "PASS" ] , ipmi [ "HOST" ] , ipmi [ "PORT" ] ) ) cij . env_export ( PREFIX , EXPORTED , ipmi ) return 0
7988	def _setup_stream_element_handlers ( self ) : if self . initiator : mode = "initiator" else : mode = "receiver" self . _element_handlers = { } for handler in self . handlers : if not isinstance ( handler , StreamFeatureHandler ) : continue for _unused , meth in inspect . getmembers ( handler , callable ) : if not hasattr ( meth , "_pyxmpp_stream_element_handled" ) : continue element_handled = meth . _pyxmpp_stream_element_handled if element_handled in self . _element_handlers : continue if meth . _pyxmpp_usage_restriction in ( None , mode ) : self . _element_handlers [ element_handled ] = meth
1821	def SETPE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . PF , 1 , 0 ) )
8384	def draw ( self ) : if len ( self . q ) > 0 : self . update ( ) if self . delay == 0 : p , h = self . textpath ( self . i ) f = self . fontsize self . _ctx . fill ( self . background ) self . _ctx . rect ( self . node . x + f * 1.0 , self . node . y + f * 0.5 , self . _w + f , h + f * 1.5 , roundness = 0.2 ) alpha = 1.0 if self . fi < 5 : alpha = 0.2 * self . fi if self . fn - self . fi < 5 : alpha = 0.2 * ( self . fn - self . fi ) self . _ctx . fill ( self . text . r , self . text . g , self . text . b , self . text . a * alpha ) self . _ctx . translate ( self . node . x + f * 2.0 , self . node . y + f * 2.5 ) self . _ctx . drawpath ( p )
4398	def adsSyncDelDeviceNotificationReqEx ( port , adr , notification_handle , user_handle ) : adsSyncDelDeviceNotificationReqFct = _adsDLL . AdsSyncDelDeviceNotificationReqEx pAmsAddr = ctypes . pointer ( adr . amsAddrStruct ( ) ) nHNotification = ctypes . c_ulong ( notification_handle ) err_code = adsSyncDelDeviceNotificationReqFct ( port , pAmsAddr , nHNotification ) callback_store . pop ( notification_handle , None ) if err_code : raise ADSError ( err_code ) adsSyncWriteReqEx ( port , adr , ADSIGRP_SYM_RELEASEHND , 0 , user_handle , PLCTYPE_UDINT )
8033	def find_dupes ( paths , exact = False , ignores = None , min_size = 0 ) : groups = { '' : getPaths ( paths , ignores ) } groups = groupBy ( groups , sizeClassifier , 'sizes' , min_size = min_size ) groups = groupBy ( groups , hashClassifier , 'header hashes' , limit = HEAD_SIZE ) if exact : groups = groupBy ( groups , groupByContent , fun_desc = 'contents' ) else : groups = groupBy ( groups , hashClassifier , fun_desc = 'hashes' ) return groups
5889	def smart_unicode ( string , encoding = 'utf-8' , strings_only = False , errors = 'strict' ) : return force_unicode ( string , encoding , strings_only , errors )
7666	def _key ( cls , obs ) : if not isinstance ( obs , Observation ) : raise JamsError ( '{} must be of type jams.Observation' . format ( obs ) ) return obs . time
5319	def readattr ( path , name ) : try : f = open ( USB_SYS_PREFIX + path + "/" + name ) return f . readline ( ) . rstrip ( "\n" ) except IOError : return None
1727	def parse_identifier ( source , start , throw = True ) : start = pass_white ( source , start ) end = start if not end < len ( source ) : if throw : raise SyntaxError ( 'Missing identifier!' ) return None if source [ end ] not in IDENTIFIER_START : if throw : raise SyntaxError ( 'Invalid identifier start: "%s"' % source [ end ] ) return None end += 1 while end < len ( source ) and source [ end ] in IDENTIFIER_PART : end += 1 if not is_valid_lval ( source [ start : end ] ) : if throw : raise SyntaxError ( 'Invalid identifier name: "%s"' % source [ start : end ] ) return None return source [ start : end ] , end
2371	def settings ( self ) : for table in self . tables : if isinstance ( table , SettingTable ) : for statement in table . statements : yield statement
3038	def put ( self , credentials ) : self . acquire_lock ( ) try : self . locked_put ( credentials ) finally : self . release_lock ( )
1744	def pythonize_arguments ( arg_str ) : out_args = [ ] if arg_str is None : return out_str args = arg_str . split ( ',' ) for arg in args : components = arg . split ( '=' ) name_and_type = components [ 0 ] . split ( ' ' ) if name_and_type [ - 1 ] == '' and len ( name_and_type ) > 1 : name = name_and_type [ - 2 ] else : name = name_and_type [ - 1 ] if len ( components ) > 1 : name += '=' + components [ 1 ] out_args . append ( name ) return ',' . join ( out_args )
13744	def create_table ( self ) : table = self . conn . create_table ( name = self . get_table_name ( ) , schema = self . get_schema ( ) , read_units = self . get_read_units ( ) , write_units = self . get_write_units ( ) , ) if table . status != 'ACTIVE' : table . refresh ( wait_for_active = True , retry_seconds = 1 ) return table
6724	def exists ( name = None , group = None , release = None , except_release = None , verbose = 1 ) : verbose = int ( verbose ) instances = list_instances ( name = name , group = group , release = release , except_release = except_release , verbose = verbose , show = verbose ) ret = bool ( instances ) if verbose : print ( '\ninstance %s exist' % ( 'DOES' if ret else 'does NOT' ) ) return instances
4771	def contains ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) elif len ( items ) == 1 : if items [ 0 ] not in self . val : if self . _check_dict_like ( self . val , return_as_bool = True ) : self . _err ( 'Expected <%s> to contain key <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : self . _err ( 'Expected <%s> to contain item <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : missing = [ ] for i in items : if i not in self . val : missing . append ( i ) if missing : if self . _check_dict_like ( self . val , return_as_bool = True ) : self . _err ( 'Expected <%s> to contain keys %s, but did not contain key%s %s.' % ( self . val , self . _fmt_items ( items ) , '' if len ( missing ) == 0 else 's' , self . _fmt_items ( missing ) ) ) else : self . _err ( 'Expected <%s> to contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) return self
7461	def save_json ( data ) : datadict = OrderedDict ( [ ( "_version" , data . __dict__ [ "_version" ] ) , ( "_checkpoint" , data . __dict__ [ "_checkpoint" ] ) , ( "name" , data . __dict__ [ "name" ] ) , ( "dirs" , data . __dict__ [ "dirs" ] ) , ( "paramsdict" , data . __dict__ [ "paramsdict" ] ) , ( "samples" , data . __dict__ [ "samples" ] . keys ( ) ) , ( "populations" , data . __dict__ [ "populations" ] ) , ( "database" , data . __dict__ [ "database" ] ) , ( "clust_database" , data . __dict__ [ "clust_database" ] ) , ( "outfiles" , data . __dict__ [ "outfiles" ] ) , ( "barcodes" , data . __dict__ [ "barcodes" ] ) , ( "stats_files" , data . __dict__ [ "stats_files" ] ) , ( "_hackersonly" , data . __dict__ [ "_hackersonly" ] ) , ] ) sampledict = OrderedDict ( [ ] ) for key , sample in data . samples . iteritems ( ) : sampledict [ key ] = sample . _to_fulldict ( ) fulldumps = json . dumps ( { "assembly" : datadict , "samples" : sampledict } , cls = Encoder , sort_keys = False , indent = 4 , separators = ( "," , ":" ) , ) assemblypath = os . path . join ( data . dirs . project , data . name + ".json" ) if not os . path . exists ( data . dirs . project ) : os . mkdir ( data . dirs . project ) done = 0 while not done : try : with open ( assemblypath , 'w' ) as jout : jout . write ( fulldumps ) done = 1 except ( KeyboardInterrupt , SystemExit ) : print ( '.' ) continue
8241	def compound ( clr , flip = False ) : def _wrap ( x , min , threshold , plus ) : if x - min < threshold : return x + plus else : return x - min d = 1 if flip : d = - 1 clr = color ( clr ) colors = colorlist ( clr ) c = clr . rotate_ryb ( 30 * d ) c . brightness = _wrap ( clr . brightness , 0.25 , 0.6 , 0.25 ) colors . append ( c ) c = clr . rotate_ryb ( 30 * d ) c . saturation = _wrap ( clr . saturation , 0.4 , 0.1 , 0.4 ) c . brightness = _wrap ( clr . brightness , 0.4 , 0.2 , 0.4 ) colors . append ( c ) c = clr . rotate_ryb ( 160 * d ) c . saturation = _wrap ( clr . saturation , 0.25 , 0.1 , 0.25 ) c . brightness = max ( 0.2 , clr . brightness ) colors . append ( c ) c = clr . rotate_ryb ( 150 * d ) c . saturation = _wrap ( clr . saturation , 0.1 , 0.8 , 0.1 ) c . brightness = _wrap ( clr . brightness , 0.3 , 0.6 , 0.3 ) colors . append ( c ) c = clr . rotate_ryb ( 150 * d ) c . saturation = _wrap ( clr . saturation , 0.1 , 0.8 , 0.1 ) c . brightness = _wrap ( clr . brightness , 0.4 , 0.2 , 0.4 ) return colors
9488	def generate_simple_call ( opcode : int , index : int ) : bs = b"" bs += opcode . to_bytes ( 1 , byteorder = "little" ) if isinstance ( index , int ) : if PY36 : bs += index . to_bytes ( 1 , byteorder = "little" ) else : bs += index . to_bytes ( 2 , byteorder = "little" ) else : bs += index return bs
9648	def parse_log_messages ( self , text ) : regex = r"commit ([0-9a-f]+)\nAuthor: (.*?)\n\n(.*?)(?:\n\n|$)" messages = re . findall ( regex , text , re . DOTALL ) parsed = [ ] for commit , author , message in messages : parsed . append ( ( commit [ : 10 ] , re . sub ( r"\s*<.*?>" , "" , author ) , message . strip ( ) ) ) return parsed
4401	def fetch ( self ) : xml = urllib . request . urlopen ( self . URL ) tree = ET . ElementTree ( file = xml ) records = self . _parse_deputies ( tree . getroot ( ) ) df = pd . DataFrame ( records , columns = ( 'congressperson_id' , 'budget_id' , 'condition' , 'congressperson_document' , 'civil_name' , 'congressperson_name' , 'picture_url' , 'gender' , 'state' , 'party' , 'phone_number' , 'email' ) ) return self . _translate ( df )
4460	def geo ( lat , lon , radius , unit = 'km' ) : return GeoValue ( lat , lon , radius , unit )
1461	def import_and_get_class ( path_to_pex , python_class_name ) : abs_path_to_pex = os . path . abspath ( path_to_pex ) Log . debug ( "Add a pex to the path: %s" % abs_path_to_pex ) Log . debug ( "In import_and_get_class with cls_name: %s" % python_class_name ) split = python_class_name . split ( '.' ) from_path = '.' . join ( split [ : - 1 ] ) import_name = python_class_name . split ( '.' ) [ - 1 ] Log . debug ( "From path: %s, import name: %s" % ( from_path , import_name ) ) if python_class_name . startswith ( "heron." ) : try : mod = resolve_heron_suffix_issue ( abs_path_to_pex , python_class_name ) return getattr ( mod , import_name ) except : Log . error ( "Could not resolve class %s with special handling" % python_class_name ) mod = __import__ ( from_path , fromlist = [ import_name ] , level = - 1 ) Log . debug ( "Imported module: %s" % str ( mod ) ) return getattr ( mod , import_name )
9145	def clear ( skip ) : for name in sorted ( MODULES ) : if name in skip : continue click . secho ( f'clearing cache for {name}' , fg = 'cyan' , bold = True ) clear_cache ( name )
9478	def parse_string ( self , string ) : dom = minidom . parseString ( string ) return self . parse_dom ( dom )
9034	def instruction_in_grid ( self , instruction ) : row_position = self . _rows_in_grid [ instruction . row ] . xy x = instruction . index_of_first_consumed_mesh_in_row position = Point ( row_position . x + x , row_position . y ) return InstructionInGrid ( instruction , position )
763	def createRecordSensor ( network , name , dataSource ) : regionType = "py.RecordSensor" regionParams = json . dumps ( { "verbosity" : _VERBOSITY } ) network . addRegion ( name , regionType , regionParams ) sensorRegion = network . regions [ name ] . getSelf ( ) sensorRegion . encoder = createEncoder ( ) network . regions [ name ] . setParameter ( "predictedField" , "consumption" ) sensorRegion . dataSource = dataSource return sensorRegion
7838	def set_node ( self , node ) : if node is None : if self . xmlnode . hasProp ( "node" ) : self . xmlnode . unsetProp ( "node" ) return node = unicode ( node ) self . xmlnode . setProp ( "node" , node . encode ( "utf-8" ) )
12260	def columns ( x , rho , proxop ) : xnext = np . zeros_like ( x ) for ix in range ( x . shape [ 1 ] ) : xnext [ : , ix ] = proxop ( x [ : , ix ] , rho ) return xnext
5284	def formset_valid ( self , formset ) : self . object_list = formset . save ( ) return super ( ModelFormSetMixin , self ) . formset_valid ( formset )
7279	def play ( self ) : if not self . is_playing ( ) : self . play_pause ( ) self . _is_playing = True self . playEvent ( self )
8697	def __clear_buffers ( self ) : try : self . _port . reset_input_buffer ( ) self . _port . reset_output_buffer ( ) except AttributeError : self . _port . flushInput ( ) self . _port . flushOutput ( )
1657	def IsInitializerList ( clean_lines , linenum ) : for i in xrange ( linenum , 1 , - 1 ) : line = clean_lines . elided [ i ] if i == linenum : remove_function_body = Match ( r'^(.*)\{\s*$' , line ) if remove_function_body : line = remove_function_body . group ( 1 ) if Search ( r'\s:\s*\w+[({]' , line ) : return True if Search ( r'\}\s*,\s*$' , line ) : return True if Search ( r'[{};]\s*$' , line ) : return False return False
8838	def get_var ( data , var_name , not_found = None ) : try : for key in str ( var_name ) . split ( '.' ) : try : data = data [ key ] except TypeError : data = data [ int ( key ) ] except ( KeyError , TypeError , ValueError ) : return not_found else : return data
898	def addSpatialNoise ( self , sequence , amount ) : newSequence = [ ] for pattern in sequence : if pattern is not None : pattern = self . patternMachine . addNoise ( pattern , amount ) newSequence . append ( pattern ) return newSequence
4049	def fulltext_item ( self , itemkey , ** kwargs ) : query_string = "/{t}/{u}/items/{itemkey}/fulltext" . format ( t = self . library_type , u = self . library_id , itemkey = itemkey ) return self . _build_query ( query_string )
512	def _updateMinDutyCyclesLocal ( self ) : for column in xrange ( self . _numColumns ) : neighborhood = self . _getColumnNeighborhood ( column ) maxActiveDuty = self . _activeDutyCycles [ neighborhood ] . max ( ) maxOverlapDuty = self . _overlapDutyCycles [ neighborhood ] . max ( ) self . _minOverlapDutyCycles [ column ] = ( maxOverlapDuty * self . _minPctOverlapDutyCycles )
3420	def create_mat_dict ( model ) : rxns = model . reactions mets = model . metabolites mat = OrderedDict ( ) mat [ "mets" ] = _cell ( [ met_id for met_id in create_mat_metabolite_id ( model ) ] ) mat [ "metNames" ] = _cell ( mets . list_attr ( "name" ) ) mat [ "metFormulas" ] = _cell ( [ str ( m . formula ) for m in mets ] ) try : mat [ "metCharge" ] = array ( mets . list_attr ( "charge" ) ) * 1. except TypeError : pass mat [ "genes" ] = _cell ( model . genes . list_attr ( "id" ) ) rxn_gene = scipy_sparse . dok_matrix ( ( len ( model . reactions ) , len ( model . genes ) ) ) if min ( rxn_gene . shape ) > 0 : for i , reaction in enumerate ( model . reactions ) : for gene in reaction . genes : rxn_gene [ i , model . genes . index ( gene ) ] = 1 mat [ "rxnGeneMat" ] = rxn_gene mat [ "grRules" ] = _cell ( rxns . list_attr ( "gene_reaction_rule" ) ) mat [ "rxns" ] = _cell ( rxns . list_attr ( "id" ) ) mat [ "rxnNames" ] = _cell ( rxns . list_attr ( "name" ) ) mat [ "subSystems" ] = _cell ( rxns . list_attr ( "subsystem" ) ) stoich_mat = create_stoichiometric_matrix ( model ) mat [ "S" ] = stoich_mat if stoich_mat is not None else [ [ ] ] mat [ "lb" ] = array ( rxns . list_attr ( "lower_bound" ) ) * 1. mat [ "ub" ] = array ( rxns . list_attr ( "upper_bound" ) ) * 1. mat [ "b" ] = array ( mets . list_attr ( "_bound" ) ) * 1. mat [ "c" ] = array ( rxns . list_attr ( "objective_coefficient" ) ) * 1. mat [ "rev" ] = array ( rxns . list_attr ( "reversibility" ) ) * 1 mat [ "description" ] = str ( model . id ) return mat
1477	def _get_instance_plans ( self , packing_plan , container_id ) : this_container_plan = None for container_plan in packing_plan . container_plans : if container_plan . id == container_id : this_container_plan = container_plan if this_container_plan is None : return None return this_container_plan . instance_plans
12695	def contains_all ( set1 , set2 , warn ) : for elem in set2 : if elem not in set1 : raise ValueError ( warn ) return True
6532	def get_local_config ( project_path , use_cache = True ) : pyproject_path = os . path . join ( project_path , 'pyproject.toml' ) if os . path . exists ( pyproject_path ) : with open ( pyproject_path , 'r' ) as config_file : config = pytoml . load ( config_file ) config = config . get ( 'tool' , { } ) . get ( 'tidypy' , { } ) config = merge_dict ( get_default_config ( ) , config ) config = process_extensions ( config , project_path , use_cache = use_cache ) return config return None
6511	def _eat_name_line ( self , line ) : if line [ 0 ] not in "#=" : parts = line . split ( ) country_values = line [ 30 : - 1 ] name = map_name ( parts [ 1 ] ) if not self . case_sensitive : name = name . lower ( ) if parts [ 0 ] == "M" : self . _set ( name , u"male" , country_values ) elif parts [ 0 ] == "1M" or parts [ 0 ] == "?M" : self . _set ( name , u"mostly_male" , country_values ) elif parts [ 0 ] == "F" : self . _set ( name , u"female" , country_values ) elif parts [ 0 ] == "1F" or parts [ 0 ] == "?F" : self . _set ( name , u"mostly_female" , country_values ) elif parts [ 0 ] == "?" : self . _set ( name , self . unknown_value , country_values ) else : raise "Not sure what to do with a sex of %s" % parts [ 0 ]
6000	def pix_to_regular ( self ) : pix_to_regular = [ [ ] for _ in range ( self . pixels ) ] for regular_pixel , pix_pixel in enumerate ( self . regular_to_pix ) : pix_to_regular [ pix_pixel ] . append ( regular_pixel ) return pix_to_regular
13465	def __register_library ( self , module_name : str , attr : str , fallback : str = None ) : try : module = importlib . import_module ( module_name ) except ImportError : if fallback is not None : module = importlib . import_module ( fallback ) self . __logger . warn ( module_name + " not available: Replaced with " + fallback ) else : self . __logger . warn ( module_name + " not available: No Replacement Specified" ) if not attr in dir ( self . __sketch ) : setattr ( self . __sketch , attr , module ) else : self . __logger . warn ( attr + " could not be imported as it's label is already used in the sketch" )
5413	def lookup_job_tasks ( self , statuses , user_ids = None , job_ids = None , job_names = None , task_ids = None , task_attempts = None , labels = None , create_time_min = None , create_time_max = None , max_tasks = 0 ) : statuses = None if statuses == { '*' } else statuses user_ids = None if user_ids == { '*' } else user_ids job_ids = None if job_ids == { '*' } else job_ids job_names = None if job_names == { '*' } else job_names task_ids = None if task_ids == { '*' } else task_ids task_attempts = None if task_attempts == { '*' } else task_attempts if labels or create_time_min or create_time_max : raise NotImplementedError ( 'Lookup by labels and create_time not yet supported by stub.' ) operations = [ x for x in self . _operations if ( ( not statuses or x . get_field ( 'status' , ( None , None ) ) [ 0 ] in statuses ) and ( not user_ids or x . get_field ( 'user' , None ) in user_ids ) and ( not job_ids or x . get_field ( 'job-id' , None ) in job_ids ) and ( not job_names or x . get_field ( 'job-name' , None ) in job_names ) and ( not task_ids or x . get_field ( 'task-id' , None ) in task_ids ) and ( not task_attempts or x . get_field ( 'task-attempt' , None ) in task_attempts ) ) ] if max_tasks > 0 : operations = operations [ : max_tasks ] return operations
1562	def get_component_tasks ( self , component_id ) : ret = [ ] for task_id , comp_id in self . task_to_component_map . items ( ) : if comp_id == component_id : ret . append ( task_id ) return ret
4458	def sort_by ( self , field , asc = True ) : self . _sortby = SortbyField ( field , asc ) return self
11525	def create_big_thumbnail ( self , token , bitstream_id , item_id , width = 575 ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'bitstreamId' ] = bitstream_id parameters [ 'itemId' ] = item_id parameters [ 'width' ] = width response = self . request ( 'midas.thumbnailcreator.create.big.thumbnail' , parameters ) return response
13351	def add_files ( self , filelist , ** kwargs ) : if not isinstance ( filelist , list ) : raise TypeError ( "request the list type." ) for file in filelist : self . add_file ( file )
3223	def _googleauth ( key_file = None , scopes = [ ] , user_agent = None ) : if key_file : if not scopes : scopes = DEFAULT_SCOPES creds = ServiceAccountCredentials . from_json_keyfile_name ( key_file , scopes = scopes ) else : creds = GoogleCredentials . get_application_default ( ) http = Http ( ) if user_agent : http = set_user_agent ( http , user_agent ) http_auth = creds . authorize ( http ) return http_auth
9736	def get_3d_markers_residual ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPositionResidual , component_info , data , component_position )
8898	def _deserialize_from_store ( profile ) : _serialize_into_store ( profile ) fk_cache = { } with transaction . atomic ( ) : syncable_dict = _profile_models [ profile ] excluded_list = [ ] for model_name , klass_model in six . iteritems ( syncable_dict ) : self_ref_fk = _self_referential_fk ( klass_model ) query = Q ( model_name = klass_model . morango_model_name ) for klass in klass_model . morango_model_dependencies : query |= Q ( model_name = klass . morango_model_name ) if self_ref_fk : clean_parents = Store . objects . filter ( dirty_bit = False , profile = profile ) . filter ( query ) . char_ids_list ( ) dirty_children = Store . objects . filter ( dirty_bit = True , profile = profile ) . filter ( Q ( _self_ref_fk__in = clean_parents ) | Q ( _self_ref_fk = '' ) ) . filter ( query ) while len ( dirty_children ) > 0 : for store_model in dirty_children : try : app_model = store_model . _deserialize_store_model ( fk_cache ) if app_model : with mute_signals ( signals . pre_save , signals . post_save ) : app_model . save ( update_dirty_bit_to = False ) store_model . dirty_bit = False store_model . save ( update_fields = [ 'dirty_bit' ] ) except exceptions . ValidationError : excluded_list . append ( store_model . id ) clean_parents = Store . objects . filter ( dirty_bit = False , profile = profile ) . filter ( query ) . char_ids_list ( ) dirty_children = Store . objects . filter ( dirty_bit = True , profile = profile , _self_ref_fk__in = clean_parents ) . filter ( query ) else : db_values = [ ] fields = klass_model . _meta . fields for store_model in Store . objects . filter ( model_name = model_name , profile = profile , dirty_bit = True ) : try : app_model = store_model . _deserialize_store_model ( fk_cache ) if app_model : for f in fields : value = getattr ( app_model , f . attname ) db_value = f . get_db_prep_value ( value , connection ) db_values . append ( db_value ) except exceptions . ValidationError : excluded_list . append ( store_model . id ) if db_values : num_of_rows = len ( db_values ) // len ( fields ) placeholder_tuple = tuple ( [ '%s' for _ in range ( len ( fields ) ) ] ) placeholder_list = [ str ( placeholder_tuple ) for _ in range ( num_of_rows ) ] with connection . cursor ( ) as cursor : DBBackend . _bulk_insert_into_app_models ( cursor , klass_model . _meta . db_table , fields , db_values , placeholder_list ) Store . objects . exclude ( id__in = excluded_list ) . filter ( profile = profile , dirty_bit = True ) . update ( dirty_bit = False )
2293	def eval_entropy ( x ) : hx = 0. sx = sorted ( x ) for i , j in zip ( sx [ : - 1 ] , sx [ 1 : ] ) : delta = j - i if bool ( delta ) : hx += np . log ( np . abs ( delta ) ) hx = hx / ( len ( x ) - 1 ) + psi ( len ( x ) ) - psi ( 1 ) return hx
10658	def amount_fractions ( masses ) : n = amounts ( masses ) n_total = sum ( n . values ( ) ) return { compound : n [ compound ] / n_total for compound in n . keys ( ) }
5714	def _validate_zip ( the_zip ) : datapackage_jsons = [ f for f in the_zip . namelist ( ) if f . endswith ( 'datapackage.json' ) ] if len ( datapackage_jsons ) != 1 : msg = 'DataPackage must have only one "datapackage.json" (had {n})' raise exceptions . DataPackageException ( msg . format ( n = len ( datapackage_jsons ) ) )
5955	def find_executables ( path ) : execs = [ ] for exe in os . listdir ( path ) : fullexe = os . path . join ( path , exe ) if ( os . access ( fullexe , os . X_OK ) and not os . path . isdir ( fullexe ) and exe not in [ 'GMXRC' , 'GMXRC.bash' , 'GMXRC.csh' , 'GMXRC.zsh' , 'demux.pl' , 'xplor2gmx.pl' ] ) : execs . append ( exe ) return execs
4569	def dump ( data , file = sys . stdout , use_yaml = None , ** kwds ) : if use_yaml is None : use_yaml = ALWAYS_DUMP_YAML def dump ( fp ) : if use_yaml : yaml . safe_dump ( data , stream = fp , ** kwds ) else : json . dump ( data , fp , indent = 4 , sort_keys = True , ** kwds ) if not isinstance ( file , str ) : return dump ( file ) if os . path . isabs ( file ) : parent = os . path . dirname ( file ) if not os . path . exists ( parent ) : os . makedirs ( parent , exist_ok = True ) with open ( file , 'w' ) as fp : return dump ( fp )
8420	def same_log10_order_of_magnitude ( x , delta = 0.1 ) : dmin = np . log10 ( np . min ( x ) * ( 1 - delta ) ) dmax = np . log10 ( np . max ( x ) * ( 1 + delta ) ) return np . floor ( dmin ) == np . floor ( dmax )
10668	def get_datetime_at_period_ix ( self , ix ) : if self . timestep_period_duration == TimePeriod . millisecond : return self . start_datetime + timedelta ( milliseconds = ix ) elif self . timestep_period_duration == TimePeriod . second : return self . start_datetime + timedelta ( seconds = ix ) elif self . timestep_period_duration == TimePeriod . minute : return self . start_datetime + timedelta ( minutes = ix ) elif self . timestep_period_duration == TimePeriod . hour : return self . start_datetime + timedelta ( hours = ix ) elif self . timestep_period_duration == TimePeriod . day : return self . start_datetime + relativedelta ( days = ix ) elif self . timestep_period_duration == TimePeriod . week : return self . start_datetime + relativedelta ( days = ix * 7 ) elif self . timestep_period_duration == TimePeriod . month : return self . start_datetime + relativedelta ( months = ix ) elif self . timestep_period_duration == TimePeriod . year : return self . start_datetime + relativedelta ( years = ix )
7908	def __presence_error ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : return False rs . process_error_presence ( stanza ) return True
667	def logProbability ( self , distn ) : x = numpy . asarray ( distn ) n = x . sum ( ) return ( logFactorial ( n ) - numpy . sum ( [ logFactorial ( k ) for k in x ] ) + numpy . sum ( x * numpy . log ( self . dist . pmf ) ) )
13556	def all_comments ( self ) : ctype = ContentType . objects . get ( app_label__exact = "happenings" , model__exact = 'event' ) update_ctype = ContentType . objects . get ( app_label__exact = "happenings" , model__exact = 'update' ) update_ids = self . update_set . values_list ( 'id' , flat = True ) return Comment . objects . filter ( Q ( content_type = ctype . id , object_pk = self . id ) | Q ( content_type = update_ctype . id , object_pk__in = update_ids ) )
7638	def find_with_extension ( in_dir , ext , depth = 3 , sort = True ) : assert depth >= 1 ext = ext . strip ( os . extsep ) match = list ( ) for n in range ( 1 , depth + 1 ) : wildcard = os . path . sep . join ( [ "*" ] * n ) search_path = os . path . join ( in_dir , os . extsep . join ( [ wildcard , ext ] ) ) match += glob . glob ( search_path ) if sort : match . sort ( ) return match
676	def __shouldSysExit ( self , iteration ) : if self . _exitAfter is None or iteration < self . _exitAfter : return False results = self . _jobsDAO . modelsGetFieldsForJob ( self . _jobID , [ 'params' ] ) modelIDs = [ e [ 0 ] for e in results ] modelNums = [ json . loads ( e [ 1 ] [ 0 ] ) [ 'structuredParams' ] [ '__model_num' ] for e in results ] sameModelNumbers = filter ( lambda x : x [ 1 ] == self . modelIndex , zip ( modelIDs , modelNums ) ) firstModelID = min ( zip ( * sameModelNumbers ) [ 0 ] ) return firstModelID == self . _modelID
6967	def smooth_magseries_gaussfilt ( mags , windowsize , windowfwhm = 7 ) : convkernel = Gaussian1DKernel ( windowfwhm , x_size = windowsize ) smoothed = convolve ( mags , convkernel , boundary = 'extend' ) return smoothed
445	def prefetch_input_data ( reader , file_pattern , is_training , batch_size , values_per_shard , input_queue_capacity_factor = 16 , num_reader_threads = 1 , shard_queue_name = "filename_queue" , value_queue_name = "input_queue" ) : data_files = [ ] for pattern in file_pattern . split ( "," ) : data_files . extend ( tf . gfile . Glob ( pattern ) ) if not data_files : tl . logging . fatal ( "Found no input files matching %s" , file_pattern ) else : tl . logging . info ( "Prefetching values from %d files matching %s" , len ( data_files ) , file_pattern ) if is_training : print ( " is_training == True : RandomShuffleQueue" ) filename_queue = tf . train . string_input_producer ( data_files , shuffle = True , capacity = 16 , name = shard_queue_name ) min_queue_examples = values_per_shard * input_queue_capacity_factor capacity = min_queue_examples + 100 * batch_size values_queue = tf . RandomShuffleQueue ( capacity = capacity , min_after_dequeue = min_queue_examples , dtypes = [ tf . string ] , name = "random_" + value_queue_name ) else : print ( " is_training == False : FIFOQueue" ) filename_queue = tf . train . string_input_producer ( data_files , shuffle = False , capacity = 1 , name = shard_queue_name ) capacity = values_per_shard + 3 * batch_size values_queue = tf . FIFOQueue ( capacity = capacity , dtypes = [ tf . string ] , name = "fifo_" + value_queue_name ) enqueue_ops = [ ] for _ in range ( num_reader_threads ) : _ , value = reader . read ( filename_queue ) enqueue_ops . append ( values_queue . enqueue ( [ value ] ) ) tf . train . queue_runner . add_queue_runner ( tf . train . queue_runner . QueueRunner ( values_queue , enqueue_ops ) ) tf . summary . scalar ( "queue/%s/fraction_of_%d_full" % ( values_queue . name , capacity ) , tf . cast ( values_queue . size ( ) , tf . float32 ) * ( 1. / capacity ) ) return values_queue
3211	def insert ( self , key , obj , future_expiration_minutes = 15 ) : expiration_time = self . _calculate_expiration ( future_expiration_minutes ) self . _CACHE [ key ] = ( expiration_time , obj ) return True
1822	def SETPO ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . PF == False , 1 , 0 ) )
10676	def Cp ( compound_string , T , mass = 1.0 ) : formula , phase = _split_compound_string_ ( compound_string ) TK = T + 273.15 compound = compounds [ formula ] result = compound . Cp ( phase , TK ) return _finalise_result_ ( compound , result , mass )
4883	def transmit ( self , payload , ** kwargs ) : kwargs [ 'app_label' ] = 'sap_success_factors' kwargs [ 'model_name' ] = 'SapSuccessFactorsLearnerDataTransmissionAudit' kwargs [ 'remote_user_id' ] = 'sapsf_user_id' super ( SapSuccessFactorsLearnerTransmitter , self ) . transmit ( payload , ** kwargs )
2944	def accept_message ( self , message ) : assert not self . read_only self . refresh_waiting_tasks ( ) self . do_engine_steps ( ) for my_task in Task . Iterator ( self . task_tree , Task . WAITING ) : my_task . task_spec . accept_message ( my_task , message )
4320	def set_globals ( self , dither = False , guard = False , multithread = False , replay_gain = False , verbosity = 2 ) : if not isinstance ( dither , bool ) : raise ValueError ( 'dither must be a boolean.' ) if not isinstance ( guard , bool ) : raise ValueError ( 'guard must be a boolean.' ) if not isinstance ( multithread , bool ) : raise ValueError ( 'multithread must be a boolean.' ) if not isinstance ( replay_gain , bool ) : raise ValueError ( 'replay_gain must be a boolean.' ) if verbosity not in VERBOSITY_VALS : raise ValueError ( 'Invalid value for VERBOSITY. Must be one {}' . format ( VERBOSITY_VALS ) ) global_args = [ ] if not dither : global_args . append ( '-D' ) if guard : global_args . append ( '-G' ) if multithread : global_args . append ( '--multi-threaded' ) if replay_gain : global_args . append ( '--replay-gain' ) global_args . append ( 'track' ) global_args . append ( '-V{}' . format ( verbosity ) ) self . globals = global_args return self
8518	def fromdict ( cls , config , check_fields = True ) : m = super ( Config , cls ) . __new__ ( cls ) m . path = '.' m . verbose = False m . config = m . _merge_defaults ( config ) if check_fields : m . _check_fields ( ) return m
13643	def command_list ( ) : from cliez . conf import COMPONENT_ROOT root = COMPONENT_ROOT if root is None : sys . stderr . write ( "cliez.conf.COMPONENT_ROOT not set.\n" ) sys . exit ( 2 ) pass if not os . path . exists ( root ) : sys . stderr . write ( "please set a valid path for `cliez.conf.COMPONENT_ROOT`\n" ) sys . exit ( 2 ) pass try : path = os . listdir ( os . path . join ( root , 'components' ) ) return [ f [ : - 3 ] for f in path if f . endswith ( '.py' ) and f != '__init__.py' ] except FileNotFoundError : return [ ]
7030	def specwindow_lsp_value ( times , mags , errs , omega ) : norm_times = times - times . min ( ) tau = ( ( 1.0 / ( 2.0 * omega ) ) * nparctan ( npsum ( npsin ( 2.0 * omega * norm_times ) ) / npsum ( npcos ( 2.0 * omega * norm_times ) ) ) ) lspval_top_cos = ( npsum ( 1.0 * npcos ( omega * ( norm_times - tau ) ) ) * npsum ( 1.0 * npcos ( omega * ( norm_times - tau ) ) ) ) lspval_bot_cos = npsum ( ( npcos ( omega * ( norm_times - tau ) ) ) * ( npcos ( omega * ( norm_times - tau ) ) ) ) lspval_top_sin = ( npsum ( 1.0 * npsin ( omega * ( norm_times - tau ) ) ) * npsum ( 1.0 * npsin ( omega * ( norm_times - tau ) ) ) ) lspval_bot_sin = npsum ( ( npsin ( omega * ( norm_times - tau ) ) ) * ( npsin ( omega * ( norm_times - tau ) ) ) ) lspval = 0.5 * ( ( lspval_top_cos / lspval_bot_cos ) + ( lspval_top_sin / lspval_bot_sin ) ) return lspval
9798	def update ( ctx , name , description , tags ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the experiment group.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . experiment_group . update_experiment_group ( user , project_name , _group , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update experiment group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiment group updated." ) get_group_details ( response )
2615	def cancel ( self , job_ids ) : for job in job_ids : logger . debug ( "Terminating job/proc_id: {0}" . format ( job ) ) if self . resources [ job ] [ 'proc' ] : proc = self . resources [ job ] [ 'proc' ] os . killpg ( os . getpgid ( proc . pid ) , signal . SIGTERM ) self . resources [ job ] [ 'status' ] = 'CANCELLED' elif self . resources [ job ] [ 'remote_pid' ] : cmd = "kill -- -$(ps -o pgid={} | grep -o '[0-9]*')" . format ( self . resources [ job ] [ 'remote_pid' ] ) retcode , stdout , stderr = self . channel . execute_wait ( cmd , self . cmd_timeout ) if retcode != 0 : logger . warning ( "Failed to kill PID: {} and child processes on {}" . format ( self . resources [ job ] [ 'remote_pid' ] , self . label ) ) rets = [ True for i in job_ids ] return rets
4768	def is_type_of ( self , some_type ) : if type ( some_type ) is not type and not issubclass ( type ( some_type ) , type ) : raise TypeError ( 'given arg must be a type' ) if type ( self . val ) is not some_type : if hasattr ( self . val , '__name__' ) : t = self . val . __name__ elif hasattr ( self . val , '__class__' ) : t = self . val . __class__ . __name__ else : t = 'unknown' self . _err ( 'Expected <%s:%s> to be of type <%s>, but was not.' % ( self . val , t , some_type . __name__ ) ) return self
8071	def not_found ( url , wait = 10 ) : try : connection = open ( url , wait ) except HTTP404NotFound : return True except : return False return False
1149	def warnpy3k ( message , category = None , stacklevel = 1 ) : if sys . py3kwarning : if category is None : category = DeprecationWarning warn ( message , category , stacklevel + 1 )
8248	def rotate_ryb ( self , angle = 180 ) : h = self . h * 360 angle = angle % 360 wheel = [ ( 0 , 0 ) , ( 15 , 8 ) , ( 30 , 17 ) , ( 45 , 26 ) , ( 60 , 34 ) , ( 75 , 41 ) , ( 90 , 48 ) , ( 105 , 54 ) , ( 120 , 60 ) , ( 135 , 81 ) , ( 150 , 103 ) , ( 165 , 123 ) , ( 180 , 138 ) , ( 195 , 155 ) , ( 210 , 171 ) , ( 225 , 187 ) , ( 240 , 204 ) , ( 255 , 219 ) , ( 270 , 234 ) , ( 285 , 251 ) , ( 300 , 267 ) , ( 315 , 282 ) , ( 330 , 298 ) , ( 345 , 329 ) , ( 360 , 0 ) ] for i in _range ( len ( wheel ) - 1 ) : x0 , y0 = wheel [ i ] x1 , y1 = wheel [ i + 1 ] if y1 < y0 : y1 += 360 if y0 <= h <= y1 : a = 1.0 * x0 + ( x1 - x0 ) * ( h - y0 ) / ( y1 - y0 ) break a = ( a + angle ) % 360 for i in _range ( len ( wheel ) - 1 ) : x0 , y0 = wheel [ i ] x1 , y1 = wheel [ i + 1 ] if y1 < y0 : y1 += 360 if x0 <= a <= x1 : h = 1.0 * y0 + ( y1 - y0 ) * ( a - x0 ) / ( x1 - x0 ) break h = h % 360 return Color ( h / 360 , self . s , self . brightness , self . a , mode = "hsb" , name = "" )
1261	def get_component ( self , component_name ) : mapping = self . get_components ( ) return mapping [ component_name ] if component_name in mapping else None
3206	def get ( self , batch_id , ** queryparams ) : self . batch_id = batch_id self . operation_status = None return self . _mc_client . _get ( url = self . _build_path ( batch_id ) , ** queryparams )
5607	def resample_from_array ( in_raster = None , in_affine = None , out_tile = None , in_crs = None , resampling = "nearest" , nodataval = 0 ) : if isinstance ( in_raster , ma . MaskedArray ) : pass if isinstance ( in_raster , np . ndarray ) : in_raster = ma . MaskedArray ( in_raster , mask = in_raster == nodataval ) elif isinstance ( in_raster , ReferencedRaster ) : in_affine = in_raster . affine in_crs = in_raster . crs in_raster = in_raster . data elif isinstance ( in_raster , tuple ) : in_raster = ma . MaskedArray ( data = np . stack ( in_raster ) , mask = np . stack ( [ band . mask if isinstance ( band , ma . masked_array ) else np . where ( band == nodataval , True , False ) for band in in_raster ] ) , fill_value = nodataval ) else : raise TypeError ( "wrong input data type: %s" % type ( in_raster ) ) if in_raster . ndim == 2 : in_raster = ma . expand_dims ( in_raster , axis = 0 ) elif in_raster . ndim == 3 : pass else : raise TypeError ( "input array must have 2 or 3 dimensions" ) if in_raster . fill_value != nodataval : ma . set_fill_value ( in_raster , nodataval ) out_shape = ( in_raster . shape [ 0 ] , ) + out_tile . shape dst_data = np . empty ( out_shape , in_raster . dtype ) in_raster = ma . masked_array ( data = in_raster . filled ( ) , mask = in_raster . mask , fill_value = nodataval ) reproject ( in_raster , dst_data , src_transform = in_affine , src_crs = in_crs if in_crs else out_tile . crs , dst_transform = out_tile . affine , dst_crs = out_tile . crs , resampling = Resampling [ resampling ] ) return ma . MaskedArray ( dst_data , mask = dst_data == nodataval )
7474	def write_to_fullarr ( data , sample , sidx ) : LOGGER . info ( "writing fullarr %s %s" , sample . name , sidx ) with h5py . File ( data . clust_database , 'r+' ) as io5 : chunk = io5 [ "catgs" ] . attrs [ "chunksize" ] [ 0 ] catg = io5 [ "catgs" ] nall = io5 [ "nalleles" ] smpio = os . path . join ( data . dirs . across , sample . name + '.tmp.h5' ) with h5py . File ( smpio ) as indat : newcatg = indat [ "icatg" ] onall = indat [ "inall" ] for cidx in xrange ( 0 , catg . shape [ 0 ] , chunk ) : end = cidx + chunk catg [ cidx : end , sidx : sidx + 1 , : ] = np . expand_dims ( newcatg [ cidx : end , : ] , axis = 1 ) nall [ : , sidx : sidx + 1 ] = np . expand_dims ( onall , axis = 1 )
1574	def add_tracker_url ( parser ) : parser . add_argument ( '--tracker_url' , metavar = '(tracker url; default: "' + DEFAULT_TRACKER_URL + '")' , type = str , default = DEFAULT_TRACKER_URL ) return parser
7556	def random_combination ( nsets , n , k ) : sets = set ( ) while len ( sets ) < nsets : newset = tuple ( sorted ( np . random . choice ( n , k , replace = False ) ) ) sets . add ( newset ) return tuple ( sets )
9655	def run_commands ( commands , settings ) : sprint = settings [ "sprint" ] quiet = settings [ "quiet" ] error = settings [ "error" ] enhanced_errors = True the_shell = None if settings [ "no_enhanced_errors" ] : enhanced_errors = False if "shell" in settings : the_shell = settings [ "shell" ] windows_p = sys . platform == "win32" STDOUT = None STDERR = None if quiet : STDOUT = PIPE STDERR = PIPE commands = commands . rstrip ( ) sprint ( "About to run commands '{}'" . format ( commands ) , level = "verbose" ) if not quiet : sprint ( commands ) if the_shell : tmp = shlex . split ( the_shell ) the_shell = tmp [ 0 ] tmp = tmp [ 1 : ] if enhanced_errors and not windows_p : tmp . append ( "-e" ) tmp . append ( commands ) commands = tmp else : if enhanced_errors and not windows_p : commands = [ "-e" , commands ] p = Popen ( commands , shell = True , stdout = STDOUT , stderr = STDERR , executable = the_shell ) out , err = p . communicate ( ) if p . returncode : if quiet : error ( err . decode ( locale . getpreferredencoding ( ) ) ) error ( "Command failed to run" ) sys . exit ( 1 )
1854	def SHLD ( cpu , dest , src , count ) : OperandSize = dest . size tempCount = Operators . ZEXTEND ( count . read ( ) , OperandSize ) & ( OperandSize - 1 ) arg0 = dest . read ( ) arg1 = src . read ( ) MASK = ( ( 1 << OperandSize ) - 1 ) t0 = ( arg0 << tempCount ) t1 = arg1 >> ( OperandSize - tempCount ) res = Operators . ITEBV ( OperandSize , tempCount == 0 , arg0 , t0 | t1 ) res = res & MASK dest . write ( res ) if isinstance ( tempCount , int ) and tempCount == 0 : pass else : SIGN_MASK = 1 << ( OperandSize - 1 ) lastbit = 0 != ( ( arg0 << ( tempCount - 1 ) ) & SIGN_MASK ) cpu . _set_shiftd_flags ( OperandSize , arg0 , res , lastbit , tempCount )
8006	def handle_read ( self ) : with self . _lock : logger . debug ( "handle_read()" ) if self . _socket is None : return while True : try : sock , address = self . _socket . accept ( ) except socket . error , err : if err . args [ 0 ] in BLOCKING_ERRORS : break else : raise logger . debug ( "Accepted connection from: {0!r}" . format ( address ) ) self . _target ( sock , address )
10957	def _calc_loglikelihood ( self , model = None , tile = None ) : if model is None : res = self . residuals else : res = model - self . _data [ tile . slicer ] sig , isig = self . sigma , 1.0 / self . sigma nlogs = - np . log ( np . sqrt ( 2 * np . pi ) * sig ) * res . size return - 0.5 * isig * isig * np . dot ( res . flat , res . flat ) + nlogs
8155	def create_table ( self , name , fields = [ ] , key = "id" ) : for f in fields : if f == key : fields . remove ( key ) sql = "create table " + name + " " sql += "(" + key + " integer primary key" for f in fields : sql += ", " + f + " varchar(255)" sql += ")" self . _cur . execute ( sql ) self . _con . commit ( ) self . index ( name , key , unique = True ) self . connect ( self . _name )
6408	def lmean ( nums ) : r if len ( nums ) != len ( set ( nums ) ) : raise AttributeError ( 'No two values in the nums list may be equal' ) rolling_sum = 0 for i in range ( len ( nums ) ) : rolling_prod = 1 for j in range ( len ( nums ) ) : if i != j : rolling_prod *= math . log ( nums [ i ] / nums [ j ] ) rolling_sum += nums [ i ] / rolling_prod return math . factorial ( len ( nums ) - 1 ) * rolling_sum
11338	def schedule_mode ( self , mode ) : modes = [ config . SCHEDULE_RUN , config . SCHEDULE_TEMPORARY_HOLD , config . SCHEDULE_HOLD ] if mode not in modes : raise Exception ( "Invalid mode. Please use one of: {}" . format ( modes ) ) self . set_data ( { "ScheduleMode" : mode } )
4609	def recover_public_key ( digest , signature , i , message = None ) : curve = ecdsa . SECP256k1 . curve G = ecdsa . SECP256k1 . generator order = ecdsa . SECP256k1 . order yp = i % 2 r , s = ecdsa . util . sigdecode_string ( signature , order ) x = r + ( i // 2 ) * order alpha = ( ( x * x * x ) + ( curve . a ( ) * x ) + curve . b ( ) ) % curve . p ( ) beta = ecdsa . numbertheory . square_root_mod_prime ( alpha , curve . p ( ) ) y = beta if ( beta - yp ) % 2 == 0 else curve . p ( ) - beta R = ecdsa . ellipticcurve . Point ( curve , x , y , order ) e = ecdsa . util . string_to_number ( digest ) Q = ecdsa . numbertheory . inverse_mod ( r , order ) * ( s * R + ( - e % order ) * G ) if SECP256K1_MODULE == "cryptography" and message is not None : if not isinstance ( message , bytes ) : message = bytes ( message , "utf-8" ) sigder = encode_dss_signature ( r , s ) public_key = ec . EllipticCurvePublicNumbers ( Q . _Point__x , Q . _Point__y , ec . SECP256K1 ( ) ) . public_key ( default_backend ( ) ) public_key . verify ( sigder , message , ec . ECDSA ( hashes . SHA256 ( ) ) ) return public_key else : if not ecdsa . VerifyingKey . from_public_point ( Q , curve = ecdsa . SECP256k1 ) . verify_digest ( signature , digest , sigdecode = ecdsa . util . sigdecode_string ) : return None return ecdsa . VerifyingKey . from_public_point ( Q , curve = ecdsa . SECP256k1 )
12394	def try_delegation ( method ) : @ functools . wraps ( method ) def delegator ( self , * args , ** kwargs ) : if self . try_delegation : inst = getattr ( self , 'inst' , None ) if inst is not None : method_name = ( self . delegator_prefix or '' ) + method . __name__ func = getattr ( inst , method_name , None ) if func is not None : return func ( * args , ** kwargs ) return method ( self , * args , ** kwargs ) return delegator
13630	def _renderResource ( resource , request ) : meth = getattr ( resource , 'render_' + nativeString ( request . method ) , None ) if meth is None : try : allowedMethods = resource . allowedMethods except AttributeError : allowedMethods = _computeAllowedMethods ( resource ) raise UnsupportedMethod ( allowedMethods ) return meth ( request )
12825	def handle_extends ( self , text ) : match = self . re_extends . match ( text ) if match : extra_text = self . re_extends . sub ( '' , text , count = 1 ) blocks = self . get_blocks ( extra_text ) path = os . path . join ( self . base_dir , match . group ( 'path' ) ) with open ( path , encoding = 'utf-8' ) as fp : return self . replace_blocks_in_extends ( fp . read ( ) , blocks ) else : return None
3674	def draw_3d ( self , width = 300 , height = 500 , style = 'stick' , Hs = True ) : r try : import py3Dmol from IPython . display import display if Hs : mol = self . rdkitmol_Hs else : mol = self . rdkitmol AllChem . EmbedMultipleConfs ( mol ) mb = Chem . MolToMolBlock ( mol ) p = py3Dmol . view ( width = width , height = height ) p . addModel ( mb , 'sdf' ) p . setStyle ( { style : { } } ) p . zoomTo ( ) display ( p . show ( ) ) except : return 'py3Dmol, RDKit, and IPython are required for this feature.'
13242	def period ( self ) : start_time = self . root . findtext ( 'daily_start_time' ) if start_time : return Period ( text_to_time ( start_time ) , text_to_time ( self . root . findtext ( 'daily_end_time' ) ) ) return Period ( datetime . time ( 0 , 0 ) , datetime . time ( 23 , 59 ) )
12627	def iter_recursive_find ( folder_path , * regex ) : for root , dirs , files in os . walk ( folder_path ) : if len ( files ) > 0 : outlist = [ ] for f in files : for reg in regex : if re . search ( reg , f ) : outlist . append ( op . join ( root , f ) ) if len ( outlist ) == len ( regex ) : yield outlist
8578	def list_servers ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
8373	def widget_changed ( self , widget , v ) : if v . type is NUMBER : self . bot . _namespace [ v . name ] = widget . get_value ( ) self . bot . _vars [ v . name ] . value = widget . get_value ( ) publish_event ( VARIABLE_UPDATED_EVENT , v ) elif v . type is BOOLEAN : self . bot . _namespace [ v . name ] = widget . get_active ( ) self . bot . _vars [ v . name ] . value = widget . get_active ( ) publish_event ( VARIABLE_UPDATED_EVENT , v ) elif v . type is TEXT : self . bot . _namespace [ v . name ] = widget . get_text ( ) self . bot . _vars [ v . name ] . value = widget . get_text ( ) publish_event ( VARIABLE_UPDATED_EVENT , v )
13284	def list_from_document ( cls , doc ) : objs = [ ] for feu in doc . xpath ( '//FEU' ) : detail_els = feu . xpath ( 'event-element-details/event-element-detail' ) for idx , detail in enumerate ( detail_els ) : objs . append ( cls ( feu , detail , id_suffix = idx , number_in_group = len ( detail_els ) ) ) return objs
10575	def get_local_playlist_songs ( playlist , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False , exclude_patterns = None ) : logger . info ( "Loading local playlist songs..." ) if os . name == 'nt' and CYGPATH_RE . match ( playlist ) : playlist = convert_cygwin_path ( playlist ) filepaths = [ ] base_filepath = os . path . dirname ( os . path . abspath ( playlist ) ) with open ( playlist ) as local_playlist : for line in local_playlist . readlines ( ) : line = line . strip ( ) if line . lower ( ) . endswith ( SUPPORTED_SONG_FORMATS ) : path = line if not os . path . isabs ( path ) : path = os . path . join ( base_filepath , path ) if os . path . isfile ( path ) : filepaths . append ( path ) supported_filepaths = get_supported_filepaths ( filepaths , SUPPORTED_SONG_FORMATS ) included_songs , excluded_songs = exclude_filepaths ( supported_filepaths , exclude_patterns = exclude_patterns ) matched_songs , filtered_songs = filter_local_songs ( included_songs , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) logger . info ( "Excluded {0} local playlist songs" . format ( len ( excluded_songs ) ) ) logger . info ( "Filtered {0} local playlist songs" . format ( len ( filtered_songs ) ) ) logger . info ( "Loaded {0} local playlist songs" . format ( len ( matched_songs ) ) ) return matched_songs , filtered_songs , excluded_songs
2489	def create_extracted_license ( self , lic ) : licenses = list ( self . graph . triples ( ( None , self . spdx_namespace . licenseId , lic . identifier ) ) ) if len ( licenses ) != 0 : return licenses [ 0 ] [ 0 ] else : license_node = BNode ( ) type_triple = ( license_node , RDF . type , self . spdx_namespace . ExtractedLicensingInfo ) self . graph . add ( type_triple ) ident_triple = ( license_node , self . spdx_namespace . licenseId , Literal ( lic . identifier ) ) self . graph . add ( ident_triple ) text_triple = ( license_node , self . spdx_namespace . extractedText , Literal ( lic . text ) ) self . graph . add ( text_triple ) if lic . full_name is not None : name_triple = ( license_node , self . spdx_namespace . licenseName , self . to_special_value ( lic . full_name ) ) self . graph . add ( name_triple ) for ref in lic . cross_ref : triple = ( license_node , RDFS . seeAlso , URIRef ( ref ) ) self . graph . add ( triple ) if lic . comment is not None : comment_triple = ( license_node , RDFS . comment , Literal ( lic . comment ) ) self . graph . add ( comment_triple ) return license_node
8150	def _frame_limit ( self , start_time ) : if self . _speed : completion_time = time ( ) exc_time = completion_time - start_time sleep_for = ( 1.0 / abs ( self . _speed ) ) - exc_time if sleep_for > 0 : sleep ( sleep_for )
4236	def login ( self ) : if not self . force_login_v2 : v1_result = self . login_v1 ( ) if v1_result : return v1_result return self . login_v2 ( )
8162	def publish_event ( event_t , data = None , extra_channels = None , wait = None ) : event = Event ( event_t , data ) pubsub . publish ( "shoebot" , event ) for channel_name in extra_channels or [ ] : pubsub . publish ( channel_name , event ) if wait is not None : channel = pubsub . subscribe ( wait ) channel . listen ( wait )
11295	def make_request_data ( self , zipcode , city , state ) : data = { 'key' : self . api_key , 'postalcode' : str ( zipcode ) , 'city' : city , 'state' : state } data = ZipTaxClient . _clean_request_data ( data ) return data
8696	def set_timeout ( self , timeout ) : timeout = int ( timeout ) self . _timeout = timeout == 0 and 999999 or timeout
12463	def print_error ( message , wrap = True ) : if wrap : message = 'ERROR: {0}. Exit...' . format ( message . rstrip ( '.' ) ) colorizer = ( _color_wrap ( colorama . Fore . RED ) if colorama else lambda message : message ) return print ( colorizer ( message ) , file = sys . stderr )
3966	def case_insensitive_rename ( src , dst ) : temp_dir = tempfile . mkdtemp ( ) shutil . rmtree ( temp_dir ) shutil . move ( src , temp_dir ) shutil . move ( temp_dir , dst )
4060	def item_template ( self , itemtype ) : template_name = "item_template_" + itemtype query_string = "/items/new?itemType={i}" . format ( i = itemtype ) if self . templates . get ( template_name ) and not self . _updated ( query_string , self . templates [ template_name ] , template_name ) : return copy . deepcopy ( self . templates [ template_name ] [ "tmplt" ] ) retrieved = self . _retrieve_data ( query_string ) return self . _cache ( retrieved , template_name )
7310	def without_tz ( request ) : t = Template ( '{% load tz %}{% get_current_timezone as TIME_ZONE %}{{ TIME_ZONE }}' ) c = RequestContext ( request ) response = t . render ( c ) return HttpResponse ( response )
8670	def get_key ( key_name , value_name , jsonify , no_decrypt , stash , passphrase , backend ) : if value_name and no_decrypt : sys . exit ( 'VALUE_NAME cannot be used in conjuction with --no-decrypt' ) stash = _get_stash ( backend , stash , passphrase , quiet = jsonify or value_name ) try : key = stash . get ( key_name = key_name , decrypt = not no_decrypt ) except GhostError as ex : sys . exit ( ex ) if not key : sys . exit ( 'Key `{0}` not found' . format ( key_name ) ) if value_name : key = key [ 'value' ] . get ( value_name ) if not key : sys . exit ( 'Value name `{0}` could not be found under key `{1}`' . format ( value_name , key_name ) ) if jsonify or value_name : click . echo ( json . dumps ( key , indent = 4 , sort_keys = False ) . strip ( '"' ) , nl = True ) else : click . echo ( 'Retrieving key...' ) click . echo ( '\n' + _prettify_dict ( key ) )
8854	def on_save_as ( self ) : path = self . tabWidget . current_widget ( ) . file . path path = os . path . dirname ( path ) if path else '' filename , filter = QtWidgets . QFileDialog . getSaveFileName ( self , 'Save' , path ) if filename : self . tabWidget . save_current ( filename ) self . recent_files_manager . open_file ( filename ) self . menu_recents . update_actions ( ) self . actionRun . setEnabled ( True ) self . actionConfigure_run . setEnabled ( True ) self . _update_status_bar ( self . tabWidget . current_widget ( ) )
1685	def RepositoryName ( self ) : r fullname = self . FullName ( ) if os . path . exists ( fullname ) : project_dir = os . path . dirname ( fullname ) if _repository : repo = FileInfo ( _repository ) . FullName ( ) root_dir = project_dir while os . path . exists ( root_dir ) : if os . path . normcase ( root_dir ) == os . path . normcase ( repo ) : return os . path . relpath ( fullname , root_dir ) . replace ( '\\' , '/' ) one_up_dir = os . path . dirname ( root_dir ) if one_up_dir == root_dir : break root_dir = one_up_dir if os . path . exists ( os . path . join ( project_dir , ".svn" ) ) : root_dir = project_dir one_up_dir = os . path . dirname ( root_dir ) while os . path . exists ( os . path . join ( one_up_dir , ".svn" ) ) : root_dir = os . path . dirname ( root_dir ) one_up_dir = os . path . dirname ( one_up_dir ) prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) return fullname [ len ( prefix ) + 1 : ] root_dir = current_dir = os . path . dirname ( fullname ) while current_dir != os . path . dirname ( current_dir ) : if ( os . path . exists ( os . path . join ( current_dir , ".git" ) ) or os . path . exists ( os . path . join ( current_dir , ".hg" ) ) or os . path . exists ( os . path . join ( current_dir , ".svn" ) ) ) : root_dir = current_dir current_dir = os . path . dirname ( current_dir ) if ( os . path . exists ( os . path . join ( root_dir , ".git" ) ) or os . path . exists ( os . path . join ( root_dir , ".hg" ) ) or os . path . exists ( os . path . join ( root_dir , ".svn" ) ) ) : prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) return fullname [ len ( prefix ) + 1 : ] return fullname
6538	def compile_masks ( masks ) : if not masks : masks = [ ] elif not isinstance ( masks , ( list , tuple ) ) : masks = [ masks ] return [ re . compile ( mask ) for mask in masks ]
1934	def get_constructor_arguments ( self ) -> str : item = self . _constructor_abi_item return '()' if item is None else self . tuple_signature_for_components ( item [ 'inputs' ] )
1365	def get_required_arguments_metricnames ( self ) : try : metricnames = self . get_arguments ( constants . PARAM_METRICNAME ) if not metricnames : raise tornado . web . MissingArgumentError ( constants . PARAM_METRICNAME ) return metricnames except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
11608	def add ( self , addend_mat , axis = 1 ) : if self . finalized : if axis == 0 : raise NotImplementedError ( 'The method is not yet implemented for the axis.' ) elif axis == 1 : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] + addend_mat elif axis == 2 : raise NotImplementedError ( 'The method is not yet implemented for the axis.' ) else : raise RuntimeError ( 'The axis should be 0, 1, or 2.' ) else : raise RuntimeError ( 'The original matrix must be finalized.' )
3325	def _generate_lock ( self , principal , lock_type , lock_scope , lock_depth , lock_owner , path , timeout ) : if timeout is None : timeout = LockManager . LOCK_TIME_OUT_DEFAULT elif timeout < 0 : timeout = - 1 lock_dict = { "root" : path , "type" : lock_type , "scope" : lock_scope , "depth" : lock_depth , "owner" : lock_owner , "timeout" : timeout , "principal" : principal , } self . storage . create ( path , lock_dict ) return lock_dict
997	def printState ( self , aState ) : def formatRow ( var , i ) : s = '' for c in range ( self . numberOfCols ) : if c > 0 and c % 10 == 0 : s += ' ' s += str ( var [ c , i ] ) s += ' ' return s for i in xrange ( self . cellsPerColumn ) : print formatRow ( aState , i )
10776	def finalize ( self , result = None ) : if not self . settings_path : return from django . test . utils import teardown_test_environment from django . db import connection from django . conf import settings self . call_plugins_method ( 'beforeDestroyTestDb' , settings , connection ) try : connection . creation . destroy_test_db ( self . old_db , verbosity = self . verbosity , ) except Exception : pass self . call_plugins_method ( 'afterDestroyTestDb' , settings , connection ) self . call_plugins_method ( 'beforeTeardownTestEnv' , settings , teardown_test_environment ) teardown_test_environment ( ) self . call_plugins_method ( 'afterTeardownTestEnv' , settings )
9482	def to_bytes_35 ( self , previous : bytes ) : bc = b"" it_bc = util . generate_bytecode_from_obb ( self . iterator , previous ) bc += it_bc bc += util . generate_bytecode_from_obb ( tokens . GET_ITER , b"" ) prev_len = len ( previous ) + len ( bc ) body_bc = b"" for op in self . _body : padded_bc = previous padded_bc += b"\x00\x00\x00" padded_bc += bc padded_bc += b"\x00\x00\x00" padded_bc += body_bc body_bc += util . generate_bytecode_from_obb ( op , padded_bc ) body_bc += util . generate_simple_call ( tokens . JUMP_ABSOLUTE , prev_len + 3 ) body_bc += util . generate_bytecode_from_obb ( tokens . POP_BLOCK , b"" ) body_bc = util . generate_simple_call ( tokens . FOR_ITER , len ( body_bc ) - 1 ) + body_bc bc = util . generate_simple_call ( tokens . SETUP_LOOP , prev_len + len ( body_bc ) - 6 ) + bc + body_bc return bc
11451	def get_collection ( self , journal ) : conference = '' for tag in self . document . getElementsByTagName ( 'conference' ) : conference = xml_to_text ( tag ) if conference or journal == "International Journal of Modern Physics: Conference Series" : return [ ( 'a' , 'HEP' ) , ( 'a' , 'ConferencePaper' ) ] elif self . _get_article_type ( ) == "review-article" : return [ ( 'a' , 'HEP' ) , ( 'a' , 'Review' ) ] else : return [ ( 'a' , 'HEP' ) , ( 'a' , 'Published' ) ]
6346	def _language ( self , name , name_mode ) : name = name . strip ( ) . lower ( ) rules = BMDATA [ name_mode ] [ 'language_rules' ] all_langs = ( sum ( _LANG_DICT [ _ ] for _ in BMDATA [ name_mode ] [ 'languages' ] ) - 1 ) choices_remaining = all_langs for rule in rules : letters , languages , accept = rule if search ( letters , name ) is not None : if accept : choices_remaining &= languages else : choices_remaining &= ( ~ languages ) % ( all_langs + 1 ) if choices_remaining == L_NONE : choices_remaining = L_ANY return choices_remaining
4981	def set_final_prices ( self , modes , request ) : result = [ ] for mode in modes : if mode [ 'premium' ] : mode [ 'final_price' ] = EcommerceApiClient ( request . user ) . get_course_final_price ( mode = mode , enterprise_catalog_uuid = request . GET . get ( 'catalog' ) if request . method == 'GET' else None , ) result . append ( mode ) return result
4019	def _dusty_vm_exists ( ) : existing_vms = check_output_demoted ( [ 'VBoxManage' , 'list' , 'vms' ] ) for line in existing_vms . splitlines ( ) : if '"{}"' . format ( constants . VM_MACHINE_NAME ) in line : return True return False
6513	def _most_popular_gender ( self , name , counter ) : if name not in self . names : return self . unknown_value max_count , max_tie = ( 0 , 0 ) best = self . names [ name ] . keys ( ) [ 0 ] for gender , country_values in self . names [ name ] . items ( ) : count , tie = counter ( country_values ) if count > max_count or ( count == max_count and tie > max_tie ) : max_count , max_tie , best = count , tie , gender return best if max_count > 0 else self . unknown_value
12323	def api_call_action ( func ) : def _inner ( * args , ** kwargs ) : return func ( * args , ** kwargs ) _inner . __name__ = func . __name__ _inner . __doc__ = func . __doc__ return _inner
3021	def get_access_token ( self , http = None , additional_claims = None ) : if additional_claims is None : if self . access_token is None or self . access_token_expired : self . refresh ( None ) return client . AccessTokenInfo ( access_token = self . access_token , expires_in = self . _expires_in ( ) ) else : token , unused_expiry = self . _create_token ( additional_claims ) return client . AccessTokenInfo ( access_token = token , expires_in = self . _MAX_TOKEN_LIFETIME_SECS )
11840	def score ( self ) : "The total score for the words found, according to the rules." return sum ( [ self . scores [ len ( w ) ] for w in self . words ( ) ] )
3109	def locked_put ( self , credentials ) : entity , _ = self . model_class . objects . get_or_create ( ** { self . key_name : self . key_value } ) setattr ( entity , self . property_name , credentials ) entity . save ( )
3190	def update ( self , list_id , segment_id , data ) : self . list_id = list_id self . segment_id = segment_id if 'name' not in data : raise KeyError ( 'The list segment must have a name' ) return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'segments' , segment_id ) , data = data )
2211	def inject_method ( self , func , name = None ) : new_method = func . __get__ ( self , self . __class__ ) if name is None : name = func . __name__ setattr ( self , name , new_method )
4044	def publications ( self ) : if self . library_type != "users" : raise ze . CallDoesNotExist ( "This API call does not exist for group libraries" ) query_string = "/{t}/{u}/publications/items" return self . _build_query ( query_string )
2017	def DIV ( self , a , b ) : try : result = Operators . UDIV ( a , b ) except ZeroDivisionError : result = 0 return Operators . ITEBV ( 256 , b == 0 , 0 , result )
11331	def progress ( length , ** kwargs ) : quiet = False progress_class = kwargs . pop ( "progress_class" , Progress ) kwargs [ "write_method" ] = istdout . info kwargs [ "width" ] = kwargs . get ( "width" , globals ( ) [ "WIDTH" ] ) kwargs [ "length" ] = length pbar = progress_class ( ** kwargs ) pbar . update ( 0 ) yield pbar pbar . update ( length ) br ( )
228	def get_long_short_pos ( positions ) : pos_wo_cash = positions . drop ( 'cash' , axis = 1 ) longs = pos_wo_cash [ pos_wo_cash > 0 ] . sum ( axis = 1 ) . fillna ( 0 ) shorts = pos_wo_cash [ pos_wo_cash < 0 ] . sum ( axis = 1 ) . fillna ( 0 ) cash = positions . cash net_liquidation = longs + shorts + cash df_pos = pd . DataFrame ( { 'long' : longs . divide ( net_liquidation , axis = 'index' ) , 'short' : shorts . divide ( net_liquidation , axis = 'index' ) } ) df_pos [ 'net exposure' ] = df_pos [ 'long' ] + df_pos [ 'short' ] return df_pos
4942	def enterprise_customer_uuid ( self ) : try : enterprise_user = EnterpriseCustomerUser . objects . get ( user_id = self . user . id ) except ObjectDoesNotExist : LOGGER . warning ( 'User {} has a {} assignment but is not linked to an enterprise!' . format ( self . __class__ , self . user . id ) ) return None except MultipleObjectsReturned : LOGGER . warning ( 'User {} is linked to multiple enterprises, which is not yet supported!' . format ( self . user . id ) ) return None return str ( enterprise_user . enterprise_customer . uuid )
4527	def report ( function , * args , ** kwds ) : try : function ( * args , ** kwds ) except Exception : traceback . print_exc ( )
8561	def get_loadbalancer ( self , datacenter_id , loadbalancer_id ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) ) return response
3364	def load_yaml_model ( filename ) : if isinstance ( filename , string_types ) : with io . open ( filename , "r" ) as file_handle : return model_from_dict ( yaml . load ( file_handle ) ) else : return model_from_dict ( yaml . load ( filename ) )
2130	def list ( self , ** kwargs ) : data , self . endpoint = self . data_endpoint ( kwargs ) r = super ( Resource , self ) . list ( ** data ) self . configure_display ( r ) return r
1009	def compute ( self , bottomUpInput , enableLearn , enableInference = None ) : if enableInference is None : if enableLearn : enableInference = False else : enableInference = True assert ( enableLearn or enableInference ) activeColumns = bottomUpInput . nonzero ( ) [ 0 ] if enableLearn : self . lrnIterationIdx += 1 self . iterationIdx += 1 if self . verbosity >= 3 : print "\n==== PY Iteration: %d =====" % ( self . iterationIdx ) print "Active cols:" , activeColumns if enableLearn : if self . lrnIterationIdx in Segment . dutyCycleTiers : for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : for segment in self . cells [ c ] [ i ] : segment . dutyCycle ( ) if self . avgInputDensity is None : self . avgInputDensity = len ( activeColumns ) else : self . avgInputDensity = ( 0.99 * self . avgInputDensity + 0.01 * len ( activeColumns ) ) if enableInference : self . _updateInferenceState ( activeColumns ) if enableLearn : self . _updateLearningState ( activeColumns ) if self . globalDecay > 0.0 and ( ( self . lrnIterationIdx % self . maxAge ) == 0 ) : for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : segsToDel = [ ] for segment in self . cells [ c ] [ i ] : age = self . lrnIterationIdx - segment . lastActiveIteration if age <= self . maxAge : continue synsToDel = [ ] for synapse in segment . syns : synapse [ 2 ] = synapse [ 2 ] - self . globalDecay if synapse [ 2 ] <= 0 : synsToDel . append ( synapse ) if len ( synsToDel ) == segment . getNumSynapses ( ) : segsToDel . append ( segment ) elif len ( synsToDel ) > 0 : for syn in synsToDel : segment . syns . remove ( syn ) for seg in segsToDel : self . _cleanUpdatesList ( c , i , seg ) self . cells [ c ] [ i ] . remove ( seg ) if self . collectStats : if enableInference : predictedState = self . infPredictedState [ 't-1' ] else : predictedState = self . lrnPredictedState [ 't-1' ] self . _updateStatsInferEnd ( self . _internalStats , activeColumns , predictedState , self . colConfidence [ 't-1' ] ) output = self . _computeOutput ( ) self . printComputeEnd ( output , learn = enableLearn ) self . resetCalled = False return output
9314	def _format_datetime ( dttm ) : if dttm . tzinfo is None or dttm . tzinfo . utcoffset ( dttm ) is None : zoned = pytz . utc . localize ( dttm ) else : zoned = dttm . astimezone ( pytz . utc ) ts = zoned . strftime ( "%Y-%m-%dT%H:%M:%S" ) ms = zoned . strftime ( "%f" ) precision = getattr ( dttm , "precision" , None ) if precision == "second" : pass elif precision == "millisecond" : ts = ts + "." + ms [ : 3 ] elif zoned . microsecond > 0 : ts = ts + "." + ms . rstrip ( "0" ) return ts + "Z"
13057	def get_locale ( self ) : best_match = request . accept_languages . best_match ( [ 'de' , 'fr' , 'en' , 'la' ] ) if best_match is None : if len ( request . accept_languages ) > 0 : best_match = request . accept_languages [ 0 ] [ 0 ] [ : 2 ] else : return self . __default_lang__ lang = self . __default_lang__ if best_match == "de" : lang = "ger" elif best_match == "fr" : lang = "fre" elif best_match == "en" : lang = "eng" elif best_match == "la" : lang = "lat" return lang
13786	def require ( name , field , data_type ) : if not isinstance ( field , data_type ) : msg = '{0} must have {1}, got: {2}' . format ( name , data_type , field ) raise AssertionError ( msg )
6375	def stem ( self , word ) : word = normalize ( 'NFC' , text_type ( word . lower ( ) ) ) word = word . translate ( self . _umlauts ) wlen = len ( word ) - 1 if wlen > 3 : if wlen > 5 : if word [ - 3 : ] == 'nen' : return word [ : - 3 ] if wlen > 4 : if word [ - 2 : ] in { 'en' , 'se' , 'es' , 'er' } : return word [ : - 2 ] if word [ - 1 ] in { 'e' , 'n' , 'r' , 's' } : return word [ : - 1 ] return word
13176	def get_observations ( self ) : if self . empty : return [ ] rows = list ( self . tbody ) observations = [ ] for row_observation , row_details in zip ( rows [ : : 2 ] , rows [ 1 : : 2 ] ) : data = { } cells = OBSERVATION_XPATH ( row_observation ) data [ 'name' ] = _clean_cell ( cells [ 0 ] ) data [ 'date' ] = _clean_cell ( cells [ 1 ] ) data [ 'magnitude' ] = _clean_cell ( cells [ 3 ] ) data [ 'obscode' ] = _clean_cell ( cells [ 6 ] ) cells = DETAILS_XPATH ( row_details ) data [ 'comp1' ] = _clean_cell ( cells [ 0 ] ) data [ 'chart' ] = _clean_cell ( cells [ 3 ] ) . replace ( 'None' , '' ) data [ 'comment_code' ] = _clean_cell ( cells [ 4 ] ) data [ 'notes' ] = _clean_cell ( cells [ 5 ] ) observations . append ( data ) return observations
11962	def _dot_to_dec ( ip , check = True ) : if check and not is_dot ( ip ) : raise ValueError ( '_dot_to_dec: invalid IP: "%s"' % ip ) octets = str ( ip ) . split ( '.' ) dec = 0 dec |= int ( octets [ 0 ] ) << 24 dec |= int ( octets [ 1 ] ) << 16 dec |= int ( octets [ 2 ] ) << 8 dec |= int ( octets [ 3 ] ) return dec
12737	def parse_amc ( source ) : lines = 0 frames = 1 frame = { } degrees = False for line in source : lines += 1 line = line . split ( '#' ) [ 0 ] . strip ( ) if not line : continue if line . startswith ( ':' ) : if line . lower ( ) . startswith ( ':deg' ) : degrees = True continue if line . isdigit ( ) : if int ( line ) != frames : raise RuntimeError ( 'frame mismatch on line {}: ' 'produced {} but file claims {}' . format ( lines , frames , line ) ) yield frame frames += 1 frame = { } continue fields = line . split ( ) frame [ fields [ 0 ] ] = list ( map ( float , fields [ 1 : ] ) )
10342	def overlay_data ( graph : BELGraph , data : Mapping [ BaseEntity , Any ] , label : Optional [ str ] = None , overwrite : bool = False , ) -> None : if label is None : label = WEIGHT for node , value in data . items ( ) : if node not in graph : log . debug ( '%s not in graph' , node ) continue if label in graph . nodes [ node ] and not overwrite : log . debug ( '%s already on %s' , label , node ) continue graph . nodes [ node ] [ label ] = value
3897	def generate_message_doc ( message_descriptor , locations , path , name_prefix = '' ) : prefixed_name = name_prefix + message_descriptor . name print ( make_subsection ( prefixed_name ) ) location = locations [ path ] if location . HasField ( 'leading_comments' ) : print ( textwrap . dedent ( location . leading_comments ) ) row_tuples = [ ] for field_index , field in enumerate ( message_descriptor . field ) : field_location = locations [ path + ( 2 , field_index ) ] if field . type not in [ 11 , 14 ] : type_str = TYPE_TO_STR [ field . type ] else : type_str = make_link ( field . type_name . lstrip ( '.' ) ) row_tuples . append ( ( make_code ( field . name ) , field . number , type_str , LABEL_TO_STR [ field . label ] , textwrap . fill ( get_comment_from_location ( field_location ) , INFINITY ) , ) ) print_table ( ( 'Field' , 'Number' , 'Type' , 'Label' , 'Description' ) , row_tuples ) nested_types = enumerate ( message_descriptor . nested_type ) for index , nested_message_desc in nested_types : generate_message_doc ( nested_message_desc , locations , path + ( 3 , index ) , name_prefix = prefixed_name + '.' ) for index , nested_enum_desc in enumerate ( message_descriptor . enum_type ) : generate_enum_doc ( nested_enum_desc , locations , path + ( 4 , index ) , name_prefix = prefixed_name + '.' )
2767	def get_all_volumes ( self , region = None ) : if region : url = "volumes?region={}" . format ( region ) else : url = "volumes" data = self . get_data ( url ) volumes = list ( ) for jsoned in data [ 'volumes' ] : volume = Volume ( ** jsoned ) volume . token = self . token volumes . append ( volume ) return volumes
13155	def nt_cursor ( func ) : @ wraps ( func ) def wrapper ( cls , * args , ** kwargs ) : with ( yield from cls . get_cursor ( _CursorType . NAMEDTUPLE ) ) as c : return ( yield from func ( cls , c , * args , ** kwargs ) ) return wrapper
13239	def intervals ( self , range_start = datetime . datetime . min , range_end = datetime . datetime . max ) : current_period = None max_continuous_days = 60 range_start = self . to_timezone ( range_start ) range_end = self . to_timezone ( range_end ) for period in self . _daily_periods ( range_start . date ( ) , range_end . date ( ) ) : if period . end < range_start or period . start > range_end : continue if current_period is None : current_period = period else : if ( ( ( period . start < current_period . end ) or ( period . start - current_period . end ) <= datetime . timedelta ( minutes = 1 ) ) and ( current_period . end - current_period . start ) < datetime . timedelta ( days = max_continuous_days ) ) : current_period = Period ( current_period . start , period . end ) else : yield current_period current_period = period if current_period : yield current_period
12876	def many ( parser ) : results = [ ] terminate = object ( ) while local_ps . value : result = optional ( parser , terminate ) if result == terminate : break results . append ( result ) return results
8082	def relcurveto ( self , h1x , h1y , h2x , h2y , x , y ) : if self . _path is None : raise ShoebotError ( _ ( "No current path. Use beginpath() first." ) ) self . _path . relcurveto ( h1x , h1y , h2x , h2y , x , y )
2194	def encoding ( self ) : if self . redirect is not None : return self . redirect . encoding else : return super ( TeeStringIO , self ) . encoding
10324	def microcanonical_averages_arrays ( microcanonical_averages ) : ret = dict ( ) for n , microcanonical_average in enumerate ( microcanonical_averages ) : assert n == microcanonical_average [ 'n' ] if n == 0 : num_edges = microcanonical_average [ 'M' ] num_sites = microcanonical_average [ 'N' ] spanning_cluster = ( 'spanning_cluster' in microcanonical_average ) ret [ 'max_cluster_size' ] = np . empty ( num_edges + 1 ) ret [ 'max_cluster_size_ci' ] = np . empty ( ( num_edges + 1 , 2 ) ) if spanning_cluster : ret [ 'spanning_cluster' ] = np . empty ( num_edges + 1 ) ret [ 'spanning_cluster_ci' ] = np . empty ( ( num_edges + 1 , 2 ) ) ret [ 'moments' ] = np . empty ( ( 5 , num_edges + 1 ) ) ret [ 'moments_ci' ] = np . empty ( ( 5 , num_edges + 1 , 2 ) ) ret [ 'max_cluster_size' ] [ n ] = microcanonical_average [ 'max_cluster_size' ] ret [ 'max_cluster_size_ci' ] [ n ] = ( microcanonical_average [ 'max_cluster_size_ci' ] ) if spanning_cluster : ret [ 'spanning_cluster' ] [ n ] = ( microcanonical_average [ 'spanning_cluster' ] ) ret [ 'spanning_cluster_ci' ] [ n ] = ( microcanonical_average [ 'spanning_cluster_ci' ] ) ret [ 'moments' ] [ : , n ] = microcanonical_average [ 'moments' ] ret [ 'moments_ci' ] [ : , n ] = microcanonical_average [ 'moments_ci' ] for key in ret : if 'spanning_cluster' in key : continue ret [ key ] /= num_sites ret [ 'M' ] = num_edges ret [ 'N' ] = num_sites return ret
12986	def toBytes ( self , value ) : if type ( value ) == bytes : return value return value . encode ( self . getEncoding ( ) )
2770	def get_firewall ( self , firewall_id ) : return Firewall . get_object ( api_token = self . token , firewall_id = firewall_id , )
2268	def to_dict ( self ) : return self . _base ( ( key , ( value . to_dict ( ) if isinstance ( value , AutoDict ) else value ) ) for key , value in self . items ( ) )
12653	def call_dcm2nii ( work_dir , arguments = '' ) : if not op . exists ( work_dir ) : raise IOError ( 'Folder {} not found.' . format ( work_dir ) ) cmd_line = 'dcm2nii {0} "{1}"' . format ( arguments , work_dir ) log . info ( cmd_line ) return subprocess . check_call ( cmd_line , shell = True )
7411	def sample_loci ( self ) : idxs = np . random . choice ( self . idxs , self . ntests ) with open ( self . data ) as indata : liter = ( indata . read ( ) . strip ( ) . split ( "|\n" ) ) seqdata = { i : "" for i in self . samples } for idx , loc in enumerate ( liter ) : if idx in idxs : lines = loc . split ( "\n" ) [ : - 1 ] names = [ i . split ( ) [ 0 ] for i in lines ] seqs = [ i . split ( ) [ 1 ] for i in lines ] dd = { i : j for i , j in zip ( names , seqs ) } for name in seqdata : if name in names : seqdata [ name ] += dd [ name ] else : seqdata [ name ] += "N" * len ( seqs [ 0 ] ) return seqdata
5421	def _get_job_metadata ( provider , user_id , job_name , script , task_ids , user_project , unique_job_id ) : create_time = dsub_util . replace_timezone ( datetime . datetime . now ( ) , tzlocal ( ) ) user_id = user_id or dsub_util . get_os_user ( ) job_metadata = provider . prepare_job_metadata ( script . name , job_name , user_id , create_time ) if unique_job_id : job_metadata [ 'job-id' ] = uuid . uuid4 ( ) . hex job_metadata [ 'create-time' ] = create_time job_metadata [ 'script' ] = script job_metadata [ 'user-project' ] = user_project if task_ids : job_metadata [ 'task-ids' ] = dsub_util . compact_interval_string ( list ( task_ids ) ) return job_metadata
5634	def make_toc ( sections , maxdepth = 0 ) : if not sections : return [ ] outer = min ( n for n , t in sections ) refs = [ ] for ind , sec in sections : if maxdepth and ind - outer + 1 > maxdepth : continue ref = sec . lower ( ) ref = ref . replace ( '`' , '' ) ref = ref . replace ( ' ' , '-' ) ref = ref . replace ( '?' , '' ) refs . append ( " " * ( ind - outer ) + "- [%s](#%s)" % ( sec , ref ) ) return refs
13093	def write_targets ( self ) : if len ( self . ldap_strings ) == 0 and len ( self . ips ) == 0 : print_notification ( "No targets left" ) if self . auto_exit : if self . notifier : self . notifier . stop ( ) self . terminate_processes ( ) with open ( self . targets_file , 'w' ) as f : f . write ( '\n' . join ( self . ldap_strings + self . ips ) )
3739	def Hf_g ( CASRN , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in ATcT_g . index : methods . append ( ATCT_G ) if CASRN in TRC_gas_data . index and not np . isnan ( TRC_gas_data . at [ CASRN , 'Hf' ] ) : methods . append ( TRC ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ATCT_G : _Hfg = float ( ATcT_g . at [ CASRN , 'Hf_298K' ] ) elif Method == TRC : _Hfg = float ( TRC_gas_data . at [ CASRN , 'Hf' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' ) return _Hfg
3428	def remove_metabolites ( self , metabolite_list , destructive = False ) : if not hasattr ( metabolite_list , '__iter__' ) : metabolite_list = [ metabolite_list ] metabolite_list = [ x for x in metabolite_list if x . id in self . metabolites ] for x in metabolite_list : x . _model = None associated_groups = self . get_associated_groups ( x ) for group in associated_groups : group . remove_members ( x ) if not destructive : for the_reaction in list ( x . _reaction ) : the_coefficient = the_reaction . _metabolites [ x ] the_reaction . subtract_metabolites ( { x : the_coefficient } ) else : for x in list ( x . _reaction ) : x . remove_from_model ( ) self . metabolites -= metabolite_list to_remove = [ self . solver . constraints [ m . id ] for m in metabolite_list ] self . remove_cons_vars ( to_remove ) context = get_context ( self ) if context : context ( partial ( self . metabolites . __iadd__ , metabolite_list ) ) for x in metabolite_list : context ( partial ( setattr , x , '_model' , self ) )
1829	def JA ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . AND ( cpu . CF == False , cpu . ZF == False ) , target . read ( ) , cpu . PC )
12845	def _relay_message ( self , message ) : info ( "relaying message: {message}" ) if not message . was_sent_by ( self . _id_factory ) : self . pipe . send ( message ) self . pipe . deliver ( )
8608	def remove_group_user ( self , group_id , user_id ) : response = self . _perform_request ( url = '/um/groups/%s/users/%s' % ( group_id , user_id ) , method = 'DELETE' ) return response
5911	def gmx_resid ( self , resid ) : try : gmx_resid = int ( self . offset [ resid ] ) except ( TypeError , IndexError ) : gmx_resid = resid + self . offset except KeyError : raise KeyError ( "offset must be a dict that contains the gmx resid for {0:d}" . format ( resid ) ) return gmx_resid
13479	def _hyphens_to_dashes ( self ) : problematic_hyphens = [ ( r'-([.,!)])' , r'---\1' ) , ( r'(?<=\d)-(?=\d)' , '--' ) , ( r'(?<=\s)-(?=\s)' , '---' ) ] for problem_case in problematic_hyphens : self . _regex_replacement ( * problem_case )
7887	def filter_mechanism_list ( mechanisms , properties , allow_insecure = False , server_side = False ) : result = [ ] for mechanism in mechanisms : try : if server_side : klass = SERVER_MECHANISMS_D [ mechanism ] else : klass = CLIENT_MECHANISMS_D [ mechanism ] except KeyError : logger . debug ( " skipping {0} - not supported" . format ( mechanism ) ) continue secure = properties . get ( "security-layer" ) if not allow_insecure and not klass . _pyxmpp_sasl_secure and not secure : logger . debug ( " skipping {0}, as it is not secure" . format ( mechanism ) ) continue if not klass . are_properties_sufficient ( properties ) : logger . debug ( " skipping {0}, as the properties are not sufficient" . format ( mechanism ) ) continue result . append ( mechanism ) return result
12821	def _str_to_path ( s , result_type ) : assert isinstance ( s , str ) if isinstance ( s , bytes ) and result_type is text_type : return s . decode ( 'ascii' ) elif isinstance ( s , text_type ) and result_type is bytes : return s . encode ( 'ascii' ) return s
4659	def as_quote ( self , quote ) : if quote == self [ "quote" ] [ "symbol" ] : return self . copy ( ) elif quote == self [ "base" ] [ "symbol" ] : return self . copy ( ) . invert ( ) else : raise InvalidAssetException
3400	def fill ( self , iterations = 1 ) : used_reactions = list ( ) for i in range ( iterations ) : self . model . slim_optimize ( error_value = None , message = 'gapfilling optimization failed' ) solution = [ self . model . reactions . get_by_id ( ind . rxn_id ) for ind in self . indicators if ind . _get_primal ( ) > self . integer_threshold ] if not self . validate ( solution ) : raise RuntimeError ( 'failed to validate gapfilled model, ' 'try lowering the integer_threshold' ) used_reactions . append ( solution ) self . update_costs ( ) return used_reactions
10047	def check_oauth2_scope ( can_method , * myscopes ) : def check ( record , * args , ** kwargs ) : @ require_api_auth ( ) @ require_oauth_scopes ( * myscopes ) def can ( self ) : return can_method ( record ) return type ( 'CheckOAuth2Scope' , ( ) , { 'can' : can } ) ( ) return check
8746	def get_floatingips_count ( context , filters = None ) : LOG . info ( 'get_floatingips_count for tenant %s filters %s' % ( context . tenant_id , filters ) ) if filters is None : filters = { } filters [ '_deallocated' ] = False filters [ 'address_type' ] = ip_types . FLOATING count = db_api . ip_address_count_all ( context , filters ) LOG . info ( 'Found %s floating ips for tenant %s' % ( count , context . tenant_id ) ) return count
6409	def seiffert_mean ( nums ) : r if len ( nums ) == 1 : return nums [ 0 ] if len ( nums ) > 2 : raise AttributeError ( 'seiffert_mean supports no more than two values' ) if nums [ 0 ] + nums [ 1 ] == 0 or nums [ 0 ] - nums [ 1 ] == 0 : return float ( 'NaN' ) return ( nums [ 0 ] - nums [ 1 ] ) / ( 2 * math . asin ( ( nums [ 0 ] - nums [ 1 ] ) / ( nums [ 0 ] + nums [ 1 ] ) ) )
1321	def Maximize ( self , waitTime : float = OPERATION_WAIT_TIME ) -> bool : if self . IsTopLevel ( ) : return self . ShowWindow ( SW . ShowMaximized , waitTime ) return False
5931	def scale_dihedrals ( mol , dihedrals , scale , banned_lines = None ) : if banned_lines is None : banned_lines = [ ] new_dihedrals = [ ] for dh in mol . dihedrals : atypes = dh . atom1 . get_atomtype ( ) , dh . atom2 . get_atomtype ( ) , dh . atom3 . get_atomtype ( ) , dh . atom4 . get_atomtype ( ) atypes = [ a . replace ( "_" , "" ) . replace ( "=" , "" ) for a in atypes ] if dh . gromacs [ 'param' ] != [ ] : for p in dh . gromacs [ 'param' ] : p [ 'kch' ] *= scale new_dihedrals . append ( dh ) continue for iswitch in range ( 32 ) : if ( iswitch % 2 == 0 ) : a1 = atypes [ 0 ] a2 = atypes [ 1 ] a3 = atypes [ 2 ] a4 = atypes [ 3 ] else : a1 = atypes [ 3 ] a2 = atypes [ 2 ] a3 = atypes [ 1 ] a4 = atypes [ 0 ] if ( ( iswitch // 2 ) % 2 == 1 ) : a1 = "X" if ( ( iswitch // 4 ) % 2 == 1 ) : a2 = "X" if ( ( iswitch // 8 ) % 2 == 1 ) : a3 = "X" if ( ( iswitch // 16 ) % 2 == 1 ) : a4 = "X" key = "{0}-{1}-{2}-{3}-{4}" . format ( a1 , a2 , a3 , a4 , dh . gromacs [ 'func' ] ) if ( key in dihedrals ) : for i , dt in enumerate ( dihedrals [ key ] ) : dhA = copy . deepcopy ( dh ) param = copy . deepcopy ( dt . gromacs [ 'param' ] ) if not dihedrals [ key ] [ 0 ] . line in banned_lines : for p in param : p [ 'kchi' ] *= scale dhA . gromacs [ 'param' ] = param if i == 0 : dhA . comment = "; banned lines {0} found={1}\n" . format ( " " . join ( map ( str , banned_lines ) ) , 1 if dt . line in banned_lines else 0 ) dhA . comment += "; parameters for types {}-{}-{}-{}-9 at LINE({})\n" . format ( dhA . atom1 . atomtype , dhA . atom2 . atomtype , dhA . atom3 . atomtype , dhA . atom4 . atomtype , dt . line ) . replace ( "_" , "" ) name = "{}-{}-{}-{}-9" . format ( dhA . atom1 . atomtype , dhA . atom2 . atomtype , dhA . atom3 . atomtype , dhA . atom4 . atomtype ) . replace ( "_" , "" ) new_dihedrals . append ( dhA ) break mol . dihedrals = new_dihedrals return mol
6479	def _normalised_python ( self ) : dx = ( self . screen . width / float ( len ( self . points ) ) ) oy = ( self . screen . height ) for x , point in enumerate ( self . points ) : y = ( point - self . minimum ) * 4.0 / self . extents * self . size . y yield Point ( ( dx * x , min ( oy , oy - y ) , ) )
12578	def apply_smoothing ( self , smooth_fwhm ) : if smooth_fwhm <= 0 : return old_smooth_fwhm = self . _smooth_fwhm self . _smooth_fwhm = smooth_fwhm try : data = self . get_data ( smoothed = True , masked = True , safe_copy = True ) except ValueError as ve : self . _smooth_fwhm = old_smooth_fwhm raise else : self . _smooth_fwhm = smooth_fwhm return data
6512	def _set ( self , name , gender , country_values ) : if '+' in name : for replacement in [ '' , ' ' , '-' ] : self . _set ( name . replace ( '+' , replacement ) , gender , country_values ) else : if name not in self . names : self . names [ name ] = { } self . names [ name ] [ gender ] = country_values
6368	def precision_gain ( self ) : r if self . population ( ) == 0 : return float ( 'NaN' ) random_precision = self . cond_pos_pop ( ) / self . population ( ) return self . precision ( ) / random_precision
3689	def solve_T ( self , P , V ) : r return ( P * V ** 2 * ( V - self . b ) + V * self . a - self . a * self . b ) / ( R * V ** 2 )
3155	def update ( self , list_id , segment_id , data ) : return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'segments' , segment_id ) , data = data )
5877	def get_local_image ( self , src ) : return ImageUtils . store_image ( self . fetcher , self . article . link_hash , src , self . config )
4188	def window_riemann ( N ) : r n = linspace ( - N / 2. , ( N ) / 2. , N ) w = sin ( n / float ( N ) * 2. * pi ) / ( n / float ( N ) * 2. * pi ) return w
2694	def fix_hypenation ( foo ) : i = 0 bar = [ ] while i < len ( foo ) : text , lemma , pos , tag = foo [ i ] if ( tag == "HYPH" ) and ( i > 0 ) and ( i < len ( foo ) - 1 ) : prev_tok = bar [ - 1 ] next_tok = foo [ i + 1 ] prev_tok [ 0 ] += "-" + next_tok [ 0 ] prev_tok [ 1 ] += "-" + next_tok [ 1 ] bar [ - 1 ] = prev_tok i += 2 else : bar . append ( foo [ i ] ) i += 1 return bar
2230	def register ( self , hash_types ) : if not isinstance ( hash_types , ( list , tuple ) ) : hash_types = [ hash_types ] def _decor_closure ( hash_func ) : for hash_type in hash_types : key = ( hash_type . __module__ , hash_type . __name__ ) self . keyed_extensions [ key ] = ( hash_type , hash_func ) return hash_func return _decor_closure
9542	def add_header_check ( self , code = HEADER_CHECK_FAILED , message = MESSAGES [ HEADER_CHECK_FAILED ] ) : t = code , message self . _header_checks . append ( t )
2240	def modname_to_modpath ( modname , hide_init = True , hide_main = False , sys_path = None ) : modpath = _syspath_modname_to_modpath ( modname , sys_path ) if modpath is None : return None modpath = normalize_modpath ( modpath , hide_init = hide_init , hide_main = hide_main ) return modpath
7130	def create_log_config ( verbose , quiet ) : if verbose and quiet : raise ValueError ( "Supplying both --quiet and --verbose makes no sense." ) elif verbose : level = logging . DEBUG elif quiet : level = logging . ERROR else : level = logging . INFO logger_cfg = { "handlers" : [ "click_handler" ] , "level" : level } return { "version" : 1 , "formatters" : { "click_formatter" : { "format" : "%(message)s" } } , "handlers" : { "click_handler" : { "level" : level , "class" : "doc2dash.__main__.ClickEchoHandler" , "formatter" : "click_formatter" , } } , "loggers" : { "doc2dash" : logger_cfg , "__main__" : logger_cfg } , }
4109	def mexican ( lb , ub , n ) : r if n <= 0 : raise ValueError ( "n must be strictly positive" ) x = numpy . linspace ( lb , ub , n ) psi = ( 1. - x ** 2. ) * ( 2. / ( numpy . sqrt ( 3. ) * pi ** 0.25 ) ) * numpy . exp ( - x ** 2 / 2. ) return psi
8944	def pushd ( path ) : saved = os . getcwd ( ) os . chdir ( path ) try : yield saved finally : os . chdir ( saved )
7487	def concat_multiple_inputs ( data , sample ) : if len ( sample . files . fastqs ) > 1 : cmd1 = [ "cat" ] + [ i [ 0 ] for i in sample . files . fastqs ] isgzip = ".gz" if not sample . files . fastqs [ 0 ] [ 0 ] . endswith ( ".gz" ) : isgzip = "" conc1 = os . path . join ( data . dirs . edits , sample . name + "_R1_concat.fq{}" . format ( isgzip ) ) with open ( conc1 , 'w' ) as cout1 : proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = cout1 , close_fds = True ) res1 = proc1 . communicate ( ) [ 0 ] if proc1 . returncode : raise IPyradWarningExit ( "error in: {}, {}" . format ( cmd1 , res1 ) ) conc2 = 0 if "pair" in data . paramsdict [ "datatype" ] : cmd2 = [ "cat" ] + [ i [ 1 ] for i in sample . files . fastqs ] conc2 = os . path . join ( data . dirs . edits , sample . name + "_R2_concat.fq{}" . format ( isgzip ) ) with open ( conc2 , 'w' ) as cout2 : proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = cout2 , close_fds = True ) res2 = proc2 . communicate ( ) [ 0 ] if proc2 . returncode : raise IPyradWarningExit ( "Error concatenating fastq files. Make sure all " + "these files exist: {}\nError message: {}" . format ( cmd2 , proc2 . returncode ) ) sample . files . concat = [ ( conc1 , conc2 ) ] return sample . files . concat
4474	def __serial_transform ( self , jam , steps ) : if six . PY2 : attr = 'next' else : attr = '__next__' pending = len ( steps ) nexts = itertools . cycle ( getattr ( iter ( D . transform ( jam ) ) , attr ) for ( name , D ) in steps ) while pending : try : for next_jam in nexts : yield next_jam ( ) except StopIteration : pending -= 1 nexts = itertools . cycle ( itertools . islice ( nexts , pending ) )
7136	def redirect_stdout ( new_stdout ) : old_stdout , sys . stdout = sys . stdout , new_stdout try : yield None finally : sys . stdout = old_stdout
4062	def show_condition_operators ( self , condition ) : permitted_operators = self . savedsearch . conditions_operators . get ( condition ) permitted_operators_list = set ( [ self . savedsearch . operators . get ( op ) for op in permitted_operators ] ) return permitted_operators_list
6445	def _cond_x ( self , word , suffix_len ) : return word [ - suffix_len - 1 ] in { 'i' , 'l' } or ( word [ - suffix_len - 3 : - suffix_len ] == 'u' and word [ - suffix_len - 1 ] == 'e' )
6420	def readfile ( fn ) : with open ( path . join ( HERE , fn ) , 'r' , encoding = 'utf-8' ) as f : return f . read ( )
1282	def block_html ( self , html ) : if self . options . get ( 'skip_style' ) and html . lower ( ) . startswith ( '<style' ) : return '' if self . options . get ( 'escape' ) : return escape ( html ) return html
1143	def _bytelist2longBigEndian ( list ) : "Transform a list of characters into a list of longs." imax = len ( list ) // 4 hl = [ 0 ] * imax j = 0 i = 0 while i < imax : b0 = ord ( list [ j ] ) << 24 b1 = ord ( list [ j + 1 ] ) << 16 b2 = ord ( list [ j + 2 ] ) << 8 b3 = ord ( list [ j + 3 ] ) hl [ i ] = b0 | b1 | b2 | b3 i = i + 1 j = j + 4 return hl
5151	def merge_config ( template , config , list_identifiers = None ) : result = template . copy ( ) for key , value in config . items ( ) : if isinstance ( value , dict ) : node = result . get ( key , OrderedDict ( ) ) result [ key ] = merge_config ( node , value ) elif isinstance ( value , list ) and isinstance ( result . get ( key ) , list ) : result [ key ] = merge_list ( result [ key ] , value , list_identifiers ) else : result [ key ] = value return result
5039	def is_user_enrolled ( cls , user , course_id , course_mode ) : enrollment_client = EnrollmentApiClient ( ) try : enrollments = enrollment_client . get_course_enrollment ( user . username , course_id ) if enrollments and course_mode == enrollments . get ( 'mode' ) : return True except HttpClientError as exc : logging . error ( 'Error while checking enrollment status of user %(user)s: %(message)s' , dict ( user = user . username , message = str ( exc ) ) ) except KeyError as exc : logging . warning ( 'Error while parsing enrollment data of user %(user)s: %(message)s' , dict ( user = user . username , message = str ( exc ) ) ) return False
11829	def expand ( self , problem ) : "List the nodes reachable in one step from this node." return [ self . child_node ( problem , action ) for action in problem . actions ( self . state ) ]
9880	def _distances ( value_domain , distance_metric , n_v ) : return np . array ( [ [ distance_metric ( v1 , v2 , i1 = i1 , i2 = i2 , n_v = n_v ) for i2 , v2 in enumerate ( value_domain ) ] for i1 , v1 in enumerate ( value_domain ) ] )
13031	def poll_once ( self , timeout = 0.0 ) : if self . _map : self . _poll_func ( timeout , self . _map )
894	def mapCellsToColumns ( self , cells ) : cellsForColumns = defaultdict ( set ) for cell in cells : column = self . columnForCell ( cell ) cellsForColumns [ column ] . add ( cell ) return cellsForColumns
754	def setLoggedMetrics ( self , metricNames ) : if metricNames is None : self . __metricNames = set ( [ ] ) else : self . __metricNames = set ( metricNames )
13017	def addHook ( self , name , callable ) : if name not in self . _hooks : self . _hooks [ name ] = [ ] self . _hooks [ name ] . append ( callable )
11441	def _correct_record ( record ) : errors = [ ] for tag in record . keys ( ) : upper_bound = '999' n = len ( tag ) if n > 3 : i = n - 3 while i > 0 : upper_bound = '%s%s' % ( '0' , upper_bound ) i -= 1 if tag == '!' : errors . append ( ( 1 , '(field number(s): ' + str ( [ f [ 4 ] for f in record [ tag ] ] ) + ')' ) ) record [ '000' ] = record . pop ( tag ) tag = '000' elif not ( '001' <= tag <= upper_bound or tag in ( 'FMT' , 'FFT' , 'BDR' , 'BDM' ) ) : errors . append ( 2 ) record [ '000' ] = record . pop ( tag ) tag = '000' fields = [ ] for field in record [ tag ] : if field [ 0 ] == [ ] and field [ 3 ] == '' : errors . append ( ( 8 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) subfields = [ ] for subfield in field [ 0 ] : if subfield [ 0 ] == '!' : errors . append ( ( 3 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) newsub = ( '' , subfield [ 1 ] ) else : newsub = subfield subfields . append ( newsub ) if field [ 1 ] == '!' : errors . append ( ( 4 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) ind1 = " " else : ind1 = field [ 1 ] if field [ 2 ] == '!' : errors . append ( ( 5 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) ind2 = " " else : ind2 = field [ 2 ] fields . append ( ( subfields , ind1 , ind2 , field [ 3 ] , field [ 4 ] ) ) record [ tag ] = fields return errors
368	def crop ( x , wrg , hrg , is_random = False , row_index = 0 , col_index = 1 ) : h , w = x . shape [ row_index ] , x . shape [ col_index ] if ( h < hrg ) or ( w < wrg ) : raise AssertionError ( "The size of cropping should smaller than or equal to the original image" ) if is_random : h_offset = int ( np . random . uniform ( 0 , h - hrg ) ) w_offset = int ( np . random . uniform ( 0 , w - wrg ) ) return x [ h_offset : hrg + h_offset , w_offset : wrg + w_offset ] else : h_offset = int ( np . floor ( ( h - hrg ) / 2. ) ) w_offset = int ( np . floor ( ( w - wrg ) / 2. ) ) h_end = h_offset + hrg w_end = w_offset + wrg return x [ h_offset : h_end , w_offset : w_end ]
1114	def _collect_lines ( self , diffs ) : fromlist , tolist , flaglist = [ ] , [ ] , [ ] for fromdata , todata , flag in diffs : try : fromlist . append ( self . _format_line ( 0 , flag , * fromdata ) ) tolist . append ( self . _format_line ( 1 , flag , * todata ) ) except TypeError : fromlist . append ( None ) tolist . append ( None ) flaglist . append ( flag ) return fromlist , tolist , flaglist
7930	def resolve_address ( self , hostname , callback , allow_cname = True ) : if self . settings [ "ipv6" ] : if self . settings [ "ipv4" ] : family = socket . AF_UNSPEC else : family = socket . AF_INET6 elif self . settings [ "ipv4" ] : family = socket . AF_INET else : logger . warning ( "Neither IPv6 or IPv4 allowed." ) callback ( [ ] ) return try : ret = socket . getaddrinfo ( hostname , 0 , family , socket . SOCK_STREAM , 0 ) except socket . gaierror , err : logger . warning ( "Couldn't resolve {0!r}: {1}" . format ( hostname , err ) ) callback ( [ ] ) return except IOError as err : logger . warning ( "Couldn't resolve {0!r}, unexpected error: {1}" . format ( hostname , err ) ) callback ( [ ] ) return if family == socket . AF_UNSPEC : tmp = ret if self . settings [ "prefer_ipv6" ] : ret = [ addr for addr in tmp if addr [ 0 ] == socket . AF_INET6 ] ret += [ addr for addr in tmp if addr [ 0 ] == socket . AF_INET ] else : ret = [ addr for addr in tmp if addr [ 0 ] == socket . AF_INET ] ret += [ addr for addr in tmp if addr [ 0 ] == socket . AF_INET6 ] callback ( [ ( addr [ 0 ] , addr [ 4 ] [ 0 ] ) for addr in ret ] )
11760	def retract ( self , sentence ) : "Remove the sentence's clauses from the KB." for c in conjuncts ( to_cnf ( sentence ) ) : if c in self . clauses : self . clauses . remove ( c )
4244	def _get_record ( self , ipnum ) : seek_country = self . _seek_country ( ipnum ) if seek_country == self . _databaseSegments : return { } read_length = ( 2 * self . _recordLength - 1 ) * self . _databaseSegments try : self . _lock . acquire ( ) self . _fp . seek ( seek_country + read_length , os . SEEK_SET ) buf = self . _fp . read ( const . FULL_RECORD_LENGTH ) finally : self . _lock . release ( ) if PY3 and type ( buf ) is bytes : buf = buf . decode ( ENCODING ) record = { 'dma_code' : 0 , 'area_code' : 0 , 'metro_code' : None , 'postal_code' : None } latitude = 0 longitude = 0 char = ord ( buf [ 0 ] ) record [ 'country_code' ] = const . COUNTRY_CODES [ char ] record [ 'country_code3' ] = const . COUNTRY_CODES3 [ char ] record [ 'country_name' ] = const . COUNTRY_NAMES [ char ] record [ 'continent' ] = const . CONTINENT_NAMES [ char ] def read_data ( buf , pos ) : cur = pos while buf [ cur ] != '\0' : cur += 1 return cur , buf [ pos : cur ] if cur > pos else None offset , record [ 'region_code' ] = read_data ( buf , 1 ) offset , record [ 'city' ] = read_data ( buf , offset + 1 ) offset , record [ 'postal_code' ] = read_data ( buf , offset + 1 ) offset = offset + 1 for j in range ( 3 ) : latitude += ( ord ( buf [ offset + j ] ) << ( j * 8 ) ) for j in range ( 3 ) : longitude += ( ord ( buf [ offset + j + 3 ] ) << ( j * 8 ) ) record [ 'latitude' ] = ( latitude / 10000.0 ) - 180.0 record [ 'longitude' ] = ( longitude / 10000.0 ) - 180.0 if self . _databaseType in ( const . CITY_EDITION_REV1 , const . CITY_EDITION_REV1_V6 ) : if record [ 'country_code' ] == 'US' : dma_area = 0 for j in range ( 3 ) : dma_area += ord ( buf [ offset + j + 6 ] ) << ( j * 8 ) record [ 'dma_code' ] = int ( floor ( dma_area / 1000 ) ) record [ 'area_code' ] = dma_area % 1000 record [ 'metro_code' ] = const . DMA_MAP . get ( record [ 'dma_code' ] ) params = ( record [ 'country_code' ] , record [ 'region_code' ] ) record [ 'time_zone' ] = time_zone_by_country_and_region ( * params ) return record
13171	def iter ( self , name = None ) : for c in self . _children : if name is None or c . tagname == name : yield c for gc in c . find ( name ) : yield gc
13335	def cache_resolver ( resolver , path ) : env = resolver . cache . find ( path ) if env : return env raise ResolveError
3052	def step1_get_authorize_url ( self , redirect_uri = None , state = None ) : if redirect_uri is not None : logger . warning ( ( 'The redirect_uri parameter for ' 'OAuth2WebServerFlow.step1_get_authorize_url is deprecated. ' 'Please move to passing the redirect_uri in via the ' 'constructor.' ) ) self . redirect_uri = redirect_uri if self . redirect_uri is None : raise ValueError ( 'The value of redirect_uri must not be None.' ) query_params = { 'client_id' : self . client_id , 'redirect_uri' : self . redirect_uri , 'scope' : self . scope , } if state is not None : query_params [ 'state' ] = state if self . login_hint is not None : query_params [ 'login_hint' ] = self . login_hint if self . _pkce : if not self . code_verifier : self . code_verifier = _pkce . code_verifier ( ) challenge = _pkce . code_challenge ( self . code_verifier ) query_params [ 'code_challenge' ] = challenge query_params [ 'code_challenge_method' ] = 'S256' query_params . update ( self . params ) return _helpers . update_query_params ( self . auth_uri , query_params )
1591	def _setup_custom_grouping ( self , topology ) : for i in range ( len ( topology . bolts ) ) : for in_stream in topology . bolts [ i ] . inputs : if in_stream . stream . component_name == self . my_component_name and in_stream . gtype == topology_pb2 . Grouping . Value ( "CUSTOM" ) : if in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "PYTHON_OBJECT" ) : custom_grouping_obj = default_serializer . deserialize ( in_stream . custom_grouping_object ) if isinstance ( custom_grouping_obj , str ) : pex_loader . load_pex ( self . topology_pex_abs_path ) grouping_cls = pex_loader . import_and_get_class ( self . topology_pex_abs_path , custom_grouping_obj ) custom_grouping_obj = grouping_cls ( ) assert isinstance ( custom_grouping_obj , ICustomGrouping ) self . custom_grouper . add ( in_stream . stream . id , self . _get_taskids_for_component ( topology . bolts [ i ] . comp . name ) , custom_grouping_obj , self . my_component_name ) elif in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "JAVA_OBJECT" ) : raise NotImplementedError ( "Java-serialized custom grouping is not yet supported " "for python topology" ) else : raise ValueError ( "Unrecognized custom grouping type found: %s" % str ( in_stream . type ) )
11410	def record_move_fields ( rec , tag , field_positions_local , field_position_local = None ) : fields = record_delete_fields ( rec , tag , field_positions_local = field_positions_local ) return record_add_fields ( rec , tag , fields , field_position_local = field_position_local )
10235	def reaction_cartesian_expansion ( graph : BELGraph , accept_unqualified_edges : bool = True ) -> None : for u , v , d in list ( graph . edges ( data = True ) ) : if CITATION not in d and accept_unqualified_edges : _reaction_cartesion_expansion_unqualified_helper ( graph , u , v , d ) continue if isinstance ( u , Reaction ) and isinstance ( v , Reaction ) : catalysts = _get_catalysts_in_reaction ( u ) | _get_catalysts_in_reaction ( v ) for reactant , product in chain ( itt . product ( u . reactants , u . products ) , itt . product ( v . reactants , v . products ) ) : if reactant in catalysts or product in catalysts : continue graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for product , reactant in itt . product ( u . products , u . reactants ) : if reactant in catalysts or product in catalysts : continue graph . add_qualified_edge ( product , reactant , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( u , Reaction ) : catalysts = _get_catalysts_in_reaction ( u ) for product in u . products : if product in catalysts : continue if v not in u . products and v not in u . reactants : graph . add_increases ( product , v , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for reactant in u . reactants : graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( v , Reaction ) : for reactant in v . reactants : catalysts = _get_catalysts_in_reaction ( v ) if reactant in catalysts : continue if u not in v . products and u not in v . reactants : graph . add_increases ( u , reactant , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for product in v . products : graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) _remove_reaction_nodes ( graph )
1253	def print_state ( self ) : def tile_string ( value ) : if value > 0 : return '% 5d' % ( 2 ** value , ) return " " separator_line = '-' * 25 print ( separator_line ) for row in range ( 4 ) : print ( "|" + "|" . join ( [ tile_string ( v ) for v in self . _state [ row , : ] ] ) + "|" ) print ( separator_line )
1818	def SETNZ ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . ZF == False , 1 , 0 ) )
5581	def _get_contour_values ( min_val , max_val , base = 0 , interval = 100 ) : i = base out = [ ] if min_val < base : while i >= min_val : i -= interval while i <= max_val : if i >= min_val : out . append ( i ) i += interval return out
9558	def _apply_unique_checks ( self , i , r , unique_sets , summarize = False , context = None ) : for key , code , message in self . _unique_checks : value = None values = unique_sets [ key ] if isinstance ( key , basestring ) : fi = self . _field_names . index ( key ) if fi >= len ( r ) : continue value = r [ fi ] else : value = [ ] for f in key : fi = self . _field_names . index ( f ) if fi >= len ( r ) : break value . append ( r [ fi ] ) value = tuple ( value ) if value in values : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'key' ] = key p [ 'value' ] = value if context is not None : p [ 'context' ] = context yield p values . add ( value )
501	def _recomputeRecordFromKNN ( self , record ) : inputs = { "categoryIn" : [ None ] , "bottomUpIn" : self . _getStateAnomalyVector ( record ) , } outputs = { "categoriesOut" : numpy . zeros ( ( 1 , ) ) , "bestPrototypeIndices" : numpy . zeros ( ( 1 , ) ) , "categoryProbabilitiesOut" : numpy . zeros ( ( 1 , ) ) } classifier_indexes = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) valid_idx = numpy . where ( ( classifier_indexes >= self . getParameter ( 'trainRecords' ) ) & ( classifier_indexes < record . ROWID ) ) [ 0 ] . tolist ( ) if len ( valid_idx ) == 0 : return None self . _knnclassifier . setParameter ( 'inferenceMode' , None , True ) self . _knnclassifier . setParameter ( 'learningMode' , None , False ) self . _knnclassifier . compute ( inputs , outputs ) self . _knnclassifier . setParameter ( 'learningMode' , None , True ) classifier_distances = self . _knnclassifier . getLatestDistances ( ) valid_distances = classifier_distances [ valid_idx ] if valid_distances . min ( ) <= self . _classificationMaxDist : classifier_indexes_prev = classifier_indexes [ valid_idx ] rowID = classifier_indexes_prev [ valid_distances . argmin ( ) ] indexID = numpy . where ( classifier_indexes == rowID ) [ 0 ] [ 0 ] category = self . _knnclassifier . getCategoryList ( ) [ indexID ] return category return None
6009	def load_background_noise_map ( background_noise_map_path , background_noise_map_hdu , pixel_scale , convert_background_noise_map_from_weight_map , convert_background_noise_map_from_inverse_noise_map ) : background_noise_map_options = sum ( [ convert_background_noise_map_from_weight_map , convert_background_noise_map_from_inverse_noise_map ] ) if background_noise_map_options == 0 and background_noise_map_path is not None : return NoiseMap . from_fits_with_pixel_scale ( file_path = background_noise_map_path , hdu = background_noise_map_hdu , pixel_scale = pixel_scale ) elif convert_background_noise_map_from_weight_map and background_noise_map_path is not None : weight_map = Array . from_fits ( file_path = background_noise_map_path , hdu = background_noise_map_hdu ) return NoiseMap . from_weight_map ( weight_map = weight_map , pixel_scale = pixel_scale ) elif convert_background_noise_map_from_inverse_noise_map and background_noise_map_path is not None : inverse_noise_map = Array . from_fits ( file_path = background_noise_map_path , hdu = background_noise_map_hdu ) return NoiseMap . from_inverse_noise_map ( inverse_noise_map = inverse_noise_map , pixel_scale = pixel_scale ) else : return None
4984	def extend_course ( course , enterprise_customer , request ) : course_run_id = course [ 'course_runs' ] [ 0 ] [ 'key' ] try : catalog_api_client = CourseCatalogApiServiceClient ( enterprise_customer . site ) except ImproperlyConfigured : error_code = 'ENTPEV000' LOGGER . error ( 'CourseCatalogApiServiceClient is improperly configured. ' 'Returned error code {error_code} to user {userid} ' 'and enterprise_customer {enterprise_customer} ' 'for course_run_id {course_run_id}' . format ( error_code = error_code , userid = request . user . id , enterprise_customer = enterprise_customer . uuid , course_run_id = course_run_id , ) ) messages . add_generic_error_message_with_code ( request , error_code ) return ( { } , error_code ) course_details , course_run_details = catalog_api_client . get_course_and_course_run ( course_run_id ) if not course_details or not course_run_details : error_code = 'ENTPEV001' LOGGER . error ( 'User {userid} of enterprise customer {enterprise_customer} encountered an error.' 'No course_details or course_run_details found for ' 'course_run_id {course_run_id}. ' 'The following error code reported to the user: {error_code}' . format ( userid = request . user . id , enterprise_customer = enterprise_customer . uuid , course_run_id = course_run_id , error_code = error_code , ) ) messages . add_generic_error_message_with_code ( request , error_code ) return ( { } , error_code ) weeks_to_complete = course_run_details [ 'weeks_to_complete' ] course_run_image = course_run_details [ 'image' ] or { } course . update ( { 'course_image_uri' : course_run_image . get ( 'src' , '' ) , 'course_title' : course_run_details [ 'title' ] , 'course_level_type' : course_run_details . get ( 'level_type' , '' ) , 'course_short_description' : course_run_details [ 'short_description' ] or '' , 'course_full_description' : clean_html_for_template_rendering ( course_run_details [ 'full_description' ] or '' ) , 'expected_learning_items' : course_details . get ( 'expected_learning_items' , [ ] ) , 'staff' : course_run_details . get ( 'staff' , [ ] ) , 'course_effort' : ungettext_min_max ( '{} hour per week' , '{} hours per week' , '{}-{} hours per week' , course_run_details [ 'min_effort' ] or None , course_run_details [ 'max_effort' ] or None , ) or '' , 'weeks_to_complete' : ungettext ( '{} week' , '{} weeks' , weeks_to_complete ) . format ( weeks_to_complete ) if weeks_to_complete else '' , } ) return course , None
2295	def predict_proba ( self , a , b , ** kwargs ) : estimators = { 'entropy' : lambda x , y : eval_entropy ( y ) - eval_entropy ( x ) , 'integral' : integral_approx_estimator } ref_measures = { 'gaussian' : lambda x : standard_scale . fit_transform ( x . reshape ( ( - 1 , 1 ) ) ) , 'uniform' : lambda x : min_max_scale . fit_transform ( x . reshape ( ( - 1 , 1 ) ) ) , 'None' : lambda x : x } ref_measure = ref_measures [ kwargs . get ( 'refMeasure' , 'gaussian' ) ] estimator = estimators [ kwargs . get ( 'estimator' , 'entropy' ) ] a = ref_measure ( a ) b = ref_measure ( b ) return estimator ( a , b )
13875	def CopyFiles ( source_dir , target_dir , create_target_dir = False , md5_check = False ) : import fnmatch if IsDir ( source_dir ) : source_mask = '*' else : source_dir , source_mask = os . path . split ( source_dir ) if not IsDir ( target_dir ) : if create_target_dir : CreateDirectory ( target_dir ) else : from . _exceptions import DirectoryNotFoundError raise DirectoryNotFoundError ( target_dir ) filenames = ListFiles ( source_dir ) if filenames is None : return for i_filename in filenames : if md5_check and i_filename . endswith ( '.md5' ) : continue if fnmatch . fnmatch ( i_filename , source_mask ) : source_path = source_dir + '/' + i_filename target_path = target_dir + '/' + i_filename if IsDir ( source_path ) : CopyFiles ( source_path , target_path , create_target_dir = True , md5_check = md5_check ) else : CopyFile ( source_path , target_path , md5_check = md5_check )
7313	def process_request ( self , request ) : if not request : return if not db_loaded : load_db ( ) tz = request . session . get ( 'django_timezone' ) if not tz : tz = timezone . get_default_timezone ( ) client_ip = get_ip_address_from_request ( request ) ip_addrs = client_ip . split ( ',' ) for ip in ip_addrs : if is_valid_ip ( ip ) and not is_local_ip ( ip ) : if ':' in ip : tz = db_v6 . time_zone_by_addr ( ip ) break else : tz = db . time_zone_by_addr ( ip ) break if tz : timezone . activate ( tz ) request . session [ 'django_timezone' ] = str ( tz ) if getattr ( settings , 'AUTH_USER_MODEL' , None ) and getattr ( request , 'user' , None ) : detected_timezone . send ( sender = get_user_model ( ) , instance = request . user , timezone = tz ) else : timezone . deactivate ( )
9330	def total_memory ( ) : with file ( '/proc/meminfo' , 'r' ) as f : for line in f : words = line . split ( ) if words [ 0 ] . upper ( ) == 'MEMTOTAL:' : return int ( words [ 1 ] ) * 1024 raise IOError ( 'MemTotal unknown' )
12930	def get_pos ( vcf_line ) : if not vcf_line : return None vcf_data = vcf_line . strip ( ) . split ( '\t' ) return_data = dict ( ) return_data [ 'chrom' ] = CHROM_INDEX [ vcf_data [ 0 ] ] return_data [ 'pos' ] = int ( vcf_data [ 1 ] ) return return_data
6958	def list_trilegal_filtersystems ( ) : print ( '%-40s %s' % ( 'FILTER SYSTEM NAME' , 'DESCRIPTION' ) ) print ( '%-40s %s' % ( '------------------' , '-----------' ) ) for key in sorted ( TRILEGAL_FILTER_SYSTEMS . keys ( ) ) : print ( '%-40s %s' % ( key , TRILEGAL_FILTER_SYSTEMS [ key ] [ 'desc' ] ) )
7229	def paint ( self ) : snippet = { 'heatmap-radius' : VectorStyle . get_style_value ( self . radius ) , 'heatmap-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'heatmap-color' : VectorStyle . get_style_value ( self . color ) , 'heatmap-intensity' : VectorStyle . get_style_value ( self . intensity ) , 'heatmap-weight' : VectorStyle . get_style_value ( self . weight ) } return snippet
1813	def SETNBE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . AND ( cpu . CF == False , cpu . ZF == False ) , 1 , 0 ) )
7816	def add_handler ( self , handler ) : if not isinstance ( handler , EventHandler ) : raise TypeError , "Not an EventHandler" with self . lock : if handler in self . handlers : return self . handlers . append ( handler ) self . _update_handlers ( )
2699	def render_ranks ( graph , ranks , dot_file = "graph.dot" ) : if dot_file : write_dot ( graph , ranks , path = dot_file )
13752	def _reference_table ( cls , ref_table ) : cols = [ ( sa . Column ( ) , refcol ) for refcol in ref_table . primary_key ] for col , refcol in cols : setattr ( cls , "%s_%s" % ( ref_table . name , refcol . name ) , col ) cls . __table__ . append_constraint ( sa . ForeignKeyConstraint ( * zip ( * cols ) ) )
12103	def summary ( self ) : print ( "Type: %s" % self . __class__ . __name__ ) print ( "Batch Name: %r" % self . batch_name ) if self . tag : print ( "Tag: %s" % self . tag ) print ( "Root directory: %r" % self . get_root_directory ( ) ) print ( "Maximum concurrency: %s" % self . max_concurrency ) if self . description : print ( "Description: %s" % self . description )
8477	def run ( self ) : self . checkProperties ( ) self . debug ( "[*] Iniciando escaneo de AtomShields con las siguientes propiedades. . . " ) self . showScanProperties ( ) self . loadConfig ( ) init_ts = datetime . now ( ) cwd = os . getcwd ( ) os . chdir ( self . path ) issues = self . executeCheckers ( ) os . chdir ( cwd ) end_ts = datetime . now ( ) duration = '{}' . format ( end_ts - init_ts ) for plugin in issues . keys ( ) : value = issues [ plugin ] if isinstance ( value , list ) : map ( self . saveIssue , value ) else : self . saveIssue ( value ) print "" self . executeReports ( ) self . debug ( "" ) self . debug ( "Duration: {t}" . format ( t = duration ) ) self . showSummary ( ) return self . issues
11476	def _create_or_reuse_item ( local_file , parent_folder_id , reuse_existing = False ) : local_item_name = os . path . basename ( local_file ) item_id = None if reuse_existing : children = session . communicator . folder_children ( session . token , parent_folder_id ) items = children [ 'items' ] for item in items : if item [ 'name' ] == local_item_name : item_id = item [ 'item_id' ] break if item_id is None : new_item = session . communicator . create_item ( session . token , local_item_name , parent_folder_id ) item_id = new_item [ 'item_id' ] return item_id
2274	def _win32_rmtree ( path , verbose = 0 ) : def _rmjunctions ( root ) : subdirs = [ ] for name in os . listdir ( root ) : current = join ( root , name ) if os . path . isdir ( current ) : if _win32_is_junction ( current ) : os . rmdir ( current ) elif not os . path . islink ( current ) : subdirs . append ( current ) for subdir in subdirs : _rmjunctions ( subdir ) if _win32_is_junction ( path ) : if verbose : print ( 'Deleting <JUNCTION> directory="{}"' . format ( path ) ) os . rmdir ( path ) else : if verbose : print ( 'Deleting directory="{}"' . format ( path ) ) _rmjunctions ( path ) import shutil shutil . rmtree ( path )
5735	def _get_or_create_subscription ( self ) : topic_path = self . _get_topic_path ( ) subscription_name = '{}-{}-{}-worker' . format ( queue . PUBSUB_OBJECT_PREFIX , self . name , uuid4 ( ) . hex ) subscription_path = self . subscriber_client . subscription_path ( self . project , subscription_name ) try : self . subscriber_client . get_subscription ( subscription_path ) except google . cloud . exceptions . NotFound : logger . info ( "Creating worker subscription {}" . format ( subscription_name ) ) self . subscriber_client . create_subscription ( subscription_path , topic_path ) return subscription_path
8569	def remove_loadbalanced_nic ( self , datacenter_id , loadbalancer_id , nic_id ) : response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s/balancednics/%s' % ( datacenter_id , loadbalancer_id , nic_id ) , method = 'DELETE' ) return response
9201	def extract_cycles ( series , left = False , right = False ) : points = deque ( ) for x in reversals ( series , left = left , right = right ) : points . append ( x ) while len ( points ) >= 3 : X = abs ( points [ - 2 ] - points [ - 1 ] ) Y = abs ( points [ - 3 ] - points [ - 2 ] ) if X < Y : break elif len ( points ) == 3 : yield points [ 0 ] , points [ 1 ] , 0.5 points . popleft ( ) else : yield points [ - 3 ] , points [ - 2 ] , 1.0 last = points . pop ( ) points . pop ( ) points . pop ( ) points . append ( last ) else : while len ( points ) > 1 : yield points [ 0 ] , points [ 1 ] , 0.5 points . popleft ( )
374	def illumination ( x , gamma = 1. , contrast = 1. , saturation = 1. , is_random = False ) : if is_random : if not ( len ( gamma ) == len ( contrast ) == len ( saturation ) == 2 ) : raise AssertionError ( "if is_random = True, the arguments are (min, max)" ) illum_settings = np . random . randint ( 0 , 3 ) if illum_settings == 0 : gamma = np . random . uniform ( gamma [ 0 ] , 1.0 ) elif illum_settings == 1 : gamma = np . random . uniform ( 1.0 , gamma [ 1 ] ) else : gamma = 1 im_ = brightness ( x , gamma = gamma , gain = 1 , is_random = False ) image = PIL . Image . fromarray ( im_ ) contrast_adjust = PIL . ImageEnhance . Contrast ( image ) image = contrast_adjust . enhance ( np . random . uniform ( contrast [ 0 ] , contrast [ 1 ] ) ) saturation_adjust = PIL . ImageEnhance . Color ( image ) image = saturation_adjust . enhance ( np . random . uniform ( saturation [ 0 ] , saturation [ 1 ] ) ) im_ = np . array ( image ) else : im_ = brightness ( x , gamma = gamma , gain = 1 , is_random = False ) image = PIL . Image . fromarray ( im_ ) contrast_adjust = PIL . ImageEnhance . Contrast ( image ) image = contrast_adjust . enhance ( contrast ) saturation_adjust = PIL . ImageEnhance . Color ( image ) image = saturation_adjust . enhance ( saturation ) im_ = np . array ( image ) return np . asarray ( im_ )
3381	def shared_np_array ( shape , data = None , integer = False ) : size = np . prod ( shape ) if integer : array = Array ( ctypes . c_int64 , int ( size ) ) np_array = np . frombuffer ( array . get_obj ( ) , dtype = "int64" ) else : array = Array ( ctypes . c_double , int ( size ) ) np_array = np . frombuffer ( array . get_obj ( ) ) np_array = np_array . reshape ( shape ) if data is not None : if len ( shape ) != len ( data . shape ) : raise ValueError ( "`data` must have the same dimensions" "as the created array." ) same = all ( x == y for x , y in zip ( shape , data . shape ) ) if not same : raise ValueError ( "`data` must have the same shape" "as the created array." ) np_array [ : ] = data return np_array
11387	def call_path ( self , basepath ) : rel_filepath = self . path if basepath : rel_filepath = os . path . relpath ( self . path , basepath ) basename = self . name if basename in set ( [ '__init__.py' , '__main__.py' ] ) : rel_filepath = os . path . dirname ( rel_filepath ) return rel_filepath
8202	def size_or_default ( self ) : if not self . size : self . size = self . DEFAULT_SIZE return self . size
13720	def main ( ) : ep = requests . get ( TRELLO_API_DOC ) . content root = html . fromstring ( ep ) links = root . xpath ( '//a[contains(@class, "reference internal")]/@href' ) pages = [ requests . get ( TRELLO_API_DOC + u ) for u in links if u . endswith ( 'index.html' ) ] endpoints = [ ] for page in pages : root = html . fromstring ( page . content ) sections = root . xpath ( '//div[@class="section"]/h2/..' ) for sec in sections : ep_html = etree . tostring ( sec ) . decode ( 'utf-8' ) ep_text = html2text ( ep_html ) . splitlines ( ) match = EP_DESC_REGEX . match ( ep_text [ 0 ] ) if not match : continue ep_method , ep_url = match . groups ( ) ep_text [ 0 ] = ' ' . join ( [ ep_method , ep_url ] ) ep_doc = b64encode ( gzip . compress ( '\n' . join ( ep_text ) . encode ( 'utf-8' ) ) ) endpoints . append ( ( ep_method , ep_url , ep_doc ) ) print ( yaml . dump ( create_tree ( endpoints ) ) )
10062	def deposit_links_factory ( pid ) : links = default_links_factory ( pid ) def _url ( name , ** kwargs ) : endpoint = '.{0}_{1}' . format ( current_records_rest . default_endpoint_prefixes [ pid . pid_type ] , name , ) return url_for ( endpoint , pid_value = pid . pid_value , _external = True , ** kwargs ) links [ 'files' ] = _url ( 'files' ) ui_endpoint = current_app . config . get ( 'DEPOSIT_UI_ENDPOINT' ) if ui_endpoint is not None : links [ 'html' ] = ui_endpoint . format ( host = request . host , scheme = request . scheme , pid_value = pid . pid_value , ) deposit_cls = Deposit if 'pid_value' in request . view_args : deposit_cls = request . view_args [ 'pid_value' ] . data [ 1 ] . __class__ for action in extract_actions_from_class ( deposit_cls ) : links [ action ] = _url ( 'actions' , action = action ) return links
410	def _tf_batch_map_offsets ( self , inputs , offsets , grid_offset ) : input_shape = inputs . get_shape ( ) batch_size = tf . shape ( inputs ) [ 0 ] kernel_n = int ( int ( offsets . get_shape ( ) [ 3 ] ) / 2 ) input_h = input_shape [ 1 ] input_w = input_shape [ 2 ] channel = input_shape [ 3 ] inputs = self . _to_bc_h_w ( inputs , input_shape ) offsets = tf . reshape ( offsets , ( batch_size , input_h , input_w , kernel_n , 2 ) ) coords = tf . expand_dims ( grid_offset , 0 ) coords = tf . tile ( coords , [ batch_size , 1 , 1 , 1 , 1 ] ) + offsets coords = tf . stack ( [ tf . clip_by_value ( coords [ : , : , : , : , 0 ] , 0.0 , tf . cast ( input_h - 1 , 'float32' ) ) , tf . clip_by_value ( coords [ : , : , : , : , 1 ] , 0.0 , tf . cast ( input_w - 1 , 'float32' ) ) ] , axis = - 1 ) coords = tf . tile ( coords , [ channel , 1 , 1 , 1 , 1 ] ) mapped_vals = self . _tf_batch_map_coordinates ( inputs , coords ) mapped_vals = self . _to_b_h_w_n_c ( mapped_vals , [ batch_size , input_h , input_w , kernel_n , channel ] ) return mapped_vals
2091	def copy ( self , pk = None , new_name = None , ** kwargs ) : orig = self . read ( pk , fail_on_no_results = True , fail_on_multiple_results = True ) orig = orig [ 'results' ] [ 0 ] self . _pop_none ( kwargs ) newresource = copy ( orig ) newresource . pop ( 'id' ) basename = newresource [ 'name' ] . split ( '@' , 1 ) [ 0 ] . strip ( ) for field in self . fields : if field . multiple and field . name in newresource : newresource [ field . name ] = ( newresource . get ( field . name ) , ) if new_name is None : newresource [ 'name' ] = "%s @ %s" % ( basename , time . strftime ( '%X' ) ) newresource . update ( kwargs ) return self . write ( create_on_missing = True , fail_on_found = True , ** newresource ) else : if kwargs : raise exc . TowerCLIError ( 'Cannot override {} and also use --new-name.' . format ( kwargs . keys ( ) ) ) copy_endpoint = '{}/{}/copy/' . format ( self . endpoint . strip ( '/' ) , pk ) return client . post ( copy_endpoint , data = { 'name' : new_name } ) . json ( )
5666	def _run ( self ) : if self . _has_run : raise RuntimeError ( "This spreader instance has already been run: " "create a new Spreader object for a new run." ) i = 1 while self . event_heap . size ( ) > 0 and len ( self . _uninfected_stops ) > 0 : event = self . event_heap . pop_next_event ( ) this_stop = self . _stop_I_to_spreading_stop [ event . from_stop_I ] if event . arr_time_ut > self . start_time_ut + self . max_duration_ut : break if this_stop . can_infect ( event ) : target_stop = self . _stop_I_to_spreading_stop [ event . to_stop_I ] already_visited = target_stop . has_been_visited ( ) target_stop . visit ( event ) if not already_visited : self . _uninfected_stops . remove ( event . to_stop_I ) print ( i , self . event_heap . size ( ) ) transfer_distances = self . gtfs . get_straight_line_transfer_distances ( event . to_stop_I ) self . event_heap . add_walk_events_to_heap ( transfer_distances , event , self . start_time_ut , self . walk_speed , self . _uninfected_stops , self . max_duration_ut ) i += 1 self . _has_run = True
5827	def dataset_search ( self , dataset_returning_query ) : self . _validate_search_query ( dataset_returning_query ) return self . _execute_search_query ( dataset_returning_query , DatasetSearchResult )
13514	def froude_number ( speed , length ) : g = 9.80665 Fr = speed / np . sqrt ( g * length ) return Fr
24	def update ( self , new_val ) : if self . _value is None : self . _value = new_val else : self . _value = self . _gamma * self . _value + ( 1.0 - self . _gamma ) * new_val
3851	async def lookup_entities ( client , args ) : lookup_spec = _get_lookup_spec ( args . entity_identifier ) request = hangups . hangouts_pb2 . GetEntityByIdRequest ( request_header = client . get_request_header ( ) , batch_lookup_spec = [ lookup_spec ] , ) res = await client . get_entity_by_id ( request ) for entity_result in res . entity_result : for entity in entity_result . entity : print ( entity )
745	def anomalyRemoveLabels ( self , start , end , labelFilter ) : self . _getAnomalyClassifier ( ) . getSelf ( ) . removeLabels ( start , end , labelFilter )
2436	def add_reviewer ( self , doc , reviewer ) : self . reset_reviews ( ) if validations . validate_reviewer ( reviewer ) : doc . add_review ( review . Review ( reviewer = reviewer ) ) return True else : raise SPDXValueError ( 'Review::Reviewer' )
10755	def iso_name_increment ( name , is_dir = False , max_length = 8 ) : if not is_dir and '.' in name : name , ext = name . rsplit ( '.' ) ext = '.{}' . format ( ext ) else : ext = '' for position , char in reversed ( list ( enumerate ( name ) ) ) : if char not in string . digits : break base , tag = name [ : position + 1 ] , name [ position + 1 : ] tag = str ( int ( tag or 0 ) + 1 ) if len ( tag ) + len ( base ) > max_length : base = base [ : max_length - len ( tag ) ] return '' . join ( [ base , tag , ext ] )
11391	def contribute_to_class ( self , cls , name ) : super ( EmbeddedMediaField , self ) . contribute_to_class ( cls , name ) register_field ( cls , self ) cls . _meta . add_virtual_field ( EmbeddedSignalCreator ( self ) )
3513	def clicky ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ClickyNode ( )
12633	def calculate_file_distances ( dicom_files , field_weights = None , dist_method_cls = None , ** kwargs ) : if dist_method_cls is None : dist_method = LevenshteinDicomFileDistance ( field_weights ) else : try : dist_method = dist_method_cls ( field_weights = field_weights , ** kwargs ) except : log . exception ( 'Could not instantiate {} object with field_weights ' 'and {}' . format ( dist_method_cls , kwargs ) ) dist_dtype = np . float16 n_files = len ( dicom_files ) try : file_dists = np . zeros ( ( n_files , n_files ) , dtype = dist_dtype ) except MemoryError as mee : import scipy . sparse file_dists = scipy . sparse . lil_matrix ( ( n_files , n_files ) , dtype = dist_dtype ) for idxi in range ( n_files ) : dist_method . set_dicom_file1 ( dicom_files [ idxi ] ) for idxj in range ( idxi + 1 , n_files ) : dist_method . set_dicom_file2 ( dicom_files [ idxj ] ) if idxi != idxj : file_dists [ idxi , idxj ] = dist_method . transform ( ) return file_dists
5114	def clear_data ( self , queues = None , edge = None , edge_type = None ) : queues = _get_queues ( self . g , queues , edge , edge_type ) for k in queues : self . edge2queue [ k ] . data = { }
5542	def contours ( self , elevation , interval = 100 , field = 'elev' , base = 0 ) : return commons_contours . extract_contours ( elevation , self . tile , interval = interval , field = field , base = base )
12058	def TK_askPassword ( title = "input" , msg = "type here:" ) : root = tkinter . Tk ( ) root . withdraw ( ) root . attributes ( "-topmost" , True ) root . lift ( ) value = tkinter . simpledialog . askstring ( title , msg ) root . destroy ( ) return value
10434	def getcellvalue ( self , window_name , object_name , row_index , column = 0 ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) count = len ( object_handle . AXRows ) if row_index < 0 or row_index > count : raise LdtpServerException ( 'Row index out of range: %d' % row_index ) cell = object_handle . AXRows [ row_index ] count = len ( cell . AXChildren ) if column < 0 or column > count : raise LdtpServerException ( 'Column index out of range: %d' % column ) obj = cell . AXChildren [ column ] if not re . search ( "AXColumn" , obj . AXRole ) : obj = cell . AXChildren [ column ] return obj . AXValue
6423	def dist ( self , src , tar , word_approx_min = 0.3 , char_approx_min = 0.73 , tests = 2 ** 12 - 1 , ) : return ( synoname ( src , tar , word_approx_min , char_approx_min , tests , False ) / 14 )
5907	def edit_txt ( filename , substitutions , newname = None ) : if newname is None : newname = filename _substitutions = [ { 'lRE' : re . compile ( str ( lRE ) ) , 'sRE' : re . compile ( str ( sRE ) ) , 'repl' : repl } for lRE , sRE , repl in substitutions if repl is not None ] with tempfile . TemporaryFile ( ) as target : with open ( filename , 'rb' ) as src : logger . info ( "editing txt = {0!r} ({1:d} substitutions)" . format ( filename , len ( substitutions ) ) ) for line in src : line = line . decode ( "utf-8" ) keep_line = True for subst in _substitutions : m = subst [ 'lRE' ] . match ( line ) if m : logger . debug ( 'match: ' + line . rstrip ( ) ) if subst [ 'repl' ] is False : keep_line = False else : line = subst [ 'sRE' ] . sub ( str ( subst [ 'repl' ] ) , line ) logger . debug ( 'replaced: ' + line . rstrip ( ) ) if keep_line : target . write ( line . encode ( 'utf-8' ) ) else : logger . debug ( "Deleting line %r" , line ) target . seek ( 0 ) with open ( newname , 'wb' ) as final : shutil . copyfileobj ( target , final ) logger . info ( "edited txt = {newname!r}" . format ( ** vars ( ) ) )
1070	def getaddrlist ( self ) : result = [ ] ad = self . getaddress ( ) while ad : result += ad ad = self . getaddress ( ) return result
4194	def plot_window ( self ) : from pylab import plot , xlim , grid , title , ylabel , axis x = linspace ( 0 , 1 , self . N ) xlim ( 0 , 1 ) plot ( x , self . data ) grid ( True ) title ( '%s Window (%s points)' % ( self . name . capitalize ( ) , self . N ) ) ylabel ( 'Amplitude' ) axis ( [ 0 , 1 , 0 , 1.1 ] )
4498	def guid ( self , guid ) : return self . _json ( self . _get ( self . _build_url ( 'guids' , guid ) ) , 200 ) [ 'data' ] [ 'type' ]
11233	def get_inner_template ( self , language , template_type , indentation , key , val ) : inner_templates = { 'php' : { 'iterable' : '%s%s => array \n%s( \n%s%s),\n' % ( indentation , key , indentation , val , indentation ) , 'singular' : '%s%s => %s, \n' % ( indentation , key , val ) } , 'javascript' : { 'iterable' : '%s%s : {\n%s\n%s},\n' % ( indentation , key , val , indentation ) , 'singular' : '%s%s: %s,\n' % ( indentation , key , val ) } , 'ocaml' : { 'iterable' : '%s[| (%s, (\n%s\n%s))|] ;;\n' % ( indentation , key , val , indentation ) , 'singular' : '%s(%s, %s);\n' % ( indentation , key , val ) } } return inner_templates [ language ] [ template_type ]
10254	def get_causal_out_edges ( graph : BELGraph , nbunch : Union [ BaseEntity , Iterable [ BaseEntity ] ] , ) -> Set [ Tuple [ BaseEntity , BaseEntity ] ] : return { ( u , v ) for u , v , k , d in graph . out_edges ( nbunch , keys = True , data = True ) if is_causal_relation ( graph , u , v , k , d ) }
5853	def get_pif ( self , dataset_id , uid , dataset_version = None ) : failure_message = "An error occurred retrieving PIF {}" . format ( uid ) if dataset_version == None : response = self . _get ( routes . pif_dataset_uid ( dataset_id , uid ) , failure_message = failure_message ) else : response = self . _get ( routes . pif_dataset_version_uid ( dataset_id , uid , dataset_version ) , failure_message = failure_message ) return pif . loads ( response . content . decode ( "utf-8" ) )
6373	def accuracy_gain ( self ) : r if self . population ( ) == 0 : return float ( 'NaN' ) random_accuracy = ( self . cond_pos_pop ( ) / self . population ( ) ) ** 2 + ( self . cond_neg_pop ( ) / self . population ( ) ) ** 2 return self . accuracy ( ) / random_accuracy
2150	def delete ( self , pk = None , fail_on_missing = False , ** kwargs ) : self . _separate ( kwargs ) return super ( Resource , self ) . delete ( pk = pk , fail_on_missing = fail_on_missing , ** kwargs )
1353	def make_error_response ( self , message ) : response = self . make_response ( constants . RESPONSE_STATUS_FAILURE ) response [ constants . RESPONSE_KEY_MESSAGE ] = message return response
520	def _initPermNonConnected ( self ) : p = self . _synPermConnected * self . _random . getReal64 ( ) p = int ( p * 100000 ) / 100000.0 return p
12342	def images ( self ) : "List of paths to images." tifs = _pattern ( self . _image_path , extension = 'tif' ) pngs = _pattern ( self . _image_path , extension = 'png' ) imgs = [ ] imgs . extend ( glob ( tifs ) ) imgs . extend ( glob ( pngs ) ) return imgs
4092	def addSearchers ( self , * searchers ) : self . _searchers . extend ( searchers ) debug . logger & debug . flagCompiler and debug . logger ( 'current compiled MIBs location(s): %s' % ', ' . join ( [ str ( x ) for x in self . _searchers ] ) ) return self
8728	def strftime ( fmt , t ) : if isinstance ( t , ( time . struct_time , tuple ) ) : t = datetime . datetime ( * t [ : 6 ] ) assert isinstance ( t , ( datetime . datetime , datetime . time , datetime . date ) ) try : year = t . year if year < 1900 : t = t . replace ( year = 1900 ) except AttributeError : year = 1900 subs = ( ( '%Y' , '%04d' % year ) , ( '%y' , '%02d' % ( year % 100 ) ) , ( '%s' , '%03d' % ( t . microsecond // 1000 ) ) , ( '%u' , '%03d' % ( t . microsecond % 1000 ) ) ) def doSub ( s , sub ) : return s . replace ( * sub ) def doSubs ( s ) : return functools . reduce ( doSub , subs , s ) fmt = '%%' . join ( map ( doSubs , fmt . split ( '%%' ) ) ) return t . strftime ( fmt )
603	def add2DArray ( self , data , position = 111 , xlabel = None , ylabel = None , cmap = None , aspect = "auto" , interpolation = "nearest" , name = None ) : if cmap is None : cmap = cm . Greys ax = self . _addBase ( position , xlabel = xlabel , ylabel = ylabel ) ax . imshow ( data , cmap = cmap , aspect = aspect , interpolation = interpolation ) if self . _show : plt . draw ( ) if name is not None : if not os . path . exists ( "log" ) : os . mkdir ( "log" ) plt . savefig ( "log/{name}.png" . format ( name = name ) , bbox_inches = "tight" , figsize = ( 8 , 6 ) , dpi = 400 )
13268	def _gmlv2_to_geojson ( el ) : tag = el . tag . replace ( '{%s}' % NS_GML , '' ) if tag == 'Point' : coordinates = [ float ( c ) for c in el . findtext ( '{%s}coordinates' % NS_GML ) . split ( ',' ) ] elif tag == 'LineString' : coordinates = [ [ float ( x ) for x in pair . split ( ',' ) ] for pair in el . findtext ( '{%s}coordinates' % NS_GML ) . split ( ' ' ) ] elif tag == 'Polygon' : coordinates = [ ] for ring in el . xpath ( 'gml:outerBoundaryIs/gml:LinearRing/gml:coordinates' , namespaces = NSMAP ) + el . xpath ( 'gml:innerBoundaryIs/gml:LinearRing/gml:coordinates' , namespaces = NSMAP ) : coordinates . append ( [ [ float ( x ) for x in pair . split ( ',' ) ] for pair in ring . text . split ( ' ' ) ] ) elif tag in ( 'MultiPoint' , 'MultiLineString' , 'MultiPolygon' , 'MultiCurve' ) : if tag == 'MultiCurve' : single_type = 'LineString' member_tag = 'curveMember' else : single_type = tag [ 5 : ] member_tag = single_type [ 0 ] . lower ( ) + single_type [ 1 : ] + 'Member' coordinates = [ gml_to_geojson ( member ) [ 'coordinates' ] for member in el . xpath ( 'gml:%s/gml:%s' % ( member_tag , single_type ) , namespaces = NSMAP ) ] else : raise NotImplementedError return { 'type' : tag , 'coordinates' : coordinates }
2494	def package_verif_node ( self , package ) : verif_node = BNode ( ) type_triple = ( verif_node , RDF . type , self . spdx_namespace . PackageVerificationCode ) self . graph . add ( type_triple ) value_triple = ( verif_node , self . spdx_namespace . packageVerificationCodeValue , Literal ( package . verif_code ) ) self . graph . add ( value_triple ) excl_file_nodes = map ( lambda excl : Literal ( excl ) , package . verif_exc_files ) excl_predicate = self . spdx_namespace . packageVerificationCodeExcludedFile excl_file_triples = [ ( verif_node , excl_predicate , xcl_file ) for xcl_file in excl_file_nodes ] for trp in excl_file_triples : self . graph . add ( trp ) return verif_node
1569	def invoke_hook_bolt_fail ( self , heron_tuple , fail_latency_ns ) : if len ( self . task_hooks ) > 0 : bolt_fail_info = BoltFailInfo ( heron_tuple = heron_tuple , failing_task_id = self . get_task_id ( ) , fail_latency_ms = fail_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . bolt_fail ( bolt_fail_info )
7004	def train_rf_classifier ( collected_features , test_fraction = 0.25 , n_crossval_iterations = 20 , n_kfolds = 5 , crossval_scoring_metric = 'f1' , classifier_to_pickle = None , nworkers = - 1 , ) : if ( isinstance ( collected_features , str ) and os . path . exists ( collected_features ) ) : with open ( collected_features , 'rb' ) as infd : fdict = pickle . load ( infd ) elif isinstance ( collected_features , dict ) : fdict = collected_features else : LOGERROR ( "can't figure out the input collected_features arg" ) return None tfeatures = fdict [ 'features_array' ] tlabels = fdict [ 'labels_array' ] tfeaturenames = fdict [ 'availablefeatures' ] tmagcol = fdict [ 'magcol' ] tobjectids = fdict [ 'objectids' ] training_features , testing_features , training_labels , testing_labels = ( train_test_split ( tfeatures , tlabels , test_size = test_fraction , random_state = RANDSEED , stratify = tlabels ) ) clf = RandomForestClassifier ( n_jobs = nworkers , random_state = RANDSEED ) rf_hyperparams = { "max_depth" : [ 3 , 4 , 5 , None ] , "n_estimators" : sp_randint ( 100 , 2000 ) , "max_features" : sp_randint ( 1 , 5 ) , "min_samples_split" : sp_randint ( 2 , 11 ) , "min_samples_leaf" : sp_randint ( 2 , 11 ) , } cvsearch = RandomizedSearchCV ( clf , param_distributions = rf_hyperparams , n_iter = n_crossval_iterations , scoring = crossval_scoring_metric , cv = StratifiedKFold ( n_splits = n_kfolds , shuffle = True , random_state = RANDSEED ) , random_state = RANDSEED ) LOGINFO ( 'running grid-search CV to optimize RF hyperparameters...' ) cvsearch_classifiers = cvsearch . fit ( training_features , training_labels ) _gridsearch_report ( cvsearch_classifiers . cv_results_ ) bestclf = cvsearch_classifiers . best_estimator_ bestclf_score = cvsearch_classifiers . best_score_ bestclf_hyperparams = cvsearch_classifiers . best_params_ test_predicted_labels = bestclf . predict ( testing_features ) recscore = recall_score ( testing_labels , test_predicted_labels ) precscore = precision_score ( testing_labels , test_predicted_labels ) f1score = f1_score ( testing_labels , test_predicted_labels ) confmatrix = confusion_matrix ( testing_labels , test_predicted_labels ) outdict = { 'features' : tfeatures , 'labels' : tlabels , 'feature_names' : tfeaturenames , 'magcol' : tmagcol , 'objectids' : tobjectids , 'kwargs' : { 'test_fraction' : test_fraction , 'n_crossval_iterations' : n_crossval_iterations , 'n_kfolds' : n_kfolds , 'crossval_scoring_metric' : crossval_scoring_metric , 'nworkers' : nworkers } , 'collect_kwargs' : fdict [ 'kwargs' ] , 'testing_features' : testing_features , 'testing_labels' : testing_labels , 'training_features' : training_features , 'training_labels' : training_labels , 'best_classifier' : bestclf , 'best_score' : bestclf_score , 'best_hyperparams' : bestclf_hyperparams , 'best_recall' : recscore , 'best_precision' : precscore , 'best_f1' : f1score , 'best_confmatrix' : confmatrix } if classifier_to_pickle : with open ( classifier_to_pickle , 'wb' ) as outfd : pickle . dump ( outdict , outfd , pickle . HIGHEST_PROTOCOL ) return outdict
10565	def exclude_filepaths ( filepaths , exclude_patterns = None ) : if not exclude_patterns : return filepaths , [ ] exclude_re = re . compile ( "|" . join ( pattern for pattern in exclude_patterns ) ) included_songs = [ ] excluded_songs = [ ] for filepath in filepaths : if exclude_patterns and exclude_re . search ( filepath ) : excluded_songs . append ( filepath ) else : included_songs . append ( filepath ) return included_songs , excluded_songs
7651	def query_pop ( query , prefix , sep = '.' ) : terms = query . split ( sep ) if terms [ 0 ] == prefix : terms = terms [ 1 : ] return sep . join ( terms )
8560	def get_lan_members ( self , datacenter_id , lan_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/lans/%s/nics?depth=%s' % ( datacenter_id , lan_id , str ( depth ) ) ) return response
11656	def fit_transform ( self , X , y = None , ** params ) : X = as_features ( X , stack = True ) X_new = self . transformer . fit_transform ( X . stacked_features , y , ** params ) return self . _gather_outputs ( X , X_new )
5813	def detect_other_protocol ( server_handshake_bytes ) : if server_handshake_bytes [ 0 : 5 ] == b'HTTP/' : return 'HTTP' if server_handshake_bytes [ 0 : 4 ] == b'220 ' : if re . match ( b'^[^\r\n]*ftp' , server_handshake_bytes , re . I ) : return 'FTP' else : return 'SMTP' if server_handshake_bytes [ 0 : 4 ] == b'220-' : return 'FTP' if server_handshake_bytes [ 0 : 4 ] == b'+OK ' : return 'POP3' if server_handshake_bytes [ 0 : 4 ] == b'* OK' or server_handshake_bytes [ 0 : 9 ] == b'* PREAUTH' : return 'IMAP' return None
9484	def validate_content ( * objs ) : from . main import Collection , Module validator = { Collection : cnxml . validate_collxml , Module : cnxml . validate_cnxml , } [ type ( objs [ 0 ] ) ] return validator ( * [ obj . file for obj in objs ] )
13331	def add ( name , path , branch , type ) : if not name and not path : ctx = click . get_current_context ( ) click . echo ( ctx . get_help ( ) ) examples = ( '\nExamples:\n' ' cpenv module add my_module ./path/to/my_module\n' ' cpenv module add my_module git@github.com:user/my_module.git' ' cpenv module add my_module git@github.com:user/my_module.git --branch=master --type=shared' ) click . echo ( examples ) return if not name : click . echo ( 'Missing required argument: name' ) return if not path : click . echo ( 'Missing required argument: path' ) env = cpenv . get_active_env ( ) if type == 'local' : if not env : click . echo ( '\nActivate an environment to add a local module.\n' ) return if click . confirm ( '\nAdd {} to active env {}?' . format ( name , env . name ) ) : click . echo ( 'Adding module...' , nl = False ) try : env . add_module ( name , path , branch ) except : click . echo ( bold_red ( 'FAILED' ) ) raise else : click . echo ( bold_green ( 'OK!' ) ) return module_paths = cpenv . get_module_paths ( ) click . echo ( '\nAvailable module paths:\n' ) for i , mod_path in enumerate ( module_paths ) : click . echo ( ' {}. {}' . format ( i , mod_path ) ) choice = click . prompt ( 'Where do you want to add your module?' , type = int , default = 0 ) module_root = module_paths [ choice ] module_path = utils . unipath ( module_root , name ) click . echo ( 'Creating module {}...' . format ( module_path ) , nl = False ) try : cpenv . create_module ( module_path , path , branch ) except : click . echo ( bold_red ( 'FAILED' ) ) raise else : click . echo ( bold_green ( 'OK!' ) )
9231	def fetch_events_async ( self , issues , tag_name ) : if not issues : return issues max_simultaneous_requests = self . options . max_simultaneous_requests verbose = self . options . verbose gh = self . github user = self . options . user repo = self . options . project self . events_cnt = 0 if verbose : print ( "fetching events for {} {}... " . format ( len ( issues ) , tag_name ) ) def worker ( issue ) : page = 1 issue [ 'events' ] = [ ] while page > 0 : rc , data = gh . repos [ user ] [ repo ] . issues [ issue [ 'number' ] ] . events . get ( page = page , per_page = PER_PAGE_NUMBER ) if rc == 200 : issue [ 'events' ] . extend ( data ) self . events_cnt += len ( data ) else : self . raise_GitHubError ( rc , data , gh . getheaders ( ) ) page = NextPage ( gh ) threads = [ ] cnt = len ( issues ) for i in range ( 0 , ( cnt // max_simultaneous_requests ) + 1 ) : for j in range ( max_simultaneous_requests ) : idx = i * max_simultaneous_requests + j if idx == cnt : break t = threading . Thread ( target = worker , args = ( issues [ idx ] , ) ) threads . append ( t ) t . start ( ) if verbose > 2 : print ( "." , end = "" ) if not idx % PER_PAGE_NUMBER : print ( "" ) for t in threads : t . join ( ) if verbose > 2 : print ( "." )
12502	def smooth_imgs ( images , fwhm ) : if fwhm <= 0 : return images if not isinstance ( images , string_types ) and hasattr ( images , '__iter__' ) : only_one = False else : only_one = True images = [ images ] result = [ ] for img in images : img = check_img ( img ) affine = img . get_affine ( ) smooth = _smooth_data_array ( img . get_data ( ) , affine , fwhm = fwhm , copy = True ) result . append ( nib . Nifti1Image ( smooth , affine ) ) if only_one : return result [ 0 ] else : return result
13463	def add_event ( request ) : form = AddEventForm ( request . POST or None ) if form . is_valid ( ) : instance = form . save ( commit = False ) instance . sites = settings . SITE_ID instance . submitted_by = request . user instance . approved = True instance . slug = slugify ( instance . name ) instance . save ( ) messages . success ( request , 'Your event has been added.' ) return HttpResponseRedirect ( reverse ( 'events_index' ) ) return render ( request , 'happenings/event_form.html' , { 'form' : form , 'form_title' : 'Add an event' } )
818	def grow ( self , rows , cols ) : if not self . hist_ : self . hist_ = SparseMatrix ( rows , cols ) self . rowSums_ = numpy . zeros ( rows , dtype = dtype ) self . colSums_ = numpy . zeros ( cols , dtype = dtype ) self . hack_ = None else : oldRows = self . hist_ . nRows ( ) oldCols = self . hist_ . nCols ( ) nextRows = max ( oldRows , rows ) nextCols = max ( oldCols , cols ) if ( oldRows < nextRows ) or ( oldCols < nextCols ) : self . hist_ . resize ( nextRows , nextCols ) if oldRows < nextRows : oldSums = self . rowSums_ self . rowSums_ = numpy . zeros ( nextRows , dtype = dtype ) self . rowSums_ [ 0 : len ( oldSums ) ] = oldSums self . hack_ = None if oldCols < nextCols : oldSums = self . colSums_ self . colSums_ = numpy . zeros ( nextCols , dtype = dtype ) self . colSums_ [ 0 : len ( oldSums ) ] = oldSums self . hack_ = None
7146	def to_atomic ( amount ) : if not isinstance ( amount , ( Decimal , float ) + _integer_types ) : raise ValueError ( "Amount '{}' doesn't have numeric type. Only Decimal, int, long and " "float (not recommended) are accepted as amounts." ) return int ( amount * 10 ** 12 )
4935	def chunks ( dictionary , chunk_size ) : iterable = iter ( dictionary ) for __ in range ( 0 , len ( dictionary ) , chunk_size ) : yield { key : dictionary [ key ] for key in islice ( iterable , chunk_size ) }
5125	def simulate ( self , n = 1 , t = None ) : if not self . _initialized : msg = ( "Network has not been initialized. " "Call '.initialize()' first." ) raise QueueingToolError ( msg ) if t is None : for dummy in range ( n ) : self . _simulate_next_event ( slow = False ) else : now = self . _t while self . _t < now + t : self . _simulate_next_event ( slow = False )
11237	def sendreturn ( gen , value ) : try : gen . send ( value ) except StopIteration as e : return stopiter_value ( e ) else : raise RuntimeError ( 'generator did not return as expected' )
10031	def execute ( helper , config , args ) : env = parse_env_config ( config , args . environment ) option_settings = env . get ( 'option_settings' , { } ) settings = parse_option_settings ( option_settings ) for setting in settings : out ( str ( setting ) )
12008	def _generate_key ( pass_id , passphrases , salt , algorithm ) : if pass_id not in passphrases : raise Exception ( 'Passphrase not defined for id: %d' % pass_id ) passphrase = passphrases [ pass_id ] if len ( passphrase ) < 32 : raise Exception ( 'Passphrase less than 32 characters long' ) digestmod = EncryptedPickle . _get_hashlib ( algorithm [ 'pbkdf2_algorithm' ] ) encoder = PBKDF2 ( passphrase , salt , iterations = algorithm [ 'pbkdf2_iterations' ] , digestmodule = digestmod ) return encoder . read ( algorithm [ 'key_size' ] )
11758	def dpll ( clauses , symbols , model ) : "See if the clauses are true in a partial model." unknown_clauses = [ ] for c in clauses : val = pl_true ( c , model ) if val == False : return False if val != True : unknown_clauses . append ( c ) if not unknown_clauses : return model P , value = find_pure_symbol ( symbols , unknown_clauses ) if P : return dpll ( clauses , removeall ( P , symbols ) , extend ( model , P , value ) ) P , value = find_unit_clause ( clauses , model ) if P : return dpll ( clauses , removeall ( P , symbols ) , extend ( model , P , value ) ) P , symbols = symbols [ 0 ] , symbols [ 1 : ] return ( dpll ( clauses , symbols , extend ( model , P , True ) ) or dpll ( clauses , symbols , extend ( model , P , False ) ) )
11872	def wait ( self , sec = 0.1 ) : sec = max ( sec , 0 ) reps = int ( floor ( sec / 0.1 ) ) commands = [ ] for i in range ( 0 , reps ) : commands . append ( Command ( 0x00 , wait = True ) ) return tuple ( commands )
9534	def get_version ( version = None ) : version = get_complete_version ( version ) main = get_main_version ( version ) sub = '' if version [ 3 ] == 'alpha' and version [ 4 ] == 0 : git_changeset = get_git_changeset ( ) if git_changeset : sub = '.dev%s' % git_changeset elif version [ 3 ] != 'final' : mapping = { 'alpha' : 'a' , 'beta' : 'b' , 'rc' : 'c' } sub = mapping [ version [ 3 ] ] + str ( version [ 4 ] ) return str ( main + sub )
2920	def _send_call ( self , my_task ) : args , kwargs = None , None if self . args : args = _eval_args ( self . args , my_task ) if self . kwargs : kwargs = _eval_kwargs ( self . kwargs , my_task ) LOG . debug ( "%s (task id %s) calling %s" % ( self . name , my_task . id , self . call ) , extra = dict ( data = dict ( args = args , kwargs = kwargs ) ) ) async_call = default_app . send_task ( self . call , args = args , kwargs = kwargs ) my_task . _set_internal_data ( task_id = async_call . task_id ) my_task . async_call = async_call LOG . debug ( "'%s' called: %s" % ( self . call , my_task . async_call . task_id ) )
1316	def GetAllPixelColors ( self ) -> ctypes . Array : return self . GetPixelColorsOfRect ( 0 , 0 , self . Width , self . Height )
9168	def post_publication_processing ( event , cursor ) : module_ident , ident_hash = event . module_ident , event . ident_hash celery_app = get_current_registry ( ) . celery_app cursor . execute ( 'SELECT result_id::text ' 'FROM document_baking_result_associations ' 'WHERE module_ident = %s' , ( module_ident , ) ) for result in cursor . fetchall ( ) : state = celery_app . AsyncResult ( result [ 0 ] ) . state if state in ( 'QUEUED' , 'STARTED' , 'RETRY' ) : logger . debug ( 'Already queued module_ident={} ident_hash={}' . format ( module_ident , ident_hash ) ) return logger . debug ( 'Queued for processing module_ident={} ident_hash={}' . format ( module_ident , ident_hash ) ) recipe_ids = _get_recipe_ids ( module_ident , cursor ) update_module_state ( cursor , module_ident , 'processing' , recipe_ids [ 0 ] ) cursor . connection . commit ( ) task_name = 'cnxpublishing.subscribers.baking_processor' baking_processor = celery_app . tasks [ task_name ] result = baking_processor . delay ( module_ident , ident_hash ) baking_processor . backend . store_result ( result . id , None , 'QUEUED' ) track_baking_proc_state ( result , module_ident , cursor )
12064	def lazygo ( watchFolder = '../abfs/' , reAnalyze = False , rebuildSite = False , keepGoing = True , matching = False ) : abfsKnown = [ ] while True : print ( ) pagesNeeded = [ ] for fname in glob . glob ( watchFolder + "/*.abf" ) : ID = os . path . basename ( fname ) . replace ( ".abf" , "" ) if not fname in abfsKnown : if os . path . exists ( fname . replace ( ".abf" , ".rsv" ) ) : continue if matching and not matching in fname : continue abfsKnown . append ( fname ) if os . path . exists ( os . path . dirname ( fname ) + "/swhlab4/" + os . path . basename ( fname ) . replace ( ".abf" , "_info.pkl" ) ) and reAnalyze == False : print ( "already analyzed" , os . path . basename ( fname ) ) if rebuildSite : pagesNeeded . append ( ID ) else : handleNewABF ( fname ) pagesNeeded . append ( ID ) if len ( pagesNeeded ) : print ( " -- rebuilding index page" ) indexing . genIndex ( os . path . dirname ( fname ) , forceIDs = pagesNeeded ) if not keepGoing : return for i in range ( 50 ) : print ( '.' , end = '' ) time . sleep ( .2 )
5865	def course_key_is_valid ( course_key ) : if course_key is None : return False try : CourseKey . from_string ( text_type ( course_key ) ) except ( InvalidKeyError , UnicodeDecodeError ) : return False return True
9780	def build ( ctx , project , build ) : ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'build' ] = build
7333	async def close ( self ) : tasks = self . _get_close_tasks ( ) if tasks : await asyncio . wait ( tasks ) self . _session = None
1523	def log ( self , message , level = None ) : if level is None : _log_level = logging . INFO else : if level == "trace" or level == "debug" : _log_level = logging . DEBUG elif level == "info" : _log_level = logging . INFO elif level == "warn" : _log_level = logging . WARNING elif level == "error" : _log_level = logging . ERROR else : raise ValueError ( "%s is not supported as logging level" % str ( level ) ) self . logger . log ( _log_level , message )
4610	def recoverPubkeyParameter ( message , digest , signature , pubkey ) : if not isinstance ( message , bytes ) : message = bytes ( message , "utf-8" ) for i in range ( 0 , 4 ) : if SECP256K1_MODULE == "secp256k1" : sig = pubkey . ecdsa_recoverable_deserialize ( signature , i ) p = secp256k1 . PublicKey ( pubkey . ecdsa_recover ( message , sig ) ) if p . serialize ( ) == pubkey . serialize ( ) : return i elif SECP256K1_MODULE == "cryptography" and not isinstance ( pubkey , PublicKey ) : p = recover_public_key ( digest , signature , i , message ) p_comp = hexlify ( compressedPubkey ( p ) ) pubkey_comp = hexlify ( compressedPubkey ( pubkey ) ) if p_comp == pubkey_comp : return i else : p = recover_public_key ( digest , signature , i ) p_comp = hexlify ( compressedPubkey ( p ) ) p_string = hexlify ( p . to_string ( ) ) if isinstance ( pubkey , PublicKey ) : pubkey_string = bytes ( repr ( pubkey ) , "ascii" ) else : pubkey_string = hexlify ( pubkey . to_string ( ) ) if p_string == pubkey_string or p_comp == pubkey_string : return i
8581	def update_server ( self , datacenter_id , server_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : if attr == 'boot_volume' : boot_volume_properties = { "id" : value } boot_volume_entities = { "bootVolume" : boot_volume_properties } data . update ( boot_volume_entities ) else : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/servers/%s' % ( datacenter_id , server_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
6999	def parallel_cp_pfdir ( pfpickledir , outdir , lcbasedir , pfpickleglob = 'periodfinding-*.pkl*' , lclistpkl = None , cprenorm = False , nbrradiusarcsec = 60.0 , maxnumneighbors = 5 , makeneighborlcs = True , fast_mode = False , gaia_max_timeout = 60.0 , gaia_mirror = None , xmatchinfo = None , xmatchradiusarcsec = 3.0 , minobservations = 99 , sigclip = 10.0 , lcformat = 'hat-sql' , lcformatdir = None , timecols = None , magcols = None , errcols = None , skipdone = False , done_callback = None , done_callback_args = None , done_callback_kwargs = None , maxobjects = None , nworkers = 32 ) : pfpicklelist = sorted ( glob . glob ( os . path . join ( pfpickledir , pfpickleglob ) ) ) LOGINFO ( 'found %s period-finding pickles, running cp...' % len ( pfpicklelist ) ) return parallel_cp ( pfpicklelist , outdir , lcbasedir , fast_mode = fast_mode , lclistpkl = lclistpkl , nbrradiusarcsec = nbrradiusarcsec , gaia_max_timeout = gaia_max_timeout , gaia_mirror = gaia_mirror , maxnumneighbors = maxnumneighbors , makeneighborlcs = makeneighborlcs , xmatchinfo = xmatchinfo , xmatchradiusarcsec = xmatchradiusarcsec , sigclip = sigclip , minobservations = minobservations , cprenorm = cprenorm , maxobjects = maxobjects , lcformat = lcformat , lcformatdir = lcformatdir , timecols = timecols , magcols = magcols , errcols = errcols , skipdone = skipdone , nworkers = nworkers , done_callback = done_callback , done_callback_args = done_callback_args , done_callback_kwargs = done_callback_kwargs )
7331	def get_tasks ( self ) : tasks = self . _get_tasks ( ) tasks . extend ( self . _streams . get_tasks ( self ) ) return tasks
9950	def get_node ( obj , args , kwargs ) : if args is None and kwargs is None : return ( obj , ) if kwargs is None : kwargs = { } return obj , _bind_args ( obj , args , kwargs )
8738	def get_ports_count ( context , filters = None ) : LOG . info ( "get_ports_count for tenant %s filters %s" % ( context . tenant_id , filters ) ) return db_api . port_count_all ( context , join_security_groups = True , ** filters )
11136	def create_client ( ) -> APIClient : global _client client = _client ( ) if client is None : docker_environment = kwargs_from_env ( assert_hostname = False ) if "base_url" in docker_environment : client = _create_client ( docker_environment . get ( "base_url" ) , docker_environment . get ( "tls" ) ) if client is None : raise ConnectionError ( "Could not connect to the Docker daemon specified by the `DOCKER_X` environment variables: %s" % docker_environment ) else : logging . info ( "Connected to Docker daemon specified by the environment variables" ) else : client = _create_client ( "unix://var/run/docker.sock" ) if client is not None : logging . info ( "Connected to Docker daemon running on UNIX socket" ) else : raise ConnectionError ( "Cannot connect to Docker - is the Docker daemon running? `$DOCKER_HOST` should be set or the " "daemon should be accessible via the standard UNIX socket." ) _client = weakref . ref ( client ) assert isinstance ( client , APIClient ) return client
13248	def get_bibliography ( lsst_bib_names = None , bibtex = None ) : bibtex_data = get_lsst_bibtex ( bibtex_filenames = lsst_bib_names ) pybtex_data = [ pybtex . database . parse_string ( _bibtex , 'bibtex' ) for _bibtex in bibtex_data . values ( ) ] if bibtex is not None : pybtex_data . append ( pybtex . database . parse_string ( bibtex , 'bibtex' ) ) bib = pybtex_data [ 0 ] if len ( pybtex_data ) > 1 : for other_bib in pybtex_data [ 1 : ] : for key , entry in other_bib . entries . items ( ) : bib . add_entry ( key , entry ) return bib
12131	def lexsort ( self , * order ) : if order == [ ] : raise Exception ( "Please specify the keys for sorting, use" "'+' prefix for ascending," "'-' for descending.)" ) if not set ( el [ 1 : ] for el in order ) . issubset ( set ( self . varying_keys ) ) : raise Exception ( "Key(s) specified not in the set of varying keys." ) sorted_args = copy . deepcopy ( self ) specs_param = sorted_args . params ( 'specs' ) specs_param . constant = False sorted_args . specs = self . _lexsorted_specs ( order ) specs_param . constant = True sorted_args . _lexorder = order return sorted_args
1927	def parse_config ( f ) : try : c = yaml . safe_load ( f ) for section_name , section in c . items ( ) : group = get_group ( section_name ) for key , val in section . items ( ) : group . update ( key ) setattr ( group , key , val ) except Exception : raise ConfigError ( "Failed reading config file. Do you have a local [.]manticore.yml file?" )
8587	def attach_cdrom ( self , datacenter_id , server_id , cdrom_id ) : data = '{ "id": "' + cdrom_id + '" }' response = self . _perform_request ( url = '/datacenters/%s/servers/%s/cdroms' % ( datacenter_id , server_id ) , method = 'POST' , data = data ) return response
13747	def get_item ( self , hash_key , start = 0 , extra_attrs = None ) : table = self . get_table ( ) try : item = table . get_item ( hash_key = hash_key ) except DynamoDBKeyNotFoundError : item = None if item is None : item = self . create_item ( hash_key = hash_key , start = start , extra_attrs = extra_attrs , ) return item
6419	def dist ( self , src , tar , probs = None ) : if src == tar : return 0.0 if probs is None : self . _coder . train ( src + tar ) else : self . _coder . set_probs ( probs ) src_comp = self . _coder . encode ( src ) [ 1 ] tar_comp = self . _coder . encode ( tar ) [ 1 ] concat_comp = self . _coder . encode ( src + tar ) [ 1 ] concat_comp2 = self . _coder . encode ( tar + src ) [ 1 ] return ( min ( concat_comp , concat_comp2 ) - min ( src_comp , tar_comp ) ) / max ( src_comp , tar_comp )
7413	def plot ( self ) : if self . results_table == None : return "no results found" else : bb = self . results_table . sort_values ( by = [ "ABCD" , "ACBD" ] , ascending = [ False , True ] , ) import toyplot c = toyplot . Canvas ( width = 600 , height = 200 ) a = c . cartesian ( ) m = a . bars ( bb ) return c , a , m
13613	def combine_filenames ( filenames , max_length = 40 ) : path = None names = [ ] extension = None timestamps = [ ] shas = [ ] filenames . sort ( ) concat_names = "_" . join ( filenames ) if concat_names in COMBINED_FILENAMES_GENERATED : return COMBINED_FILENAMES_GENERATED [ concat_names ] for filename in filenames : name = os . path . basename ( filename ) if not extension : extension = os . path . splitext ( name ) [ 1 ] elif os . path . splitext ( name ) [ 1 ] != extension : raise ValueError ( "Can't combine multiple file extensions" ) for base in MEDIA_ROOTS : try : shas . append ( md5 ( os . path . join ( base , filename ) ) ) break except IOError : pass if path is None : path = os . path . dirname ( filename ) else : if len ( os . path . dirname ( filename ) ) < len ( path ) : path = os . path . dirname ( filename ) m = hashlib . md5 ( ) m . update ( "," . join ( shas ) ) new_filename = "%s-inkmd" % m . hexdigest ( ) new_filename = new_filename [ : max_length ] new_filename += extension COMBINED_FILENAMES_GENERATED [ concat_names ] = new_filename return os . path . join ( path , new_filename )
10836	def filter ( self , ** kwargs ) : if not len ( self ) : self . all ( ) new_list = filter ( lambda item : [ True for arg in kwargs if item [ arg ] == kwargs [ arg ] ] != [ ] , self ) return Profiles ( self . api , new_list )
4489	def might_need_auth ( f ) : @ wraps ( f ) def wrapper ( cli_args ) : try : return_value = f ( cli_args ) except UnauthorizedException as e : config = config_from_env ( config_from_file ( ) ) username = _get_username ( cli_args , config ) if username is None : sys . exit ( "Please set a username (run `osf -h` for details)." ) else : sys . exit ( "You are not authorized to access this project." ) return return_value return wrapper
4324	def convert ( self , samplerate = None , n_channels = None , bitdepth = None ) : bitdepths = [ 8 , 16 , 24 , 32 , 64 ] if bitdepth is not None : if bitdepth not in bitdepths : raise ValueError ( "bitdepth must be one of {}." . format ( str ( bitdepths ) ) ) self . output_format . extend ( [ '-b' , '{}' . format ( bitdepth ) ] ) if n_channels is not None : if not isinstance ( n_channels , int ) or n_channels <= 0 : raise ValueError ( "n_channels must be a positive integer." ) self . output_format . extend ( [ '-c' , '{}' . format ( n_channels ) ] ) if samplerate is not None : if not is_number ( samplerate ) or samplerate <= 0 : raise ValueError ( "samplerate must be a positive number." ) self . rate ( samplerate ) return self
1838	def JNB ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CF == False , target . read ( ) , cpu . PC )
4946	def send_course_enrollment_statement ( lrs_configuration , course_enrollment ) : user_details = LearnerInfoSerializer ( course_enrollment . user ) course_details = CourseInfoSerializer ( course_enrollment . course ) statement = LearnerCourseEnrollmentStatement ( course_enrollment . user , course_enrollment . course , user_details . data , course_details . data , ) EnterpriseXAPIClient ( lrs_configuration ) . save_statement ( statement )
6733	def str_to_list ( s ) : if s is None : return [ ] elif isinstance ( s , ( tuple , list ) ) : return s elif not isinstance ( s , six . string_types ) : raise NotImplementedError ( 'Unknown type: %s' % type ( s ) ) return [ _ . strip ( ) . lower ( ) for _ in ( s or '' ) . split ( ',' ) if _ . strip ( ) ]
6094	def mapping_matrix_from_sub_to_pix ( sub_to_pix , pixels , regular_pixels , sub_to_regular , sub_grid_fraction ) : mapping_matrix = np . zeros ( ( regular_pixels , pixels ) ) for sub_index in range ( sub_to_regular . shape [ 0 ] ) : mapping_matrix [ sub_to_regular [ sub_index ] , sub_to_pix [ sub_index ] ] += sub_grid_fraction return mapping_matrix
1615	def ReplaceAll ( pattern , rep , s ) : if pattern not in _regexp_compile_cache : _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) return _regexp_compile_cache [ pattern ] . sub ( rep , s )
4143	def _numpy_solver ( A , B ) : x = numpy . linalg . solve ( A , B ) return x
7425	def bedtools_merge ( data , sample ) : LOGGER . info ( "Entering bedtools_merge: %s" , sample . name ) mappedreads = os . path . join ( data . dirs . refmapping , sample . name + "-mapped-sorted.bam" ) cmd1 = [ ipyrad . bins . bedtools , "bamtobed" , "-i" , mappedreads ] cmd2 = [ ipyrad . bins . bedtools , "merge" , "-i" , "-" ] if 'pair' in data . paramsdict [ "datatype" ] : check_insert_size ( data , sample ) cmd2 . insert ( 2 , str ( data . _hackersonly [ "max_inner_mate_distance" ] ) ) cmd2 . insert ( 2 , "-d" ) else : cmd2 . insert ( 2 , str ( - 1 * data . _hackersonly [ "min_SE_refmap_overlap" ] ) ) cmd2 . insert ( 2 , "-d" ) LOGGER . info ( "stdv: bedtools merge cmds: %s %s" , cmd1 , cmd2 ) proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = sps . PIPE ) proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = sps . PIPE , stdin = proc1 . stdout ) result = proc2 . communicate ( ) [ 0 ] proc1 . stdout . close ( ) if proc2 . returncode : raise IPyradWarningExit ( "error in %s: %s" , cmd2 , result ) if os . path . exists ( ipyrad . __debugflag__ ) : with open ( os . path . join ( data . dirs . refmapping , sample . name + ".bed" ) , 'w' ) as outfile : outfile . write ( result ) nregions = len ( result . strip ( ) . split ( "\n" ) ) LOGGER . info ( "bedtools_merge: Got # regions: %s" , nregions ) return result
12124	def to_table ( args , vdims = [ ] ) : "Helper function to convet an Args object to a HoloViews Table" if not Table : return "HoloViews Table not available" kdims = [ dim for dim in args . constant_keys + args . varying_keys if dim not in vdims ] items = [ tuple ( [ spec [ k ] for k in kdims + vdims ] ) for spec in args . specs ] return Table ( items , kdims = kdims , vdims = vdims )
4973	def clean_channel_worker_username ( self ) : channel_worker_username = self . cleaned_data [ 'channel_worker_username' ] . strip ( ) try : User . objects . get ( username = channel_worker_username ) except User . DoesNotExist : raise ValidationError ( ValidationMessages . INVALID_CHANNEL_WORKER . format ( channel_worker_username = channel_worker_username ) ) return channel_worker_username
12089	def proto_01_12_steps025 ( abf = exampleABF ) : swhlab . ap . detect ( abf ) standard_groupingForInj ( abf , 200 ) for feature in [ 'freq' , 'downslope' ] : swhlab . ap . plot_values ( abf , feature , continuous = False ) swhlab . plot . save ( abf , tag = 'A_' + feature ) swhlab . plot . gain ( abf ) swhlab . plot . save ( abf , tag = '05-gain' )
1539	def add_bolt ( self , name , bolt_cls , par , inputs , config = None , optional_outputs = None ) : bolt_spec = bolt_cls . spec ( name = name , par = par , inputs = inputs , config = config , optional_outputs = optional_outputs ) self . add_spec ( bolt_spec ) return bolt_spec
7289	def has_digit ( string_or_list , sep = "_" ) : if isinstance ( string_or_list , ( tuple , list ) ) : list_length = len ( string_or_list ) if list_length : return six . text_type ( string_or_list [ - 1 ] ) . isdigit ( ) else : return False else : return has_digit ( string_or_list . split ( sep ) )
11854	def scanner ( self , j , word ) : "For each edge expecting a word of this category here, extend the edge." for ( i , j , A , alpha , Bb ) in self . chart [ j ] : if Bb and self . grammar . isa ( word , Bb [ 0 ] ) : self . add_edge ( [ i , j + 1 , A , alpha + [ ( Bb [ 0 ] , word ) ] , Bb [ 1 : ] ] )
1870	def MOVSX ( cpu , op0 , op1 ) : op0 . write ( Operators . SEXTEND ( op1 . read ( ) , op1 . size , op0 . size ) )
10070	def preserve ( method = None , result = True , fields = None ) : if method is None : return partial ( preserve , result = result , fields = fields ) fields = fields or ( '_deposit' , ) @ wraps ( method ) def wrapper ( self , * args , ** kwargs ) : data = { field : self [ field ] for field in fields if field in self } result_ = method ( self , * args , ** kwargs ) replace = result_ if result else self for field in data : replace [ field ] = data [ field ] return result_ return wrapper
4708	def power_off ( self , interval = 200 ) : if self . __power_off_port is None : cij . err ( "cij.usb.relay: Invalid USB_RELAY_POWER_OFF" ) return 1 return self . __press ( self . __power_off_port , interval = interval )
13027	def detect_os ( self , ip ) : process = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'checker.py' ) , str ( ip ) ] , stdout = subprocess . PIPE ) out = process . stdout . decode ( 'utf-8' ) . split ( '\n' ) system_os = '' for line in out : if line . startswith ( 'Target OS:' ) : system_os = line . replace ( 'Target OS: ' , '' ) break return system_os
10486	def _generateFindR ( self , ** kwargs ) : for needle in self . _generateChildrenR ( ) : if needle . _match ( ** kwargs ) : yield needle
11083	def help ( self , msg , args ) : output = [ ] if len ( args ) == 0 : commands = sorted ( self . _bot . dispatcher . commands . items ( ) , key = itemgetter ( 0 ) ) commands = filter ( lambda x : x [ 1 ] . is_subcmd is False , commands ) if self . _should_filter_help_commands ( msg . user ) : commands = filter ( lambda x : x [ 1 ] . admin_only is False , commands ) for name , cmd in commands : output . append ( self . _get_short_help_for_command ( name ) ) else : name = '!' + args [ 0 ] output = [ self . _get_help_for_command ( name ) ] return '\n' . join ( output )
10149	def _ref ( self , resp , base_name = None ) : name = base_name or resp . get ( 'title' , '' ) or resp . get ( 'name' , '' ) pointer = self . json_pointer + name self . response_registry [ name ] = resp return { '$ref' : pointer }
5866	def organization_data_is_valid ( organization_data ) : if organization_data is None : return False if 'id' in organization_data and not organization_data . get ( 'id' ) : return False if 'name' in organization_data and not organization_data . get ( 'name' ) : return False return True
13703	def expand_words ( self , line , width = 60 ) : if not line . strip ( ) : return line wordi = 1 while len ( strip_codes ( line ) ) < width : wordendi = self . find_word_end ( line , wordi ) if wordendi < 0 : wordi = 1 wordendi = self . find_word_end ( line , wordi ) if wordendi < 0 : line = '' . join ( ( ' ' , line ) ) else : line = ' ' . join ( ( line [ : wordendi ] , line [ wordendi : ] ) ) wordi += 1 if ' ' not in strip_codes ( line ) . strip ( ) : return line . replace ( ' ' , '' ) return line
2781	def create ( self ) : input_params = { "type" : self . type , "data" : self . data , "name" : self . name , "priority" : self . priority , "port" : self . port , "ttl" : self . ttl , "weight" : self . weight , "flags" : self . flags , "tags" : self . tags } data = self . get_data ( "domains/%s/records" % ( self . domain ) , type = POST , params = input_params , ) if data : self . id = data [ 'domain_record' ] [ 'id' ]
3099	def _validate_clientsecrets ( clientsecrets_dict ) : _INVALID_FILE_FORMAT_MSG = ( 'Invalid file format. See ' 'https://developers.google.com/api-client-library/' 'python/guide/aaa_client_secrets' ) if clientsecrets_dict is None : raise InvalidClientSecretsError ( _INVALID_FILE_FORMAT_MSG ) try : ( client_type , client_info ) , = clientsecrets_dict . items ( ) except ( ValueError , AttributeError ) : raise InvalidClientSecretsError ( _INVALID_FILE_FORMAT_MSG + ' ' 'Expected a JSON object with a single property for a "web" or ' '"installed" application' ) if client_type not in VALID_CLIENT : raise InvalidClientSecretsError ( 'Unknown client type: {0}.' . format ( client_type ) ) for prop_name in VALID_CLIENT [ client_type ] [ 'required' ] : if prop_name not in client_info : raise InvalidClientSecretsError ( 'Missing property "{0}" in a client type of "{1}".' . format ( prop_name , client_type ) ) for prop_name in VALID_CLIENT [ client_type ] [ 'string' ] : if client_info [ prop_name ] . startswith ( '[[' ) : raise InvalidClientSecretsError ( 'Property "{0}" is not configured.' . format ( prop_name ) ) return client_type , client_info
9758	def restart ( ctx , copy , file , u ) : config = None update_code = None if file : config = rhea . read ( file ) if u : ctx . invoke ( upload , sync = False ) update_code = True user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) try : if copy : response = PolyaxonClient ( ) . experiment . copy ( user , project_name , _experiment , config = config , update_code = update_code ) Printer . print_success ( 'Experiment was copied with id {}' . format ( response . id ) ) else : response = PolyaxonClient ( ) . experiment . restart ( user , project_name , _experiment , config = config , update_code = update_code ) Printer . print_success ( 'Experiment was restarted with id {}' . format ( response . id ) ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not restart experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
9126	def _make_session ( connection : Optional [ str ] = None ) -> Session : if connection is None : connection = get_global_connection ( ) engine = create_engine ( connection ) create_all ( engine ) session_cls = sessionmaker ( bind = engine ) session = session_cls ( ) return session
880	def create ( modelConfig , logLevel = logging . ERROR ) : logger = ModelFactory . __getLogger ( ) logger . setLevel ( logLevel ) logger . debug ( "ModelFactory returning Model from dict: %s" , modelConfig ) modelClass = None if modelConfig [ 'model' ] == "HTMPrediction" : modelClass = HTMPredictionModel elif modelConfig [ 'model' ] == "TwoGram" : modelClass = TwoGramModel elif modelConfig [ 'model' ] == "PreviousValue" : modelClass = PreviousValueModel else : raise Exception ( "ModelFactory received unsupported Model type: %s" % modelConfig [ 'model' ] ) return modelClass ( ** modelConfig [ 'modelParams' ] )
6217	def prepare_attrib_mapping ( self , primitive ) : buffer_info = [ ] for name , accessor in primitive . attributes . items ( ) : info = VBOInfo ( * accessor . info ( ) ) info . attributes . append ( ( name , info . components ) ) if buffer_info and buffer_info [ - 1 ] . buffer_view == info . buffer_view : if buffer_info [ - 1 ] . interleaves ( info ) : buffer_info [ - 1 ] . merge ( info ) continue buffer_info . append ( info ) return buffer_info
11472	def parse_data ( self , text , maxwidth , maxheight , template_dir , context , urlize_all_links ) : replacements = { } user_urls = set ( re . findall ( URL_RE , text ) ) for user_url in user_urls : try : resource = oembed . site . embed ( user_url , maxwidth = maxwidth , maxheight = maxheight ) except OEmbedException : if urlize_all_links : replacements [ user_url ] = '<a href="%(LINK)s">%(LINK)s</a>' % { 'LINK' : user_url } else : context [ 'minwidth' ] = min ( maxwidth , resource . width ) context [ 'minheight' ] = min ( maxheight , resource . height ) replacement = self . render_oembed ( resource , user_url , template_dir = template_dir , context = context ) replacements [ user_url ] = replacement . strip ( ) user_urls = re . finditer ( URL_RE , text ) matches = [ ] for match in user_urls : if match . group ( ) in replacements : matches . append ( [ match . start ( ) , match . end ( ) , match . group ( ) ] ) for indx , ( start , end , user_url ) in enumerate ( matches ) : replacement = replacements [ user_url ] difference = len ( replacement ) - len ( user_url ) text = text [ : start ] + replacement + text [ end : ] for j in xrange ( indx + 1 , len ( matches ) ) : matches [ j ] [ 0 ] += difference matches [ j ] [ 1 ] += difference return mark_safe ( text )
5811	def raise_hostname ( certificate , hostname ) : is_ip = re . match ( '^\\d+\\.\\d+\\.\\d+\\.\\d+$' , hostname ) or hostname . find ( ':' ) != - 1 if is_ip : hostname_type = 'IP address %s' % hostname else : hostname_type = 'domain name %s' % hostname message = 'Server certificate verification failed - %s does not match' % hostname_type valid_ips = ', ' . join ( certificate . valid_ips ) valid_domains = ', ' . join ( certificate . valid_domains ) if valid_domains : message += ' valid domains: %s' % valid_domains if valid_domains and valid_ips : message += ' or' if valid_ips : message += ' valid IP addresses: %s' % valid_ips raise TLSVerificationError ( message , certificate )
5530	def _process_worker ( process , process_tile ) : logger . debug ( ( process_tile . id , "running on %s" % current_process ( ) . name ) ) if ( process . config . mode == "continue" and process . config . output . tiles_exist ( process_tile ) ) : logger . debug ( ( process_tile . id , "tile exists, skipping" ) ) return ProcessInfo ( tile = process_tile , processed = False , process_msg = "output already exists" , written = False , write_msg = "nothing written" ) else : with Timer ( ) as t : try : output = process . execute ( process_tile , raise_nodata = True ) except MapcheteNodataTile : output = None processor_message = "processed in %s" % t logger . debug ( ( process_tile . id , processor_message ) ) writer_info = process . write ( process_tile , output ) return ProcessInfo ( tile = process_tile , processed = True , process_msg = processor_message , written = writer_info . written , write_msg = writer_info . write_msg )
671	def createNetwork ( dataSource ) : with open ( _PARAMS_PATH , "r" ) as f : modelParams = yaml . safe_load ( f ) [ "modelParams" ] network = Network ( ) network . addRegion ( "sensor" , "py.RecordSensor" , '{}' ) sensorRegion = network . regions [ "sensor" ] . getSelf ( ) sensorRegion . encoder = createEncoder ( modelParams [ "sensorParams" ] [ "encoders" ] ) sensorRegion . dataSource = dataSource modelParams [ "spParams" ] [ "inputWidth" ] = sensorRegion . encoder . getWidth ( ) network . addRegion ( "SP" , "py.SPRegion" , json . dumps ( modelParams [ "spParams" ] ) ) network . addRegion ( "TM" , "py.TMRegion" , json . dumps ( modelParams [ "tmParams" ] ) ) clName = "py.%s" % modelParams [ "clParams" ] . pop ( "regionName" ) network . addRegion ( "classifier" , clName , json . dumps ( modelParams [ "clParams" ] ) ) createSensorToClassifierLinks ( network , "sensor" , "classifier" ) createDataOutLink ( network , "sensor" , "SP" ) createFeedForwardLink ( network , "SP" , "TM" ) createFeedForwardLink ( network , "TM" , "classifier" ) createResetLink ( network , "sensor" , "SP" ) createResetLink ( network , "sensor" , "TM" ) network . initialize ( ) return network
5771	def _advapi32_verify ( certificate_or_public_key , signature , data , hash_algorithm , rsa_pss_padding = False ) : algo = certificate_or_public_key . algorithm if algo == 'rsa' and rsa_pss_padding : hash_length = { 'sha1' : 20 , 'sha224' : 28 , 'sha256' : 32 , 'sha384' : 48 , 'sha512' : 64 } . get ( hash_algorithm , 0 ) decrypted_signature = raw_rsa_public_crypt ( certificate_or_public_key , signature ) key_size = certificate_or_public_key . bit_size if not verify_pss_padding ( hash_algorithm , hash_length , key_size , data , decrypted_signature ) : raise SignatureError ( 'Signature is invalid' ) return if algo == 'rsa' and hash_algorithm == 'raw' : padded_plaintext = raw_rsa_public_crypt ( certificate_or_public_key , signature ) try : plaintext = remove_pkcs1v15_signature_padding ( certificate_or_public_key . byte_size , padded_plaintext ) if not constant_compare ( plaintext , data ) : raise ValueError ( ) except ( ValueError ) : raise SignatureError ( 'Signature is invalid' ) return hash_handle = None try : alg_id = { 'md5' : Advapi32Const . CALG_MD5 , 'sha1' : Advapi32Const . CALG_SHA1 , 'sha256' : Advapi32Const . CALG_SHA_256 , 'sha384' : Advapi32Const . CALG_SHA_384 , 'sha512' : Advapi32Const . CALG_SHA_512 , } [ hash_algorithm ] hash_handle_pointer = new ( advapi32 , 'HCRYPTHASH *' ) res = advapi32 . CryptCreateHash ( certificate_or_public_key . context_handle , alg_id , null ( ) , 0 , hash_handle_pointer ) handle_error ( res ) hash_handle = unwrap ( hash_handle_pointer ) res = advapi32 . CryptHashData ( hash_handle , data , len ( data ) , 0 ) handle_error ( res ) if algo == 'dsa' : try : signature = algos . DSASignature . load ( signature ) . to_p1363 ( ) half_len = len ( signature ) // 2 signature = signature [ half_len : ] + signature [ : half_len ] except ( ValueError , OverflowError , TypeError ) : raise SignatureError ( 'Signature is invalid' ) reversed_signature = signature [ : : - 1 ] res = advapi32 . CryptVerifySignatureW ( hash_handle , reversed_signature , len ( signature ) , certificate_or_public_key . key_handle , null ( ) , 0 ) handle_error ( res ) finally : if hash_handle : advapi32 . CryptDestroyHash ( hash_handle )
9520	def make_random_contigs ( contigs , length , outfile , name_by_letters = False , prefix = '' , seed = None , first_number = 1 ) : random . seed ( a = seed ) fout = utils . open_file_write ( outfile ) letters = list ( 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' ) letters_index = 0 for i in range ( contigs ) : if name_by_letters : name = letters [ letters_index ] letters_index += 1 if letters_index == len ( letters ) : letters_index = 0 else : name = str ( i + first_number ) fa = sequences . Fasta ( prefix + name , '' . join ( [ random . choice ( 'ACGT' ) for x in range ( length ) ] ) ) print ( fa , file = fout ) utils . close ( fout )
11879	def scanProcessForCwd ( pid , searchPortion , isExactMatch = False ) : try : try : pid = int ( pid ) except ValueError as e : sys . stderr . write ( 'Expected an integer, got %s for pid.\n' % ( str ( type ( pid ) ) , ) ) raise e cwd = getProcessCwd ( pid ) if not cwd : return None isMatch = False if isExactMatch is True : if searchPortion == cwd : isMatch = True else : if searchPortion . endswith ( '/' ) and searchPortion [ : - 1 ] == cwd : isMatch = True else : if searchPortion in cwd : isMatch = True else : if searchPortion . endswith ( '/' ) and searchPortion [ : - 1 ] in cwd : isMatch = True if not isMatch : return None cmdline = getProcessCommandLineStr ( pid ) owner = getProcessOwnerStr ( pid ) return { 'searchPortion' : searchPortion , 'pid' : pid , 'owner' : owner , 'cmdline' : cmdline , 'cwd' : cwd , } except OSError : return None except IOError : return None except FileNotFoundError : return None except PermissionError : return None
4505	def get_and_run_edits ( self ) : if self . empty ( ) : return edits = [ ] while True : try : edits . append ( self . get_nowait ( ) ) except queue . Empty : break for e in edits : try : e ( ) except : log . error ( 'Error on edit %s' , e ) traceback . print_exc ( )
10425	def infer_missing_backwards_edge ( graph , u , v , k ) : if u in graph [ v ] : for attr_dict in graph [ v ] [ u ] . values ( ) : if attr_dict == graph [ u ] [ v ] [ k ] : return graph . add_edge ( v , u , key = k , ** graph [ u ] [ v ] [ k ] )
11578	def send_command ( self , command ) : send_message = "" for i in command : send_message += chr ( i ) for data in send_message : self . pymata . transport . write ( data )
7583	def run ( self , ipyclient = None , quiet = False , force = False , block = False , ) : if force : for key , oldfile in self . trees : if os . path . exists ( oldfile ) : os . remove ( oldfile ) if os . path . exists ( self . trees . info ) : print ( "Error: set a new name for this job or use Force flag.\nFile exists: {}" . format ( self . trees . info ) ) return if not ipyclient : proc = _call_raxml ( self . _command_list ) self . stdout = proc [ 0 ] self . stderr = proc [ 1 ] else : lbview = ipyclient . load_balanced_view ( ) self . async = lbview . apply ( _call_raxml , self . _command_list ) if not quiet : if not ipyclient : if "Overall execution time" not in self . stdout : print ( "Error in raxml run\n" + self . stdout ) else : print ( "job {} finished successfully" . format ( self . params . n ) ) else : print ( "job {} submitted to cluster" . format ( self . params . n ) )
3701	def solubility_eutectic ( T , Tm , Hm , Cpl = 0 , Cps = 0 , gamma = 1 ) : r dCp = Cpl - Cps x = exp ( - Hm / R / T * ( 1 - T / Tm ) + dCp * ( Tm - T ) / R / T - dCp / R * log ( Tm / T ) ) / gamma return x
7434	def _bufcountlines ( filename , gzipped ) : if gzipped : fin = gzip . open ( filename ) else : fin = open ( filename ) nlines = 0 buf_size = 1024 * 1024 read_f = fin . read buf = read_f ( buf_size ) while buf : nlines += buf . count ( '\n' ) buf = read_f ( buf_size ) fin . close ( ) return nlines
328	def extract_interesting_date_ranges ( returns ) : returns_dupe = returns . copy ( ) returns_dupe . index = returns_dupe . index . map ( pd . Timestamp ) ranges = OrderedDict ( ) for name , ( start , end ) in PERIODS . items ( ) : try : period = returns_dupe . loc [ start : end ] if len ( period ) == 0 : continue ranges [ name ] = period except BaseException : continue return ranges
11526	def create_small_thumbnail ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'itemId' ] = item_id response = self . request ( 'midas.thumbnailcreator.create.small.thumbnail' , parameters ) return response
547	def __flushPredictionCache ( self ) : if not self . __predictionCache : return if self . _predictionLogger is None : self . _createPredictionLogger ( ) startTime = time . time ( ) self . _predictionLogger . writeRecords ( self . __predictionCache , progressCB = self . __writeRecordsCallback ) self . _logger . info ( "Flushed prediction cache; numrows=%s; elapsed=%s sec." , len ( self . __predictionCache ) , time . time ( ) - startTime ) self . __predictionCache . clear ( )
8178	def can_reach ( self , node , traversable = lambda node , edge : True ) : if isinstance ( node , str ) : node = self . graph [ node ] for n in self . graph . nodes : n . _visited = False return proximity . depth_first_search ( self , visit = lambda n : node == n , traversable = traversable )
1697	def reduce_by_window ( self , window_config , reduce_function ) : from heronpy . streamlet . impl . reducebywindowbolt import ReduceByWindowStreamlet reduce_streamlet = ReduceByWindowStreamlet ( window_config , reduce_function , self ) self . _add_child ( reduce_streamlet ) return reduce_streamlet
9665	def clean_all ( G , settings ) : quiet = settings [ "quiet" ] recon = settings [ "recon" ] sprint = settings [ "sprint" ] error = settings [ "error" ] all_outputs = [ ] for node in G . nodes ( data = True ) : if "output" in node [ 1 ] : for item in get_all_outputs ( node [ 1 ] ) : all_outputs . append ( item ) all_outputs . append ( ".shastore" ) retcode = 0 for item in sorted ( all_outputs ) : if os . path . isfile ( item ) : if recon : sprint ( "Would remove file: {}" . format ( item ) ) continue sprint ( "Attempting to remove file '{}'" , level = "verbose" ) try : os . remove ( item ) sprint ( "Removed file" , level = "verbose" ) except : errmes = "Error: file '{}' failed to be removed" error ( errmes . format ( item ) ) retcode = 1 if not retcode and not recon : sprint ( "All clean" , color = True ) return retcode
5246	def update_missing ( ** kwargs ) : data_path = os . environ . get ( BBG_ROOT , '' ) . replace ( '\\' , '/' ) if not data_path : return if len ( kwargs ) == 0 : return log_path = f'{data_path}/Logs/{missing_info(**kwargs)}' cnt = len ( files . all_files ( log_path ) ) + 1 files . create_folder ( log_path ) open ( f'{log_path}/{cnt}.log' , 'a' ) . close ( )
1980	def sys_receive ( self , cpu , fd , buf , count , rx_bytes ) : if issymbolic ( fd ) : logger . info ( "Ask to read from a symbolic file descriptor!!" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 0 ) if issymbolic ( buf ) : logger . info ( "Ask to read to a symbolic buffer" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 1 ) if issymbolic ( count ) : logger . info ( "Ask to read a symbolic number of bytes " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 2 ) if issymbolic ( rx_bytes ) : logger . info ( "Ask to return size to a symbolic address " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 3 ) return super ( ) . sys_receive ( cpu , fd , buf , count , rx_bytes )
11157	def print_big_dir_and_big_file ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table1 = sorted ( [ ( p , p . dirsize ) for p in self . select_dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p1 , size1 in size_table1 [ : top_n ] : print ( "{:<9} {:<9}" . format ( repr_data_size ( size1 ) , p1 . abspath ) ) size_table2 = sorted ( [ ( p , p . size ) for p in p1 . select_file ( recursive = True ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p2 , size2 in size_table2 [ : top_n ] : print ( " {:<9} {:<9}" . format ( repr_data_size ( size2 ) , p2 . abspath ) )
3460	def single_reaction_deletion ( model , reaction_list = None , method = "fba" , solution = None , processes = None , ** kwargs ) : return _multi_deletion ( model , 'reaction' , element_lists = _element_lists ( model . reactions , reaction_list ) , method = method , solution = solution , processes = processes , ** kwargs )
12248	def sync ( self , * buckets ) : if buckets : for _bucket in buckets : for key in mimicdb . backend . smembers ( tpl . bucket % _bucket ) : mimicdb . backend . delete ( tpl . key % ( _bucket , key ) ) mimicdb . backend . delete ( tpl . bucket % _bucket ) bucket = self . get_bucket ( _bucket , force = True ) for key in bucket . list ( force = True ) : mimicdb . backend . sadd ( tpl . bucket % bucket . name , key . name ) mimicdb . backend . hmset ( tpl . key % ( bucket . name , key . name ) , dict ( size = key . size , md5 = key . etag . strip ( '"' ) ) ) else : for bucket in mimicdb . backend . smembers ( tpl . connection ) : for key in mimicdb . backend . smembers ( tpl . bucket % bucket ) : mimicdb . backend . delete ( tpl . key % ( bucket , key ) ) mimicdb . backend . delete ( tpl . bucket % bucket ) for bucket in self . get_all_buckets ( force = True ) : for key in bucket . list ( force = True ) : mimicdb . backend . sadd ( tpl . bucket % bucket . name , key . name ) mimicdb . backend . hmset ( tpl . key % ( bucket . name , key . name ) , dict ( size = key . size , md5 = key . etag . strip ( '"' ) ) )
5639	def _temporal_distance_pdf ( self ) : temporal_distance_split_points_ordered , norm_cdf = self . _temporal_distance_cdf ( ) delta_peak_loc_to_probability_mass = { } non_delta_peak_split_points = [ temporal_distance_split_points_ordered [ 0 ] ] non_delta_peak_densities = [ ] for i in range ( 0 , len ( temporal_distance_split_points_ordered ) - 1 ) : left = temporal_distance_split_points_ordered [ i ] right = temporal_distance_split_points_ordered [ i + 1 ] width = right - left prob_mass = norm_cdf [ i + 1 ] - norm_cdf [ i ] if width == 0.0 : delta_peak_loc_to_probability_mass [ left ] = prob_mass else : non_delta_peak_split_points . append ( right ) non_delta_peak_densities . append ( prob_mass / float ( width ) ) assert ( len ( non_delta_peak_densities ) == len ( non_delta_peak_split_points ) - 1 ) return numpy . array ( non_delta_peak_split_points ) , numpy . array ( non_delta_peak_densities ) , delta_peak_loc_to_probability_mass
9130	def store_drop ( cls , resource : str , session : Optional [ Session ] = None ) -> 'Action' : action = cls . make_drop ( resource ) _store_helper ( action , session = session ) return action
7022	def pklc_fovcatalog_objectinfo ( pklcdir , fovcatalog , fovcatalog_columns = [ 0 , 1 , 2 , 6 , 7 , 8 , 9 , 10 , 11 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 ] , fovcatalog_colnames = [ 'objectid' , 'ra' , 'decl' , 'jmag' , 'jmag_err' , 'hmag' , 'hmag_err' , 'kmag' , 'kmag_err' , 'bmag' , 'vmag' , 'rmag' , 'imag' , 'sdssu' , 'sdssg' , 'sdssr' , 'sdssi' , 'sdssz' ] , fovcatalog_colformats = ( 'U20,f8,f8,' 'f8,f8,' 'f8,f8,' 'f8,f8,' 'f8,f8,f8,f8,' 'f8,f8,f8,' 'f8,f8' ) ) : if fovcatalog . endswith ( '.gz' ) : catfd = gzip . open ( fovcatalog ) else : catfd = open ( fovcatalog ) fovcat = np . genfromtxt ( catfd , usecols = fovcatalog_columns , names = fovcatalog_colnames , dtype = fovcatalog_colformats ) catfd . close ( ) pklclist = sorted ( glob . glob ( os . path . join ( pklcdir , '*HAT*-pklc.pkl' ) ) ) updatedpklcs , failedpklcs = [ ] , [ ] for pklc in pklclist : lcdict = read_hatpi_pklc ( pklc ) objectid = lcdict [ 'objectid' ] catind = np . where ( fovcat [ 'objectid' ] == objectid ) if len ( catind ) > 0 and catind [ 0 ] : lcdict [ 'objectinfo' ] . update ( { x : y for x , y in zip ( fovcatalog_colnames , [ np . asscalar ( fovcat [ z ] [ catind ] ) for z in fovcatalog_colnames ] ) } ) with open ( pklc + '-tmp' , 'wb' ) as outfd : pickle . dump ( lcdict , outfd , pickle . HIGHEST_PROTOCOL ) if os . path . exists ( pklc + '-tmp' ) : shutil . move ( pklc + '-tmp' , pklc ) LOGINFO ( 'updated %s with catalog info for %s at %.3f, %.3f OK' % ( pklc , objectid , lcdict [ 'objectinfo' ] [ 'ra' ] , lcdict [ 'objectinfo' ] [ 'decl' ] ) ) updatedpklcs . append ( pklc ) else : failedpklcs . append ( pklc ) return updatedpklcs , failedpklcs
3111	def locked_get ( self ) : serialized = self . _dictionary . get ( self . _key ) if serialized is None : return None credentials = client . OAuth2Credentials . from_json ( serialized ) credentials . set_store ( self ) return credentials
521	def _initPermanence ( self , potential , connectedPct ) : perm = numpy . zeros ( self . _numInputs , dtype = realDType ) for i in xrange ( self . _numInputs ) : if ( potential [ i ] < 1 ) : continue if ( self . _random . getReal64 ( ) <= connectedPct ) : perm [ i ] = self . _initPermConnected ( ) else : perm [ i ] = self . _initPermNonConnected ( ) perm [ perm < self . _synPermTrimThreshold ] = 0 return perm
11360	def fix_dashes ( string ) : string = string . replace ( u'\u05BE' , '-' ) string = string . replace ( u'\u1806' , '-' ) string = string . replace ( u'\u2E3A' , '-' ) string = string . replace ( u'\u2E3B' , '-' ) string = unidecode ( string ) return re . sub ( r'--+' , '-' , string )
8876	def compute_dosage ( expec , alt = None ) : r if alt is None : return expec [ ... , - 1 ] try : return expec [ : , alt ] except NotImplementedError : alt = asarray ( alt , int ) return asarray ( expec , float ) [ : , alt ]
5047	def get ( self , request , customer_uuid ) : context = self . _build_context ( request , customer_uuid ) manage_learners_form = ManageLearnersForm ( user = request . user , enterprise_customer = context [ self . ContextParameters . ENTERPRISE_CUSTOMER ] ) context . update ( { self . ContextParameters . MANAGE_LEARNERS_FORM : manage_learners_form } ) return render ( request , self . template , context )
13794	def handle_map_doc ( self , document ) : for function in sorted ( self . functions . values ( ) , key = lambda x : x [ 0 ] ) : try : yield [ list ( function ( document ) ) ] except Exception , exc : yield [ ] self . log ( repr ( exc ) )
4103	def setup ( app ) : app . add_config_value ( 'plot_gallery' , True , 'html' ) app . add_config_value ( 'abort_on_example_error' , False , 'html' ) app . add_config_value ( 'sphinx_gallery_conf' , gallery_conf , 'html' ) app . add_stylesheet ( 'gallery.css' ) app . connect ( 'builder-inited' , generate_gallery_rst ) app . connect ( 'build-finished' , embed_code_links )
13395	def _update_settings ( self , new_settings , enforce_helpstring = True ) : for raw_setting_name , value in six . iteritems ( new_settings ) : setting_name = raw_setting_name . replace ( "_" , "-" ) setting_already_exists = setting_name in self . _instance_settings value_is_list_len_2 = isinstance ( value , list ) and len ( value ) == 2 treat_as_tuple = not setting_already_exists and value_is_list_len_2 if isinstance ( value , tuple ) or treat_as_tuple : self . _instance_settings [ setting_name ] = value else : if setting_name not in self . _instance_settings : if enforce_helpstring : msg = "You must specify param '%s' as a tuple of (helpstring, value)" raise InternalCashewException ( msg % setting_name ) else : self . _instance_settings [ setting_name ] = ( '' , value , ) else : orig = self . _instance_settings [ setting_name ] self . _instance_settings [ setting_name ] = ( orig [ 0 ] , value , )
13042	def create_query ( section ) : query = { } if 'ports' in section : query [ 'ports' ] = [ section [ 'ports' ] ] if 'up' in section : query [ 'up' ] = bool ( section [ 'up' ] ) if 'search' in section : query [ 'search' ] = [ section [ 'search' ] ] if 'tags' in section : query [ 'tags' ] = [ section [ 'tags' ] ] if 'groups' in section : query [ 'groups' ] = [ section [ 'groups' ] ] return query
6525	def get_grouped_issues ( self , keyfunc = None , sortby = None ) : if not keyfunc : keyfunc = default_group if not sortby : sortby = self . DEFAULT_SORT self . _ensure_cleaned_issues ( ) return self . _group_issues ( self . _cleaned_issues , keyfunc , sortby )
7326	def with_continuations ( ** c ) : if len ( c ) : keys , k = zip ( * c . items ( ) ) else : keys , k = tuple ( [ ] ) , tuple ( [ ] ) def d ( f ) : return C ( lambda kself , * conts : lambda * args : f ( * args , self = kself , ** dict ( zip ( keys , conts ) ) ) ) ( * k ) return d
5227	def to_str ( data : dict , fmt = '{key}={value}' , sep = ', ' , public_only = True ) -> str : if public_only : keys = list ( filter ( lambda vv : vv [ 0 ] != '_' , data . keys ( ) ) ) else : keys = list ( data . keys ( ) ) return '{' + sep . join ( [ to_str ( data = v , fmt = fmt , sep = sep ) if isinstance ( v , dict ) else fstr ( fmt = fmt , key = k , value = v ) for k , v in data . items ( ) if k in keys ] ) + '}'
3921	def get_menu_widget ( self , close_callback ) : return ConversationMenu ( self . _coroutine_queue , self . _conversation , close_callback , self . _keys )
2981	def cmd_daemon ( opts ) : if opts . data_dir is None : raise BlockadeError ( "You must supply a data directory for the daemon" ) rest . start ( data_dir = opts . data_dir , port = opts . port , debug = opts . debug , host_exec = get_host_exec ( ) )
12872	def one_of ( these ) : ch = peek ( ) try : if ( ch is EndOfFile ) or ( ch not in these ) : fail ( list ( these ) ) except TypeError : if ch != these : fail ( [ these ] ) next ( ) return ch
244	def days_to_liquidate_positions ( positions , market_data , max_bar_consumption = 0.2 , capital_base = 1e6 , mean_volume_window = 5 ) : DV = market_data [ 'volume' ] * market_data [ 'price' ] roll_mean_dv = DV . rolling ( window = mean_volume_window , center = False ) . mean ( ) . shift ( ) roll_mean_dv = roll_mean_dv . replace ( 0 , np . nan ) positions_alloc = pos . get_percent_alloc ( positions ) positions_alloc = positions_alloc . drop ( 'cash' , axis = 1 ) days_to_liquidate = ( positions_alloc * capital_base ) / ( max_bar_consumption * roll_mean_dv ) return days_to_liquidate . iloc [ mean_volume_window : ]
4359	def spawn ( self , fn , * args , ** kwargs ) : log . debug ( "Spawning sub-Socket Greenlet: %s" % fn . __name__ ) job = gevent . spawn ( fn , * args , ** kwargs ) self . jobs . append ( job ) return job
7812	def _decode_validity ( self , validity ) : not_after = validity . getComponentByName ( 'notAfter' ) not_after = str ( not_after . getComponent ( ) ) if isinstance ( not_after , GeneralizedTime ) : self . not_after = datetime . strptime ( not_after , "%Y%m%d%H%M%SZ" ) else : self . not_after = datetime . strptime ( not_after , "%y%m%d%H%M%SZ" ) self . alt_names = defaultdict ( list )
4663	def new_tx ( self , * args , ** kwargs ) : builder = self . transactionbuilder_class ( * args , blockchain_instance = self , ** kwargs ) self . _txbuffers . append ( builder ) return builder
8474	def getConfig ( self , section = None ) : data = { } if section is None : for s in self . config . sections ( ) : if '/' in s : parent , _s = s . split ( '/' ) data [ parent ] [ _s ] = dict ( self . config . items ( s ) ) else : data [ s ] = dict ( self . config . items ( s ) ) else : data = dict ( self . config . items ( section ) ) return data
5037	def _handle_bulk_upload ( cls , enterprise_customer , manage_learners_form , request , email_list = None ) : errors = [ ] emails = set ( ) already_linked_emails = [ ] duplicate_emails = [ ] csv_file = manage_learners_form . cleaned_data [ ManageLearnersForm . Fields . BULK_UPLOAD ] if email_list : parsed_csv = [ { ManageLearnersForm . CsvColumns . EMAIL : email } for email in email_list ] else : parsed_csv = parse_csv ( csv_file , expected_columns = { ManageLearnersForm . CsvColumns . EMAIL } ) try : for index , row in enumerate ( parsed_csv ) : email = row [ ManageLearnersForm . CsvColumns . EMAIL ] try : already_linked = validate_email_to_link ( email , ignore_existing = True ) except ValidationError as exc : message = _ ( "Error at line {line}: {message}\n" ) . format ( line = index + 1 , message = exc ) errors . append ( message ) else : if already_linked : already_linked_emails . append ( ( email , already_linked . enterprise_customer ) ) elif email in emails : duplicate_emails . append ( email ) else : emails . add ( email ) except ValidationError as exc : errors . append ( exc ) if errors : manage_learners_form . add_error ( ManageLearnersForm . Fields . GENERAL_ERRORS , ValidationMessages . BULK_LINK_FAILED ) for error in errors : manage_learners_form . add_error ( ManageLearnersForm . Fields . BULK_UPLOAD , error ) return for email in emails : EnterpriseCustomerUser . objects . link_user ( enterprise_customer , email ) count = len ( emails ) messages . success ( request , ungettext ( "{count} new learner was added to {enterprise_customer_name}." , "{count} new learners were added to {enterprise_customer_name}." , count ) . format ( count = count , enterprise_customer_name = enterprise_customer . name ) ) this_customer_linked_emails = [ email for email , customer in already_linked_emails if customer == enterprise_customer ] other_customer_linked_emails = [ email for email , __ in already_linked_emails if email not in this_customer_linked_emails ] if this_customer_linked_emails : messages . warning ( request , _ ( "The following learners were already associated with this Enterprise " "Customer: {list_of_emails}" ) . format ( list_of_emails = ", " . join ( this_customer_linked_emails ) ) ) if other_customer_linked_emails : messages . warning ( request , _ ( "The following learners are already associated with " "another Enterprise Customer. These learners were not " "added to {enterprise_customer_name}: {list_of_emails}" ) . format ( enterprise_customer_name = enterprise_customer . name , list_of_emails = ", " . join ( other_customer_linked_emails ) , ) ) if duplicate_emails : messages . warning ( request , _ ( "The following duplicate email addresses were not added: " "{list_of_emails}" ) . format ( list_of_emails = ", " . join ( duplicate_emails ) ) ) all_processable_emails = list ( emails ) + this_customer_linked_emails return all_processable_emails
9429	def extract ( self , member , path = None , pwd = None ) : if isinstance ( member , RarInfo ) : member = member . filename if path is None : path = os . getcwd ( ) self . _extract_members ( [ member ] , path , pwd ) return os . path . join ( path , member )
9571	def discovery_view ( self , message ) : for handler in self . registered_handlers : if handler . check ( message ) : return handler . view return None
5609	def tiles_to_affine_shape ( tiles ) : if not tiles : raise TypeError ( "no tiles provided" ) pixel_size = tiles [ 0 ] . pixel_x_size left , bottom , right , top = ( min ( [ t . left for t in tiles ] ) , min ( [ t . bottom for t in tiles ] ) , max ( [ t . right for t in tiles ] ) , max ( [ t . top for t in tiles ] ) , ) return ( Affine ( pixel_size , 0 , left , 0 , - pixel_size , top ) , Shape ( width = int ( round ( ( right - left ) / pixel_size , 0 ) ) , height = int ( round ( ( top - bottom ) / pixel_size , 0 ) ) , ) )
12636	def dist_percentile_threshold ( dist_matrix , perc_thr = 0.05 , k = 1 ) : triu_idx = np . triu_indices ( dist_matrix . shape [ 0 ] , k = k ) upper = np . zeros_like ( dist_matrix ) upper [ triu_idx ] = dist_matrix [ triu_idx ] < np . percentile ( dist_matrix [ triu_idx ] , perc_thr ) return upper
10674	def load_data_auxi ( path = '' ) : compounds . clear ( ) if path == '' : path = default_data_path if not os . path . exists ( path ) : warnings . warn ( 'The specified data file path does not exist. (%s)' % path ) return files = glob . glob ( os . path . join ( path , 'Compound_*.json' ) ) for file in files : compound = Compound . read ( file ) compounds [ compound . formula ] = compound
4604	def history ( self , first = 0 , last = 0 , limit = - 1 , only_ops = [ ] , exclude_ops = [ ] ) : _limit = 100 cnt = 0 if first < 0 : first = 0 while True : txs = self . blockchain . rpc . get_account_history ( self [ "id" ] , "1.11.{}" . format ( last ) , _limit , "1.11.{}" . format ( first - 1 ) , api = "history" , ) for i in txs : if ( exclude_ops and self . operations . getOperationNameForId ( i [ "op" ] [ 0 ] ) in exclude_ops ) : continue if ( not only_ops or self . operations . getOperationNameForId ( i [ "op" ] [ 0 ] ) in only_ops ) : cnt += 1 yield i if limit >= 0 and cnt >= limit : return if not txs : log . info ( "No more history returned from API node" ) break if len ( txs ) < _limit : log . info ( "Less than {} have been returned." . format ( _limit ) ) break first = int ( txs [ - 1 ] [ "id" ] . split ( "." ) [ 2 ] )
8881	def fit ( self , X , y = None ) : X = check_array ( X ) self . inverse_influence_matrix = self . __make_inverse_matrix ( X ) if self . threshold == 'auto' : self . threshold_value = 3 * ( 1 + X . shape [ 1 ] ) / X . shape [ 0 ] elif self . threshold == 'cv' : if y is None : raise ValueError ( "Y must be specified to find the optimal threshold." ) y = check_array ( y , accept_sparse = 'csc' , ensure_2d = False , dtype = None ) self . threshold_value = 0 score = 0 Y_pred , Y_true , AD = [ ] , [ ] , [ ] cv = KFold ( n_splits = 5 , random_state = 1 , shuffle = True ) for train_index , test_index in cv . split ( X ) : x_train = safe_indexing ( X , train_index ) x_test = safe_indexing ( X , test_index ) y_train = safe_indexing ( y , train_index ) y_test = safe_indexing ( y , test_index ) if self . reg_model is None : reg_model = RandomForestRegressor ( n_estimators = 500 , random_state = 1 ) . fit ( x_train , y_train ) else : reg_model = clone ( self . reg_model ) . fit ( x_train , y_train ) Y_pred . append ( reg_model . predict ( x_test ) ) Y_true . append ( y_test ) ad_model = self . __make_inverse_matrix ( x_train ) AD . append ( self . __find_leverages ( x_test , ad_model ) ) AD_ = unique ( hstack ( AD ) ) for z in AD_ : AD_new = hstack ( AD ) <= z if self . score == 'ba_ad' : val = balanced_accuracy_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) elif self . score == 'rmse_ad' : val = rmse_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) if val >= score : score = val self . threshold_value = z else : self . threshold_value = self . threshold return self
5936	def col ( self , c ) : m = self . COLOUR . search ( c ) if not m : self . logger . fatal ( "Cannot parse colour specification %r." , c ) raise ParseError ( "XPM reader: Cannot parse colour specification {0!r}." . format ( c ) ) value = m . group ( 'value' ) color = m . group ( 'symbol' ) self . logger . debug ( "%s: %s %s\n" , c . strip ( ) , color , value ) return color , value
8286	def _get_elements ( self ) : for index , el in enumerate ( self . _elements ) : if isinstance ( el , tuple ) : el = PathElement ( * el ) self . _elements [ index ] = el yield el
13358	def aot40_vegetation ( df , nb_an ) : return _aot ( df . tshift ( 1 ) , nb_an = nb_an , limite = 80 , mois_debut = 5 , mois_fin = 7 , heure_debut = 8 , heure_fin = 19 )
6651	def build ( self , builddir , component , args , release_build = False , build_args = None , targets = None , release_no_debug_info_build = False ) : if build_args is None : build_args = [ ] if targets is None : targets = [ ] if release_no_debug_info_build : build_type = 'Release' elif release_build : build_type = 'RelWithDebInfo' else : build_type = 'Debug' cmd = [ 'cmake' , '-D' , 'CMAKE_BUILD_TYPE=%s' % build_type , '-G' , args . cmake_generator , '.' ] res = self . exec_helper ( cmd , builddir ) if res is not None : return res from yotta . lib import cmake_fixups cmake_fixups . applyFixupsForFenerator ( args . cmake_generator , builddir , component ) build_command = self . overrideBuildCommand ( args . cmake_generator , targets = targets ) if build_command : cmd = build_command + build_args else : cmd = [ 'cmake' , '--build' , builddir ] if len ( targets ) : cmd += [ '--target' , targets [ 0 ] ] cmd += build_args res = self . exec_helper ( cmd , builddir ) if res is not None : return res hint = self . hintForCMakeGenerator ( args . cmake_generator , component ) if hint : logger . info ( hint )
3006	def get_storage ( request ) : storage_model = oauth2_settings . storage_model user_property = oauth2_settings . storage_model_user_property credentials_property = oauth2_settings . storage_model_credentials_property if storage_model : module_name , class_name = storage_model . rsplit ( '.' , 1 ) module = importlib . import_module ( module_name ) storage_model_class = getattr ( module , class_name ) return storage . DjangoORMStorage ( storage_model_class , user_property , request . user , credentials_property ) else : return dictionary_storage . DictionaryStorage ( request . session , key = _CREDENTIALS_KEY )
4917	def contains_content_items ( self , request , pk , course_run_ids , program_uuids ) : enterprise_customer_catalog = self . get_object ( ) course_run_ids = [ unquote ( quote_plus ( course_run_id ) ) for course_run_id in course_run_ids ] contains_content_items = True if course_run_ids : contains_content_items = enterprise_customer_catalog . contains_courses ( course_run_ids ) if program_uuids : contains_content_items = ( contains_content_items and enterprise_customer_catalog . contains_programs ( program_uuids ) ) return Response ( { 'contains_content_items' : contains_content_items } )
9616	def is_displayed ( target ) : is_displayed = getattr ( target , 'is_displayed' , None ) if not is_displayed or not callable ( is_displayed ) : raise TypeError ( 'Target has no attribute \'is_displayed\' or not callable' ) if not is_displayed ( ) : raise WebDriverException ( 'element not visible' )
11297	def _check_for_exceptions ( self , resp , multiple_rates ) : if resp [ 'rCode' ] != 100 : raise exceptions . get_exception_for_code ( resp [ 'rCode' ] ) ( resp ) results = resp [ 'results' ] if len ( results ) == 0 : raise exceptions . ZipTaxNoResults ( 'No results found' ) if len ( results ) > 1 and not multiple_rates : rates = [ result [ 'taxSales' ] for result in results ] if len ( set ( rates ) ) != 1 : raise exceptions . ZipTaxMultipleResults ( 'Multiple results found but requested only one' )
8690	def put ( self , key ) : self . _consul_request ( 'PUT' , self . _key_url ( key [ 'name' ] ) , json = key ) return key [ 'name' ]
2636	def update_parent ( self , fut ) : self . parent = fut try : fut . add_done_callback ( self . parent_callback ) except Exception as e : logger . error ( "add_done_callback got an exception {} which will be ignored" . format ( e ) )
11456	def load_config ( from_key , to_key ) : from . mappings import mappings kbs = { } for key , values in mappings [ 'config' ] . iteritems ( ) : parse_dict = { } for mapping in values : parse_dict [ mapping [ from_key ] ] = mapping [ to_key ] kbs [ key ] = parse_dict return kbs
1601	def chain ( cmd_list ) : command = ' | ' . join ( map ( lambda x : ' ' . join ( x ) , cmd_list ) ) chained_proc = functools . reduce ( pipe , [ None ] + cmd_list ) stdout_builder = proc . async_stdout_builder ( chained_proc ) chained_proc . wait ( ) return { 'command' : command , 'stdout' : stdout_builder . result ( ) }
2259	def dzip ( items1 , items2 , cls = dict ) : try : len ( items1 ) except TypeError : items1 = list ( items1 ) try : len ( items2 ) except TypeError : items2 = list ( items2 ) if len ( items1 ) == 0 and len ( items2 ) == 1 : items2 = [ ] if len ( items2 ) == 1 and len ( items1 ) > 1 : items2 = items2 * len ( items1 ) if len ( items1 ) != len ( items2 ) : raise ValueError ( 'out of alignment len(items1)=%r, len(items2)=%r' % ( len ( items1 ) , len ( items2 ) ) ) return cls ( zip ( items1 , items2 ) )
443	def _get_init_args ( self , skip = 4 ) : stack = inspect . stack ( ) if len ( stack ) < skip + 1 : raise ValueError ( "The length of the inspection stack is shorter than the requested start position." ) args , _ , _ , values = inspect . getargvalues ( stack [ skip ] [ 0 ] ) params = { } for arg in args : if values [ arg ] is not None and arg not in [ 'self' , 'prev_layer' , 'inputs' ] : val = values [ arg ] if inspect . isfunction ( val ) : params [ arg ] = { "module_path" : val . __module__ , "func_name" : val . __name__ } elif arg . endswith ( 'init' ) : continue else : params [ arg ] = val return params
1141	def dedent ( text ) : margin = None text = _whitespace_only_re . sub ( '' , text ) indents = _leading_whitespace_re . findall ( text ) for indent in indents : if margin is None : margin = indent elif indent . startswith ( margin ) : pass elif margin . startswith ( indent ) : margin = indent else : for i , ( x , y ) in enumerate ( zip ( margin , indent ) ) : if x != y : margin = margin [ : i ] break else : margin = margin [ : len ( indent ) ] if 0 and margin : for line in text . split ( "\n" ) : assert not line or line . startswith ( margin ) , "line = %r, margin = %r" % ( line , margin ) if margin : text = re . sub ( r'(?m)^' + margin , '' , text ) return text
5630	def _postreceive ( self ) : digest = self . _get_digest ( ) if digest is not None : sig_parts = _get_header ( 'X-Hub-Signature' ) . split ( '=' , 1 ) if not isinstance ( digest , six . text_type ) : digest = six . text_type ( digest ) if ( len ( sig_parts ) < 2 or sig_parts [ 0 ] != 'sha1' or not hmac . compare_digest ( sig_parts [ 1 ] , digest ) ) : abort ( 400 , 'Invalid signature' ) event_type = _get_header ( 'X-Github-Event' ) data = request . get_json ( ) if data is None : abort ( 400 , 'Request body must contain json' ) self . _logger . info ( '%s (%s)' , _format_event ( event_type , data ) , _get_header ( 'X-Github-Delivery' ) ) for hook in self . _hooks . get ( event_type , [ ] ) : hook ( data ) return '' , 204
5165	def __intermediate_proto ( self , interface , address ) : address_proto = address . pop ( 'proto' , 'static' ) if 'proto' not in interface : return address_proto else : return interface . pop ( 'proto' )
9700	def worker ( wrapped , dkwargs , hash_value = None , * args , ** kwargs ) : if "event" not in dkwargs : msg = "djwebhooks.decorators.redis_hook requires an 'event' argument in the decorator." raise TypeError ( msg ) event = dkwargs [ 'event' ] if "owner" not in kwargs : msg = "djwebhooks.senders.redis_callable requires an 'owner' argument in the decorated function." raise TypeError ( msg ) owner = kwargs [ 'owner' ] if "identifier" not in kwargs : msg = "djwebhooks.senders.orm_callable requires an 'identifier' argument in the decorated function." raise TypeError ( msg ) identifier = kwargs [ 'identifier' ] senderobj = DjangoRQSenderable ( wrapped , dkwargs , hash_value , WEBHOOK_ATTEMPTS , * args , ** kwargs ) senderobj . webhook_target = WebhookTarget . objects . get ( event = event , owner = owner , identifier = identifier ) senderobj . url = senderobj . webhook_target . target_url senderobj . payload = senderobj . get_payload ( ) senderobj . payload [ 'owner' ] = getattr ( kwargs [ 'owner' ] , WEBHOOK_OWNER_FIELD ) senderobj . payload [ 'event' ] = dkwargs [ 'event' ] return senderobj . send ( )
12994	def table ( cluster ) : teffs = teff ( cluster ) lums = luminosity ( cluster ) arr = cluster . to_array ( ) i = 0 for row in arr : row [ 'lum' ] [ 0 ] = np . array ( [ lums [ i ] ] , dtype = 'f' ) row [ 'temp' ] [ 0 ] = np . array ( [ teffs [ i ] ] , dtype = 'f' ) i += 1 arr = round_arr_teff_luminosity ( arr ) return arr
8691	def put ( self , key ) : self . client . write ( self . _key_path ( key [ 'name' ] ) , ** key ) return self . _key_path ( key [ 'name' ] )
484	def enableConcurrencyChecks ( maxConcurrency , raiseException = True ) : global g_max_concurrency , g_max_concurrency_raise_exception assert maxConcurrency >= 0 g_max_concurrency = maxConcurrency g_max_concurrency_raise_exception = raiseException return
1719	def emit ( self , what , * args ) : if isinstance ( what , basestring ) : return self . exe . emit ( what , * args ) elif isinstance ( what , list ) : self . _emit_statement_list ( what ) else : return getattr ( self , what [ 'type' ] ) ( ** what )
4815	def create_feature_array ( text , n_pad = 21 ) : n = len ( text ) n_pad_2 = int ( ( n_pad - 1 ) / 2 ) text_pad = [ ' ' ] * n_pad_2 + [ t for t in text ] + [ ' ' ] * n_pad_2 x_char , x_type = [ ] , [ ] for i in range ( n_pad_2 , n_pad_2 + n ) : char_list = text_pad [ i + 1 : i + n_pad_2 + 1 ] + list ( reversed ( text_pad [ i - n_pad_2 : i ] ) ) + [ text_pad [ i ] ] char_map = [ CHARS_MAP . get ( c , 80 ) for c in char_list ] char_type = [ CHAR_TYPES_MAP . get ( CHAR_TYPE_FLATTEN . get ( c , 'o' ) , 4 ) for c in char_list ] x_char . append ( char_map ) x_type . append ( char_type ) x_char = np . array ( x_char ) . astype ( float ) x_type = np . array ( x_type ) . astype ( float ) return x_char , x_type
10096	def create_new_locale ( self , template_id , locale , version_name , subject , text = '' , html = '' , timeout = None ) : payload = { 'locale' : locale , 'name' : version_name , 'subject' : subject } if html : payload [ 'html' ] = html if text : payload [ 'text' ] = text return self . _api_request ( self . TEMPLATES_LOCALES_ENDPOINT % template_id , self . HTTP_POST , payload = payload , timeout = timeout )
12393	def use ( ** kwargs ) : config = dict ( use . config ) use . config . update ( kwargs ) return config
8942	def upload ( self , docs_base , release ) : return getattr ( self , '_to_' + self . target ) ( docs_base , release )
10414	def node_exclusion_filter_builder ( nodes : Iterable [ BaseEntity ] ) -> NodePredicate : node_set = set ( nodes ) def exclusion_filter ( _ : BELGraph , node : BaseEntity ) -> bool : return node not in node_set return exclusion_filter
5419	def _google_v2_parse_arguments ( args ) : if ( args . zones and args . regions ) or ( not args . zones and not args . regions ) : raise ValueError ( 'Exactly one of --regions and --zones must be specified' ) if args . machine_type and ( args . min_cores or args . min_ram ) : raise ValueError ( '--machine-type not supported together with --min-cores or --min-ram.' )
13513	def reynolds_number ( length , speed , temperature = 25 ) : kinematic_viscosity = interpolate . interp1d ( [ 0 , 10 , 20 , 25 , 30 , 40 ] , np . array ( [ 18.54 , 13.60 , 10.50 , 9.37 , 8.42 , 6.95 ] ) / 10 ** 7 ) Re = length * speed / kinematic_viscosity ( temperature ) return Re
6956	def _transit_model ( times , t0 , per , rp , a , inc , ecc , w , u , limb_dark , exp_time_minutes = 2 , supersample_factor = 7 ) : params = batman . TransitParams ( ) params . t0 = t0 params . per = per params . rp = rp params . a = a params . inc = inc params . ecc = ecc params . w = w params . u = u params . limb_dark = limb_dark t = times m = batman . TransitModel ( params , t , exp_time = exp_time_minutes / 60. / 24. , supersample_factor = supersample_factor ) return params , m
3861	def add_event ( self , event_ ) : conv_event = self . _wrap_event ( event_ ) if conv_event . id_ not in self . _events_dict : self . _events . append ( conv_event ) self . _events_dict [ conv_event . id_ ] = conv_event else : logger . info ( 'Conversation %s ignoring duplicate event %s' , self . id_ , conv_event . id_ ) return None return conv_event
998	def printConfidence ( self , aState , maxCols = 20 ) : def formatFPRow ( var , i ) : s = '' for c in range ( min ( maxCols , self . numberOfCols ) ) : if c > 0 and c % 10 == 0 : s += ' ' s += ' %5.3f' % var [ c , i ] s += ' ' return s for i in xrange ( self . cellsPerColumn ) : print formatFPRow ( aState , i )
2101	def log ( s , header = '' , file = sys . stderr , nl = 1 , ** kwargs ) : if not settings . verbose : return if header : word_arr = s . split ( ' ' ) multi = [ ] word_arr . insert ( 0 , '%s:' % header . upper ( ) ) i = 0 while i < len ( word_arr ) : to_add = [ '***' ] count = 3 while count <= 79 : count += len ( word_arr [ i ] ) + 1 if count <= 79 : to_add . append ( word_arr [ i ] ) i += 1 if i == len ( word_arr ) : break if len ( to_add ) == 1 : to_add . append ( word_arr [ i ] ) i += 1 if i != len ( word_arr ) : count -= len ( word_arr [ i ] ) + 1 to_add . append ( '*' * ( 78 - count ) ) multi . append ( ' ' . join ( to_add ) ) s = '\n' . join ( multi ) lines = len ( multi ) else : lines = 1 if isinstance ( nl , int ) and nl > lines : s += '\n' * ( nl - lines ) return secho ( s , file = file , ** kwargs )
6741	def get_os_version ( ) : import warnings warnings . filterwarnings ( "ignore" , category = DeprecationWarning ) common_os_version = get_rc ( 'common_os_version' ) if common_os_version : return common_os_version with settings ( warn_only = True ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : ret = _run_or_local ( 'cat /etc/lsb-release' ) if ret . succeeded : return OS ( type = LINUX , distro = UBUNTU , release = re . findall ( r'DISTRIB_RELEASE=([0-9\.]+)' , ret ) [ 0 ] ) ret = _run_or_local ( 'cat /etc/debian_version' ) if ret . succeeded : return OS ( type = LINUX , distro = DEBIAN , release = re . findall ( r'([0-9\.]+)' , ret ) [ 0 ] ) ret = _run_or_local ( 'cat /etc/fedora-release' ) if ret . succeeded : return OS ( type = LINUX , distro = FEDORA , release = re . findall ( r'release ([0-9]+)' , ret ) [ 0 ] ) raise Exception ( 'Unable to determine OS version.' )
6601	def put_package ( self , package ) : self . last_package_index += 1 package_index = self . last_package_index package_fullpath = self . package_fullpath ( package_index ) with gzip . open ( package_fullpath , 'wb' ) as f : pickle . dump ( package , f , protocol = pickle . HIGHEST_PROTOCOL ) f . close ( ) result_fullpath = self . result_fullpath ( package_index ) result_dir = os . path . dirname ( result_fullpath ) alphatwirl . mkdir_p ( result_dir ) return package_index
8879	def fit ( self , X , y = None ) : X = check_array ( X ) self . tree = BallTree ( X , leaf_size = self . leaf_size , metric = self . metric ) dist_train = self . tree . query ( X , k = 2 ) [ 0 ] if self . threshold == 'auto' : self . threshold_value = 0.5 * sqrt ( var ( dist_train [ : , 1 ] ) ) + mean ( dist_train [ : , 1 ] ) elif self . threshold == 'cv' : if y is None : raise ValueError ( "Y must be specified to find the optimal threshold." ) y = check_array ( y , accept_sparse = 'csc' , ensure_2d = False , dtype = None ) self . threshold_value = 0 score = 0 Y_pred , Y_true , AD = [ ] , [ ] , [ ] cv = KFold ( n_splits = 5 , random_state = 1 , shuffle = True ) for train_index , test_index in cv . split ( X ) : x_train = safe_indexing ( X , train_index ) x_test = safe_indexing ( X , test_index ) y_train = safe_indexing ( y , train_index ) y_test = safe_indexing ( y , test_index ) data_test = safe_indexing ( dist_train [ : , 1 ] , test_index ) if self . reg_model is None : reg_model = RandomForestRegressor ( n_estimators = 500 , random_state = 1 ) . fit ( x_train , y_train ) else : reg_model = clone ( self . reg_model ) . fit ( x_train , y_train ) Y_pred . append ( reg_model . predict ( x_test ) ) Y_true . append ( y_test ) AD . append ( data_test ) AD_ = unique ( hstack ( AD ) ) for z in AD_ : AD_new = hstack ( AD ) <= z if self . score == 'ba_ad' : val = balanced_accuracy_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) elif self . score == 'rmse_ad' : val = rmse_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) if val >= score : score = val self . threshold_value = z else : self . threshold_value = self . threshold return self
6784	def unlock ( self ) : self . init ( ) r = self . local_renderer if self . file_exists ( r . env . lockfile_path ) : self . vprint ( 'Unlocking %s.' % r . env . lockfile_path ) r . run_or_local ( 'rm -f {lockfile_path}' )
11471	def upload ( self , filename , location = '' ) : current_folder = self . _ftp . pwd ( ) self . mkdir ( location ) self . cd ( location ) fl = open ( filename , 'rb' ) filename = filename . split ( '/' ) [ - 1 ] self . _ftp . storbinary ( 'STOR %s' % filename , fl ) fl . close ( ) self . cd ( current_folder )
4263	def filter_nomedia ( album , settings = None ) : nomediapath = os . path . join ( album . src_path , ".nomedia" ) if os . path . isfile ( nomediapath ) : if os . path . getsize ( nomediapath ) == 0 : logger . info ( "Ignoring album '%s' because of present 0-byte " ".nomedia file" , album . name ) _remove_albums_with_subdirs ( album . gallery . albums , [ album . path ] ) try : os . rmdir ( album . dst_path ) except OSError as e : pass album . subdirs = [ ] album . medias = [ ] else : with open ( nomediapath , "r" ) as nomediaFile : logger . info ( "Found a .nomedia file in %s, ignoring its " "entries" , album . name ) ignored = nomediaFile . read ( ) . split ( "\n" ) album . medias = [ media for media in album . medias if media . src_filename not in ignored ] album . subdirs = [ dirname for dirname in album . subdirs if dirname not in ignored ] _remove_albums_with_subdirs ( album . gallery . albums , ignored , album . path + os . path . sep )
13603	def error_message ( self , message , fh = None , prefix = "[error]:" , suffix = "..." ) : msg = prefix + message + suffix fh = fh or sys . stderr if fh is sys . stderr : termcolor . cprint ( msg , color = "red" ) else : fh . write ( msg ) pass
9878	def _coincidences ( value_counts , value_domain , dtype = np . float64 ) : value_counts_matrices = value_counts . reshape ( value_counts . shape + ( 1 , ) ) pairable = np . maximum ( np . sum ( value_counts , axis = 1 ) , 2 ) diagonals = np . tile ( np . eye ( len ( value_domain ) ) , ( len ( value_counts ) , 1 , 1 ) ) * value_counts . reshape ( ( value_counts . shape [ 0 ] , 1 , value_counts . shape [ 1 ] ) ) unnormalized_coincidences = value_counts_matrices * value_counts_matrices . transpose ( ( 0 , 2 , 1 ) ) - diagonals return np . sum ( np . divide ( unnormalized_coincidences , ( pairable - 1 ) . reshape ( ( - 1 , 1 , 1 ) ) , dtype = dtype ) , axis = 0 )
12497	def as_ndarray ( arr , copy = False , dtype = None , order = 'K' ) : if order not in ( 'C' , 'F' , 'A' , 'K' , None ) : raise ValueError ( "Invalid value for 'order': {}" . format ( str ( order ) ) ) if isinstance ( arr , np . memmap ) : if dtype is None : if order in ( 'K' , 'A' , None ) : ret = np . array ( np . asarray ( arr ) , copy = True ) else : ret = np . array ( np . asarray ( arr ) , copy = True , order = order ) else : if order in ( 'K' , 'A' , None ) : ret = np . asarray ( arr ) . astype ( dtype ) else : ret = _asarray ( np . array ( arr , copy = True ) , dtype = dtype , order = order ) elif isinstance ( arr , np . ndarray ) : ret = _asarray ( arr , dtype = dtype , order = order ) if np . may_share_memory ( ret , arr ) and copy : ret = ret . T . copy ( ) . T if ret . flags [ 'F_CONTIGUOUS' ] else ret . copy ( ) elif isinstance ( arr , ( list , tuple ) ) : if order in ( "A" , "K" ) : ret = np . asarray ( arr , dtype = dtype ) else : ret = np . asarray ( arr , dtype = dtype , order = order ) else : raise ValueError ( "Type not handled: {}" . format ( arr . __class__ ) ) return ret
13061	def get_passage ( self , objectId , subreference ) : passage = self . resolver . getTextualNode ( textId = objectId , subreference = subreference , metadata = True ) return passage
12298	def discover_all_plugins ( self ) : for v in pkg_resources . iter_entry_points ( 'dgit.plugins' ) : m = v . load ( ) m . setup ( self )
6975	def stellingwerf_pdm_theta ( times , mags , errs , frequency , binsize = 0.05 , minbin = 9 ) : period = 1.0 / frequency fold_time = times [ 0 ] phased = phase_magseries ( times , mags , period , fold_time , wrap = False , sort = True ) phases = phased [ 'phase' ] pmags = phased [ 'mags' ] bins = nparange ( 0.0 , 1.0 , binsize ) binnedphaseinds = npdigitize ( phases , bins ) binvariances = [ ] binndets = [ ] goodbins = 0 for x in npunique ( binnedphaseinds ) : thisbin_inds = binnedphaseinds == x thisbin_mags = pmags [ thisbin_inds ] if thisbin_mags . size > minbin : thisbin_variance = npvar ( thisbin_mags , ddof = 1 ) binvariances . append ( thisbin_variance ) binndets . append ( thisbin_mags . size ) goodbins = goodbins + 1 binvariances = nparray ( binvariances ) binndets = nparray ( binndets ) theta_top = npsum ( binvariances * ( binndets - 1 ) ) / ( npsum ( binndets ) - goodbins ) theta_bot = npvar ( pmags , ddof = 1 ) theta = theta_top / theta_bot return theta
3030	def _extract_id_token ( id_token ) : if type ( id_token ) == bytes : segments = id_token . split ( b'.' ) else : segments = id_token . split ( u'.' ) if len ( segments ) != 3 : raise VerifyJwtTokenError ( 'Wrong number of segments in token: {0}' . format ( id_token ) ) return json . loads ( _helpers . _from_bytes ( _helpers . _urlsafe_b64decode ( segments [ 1 ] ) ) )
318	def calc_bootstrap ( func , returns , * args , ** kwargs ) : n_samples = kwargs . pop ( 'n_samples' , 1000 ) out = np . empty ( n_samples ) factor_returns = kwargs . pop ( 'factor_returns' , None ) for i in range ( n_samples ) : idx = np . random . randint ( len ( returns ) , size = len ( returns ) ) returns_i = returns . iloc [ idx ] . reset_index ( drop = True ) if factor_returns is not None : factor_returns_i = factor_returns . iloc [ idx ] . reset_index ( drop = True ) out [ i ] = func ( returns_i , factor_returns_i , * args , ** kwargs ) else : out [ i ] = func ( returns_i , * args , ** kwargs ) return out
4696	def env ( ) : if cij . ssh . env ( ) : cij . err ( "board.env: invalid SSH environment" ) return 1 board = cij . env_to_dict ( PREFIX , REQUIRED ) if board is None : cij . err ( "board.env: invalid BOARD environment" ) return 1 board [ "CLASS" ] = "_" . join ( [ board [ r ] for r in REQUIRED [ : - 1 ] ] ) board [ "IDENT" ] = "-" . join ( [ board [ "CLASS" ] , board [ "ALIAS" ] ] ) cij . env_export ( PREFIX , EXPORTED , board ) return 0
6629	def read ( self , filenames ) : for fn in filenames : try : self . configs [ fn ] = ordered_json . load ( fn ) except IOError : self . configs [ fn ] = OrderedDict ( ) except Exception as e : self . configs [ fn ] = OrderedDict ( ) logging . warning ( "Failed to read settings file %s, it will be ignored. The error was: %s" , fn , e )
159	def Grayscale ( alpha = 0 , from_colorspace = "RGB" , name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return ChangeColorspace ( to_colorspace = ChangeColorspace . GRAY , alpha = alpha , from_colorspace = from_colorspace , name = name , deterministic = deterministic , random_state = random_state )
11461	def add_control_number ( self , tag , value ) : record_add_field ( self . record , tag , controlfield_value = value )
9007	def transfer_to_row ( self , new_row ) : if new_row != self . _row : index = self . get_index_in_row ( ) if index is not None : self . _row . instructions . pop ( index ) self . _row = new_row
10147	def _ref ( self , param , base_name = None ) : name = base_name or param . get ( 'title' , '' ) or param . get ( 'name' , '' ) pointer = self . json_pointer + name self . parameter_registry [ name ] = param return { '$ref' : pointer }
12259	def simplex ( x , rho ) : u = np . flipud ( np . sort ( x . ravel ( ) ) ) lambdas = ( 1 - np . cumsum ( u ) ) / ( 1. + np . arange ( u . size ) ) ix = np . where ( u + lambdas > 0 ) [ 0 ] . max ( ) return np . maximum ( x + lambdas [ ix ] , 0 )
7888	def error ( self , stanza ) : err = stanza . get_error ( ) self . __logger . debug ( "Error from: %r Condition: %r" % ( stanza . get_from ( ) , err . get_condition ) )
1260	def restore_component ( self , component_name , save_path ) : component = self . get_component ( component_name = component_name ) self . _validate_savable ( component = component , component_name = component_name ) component . restore ( sess = self . session , save_path = save_path )
4487	def update ( self , fp ) : if 'b' not in fp . mode : raise ValueError ( "File has to be opened in binary mode." ) url = self . _upload_url if fp . peek ( 1 ) : response = self . _put ( url , data = fp ) else : response = self . _put ( url , data = b'' ) if response . status_code != 200 : msg = ( 'Could not update {} (status ' 'code: {}).' . format ( self . path , response . status_code ) ) raise RuntimeError ( msg )
2757	def get_all_floating_ips ( self ) : data = self . get_data ( "floating_ips" ) floating_ips = list ( ) for jsoned in data [ 'floating_ips' ] : floating_ip = FloatingIP ( ** jsoned ) floating_ip . token = self . token floating_ips . append ( floating_ip ) return floating_ips
8191	def nodes_by_eigenvalue ( self , treshold = 0.0 ) : nodes = [ ( n . eigenvalue , n ) for n in self . nodes if n . eigenvalue > treshold ] nodes . sort ( ) nodes . reverse ( ) return [ n for w , n in nodes ]
4549	def draw_round_rect ( setter , x , y , w , h , r , color = None , aa = False ) : _draw_fast_hline ( setter , x + r , y , w - 2 * r , color , aa ) _draw_fast_hline ( setter , x + r , y + h - 1 , w - 2 * r , color , aa ) _draw_fast_vline ( setter , x , y + r , h - 2 * r , color , aa ) _draw_fast_vline ( setter , x + w - 1 , y + r , h - 2 * r , color , aa ) _draw_circle_helper ( setter , x + r , y + r , r , 1 , color , aa ) _draw_circle_helper ( setter , x + w - r - 1 , y + r , r , 2 , color , aa ) _draw_circle_helper ( setter , x + w - r - 1 , y + h - r - 1 , r , 4 , color , aa ) _draw_circle_helper ( setter , x + r , y + h - r - 1 , r , 8 , color , aa )
11333	def banner ( * lines , ** kwargs ) : sep = kwargs . get ( "sep" , "*" ) count = kwargs . get ( "width" , globals ( ) [ "WIDTH" ] ) out ( sep * count ) if lines : out ( sep ) for line in lines : out ( "{} {}" . format ( sep , line ) ) out ( sep ) out ( sep * count )
12110	def input_options ( self , options , prompt = 'Select option' , default = None ) : check_options = [ x . lower ( ) for x in options ] while True : response = input ( '%s [%s]: ' % ( prompt , ', ' . join ( options ) ) ) . lower ( ) if response in check_options : return response . strip ( ) elif response == '' and default is not None : return default . lower ( ) . strip ( )
2819	def convert_batchnorm ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting batchnorm ...' ) if names == 'short' : tf_name = 'BN' + random_string ( 6 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) bias_name = '{0}.bias' . format ( w_name ) weights_name = '{0}.weight' . format ( w_name ) mean_name = '{0}.running_mean' . format ( w_name ) var_name = '{0}.running_var' . format ( w_name ) if bias_name in weights : beta = weights [ bias_name ] . numpy ( ) if weights_name in weights : gamma = weights [ weights_name ] . numpy ( ) mean = weights [ mean_name ] . numpy ( ) variance = weights [ var_name ] . numpy ( ) eps = params [ 'epsilon' ] momentum = params [ 'momentum' ] if weights_name not in weights : bn = keras . layers . BatchNormalization ( axis = 1 , momentum = momentum , epsilon = eps , center = False , scale = False , weights = [ mean , variance ] , name = tf_name ) else : bn = keras . layers . BatchNormalization ( axis = 1 , momentum = momentum , epsilon = eps , weights = [ gamma , beta , mean , variance ] , name = tf_name ) layers [ scope_name ] = bn ( layers [ inputs [ 0 ] ] )
12163	def _check_limit ( self , event ) : if self . count ( event ) > self . max_listeners : warnings . warn ( 'Too many listeners for event {}' . format ( event ) , ResourceWarning , )
10746	def get_default_fields ( self ) : field_names = self . _meta . get_all_field_names ( ) if 'id' in field_names : field_names . remove ( 'id' ) return field_names
10588	def _get_account_and_descendants_ ( self , account , result ) : result . append ( account ) for child in account . accounts : self . _get_account_and_descendants_ ( child , result )
11772	def NaiveBayesLearner ( dataset ) : targetvals = dataset . values [ dataset . target ] target_dist = CountingProbDist ( targetvals ) attr_dists = dict ( ( ( gv , attr ) , CountingProbDist ( dataset . values [ attr ] ) ) for gv in targetvals for attr in dataset . inputs ) for example in dataset . examples : targetval = example [ dataset . target ] target_dist . add ( targetval ) for attr in dataset . inputs : attr_dists [ targetval , attr ] . add ( example [ attr ] ) def predict ( example ) : def class_probability ( targetval ) : return ( target_dist [ targetval ] * product ( attr_dists [ targetval , attr ] [ example [ attr ] ] for attr in dataset . inputs ) ) return argmax ( targetvals , class_probability ) return predict
6242	def load_shader ( self , shader_type : str , path : str ) : if path : resolved_path = self . find_program ( path ) if not resolved_path : raise ValueError ( "Cannot find {} shader '{}'" . format ( shader_type , path ) ) print ( "Loading:" , path ) with open ( resolved_path , 'r' ) as fd : return fd . read ( )
13494	def write ( args ) : logging . info ( "Writing configure file: %s" % args . config_file ) if args . config_file is None : return config = cparser . ConfigParser ( ) config . add_section ( "lrcloud" ) for p in [ x for x in dir ( args ) if not x . startswith ( "_" ) ] : if p in IGNORE_ARGS : continue value = getattr ( args , p ) if value is not None : config . set ( 'lrcloud' , p , str ( value ) ) with open ( args . config_file , 'w' ) as f : config . write ( f )
12808	def fetch ( self ) : try : if not self . _last_message_id : messages = self . _connection . get ( "room/%s/recent" % self . _room_id , key = "messages" , parameters = { "limit" : 1 } ) self . _last_message_id = messages [ - 1 ] [ "id" ] messages = self . _connection . get ( "room/%s/recent" % self . _room_id , key = "messages" , parameters = { "since_message_id" : self . _last_message_id } ) except : messages = [ ] if messages : self . _last_message_id = messages [ - 1 ] [ "id" ] self . received ( messages )
1015	def _getCellForNewSegment ( self , colIdx ) : if self . maxSegmentsPerCell < 0 : if self . cellsPerColumn > 1 : i = self . _random . getUInt32 ( self . cellsPerColumn - 1 ) + 1 else : i = 0 return i candidateCellIdxs = [ ] if self . cellsPerColumn == 1 : minIdx = 0 maxIdx = 0 else : minIdx = 1 maxIdx = self . cellsPerColumn - 1 for i in xrange ( minIdx , maxIdx + 1 ) : numSegs = len ( self . cells [ colIdx ] [ i ] ) if numSegs < self . maxSegmentsPerCell : candidateCellIdxs . append ( i ) if len ( candidateCellIdxs ) > 0 : candidateCellIdx = ( candidateCellIdxs [ self . _random . getUInt32 ( len ( candidateCellIdxs ) ) ] ) if self . verbosity >= 5 : print "Cell [%d,%d] chosen for new segment, # of segs is %d" % ( colIdx , candidateCellIdx , len ( self . cells [ colIdx ] [ candidateCellIdx ] ) ) return candidateCellIdx candidateSegment = None candidateSegmentDC = 1.0 for i in xrange ( minIdx , maxIdx + 1 ) : for s in self . cells [ colIdx ] [ i ] : dc = s . dutyCycle ( ) if dc < candidateSegmentDC : candidateCellIdx = i candidateSegmentDC = dc candidateSegment = s if self . verbosity >= 5 : print ( "Deleting segment #%d for cell[%d,%d] to make room for new " "segment" % ( candidateSegment . segID , colIdx , candidateCellIdx ) ) candidateSegment . debugPrint ( ) self . _cleanUpdatesList ( colIdx , candidateCellIdx , candidateSegment ) self . cells [ colIdx ] [ candidateCellIdx ] . remove ( candidateSegment ) return candidateCellIdx
762	def getRandomWithMods ( inputSpace , maxChanges ) : size = len ( inputSpace ) ind = np . random . random_integers ( 0 , size - 1 , 1 ) [ 0 ] value = copy . deepcopy ( inputSpace [ ind ] ) if maxChanges == 0 : return value return modifyBits ( value , maxChanges )
10382	def multi_run_epicom ( graphs : Iterable [ BELGraph ] , path : Union [ None , str , TextIO ] ) -> None : if isinstance ( path , str ) : with open ( path , 'w' ) as file : _multi_run_helper_file_wrapper ( graphs , file ) else : _multi_run_helper_file_wrapper ( graphs , path )
5591	def tiles_from_bounds ( self , bounds , zoom ) : for tile in self . tiles_from_bbox ( box ( * bounds ) , zoom ) : yield self . tile ( * tile . id )
5230	def to_hour ( num ) -> str : to_str = str ( int ( num ) ) return pd . Timestamp ( f'{to_str[:-2]}:{to_str[-2:]}' ) . strftime ( '%H:%M' )
865	def _readConfigFile ( cls , filename , path = None ) : outputProperties = dict ( ) if path is None : filePath = cls . findConfigFile ( filename ) else : filePath = os . path . join ( path , filename ) try : if filePath is not None : try : _getLoggerBase ( ) . debug ( "Loading config file: %s" , filePath ) with open ( filePath , 'r' ) as inp : contents = inp . read ( ) except Exception : raise RuntimeError ( "Expected configuration file at %s" % filePath ) else : try : contents = resource_string ( "nupic.support" , filename ) except Exception as resourceException : if filename in [ USER_CONFIG , CUSTOM_CONFIG ] : contents = '<configuration/>' else : raise resourceException elements = ElementTree . XML ( contents ) if elements . tag != 'configuration' : raise RuntimeError ( "Expected top-level element to be 'configuration' " "but got '%s'" % ( elements . tag ) ) propertyElements = elements . findall ( './property' ) for propertyItem in propertyElements : propInfo = dict ( ) propertyAttributes = list ( propertyItem ) for propertyAttribute in propertyAttributes : propInfo [ propertyAttribute . tag ] = propertyAttribute . text name = propInfo . get ( 'name' , None ) if 'value' in propInfo and propInfo [ 'value' ] is None : value = '' else : value = propInfo . get ( 'value' , None ) if value is None : if 'novalue' in propInfo : continue else : raise RuntimeError ( "Missing 'value' element within the property " "element: => %s " % ( str ( propInfo ) ) ) restOfValue = value value = '' while True : pos = restOfValue . find ( '${env.' ) if pos == - 1 : value += restOfValue break value += restOfValue [ 0 : pos ] varTailPos = restOfValue . find ( '}' , pos ) if varTailPos == - 1 : raise RuntimeError ( "Trailing environment variable tag delimiter '}'" " not found in %r" % ( restOfValue ) ) varname = restOfValue [ pos + 6 : varTailPos ] if varname not in os . environ : raise RuntimeError ( "Attempting to use the value of the environment" " variable %r, which is not defined" % ( varname ) ) envVarValue = os . environ [ varname ] value += envVarValue restOfValue = restOfValue [ varTailPos + 1 : ] if name is None : raise RuntimeError ( "Missing 'name' element within following property " "element:\n => %s " % ( str ( propInfo ) ) ) propInfo [ 'value' ] = value outputProperties [ name ] = propInfo return outputProperties except Exception : _getLoggerBase ( ) . exception ( "Error while parsing configuration file: %s." , filePath ) raise
4851	def _transmit_update ( self , channel_metadata_item_map , transmission_map ) : for chunk in chunks ( channel_metadata_item_map , self . enterprise_configuration . transmission_chunk_size ) : serialized_chunk = self . _serialize_items ( list ( chunk . values ( ) ) ) try : self . client . update_content_metadata ( serialized_chunk ) except ClientError as exc : LOGGER . error ( 'Failed to update [%s] content metadata items for integrated channel [%s] [%s]' , len ( chunk ) , self . enterprise_configuration . enterprise_customer . name , self . enterprise_configuration . channel_code , ) LOGGER . error ( exc ) else : self . _update_transmissions ( chunk , transmission_map )
11905	def snoise2d ( size , z = 0.0 , scale = 0.05 , octaves = 1 , persistence = 0.25 , lacunarity = 2.0 ) : import noise data = np . empty ( size , dtype = 'float32' ) for y in range ( size [ 0 ] ) : for x in range ( size [ 1 ] ) : v = noise . snoise3 ( x * scale , y * scale , z , octaves = octaves , persistence = persistence , lacunarity = lacunarity ) data [ x , y ] = v data = data * 0.5 + 0.5 if __debug__ : assert data . min ( ) >= 0. and data . max ( ) <= 1.0 return data
5785	def _raw_read ( self ) : data = self . _raw_bytes try : data += self . _socket . recv ( 8192 ) except ( socket_ . error ) : pass output = data written = libssl . BIO_write ( self . _rbio , data , len ( data ) ) self . _raw_bytes = data [ written : ] return output
3592	def encryptPassword ( self , login , passwd ) : binaryKey = b64decode ( config . GOOGLE_PUBKEY ) i = utils . readInt ( binaryKey , 0 ) modulus = utils . toBigInt ( binaryKey [ 4 : ] [ 0 : i ] ) j = utils . readInt ( binaryKey , i + 4 ) exponent = utils . toBigInt ( binaryKey [ i + 8 : ] [ 0 : j ] ) digest = hashes . Hash ( hashes . SHA1 ( ) , backend = default_backend ( ) ) digest . update ( binaryKey ) h = b'\x00' + digest . finalize ( ) [ 0 : 4 ] der_data = encode_dss_signature ( modulus , exponent ) publicKey = load_der_public_key ( der_data , backend = default_backend ( ) ) to_be_encrypted = login . encode ( ) + b'\x00' + passwd . encode ( ) ciphertext = publicKey . encrypt ( to_be_encrypted , padding . OAEP ( mgf = padding . MGF1 ( algorithm = hashes . SHA1 ( ) ) , algorithm = hashes . SHA1 ( ) , label = None ) ) return urlsafe_b64encode ( h + ciphertext )
12508	def get_3D_coordmap ( img ) : if isinstance ( img , nib . Nifti1Image ) : img = nifti2nipy ( img ) if img . ndim == 4 : from nipy . core . reference . coordinate_map import drop_io_dim cm = drop_io_dim ( img . coordmap , 3 ) else : cm = img . coordmap return cm
12341	def _set_path ( self , path ) : "Set self.path, self.dirname and self.basename." import os . path self . path = os . path . abspath ( path ) self . dirname = os . path . dirname ( path ) self . basename = os . path . basename ( path )
9621	def gamepad ( self ) : state = _xinput_state ( ) _xinput . XInputGetState ( self . ControllerID - 1 , pointer ( state ) ) self . dwPacketNumber = state . dwPacketNumber return state . XINPUT_GAMEPAD
8203	def set_size ( self , size ) : if self . size is None : self . size = size return size else : return self . size
10167	def get_md_status ( self , line ) : ret = { } splitted = split ( '\W+' , line ) if len ( splitted ) < 7 : ret [ 'available' ] = None ret [ 'used' ] = None ret [ 'config' ] = None else : ret [ 'available' ] = splitted [ - 4 ] ret [ 'used' ] = splitted [ - 3 ] ret [ 'config' ] = splitted [ - 2 ] return ret
2305	def plot_curves ( i_batch , adv_loss , gen_loss , l1_reg , cols ) : from matplotlib import pyplot as plt if i_batch == 0 : try : ax . clear ( ) ax . plot ( range ( len ( adv_plt ) ) , adv_plt , "r-" , linewidth = 1.5 , markersize = 4 , label = "Discriminator" ) ax . plot ( range ( len ( adv_plt ) ) , gen_plt , "g-" , linewidth = 1.5 , markersize = 4 , label = "Generators" ) ax . plot ( range ( len ( adv_plt ) ) , l1_plt , "b-" , linewidth = 1.5 , markersize = 4 , label = "L1-Regularization" ) plt . legend ( ) adv_plt . append ( adv_loss . cpu ( ) . data [ 0 ] ) gen_plt . append ( gen_loss . cpu ( ) . data [ 0 ] / cols ) l1_plt . append ( l1_reg . cpu ( ) . data [ 0 ] ) plt . pause ( 0.0001 ) except NameError : plt . ion ( ) fig , ax = plt . figure ( ) plt . xlabel ( "Epoch" ) plt . ylabel ( "Losses" ) plt . pause ( 0.0001 ) adv_plt = [ adv_loss . cpu ( ) . data [ 0 ] ] gen_plt = [ gen_loss . cpu ( ) . data [ 0 ] / cols ] l1_plt = [ l1_reg . cpu ( ) . data [ 0 ] ] else : adv_plt . append ( adv_loss . cpu ( ) . data [ 0 ] ) gen_plt . append ( gen_loss . cpu ( ) . data [ 0 ] / cols ) l1_plt . append ( l1_reg . cpu ( ) . data [ 0 ] )
9751	def find_fann ( ) : if sys . platform == "win32" : dirs = sys . path for ver in dirs : if os . path . isdir ( ver ) : if find_x ( ver ) : return True raise Exception ( "Couldn't find FANN source libs!" ) else : dirs = [ '/lib' , '/usr/lib' , '/usr/lib64' , '/usr/local/lib' , '/usr/pkg/lib' ] for path in dirs : if os . path . isdir ( path ) : if find_x ( path ) : return True raise Exception ( "Couldn't find FANN source libs!" )
6676	def upload_template ( self , filename , destination , context = None , use_jinja = False , template_dir = None , use_sudo = False , backup = True , mirror_local_mode = False , mode = None , mkdir = False , chown = False , user = None ) : if mkdir : remote_dir = os . path . dirname ( destination ) if use_sudo : self . sudo ( 'mkdir -p %s' % quote ( remote_dir ) , user = user ) else : self . run ( 'mkdir -p %s' % quote ( remote_dir ) ) if not self . dryrun : _upload_template ( filename = filename , destination = destination , context = context , use_jinja = use_jinja , template_dir = template_dir , use_sudo = use_sudo , backup = backup , mirror_local_mode = mirror_local_mode , mode = mode , ) if chown : if user is None : user = self . genv . user run_as_root ( 'chown %s: %s' % ( user , quote ( destination ) ) )
10516	def verifyscrollbarhorizontal ( self , window_name , object_name ) : try : object_handle = self . _get_object_handle ( window_name , object_name ) if object_handle . AXOrientation == "AXHorizontalOrientation" : return 1 except : pass return 0
13308	def gmv ( a , b ) : return np . exp ( np . square ( np . log ( a ) - np . log ( b ) ) . mean ( ) )
6926	def cursor ( self , handle , dictcursor = False ) : if handle in self . cursors : return self . cursors [ handle ] else : if dictcursor : self . cursors [ handle ] = self . connection . cursor ( cursor_factory = psycopg2 . extras . DictCursor ) else : self . cursors [ handle ] = self . connection . cursor ( ) return self . cursors [ handle ]
10044	def default_view_method ( pid , record , template = None ) : record_viewed . send ( current_app . _get_current_object ( ) , pid = pid , record = record , ) deposit_type = request . values . get ( 'type' ) return render_template ( template , pid = pid , record = record , jsonschema = current_deposit . jsonschemas [ deposit_type ] , schemaform = current_deposit . schemaforms [ deposit_type ] , )
13880	def MoveFile ( source_filename , target_filename ) : _AssertIsLocal ( source_filename ) _AssertIsLocal ( target_filename ) import shutil shutil . move ( source_filename , target_filename )
4345	def stats ( self , input_filepath ) : effect_args = [ 'channels' , '1' , 'stats' ] _ , _ , stats_output = self . build ( input_filepath , None , extra_args = effect_args , return_output = True ) stats_dict = { } lines = stats_output . split ( '\n' ) for line in lines : split_line = line . split ( ) if len ( split_line ) == 0 : continue value = split_line [ - 1 ] key = ' ' . join ( split_line [ : - 1 ] ) stats_dict [ key ] = value return stats_dict
688	def saveRecords ( self , path = 'myOutput' ) : numRecords = self . fields [ 0 ] . numRecords assert ( all ( field . numRecords == numRecords for field in self . fields ) ) import csv with open ( path + '.csv' , 'wb' ) as f : writer = csv . writer ( f ) writer . writerow ( self . getAllFieldNames ( ) ) writer . writerow ( self . getAllDataTypes ( ) ) writer . writerow ( self . getAllFlags ( ) ) writer . writerows ( self . getAllRecords ( ) ) if self . verbosity > 0 : print '******' , numRecords , 'records exported in numenta format to file:' , path , '******\n'
7510	def _insert_to_array ( self , start , results ) : qrts , wgts , qsts = results with h5py . File ( self . database . output , 'r+' ) as out : chunk = self . _chunksize out [ 'quartets' ] [ start : start + chunk ] = qrts if self . checkpoint . boots : key = "qboots/b{}" . format ( self . checkpoint . boots - 1 ) out [ key ] [ start : start + chunk ] = qsts else : out [ "qstats" ] [ start : start + chunk ] = qsts
7512	def padnames ( names ) : longname_len = max ( len ( i ) for i in names ) padding = 5 pnames = [ name + " " * ( longname_len - len ( name ) + padding ) for name in names ] snppad = "//" + " " * ( longname_len - 2 + padding ) return np . array ( pnames ) , snppad
9619	def set_value ( self , control , value = None ) : func = getattr ( _xinput , 'Set' + control ) if 'Axis' in control : target_type = c_short if self . percent : target_value = int ( 32767 * value ) else : target_value = value elif 'Btn' in control : target_type = c_bool target_value = bool ( value ) elif 'Trigger' in control : target_type = c_byte if self . percent : target_value = int ( 255 * value ) else : target_value = value elif 'Dpad' in control : target_type = c_int target_value = int ( value ) func ( c_uint ( self . id ) , target_type ( target_value ) )
2863	def ping ( self ) : self . _idle ( ) self . _transaction_start ( ) self . _i2c_start ( ) self . _i2c_write_bytes ( [ self . _address_byte ( False ) ] ) self . _i2c_stop ( ) response = self . _transaction_end ( ) if len ( response ) != 1 : raise RuntimeError ( 'Expected 1 response byte but received {0} byte(s).' . format ( len ( response ) ) ) return ( ( response [ 0 ] & 0x01 ) == 0x00 )
559	def setSwarmState ( self , swarmId , newStatus ) : assert ( newStatus in [ 'active' , 'completing' , 'completed' , 'killed' ] ) swarmInfo = self . _state [ 'swarms' ] [ swarmId ] if swarmInfo [ 'status' ] == newStatus : return if swarmInfo [ 'status' ] == 'completed' and newStatus == 'completing' : return self . _dirty = True swarmInfo [ 'status' ] = newStatus if newStatus == 'completed' : ( modelId , errScore ) = self . _hsObj . _resultsDB . bestModelIdAndErrScore ( swarmId ) swarmInfo [ 'bestModelId' ] = modelId swarmInfo [ 'bestErrScore' ] = errScore if newStatus != 'active' and swarmId in self . _state [ 'activeSwarms' ] : self . _state [ 'activeSwarms' ] . remove ( swarmId ) if newStatus == 'killed' : self . _hsObj . killSwarmParticles ( swarmId ) sprintIdx = swarmInfo [ 'sprintIdx' ] self . isSprintActive ( sprintIdx ) sprintInfo = self . _state [ 'sprints' ] [ sprintIdx ] statusCounts = dict ( active = 0 , completing = 0 , completed = 0 , killed = 0 ) bestModelIds = [ ] bestErrScores = [ ] for info in self . _state [ 'swarms' ] . itervalues ( ) : if info [ 'sprintIdx' ] != sprintIdx : continue statusCounts [ info [ 'status' ] ] += 1 if info [ 'status' ] == 'completed' : bestModelIds . append ( info [ 'bestModelId' ] ) bestErrScores . append ( info [ 'bestErrScore' ] ) if statusCounts [ 'active' ] > 0 : sprintStatus = 'active' elif statusCounts [ 'completing' ] > 0 : sprintStatus = 'completing' else : sprintStatus = 'completed' sprintInfo [ 'status' ] = sprintStatus if sprintStatus == 'completed' : if len ( bestErrScores ) > 0 : whichIdx = numpy . array ( bestErrScores ) . argmin ( ) sprintInfo [ 'bestModelId' ] = bestModelIds [ whichIdx ] sprintInfo [ 'bestErrScore' ] = bestErrScores [ whichIdx ] else : sprintInfo [ 'bestModelId' ] = 0 sprintInfo [ 'bestErrScore' ] = numpy . inf bestPrior = numpy . inf for idx in range ( sprintIdx ) : if self . _state [ 'sprints' ] [ idx ] [ 'status' ] == 'completed' : ( _ , errScore ) = self . bestModelInCompletedSprint ( idx ) if errScore is None : errScore = numpy . inf else : errScore = numpy . inf if errScore < bestPrior : bestPrior = errScore if sprintInfo [ 'bestErrScore' ] >= bestPrior : self . _state [ 'lastGoodSprint' ] = sprintIdx - 1 if self . _state [ 'lastGoodSprint' ] is not None and not self . anyGoodSprintsActive ( ) : self . _state [ 'searchOver' ] = True
12063	def getAvgBySweep ( abf , feature , T0 = None , T1 = None ) : if T1 is None : T1 = abf . sweepLength if T0 is None : T0 = 0 data = [ np . empty ( ( 0 ) ) ] * abf . sweeps for AP in cm . dictFlat ( cm . matrixToDicts ( abf . APs ) ) : if T0 < AP [ 'sweepT' ] < T1 : val = AP [ feature ] data [ int ( AP [ 'sweep' ] ) ] = np . concatenate ( ( data [ int ( AP [ 'sweep' ] ) ] , [ val ] ) ) for sweep in range ( abf . sweeps ) : if len ( data [ sweep ] ) > 1 and np . any ( data [ sweep ] ) : data [ sweep ] = np . nanmean ( data [ sweep ] ) elif len ( data [ sweep ] ) == 1 : data [ sweep ] = data [ sweep ] [ 0 ] else : data [ sweep ] = np . nan return data
13224	def main ( ) : parser = argparse . ArgumentParser ( description = 'Discover and ingest metadata from document sources, ' 'including lsstdoc-based LaTeX documents and ' 'reStructuredText-based technotes. Metadata can be ' 'upserted into the LSST Projectmeta MongoDB.' ) parser . add_argument ( '--ltd-product' , dest = 'ltd_product_url' , help = 'URL of an LSST the Docs product ' '(https://keeper.lsst.codes/products/<slug>). If provided, ' 'only this document will be ingested.' ) parser . add_argument ( '--github-token' , help = 'GitHub personal access token.' ) parser . add_argument ( '--mongodb-uri' , help = 'MongoDB connection URI. If provided, metadata will be loaded ' 'into the Projectmeta database. Omit this argument to just ' 'test the ingest pipeline.' ) parser . add_argument ( '--mongodb-db' , default = 'lsstprojectmeta' , help = 'Name of MongoDB database' ) parser . add_argument ( '--mongodb-collection' , default = 'resources' , help = 'Name of the MongoDB collection for projectmeta resources' ) args = parser . parse_args ( ) stream_handler = logging . StreamHandler ( ) stream_formatter = logging . Formatter ( '%(asctime)s %(levelname)8s %(name)s | %(message)s' ) stream_handler . setFormatter ( stream_formatter ) root_logger = logging . getLogger ( ) root_logger . addHandler ( stream_handler ) root_logger . setLevel ( logging . WARNING ) app_logger = logging . getLogger ( 'lsstprojectmeta' ) app_logger . setLevel ( logging . DEBUG ) if args . mongodb_uri is not None : mongo_client = AsyncIOMotorClient ( args . mongodb_uri , ssl = True ) collection = mongo_client [ args . mongodb_db ] [ args . mongodb_collection ] else : collection = None loop = asyncio . get_event_loop ( ) if args . ltd_product_url is not None : loop . run_until_complete ( run_single_ltd_doc ( args . ltd_product_url , args . github_token , collection ) ) else : loop . run_until_complete ( run_bulk_etl ( args . github_token , collection ) )
11292	def consume_json ( request ) : client = OEmbedConsumer ( ) urls = request . GET . getlist ( 'urls' ) width = request . GET . get ( 'width' ) height = request . GET . get ( 'height' ) template_dir = request . GET . get ( 'template_dir' ) output = { } ctx = RequestContext ( request ) for url in urls : try : provider = oembed . site . provider_for_url ( url ) except OEmbedMissingEndpoint : oembeds = None rendered = None else : oembeds = url rendered = client . parse_text ( url , width , height , context = ctx , template_dir = template_dir ) output [ url ] = { 'oembeds' : oembeds , 'rendered' : rendered , } return HttpResponse ( simplejson . dumps ( output ) , mimetype = 'application/json' )
7221	def ingest_vectors ( self , output_port_value ) : ingest_task = Task ( 'IngestItemJsonToVectorServices' ) ingest_task . inputs . items = output_port_value ingest_task . impersonation_allowed = True stage_task = Task ( 'StageDataToS3' ) stage_task . inputs . destination = 's3://{vector_ingest_bucket}/{recipe_id}/{run_id}/{task_name}' stage_task . inputs . data = ingest_task . outputs . result . value self . definition [ 'tasks' ] . append ( ingest_task . generate_task_workflow_json ( ) ) self . definition [ 'tasks' ] . append ( stage_task . generate_task_workflow_json ( ) )
2391	def regenerate_good_tokens ( string ) : toks = nltk . word_tokenize ( string ) pos_string = nltk . pos_tag ( toks ) pos_seq = [ tag [ 1 ] for tag in pos_string ] pos_ngrams = ngrams ( pos_seq , 2 , 4 ) sel_pos_ngrams = f7 ( pos_ngrams ) return sel_pos_ngrams
1197	def nested ( * managers ) : warn ( "With-statements now directly support multiple context managers" , DeprecationWarning , 3 ) exits = [ ] vars = [ ] exc = ( None , None , None ) try : for mgr in managers : exit = mgr . __exit__ enter = mgr . __enter__ vars . append ( enter ( ) ) exits . append ( exit ) yield vars except : exc = sys . exc_info ( ) finally : while exits : exit = exits . pop ( ) try : if exit ( * exc ) : exc = ( None , None , None ) except : exc = sys . exc_info ( ) if exc != ( None , None , None ) : raise exc [ 0 ] , exc [ 1 ] , exc [ 2 ]
10803	def tk ( self , k , x ) : weights = np . diag ( np . ones ( k + 1 ) ) [ k ] return np . polynomial . chebyshev . chebval ( self . _x2c ( x ) , weights )
4201	def modcovar ( x , order ) : from spectrum import corrmtx import scipy . linalg X = corrmtx ( x , order , 'modified' ) Xc = np . matrix ( X [ : , 1 : ] ) X1 = np . array ( X [ : , 0 ] ) a , residues , rank , singular_values = scipy . linalg . lstsq ( - Xc , X1 ) Cz = np . dot ( X1 . conj ( ) . transpose ( ) , Xc ) e = np . dot ( X1 . conj ( ) . transpose ( ) , X1 ) + np . dot ( Cz , a ) assert e . imag < 1e-4 , 'wierd behaviour' e = float ( e . real ) return a , e
3680	def T_converter ( T , current , desired ) : r def range_check ( T , Tmin , Tmax ) : if T < Tmin or T > Tmax : raise Exception ( 'Temperature conversion is outside one or both scales' ) try : if current == 'ITS-90' : pass elif current == 'ITS-68' : range_check ( T , 13.999 , 4300.0001 ) T = T68_to_T90 ( T ) elif current == 'ITS-76' : range_check ( T , 4.9999 , 27.0001 ) T = T76_to_T90 ( T ) elif current == 'ITS-48' : range_check ( T , 93.149999 , 4273.15001 ) T = T48_to_T90 ( T ) elif current == 'ITS-27' : range_check ( T , 903.15 , 4273.15 ) T = T27_to_T90 ( T ) else : raise Exception ( 'Current scale not supported' ) if desired == 'ITS-90' : pass elif desired == 'ITS-68' : range_check ( T , 13.999 , 4300.0001 ) T = T90_to_T68 ( T ) elif desired == 'ITS-76' : range_check ( T , 4.9999 , 27.0001 ) T = T90_to_T76 ( T ) elif desired == 'ITS-48' : range_check ( T , 93.149999 , 4273.15001 ) T = T90_to_T48 ( T ) elif desired == 'ITS-27' : range_check ( T , 903.15 , 4273.15 ) T = T90_to_T27 ( T ) else : raise Exception ( 'Desired scale not supported' ) except ValueError : raise Exception ( 'Temperature could not be converted to desired scale' ) return float ( T )
10681	def H ( self , T ) : result = self . DHref for Tmax in sorted ( [ float ( TT ) for TT in self . _Cp_records . keys ( ) ] ) : result += self . _Cp_records [ str ( Tmax ) ] . H ( T ) if T <= Tmax : return result + self . H_mag ( T ) Tmax = max ( [ float ( TT ) for TT in self . _Cp_records . keys ( ) ] ) result += self . Cp ( Tmax ) * ( T - Tmax ) return result + self . H_mag ( T )
6493	def get_mappings ( cls , index_name , doc_type ) : return cache . get ( cls . get_cache_item_name ( index_name , doc_type ) , { } )
9594	def execute_script ( self , script , * args ) : return self . _execute ( Command . EXECUTE_SCRIPT , { 'script' : script , 'args' : list ( args ) } )
3914	def _on_typing ( self , typing_message ) : self . _typing_statuses [ typing_message . user_id ] = typing_message . status self . _update ( )
7153	def many ( prompt , * args , ** kwargs ) : def get_options ( options , chosen ) : return [ options [ i ] for i , c in enumerate ( chosen ) if c ] def get_verbose_options ( verbose_options , chosen ) : no , yes = ' ' , '' if sys . version_info < ( 3 , 3 ) : no , yes = ' ' , '@' opts = [ '{} {}' . format ( yes if c else no , verbose_options [ i ] ) for i , c in enumerate ( chosen ) ] return opts + [ '{}{}' . format ( ' ' , kwargs . get ( 'done' , 'done...' ) ) ] options , verbose_options = prepare_options ( args ) chosen = [ False ] * len ( options ) index = kwargs . get ( 'idx' , 0 ) default = kwargs . get ( 'default' , None ) if isinstance ( default , list ) : for idx in default : chosen [ idx ] = True if isinstance ( default , int ) : chosen [ default ] = True while True : try : index = one ( prompt , * get_verbose_options ( verbose_options , chosen ) , return_index = True , idx = index ) except QuestionnaireGoBack : if any ( chosen ) : raise QuestionnaireGoBack ( 0 ) else : raise QuestionnaireGoBack if index == len ( options ) : return get_options ( options , chosen ) chosen [ index ] = not chosen [ index ]
5302	def parse_rgb_txt_file ( path ) : color_dict = { } with open ( path , 'r' ) as rgb_txt : for line in rgb_txt : line = line . strip ( ) if not line or line . startswith ( '!' ) : continue parts = line . split ( ) color_dict [ " " . join ( parts [ 3 : ] ) ] = ( int ( parts [ 0 ] ) , int ( parts [ 1 ] ) , int ( parts [ 2 ] ) ) return color_dict
493	def close ( self ) : self . _logger . info ( "Closing" ) if self . _opened : self . _opened = False else : self . _logger . warning ( "close() called, but connection policy was alredy closed" ) return
12438	def deserialize ( self , request = None , text = None , format = None ) : if isinstance ( self , Resource ) : if not request : request = self . _request Deserializer = None if format : Deserializer = self . meta . deserializers [ format ] if not Deserializer : media_ranges = request . get ( 'Content-Type' ) if media_ranges : media_types = six . iterkeys ( self . _deserializer_map ) media_type = mimeparse . best_match ( media_types , media_ranges ) if media_type : format = self . _deserializer_map [ media_type ] Deserializer = self . meta . deserializers [ format ] else : pass if Deserializer : try : deserializer = Deserializer ( ) data = deserializer . deserialize ( request = request , text = text ) return data , deserializer except ValueError : pass raise http . exceptions . UnsupportedMediaType ( )
3432	def add_groups ( self , group_list ) : def existing_filter ( group ) : if group . id in self . groups : LOGGER . warning ( "Ignoring group '%s' since it already exists." , group . id ) return False return True if isinstance ( group_list , string_types ) or hasattr ( group_list , "id" ) : warn ( "need to pass in a list" ) group_list = [ group_list ] pruned = DictList ( filter ( existing_filter , group_list ) ) for group in pruned : group . _model = self for member in group . members : if isinstance ( member , Metabolite ) : if member not in self . metabolites : self . add_metabolites ( [ member ] ) if isinstance ( member , Reaction ) : if member not in self . reactions : self . add_reactions ( [ member ] ) self . groups += [ group ]
2132	def _compare_node_lists ( old , new ) : to_expand = [ ] to_delete = [ ] to_recurse = [ ] old_records = { } new_records = { } for tree_node in old : old_records . setdefault ( tree_node . unified_job_template , [ ] ) old_records [ tree_node . unified_job_template ] . append ( tree_node ) for tree_node in new : new_records . setdefault ( tree_node . unified_job_template , [ ] ) new_records [ tree_node . unified_job_template ] . append ( tree_node ) for ujt_id in old_records : if ujt_id not in new_records : to_delete . extend ( old_records [ ujt_id ] ) continue old_list = old_records [ ujt_id ] new_list = new_records . pop ( ujt_id ) if len ( old_list ) == 1 and len ( new_list ) == 1 : to_recurse . append ( ( old_list [ 0 ] , new_list [ 0 ] ) ) else : to_delete . extend ( old_list ) to_expand . extend ( new_list ) for nodes in new_records . values ( ) : to_expand . extend ( nodes ) return to_expand , to_delete , to_recurse
3273	def as_DAVError ( e ) : if isinstance ( e , DAVError ) : return e elif isinstance ( e , Exception ) : return DAVError ( HTTP_INTERNAL_ERROR , src_exception = e ) else : return DAVError ( HTTP_INTERNAL_ERROR , "{}" . format ( e ) )
5478	def get_operation_full_job_id ( op ) : job_id = op . get_field ( 'job-id' ) task_id = op . get_field ( 'task-id' ) if task_id : return '%s.%s' % ( job_id , task_id ) else : return job_id
8703	def download_file ( self , filename ) : res = self . __exchange ( 'send("{filename}")' . format ( filename = filename ) ) if ( 'unexpected' in res ) or ( 'stdin' in res ) : log . error ( 'Unexpected error downloading file: %s' , res ) raise Exception ( 'Unexpected error downloading file' ) self . __write ( 'C' ) sent_filename = self . __expect ( NUL ) . strip ( ) log . info ( 'receiveing ' + sent_filename ) self . __write ( ACK , True ) buf = '' data = '' chunk , buf = self . __read_chunk ( buf ) while chunk != '' : self . __write ( ACK , True ) data = data + chunk chunk , buf = self . __read_chunk ( buf ) return data
4833	def traverse_pagination ( response , endpoint , content_filter_query , query_params ) : results = response . get ( 'results' , [ ] ) page = 1 while response . get ( 'next' ) : page += 1 response = endpoint ( ) . post ( content_filter_query , ** dict ( query_params , page = page ) ) results += response . get ( 'results' , [ ] ) return results
10882	def aN ( a , dim = 3 , dtype = 'int' ) : if not hasattr ( a , '__iter__' ) : return np . array ( [ a ] * dim , dtype = dtype ) return np . array ( a ) . astype ( dtype )
5687	def stop ( self , stop_I ) : return pd . read_sql_query ( "SELECT * FROM stops WHERE stop_I={stop_I}" . format ( stop_I = stop_I ) , self . conn )
10872	def get_Kprefactor ( z , cos_theta , zint = 100.0 , n2n1 = 0.95 , get_hdet = False , ** kwargs ) : phase = f_theta ( cos_theta , zint , z , n2n1 = n2n1 , ** kwargs ) to_return = np . exp ( - 1j * phase ) if not get_hdet : to_return *= np . outer ( np . ones_like ( z ) , np . sqrt ( cos_theta ) ) return to_return
6126	def norm_and_check ( source_tree , requested ) : if os . path . isabs ( requested ) : raise ValueError ( "paths must be relative" ) abs_source = os . path . abspath ( source_tree ) abs_requested = os . path . normpath ( os . path . join ( abs_source , requested ) ) norm_source = os . path . normcase ( abs_source ) norm_requested = os . path . normcase ( abs_requested ) if os . path . commonprefix ( [ norm_source , norm_requested ] ) != norm_source : raise ValueError ( "paths must be inside source tree" ) return abs_requested
3957	def update_running_containers_from_spec ( compose_config , recreate_containers = True ) : write_composefile ( compose_config , constants . COMPOSEFILE_PATH ) compose_up ( constants . COMPOSEFILE_PATH , 'dusty' , recreate_containers = recreate_containers )
9174	def bake ( binder , recipe_id , publisher , message , cursor ) : recipe = _get_recipe ( recipe_id , cursor ) includes = _formatter_callback_factory ( ) binder = collate_models ( binder , ruleset = recipe , includes = includes ) def flatten_filter ( model ) : return ( isinstance ( model , cnxepub . CompositeDocument ) or ( isinstance ( model , cnxepub . Binder ) and model . metadata . get ( 'type' ) == 'composite-chapter' ) ) def only_documents_filter ( model ) : return isinstance ( model , cnxepub . Document ) and not isinstance ( model , cnxepub . CompositeDocument ) for doc in cnxepub . flatten_to ( binder , flatten_filter ) : publish_composite_model ( cursor , doc , binder , publisher , message ) for doc in cnxepub . flatten_to ( binder , only_documents_filter ) : publish_collated_document ( cursor , doc , binder ) tree = cnxepub . model_to_tree ( binder ) publish_collated_tree ( cursor , tree ) return [ ]
10228	def get_separate_unstable_correlation_triples ( graph : BELGraph ) -> Iterable [ NodeTriple ] : cg = get_correlation_graph ( graph ) for a , b , c in get_correlation_triangles ( cg ) : if POSITIVE_CORRELATION in cg [ a ] [ b ] and POSITIVE_CORRELATION in cg [ b ] [ c ] and NEGATIVE_CORRELATION in cg [ a ] [ c ] : yield b , a , c if POSITIVE_CORRELATION in cg [ a ] [ b ] and NEGATIVE_CORRELATION in cg [ b ] [ c ] and POSITIVE_CORRELATION in cg [ a ] [ c ] : yield a , b , c if NEGATIVE_CORRELATION in cg [ a ] [ b ] and POSITIVE_CORRELATION in cg [ b ] [ c ] and POSITIVE_CORRELATION in cg [ a ] [ c ] : yield c , a , b
12520	def _load_images_and_labels ( self , images , labels = None ) : if not isinstance ( images , ( list , tuple ) ) : raise ValueError ( 'Expected an iterable (list or tuple) of strings or img-like objects. ' 'Got a {}.' . format ( type ( images ) ) ) if not len ( images ) > 0 : raise ValueError ( 'Expected an iterable (list or tuple) of strings or img-like objects ' 'of size higher than 0. Got {} items.' . format ( len ( images ) ) ) if labels is not None and len ( labels ) != len ( images ) : raise ValueError ( 'Expected the same length for image set ({}) and ' 'labels list ({}).' . format ( len ( images ) , len ( labels ) ) ) first_file = images [ 0 ] if first_file : first_img = NeuroImage ( first_file ) else : raise ( 'Error reading image {}.' . format ( repr_imgs ( first_file ) ) ) for idx , image in enumerate ( images ) : try : img = NeuroImage ( image ) self . check_compatibility ( img , first_img ) except : log . exception ( 'Error reading image {}.' . format ( repr_imgs ( image ) ) ) raise else : self . items . append ( img ) self . set_labels ( labels )
8483	def set ( self , name , value ) : if not self . settings . get ( 'pyconfig.case_sensitive' , False ) : name = name . lower ( ) log . info ( " %s = %s" , name , repr ( value ) ) with self . mut_lock : self . settings [ name ] = value
2573	def _create_task_log_info ( self , task_id , fail_mode = None ) : info_to_monitor = [ 'func_name' , 'fn_hash' , 'memoize' , 'checkpoint' , 'fail_count' , 'fail_history' , 'status' , 'id' , 'time_submitted' , 'time_returned' , 'executor' ] task_log_info = { "task_" + k : self . tasks [ task_id ] [ k ] for k in info_to_monitor } task_log_info [ 'run_id' ] = self . run_id task_log_info [ 'timestamp' ] = datetime . datetime . now ( ) task_log_info [ 'task_status_name' ] = self . tasks [ task_id ] [ 'status' ] . name task_log_info [ 'tasks_failed_count' ] = self . tasks_failed_count task_log_info [ 'tasks_completed_count' ] = self . tasks_completed_count task_log_info [ 'task_inputs' ] = str ( self . tasks [ task_id ] [ 'kwargs' ] . get ( 'inputs' , None ) ) task_log_info [ 'task_outputs' ] = str ( self . tasks [ task_id ] [ 'kwargs' ] . get ( 'outputs' , None ) ) task_log_info [ 'task_stdin' ] = self . tasks [ task_id ] [ 'kwargs' ] . get ( 'stdin' , None ) task_log_info [ 'task_stdout' ] = self . tasks [ task_id ] [ 'kwargs' ] . get ( 'stdout' , None ) task_log_info [ 'task_depends' ] = None if self . tasks [ task_id ] [ 'depends' ] is not None : task_log_info [ 'task_depends' ] = "," . join ( [ str ( t . _tid ) for t in self . tasks [ task_id ] [ 'depends' ] ] ) task_log_info [ 'task_elapsed_time' ] = None if self . tasks [ task_id ] [ 'time_returned' ] is not None : task_log_info [ 'task_elapsed_time' ] = ( self . tasks [ task_id ] [ 'time_returned' ] - self . tasks [ task_id ] [ 'time_submitted' ] ) . total_seconds ( ) if fail_mode is not None : task_log_info [ 'task_fail_mode' ] = fail_mode return task_log_info
11261	def resplit ( prev , pattern , * args , ** kw ) : maxsplit = 0 if 'maxsplit' not in kw else kw . pop ( 'maxsplit' ) pattern_obj = re . compile ( pattern , * args , ** kw ) for s in prev : yield pattern_obj . split ( s , maxsplit = maxsplit )
4207	def arcovar ( x , order ) : r from spectrum import corrmtx import scipy . linalg X = corrmtx ( x , order , 'covariance' ) Xc = np . matrix ( X [ : , 1 : ] ) X1 = np . array ( X [ : , 0 ] ) a , _residues , _rank , _singular_values = scipy . linalg . lstsq ( - Xc , X1 ) Cz = np . dot ( X1 . conj ( ) . transpose ( ) , Xc ) e = np . dot ( X1 . conj ( ) . transpose ( ) , X1 ) + np . dot ( Cz , a ) assert e . imag < 1e-4 , 'wierd behaviour' e = float ( e . real ) return a , e
802	def modelsGetParams ( self , modelIDs ) : assert isinstance ( modelIDs , self . _SEQUENCE_TYPES ) , ( "Wrong modelIDs type: %r" ) % ( type ( modelIDs ) , ) assert len ( modelIDs ) >= 1 , "modelIDs is empty" rows = self . _getMatchingRowsWithRetries ( self . _models , { 'model_id' : modelIDs } , [ self . _models . pubToDBNameDict [ f ] for f in self . _models . getParamsNamedTuple . _fields ] ) assert len ( rows ) == len ( modelIDs ) , "Didn't find modelIDs: %r" % ( ( set ( modelIDs ) - set ( r [ 0 ] for r in rows ) ) , ) return [ self . _models . getParamsNamedTuple . _make ( r ) for r in rows ]
9316	def _to_json ( resp ) : try : return resp . json ( ) except ValueError as e : six . raise_from ( InvalidJSONError ( "Invalid JSON was received from " + resp . request . url ) , e )
5352	def __studies ( self , retention_time ) : cfg = self . config . get_conf ( ) if 'studies' not in cfg [ self . backend_section ] or not cfg [ self . backend_section ] [ 'studies' ] : logger . debug ( 'No studies for %s' % self . backend_section ) return studies = [ study for study in cfg [ self . backend_section ] [ 'studies' ] if study . strip ( ) != "" ] if not studies : logger . debug ( 'No studies for %s' % self . backend_section ) return logger . debug ( "Executing studies for %s: %s" % ( self . backend_section , studies ) ) time . sleep ( 2 ) enrich_backend = self . _get_enrich_backend ( ) ocean_backend = self . _get_ocean_backend ( enrich_backend ) active_studies = [ ] all_studies = enrich_backend . studies all_studies_names = [ study . __name__ for study in enrich_backend . studies ] logger . debug ( "All studies in %s: %s" , self . backend_section , all_studies_names ) logger . debug ( "Configured studies %s" , studies ) cfg_studies_types = [ study . split ( ":" ) [ 0 ] for study in studies ] if not set ( cfg_studies_types ) . issubset ( set ( all_studies_names ) ) : logger . error ( 'Wrong studies names for %s: %s' , self . backend_section , studies ) raise RuntimeError ( 'Wrong studies names ' , self . backend_section , studies ) for study in enrich_backend . studies : if study . __name__ in cfg_studies_types : active_studies . append ( study ) enrich_backend . studies = active_studies print ( "Executing for %s the studies %s" % ( self . backend_section , [ study for study in studies ] ) ) studies_args = self . __load_studies ( ) do_studies ( ocean_backend , enrich_backend , studies_args , retention_time = retention_time ) enrich_backend . studies = all_studies
1607	def spec ( cls , name = None , inputs = None , par = 1 , config = None , optional_outputs = None ) : python_class_path = "%s.%s" % ( cls . __module__ , cls . __name__ ) if hasattr ( cls , 'outputs' ) : _outputs = copy . copy ( cls . outputs ) else : _outputs = [ ] if optional_outputs is not None : assert isinstance ( optional_outputs , ( list , tuple ) ) for out in optional_outputs : assert isinstance ( out , ( str , Stream ) ) _outputs . append ( out ) return HeronComponentSpec ( name , python_class_path , is_spout = False , par = par , inputs = inputs , outputs = _outputs , config = config )
4502	def clear ( self ) : self . _desc = { } for key , value in merge . DEFAULT_PROJECT . items ( ) : if key not in self . _HIDDEN : self . _desc [ key ] = type ( value ) ( )
9376	def extract_diff_sla_from_config_file ( obj , options_file ) : rule_strings = { } config_obj = ConfigParser . ConfigParser ( ) config_obj . optionxform = str config_obj . read ( options_file ) for section in config_obj . sections ( ) : rule_strings , kwargs = get_rule_strings ( config_obj , section ) for ( key , val ) in rule_strings . iteritems ( ) : set_sla ( obj , section , key , val )
10851	def local_max_featuring ( im , radius = 2.5 , noise_size = 1. , bkg_size = None , minmass = 1. , trim_edge = False ) : if radius <= 0 : raise ValueError ( '`radius` must be > 0' ) filtered = nd . gaussian_filter ( im , noise_size , mode = 'mirror' ) if bkg_size is None : bkg_size = 2 * radius filtered -= nd . gaussian_filter ( filtered , bkg_size , mode = 'mirror' ) footprint = generate_sphere ( radius ) e = nd . maximum_filter ( filtered , footprint = footprint ) mass_im = nd . convolve ( filtered , footprint , mode = 'mirror' ) good_im = ( e == filtered ) * ( mass_im > minmass ) pos = np . transpose ( np . nonzero ( good_im ) ) if trim_edge : good = np . all ( pos > 0 , axis = 1 ) & np . all ( pos + 1 < im . shape , axis = 1 ) pos = pos [ good , : ] . copy ( ) masses = mass_im [ pos [ : , 0 ] , pos [ : , 1 ] , pos [ : , 2 ] ] . copy ( ) return pos , masses
12234	def pref ( preference , field = None , verbose_name = None , help_text = '' , static = True , readonly = False ) : try : bound = bind_proxy ( ( preference , ) , field = field , verbose_name = verbose_name , help_text = help_text , static = static , readonly = readonly , ) return bound [ 0 ] except IndexError : return
1638	def CheckSpacing ( filename , clean_lines , linenum , nesting_state , error ) : raw = clean_lines . lines_without_raw_strings line = raw [ linenum ] if ( IsBlankLine ( line ) and not nesting_state . InNamespaceBody ( ) and not nesting_state . InExternC ( ) ) : elided = clean_lines . elided prev_line = elided [ linenum - 1 ] prevbrace = prev_line . rfind ( '{' ) if prevbrace != - 1 and prev_line [ prevbrace : ] . find ( '}' ) == - 1 : exception = False if Match ( r' {6}\w' , prev_line ) : search_position = linenum - 2 while ( search_position >= 0 and Match ( r' {6}\w' , elided [ search_position ] ) ) : search_position -= 1 exception = ( search_position >= 0 and elided [ search_position ] [ : 5 ] == ' :' ) else : exception = ( Match ( r' {4}\w[^\(]*\)\s*(const\s*)?(\{\s*$|:)' , prev_line ) or Match ( r' {4}:' , prev_line ) ) if not exception : error ( filename , linenum , 'whitespace/blank_line' , 2 , 'Redundant blank line at the start of a code block ' 'should be deleted.' ) if linenum + 1 < clean_lines . NumLines ( ) : next_line = raw [ linenum + 1 ] if ( next_line and Match ( r'\s*}' , next_line ) and next_line . find ( '} else ' ) == - 1 ) : error ( filename , linenum , 'whitespace/blank_line' , 3 , 'Redundant blank line at the end of a code block ' 'should be deleted.' ) matched = Match ( r'\s*(public|protected|private):' , prev_line ) if matched : error ( filename , linenum , 'whitespace/blank_line' , 3 , 'Do not leave a blank line after "%s:"' % matched . group ( 1 ) ) next_line_start = 0 if linenum + 1 < clean_lines . NumLines ( ) : next_line = raw [ linenum + 1 ] next_line_start = len ( next_line ) - len ( next_line . lstrip ( ) ) CheckComment ( line , filename , linenum , next_line_start , error ) line = clean_lines . elided [ linenum ] if Search ( r'\w\s+\[' , line ) and not Search ( r'(?:delete|return)\s+\[' , line ) : error ( filename , linenum , 'whitespace/braces' , 5 , 'Extra space before [' ) if ( Search ( r'for *\(.*[^:]:[^: ]' , line ) or Search ( r'for *\(.*[^: ]:[^:]' , line ) ) : error ( filename , linenum , 'whitespace/forcolon' , 2 , 'Missing space around colon in range-based for loop' )
11929	def watch_files ( self ) : try : while 1 : sleep ( 1 ) try : files_stat = self . get_files_stat ( ) except SystemExit : logger . error ( "Error occurred, server shut down" ) self . shutdown_server ( ) if self . files_stat != files_stat : logger . info ( "Changes detected, start rebuilding.." ) try : generator . re_generate ( ) global _root _root = generator . root except SystemExit : logger . error ( "Error occurred, server shut down" ) self . shutdown_server ( ) self . files_stat = files_stat except KeyboardInterrupt : logger . info ( "^C received, shutting down watcher" ) self . shutdown_watcher ( )
758	def generateRandomInput ( numRecords , elemSize = 400 , numSet = 42 ) : inputs = [ ] for _ in xrange ( numRecords ) : input = np . zeros ( elemSize , dtype = realDType ) for _ in range ( 0 , numSet ) : ind = np . random . random_integers ( 0 , elemSize - 1 , 1 ) [ 0 ] input [ ind ] = 1 while abs ( input . sum ( ) - numSet ) > 0.1 : ind = np . random . random_integers ( 0 , elemSize - 1 , 1 ) [ 0 ] input [ ind ] = 1 inputs . append ( input ) return inputs
5683	def tripI_takes_place_on_dsut ( self , trip_I , day_start_ut ) : query = "SELECT * FROM days WHERE trip_I=? AND day_start_ut=?" params = ( trip_I , day_start_ut ) cur = self . conn . cursor ( ) rows = list ( cur . execute ( query , params ) ) if len ( rows ) == 0 : return False else : assert len ( rows ) == 1 , 'On a day, a trip_I should be present at most once' return True
3709	def calculate ( self , T , method ) : r if method == RACKETT : Vm = Rackett ( T , self . Tc , self . Pc , self . Zc ) elif method == YAMADA_GUNN : Vm = Yamada_Gunn ( T , self . Tc , self . Pc , self . omega ) elif method == BHIRUD_NORMAL : Vm = Bhirud_normal ( T , self . Tc , self . Pc , self . omega ) elif method == TOWNSEND_HALES : Vm = Townsend_Hales ( T , self . Tc , self . Vc , self . omega ) elif method == HTCOSTALD : Vm = COSTALD ( T , self . Tc , self . Vc , self . omega ) elif method == YEN_WOODS_SAT : Vm = Yen_Woods_saturation ( T , self . Tc , self . Vc , self . Zc ) elif method == MMSNM0 : Vm = SNM0 ( T , self . Tc , self . Vc , self . omega ) elif method == MMSNM0FIT : Vm = SNM0 ( T , self . Tc , self . Vc , self . omega , self . SNM0_delta_SRK ) elif method == CAMPBELL_THODOS : Vm = Campbell_Thodos ( T , self . Tb , self . Tc , self . Pc , self . MW , self . dipole ) elif method == HTCOSTALDFIT : Vm = COSTALD ( T , self . Tc , self . COSTALD_Vchar , self . COSTALD_omega_SRK ) elif method == RACKETTFIT : Vm = Rackett ( T , self . Tc , self . Pc , self . RACKETT_Z_RA ) elif method == PERRYDIPPR : A , B , C , D = self . DIPPR_coeffs Vm = 1. / EQ105 ( T , A , B , C , D ) elif method == CRC_INORG_L : rho = CRC_inorganic ( T , self . CRC_INORG_L_rho , self . CRC_INORG_L_k , self . CRC_INORG_L_Tm ) Vm = rho_to_Vm ( rho , self . CRC_INORG_L_MW ) elif method == VDI_PPDS : A , B , C , D = self . VDI_PPDS_coeffs tau = 1. - T / self . VDI_PPDS_Tc rho = self . VDI_PPDS_rhoc + A * tau ** 0.35 + B * tau ** ( 2 / 3. ) + C * tau + D * tau ** ( 4 / 3. ) Vm = rho_to_Vm ( rho , self . VDI_PPDS_MW ) elif method == CRC_INORG_L_CONST : Vm = self . CRC_INORG_L_CONST_Vm elif method == COOLPROP : Vm = 1. / CoolProp_T_dependent_property ( T , self . CASRN , 'DMOLAR' , 'l' ) elif method in self . tabular_data : Vm = self . interpolate ( T , method ) return Vm
2258	def argsort ( indexable , key = None , reverse = False ) : if isinstance ( indexable , collections_abc . Mapping ) : vk_iter = ( ( v , k ) for k , v in indexable . items ( ) ) else : vk_iter = ( ( v , k ) for k , v in enumerate ( indexable ) ) if key is None : indices = [ k for v , k in sorted ( vk_iter , reverse = reverse ) ] else : indices = [ k for v , k in sorted ( vk_iter , key = lambda vk : key ( vk [ 0 ] ) , reverse = reverse ) ] return indices
455	def list_remove_repeat ( x ) : y = [ ] for i in x : if i not in y : y . append ( i ) return y
389	def sequences_get_mask ( sequences , pad_val = 0 ) : mask = np . ones_like ( sequences ) for i , seq in enumerate ( sequences ) : for i_w in reversed ( range ( len ( seq ) ) ) : if seq [ i_w ] == pad_val : mask [ i , i_w ] = 0 else : break return mask
6688	def groupinstall ( group , options = None ) : manager = MANAGER if options is None : options = [ ] elif isinstance ( options , str ) : options = [ options ] options = " " . join ( options ) run_as_root ( '%(manager)s %(options)s groupinstall "%(group)s"' % locals ( ) , pty = False )
11096	def select_by_pattern_in_fname ( self , pattern , recursive = True , case_sensitive = False ) : if case_sensitive : def filters ( p ) : return pattern in p . fname else : pattern = pattern . lower ( ) def filters ( p ) : return pattern in p . fname . lower ( ) return self . select_file ( filters , recursive )
8044	def leapfrog ( self , kind , value = None ) : while self . current is not None : if self . current . kind == kind and ( value is None or self . current . value == value ) : self . consume ( kind ) return self . stream . move ( )
5395	def _get_input_target_path ( self , local_file_path ) : path , filename = os . path . split ( local_file_path ) if '*' in filename : return path + '/' else : return local_file_path
10663	def elements ( compounds ) : elementlist = [ parse_compound ( compound ) . count ( ) . keys ( ) for compound in compounds ] return set ( ) . union ( * elementlist )
5655	def _finalize_profiles ( self ) : for stop , stop_profile in self . _stop_profiles . items ( ) : assert ( isinstance ( stop_profile , NodeProfileMultiObjective ) ) neighbor_label_bags = [ ] walk_durations_to_neighbors = [ ] departure_arrival_stop_pairs = [ ] if stop_profile . get_walk_to_target_duration ( ) != 0 and stop in self . _walk_network . node : neighbors = networkx . all_neighbors ( self . _walk_network , stop ) for neighbor in neighbors : neighbor_profile = self . _stop_profiles [ neighbor ] assert ( isinstance ( neighbor_profile , NodeProfileMultiObjective ) ) neighbor_real_connection_labels = neighbor_profile . get_labels_for_real_connections ( ) neighbor_label_bags . append ( neighbor_real_connection_labels ) walk_durations_to_neighbors . append ( int ( self . _walk_network . get_edge_data ( stop , neighbor ) [ "d_walk" ] / self . _walk_speed ) ) departure_arrival_stop_pairs . append ( ( stop , neighbor ) ) stop_profile . finalize ( neighbor_label_bags , walk_durations_to_neighbors , departure_arrival_stop_pairs )
2541	def set_pkg_desc ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_desc_set : self . package_desc_set = True doc . package . description = text else : raise CardinalityError ( 'Package::Description' )
8087	def font ( self , fontpath = None , fontsize = None ) : if fontpath is not None : self . _canvas . fontfile = fontpath else : return self . _canvas . fontfile if fontsize is not None : self . _canvas . fontsize = fontsize
2486	def licenses_from_tree ( self , tree ) : licenses = set ( ) self . licenses_from_tree_helper ( tree , licenses ) return licenses
11274	def check_pidfile ( pidfile , debug ) : if os . path . isfile ( pidfile ) : pidfile_handle = open ( pidfile , 'r' ) try : pid = int ( pidfile_handle . read ( ) ) pidfile_handle . close ( ) if check_pid ( pid , debug ) : return True except : pass os . unlink ( pidfile ) pid = str ( os . getpid ( ) ) open ( pidfile , 'w' ) . write ( pid ) return False
13312	def _pre_activate ( self ) : if 'CPENV_CLEAN_ENV' not in os . environ : if platform == 'win' : os . environ [ 'PROMPT' ] = '$P$G' else : os . environ [ 'PS1' ] = '\\u@\\h:\\w\\$' clean_env_path = utils . get_store_env_tmp ( ) os . environ [ 'CPENV_CLEAN_ENV' ] = clean_env_path utils . store_env ( path = clean_env_path ) else : utils . restore_env_from_file ( os . environ [ 'CPENV_CLEAN_ENV' ] )
4117	def poly2lsf ( a ) : a = numpy . array ( a ) if a [ 0 ] != 1 : a /= a [ 0 ] if max ( numpy . abs ( numpy . roots ( a ) ) ) >= 1.0 : error ( 'The polynomial must have all roots inside of the unit circle.' ) p = len ( a ) - 1 a1 = numpy . concatenate ( ( a , numpy . array ( [ 0 ] ) ) ) a2 = a1 [ - 1 : : - 1 ] P1 = a1 - a2 Q1 = a1 + a2 if p % 2 : P , r = deconvolve ( P1 , [ 1 , 0 , - 1 ] ) Q = Q1 else : P , r = deconvolve ( P1 , [ 1 , - 1 ] ) Q , r = deconvolve ( Q1 , [ 1 , 1 ] ) rP = numpy . roots ( P ) rQ = numpy . roots ( Q ) aP = numpy . angle ( rP [ 1 : : 2 ] ) aQ = numpy . angle ( rQ [ 1 : : 2 ] ) lsf = sorted ( numpy . concatenate ( ( - aP , - aQ ) ) ) return lsf
5715	def _slugify_foreign_key ( schema ) : for foreign_key in schema . get ( 'foreignKeys' , [ ] ) : foreign_key [ 'reference' ] [ 'resource' ] = _slugify_resource_name ( foreign_key [ 'reference' ] . get ( 'resource' , '' ) ) return schema
8830	def segment_allocation_find ( context , lock_mode = False , ** filters ) : range_ids = filters . pop ( "segment_allocation_range_ids" , None ) query = context . session . query ( models . SegmentAllocation ) if lock_mode : query = query . with_lockmode ( "update" ) query = query . filter_by ( ** filters ) if range_ids : query . filter ( models . SegmentAllocation . segment_allocation_range_id . in_ ( range_ids ) ) return query
12095	def indexImages ( folder , fname = "index.html" ) : html = "<html><body>" for item in glob . glob ( folder + "/*.*" ) : if item . split ( "." ) [ - 1 ] in [ 'jpg' , 'png' ] : html += "<h3>%s</h3>" % os . path . basename ( item ) html += '<img src="%s">' % os . path . basename ( item ) html += '<br>' * 10 html += "</html></body>" f = open ( folder + "/" + fname , 'w' ) f . write ( html ) f . close print ( "indexed:" ) print ( " " , os . path . abspath ( folder + "/" + fname ) ) return
8158	def sql ( self , sql ) : self . _cur . execute ( sql ) if sql . lower ( ) . find ( "select" ) >= 0 : matches = [ ] for r in self . _cur : matches . append ( r ) return matches
11585	def _getnodenamefor ( self , name ) : "Return the node name where the ``name`` would land to" return 'node_' + str ( ( abs ( binascii . crc32 ( b ( name ) ) & 0xffffffff ) % self . no_servers ) + 1 )
11211	def normalized ( self ) : days = int ( self . days ) hours_f = round ( self . hours + 24 * ( self . days - days ) , 11 ) hours = int ( hours_f ) minutes_f = round ( self . minutes + 60 * ( hours_f - hours ) , 10 ) minutes = int ( minutes_f ) seconds_f = round ( self . seconds + 60 * ( minutes_f - minutes ) , 8 ) seconds = int ( seconds_f ) microseconds = round ( self . microseconds + 1e6 * ( seconds_f - seconds ) ) return self . __class__ ( years = self . years , months = self . months , days = days , hours = hours , minutes = minutes , seconds = seconds , microseconds = microseconds , leapdays = self . leapdays , year = self . year , month = self . month , day = self . day , weekday = self . weekday , hour = self . hour , minute = self . minute , second = self . second , microsecond = self . microsecond )
4574	def hsv2rgb_rainbow ( hsv ) : def nscale8x3_video ( r , g , b , scale ) : nonzeroscale = 0 if scale != 0 : nonzeroscale = 1 if r != 0 : r = ( ( r * scale ) >> 8 ) + nonzeroscale if g != 0 : g = ( ( g * scale ) >> 8 ) + nonzeroscale if b != 0 : b = ( ( b * scale ) >> 8 ) + nonzeroscale return ( r , g , b ) def scale8_video_LEAVING_R1_DIRTY ( i , scale ) : nonzeroscale = 0 if scale != 0 : nonzeroscale = 1 if i != 0 : i = ( ( i * scale ) >> 8 ) + nonzeroscale return i h , s , v = hsv offset = h & 0x1F offset8 = offset * 8 third = ( offset8 * ( 256 // 3 ) ) >> 8 r , g , b = ( 0 , 0 , 0 ) if not ( h & 0x80 ) : if not ( h & 0x40 ) : if not ( h & 0x20 ) : r = 255 - third g = third b = 0 else : r = 171 g = 85 + third b = 0x00 else : if not ( h & 0x20 ) : twothirds = ( third << 1 ) r = 171 - twothirds g = 171 + third b = 0 else : r = 0 g = 255 - third b = third else : if not ( h & 0x40 ) : if not ( h & 0x20 ) : r = 0x00 twothirds = ( third << 1 ) g = 171 - twothirds b = 85 + twothirds else : r = third g = 0 b = 255 - third else : if not ( h & 0x20 ) : r = 85 + third g = 0 b = 171 - third else : r = 171 + third g = 0x00 b = 85 - third if s != 255 : r , g , b = nscale8x3_video ( r , g , b , s ) desat = 255 - s desat = ( desat * desat ) >> 8 brightness_floor = desat r = r + brightness_floor g = g + brightness_floor b = b + brightness_floor if v != 255 : v = scale8_video_LEAVING_R1_DIRTY ( v , v ) r , g , b = nscale8x3_video ( r , g , b , v ) return ( r , g , b )
2001	def visit_BitVecOr ( self , expression , * operands ) : left = expression . operands [ 0 ] right = expression . operands [ 1 ] if isinstance ( right , BitVecConstant ) : if right . value == 0 : return left elif right . value == left . mask : return right elif isinstance ( left , BitVecOr ) : left_left = left . operands [ 0 ] left_right = left . operands [ 1 ] if isinstance ( right , Constant ) : return BitVecOr ( left_left , ( left_right | right ) , taint = expression . taint ) elif isinstance ( left , BitVecConstant ) : return BitVecOr ( right , left , taint = expression . taint )
7576	def _call_structure ( mname , ename , sname , name , workdir , seed , ntaxa , nsites , kpop , rep ) : outname = os . path . join ( workdir , "{}-K-{}-rep-{}" . format ( name , kpop , rep ) ) cmd = [ "structure" , "-m" , mname , "-e" , ename , "-K" , str ( kpop ) , "-D" , str ( seed ) , "-N" , str ( ntaxa ) , "-L" , str ( nsites ) , "-i" , sname , "-o" , outname ] proc = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) comm = proc . communicate ( ) oldfiles = [ mname , ename , sname ] for oldfile in oldfiles : if os . path . exists ( oldfile ) : os . remove ( oldfile ) return comm
13006	def utime ( self , * args , ** kwargs ) : os . utime ( self . extended_path , * args , ** kwargs )
4367	def call_method_with_acl ( self , method_name , packet , * args ) : if not self . is_method_allowed ( method_name ) : self . error ( 'method_access_denied' , 'You do not have access to method "%s"' % method_name ) return return self . call_method ( method_name , packet , * args )
13112	def get_configured_dns ( ) : ips = [ ] try : output = subprocess . check_output ( [ 'nmcli' , 'device' , 'show' ] ) output = output . decode ( 'utf-8' ) for line in output . split ( '\n' ) : if 'DNS' in line : pattern = r"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}" for hit in re . findall ( pattern , line ) : ips . append ( hit ) except FileNotFoundError : pass return ips
3589	def set_color ( self , r , g , b ) : command = '\x58\x01\x03\x01\xFF\x00{0}{1}{2}' . format ( chr ( r & 0xFF ) , chr ( g & 0xFF ) , chr ( b & 0xFF ) ) self . _color . write_value ( command )
7601	def get_popular_decks ( self , ** params : keys ) : url = self . api . POPULAR + '/decks' return self . _get_model ( url , ** params )
8435	def map ( cls , x , palette , limits , na_value = None , oob = censor ) : x = oob ( rescale ( x , _from = limits ) ) pal = palette ( x ) try : pal [ pd . isnull ( x ) ] = na_value except TypeError : pal = [ v if not pd . isnull ( v ) else na_value for v in pal ] return pal
7206	def generate_workflow_description ( self ) : if not self . tasks : raise WorkflowError ( 'Workflow contains no tasks, and cannot be executed.' ) self . definition = self . workflow_skeleton ( ) if self . batch_values : self . definition [ "batch_values" ] = self . batch_values all_input_port_values = [ t . inputs . __getattribute__ ( input_port_name ) . value for t in self . tasks for input_port_name in t . inputs . _portnames ] for task in self . tasks : output_multiplex_ports_to_exclude = [ ] multiplex_output_port_names = [ portname for portname in task . outputs . _portnames if task . outputs . __getattribute__ ( portname ) . is_multiplex ] for p in multiplex_output_port_names : output_port_reference = 'source:' + task . name + ':' + p if output_port_reference not in all_input_port_values : output_multiplex_ports_to_exclude . append ( p ) task_def = task . generate_task_workflow_json ( output_multiplex_ports_to_exclude = output_multiplex_ports_to_exclude ) self . definition [ 'tasks' ] . append ( task_def ) if self . callback : self . definition [ 'callback' ] = self . callback return self . definition
4342	def reverse ( self ) : effect_args = [ 'reverse' ] self . effects . extend ( effect_args ) self . effects_log . append ( 'reverse' ) return self
1010	def _trimSegmentsInCell ( self , colIdx , cellIdx , segList , minPermanence , minNumSyns ) : if minPermanence is None : minPermanence = self . connectedPerm if minNumSyns is None : minNumSyns = self . activationThreshold nSegsRemoved , nSynsRemoved = 0 , 0 segsToDel = [ ] for segment in segList : synsToDel = [ syn for syn in segment . syns if syn [ 2 ] < minPermanence ] if len ( synsToDel ) == len ( segment . syns ) : segsToDel . append ( segment ) else : if len ( synsToDel ) > 0 : for syn in synsToDel : segment . syns . remove ( syn ) nSynsRemoved += 1 if len ( segment . syns ) < minNumSyns : segsToDel . append ( segment ) nSegsRemoved += len ( segsToDel ) for seg in segsToDel : self . _cleanUpdatesList ( colIdx , cellIdx , seg ) self . cells [ colIdx ] [ cellIdx ] . remove ( seg ) nSynsRemoved += len ( seg . syns ) return nSegsRemoved , nSynsRemoved
5008	def _create_session ( self ) : session = requests . Session ( ) session . timeout = self . SESSION_TIMEOUT oauth_access_token , expires_at = SAPSuccessFactorsAPIClient . get_oauth_access_token ( self . enterprise_configuration . sapsf_base_url , self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . sapsf_company_id , self . enterprise_configuration . sapsf_user_id , self . enterprise_configuration . user_type ) session . headers [ 'Authorization' ] = 'Bearer {}' . format ( oauth_access_token ) session . headers [ 'content-type' ] = 'application/json' self . session = session self . expires_at = expires_at
9088	async def _update_loop ( self ) -> None : await asyncio . sleep ( self . _update_interval ) while not self . _closed : await self . update ( ) await asyncio . sleep ( self . _update_interval )
4304	def _get_valid_formats ( ) : if NO_SOX : return [ ] so = subprocess . check_output ( [ 'sox' , '-h' ] ) if type ( so ) is not str : so = str ( so , encoding = 'UTF-8' ) so = so . split ( '\n' ) idx = [ i for i in range ( len ( so ) ) if 'AUDIO FILE FORMATS:' in so [ i ] ] [ 0 ] formats = so [ idx ] . split ( ' ' ) [ 3 : ] return formats
10154	def _extract_transform_colander_schema ( self , args ) : schema = args . get ( 'schema' , colander . MappingSchema ( ) ) if not isinstance ( schema , colander . Schema ) : schema = schema ( ) schema = schema . clone ( ) for transformer in self . schema_transformers : schema = transformer ( schema , args ) return schema
12533	def from_set ( self , fileset , check_if_dicoms = True ) : if check_if_dicoms : self . items = [ ] for f in fileset : if is_dicom_file ( f ) : self . items . append ( f ) else : self . items = fileset
7910	def __presence_unavailable ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : return False rs . process_unavailable_presence ( MucPresence ( stanza ) ) return True
6169	def filter ( self , x ) : y = signal . lfilter ( self . b , [ 1 ] , x ) return y
4520	def set ( self , ring , angle , color ) : pixel = self . angleToPixel ( angle , ring ) self . _set_base ( pixel , color )
5091	def get_clear_catalog_id_action ( description = None ) : description = description or _ ( "Unlink selected objects from existing course catalogs" ) def clear_catalog_id ( modeladmin , request , queryset ) : queryset . update ( catalog = None ) clear_catalog_id . short_description = description return clear_catalog_id
9070	def value ( self ) : r from numpy_sugar . linalg import ddot , sum2diag if self . _cache [ "value" ] is not None : return self . _cache [ "value" ] scale = exp ( self . logscale ) delta = 1 / ( 1 + exp ( - self . logitdelta ) ) v0 = scale * ( 1 - delta ) v1 = scale * delta mu = self . eta / self . tau n = len ( mu ) if self . _QS is None : K = zeros ( ( n , n ) ) else : Q0 = self . _QS [ 0 ] [ 0 ] S0 = self . _QS [ 1 ] K = dot ( ddot ( Q0 , S0 ) , Q0 . T ) A = sum2diag ( sum2diag ( v0 * K , v1 ) , 1 / self . tau ) m = mu - self . mean ( ) v = - n * log ( 2 * pi ) v -= slogdet ( A ) [ 1 ] v -= dot ( m , solve ( A , m ) ) self . _cache [ "value" ] = v / 2 return self . _cache [ "value" ]
11961	def is_wildcard_nm ( nm ) : try : dec = 0xFFFFFFFF - _dot_to_dec ( nm , check = True ) except ValueError : return False if dec in _NETMASKS_VALUES : return True return False
13868	def weekday ( when , weekday , start = mon ) : if isinstance ( when , datetime ) : when = when . date ( ) today = when . weekday ( ) delta = weekday - today if weekday < start and today >= start : delta += 7 elif weekday >= start and today < start : delta -= 7 return when + timedelta ( days = delta )
9569	def build_message ( self , data ) : if not data : return None return Message ( id = data [ 'message' ] [ 'mid' ] , platform = self . platform , text = data [ 'message' ] [ 'text' ] , user = data [ 'sender' ] [ 'id' ] , timestamp = data [ 'timestamp' ] , raw = data , chat = None , )
8369	def create_canvas ( src , format = None , outputfile = None , multifile = False , buff = None , window = False , title = None , fullscreen = None , show_vars = False ) : from core import CairoCanvas , CairoImageSink if outputfile : sink = CairoImageSink ( outputfile , format , multifile , buff ) elif window or show_vars : from gui import ShoebotWindow if not title : if src and os . path . isfile ( src ) : title = os . path . splitext ( os . path . basename ( src ) ) [ 0 ] + ' - Shoebot' else : title = 'Untitled - Shoebot' sink = ShoebotWindow ( title , show_vars , fullscreen = fullscreen ) else : if src and isinstance ( src , cairo . Surface ) : outputfile = src format = 'surface' elif src and os . path . isfile ( src ) : outputfile = os . path . splitext ( os . path . basename ( src ) ) [ 0 ] + '.' + ( format or 'svg' ) else : outputfile = 'output.svg' sink = CairoImageSink ( outputfile , format , multifile , buff ) canvas = CairoCanvas ( sink ) return canvas
2567	def check_tracking_enabled ( self ) : track = True test = False testvar = str ( os . environ . get ( "PARSL_TESTING" , 'None' ) ) . lower ( ) if testvar == 'true' : test = True if not self . config . usage_tracking : track = False envvar = str ( os . environ . get ( "PARSL_TRACKING" , True ) ) . lower ( ) if envvar == "false" : track = False return test , track
9898	def _initfile ( path , data = "dict" ) : data = { } if data . lower ( ) == "dict" else [ ] if not os . path . exists ( path ) : dirname = os . path . dirname ( path ) if dirname and not os . path . exists ( dirname ) : raise IOError ( ( "Could not initialize empty JSON file in non-existant " "directory '{}'" ) . format ( os . path . dirname ( path ) ) ) with open ( path , "w" ) as f : json . dump ( data , f ) return True elif os . path . getsize ( path ) == 0 : with open ( path , "w" ) as f : json . dump ( data , f ) else : return False
977	def _newRepresentationOK ( self , newRep , newIndex ) : if newRep . size != self . w : return False if ( newIndex < self . minIndex - 1 ) or ( newIndex > self . maxIndex + 1 ) : raise ValueError ( "newIndex must be within one of existing indices" ) newRepBinary = numpy . array ( [ False ] * self . n ) newRepBinary [ newRep ] = True midIdx = self . _maxBuckets / 2 runningOverlap = self . _countOverlap ( self . bucketMap [ self . minIndex ] , newRep ) if not self . _overlapOK ( self . minIndex , newIndex , overlap = runningOverlap ) : return False for i in range ( self . minIndex + 1 , midIdx + 1 ) : newBit = ( i - 1 ) % self . w if newRepBinary [ self . bucketMap [ i - 1 ] [ newBit ] ] : runningOverlap -= 1 if newRepBinary [ self . bucketMap [ i ] [ newBit ] ] : runningOverlap += 1 if not self . _overlapOK ( i , newIndex , overlap = runningOverlap ) : return False for i in range ( midIdx + 1 , self . maxIndex + 1 ) : newBit = i % self . w if newRepBinary [ self . bucketMap [ i - 1 ] [ newBit ] ] : runningOverlap -= 1 if newRepBinary [ self . bucketMap [ i ] [ newBit ] ] : runningOverlap += 1 if not self . _overlapOK ( i , newIndex , overlap = runningOverlap ) : return False return True
1469	def process_tick ( self , tup ) : curtime = int ( time . time ( ) ) window_info = WindowContext ( curtime - self . window_duration , curtime ) self . processWindow ( window_info , list ( self . current_tuples ) ) for tup in self . current_tuples : self . ack ( tup ) self . current_tuples . clear ( )
1741	def add_inputs ( self , xs ) : states = [ ] cur = self for x in xs : cur = cur . add_input ( x ) states . append ( cur ) return states
5199	def process_point_value ( cls , command_type , command , index , op_type ) : _log . debug ( 'Processing received point value for index {}: {}' . format ( index , command ) )
8884	def fit ( self , X , y = None ) : X = check_array ( X ) self . _x_min = X . min ( axis = 0 ) self . _x_max = X . max ( axis = 0 ) return self
685	def getTotalw ( self ) : w = sum ( [ field . w for field in self . fields ] ) return w
11775	def EnsembleLearner ( learners ) : def train ( dataset ) : predictors = [ learner ( dataset ) for learner in learners ] def predict ( example ) : return mode ( predictor ( example ) for predictor in predictors ) return predict return train
6263	def check_glfw_version ( self ) : print ( "glfw version: {} (python wrapper version {})" . format ( glfw . get_version ( ) , glfw . __version__ ) ) if glfw . get_version ( ) < self . min_glfw_version : raise ValueError ( "Please update glfw binaries to version {} or later" . format ( self . min_glfw_version ) )
12495	def column_or_1d ( y , warn = False ) : shape = np . shape ( y ) if len ( shape ) == 1 : return np . ravel ( y ) if len ( shape ) == 2 and shape [ 1 ] == 1 : if warn : warnings . warn ( "A column-vector y was passed when a 1d array was" " expected. Please change the shape of y to " "(n_samples, ), for example using ravel()." , DataConversionWarning , stacklevel = 2 ) return np . ravel ( y ) raise ValueError ( "bad input shape {0}" . format ( shape ) )
11407	def record_get_field_instances ( rec , tag = "" , ind1 = " " , ind2 = " " ) : if not rec : return [ ] if not tag : return rec . items ( ) else : out = [ ] ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) if '%' in tag : for field_tag in rec : if _tag_matches_pattern ( field_tag , tag ) : for possible_field_instance in rec [ field_tag ] : if ( ind1 in ( '%' , possible_field_instance [ 1 ] ) and ind2 in ( '%' , possible_field_instance [ 2 ] ) ) : out . append ( possible_field_instance ) else : for possible_field_instance in rec . get ( tag , [ ] ) : if ( ind1 in ( '%' , possible_field_instance [ 1 ] ) and ind2 in ( '%' , possible_field_instance [ 2 ] ) ) : out . append ( possible_field_instance ) return out
5685	def increment_day_start_ut ( self , day_start_ut , n_days = 1 ) : old_tz = self . set_current_process_time_zone ( ) day0 = time . localtime ( day_start_ut + 43200 ) dayN = time . mktime ( day0 [ : 2 ] + ( day0 [ 2 ] + n_days , ) + ( 12 , 00 , 0 , 0 , 0 , - 1 ) ) - 43200 set_process_timezone ( old_tz ) return dayN
4315	def validate_input_file_list ( input_filepath_list ) : if not isinstance ( input_filepath_list , list ) : raise TypeError ( "input_filepath_list must be a list." ) elif len ( input_filepath_list ) < 2 : raise ValueError ( "input_filepath_list must have at least 2 files." ) for input_filepath in input_filepath_list : validate_input_file ( input_filepath )
8189	def eigenvector_centrality ( self , normalized = True , reversed = True , rating = { } , start = None , iterations = 100 , tolerance = 0.0001 ) : ec = proximity . eigenvector_centrality ( self , normalized , reversed , rating , start , iterations , tolerance ) for id , w in ec . iteritems ( ) : self [ id ] . _eigenvalue = w return ec
10366	def complex_has_member ( graph : BELGraph , complex_node : ComplexAbundance , member_node : BaseEntity ) -> bool : return any ( v == member_node for _ , v , data in graph . out_edges ( complex_node , data = True ) if data [ RELATION ] == HAS_COMPONENT )
11721	def config_loader ( app , ** kwargs_config ) : local_templates_path = os . path . join ( app . instance_path , 'templates' ) if os . path . exists ( local_templates_path ) : app . jinja_loader = ChoiceLoader ( [ FileSystemLoader ( local_templates_path ) , app . jinja_loader , ] ) app . jinja_options = dict ( app . jinja_options , cache_size = 1000 , bytecode_cache = BytecodeCache ( app ) ) invenio_config_loader ( app , ** kwargs_config )
4205	def levdown ( anxt , enxt = None ) : if anxt [ 0 ] != 1 : raise ValueError ( 'At least one of the reflection coefficients is equal to one.' ) anxt = anxt [ 1 : ] knxt = anxt [ - 1 ] if knxt == 1.0 : raise ValueError ( 'At least one of the reflection coefficients is equal to one.' ) acur = ( anxt [ 0 : - 1 ] - knxt * numpy . conj ( anxt [ - 2 : : - 1 ] ) ) / ( 1. - abs ( knxt ) ** 2 ) ecur = None if enxt is not None : ecur = enxt / ( 1. - numpy . dot ( knxt . conj ( ) . transpose ( ) , knxt ) ) acur = numpy . insert ( acur , 0 , 1 ) return acur , ecur
3365	def add_pfba ( model , objective = None , fraction_of_optimum = 1.0 ) : if objective is not None : model . objective = objective if model . solver . objective . name == '_pfba_objective' : raise ValueError ( 'The model already has a pFBA objective.' ) sutil . fix_objective_as_constraint ( model , fraction = fraction_of_optimum ) reaction_variables = ( ( rxn . forward_variable , rxn . reverse_variable ) for rxn in model . reactions ) variables = chain ( * reaction_variables ) model . objective = model . problem . Objective ( Zero , direction = 'min' , sloppy = True , name = "_pfba_objective" ) model . objective . set_linear_coefficients ( { v : 1.0 for v in variables } )
12296	def post ( repo , args = [ ] ) : mgr = plugins_get_mgr ( ) keys = mgr . search ( what = 'metadata' ) keys = keys [ 'metadata' ] if len ( keys ) == 0 : return if 'pipeline' in repo . options : for name , details in repo . options [ 'pipeline' ] . items ( ) : patterns = details [ 'files' ] matching_files = repo . find_matching_files ( patterns ) matching_files . sort ( ) details [ 'files' ] = matching_files for i , f in enumerate ( matching_files ) : r = repo . get_resource ( f ) if 'pipeline' not in r : r [ 'pipeline' ] = [ ] r [ 'pipeline' ] . append ( name + " [Step {}]" . format ( i ) ) if 'metadata-management' in repo . options : print ( "Collecting all the required metadata to post" ) metadata = repo . options [ 'metadata-management' ] if 'include-data-history' in metadata and metadata [ 'include-data-history' ] : repo . package [ 'history' ] = get_history ( repo . rootdir ) if 'include-action-history' in metadata and metadata [ 'include-action-history' ] : annotate_metadata_action ( repo ) if 'include-preview' in metadata : annotate_metadata_data ( repo , task = 'preview' , patterns = metadata [ 'include-preview' ] [ 'files' ] , size = metadata [ 'include-preview' ] [ 'length' ] ) if ( ( 'include-schema' in metadata ) and metadata [ 'include-schema' ] ) : annotate_metadata_data ( repo , task = 'schema' ) if 'include-code-history' in metadata : annotate_metadata_code ( repo , files = metadata [ 'include-code-history' ] ) if 'include-platform' in metadata : annotate_metadata_platform ( repo ) if 'include-validation' in metadata : annotate_metadata_validation ( repo ) if 'include-dependencies' in metadata : annotate_metadata_dependencies ( repo ) history = repo . package . get ( 'history' , None ) if ( ( 'include-tab-diffs' in metadata ) and metadata [ 'include-tab-diffs' ] and history is not None ) : annotate_metadata_diffs ( repo ) repo . package [ 'config' ] = repo . options try : for k in keys : metadatamgr = mgr . get_by_key ( 'metadata' , k ) url = metadatamgr . url o = urlparse ( url ) print ( "Posting to " , o . netloc ) response = metadatamgr . post ( repo ) if isinstance ( response , str ) : print ( "Error while posting:" , response ) elif response . status_code in [ 400 ] : content = response . json ( ) print ( "Error while posting:" ) for k in content : print ( " " , k , "- " , "," . join ( content [ k ] ) ) except NetworkError as e : print ( "Unable to reach metadata server!" ) except NetworkInvalidConfiguration as e : print ( "Invalid network configuration in the INI file" ) print ( e . message ) except Exception as e : print ( "Could not post. Unknown error" ) print ( e )
9303	def get_request_date ( cls , req ) : date = None for header in [ 'x-amz-date' , 'date' ] : if header not in req . headers : continue try : date_str = cls . parse_date ( req . headers [ header ] ) except DateFormatError : continue try : date = datetime . datetime . strptime ( date_str , '%Y-%m-%d' ) . date ( ) except ValueError : continue else : break return date
6395	def fingerprint ( self , word ) : word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = '' . join ( c for c in word if c in self . _letters ) key = '' for char in self . _consonants : if char in word : key += char for char in word : if char not in self . _consonants and char not in key : key += char return key
8844	def _handle_indent_between_paren ( self , column , line , parent_impl , tc ) : pre , post = parent_impl next_char = self . _get_next_char ( tc ) prev_char = self . _get_prev_char ( tc ) prev_open = prev_char in [ '[' , '(' , '{' ] next_close = next_char in [ ']' , ')' , '}' ] ( open_line , open_symbol_col ) , ( close_line , close_col ) = self . _get_paren_pos ( tc , column ) open_line_txt = self . _helper . line_text ( open_line ) open_line_indent = len ( open_line_txt ) - len ( open_line_txt . lstrip ( ) ) if prev_open : post = ( open_line_indent + self . editor . tab_length ) * ' ' elif next_close and prev_char != ',' : post = open_line_indent * ' ' elif tc . block ( ) . blockNumber ( ) == open_line : post = open_symbol_col * ' ' if close_line and close_col : txt = self . _helper . line_text ( close_line ) bn = tc . block ( ) . blockNumber ( ) flg = bn == close_line next_indent = self . _helper . line_indent ( bn + 1 ) * ' ' if flg and txt . strip ( ) . endswith ( ':' ) and next_indent == post : post += self . editor . tab_length * ' ' if next_char in [ '"' , "'" ] : tc . movePosition ( tc . Left ) is_string = self . _helper . is_comment_or_string ( tc , formats = [ 'string' ] ) if next_char in [ '"' , "'" ] : tc . movePosition ( tc . Right ) if is_string : trav = QTextCursor ( tc ) while self . _helper . is_comment_or_string ( trav , formats = [ 'string' ] ) : trav . movePosition ( trav . Left ) trav . movePosition ( trav . Right ) symbol = '%s' % self . _get_next_char ( trav ) pre += symbol post += symbol return pre , post
1017	def _adaptSegment ( self , segUpdate ) : trimSegment = False c , i , segment = segUpdate . columnIdx , segUpdate . cellIdx , segUpdate . segment activeSynapses = segUpdate . activeSynapses synToUpdate = set ( [ syn for syn in activeSynapses if type ( syn ) == int ] ) if segment is not None : if self . verbosity >= 4 : print "Reinforcing segment #%d for cell[%d,%d]" % ( segment . segID , c , i ) print " before:" , segment . debugPrint ( ) segment . lastActiveIteration = self . lrnIterationIdx segment . positiveActivations += 1 segment . dutyCycle ( active = True ) lastSynIndex = len ( segment . syns ) - 1 inactiveSynIndices = [ s for s in xrange ( 0 , lastSynIndex + 1 ) if s not in synToUpdate ] trimSegment = segment . updateSynapses ( inactiveSynIndices , - self . permanenceDec ) activeSynIndices = [ syn for syn in synToUpdate if syn <= lastSynIndex ] segment . updateSynapses ( activeSynIndices , self . permanenceInc ) synsToAdd = [ syn for syn in activeSynapses if type ( syn ) != int ] if self . maxSynapsesPerSegment > 0 and len ( synsToAdd ) + len ( segment . syns ) > self . maxSynapsesPerSegment : numToFree = ( len ( segment . syns ) + len ( synsToAdd ) - self . maxSynapsesPerSegment ) segment . freeNSynapses ( numToFree , inactiveSynIndices , self . verbosity ) for newSyn in synsToAdd : segment . addSynapse ( newSyn [ 0 ] , newSyn [ 1 ] , self . initialPerm ) if self . verbosity >= 4 : print " after:" , segment . debugPrint ( ) else : newSegment = Segment ( tm = self , isSequenceSeg = segUpdate . sequenceSegment ) for synapse in activeSynapses : newSegment . addSynapse ( synapse [ 0 ] , synapse [ 1 ] , self . initialPerm ) if self . verbosity >= 3 : print "New segment #%d for cell[%d,%d]" % ( self . segID - 1 , c , i ) , newSegment . debugPrint ( ) self . cells [ c ] [ i ] . append ( newSegment ) return trimSegment
8165	def load_edited_source ( self , source , good_cb = None , bad_cb = None , filename = None ) : with LiveExecution . lock : self . good_cb = good_cb self . bad_cb = bad_cb try : compile ( source + '\n\n' , filename or self . filename , "exec" ) self . edited_source = source except Exception as e : if bad_cb : self . edited_source = None tb = traceback . format_exc ( ) self . call_bad_cb ( tb ) return if filename is not None : self . filename = filename
5894	def render ( self , name , value , attrs = { } ) : if value is None : value = '' final_attrs = self . build_attrs ( attrs , name = name ) quill_app = apps . get_app_config ( 'quill' ) quill_config = getattr ( quill_app , self . config ) return mark_safe ( render_to_string ( quill_config [ 'template' ] , { 'final_attrs' : flatatt ( final_attrs ) , 'value' : value , 'id' : final_attrs [ 'id' ] , 'config' : self . config , } ) )
8858	def on_goto_out_of_doc ( self , assignment ) : editor = self . open_file ( assignment . module_path ) if editor : TextHelper ( editor ) . goto_line ( assignment . line , assignment . column )
12574	def smooth_fwhm ( self , fwhm ) : if fwhm != self . _smooth_fwhm : self . _is_data_smooth = False self . _smooth_fwhm = fwhm
8271	def _save ( self ) : if not os . path . exists ( self . cache ) : os . makedirs ( self . cache ) path = os . path . join ( self . cache , self . name + ".xml" ) f = open ( path , "w" ) f . write ( self . xml ) f . close ( )
8628	def create_project ( session , title , description , currency , budget , jobs ) : project_data = { 'title' : title , 'description' : description , 'currency' : currency , 'budget' : budget , 'jobs' : jobs } response = make_post_request ( session , 'projects' , json_data = project_data ) json_data = response . json ( ) if response . status_code == 200 : project_data = json_data [ 'result' ] p = Project ( project_data ) p . url = urljoin ( session . url , 'projects/%s' % p . seo_url ) return p else : raise ProjectNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] , )
1977	def sys_deallocate ( self , cpu , addr , size ) : logger . info ( "DEALLOCATE(0x%08x, %d)" % ( addr , size ) ) if addr & 0xfff != 0 : logger . info ( "DEALLOCATE: addr is not page aligned" ) return Decree . CGC_EINVAL if size == 0 : logger . info ( "DEALLOCATE:length is zero" ) return Decree . CGC_EINVAL cpu . memory . munmap ( addr , size ) self . syscall_trace . append ( ( "_deallocate" , - 1 , size ) ) return 0
2678	def get_role_name ( region , account_id , role ) : prefix = ARN_PREFIXES . get ( region , 'aws' ) return 'arn:{0}:iam::{1}:role/{2}' . format ( prefix , account_id , role )
11107	def walk_files_relative_path ( self , relativePath = "" ) : def walk_files ( directory , relativePath ) : directories = dict . __getitem__ ( directory , 'directories' ) files = dict . __getitem__ ( directory , 'files' ) for f in sorted ( files ) : yield os . path . join ( relativePath , f ) for k in sorted ( dict . keys ( directories ) ) : path = os . path . join ( relativePath , k ) dir = directories . __getitem__ ( k ) for e in walk_files ( dir , path ) : yield e dir , errorMessage = self . get_directory_info ( relativePath ) assert dir is not None , errorMessage return walk_files ( dir , relativePath = '' )
11666	def _get_rhos ( X , indices , Ks , max_K , save_all_Ks , min_dist ) : "Gets within-bag distances for each bag." logger . info ( "Getting within-bag distances..." ) if max_K >= X . n_pts . min ( ) : msg = "asked for K = {}, but there's a bag with only {} points" raise ValueError ( msg . format ( max_K , X . n_pts . min ( ) ) ) which_Ks = slice ( 1 , None ) if save_all_Ks else Ks indices = plog ( indices , name = "within-bag distances" ) rhos = [ None ] * len ( X ) for i , ( idx , bag ) in enumerate ( zip ( indices , X ) ) : r = np . sqrt ( idx . nn_index ( bag , max_K + 1 ) [ 1 ] [ : , which_Ks ] ) np . maximum ( min_dist , r , out = r ) rhos [ i ] = r return rhos
6899	def parallel_periodicfeatures ( pfpkl_list , lcbasedir , outdir , starfeaturesdir = None , fourierorder = 5 , transitparams = ( - 0.01 , 0.1 , 0.1 ) , ebparams = ( - 0.2 , 0.3 , 0.7 , 0.5 ) , pdiff_threshold = 1.0e-4 , sidereal_threshold = 1.0e-4 , sampling_peak_multiplier = 5.0 , sampling_startp = None , sampling_endp = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , sigclip = 10.0 , verbose = False , maxobjects = None , nworkers = NCPUS ) : if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if maxobjects : pfpkl_list = pfpkl_list [ : maxobjects ] LOGINFO ( '%s periodfinding pickles to process' % len ( pfpkl_list ) ) if starfeaturesdir and os . path . exists ( starfeaturesdir ) : starfeatures_list = [ ] LOGINFO ( 'collecting starfeatures pickles...' ) for pfpkl in pfpkl_list : sfpkl1 = os . path . basename ( pfpkl ) . replace ( 'periodfinding' , 'starfeatures' ) sfpkl2 = sfpkl1 . replace ( '.gz' , '' ) sfpath1 = os . path . join ( starfeaturesdir , sfpkl1 ) sfpath2 = os . path . join ( starfeaturesdir , sfpkl2 ) if os . path . exists ( sfpath1 ) : starfeatures_list . append ( sfpkl1 ) elif os . path . exists ( sfpath2 ) : starfeatures_list . append ( sfpkl2 ) else : starfeatures_list . append ( None ) else : starfeatures_list = [ None for x in pfpkl_list ] kwargs = { 'fourierorder' : fourierorder , 'transitparams' : transitparams , 'ebparams' : ebparams , 'pdiff_threshold' : pdiff_threshold , 'sidereal_threshold' : sidereal_threshold , 'sampling_peak_multiplier' : sampling_peak_multiplier , 'sampling_startp' : sampling_startp , 'sampling_endp' : sampling_endp , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'lcformat' : lcformat , 'lcformatdir' : lcformat , 'sigclip' : sigclip , 'verbose' : verbose } tasks = [ ( x , lcbasedir , outdir , y , kwargs ) for ( x , y ) in zip ( pfpkl_list , starfeatures_list ) ] LOGINFO ( 'processing periodfinding pickles...' ) with ProcessPoolExecutor ( max_workers = nworkers ) as executor : resultfutures = executor . map ( _periodicfeatures_worker , tasks ) results = [ x for x in resultfutures ] resdict = { os . path . basename ( x ) : y for ( x , y ) in zip ( pfpkl_list , results ) } return resdict
5386	def _format_task_name ( job_id , task_id , task_attempt ) : docker_name = '%s.%s' % ( job_id , 'task' if task_id is None else task_id ) if task_attempt is not None : docker_name += '.' + str ( task_attempt ) return 'dsub-{}' . format ( _convert_suffix_to_docker_chars ( docker_name ) )
2273	def _win32_read_junction ( path ) : if not jwfs . is_reparse_point ( path ) : raise ValueError ( 'not a junction' ) handle = jwfs . api . CreateFile ( path , 0 , 0 , None , jwfs . api . OPEN_EXISTING , jwfs . api . FILE_FLAG_OPEN_REPARSE_POINT | jwfs . api . FILE_FLAG_BACKUP_SEMANTICS , None ) if handle == jwfs . api . INVALID_HANDLE_VALUE : raise WindowsError ( ) res = jwfs . reparse . DeviceIoControl ( handle , jwfs . api . FSCTL_GET_REPARSE_POINT , None , 10240 ) bytes = jwfs . create_string_buffer ( res ) p_rdb = jwfs . cast ( bytes , jwfs . POINTER ( jwfs . api . REPARSE_DATA_BUFFER ) ) rdb = p_rdb . contents if rdb . tag not in [ 2684354563 , jwfs . api . IO_REPARSE_TAG_SYMLINK ] : raise RuntimeError ( "Expected <2684354563 or 2684354572>, but got %d" % rdb . tag ) jwfs . handle_nonzero_success ( jwfs . api . CloseHandle ( handle ) ) subname = rdb . get_substitute_name ( ) if subname . startswith ( '?\\' ) : subname = subname [ 2 : ] return subname
1275	def tf_retrieve_indices ( self , indices ) : states = dict ( ) for name in sorted ( self . states_memory ) : states [ name ] = tf . gather ( params = self . states_memory [ name ] , indices = indices ) internals = dict ( ) for name in sorted ( self . internals_memory ) : internals [ name ] = tf . gather ( params = self . internals_memory [ name ] , indices = indices ) actions = dict ( ) for name in sorted ( self . actions_memory ) : actions [ name ] = tf . gather ( params = self . actions_memory [ name ] , indices = indices ) terminal = tf . gather ( params = self . terminal_memory , indices = indices ) reward = tf . gather ( params = self . reward_memory , indices = indices ) if self . include_next_states : assert util . rank ( indices ) == 1 next_indices = ( indices + 1 ) % self . capacity next_states = dict ( ) for name in sorted ( self . states_memory ) : next_states [ name ] = tf . gather ( params = self . states_memory [ name ] , indices = next_indices ) next_internals = dict ( ) for name in sorted ( self . internals_memory ) : next_internals [ name ] = tf . gather ( params = self . internals_memory [ name ] , indices = next_indices ) return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals ) else : return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
9159	def delete_license_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] posted_uids = [ x [ 'uid' ] for x in request . json . get ( 'licensors' , [ ] ) ] with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : remove_license_requests ( cursor , uuid_ , posted_uids ) resp = request . response resp . status_int = 200 return resp
10940	def calc_J ( self ) : del self . J self . J = np . zeros ( [ self . param_vals . size , self . data . size ] ) dp = np . zeros_like ( self . param_vals ) f0 = self . model . copy ( ) for a in range ( self . param_vals . size ) : dp *= 0 dp [ a ] = self . dl [ a ] f1 = self . func ( self . param_vals + dp , * self . func_args , ** self . func_kwargs ) grad_func = ( f1 - f0 ) / dp [ a ] self . J [ a ] = - grad_func
11972	def _convert ( ip , notation , inotation , _check , _isnm ) : inotation_orig = inotation notation_orig = notation inotation = _get_notation ( inotation ) notation = _get_notation ( notation ) if inotation is None : raise ValueError ( '_convert: unknown input notation: "%s"' % inotation_orig ) if notation is None : raise ValueError ( '_convert: unknown output notation: "%s"' % notation_orig ) docheck = _check or False if inotation == IP_UNKNOWN : inotation = _detect ( ip , _isnm ) if inotation == IP_UNKNOWN : raise ValueError ( '_convert: unable to guess input notation or invalid value' ) if _check is None : docheck = True if _isnm : docheck = False dec = 0 if inotation == IP_DOT : dec = _dot_to_dec ( ip , docheck ) elif inotation == IP_HEX : dec = _hex_to_dec ( ip , docheck ) elif inotation == IP_BIN : dec = _bin_to_dec ( ip , docheck ) elif inotation == IP_OCT : dec = _oct_to_dec ( ip , docheck ) elif inotation == IP_DEC : dec = _dec_to_dec_long ( ip , docheck ) elif _isnm and inotation == NM_BITS : dec = _bits_to_dec ( ip , docheck ) elif _isnm and inotation == NM_WILDCARD : dec = _wildcard_to_dec ( ip , docheck ) else : raise ValueError ( '_convert: unknown IP/netmask notation: "%s"' % inotation_orig ) if _isnm and dec not in _NETMASKS_VALUES : raise ValueError ( '_convert: invalid netmask: "%s"' % ip ) if notation == IP_DOT : return _dec_to_dot ( dec ) elif notation == IP_HEX : return _dec_to_hex ( dec ) elif notation == IP_BIN : return _dec_to_bin ( dec ) elif notation == IP_OCT : return _dec_to_oct ( dec ) elif notation == IP_DEC : return _dec_to_dec_str ( dec ) elif _isnm and notation == NM_BITS : return _dec_to_bits ( dec ) elif _isnm and notation == NM_WILDCARD : return _dec_to_wildcard ( dec ) else : raise ValueError ( 'convert: unknown notation: "%s"' % notation_orig )
1158	def release ( self ) : if self . __owner != _get_ident ( ) : raise RuntimeError ( "cannot release un-acquired lock" ) self . __count = count = self . __count - 1 if not count : self . __owner = None self . __block . release ( ) if __debug__ : self . _note ( "%s.release(): final release" , self ) else : if __debug__ : self . _note ( "%s.release(): non-final release" , self )
12117	def ndist ( data , Xs ) : sigma = np . sqrt ( np . var ( data ) ) center = np . average ( data ) curve = mlab . normpdf ( Xs , center , sigma ) curve *= len ( data ) * HIST_RESOLUTION return curve
9834	def parse ( self , DXfield ) : self . DXfield = DXfield self . currentobject = None self . objects = [ ] self . tokens = [ ] with open ( self . filename , 'r' ) as self . dxfile : self . use_parser ( 'general' ) for o in self . objects : if o . type == 'field' : DXfield . id = o . id continue c = o . initialize ( ) self . DXfield . add ( c . component , c ) del self . currentobject , self . objects
9000	def unique ( iterables ) : included_elements = set ( ) def included ( element ) : result = element in included_elements included_elements . add ( element ) return result return [ element for elements in iterables for element in elements if not included ( element ) ]
9154	def bezier ( self , points ) : coordinates = pgmagick . CoordinateList ( ) for point in points : x , y = float ( point [ 0 ] ) , float ( point [ 1 ] ) coordinates . append ( pgmagick . Coordinate ( x , y ) ) self . drawer . append ( pgmagick . DrawableBezier ( coordinates ) )
11072	def _to_primary_key ( self , value ) : if value is None : return None if isinstance ( value , self . base_class ) : if not value . _is_loaded : raise exceptions . DatabaseError ( 'Record must be loaded.' ) return value . _primary_key return self . base_class . _to_primary_key ( value )
7378	def process_keys ( func ) : @ wraps ( func ) def decorated ( self , k , * args ) : if not isinstance ( k , str ) : msg = "%s: key must be a string" % self . __class__ . __name__ raise ValueError ( msg ) if not k . startswith ( self . prefix ) : k = self . prefix + k return func ( self , k , * args ) return decorated
5660	def print_coords ( rows , prefix = '' ) : lat = [ row [ 'lat' ] for row in rows ] lon = [ row [ 'lon' ] for row in rows ] print ( 'COORDS' + '-' * 5 ) print ( "%slat, %slon = %r, %r" % ( prefix , prefix , lat , lon ) ) print ( '-' * 5 )
8161	def next_event ( block = False , timeout = None ) : try : return channel . listen ( block = block , timeout = timeout ) . next ( ) [ 'data' ] except StopIteration : return None
13775	def includeme ( config ) : settings = config . get_settings ( ) should_create = asbool ( settings . get ( 'baka_model.should_create_all' , False ) ) should_drop = asbool ( settings . get ( 'baka_model.should_drop_all' , False ) ) config . add_settings ( { "retry.attempts" : 3 , "tm.activate_hook" : tm_activate_hook , "tm.annotate_user" : False , } ) config . include ( 'pyramid_retry' ) config . include ( 'pyramid_tm' ) engine = get_engine ( settings ) session_factory = get_session_factory ( engine ) config . registry [ 'db_session_factory' ] = session_factory config . add_request_method ( lambda r : get_tm_session ( session_factory , r . tm ) , 'db' , reify = True ) config . include ( '.service' ) config . action ( None , bind_engine , ( engine , ) , { 'should_create' : should_create , 'should_drop' : should_drop } , order = 10 )
8773	def get_lswitch_ids_for_network ( self , context , network_id ) : lswitches = self . _lswitches_for_network ( context , network_id ) . results ( ) return [ s [ 'uuid' ] for s in lswitches [ "results" ] ]
3321	def delete ( self , token ) : self . _lock . acquire_write ( ) try : lock = self . _dict . get ( token ) _logger . debug ( "delete {}" . format ( lock_string ( lock ) ) ) if lock is None : return False key = "URL2TOKEN:{}" . format ( lock . get ( "root" ) ) if key in self . _dict : tokList = self . _dict [ key ] if len ( tokList ) > 1 : tokList . remove ( token ) self . _dict [ key ] = tokList else : del self . _dict [ key ] del self . _dict [ token ] self . _flush ( ) finally : self . _lock . release ( ) return True
185	def almost_equals ( self , other , max_distance = 1e-4 , points_per_edge = 8 ) : if self . label != other . label : return False return self . coords_almost_equals ( other , max_distance = max_distance , points_per_edge = points_per_edge )
4705	def write ( self , path ) : with open ( path , "wb" ) as fout : fout . write ( self . m_buf )
4229	def make_formatter ( format_name ) : if "json" in format_name : from json import dumps import datetime def jsonhandler ( obj ) : obj . isoformat ( ) if isinstance ( obj , ( datetime . datetime , datetime . date ) ) else obj if format_name == "prettyjson" : def jsondumps ( data ) : return dumps ( data , default = jsonhandler , indent = 2 , separators = ( ',' , ': ' ) ) else : def jsondumps ( data ) : return dumps ( data , default = jsonhandler ) def jsonify ( data ) : if isinstance ( data , dict ) : print ( jsondumps ( data ) ) elif isinstance ( data , list ) : print ( jsondumps ( [ device . _asdict ( ) for device in data ] ) ) else : print ( dumps ( { 'result' : data } ) ) return jsonify else : def printer ( data ) : if isinstance ( data , dict ) : print ( data ) else : for row in data : print ( row ) return printer
3778	def T_dependent_property_integral ( self , T1 , T2 ) : r Tavg = 0.5 * ( T1 + T2 ) if self . method : if self . test_method_validity ( Tavg , self . method ) : try : return self . calculate_integral ( T1 , T2 , self . method ) except : pass sorted_valid_methods = self . select_valid_methods ( Tavg ) for method in sorted_valid_methods : try : return self . calculate_integral ( T1 , T2 , method ) except : pass return None
2574	def handle_app_update ( self , task_id , future , memo_cbk = False ) : if not self . tasks [ task_id ] [ 'app_fu' ] . done ( ) : logger . error ( "Internal consistency error: app_fu is not done for task {}" . format ( task_id ) ) if not self . tasks [ task_id ] [ 'app_fu' ] == future : logger . error ( "Internal consistency error: callback future is not the app_fu in task structure, for task {}" . format ( task_id ) ) if not memo_cbk : self . memoizer . update_memo ( task_id , self . tasks [ task_id ] , future ) if self . checkpoint_mode == 'task_exit' : self . checkpoint ( tasks = [ task_id ] ) if ( self . tasks [ task_id ] [ 'app_fu' ] and self . tasks [ task_id ] [ 'app_fu' ] . done ( ) and self . tasks [ task_id ] [ 'app_fu' ] . exception ( ) is None and self . tasks [ task_id ] [ 'executor' ] != 'data_manager' and self . tasks [ task_id ] [ 'func_name' ] != '_ftp_stage_in' and self . tasks [ task_id ] [ 'func_name' ] != '_http_stage_in' ) : for dfu in self . tasks [ task_id ] [ 'app_fu' ] . outputs : f = dfu . file_obj if isinstance ( f , File ) and f . is_remote ( ) : self . data_manager . stage_out ( f , self . tasks [ task_id ] [ 'executor' ] ) return
13191	def json_struct_to_xml ( json_obj , root , custom_namespace = None ) : if isinstance ( root , ( str , unicode ) ) : if root . startswith ( '!' ) : root = etree . Element ( '{%s}%s' % ( NS_PROTECTED , root [ 1 : ] ) ) elif root . startswith ( '+' ) : if not custom_namespace : raise Exception ( "JSON fields starts with +, but no custom namespace provided" ) root = etree . Element ( '{%s}%s' % ( custom_namespace , root [ 1 : ] ) ) else : root = etree . Element ( root ) if root . tag in ( 'attachments' , 'grouped_events' , 'media_files' ) : for link in json_obj : root . append ( json_link_to_xml ( link ) ) elif isinstance ( json_obj , ( str , unicode ) ) : root . text = json_obj elif isinstance ( json_obj , ( int , float ) ) : root . text = unicode ( json_obj ) elif isinstance ( json_obj , dict ) : if frozenset ( json_obj . keys ( ) ) == frozenset ( ( 'type' , 'coordinates' ) ) : root . append ( geojson_to_gml ( json_obj ) ) else : for key , val in json_obj . items ( ) : if key == 'url' or key . endswith ( '_url' ) : el = json_link_to_xml ( val , json_link_key_to_xml_rel ( key ) ) else : el = json_struct_to_xml ( val , key , custom_namespace = custom_namespace ) if el is not None : root . append ( el ) elif isinstance ( json_obj , list ) : tag_name = root . tag if tag_name . endswith ( 'ies' ) : tag_name = tag_name [ : - 3 ] + 'y' elif tag_name . endswith ( 's' ) : tag_name = tag_name [ : - 1 ] for val in json_obj : el = json_struct_to_xml ( val , tag_name , custom_namespace = custom_namespace ) if el is not None : root . append ( el ) elif json_obj is None : return None else : raise NotImplementedError return root
2239	def _syspath_modname_to_modpath ( modname , sys_path = None , exclude = None ) : def _isvalid ( modpath , base ) : subdir = dirname ( modpath ) while subdir and subdir != base : if not exists ( join ( subdir , '__init__.py' ) ) : return False subdir = dirname ( subdir ) return True _fname_we = modname . replace ( '.' , os . path . sep ) candidate_fnames = [ _fname_we + '.py' , ] candidate_fnames += [ _fname_we + ext for ext in _platform_pylib_exts ( ) ] if sys_path is None : sys_path = sys . path candidate_dpaths = [ '.' if p == '' else p for p in sys_path ] if exclude : def normalize ( p ) : if sys . platform . startswith ( 'win32' ) : return realpath ( p ) . lower ( ) else : return realpath ( p ) real_exclude = { normalize ( p ) for p in exclude } candidate_dpaths = [ p for p in candidate_dpaths if normalize ( p ) not in real_exclude ] for dpath in candidate_dpaths : modpath = join ( dpath , _fname_we ) if exists ( modpath ) : if isfile ( join ( modpath , '__init__.py' ) ) : if _isvalid ( modpath , dpath ) : return modpath for fname in candidate_fnames : modpath = join ( dpath , fname ) if isfile ( modpath ) : if _isvalid ( modpath , dpath ) : return modpath
10642	def Ra ( L : float , Ts : float , Tf : float , alpha : float , beta : float , nu : float ) -> float : return g * beta * ( Ts - Tinf ) * L ** 3.0 / ( nu * alpha )
8326	def setup ( self , parent = None , previous = None ) : self . parent = parent self . previous = previous self . next = None self . previousSibling = None self . nextSibling = None if self . parent and self . parent . contents : self . previousSibling = self . parent . contents [ - 1 ] self . previousSibling . nextSibling = self
1047	def format_list ( extracted_list ) : list = [ ] for filename , lineno , name , line in extracted_list : item = ' File "%s", line %d, in %s\n' % ( filename , lineno , name ) if line : item = item + ' %s\n' % line . strip ( ) list . append ( item ) return list
9026	def insert_defs ( self , defs ) : if self . _svg [ "defs" ] is None : self . _svg [ "defs" ] = { } for def_ in defs : for key , value in def_ . items ( ) : if key . startswith ( "@" ) : continue if key not in self . _svg [ "defs" ] : self . _svg [ "defs" ] [ key ] = [ ] if not isinstance ( value , list ) : value = [ value ] self . _svg [ "defs" ] [ key ] . extend ( value )
286	def plot_perf_stats ( returns , factor_returns , ax = None ) : if ax is None : ax = plt . gca ( ) bootstrap_values = timeseries . perf_stats_bootstrap ( returns , factor_returns , return_stats = False ) bootstrap_values = bootstrap_values . drop ( 'Kurtosis' , axis = 'columns' ) sns . boxplot ( data = bootstrap_values , orient = 'h' , ax = ax ) return ax
8142	def scale ( self , w = 1.0 , h = 1.0 ) : from types import FloatType w0 , h0 = self . img . size if type ( w ) == FloatType : w = int ( w * w0 ) if type ( h ) == FloatType : h = int ( h * h0 ) self . img = self . img . resize ( ( w , h ) , INTERPOLATION ) self . w = w self . h = h
7098	def destroy ( self ) : marker = self . marker parent = self . parent ( ) if marker : if parent : del parent . markers [ marker . __id__ ] marker . remove ( ) super ( AndroidMapItemBase , self ) . destroy ( )
11411	def record_delete_subfield ( rec , tag , subfield_code , ind1 = ' ' , ind2 = ' ' ) : ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) for field in rec . get ( tag , [ ] ) : if field [ 1 ] == ind1 and field [ 2 ] == ind2 : field [ 0 ] [ : ] = [ subfield for subfield in field [ 0 ] if subfield_code != subfield [ 0 ] ]
4046	def num_tagitems ( self , tag ) : query = "/{t}/{u}/tags/{ta}/items" . format ( u = self . library_id , t = self . library_type , ta = tag ) return self . _totals ( query )
2677	def _install_packages ( path , packages ) : def _filter_blacklist ( package ) : blacklist = [ '-i' , '#' , 'Python==' , 'python-lambda==' ] return all ( package . startswith ( entry ) is False for entry in blacklist ) filtered_packages = filter ( _filter_blacklist , packages ) for package in filtered_packages : if package . startswith ( '-e ' ) : package = package . replace ( '-e ' , '' ) print ( 'Installing {package}' . format ( package = package ) ) subprocess . check_call ( [ sys . executable , '-m' , 'pip' , 'install' , package , '-t' , path , '--ignore-installed' ] ) print ( 'Install directory contents are now: {directory}' . format ( directory = os . listdir ( path ) ) )
4118	def _swapsides ( data ) : N = len ( data ) return np . concatenate ( ( data [ N // 2 + 1 : ] , data [ 0 : N // 2 ] ) )
6940	def _gaussian ( x , amp , loc , std ) : return amp * np . exp ( - ( ( x - loc ) * ( x - loc ) ) / ( 2.0 * std * std ) )
6780	def manifest_filename ( self ) : r = self . local_renderer tp_fn = r . format ( r . env . data_dir + '/manifest.yaml' ) return tp_fn
1181	def group ( self , * args ) : if len ( args ) == 0 : args = ( 0 , ) grouplist = [ ] for group in args : grouplist . append ( self . _get_slice ( self . _get_index ( group ) , None ) ) if len ( grouplist ) == 1 : return grouplist [ 0 ] else : return tuple ( grouplist )
12604	def duplicated ( values : Sequence ) : vals = pd . Series ( values ) return vals [ vals . duplicated ( ) ]
2467	def set_file_license_in_file ( self , doc , lic ) : if self . has_package ( doc ) and self . has_file ( doc ) : if validations . validate_file_lics_in_file ( lic ) : self . file ( doc ) . add_lics ( lic ) return True else : raise SPDXValueError ( 'File::LicenseInFile' ) else : raise OrderError ( 'File::LicenseInFile' )
5792	def _cert_callback ( callback , der_cert , reason ) : if not callback : return callback ( x509 . Certificate . load ( der_cert ) , reason )
11011	def get_collection_endpoint ( cls ) : return cls . Meta . collection_endpoint if cls . Meta . collection_endpoint is not None else cls . __name__ . lower ( ) + "s/"
13851	def is_hidden ( path ) : full_path = os . path . abspath ( path ) name = os . path . basename ( full_path ) def no ( path ) : return False platform_hidden = globals ( ) . get ( 'is_hidden_' + platform . system ( ) , no ) return name . startswith ( '.' ) or platform_hidden ( full_path )
5574	def available_output_formats ( ) : output_formats = [ ] for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : driver_ = v . load ( ) if hasattr ( driver_ , "METADATA" ) and ( driver_ . METADATA [ "mode" ] in [ "w" , "rw" ] ) : output_formats . append ( driver_ . METADATA [ "driver_name" ] ) return output_formats
4323	def contrast ( self , amount = 75 ) : if not is_number ( amount ) or amount < 0 or amount > 100 : raise ValueError ( 'amount must be a number between 0 and 100.' ) effect_args = [ 'contrast' , '{:f}' . format ( amount ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'contrast' ) return self
13508	def create_position ( self , params = { } ) : url = "/2/positions/" body = params data = self . _post_resource ( url , body ) return self . position_from_json ( data [ "position" ] )
13599	def push ( self , k ) : if not self . _first : self . _first = self . _last = node = DLL . Node ( k ) elif self . _first . value == k : return else : try : self . delete ( k ) except KeyError : pass self . _first = node = self . _first . insert_before ( k ) self . _index [ k ] = node self . _size += 1
5917	def check_output ( self , make_ndx_output , message = None , err = None ) : if message is None : message = "" else : message = '\n' + message def format ( output , w = 60 ) : hrule = "====[ GromacsError (diagnostic output) ]" . ljust ( w , "=" ) return hrule + '\n' + str ( output ) + hrule rc = True if self . _is_empty_group ( make_ndx_output ) : warnings . warn ( "Selection produced empty group.{message!s}" . format ( ** vars ( ) ) , category = GromacsValueWarning ) rc = False if self . _has_syntax_error ( make_ndx_output ) : rc = False out_formatted = format ( make_ndx_output ) raise GromacsError ( "make_ndx encountered a Syntax Error, " "%(message)s\noutput:\n%(out_formatted)s" % vars ( ) ) if make_ndx_output . strip ( ) == "" : rc = False out_formatted = format ( err ) raise GromacsError ( "make_ndx produced no output, " "%(message)s\nerror output:\n%(out_formatted)s" % vars ( ) ) return rc
7019	def merge_hatpi_textlc_apertures ( lclist ) : lcaps = { } framekeys = [ ] for lc in lclist : lcd = read_hatpi_textlc ( lc ) for col in lcd [ 'columns' ] : if col . startswith ( 'itf' ) : lcaps [ col ] = lcd thisframekeys = lcd [ 'frk' ] . tolist ( ) framekeys . extend ( thisframekeys ) framekeys = sorted ( list ( set ( framekeys ) ) )
805	def _initEphemerals ( self ) : self . _firstComputeCall = True self . _accuracy = None self . _protoScores = None self . _categoryDistances = None self . _knn = knn_classifier . KNNClassifier ( ** self . knnParams ) for x in ( '_partitions' , '_useAuxiliary' , '_doSphering' , '_scanInfo' , '_protoScores' ) : if not hasattr ( self , x ) : setattr ( self , x , None )
11248	def median ( data ) : ordered = sorted ( data ) length = len ( ordered ) if length % 2 == 0 : return ( ordered [ math . floor ( length / 2 ) - 1 ] + ordered [ math . floor ( length / 2 ) ] ) / 2.0 elif length % 2 != 0 : return ordered [ math . floor ( length / 2 ) ]
13040	def process ( self , nemo ) : self . __nemo__ = nemo for annotation in self . __annotations__ : annotation . target . expanded = frozenset ( self . __getinnerreffs__ ( objectId = annotation . target . objectId , subreference = annotation . target . subreference ) )
8230	def size ( self , w = None , h = None ) : if not w : w = self . _canvas . width if not h : h = self . _canvas . height if not w and not h : return ( self . _canvas . width , self . _canvas . height ) w , h = self . _canvas . set_size ( ( w , h ) ) self . _namespace [ 'WIDTH' ] = w self . _namespace [ 'HEIGHT' ] = h self . WIDTH = w self . HEIGHT = h
8727	def daily_at ( cls , at , target ) : daily = datetime . timedelta ( days = 1 ) when = datetime . datetime . combine ( datetime . date . today ( ) , at ) if when < now ( ) : when += daily return cls . at_time ( cls . _localize ( when ) , daily , target )
1499	def ack ( self , tup ) : if not isinstance ( tup , HeronTuple ) : Log . error ( "Only HeronTuple type is supported in ack()" ) return if self . acking_enabled : ack_tuple = tuple_pb2 . AckTuple ( ) ack_tuple . ackedtuple = int ( tup . id ) tuple_size_in_bytes = 0 for rt in tup . roots : to_add = ack_tuple . roots . add ( ) to_add . CopyFrom ( rt ) tuple_size_in_bytes += rt . ByteSize ( ) super ( BoltInstance , self ) . admit_control_tuple ( ack_tuple , tuple_size_in_bytes , True ) process_latency_ns = ( time . time ( ) - tup . creation_time ) * system_constants . SEC_TO_NS self . pplan_helper . context . invoke_hook_bolt_ack ( tup , process_latency_ns ) self . bolt_metrics . acked_tuple ( tup . stream , tup . component , process_latency_ns )
8563	def delete_loadbalancer ( self , datacenter_id , loadbalancer_id ) : response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) , method = 'DELETE' ) return response
11895	def _clean_up ( paths ) : print ( 'Cleaning up' ) for path in paths : print ( 'Removing %s' % path ) os . unlink ( path )
10816	def is_member ( self , user , with_pending = False ) : m = Membership . get ( self , user ) if m is not None : if with_pending : return True elif m . state == MembershipState . ACTIVE : return True return False
9046	def rsolve ( A , y ) : from numpy_sugar . linalg import rsolve as _rsolve try : beta = _rsolve ( A , y ) except LinAlgError : msg = "Could not converge to solve Ax=y." msg += " Setting x to zero." warnings . warn ( msg , RuntimeWarning ) beta = zeros ( A . shape [ 0 ] ) return beta
2404	def gen_prompt_feats ( self , e_set ) : prompt_toks = nltk . word_tokenize ( e_set . _prompt ) expand_syns = [ ] for word in prompt_toks : synonyms = util_functions . get_wordnet_syns ( word ) expand_syns . append ( synonyms ) expand_syns = list ( chain . from_iterable ( expand_syns ) ) prompt_overlap = [ ] prompt_overlap_prop = [ ] for j in e_set . _tokens : tok_length = len ( j ) if ( tok_length == 0 ) : tok_length = 1 prompt_overlap . append ( len ( [ i for i in j if i in prompt_toks ] ) ) prompt_overlap_prop . append ( prompt_overlap [ len ( prompt_overlap ) - 1 ] / float ( tok_length ) ) expand_overlap = [ ] expand_overlap_prop = [ ] for j in e_set . _tokens : tok_length = len ( j ) if ( tok_length == 0 ) : tok_length = 1 expand_overlap . append ( len ( [ i for i in j if i in expand_syns ] ) ) expand_overlap_prop . append ( expand_overlap [ len ( expand_overlap ) - 1 ] / float ( tok_length ) ) prompt_arr = numpy . array ( ( prompt_overlap , prompt_overlap_prop , expand_overlap , expand_overlap_prop ) ) . transpose ( ) return prompt_arr . copy ( )
11046	def init_logging ( log_level ) : log_level_filter = LogLevelFilterPredicate ( LogLevel . levelWithName ( log_level ) ) log_level_filter . setLogLevelForNamespace ( 'twisted.web.client._HTTP11ClientFactory' , LogLevel . warn ) log_observer = FilteringLogObserver ( textFileLogObserver ( sys . stdout ) , [ log_level_filter ] ) globalLogPublisher . addObserver ( log_observer )
7031	def specwindow_lsp ( times , mags , errs , magsarefluxes = False , startp = None , endp = None , stepsize = 1.0e-4 , autofreq = True , nbestpeaks = 5 , periodepsilon = 0.1 , sigclip = 10.0 , nworkers = None , glspfunc = _glsp_worker_specwindow , verbose = True ) : lspres = pgen_lsp ( times , mags , errs , magsarefluxes = magsarefluxes , startp = startp , endp = endp , autofreq = autofreq , nbestpeaks = nbestpeaks , periodepsilon = periodepsilon , stepsize = stepsize , nworkers = nworkers , sigclip = sigclip , glspfunc = glspfunc , verbose = verbose ) lspres [ 'method' ] = 'win' if lspres [ 'lspvals' ] is not None : lspmax = npnanmax ( lspres [ 'lspvals' ] ) if npisfinite ( lspmax ) : lspres [ 'lspvals' ] = lspres [ 'lspvals' ] / lspmax lspres [ 'nbestlspvals' ] = [ x / lspmax for x in lspres [ 'nbestlspvals' ] ] lspres [ 'bestlspval' ] = lspres [ 'bestlspval' ] / lspmax return lspres
3250	def get_short_version ( self ) : gs_version = self . get_version ( ) match = re . compile ( r'[^\d.]+' ) return match . sub ( '' , gs_version ) . strip ( '.' )
12161	def userFolder ( ) : path = os . path . expanduser ( "~" ) + "/.swhlab/" if not os . path . exists ( path ) : print ( "creating" , path ) os . mkdir ( path ) return os . path . abspath ( path )
13218	def connection_url ( self , name = None ) : return 'postgresql://{user}@{host}:{port}/{dbname}' . format ( ** { k : v for k , v in self . _connect_options ( name ) } )
11959	def _check_nm ( nm , notation ) : _NM_CHECK_FUNCT = { NM_DOT : _dot_to_dec , NM_HEX : _hex_to_dec , NM_BIN : _bin_to_dec , NM_OCT : _oct_to_dec , NM_DEC : _dec_to_dec_long } try : dec = _NM_CHECK_FUNCT [ notation ] ( nm , check = True ) except ValueError : return False if dec in _NETMASKS_VALUES : return True return False
10039	def pick_coda_from_decimal ( decimal ) : decimal = Decimal ( decimal ) __ , digits , exp = decimal . as_tuple ( ) if exp < 0 : return DIGIT_CODAS [ digits [ - 1 ] ] __ , digits , exp = decimal . normalize ( ) . as_tuple ( ) index = bisect_right ( EXP_INDICES , exp ) - 1 if index < 0 : return DIGIT_CODAS [ digits [ - 1 ] ] else : return EXP_CODAS [ EXP_INDICES [ index ] ]
13764	def RegisterMessage ( self , message ) : desc = message . DESCRIPTOR self . _symbols [ desc . full_name ] = message if desc . file . name not in self . _symbols_by_file : self . _symbols_by_file [ desc . file . name ] = { } self . _symbols_by_file [ desc . file . name ] [ desc . full_name ] = message self . pool . AddDescriptor ( desc ) return message
7334	async def _chunked_upload ( self , media , media_size , path = None , media_type = None , media_category = None , chunk_size = 2 ** 20 , ** params ) : if isinstance ( media , bytes ) : media = io . BytesIO ( media ) chunk = media . read ( chunk_size ) is_coro = asyncio . iscoroutine ( chunk ) if is_coro : chunk = await chunk if media_type is None : media_metadata = await utils . get_media_metadata ( chunk , path ) media_type , media_category = media_metadata elif media_category is None : media_category = utils . get_category ( media_type ) response = await self . upload . media . upload . post ( command = "INIT" , total_bytes = media_size , media_type = media_type , media_category = media_category , ** params ) media_id = response [ 'media_id' ] i = 0 while chunk : if is_coro : req = self . upload . media . upload . post ( command = "APPEND" , media_id = media_id , media = chunk , segment_index = i ) chunk , _ = await asyncio . gather ( media . read ( chunk_size ) , req ) else : await self . upload . media . upload . post ( command = "APPEND" , media_id = media_id , media = chunk , segment_index = i ) chunk = media . read ( chunk_size ) i += 1 status = await self . upload . media . upload . post ( command = "FINALIZE" , media_id = media_id ) if 'processing_info' in status : while status [ 'processing_info' ] . get ( 'state' ) != "succeeded" : processing_info = status [ 'processing_info' ] if processing_info . get ( 'state' ) == "failed" : error = processing_info . get ( 'error' , { } ) message = error . get ( 'message' , str ( status ) ) raise exceptions . MediaProcessingError ( data = status , message = message , ** params ) delay = processing_info [ 'check_after_secs' ] await asyncio . sleep ( delay ) status = await self . upload . media . upload . get ( command = "STATUS" , media_id = media_id , ** params ) return response
10176	def agg_iter ( self , lower_limit = None , upper_limit = None ) : lower_limit = lower_limit or self . get_bookmark ( ) . isoformat ( ) upper_limit = upper_limit or ( datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) ) aggregation_data = { } self . agg_query = Search ( using = self . client , index = self . event_index ) . filter ( 'range' , timestamp = { 'gte' : self . _format_range_dt ( lower_limit ) , 'lte' : self . _format_range_dt ( upper_limit ) } ) for modifier in self . query_modifiers : self . agg_query = modifier ( self . agg_query ) hist = self . agg_query . aggs . bucket ( 'histogram' , 'date_histogram' , field = 'timestamp' , interval = self . aggregation_interval ) terms = hist . bucket ( 'terms' , 'terms' , field = self . aggregation_field , size = 0 ) top = terms . metric ( 'top_hit' , 'top_hits' , size = 1 , sort = { 'timestamp' : 'desc' } ) for dst , ( metric , src , opts ) in self . metric_aggregation_fields . items ( ) : terms . metric ( dst , metric , field = src , ** opts ) results = self . agg_query . execute ( ) index_name = None for interval in results . aggregations [ 'histogram' ] . buckets : interval_date = datetime . datetime . strptime ( interval [ 'key_as_string' ] , '%Y-%m-%dT%H:%M:%S' ) for aggregation in interval [ 'terms' ] . buckets : aggregation_data [ 'timestamp' ] = interval_date . isoformat ( ) aggregation_data [ self . aggregation_field ] = aggregation [ 'key' ] aggregation_data [ 'count' ] = aggregation [ 'doc_count' ] if self . metric_aggregation_fields : for f in self . metric_aggregation_fields : aggregation_data [ f ] = aggregation [ f ] [ 'value' ] doc = aggregation . top_hit . hits . hits [ 0 ] [ '_source' ] for destination , source in self . copy_fields . items ( ) : if isinstance ( source , six . string_types ) : aggregation_data [ destination ] = doc [ source ] else : aggregation_data [ destination ] = source ( doc , aggregation_data ) index_name = 'stats-{0}-{1}' . format ( self . event , interval_date . strftime ( self . index_name_suffix ) ) self . indices . add ( index_name ) yield dict ( _id = '{0}-{1}' . format ( aggregation [ 'key' ] , interval_date . strftime ( self . doc_id_suffix ) ) , _index = index_name , _type = self . aggregation_doc_type , _source = aggregation_data ) self . last_index_written = index_name
3665	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : Cplms = [ i ( T ) for i in self . HeatCapacityLiquids ] return mixing_simple ( zs , Cplms ) elif method == LALIBERTE : ws = list ( ws ) ws . pop ( self . index_w ) Cpl = Laliberte_heat_capacity ( T , ws , self . wCASs ) MW = mixing_simple ( zs , self . MWs ) return property_mass_to_molar ( Cpl , MW ) else : raise Exception ( 'Method not valid' )
8146	def levels ( self ) : h = self . img . histogram ( ) r = h [ 0 : 255 ] g = h [ 256 : 511 ] b = h [ 512 : 767 ] a = h [ 768 : 1024 ] return r , g , b , a
1986	def load_stream ( self , key , binary = False ) : value = self . load_value ( key , binary = binary ) yield io . BytesIO ( value ) if binary else io . StringIO ( value )
6885	def main ( ) : import signal signal . signal ( signal . SIGPIPE , signal . SIG_DFL ) import argparse aparser = argparse . ArgumentParser ( description = 'read a HAT LC of any format and output to stdout' ) aparser . add_argument ( 'hatlcfile' , action = 'store' , type = str , help = ( "path to the light curve you want to read and pipe to stdout" ) ) aparser . add_argument ( '--describe' , action = 'store_true' , default = False , help = ( "don't dump the columns, show only object info and LC metadata" ) ) args = aparser . parse_args ( ) filetoread = args . hatlcfile if not os . path . exists ( filetoread ) : LOGERROR ( "file provided: %s doesn't seem to exist" % filetoread ) sys . exit ( 1 ) filename = os . path . basename ( filetoread ) if filename . endswith ( '-hatlc.csv.gz' ) or filename . endswith ( '-csvlc.gz' ) : if args . describe : describe ( read_csvlc ( filename ) ) sys . exit ( 0 ) else : with gzip . open ( filename , 'rb' ) as infd : for line in infd : print ( line . decode ( ) , end = '' ) elif filename . endswith ( '-hatlc.sqlite.gz' ) : lcdict , msg = read_and_filter_sqlitecurve ( filetoread ) describe ( lcdict , offsetwith = '#' ) if args . describe : sys . exit ( 0 ) apertures = sorted ( lcdict [ 'lcapertures' ] . keys ( ) ) for aper in apertures : COLUMNDEFS . update ( { '%s_%s' % ( x , aper ) : COLUMNDEFS [ x ] for x in LC_MAG_COLUMNS } ) COLUMNDEFS . update ( { '%s_%s' % ( x , aper ) : COLUMNDEFS [ x ] for x in LC_ERR_COLUMNS } ) COLUMNDEFS . update ( { '%s_%s' % ( x , aper ) : COLUMNDEFS [ x ] for x in LC_FLAG_COLUMNS } ) formstr = ',' . join ( [ COLUMNDEFS [ x ] [ 1 ] for x in lcdict [ 'columns' ] ] ) ndet = lcdict [ 'objectinfo' ] [ 'ndet' ] for ind in range ( ndet ) : line = [ lcdict [ x ] [ ind ] for x in lcdict [ 'columns' ] ] formline = formstr % tuple ( line ) print ( formline ) else : LOGERROR ( 'unrecognized HATLC file: %s' % filetoread ) sys . exit ( 1 )
1724	def execute ( self , js = None , use_compilation_plan = False ) : try : cache = self . __dict__ [ 'cache' ] except KeyError : cache = self . __dict__ [ 'cache' ] = { } hashkey = hashlib . md5 ( js . encode ( 'utf-8' ) ) . digest ( ) try : compiled = cache [ hashkey ] except KeyError : code = translate_js ( js , '' , use_compilation_plan = use_compilation_plan ) compiled = cache [ hashkey ] = compile ( code , '<EvalJS snippet>' , 'exec' ) exec ( compiled , self . _context )
4559	def next ( self , length ) : return Segment ( self . strip , length , self . offset + self . length )
9468	def conference_speak ( self , call_params ) : path = '/' + self . api_version + '/ConferenceSpeak/' method = 'POST' return self . request ( path , method , call_params )
13471	def apply_changesets ( args , changesets , catalog ) : tmpdir = tempfile . mkdtemp ( ) tmp_patch = join ( tmpdir , "tmp.patch" ) tmp_lcat = join ( tmpdir , "tmp.lcat" ) for node in changesets : remove ( tmp_patch ) copy ( node . mfile [ 'changeset' ] [ 'filename' ] , tmp_patch ) logging . info ( "mv %s %s" % ( catalog , tmp_lcat ) ) shutil . move ( catalog , tmp_lcat ) cmd = args . patch_cmd . replace ( "$in1" , tmp_lcat ) . replace ( "$patch" , tmp_patch ) . replace ( "$out" , catalog ) logging . info ( "Patch: %s" % cmd ) subprocess . check_call ( cmd , shell = True ) shutil . rmtree ( tmpdir , ignore_errors = True )
2226	def _convert_hexstr_base ( hexstr , base ) : r if base is _ALPHABET_16 : return hexstr baselen = len ( base ) x = int ( hexstr , 16 ) if x == 0 : return '0' sign = 1 if x > 0 else - 1 x *= sign digits = [ ] while x : digits . append ( base [ x % baselen ] ) x //= baselen if sign < 0 : digits . append ( '-' ) digits . reverse ( ) newbase_str = '' . join ( digits ) return newbase_str
1955	def empty_platform ( cls , arch ) : platform = cls ( None ) platform . _init_cpu ( arch ) platform . _init_std_fds ( ) return platform
11064	def _ignore_event ( self , message ) : if hasattr ( message , 'subtype' ) and message . subtype in self . ignored_events : return True return False
2279	def retrieve_adjacency_matrix ( graph , order_nodes = None , weight = False ) : if isinstance ( graph , np . ndarray ) : return graph elif isinstance ( graph , nx . DiGraph ) : if order_nodes is None : order_nodes = graph . nodes ( ) if not weight : return np . array ( nx . adjacency_matrix ( graph , order_nodes , weight = None ) . todense ( ) ) else : return np . array ( nx . adjacency_matrix ( graph , order_nodes ) . todense ( ) ) else : raise TypeError ( "Only networkx.DiGraph and np.ndarray (adjacency matrixes) are supported." )
12048	def determineProtocol ( fname ) : f = open ( fname , 'rb' ) raw = f . read ( 5000 ) f . close ( ) protoComment = "unknown" if b"SWHLab4[" in raw : protoComment = raw . split ( b"SWHLab4[" ) [ 1 ] . split ( b"]" , 1 ) [ 0 ] elif b"SWH[" in raw : protoComment = raw . split ( b"SWH[" ) [ 1 ] . split ( b"]" , 1 ) [ 0 ] else : protoComment = "?" if not type ( protoComment ) is str : protoComment = protoComment . decode ( "utf-8" ) return protoComment
8649	def delete_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'delete' , } endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
4146	def WelchPeriodogram ( data , NFFT = None , sampling = 1. , ** kargs ) : r from pylab import psd spectrum = Spectrum ( data , sampling = 1. ) P = psd ( data , NFFT , Fs = sampling , ** kargs ) spectrum . psd = P [ 0 ] return P , spectrum
6388	def _sb_ends_in_short_syllable ( self , term ) : if not term : return False if len ( term ) == 2 : if term [ - 2 ] in self . _vowels and term [ - 1 ] not in self . _vowels : return True elif len ( term ) >= 3 : if ( term [ - 3 ] not in self . _vowels and term [ - 2 ] in self . _vowels and term [ - 1 ] in self . _codanonvowels ) : return True return False
10936	def update_J ( self ) : self . calc_J ( ) step = np . ceil ( 1e-2 * self . J . shape [ 1 ] ) . astype ( 'int' ) self . JTJ = low_mem_sq ( self . J , step = step ) self . _fresh_JTJ = True self . _J_update_counter = 0 if np . any ( np . isnan ( self . JTJ ) ) : raise FloatingPointError ( 'J, JTJ have nans.' ) self . _exp_err = self . error - self . find_expected_error ( delta_params = 'perfect' )
13305	def foex ( a , b ) : return ( np . sum ( a > b , dtype = float ) / len ( a ) - 0.5 ) * 100
12984	def keywords ( func ) : @ wraps ( func ) def decorator ( * args , ** kwargs ) : idx = 0 if inspect . ismethod ( func ) else 1 if len ( args ) > idx : if isinstance ( args [ idx ] , ( dict , composite ) ) : for key in args [ idx ] : kwargs [ key ] = args [ idx ] [ key ] args = args [ : idx ] return func ( * args , ** kwargs ) return decorator
3203	def delete ( self , store_id , cart_id , line_id ) : self . store_id = store_id self . cart_id = cart_id self . line_id = line_id return self . _mc_client . _delete ( url = self . _build_path ( store_id , 'carts' , cart_id , 'lines' , line_id ) )
6262	def resize ( self , width , height ) : self . width = width self . height = height self . buffer_width , self . buffer_height = glfw . get_framebuffer_size ( self . window ) self . set_default_viewport ( )
567	def __validateExperimentControl ( self , control ) : taskList = control . get ( 'tasks' , None ) if taskList is not None : taskLabelsList = [ ] for task in taskList : validateOpfJsonValue ( task , "opfTaskSchema.json" ) validateOpfJsonValue ( task [ 'taskControl' ] , "opfTaskControlSchema.json" ) taskLabel = task [ 'taskLabel' ] assert isinstance ( taskLabel , types . StringTypes ) , "taskLabel type: %r" % type ( taskLabel ) assert len ( taskLabel ) > 0 , "empty string taskLabel not is allowed" taskLabelsList . append ( taskLabel . lower ( ) ) taskLabelDuplicates = filter ( lambda x : taskLabelsList . count ( x ) > 1 , taskLabelsList ) assert len ( taskLabelDuplicates ) == 0 , "Duplcate task labels are not allowed: %s" % taskLabelDuplicates return
13887	def CreateDirectory ( directory ) : from six . moves . urllib . parse import urlparse directory_url = urlparse ( directory ) if _UrlIsLocal ( directory_url ) : if not os . path . exists ( directory ) : os . makedirs ( directory ) return directory elif directory_url . scheme == 'ftp' : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( directory_url . scheme ) else : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( directory_url . scheme )
655	def _fillInOnTimes ( vector , durations ) : nonzeros = numpy . array ( vector ) . nonzero ( ) [ 0 ] if len ( nonzeros ) == 0 : return if len ( nonzeros ) == 1 : durations [ nonzeros [ 0 ] ] = 1 return prev = nonzeros [ 0 ] onTime = 1 onStartIdx = prev endIdx = nonzeros [ - 1 ] for idx in nonzeros [ 1 : ] : if idx != prev + 1 : durations [ onStartIdx : onStartIdx + onTime ] = range ( 1 , onTime + 1 ) onTime = 1 onStartIdx = idx else : onTime += 1 prev = idx durations [ onStartIdx : onStartIdx + onTime ] = range ( 1 , onTime + 1 )
10518	def setmin ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) object_handle . AXValue = 0 return 1
1533	def get_scheduler_location ( self , topologyName , callback = None ) : if callback : self . scheduler_location_watchers [ topologyName ] . append ( callback ) else : scheduler_location_path = self . get_scheduler_location_path ( topologyName ) with open ( scheduler_location_path ) as f : data = f . read ( ) scheduler_location = SchedulerLocation ( ) scheduler_location . ParseFromString ( data ) return scheduler_location
7615	def get_datetime ( self , timestamp : str , unix = True ) : time = datetime . strptime ( timestamp , '%Y%m%dT%H%M%S.%fZ' ) if unix : return int ( time . timestamp ( ) ) else : return time
6336	def dist_abs ( self , src , tar , * args , ** kwargs ) : return self . dist ( src , tar , * args , ** kwargs )
7166	def load_entity ( self , name , file_name , reload_cache = False ) : Entity . verify_name ( name ) self . entities . load ( Entity . wrap_name ( name ) , file_name , reload_cache ) with open ( file_name ) as f : self . padaos . add_entity ( name , f . read ( ) . split ( '\n' ) ) self . must_train = True
12846	def generate ( request ) : models . DataItem . create ( content = '' . join ( random . choice ( string . ascii_uppercase + string . digits ) for _ in range ( 20 ) ) ) return muffin . HTTPFound ( '/' )
6870	def get_snr_of_dip ( times , mags , modeltimes , modelmags , atol_normalization = 1e-8 , indsforrms = None , magsarefluxes = False , verbose = True , transitdepth = None , npoints_in_transit = None ) : if magsarefluxes : if not np . isclose ( np . nanmedian ( modelmags ) , 1 , atol = atol_normalization ) : raise AssertionError ( 'snr calculation assumes modelmags are ' 'median-normalized' ) else : raise NotImplementedError ( 'need to implement a method for identifying in-transit points when' 'mags are mags, and not fluxes' ) if not transitdepth : transitdepth = np . abs ( np . max ( modelmags ) - np . min ( modelmags ) ) if not len ( mags ) == len ( modelmags ) : from scipy . interpolate import interp1d fn = interp1d ( modeltimes , modelmags , kind = 'cubic' , bounds_error = True , fill_value = np . nan ) modelmags = fn ( times ) if verbose : LOGINFO ( 'interpolated model timeseries onto the data timeseries' ) subtractedmags = mags - modelmags if isinstance ( indsforrms , np . ndarray ) : subtractedrms = np . std ( subtractedmags [ indsforrms ] ) if verbose : LOGINFO ( 'using selected points to measure RMS' ) else : subtractedrms = np . std ( subtractedmags ) if verbose : LOGINFO ( 'using all points to measure RMS' ) def _get_npoints_in_transit ( modelmags ) : if np . nanmedian ( modelmags ) == 1 : return len ( modelmags [ ( modelmags != 1 ) ] ) else : raise NotImplementedError if not npoints_in_transit : npoints_in_transit = _get_npoints_in_transit ( modelmags ) snr = np . sqrt ( npoints_in_transit ) * transitdepth / subtractedrms if verbose : LOGINFO ( '\npoints in transit: {:d}' . format ( npoints_in_transit ) + '\ndepth: {:.2e}' . format ( transitdepth ) + '\nrms in residual: {:.2e}' . format ( subtractedrms ) + '\n\t SNR: {:.2e}' . format ( snr ) ) return snr , transitdepth , subtractedrms
12889	def handle_set ( self , item , value ) : doc = yield from self . call ( 'SET/{}' . format ( item ) , dict ( value = value ) ) if doc is None : return None return doc . status == 'FS_OK'
13347	def cmd ( ) : if platform == 'win' : return [ 'cmd.exe' , '/K' ] elif platform == 'linux' : ppid = os . getppid ( ) ppid_cmdline_file = '/proc/{0}/cmdline' . format ( ppid ) try : with open ( ppid_cmdline_file ) as f : cmd = f . read ( ) if cmd . endswith ( '\x00' ) : cmd = cmd [ : - 1 ] cmd = cmd . split ( '\x00' ) return cmd + [ binpath ( 'subshell.sh' ) ] except : cmd = 'bash' else : cmd = 'bash' return [ cmd , binpath ( 'subshell.sh' ) ]
4867	def to_representation ( self , instance ) : updated_course = copy . deepcopy ( instance ) enterprise_customer_catalog = self . context [ 'enterprise_customer_catalog' ] updated_course [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_enrollment_url ( updated_course [ 'key' ] ) for course_run in updated_course [ 'course_runs' ] : course_run [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_run_enrollment_url ( course_run [ 'key' ] ) return updated_course
2915	def cancel ( self ) : if self . _is_finished ( ) : for child in self . children : child . cancel ( ) return self . _set_state ( self . CANCELLED ) self . _drop_children ( ) self . task_spec . _on_cancel ( self )
2651	def monitor_wrapper ( f , task_id , monitoring_hub_url , run_id , sleep_dur ) : def wrapped ( * args , ** kwargs ) : p = Process ( target = monitor , args = ( os . getpid ( ) , task_id , monitoring_hub_url , run_id , sleep_dur ) ) p . start ( ) try : return f ( * args , ** kwargs ) finally : p . terminate ( ) p . join ( ) return wrapped
4766	def is_same_as ( self , other ) : if self . val is not other : self . _err ( 'Expected <%s> to be identical to <%s>, but was not.' % ( self . val , other ) ) return self
5095	def get_map_image ( url , dest_path = None ) : image = requests . get ( url , stream = True , timeout = 10 ) if dest_path : image_url = url . rsplit ( '/' , 2 ) [ 1 ] + '-' + url . rsplit ( '/' , 1 ) [ 1 ] image_filename = image_url . split ( '?' ) [ 0 ] dest = os . path . join ( dest_path , image_filename ) image . raise_for_status ( ) with open ( dest , 'wb' ) as data : image . raw . decode_content = True shutil . copyfileobj ( image . raw , data ) return image . raw
3533	def gosquared ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return GoSquaredNode ( )
8624	def add_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_post_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise UserJobsNotAddedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
8394	def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] if not argv or argv [ 0 ] == "help" : show_help ( ) return 0 elif argv [ 0 ] == "check" : return check_main ( argv [ 1 : ] ) elif argv [ 0 ] == "list" : return list_main ( argv [ 1 : ] ) elif argv [ 0 ] == "write" : return write_main ( argv [ 1 : ] ) else : print ( u"Don't understand {!r}" . format ( " " . join ( argv ) ) ) show_help ( ) return 1
4064	def add_tags ( self , item , * tags ) : try : assert item [ "data" ] [ "tags" ] except AssertionError : item [ "data" ] [ "tags" ] = list ( ) for tag in tags : item [ "data" ] [ "tags" ] . append ( { "tag" : "%s" % tag } ) assert self . check_items ( [ item ] ) return self . update_item ( item )
10029	def add_arguments ( parser ) : parser . add_argument ( '-o' , '--old-environment' , help = 'Old environment name' , required = True ) parser . add_argument ( '-n' , '--new-environment' , help = 'New environment name' , required = True )
5620	def get_best_zoom_level ( input_file , tile_pyramid_type ) : tile_pyramid = BufferedTilePyramid ( tile_pyramid_type ) with rasterio . open ( input_file , "r" ) as src : xmin , ymin , xmax , ymax = reproject_geometry ( segmentize_geometry ( box ( src . bounds . left , src . bounds . bottom , src . bounds . right , src . bounds . top ) , get_segmentize_value ( input_file , tile_pyramid ) ) , src_crs = src . crs , dst_crs = tile_pyramid . crs ) . bounds x_dif = xmax - xmin y_dif = ymax - ymin size = float ( src . width + src . height ) avg_resolution = ( ( x_dif / float ( src . width ) ) * ( float ( src . width ) / size ) + ( y_dif / float ( src . height ) ) * ( float ( src . height ) / size ) ) for zoom in range ( 0 , 40 ) : if tile_pyramid . pixel_x_size ( zoom ) <= avg_resolution : return zoom - 1
1692	def CheckCompletedBlocks ( self , filename , error ) : for obj in self . stack : if isinstance ( obj , _ClassInfo ) : error ( filename , obj . starting_linenum , 'build/class' , 5 , 'Failed to find complete declaration of class %s' % obj . name ) elif isinstance ( obj , _NamespaceInfo ) : error ( filename , obj . starting_linenum , 'build/namespaces' , 5 , 'Failed to find complete declaration of namespace %s' % obj . name )
1855	def BSF ( cpu , dest , src ) : value = src . read ( ) flag = Operators . EXTRACT ( value , 0 , 1 ) == 1 res = 0 for pos in range ( 1 , src . size ) : res = Operators . ITEBV ( dest . size , flag , res , pos ) flag = Operators . OR ( flag , Operators . EXTRACT ( value , pos , 1 ) == 1 ) cpu . ZF = value == 0 dest . write ( Operators . ITEBV ( dest . size , cpu . ZF , dest . read ( ) , res ) )
12700	def get_i_name ( self , num , is_oai = None ) : if num not in ( 1 , 2 ) : raise ValueError ( "`num` parameter have to be 1 or 2!" ) if is_oai is None : is_oai = self . oai_marc i_name = "ind" if not is_oai else "i" return i_name + str ( num )
4157	def arma_estimate ( X , P , Q , lag ) : R = CORRELATION ( X , maxlags = lag , norm = 'unbiased' ) R0 = R [ 0 ] MPQ = lag - Q + P N = len ( X ) Y = np . zeros ( N - P , dtype = complex ) for K in range ( 0 , MPQ ) : KPQ = K + Q - P + 1 if KPQ < 0 : Y [ K ] = R [ - KPQ ] . conjugate ( ) if KPQ == 0 : Y [ K ] = R0 if KPQ > 0 : Y [ K ] = R [ KPQ ] Y . resize ( lag ) if P <= 4 : res = arcovar_marple ( Y . copy ( ) , P ) ar_params = res [ 0 ] else : res = arcovar ( Y . copy ( ) , P ) ar_params = res [ 0 ] Y . resize ( N - P ) for k in range ( P , N ) : SUM = X [ k ] for j in range ( 0 , P ) : SUM = SUM + ar_params [ j ] * X [ k - j - 1 ] Y [ k - P ] = SUM ma_params , rho = ma ( Y , Q , 2 * Q ) return ar_params , ma_params , rho
7573	def progressbar ( njobs , finished , msg = "" , spacer = " " ) : if njobs : progress = 100 * ( finished / float ( njobs ) ) else : progress = 100 hashes = '#' * int ( progress / 5. ) nohash = ' ' * int ( 20 - len ( hashes ) ) if not ipyrad . __interactive__ : msg = msg . rsplit ( "|" , 2 ) [ 0 ] args = [ spacer , hashes + nohash , int ( progress ) , msg ] print ( "\r{}[{}] {:>3}% {} " . format ( * args ) , end = "" ) sys . stdout . flush ( )
12561	def create_rois_mask ( roislist , filelist ) : roifiles = [ ] for roi in roislist : try : roi_file = search_list ( roi , filelist ) [ 0 ] except Exception as exc : raise Exception ( 'Error creating list of roi files. \n {}' . format ( str ( exc ) ) ) else : roifiles . append ( roi_file ) return binarise ( roifiles )
6466	def color ( self , index ) : if self . colors == 16 : if index >= 8 : return self . csi ( 'bold' ) + self . csi ( 'setaf' , index - 8 ) else : return self . csi ( 'sgr0' ) + self . csi ( 'setaf' , index ) else : return self . csi ( 'setaf' , index )
4996	def default_content_filter ( sender , instance , ** kwargs ) : if kwargs [ 'created' ] and not instance . content_filter : instance . content_filter = get_default_catalog_content_filter ( ) instance . save ( )
6960	def read_model_table ( modelfile ) : infd = gzip . open ( modelfile ) model = np . genfromtxt ( infd , names = True ) infd . close ( ) return model
882	def activateCells ( self , activeColumns , learn = True ) : prevActiveCells = self . activeCells prevWinnerCells = self . winnerCells self . activeCells = [ ] self . winnerCells = [ ] segToCol = lambda segment : int ( segment . cell / self . cellsPerColumn ) identity = lambda x : x for columnData in groupby2 ( activeColumns , identity , self . activeSegments , segToCol , self . matchingSegments , segToCol ) : ( column , activeColumns , columnActiveSegments , columnMatchingSegments ) = columnData if activeColumns is not None : if columnActiveSegments is not None : cellsToAdd = self . activatePredictedColumn ( column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells , learn ) self . activeCells += cellsToAdd self . winnerCells += cellsToAdd else : ( cellsToAdd , winnerCell ) = self . burstColumn ( column , columnMatchingSegments , prevActiveCells , prevWinnerCells , learn ) self . activeCells += cellsToAdd self . winnerCells . append ( winnerCell ) else : if learn : self . punishPredictedColumn ( column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells )
5545	def pyramid ( input_raster , output_dir , pyramid_type = None , output_format = None , resampling_method = None , scale_method = None , zoom = None , bounds = None , overwrite = False , debug = False ) : bounds = bounds if bounds else None options = dict ( pyramid_type = pyramid_type , scale_method = scale_method , output_format = output_format , resampling = resampling_method , zoom = zoom , bounds = bounds , overwrite = overwrite ) raster2pyramid ( input_raster , output_dir , options )
13795	def handle_reduce ( self , reduce_function_names , mapped_docs ) : reduce_functions = [ ] for reduce_function_name in reduce_function_names : try : reduce_function = get_function ( reduce_function_name ) if getattr ( reduce_function , 'view_decorated' , None ) : reduce_function = reduce_function ( self . log ) reduce_functions . append ( reduce_function ) except Exception , exc : self . log ( repr ( exc ) ) reduce_functions . append ( lambda * args , ** kwargs : None ) keys , values = zip ( ( key , value ) for ( ( key , doc_id ) , value ) in mapped_docs ) results = [ ] for reduce_function in reduce_functions : try : results . append ( reduce_function ( keys , values , rereduce = False ) ) except Exception , exc : self . log ( repr ( exc ) ) results . append ( None ) return [ True , results ]
4318	def _stat_call ( filepath ) : validate_input_file ( filepath ) args = [ 'sox' , filepath , '-n' , 'stat' ] _ , _ , stat_output = sox ( args ) return stat_output
2400	def get_good_pos_ngrams ( self ) : if ( os . path . isfile ( NGRAM_PATH ) ) : good_pos_ngrams = pickle . load ( open ( NGRAM_PATH , 'rb' ) ) elif os . path . isfile ( ESSAY_CORPUS_PATH ) : essay_corpus = open ( ESSAY_CORPUS_PATH ) . read ( ) essay_corpus = util_functions . sub_chars ( essay_corpus ) good_pos_ngrams = util_functions . regenerate_good_tokens ( essay_corpus ) pickle . dump ( good_pos_ngrams , open ( NGRAM_PATH , 'wb' ) ) else : good_pos_ngrams = [ 'NN PRP' , 'NN PRP .' , 'NN PRP . DT' , 'PRP .' , 'PRP . DT' , 'PRP . DT NNP' , '. DT' , '. DT NNP' , '. DT NNP NNP' , 'DT NNP' , 'DT NNP NNP' , 'DT NNP NNP NNP' , 'NNP NNP' , 'NNP NNP NNP' , 'NNP NNP NNP NNP' , 'NNP NNP NNP .' , 'NNP NNP .' , 'NNP NNP . TO' , 'NNP .' , 'NNP . TO' , 'NNP . TO NNP' , '. TO' , '. TO NNP' , '. TO NNP NNP' , 'TO NNP' , 'TO NNP NNP' ] return set ( good_pos_ngrams )
10589	def validate_account_names ( self , names ) : for name in names : if self . get_account ( name ) is None : raise ValueError ( "The account '{}' does not exist in the" " general ledger structure." . format ( name ) )
2972	def from_dict ( name , values ) : count = 1 count_value = values . get ( 'count' , 1 ) if isinstance ( count_value , int ) : count = max ( count_value , 1 ) def with_index ( name , idx ) : if name and idx : return '%s_%d' % ( name , idx ) return name def get_instance ( n , idx = None ) : return BlockadeContainerConfig ( with_index ( n , idx ) , values [ 'image' ] , command = values . get ( 'command' ) , links = values . get ( 'links' ) , volumes = values . get ( 'volumes' ) , publish_ports = values . get ( 'ports' ) , expose_ports = values . get ( 'expose' ) , environment = values . get ( 'environment' ) , hostname = values . get ( 'hostname' ) , dns = values . get ( 'dns' ) , start_delay = values . get ( 'start_delay' , 0 ) , neutral = values . get ( 'neutral' , False ) , holy = values . get ( 'holy' , False ) , container_name = with_index ( values . get ( 'container_name' ) , idx ) , cap_add = values . get ( 'cap_add' ) ) if count == 1 : yield get_instance ( name ) else : for idx in range ( 1 , count + 1 ) : yield get_instance ( name , idx )
13823	def end_timing ( self ) : if self . _callback != None : elapsed = time . clock ( ) * 1000 - self . _start self . _callback . end_timing ( self . _counter , elapsed )
11572	def output_entire_buffer ( self ) : green = 0 red = 0 for row in range ( 0 , 8 ) : for col in range ( 0 , 8 ) : if self . display_buffer [ row ] [ col ] == self . LED_GREEN : green |= 1 << col elif self . display_buffer [ row ] [ col ] == self . LED_RED : red |= 1 << col elif self . display_buffer [ row ] [ col ] == self . LED_YELLOW : green |= 1 << col red |= 1 << col elif self . display_buffer [ row ] [ col ] == self . LED_OFF : green &= ~ ( 1 << col ) red &= ~ ( 1 << col ) self . firmata . i2c_write ( 0x70 , row * 2 , 0 , green ) self . firmata . i2c_write ( 0x70 , row * 2 + 1 , 0 , red )
10490	def popUpItem ( self , * args ) : self . Press ( ) time . sleep ( .5 ) return self . _menuItem ( self , * args )
845	def _calcDistance ( self , inputPattern , distanceNorm = None ) : if distanceNorm is None : distanceNorm = self . distanceNorm if self . useSparseMemory : if self . _protoSizes is None : self . _protoSizes = self . _Memory . rowSums ( ) overlapsWithProtos = self . _Memory . rightVecSumAtNZ ( inputPattern ) inputPatternSum = inputPattern . sum ( ) if self . distanceMethod == "rawOverlap" : dist = inputPattern . sum ( ) - overlapsWithProtos elif self . distanceMethod == "pctOverlapOfInput" : dist = inputPatternSum - overlapsWithProtos if inputPatternSum > 0 : dist /= inputPatternSum elif self . distanceMethod == "pctOverlapOfProto" : overlapsWithProtos /= self . _protoSizes dist = 1.0 - overlapsWithProtos elif self . distanceMethod == "pctOverlapOfLarger" : maxVal = numpy . maximum ( self . _protoSizes , inputPatternSum ) if maxVal . all ( ) > 0 : overlapsWithProtos /= maxVal dist = 1.0 - overlapsWithProtos elif self . distanceMethod == "norm" : dist = self . _Memory . vecLpDist ( self . distanceNorm , inputPattern ) distMax = dist . max ( ) if distMax > 0 : dist /= distMax else : raise RuntimeError ( "Unimplemented distance method %s" % self . distanceMethod ) else : if self . distanceMethod == "norm" : dist = numpy . power ( numpy . abs ( self . _M - inputPattern ) , self . distanceNorm ) dist = dist . sum ( 1 ) dist = numpy . power ( dist , 1.0 / self . distanceNorm ) dist /= dist . max ( ) else : raise RuntimeError ( "Not implemented yet for dense storage...." ) return dist
9643	def pydevd ( context ) : global pdevd_not_available if pdevd_not_available : return '' try : import pydevd except ImportError : pdevd_not_available = True return '' render = lambda s : template . Template ( s ) . render ( context ) availables = get_variables ( context ) for var in availables : locals ( ) [ var ] = context [ var ] try : pydevd . settrace ( ) except socket . error : pdevd_not_available = True return ''
583	def _addRecordToKNN ( self , record ) : classifier = self . htm_prediction_model . _getAnomalyClassifier ( ) knn = classifier . getSelf ( ) . _knn prototype_idx = classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) category = self . _labelListToCategoryNumber ( record . anomalyLabel ) if record . ROWID in prototype_idx : knn . prototypeSetCategory ( record . ROWID , category ) return pattern = self . _getStateAnomalyVector ( record ) rowID = record . ROWID knn . learn ( pattern , category , rowID = rowID )
11998	def _encode ( self , data , algorithm , key = None ) : if algorithm [ 'type' ] == 'hmac' : return data + self . _hmac_generate ( data , algorithm , key ) elif algorithm [ 'type' ] == 'aes' : return self . _aes_encrypt ( data , algorithm , key ) elif algorithm [ 'type' ] == 'no-serialization' : return data elif algorithm [ 'type' ] == 'json' : return json . dumps ( data ) elif algorithm [ 'type' ] == 'no-compression' : return data elif algorithm [ 'type' ] == 'gzip' : return self . _zlib_compress ( data , algorithm ) else : raise Exception ( 'Algorithm not supported: %s' % algorithm [ 'type' ] )
4363	def _spawn_heartbeat ( self ) : self . spawn ( self . _heartbeat ) self . spawn ( self . _heartbeat_timeout )
12302	def validate ( repo , validator_name = None , filename = None , rulesfiles = None , args = [ ] ) : mgr = plugins_get_mgr ( ) validator_specs = instantiate ( repo , validator_name , filename , rulesfiles ) allresults = [ ] for v in validator_specs : keys = mgr . search ( what = 'validator' , name = v ) [ 'validator' ] for k in keys : validator = mgr . get_by_key ( 'validator' , k ) result = validator . evaluate ( repo , validator_specs [ v ] , args ) allresults . extend ( result ) return allresults
7998	def set_authenticated ( self , me , restart_stream = False ) : with self . lock : self . authenticated = True self . me = me if restart_stream : self . _restart_stream ( ) self . event ( AuthenticatedEvent ( self . me ) )
3048	def _implicit_credentials_from_files ( ) : credentials_filename = _get_environment_variable_file ( ) if not credentials_filename : credentials_filename = _get_well_known_file ( ) if os . path . isfile ( credentials_filename ) : extra_help = ( ' (produced automatically when running' ' "gcloud auth login" command)' ) else : credentials_filename = None else : extra_help = ( ' (pointed to by ' + GOOGLE_APPLICATION_CREDENTIALS + ' environment variable)' ) if not credentials_filename : return SETTINGS . env_name = DEFAULT_ENV_NAME try : return _get_application_default_credential_from_file ( credentials_filename ) except ( ApplicationDefaultCredentialsError , ValueError ) as error : _raise_exception_for_reading_json ( credentials_filename , extra_help , error )
13731	def validate_is_not_none ( config_val , evar ) : if config_val is None : raise ValueError ( "Value for environment variable '{evar_name}' can't " "be empty." . format ( evar_name = evar . name ) ) return config_val
5783	def read_exactly ( self , num_bytes ) : output = b'' remaining = num_bytes while remaining > 0 : output += self . read ( remaining ) remaining = num_bytes - len ( output ) return output
6061	def convolve_mapping_matrix ( self , mapping_matrix ) : return self . convolve_matrix_jit ( mapping_matrix , self . image_frame_indexes , self . image_frame_psfs , self . image_frame_lengths )
1635	def CheckSpacingForFunctionCall ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] fncall = line for pattern in ( r'\bif\s*\((.*)\)\s*{' , r'\bfor\s*\((.*)\)\s*{' , r'\bwhile\s*\((.*)\)\s*[{;]' , r'\bswitch\s*\((.*)\)\s*{' ) : match = Search ( pattern , line ) if match : fncall = match . group ( 1 ) break if ( not Search ( r'\b(if|for|while|switch|return|new|delete|catch|sizeof)\b' , fncall ) and not Search ( r' \([^)]+\)\([^)]*(\)|,$)' , fncall ) and not Search ( r' \([^)]+\)\[[^\]]+\]' , fncall ) ) : if Search ( r'\w\s*\(\s(?!\s*\\$)' , fncall ) : error ( filename , linenum , 'whitespace/parens' , 4 , 'Extra space after ( in function call' ) elif Search ( r'\(\s+(?!(\s*\\)|\()' , fncall ) : error ( filename , linenum , 'whitespace/parens' , 2 , 'Extra space after (' ) if ( Search ( r'\w\s+\(' , fncall ) and not Search ( r'_{0,2}asm_{0,2}\s+_{0,2}volatile_{0,2}\s+\(' , fncall ) and not Search ( r'#\s*define|typedef|using\s+\w+\s*=' , fncall ) and not Search ( r'\w\s+\((\w+::)*\*\w+\)\(' , fncall ) and not Search ( r'\bcase\s+\(' , fncall ) ) : if Search ( r'\boperator_*\b' , line ) : error ( filename , linenum , 'whitespace/parens' , 0 , 'Extra space before ( in function call' ) else : error ( filename , linenum , 'whitespace/parens' , 4 , 'Extra space before ( in function call' ) if Search ( r'[^)]\s+\)\s*[^{\s]' , fncall ) : if Search ( r'^\s+\)' , fncall ) : error ( filename , linenum , 'whitespace/parens' , 2 , 'Closing ) should be moved to the previous line' ) else : error ( filename , linenum , 'whitespace/parens' , 2 , 'Extra space before )' )
1749	def mmap ( self , addr , size , perms , data_init = None , name = None ) : assert addr is None or isinstance ( addr , int ) , 'Address shall be concrete' self . cpu . _publish ( 'will_map_memory' , addr , size , perms , None , None ) if addr is not None : assert addr < self . memory_size , 'Address too big' addr = self . _floor ( addr ) size = self . _ceil ( size ) addr = self . _search ( size , addr ) for i in range ( self . _page ( addr ) , self . _page ( addr + size ) ) : assert i not in self . _page2map , 'Map already used' m = AnonMap ( start = addr , size = size , perms = perms , data_init = data_init , name = name ) self . _add ( m ) logger . debug ( 'New memory map @%x size:%x' , addr , size ) self . cpu . _publish ( 'did_map_memory' , addr , size , perms , None , None , addr ) return addr
11945	def _store ( self , messages , response , * args , ** kwargs ) : contrib_messages = [ ] if self . user . is_authenticated ( ) : if not messages : self . backend . inbox_purge ( self . user ) else : for m in messages : try : self . backend . inbox_store ( [ self . user ] , m ) except MessageTypeNotSupported : contrib_messages . append ( m ) super ( StorageMixin , self ) . _store ( contrib_messages , response , * args , ** kwargs )
9253	def get_string_for_issue ( self , issue ) : encapsulated_title = self . encapsulate_string ( issue [ 'title' ] ) try : title_with_number = u"{0} [\\#{1}]({2})" . format ( encapsulated_title , issue [ "number" ] , issue [ "html_url" ] ) except UnicodeEncodeError : title_with_number = "ERROR ERROR ERROR: #{0} {1}" . format ( issue [ "number" ] , issue [ 'title' ] ) print ( title_with_number , '\n' , issue [ "html_url" ] ) return self . issue_line_with_user ( title_with_number , issue )
10907	def twoslice ( field , center = None , size = 6.0 , cmap = 'bone_r' , vmin = 0 , vmax = 1 , orientation = 'vertical' , figpad = 1.09 , off = 0.01 ) : center = center or [ i // 2 for i in field . shape ] slices = [ ] for i , c in enumerate ( center ) : blank = [ np . s_ [ : ] ] * len ( center ) blank [ i ] = c slices . append ( tuple ( blank ) ) z , y , x = [ float ( i ) for i in field . shape ] w = float ( x + z ) h = float ( y + z ) def show ( field , ax , slicer , transpose = False ) : tmp = field [ slicer ] if not transpose else field [ slicer ] . T ax . imshow ( tmp , cmap = cmap , interpolation = 'nearest' , vmin = vmin , vmax = vmax ) ax . set_xticks ( [ ] ) ax . set_yticks ( [ ] ) ax . grid ( 'off' ) if orientation . startswith ( 'v' ) : log . info ( '{} {} {} {} {} {}' . format ( x , y , z , w , h , x / h ) ) r = x / h q = y / h f = 1 / ( 1 + 3 * off ) fig = pl . figure ( figsize = ( size * r , size * f ) ) ax1 = fig . add_axes ( ( off , f * ( 1 - q ) + 2 * off , f , f * q ) ) ax2 = fig . add_axes ( ( off , off , f , f * ( 1 - q ) ) ) show ( field , ax1 , slices [ 0 ] ) show ( field , ax2 , slices [ 1 ] ) else : r = y / w q = x / w f = 1 / ( 1 + 3 * off ) fig = pl . figure ( figsize = ( size * f , size * r ) ) ax1 = fig . add_axes ( ( off , off , f * q , f ) ) ax2 = fig . add_axes ( ( 2 * off + f * q , off , f * ( 1 - q ) , f ) ) show ( field , ax1 , slices [ 0 ] ) show ( field , ax2 , slices [ 2 ] , transpose = True ) return fig , ax1 , ax2
7348	async def get_access_token ( consumer_key , consumer_secret , oauth_token , oauth_token_secret , oauth_verifier , ** kwargs ) : client = BasePeonyClient ( consumer_key = consumer_key , consumer_secret = consumer_secret , access_token = oauth_token , access_token_secret = oauth_token_secret , api_version = "" , suffix = "" ) response = await client . api . oauth . access_token . get ( _suffix = "" , oauth_verifier = oauth_verifier ) return parse_token ( response )
5267	def snakecase ( string ) : string = re . sub ( r"[\-\.\s]" , '_' , str ( string ) ) if not string : return string return lowercase ( string [ 0 ] ) + re . sub ( r"[A-Z]" , lambda matched : '_' + lowercase ( matched . group ( 0 ) ) , string [ 1 : ] )
12782	def set_topic ( self , topic ) : if not topic : topic = '' result = self . _connection . put ( "room/%s" % self . id , { "room" : { "topic" : topic } } ) if result [ "success" ] : self . _load ( ) return result [ "success" ]
7099	def child_added ( self , child ) : if child . widget : self . parent ( ) . init_info_window_adapter ( ) super ( AndroidMapMarker , self ) . child_added ( child )
9480	def _arg_parser ( ) : description = "Converts a completezip to a litezip" parser = argparse . ArgumentParser ( description = description ) verbose_group = parser . add_mutually_exclusive_group ( ) verbose_group . add_argument ( '-v' , '--verbose' , action = 'store_true' , dest = 'verbose' , default = None , help = "increase verbosity" ) verbose_group . add_argument ( '-q' , '--quiet' , action = 'store_false' , dest = 'verbose' , default = None , help = "print nothing to stdout or stderr" ) parser . add_argument ( 'location' , help = "Location of the unpacked litezip" ) return parser
9927	def get_user ( self , user_id ) : try : return get_user_model ( ) . objects . get ( id = user_id ) except get_user_model ( ) . DoesNotExist : return None
9590	def switch_to_window ( self , window_name ) : data = { 'name' : window_name } self . _execute ( Command . SWITCH_TO_WINDOW , data )
3498	def find_carbon_sources ( model ) : try : model . slim_optimize ( error_value = None ) except OptimizationError : return [ ] reactions = model . reactions . get_by_any ( list ( model . medium ) ) reactions_fluxes = [ ( rxn , total_components_flux ( rxn . flux , reaction_elements ( rxn ) , consumption = True ) ) for rxn in reactions ] return [ rxn for rxn , c_flux in reactions_fluxes if c_flux > 0 ]
13092	def load_targets ( self ) : ldap_services = [ ] if self . ldap : ldap_services = self . search . get_services ( ports = [ 389 ] , up = True ) self . ldap_strings = [ "ldap://{}" . format ( service . address ) for service in ldap_services ] self . services = self . search . get_services ( tags = [ 'smb_signing_disabled' ] ) self . ips = [ str ( service . address ) for service in self . services ]
11018	def balance ( ctx ) : backend = plugins_registry . get_backends_by_class ( ZebraBackend ) [ 0 ] timesheet_collection = get_timesheet_collection_for_context ( ctx , None ) hours_to_be_pushed = timesheet_collection . get_hours ( pushed = False , ignored = False , unmapped = False ) today = datetime . date . today ( ) user_info = backend . get_user_info ( ) timesheets = backend . get_timesheets ( get_first_dow ( today ) , get_last_dow ( today ) ) total_duration = sum ( [ float ( timesheet [ 'time' ] ) for timesheet in timesheets ] ) vacation = hours_to_days ( user_info [ 'vacation' ] [ 'difference' ] ) vacation_balance = '{} days, {:.2f} hours' . format ( * vacation ) hours_balance = user_info [ 'hours' ] [ 'hours' ] [ 'balance' ] click . echo ( "Hours balance: {}" . format ( signed_number ( hours_balance ) ) ) click . echo ( "Hours balance after push: {}" . format ( signed_number ( hours_balance + hours_to_be_pushed ) ) ) click . echo ( "Hours done this week: {:.2f}" . format ( total_duration ) ) click . echo ( "Vacation left: {}" . format ( vacation_balance ) )
261	def show_perf_attrib_stats ( returns , positions , factor_returns , factor_loadings , transactions = None , pos_in_dollars = True ) : risk_exposures , perf_attrib_data = perf_attrib ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars = pos_in_dollars , ) perf_attrib_stats , risk_exposure_stats = create_perf_attrib_stats ( perf_attrib_data , risk_exposures ) percentage_formatter = '{:.2%}' . format float_formatter = '{:.2f}' . format summary_stats = perf_attrib_stats . loc [ [ 'Annualized Specific Return' , 'Annualized Common Return' , 'Annualized Total Return' , 'Specific Sharpe Ratio' ] ] for col_name in ( 'Annualized Specific Return' , 'Annualized Common Return' , 'Annualized Total Return' , ) : summary_stats [ col_name ] = percentage_formatter ( summary_stats [ col_name ] ) summary_stats [ 'Specific Sharpe Ratio' ] = float_formatter ( summary_stats [ 'Specific Sharpe Ratio' ] ) print_table ( summary_stats , name = 'Summary Statistics' ) print_table ( risk_exposure_stats , name = 'Exposures Summary' , formatters = { 'Average Risk Factor Exposure' : float_formatter , 'Annualized Return' : percentage_formatter , 'Cumulative Return' : percentage_formatter , } , )
13183	def dict_to_row ( cls , observation_data ) : row = [ ] row . append ( observation_data [ 'name' ] ) row . append ( observation_data [ 'date' ] ) row . append ( observation_data [ 'magnitude' ] ) comment_code = observation_data . get ( 'comment_code' , 'na' ) if not comment_code : comment_code = 'na' row . append ( comment_code ) comp1 = observation_data . get ( 'comp1' , 'na' ) if not comp1 : comp1 = 'na' row . append ( comp1 ) comp2 = observation_data . get ( 'comp2' , 'na' ) if not comp2 : comp2 = 'na' row . append ( comp2 ) chart = observation_data . get ( 'chart' , 'na' ) if not chart : chart = 'na' row . append ( chart ) notes = observation_data . get ( 'notes' , 'na' ) if not notes : notes = 'na' row . append ( notes ) return row
12721	def hi_stops ( self , hi_stops ) : _set_params ( self . ode_obj , 'HiStop' , hi_stops , self . ADOF + self . LDOF )
13721	def query ( self , wql ) : try : self . __wql = [ 'wmic' , '-U' , self . args . domain + '\\' + self . args . user + '%' + self . args . password , '//' + self . args . host , '--namespace' , self . args . namespace , '--delimiter' , self . args . delimiter , wql ] self . logger . debug ( "wql: {}" . format ( self . __wql ) ) self . __output = subprocess . check_output ( self . __wql ) self . logger . debug ( "output: {}" . format ( self . __output ) ) self . logger . debug ( "wmi connect succeed." ) self . __wmi_output = self . __output . splitlines ( ) [ 1 : ] self . logger . debug ( "wmi_output: {}" . format ( self . __wmi_output ) ) self . __csv_header = csv . DictReader ( self . __wmi_output , delimiter = '|' ) self . logger . debug ( "csv_header: {}" . format ( self . __csv_header ) ) return list ( self . __csv_header ) except subprocess . CalledProcessError as e : self . unknown ( "Connect by wmi and run wql error: %s" % e )
7100	def on_marker ( self , marker ) : mid , pos = marker self . marker = Marker ( __id__ = mid ) mapview = self . parent ( ) mapview . markers [ mid ] = self self . marker . setTag ( mid ) for w in self . child_widgets ( ) : mapview . init_info_window_adapter ( ) break d = self . declaration if d . show_info : self . set_show_info ( d . show_info ) del self . options
4514	def fillCircle ( self , x0 , y0 , r , color = None ) : md . fill_circle ( self . set , x0 , y0 , r , color )
12596	def _openpyxl_read_xl ( xl_path : str ) : try : wb = load_workbook ( filename = xl_path , read_only = True ) except : raise else : return wb
8280	def _render_closure ( self ) : fillcolor = self . fill strokecolor = self . stroke strokewidth = self . strokewidth def _render ( cairo_ctx ) : transform = self . _call_transform_mode ( self . _transform ) if fillcolor is None and strokecolor is None : return cairo_ctx . set_matrix ( transform ) self . _traverse ( cairo_ctx ) cairo_ctx . set_matrix ( cairo . Matrix ( ) ) if fillcolor is not None and strokecolor is not None : if strokecolor [ 3 ] < 1 : cairo_ctx . push_group ( ) cairo_ctx . set_source_rgba ( * fillcolor ) cairo_ctx . fill_preserve ( ) e = cairo_ctx . stroke_extents ( ) cairo_ctx . set_source_rgba ( * strokecolor ) cairo_ctx . set_operator ( cairo . OPERATOR_SOURCE ) cairo_ctx . set_line_width ( strokewidth ) cairo_ctx . stroke ( ) cairo_ctx . pop_group_to_source ( ) cairo_ctx . paint ( ) else : cairo_ctx . set_source_rgba ( * fillcolor ) cairo_ctx . fill_preserve ( ) cairo_ctx . set_source_rgba ( * strokecolor ) cairo_ctx . set_line_width ( strokewidth ) cairo_ctx . stroke ( ) elif fillcolor is not None : cairo_ctx . set_source_rgba ( * fillcolor ) cairo_ctx . fill ( ) elif strokecolor is not None : cairo_ctx . set_source_rgba ( * strokecolor ) cairo_ctx . set_line_width ( strokewidth ) cairo_ctx . stroke ( ) return _render
9716	async def await_event ( self , event = None , timeout = 30 ) : return await self . _protocol . await_event ( event , timeout = timeout )
8341	def _invert ( h ) : "Cheap function to invert a hash." i = { } for k , v in h . items ( ) : i [ v ] = k return i
3406	def ast2str ( expr , level = 0 , names = None ) : if isinstance ( expr , Expression ) : return ast2str ( expr . body , 0 , names ) if hasattr ( expr , "body" ) else "" elif isinstance ( expr , Name ) : return names . get ( expr . id , expr . id ) if names else expr . id elif isinstance ( expr , BoolOp ) : op = expr . op if isinstance ( op , Or ) : str_exp = " or " . join ( ast2str ( i , level + 1 , names ) for i in expr . values ) elif isinstance ( op , And ) : str_exp = " and " . join ( ast2str ( i , level + 1 , names ) for i in expr . values ) else : raise TypeError ( "unsupported operation " + op . __class__ . __name ) return "(" + str_exp + ")" if level else str_exp elif expr is None : return "" else : raise TypeError ( "unsupported operation " + repr ( expr ) )
1764	def push_int ( self , value , force = False ) : self . STACK -= self . address_bit_size // 8 self . write_int ( self . STACK , value , force = force ) return self . STACK
10694	def hex_to_rgb ( _hex ) : _hex = _hex . strip ( '#' ) n = len ( _hex ) // 3 if len ( _hex ) == 3 : r = int ( _hex [ : n ] * 2 , 16 ) g = int ( _hex [ n : 2 * n ] * 2 , 16 ) b = int ( _hex [ 2 * n : 3 * n ] * 2 , 16 ) else : r = int ( _hex [ : n ] , 16 ) g = int ( _hex [ n : 2 * n ] , 16 ) b = int ( _hex [ 2 * n : 3 * n ] , 16 ) return r , g , b
5484	def execute ( api ) : try : return api . execute ( ) except Exception as exception : now = datetime . now ( ) . strftime ( '%Y-%m-%d %H:%M:%S.%f' ) _print_error ( '%s: Exception %s: %s' % ( now , type ( exception ) . __name__ , str ( exception ) ) ) raise exception
7118	def merge_dicts ( d1 , d2 , _path = None ) : if _path is None : _path = ( ) if isinstance ( d1 , dict ) and isinstance ( d2 , dict ) : for k , v in d2 . items ( ) : if isinstance ( v , MissingValue ) and v . name is None : v . name = '.' . join ( _path + ( k , ) ) if isinstance ( v , DeletedValue ) : d1 . pop ( k , None ) elif k not in d1 : if isinstance ( v , dict ) : d1 [ k ] = merge_dicts ( { } , v , _path + ( k , ) ) else : d1 [ k ] = v else : if isinstance ( d1 [ k ] , dict ) and isinstance ( v , dict ) : d1 [ k ] = merge_dicts ( d1 [ k ] , v , _path + ( k , ) ) elif isinstance ( d1 [ k ] , list ) and isinstance ( v , list ) : d1 [ k ] += v elif isinstance ( d1 [ k ] , MissingValue ) : d1 [ k ] = v elif d1 [ k ] is None : d1 [ k ] = v elif type ( d1 [ k ] ) == type ( v ) : d1 [ k ] = v else : raise TypeError ( 'Refusing to replace a %s with a %s' % ( type ( d1 [ k ] ) , type ( v ) ) ) else : raise TypeError ( 'Cannot merge a %s with a %s' % ( type ( d1 ) , type ( d2 ) ) ) return d1
10498	def tripleClickMouse ( self , coord ) : modFlags = 0 for i in range ( 2 ) : self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags ) self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , clickCount = 3 ) self . _postQueuedEvents ( )
3979	def _get_referenced_services ( specs ) : active_services = set ( ) for app_spec in specs [ 'apps' ] . values ( ) : for service in app_spec [ 'depends' ] [ 'services' ] : active_services . add ( service ) for bundle_spec in specs [ 'bundles' ] . values ( ) : for service in bundle_spec [ 'services' ] : active_services . add ( service ) return active_services
8764	def get_public_net_id ( self ) : for id , net_params in self . strategy . iteritems ( ) : if id == CONF . QUARK . public_net_id : return id return None
10456	def verifycheck ( self , window_name , object_name ) : try : object_handle = self . _get_object_handle ( window_name , object_name , wait_for_object = False ) if object_handle . AXValue == 1 : return 1 except LdtpServerException : pass return 0
10750	def download ( self , bands , download_dir = None , metadata = False ) : super ( GoogleDownloader , self ) . validate_bands ( bands ) pattern = re . compile ( '^[^\s]+_(.+)\.tiff?' , re . I ) image_list = [ ] band_list = [ 'B%i' % ( i , ) if isinstance ( i , int ) else i for i in bands ] if download_dir is None : download_dir = DOWNLOAD_DIR check_create_folder ( join ( download_dir , self . sceneInfo . name ) ) filename = "%s%s" % ( self . sceneInfo . name , self . __remote_file_ext ) downloaded = self . fetch ( self . remote_file_url , download_dir , filename ) try : tar = tarfile . open ( downloaded [ 0 ] , 'r' ) folder_path = join ( download_dir , self . sceneInfo . name ) logger . debug ( 'Starting data extraction in directory ' , folder_path ) tar . extractall ( folder_path ) remove ( downloaded [ 0 ] ) images_path = listdir ( folder_path ) for image_path in images_path : matched = pattern . match ( image_path ) file_path = join ( folder_path , image_path ) if matched and matched . group ( 1 ) in band_list : image_list . append ( [ file_path , getsize ( file_path ) ] ) elif matched : remove ( file_path ) except tarfile . ReadError as error : logger . error ( 'Error when extracting files: ' , error ) print ( 'Error when extracting files.' ) return image_list
6779	def get_deploy_funcs ( components , current_thumbprint , previous_thumbprint , preview = False ) : for component in components : funcs = manifest_deployers . get ( component , [ ] ) for func_name in funcs : if func_name . startswith ( 'burlap.' ) : print ( 'skipping %s' % func_name ) continue takes_diff = manifest_deployers_takes_diff . get ( func_name , False ) func = resolve_deployer ( func_name ) current = current_thumbprint . get ( component ) last = previous_thumbprint . get ( component ) if takes_diff : yield func_name , partial ( func , last = last , current = current ) else : yield func_name , partial ( func )
7548	def cluster_info ( ipyclient , spacer = "" ) : hosts = [ ] for eid in ipyclient . ids : engine = ipyclient [ eid ] if not engine . outstanding : hosts . append ( engine . apply ( _socket . gethostname ) ) hosts = [ i . get ( ) for i in hosts ] result = [ ] for hostname in set ( hosts ) : result . append ( "{}host compute node: [{} cores] on {}" . format ( spacer , hosts . count ( hostname ) , hostname ) ) print "\n" . join ( result )
7710	def request_roster ( self , version = None ) : processor = self . stanza_processor request = Iq ( stanza_type = "get" ) request . set_payload ( RosterPayload ( version = version ) ) processor . set_response_handlers ( request , self . _get_success , self . _get_error ) processor . send ( request )
768	def getMetricDetails ( self , metricLabel ) : try : metricIndex = self . __metricLabels . index ( metricLabel ) except IndexError : return None return self . __metrics [ metricIndex ] . getMetric ( )
1051	def format_exception_only ( etype , value ) : if ( isinstance ( etype , BaseException ) or etype is None or type ( etype ) is str ) : return [ _format_final_exc_line ( etype , value ) ] stype = etype . __name__ if not issubclass ( etype , SyntaxError ) : return [ _format_final_exc_line ( stype , value ) ] lines = [ ] try : msg , ( filename , lineno , offset , badline ) = value . args except Exception : pass else : filename = filename or "<string>" lines . append ( ' File "%s", line %d\n' % ( filename , lineno ) ) if badline is not None : lines . append ( ' %s\n' % badline . strip ( ) ) if offset is not None : caretspace = badline . rstrip ( '\n' ) offset = min ( len ( caretspace ) , offset ) - 1 caretspace = caretspace [ : offset ] . lstrip ( ) caretspace = ( ( c . isspace ( ) and c or ' ' ) for c in caretspace ) lines . append ( ' %s^\n' % '' . join ( caretspace ) ) value = msg lines . append ( _format_final_exc_line ( stype , value ) ) return lines
11393	def relative_to_full ( url , example_url ) : if re . match ( 'https?:\/\/' , url ) : return url domain = get_domain ( example_url ) if domain : return '%s%s' % ( domain , url ) return url
12550	def write_meta_header ( filename , meta_dict ) : header = '' for tag in MHD_TAGS : if tag in meta_dict . keys ( ) : header += '{} = {}\n' . format ( tag , meta_dict [ tag ] ) with open ( filename , 'w' ) as f : f . write ( header )
12794	def get_headers ( self ) : headers = { "User-Agent" : "kFlame 1.0" } password_url = self . _get_password_url ( ) if password_url and password_url in self . _settings [ "authorizations" ] : headers [ "Authorization" ] = self . _settings [ "authorizations" ] [ password_url ] return headers
11643	def transform ( self , X ) : X = check_array ( X ) X_rbf = np . empty_like ( X ) if self . copy else X X_in = X if not self . squared : np . power ( X_in , 2 , out = X_rbf ) X_in = X_rbf if self . scale_by_median : scale = self . median_ if self . squared else self . median_ ** 2 gamma = self . gamma * scale else : gamma = self . gamma np . multiply ( X_in , - gamma , out = X_rbf ) np . exp ( X_rbf , out = X_rbf ) return X_rbf
1851	def RCL ( cpu , dest , src ) : OperandSize = dest . size count = src . read ( ) countMask = { 8 : 0x1f , 16 : 0x1f , 32 : 0x1f , 64 : 0x3f } [ OperandSize ] tempCount = Operators . ZEXTEND ( ( count & countMask ) % ( src . size + 1 ) , OperandSize ) value = dest . read ( ) if isinstance ( tempCount , int ) and tempCount == 0 : new_val = value dest . write ( new_val ) else : carry = Operators . ITEBV ( OperandSize , cpu . CF , 1 , 0 ) right = value >> ( OperandSize - tempCount ) new_val = ( value << tempCount ) | ( carry << ( tempCount - 1 ) ) | ( right >> 1 ) dest . write ( new_val ) def sf ( v , size ) : return ( v & ( 1 << ( size - 1 ) ) ) != 0 cpu . CF = sf ( value << ( tempCount - 1 ) , OperandSize ) cpu . OF = Operators . ITE ( tempCount == 1 , sf ( new_val , OperandSize ) != cpu . CF , cpu . OF )
5479	def _cancel_batch ( batch_fn , cancel_fn , ops ) : canceled = [ ] failed = [ ] def handle_cancel_response ( request_id , response , exception ) : del response if exception : msg = 'error %s: %s' % ( exception . resp . status , exception . resp . reason ) if exception . resp . status == FAILED_PRECONDITION_CODE : detail = json . loads ( exception . content ) status = detail . get ( 'error' , { } ) . get ( 'status' ) if status == FAILED_PRECONDITION_STATUS : msg = 'Not running' failed . append ( { 'name' : request_id , 'msg' : msg } ) else : canceled . append ( { 'name' : request_id } ) return batch = batch_fn ( callback = handle_cancel_response ) ops_by_name = { } for op in ops : op_name = op . get_field ( 'internal-id' ) ops_by_name [ op_name ] = op batch . add ( cancel_fn ( name = op_name , body = { } ) , request_id = op_name ) batch . execute ( ) canceled_ops = [ ops_by_name [ op [ 'name' ] ] for op in canceled ] error_messages = [ ] for fail in failed : op = ops_by_name [ fail [ 'name' ] ] error_messages . append ( "Error canceling '%s': %s" % ( get_operation_full_job_id ( op ) , fail [ 'msg' ] ) ) return canceled_ops , error_messages
11622	def _unrecognised ( chr ) : if options [ 'handleUnrecognised' ] == UNRECOGNISED_ECHO : return chr elif options [ 'handleUnrecognised' ] == UNRECOGNISED_SUBSTITUTE : return options [ 'substituteChar' ] else : raise ( KeyError , chr )
12151	def html_single_basic ( self , abfID , launch = False , overwrite = False ) : if type ( abfID ) is str : abfID = [ abfID ] for thisABFid in cm . abfSort ( abfID ) : parentID = cm . parent ( self . groups , thisABFid ) saveAs = os . path . abspath ( "%s/%s_basic.html" % ( self . folder2 , parentID ) ) if overwrite is False and os . path . basename ( saveAs ) in self . files2 : continue filesByType = cm . filesByType ( self . groupFiles [ parentID ] ) html = "" html += '<div style="background-color: #DDDDDD;">' html += '<span class="title">summary of data from: %s</span></br>' % parentID html += '<code>%s</code>' % os . path . abspath ( self . folder1 + "/" + parentID + ".abf" ) html += '</div>' catOrder = [ "experiment" , "plot" , "tif" , "other" ] categories = cm . list_order_by ( filesByType . keys ( ) , catOrder ) for category in [ x for x in categories if len ( filesByType [ x ] ) ] : if category == 'experiment' : html += "<h3>Experimental Data:</h3>" elif category == 'plot' : html += "<h3>Intrinsic Properties:</h3>" elif category == 'tif' : html += "<h3>Micrographs:</h3>" elif category == 'other' : html += "<h3>Additional Files:</h3>" else : html += "<h3>????:</h3>" for fname in filesByType [ category ] : html += self . htmlFor ( fname ) html += '<br>' * 3 print ( "creating" , saveAs , '...' ) style . save ( html , saveAs , launch = launch )
4301	def setup_database ( config_data ) : with chdir ( config_data . project_directory ) : env = deepcopy ( dict ( os . environ ) ) env [ str ( 'DJANGO_SETTINGS_MODULE' ) ] = str ( '{0}.settings' . format ( config_data . project_name ) ) env [ str ( 'PYTHONPATH' ) ] = str ( os . pathsep . join ( map ( shlex_quote , sys . path ) ) ) commands = [ ] commands . append ( [ sys . executable , '-W' , 'ignore' , 'manage.py' , 'migrate' ] , ) if config_data . verbose : sys . stdout . write ( 'Database setup commands: {0}\n' . format ( ', ' . join ( [ ' ' . join ( cmd ) for cmd in commands ] ) ) ) for command in commands : try : output = subprocess . check_output ( command , env = env , stderr = subprocess . STDOUT ) sys . stdout . write ( output . decode ( 'utf-8' ) ) except subprocess . CalledProcessError as e : if config_data . verbose : sys . stdout . write ( e . output . decode ( 'utf-8' ) ) raise if not config_data . no_user : sys . stdout . write ( 'Creating admin user\n' ) if config_data . noinput : create_user ( config_data ) else : subprocess . check_call ( ' ' . join ( [ sys . executable , '-W' , 'ignore' , 'manage.py' , 'createsuperuser' ] ) , shell = True , stderr = subprocess . STDOUT )
10077	def create ( cls , data , id_ = None ) : data . setdefault ( '$schema' , current_jsonschemas . path_to_url ( current_app . config [ 'DEPOSIT_DEFAULT_JSONSCHEMA' ] ) ) if '_deposit' not in data : id_ = id_ or uuid . uuid4 ( ) cls . deposit_minter ( id_ , data ) data [ '_deposit' ] . setdefault ( 'owners' , list ( ) ) if current_user and current_user . is_authenticated : creator_id = int ( current_user . get_id ( ) ) if creator_id not in data [ '_deposit' ] [ 'owners' ] : data [ '_deposit' ] [ 'owners' ] . append ( creator_id ) data [ '_deposit' ] [ 'created_by' ] = creator_id return super ( Deposit , cls ) . create ( data , id_ = id_ )
4486	def remove ( self ) : response = self . _delete ( self . _delete_url ) if response . status_code != 204 : raise RuntimeError ( 'Could not delete {}.' . format ( self . path ) )
7543	def chunk_clusters ( data , sample ) : num = 0 optim = int ( ( sample . stats . clusters_total // data . cpus ) + ( sample . stats . clusters_total % data . cpus ) ) chunkslist = [ ] with gzip . open ( sample . files . clusters , 'rb' ) as clusters : pairdealer = itertools . izip ( * [ iter ( clusters ) ] * 2 ) done = 0 while not done : done , chunk = clustdealer ( pairdealer , optim ) chunkhandle = os . path . join ( data . dirs . clusts , "tmp_" + str ( sample . name ) + "." + str ( num * optim ) ) if chunk : chunkslist . append ( ( optim , chunkhandle ) ) with open ( chunkhandle , 'wb' ) as outchunk : outchunk . write ( "//\n//\n" . join ( chunk ) + "//\n//\n" ) num += 1 return chunkslist
1220	def reset ( self ) : fetches = [ ] for processor in self . preprocessors : fetches . extend ( processor . reset ( ) or [ ] ) return fetches
12993	def level_chunker ( text , getreffs , level = 1 ) : references = getreffs ( level = level ) return [ ( ref . split ( ":" ) [ - 1 ] , ref . split ( ":" ) [ - 1 ] ) for ref in references ]
4927	def transform_description ( self , content_metadata_item ) : description_with_locales = [ ] for locale in self . enterprise_configuration . get_locales ( ) : description_with_locales . append ( { 'locale' : locale , 'value' : ( content_metadata_item . get ( 'full_description' ) or content_metadata_item . get ( 'short_description' ) or content_metadata_item . get ( 'title' , '' ) ) } ) return description_with_locales
10696	def hsv_to_rgb ( hsv ) : h , s , v = hsv c = v * s h /= 60 x = c * ( 1 - abs ( ( h % 2 ) - 1 ) ) m = v - c if h < 1 : res = ( c , x , 0 ) elif h < 2 : res = ( x , c , 0 ) elif h < 3 : res = ( 0 , c , x ) elif h < 4 : res = ( 0 , x , c ) elif h < 5 : res = ( x , 0 , c ) elif h < 6 : res = ( c , 0 , x ) else : raise ColorException ( "Unable to convert from HSV to RGB" ) r , g , b = res return round ( ( r + m ) * 255 , 3 ) , round ( ( g + m ) * 255 , 3 ) , round ( ( b + m ) * 255 , 3 )
10655	def run ( self , clock ) : if clock . timestep_ix >= self . period_count : return for c in self . components : c . run ( clock , self . gl ) self . _perform_year_end_procedure ( clock )
11184	def wrap_state_dict ( self , typename : str , state ) -> Dict [ str , Any ] : return { self . type_key : typename , self . state_key : state }
1100	def unified_diff ( a , b , fromfile = '' , tofile = '' , fromfiledate = '' , tofiledate = '' , n = 3 , lineterm = '\n' ) : r started = False for group in SequenceMatcher ( None , a , b ) . get_grouped_opcodes ( n ) : if not started : started = True fromdate = '\t%s' % ( fromfiledate ) if fromfiledate else '' todate = '\t%s' % ( tofiledate ) if tofiledate else '' yield '--- %s%s%s' % ( fromfile , fromdate , lineterm ) yield '+++ %s%s%s' % ( tofile , todate , lineterm ) first , last = group [ 0 ] , group [ - 1 ] file1_range = _format_range_unified ( first [ 1 ] , last [ 2 ] ) file2_range = _format_range_unified ( first [ 3 ] , last [ 4 ] ) yield '@@ -%s +%s @@%s' % ( file1_range , file2_range , lineterm ) for tag , i1 , i2 , j1 , j2 in group : if tag == 'equal' : for line in a [ i1 : i2 ] : yield ' ' + line continue if tag in ( 'replace' , 'delete' ) : for line in a [ i1 : i2 ] : yield '-' + line if tag in ( 'replace' , 'insert' ) : for line in b [ j1 : j2 ] : yield '+' + line
4300	def _install_aldryn ( config_data ) : import requests media_project = os . path . join ( config_data . project_directory , 'dist' , 'media' ) static_main = False static_project = os . path . join ( config_data . project_directory , 'dist' , 'static' ) template_target = os . path . join ( config_data . project_directory , 'templates' ) tmpdir = tempfile . mkdtemp ( ) aldrynzip = requests . get ( data . ALDRYN_BOILERPLATE ) zip_open = zipfile . ZipFile ( BytesIO ( aldrynzip . content ) ) zip_open . extractall ( path = tmpdir ) for component in os . listdir ( os . path . join ( tmpdir , 'aldryn-boilerplate-standard-master' ) ) : src = os . path . join ( tmpdir , 'aldryn-boilerplate-standard-master' , component ) dst = os . path . join ( config_data . project_directory , component ) if os . path . isfile ( src ) : shutil . copy ( src , dst ) else : shutil . copytree ( src , dst ) shutil . rmtree ( tmpdir ) return media_project , static_main , static_project , template_target
2438	def add_review_comment ( self , doc , comment ) : if len ( doc . reviews ) != 0 : if not self . review_comment_set : self . review_comment_set = True if validations . validate_review_comment ( comment ) : doc . reviews [ - 1 ] . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'ReviewComment::Comment' ) else : raise CardinalityError ( 'ReviewComment' ) else : raise OrderError ( 'ReviewComment' )
5828	def pif_multi_search ( self , multi_query ) : failure_message = "Error while making PIF multi search request" response_dict = self . _get_success_json ( self . _post ( routes . pif_multi_search , data = json . dumps ( multi_query , cls = QueryEncoder ) , failure_message = failure_message ) ) return PifMultiSearchResult ( ** keys_to_snake_case ( response_dict [ 'results' ] ) )
8231	def speed ( self , framerate = None ) : if framerate is not None : self . _speed = framerate self . _dynamic = True else : return self . _speed
418	def delete_datasets ( self , ** kwargs ) : self . _fill_project_info ( kwargs ) self . db . Dataset . delete_many ( kwargs ) logging . info ( "[Database] Delete Dataset SUCCESS" )
6087	def contribution_maps_1d_from_hyper_images_and_galaxies ( hyper_model_image_1d , hyper_galaxy_images_1d , hyper_galaxies , hyper_minimum_values ) : return list ( map ( lambda hyper_galaxy , hyper_galaxy_image_1d , hyper_minimum_value : hyper_galaxy . contributions_from_model_image_and_galaxy_image ( model_image = hyper_model_image_1d , galaxy_image = hyper_galaxy_image_1d , minimum_value = hyper_minimum_value ) , hyper_galaxies , hyper_galaxy_images_1d , hyper_minimum_values ) )
1069	def gotonext ( self ) : while self . pos < len ( self . field ) : if self . field [ self . pos ] in self . LWS + '\n\r' : self . pos = self . pos + 1 elif self . field [ self . pos ] == '(' : self . commentlist . append ( self . getcomment ( ) ) else : break
719	def queryModelIDs ( self ) : jobID = self . getJobID ( ) modelCounterPairs = _clientJobsDB ( ) . modelsGetUpdateCounters ( jobID ) modelIDs = tuple ( x [ 0 ] for x in modelCounterPairs ) return modelIDs
324	def rolling_volatility ( returns , rolling_vol_window ) : return returns . rolling ( rolling_vol_window ) . std ( ) * np . sqrt ( APPROX_BDAYS_PER_YEAR )
7598	def get_popular_clans ( self , ** params : keys ) : url = self . api . POPULAR + '/clans' return self . _get_model ( url , PartialClan , ** params )
8607	def add_group_user ( self , group_id , user_id ) : data = { "id" : user_id } response = self . _perform_request ( url = '/um/groups/%s/users' % group_id , method = 'POST' , data = json . dumps ( data ) ) return response
8310	def is_list ( str ) : for chunk in str . split ( "\n" ) : chunk = chunk . replace ( "\t" , "" ) if not chunk . lstrip ( ) . startswith ( "*" ) and not re . search ( r"^([0-9]{1,3}\. )" , chunk . lstrip ( ) ) : return False return True
12829	def parse_conll ( self , texts : List [ str ] , retry_count : int = 0 ) -> List [ str ] : post_data = { 'texts' : texts , 'output_type' : 'conll' } try : response = requests . post ( f'http://{self.hostname}:{self.port}' , json = post_data , headers = { 'Connection' : 'close' } ) response . raise_for_status ( ) except ( requests . exceptions . ConnectionError , requests . exceptions . Timeout ) as server_error : raise ServerError ( server_error , self . hostname , self . port ) except requests . exceptions . HTTPError as http_error : raise http_error else : try : return response . json ( ) except json . JSONDecodeError as json_exception : if retry_count == self . retries : self . log_error ( response . text ) raise Exception ( 'Json Decoding error cannot parse this ' f':\n{response.text}' ) return self . parse_conll ( texts , retry_count + 1 )
205	def deepcopy ( self ) : segmap = SegmentationMapOnImage ( self . arr , shape = self . shape , nb_classes = self . nb_classes ) segmap . input_was = self . input_was return segmap
720	def getOptimizationMetricInfo ( cls , searchJobParams ) : if searchJobParams [ "hsVersion" ] == "v2" : search = HypersearchV2 ( searchParams = searchJobParams ) else : raise RuntimeError ( "Unsupported hypersearch version \"%s\"" % ( searchJobParams [ "hsVersion" ] ) ) info = search . getOptimizationMetricInfo ( ) return info
2045	def set_storage_data ( self , storage_address , offset , value ) : self . _world_state [ storage_address ] [ 'storage' ] [ offset ] = value
7980	def auth_timeout ( self ) : self . lock . acquire ( ) try : self . __logger . debug ( "Timeout while waiting for jabber:iq:auth result" ) if self . _auth_methods_left : self . _auth_methods_left . pop ( 0 ) finally : self . lock . release ( )
6093	def grid_angle_to_profile ( self , grid_thetas ) : theta_coordinate_to_profile = np . add ( grid_thetas , - self . phi_radians ) return np . cos ( theta_coordinate_to_profile ) , np . sin ( theta_coordinate_to_profile )
11534	def block_resource_fitnesses ( self , block : block . Block ) : if not block . resources : return { n : 1 for n in self . config . nodes . keys ( ) } node_fitnesses = { } for resource in block . resources : resource_fitnesses = self . resource_fitnesses ( resource ) if not resource_fitnesses : raise UnassignableBlock ( block . name ) max_fit = max ( resource_fitnesses . values ( ) ) min_fit = min ( resource_fitnesses . values ( ) ) for node , fitness in resource_fitnesses . items ( ) : if node not in node_fitnesses : node_fitnesses [ node ] = { } if not fitness : node_fitnesses [ node ] [ resource . describe ( ) ] = False else : if max_fit - min_fit : node_fitnesses [ node ] [ resource . describe ( ) ] = ( fitness - min_fit ) / ( max_fit - min_fit ) else : node_fitnesses [ node ] [ resource . describe ( ) ] = 1.0 res = { } for node , res_fits in node_fitnesses . items ( ) : fit_sum = 0 for res_desc , fit in res_fits . items ( ) : if fit is False : fit_sum = False break fit_sum += fit if fit_sum is False : res [ node ] = False continue res [ node ] = fit_sum return res
13100	def render ( self , ** kwargs ) : breadcrumbs = [ ] breadcrumbs = [ ] if "collections" in kwargs : breadcrumbs = [ { "title" : "Text Collections" , "link" : ".r_collections" , "args" : { } } ] if "parents" in kwargs [ "collections" ] : breadcrumbs += [ { "title" : parent [ "label" ] , "link" : ".r_collection_semantic" , "args" : { "objectId" : parent [ "id" ] , "semantic" : f_slugify ( parent [ "label" ] ) , } , } for parent in kwargs [ "collections" ] [ "parents" ] ] [ : : - 1 ] if "current" in kwargs [ "collections" ] : breadcrumbs . append ( { "title" : kwargs [ "collections" ] [ "current" ] [ "label" ] , "link" : None , "args" : { } } ) if len ( breadcrumbs ) > 0 : breadcrumbs [ - 1 ] [ "link" ] = None return { "breadcrumbs" : breadcrumbs }
3011	def locked_get ( self ) : filters = { self . key_name : self . key_value } query = self . session . query ( self . model_class ) . filter_by ( ** filters ) entity = query . first ( ) if entity : credential = getattr ( entity , self . property_name ) if credential and hasattr ( credential , 'set_store' ) : credential . set_store ( self ) return credential else : return None
5120	def next_event_description ( self ) : if self . _fancy_heap . size == 0 : event_type = 'Nothing' edge_index = None else : s = [ q . _key ( ) for q in self . edge2queue ] s . sort ( ) e = s [ 0 ] [ 1 ] q = self . edge2queue [ e ] event_type = 'Arrival' if q . next_event_description ( ) == 1 else 'Departure' edge_index = q . edge [ 2 ] return event_type , edge_index
10763	def get_unique_token ( self ) : if self . _unique_token is None : self . _unique_token = self . _random_token ( ) return self . _unique_token
2434	def set_lics_list_ver ( self , doc , value ) : if not self . lics_list_ver_set : self . lics_list_ver_set = True vers = version . Version . from_str ( value ) if vers is not None : doc . creation_info . license_list_version = vers return True else : raise SPDXValueError ( 'CreationInfo::LicenseListVersion' ) else : raise CardinalityError ( 'CreationInfo::LicenseListVersion' )
694	def loadExperimentDescriptionScriptFromDir ( experimentDir ) : descriptionScriptPath = os . path . join ( experimentDir , "description.py" ) module = _loadDescriptionFile ( descriptionScriptPath ) return module
964	def export ( self ) : graph = nx . MultiDiGraph ( ) regions = self . network . getRegions ( ) for idx in xrange ( regions . getCount ( ) ) : regionPair = regions . getByIndex ( idx ) regionName = regionPair [ 0 ] graph . add_node ( regionName , label = regionName ) for linkName , link in self . network . getLinks ( ) : graph . add_edge ( link . getSrcRegionName ( ) , link . getDestRegionName ( ) , src = link . getSrcOutputName ( ) , dest = link . getDestInputName ( ) ) return graph
7868	def _expire_item ( self , key ) : ( timeout , callback ) = self . _timeouts [ key ] now = time . time ( ) if timeout <= now : item = dict . pop ( self , key ) del self . _timeouts [ key ] if callback : try : callback ( key , item ) except TypeError : try : callback ( key ) except TypeError : callback ( ) return None else : return timeout - now
1549	def set_logging_level ( cl_args ) : if 'verbose' in cl_args and cl_args [ 'verbose' ] : configure ( logging . DEBUG ) else : configure ( logging . INFO )
8842	def indent ( self ) : if not self . tab_always_indent : super ( PyIndenterMode , self ) . indent ( ) else : cursor = self . editor . textCursor ( ) assert isinstance ( cursor , QtGui . QTextCursor ) if cursor . hasSelection ( ) : self . indent_selection ( cursor ) else : tab_len = self . editor . tab_length cursor . beginEditBlock ( ) if self . editor . use_spaces_instead_of_tabs : cursor . insertText ( tab_len * " " ) else : cursor . insertText ( '\t' ) cursor . endEditBlock ( ) self . editor . setTextCursor ( cursor )
3730	def third_property ( CASRN = None , T = False , P = False , V = False ) : r Third = None if V : Tc_methods = Tc ( CASRN , AvailableMethods = True ) [ 0 : - 2 ] Pc_methods = Pc ( CASRN , AvailableMethods = True ) [ 0 : - 2 ] if Tc_methods and Pc_methods : _Tc = Tc ( CASRN = CASRN , Method = Tc_methods [ 0 ] ) _Pc = Pc ( CASRN = CASRN , Method = Pc_methods [ 0 ] ) Third = critical_surface ( Tc = _Tc , Pc = _Pc , Vc = None ) elif P : Tc_methods = Tc ( CASRN , AvailableMethods = True ) [ 0 : - 2 ] Vc_methods = Vc ( CASRN , AvailableMethods = True ) [ 0 : - 2 ] if Tc_methods and Vc_methods : _Tc = Tc ( CASRN = CASRN , Method = Tc_methods [ 0 ] ) _Vc = Vc ( CASRN = CASRN , Method = Vc_methods [ 0 ] ) Third = critical_surface ( Tc = _Tc , Vc = _Vc , Pc = None ) elif T : Pc_methods = Pc ( CASRN , AvailableMethods = True ) [ 0 : - 2 ] Vc_methods = Vc ( CASRN , AvailableMethods = True ) [ 0 : - 2 ] if Pc_methods and Vc_methods : _Pc = Pc ( CASRN = CASRN , Method = Pc_methods [ 0 ] ) _Vc = Vc ( CASRN = CASRN , Method = Vc_methods [ 0 ] ) Third = critical_surface ( Pc = _Pc , Vc = _Vc , Tc = None ) else : raise Exception ( 'Error in function' ) if not Third : return None return Third
2047	def has_storage ( self , address ) : storage = self . _world_state [ address ] [ 'storage' ] array = storage . array while not isinstance ( array , ArrayVariable ) : if isinstance ( array , ArrayStore ) : return True array = array . array return False
4551	def draw_triangle ( setter , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : draw_line ( setter , x0 , y0 , x1 , y1 , color , aa ) draw_line ( setter , x1 , y1 , x2 , y2 , color , aa ) draw_line ( setter , x2 , y2 , x0 , y0 , color , aa )
4739	def err ( txt ) : print ( "%s# %s%s%s" % ( PR_ERR_CC , get_time_stamp ( ) , txt , PR_NC ) ) sys . stdout . flush ( )
2758	def get_floating_ip ( self , ip ) : return FloatingIP . get_object ( api_token = self . token , ip = ip )
13271	def unique_justseen ( iterable , key = None ) : "List unique elements, preserving order. Remember only the element just seen." try : from itertools import imap as map except ImportError : from builtins import map return map ( next , map ( operator . itemgetter ( 1 ) , itertools . groupby ( iterable , key ) ) )
298	def plot_turnover ( returns , transactions , positions , legend_loc = 'best' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) df_turnover = txn . get_turnover ( positions , transactions ) df_turnover_by_month = df_turnover . resample ( "M" ) . mean ( ) df_turnover . plot ( color = 'steelblue' , alpha = 1.0 , lw = 0.5 , ax = ax , ** kwargs ) df_turnover_by_month . plot ( color = 'orangered' , alpha = 0.5 , lw = 2 , ax = ax , ** kwargs ) ax . axhline ( df_turnover . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 , alpha = 1.0 ) ax . legend ( [ 'Daily turnover' , 'Average daily turnover, by month' , 'Average daily turnover, net' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) ax . set_title ( 'Daily turnover' ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) ax . set_ylim ( ( 0 , 2 ) ) ax . set_ylabel ( 'Turnover' ) ax . set_xlabel ( '' ) return ax
308	def plot_prob_profit_trade ( round_trips , ax = None ) : x = np . linspace ( 0 , 1. , 500 ) round_trips [ 'profitable' ] = round_trips . pnl > 0 dist = sp . stats . beta ( round_trips . profitable . sum ( ) , ( ~ round_trips . profitable ) . sum ( ) ) y = dist . pdf ( x ) lower_perc = dist . ppf ( .025 ) upper_perc = dist . ppf ( .975 ) lower_plot = dist . ppf ( .001 ) upper_plot = dist . ppf ( .999 ) if ax is None : ax = plt . subplot ( ) ax . plot ( x , y ) ax . axvline ( lower_perc , color = '0.5' ) ax . axvline ( upper_perc , color = '0.5' ) ax . set_xlabel ( 'Probability of making a profitable decision' ) ax . set_ylabel ( 'Belief' ) ax . set_xlim ( lower_plot , upper_plot ) ax . set_ylim ( ( 0 , y . max ( ) + 1. ) ) return ax
7484	def run2 ( data , samples , force , ipyclient ) : data . dirs . edits = os . path . join ( os . path . realpath ( data . paramsdict [ "project_dir" ] ) , data . name + "_edits" ) if not os . path . exists ( data . dirs . edits ) : os . makedirs ( data . dirs . edits ) subsamples = choose_samples ( samples , force ) if int ( data . paramsdict [ "filter_adapters" ] ) == 3 : if not data . _hackersonly [ "p3_adapters_extra" ] : for poly in [ "A" * 8 , "T" * 8 , "C" * 8 , "G" * 8 ] : data . _hackersonly [ "p3_adapters_extra" ] . append ( poly ) if not data . _hackersonly [ "p5_adapters_extra" ] : for poly in [ "A" * 8 , "T" * 8 , "C" * 8 , "G" * 8 ] : data . _hackersonly [ "p5_adapters_extra" ] . append ( poly ) else : data . _hackersonly [ "p5_adapters_extra" ] = [ ] data . _hackersonly [ "p3_adapters_extra" ] = [ ] subsamples = concat_reads ( data , subsamples , ipyclient ) lbview = ipyclient . load_balanced_view ( targets = ipyclient . ids [ : : 2 ] ) run_cutadapt ( data , subsamples , lbview ) assembly_cleanup ( data )
12034	def kernel_gaussian ( self , sizeMS , sigmaMS = None , forwardOnly = False ) : sigmaMS = sizeMS / 10 if sigmaMS is None else sigmaMS size , sigma = sizeMS * self . pointsPerMs , sigmaMS * self . pointsPerMs self . kernel = swhlab . common . kernel_gaussian ( size , sigma , forwardOnly ) return self . kernel
1157	def acquire ( self , blocking = 1 ) : me = _get_ident ( ) if self . __owner == me : self . __count = self . __count + 1 if __debug__ : self . _note ( "%s.acquire(%s): recursive success" , self , blocking ) return 1 rc = self . __block . acquire ( blocking ) if rc : self . __owner = me self . __count = 1 if __debug__ : self . _note ( "%s.acquire(%s): initial success" , self , blocking ) else : if __debug__ : self . _note ( "%s.acquire(%s): failure" , self , blocking ) return rc
2109	def receive ( organization = None , user = None , team = None , credential_type = None , credential = None , notification_template = None , inventory_script = None , inventory = None , project = None , job_template = None , workflow = None , all = None ) : from tower_cli . cli . transfer . receive import Receiver receiver = Receiver ( ) assets_to_export = { } for asset_type in SEND_ORDER : assets_to_export [ asset_type ] = locals ( ) [ asset_type ] receiver . receive ( all = all , asset_input = assets_to_export )
3502	def assess_products ( model , reaction , flux_coefficient_cutoff = 0.001 , solver = None ) : warn ( 'use assess_component instead' , DeprecationWarning ) return assess_component ( model , reaction , 'products' , flux_coefficient_cutoff , solver )
13496	def bump ( self , target ) : if target == 'patch' : return Version ( self . major , self . minor , self . patch + 1 ) if target == 'minor' : return Version ( self . major , self . minor + 1 , 0 ) if target == 'major' : return Version ( self . major + 1 , 0 , 0 ) return self . clone ( )
11943	def _get ( self , * args , ** kwargs ) : messages , all_retrieved = super ( StorageMixin , self ) . _get ( * args , ** kwargs ) if self . user . is_authenticated ( ) : inbox_messages = self . backend . inbox_list ( self . user ) else : inbox_messages = [ ] return messages + inbox_messages , all_retrieved
13342	def concatenate ( tup , axis = 0 ) : from distob import engine if len ( tup ) is 0 : raise ValueError ( 'need at least one array to concatenate' ) first = tup [ 0 ] others = tup [ 1 : ] if ( hasattr ( first , 'concatenate' ) and hasattr ( type ( first ) , '__array_interface__' ) ) : return first . concatenate ( others , axis ) arrays = [ ] for ar in tup : if isinstance ( ar , DistArray ) : if axis == ar . _distaxis : arrays . extend ( ar . _subarrays ) else : arrays . append ( gather ( ar ) ) elif isinstance ( ar , RemoteArray ) : arrays . append ( ar ) elif isinstance ( ar , Remote ) : arrays . append ( _remote_to_array ( ar ) ) elif hasattr ( type ( ar ) , '__array_interface__' ) : arrays . append ( ar ) else : arrays . append ( np . array ( ar ) ) if all ( isinstance ( ar , np . ndarray ) for ar in arrays ) : return np . concatenate ( arrays , axis ) total_length = 0 commonshape = list ( arrays [ 0 ] . shape ) commonshape [ axis ] = None for ar in arrays : total_length += ar . shape [ axis ] shp = list ( ar . shape ) shp [ axis ] = None if shp != commonshape : raise ValueError ( 'incompatible shapes for concatenation' ) blocksize = ( ( total_length - 1 ) // engine . nengines ) + 1 rarrays = [ ] for ar in arrays : if isinstance ( ar , DistArray ) : rarrays . extend ( ar . _subarrays ) elif isinstance ( ar , RemoteArray ) : rarrays . append ( ar ) else : da = _scatter_ndarray ( ar , axis , blocksize ) for ra in da . _subarrays : rarrays . append ( ra ) del da del arrays eid = rarrays [ 0 ] . _id . engine if all ( ra . _id . engine == eid for ra in rarrays ) : if eid == engine . eid : return concatenate ( [ gather ( r ) for r in rarrays ] , axis ) else : return call ( concatenate , rarrays , axis ) else : return DistArray ( rarrays , axis )
404	def swish ( x , name = 'swish' ) : with tf . name_scope ( name ) : x = tf . nn . sigmoid ( x ) * x return x
8012	def check_paypal_api_key ( app_configs = None , ** kwargs ) : messages = [ ] mode = getattr ( djpaypal_settings , "PAYPAL_MODE" , None ) if mode not in VALID_MODES : msg = "Invalid PAYPAL_MODE specified: {}." . format ( repr ( mode ) ) hint = "PAYPAL_MODE must be one of {}" . format ( ", " . join ( repr ( k ) for k in VALID_MODES ) ) messages . append ( checks . Critical ( msg , hint = hint , id = "djpaypal.C001" ) ) for setting in "PAYPAL_CLIENT_ID" , "PAYPAL_CLIENT_SECRET" : if not getattr ( djpaypal_settings , setting , None ) : msg = "Invalid value specified for {}" . format ( setting ) hint = "Add PAYPAL_CLIENT_ID and PAYPAL_CLIENT_SECRET to your settings." messages . append ( checks . Critical ( msg , hint = hint , id = "djpaypal.C002" ) ) return messages
10727	def _handle_array ( toks ) : if len ( toks ) == 5 and toks [ 1 ] == '{' and toks [ 4 ] == '}' : subtree = toks [ 2 : 4 ] signature = '' . join ( s for ( _ , s ) in subtree ) [ key_func , value_func ] = [ f for ( f , _ ) in subtree ] def the_dict_func ( a_dict , variant = 0 ) : elements = [ ( key_func ( x ) , value_func ( y ) ) for ( x , y ) in a_dict . items ( ) ] level = 0 if elements == [ ] else max ( max ( x , y ) for ( ( _ , x ) , ( _ , y ) ) in elements ) ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( level , variant ) return ( dbus . types . Dictionary ( ( ( x , y ) for ( ( x , _ ) , ( y , _ ) ) in elements ) , signature = signature , variant_level = obj_level ) , func_level ) return ( the_dict_func , 'a{' + signature + '}' ) if len ( toks ) == 2 : ( func , sig ) = toks [ 1 ] def the_array_func ( a_list , variant = 0 ) : if isinstance ( a_list , dict ) : raise IntoDPValueError ( a_list , "a_list" , "is a dict, must be an array" ) elements = [ func ( x ) for x in a_list ] level = 0 if elements == [ ] else max ( x for ( _ , x ) in elements ) ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( level , variant ) return ( dbus . types . Array ( ( x for ( x , _ ) in elements ) , signature = sig , variant_level = obj_level ) , func_level ) return ( the_array_func , 'a' + sig ) raise IntoDPValueError ( toks , "toks" , "unexpected tokens" )
7587	def taxon_table ( self ) : if self . tests : keys = sorted ( self . tests [ 0 ] . keys ( ) ) if isinstance ( self . tests , list ) : ld = [ [ ( key , i [ key ] ) for key in keys ] for i in self . tests ] dd = [ dict ( i ) for i in ld ] df = pd . DataFrame ( dd ) return df else : return pd . DataFrame ( pd . Series ( self . tests ) ) . T else : return None
6680	def copy ( self , source , destination , recursive = False , use_sudo = False ) : func = use_sudo and run_as_root or self . run options = '-r ' if recursive else '' func ( '/bin/cp {0}{1} {2}' . format ( options , quote ( source ) , quote ( destination ) ) )
12571	def get ( self , key ) : node = self . get_node ( key ) if node is None : raise KeyError ( 'No object named %s in the file' % key ) if hasattr ( node , 'attrs' ) : if 'pandas_type' in node . attrs : return self . _read_group ( node ) return self . _read_array ( node )
12770	def step ( self , substeps = 2 ) : self . frame_no += 1 try : next ( self . follower ) except ( AttributeError , StopIteration ) as err : self . reset ( )
9354	def body ( quantity = 2 , separator = '\n\n' , wrap_start = '' , wrap_end = '' , html = False , sentences_quantity = 3 , as_list = False ) : return lorem_ipsum . paragraphs ( quantity = quantity , separator = separator , wrap_start = wrap_start , wrap_end = wrap_end , html = html , sentences_quantity = sentences_quantity , as_list = as_list )
13222	def lunch ( self , message = "Time for lunch" , shout : bool = False ) : return self . helper . output ( message , shout )
2790	def get_snapshots ( self ) : data = self . get_data ( "volumes/%s/snapshots/" % self . id ) snapshots = list ( ) for jsond in data [ u'snapshots' ] : snapshot = Snapshot ( ** jsond ) snapshot . token = self . token snapshots . append ( snapshot ) return snapshots
12422	def dumps ( obj , startindex = 1 , separator = DEFAULT , index_separator = DEFAULT ) : try : firstkey = next ( iter ( obj . keys ( ) ) ) except StopIteration : return str ( ) if isinstance ( firstkey , six . text_type ) : io = StringIO ( ) else : io = BytesIO ( ) dump ( obj = obj , fp = io , startindex = startindex , separator = separator , index_separator = index_separator , ) return io . getvalue ( )
10808	def delete ( self ) : with db . session . begin_nested ( ) : Membership . query_by_group ( self ) . delete ( ) GroupAdmin . query_by_group ( self ) . delete ( ) GroupAdmin . query_by_admin ( self ) . delete ( ) db . session . delete ( self )
5354	def get_repos_by_backend_section ( cls , backend_section , raw = True ) : repos = [ ] projects = TaskProjects . get_projects ( ) for pro in projects : if backend_section in projects [ pro ] : if cls . GLOBAL_PROJECT not in projects : repos += projects [ pro ] [ backend_section ] else : if raw : if pro != cls . GLOBAL_PROJECT : if backend_section not in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ pro ] [ backend_section ] elif backend_section in projects [ pro ] and backend_section in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ cls . GLOBAL_PROJECT ] [ backend_section ] else : not_in_unknown = [ projects [ pro ] for pro in projects if pro != cls . GLOBAL_PROJECT ] [ 0 ] if backend_section not in not_in_unknown : repos += projects [ cls . GLOBAL_PROJECT ] [ backend_section ] else : if pro != cls . GLOBAL_PROJECT : if backend_section not in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ pro ] [ backend_section ] elif backend_section in projects [ pro ] and backend_section in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ pro ] [ backend_section ] else : not_in_unknown_prj = [ projects [ prj ] for prj in projects if prj != cls . GLOBAL_PROJECT ] not_in_unknown_sections = list ( set ( [ section for prj in not_in_unknown_prj for section in list ( prj . keys ( ) ) ] ) ) if backend_section not in not_in_unknown_sections : repos += projects [ pro ] [ backend_section ] logger . debug ( "List of repos for %s: %s (raw=%s)" , backend_section , repos , raw ) repos = list ( set ( repos ) ) return repos
12238	def rosenbrock ( theta ) : x , y = theta obj = ( 1 - x ) ** 2 + 100 * ( y - x ** 2 ) ** 2 grad = np . zeros ( 2 ) grad [ 0 ] = 2 * x - 400 * ( x * y - x ** 3 ) - 2 grad [ 1 ] = 200 * ( y - x ** 2 ) return obj , grad
10527	def cast_to_list ( position ) : @ wrapt . decorator def wrapper ( function , instance , args , kwargs ) : if not isinstance ( args [ position ] , list ) : args = list ( args ) args [ position ] = [ args [ position ] ] args = tuple ( args ) return function ( * args , ** kwargs ) return wrapper
1398	def extract_scheduler_location ( self , topology ) : schedulerLocation = { "name" : None , "http_endpoint" : None , "job_page_link" : None , } if topology . scheduler_location : schedulerLocation [ "name" ] = topology . scheduler_location . topology_name schedulerLocation [ "http_endpoint" ] = topology . scheduler_location . http_endpoint schedulerLocation [ "job_page_link" ] = topology . scheduler_location . job_page_link [ 0 ] if len ( topology . scheduler_location . job_page_link ) > 0 else "" return schedulerLocation
2493	def create_annotation_node ( self , annotation ) : annotation_node = URIRef ( str ( annotation . spdx_id ) ) type_triple = ( annotation_node , RDF . type , self . spdx_namespace . Annotation ) self . graph . add ( type_triple ) annotator_node = Literal ( annotation . annotator . to_value ( ) ) self . graph . add ( ( annotation_node , self . spdx_namespace . annotator , annotator_node ) ) annotation_date_node = Literal ( annotation . annotation_date_iso_format ) annotation_triple = ( annotation_node , self . spdx_namespace . annotationDate , annotation_date_node ) self . graph . add ( annotation_triple ) if annotation . has_comment : comment_node = Literal ( annotation . comment ) comment_triple = ( annotation_node , RDFS . comment , comment_node ) self . graph . add ( comment_triple ) annotation_type_node = Literal ( annotation . annotation_type ) annotation_type_triple = ( annotation_node , self . spdx_namespace . annotationType , annotation_type_node ) self . graph . add ( annotation_type_triple ) return annotation_node
13190	def json_doc_to_xml ( json_obj , lang = 'en' , custom_namespace = None ) : if 'meta' not in json_obj : raise Exception ( "This function requires a conforming Open511 JSON document with a 'meta' section." ) json_obj = dict ( json_obj ) meta = json_obj . pop ( 'meta' ) elem = get_base_open511_element ( lang = lang , version = meta . pop ( 'version' ) ) pagination = json_obj . pop ( 'pagination' , None ) json_struct_to_xml ( json_obj , elem , custom_namespace = custom_namespace ) if pagination : elem . append ( json_struct_to_xml ( pagination , 'pagination' , custom_namespace = custom_namespace ) ) json_struct_to_xml ( meta , elem ) return elem
5832	def get ( self , data_view_id ) : failure_message = "Dataview get failed" return self . _get_success_json ( self . _get ( 'v1/data_views/' + data_view_id , None , failure_message = failure_message ) ) [ 'data' ] [ 'data_view' ]
7764	def connect ( self ) : with self . lock : if self . stream : logger . debug ( "Closing the previously used stream." ) self . _close_stream ( ) transport = TCPTransport ( self . settings ) addr = self . settings [ "server" ] if addr : service = None else : addr = self . jid . domain service = self . settings [ "c2s_service" ] transport . connect ( addr , self . settings [ "c2s_port" ] , service ) handlers = self . _base_handlers [ : ] handlers += self . handlers + [ self ] self . clear_response_handlers ( ) self . setup_stanza_handlers ( handlers , "pre-auth" ) stream = ClientStream ( self . jid , self , handlers , self . settings ) stream . initiate ( transport ) self . main_loop . add_handler ( transport ) self . main_loop . add_handler ( stream ) self . _ml_handlers += [ transport , stream ] self . stream = stream self . uplink = stream
6830	def get_logs_between_commits ( self , a , b ) : print ( 'REAL' ) ret = self . local ( 'git --no-pager log --pretty=oneline %s...%s' % ( a , b ) , capture = True ) if self . verbose : print ( ret ) return str ( ret )
8571	def get_nic ( self , datacenter_id , server_id , nic_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/nics/%s?depth=%s' % ( datacenter_id , server_id , nic_id , str ( depth ) ) ) return response
4898	def _remove_failed_items ( self , failed_items , items_to_create , items_to_update , items_to_delete ) : for item in failed_items : content_metadata_id = item [ 'courseID' ] items_to_create . pop ( content_metadata_id , None ) items_to_update . pop ( content_metadata_id , None ) items_to_delete . pop ( content_metadata_id , None )
4760	def wait ( timeout = 300 ) : if env ( ) : cij . err ( "cij.ssh.wait: Invalid SSH environment" ) return 1 timeout_backup = cij . ENV . get ( "SSH_CMD_TIMEOUT" ) try : time_start = time . time ( ) cij . ENV [ "SSH_CMD_TIMEOUT" ] = "3" while True : time_current = time . time ( ) if ( time_current - time_start ) > timeout : cij . err ( "cij.ssh.wait: Timeout" ) return 1 status , _ , _ = command ( [ "exit" ] , shell = True , echo = False ) if not status : break cij . info ( "cij.ssh.wait: Time elapsed: %d seconds" % ( time_current - time_start ) ) finally : if timeout_backup is None : del cij . ENV [ "SSH_CMD_TIMEOUT" ] else : cij . ENV [ "SSH_CMD_TIMEOUT" ] = timeout_backup return 0
6527	def get_tools ( ) : if not hasattr ( get_tools , '_CACHE' ) : get_tools . _CACHE = dict ( ) for entry in pkg_resources . iter_entry_points ( 'tidypy.tools' ) : try : get_tools . _CACHE [ entry . name ] = entry . load ( ) except ImportError as exc : output_error ( 'Could not load tool "%s" defined by "%s": %s' % ( entry , entry . dist , exc , ) , ) return get_tools . _CACHE
1302	def keybd_event ( bVk : int , bScan : int , dwFlags : int , dwExtraInfo : int ) -> None : ctypes . windll . user32 . keybd_event ( bVk , bScan , dwFlags , dwExtraInfo )
2492	def create_review_node ( self , review ) : review_node = BNode ( ) type_triple = ( review_node , RDF . type , self . spdx_namespace . Review ) self . graph . add ( type_triple ) reviewer_node = Literal ( review . reviewer . to_value ( ) ) self . graph . add ( ( review_node , self . spdx_namespace . reviewer , reviewer_node ) ) reviewed_date_node = Literal ( review . review_date_iso_format ) reviewed_triple = ( review_node , self . spdx_namespace . reviewDate , reviewed_date_node ) self . graph . add ( reviewed_triple ) if review . has_comment : comment_node = Literal ( review . comment ) comment_triple = ( review_node , RDFS . comment , comment_node ) self . graph . add ( comment_triple ) return review_node
4872	def to_representation ( self , data ) : return [ self . child . to_representation ( item ) if 'detail' in item else item for item in data ]
9320	def _validate_collection ( self ) : if not self . _id : msg = "No 'id' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if not self . _title : msg = "No 'title' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _can_read is None : msg = "No 'can_read' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _can_write is None : msg = "No 'can_write' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _id not in self . url : msg = "The collection '{}' does not match the url for queries '{}'" raise ValidationError ( msg . format ( self . _id , self . url ) )
12888	def call ( self , path , extra = None ) : try : if not self . __webfsapi : self . __webfsapi = yield from self . get_fsapi_endpoint ( ) if not self . sid : self . sid = yield from self . create_session ( ) if not isinstance ( extra , dict ) : extra = dict ( ) params = dict ( pin = self . pin , sid = self . sid ) params . update ( ** extra ) req_url = ( '%s/%s' % ( self . __webfsapi , path ) ) result = yield from self . __session . get ( req_url , params = params , timeout = self . timeout ) if result . status == 200 : text = yield from result . text ( encoding = 'utf-8' ) else : self . sid = yield from self . create_session ( ) params = dict ( pin = self . pin , sid = self . sid ) params . update ( ** extra ) result = yield from self . __session . get ( req_url , params = params , timeout = self . timeout ) text = yield from result . text ( encoding = 'utf-8' ) return objectify . fromstring ( text ) except Exception as e : logging . info ( 'AFSAPI Exception: ' + traceback . format_exc ( ) ) return None
4644	def get ( self , key , default = None ) : if key in self : return self . __getitem__ ( key ) else : return default
10519	def onedown ( self , window_name , object_name , iterations ) : if not self . verifyscrollbarvertical ( window_name , object_name ) : raise LdtpServerException ( 'Object not vertical scrollbar' ) object_handle = self . _get_object_handle ( window_name , object_name ) i = 0 maxValue = 1.0 / 8 flag = False while i < iterations : if object_handle . AXValue >= 1 : raise LdtpServerException ( 'Maximum limit reached' ) object_handle . AXValue += maxValue time . sleep ( 1.0 / 100 ) flag = True i += 1 if flag : return 1 else : raise LdtpServerException ( 'Unable to increase scrollbar' )
9526	def to_fastg ( infile , outfile , circular = None ) : if circular is None : to_circularise = set ( ) elif type ( circular ) is not set : f = utils . open_file_read ( circular ) to_circularise = set ( [ x . rstrip ( ) for x in f . readlines ( ) ] ) utils . close ( f ) else : to_circularise = circular seq_reader = sequences . file_reader ( infile ) fout = utils . open_file_write ( outfile ) nodes = 1 for seq in seq_reader : new_id = '_' . join ( [ 'NODE' , str ( nodes ) , 'length' , str ( len ( seq ) ) , 'cov' , '1' , 'ID' , seq . id ] ) if seq . id in to_circularise : seq . id = new_id + ':' + new_id + ';' print ( seq , file = fout ) seq . revcomp ( ) seq . id = new_id + "':" + new_id + "';" print ( seq , file = fout ) else : seq . id = new_id + ';' print ( seq , file = fout ) seq . revcomp ( ) seq . id = new_id + "';" print ( seq , file = fout ) nodes += 1 utils . close ( fout )
6725	def get_or_create ( name = None , group = None , config = None , extra = 0 , verbose = 0 , backend_opts = None ) : require ( 'vm_type' , 'vm_group' ) backend_opts = backend_opts or { } verbose = int ( verbose ) extra = int ( extra ) if config : config_fn = common . find_template ( config ) config = yaml . load ( open ( config_fn ) ) env . update ( config ) env . vm_type = ( env . vm_type or '' ) . lower ( ) assert env . vm_type , 'No VM type specified.' group = group or env . vm_group assert group , 'No VM group specified.' ret = exists ( name = name , group = group ) if not extra and ret : if verbose : print ( 'VM %s:%s exists.' % ( name , group ) ) return ret today = datetime . date . today ( ) release = int ( '%i%02i%02i' % ( today . year , today . month , today . day ) ) if not name : existing_instances = list_instances ( group = group , release = release , verbose = verbose ) name = env . vm_name_template . format ( index = len ( existing_instances ) + 1 ) if env . vm_type == EC2 : return get_or_create_ec2_instance ( name = name , group = group , release = release , verbose = verbose , backend_opts = backend_opts ) else : raise NotImplementedError
3121	def make_signed_jwt ( signer , payload , key_id = None ) : header = { 'typ' : 'JWT' , 'alg' : 'RS256' } if key_id is not None : header [ 'kid' ] = key_id segments = [ _helpers . _urlsafe_b64encode ( _helpers . _json_encode ( header ) ) , _helpers . _urlsafe_b64encode ( _helpers . _json_encode ( payload ) ) , ] signing_input = b'.' . join ( segments ) signature = signer . sign ( signing_input ) segments . append ( _helpers . _urlsafe_b64encode ( signature ) ) logger . debug ( str ( segments ) ) return b'.' . join ( segments )
10896	def load_image ( self ) : try : image = initializers . load_tiff ( self . filename ) image = initializers . normalize ( image , invert = self . invert , scale = self . exposure , dtype = self . float_precision ) except IOError as e : log . error ( "Could not find image '%s'" % self . filename ) raise e return image
4072	def split_elements ( value ) : items = [ v . strip ( ) for v in value . split ( ',' ) ] if len ( items ) == 1 : items = value . split ( ) return items
6845	def check_ok ( self ) : import requests if not self . env . check_ok : return branch_name = self . _local ( 'git rev-parse --abbrev-ref HEAD' , capture = True ) . strip ( ) check_ok_paths = self . env . check_ok_paths or { } if branch_name in check_ok_paths : check = check_ok_paths [ branch_name ] if 'username' in check : auth = ( check [ 'username' ] , check [ 'password' ] ) else : auth = None ret = requests . get ( check [ 'url' ] , auth = auth ) passed = check [ 'text' ] in ret . content assert passed , 'Check failed: %s' % check [ 'url' ]
11552	def analog_read ( self , pin ) : with self . data_lock : data = self . _command_handler . analog_response_table [ pin ] [ self . _command_handler . RESPONSE_TABLE_PIN_DATA_VALUE ] return data
3459	def _multi_deletion ( model , entity , element_lists , method = "fba" , solution = None , processes = None , ** kwargs ) : solver = sutil . interface_to_str ( model . problem . __name__ ) if method == "moma" and solver not in sutil . qp_solvers : raise RuntimeError ( "Cannot use MOMA since '{}' is not QP-capable." "Please choose a different solver or use FBA only." . format ( solver ) ) if processes is None : processes = CONFIGURATION . processes with model : if "moma" in method : add_moma ( model , solution = solution , linear = "linear" in method ) elif "room" in method : add_room ( model , solution = solution , linear = "linear" in method , ** kwargs ) args = set ( [ frozenset ( comb ) for comb in product ( * element_lists ) ] ) processes = min ( processes , len ( args ) ) def extract_knockout_results ( result_iter ) : result = pd . DataFrame ( [ ( frozenset ( ids ) , growth , status ) for ( ids , growth , status ) in result_iter ] , columns = [ 'ids' , 'growth' , 'status' ] ) result . set_index ( 'ids' , inplace = True ) return result if processes > 1 : worker = dict ( gene = _gene_deletion_worker , reaction = _reaction_deletion_worker ) [ entity ] chunk_size = len ( args ) // processes pool = multiprocessing . Pool ( processes , initializer = _init_worker , initargs = ( model , ) ) results = extract_knockout_results ( pool . imap_unordered ( worker , args , chunksize = chunk_size ) ) pool . close ( ) pool . join ( ) else : worker = dict ( gene = _gene_deletion , reaction = _reaction_deletion ) [ entity ] results = extract_knockout_results ( map ( partial ( worker , model ) , args ) ) return results
4467	def serialize ( transform , ** kwargs ) : params = transform . get_params ( ) return jsonpickle . encode ( params , ** kwargs )
8617	def _underscore_to_camelcase ( value ) : def camelcase ( ) : yield str . lower while True : yield str . capitalize c = camelcase ( ) return "" . join ( next ( c ) ( x ) if x else '_' for x in value . split ( "_" ) )
5586	def output_cleaned ( self , process_data ) : if self . METADATA [ "data_type" ] == "raster" : if is_numpy_or_masked_array ( process_data ) : return process_data elif is_numpy_or_masked_array_with_tags ( process_data ) : data , tags = process_data return self . output_cleaned ( data ) , tags elif self . METADATA [ "data_type" ] == "vector" : return list ( process_data )
6075	def einstein_radius_in_units ( self , unit_length = 'arcsec' , kpc_per_arcsec = None ) : if self . has_mass_profile : return sum ( map ( lambda p : p . einstein_radius_in_units ( unit_length = unit_length , kpc_per_arcsec = kpc_per_arcsec ) , self . mass_profiles ) ) else : return None
11133	def tear_down ( self ) : while len ( self . _temp_directories ) > 0 : directory = self . _temp_directories . pop ( ) shutil . rmtree ( directory , ignore_errors = True ) while len ( self . _temp_files ) > 0 : file = self . _temp_files . pop ( ) try : os . remove ( file ) except OSError : pass
3892	def run_example ( example_coroutine , * extra_args ) : args = _get_parser ( extra_args ) . parse_args ( ) logging . basicConfig ( level = logging . DEBUG if args . debug else logging . WARNING ) cookies = hangups . auth . get_auth_stdin ( args . token_path ) client = hangups . Client ( cookies ) loop = asyncio . get_event_loop ( ) task = asyncio . ensure_future ( _async_main ( example_coroutine , client , args ) , loop = loop ) try : loop . run_until_complete ( task ) except KeyboardInterrupt : task . cancel ( ) loop . run_until_complete ( task ) finally : loop . close ( )
2437	def add_review_date ( self , doc , reviewed ) : if len ( doc . reviews ) != 0 : if not self . review_date_set : self . review_date_set = True date = utils . datetime_from_iso_format ( reviewed ) if date is not None : doc . reviews [ - 1 ] . review_date = date return True else : raise SPDXValueError ( 'Review::ReviewDate' ) else : raise CardinalityError ( 'Review::ReviewDate' ) else : raise OrderError ( 'Review::ReviewDate' )
5525	def grab ( self , bbox = None ) : w = Gdk . get_default_root_window ( ) if bbox is not None : g = [ bbox [ 0 ] , bbox [ 1 ] , bbox [ 2 ] - bbox [ 0 ] , bbox [ 3 ] - bbox [ 1 ] ] else : g = w . get_geometry ( ) pb = Gdk . pixbuf_get_from_window ( w , * g ) if pb . get_bits_per_sample ( ) != 8 : raise ValueError ( 'Expected 8 bits per pixel.' ) elif pb . get_n_channels ( ) != 3 : raise ValueError ( 'Expected RGB image.' ) pixel_bytes = pb . read_pixel_bytes ( ) . get_data ( ) width , height = g [ 2 ] , g [ 3 ] return Image . frombytes ( 'RGB' , ( width , height ) , pixel_bytes , 'raw' , 'RGB' , pb . get_rowstride ( ) , 1 )
4507	def get_device ( self , id = None ) : if id is None : if not self . devices : raise ValueError ( 'No default device for %s' % self . hardware_id ) id , ( device , version ) = sorted ( self . devices . items ( ) ) [ 0 ] elif id in self . devices : device , version = self . devices [ id ] else : error = 'Unable to find device with ID %s' % id log . error ( error ) raise ValueError ( error ) log . info ( "Using COM Port: %s, Device ID: %s, Device Ver: %s" , device , id , version ) return id , device , version
11555	def enable_analog_reporting ( self , pin ) : command = [ self . _command_handler . REPORT_ANALOG + pin , self . REPORTING_ENABLE ] self . _command_handler . send_command ( command )
8244	def shader ( x , y , dx , dy , radius = 300 , angle = 0 , spread = 90 ) : if angle != None : radius *= 2 d = sqrt ( ( dx - x ) ** 2 + ( dy - y ) ** 2 ) a = degrees ( atan2 ( dy - y , dx - x ) ) + 180 if d <= radius : d1 = 1.0 * d / radius else : d1 = 1.0 if angle is None : return 1 - d1 angle = 360 - angle % 360 spread = max ( 0 , min ( spread , 360 ) ) if spread == 0 : return 0.0 d = abs ( a - angle ) if d <= spread / 2 : d2 = d / spread + d1 else : d2 = 1.0 if 360 - angle <= spread / 2 : d = abs ( 360 - angle + a ) if d <= spread / 2 : d2 = d / spread + d1 if angle < spread / 2 : d = abs ( 360 + angle - a ) if d <= spread / 2 : d2 = d / spread + d1 return 1 - max ( 0 , min ( d2 , 1 ) )
3327	def refresh ( self , token , timeout = None ) : if timeout is None : timeout = LockManager . LOCK_TIME_OUT_DEFAULT return self . storage . refresh ( token , timeout )
4670	def setKeys ( self , loadkeys ) : log . debug ( "Force setting of private keys. Not using the wallet database!" ) if isinstance ( loadkeys , dict ) : loadkeys = list ( loadkeys . values ( ) ) elif not isinstance ( loadkeys , ( list , set ) ) : loadkeys = [ loadkeys ] for wif in loadkeys : pub = self . publickey_from_wif ( wif ) self . store . add ( str ( wif ) , pub )
9360	def _to_lower_alpha_only ( s ) : s = re . sub ( r'\n' , ' ' , s . lower ( ) ) return re . sub ( r'[^a-z\s]' , '' , s )
1112	def make_file ( self , fromlines , tolines , fromdesc = '' , todesc = '' , context = False , numlines = 5 ) : return self . _file_template % dict ( styles = self . _styles , legend = self . _legend , table = self . make_table ( fromlines , tolines , fromdesc , todesc , context = context , numlines = numlines ) )
10128	def update ( self , dt ) : self . translate ( dt * self . velocity ) self . rotate ( dt * self . angular_velocity )
9350	def check_digit ( num ) : sum = 0 digits = str ( num ) [ : - 1 ] [ : : - 1 ] for i , n in enumerate ( digits ) : if ( i + 1 ) % 2 != 0 : digit = int ( n ) * 2 if digit > 9 : sum += ( digit - 9 ) else : sum += digit else : sum += int ( n ) return ( ( divmod ( sum , 10 ) [ 0 ] + 1 ) * 10 - sum ) % 10
8038	def code_mapping ( level , msg , default = 99 ) : try : return code_mappings_by_level [ level ] [ msg ] except KeyError : pass if msg . count ( '"' ) == 2 and ' "' in msg and msg . endswith ( '".' ) : txt = msg [ : msg . index ( ' "' ) ] return code_mappings_by_level [ level ] . get ( txt , default ) return default
314	def rolling_beta ( returns , factor_returns , rolling_window = APPROX_BDAYS_PER_MONTH * 6 ) : if factor_returns . ndim > 1 : return factor_returns . apply ( partial ( rolling_beta , returns ) , rolling_window = rolling_window ) else : out = pd . Series ( index = returns . index ) for beg , end in zip ( returns . index [ 0 : - rolling_window ] , returns . index [ rolling_window : ] ) : out . loc [ end ] = ep . beta ( returns . loc [ beg : end ] , factor_returns . loc [ beg : end ] ) return out
1194	def put ( self , item , block = True , timeout = None ) : self . not_full . acquire ( ) try : if self . maxsize > 0 : if not block : if self . _qsize ( ) == self . maxsize : raise Full elif timeout is None : while self . _qsize ( ) == self . maxsize : self . not_full . wait ( ) elif timeout < 0 : raise ValueError ( "'timeout' must be a non-negative number" ) else : endtime = _time ( ) + timeout while self . _qsize ( ) == self . maxsize : remaining = endtime - _time ( ) if remaining <= 0.0 : raise Full self . not_full . wait ( remaining ) self . _put ( item ) self . unfinished_tasks += 1 self . not_empty . notify ( ) finally : self . not_full . release ( )
10609	def create_stream ( self , assay = None , mfr = 0.0 , P = 1.0 , T = 25.0 , normalise = True ) : if assay is None : return MaterialStream ( self , self . create_empty_assay ( ) , P , T ) if normalise : assay_total = self . get_assay_total ( assay ) else : assay_total = 1.0 return MaterialStream ( self , mfr * self . converted_assays [ assay ] / assay_total , P , T , self . _isCoal ( assay ) , self . _get_HHV ( assay ) )
4050	def last_modified_version ( self , ** kwargs ) : self . items ( ** kwargs ) return int ( self . request . headers . get ( "last-modified-version" , 0 ) )
7014	def concatenate_textlcs ( lclist , sortby = 'rjd' , normalize = True ) : lcdict = read_hatpi_textlc ( lclist [ 0 ] ) lccounter = 0 lcdict [ 'concatenated' ] = { lccounter : os . path . abspath ( lclist [ 0 ] ) } lcdict [ 'lcn' ] = np . full_like ( lcdict [ 'rjd' ] , lccounter ) if normalize : for col in MAGCOLS : if col in lcdict : thismedval = np . nanmedian ( lcdict [ col ] ) if col in ( 'ifl1' , 'ifl2' , 'ifl3' ) : lcdict [ col ] = lcdict [ col ] / thismedval else : lcdict [ col ] = lcdict [ col ] - thismedval for lcf in lclist [ 1 : ] : thislcd = read_hatpi_textlc ( lcf ) if thislcd [ 'columns' ] != lcdict [ 'columns' ] : LOGERROR ( 'file %s does not have the ' 'same columns as first file %s, skipping...' % ( lcf , lclist [ 0 ] ) ) continue else : LOGINFO ( 'adding %s (ndet: %s) to %s (ndet: %s)' % ( lcf , thislcd [ 'objectinfo' ] [ 'ndet' ] , lclist [ 0 ] , lcdict [ lcdict [ 'columns' ] [ 0 ] ] . size ) ) lccounter = lccounter + 1 lcdict [ 'concatenated' ] [ lccounter ] = os . path . abspath ( lcf ) lcdict [ 'lcn' ] = np . concatenate ( ( lcdict [ 'lcn' ] , np . full_like ( thislcd [ 'rjd' ] , lccounter ) ) ) for col in lcdict [ 'columns' ] : if normalize and col in MAGCOLS : thismedval = np . nanmedian ( thislcd [ col ] ) if col in ( 'ifl1' , 'ifl2' , 'ifl3' ) : thislcd [ col ] = thislcd [ col ] / thismedval else : thislcd [ col ] = thislcd [ col ] - thismedval lcdict [ col ] = np . concatenate ( ( lcdict [ col ] , thislcd [ col ] ) ) lcdict [ 'objectinfo' ] [ 'ndet' ] = lcdict [ lcdict [ 'columns' ] [ 0 ] ] . size lcdict [ 'objectinfo' ] [ 'stations' ] = [ 'HP%s' % x for x in np . unique ( lcdict [ 'stf' ] ) . tolist ( ) ] lcdict [ 'nconcatenated' ] = lccounter + 1 if sortby and sortby in [ x [ 0 ] for x in COLDEFS ] : LOGINFO ( 'sorting concatenated light curve by %s...' % sortby ) sortind = np . argsort ( lcdict [ sortby ] ) for col in lcdict [ 'columns' ] : lcdict [ col ] = lcdict [ col ] [ sortind ] lcdict [ 'lcn' ] = lcdict [ 'lcn' ] [ sortind ] LOGINFO ( 'done. concatenated light curve has %s detections' % lcdict [ 'objectinfo' ] [ 'ndet' ] ) return lcdict
11870	def color_from_rgb ( red , green , blue ) : r = min ( red , 255 ) g = min ( green , 255 ) b = min ( blue , 255 ) if r > 1 or g > 1 or b > 1 : r = r / 255.0 g = g / 255.0 b = b / 255.0 return color_from_hls ( * rgb_to_hls ( r , g , b ) )
9952	def get_object ( name : str ) : elms = name . split ( "." ) parent = get_models ( ) [ elms . pop ( 0 ) ] while len ( elms ) > 0 : obj = elms . pop ( 0 ) parent = getattr ( parent , obj ) return parent
5203	def delete_connection ( ) : if _CON_SYM_ in globals ( ) : con = globals ( ) . pop ( _CON_SYM_ ) if not getattr ( con , '_session' ) . start ( ) : con . stop ( )
5709	def redirect ( self , request ) : url = request . path querystring = request . GET . copy ( ) if self . logout_key and self . logout_key in request . GET : del querystring [ self . logout_key ] if querystring : url = '%s?%s' % ( url , querystring . urlencode ( ) ) return HttpResponseRedirect ( url )
13680	def get_translated_data ( self ) : j = { } for k in self . data : d = { } for l in self . data [ k ] : d [ self . translation_keys [ l ] ] = self . data [ k ] [ l ] j [ k ] = d return j
11740	def first ( self , symbols ) : ret = set ( ) if EPSILON in symbols : return set ( [ EPSILON ] ) for symbol in symbols : ret |= self . _first [ symbol ] - set ( [ EPSILON ] ) if EPSILON not in self . _first [ symbol ] : break else : ret . add ( EPSILON ) return ret
8750	def get_scalingip ( context , id , fields = None ) : LOG . info ( 'get_scalingip %s for tenant %s' % ( id , context . tenant_id ) ) filters = { 'address_type' : ip_types . SCALING , '_deallocated' : False } scaling_ip = db_api . floating_ip_find ( context , id = id , scope = db_api . ONE , ** filters ) if not scaling_ip : raise q_exc . ScalingIpNotFound ( id = id ) return v . _make_scaling_ip_dict ( scaling_ip )
12850	def _remove_from_world ( self ) : self . on_remove_from_world ( ) self . _extensions = { } self . _disable_forum_observation ( ) self . _world = None self . _id = None
10334	def build_expand_node_neighborhood_by_hash ( manager : Manager ) -> Callable [ [ BELGraph , BELGraph , str ] , None ] : @ uni_in_place_transformation def expand_node_neighborhood_by_hash ( universe : BELGraph , graph : BELGraph , node_hash : str ) -> None : node = manager . get_dsl_by_hash ( node_hash ) return expand_node_neighborhood ( universe , graph , node ) return expand_node_neighborhood_by_hash
1707	def run ( command , data = None , timeout = None , kill_timeout = None , env = None , cwd = None ) : command = expand_args ( command ) history = [ ] for c in command : if len ( history ) : data = history [ - 1 ] . std_out [ 0 : 10 * 1024 ] cmd = Command ( c ) try : out , err = cmd . run ( data , timeout , kill_timeout , env , cwd ) status_code = cmd . returncode except OSError as e : out , err = '' , u"\n" . join ( [ e . strerror , traceback . format_exc ( ) ] ) status_code = 127 r = Response ( process = cmd ) r . command = c r . std_out = out r . std_err = err r . status_code = status_code history . append ( r ) r = history . pop ( ) r . history = history return r
11008	def get_project_slug ( self , bet ) : if bet . get ( 'form_params' ) : params = json . loads ( bet [ 'form_params' ] ) return params . get ( 'project' ) return None
11144	def to_repo_relative_path ( self , path , split = False ) : path = os . path . normpath ( path ) if path == '.' : path = '' path = path . split ( self . __path ) [ - 1 ] . strip ( os . sep ) if split : return path . split ( os . sep ) else : return path
4792	def is_unicode ( self ) : if type ( self . val ) is not unicode : self . _err ( 'Expected <%s> to be unicode, but was <%s>.' % ( self . val , type ( self . val ) . __name__ ) ) return self
1393	def getTopologyByClusterRoleEnvironAndName ( self , cluster , role , environ , topologyName ) : topologies = list ( filter ( lambda t : t . name == topologyName and t . cluster == cluster and ( not role or t . execution_state . role == role ) and t . environ == environ , self . topologies ) ) if not topologies or len ( topologies ) > 1 : if role is not None : raise Exception ( "Topology not found for {0}, {1}, {2}, {3}" . format ( cluster , role , environ , topologyName ) ) else : raise Exception ( "Topology not found for {0}, {1}, {2}" . format ( cluster , environ , topologyName ) ) return topologies [ 0 ]
6012	def load_exposure_time_map ( exposure_time_map_path , exposure_time_map_hdu , pixel_scale , shape , exposure_time , exposure_time_map_from_inverse_noise_map , inverse_noise_map ) : exposure_time_map_options = sum ( [ exposure_time_map_from_inverse_noise_map ] ) if exposure_time is not None and exposure_time_map_path is not None : raise exc . DataException ( 'You have supplied both a exposure_time_map_path to an exposure time map and an exposure time. Only' 'one quantity should be supplied.' ) if exposure_time_map_options == 0 : if exposure_time is not None and exposure_time_map_path is None : return ExposureTimeMap . single_value ( value = exposure_time , pixel_scale = pixel_scale , shape = shape ) elif exposure_time is None and exposure_time_map_path is not None : return ExposureTimeMap . from_fits_with_pixel_scale ( file_path = exposure_time_map_path , hdu = exposure_time_map_hdu , pixel_scale = pixel_scale ) else : if exposure_time_map_from_inverse_noise_map : return ExposureTimeMap . from_exposure_time_and_inverse_noise_map ( pixel_scale = pixel_scale , exposure_time = exposure_time , inverse_noise_map = inverse_noise_map )
13841	def _ConsumeSingleByteString ( self ) : text = self . token if len ( text ) < 1 or text [ 0 ] not in _QUOTES : raise self . _ParseError ( 'Expected string but found: %r' % ( text , ) ) if len ( text ) < 2 or text [ - 1 ] != text [ 0 ] : raise self . _ParseError ( 'String missing ending quote: %r' % ( text , ) ) try : result = text_encoding . CUnescape ( text [ 1 : - 1 ] ) except ValueError as e : raise self . _ParseError ( str ( e ) ) self . NextToken ( ) return result
2472	def reset_file_stat ( self ) : self . file_spdx_id_set = False self . file_comment_set = False self . file_type_set = False self . file_chksum_set = False self . file_conc_lics_set = False self . file_license_comment_set = False self . file_notice_set = False self . file_copytext_set = False
9363	def user_name ( with_num = False ) : result = first_name ( ) if with_num : result += str ( random . randint ( 63 , 94 ) ) return result . lower ( )
5098	def graph2dict ( g , return_dict_of_dict = True ) : if not isinstance ( g , nx . DiGraph ) : g = QueueNetworkDiGraph ( g ) dict_of_dicts = nx . to_dict_of_dicts ( g ) if return_dict_of_dict : return dict_of_dicts else : return { k : list ( val . keys ( ) ) for k , val in dict_of_dicts . items ( ) }
5282	def construct_formset ( self ) : formset_class = self . get_formset ( ) if hasattr ( self , 'get_extra_form_kwargs' ) : klass = type ( self ) . __name__ raise DeprecationWarning ( 'Calling {0}.get_extra_form_kwargs is no longer supported. ' 'Set `form_kwargs` in {0}.formset_kwargs or override ' '{0}.get_formset_kwargs() directly.' . format ( klass ) , ) return formset_class ( ** self . get_formset_kwargs ( ) )
2074	def convert_input_vector ( y , index ) : if y is None : return None if isinstance ( y , pd . Series ) : return y elif isinstance ( y , np . ndarray ) : if len ( np . shape ( y ) ) == 1 : return pd . Series ( y , name = 'target' , index = index ) elif len ( np . shape ( y ) ) == 2 and np . shape ( y ) [ 0 ] == 1 : return pd . Series ( y [ 0 , : ] , name = 'target' , index = index ) elif len ( np . shape ( y ) ) == 2 and np . shape ( y ) [ 1 ] == 1 : return pd . Series ( y [ : , 0 ] , name = 'target' , index = index ) else : raise ValueError ( 'Unexpected input shape: %s' % ( str ( np . shape ( y ) ) ) ) elif np . isscalar ( y ) : return pd . Series ( [ y ] , name = 'target' , index = index ) elif isinstance ( y , list ) : if len ( y ) == 0 or ( len ( y ) > 0 and not isinstance ( y [ 0 ] , list ) ) : return pd . Series ( y , name = 'target' , index = index ) elif len ( y ) > 0 and isinstance ( y [ 0 ] , list ) and len ( y [ 0 ] ) == 1 : flatten = lambda y : [ item for sublist in y for item in sublist ] return pd . Series ( flatten ( y ) , name = 'target' , index = index ) elif len ( y ) == 1 and isinstance ( y [ 0 ] , list ) : return pd . Series ( y [ 0 ] , name = 'target' , index = index ) else : raise ValueError ( 'Unexpected input shape' ) elif isinstance ( y , pd . DataFrame ) : if len ( list ( y ) ) == 0 : return pd . Series ( y , name = 'target' ) if len ( list ( y ) ) == 1 : return y . iloc [ : , 0 ] else : raise ValueError ( 'Unexpected input shape: %s' % ( str ( y . shape ) ) ) else : return pd . Series ( y , name = 'target' , index = index )
3526	def kiss_metrics ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return KissMetricsNode ( )
11280	def get_item_creator ( item_type ) : if item_type not in Pipe . pipe_item_types : for registered_type in Pipe . pipe_item_types : if issubclass ( item_type , registered_type ) : return Pipe . pipe_item_types [ registered_type ] return None else : return Pipe . pipe_item_types [ item_type ]
4035	def cleanwrap ( func ) : def enc ( self , * args , ** kwargs ) : return ( func ( self , item , ** kwargs ) for item in args ) return enc
6110	def unmasked_blurred_image_of_galaxies_from_psf ( self , padded_grid_stack , psf ) : return [ padded_grid_stack . unmasked_blurred_image_from_psf_and_unmasked_image ( psf , image ) if not galaxy . has_pixelization else None for galaxy , image in zip ( self . galaxies , self . image_plane_image_1d_of_galaxies ) ]
7502	def fill_boot ( seqarr , newboot , newmap , spans , loci ) : cidx = 0 for i in xrange ( loci . shape [ 0 ] ) : x1 = spans [ loci [ i ] ] [ 0 ] x2 = spans [ loci [ i ] ] [ 1 ] cols = seqarr [ : , x1 : x2 ] cord = np . random . choice ( cols . shape [ 1 ] , cols . shape [ 1 ] , replace = False ) rcols = cols [ : , cord ] newboot [ : , cidx : cidx + cols . shape [ 1 ] ] = rcols newmap [ cidx : cidx + cols . shape [ 1 ] , 0 ] = i + 1 cidx += cols . shape [ 1 ] return newboot , newmap
13736	def get_param_values ( request , model = None ) : if type ( request ) == dict : return request params = get_payload ( request ) try : del params [ 'pk' ] params [ params . pop ( 'name' ) ] = params . pop ( 'value' ) except KeyError : pass return { k . rstrip ( '[]' ) : safe_eval ( v ) if not type ( v ) == list else [ safe_eval ( sv ) for sv in v ] for k , v in params . items ( ) }
6122	def zoom_region ( self ) : where = np . array ( np . where ( np . invert ( self . astype ( 'bool' ) ) ) ) y0 , x0 = np . amin ( where , axis = 1 ) y1 , x1 = np . amax ( where , axis = 1 ) return [ y0 , y1 + 1 , x0 , x1 + 1 ]
6593	def receive_one ( self ) : if not self . runid_pkgidx_map : return None while True : if not self . runid_to_return : self . runid_to_return . extend ( self . dispatcher . poll ( ) ) ret = self . _collect_next_finished_pkgidx_result_pair ( ) if ret is not None : break if self . runid_pkgidx_map : time . sleep ( self . sleep ) return ret
3679	def economic_status ( self ) : r if self . __economic_status : return self . __economic_status else : self . __economic_status = economic_status ( self . CAS , Method = 'Combined' ) return self . __economic_status
814	def pickByDistribution ( distribution , r = None ) : if r is None : r = random x = r . uniform ( 0 , sum ( distribution ) ) for i , d in enumerate ( distribution ) : if x <= d : return i x -= d
12193	def _respond ( self , channel , text ) : result = self . _format_message ( channel , text ) if result is not None : logger . info ( 'Sending message: %r' , truncate ( result , max_len = 50 ) , ) self . socket . send_str ( result )
9522	def merge_to_one_seq ( infile , outfile , seqname = 'union' ) : seq_reader = sequences . file_reader ( infile ) seqs = [ ] for seq in seq_reader : seqs . append ( copy . copy ( seq ) ) new_seq = '' . join ( [ seq . seq for seq in seqs ] ) if type ( seqs [ 0 ] ) == sequences . Fastq : new_qual = '' . join ( [ seq . qual for seq in seqs ] ) seqs [ : ] = [ ] merged = sequences . Fastq ( seqname , new_seq , new_qual ) else : merged = sequences . Fasta ( seqname , new_seq ) seqs [ : ] = [ ] f = utils . open_file_write ( outfile ) print ( merged , file = f ) utils . close ( f )
8534	def read ( cls , data , protocol = None , fallback_protocol = TBinaryProtocol , finagle_thrift = False , max_fields = MAX_FIELDS , max_list_size = MAX_LIST_SIZE , max_map_size = MAX_MAP_SIZE , max_set_size = MAX_SET_SIZE , read_values = False ) : if len ( data ) < cls . MIN_MESSAGE_SIZE : raise ValueError ( 'not enough data' ) if protocol is None : protocol = cls . detect_protocol ( data , fallback_protocol ) trans = TTransport . TMemoryBuffer ( data ) proto = protocol ( trans ) header = None if finagle_thrift : try : header = ThriftStruct . read ( proto , max_fields , max_list_size , max_map_size , max_set_size , read_values ) except : trans = TTransport . TMemoryBuffer ( data ) proto = protocol ( trans ) method , mtype , seqid = proto . readMessageBegin ( ) mtype = cls . message_type_to_str ( mtype ) if len ( method ) == 0 or method . isspace ( ) or method . startswith ( ' ' ) : raise ValueError ( 'no method name' ) if len ( method ) > cls . MAX_METHOD_LENGTH : raise ValueError ( 'method name too long' ) valid = range ( 33 , 127 ) if any ( ord ( char ) not in valid for char in method ) : raise ValueError ( 'invalid method name' % method ) args = ThriftStruct . read ( proto , max_fields , max_list_size , max_map_size , max_set_size , read_values ) proto . readMessageEnd ( ) msglen = trans . _buffer . tell ( ) return cls ( method , mtype , seqid , args , header , msglen ) , msglen
9993	def get_dynspace ( self , args , kwargs = None ) : node = get_node ( self , * convert_args ( args , kwargs ) ) key = node [ KEY ] if key in self . param_spaces : return self . param_spaces [ key ] else : last_self = self . system . self self . system . self = self try : space_args = self . eval_formula ( node ) finally : self . system . self = last_self if space_args is None : space_args = { "bases" : [ self ] } else : if "bases" in space_args : bases = get_impls ( space_args [ "bases" ] ) if isinstance ( bases , StaticSpaceImpl ) : space_args [ "bases" ] = [ bases ] elif bases is None : space_args [ "bases" ] = [ self ] else : space_args [ "bases" ] = bases else : space_args [ "bases" ] = [ self ] space_args [ "arguments" ] = node_get_args ( node ) space = self . _new_dynspace ( ** space_args ) self . param_spaces [ key ] = space space . inherit ( clear_value = False ) return space
1731	def call ( self , this , args = ( ) ) : if self . is_native : _args = SpaceTuple ( args ) _args . space = self . space return self . code ( this , _args ) else : return self . space . exe . _call ( self , this , args )
2006	def _serialize_int ( value , size = 32 , padding = 0 ) : if size <= 0 or size > 32 : raise ValueError if not isinstance ( value , ( int , BitVec ) ) : raise ValueError if issymbolic ( value ) : buf = ArrayVariable ( index_bits = 256 , index_max = 32 , value_bits = 8 , name = 'temp{}' . format ( uuid . uuid1 ( ) ) ) value = Operators . SEXTEND ( value , value . size , size * 8 ) buf = ArrayProxy ( buf . write_BE ( padding , value , size ) ) else : value = int ( value ) buf = bytearray ( ) for _ in range ( padding ) : buf . append ( 0 ) for position in reversed ( range ( size ) ) : buf . append ( Operators . EXTRACT ( value , position * 8 , 8 ) ) return buf
11399	def update_keywords ( self ) : for field in record_get_field_instances ( self . record , '653' , ind1 = '1' ) : subs = field_get_subfields ( field ) new_subs = [ ] if 'a' in subs : for val in subs [ 'a' ] : new_subs . extend ( [ ( '9' , 'author' ) , ( 'a' , val ) ] ) new_field = create_field ( subfields = new_subs , ind1 = '1' ) record_replace_field ( self . record , '653' , new_field , field_position_global = field [ 4 ] )
7185	def copy_type_comments_to_annotations ( args ) : for arg in args . args : copy_type_comment_to_annotation ( arg ) if args . vararg : copy_type_comment_to_annotation ( args . vararg ) for arg in args . kwonlyargs : copy_type_comment_to_annotation ( arg ) if args . kwarg : copy_type_comment_to_annotation ( args . kwarg )
11609	def multiply ( self , multiplier , axis = None ) : if self . finalized : if multiplier . ndim == 1 : if axis == 0 : raise NotImplementedError ( 'The method is not yet implemented for the axis.' ) elif axis == 1 : sz = len ( multiplier ) multiplier_mat = lil_matrix ( ( sz , sz ) ) multiplier_mat . setdiag ( multiplier ) for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] * multiplier_mat elif axis == 2 : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] . data *= multiplier [ self . data [ hid ] . indices ] else : raise RuntimeError ( 'The axis should be 0, 1, or 2.' ) elif multiplier . ndim == 2 : if axis == 0 : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] . data *= multiplier [ self . data [ hid ] . indices , hid ] elif axis == 1 : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] . multiply ( multiplier ) elif axis == 2 : for hid in xrange ( self . shape [ 1 ] ) : multiplier_vec = multiplier [ hid , : ] multiplier_vec = multiplier_vec . ravel ( ) self . data [ hid ] . data *= multiplier_vec . repeat ( np . diff ( self . data [ hid ] . indptr ) ) else : raise RuntimeError ( 'The axis should be 0, 1, or 2.' ) elif isinstance ( multiplier , Sparse3DMatrix ) : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] . multiply ( multiplier . data [ hid ] ) else : raise RuntimeError ( 'The multiplier should be 1, 2 dimensional numpy array or a Sparse3DMatrix object.' ) else : raise RuntimeError ( 'The original matrix must be finalized.' )
13520	def configure ( self , url = None , token = None , test = False ) : if url is None : url = Config . get_value ( "url" ) if token is None : token = Config . get_value ( "token" ) self . server_url = url self . auth_header = { "Authorization" : "Basic {0}" . format ( token ) } self . configured = True if test : self . test_connection ( ) Config . set ( "url" , url ) Config . set ( "token" , token )
12218	def _bind_args ( sig , param_matchers , args , kwargs ) : bound = sig . bind ( * args , ** kwargs ) if not all ( param_matcher ( bound . arguments [ param_name ] ) for param_name , param_matcher in param_matchers ) : raise TypeError return bound
7211	def stderr ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get stderr.' ) if self . batch_values : raise NotImplementedError ( "Query Each Workflow Id within the Batch Workflow for stderr." ) wf = self . workflow . get ( self . id ) stderr_list = [ ] for task in wf [ 'tasks' ] : stderr_list . append ( { 'id' : task [ 'id' ] , 'taskType' : task [ 'taskType' ] , 'name' : task [ 'name' ] , 'stderr' : self . workflow . get_stderr ( self . id , task [ 'id' ] ) } ) return stderr_list
8425	def grey_pal ( start = 0.2 , end = 0.8 ) : gamma = 2.2 ends = ( ( 0.0 , start , start ) , ( 1.0 , end , end ) ) cdict = { 'red' : ends , 'green' : ends , 'blue' : ends } grey_cmap = mcolors . LinearSegmentedColormap ( 'grey' , cdict ) def continuous_grey_palette ( n ) : colors = [ ] for x in np . linspace ( start ** gamma , end ** gamma , n ) : x = ( x ** ( 1. / gamma ) - start ) / ( end - start ) colors . append ( mcolors . rgb2hex ( grey_cmap ( x ) ) ) return colors return continuous_grey_palette
2663	def scale_out ( self , blocks = 1 ) : r = [ ] for i in range ( blocks ) : if self . provider : external_block_id = str ( len ( self . blocks ) ) launch_cmd = self . launch_cmd . format ( block_id = external_block_id ) internal_block = self . provider . submit ( launch_cmd , 1 , 1 ) logger . debug ( "Launched block {}->{}" . format ( external_block_id , internal_block ) ) if not internal_block : raise ( ScalingFailed ( self . provider . label , "Attempts to provision nodes via provider has failed" ) ) r . extend ( [ external_block_id ] ) self . blocks [ external_block_id ] = internal_block else : logger . error ( "No execution provider available" ) r = None return r
13785	def generate ( length = DEFAULT_LENGTH ) : return '' . join ( random . SystemRandom ( ) . choice ( ALPHABET ) for _ in range ( length ) )
7309	def with_tz ( request ) : dt = datetime . now ( ) t = Template ( '{% load tz %}{% localtime on %}{% get_current_timezone as TIME_ZONE %}{{ TIME_ZONE }}{% endlocaltime %}' ) c = RequestContext ( request ) response = t . render ( c ) return HttpResponse ( response )
4543	def compose_events ( events , condition = all ) : events = list ( events ) master_event = threading . Event ( ) def changed ( ) : if condition ( e . is_set ( ) for e in events ) : master_event . set ( ) else : master_event . clear ( ) def add_changed ( f ) : @ functools . wraps ( f ) def wrapped ( ) : f ( ) changed ( ) return wrapped for e in events : e . set = add_changed ( e . set ) e . clear = add_changed ( e . clear ) changed ( ) return master_event
8268	def color ( self , clr = None , d = 0.035 ) : if clr != None and not isinstance ( clr , Color ) : clr = color ( clr ) if clr != None and not self . grayscale : if clr . is_black : return self . black . color ( clr , d ) if clr . is_white : return self . white . color ( clr , d ) if clr . is_grey : return choice ( ( self . black . color ( clr , d ) , self . white . color ( clr , d ) ) ) h , s , b , a = self . h , self . s , self . b , self . a if clr != None : h , a = clr . h + d * ( random ( ) * 2 - 1 ) , clr . a hsba = [ ] for v in [ h , s , b , a ] : if isinstance ( v , _list ) : min , max = choice ( v ) elif isinstance ( v , tuple ) : min , max = v else : min , max = v , v hsba . append ( min + ( max - min ) * random ( ) ) h , s , b , a = hsba return color ( h , s , b , a , mode = "hsb" )
10432	def selectrowindex ( self , window_name , object_name , row_index ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) count = len ( object_handle . AXRows ) if row_index < 0 or row_index > count : raise LdtpServerException ( 'Row index out of range: %d' % row_index ) cell = object_handle . AXRows [ row_index ] if not cell . AXSelected : object_handle . activate ( ) cell . AXSelected = True else : pass return 1
11968	def _dec_to_bin ( ip ) : bits = [ ] while ip : bits . append ( _BYTES_TO_BITS [ ip & 255 ] ) ip >>= 8 bits . reverse ( ) return '' . join ( bits ) or 32 * '0'
151	def from_shapely ( geometry , label = None ) : import shapely . geometry if isinstance ( geometry , shapely . geometry . MultiPolygon ) : return MultiPolygon ( [ Polygon . from_shapely ( poly , label = label ) for poly in geometry . geoms ] ) elif isinstance ( geometry , shapely . geometry . Polygon ) : return MultiPolygon ( [ Polygon . from_shapely ( geometry , label = label ) ] ) elif isinstance ( geometry , shapely . geometry . collection . GeometryCollection ) : ia . do_assert ( all ( [ isinstance ( poly , shapely . geometry . Polygon ) for poly in geometry . geoms ] ) ) return MultiPolygon ( [ Polygon . from_shapely ( poly , label = label ) for poly in geometry . geoms ] ) else : raise Exception ( "Unknown datatype '%s'. Expected shapely.geometry.Polygon or " "shapely.geometry.MultiPolygon or " "shapely.geometry.collections.GeometryCollection." % ( type ( geometry ) , ) )
3138	def get ( self , app_id , ** queryparams ) : self . app_id = app_id return self . _mc_client . _get ( url = self . _build_path ( app_id ) , ** queryparams )
6015	def output_positions ( positions , positions_path ) : with open ( positions_path , 'w' ) as f : for position in positions : f . write ( "%s\n" % position )
6129	def build_sdist ( sdist_directory , config_settings ) : backend = _build_backend ( ) try : return backend . build_sdist ( sdist_directory , config_settings ) except getattr ( backend , 'UnsupportedOperation' , _DummyException ) : raise GotUnsupportedOperation ( traceback . format_exc ( ) )
1708	def connect ( command , data = None , env = None , cwd = None ) : command_str = expand_args ( command ) . pop ( ) environ = dict ( os . environ ) environ . update ( env or { } ) process = subprocess . Popen ( command_str , universal_newlines = True , shell = False , env = environ , stdin = subprocess . PIPE , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = 0 , cwd = cwd , ) return ConnectedCommand ( process = process )
8739	def _allocate_from_v6_subnet ( self , context , net_id , subnet , port_id , reuse_after , ip_address = None , ** kwargs ) : LOG . info ( "Attempting to allocate a v6 address - [{0}]" . format ( utils . pretty_kwargs ( network_id = net_id , subnet = subnet , port_id = port_id , ip_address = ip_address ) ) ) if ip_address : LOG . info ( "IP %s explicitly requested, deferring to standard " "allocation" % ip_address ) return self . _allocate_from_subnet ( context , net_id = net_id , subnet = subnet , port_id = port_id , reuse_after = reuse_after , ip_address = ip_address , ** kwargs ) else : mac = kwargs . get ( "mac_address" ) if mac : mac = kwargs [ "mac_address" ] . get ( "address" ) if subnet and subnet [ "ip_policy" ] : ip_policy_cidrs = subnet [ "ip_policy" ] . get_cidrs_ip_set ( ) else : ip_policy_cidrs = netaddr . IPSet ( [ ] ) for tries , ip_address in enumerate ( generate_v6 ( mac , port_id , subnet [ "cidr" ] ) ) : LOG . info ( "Attempt {0} of {1}" . format ( tries + 1 , CONF . QUARK . v6_allocation_attempts ) ) if tries > CONF . QUARK . v6_allocation_attempts - 1 : LOG . info ( "Exceeded v6 allocation attempts, bailing" ) raise ip_address_failure ( net_id ) ip_address = netaddr . IPAddress ( ip_address ) . ipv6 ( ) LOG . info ( "Generated a new v6 address {0}" . format ( str ( ip_address ) ) ) if ( ip_policy_cidrs is not None and ip_address in ip_policy_cidrs ) : LOG . info ( "Address {0} excluded by policy" . format ( str ( ip_address ) ) ) continue try : with context . session . begin ( ) : address = db_api . ip_address_create ( context , address = ip_address , subnet_id = subnet [ "id" ] , version = subnet [ "ip_version" ] , network_id = net_id , address_type = kwargs . get ( 'address_type' , ip_types . FIXED ) ) return address except db_exception . DBDuplicateEntry : LOG . info ( "{0} exists but was already " "allocated" . format ( str ( ip_address ) ) ) LOG . debug ( "Duplicate entry found when inserting subnet_id" " %s ip_address %s" , subnet [ "id" ] , ip_address )
11342	def load_config ( filename = None , section_option_dict = { } ) : config = ConfigParser ( ) config . read ( filename ) working_dict = _prepare_working_dict ( config , section_option_dict ) tmp_dict = { } for section , options in working_dict . iteritems ( ) : tmp_dict [ section ] = { } for option in options : tmp_dict [ section ] [ option ] = config . get ( section , option ) return Bunch ( tmp_dict )
13339	def transpose ( a , axes = None ) : if isinstance ( a , np . ndarray ) : return np . transpose ( a , axes ) elif isinstance ( a , RemoteArray ) : return a . transpose ( * axes ) elif isinstance ( a , Remote ) : return _remote_to_array ( a ) . transpose ( * axes ) elif isinstance ( a , DistArray ) : if axes is None : axes = range ( a . ndim - 1 , - 1 , - 1 ) axes = list ( axes ) if len ( set ( axes ) ) < len ( axes ) : raise ValueError ( "repeated axis in transpose" ) if sorted ( axes ) != list ( range ( a . ndim ) ) : raise ValueError ( "axes don't match array" ) distaxis = a . _distaxis new_distaxis = axes . index ( distaxis ) new_subarrays = [ ra . transpose ( * axes ) for ra in a . _subarrays ] return DistArray ( new_subarrays , new_distaxis ) else : return np . transpose ( a , axes )
6421	def encode ( self , word ) : word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = word . translate ( { 198 : 'AE' , 338 : 'OE' } ) word = '' . join ( c for c in word if c in self . _uc_set ) for rule in self . _rule_order : regex , repl = self . _rule_table [ rule ] if isinstance ( regex , text_type ) : word = word . replace ( regex , repl ) else : word = regex . sub ( repl , word ) return word
6212	def plane_xz ( size = ( 10 , 10 ) , resolution = ( 10 , 10 ) ) -> VAO : sx , sz = size rx , rz = resolution dx , dz = sx / rx , sz / rz ox , oz = - sx / 2 , - sz / 2 def gen_pos ( ) : for z in range ( rz ) : for x in range ( rx ) : yield ox + x * dx yield 0 yield oz + z * dz def gen_uv ( ) : for z in range ( rz ) : for x in range ( rx ) : yield x / ( rx - 1 ) yield 1 - z / ( rz - 1 ) def gen_normal ( ) : for _ in range ( rx * rz ) : yield 0.0 yield 1.0 yield 0.0 def gen_index ( ) : for z in range ( rz - 1 ) : for x in range ( rx - 1 ) : yield z * rz + x + 1 yield z * rz + x yield z * rz + x + rx yield z * rz + x + 1 yield z * rz + x + rx yield z * rz + x + rx + 1 pos_data = numpy . fromiter ( gen_pos ( ) , dtype = numpy . float32 ) uv_data = numpy . fromiter ( gen_uv ( ) , dtype = numpy . float32 ) normal_data = numpy . fromiter ( gen_normal ( ) , dtype = numpy . float32 ) index_data = numpy . fromiter ( gen_index ( ) , dtype = numpy . uint32 ) vao = VAO ( "plane_xz" , mode = moderngl . TRIANGLES ) vao . buffer ( pos_data , '3f' , [ 'in_position' ] ) vao . buffer ( uv_data , '2f' , [ 'in_uv' ] ) vao . buffer ( normal_data , '3f' , [ 'in_normal' ] ) vao . index_buffer ( index_data , index_element_size = 4 ) return vao
12776	def forward_dynamics ( self , torques , start = 0 , states = None ) : if states is not None : self . skeleton . set_body_states ( states ) for frame_no , torque in enumerate ( torques ) : if frame_no < start : continue if frame_no >= end : break self . ode_space . collide ( None , self . on_collision ) self . skeleton . add_torques ( torque ) self . ode_world . step ( self . dt ) yield self . ode_contactgroup . empty ( )
4016	def get_app_volume_mounts ( app_name , assembled_specs , test = False ) : app_spec = assembled_specs [ 'apps' ] [ app_name ] volumes = [ get_command_files_volume_mount ( app_name , test = test ) ] volumes . append ( get_asset_volume_mount ( app_name ) ) repo_mount = _get_app_repo_volume_mount ( app_spec ) if repo_mount : volumes . append ( repo_mount ) volumes += _get_app_libs_volume_mounts ( app_name , assembled_specs ) return volumes
7274	def play_pause ( self ) : self . _player_interface . PlayPause ( ) self . _is_playing = not self . _is_playing if self . _is_playing : self . playEvent ( self ) else : self . pauseEvent ( self )
8317	def parse_balanced_image ( self , markup ) : opened = 0 closed = 0 for i in range ( len ( markup ) ) : if markup [ i ] == "[" : opened += 1 if markup [ i ] == "]" : closed += 1 if opened == closed : return markup [ : i + 1 ] return markup
16	def value ( self , t ) : for ( l_t , l ) , ( r_t , r ) in zip ( self . _endpoints [ : - 1 ] , self . _endpoints [ 1 : ] ) : if l_t <= t and t < r_t : alpha = float ( t - l_t ) / ( r_t - l_t ) return self . _interpolation ( l , r , alpha ) assert self . _outside_value is not None return self . _outside_value
193	def OneOf ( children , name = None , deterministic = False , random_state = None ) : return SomeOf ( n = 1 , children = children , random_order = False , name = name , deterministic = deterministic , random_state = random_state )
3326	def acquire ( self , url , lock_type , lock_scope , lock_depth , lock_owner , timeout , principal , token_list , ) : url = normalize_lock_root ( url ) self . _lock . acquire_write ( ) try : self . _check_lock_permission ( url , lock_type , lock_scope , lock_depth , token_list , principal ) return self . _generate_lock ( principal , lock_type , lock_scope , lock_depth , lock_owner , url , timeout ) finally : self . _lock . release ( )
9617	def PlugIn ( self ) : ids = self . available_ids ( ) if len ( ids ) == 0 : raise MaxInputsReachedError ( 'Max Inputs Reached' ) self . id = ids [ 0 ] _xinput . PlugIn ( self . id ) while self . id in self . available_ids ( ) : pass
12849	def watch_method ( self , method_name , callback ) : try : method = getattr ( self , method_name ) except AttributeError : raise ApiUsageError ( ) if not isinstance ( method , Token . WatchedMethod ) : setattr ( self , method_name , Token . WatchedMethod ( method ) ) method = getattr ( self , method_name ) method . add_watcher ( callback )
3826	async def get_self_info ( self , get_self_info_request ) : response = hangouts_pb2 . GetSelfInfoResponse ( ) await self . _pb_request ( 'contacts/getselfinfo' , get_self_info_request , response ) return response
12374	def take_snapshot ( droplet , name ) : print "powering off" droplet . power_off ( ) droplet . wait ( ) print "taking snapshot" droplet . take_snapshot ( name ) droplet . wait ( ) snapshots = droplet . snapshots ( ) print "Current snapshots" print snapshots
1794	def NEG ( cpu , dest ) : source = dest . read ( ) res = dest . write ( - source ) cpu . _calculate_logic_flags ( dest . size , res ) cpu . CF = source != 0 cpu . AF = ( res & 0x0f ) != 0x00
1342	def samples ( dataset = 'imagenet' , index = 0 , batchsize = 1 , shape = ( 224 , 224 ) , data_format = 'channels_last' ) : from PIL import Image images , labels = [ ] , [ ] basepath = os . path . dirname ( __file__ ) samplepath = os . path . join ( basepath , 'data' ) files = os . listdir ( samplepath ) for idx in range ( index , index + batchsize ) : i = idx % 20 file = [ n for n in files if '{}_{:02d}_' . format ( dataset , i ) in n ] [ 0 ] label = int ( file . split ( '.' ) [ 0 ] . split ( '_' ) [ - 1 ] ) path = os . path . join ( samplepath , file ) image = Image . open ( path ) if dataset == 'imagenet' : image = image . resize ( shape ) image = np . asarray ( image , dtype = np . float32 ) if dataset != 'mnist' and data_format == 'channels_first' : image = np . transpose ( image , ( 2 , 0 , 1 ) ) images . append ( image ) labels . append ( label ) labels = np . array ( labels ) images = np . stack ( images ) return images , labels
976	def _newRepresentation ( self , index , newIndex ) : newRepresentation = self . bucketMap [ index ] . copy ( ) ri = newIndex % self . w newBit = self . random . getUInt32 ( self . n ) newRepresentation [ ri ] = newBit while newBit in self . bucketMap [ index ] or not self . _newRepresentationOK ( newRepresentation , newIndex ) : self . numTries += 1 newBit = self . random . getUInt32 ( self . n ) newRepresentation [ ri ] = newBit return newRepresentation
1670	def ProcessFileData ( filename , file_extension , lines , error , extra_check_functions = None ) : lines = ( [ '// marker so line numbers and indices both start at 1' ] + lines + [ '// marker so line numbers end in a known way' ] ) include_state = _IncludeState ( ) function_state = _FunctionState ( ) nesting_state = NestingState ( ) ResetNolintSuppressions ( ) CheckForCopyright ( filename , lines , error ) ProcessGlobalSuppresions ( lines ) RemoveMultiLineComments ( filename , lines , error ) clean_lines = CleansedLines ( lines ) if file_extension in GetHeaderExtensions ( ) : CheckForHeaderGuard ( filename , clean_lines , error ) for line in range ( clean_lines . NumLines ( ) ) : ProcessLine ( filename , file_extension , clean_lines , line , include_state , function_state , nesting_state , error , extra_check_functions ) FlagCxx11Features ( filename , clean_lines , line , error ) nesting_state . CheckCompletedBlocks ( filename , error ) CheckForIncludeWhatYouUse ( filename , clean_lines , include_state , error ) if _IsSourceExtension ( file_extension ) : CheckHeaderFileIncluded ( filename , include_state , error ) CheckForBadCharacters ( filename , lines , error ) CheckForNewlineAtEOF ( filename , lines , error )
5084	def unlink_learners ( self ) : sap_inactive_learners = self . client . get_inactive_sap_learners ( ) enterprise_customer = self . enterprise_configuration . enterprise_customer if not sap_inactive_learners : LOGGER . info ( 'Enterprise customer {%s} has no SAPSF inactive learners' , enterprise_customer . name ) return provider_id = enterprise_customer . identity_provider tpa_provider = get_identity_provider ( provider_id ) if not tpa_provider : LOGGER . info ( 'Enterprise customer {%s} has no associated identity provider' , enterprise_customer . name ) return None for sap_inactive_learner in sap_inactive_learners : social_auth_user = get_user_from_social_auth ( tpa_provider , sap_inactive_learner [ 'studentID' ] ) if not social_auth_user : continue try : EnterpriseCustomerUser . objects . unlink_user ( enterprise_customer = enterprise_customer , user_email = social_auth_user . email , ) except ( EnterpriseCustomerUser . DoesNotExist , PendingEnterpriseCustomerUser . DoesNotExist ) : LOGGER . info ( 'Learner with email {%s} is not associated with Enterprise Customer {%s}' , social_auth_user . email , enterprise_customer . name )
13490	def reconcile ( self , server ) : if not self . challenge . exists ( server ) : raise Exception ( 'Challenge does not exist on server' ) existing = MapRouletteTaskCollection . from_server ( server , self . challenge ) same = [ ] new = [ ] changed = [ ] deleted = [ ] for task in self . tasks : if task . identifier in [ existing_task . identifier for existing_task in existing . tasks ] : if task == existing . get_by_identifier ( task . identifier ) : same . append ( task ) else : changed . append ( task ) else : new . append ( task ) for task in existing . tasks : if task . identifier not in [ task . identifier for task in self . tasks ] : deleted . append ( task ) if new : newCollection = MapRouletteTaskCollection ( self . challenge , tasks = new ) newCollection . create ( server ) if changed : changedCollection = MapRouletteTaskCollection ( self . challenge , tasks = changed ) changedCollection . update ( server ) if deleted : deletedCollection = MapRouletteTaskCollection ( self . challenge , tasks = deleted ) for task in deletedCollection . tasks : task . status = 'deleted' deletedCollection . update ( server ) return { 'same' : same , 'new' : new , 'changed' : changed , 'deleted' : deleted }
12541	def group_dicom_files ( dicom_paths , hdr_field = 'PatientID' ) : dicom_groups = defaultdict ( list ) try : for dcm in dicom_paths : hdr = dicom . read_file ( dcm ) group_key = getattr ( hdr , hdr_field ) dicom_groups [ group_key ] . append ( dcm ) except KeyError as ke : raise KeyError ( 'Error reading field {} from file {}.' . format ( hdr_field , dcm ) ) from ke return dicom_groups
3133	def update_members ( self , list_id , data ) : self . list_id = list_id if 'members' not in data : raise KeyError ( 'The update must have at least one member' ) else : if not len ( data [ 'members' ] ) <= 500 : raise ValueError ( 'You may only batch sub/unsub 500 members at a time' ) for member in data [ 'members' ] : if 'email_address' not in member : raise KeyError ( 'Each list member must have an email_address' ) check_email ( member [ 'email_address' ] ) if 'status' not in member and 'status_if_new' not in member : raise KeyError ( 'Each list member must have either a status or a status_if_new' ) valid_statuses = [ 'subscribed' , 'unsubscribed' , 'cleaned' , 'pending' ] if 'status' in member and member [ 'status' ] not in valid_statuses : raise ValueError ( 'The list member status must be one of "subscribed", "unsubscribed", "cleaned", or ' '"pending"' ) if 'status_if_new' in member and member [ 'status_if_new' ] not in valid_statuses : raise ValueError ( 'The list member status_if_new must be one of "subscribed", "unsubscribed", ' '"cleaned", or "pending"' ) if 'update_existing' not in data : data [ 'update_existing' ] = False return self . _mc_client . _post ( url = self . _build_path ( list_id ) , data = data )
8651	def get_jobs ( session , job_ids , seo_details , lang ) : get_jobs_data = { 'jobs[]' : job_ids , 'seo_details' : seo_details , 'lang' : lang , } response = make_get_request ( session , 'jobs' , params_data = get_jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise JobsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
7569	def fullcomp ( seq ) : seq = seq . replace ( "A" , 'u' ) . replace ( 'T' , 'v' ) . replace ( 'C' , 'p' ) . replace ( 'G' , 'z' ) . replace ( 'u' , 'T' ) . replace ( 'v' , 'A' ) . replace ( 'p' , 'G' ) . replace ( 'z' , 'C' ) seq = seq . replace ( 'R' , 'u' ) . replace ( 'K' , 'v' ) . replace ( 'Y' , 'b' ) . replace ( 'M' , 'o' ) . replace ( 'u' , 'Y' ) . replace ( 'v' , 'M' ) . replace ( 'b' , 'R' ) . replace ( 'o' , 'K' ) seq = seq . replace ( 'r' , 'u' ) . replace ( 'k' , 'v' ) . replace ( 'y' , 'b' ) . replace ( 'm' , 'o' ) . replace ( 'u' , 'y' ) . replace ( 'v' , 'm' ) . replace ( 'b' , 'r' ) . replace ( 'o' , 'k' ) return seq
8851	def open_file ( self , path , line = None ) : editor = None if path : interpreter , pyserver , args = self . _get_backend_parameters ( ) editor = self . tabWidget . open_document ( path , None , interpreter = interpreter , server_script = pyserver , args = args ) if editor : self . setup_editor ( editor ) self . recent_files_manager . open_file ( path ) self . menu_recents . update_actions ( ) if line is not None : TextHelper ( self . tabWidget . current_widget ( ) ) . goto_line ( line ) return editor
9729	def get_analog_single ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . device_count ) : component_position , device = QRTPacket . _get_exact ( RTAnalogDeviceSingle , data , component_position ) RTAnalogDeviceSamples . format = struct . Struct ( RTAnalogDeviceSamples . format_str % device . channel_count ) component_position , sample = QRTPacket . _get_tuple ( RTAnalogDeviceSamples , data , component_position ) append_components ( ( device , sample ) ) return components
6807	def create_raspbian_vagrant_box ( self ) : r = self . local_renderer r . sudo ( 'adduser --disabled-password --gecos "" vagrant' ) r . sudo ( 'echo "vagrant ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/vagrant' ) r . sudo ( 'chmod 0440 /etc/sudoers.d/vagrant' ) r . sudo ( 'apt-get update' ) r . sudo ( 'apt-get install -y openssh-server' ) r . sudo ( 'mkdir -p /home/vagrant/.ssh' ) r . sudo ( 'chmod 0700 /home/vagrant/.ssh' ) r . sudo ( 'wget --no-check-certificate https://raw.github.com/mitchellh/vagrant/master/keys/vagrant.pub -O /home/vagrant/.ssh/authorized_keys' ) r . sudo ( 'chmod 0600 /home/vagrant/.ssh/authorized_keys' ) r . sudo ( 'chown -R vagrant /home/vagrant/.ssh' ) r . sudo ( "sed -i '/AuthorizedKeysFile/s/^#//g' /etc/ssh/sshd_config" ) r . sudo ( "sed -i '/PasswordAuthentication/s/^#//g' /etc/ssh/sshd_config" ) r . sudo ( "sed -i 's/PasswordAuthentication yes/PasswordAuthentication no/g' /etc/ssh/sshd_config" ) r . sudo ( 'apt-get upgrade' ) r . sudo ( 'apt-get install -y gcc build-essential' ) r . sudo ( 'mkdir /tmp/test' ) r . sudo ( 'cp {libvirt_images_dir}/{raspbian_image} /tmp/test' ) r . sudo ( 'cp {libvirt_boot_dir}/{raspbian_kernel} /tmp/test' ) r . render_to_file ( 'rpi/metadata.json' , '/tmp/test/metadata.json' ) r . render_to_file ( 'rpi/Vagrantfile' , '/tmp/test/Vagrantfile' ) r . sudo ( 'qemu-img convert -f raw -O qcow2 {libvirt_images_dir}/{raspbian_image} {libvirt_images_dir}/{raspbian_image}.qcow2' ) r . sudo ( 'mv {libvirt_images_dir}/{raspbian_image}.qcow2 {libvirt_images_dir}/box.img' ) r . sudo ( 'cd /tmp/test; tar cvzf custom_box.box ./metadata.json ./Vagrantfile ./{raspbian_kernel} ./box.img' )
2816	def convert_maxpool3 ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting pooling ...' ) if names == 'short' : tf_name = 'P' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'kernel_shape' in params : height , width , depth = params [ 'kernel_shape' ] else : height , width , depth = params [ 'kernel_size' ] if 'strides' in params : stride_height , stride_width , stride_depth = params [ 'strides' ] else : stride_height , stride_width , stride_depth = params [ 'stride' ] if 'pads' in params : padding_h , padding_w , padding_d , _ , _ = params [ 'pads' ] else : padding_h , padding_w , padding_d = params [ 'padding' ] input_name = inputs [ 0 ] if padding_h > 0 and padding_w > 0 and padding_d > 0 : padding_name = tf_name + '_pad' padding_layer = keras . layers . ZeroPadding3D ( padding = ( padding_h , padding_w , padding_d ) , name = padding_name ) layers [ padding_name ] = padding_layer ( layers [ inputs [ 0 ] ] ) input_name = padding_name pooling = keras . layers . MaxPooling3D ( pool_size = ( height , width , depth ) , strides = ( stride_height , stride_width , stride_depth ) , padding = 'valid' , name = tf_name ) layers [ scope_name ] = pooling ( layers [ input_name ] )
2207	def truepath ( path , real = False ) : path = expanduser ( path ) path = expandvars ( path ) if real : path = realpath ( path ) else : path = abspath ( path ) path = normpath ( path ) return path
1711	def _set_name ( self , name ) : if self . own . get ( 'name' ) : self . func_name = name self . own [ 'name' ] [ 'value' ] = Js ( name )
6034	def padded_grid_stack_from_mask_sub_grid_size_and_psf_shape ( cls , mask , sub_grid_size , psf_shape ) : regular_padded_grid = PaddedRegularGrid . padded_grid_from_shape_psf_shape_and_pixel_scale ( shape = mask . shape , psf_shape = psf_shape , pixel_scale = mask . pixel_scale ) sub_padded_grid = PaddedSubGrid . padded_grid_from_mask_sub_grid_size_and_psf_shape ( mask = mask , sub_grid_size = sub_grid_size , psf_shape = psf_shape ) return GridStack ( regular = regular_padded_grid , sub = sub_padded_grid , blurring = np . array ( [ [ 0.0 , 0.0 ] ] ) )
3846	def parse_typing_status_message ( p ) : return TypingStatusMessage ( conv_id = p . conversation_id . id , user_id = from_participantid ( p . sender_id ) , timestamp = from_timestamp ( p . timestamp ) , status = p . type , )
5382	def _build_pipeline_request ( self , task_view ) : job_metadata = task_view . job_metadata job_params = task_view . job_params job_resources = task_view . job_resources task_metadata = task_view . task_descriptors [ 0 ] . task_metadata task_params = task_view . task_descriptors [ 0 ] . task_params task_resources = task_view . task_descriptors [ 0 ] . task_resources script = task_view . job_metadata [ 'script' ] reserved_labels = google_base . build_pipeline_labels ( job_metadata , task_metadata , task_id_pattern = 'task-%d' ) pipeline = _Pipelines . build_pipeline ( project = self . _project , zones = job_resources . zones , min_cores = job_resources . min_cores , min_ram = job_resources . min_ram , disk_size = job_resources . disk_size , boot_disk_size = job_resources . boot_disk_size , preemptible = job_resources . preemptible , accelerator_type = job_resources . accelerator_type , accelerator_count = job_resources . accelerator_count , image = job_resources . image , script_name = script . name , envs = job_params [ 'envs' ] | task_params [ 'envs' ] , inputs = job_params [ 'inputs' ] | task_params [ 'inputs' ] , outputs = job_params [ 'outputs' ] | task_params [ 'outputs' ] , pipeline_name = job_metadata [ 'pipeline-name' ] ) logging_uri = task_resources . logging_path . uri scopes = job_resources . scopes or google_base . DEFAULT_SCOPES pipeline . update ( _Pipelines . build_pipeline_args ( self . _project , script . value , job_params , task_params , reserved_labels , job_resources . preemptible , logging_uri , scopes , job_resources . keep_alive ) ) return pipeline
13813	def MessageToJson ( message , including_default_value_fields = False ) : js = _MessageToJsonObject ( message , including_default_value_fields ) return json . dumps ( js , indent = 2 )
1961	def sys_fsync ( self , fd ) : ret = 0 try : self . files [ fd ] . sync ( ) except IndexError : ret = - errno . EBADF except FdError : ret = - errno . EINVAL return ret
8984	def to_svg ( self , instruction_or_id , i_promise_not_to_change_the_result = False ) : return self . _new_svg_dumper ( lambda : self . instruction_to_svg_dict ( instruction_or_id , not i_promise_not_to_change_the_result ) )
4453	def alias ( self , alias ) : if alias is FIELDNAME : if not self . _field : raise ValueError ( "Cannot use FIELDNAME alias with no field" ) alias = self . _field [ 1 : ] self . _alias = alias return self
2476	def set_lic_comment ( self , doc , comment ) : if self . has_extr_lic ( doc ) : if not self . extr_lic_comment_set : self . extr_lic_comment_set = True if validations . validate_is_free_form_text ( comment ) : self . extr_lic ( doc ) . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'ExtractedLicense::comment' ) else : raise CardinalityError ( 'ExtractedLicense::comment' ) else : raise OrderError ( 'ExtractedLicense::comment' )
3797	def setup_a_alpha_and_derivatives ( self , i , T = None ) : r self . a , self . Tc , self . omega = self . ais [ i ] , self . Tcs [ i ] , self . omegas [ i ]
13844	def process_macros ( self , content : str ) -> str : def _sub ( macro ) : name = macro . group ( 'body' ) params = self . get_options ( macro . group ( 'options' ) ) return self . options [ 'macros' ] . get ( name , '' ) . format_map ( params ) return self . pattern . sub ( _sub , content )
6232	def calc_scene_bbox ( self ) : bbox_min , bbox_max = None , None for node in self . root_nodes : bbox_min , bbox_max = node . calc_global_bbox ( matrix44 . create_identity ( ) , bbox_min , bbox_max ) self . bbox_min = bbox_min self . bbox_max = bbox_max self . diagonal_size = vector3 . length ( self . bbox_max - self . bbox_min )
9246	def compound_changelog ( self ) : self . fetch_and_filter_tags ( ) tags_sorted = self . sort_tags_by_date ( self . filtered_tags ) self . filtered_tags = tags_sorted self . fetch_and_filter_issues_and_pr ( ) log = str ( self . options . frontmatter ) if self . options . frontmatter else u"" log += u"{0}\n\n" . format ( self . options . header ) if self . options . unreleased_only : log += self . generate_unreleased_section ( ) else : log += self . generate_log_for_all_tags ( ) try : with open ( self . options . base ) as fh : log += fh . read ( ) except ( TypeError , IOError ) : pass return log
354	def load_and_assign_npz ( sess = None , name = None , network = None ) : if network is None : raise ValueError ( "network is None." ) if sess is None : raise ValueError ( "session is None." ) if not os . path . exists ( name ) : logging . error ( "file {} doesn't exist." . format ( name ) ) return False else : params = load_npz ( name = name ) assign_params ( sess , params , network ) logging . info ( "[*] Load {} SUCCESS!" . format ( name ) ) return network
3931	def _auth_with_refresh_token ( session , refresh_token ) : token_request_data = { 'client_id' : OAUTH2_CLIENT_ID , 'client_secret' : OAUTH2_CLIENT_SECRET , 'grant_type' : 'refresh_token' , 'refresh_token' : refresh_token , } res = _make_token_request ( session , token_request_data ) return res [ 'access_token' ]
1686	def Split ( self ) : googlename = self . RepositoryName ( ) project , rest = os . path . split ( googlename ) return ( project , ) + os . path . splitext ( rest )
3585	def get_all ( self , cbobjects ) : try : with self . _lock : return [ self . _metadata [ x ] for x in cbobjects ] except KeyError : raise RuntimeError ( 'Failed to find expected metadata for CoreBluetooth object!' )
10485	def _generateFind ( self , ** kwargs ) : for needle in self . _generateChildren ( ) : if needle . _match ( ** kwargs ) : yield needle
67	def extract_from_image ( self , image , pad = True , pad_max = None , prevent_zero_size = True ) : pad_top = 0 pad_right = 0 pad_bottom = 0 pad_left = 0 height , width = image . shape [ 0 ] , image . shape [ 1 ] x1 , x2 , y1 , y2 = self . x1_int , self . x2_int , self . y1_int , self . y2_int fully_within = self . is_fully_within_image ( image ) if fully_within : y1 , y2 = np . clip ( [ y1 , y2 ] , 0 , height - 1 ) x1 , x2 = np . clip ( [ x1 , x2 ] , 0 , width - 1 ) if prevent_zero_size : if abs ( x2 - x1 ) < 1 : x2 = x1 + 1 if abs ( y2 - y1 ) < 1 : y2 = y1 + 1 if pad : if x1 < 0 : pad_left = abs ( x1 ) x2 = x2 + pad_left width = width + pad_left x1 = 0 if y1 < 0 : pad_top = abs ( y1 ) y2 = y2 + pad_top height = height + pad_top y1 = 0 if x2 >= width : pad_right = x2 - width if y2 >= height : pad_bottom = y2 - height paddings = [ pad_top , pad_right , pad_bottom , pad_left ] any_padded = any ( [ val > 0 for val in paddings ] ) if any_padded : if pad_max is None : pad_max = max ( paddings ) image = ia . pad ( image , top = min ( pad_top , pad_max ) , right = min ( pad_right , pad_max ) , bottom = min ( pad_bottom , pad_max ) , left = min ( pad_left , pad_max ) ) return image [ y1 : y2 , x1 : x2 ] else : within_image = ( ( 0 , 0 , 0 , 0 ) <= ( x1 , y1 , x2 , y2 ) < ( width , height , width , height ) ) out_height , out_width = ( y2 - y1 ) , ( x2 - x1 ) nonzero_height = ( out_height > 0 ) nonzero_width = ( out_width > 0 ) if within_image and nonzero_height and nonzero_width : return image [ y1 : y2 , x1 : x2 ] if prevent_zero_size : out_height = 1 out_width = 1 else : out_height = 0 out_width = 0 if image . ndim == 2 : return np . zeros ( ( out_height , out_width ) , dtype = image . dtype ) return np . zeros ( ( out_height , out_width , image . shape [ - 1 ] ) , dtype = image . dtype )
7702	def get_items_by_name ( self , name , case_sensitive = True ) : if not case_sensitive and name : name = name . lower ( ) result = [ ] for item in self . _items : if item . name == name : result . append ( item ) elif item . name is None : continue elif not case_sensitive and item . name . lower ( ) == name : result . append ( item ) return result
9290	def _socket_readlines ( self , blocking = False ) : try : self . sock . setblocking ( 0 ) except socket . error as e : self . logger . error ( "socket error when setblocking(0): %s" % str ( e ) ) raise ConnectionDrop ( "connection dropped" ) while True : short_buf = b'' newline = b'\r\n' select . select ( [ self . sock ] , [ ] , [ ] , None if blocking else 0 ) try : short_buf = self . sock . recv ( 4096 ) if not short_buf : self . logger . error ( "socket.recv(): returned empty" ) raise ConnectionDrop ( "connection dropped" ) except socket . error as e : self . logger . error ( "socket error on recv(): %s" % str ( e ) ) if "Resource temporarily unavailable" in str ( e ) : if not blocking : if len ( self . buf ) == 0 : break self . buf += short_buf while newline in self . buf : line , self . buf = self . buf . split ( newline , 1 ) yield line
2792	def load ( self ) : data = self . get_data ( "certificates/%s" % self . id ) certificate = data [ "certificate" ] for attr in certificate . keys ( ) : setattr ( self , attr , certificate [ attr ] ) return self
9122	def make_obo_getter ( data_url : str , data_path : str , * , preparsed_path : Optional [ str ] = None , ) -> Callable [ [ Optional [ str ] , bool , bool ] , MultiDiGraph ] : download_function = make_downloader ( data_url , data_path ) def get_obo ( url : Optional [ str ] = None , cache : bool = True , force_download : bool = False ) -> MultiDiGraph : if preparsed_path is not None and os . path . exists ( preparsed_path ) : return read_gpickle ( preparsed_path ) if url is None and cache : url = download_function ( force_download = force_download ) result = obonet . read_obo ( url ) if preparsed_path is not None : write_gpickle ( result , preparsed_path ) return result return get_obo
7448	def _samples_precheck ( self , samples , mystep , force ) : subsample = [ ] for sample in samples : if sample . stats . state < mystep - 1 : LOGGER . debug ( "Sample {} not in proper state." . format ( sample . name ) ) else : subsample . append ( sample ) return subsample
9719	async def take_control ( self , password ) : cmd = "takecontrol %s" % password return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
5051	def commit ( self ) : if self . _child_consents : consents = [ ] for consent in self . _child_consents : consent . granted = self . granted consents . append ( consent . save ( ) or consent ) return ProxyDataSharingConsent . from_children ( self . program_uuid , * consents ) consent , _ = DataSharingConsent . objects . update_or_create ( enterprise_customer = self . enterprise_customer , username = self . username , course_id = self . course_id , defaults = { 'granted' : self . granted } ) self . _exists = consent . exists return consent
11207	def gettz_db_metadata ( ) : warnings . warn ( "zoneinfo.gettz_db_metadata() will be removed in future " "versions, to use the dateutil-provided zoneinfo files, " "ZoneInfoFile object and query the 'metadata' attribute " "instead. See the documentation for details." , DeprecationWarning ) if len ( _CLASS_ZONE_INSTANCE ) == 0 : _CLASS_ZONE_INSTANCE . append ( ZoneInfoFile ( getzoneinfofile_stream ( ) ) ) return _CLASS_ZONE_INSTANCE [ 0 ] . metadata
4855	def _update_transmissions ( self , content_metadata_item_map , transmission_map ) : for content_id , channel_metadata in content_metadata_item_map . items ( ) : transmission = transmission_map [ content_id ] transmission . channel_metadata = channel_metadata transmission . save ( )
9281	def parse_header ( head ) : try : ( fromcall , path ) = head . split ( '>' , 1 ) except : raise ParseError ( "invalid packet header" ) if ( not 1 <= len ( fromcall ) <= 9 or not re . findall ( r"^[a-z0-9]{0,9}(\-[a-z0-9]{1,8})?$" , fromcall , re . I ) ) : raise ParseError ( "fromcallsign is invalid" ) path = path . split ( ',' ) if len ( path [ 0 ] ) == 0 : raise ParseError ( "no tocallsign in header" ) tocall = path [ 0 ] path = path [ 1 : ] validate_callsign ( tocall , "tocallsign" ) for digi in path : if not re . findall ( r"^[A-Z0-9\-]{1,9}\*?$" , digi , re . I ) : raise ParseError ( "invalid callsign in path" ) parsed = { 'from' : fromcall , 'to' : tocall , 'path' : path , } viacall = "" if len ( path ) >= 2 and re . match ( r"^q..$" , path [ - 2 ] ) : viacall = path [ - 1 ] parsed . update ( { 'via' : viacall } ) return parsed
9523	def scaffolds_to_contigs ( infile , outfile , number_contigs = False ) : seq_reader = sequences . file_reader ( infile ) fout = utils . open_file_write ( outfile ) for seq in seq_reader : contigs = seq . contig_coords ( ) counter = 1 for contig in contigs : if number_contigs : name = seq . id + '.' + str ( counter ) counter += 1 else : name = '.' . join ( [ seq . id , str ( contig . start + 1 ) , str ( contig . end + 1 ) ] ) print ( sequences . Fasta ( name , seq [ contig . start : contig . end + 1 ] ) , file = fout ) utils . close ( fout )
10087	def update ( self , * args , ** kwargs ) : super ( Deposit , self ) . update ( * args , ** kwargs )
3838	async def set_group_link_sharing_enabled ( self , set_group_link_sharing_enabled_request ) : response = hangouts_pb2 . SetGroupLinkSharingEnabledResponse ( ) await self . _pb_request ( 'conversations/setgrouplinksharingenabled' , set_group_link_sharing_enabled_request , response ) return response
4406	async def connect ( self ) : await self . _lavalink . bot . wait_until_ready ( ) if self . _ws and self . _ws . open : log . debug ( 'WebSocket still open, closing...' ) await self . _ws . close ( ) user_id = self . _lavalink . bot . user . id shard_count = self . _lavalink . bot . shard_count or self . _shards headers = { 'Authorization' : self . _password , 'Num-Shards' : shard_count , 'User-Id' : str ( user_id ) } log . debug ( 'Preparing to connect to Lavalink' ) log . debug ( ' with URI: {}' . format ( self . _uri ) ) log . debug ( ' with headers: {}' . format ( str ( headers ) ) ) log . info ( 'Connecting to Lavalink...' ) try : self . _ws = await websockets . connect ( self . _uri , loop = self . _loop , extra_headers = headers ) except OSError as error : log . exception ( 'Failed to connect to Lavalink: {}' . format ( str ( error ) ) ) else : log . info ( 'Connected to Lavalink!' ) self . _loop . create_task ( self . listen ( ) ) version = self . _ws . response_headers . get ( 'Lavalink-Major-Version' , 2 ) try : self . _lavalink . _server_version = int ( version ) except ValueError : self . _lavalink . _server_version = 2 log . info ( 'Lavalink server version is {}' . format ( version ) ) if self . _queue : log . info ( 'Replaying {} queued events...' . format ( len ( self . _queue ) ) ) for task in self . _queue : await self . send ( ** task )
5191	def send_direct_operate_command_set ( self , command_set , callback = asiodnp3 . PrintingCommandCallback . Get ( ) , config = opendnp3 . TaskConfig ( ) . Default ( ) ) : self . master . DirectOperate ( command_set , callback , config )
9326	def refresh ( self ) : response = self . __raw = self . _conn . get ( self . url ) self . _populate_fields ( ** response ) self . _loaded = True
91	def derive_random_states ( random_state , n = 1 ) : seed_ = random_state . randint ( SEED_MIN_VALUE , SEED_MAX_VALUE , 1 ) [ 0 ] return [ new_random_state ( seed_ + i ) for i in sm . xrange ( n ) ]
1383	def unregister_watch ( self , uid ) : Log . info ( "Unregister a watch with uid: " + str ( uid ) ) self . watches . pop ( uid , None )
6883	def read_csvlc ( lcfile ) : if '.gz' in os . path . basename ( lcfile ) : LOGINFO ( 'reading gzipped HATLC: %s' % lcfile ) infd = gzip . open ( lcfile , 'rb' ) else : LOGINFO ( 'reading HATLC: %s' % lcfile ) infd = open ( lcfile , 'rb' ) lcformat_check = infd . read ( 12 ) . decode ( ) if 'LCC-CSVLC' in lcformat_check : infd . close ( ) return read_lcc_csvlc ( lcfile ) else : infd . seek ( 0 ) lctext = infd . read ( ) . decode ( ) infd . close ( ) lcstart = lctext . index ( '# LIGHTCURVE\n' ) lcheader = lctext [ : lcstart + 12 ] lccolumns = lctext [ lcstart + 13 : ] . split ( '\n' ) lccolumns = [ x for x in lccolumns if len ( x ) > 0 ] lcdict = _parse_csv_header ( lcheader ) lccolumns = [ x . split ( ',' ) for x in lccolumns ] lccolumns = list ( zip ( * lccolumns ) ) for colind , col in enumerate ( lcdict [ 'columns' ] ) : if ( col . split ( '_' ) [ 0 ] in LC_MAG_COLUMNS or col . split ( '_' ) [ 0 ] in LC_ERR_COLUMNS or col . split ( '_' ) [ 0 ] in LC_FLAG_COLUMNS ) : lcdict [ col ] = np . array ( [ _smartcast ( x , COLUMNDEFS [ col . split ( '_' ) [ 0 ] ] [ 2 ] ) for x in lccolumns [ colind ] ] ) elif col in COLUMNDEFS : lcdict [ col ] = np . array ( [ _smartcast ( x , COLUMNDEFS [ col ] [ 2 ] ) for x in lccolumns [ colind ] ] ) else : LOGWARNING ( 'lcdict col %s has no formatter available' % col ) continue return lcdict
10399	def run_with_graph_transformation ( self ) -> Iterable [ BELGraph ] : yield self . get_remaining_graph ( ) while not self . done_chomping ( ) : while not list ( self . iter_leaves ( ) ) : self . remove_random_edge ( ) yield self . get_remaining_graph ( ) self . score_leaves ( ) yield self . get_remaining_graph ( )
13256	def as_dict ( self ) : entry_dict = { } entry_dict [ 'UUID' ] = self . uuid entry_dict [ 'Creation Date' ] = self . time entry_dict [ 'Time Zone' ] = self . tz if self . tags : entry_dict [ 'Tags' ] = self . tags entry_dict [ 'Entry Text' ] = self . text entry_dict [ 'Starred' ] = self . starred entry_dict [ 'Location' ] = self . location return entry_dict
7082	def fourier_sinusoidal_func ( fourierparams , times , mags , errs ) : period , epoch , famps , fphases = fourierparams forder = len ( famps ) iphase = ( times - epoch ) / period iphase = iphase - np . floor ( iphase ) phasesortind = np . argsort ( iphase ) phase = iphase [ phasesortind ] ptimes = times [ phasesortind ] pmags = mags [ phasesortind ] perrs = errs [ phasesortind ] fseries = [ famps [ x ] * np . cos ( 2.0 * np . pi * x * phase + fphases [ x ] ) for x in range ( forder ) ] modelmags = np . median ( mags ) for fo in fseries : modelmags += fo return modelmags , phase , ptimes , pmags , perrs
4653	def add_required_fees ( self , ops , asset_id = "1.3.0" ) : ws = self . blockchain . rpc fees = ws . get_required_fees ( [ i . json ( ) for i in ops ] , asset_id ) for i , d in enumerate ( ops ) : if isinstance ( fees [ i ] , list ) : ops [ i ] . op . data [ "fee" ] = Asset ( amount = fees [ i ] [ 0 ] [ "amount" ] , asset_id = fees [ i ] [ 0 ] [ "asset_id" ] ) for j , _ in enumerate ( ops [ i ] . op . data [ "proposed_ops" ] . data ) : ops [ i ] . op . data [ "proposed_ops" ] . data [ j ] . data [ "op" ] . op . data [ "fee" ] = Asset ( amount = fees [ i ] [ 1 ] [ j ] [ "amount" ] , asset_id = fees [ i ] [ 1 ] [ j ] [ "asset_id" ] , ) else : ops [ i ] . op . data [ "fee" ] = Asset ( amount = fees [ i ] [ "amount" ] , asset_id = fees [ i ] [ "asset_id" ] ) return ops
9796	def get ( ctx ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) try : response = PolyaxonClient ( ) . experiment_group . get_experiment_group ( user , project_name , _group ) cache . cache ( config_manager = GroupManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get experiment group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_group_details ( response )
12841	def _close ( self , conn ) : super ( PooledAIODatabase , self ) . _close ( conn ) for waiter in self . _waiters : if not waiter . done ( ) : logger . debug ( 'Release a waiter' ) waiter . set_result ( True ) break
1222	def processed_shape ( self , shape ) : for processor in self . preprocessors : shape = processor . processed_shape ( shape = shape ) return shape
12006	def _read_version ( self , data ) : version = ord ( data [ 0 ] ) if version not in self . VERSIONS : raise Exception ( 'Version not defined: %d' % version ) return version
8167	def run_tenuous ( self ) : with LiveExecution . lock : ns_snapshot = copy . copy ( self . ns ) try : source = self . edited_source self . edited_source = None self . do_exec ( source , ns_snapshot ) self . known_good = source self . call_good_cb ( ) return True , None except Exception as ex : tb = traceback . format_exc ( ) self . call_bad_cb ( tb ) self . ns . clear ( ) self . ns . update ( ns_snapshot ) return False , ex
2696	def get_tiles ( graf , size = 3 ) : keeps = list ( filter ( lambda w : w . word_id > 0 , graf ) ) keeps_len = len ( keeps ) for i in iter ( range ( 0 , keeps_len - 1 ) ) : w0 = keeps [ i ] for j in iter ( range ( i + 1 , min ( keeps_len , i + 1 + size ) ) ) : w1 = keeps [ j ] if ( w1 . idx - w0 . idx ) <= size : yield ( w0 . root , w1 . root , )
8779	def _try_allocate ( self , context , segment_id , network_id ) : LOG . info ( "Attempting to allocate segment for network %s " "segment_id %s segment_type %s" % ( network_id , segment_id , self . segment_type ) ) filter_dict = { "segment_id" : segment_id , "segment_type" : self . segment_type , "do_not_use" : False } available_ranges = db_api . segment_allocation_range_find ( context , scope = db_api . ALL , ** filter_dict ) available_range_ids = [ r [ "id" ] for r in available_ranges ] try : with context . session . begin ( subtransactions = True ) : filter_dict = { "deallocated" : True , "segment_id" : segment_id , "segment_type" : self . segment_type , "segment_allocation_range_ids" : available_range_ids } allocations = db_api . segment_allocation_find ( context , lock_mode = True , ** filter_dict ) . limit ( 100 ) . all ( ) if allocations : allocation = random . choice ( allocations ) update_dict = { "deallocated" : False , "deallocated_at" : None , "network_id" : network_id } allocation = db_api . segment_allocation_update ( context , allocation , ** update_dict ) LOG . info ( "Allocated segment %s for network %s " "segment_id %s segment_type %s" % ( allocation [ "id" ] , network_id , segment_id , self . segment_type ) ) return allocation except Exception : LOG . exception ( "Error in segment reallocation." ) LOG . info ( "Cannot find reallocatable segment for network %s " "segment_id %s segment_type %s" % ( network_id , segment_id , self . segment_type ) )
7403	def above ( self , ref ) : if not self . _valid_ordering_reference ( ref ) : raise ValueError ( "%r can only be moved above instances of %r which %s equals %r." % ( self , self . __class__ , self . order_with_respect_to , self . _get_order_with_respect_to ( ) ) ) if self . order == ref . order : return if self . order > ref . order : o = ref . order else : o = self . get_ordering_queryset ( ) . filter ( order__lt = ref . order ) . aggregate ( Max ( 'order' ) ) . get ( 'order__max' ) or 0 self . to ( o )
13697	def try_read_file ( s ) : try : with open ( s , 'r' ) as f : data = f . read ( ) except FileNotFoundError : return s except EnvironmentError as ex : print_err ( '\nFailed to read file: {}\n {}' . format ( s , ex ) ) return None return data
8517	def _warn_if_not_finite ( X ) : X = np . asanyarray ( X ) if ( X . dtype . char in np . typecodes [ 'AllFloat' ] and not np . isfinite ( X . sum ( ) ) and not np . isfinite ( X ) . all ( ) ) : warnings . warn ( "Result contains NaN, infinity" " or a value too large for %r." % X . dtype , category = UserWarning )
4804	def when_called_with ( self , * some_args , ** some_kwargs ) : if not self . expected : raise TypeError ( 'expected exception not set, raises() must be called first' ) try : self . val ( * some_args , ** some_kwargs ) except BaseException as e : if issubclass ( type ( e ) , self . expected ) : return AssertionBuilder ( str ( e ) , self . description , self . kind ) else : self . _err ( 'Expected <%s> to raise <%s> when called with (%s), but raised <%s>.' % ( self . val . __name__ , self . expected . __name__ , self . _fmt_args_kwargs ( * some_args , ** some_kwargs ) , type ( e ) . __name__ ) ) self . _err ( 'Expected <%s> to raise <%s> when called with (%s).' % ( self . val . __name__ , self . expected . __name__ , self . _fmt_args_kwargs ( * some_args , ** some_kwargs ) ) )
5428	def _validate_job_and_task_arguments ( job_params , task_descriptors ) : if not task_descriptors : return task_params = task_descriptors [ 0 ] . task_params from_jobs = { label . name for label in job_params [ 'labels' ] } from_tasks = { label . name for label in task_params [ 'labels' ] } intersect = from_jobs & from_tasks if intersect : raise ValueError ( 'Names for labels on the command-line and in the --tasks file must not ' 'be repeated: {}' . format ( ',' . join ( intersect ) ) ) from_jobs = { item . name for item in job_params [ 'envs' ] | job_params [ 'inputs' ] | job_params [ 'outputs' ] } from_tasks = { item . name for item in task_params [ 'envs' ] | task_params [ 'inputs' ] | task_params [ 'outputs' ] } intersect = from_jobs & from_tasks if intersect : raise ValueError ( 'Names for envs, inputs, and outputs on the command-line and in the ' '--tasks file must not be repeated: {}' . format ( ',' . join ( intersect ) ) )
5389	def _datetime_in_range ( self , dt , dt_min = None , dt_max = None ) : dt = dt . replace ( microsecond = 0 ) if dt_min : dt_min = dt_min . replace ( microsecond = 0 ) else : dt_min = dsub_util . replace_timezone ( datetime . datetime . min , pytz . utc ) if dt_max : dt_max = dt_max . replace ( microsecond = 0 ) else : dt_max = dsub_util . replace_timezone ( datetime . datetime . max , pytz . utc ) return dt_min <= dt <= dt_max
4763	def soft_fail ( msg = '' ) : global _soft_ctx if _soft_ctx : global _soft_err _soft_err . append ( 'Fail: %s!' % msg if msg else 'Fail!' ) return fail ( msg )
11351	def merge_from_list ( self , list_args ) : def xs ( name , parser_args , list_args ) : for args , kwargs in list_args : if len ( set ( args ) & parser_args ) > 0 : yield args , kwargs else : if 'dest' in kwargs : if kwargs [ 'dest' ] == name : yield args , kwargs for args , kwargs in xs ( self . name , self . parser_args , list_args ) : self . merge_args ( args ) self . merge_kwargs ( kwargs )
5299	def get_context_data ( self , ** kwargs ) : data = super ( BaseCalendarMonthView , self ) . get_context_data ( ** kwargs ) year = self . get_year ( ) month = self . get_month ( ) date = _date_from_string ( year , self . get_year_format ( ) , month , self . get_month_format ( ) ) cal = Calendar ( self . get_first_of_week ( ) ) month_calendar = [ ] now = datetime . datetime . utcnow ( ) date_lists = defaultdict ( list ) multidate_objs = [ ] for obj in data [ 'object_list' ] : obj_date = self . get_start_date ( obj ) end_date_field = self . get_end_date_field ( ) if end_date_field : end_date = self . get_end_date ( obj ) if end_date and end_date != obj_date : multidate_objs . append ( { 'obj' : obj , 'range' : [ x for x in daterange ( obj_date , end_date ) ] } ) continue date_lists [ obj_date ] . append ( obj ) for week in cal . monthdatescalendar ( date . year , date . month ) : week_range = set ( daterange ( week [ 0 ] , week [ 6 ] ) ) week_events = [ ] for val in multidate_objs : intersect_length = len ( week_range . intersection ( val [ 'range' ] ) ) if intersect_length : slot = 1 width = intersect_length nowrap_previous = True nowrap_next = True if val [ 'range' ] [ 0 ] >= week [ 0 ] : slot = 1 + ( val [ 'range' ] [ 0 ] - week [ 0 ] ) . days else : nowrap_previous = False if val [ 'range' ] [ - 1 ] > week [ 6 ] : nowrap_next = False week_events . append ( { 'event' : val [ 'obj' ] , 'slot' : slot , 'width' : width , 'nowrap_previous' : nowrap_previous , 'nowrap_next' : nowrap_next , } ) week_calendar = { 'events' : week_events , 'date_list' : [ ] , } for day in week : week_calendar [ 'date_list' ] . append ( { 'day' : day , 'events' : date_lists [ day ] , 'today' : day == now . date ( ) , 'is_current_month' : day . month == date . month , } ) month_calendar . append ( week_calendar ) data [ 'calendar' ] = month_calendar data [ 'weekdays' ] = [ DAYS [ x ] for x in cal . iterweekdays ( ) ] data [ 'month' ] = date data [ 'next_month' ] = self . get_next_month ( date ) data [ 'previous_month' ] = self . get_previous_month ( date ) return data
2348	def seed_url ( self ) : url = self . base_url if self . URL_TEMPLATE is not None : url = urlparse . urljoin ( self . base_url , self . URL_TEMPLATE . format ( ** self . url_kwargs ) ) if not url : return None url_parts = list ( urlparse . urlparse ( url ) ) query = urlparse . parse_qsl ( url_parts [ 4 ] ) for k , v in self . url_kwargs . items ( ) : if v is None : continue if "{{{}}}" . format ( k ) not in str ( self . URL_TEMPLATE ) : for i in iterable ( v ) : query . append ( ( k , i ) ) url_parts [ 4 ] = urlencode ( query ) return urlparse . urlunparse ( url_parts )
3219	def get_network_acls ( vpc , ** conn ) : route_tables = describe_network_acls ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , ** conn ) nacl_ids = [ ] for r in route_tables : nacl_ids . append ( r [ "NetworkAclId" ] ) return nacl_ids
4848	def _partition_items ( self , channel_metadata_item_map ) : items_to_create = { } items_to_update = { } items_to_delete = { } transmission_map = { } export_content_ids = channel_metadata_item_map . keys ( ) for transmission in self . _get_transmissions ( ) : transmission_map [ transmission . content_id ] = transmission if transmission . content_id not in export_content_ids : items_to_delete [ transmission . content_id ] = transmission . channel_metadata for item in channel_metadata_item_map . values ( ) : content_id = item . content_id channel_metadata = item . channel_metadata transmitted_item = transmission_map . get ( content_id , None ) if transmitted_item is not None : if diff ( channel_metadata , transmitted_item . channel_metadata ) : items_to_update [ content_id ] = channel_metadata else : items_to_create [ content_id ] = channel_metadata LOGGER . info ( 'Preparing to transmit creation of [%s] content metadata items with plugin configuration [%s]: [%s]' , len ( items_to_create ) , self . enterprise_configuration , items_to_create . keys ( ) , ) LOGGER . info ( 'Preparing to transmit update of [%s] content metadata items with plugin configuration [%s]: [%s]' , len ( items_to_update ) , self . enterprise_configuration , items_to_update . keys ( ) , ) LOGGER . info ( 'Preparing to transmit deletion of [%s] content metadata items with plugin configuration [%s]: [%s]' , len ( items_to_delete ) , self . enterprise_configuration , items_to_delete . keys ( ) , ) return items_to_create , items_to_update , items_to_delete , transmission_map
3346	def guess_mime_type ( url ) : ( mimetype , _mimeencoding ) = mimetypes . guess_type ( url ) if not mimetype : ext = os . path . splitext ( url ) [ 1 ] mimetype = _MIME_TYPES . get ( ext ) _logger . debug ( "mimetype({}): {}" . format ( url , mimetype ) ) if not mimetype : mimetype = "application/octet-stream" return mimetype
7200	def create_leaflet_viewer ( self , idaho_image_results , filename ) : description = self . describe_images ( idaho_image_results ) if len ( description ) > 0 : functionstring = '' for catid , images in description . items ( ) : for partnum , part in images [ 'parts' ] . items ( ) : num_images = len ( list ( part . keys ( ) ) ) partname = None if num_images == 1 : partname = [ p for p in list ( part . keys ( ) ) ] [ 0 ] pan_image_id = '' elif num_images == 2 : partname = [ p for p in list ( part . keys ( ) ) if p is not 'PAN' ] [ 0 ] pan_image_id = part [ 'PAN' ] [ 'id' ] if not partname : self . logger . debug ( "Cannot find part for idaho image." ) continue bandstr = { 'RGBN' : '0,1,2' , 'WORLDVIEW_8_BAND' : '4,2,1' , 'PAN' : '0' } . get ( partname , '0,1,2' ) part_boundstr_wkt = part [ partname ] [ 'boundstr' ] part_polygon = from_wkt ( part_boundstr_wkt ) bucketname = part [ partname ] [ 'bucket' ] image_id = part [ partname ] [ 'id' ] W , S , E , N = part_polygon . bounds functionstring += "addLayerToMap('%s','%s',%s,%s,%s,%s,'%s');\n" % ( bucketname , image_id , W , S , E , N , pan_image_id ) __location__ = os . path . realpath ( os . path . join ( os . getcwd ( ) , os . path . dirname ( __file__ ) ) ) try : with open ( os . path . join ( __location__ , 'leafletmap_template.html' ) , 'r' ) as htmlfile : data = htmlfile . read ( ) . decode ( "utf8" ) except AttributeError : with open ( os . path . join ( __location__ , 'leafletmap_template.html' ) , 'r' ) as htmlfile : data = htmlfile . read ( ) data = data . replace ( 'FUNCTIONSTRING' , functionstring ) data = data . replace ( 'CENTERLAT' , str ( S ) ) data = data . replace ( 'CENTERLON' , str ( W ) ) data = data . replace ( 'BANDS' , bandstr ) data = data . replace ( 'TOKEN' , self . gbdx_connection . access_token ) with codecs . open ( filename , 'w' , 'utf8' ) as outputfile : self . logger . debug ( "Saving %s" % filename ) outputfile . write ( data ) else : print ( 'No items returned.' )
11316	def update_title_to_proceeding ( self ) : titles = record_get_field_instances ( self . record , tag = "245" ) for title in titles : subs = field_get_subfields ( title ) new_subs = [ ] if "a" in subs : new_subs . append ( ( "a" , subs [ 'a' ] [ 0 ] ) ) if "b" in subs : new_subs . append ( ( "c" , subs [ 'b' ] [ 0 ] ) ) record_add_field ( self . record , tag = "111" , subfields = new_subs ) record_delete_fields ( self . record , tag = "245" ) record_delete_fields ( self . record , tag = "246" )
8823	def start_rpc_listeners ( self ) : self . _setup_rpc ( ) if not self . endpoints : return [ ] self . conn = n_rpc . create_connection ( ) self . conn . create_consumer ( self . topic , self . endpoints , fanout = False ) return self . conn . consume_in_threads ( )
943	def _getModelCheckpointDir ( experimentDir , checkpointLabel ) : checkpointDir = os . path . join ( getCheckpointParentDir ( experimentDir ) , checkpointLabel + g_defaultCheckpointExtension ) checkpointDir = os . path . abspath ( checkpointDir ) return checkpointDir
10	def save_policy ( self , path ) : with open ( path , 'wb' ) as f : pickle . dump ( self . policy , f )
7963	def _feed_reader ( self , data ) : IN_LOGGER . debug ( "IN: %r" , data ) if data : self . lock . release ( ) try : self . _reader . feed ( data ) finally : self . lock . acquire ( ) else : self . _eof = True self . lock . release ( ) try : self . _stream . stream_eof ( ) finally : self . lock . acquire ( ) if not self . _serializer : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" )
6266	def stop ( self ) -> float : self . stop_time = time . time ( ) return self . stop_time - self . start_time - self . offset
11028	def _sse_content_with_protocol ( response , handler , ** sse_kwargs ) : protocol = SseProtocol ( handler , ** sse_kwargs ) finished = protocol . when_finished ( ) response . deliverBody ( protocol ) return finished , protocol
5920	def fit ( self , xy = False , ** kwargs ) : kwargs . setdefault ( 's' , self . tpr ) kwargs . setdefault ( 'n' , self . ndx ) kwargs [ 'f' ] = self . xtc force = kwargs . pop ( 'force' , self . force ) if xy : fitmode = 'rotxy+transxy' kwargs . pop ( 'fit' , None ) infix_default = '_fitxy' else : fitmode = kwargs . pop ( 'fit' , 'rot+trans' ) infix_default = '_fit' dt = kwargs . get ( 'dt' ) if dt : infix_default += '_dt{0:d}ps' . format ( int ( dt ) ) kwargs . setdefault ( 'o' , self . outfile ( self . infix_filename ( None , self . xtc , infix_default , 'xtc' ) ) ) fitgroup = kwargs . pop ( 'fitgroup' , 'backbone' ) kwargs . setdefault ( 'input' , [ fitgroup , "system" ] ) if kwargs . get ( 'center' , False ) : logger . warn ( "Transformer.fit(): center=%(center)r used: centering should not be combined with fitting." , kwargs ) if len ( kwargs [ 'inputs' ] ) != 3 : logger . error ( "If you insist on centering you must provide three groups in the 'input' kwarg: (center, fit, output)" ) raise ValuError ( "Insufficient index groups for centering,fitting,output" ) logger . info ( "Fitting trajectory %r to with xy=%r..." , kwargs [ 'f' ] , xy ) logger . info ( "Fitting on index group %(fitgroup)r" , vars ( ) ) with utilities . in_dir ( self . dirname ) : if self . check_file_exists ( kwargs [ 'o' ] , resolve = "indicate" , force = force ) : logger . warn ( "File %r exists; force regenerating it with force=True." , kwargs [ 'o' ] ) else : gromacs . trjconv ( fit = fitmode , ** kwargs ) logger . info ( "Fitted trajectory (fitmode=%s): %r." , fitmode , kwargs [ 'o' ] ) return { 'tpr' : self . rp ( kwargs [ 's' ] ) , 'xtc' : self . rp ( kwargs [ 'o' ] ) }
9754	def get ( ctx , job ) : def get_experiment ( ) : try : response = PolyaxonClient ( ) . experiment . get_experiment ( user , project_name , _experiment ) cache . cache ( config_manager = ExperimentManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load experiment `{}` info.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_experiment_details ( response ) def get_experiment_job ( ) : try : response = PolyaxonClient ( ) . experiment_job . get_job ( user , project_name , _experiment , _job ) cache . cache ( config_manager = ExperimentJobManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . resources : get_resources ( response . resources . to_dict ( ) , header = "Job resources:" ) response = Printer . add_status_color ( response . to_light_dict ( humanize_values = True , exclude_attrs = [ 'uuid' , 'definition' , 'experiment' , 'unique_name' , 'resources' ] ) ) Printer . print_header ( "Job info:" ) dict_tabulate ( response ) user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if job : _job = get_experiment_job_or_local ( job ) get_experiment_job ( ) else : get_experiment ( )
6577	def from_json ( cls , api_client , data ) : self = cls ( api_client ) PandoraModel . populate_fields ( api_client , self , data ) return self
10645	def create ( dataset , symbol , degree ) : x_vals = dataset . data [ 'T' ] . tolist ( ) y_vals = dataset . data [ symbol ] . tolist ( ) coeffs = np . polyfit ( x_vals , y_vals , degree ) result = PolynomialModelT ( dataset . material , dataset . names_dict [ symbol ] , symbol , dataset . display_symbols_dict [ symbol ] , dataset . units_dict [ symbol ] , None , [ dataset . name ] , coeffs ) result . state_schema [ 'T' ] [ 'min' ] = float ( min ( x_vals ) ) result . state_schema [ 'T' ] [ 'max' ] = float ( max ( x_vals ) ) return result
10148	def from_schema_mapping ( self , schema_mapping ) : responses = { } for status , response_schema in schema_mapping . items ( ) : response = { } if response_schema . description : response [ 'description' ] = response_schema . description else : raise CorniceSwaggerException ( 'Responses must have a description.' ) for field_schema in response_schema . children : location = field_schema . name if location == 'body' : title = field_schema . __class__ . __name__ if title == 'body' : title = response_schema . __class__ . __name__ + 'Body' field_schema . title = title response [ 'schema' ] = self . definitions . from_schema ( field_schema ) elif location in ( 'header' , 'headers' ) : header_schema = self . type_converter ( field_schema ) headers = header_schema . get ( 'properties' ) if headers : for header in headers . values ( ) : header . pop ( 'title' ) response [ 'headers' ] = headers pointer = response_schema . __class__ . __name__ if self . ref : response = self . _ref ( response , pointer ) responses [ status ] = response return responses
7806	def verify_jid_against_srv_name ( self , jid , srv_type ) : srv_prefix = u"_" + srv_type + u"." srv_prefix_l = len ( srv_prefix ) for srv in self . alt_names . get ( "SRVName" , [ ] ) : logger . debug ( "checking {0!r} against {1!r}" . format ( jid , srv ) ) if not srv . startswith ( srv_prefix ) : logger . debug ( "{0!r} does not start with {1!r}" . format ( srv , srv_prefix ) ) continue try : srv_jid = JID ( srv [ srv_prefix_l : ] ) except ValueError : continue if srv_jid == jid : logger . debug ( "Match!" ) return True return False
5169	def __netjson_protocol ( self , radio ) : htmode = radio . get ( 'htmode' ) hwmode = radio . get ( 'hwmode' , None ) if htmode . startswith ( 'HT' ) : return '802.11n' elif htmode . startswith ( 'VHT' ) : return '802.11ac' return '802.{0}' . format ( hwmode )
2988	def ensure_iterable ( inst ) : if isinstance ( inst , string_types ) : return [ inst ] elif not isinstance ( inst , collections . Iterable ) : return [ inst ] else : return inst
7492	def random_product ( iter1 , iter2 ) : pool1 = tuple ( iter1 ) pool2 = tuple ( iter2 ) ind1 = random . sample ( pool1 , 2 ) ind2 = random . sample ( pool2 , 2 ) return tuple ( ind1 + ind2 )
12529	def load_command_table ( self , args ) : with CommandSuperGroup ( __name__ , self , 'rcctl.custom_cluster#{}' ) as super_group : with super_group . group ( 'cluster' ) as group : group . command ( 'select' , 'select' ) with CommandSuperGroup ( __name__ , self , 'rcctl.custom_reliablecollections#{}' , client_factory = client_create ) as super_group : with super_group . group ( 'dictionary' ) as group : group . command ( 'query' , 'query_reliabledictionary' ) group . command ( 'execute' , 'execute_reliabledictionary' ) group . command ( 'schema' , 'get_reliabledictionary_schema' ) group . command ( 'list' , 'get_reliabledictionary_list' ) group . command ( 'type-schema' , 'get_reliabledictionary_type_schema' ) with ArgumentsContext ( self , 'dictionary' ) as ac : ac . argument ( 'application_name' , options_list = [ '--application-name' , '-a' ] ) ac . argument ( 'service_name' , options_list = [ '--service-name' , '-s' ] ) ac . argument ( 'dictionary_name' , options_list = [ '--dictionary-name' , '-d' ] ) ac . argument ( 'output_file' , options_list = [ '--output-file' , '-out' ] ) ac . argument ( 'input_file' , options_list = [ '--input-file' , '-in' ] ) ac . argument ( 'query_string' , options_list = [ '--query-string' , '-q' ] ) ac . argument ( 'type_name' , options_list = [ '--type-name' , '-t' ] ) return OrderedDict ( self . command_table )
4014	def register_consumer ( ) : global _consumers hostname , port = request . form [ 'hostname' ] , request . form [ 'port' ] app_name = _app_name_from_forwarding_info ( hostname , port ) containers = get_dusty_containers ( [ app_name ] , include_exited = True ) if not containers : raise ValueError ( 'No container exists for app {}' . format ( app_name ) ) container = containers [ 0 ] new_id = uuid1 ( ) new_consumer = Consumer ( container [ 'Id' ] , datetime . utcnow ( ) ) _consumers [ str ( new_id ) ] = new_consumer response = jsonify ( { 'app_name' : app_name , 'consumer_id' : new_id } ) response . headers [ 'Access-Control-Allow-Origin' ] = '*' response . headers [ 'Access-Control-Allow-Methods' ] = 'GET, POST' return response
6632	def islast ( generator ) : next_x = None first = True for x in generator : if not first : yield ( next_x , False ) next_x = x first = False if not first : yield ( next_x , True )
8305	def live_source_load ( self , source ) : source = source . rstrip ( '\n' ) if source != self . source : self . source = source b64_source = base64 . b64encode ( bytes ( bytearray ( source , "ascii" ) ) ) self . send_command ( CMD_LOAD_BASE64 , b64_source )
3243	def get_rules ( security_group , ** kwargs ) : rules = security_group . pop ( 'security_group_rules' , [ ] ) for rule in rules : rule [ 'ip_protocol' ] = rule . pop ( 'protocol' ) rule [ 'from_port' ] = rule . pop ( 'port_range_max' ) rule [ 'to_port' ] = rule . pop ( 'port_range_min' ) rule [ 'cidr_ip' ] = rule . pop ( 'remote_ip_prefix' ) rule [ 'rule_type' ] = rule . pop ( 'direction' ) security_group [ 'rules' ] = sorted ( rules ) return security_group
12966	def allOnlyIndexedFields ( self ) : matchedKeys = self . getPrimaryKeys ( ) if matchedKeys : return self . getMultipleOnlyIndexedFields ( matchedKeys ) return IRQueryableList ( [ ] , mdl = self . mdl )
2530	def parse_creation_info ( self , ci_term ) : for _s , _p , o in self . graph . triples ( ( ci_term , self . spdx_namespace [ 'creator' ] , None ) ) : try : ent = self . builder . create_entity ( self . doc , six . text_type ( o ) ) self . builder . add_creator ( self . doc , ent ) except SPDXValueError : self . value_error ( 'CREATOR_VALUE' , o ) for _s , _p , o in self . graph . triples ( ( ci_term , self . spdx_namespace [ 'created' ] , None ) ) : try : self . builder . set_created_date ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'CREATED_VALUE' , o ) except CardinalityError : self . more_than_one_error ( 'created' ) break for _s , _p , o in self . graph . triples ( ( ci_term , RDFS . comment , None ) ) : try : self . builder . set_creation_comment ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'CreationInfo comment' ) break for _s , _p , o in self . graph . triples ( ( ci_term , self . spdx_namespace [ 'licenseListVersion' ] , None ) ) : try : self . builder . set_lics_list_ver ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'licenseListVersion' ) break except SPDXValueError : self . value_error ( 'LL_VALUE' , o )
5758	def get_package_counts ( package_descriptors , targets , repos_data ) : counts = { } for target in targets : counts [ target ] = [ 0 ] * len ( repos_data ) for package_descriptor in package_descriptors . values ( ) : debian_pkg_name = package_descriptor . debian_pkg_name for target in targets : for i , repo_data in enumerate ( repos_data ) : version = repo_data . get ( target , { } ) . get ( debian_pkg_name , None ) if version : counts [ target ] [ i ] += 1 return counts
7623	def melody ( ref , est , ** kwargs ) : r namespace = 'pitch_contour' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_times , ref_p = ref . to_event_values ( ) est_times , est_p = est . to_event_values ( ) ref_freq = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in ref_p ] ) est_freq = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in est_p ] ) return mir_eval . melody . evaluate ( ref_times , ref_freq , est_times , est_freq , ** kwargs )
12908	def load ( cls , fh ) : dat = fh . read ( ) try : ret = cls . from_json ( dat ) except : ret = cls . from_yaml ( dat ) return ret
6762	def write_pgpass ( self , name = None , site = None , use_sudo = 0 , root = 0 ) : r = self . database_renderer ( name = name , site = site ) root = int ( root ) use_sudo = int ( use_sudo ) r . run ( 'touch {pgpass_path}' ) if '~' in r . env . pgpass_path : r . run ( 'chmod {pgpass_chmod} {pgpass_path}' ) else : r . sudo ( 'chmod {pgpass_chmod} {pgpass_path}' ) if root : r . env . shell_username = r . env . get ( 'db_root_username' , 'postgres' ) r . env . shell_password = r . env . get ( 'db_root_password' , 'password' ) else : r . env . shell_username = r . env . db_user r . env . shell_password = r . env . db_password r . append ( '{db_host}:{port}:*:{shell_username}:{shell_password}' , r . env . pgpass_path , use_sudo = use_sudo )
11719	def pipelines ( self ) : if not self . response : return set ( ) elif self . _pipelines is None and self . response : self . _pipelines = set ( ) for group in self . response . payload : for pipeline in group [ 'pipelines' ] : self . _pipelines . add ( pipeline [ 'name' ] ) return self . _pipelines
8902	def _multiple_self_ref_fk_check ( class_model ) : self_fk = [ ] for f in class_model . _meta . concrete_fields : if f . related_model in self_fk : return True if f . related_model == class_model : self_fk . append ( class_model ) return False
2231	def lookup ( self , data ) : query_hash_type = data . __class__ key = ( query_hash_type . __module__ , query_hash_type . __name__ ) try : hash_type , hash_func = self . keyed_extensions [ key ] except KeyError : raise TypeError ( 'No registered hash func for hashable type=%r' % ( query_hash_type ) ) return hash_func
6304	def find_effect_class ( self , path ) -> Type [ Effect ] : package_name , class_name = parse_package_string ( path ) if package_name : package = self . get_package ( package_name ) return package . find_effect_class ( class_name , raise_for_error = True ) for package in self . packages : effect_cls = package . find_effect_class ( class_name ) if effect_cls : return effect_cls raise EffectError ( "No effect class '{}' found in any packages" . format ( class_name ) )
8701	def close ( self ) : try : if self . baud != self . start_baud : self . __set_baudrate ( self . start_baud ) self . _port . flush ( ) self . __clear_buffers ( ) except serial . serialutil . SerialException : pass log . debug ( 'closing port' ) self . _port . close ( )
1449	def get_all_zk_state_managers ( conf ) : state_managers = [ ] state_locations = conf . get_state_locations_of_type ( "zookeeper" ) for location in state_locations : name = location [ 'name' ] hostport = location [ 'hostport' ] hostportlist = [ ] for hostportpair in hostport . split ( ',' ) : host = None port = None if ':' in hostport : hostandport = hostportpair . split ( ':' ) if len ( hostandport ) == 2 : host = hostandport [ 0 ] port = int ( hostandport [ 1 ] ) if not host or not port : raise Exception ( "Hostport for %s must be of the format 'host:port'." % ( name ) ) hostportlist . append ( ( host , port ) ) tunnelhost = location [ 'tunnelhost' ] rootpath = location [ 'rootpath' ] LOG . info ( "Connecting to zk hostports: " + str ( hostportlist ) + " rootpath: " + rootpath ) state_manager = ZkStateManager ( name , hostportlist , rootpath , tunnelhost ) state_managers . append ( state_manager ) return state_managers
12868	def register ( self , model ) : self . models [ model . _meta . table_name ] = model model . _meta . database = self . database return model
3201	def update ( self , campaign_id , data ) : self . campaign_id = campaign_id if 'settings' not in data : raise KeyError ( 'The campaign must have settings' ) if 'subject_line' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a subject_line' ) if 'from_name' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a from_name' ) if 'reply_to' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a reply_to' ) check_email ( data [ 'settings' ] [ 'reply_to' ] ) return self . _mc_client . _patch ( url = self . _build_path ( campaign_id ) , data = data )
11815	def decode ( self , ciphertext ) : "Search for a decoding of the ciphertext." self . ciphertext = ciphertext problem = PermutationDecoderProblem ( decoder = self ) return search . best_first_tree_search ( problem , lambda node : self . score ( node . state ) )
12139	def from_pattern ( cls , pattern , filetype = None , key = 'filename' , root = None , ignore = [ ] ) : filepattern = FilePattern ( key , pattern , root = root ) if FileInfo . filetype and filetype is None : filetype = FileInfo . filetype elif filetype is None : raise Exception ( "The filetype argument must be supplied unless " "an appropriate default has been specified as " "FileInfo.filetype" ) return FileInfo ( filepattern , key , filetype , ignore = ignore )
2978	def cmd_partition ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) if opts . random : if opts . partitions : raise BlockadeError ( "Either specify individual partitions " "or --random, but not both" ) b . random_partition ( ) else : partitions = [ ] for partition in opts . partitions : names = [ ] for name in partition . split ( "," ) : name = name . strip ( ) if name : names . append ( name ) partitions . append ( names ) if not partitions : raise BlockadeError ( "Either specify individual partitions " "or random" ) b . partition ( partitions )
823	def addInstance ( self , groundTruth , prediction , record = None , result = None ) : self . value = self . avg ( prediction )
10433	def selectlastrow ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) cell = object_handle . AXRows [ - 1 ] if not cell . AXSelected : object_handle . activate ( ) cell . AXSelected = True else : pass return 1
12080	def figure_protocol ( self ) : self . log . debug ( "creating overlayed protocols plot" ) self . figure ( ) plt . plot ( self . abf . protoX , self . abf . protoY , color = 'r' ) self . marginX = 0 self . decorate ( protocol = True )
11852	def parse ( self , words , S = 'S' ) : self . chart = [ [ ] for i in range ( len ( words ) + 1 ) ] self . add_edge ( [ 0 , 0 , 'S_' , [ ] , [ S ] ] ) for i in range ( len ( words ) ) : self . scanner ( i , words [ i ] ) return self . chart
13170	def path ( self , include_root = False ) : path = '%s[%d]' % ( self . tagname , self . index or 0 ) p = self . parent while p is not None : if p . parent or include_root : path = '%s[%d]/%s' % ( p . tagname , p . index or 0 , path ) p = p . parent return path
7156	def get_operator ( self , op ) : if op in self . OPERATORS : return self . OPERATORS . get ( op ) try : n_args = len ( inspect . getargspec ( op ) [ 0 ] ) if n_args != 2 : raise TypeError except : eprint ( 'Error: invalid operator function. Operators must accept two args.' ) raise else : return op
12368	def records ( self , name ) : if self . get ( name ) : return DomainRecords ( self . api , name )
2721	def get_object ( cls , api_token , droplet_id ) : droplet = cls ( token = api_token , id = droplet_id ) droplet . load ( ) return droplet
7919	def __from_unicode ( cls , data , check = True ) : parts1 = data . split ( u"/" , 1 ) parts2 = parts1 [ 0 ] . split ( u"@" , 1 ) if len ( parts2 ) == 2 : local = parts2 [ 0 ] domain = parts2 [ 1 ] if check : local = cls . __prepare_local ( local ) domain = cls . __prepare_domain ( domain ) else : local = None domain = parts2 [ 0 ] if check : domain = cls . __prepare_domain ( domain ) if len ( parts1 ) == 2 : resource = parts1 [ 1 ] if check : resource = cls . __prepare_resource ( parts1 [ 1 ] ) else : resource = None if not domain : raise JIDError ( "Domain is required in JID." ) return ( local , domain , resource )
4020	def _init_docker_vm ( ) : if not _dusty_vm_exists ( ) : log_to_client ( 'Initializing new Dusty VM with Docker Machine' ) machine_options = [ '--driver' , 'virtualbox' , '--virtualbox-cpu-count' , '-1' , '--virtualbox-boot2docker-url' , constants . CONFIG_BOOT2DOCKER_URL , '--virtualbox-memory' , str ( get_config_value ( constants . CONFIG_VM_MEM_SIZE ) ) , '--virtualbox-hostonly-nictype' , constants . VM_NIC_TYPE ] check_call_demoted ( [ 'docker-machine' , 'create' ] + machine_options + [ constants . VM_MACHINE_NAME ] , redirect_stderr = True )
12309	def auto_add ( repo , autooptions , files ) : mapping = { "." : "" } if ( ( 'import' in autooptions ) and ( 'directory-mapping' in autooptions [ 'import' ] ) ) : mapping = autooptions [ 'import' ] [ 'directory-mapping' ] keys = mapping . keys ( ) keys = sorted ( keys , key = lambda k : len ( k ) , reverse = True ) count = 0 params = [ ] for f in files : relativepath = f for k in keys : v = mapping [ k ] if f . startswith ( k + "/" ) : relativepath = f . replace ( k + "/" , v ) break count += files_add ( repo = repo , args = [ f ] , targetdir = os . path . dirname ( relativepath ) ) return count
1498	def process_incoming_tuples ( self ) : if self . output_helper . is_out_queue_available ( ) : self . _read_tuples_and_execute ( ) self . output_helper . send_out_tuples ( ) else : self . bolt_metrics . update_out_queue_full_count ( )
12516	def get_h5file ( file_path , mode = 'r' ) : if not op . exists ( file_path ) : raise IOError ( 'Could not find file {}.' . format ( file_path ) ) try : h5file = h5py . File ( file_path , mode = mode ) except : raise else : return h5file
8630	def get_projects ( session , query ) : response = make_get_request ( session , 'projects' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
6121	def elliptical_annular ( cls , shape , pixel_scale , inner_major_axis_radius_arcsec , inner_axis_ratio , inner_phi , outer_major_axis_radius_arcsec , outer_axis_ratio , outer_phi , centre = ( 0.0 , 0.0 ) , invert = False ) : mask = mask_util . mask_elliptical_annular_from_shape_pixel_scale_and_radius ( shape , pixel_scale , inner_major_axis_radius_arcsec , inner_axis_ratio , inner_phi , outer_major_axis_radius_arcsec , outer_axis_ratio , outer_phi , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
3718	def estimate ( self ) : self . mul ( 300 ) self . Cpig ( 300 ) estimates = { 'Tb' : self . Tb ( self . counts ) , 'Tm' : self . Tm ( self . counts ) , 'Tc' : self . Tc ( self . counts , self . Tb_estimated ) , 'Pc' : self . Pc ( self . counts , self . atom_count ) , 'Vc' : self . Vc ( self . counts ) , 'Hf' : self . Hf ( self . counts ) , 'Gf' : self . Gf ( self . counts ) , 'Hfus' : self . Hfus ( self . counts ) , 'Hvap' : self . Hvap ( self . counts ) , 'mul' : self . mul , 'mul_coeffs' : self . calculated_mul_coeffs , 'Cpig' : self . Cpig , 'Cpig_coeffs' : self . calculated_Cpig_coeffs } return estimates
13503	def create ( self , server ) : return server . post ( 'challenge_admin' , self . as_payload ( ) , replacements = { 'slug' : self . slug } )
9622	def buttons ( self ) : return [ name for name , value in rController . _buttons . items ( ) if self . gamepad . wButtons & value == value ]
12292	def annotate_metadata_code ( repo , files ) : package = repo . package package [ 'code' ] = [ ] for p in files : matching_files = glob2 . glob ( "**/{}" . format ( p ) ) for f in matching_files : absf = os . path . abspath ( f ) print ( "Add commit data for {}" . format ( f ) ) package [ 'code' ] . append ( OrderedDict ( [ ( 'script' , f ) , ( 'permalink' , repo . manager . permalink ( repo , absf ) ) , ( 'mimetypes' , mimetypes . guess_type ( absf ) [ 0 ] ) , ( 'sha256' , compute_sha256 ( absf ) ) ] ) )
10835	def all ( self ) : response = self . api . get ( url = PATHS [ 'GET_PROFILES' ] ) for raw_profile in response : self . append ( Profile ( self . api , raw_profile ) ) return self
6180	def merge_DA_ph_times ( ph_times_d , ph_times_a ) : ph_times = np . hstack ( [ ph_times_d , ph_times_a ] ) a_em = np . hstack ( [ np . zeros ( ph_times_d . size , dtype = np . bool ) , np . ones ( ph_times_a . size , dtype = np . bool ) ] ) index_sort = ph_times . argsort ( ) return ph_times [ index_sort ] , a_em [ index_sort ]
2135	def associate_notification_template ( self , workflow , notification_template , status ) : return self . _assoc ( 'notification_templates_%s' % status , workflow , notification_template )
5137	def add ( self , string , start , end , line ) : if string . strip ( ) : self . start_lineno = min ( self . start_lineno , start [ 0 ] ) self . end_lineno = max ( self . end_lineno , end [ 0 ] )
1683	def PrintErrorCounts ( self ) : for category , count in sorted ( iteritems ( self . errors_by_category ) ) : self . PrintInfo ( 'Category \'%s\' errors found: %d\n' % ( category , count ) ) if self . error_count > 0 : self . PrintInfo ( 'Total errors found: %d\n' % self . error_count )
7962	def _close ( self ) : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" ) if self . _socket is None : return try : self . _socket . shutdown ( socket . SHUT_RDWR ) except socket . error : pass self . _socket . close ( ) self . _socket = None self . _write_queue . clear ( ) self . _write_queue_cond . notify ( )
3398	def update_costs ( self ) : for var in self . indicators : if var not in self . costs : self . costs [ var ] = var . cost else : if var . _get_primal ( ) > self . integer_threshold : self . costs [ var ] += var . cost self . model . objective . set_linear_coefficients ( self . costs )
11262	def sub ( prev , pattern , repl , * args , ** kw ) : count = 0 if 'count' not in kw else kw . pop ( 'count' ) pattern_obj = re . compile ( pattern , * args , ** kw ) for s in prev : yield pattern_obj . sub ( repl , s , count = count )
5062	def get_enterprise_customer_user ( user_id , enterprise_uuid ) : EnterpriseCustomerUser = apps . get_model ( 'enterprise' , 'EnterpriseCustomerUser' ) try : return EnterpriseCustomerUser . objects . get ( enterprise_customer__uuid = enterprise_uuid , user_id = user_id ) except EnterpriseCustomerUser . DoesNotExist : return None
8904	def _request ( self , endpoint , method = "GET" , lookup = None , data = { } , params = { } , userargs = None , password = None ) : if isinstance ( userargs , dict ) : userargs = "&" . join ( [ "{}={}" . format ( key , val ) for ( key , val ) in iteritems ( userargs ) ] ) if lookup : lookup = lookup + '/' url = urljoin ( urljoin ( self . base_url , endpoint ) , lookup ) auth = ( userargs , password ) if userargs else None resp = requests . request ( method , url , json = data , params = params , auth = auth ) resp . raise_for_status ( ) return resp
10553	def create_helpingmaterial ( project_id , info , media_url = None , file_path = None ) : try : helping = dict ( project_id = project_id , info = info , media_url = None , ) if file_path : files = { 'file' : open ( file_path , 'rb' ) } payload = { 'project_id' : project_id } res = _pybossa_req ( 'post' , 'helpingmaterial' , payload = payload , files = files ) else : res = _pybossa_req ( 'post' , 'helpingmaterial' , payload = helping ) if res . get ( 'id' ) : return HelpingMaterial ( res ) else : return res except : raise
12822	def _path_root ( draw , result_type ) : def tp ( s = '' ) : return _str_to_path ( s , result_type ) if os . name != 'nt' : return tp ( os . sep ) sep = sampled_from ( [ os . sep , os . altsep or os . sep ] ) . map ( tp ) name = _filename ( result_type ) char = characters ( min_codepoint = ord ( "A" ) , max_codepoint = ord ( "z" ) ) . map ( lambda c : tp ( str ( c ) ) ) relative = sep drive = builds ( lambda * x : tp ( ) . join ( x ) , char , just ( tp ( ':' ) ) , sep ) extended = builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '?' ) ) , sep , drive ) network = one_of ( [ builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , name , sep , name , sep ) , builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '?' ) ) , sep , name , sep , name , sep ) , builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '?' ) ) , sep , just ( tp ( 'UNC' ) ) , sep , name , sep , name , sep ) , builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '.' ) ) , sep , name , sep ) , ] ) final = one_of ( relative , drive , extended , network ) return draw ( final )
10249	def highlight_nodes ( graph : BELGraph , nodes : Optional [ Iterable [ BaseEntity ] ] = None , color : Optional [ str ] = None ) : color = color or NODE_HIGHLIGHT_DEFAULT_COLOR for node in nodes if nodes is not None else graph : graph . node [ node ] [ NODE_HIGHLIGHT ] = color
2848	def _check ( self , command , * args ) : ret = command ( self . _ctx , * args ) logger . debug ( 'Called ftdi_{0} and got response {1}.' . format ( command . __name__ , ret ) ) if ret != 0 : raise RuntimeError ( 'ftdi_{0} failed with error {1}: {2}' . format ( command . __name__ , ret , ftdi . get_error_string ( self . _ctx ) ) )
9683	def toggle_laser ( self , state ) : a = self . cnxn . xfer ( [ 0x03 ] ) [ 0 ] sleep ( 10e-3 ) if state : b = self . cnxn . xfer ( [ 0x02 ] ) [ 0 ] else : b = self . cnxn . xfer ( [ 0x03 ] ) [ 0 ] sleep ( 0.1 ) return True if a == 0xF3 and b == 0x03 else False
13491	def yn_prompt ( msg , default = True ) : ret = custom_prompt ( msg , [ "y" , "n" ] , "y" if default else "n" ) if ret == "y" : return True return False
9947	def new_space ( self , name = None , bases = None , formula = None , refs = None ) : space = self . _impl . model . currentspace = self . _impl . new_space ( name = name , bases = get_impls ( bases ) , formula = formula , refs = refs ) return space . interface
11956	def is_bin ( ip ) : try : ip = str ( ip ) if len ( ip ) != 32 : return False dec = int ( ip , 2 ) except ( TypeError , ValueError ) : return False if dec > 4294967295 or dec < 0 : return False return True
743	def writeToFile ( self , f , packed = True ) : schema = self . getSchema ( ) proto = schema . new_message ( ) self . write ( proto ) if packed : proto . write_packed ( f ) else : proto . write ( f )
1779	def AAA ( cpu ) : cpu . AF = Operators . OR ( cpu . AL & 0x0F > 9 , cpu . AF ) cpu . CF = cpu . AF cpu . AH = Operators . ITEBV ( 8 , cpu . AF , cpu . AH + 1 , cpu . AH ) cpu . AL = Operators . ITEBV ( 8 , cpu . AF , cpu . AL + 6 , cpu . AL ) cpu . AL = cpu . AL & 0x0f
2018	def MOD ( self , a , b ) : try : result = Operators . ITEBV ( 256 , b == 0 , 0 , a % b ) except ZeroDivisionError : result = 0 return result
13371	def redirect_to_env_paths ( path ) : with open ( path , 'r' ) as f : redirected = f . read ( ) return shlex . split ( redirected )
6023	def new_psf_with_renormalized_array ( self ) : return PSF ( array = self , pixel_scale = self . pixel_scale , renormalize = True )
3604	def _authenticate ( self , params , headers ) : if self . authentication : user = self . authentication . get_user ( ) params . update ( { 'auth' : user . firebase_auth_token } ) headers . update ( self . authentication . authenticator . HEADERS )
10653	def run ( self , clock , generalLedger ) : for c in self . components : c . run ( clock , generalLedger ) for a in self . activities : a . run ( clock , generalLedger )
5928	def getpath ( self , section , option ) : return os . path . expanduser ( os . path . expandvars ( self . get ( section , option ) ) )
2867	def readU8 ( self , register ) : result = self . _bus . read_byte_data ( self . _address , register ) & 0xFF self . _logger . debug ( "Read 0x%02X from register 0x%02X" , result , register ) return result
9103	def get_long_description ( ) : with codecs . open ( os . path . join ( HERE , 'README.rst' ) , encoding = 'utf-8' ) as f : long_description = f . read ( ) return long_description
5585	def output_is_valid ( self , process_data ) : if self . METADATA [ "data_type" ] == "raster" : return ( is_numpy_or_masked_array ( process_data ) or is_numpy_or_masked_array_with_tags ( process_data ) ) elif self . METADATA [ "data_type" ] == "vector" : return is_feature_list ( process_data )
1245	def import_experience ( self , experiences ) : if isinstance ( experiences , dict ) : if self . unique_state : experiences [ 'states' ] = dict ( state = experiences [ 'states' ] ) if self . unique_action : experiences [ 'actions' ] = dict ( action = experiences [ 'actions' ] ) self . model . import_experience ( ** experiences ) else : if self . unique_state : states = dict ( state = list ( ) ) else : states = { name : list ( ) for name in experiences [ 0 ] [ 'states' ] } internals = [ list ( ) for _ in experiences [ 0 ] [ 'internals' ] ] if self . unique_action : actions = dict ( action = list ( ) ) else : actions = { name : list ( ) for name in experiences [ 0 ] [ 'actions' ] } terminal = list ( ) reward = list ( ) for experience in experiences : if self . unique_state : states [ 'state' ] . append ( experience [ 'states' ] ) else : for name in sorted ( states ) : states [ name ] . append ( experience [ 'states' ] [ name ] ) for n , internal in enumerate ( internals ) : internal . append ( experience [ 'internals' ] [ n ] ) if self . unique_action : actions [ 'action' ] . append ( experience [ 'actions' ] ) else : for name in sorted ( actions ) : actions [ name ] . append ( experience [ 'actions' ] [ name ] ) terminal . append ( experience [ 'terminal' ] ) reward . append ( experience [ 'reward' ] ) self . model . import_experience ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
13854	def append_main_thread ( self ) : thread = MainThread ( main_queue = self . main_queue , main_spider = self . main_spider , branch_spider = self . branch_spider ) thread . daemon = True thread . start ( )
8983	def get_instruction_id ( self , instruction_or_id ) : if isinstance ( instruction_or_id , tuple ) : return _InstructionId ( instruction_or_id ) return _InstructionId ( instruction_or_id . type , instruction_or_id . hex_color )
2251	def color_text ( text , color ) : r if color is None : return text try : import pygments import pygments . console if sys . platform . startswith ( 'win32' ) : import colorama colorama . init ( ) ansi_text = pygments . console . colorize ( color , text ) return ansi_text except ImportError : import warnings warnings . warn ( 'pygments is not installed, text will not be colored' ) return text
5474	def format_pairs ( self , values ) : return ', ' . join ( '%s=%s' % ( key , value ) for key , value in sorted ( values . items ( ) ) )
13075	def create_blueprint ( self ) : self . register_plugins ( ) self . blueprint = Blueprint ( self . name , "nemo" , url_prefix = self . prefix , template_folder = self . template_folder , static_folder = self . static_folder , static_url_path = self . static_url_path ) for url , name , methods , instance in self . _urls : self . blueprint . add_url_rule ( url , view_func = self . view_maker ( name , instance ) , endpoint = _plugin_endpoint_rename ( name , instance ) , methods = methods ) for url , name , methods , instance in self . _semantic_url : self . blueprint . add_url_rule ( url , view_func = self . view_maker ( name , instance ) , endpoint = _plugin_endpoint_rename ( name , instance ) + "_semantic" , methods = methods ) self . register_assets ( ) self . register_filters ( ) self . __templates_namespaces__ . extend ( self . __instance_templates__ ) for namespace , directory in self . __templates_namespaces__ [ : : - 1 ] : if namespace not in self . __template_loader__ : self . __template_loader__ [ namespace ] = [ ] self . __template_loader__ [ namespace ] . append ( jinja2 . FileSystemLoader ( op . abspath ( directory ) ) ) self . blueprint . jinja_loader = jinja2 . PrefixLoader ( { namespace : jinja2 . ChoiceLoader ( paths ) for namespace , paths in self . __template_loader__ . items ( ) } , "::" ) if self . cache is not None : for func , instance in self . cached : setattr ( instance , func . __name__ , self . cache . memoize ( ) ( func ) ) return self . blueprint
2059	def disassemble_instruction ( self , code , pc ) : return next ( self . disasm . disasm ( code , pc ) )
13835	def _ParseOrMerge ( self , lines , message ) : tokenizer = _Tokenizer ( lines ) while not tokenizer . AtEnd ( ) : self . _MergeField ( tokenizer , message )
5043	def send_messages ( cls , http_request , message_requests ) : deduplicated_messages = set ( message_requests ) for msg_type , text in deduplicated_messages : message_function = getattr ( messages , msg_type ) message_function ( http_request , text )
788	def jobInfoWithModels ( self , jobID ) : combinedResults = None with ConnectionFactory . get ( ) as conn : query = ' ' . join ( [ 'SELECT %s.*, %s.*' % ( self . jobsTableName , self . modelsTableName ) , 'FROM %s' % self . jobsTableName , 'LEFT JOIN %s USING(job_id)' % self . modelsTableName , 'WHERE job_id=%s' ] ) conn . cursor . execute ( query , ( jobID , ) ) if conn . cursor . rowcount > 0 : combinedResults = [ ClientJobsDAO . _combineResults ( result , self . _jobs . jobInfoNamedTuple , self . _models . modelInfoNamedTuple ) for result in conn . cursor . fetchall ( ) ] if combinedResults is not None : return combinedResults raise RuntimeError ( "jobID=%s not found within the jobs table" % ( jobID ) )
7593	def get_constants ( self , ** params : keys ) : url = self . api . CONSTANTS return self . _get_model ( url , ** params )
8692	def init ( self ) : self . es . indices . create ( index = self . params [ 'index' ] , ignore = 400 )
9803	def set ( verbose , host , http_port , ws_port , use_https , verify_ssl ) : _config = GlobalConfigManager . get_config_or_default ( ) if verbose is not None : _config . verbose = verbose if host is not None : _config . host = host if http_port is not None : _config . http_port = http_port if ws_port is not None : _config . ws_port = ws_port if use_https is not None : _config . use_https = use_https if verify_ssl is False : _config . verify_ssl = verify_ssl GlobalConfigManager . set_config ( _config ) Printer . print_success ( 'Config was updated.' ) CliConfigManager . purge ( )
9613	def element ( self , using , value ) : return self . _execute ( Command . FIND_CHILD_ELEMENT , { 'using' : using , 'value' : value } )
5272	def _find_lcs ( self , node , stringIdxs ) : nodes = [ self . _find_lcs ( n , stringIdxs ) for ( n , _ ) in node . transition_links if n . generalized_idxs . issuperset ( stringIdxs ) ] if nodes == [ ] : return node deepestNode = max ( nodes , key = lambda n : n . depth ) return deepestNode
12236	def objective ( param_scales = ( 1 , 1 ) , xstar = None , seed = None ) : ndim = len ( param_scales ) def decorator ( func ) : @ wraps ( func ) def wrapper ( theta ) : return func ( theta ) def param_init ( ) : np . random . seed ( seed ) return np . random . randn ( ndim , ) * np . array ( param_scales ) wrapper . ndim = ndim wrapper . param_init = param_init wrapper . xstar = xstar return wrapper return decorator
380	def get_zca_whitening_principal_components_img ( X ) : flatX = np . reshape ( X , ( X . shape [ 0 ] , X . shape [ 1 ] * X . shape [ 2 ] * X . shape [ 3 ] ) ) tl . logging . info ( "zca : computing sigma .." ) sigma = np . dot ( flatX . T , flatX ) / flatX . shape [ 0 ] tl . logging . info ( "zca : computing U, S and V .." ) U , S , _ = linalg . svd ( sigma ) tl . logging . info ( "zca : computing principal components .." ) principal_components = np . dot ( np . dot ( U , np . diag ( 1. / np . sqrt ( S + 10e-7 ) ) ) , U . T ) return principal_components
4286	def write ( self , album ) : page = self . template . render ( ** self . generate_context ( album ) ) output_file = os . path . join ( album . dst_path , album . output_file ) with open ( output_file , 'w' , encoding = 'utf-8' ) as f : f . write ( page )
6670	def task ( * args , ** kwargs ) : precursors = kwargs . pop ( 'precursors' , None ) post_callback = kwargs . pop ( 'post_callback' , False ) if args and callable ( args [ 0 ] ) : return _task ( * args ) def wrapper ( meth ) : if precursors : meth . deploy_before = list ( precursors ) if post_callback : meth . is_post_callback = True return _task ( meth ) return wrapper
13292	def json_attributes ( self , vfuncs = None ) : vfuncs = vfuncs or [ ] js = { 'global' : { } } for k in self . ncattrs ( ) : js [ 'global' ] [ k ] = self . getncattr ( k ) for varname , var in self . variables . items ( ) : js [ varname ] = { } for k in var . ncattrs ( ) : z = var . getncattr ( k ) try : assert not np . isnan ( z ) . all ( ) js [ varname ] [ k ] = z except AssertionError : js [ varname ] [ k ] = None except TypeError : js [ varname ] [ k ] = z for vf in vfuncs : try : js [ varname ] . update ( vfuncs ( var ) ) except BaseException : logger . exception ( "Could not apply custom variable attribue function" ) return json . loads ( json . dumps ( js , cls = BasicNumpyEncoder ) )
4476	def slice_clip ( filename , start , stop , n_samples , sr , mono = True ) : with psf . SoundFile ( str ( filename ) , mode = 'r' ) as soundf : n_target = stop - start soundf . seek ( start ) y = soundf . read ( n_target ) . T if mono : y = librosa . to_mono ( y ) y = librosa . resample ( y , soundf . samplerate , sr ) y = librosa . util . fix_length ( y , n_samples ) return y
9483	def to_bytes_36 ( self , previous : bytes ) : bc = b"" it_bc = util . generate_bytecode_from_obb ( self . iterator , previous ) bc += it_bc bc += util . ensure_instruction ( tokens . GET_ITER )
4953	def ready ( self ) : from enterprise . signals import handle_user_post_save from django . db . models . signals import pre_migrate , post_save post_save . connect ( handle_user_post_save , sender = self . auth_user_model , dispatch_uid = USER_POST_SAVE_DISPATCH_UID ) pre_migrate . connect ( self . _disconnect_user_post_save_for_migrations )
9039	def _dump_to_file ( self , file ) : xmltodict . unparse ( self . object ( ) , file , pretty = True )
2087	def _convert_pagenum ( self , kwargs ) : for key in ( 'next' , 'previous' ) : if not kwargs . get ( key ) : continue match = re . search ( r'page=(?P<num>[\d]+)' , kwargs [ key ] ) if match is None and key == 'previous' : kwargs [ key ] = 1 continue kwargs [ key ] = int ( match . groupdict ( ) [ 'num' ] )
13555	def delete_shifts ( self , shifts ) : url = "/2/shifts/?%s" % urlencode ( { 'ids' : "," . join ( str ( s ) for s in shifts ) } ) data = self . _delete_resource ( url ) return data
10990	def finish_state ( st , desc = 'finish-state' , invert = 'guess' ) : for minmass in [ None , 0 ] : for _ in range ( 3 ) : npart , poses = addsub . add_subtract_locally ( st , region_depth = 7 , minmass = minmass , invert = invert ) if npart == 0 : break opt . finish ( st , n_loop = 1 , separate_psf = True , desc = desc , dowarn = False ) opt . burn ( st , mode = 'polish' , desc = desc , n_loop = 2 , dowarn = False ) d = opt . finish ( st , desc = desc , n_loop = 4 , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' )
7727	def get_items ( self ) : if not self . xmlnode . children : return [ ] ret = [ ] n = self . xmlnode . children while n : ns = n . ns ( ) if ns and ns . getContent ( ) != self . ns : pass elif n . name == "item" : ret . append ( MucItem ( n ) ) elif n . name == "status" : ret . append ( MucStatus ( n ) ) n = n . next return ret
2223	def _hashable_sequence ( data , types = True ) : r hasher = _HashTracer ( ) _update_hasher ( hasher , data , types = types ) return hasher . sequence
13895	def FindFiles ( dir_ , in_filters = None , out_filters = None , recursive = True , include_root_dir = True , standard_paths = False ) : if in_filters is None : in_filters = [ '*' ] if out_filters is None : out_filters = [ ] result = [ ] for dir_root , directories , filenames in os . walk ( dir_ ) : for i_directory in directories [ : ] : if MatchMasks ( i_directory , out_filters ) : directories . remove ( i_directory ) for filename in directories + filenames : if MatchMasks ( filename , in_filters ) and not MatchMasks ( filename , out_filters ) : result . append ( os . path . join ( dir_root , filename ) ) if not recursive : break if not include_root_dir : dir_prefix = len ( dir_ ) + 1 result = [ file [ dir_prefix : ] for file in result ] if standard_paths : result = map ( StandardizePath , result ) return result
1600	def str_cmd ( cmd , cwd , env ) : process = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE , cwd = cwd , env = env ) stdout_builder , stderr_builder = proc . async_stdout_stderr_builder ( process ) process . wait ( ) stdout , stderr = stdout_builder . result ( ) , stderr_builder . result ( ) return { 'command' : ' ' . join ( cmd ) , 'stderr' : stderr , 'stdout' : stdout }
12893	def get_power ( self ) : power = ( yield from self . handle_int ( self . API . get ( 'power' ) ) ) return bool ( power )
1859	def CMPS ( cpu , dest , src ) : src_reg = { 8 : 'SI' , 32 : 'ESI' , 64 : 'RSI' } [ cpu . address_bit_size ] dest_reg = { 8 : 'DI' , 32 : 'EDI' , 64 : 'RDI' } [ cpu . address_bit_size ] base , _ , ty = cpu . get_descriptor ( cpu . DS ) src_addr = cpu . read_register ( src_reg ) + base dest_addr = cpu . read_register ( dest_reg ) + base size = dest . size arg1 = cpu . read_int ( dest_addr , size ) arg0 = cpu . read_int ( src_addr , size ) res = ( arg0 - arg1 ) & ( ( 1 << size ) - 1 ) cpu . _calculate_CMP_flags ( size , res , arg0 , arg1 ) increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( src_reg , cpu . read_register ( src_reg ) + increment ) cpu . write_register ( dest_reg , cpu . read_register ( dest_reg ) + increment )
10335	def build_delete_node_by_hash ( manager : Manager ) -> Callable [ [ BELGraph , str ] , None ] : @ in_place_transformation def delete_node_by_hash ( graph : BELGraph , node_hash : str ) -> None : node = manager . get_dsl_by_hash ( node_hash ) graph . remove_node ( node ) return delete_node_by_hash
4331	def loudness ( self , gain_db = - 10.0 , reference_level = 65.0 ) : if not is_number ( gain_db ) : raise ValueError ( 'gain_db must be a number.' ) if not is_number ( reference_level ) : raise ValueError ( 'reference_level must be a number' ) if reference_level > 75 or reference_level < 50 : raise ValueError ( 'reference_level must be between 50 and 75' ) effect_args = [ 'loudness' , '{:f}' . format ( gain_db ) , '{:f}' . format ( reference_level ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'loudness' ) return self
3998	def copy_from_local ( local_path , remote_name , remote_path , demote = True ) : if not os . path . exists ( local_path ) : raise RuntimeError ( 'ERROR: Path {} does not exist' . format ( local_path ) ) temp_identifier = str ( uuid . uuid1 ( ) ) if os . path . isdir ( local_path ) : sync_local_path_to_vm ( local_path , os . path . join ( vm_cp_path ( remote_name ) , temp_identifier ) , demote = demote ) move_dir_inside_container ( remote_name , os . path . join ( constants . CONTAINER_CP_DIR , temp_identifier ) , remote_path ) else : sync_local_path_to_vm ( local_path , os . path . join ( vm_cp_path ( remote_name ) , temp_identifier ) , demote = demote ) move_file_inside_container ( remote_name , os . path . join ( constants . CONTAINER_CP_DIR , temp_identifier ) , remote_path )
6803	def assume_localhost ( self ) : if not self . genv . host_string : self . genv . host_string = 'localhost' self . genv . hosts = [ 'localhost' ] self . genv . user = getpass . getuser ( )
11408	def record_delete_field ( rec , tag , ind1 = ' ' , ind2 = ' ' , field_position_global = None , field_position_local = None ) : error = _validate_record_field_positions_global ( rec ) if error : pass if tag not in rec : return False ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) deleted = [ ] newfields = [ ] if field_position_global is None and field_position_local is None : for field in rec [ tag ] : if field [ 1 ] != ind1 or field [ 2 ] != ind2 : newfields . append ( field ) else : deleted . append ( field ) rec [ tag ] = newfields elif field_position_global is not None : for field in rec [ tag ] : if ( field [ 1 ] != ind1 and field [ 2 ] != ind2 or field [ 4 ] != field_position_global ) : newfields . append ( field ) else : deleted . append ( field ) rec [ tag ] = newfields elif field_position_local is not None : try : del rec [ tag ] [ field_position_local ] except IndexError : return [ ] if not rec [ tag ] : del rec [ tag ] return deleted
12883	def _run_supervisor ( self ) : import time still_supervising = lambda : ( multiprocessing . active_children ( ) or not self . log_queue . empty ( ) or not self . exception_queue . empty ( ) ) try : while still_supervising ( ) : try : record = self . log_queue . get_nowait ( ) logger = logging . getLogger ( record . name ) logger . handle ( record ) except queue . Empty : pass try : exception = self . exception_queue . get_nowait ( ) except queue . Empty : pass else : raise exception time . sleep ( 1 / self . frame_rate ) self . elapsed_time += 1 / self . frame_rate if self . time_limit and self . elapsed_time > self . time_limit : raise RuntimeError ( "timeout" ) finally : for process in multiprocessing . active_children ( ) : process . terminate ( )
10411	def compare ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Mapping [ str , Mapping [ str , float ] ] : canonical_mechanisms = get_subgraphs_by_annotation ( graph , annotation ) canonical_nodes = _transform_graph_dict_to_node_dict ( canonical_mechanisms ) candidate_mechanisms = generate_bioprocess_mechanisms ( graph ) candidate_nodes = _transform_graph_dict_to_node_dict ( candidate_mechanisms ) results : Dict [ str , Dict [ str , float ] ] = defaultdict ( dict ) it = itt . product ( canonical_nodes . items ( ) , candidate_nodes . items ( ) ) for ( canonical_name , canonical_graph ) , ( candidate_bp , candidate_graph ) in it : tanimoto = tanimoto_set_similarity ( candidate_nodes , canonical_nodes ) results [ canonical_name ] [ candidate_bp ] = tanimoto return dict ( results )
10408	def bond_reduce ( row_a , row_b ) : spanning_cluster = ( 'percolation_probability_mean' in row_a . dtype . names and 'percolation_probability_mean' in row_b . dtype . names and 'percolation_probability_m2' in row_a . dtype . names and 'percolation_probability_m2' in row_b . dtype . names ) ret = np . empty_like ( row_a ) def _reducer ( key , transpose = False ) : mean_key = '{}_mean' . format ( key ) m2_key = '{}_m2' . format ( key ) res = simoa . stats . online_variance ( * [ ( row [ 'number_of_runs' ] , row [ mean_key ] . T if transpose else row [ mean_key ] , row [ m2_key ] . T if transpose else row [ m2_key ] , ) for row in [ row_a , row_b ] ] ) ( ret [ mean_key ] , ret [ m2_key ] , ) = ( res [ 1 ] . T , res [ 2 ] . T , ) if transpose else res [ 1 : ] if spanning_cluster : _reducer ( 'percolation_probability' ) _reducer ( 'max_cluster_size' ) _reducer ( 'moments' , transpose = True ) ret [ 'number_of_runs' ] = row_a [ 'number_of_runs' ] + row_b [ 'number_of_runs' ] return ret
1655	def IsDerivedFunction ( clean_lines , linenum ) : for i in xrange ( linenum , max ( - 1 , linenum - 10 ) , - 1 ) : match = Match ( r'^([^()]*\w+)\(' , clean_lines . elided [ i ] ) if match : line , _ , closing_paren = CloseExpression ( clean_lines , i , len ( match . group ( 1 ) ) ) return ( closing_paren >= 0 and Search ( r'\boverride\b' , line [ closing_paren : ] ) ) return False
12525	def condor_call ( cmd , shell = True ) : log . info ( cmd ) ret = condor_submit ( cmd ) if ret != 0 : subprocess . call ( cmd , shell = shell )
12682	def row ( self , idx ) : return DataFrameRow ( idx , [ x [ idx ] for x in self ] , self . colnames )
2144	def get_prefix ( self , include_version = True ) : host = settings . host if '://' not in host : host = 'https://%s' % host . strip ( '/' ) elif host . startswith ( 'http://' ) and settings . verify_ssl : raise exc . TowerCLIError ( 'Can not verify ssl with non-https protocol. Change the ' 'verify_ssl configuration setting to continue.' ) url_pieces = urlparse ( host ) if url_pieces [ 0 ] not in [ 'http' , 'https' ] : raise exc . ConnectionError ( 'URL must be http(s), {} is not valid' . format ( url_pieces [ 0 ] ) ) prefix = urljoin ( host , '/api/' ) if include_version : prefix = urljoin ( prefix , "{}/" . format ( CUR_API_VERSION ) ) return prefix
456	def ternary_operation ( x ) : g = tf . get_default_graph ( ) with g . gradient_override_map ( { "Sign" : "Identity" } ) : threshold = _compute_threshold ( x ) x = tf . sign ( tf . add ( tf . sign ( tf . add ( x , threshold ) ) , tf . sign ( tf . add ( x , - threshold ) ) ) ) return x
7114	def fit ( self , X , y ) : word_vector_transformer = WordVectorTransformer ( padding = 'max' ) X = word_vector_transformer . fit_transform ( X ) X = LongTensor ( X ) self . word_vector_transformer = word_vector_transformer y_transformer = LabelEncoder ( ) y = y_transformer . fit_transform ( y ) y = torch . from_numpy ( y ) self . y_transformer = y_transformer dataset = CategorizedDataset ( X , y ) dataloader = DataLoader ( dataset , batch_size = self . batch_size , shuffle = True , num_workers = 4 ) KERNEL_SIZES = self . kernel_sizes NUM_KERNEL = self . num_kernel EMBEDDING_DIM = self . embedding_dim model = TextCNN ( vocab_size = word_vector_transformer . get_vocab_size ( ) , embedding_dim = EMBEDDING_DIM , output_size = len ( self . y_transformer . classes_ ) , kernel_sizes = KERNEL_SIZES , num_kernel = NUM_KERNEL ) if USE_CUDA : model = model . cuda ( ) EPOCH = self . epoch LR = self . lr loss_function = nn . CrossEntropyLoss ( ) optimizer = optim . Adam ( model . parameters ( ) , lr = LR ) for epoch in range ( EPOCH ) : losses = [ ] for i , data in enumerate ( dataloader ) : X , y = data X , y = Variable ( X ) , Variable ( y ) optimizer . zero_grad ( ) model . train ( ) output = model ( X ) loss = loss_function ( output , y ) losses . append ( loss . data . tolist ( ) [ 0 ] ) loss . backward ( ) optimizer . step ( ) if i % 100 == 0 : print ( "[%d/%d] mean_loss : %0.2f" % ( epoch , EPOCH , np . mean ( losses ) ) ) losses = [ ] self . model = model
3520	def snapengage ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return SnapEngageNode ( )
13449	def authed_post ( self , url , data , response_code = 200 , follow = False , headers = { } ) : if not self . authed : self . authorize ( ) response = self . client . post ( url , data , follow = follow , ** headers ) self . assertEqual ( response_code , response . status_code ) return response
5060	def get_enterprise_customer ( uuid ) : EnterpriseCustomer = apps . get_model ( 'enterprise' , 'EnterpriseCustomer' ) try : return EnterpriseCustomer . objects . get ( uuid = uuid ) except EnterpriseCustomer . DoesNotExist : return None
660	def percentOutputsStableOverNTimeSteps ( vectors , numSamples = None ) : totalSamples = len ( vectors ) windowSize = numSamples numWindows = 0 pctStable = 0 for wStart in range ( 0 , totalSamples - windowSize + 1 ) : data = vectors [ wStart : wStart + windowSize ] outputSums = data . sum ( axis = 0 ) stableOutputs = ( outputSums == windowSize ) . sum ( ) samplePctStable = float ( stableOutputs ) / data [ 0 ] . sum ( ) print samplePctStable pctStable += samplePctStable numWindows += 1 return float ( pctStable ) / numWindows
8535	def pop ( self , nbytes ) : size = 0 popped = [ ] with self . _lock_packets : while size < nbytes : try : packet = self . _packets . pop ( 0 ) size += len ( packet . data . data ) self . _remaining -= len ( packet . data . data ) popped . append ( packet ) except IndexError : break return popped
3339	def is_equal_or_child_uri ( parentUri , childUri ) : return ( parentUri and childUri and ( childUri . rstrip ( "/" ) + "/" ) . startswith ( parentUri . rstrip ( "/" ) + "/" ) )
6793	def load_django_settings ( self ) : r = self . local_renderer _env = { } save_vars = [ 'ALLOW_CELERY' , 'DJANGO_SETTINGS_MODULE' ] for var_name in save_vars : _env [ var_name ] = os . environ . get ( var_name ) try : if r . env . local_project_dir : sys . path . insert ( 0 , r . env . local_project_dir ) os . environ [ 'ALLOW_CELERY' ] = '0' os . environ [ 'DJANGO_SETTINGS_MODULE' ] = r . format ( r . env . settings_module ) try : import django django . setup ( ) except AttributeError : pass settings = self . get_settings ( ) try : from django . contrib import staticfiles from django . conf import settings as _settings if settings is not None : for k , v in settings . __dict__ . items ( ) : setattr ( _settings , k , v ) else : raise ImportError except ( ImportError , RuntimeError ) : print ( 'Unable to load settings.' ) traceback . print_exc ( ) finally : for var_name , var_value in _env . items ( ) : if var_value is None : del os . environ [ var_name ] else : os . environ [ var_name ] = var_value return settings
2513	def get_file_name ( self , f_term ) : for _ , _ , name in self . graph . triples ( ( f_term , self . spdx_namespace [ 'fileName' ] , None ) ) : return name return
1278	def markdown ( text , escape = True , ** kwargs ) : return Markdown ( escape = escape , ** kwargs ) ( text )
1525	def to_table ( result ) : max_count = 20 table , count = [ ] , 0 for role , envs_topos in result . items ( ) : for env , topos in envs_topos . items ( ) : for topo in topos : count += 1 if count > max_count : continue else : table . append ( [ role , env , topo ] ) header = [ 'role' , 'env' , 'topology' ] rest_count = 0 if count <= max_count else count - max_count return table , header , rest_count
2210	def parse_requirements ( fname = 'requirements.txt' ) : from os . path import dirname , join , exists import re require_fpath = join ( dirname ( __file__ ) , fname ) def parse_line ( line ) : info = { } if line . startswith ( '-e ' ) : info [ 'package' ] = line . split ( '#egg=' ) [ 1 ] else : pat = '(' + '|' . join ( [ '>=' , '==' , '>' ] ) + ')' parts = re . split ( pat , line , maxsplit = 1 ) parts = [ p . strip ( ) for p in parts ] info [ 'package' ] = parts [ 0 ] if len ( parts ) > 1 : op , rest = parts [ 1 : ] if ';' in rest : version , platform_deps = map ( str . strip , rest . split ( ';' ) ) info [ 'platform_deps' ] = platform_deps else : version = rest info [ 'version' ] = ( op , version ) return info if exists ( require_fpath ) : with open ( require_fpath , 'r' ) as f : packages = [ ] for line in f . readlines ( ) : line = line . strip ( ) if line and not line . startswith ( '#' ) : info = parse_line ( line ) package = info [ 'package' ] if not sys . version . startswith ( '3.4' ) : platform_deps = info . get ( 'platform_deps' ) if platform_deps is not None : package += ';' + platform_deps packages . append ( package ) return packages return [ ]
616	def expGenerator ( args ) : parser = OptionParser ( ) parser . set_usage ( "%prog [options] --description='{json object with args}'\n" + "%prog [options] --descriptionFromFile='{filename}'\n" + "%prog [options] --showSchema" ) parser . add_option ( "--description" , dest = "description" , help = "Tells ExpGenerator to generate an experiment description.py and " "permutations.py file using the given JSON formatted experiment " "description string." ) parser . add_option ( "--descriptionFromFile" , dest = 'descriptionFromFile' , help = "Tells ExpGenerator to open the given filename and use it's " "contents as the JSON formatted experiment description." ) parser . add_option ( "--claDescriptionTemplateFile" , dest = 'claDescriptionTemplateFile' , default = 'claDescriptionTemplate.tpl' , help = "The file containing the template description file for " " ExpGenerator [default: %default]" ) parser . add_option ( "--showSchema" , action = "store_true" , dest = "showSchema" , help = "Prints the JSON schemas for the --description arg." ) parser . add_option ( "--version" , dest = 'version' , default = 'v2' , help = "Generate the permutations file for this version of hypersearch." " Possible choices are 'v1' and 'v2' [default: %default]." ) parser . add_option ( "--outDir" , dest = "outDir" , default = None , help = "Where to generate experiment. If not specified, " "then a temp directory will be created" ) ( options , remainingArgs ) = parser . parse_args ( args ) if len ( remainingArgs ) > 0 : raise _InvalidCommandArgException ( _makeUsageErrorStr ( "Unexpected command-line args: <%s>" % ( ' ' . join ( remainingArgs ) , ) , parser . get_usage ( ) ) ) activeOptions = filter ( lambda x : getattr ( options , x ) != None , ( 'description' , 'showSchema' ) ) if len ( activeOptions ) > 1 : raise _InvalidCommandArgException ( _makeUsageErrorStr ( ( "The specified command options are " + "mutually-exclusive: %s" ) % ( activeOptions , ) , parser . get_usage ( ) ) ) if options . showSchema : _handleShowSchemaOption ( ) elif options . description : _handleDescriptionOption ( options . description , options . outDir , parser . get_usage ( ) , hsVersion = options . version , claDescriptionTemplateFile = options . claDescriptionTemplateFile ) elif options . descriptionFromFile : _handleDescriptionFromFileOption ( options . descriptionFromFile , options . outDir , parser . get_usage ( ) , hsVersion = options . version , claDescriptionTemplateFile = options . claDescriptionTemplateFile ) else : raise _InvalidCommandArgException ( _makeUsageErrorStr ( "Error in validating command options. No option " "provided:\n" , parser . get_usage ( ) ) )
6276	def get_loader ( self , meta : ResourceDescription , raise_on_error = False ) -> BaseLoader : for loader in self . _loaders : if loader . name == meta . loader : return loader if raise_on_error : raise ImproperlyConfigured ( "Resource has invalid loader '{}': {}\nAvailiable loaders: {}" . format ( meta . loader , meta , [ loader . name for loader in self . _loaders ] ) )
6610	def getArrays ( self , tree , branchName ) : itsArray = self . _getArray ( tree , branchName ) if itsArray is None : return None , None itsCountArray = self . _getCounterArray ( tree , branchName ) return itsArray , itsCountArray
13626	def Delimited ( value , parser = Text , delimiter = u',' , encoding = None ) : value = Text ( value , encoding ) if value is None or value == u'' : return [ ] return map ( parser , value . split ( delimiter ) )
7010	def skyview_stamp ( ra , decl , survey = 'DSS2 Red' , scaling = 'Linear' , flip = True , convolvewith = None , forcefetch = False , cachedir = '~/.astrobase/stamp-cache' , timeout = 10.0 , retry_failed = False , savewcsheader = True , verbose = False ) : stampdict = get_stamp ( ra , decl , survey = survey , scaling = scaling , forcefetch = forcefetch , cachedir = cachedir , timeout = timeout , retry_failed = retry_failed , verbose = verbose ) if stampdict : stampfits = pyfits . open ( stampdict [ 'fitsfile' ] ) header = stampfits [ 0 ] . header frame = stampfits [ 0 ] . data stampfits . close ( ) if flip : frame = np . flipud ( frame ) if verbose : LOGINFO ( 'fetched stamp successfully for (%.3f, %.3f)' % ( ra , decl ) ) if convolvewith : convolved = aconv . convolve ( frame , convolvewith ) if savewcsheader : return convolved , header else : return convolved else : if savewcsheader : return frame , header else : return frame else : LOGERROR ( 'could not fetch the requested stamp for ' 'coords: (%.3f, %.3f) from survey: %s and scaling: %s' % ( ra , decl , survey , scaling ) ) return None
13124	def object_to_id ( self , obj ) : search = Service . search ( ) search = search . filter ( "term" , address = obj . address ) search = search . filter ( "term" , protocol = obj . protocol ) search = search . filter ( "term" , port = obj . port ) search = search . filter ( "term" , state = obj . state ) if search . count ( ) : result = search [ 0 ] . execute ( ) [ 0 ] return result . meta . id else : return None
6771	def list_required ( self , type = None , service = None ) : from burlap . common import ( required_system_packages , required_python_packages , required_ruby_packages , ) service = ( service or '' ) . strip ( ) . upper ( ) type = ( type or '' ) . lower ( ) . strip ( ) assert not type or type in PACKAGE_TYPES , 'Unknown package type: %s' % ( type , ) packages_set = set ( ) packages = [ ] version = self . os_version for _service , satchel in self . all_other_enabled_satchels . items ( ) : _service = _service . strip ( ) . upper ( ) if service and service != _service : continue _new = [ ] if not type or type == SYSTEM : _new . extend ( required_system_packages . get ( _service , { } ) . get ( ( version . distro , version . release ) , [ ] ) ) try : _pkgs = satchel . packager_system_packages if self . verbose : print ( 'pkgs:' ) pprint ( _pkgs , indent = 4 ) for _key in [ ( version . distro , version . release ) , version . distro ] : if self . verbose : print ( 'checking key:' , _key ) if _key in _pkgs : if self . verbose : print ( 'satchel %s requires:' % satchel , _pkgs [ _key ] ) _new . extend ( _pkgs [ _key ] ) break except AttributeError : pass if not type or type == PYTHON : _new . extend ( required_python_packages . get ( _service , { } ) . get ( ( version . distro , version . release ) , [ ] ) ) try : _pkgs = satchel . packager_python_packages for _key in [ ( version . distro , version . release ) , version . distro ] : if _key in _pkgs : _new . extend ( _pkgs [ _key ] ) except AttributeError : pass print ( '_new:' , _new ) if not type or type == RUBY : _new . extend ( required_ruby_packages . get ( _service , { } ) . get ( ( version . distro , version . release ) , [ ] ) ) for _ in _new : if _ in packages_set : continue packages_set . add ( _ ) packages . append ( _ ) if self . verbose : for package in sorted ( packages ) : print ( 'package:' , package ) return packages
9080	def get_providers ( self , ** kwargs ) : if 'ids' in kwargs : ids = [ self . concept_scheme_uri_map . get ( id , id ) for id in kwargs [ 'ids' ] ] providers = [ self . providers [ k ] for k in self . providers . keys ( ) if k in ids ] else : providers = list ( self . providers . values ( ) ) if 'subject' in kwargs : providers = [ p for p in providers if kwargs [ 'subject' ] in p . metadata [ 'subject' ] ] return providers
7250	def launch_batch_workflow ( self , batch_workflow ) : url = '%(base_url)s/batch_workflows' % { 'base_url' : self . base_url } try : r = self . gbdx_connection . post ( url , json = batch_workflow ) batch_workflow_id = r . json ( ) [ 'batch_workflow_id' ] return batch_workflow_id except TypeError as e : self . logger . debug ( 'Batch Workflow not launched, reason: {0}' . format ( e ) )
9247	def generate_sub_section ( self , issues , prefix ) : log = "" if issues : if not self . options . simple_list : log += u"{0}\n\n" . format ( prefix ) for issue in issues : merge_string = self . get_string_for_issue ( issue ) log += u"- {0}\n" . format ( merge_string ) log += "\n" return log
2169	def get_command ( self , ctx , name ) : if name in misc . __all__ : return getattr ( misc , name ) try : resource = tower_cli . get_resource ( name ) return ResSubcommand ( resource ) except ImportError : pass secho ( 'No such command: %s.' % name , fg = 'red' , bold = True ) sys . exit ( 2 )
2383	def construct_mapping ( self , node , deep = False ) : mapping = super ( ExtendedSafeConstructor , self ) . construct_mapping ( node , deep ) return { ( str ( key ) if isinstance ( key , int ) else key ) : mapping [ key ] for key in mapping }
3452	def find_essential_genes ( model , threshold = None , processes = None ) : if threshold is None : threshold = model . slim_optimize ( error_value = None ) * 1E-02 deletions = single_gene_deletion ( model , method = 'fba' , processes = processes ) essential = deletions . loc [ deletions [ 'growth' ] . isna ( ) | ( deletions [ 'growth' ] < threshold ) , : ] . index return { model . genes . get_by_id ( g ) for ids in essential for g in ids }
4235	def _convert ( value , to_type , default = None ) : try : return default if value is None else to_type ( value ) except ValueError : return default
4722	def trun_enter ( trun ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun::enter" ) trun [ "stamp" ] [ "begin" ] = int ( time . time ( ) ) rcode = 0 for hook in trun [ "hooks" ] [ "enter" ] : rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun::enter { rcode: %r }" % rcode , rcode ) return rcode
10455	def check ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) if object_handle . AXValue == 1 : return 1 self . _grabfocus ( object_handle ) x , y , width , height = self . _getobjectsize ( object_handle ) self . generatemouseevent ( x + width / 2 , y + height / 2 , "b1c" ) return 1
1551	def _get_bolt ( self ) : bolt = topology_pb2 . Bolt ( ) bolt . comp . CopyFrom ( self . _get_base_component ( ) ) self . _add_in_streams ( bolt ) self . _add_out_streams ( bolt ) return bolt
13298	def find_repos ( self , depth = 10 ) : repos = [ ] for root , subdirs , files in walk_dn ( self . root , depth = depth ) : if 'modules' in root : continue if '.git' in subdirs : repos . append ( root ) return repos
11842	def ModelBasedVacuumAgent ( ) : "An agent that keeps track of what locations are clean or dirty." model = { loc_A : None , loc_B : None } def program ( ( location , status ) ) : "Same as ReflexVacuumAgent, except if everything is clean, do NoOp." model [ location ] = status if model [ loc_A ] == model [ loc_B ] == 'Clean' : return 'NoOp' elif status == 'Dirty' : return 'Suck' elif location == loc_A : return 'Right' elif location == loc_B : return 'Left' return Agent ( program )
790	def jobSetStatus ( self , jobID , status , useConnectionID = True , ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET status=%%s, ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE job_id=%%s' % ( self . jobsTableName , ) sqlParams = [ status , jobID ] if useConnectionID : query += ' AND _eng_cjm_conn_id=%s' sqlParams . append ( self . _connectionID ) result = conn . cursor . execute ( query , sqlParams ) if result != 1 : raise RuntimeError ( "Tried to change the status of job %d to %s, but " "this job belongs to some other CJM" % ( jobID , status ) )
9406	def _cleanup ( self ) : self . exit ( ) workspace = osp . join ( os . getcwd ( ) , 'octave-workspace' ) if osp . exists ( workspace ) : os . remove ( workspace )
2505	def get_extr_license_text ( self , extr_lic ) : text_tripples = list ( self . graph . triples ( ( extr_lic , self . spdx_namespace [ 'extractedText' ] , None ) ) ) if not text_tripples : self . error = True msg = 'Extracted license must have extractedText property' self . logger . log ( msg ) return if len ( text_tripples ) > 1 : self . more_than_one_error ( 'extracted license text' ) return text_tripple = text_tripples [ 0 ] _s , _p , text = text_tripple return text
6474	def apply_function ( self , points ) : if not self . option . function : return points if np is None : raise ImportError ( 'numpy is not available' ) if ':' in self . option . function : function , arguments = self . option . function . split ( ':' , 1 ) arguments = arguments . split ( ',' ) else : function = self . option . function arguments = [ ] arguments = list ( map ( self . _function_argument , arguments ) ) filter_function = FUNCTION . get ( function ) if filter_function is None : raise TypeError ( 'Invalid function "%s"' % ( function , ) ) else : return filter_function ( np . array ( list ( points ) ) , * arguments )
11769	def printf ( format , * args ) : sys . stdout . write ( str ( format ) % args ) return if_ ( args , lambda : args [ - 1 ] , lambda : format )
1049	def print_exception ( etype , value , tb , limit = None , file = None ) : if file is None : file = open ( '/dev/stderr' , 'w' ) if tb : _print ( file , 'Traceback (most recent call last):' ) print_tb ( tb , limit , file ) lines = format_exception_only ( etype , value ) for line in lines : _print ( file , line , '' )
4383	def is_denied ( self , role , method , resource ) : return ( role , method , resource ) in self . _denied
11123	def remove_directory ( self , relativePath , removeFromSystem = False ) : relativePath = os . path . normpath ( relativePath ) parentDirInfoDict , errorMessage = self . get_parent_directory_info ( relativePath ) assert parentDirInfoDict is not None , errorMessage path , name = os . path . split ( relativePath ) if dict . __getitem__ ( parentDirInfoDict , 'directories' ) . get ( name , None ) is None : raise Exception ( "'%s' is not a registered directory in repository relative path '%s'" % ( name , path ) ) if removeFromSystem : for rp in self . walk_files_relative_path ( relativePath = relativePath ) : ap = os . path . join ( self . __path , relativePath , rp ) if not os . path . isfile ( ap ) : continue if not os . path . exists ( ap ) : continue if os . path . isfile ( ap ) : os . remove ( ap ) for rp in self . walk_directories_relative_path ( relativePath = relativePath ) : ap = os . path . join ( self . __path , relativePath , rp ) if not os . path . isdir ( ap ) : continue if not os . path . exists ( ap ) : continue if not len ( os . listdir ( ap ) ) : os . rmdir ( ap ) dict . __getitem__ ( parentDirInfoDict , 'directories' ) . pop ( name , None ) ap = os . path . join ( self . __path , relativePath ) if not os . path . isdir ( ap ) : if not len ( os . listdir ( ap ) ) : os . rmdir ( ap ) self . save ( )
4878	def validate ( self , data ) : lms_user_id = data . get ( 'lms_user_id' ) tpa_user_id = data . get ( 'tpa_user_id' ) user_email = data . get ( 'user_email' ) if not lms_user_id and not tpa_user_id and not user_email : raise serializers . ValidationError ( 'At least one of the following fields must be specified and map to an EnterpriseCustomerUser: ' 'lms_user_id, tpa_user_id, user_email' ) return data
4215	def name ( cls ) : parent , sep , mod_name = cls . __module__ . rpartition ( '.' ) mod_name = mod_name . replace ( '_' , ' ' ) return ' ' . join ( [ mod_name , cls . __name__ ] )
3418	def load_matlab_model ( infile_path , variable_name = None , inf = inf ) : if not scipy_io : raise ImportError ( 'load_matlab_model requires scipy' ) data = scipy_io . loadmat ( infile_path ) possible_names = [ ] if variable_name is None : meta_vars = { "__globals__" , "__header__" , "__version__" } possible_names = sorted ( i for i in data if i not in meta_vars ) if len ( possible_names ) == 1 : variable_name = possible_names [ 0 ] if variable_name is not None : return from_mat_struct ( data [ variable_name ] , model_id = variable_name , inf = inf ) for possible_name in possible_names : try : return from_mat_struct ( data [ possible_name ] , model_id = possible_name , inf = inf ) except ValueError : pass raise IOError ( "no COBRA model found" )
10380	def calculate_concordance_probability_by_annotation ( graph , annotation , key , cutoff = None , permutations = None , percentage = None , use_ambiguous = False ) : result = [ ( value , calculate_concordance_probability ( subgraph , key , cutoff = cutoff , permutations = permutations , percentage = percentage , use_ambiguous = use_ambiguous , ) ) for value , subgraph in get_subgraphs_by_annotation ( graph , annotation ) . items ( ) ] return dict ( result )
11803	def nconflicts ( self , var , val , assignment ) : n = len ( self . vars ) c = self . rows [ val ] + self . downs [ var + val ] + self . ups [ var - val + n - 1 ] if assignment . get ( var , None ) == val : c -= 3 return c
12345	def stitch ( self , folder = None ) : debug ( 'stitching ' + self . __str__ ( ) ) if not folder : folder = self . path macros = [ ] files = [ ] for well in self . wells : f , m = stitch_macro ( well , folder ) macros . extend ( m ) files . extend ( f ) chopped_arguments = zip ( chop ( macros , _pools ) , chop ( files , _pools ) ) chopped_filenames = Parallel ( n_jobs = _pools ) ( delayed ( fijibin . macro . run ) ( macro = arg [ 0 ] , output_files = arg [ 1 ] ) for arg in chopped_arguments ) return [ f for list_ in chopped_filenames for f in list_ ]
8507	def _get_param_names ( self ) : template = Template ( self . yaml_string ) names = [ 'yaml_string' ] for match in re . finditer ( template . pattern , template . template ) : name = match . group ( 'named' ) or match . group ( 'braced' ) assert name is not None names . append ( name ) return names
12528	def upload ( ctx , repo ) : artifacts = ' ' . join ( shlex . quote ( str ( n ) ) for n in ROOT . joinpath ( 'dist' ) . glob ( 'pipfile[-_]cli-*' ) ) ctx . run ( f'twine upload --repository="{repo}" {artifacts}' )
3613	def filterchain_all ( request , app , model , field , foreign_key_app_name , foreign_key_model_name , foreign_key_field_name , value ) : model_class = get_model ( app , model ) keywords = get_keywords ( field , value ) foreign_model_class = get_model ( foreign_key_app_name , foreign_key_model_name ) if not any ( [ ( isinstance ( f , ChainedManyToManyField ) or isinstance ( f , ChainedForeignKey ) ) for f in foreign_model_class . _meta . get_fields ( ) ] ) : raise PermissionDenied ( "Smart select disallowed" ) limit_choices_to = get_limit_choices_to ( foreign_key_app_name , foreign_key_model_name , foreign_key_field_name ) queryset = get_queryset ( model_class , limit_choices_to = limit_choices_to ) filtered = list ( do_filter ( queryset , keywords ) ) if not getattr ( model_class . _meta , 'ordering' , False ) : sort_results ( list ( filtered ) ) excluded = list ( do_filter ( queryset , keywords , exclude = True ) ) if not getattr ( model_class . _meta , 'ordering' , False ) : sort_results ( list ( excluded ) ) empty_choice = { 'value' : "" , 'display' : "---------" } serialized_results = ( serialize_results ( filtered ) + [ empty_choice ] + serialize_results ( excluded ) ) return JsonResponse ( serialized_results , safe = False )
12510	def get_img_data ( image , copy = True ) : try : img = check_img ( image ) if copy : return get_data ( img ) else : return img . get_data ( ) except Exception as exc : raise Exception ( 'Error when reading file {0}.' . format ( repr_imgs ( image ) ) ) from exc
10912	def vectorize_damping ( params , damping = 1.0 , increase_list = [ [ 'psf-' , 1e4 ] ] ) : damp_vec = np . ones ( len ( params ) ) * damping for nm , fctr in increase_list : for a in range ( damp_vec . size ) : if nm in params [ a ] : damp_vec [ a ] *= fctr return damp_vec
986	def mmPrettyPrintConnections ( self ) : text = "" text += ( "Segments: (format => " "(#) [(source cell=permanence ...), ...]\n" ) text += "------------------------------------\n" columns = range ( self . numberOfColumns ( ) ) for column in columns : cells = self . cellsForColumn ( column ) for cell in cells : segmentDict = dict ( ) for seg in self . connections . segmentsForCell ( cell ) : synapseList = [ ] for synapse in self . connections . synapsesForSegment ( seg ) : synapseData = self . connections . dataForSynapse ( synapse ) synapseList . append ( ( synapseData . presynapticCell , synapseData . permanence ) ) synapseList . sort ( ) synapseStringList = [ "{0:3}={1:.2f}" . format ( sourceCell , permanence ) for sourceCell , permanence in synapseList ] segmentDict [ seg ] = "({0})" . format ( " " . join ( synapseStringList ) ) text += ( "Column {0:3} / Cell {1:3}:\t({2}) {3}\n" . format ( column , cell , len ( segmentDict . values ( ) ) , "[{0}]" . format ( ", " . join ( segmentDict . values ( ) ) ) ) ) if column < len ( columns ) - 1 : text += "\n" text += "------------------------------------\n" return text
7307	def set_embedded_doc ( self , document , form_key , current_key , remaining_key ) : embedded_doc = getattr ( document , current_key , False ) if not embedded_doc : embedded_doc = document . _fields [ current_key ] . document_type_obj ( ) new_key , new_remaining_key_array = trim_field_key ( embedded_doc , remaining_key ) self . process_document ( embedded_doc , form_key , make_key ( new_key , new_remaining_key_array ) ) setattr ( document , current_key , embedded_doc )
6310	def load ( self ) : self . _open_image ( ) components , data = image_data ( self . image ) texture = self . ctx . texture ( self . image . size , components , data , ) texture . extra = { 'meta' : self . meta } if self . meta . mipmap : texture . build_mipmaps ( ) self . _close_image ( ) return texture
11727	def ppdict ( dict_to_print , br = '\n' , html = False , key_align = 'l' , sort_keys = True , key_preffix = '' , key_suffix = '' , value_prefix = '' , value_suffix = '' , left_margin = 3 , indent = 2 ) : if dict_to_print : if sort_keys : dic = dict_to_print . copy ( ) keys = list ( dic . keys ( ) ) keys . sort ( ) dict_to_print = OrderedDict ( ) for k in keys : dict_to_print [ k ] = dic [ k ] tmp = [ '{' ] ks = [ type ( x ) == str and "'%s'" % x or x for x in dict_to_print . keys ( ) ] vs = [ type ( x ) == str and "'%s'" % x or x for x in dict_to_print . values ( ) ] max_key_len = max ( [ len ( str ( x ) ) for x in ks ] ) for i in range ( len ( ks ) ) : k = { 1 : str ( ks [ i ] ) . ljust ( max_key_len ) , key_align == 'r' : str ( ks [ i ] ) . rjust ( max_key_len ) } [ 1 ] v = vs [ i ] tmp . append ( ' ' * indent + '{}{}{}:{}{}{},' . format ( key_preffix , k , key_suffix , value_prefix , v , value_suffix ) ) tmp [ - 1 ] = tmp [ - 1 ] [ : - 1 ] tmp . append ( '}' ) if left_margin : tmp = [ ' ' * left_margin + x for x in tmp ] if html : return '<code>{}</code>' . format ( br . join ( tmp ) . replace ( ' ' , '&nbsp;' ) ) else : return br . join ( tmp ) else : return '{}'
944	def getCheckpointParentDir ( experimentDir ) : baseDir = os . path . join ( experimentDir , "savedmodels" ) baseDir = os . path . abspath ( baseDir ) return baseDir
11950	def _set_global_verbosity_level ( is_verbose_output = False ) : global verbose_output verbose_output = is_verbose_output if verbose_output : jocker_lgr . setLevel ( logging . DEBUG ) else : jocker_lgr . setLevel ( logging . INFO )
5432	def build_logging_param ( logging_uri , util_class = OutputFileParamUtil ) : if not logging_uri : return job_model . LoggingParam ( None , None ) recursive = not logging_uri . endswith ( '.log' ) oututil = util_class ( '' ) _ , uri , provider = oututil . parse_uri ( logging_uri , recursive ) if '*' in uri . basename : raise ValueError ( 'Wildcards not allowed in logging URI: %s' % uri ) return job_model . LoggingParam ( uri , provider )
9730	def get_force ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . plate_count ) : component_position , plate = QRTPacket . _get_exact ( RTForcePlate , data , component_position ) force_list = [ ] for _ in range ( plate . force_count ) : component_position , force = QRTPacket . _get_exact ( RTForce , data , component_position ) force_list . append ( force ) append_components ( ( plate , force_list ) ) return components
1237	def from_spec ( spec , kwargs = None ) : network = util . get_object ( obj = spec , default_object = LayeredNetwork , kwargs = kwargs ) assert isinstance ( network , Network ) return network
2739	def get_object ( cls , api_token , firewall_id ) : firewall = cls ( token = api_token , id = firewall_id ) firewall . load ( ) return firewall
13272	def generic_masked ( arr , attrs = None , minv = None , maxv = None , mask_nan = True ) : attrs = attrs or { } if 'valid_min' in attrs : minv = safe_attribute_typing ( arr . dtype , attrs [ 'valid_min' ] ) if 'valid_max' in attrs : maxv = safe_attribute_typing ( arr . dtype , attrs [ 'valid_max' ] ) if 'valid_range' in attrs : vr = attrs [ 'valid_range' ] minv = safe_attribute_typing ( arr . dtype , vr [ 0 ] ) maxv = safe_attribute_typing ( arr . dtype , vr [ 1 ] ) try : info = np . iinfo ( arr . dtype ) except ValueError : info = np . finfo ( arr . dtype ) minv = minv if minv is not None else info . min maxv = maxv if maxv is not None else info . max if mask_nan is True : arr = np . ma . fix_invalid ( arr ) return np . ma . masked_outside ( arr , minv , maxv )
5427	def _wait_for_any_job ( provider , job_ids , poll_interval ) : if not job_ids : return while True : tasks = provider . lookup_job_tasks ( { '*' } , job_ids = job_ids ) running_jobs = set ( ) failed_jobs = set ( ) for t in tasks : status = t . get_field ( 'task-status' ) job_id = t . get_field ( 'job-id' ) if status in [ 'FAILURE' , 'CANCELED' ] : failed_jobs . add ( job_id ) if status == 'RUNNING' : running_jobs . add ( job_id ) remaining_jobs = running_jobs . difference ( failed_jobs ) if failed_jobs or len ( remaining_jobs ) != len ( job_ids ) : return remaining_jobs SLEEP_FUNCTION ( poll_interval )
1951	def deprecated ( message : str ) : assert isinstance ( message , str ) , "The deprecated decorator requires a message string argument." def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : warnings . warn ( f"`{func.__qualname__}` is deprecated. {message}" , category = ManticoreDeprecationWarning , stacklevel = 2 ) return func ( * args , ** kwargs ) return wrapper return decorator
6453	def dist ( self , src , tar ) : if tar == src : return 0.0 if not src or not tar : return 1.0 max_length = max ( len ( src ) , len ( tar ) ) return self . dist_abs ( src , tar ) / max_length
10337	def build_spia_matrices ( nodes : Set [ str ] ) -> Dict [ str , pd . DataFrame ] : nodes = list ( sorted ( nodes ) ) matrices = OrderedDict ( ) for relation in KEGG_RELATIONS : matrices [ relation ] = pd . DataFrame ( 0 , index = nodes , columns = nodes ) return matrices
9893	def _uptime_plan9 ( ) : try : f = open ( '/dev/time' , 'r' ) s , ns , ct , cf = f . read ( ) . split ( ) f . close ( ) return float ( ct ) / float ( cf ) except ( IOError , ValueError ) : return None
2853	def mpsse_read_gpio ( self ) : self . _write ( '\x81\x83' ) data = self . _poll_read ( 2 ) low_byte = ord ( data [ 0 ] ) high_byte = ord ( data [ 1 ] ) logger . debug ( 'Read MPSSE GPIO low byte = {0:02X} and high byte = {1:02X}' . format ( low_byte , high_byte ) ) return ( high_byte << 8 ) | low_byte
3252	def get_stores ( self , names = None , workspaces = None ) : if isinstance ( workspaces , Workspace ) : workspaces = [ workspaces ] elif isinstance ( workspaces , list ) and [ w for w in workspaces if isinstance ( w , Workspace ) ] : pass else : workspaces = self . get_workspaces ( names = workspaces ) stores = [ ] for ws in workspaces : ds_list = self . get_xml ( ws . datastore_url ) cs_list = self . get_xml ( ws . coveragestore_url ) wms_list = self . get_xml ( ws . wmsstore_url ) stores . extend ( [ datastore_from_index ( self , ws , n ) for n in ds_list . findall ( "dataStore" ) ] ) stores . extend ( [ coveragestore_from_index ( self , ws , n ) for n in cs_list . findall ( "coverageStore" ) ] ) stores . extend ( [ wmsstore_from_index ( self , ws , n ) for n in wms_list . findall ( "wmsStore" ) ] ) if names is None : names = [ ] elif isinstance ( names , basestring ) : names = [ s . strip ( ) for s in names . split ( ',' ) if s . strip ( ) ] if stores and names : return ( [ store for store in stores if store . name in names ] ) return stores
5216	def fut_ticker ( gen_ticker : str , dt , freq : str , log = logs . LOG_LEVEL ) -> str : logger = logs . get_logger ( fut_ticker , level = log ) dt = pd . Timestamp ( dt ) t_info = gen_ticker . split ( ) asset = t_info [ - 1 ] if asset in [ 'Index' , 'Curncy' , 'Comdty' ] : ticker = ' ' . join ( t_info [ : - 1 ] ) prefix , idx , postfix = ticker [ : - 1 ] , int ( ticker [ - 1 ] ) - 1 , asset elif asset == 'Equity' : ticker = t_info [ 0 ] prefix , idx , postfix = ticker [ : - 1 ] , int ( ticker [ - 1 ] ) - 1 , ' ' . join ( t_info [ 1 : ] ) else : logger . error ( f'unkonwn asset type for ticker: {gen_ticker}' ) return '' month_ext = 4 if asset == 'Comdty' else 2 months = pd . date_range ( start = dt , periods = max ( idx + month_ext , 3 ) , freq = freq ) logger . debug ( f'pulling expiry dates for months: {months}' ) def to_fut ( month ) : return prefix + const . Futures [ month . strftime ( '%b' ) ] + month . strftime ( '%y' ) [ - 1 ] + ' ' + postfix fut = [ to_fut ( m ) for m in months ] logger . debug ( f'trying futures: {fut}' ) try : fut_matu = bdp ( tickers = fut , flds = 'last_tradeable_dt' , cache = True ) except Exception as e1 : logger . error ( f'error downloading futures contracts (1st trial) {e1}:\n{fut}' ) try : fut = fut [ : - 1 ] logger . debug ( f'trying futures (2nd trial): {fut}' ) fut_matu = bdp ( tickers = fut , flds = 'last_tradeable_dt' , cache = True ) except Exception as e2 : logger . error ( f'error downloading futures contracts (2nd trial) {e2}:\n{fut}' ) return '' sub_fut = fut_matu [ pd . DatetimeIndex ( fut_matu . last_tradeable_dt ) > dt ] logger . debug ( f'futures full chain:\n{fut_matu.to_string()}' ) logger . debug ( f'getting index {idx} from:\n{sub_fut.to_string()}' ) return sub_fut . index . values [ idx ]
3482	def _get_doc_from_filename ( filename ) : if isinstance ( filename , string_types ) : if ( "win" in platform ) and ( len ( filename ) < 260 ) and os . path . exists ( filename ) : doc = libsbml . readSBMLFromFile ( filename ) elif ( "win" not in platform ) and os . path . exists ( filename ) : doc = libsbml . readSBMLFromFile ( filename ) else : if "<sbml" not in filename : raise IOError ( "The file with 'filename' does not exist, " "or is not an SBML string. Provide the path to " "an existing SBML file or a valid SBML string " "representation: \n%s" , filename ) doc = libsbml . readSBMLFromString ( filename ) elif hasattr ( filename , "read" ) : doc = libsbml . readSBMLFromString ( filename . read ( ) ) else : raise CobraSBMLError ( "Input type '%s' for 'filename' is not supported." " Provide a path, SBML str, " "or file handle." , type ( filename ) ) return doc
10085	def delete ( self , force = True , pid = None ) : pid = pid or self . pid if self [ '_deposit' ] . get ( 'pid' ) : raise PIDInvalidAction ( ) if pid : pid . delete ( ) return super ( Deposit , self ) . delete ( force = force )
13436	def _setup_positions ( self , positions ) : updated_positions = [ ] for i , position in enumerate ( positions ) : ranger = re . search ( r'(?P<start>-?\d*):(?P<end>\d*)' , position ) if ranger : if i > 0 : updated_positions . append ( self . separator ) start = group_val ( ranger . group ( 'start' ) ) end = group_val ( ranger . group ( 'end' ) ) if start and end : updated_positions . extend ( self . _extendrange ( start , end + 1 ) ) elif ranger . group ( 'start' ) : updated_positions . append ( [ start ] ) else : updated_positions . extend ( self . _extendrange ( 1 , end + 1 ) ) else : updated_positions . append ( positions [ i ] ) try : if int ( position ) and int ( positions [ i + 1 ] ) : updated_positions . append ( self . separator ) except ( ValueError , IndexError ) : pass return updated_positions
11415	def record_delete_subfield_from ( rec , tag , subfield_position , field_position_global = None , field_position_local = None ) : subfields = record_get_subfields ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) try : del subfields [ subfield_position ] except IndexError : raise InvenioBibRecordFieldError ( "The record does not contain the subfield " "'%(subfieldIndex)s' inside the field (local: " "'%(fieldIndexLocal)s, global: '%(fieldIndexGlobal)s' ) of tag " "'%(tag)s'." % { "subfieldIndex" : subfield_position , "fieldIndexLocal" : str ( field_position_local ) , "fieldIndexGlobal" : str ( field_position_global ) , "tag" : tag } ) if not subfields : if field_position_global is not None : for position , field in enumerate ( rec [ tag ] ) : if field [ 4 ] == field_position_global : del rec [ tag ] [ position ] else : del rec [ tag ] [ field_position_local ] if not rec [ tag ] : del rec [ tag ]
10801	def _distance_matrix ( self , a , b ) : def sq ( x ) : return ( x * x ) matrix = sq ( a [ : , 0 ] [ : , None ] - b [ : , 0 ] [ None , : ] ) for x , y in zip ( a . T [ 1 : ] , b . T [ 1 : ] ) : matrix += sq ( x [ : , None ] - y [ None , : ] ) return matrix
6667	def check_version ( ) : global CHECK_VERSION if not CHECK_VERSION : return CHECK_VERSION = 0 from six . moves . urllib . request import urlopen try : response = urlopen ( "https://pypi.org/pypi/burlap/json" ) data = json . loads ( response . read ( ) . decode ( ) ) remote_release = sorted ( tuple ( map ( int , _ . split ( '.' ) ) ) for _ in data [ 'releases' ] . keys ( ) ) [ - 1 ] remote_release_str = '.' . join ( map ( str , remote_release ) ) local_release = VERSION local_release_str = '.' . join ( map ( str , local_release ) ) if remote_release > local_release : print ( '\033[93m' ) print ( "You are using burlap version %s, however version %s is available." % ( local_release_str , remote_release_str ) ) print ( "You should consider upgrading via the 'pip install --upgrade burlap' command." ) print ( '\033[0m' ) except Exception as exc : print ( '\033[93m' ) print ( "Unable to check for updated burlap version: %s" % exc ) print ( '\033[0m' )
13319	def deactivate ( ) : if 'CPENV_ACTIVE' not in os . environ or 'CPENV_CLEAN_ENV' not in os . environ : raise EnvironmentError ( 'Can not deactivate environment...' ) utils . restore_env_from_file ( os . environ [ 'CPENV_CLEAN_ENV' ] )
3251	def save ( self , obj , content_type = "application/xml" ) : rest_url = obj . href data = obj . message ( ) headers = { "Content-type" : content_type , "Accept" : content_type } logger . debug ( "{} {}" . format ( obj . save_method , obj . href ) ) resp = self . http_request ( rest_url , method = obj . save_method . lower ( ) , data = data , headers = headers ) if resp . status_code not in ( 200 , 201 ) : raise FailedRequestError ( 'Failed to save to Geoserver catalog: {}, {}' . format ( resp . status_code , resp . text ) ) self . _cache . clear ( ) return resp
5446	def _parse_local_mount_uri ( self , raw_uri ) : raw_uri = directory_fmt ( raw_uri ) _ , docker_path = _local_uri_rewriter ( raw_uri ) local_path = docker_path [ len ( 'file' ) : ] docker_uri = os . path . join ( self . _relative_path , docker_path ) return local_path , docker_uri
10650	def add_activity ( self , activity ) : self . gl . structure . validate_account_names ( activity . get_referenced_accounts ( ) ) self . activities . append ( activity ) activity . set_parent_path ( self . path )
13818	def _ConvertMessage ( value , message ) : message_descriptor = message . DESCRIPTOR full_name = message_descriptor . full_name if _IsWrapperMessage ( message_descriptor ) : _ConvertWrapperMessage ( value , message ) elif full_name in _WKTJSONMETHODS : _WKTJSONMETHODS [ full_name ] [ 1 ] ( value , message ) else : _ConvertFieldValuePair ( value , message )
11436	def _record_sort_by_indicators ( record ) : for tag , fields in record . items ( ) : record [ tag ] = _fields_sort_by_indicators ( fields )
2810	def convert_transpose ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting transpose ...' ) if params [ 'perm' ] [ 0 ] != 0 : if inputs [ 0 ] in layers : print ( '!!! Cannot permute batch dimension. Result may be wrong !!!' ) layers [ scope_name ] = layers [ inputs [ 0 ] ] else : print ( 'Skip weight matrix transpose, result may be wrong.' ) else : if names : tf_name = 'PERM' + random_string ( 4 ) else : tf_name = w_name + str ( random . random ( ) ) permute = keras . layers . Permute ( params [ 'perm' ] [ 1 : ] , name = tf_name ) layers [ scope_name ] = permute ( layers [ inputs [ 0 ] ] )
9715	async def qtm_version ( self ) : return await asyncio . wait_for ( self . _protocol . send_command ( "qtmversion" ) , timeout = self . _timeout )
2069	def get_splice_data ( ) : df = pd . read_csv ( 'source_data/splice/splice.csv' ) X = df . reindex ( columns = [ x for x in df . columns . values if x != 'class' ] ) X [ 'dna' ] = X [ 'dna' ] . map ( lambda x : list ( str ( x ) . strip ( ) ) ) for idx in range ( 60 ) : X [ 'dna_%d' % ( idx , ) ] = X [ 'dna' ] . map ( lambda x : x [ idx ] ) del X [ 'dna' ] y = df . reindex ( columns = [ 'class' ] ) y = preprocessing . LabelEncoder ( ) . fit_transform ( y . values . reshape ( - 1 , ) ) mapping = None return X , y , mapping
8102	def open_socket ( self ) : self . socket = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) self . socket . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) self . socket . setblocking ( 0 ) self . socket . bind ( ( self . host , self . port ) )
11564	def servo_config ( self , pin , min_pulse = 544 , max_pulse = 2400 ) : self . set_pin_mode ( pin , self . SERVO , self . OUTPUT ) command = [ pin , min_pulse & 0x7f , ( min_pulse >> 7 ) & 0x7f , max_pulse & 0x7f , ( max_pulse >> 7 ) & 0x7f ] self . _command_handler . send_sysex ( self . _command_handler . SERVO_CONFIG , command )
11733	def isValidClass ( self , class_ ) : module = inspect . getmodule ( class_ ) valid = ( module in self . _valid_modules or ( hasattr ( module , '__file__' ) and module . __file__ in self . _valid_named_modules ) ) return valid and not private ( class_ )
6195	def _get_group_randomstate ( rs , seed , group ) : if rs is None : rs = np . random . RandomState ( seed = seed ) if 'last_random_state' in group . _v_attrs : rs . set_state ( group . _v_attrs [ 'last_random_state' ] ) print ( "INFO: Random state set to last saved state in '%s'." % group . _v_name ) else : print ( "INFO: Random state initialized from seed (%d)." % seed ) return rs
11424	def record_find_field ( rec , tag , field , strict = False ) : try : _check_field_validity ( field ) except InvenioBibRecordFieldError : raise for local_position , field1 in enumerate ( rec . get ( tag , [ ] ) ) : if _compare_fields ( field , field1 , strict ) : return ( field1 [ 4 ] , local_position ) return ( None , None )
12701	def get_subfields ( self , datafield , subfield , i1 = None , i2 = None , exception = False ) : if len ( datafield ) != 3 : raise ValueError ( "`datafield` parameter have to be exactly 3 chars long!" ) if len ( subfield ) != 1 : raise ValueError ( "Bad subfield specification - subfield have to be 1 char long!" ) if datafield not in self . datafields : if exception : raise KeyError ( datafield + " is not in datafields!" ) return [ ] output = [ ] for datafield in self . datafields [ datafield ] : if subfield not in datafield : continue for sfield in datafield [ subfield ] : if i1 and sfield . i1 != i1 : continue if i2 and sfield . i2 != i2 : continue output . append ( sfield ) if not output and exception : raise KeyError ( subfield + " couldn't be found in subfields!" ) return output
11984	async def copy_storage_object ( self , source_bucket , source_key , bucket , key ) : info = await self . head_object ( Bucket = source_bucket , Key = source_key ) size = info [ 'ContentLength' ] if size > MULTI_PART_SIZE : result = await _multipart_copy ( self , source_bucket , source_key , bucket , key , size ) else : result = await self . copy_object ( Bucket = bucket , Key = key , CopySource = _source_string ( source_bucket , source_key ) ) return result
13615	def write ( ) : click . echo ( "Fantastic. Let's get started. " ) title = click . prompt ( "What's the title?" ) url = slugify ( title ) url = click . prompt ( "What's the URL?" , default = url ) click . echo ( "Got it. Creating %s..." % url ) scaffold_piece ( title , url )
5927	def configuration ( self ) : configuration = { 'configfilename' : self . filename , 'logfilename' : self . getpath ( 'Logging' , 'logfilename' ) , 'loglevel_console' : self . getLogLevel ( 'Logging' , 'loglevel_console' ) , 'loglevel_file' : self . getLogLevel ( 'Logging' , 'loglevel_file' ) , 'configdir' : self . getpath ( 'DEFAULT' , 'configdir' ) , 'qscriptdir' : self . getpath ( 'DEFAULT' , 'qscriptdir' ) , 'templatesdir' : self . getpath ( 'DEFAULT' , 'templatesdir' ) , } configuration [ 'path' ] = [ os . path . curdir , configuration [ 'qscriptdir' ] , configuration [ 'templatesdir' ] ] return configuration
4182	def window_blackman_nuttall ( N ) : r a0 = 0.3635819 a1 = 0.4891775 a2 = 0.1365995 a3 = 0.0106411 return _coeff4 ( N , a0 , a1 , a2 , a3 )
11158	def mirror_to ( self , dst ) : self . assert_is_dir_and_exists ( ) src = self . abspath dst = os . path . abspath ( dst ) if os . path . exists ( dst ) : raise Exception ( "distination already exist!" ) folder_to_create = list ( ) file_to_create = list ( ) for current_folder , _ , file_list in os . walk ( self . abspath ) : current_folder = current_folder . replace ( src , dst ) try : os . mkdir ( current_folder ) except : pass for basename in file_list : abspath = os . path . join ( current_folder , basename ) with open ( abspath , "wb" ) as _ : pass
7264	def validate ( method ) : name_error = 'configuration option "{}" is not supported' @ functools . wraps ( method ) def validator ( self , name , * args ) : if name not in self . allowed_opts : raise ValueError ( name_error . format ( name ) ) return method ( self , name , * args ) return validator
4695	def execute ( cmd = None , shell = True , echo = True ) : if echo : cij . emph ( "cij.util.execute: shell: %r, cmd: %r" % ( shell , cmd ) ) rcode = 1 stdout , stderr = ( "" , "" ) if cmd : if shell : cmd = " " . join ( cmd ) proc = Popen ( cmd , stdout = PIPE , stderr = PIPE , shell = shell , close_fds = True ) stdout , stderr = proc . communicate ( ) rcode = proc . returncode if rcode and echo : cij . warn ( "cij.util.execute: stdout: %s" % stdout ) cij . err ( "cij.util.execute: stderr: %s" % stderr ) cij . err ( "cij.util.execute: rcode: %s" % rcode ) return rcode , stdout , stderr
8249	def nearest_hue ( self , primary = False ) : if self . is_black : return "black" elif self . is_white : return "white" elif self . is_grey : return "grey" if primary : hues = primary_hues else : hues = named_hues . keys ( ) nearest , d = "" , 1.0 for hue in hues : if abs ( self . hue - named_hues [ hue ] ) % 1 < d : nearest , d = hue , abs ( self . hue - named_hues [ hue ] ) % 1 return nearest
1468	def process_tick ( self , tup ) : curtime = int ( time . time ( ) ) window_info = WindowContext ( curtime - self . window_duration , curtime ) tuple_batch = [ ] for ( tup , tm ) in self . current_tuples : tuple_batch . append ( tup ) self . processWindow ( window_info , tuple_batch ) self . _expire ( curtime )
12073	def _trace_summary ( self ) : for ( i , ( val , args ) ) in enumerate ( self . trace ) : if args is StopIteration : info = "Terminated" else : pprint = ',' . join ( '{' + ',' . join ( '%s=%r' % ( k , v ) for ( k , v ) in arg . items ( ) ) + '}' for arg in args ) info = ( "exploring arguments [%s]" % pprint ) if i == 0 : print ( "Step %d: Initially %s." % ( i , info ) ) else : print ( "Step %d: %s after receiving input(s) %s." % ( i , info . capitalize ( ) , val ) )
3993	def _load_ssh_auth_post_yosemite ( mac_username ) : user_id = subprocess . check_output ( [ 'id' , '-u' , mac_username ] ) ssh_auth_sock = subprocess . check_output ( [ 'launchctl' , 'asuser' , user_id , 'launchctl' , 'getenv' , 'SSH_AUTH_SOCK' ] ) . rstrip ( ) _set_ssh_auth_sock ( ssh_auth_sock )
6979	def filter_kepler_lcdict ( lcdict , filterflags = True , nanfilter = 'sap,pdc' , timestoignore = None ) : cols = lcdict [ 'columns' ] if filterflags : nbefore = lcdict [ 'time' ] . size filterind = lcdict [ 'sap_quality' ] == 0 for col in cols : if '.' in col : key , subkey = col . split ( '.' ) lcdict [ key ] [ subkey ] = lcdict [ key ] [ subkey ] [ filterind ] else : lcdict [ col ] = lcdict [ col ] [ filterind ] nafter = lcdict [ 'time' ] . size LOGINFO ( 'applied quality flag filter, ndet before = %s, ndet after = %s' % ( nbefore , nafter ) ) if nanfilter and nanfilter == 'sap,pdc' : notnanind = ( npisfinite ( lcdict [ 'sap' ] [ 'sap_flux' ] ) & npisfinite ( lcdict [ 'pdc' ] [ 'pdcsap_flux' ] ) & npisfinite ( lcdict [ 'time' ] ) ) elif nanfilter and nanfilter == 'sap' : notnanind = ( npisfinite ( lcdict [ 'sap' ] [ 'sap_flux' ] ) & npisfinite ( lcdict [ 'time' ] ) ) elif nanfilter and nanfilter == 'pdc' : notnanind = ( npisfinite ( lcdict [ 'pdc' ] [ 'pdcsap_flux' ] ) & npisfinite ( lcdict [ 'time' ] ) ) if nanfilter : nbefore = lcdict [ 'time' ] . size for col in cols : if '.' in col : key , subkey = col . split ( '.' ) lcdict [ key ] [ subkey ] = lcdict [ key ] [ subkey ] [ notnanind ] else : lcdict [ col ] = lcdict [ col ] [ notnanind ] nafter = lcdict [ 'time' ] . size LOGINFO ( 'removed nans, ndet before = %s, ndet after = %s' % ( nbefore , nafter ) ) if ( timestoignore and isinstance ( timestoignore , list ) and len ( timestoignore ) > 0 ) : exclind = npfull_like ( lcdict [ 'time' ] , True , dtype = np . bool_ ) nbefore = exclind . size for ignoretime in timestoignore : time0 , time1 = ignoretime [ 0 ] , ignoretime [ 1 ] thismask = ~ ( ( lcdict [ 'time' ] >= time0 ) & ( lcdict [ 'time' ] <= time1 ) ) exclind = exclind & thismask for col in cols : if '.' in col : key , subkey = col . split ( '.' ) lcdict [ key ] [ subkey ] = lcdict [ key ] [ subkey ] [ exclind ] else : lcdict [ col ] = lcdict [ col ] [ exclind ] nafter = lcdict [ 'time' ] . size LOGINFO ( 'removed timestoignore, ndet before = %s, ndet after = %s' % ( nbefore , nafter ) ) return lcdict
12378	def get ( self , request , response ) : self . assert_operations ( 'read' ) items = self . read ( ) if not items : raise http . exceptions . NotFound ( ) if ( isinstance ( items , Iterable ) and not isinstance ( items , six . string_types ) ) and items : items = pagination . paginate ( self . request , self . response , items ) self . make_response ( items )
4982	def get_available_course_modes ( self , request , course_run_id , enterprise_catalog ) : modes = EnrollmentApiClient ( ) . get_course_modes ( course_run_id ) if not modes : LOGGER . warning ( 'Unable to get course modes for course run id {course_run_id}.' . format ( course_run_id = course_run_id ) ) messages . add_generic_info_message_for_error ( request ) if enterprise_catalog : modes = [ mode for mode in modes if mode [ 'slug' ] in enterprise_catalog . enabled_course_modes ] modes . sort ( key = lambda course_mode : enterprise_catalog . enabled_course_modes . index ( course_mode [ 'slug' ] ) ) if not modes : LOGGER . info ( 'No matching course modes found for course run {course_run_id} in ' 'EnterpriseCustomerCatalog [{enterprise_catalog_uuid}]' . format ( course_run_id = course_run_id , enterprise_catalog_uuid = enterprise_catalog , ) ) messages . add_generic_info_message_for_error ( request ) return modes
5664	def interpolate_shape_times ( shape_distances , shape_breaks , stop_times ) : shape_times = np . zeros ( len ( shape_distances ) ) shape_times [ : shape_breaks [ 0 ] ] = stop_times [ 0 ] for i in range ( len ( shape_breaks ) - 1 ) : cur_break = shape_breaks [ i ] cur_time = stop_times [ i ] next_break = shape_breaks [ i + 1 ] next_time = stop_times [ i + 1 ] if cur_break == next_break : shape_times [ cur_break ] = stop_times [ i ] else : cur_distances = shape_distances [ cur_break : next_break + 1 ] norm_distances = ( ( np . array ( cur_distances ) - float ( cur_distances [ 0 ] ) ) / float ( cur_distances [ - 1 ] - cur_distances [ 0 ] ) ) times = ( 1. - norm_distances ) * cur_time + norm_distances * next_time shape_times [ cur_break : next_break ] = times [ : - 1 ] shape_times [ shape_breaks [ - 1 ] : ] = stop_times [ - 1 ] return list ( shape_times )
9181	def _validate_subjects ( cursor , model ) : subject_vocab = [ term [ 0 ] for term in acquire_subject_vocabulary ( cursor ) ] subjects = model . metadata . get ( 'subjects' , [ ] ) invalid_subjects = [ s for s in subjects if s not in subject_vocab ] if invalid_subjects : raise exceptions . InvalidMetadata ( 'subjects' , invalid_subjects )
7775	def rfc2425encode ( name , value , parameters = None , charset = "utf-8" ) : if not parameters : parameters = { } if type ( value ) is unicode : value = value . replace ( u"\r\n" , u"\\n" ) value = value . replace ( u"\n" , u"\\n" ) value = value . replace ( u"\r" , u"\\n" ) value = value . encode ( charset , "replace" ) elif type ( value ) is not str : raise TypeError ( "Bad type for rfc2425 value" ) elif not valid_string_re . match ( value ) : parameters [ "encoding" ] = "b" value = binascii . b2a_base64 ( value ) ret = str ( name ) . lower ( ) for k , v in parameters . items ( ) : ret += ";%s=%s" % ( str ( k ) , str ( v ) ) ret += ":" while ( len ( value ) > 70 ) : ret += value [ : 70 ] + "\r\n " value = value [ 70 : ] ret += value + "\r\n" return ret
12780	def get_users ( self , sort = True ) : self . _load ( ) if sort : self . users . sort ( key = operator . itemgetter ( "name" ) ) return self . users
9892	def _uptime_minix ( ) : try : f = open ( '/proc/uptime' , 'r' ) up = float ( f . read ( ) ) f . close ( ) return up except ( IOError , ValueError ) : return None
11781	def compare ( algorithms = [ PluralityLearner , NaiveBayesLearner , NearestNeighborLearner , DecisionTreeLearner ] , datasets = [ iris , orings , zoo , restaurant , SyntheticRestaurant ( 20 ) , Majority ( 7 , 100 ) , Parity ( 7 , 100 ) , Xor ( 100 ) ] , k = 10 , trials = 1 ) : print_table ( [ [ a . __name__ . replace ( 'Learner' , '' ) ] + [ cross_validation ( a , d , k , trials ) for d in datasets ] for a in algorithms ] , header = [ '' ] + [ d . name [ 0 : 7 ] for d in datasets ] , numfmt = '%.2f' )
1960	def sys_rename ( self , oldnamep , newnamep ) : oldname = self . current . read_string ( oldnamep ) newname = self . current . read_string ( newnamep ) ret = 0 try : os . rename ( oldname , newname ) except OSError as e : ret = - e . errno return ret
11561	def play_tone ( self , pin , tone_command , frequency , duration ) : if tone_command == self . TONE_TONE : if duration : data = [ tone_command , pin , frequency & 0x7f , ( frequency >> 7 ) & 0x7f , duration & 0x7f , ( duration >> 7 ) & 0x7f ] else : data = [ tone_command , pin , frequency & 0x7f , ( frequency >> 7 ) & 0x7f , 0 , 0 ] self . _command_handler . digital_response_table [ pin ] [ self . _command_handler . RESPONSE_TABLE_MODE ] = self . TONE else : data = [ tone_command , pin ] self . _command_handler . send_sysex ( self . _command_handler . TONE_PLAY , data )
10821	def query_by_user ( cls , user , ** kwargs ) : return cls . _filter ( cls . query . filter_by ( user_id = user . get_id ( ) ) , ** kwargs )
5901	def mpicommand ( self , * args , ** kwargs ) : if self . mpiexec is None : raise NotImplementedError ( "Override mpiexec to enable the simple OpenMP launcher" ) ncores = kwargs . pop ( 'ncores' , 8 ) return [ self . mpiexec , '-n' , str ( ncores ) ]
5366	def replace_print ( fileobj = sys . stderr ) : printer = _Printer ( fileobj ) previous_stdout = sys . stdout sys . stdout = printer try : yield printer finally : sys . stdout = previous_stdout
8440	def setup ( template , version = None ) : temple . check . is_git_ssh_path ( template ) temple . check . not_in_git_repo ( ) repo_path = temple . utils . get_repo_path ( template ) msg = ( 'You will be prompted for the parameters of your new project.' ' Please read the docs at https://github.com/{} before entering parameters.' ) . format ( repo_path ) print ( msg ) cc_repo_dir , config = temple . utils . get_cookiecutter_config ( template , version = version ) if not version : with temple . utils . cd ( cc_repo_dir ) : ret = temple . utils . shell ( 'git rev-parse HEAD' , stdout = subprocess . PIPE ) version = ret . stdout . decode ( 'utf-8' ) . strip ( ) _generate_files ( repo_dir = cc_repo_dir , config = config , template = template , version = version )
13118	def argument_search ( self ) : arguments , _ = self . argparser . parse_known_args ( ) return self . search ( ** vars ( arguments ) )
13412	def changeLogType ( self ) : logType = self . selectedType ( ) programs = self . logList . get ( logType ) [ 0 ] default = self . logList . get ( logType ) [ 1 ] if logType in self . logList : self . programName . clear ( ) self . programName . addItems ( programs ) self . programName . setCurrentIndex ( programs . index ( default ) )
1131	def urldefrag ( url ) : if '#' in url : s , n , p , a , q , frag = urlparse ( url ) defrag = urlunparse ( ( s , n , p , a , q , '' ) ) return defrag , frag else : return url , ''
6459	def _has_vowel ( self , term ) : for letter in term : if letter in self . _vowels : return True return False
4674	def getPrivateKeyForPublicKey ( self , pub ) : if str ( pub ) not in self . store : raise KeyNotFound return self . store . getPrivateKeyForPublicKey ( str ( pub ) )
665	def numpyStr ( array , format = '%f' , includeIndices = False , includeZeros = True ) : shape = array . shape assert ( len ( shape ) <= 2 ) items = [ '[' ] if len ( shape ) == 1 : if includeIndices : format = '%d:' + format if includeZeros : rowItems = [ format % ( c , x ) for ( c , x ) in enumerate ( array ) ] else : rowItems = [ format % ( c , x ) for ( c , x ) in enumerate ( array ) if x != 0 ] else : rowItems = [ format % ( x ) for x in array ] items . extend ( rowItems ) else : ( rows , cols ) = shape if includeIndices : format = '%d,%d:' + format for r in xrange ( rows ) : if includeIndices : rowItems = [ format % ( r , c , x ) for c , x in enumerate ( array [ r ] ) ] else : rowItems = [ format % ( x ) for x in array [ r ] ] if r > 0 : items . append ( '' ) items . append ( '[' ) items . extend ( rowItems ) if r < rows - 1 : items . append ( ']\n' ) else : items . append ( ']' ) items . append ( ']' ) return ' ' . join ( items )
10562	def _normalize_metadata ( metadata ) : metadata = str ( metadata ) metadata = metadata . lower ( ) metadata = re . sub ( r'\/\s*\d+' , '' , metadata ) metadata = re . sub ( r'^0+([0-9]+)' , r'\1' , metadata ) metadata = re . sub ( r'^\d+\.+' , '' , metadata ) metadata = re . sub ( r'[^\w\s]' , '' , metadata ) metadata = re . sub ( r'\s+' , ' ' , metadata ) metadata = re . sub ( r'^\s+' , '' , metadata ) metadata = re . sub ( r'\s+$' , '' , metadata ) metadata = re . sub ( r'^the\s+' , '' , metadata , re . I ) return metadata
5403	def _get_localization_env ( self , inputs , user_project ) : non_empty_inputs = [ var for var in inputs if var . value ] env = { 'INPUT_COUNT' : str ( len ( non_empty_inputs ) ) } for idx , var in enumerate ( non_empty_inputs ) : env [ 'INPUT_{}' . format ( idx ) ] = var . name env [ 'INPUT_RECURSIVE_{}' . format ( idx ) ] = str ( int ( var . recursive ) ) env [ 'INPUT_SRC_{}' . format ( idx ) ] = var . value dst = os . path . join ( providers_util . DATA_MOUNT_POINT , var . docker_path ) path , filename = os . path . split ( dst ) if '*' in filename : dst = '{}/' . format ( path ) env [ 'INPUT_DST_{}' . format ( idx ) ] = dst env [ 'USER_PROJECT' ] = user_project return env
2729	def get_kernel_available ( self ) : kernels = list ( ) data = self . get_data ( "droplets/%s/kernels/" % self . id ) while True : for jsond in data [ u'kernels' ] : kernel = Kernel ( ** jsond ) kernel . token = self . token kernels . append ( kernel ) try : url = data [ u'links' ] [ u'pages' ] . get ( u'next' ) if not url : break data = self . get_data ( url ) except KeyError : break return kernels
4550	def fill_round_rect ( setter , x , y , w , h , r , color = None , aa = False ) : fill_rect ( setter , x + r , y , w - 2 * r , h , color , aa ) _fill_circle_helper ( setter , x + w - r - 1 , y + r , r , 1 , h - 2 * r - 1 , color , aa ) _fill_circle_helper ( setter , x + r , y + r , r , 2 , h - 2 * r - 1 , color , aa )
8920	def _get_request_type ( self ) : value = self . document . tag . lower ( ) if value in allowed_request_types [ self . params [ 'service' ] ] : self . params [ "request" ] = value else : raise OWSInvalidParameterValue ( "Request type %s is not supported" % value , value = "request" ) return self . params [ "request" ]
11736	def _validate_schema ( obj ) : if obj is not None and not isinstance ( obj , Schema ) : raise IncompatibleSchema ( 'Schema must be of type {0}' . format ( Schema ) ) return obj
13559	def get_top_assets ( self ) : images = self . get_all_images ( ) [ 0 : 14 ] video = [ ] if supports_video : video = self . eventvideo_set . all ( ) [ 0 : 10 ] return list ( chain ( images , video ) ) [ 0 : 15 ]
3657	def remove_cti_file ( self , file_path : str ) : if file_path in self . _cti_files : self . _cti_files . remove ( file_path ) self . _logger . info ( 'Removed {0} from the CTI file list.' . format ( file_path ) )
13750	def one_to_many ( clsname , ** kw ) : @ declared_attr def o2m ( cls ) : cls . _references ( ( clsname , cls . __name__ ) ) return relationship ( clsname , ** kw ) return o2m
1548	def init_rotating_logger ( level , logfile , max_files , max_bytes ) : logging . basicConfig ( ) root_logger = logging . getLogger ( ) log_format = "[%(asctime)s] [%(levelname)s] %(filename)s: %(message)s" root_logger . setLevel ( level ) handler = RotatingFileHandler ( logfile , maxBytes = max_bytes , backupCount = max_files ) handler . setFormatter ( logging . Formatter ( fmt = log_format , datefmt = date_format ) ) root_logger . addHandler ( handler ) for handler in root_logger . handlers : root_logger . debug ( "Associated handlers - " + str ( handler ) ) if isinstance ( handler , logging . StreamHandler ) : root_logger . debug ( "Removing StreamHandler: " + str ( handler ) ) root_logger . handlers . remove ( handler )
9222	def reverse_guard ( lst ) : rev = { '<' : '>=' , '>' : '=<' , '>=' : '<' , '=<' : '>' } return [ rev [ l ] if l in rev else l for l in lst ]
7394	def get_publications ( context , template = 'publications/publications.html' ) : types = Type . objects . filter ( hidden = False ) publications = Publication . objects . select_related ( ) publications = publications . filter ( external = False , type__in = types ) publications = publications . order_by ( '-year' , '-month' , '-id' ) if not publications : return '' populate ( publications ) return render_template ( template , context [ 'request' ] , { 'publications' : publications } )
3895	def print_table ( col_tuple , row_tuples ) : col_widths = [ max ( len ( str ( row [ col ] ) ) for row in [ col_tuple ] + row_tuples ) for col in range ( len ( col_tuple ) ) ] format_str = ' ' . join ( '{{:<{}}}' . format ( col_width ) for col_width in col_widths ) header_border = ' ' . join ( '=' * col_width for col_width in col_widths ) print ( header_border ) print ( format_str . format ( * col_tuple ) ) print ( header_border ) for row_tuple in row_tuples : print ( format_str . format ( * row_tuple ) ) print ( header_border ) print ( )
593	def _initEphemerals ( self ) : if hasattr ( self , '_sfdr' ) and self . _sfdr : self . _spatialPoolerOutput = numpy . zeros ( self . columnCount , dtype = GetNTAReal ( ) ) else : self . _spatialPoolerOutput = None self . _fpLogSPInput = None self . _fpLogSP = None self . _fpLogSPDense = None self . logPathInput = "" self . logPathOutput = "" self . logPathOutputDense = ""
10373	def node_has_namespaces ( node : BaseEntity , namespaces : Set [ str ] ) -> bool : ns = node . get ( NAMESPACE ) return ns is not None and ns in namespaces
13172	def last ( self , name = None ) : for c in self . children ( name , reverse = True ) : return c
215	def deepcopy ( self ) : return HeatmapsOnImage ( self . get_arr ( ) , shape = self . shape , min_value = self . min_value , max_value = self . max_value )
7315	def parse_filter ( self , filters ) : for filter_type in filters : if filter_type == 'or' or filter_type == 'and' : conditions = [ ] for field in filters [ filter_type ] : if self . is_field_allowed ( field ) : conditions . append ( self . create_query ( self . parse_field ( field , filters [ filter_type ] [ field ] ) ) ) if filter_type == 'or' : self . model_query = self . model_query . filter ( or_ ( * conditions ) ) elif filter_type == 'and' : self . model_query = self . model_query . filter ( and_ ( * conditions ) ) else : if self . is_field_allowed ( filter_type ) : conditions = self . create_query ( self . parse_field ( filter_type , filters [ filter_type ] ) ) self . model_query = self . model_query . filter ( conditions ) return self . model_query
8915	def fetch_by_name ( self , name ) : service = self . name_index . get ( name ) if not service : raise ServiceNotFound return Service ( service )
13746	def create_item ( self , hash_key , start = 0 , extra_attrs = None ) : table = self . get_table ( ) now = datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) attrs = { 'created_on' : now , 'modified_on' : now , 'count' : start , } if extra_attrs : attrs . update ( extra_attrs ) item = table . new_item ( hash_key = hash_key , attrs = attrs , ) return item
13829	def remove ( self , collection , ** kwargs ) : callback = kwargs . pop ( 'callback' ) yield Op ( self . db [ collection ] . remove , kwargs ) callback ( )
11573	def clear_display_buffer ( self ) : for row in range ( 0 , 8 ) : self . firmata . i2c_write ( 0x70 , row * 2 , 0 , 0 ) self . firmata . i2c_write ( 0x70 , ( row * 2 ) + 1 , 0 , 0 ) for column in range ( 0 , 8 ) : self . display_buffer [ row ] [ column ] = 0
10873	def get_K ( rho , z , alpha = 1.0 , zint = 100.0 , n2n1 = 0.95 , get_hdet = False , K = 1 , Kprefactor = None , return_Kprefactor = False , npts = 20 , ** kwargs ) : if type ( rho ) != np . ndarray or type ( z ) != np . ndarray or ( rho . shape != z . shape ) : raise ValueError ( 'rho and z must be np.arrays of same shape.' ) pts , wts = np . polynomial . legendre . leggauss ( npts ) n1n2 = 1.0 / n2n1 rr = np . ravel ( rho ) zr = np . ravel ( z ) cos_theta = 0.5 * ( 1 - np . cos ( alpha ) ) * pts + 0.5 * ( 1 + np . cos ( alpha ) ) if Kprefactor is None : Kprefactor = get_Kprefactor ( z , cos_theta , zint = zint , n2n1 = n2n1 , get_hdet = get_hdet , ** kwargs ) if K == 1 : part_1 = j0 ( np . outer ( rr , np . sqrt ( 1 - cos_theta ** 2 ) ) ) * np . outer ( np . ones_like ( rr ) , 0.5 * ( get_taus ( cos_theta , n2n1 = n2n1 ) + get_taup ( cos_theta , n2n1 = n2n1 ) * csqrt ( 1 - n1n2 ** 2 * ( 1 - cos_theta ** 2 ) ) ) ) integrand = Kprefactor * part_1 elif K == 2 : part_2 = j2 ( np . outer ( rr , np . sqrt ( 1 - cos_theta ** 2 ) ) ) * np . outer ( np . ones_like ( rr ) , 0.5 * ( get_taus ( cos_theta , n2n1 = n2n1 ) - get_taup ( cos_theta , n2n1 = n2n1 ) * csqrt ( 1 - n1n2 ** 2 * ( 1 - cos_theta ** 2 ) ) ) ) integrand = Kprefactor * part_2 elif K == 3 : part_3 = j1 ( np . outer ( rho , np . sqrt ( 1 - cos_theta ** 2 ) ) ) * np . outer ( np . ones_like ( rr ) , n1n2 * get_taup ( cos_theta , n2n1 = n2n1 ) * np . sqrt ( 1 - cos_theta ** 2 ) ) integrand = Kprefactor * part_3 else : raise ValueError ( 'K=1,2,3 only...' ) big_wts = np . outer ( np . ones_like ( rr ) , wts ) kint = ( big_wts * integrand ) . sum ( axis = 1 ) * 0.5 * ( 1 - np . cos ( alpha ) ) if return_Kprefactor : return kint . reshape ( rho . shape ) , Kprefactor else : return kint . reshape ( rho . shape )
12593	def execute_reliabledictionary ( client , application_name , service_name , input_file ) : cluster = Cluster . from_sfclient ( client ) service = cluster . get_application ( application_name ) . get_service ( service_name ) with open ( input_file ) as json_file : json_data = json . load ( json_file ) service . execute ( json_data ) return
9444	def bulk_call ( self , call_params ) : path = '/' + self . api_version + '/BulkCall/' method = 'POST' return self . request ( path , method , call_params )
6553	def fix_variable ( self , v , value ) : variables = self . variables try : idx = variables . index ( v ) except ValueError : raise ValueError ( "given variable {} is not part of the constraint" . format ( v ) ) if value not in self . vartype . value : raise ValueError ( "expected value to be in {}, received {} instead" . format ( self . vartype . value , value ) ) configurations = frozenset ( config [ : idx ] + config [ idx + 1 : ] for config in self . configurations if config [ idx ] == value ) if not configurations : raise UnsatError ( "fixing {} to {} makes this constraint unsatisfiable" . format ( v , value ) ) variables = variables [ : idx ] + variables [ idx + 1 : ] self . configurations = configurations self . variables = variables def func ( * args ) : return args in configurations self . func = func self . name = '{} ({} fixed to {})' . format ( self . name , v , value )
8319	def parse_tables ( self , markup ) : tables = [ ] m = re . findall ( self . re [ "table" ] , markup ) for chunk in m : table = WikipediaTable ( ) table . properties = chunk . split ( "\n" ) [ 0 ] . strip ( "{|" ) . strip ( ) self . connect_table ( table , chunk , markup ) row = None for chunk in chunk . split ( "\n" ) : chunk = chunk . strip ( ) if chunk . startswith ( "|+" ) : title = self . plain ( chunk . strip ( "|+" ) ) table . title = title elif chunk . startswith ( "|-" ) : if row : row . properties = chunk . strip ( "|-" ) . strip ( ) table . append ( row ) row = None elif chunk . startswith ( "|}" ) : pass elif chunk . startswith ( "|" ) or chunk . startswith ( "!" ) : row = self . parse_table_row ( chunk , row ) if row : table . append ( row ) if len ( table ) > 0 : tables . append ( table ) return tables
11074	def get_by_username ( self , username ) : res = filter ( lambda x : x . username == username , self . users . values ( ) ) if len ( res ) > 0 : return res [ 0 ] return None
11917	def render ( template , ** data ) : try : return renderer . render ( template , ** data ) except JinjaTemplateNotFound as e : logger . error ( e . __doc__ + ', Template: %r' % template ) sys . exit ( e . exit_code )
776	def __getDBNameForVersion ( cls , dbVersion ) : prefix = cls . __getDBNamePrefixForVersion ( dbVersion ) suffix = Configuration . get ( 'nupic.cluster.database.nameSuffix' ) suffix = suffix . replace ( "-" , "_" ) suffix = suffix . replace ( "." , "_" ) dbName = '%s_%s' % ( prefix , suffix ) return dbName
10986	def get_particles_featuring ( feature_rad , state_name = None , im_name = None , use_full_path = False , actual_rad = None , invert = True , featuring_params = { } , ** kwargs ) : state_name , im_name = _pick_state_im_name ( state_name , im_name , use_full_path = use_full_path ) s = states . load ( state_name ) if actual_rad is None : actual_rad = np . median ( s . obj_get_radii ( ) ) im = util . RawImage ( im_name , tile = s . image . tile ) pos = locate_spheres ( im , feature_rad , invert = invert , ** featuring_params ) _ = s . obj_remove_particle ( np . arange ( s . obj_get_radii ( ) . size ) ) s . obj_add_particle ( pos , np . ones ( pos . shape [ 0 ] ) * actual_rad ) s . set_image ( im ) _translate_particles ( s , invert = invert , ** kwargs ) return s
2974	def cmd_up ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) containers = b . create ( verbose = opts . verbose , force = opts . force ) print_containers ( containers , opts . json )
2495	def handle_pkg_optional_fields ( self , package , package_node ) : self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . versionInfo , 'version' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . packageFileName , 'file_name' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . supplier , 'supplier' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . originator , 'originator' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . sourceInfo , 'source_info' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . licenseComments , 'license_comment' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . summary , 'summary' ) self . handle_package_literal_optional ( package , package_node , self . spdx_namespace . description , 'description' ) if package . has_optional_field ( 'check_sum' ) : checksum_node = self . create_checksum_node ( package . check_sum ) self . graph . add ( ( package_node , self . spdx_namespace . checksum , checksum_node ) ) if package . has_optional_field ( 'homepage' ) : homepage_node = URIRef ( self . to_special_value ( package . homepage ) ) homepage_triple = ( package_node , self . doap_namespace . homepage , homepage_node ) self . graph . add ( homepage_triple )
8063	def do_set ( self , line ) : try : name , value = [ part . strip ( ) for part in line . split ( '=' ) ] if name not in self . bot . _vars : self . print_response ( 'No such variable %s enter vars to see available vars' % name ) return variable = self . bot . _vars [ name ] variable . value = variable . sanitize ( value . strip ( ';' ) ) success , msg = self . bot . canvas . sink . var_changed ( name , variable . value ) if success : print ( '{}={}' . format ( name , variable . value ) , file = self . stdout ) else : print ( '{}\n' . format ( msg ) , file = self . stdout ) except Exception as e : print ( 'Invalid Syntax.' , e ) return
1649	def GetLineWidth ( line ) : if isinstance ( line , unicode ) : width = 0 for uc in unicodedata . normalize ( 'NFC' , line ) : if unicodedata . east_asian_width ( uc ) in ( 'W' , 'F' ) : width += 2 elif not unicodedata . combining ( uc ) : width += 1 return width else : return len ( line )
4924	def get_required_query_params ( self , request ) : email = get_request_value ( request , self . REQUIRED_PARAM_EMAIL , '' ) enterprise_name = get_request_value ( request , self . REQUIRED_PARAM_ENTERPRISE_NAME , '' ) number_of_codes = get_request_value ( request , self . OPTIONAL_PARAM_NUMBER_OF_CODES , '' ) if not ( email and enterprise_name ) : raise CodesAPIRequestError ( self . get_missing_params_message ( [ ( self . REQUIRED_PARAM_EMAIL , bool ( email ) ) , ( self . REQUIRED_PARAM_ENTERPRISE_NAME , bool ( enterprise_name ) ) , ] ) ) return email , enterprise_name , number_of_codes
8551	def delete_image ( self , image_id ) : response = self . _perform_request ( url = '/images/' + image_id , method = 'DELETE' ) return response
4298	def _convert_config_to_stdin ( config , parser ) : keys_empty_values_not_pass = ( '--extra-settings' , '--languages' , '--requirements' , '--template' , '--timezone' ) args = [ ] for key , val in config . items ( SECTION ) : keyp = '--{0}' . format ( key ) action = parser . _option_string_actions [ keyp ] if action . const : try : if config . getboolean ( SECTION , key ) : args . append ( keyp ) except ValueError : args . extend ( [ keyp , val ] ) elif any ( [ i for i in keys_empty_values_not_pass if i in action . option_strings ] ) : if val != '' : args . extend ( [ keyp , val ] ) else : args . extend ( [ keyp , val ] ) return args
2965	def _sm_start ( self , * args , ** kwargs ) : millisec = random . randint ( self . _start_min_delay , self . _start_max_delay ) self . _timer = threading . Timer ( millisec / 1000.0 , self . event_timeout ) self . _timer . start ( )
11294	def main ( path ) : basepath = os . path . abspath ( os . path . expanduser ( str ( path ) ) ) echo . h2 ( "Available scripts in {}" . format ( basepath ) ) echo . br ( ) for root_dir , dirs , files in os . walk ( basepath , topdown = True ) : for f in fnmatch . filter ( files , '*.py' ) : try : filepath = os . path . join ( root_dir , f ) with open ( filepath , encoding = "UTF-8" ) as fp : body = fp . read ( ) is_console = "InteractiveConsole" in body is_console = is_console or "code" in body is_console = is_console and "interact(" in body if is_console : continue s = captain . Script ( filepath ) if s . can_run_from_cli ( ) : rel_filepath = s . call_path ( basepath ) p = s . parser echo . h3 ( rel_filepath ) desc = p . description if desc : echo . indent ( desc , indent = ( " " * 4 ) ) subcommands = s . subcommands if subcommands : echo . br ( ) echo . indent ( "Subcommands:" , indent = ( " " * 4 ) ) for sc in subcommands . keys ( ) : echo . indent ( sc , indent = ( " " * 6 ) ) echo . br ( ) except captain . ParseError : pass except Exception as e : echo . err ( "Failed to parse {}" , f ) echo . verbose ( e . message ) echo . br ( )
5804	def detect_client_auth_request ( server_handshake_bytes ) : for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type == b'\x0d' : return True return False
11889	def get_lights ( self ) : now = datetime . datetime . now ( ) if ( now - self . _last_updated ) < datetime . timedelta ( seconds = UPDATE_INTERVAL_SECONDS ) : return self . _bulbs else : self . _last_updated = now light_data = self . get_data ( ) _LOGGER . debug ( "got: %s" , light_data ) if not light_data : return [ ] if self . _bulbs : for bulb in self . _bulbs : try : values = light_data [ bulb . zid ] bulb . _online , bulb . _red , bulb . _green , bulb . _blue , bulb . _level = values except KeyError : pass else : for light_id in light_data : self . _bulbs . append ( Bulb ( self , light_id , * light_data [ light_id ] ) ) return self . _bulbs
10447	def activatewindow ( self , window_name ) : window_handle = self . _get_window_handle ( window_name ) self . _grabfocus ( window_handle ) return 1
8705	def write_file ( self , path , destination = '' , verify = 'none' ) : filename = os . path . basename ( path ) if not destination : destination = filename log . info ( 'Transferring %s as %s' , path , destination ) self . __writeln ( "recv()" ) res = self . __expect ( 'C> ' ) if not res . endswith ( 'C> ' ) : log . error ( 'Error waiting for esp "%s"' , res ) raise CommunicationTimeout ( 'Error waiting for device to start receiving' , res ) log . debug ( 'sending destination filename "%s"' , destination ) self . __write ( destination + '\x00' , True ) if not self . __got_ack ( ) : log . error ( 'did not ack destination filename' ) raise NoAckException ( 'Device did not ACK destination filename' ) content = from_file ( path ) log . debug ( 'sending %d bytes in %s' , len ( content ) , filename ) pos = 0 chunk_size = 128 while pos < len ( content ) : rest = len ( content ) - pos if rest > chunk_size : rest = chunk_size data = content [ pos : pos + rest ] if not self . __write_chunk ( data ) : resp = self . __expect ( ) log . error ( 'Bad chunk response "%s" %s' , resp , hexify ( resp ) ) raise BadResponseException ( 'Bad chunk response' , ACK , resp ) pos += chunk_size log . debug ( 'sending zero block' ) self . __write_chunk ( '' ) if verify != 'none' : self . verify_file ( path , destination , verify )
4526	def run ( self , next_task ) : self . event . wait ( ) self . task ( ) self . event . clear ( ) next_task . event . set ( )
11340	def set_target_celsius ( self , celsius , mode = config . SCHEDULE_HOLD ) : temperature = celsius_to_nuheat ( celsius ) self . set_target_temperature ( temperature , mode )
9693	def cut ( self , by , from_start = True ) : s , e = copy ( self . start ) , copy ( self . end ) if from_start : e = s + by else : s = e - by return Range ( s , e )
7259	def search_point ( self , lat , lng , filters = None , startDate = None , endDate = None , types = None , type = None ) : searchAreaWkt = "POLYGON ((%s %s, %s %s, %s %s, %s %s, %s %s))" % ( lng , lat , lng , lat , lng , lat , lng , lat , lng , lat ) return self . search ( searchAreaWkt = searchAreaWkt , filters = filters , startDate = startDate , endDate = endDate , types = types )
8597	def delete_group ( self , group_id ) : response = self . _perform_request ( url = '/um/groups/%s' % group_id , method = 'DELETE' ) return response
6147	def freqz_cas ( sos , w ) : Ns , Mcol = sos . shape w , Hcas = signal . freqz ( sos [ 0 , : 3 ] , sos [ 0 , 3 : ] , w ) for k in range ( 1 , Ns ) : w , Htemp = signal . freqz ( sos [ k , : 3 ] , sos [ k , 3 : ] , w ) Hcas *= Htemp return w , Hcas
12897	def get_mute ( self ) : mute = ( yield from self . handle_int ( self . API . get ( 'mute' ) ) ) return bool ( mute )
7158	def add ( self , * args , ** kwargs ) : if 'question' in kwargs and isinstance ( kwargs [ 'question' ] , Question ) : question = kwargs [ 'question' ] else : question = Question ( * args , ** kwargs ) self . questions . setdefault ( question . key , [ ] ) . append ( question ) return question
8204	def snapshot ( self , target , defer = True , file_number = None ) : output_func = self . output_closure ( target , file_number ) if defer : self . _drawqueue . append ( output_func ) else : self . _drawqueue . append_immediate ( output_func )
7186	def maybe_replace_any_if_equal ( name , expected , actual ) : is_equal = expected == actual if not is_equal and Config . replace_any : actual_str = minimize_whitespace ( str ( actual ) ) if actual_str and actual_str [ 0 ] in { '"' , "'" } : actual_str = actual_str [ 1 : - 1 ] is_equal = actual_str in { 'Any' , 'typing.Any' , 't.Any' } if not is_equal : expected_annotation = minimize_whitespace ( str ( expected ) ) actual_annotation = minimize_whitespace ( str ( actual ) ) raise ValueError ( f"incompatible existing {name}. " + f"Expected: {expected_annotation!r}, actual: {actual_annotation!r}" ) return expected or actual
12981	def string ( html , start_on = None , ignore = ( ) , use_short = True , ** queries ) : if use_short : html = grow_short ( html ) return _to_template ( fromstring ( html ) , start_on = start_on , ignore = ignore , ** queries )
13910	def check_path_action ( self ) : class CheckPathAction ( argparse . Action ) : def __call__ ( self , parser , args , value , option_string = None ) : if type ( value ) is list : value = value [ 0 ] user_value = value if option_string == 'None' : if not os . path . isdir ( value ) : _current_user = os . path . expanduser ( "~" ) if not value . startswith ( _current_user ) and not value . startswith ( os . getcwd ( ) ) : if os . path . isdir ( os . path . join ( _current_user , value ) ) : value = os . path . join ( _current_user , value ) elif os . path . isdir ( os . path . join ( os . getcwd ( ) , value ) ) : value = os . path . join ( os . getcwd ( ) , value ) else : value = None else : value = None elif option_string == '--template-name' : if not os . path . isdir ( value ) : if not os . path . isdir ( os . path . join ( args . target , value ) ) : value = None if not value : logger . error ( "Could not to find path %s. Please provide " "correct path to %s option" , user_value , option_string ) exit ( 1 ) setattr ( args , self . dest , value ) return CheckPathAction
7675	def pprint_jobject ( obj , ** kwargs ) : obj_simple = { k : v for k , v in six . iteritems ( obj . __json__ ) if v } string = json . dumps ( obj_simple , ** kwargs ) string = re . sub ( r'[{}"]' , '' , string ) string = re . sub ( r',\n' , '\n' , string ) string = re . sub ( r'^\s*$' , '' , string ) return string
8791	def has_tag ( self , model ) : for tag in model . tags : if self . is_tag ( tag ) : return True return False
545	def __checkIfBestCompletedModel ( self ) : jobResultsStr = self . _jobsDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] if jobResultsStr is None : jobResults = { } else : jobResults = json . loads ( jobResultsStr ) isSaved = jobResults . get ( 'saved' , False ) bestMetric = jobResults . get ( 'bestValue' , None ) currentMetric = self . _getMetrics ( ) [ self . _optimizedMetricLabel ] self . _isBestModel = ( not isSaved ) or ( currentMetric < bestMetric ) return self . _isBestModel , jobResults , jobResultsStr
5184	def edges ( self , ** kwargs ) : edges = self . _query ( 'edges' , ** kwargs ) for edge in edges : identifier_source = edge [ 'source_type' ] + '[' + edge [ 'source_title' ] + ']' identifier_target = edge [ 'target_type' ] + '[' + edge [ 'target_title' ] + ']' yield Edge ( source = self . resources [ identifier_source ] , target = self . resources [ identifier_target ] , relationship = edge [ 'relationship' ] , node = edge [ 'certname' ] )
12833	def on_enter_stage ( self ) : with self . world . _unlock_temporarily ( ) : self . forum . connect_everyone ( self . world , self . actors ) self . forum . on_start_game ( ) with self . world . _unlock_temporarily ( ) : self . world . on_start_game ( ) num_players = len ( self . actors ) - 1 for actor in self . actors : actor . on_setup_gui ( self . gui ) for actor in self . actors : actor . on_start_game ( num_players )
1912	def SInt ( value , width ) : return Operators . ITEBV ( width , Bit ( value , width - 1 ) == 1 , GetNBits ( value , width ) - 2 ** width , GetNBits ( value , width ) )
11992	def set_signature_passphrases ( self , signature_passphrases ) : self . signature_passphrases = self . _update_dict ( signature_passphrases , { } , replace_data = True )
13610	def load_gitconfig ( self ) : gitconfig_path = os . path . expanduser ( '~/.gitconfig' ) if os . path . exists ( gitconfig_path ) : parser = Parser ( ) parser . read ( gitconfig_path ) parser . sections ( ) return parser pass
5012	def get_inactive_sap_learners ( self ) : now = datetime . datetime . utcnow ( ) if now >= self . expires_at : self . session . close ( ) self . _create_session ( ) sap_search_student_url = '{sapsf_base_url}/{search_students_path}?$filter={search_filter}' . format ( sapsf_base_url = self . enterprise_configuration . sapsf_base_url . rstrip ( '/' ) , search_students_path = self . global_sap_config . search_student_api_path . rstrip ( '/' ) , search_filter = 'criteria/isActive eq False&$select=studentID' , ) all_inactive_learners = self . _call_search_students_recursively ( sap_search_student_url , all_inactive_learners = [ ] , page_size = 500 , start_at = 0 ) return all_inactive_learners
6874	def _pycompress_sqlitecurve ( sqlitecurve , force = False ) : outfile = '%s.gz' % sqlitecurve try : if os . path . exists ( outfile ) and not force : os . remove ( sqlitecurve ) return outfile else : with open ( sqlitecurve , 'rb' ) as infd : with gzip . open ( outfile , 'wb' ) as outfd : shutil . copyfileobj ( infd , outfd ) if os . path . exists ( outfile ) : os . remove ( sqlitecurve ) return outfile except Exception as e : return None
7628	def namespace ( ns_key ) : if ns_key not in __NAMESPACE__ : raise NamespaceError ( 'Unknown namespace: {:s}' . format ( ns_key ) ) sch = copy . deepcopy ( JAMS_SCHEMA [ 'definitions' ] [ 'SparseObservation' ] ) for key in [ 'value' , 'confidence' ] : try : sch [ 'properties' ] [ key ] = __NAMESPACE__ [ ns_key ] [ key ] except KeyError : pass return sch
5167	def __intermediate_dns_search ( self , uci , address ) : if 'dns_search' in uci : return uci [ 'dns_search' ] if address [ 'proto' ] == 'none' : return None dns_search = self . netjson . get ( 'dns_search' , None ) if dns_search : return ' ' . join ( dns_search )
4073	def eval_environ ( value ) : def eval_environ_str ( value ) : parts = value . split ( ';' ) if len ( parts ) < 2 : return value expr = parts [ 1 ] . lstrip ( ) if not re . match ( "^((\\w+(\\.\\w+)?|'.*?'|\".*?\")\\s+" '(in|==|!=|not in)\\s+' "(\\w+(\\.\\w+)?|'.*?'|\".*?\")" '(\\s+(or|and)\\s+)?)+$' , expr ) : raise ValueError ( 'bad environment marker: %r' % expr ) expr = re . sub ( r"(platform\.\w+)" , r"\1()" , expr ) return parts [ 0 ] if eval ( expr ) else '' if isinstance ( value , list ) : new_value = [ ] for element in value : element = eval_environ_str ( element ) if element : new_value . append ( element ) elif isinstance ( value , str ) : new_value = eval_environ_str ( value ) else : new_value = value return new_value
1564	def get_metrics_collector ( self ) : if self . metrics_collector is None or not isinstance ( self . metrics_collector , MetricsCollector ) : raise RuntimeError ( "Metrics collector is not registered in this context" ) return self . metrics_collector
8338	def findParents ( self , name = None , attrs = { } , limit = None , ** kwargs ) : return self . _findAll ( name , attrs , None , limit , self . parentGenerator , ** kwargs )
8741	def create_floatingip ( context , content ) : LOG . info ( 'create_floatingip %s for tenant %s and body %s' % ( id , context . tenant_id , content ) ) network_id = content . get ( 'floating_network_id' ) if not network_id : raise n_exc . BadRequest ( resource = 'floating_ip' , msg = 'floating_network_id is required.' ) fixed_ip_address = content . get ( 'fixed_ip_address' ) ip_address = content . get ( 'floating_ip_address' ) port_id = content . get ( 'port_id' ) port = None port_fixed_ip = { } network = _get_network ( context , network_id ) if port_id : port = _get_port ( context , port_id ) fixed_ip = _get_fixed_ip ( context , fixed_ip_address , port ) port_fixed_ip = { port . id : { 'port' : port , 'fixed_ip' : fixed_ip } } flip = _allocate_ip ( context , network , port , ip_address , ip_types . FLOATING ) _create_flip ( context , flip , port_fixed_ip ) return v . _make_floating_ip_dict ( flip , port_id )
6850	def initrole ( self , check = True ) : if self . env . original_user is None : self . env . original_user = self . genv . user if self . env . original_key_filename is None : self . env . original_key_filename = self . genv . key_filename host_string = None user = None password = None if self . env . login_check : host_string , user , password = self . find_working_password ( usernames = [ self . genv . user , self . env . default_user ] , host_strings = [ self . genv . host_string , self . env . default_hostname ] , ) if self . verbose : print ( 'host.initrole.host_string:' , host_string ) print ( 'host.initrole.user:' , user ) print ( 'host.initrole.password:' , password ) needs = False if host_string is not None : self . genv . host_string = host_string if user is not None : self . genv . user = user if password is not None : self . genv . password = password if not needs : return assert self . env . default_hostname , 'No default hostname set.' assert self . env . default_user , 'No default user set.' self . genv . host_string = self . env . default_hostname if self . env . default_hosts : self . genv . hosts = self . env . default_hosts else : self . genv . hosts = [ self . env . default_hostname ] self . genv . user = self . env . default_user self . genv . password = self . env . default_password self . genv . key_filename = self . env . default_key_filename self . purge_keys ( ) for task_name in self . env . post_initrole_tasks : if self . verbose : print ( 'Calling post initrole task %s' % task_name ) satchel_name , method_name = task_name . split ( '.' ) satchel = self . get_satchel ( name = satchel_name ) getattr ( satchel , method_name ) ( ) print ( '^' * 80 ) print ( 'host.initrole.host_string:' , self . genv . host_string ) print ( 'host.initrole.user:' , self . genv . user ) print ( 'host.initrole.password:' , self . genv . password )
9698	def deliveries ( self ) : key = make_key ( event = self . object . event , owner_name = self . object . owner . username , identifier = self . object . identifier ) return redis . lrange ( key , 0 , 20 )
9270	def get_temp_tag_for_repo_creation ( self ) : tag_date = self . tag_times_dict . get ( REPO_CREATED_TAG_NAME , None ) if not tag_date : tag_name , tag_date = self . fetcher . fetch_repo_creation_date ( ) self . tag_times_dict [ tag_name ] = timestring_to_datetime ( tag_date ) return REPO_CREATED_TAG_NAME
11728	def _assert_contains ( haystack , needle , invert , escape = False ) : myneedle = re . escape ( needle ) if escape else needle matched = re . search ( myneedle , haystack , re . M ) if ( invert and matched ) or ( not invert and not matched ) : raise AssertionError ( "'%s' %sfound in '%s'" % ( needle , "" if invert else "not " , haystack ) )
4189	def window_poisson ( N , alpha = 2 ) : r n = linspace ( - N / 2. , ( N ) / 2. , N ) w = exp ( - alpha * abs ( n ) / ( N / 2. ) ) return w
5208	def format_intraday ( data : pd . DataFrame , ticker , ** kwargs ) -> pd . DataFrame : if data . empty : return pd . DataFrame ( ) data . columns = pd . MultiIndex . from_product ( [ [ ticker ] , data . rename ( columns = dict ( numEvents = 'num_trds' ) ) . columns ] , names = [ 'ticker' , 'field' ] ) data . index . name = None if kwargs . get ( 'price_only' , False ) : kw_xs = dict ( axis = 1 , level = 1 ) close = data . xs ( 'close' , ** kw_xs ) volume = data . xs ( 'volume' , ** kw_xs ) . iloc [ : , 0 ] return close . loc [ volume > 0 ] if volume . min ( ) > 0 else close else : return data
13208	def _parse_abstract ( self ) : command = LatexCommand ( 'setDocAbstract' , { 'name' : 'abstract' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no abstract' ) self . _abstract = None return try : content = parsed [ 'abstract' ] except KeyError : self . _logger . warning ( 'lsstdoc has no abstract' ) self . _abstract = None return content = content . strip ( ) self . _abstract = content
12472	def get_extension ( filepath , check_if_exists = False , allowed_exts = ALLOWED_EXTS ) : if check_if_exists : if not op . exists ( filepath ) : raise IOError ( 'File not found: ' + filepath ) rest , ext = op . splitext ( filepath ) if ext in allowed_exts : alloweds = allowed_exts [ ext ] _ , ext2 = op . splitext ( rest ) if ext2 in alloweds : ext = ext2 + ext return ext
999	def printColConfidence ( self , aState , maxCols = 20 ) : def formatFPRow ( var ) : s = '' for c in range ( min ( maxCols , self . numberOfCols ) ) : if c > 0 and c % 10 == 0 : s += ' ' s += ' %5.3f' % var [ c ] s += ' ' return s print formatFPRow ( aState )
3934	def _get_session_cookies ( session , access_token ) : headers = { 'Authorization' : 'Bearer {}' . format ( access_token ) } try : r = session . get ( ( 'https://accounts.google.com/accounts/OAuthLogin' '?source=hangups&issueuberauth=1' ) , headers = headers ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'OAuthLogin request failed: {}' . format ( e ) ) uberauth = r . text try : r = session . get ( ( 'https://accounts.google.com/MergeSession?' 'service=mail&' 'continue=http://www.google.com&uberauth={}' ) . format ( uberauth ) , headers = headers ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'MergeSession request failed: {}' . format ( e ) ) cookies = session . cookies . get_dict ( domain = '.google.com' ) if cookies == { } : raise GoogleAuthError ( 'Failed to find session cookies' ) return cookies
5693	def evaluate ( self , dep_time , first_leg_can_be_walk = True , connection_arrival_time = None ) : walk_labels = list ( ) if first_leg_can_be_walk and self . _walk_to_target_duration != float ( 'inf' ) : if connection_arrival_time is not None : walk_labels . append ( self . _get_label_to_target ( connection_arrival_time ) ) else : walk_labels . append ( self . _get_label_to_target ( dep_time ) ) if dep_time in self . dep_times_to_index : assert ( dep_time != float ( 'inf' ) ) index = self . dep_times_to_index [ dep_time ] labels = self . _label_bags [ index ] pareto_optimal_labels = merge_pareto_frontiers ( labels , walk_labels ) else : pareto_optimal_labels = walk_labels if not first_leg_can_be_walk : pareto_optimal_labels = [ label for label in pareto_optimal_labels if not label . first_leg_is_walk ] return pareto_optimal_labels
7920	def __prepare_local ( data ) : if not data : return None data = unicode ( data ) try : local = NODEPREP . prepare ( data ) except StringprepError , err : raise JIDError ( u"Local part invalid: {0}" . format ( err ) ) if len ( local . encode ( "utf-8" ) ) > 1023 : raise JIDError ( u"Local part too long" ) return local
11969	def _bits_to_dec ( nm , check = True ) : if check and not is_bits_nm ( nm ) : raise ValueError ( '_bits_to_dec: invalid netmask: "%s"' % nm ) bits = int ( str ( nm ) ) return VALID_NETMASKS [ bits ]
5297	def get_first_of_week ( self ) : if self . first_of_week is None : raise ImproperlyConfigured ( "%s.first_of_week is required." % self . __class__ . __name__ ) if self . first_of_week not in range ( 7 ) : raise ImproperlyConfigured ( "%s.first_of_week must be an integer between 0 and 6." % self . __class__ . __name__ ) return self . first_of_week
2563	def pull_tasks ( self , kill_event ) : logger . info ( "[TASK PULL THREAD] starting" ) poller = zmq . Poller ( ) poller . register ( self . task_incoming , zmq . POLLIN ) msg = self . create_reg_message ( ) logger . debug ( "Sending registration message: {}" . format ( msg ) ) self . task_incoming . send ( msg ) last_beat = time . time ( ) last_interchange_contact = time . time ( ) task_recv_counter = 0 poll_timer = 1 while not kill_event . is_set ( ) : time . sleep ( LOOP_SLOWDOWN ) ready_worker_count = self . ready_worker_queue . qsize ( ) pending_task_count = self . pending_task_queue . qsize ( ) logger . debug ( "[TASK_PULL_THREAD] ready workers:{}, pending tasks:{}" . format ( ready_worker_count , pending_task_count ) ) if time . time ( ) > last_beat + self . heartbeat_period : self . heartbeat ( ) last_beat = time . time ( ) if pending_task_count < self . max_queue_size and ready_worker_count > 0 : logger . debug ( "[TASK_PULL_THREAD] Requesting tasks: {}" . format ( ready_worker_count ) ) msg = ( ( ready_worker_count ) . to_bytes ( 4 , "little" ) ) self . task_incoming . send ( msg ) socks = dict ( poller . poll ( timeout = poll_timer ) ) if self . task_incoming in socks and socks [ self . task_incoming ] == zmq . POLLIN : _ , pkl_msg = self . task_incoming . recv_multipart ( ) tasks = pickle . loads ( pkl_msg ) last_interchange_contact = time . time ( ) if tasks == 'STOP' : logger . critical ( "[TASK_PULL_THREAD] Received stop request" ) kill_event . set ( ) break elif tasks == HEARTBEAT_CODE : logger . debug ( "Got heartbeat from interchange" ) else : poll_timer = 1 task_recv_counter += len ( tasks ) logger . debug ( "[TASK_PULL_THREAD] Got tasks: {} of {}" . format ( [ t [ 'task_id' ] for t in tasks ] , task_recv_counter ) ) for task in tasks : self . pending_task_queue . put ( task ) else : logger . debug ( "[TASK_PULL_THREAD] No incoming tasks" ) poll_timer = min ( self . heartbeat_period * 1000 , poll_timer * 2 ) if time . time ( ) > last_interchange_contact + self . heartbeat_threshold : logger . critical ( "[TASK_PULL_THREAD] Missing contact with interchange beyond heartbeat_threshold" ) kill_event . set ( ) logger . critical ( "[TASK_PULL_THREAD] Exiting" ) break
775	def _abbreviate ( text , threshold ) : if text is not None and len ( text ) > threshold : text = text [ : threshold ] + "..." return text
10599	def create_template ( material , path , show = False ) : file_name = 'dataset-%s.csv' % material . lower ( ) file_path = os . path . join ( path , file_name ) with open ( file_path , 'w' , newline = '' ) as csvfile : writer = csv . writer ( csvfile , delimiter = ',' , quotechar = '"' , quoting = csv . QUOTE_MINIMAL ) writer . writerow ( [ 'Name' , material ] ) writer . writerow ( [ 'Description' , '<Add a data set description ' 'here.>' ] ) writer . writerow ( [ 'Reference' , '<Add a reference to the source of ' 'the data set here.>' ] ) writer . writerow ( [ 'Temperature' , '<parameter 1 name>' , '<parameter 2 name>' , '<parameter 3 name>' ] ) writer . writerow ( [ 'T' , '<parameter 1 display symbol>' , '<parameter 2 display symbol>' , '<parameter 3 display symbol>' ] ) writer . writerow ( [ 'K' , '<parameter 1 units>' , '<parameter 2 units>' , '<parameter 3 units>' ] ) writer . writerow ( [ 'T' , '<parameter 1 symbol>' , '<parameter 2 symbol>' , '<parameter 3 symbol>' ] ) for i in range ( 10 ) : writer . writerow ( [ 100.0 + i * 50 , float ( i ) , 10.0 + i , 100.0 + i ] ) if show is True : webbrowser . open_new ( file_path )
2431	def build_tool ( self , doc , entity ) : match = self . tool_re . match ( entity ) if match and validations . validate_tool_name ( match . group ( self . TOOL_NAME_GROUP ) ) : name = match . group ( self . TOOL_NAME_GROUP ) return creationinfo . Tool ( name ) else : raise SPDXValueError ( 'Failed to extract tool name' )
11137	def path_required ( func ) : @ wraps ( func ) def wrapper ( self , * args , ** kwargs ) : if self . path is None : warnings . warn ( 'Must load (Repository.load_repository) or initialize (Repository.create_repository) the repository first !' ) return return func ( self , * args , ** kwargs ) return wrapper
8508	def _get_dataset ( self , X , y = None ) : from pylearn2 . datasets import DenseDesignMatrix X = np . asarray ( X ) assert X . ndim > 1 if y is not None : y = self . _get_labels ( y ) if X . ndim == 2 : return DenseDesignMatrix ( X = X , y = y ) return DenseDesignMatrix ( topo_view = X , y = y )
10117	def extend ( self , items , replace = True ) : if isinstance ( items , dict ) or isinstance ( items , SortableDict ) : items = list ( items . items ( ) ) for ( key , value ) in items : self . append ( key , value , replace = replace )
2941	def deserialize_workflow_spec ( self , s_state , filename = None ) : dom = minidom . parseString ( s_state ) node = dom . getElementsByTagName ( 'process-definition' ) [ 0 ] name = node . getAttribute ( 'name' ) if name == '' : _exc ( '%s without a name attribute' % node . nodeName ) workflow_spec = specs . WorkflowSpec ( name , filename ) del workflow_spec . task_specs [ 'Start' ] end = specs . Simple ( workflow_spec , 'End' ) , [ ] read_specs = dict ( end = end ) for child_node in node . childNodes : if child_node . nodeType != minidom . Node . ELEMENT_NODE : continue if child_node . nodeName == 'name' : workflow_spec . name = child_node . firstChild . nodeValue elif child_node . nodeName == 'description' : workflow_spec . description = child_node . firstChild . nodeValue elif child_node . nodeName . lower ( ) in _spec_map : self . deserialize_task_spec ( workflow_spec , child_node , read_specs ) else : _exc ( 'Unknown node: %s' % child_node . nodeName ) workflow_spec . start = read_specs [ 'start' ] [ 0 ] for name in read_specs : spec , successors = read_specs [ name ] for condition , successor_name in successors : if successor_name not in read_specs : _exc ( 'Unknown successor: "%s"' % successor_name ) successor , foo = read_specs [ successor_name ] if condition is None : spec . connect ( successor ) else : spec . connect_if ( condition , successor ) return workflow_spec
3642	def sell ( self , item_id , bid , buy_now , duration = 3600 , fast = False ) : method = 'POST' url = 'auctionhouse' data = { 'buyNowPrice' : buy_now , 'startingBid' : bid , 'duration' : duration , 'itemData' : { 'id' : item_id } } rc = self . __request__ ( method , url , data = json . dumps ( data ) , params = { 'sku_b' : self . sku_b } ) if not fast : self . tradeStatus ( rc [ 'id' ] ) return rc [ 'id' ]
967	def resetVector ( x1 , x2 ) : size = len ( x1 ) for i in range ( size ) : x2 [ i ] = x1 [ i ]
10648	def remove_component ( self , name ) : component_to_remove = None for c in self . components : if c . name == name : component_to_remove = c if component_to_remove is not None : self . components . remove ( component_to_remove )
6443	def _cond_n ( self , word , suffix_len ) : if len ( word ) - suffix_len >= 3 : if word [ - suffix_len - 3 ] == 's' : if len ( word ) - suffix_len >= 4 : return True else : return True return False
9254	def issue_line_with_user ( self , line , issue ) : if not issue . get ( "pull_request" ) or not self . options . author : return line if not issue . get ( "user" ) : line += u" (Null user)" elif self . options . username_as_tag : line += u" (@{0})" . format ( issue [ "user" ] [ "login" ] ) else : line += u" ([{0}]({1}))" . format ( issue [ "user" ] [ "login" ] , issue [ "user" ] [ "html_url" ] ) return line
7294	def create_list_dict ( self , document , list_field , doc_key ) : list_dict = { "_document" : document } if isinstance ( list_field . field , EmbeddedDocumentField ) : list_dict . update ( self . create_document_dictionary ( document = list_field . field . document_type_obj , owner_document = document ) ) list_dict . update ( { "_document_field" : list_field . field , "_key" : doc_key , "_field_type" : ListField , "_widget" : get_widget ( list_field . field ) , "_value" : getattr ( document , doc_key , None ) } ) return list_dict
7724	def __init ( self , affiliation , role , jid = None , nick = None , actor = None , reason = None ) : if not affiliation : affiliation = None elif affiliation not in affiliations : raise ValueError ( "Bad affiliation" ) self . affiliation = affiliation if not role : role = None elif role not in roles : raise ValueError ( "Bad role" ) self . role = role if jid : self . jid = JID ( jid ) else : self . jid = None if actor : self . actor = JID ( actor ) else : self . actor = None self . nick = nick self . reason = reason
9364	def domain_name ( ) : result = random . choice ( get_dictionary ( 'company_names' ) ) . strip ( ) result += '.' + top_level_domain ( ) return result . lower ( )
9158	def version ( ) : with io . open ( 'pgmagick/_version.py' ) as input_file : for line in input_file : if line . startswith ( '__version__' ) : return ast . parse ( line ) . body [ 0 ] . value . s
7350	def predict ( self , sequences ) : with tempfile . NamedTemporaryFile ( suffix = ".fsa" , mode = "w" ) as input_fd : for ( i , sequence ) in enumerate ( sequences ) : input_fd . write ( "> %d\n" % i ) input_fd . write ( sequence ) input_fd . write ( "\n" ) input_fd . flush ( ) try : output = subprocess . check_output ( [ "netChop" , input_fd . name ] ) except subprocess . CalledProcessError as e : logging . error ( "Error calling netChop: %s:\n%s" % ( e , e . output ) ) raise parsed = self . parse_netchop ( output ) assert len ( parsed ) == len ( sequences ) , "Expected %d results but got %d" % ( len ( sequences ) , len ( parsed ) ) assert [ len ( x ) for x in parsed ] == [ len ( x ) for x in sequences ] return parsed
10438	def startprocessmonitor ( self , process_name , interval = 2 ) : if process_name in self . _process_stats : self . _process_stats [ process_name ] . stop ( ) self . _process_stats [ process_name ] = ProcessStats ( process_name , interval ) self . _process_stats [ process_name ] . start ( ) return 1
253	def extract_round_trips ( transactions , portfolio_value = None ) : transactions = _groupby_consecutive ( transactions ) roundtrips = [ ] for sym , trans_sym in transactions . groupby ( 'symbol' ) : trans_sym = trans_sym . sort_index ( ) price_stack = deque ( ) dt_stack = deque ( ) trans_sym [ 'signed_price' ] = trans_sym . price * np . sign ( trans_sym . amount ) trans_sym [ 'abs_amount' ] = trans_sym . amount . abs ( ) . astype ( int ) for dt , t in trans_sym . iterrows ( ) : if t . price < 0 : warnings . warn ( 'Negative price detected, ignoring for' 'round-trip.' ) continue indiv_prices = [ t . signed_price ] * t . abs_amount if ( len ( price_stack ) == 0 ) or ( copysign ( 1 , price_stack [ - 1 ] ) == copysign ( 1 , t . amount ) ) : price_stack . extend ( indiv_prices ) dt_stack . extend ( [ dt ] * len ( indiv_prices ) ) else : pnl = 0 invested = 0 cur_open_dts = [ ] for price in indiv_prices : if len ( price_stack ) != 0 and ( copysign ( 1 , price_stack [ - 1 ] ) != copysign ( 1 , price ) ) : prev_price = price_stack . popleft ( ) prev_dt = dt_stack . popleft ( ) pnl += - ( price + prev_price ) cur_open_dts . append ( prev_dt ) invested += abs ( prev_price ) else : price_stack . append ( price ) dt_stack . append ( dt ) roundtrips . append ( { 'pnl' : pnl , 'open_dt' : cur_open_dts [ 0 ] , 'close_dt' : dt , 'long' : price < 0 , 'rt_returns' : pnl / invested , 'symbol' : sym , } ) roundtrips = pd . DataFrame ( roundtrips ) roundtrips [ 'duration' ] = roundtrips [ 'close_dt' ] . sub ( roundtrips [ 'open_dt' ] ) if portfolio_value is not None : pv = pd . DataFrame ( portfolio_value , columns = [ 'portfolio_value' ] ) . assign ( date = portfolio_value . index ) roundtrips [ 'date' ] = roundtrips . close_dt . apply ( lambda x : x . replace ( hour = 0 , minute = 0 , second = 0 ) ) tmp = roundtrips . join ( pv , on = 'date' , lsuffix = '_' ) roundtrips [ 'returns' ] = tmp . pnl / tmp . portfolio_value roundtrips = roundtrips . drop ( 'date' , axis = 'columns' ) return roundtrips
1745	def _set_perms ( self , perms ) : assert isinstance ( perms , str ) and len ( perms ) <= 3 and perms . strip ( ) in [ '' , 'r' , 'w' , 'x' , 'rw' , 'r x' , 'rx' , 'rwx' , 'wx' , ] self . _perms = perms
11899	def _get_src_from_image ( img , fallback_image_file ) : if img is None : return fallback_image_file target_format = img . format if target_format . lower ( ) in [ 'tif' , 'tiff' ] : target_format = 'JPEG' try : bytesio = io . BytesIO ( ) img . save ( bytesio , target_format ) byte_value = bytesio . getvalue ( ) b64 = base64 . b64encode ( byte_value ) return 'data:image/%s;base64,%s' % ( target_format . lower ( ) , b64 ) except IOError as exptn : print ( 'IOError while saving image bytes: %s' % exptn ) return fallback_image_file
2466	def set_concluded_license ( self , doc , lic ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_conc_lics_set : self . file_conc_lics_set = True if validations . validate_lics_conc ( lic ) : self . file ( doc ) . conc_lics = lic return True else : raise SPDXValueError ( 'File::ConcludedLicense' ) else : raise CardinalityError ( 'File::ConcludedLicense' ) else : raise OrderError ( 'File::ConcludedLicense' )
5850	def matched_file_count ( self , dataset_id , glob = "." , is_dir = False ) : list_result = self . list_files ( dataset_id , glob , is_dir ) return len ( list_result )
10363	def has_protein_modification_increases_activity ( graph : BELGraph , source : BaseEntity , target : BaseEntity , key : str , ) -> bool : edge_data = graph [ source ] [ target ] [ key ] return has_protein_modification ( graph , source ) and part_has_modifier ( edge_data , OBJECT , ACTIVITY )
2962	def expand_partitions ( containers , partitions ) : all_names = frozenset ( c . name for c in containers if not c . holy ) holy_names = frozenset ( c . name for c in containers if c . holy ) neutral_names = frozenset ( c . name for c in containers if c . neutral ) partitions = [ frozenset ( p ) for p in partitions ] unknown = set ( ) holy = set ( ) union = set ( ) for partition in partitions : unknown . update ( partition - all_names - holy_names ) holy . update ( partition - all_names ) union . update ( partition ) if unknown : raise BlockadeError ( 'Partitions contain unknown containers: %s' % list ( unknown ) ) if holy : raise BlockadeError ( 'Partitions contain holy containers: %s' % list ( holy ) ) leftover = all_names . difference ( union ) if leftover : partitions . append ( leftover ) if not neutral_names . issubset ( leftover ) : partitions . append ( neutral_names ) return partitions
9108	def cleanup ( self ) : try : remove ( join ( self . fs_path , u'message' ) ) remove ( join ( self . fs_path , 'dirty.zip.pgp' ) ) except OSError : pass shutil . rmtree ( join ( self . fs_path , u'clean' ) , ignore_errors = True ) shutil . rmtree ( join ( self . fs_path , u'attach' ) , ignore_errors = True )
2439	def reset_annotations ( self ) : self . annotation_date_set = False self . annotation_comment_set = False self . annotation_type_set = False self . annotation_spdx_id_set = False
11787	def add ( self , o ) : "Add an observation o to the distribution." self . smooth_for ( o ) self . dictionary [ o ] += 1 self . n_obs += 1 self . sampler = None
8101	def copy ( self , graph ) : g = styleguide ( graph ) g . order = self . order dict . __init__ ( g , [ ( k , v ) for k , v in self . iteritems ( ) ] ) return g
13119	def count ( self , * args , ** kwargs ) : search = self . create_search ( * args , ** kwargs ) try : return search . count ( ) except NotFoundError : print_error ( "The index was not found, have you initialized the index?" ) except ( ConnectionError , TransportError ) : print_error ( "Cannot connect to elasticsearch" )
7041	def list_lc_collections ( lcc_server ) : url = '%s/api/collections' % lcc_server try : LOGINFO ( 'getting list of recent publicly visible ' 'and owned LC collections from %s' % ( lcc_server , ) ) have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } req = Request ( url , data = None , headers = headers ) resp = urlopen ( req ) lcc_list = json . loads ( resp . read ( ) ) [ 'result' ] [ 'collections' ] return lcc_list except HTTPError as e : LOGERROR ( 'could not retrieve list of collections, ' 'URL used: %s, error code: %s, reason: %s' % ( url , e . code , e . reason ) ) return None
2753	def get_domain ( self , domain_name ) : return Domain . get_object ( api_token = self . token , domain_name = domain_name )
1788	def DAS ( cpu ) : oldAL = cpu . AL oldCF = cpu . CF cpu . AF = Operators . OR ( ( cpu . AL & 0x0f ) > 9 , cpu . AF ) cpu . AL = Operators . ITEBV ( 8 , cpu . AF , cpu . AL - 6 , cpu . AL ) cpu . CF = Operators . ITE ( cpu . AF , Operators . OR ( oldCF , cpu . AL > oldAL ) , cpu . CF ) cpu . CF = Operators . ITE ( Operators . OR ( oldAL > 0x99 , oldCF ) , True , cpu . CF ) cpu . AL = Operators . ITEBV ( 8 , Operators . OR ( oldAL > 0x99 , oldCF ) , cpu . AL - 0x60 , cpu . AL ) cpu . ZF = cpu . AL == 0 cpu . SF = ( cpu . AL & 0x80 ) != 0 cpu . PF = cpu . _calculate_parity_flag ( cpu . AL )
12537	def scrape_all_files ( self ) : try : for dcmf in self . items : yield self . read_dcm ( dcmf ) except IOError as ioe : raise IOError ( 'Error reading DICOM file: {}.' . format ( dcmf ) ) from ioe
9470	def conference_list_members ( self , call_params ) : path = '/' + self . api_version + '/ConferenceListMembers/' method = 'POST' return self . request ( path , method , call_params )
10130	def timezone ( haystack_tz , version = LATEST_VER ) : tz_map = get_tz_map ( version = version ) try : tz_name = tz_map [ haystack_tz ] except KeyError : raise ValueError ( '%s is not a recognised timezone on this host' % haystack_tz ) return pytz . timezone ( tz_name )
3546	def _characteristics_discovered ( self , service ) : self . _discovered_services . add ( service ) if self . _discovered_services >= set ( self . _peripheral . services ( ) ) : self . _discovered . set ( )
1677	def ResetSection ( self , directive ) : self . _section = self . _INITIAL_SECTION self . _last_header = '' if directive in ( 'if' , 'ifdef' , 'ifndef' ) : self . include_list . append ( [ ] ) elif directive in ( 'else' , 'elif' ) : self . include_list [ - 1 ] = [ ]
3523	def intercom_user_hash ( data ) : if getattr ( settings , 'INTERCOM_HMAC_SECRET_KEY' , None ) : return hmac . new ( key = _hashable_bytes ( settings . INTERCOM_HMAC_SECRET_KEY ) , msg = _hashable_bytes ( data ) , digestmod = hashlib . sha256 , ) . hexdigest ( ) else : return None
552	def __setAsOrphaned ( self ) : cmplReason = ClientJobsDAO . CMPL_REASON_ORPHAN cmplMessage = "Killed by Scheduler" self . _jobsDAO . modelSetCompleted ( self . _modelID , cmplReason , cmplMessage )
5092	def _login ( self , email , password ) : response = requests . post ( urljoin ( self . ENDPOINT , 'sessions' ) , json = { 'email' : email , 'password' : password , 'platform' : 'ios' , 'token' : binascii . hexlify ( os . urandom ( 64 ) ) . decode ( 'utf8' ) } , headers = self . _headers ) response . raise_for_status ( ) access_token = response . json ( ) [ 'access_token' ] self . _headers [ 'Authorization' ] = 'Token token=%s' % access_token
6876	def _gzip_sqlitecurve ( sqlitecurve , force = False ) : if force : cmd = 'gzip -k -f %s' % sqlitecurve else : cmd = 'gzip -k %s' % sqlitecurve try : outfile = '%s.gz' % sqlitecurve if os . path . exists ( outfile ) and not force : os . remove ( sqlitecurve ) return outfile else : subprocess . check_output ( cmd , shell = True ) if os . path . exists ( outfile ) : return outfile else : return None except subprocess . CalledProcessError : return None
11269	def substitute ( prev , * args , ** kw ) : template_obj = string . Template ( * args , ** kw ) for data in prev : yield template_obj . substitute ( data )
12197	def get_task_options ( ) : options = ( ) task_classes = get_tasks ( ) for cls in task_classes : options += cls . option_list return options
5882	def nodes_to_check ( self , docs ) : nodes_to_check = [ ] for doc in docs : for tag in [ 'p' , 'pre' , 'td' ] : items = self . parser . getElementsByTag ( doc , tag = tag ) nodes_to_check += items return nodes_to_check
10856	def _tile ( self , n ) : pos = self . _trans ( self . pos [ n ] ) return Tile ( pos , pos ) . pad ( self . support_pad )
2914	def _inherit_data ( self ) : LOG . debug ( "'%s' inheriting data from '%s'" % ( self . get_name ( ) , self . parent . get_name ( ) ) , extra = dict ( data = self . parent . data ) ) self . set_data ( ** self . parent . data )
760	def appendInputWithNSimilarValues ( inputs , numNear = 10 ) : numInputs = len ( inputs ) skipOne = False for i in xrange ( numInputs ) : input = inputs [ i ] numChanged = 0 newInput = copy . deepcopy ( input ) for j in xrange ( len ( input ) - 1 ) : if skipOne : skipOne = False continue if input [ j ] == 1 and input [ j + 1 ] == 0 : newInput [ j ] = 0 newInput [ j + 1 ] = 1 inputs . append ( newInput ) newInput = copy . deepcopy ( newInput ) numChanged += 1 skipOne = True if numChanged == numNear : break
12802	def get_room_by_name ( self , name ) : rooms = self . get_rooms ( ) for room in rooms or [ ] : if room [ "name" ] == name : return self . get_room ( room [ "id" ] ) raise RoomNotFoundException ( "Room %s not found" % name )
4199	def identify_names ( code ) : finder = NameFinder ( ) finder . visit ( ast . parse ( code ) ) example_code_obj = { } for name , full_name in finder . get_mapping ( ) : module , attribute = full_name . rsplit ( '.' , 1 ) module_short = get_short_module_name ( module , attribute ) cobj = { 'name' : attribute , 'module' : module , 'module_short' : module_short } example_code_obj [ name ] = cobj return example_code_obj
11980	def set_ip ( self , ip ) : self . set ( ip = ip , netmask = self . _nm )
12559	def largest_connected_component ( volume ) : volume = np . asarray ( volume ) labels , num_labels = scn . label ( volume ) if not num_labels : raise ValueError ( 'No non-zero values: no connected components found.' ) if num_labels == 1 : return volume . astype ( np . bool ) label_count = np . bincount ( labels . ravel ( ) . astype ( np . int ) ) label_count [ 0 ] = 0 return labels == label_count . argmax ( )
3131	def merge_results ( x , y ) : z = x . copy ( ) for key , value in y . items ( ) : if isinstance ( value , list ) and isinstance ( z . get ( key ) , list ) : z [ key ] += value else : z [ key ] = value return z
11505	def move_folder ( self , token , folder_id , dest_folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = folder_id parameters [ 'dstfolderid' ] = dest_folder_id response = self . request ( 'midas.folder.move' , parameters ) return response
13138	def http_get_provider ( provider , request_url , params , token_secret , token_cookie = None ) : if not validate_provider ( provider ) : raise InvalidUsage ( 'Provider not supported' ) klass = getattr ( socialauth . providers , provider . capitalize ( ) ) provider = klass ( request_url , params , token_secret , token_cookie ) if provider . status == 302 : ret = dict ( status = 302 , redirect = provider . redirect ) tc = getattr ( provider , 'set_token_cookie' , None ) if tc is not None : ret [ 'set_token_cookie' ] = tc return ret if provider . status == 200 and provider . user_id is not None : ret = dict ( status = 200 , provider_user_id = provider . user_id ) if provider . user_name is not None : ret [ 'provider_user_name' ] = provider . user_name return ret raise InvalidUsage ( 'Invalid request' )
9431	def dostime_to_timetuple ( dostime ) : dostime = dostime >> 16 dostime = dostime & 0xffff day = dostime & 0x1f month = ( dostime >> 5 ) & 0xf year = 1980 + ( dostime >> 9 ) second = 2 * ( dostime & 0x1f ) minute = ( dostime >> 5 ) & 0x3f hour = dostime >> 11 return ( year , month , day , hour , minute , second )
6268	def resolve_loader ( self , meta : SceneDescription ) : for loader_cls in self . _loaders : if loader_cls . supports_file ( meta ) : meta . loader_cls = loader_cls break else : raise ImproperlyConfigured ( "Scene {} has no loader class registered. Check settings.SCENE_LOADERS" . format ( meta . path ) )
6125	def plot_image ( image , plot_origin = True , mask = None , extract_array_from_mask = False , zoom_around_mask = False , should_plot_border = False , positions = None , as_subplot = False , units = 'arcsec' , kpc_per_arcsec = None , figsize = ( 7 , 7 ) , aspect = 'square' , cmap = 'jet' , norm = 'linear' , norm_min = None , norm_max = None , linthresh = 0.05 , linscale = 0.01 , cb_ticksize = 10 , cb_fraction = 0.047 , cb_pad = 0.01 , cb_tick_values = None , cb_tick_labels = None , title = 'Image' , titlesize = 16 , xlabelsize = 16 , ylabelsize = 16 , xyticksize = 16 , mask_pointsize = 10 , position_pointsize = 30 , grid_pointsize = 1 , output_path = None , output_format = 'show' , output_filename = 'image' ) : origin = get_origin ( array = image , plot_origin = plot_origin ) array_plotters . plot_array ( array = image , origin = origin , mask = mask , extract_array_from_mask = extract_array_from_mask , zoom_around_mask = zoom_around_mask , should_plot_border = should_plot_border , positions = positions , as_subplot = as_subplot , units = units , kpc_per_arcsec = kpc_per_arcsec , figsize = figsize , aspect = aspect , cmap = cmap , norm = norm , norm_min = norm_min , norm_max = norm_max , linthresh = linthresh , linscale = linscale , cb_ticksize = cb_ticksize , cb_fraction = cb_fraction , cb_pad = cb_pad , cb_tick_values = cb_tick_values , cb_tick_labels = cb_tick_labels , title = title , titlesize = titlesize , xlabelsize = xlabelsize , ylabelsize = ylabelsize , xyticksize = xyticksize , mask_pointsize = mask_pointsize , position_pointsize = position_pointsize , grid_pointsize = grid_pointsize , output_path = output_path , output_format = output_format , output_filename = output_filename )
2092	def last_job_data ( self , pk = None , ** kwargs ) : ujt = self . get ( pk , include_debug_header = True , ** kwargs ) if 'current_update' in ujt [ 'related' ] : debug . log ( 'A current job; retrieving it.' , header = 'details' ) return client . get ( ujt [ 'related' ] [ 'current_update' ] [ 7 : ] ) . json ( ) elif ujt [ 'related' ] . get ( 'last_update' , None ) : debug . log ( 'No current job or update exists; retrieving the most recent.' , header = 'details' ) return client . get ( ujt [ 'related' ] [ 'last_update' ] [ 7 : ] ) . json ( ) else : raise exc . NotFound ( 'No related jobs or updates exist.' )
4312	def _validate_volumes ( input_volumes ) : if not ( input_volumes is None or isinstance ( input_volumes , list ) ) : raise TypeError ( "input_volumes must be None or a list." ) if isinstance ( input_volumes , list ) : for vol in input_volumes : if not core . is_number ( vol ) : raise ValueError ( "Elements of input_volumes must be numbers: found {}" . format ( vol ) )
11613	def report_depths ( self , filename , tpm = True , grp_wise = False , reorder = 'as-is' , notes = None ) : if grp_wise : lname = self . probability . gname depths = self . allelic_expression * self . grp_conv_mat else : lname = self . probability . lname depths = self . allelic_expression if tpm : depths *= ( 1000000.0 / depths . sum ( ) ) total_depths = depths . sum ( axis = 0 ) if reorder == 'decreasing' : report_order = np . argsort ( total_depths . flatten ( ) ) report_order = report_order [ : : - 1 ] elif reorder == 'increasing' : report_order = np . argsort ( total_depths . flatten ( ) ) elif reorder == 'as-is' : report_order = np . arange ( len ( lname ) ) cntdata = np . vstack ( ( depths , total_depths ) ) fhout = open ( filename , 'w' ) fhout . write ( "locus\t" + "\t" . join ( self . probability . hname ) + "\ttotal" ) if notes is not None : fhout . write ( "\tnotes" ) fhout . write ( "\n" ) for locus_id in report_order : lname_cur = lname [ locus_id ] fhout . write ( "\t" . join ( [ lname_cur ] + map ( str , cntdata [ : , locus_id ] . ravel ( ) ) ) ) if notes is not None : fhout . write ( "\t%s" % notes [ lname_cur ] ) fhout . write ( "\n" ) fhout . close ( )
8943	def search_file_upwards ( name , base = None ) : base = base or os . getcwd ( ) while base != os . path . dirname ( base ) : if os . path . exists ( os . path . join ( base , name ) ) : return base base = os . path . dirname ( base ) return None
11068	def delete_acl ( self , name ) : if name not in self . _acl : return False del self . _acl [ name ] return True
1037	def begin ( self ) : return Range ( self . source_buffer , self . begin_pos , self . begin_pos , expanded_from = self . expanded_from )
2733	def get_object ( cls , api_token ) : acct = cls ( token = api_token ) acct . load ( ) return acct
3727	def Vc ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ SURF ] ) : r def list_methods ( ) : methods = [ ] if CASRN in _crit_IUPAC . index and not np . isnan ( _crit_IUPAC . at [ CASRN , 'Vc' ] ) : methods . append ( IUPAC ) if CASRN in _crit_Matthews . index and not np . isnan ( _crit_Matthews . at [ CASRN , 'Vc' ] ) : methods . append ( MATTHEWS ) if CASRN in _crit_CRC . index and not np . isnan ( _crit_CRC . at [ CASRN , 'Vc' ] ) : methods . append ( CRC ) if CASRN in _crit_PSRKR4 . index and not np . isnan ( _crit_PSRKR4 . at [ CASRN , 'Vc' ] ) : methods . append ( PSRK ) if CASRN in _crit_Yaws . index and not np . isnan ( _crit_Yaws . at [ CASRN , 'Vc' ] ) : methods . append ( YAWS ) if CASRN : methods . append ( SURF ) if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IUPAC : _Vc = float ( _crit_IUPAC . at [ CASRN , 'Vc' ] ) elif Method == PSRK : _Vc = float ( _crit_PSRKR4 . at [ CASRN , 'Vc' ] ) elif Method == MATTHEWS : _Vc = float ( _crit_Matthews . at [ CASRN , 'Vc' ] ) elif Method == CRC : _Vc = float ( _crit_CRC . at [ CASRN , 'Vc' ] ) elif Method == YAWS : _Vc = float ( _crit_Yaws . at [ CASRN , 'Vc' ] ) elif Method == SURF : _Vc = third_property ( CASRN = CASRN , V = True ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' ) return _Vc
9628	def detail_view ( self , request ) : context = { 'preview' : self , } kwargs = { } if self . form_class : if request . GET : form = self . form_class ( data = request . GET ) else : form = self . form_class ( ) context [ 'form' ] = form if not form . is_bound or not form . is_valid ( ) : return render ( request , 'mailviews/previews/detail.html' , context ) kwargs . update ( form . get_message_view_kwargs ( ) ) message_view = self . get_message_view ( request , ** kwargs ) message = message_view . render_to_message ( ) raw = message . message ( ) headers = OrderedDict ( ( header , maybe_decode_header ( raw [ header ] ) ) for header in self . headers ) context . update ( { 'message' : message , 'subject' : message . subject , 'body' : message . body , 'headers' : headers , 'raw' : raw . as_string ( ) , } ) alternatives = getattr ( message , 'alternatives' , [ ] ) try : html = next ( alternative [ 0 ] for alternative in alternatives if alternative [ 1 ] == 'text/html' ) context . update ( { 'html' : html , 'escaped_html' : b64encode ( html . encode ( 'utf-8' ) ) , } ) except StopIteration : pass return render ( request , self . template_name , context )
5337	def create_dashboard ( self , panel_file , data_sources = None , strict = True ) : es_enrich = self . conf [ 'es_enrichment' ] [ 'url' ] kibana_url = self . conf [ 'panels' ] [ 'kibiter_url' ] mboxes_sources = set ( [ 'pipermail' , 'hyperkitty' , 'groupsio' , 'nntp' ] ) if data_sources and any ( x in data_sources for x in mboxes_sources ) : data_sources = list ( data_sources ) data_sources . append ( 'mbox' ) if data_sources and ( 'supybot' in data_sources ) : data_sources = list ( data_sources ) data_sources . append ( 'irc' ) if data_sources and 'google_hits' in data_sources : data_sources = list ( data_sources ) data_sources . append ( 'googlehits' ) if data_sources and 'stackexchange' in data_sources : data_sources = list ( data_sources ) data_sources . append ( 'stackoverflow' ) if data_sources and 'phabricator' in data_sources : data_sources = list ( data_sources ) data_sources . append ( 'maniphest' ) try : import_dashboard ( es_enrich , kibana_url , panel_file , data_sources = data_sources , strict = strict ) except ValueError : logger . error ( "%s does not include release field. Not loading the panel." , panel_file ) except RuntimeError : logger . error ( "Can not load the panel %s" , panel_file )
7482	def assembly_cleanup ( data ) : data . stats_dfs . s2 = data . _build_stat ( "s2" ) data . stats_files . s2 = os . path . join ( data . dirs . edits , 's2_rawedit_stats.txt' ) with io . open ( data . stats_files . s2 , 'w' , encoding = 'utf-8' ) as outfile : data . stats_dfs . s2 . fillna ( value = 0 ) . astype ( np . int ) . to_string ( outfile )
9846	def resample_factor ( self , factor ) : newlengths = [ ( N - 1 ) * float ( factor ) + 1 for N in self . _len_edges ( ) ] edges = [ numpy . linspace ( start , stop , num = int ( N ) , endpoint = True ) for ( start , stop , N ) in zip ( self . _min_edges ( ) , self . _max_edges ( ) , newlengths ) ] return self . resample ( edges )
12615	def count ( self , table_name , sample ) : return len ( list ( search_sample ( table = self . table ( table_name ) , sample = sample ) ) )
9208	def remove_prefix ( bytes_ ) : prefix_int = extract_prefix ( bytes_ ) prefix = varint . encode ( prefix_int ) return bytes_ [ len ( prefix ) : ]
9536	def enumeration ( * args ) : assert len ( args ) > 0 , 'at least one argument is required' if len ( args ) == 1 : members = args [ 0 ] else : members = args def checker ( value ) : if value not in members : raise ValueError ( value ) return checker
5848	def get_preferred_credentials ( api_key , site , cred_file = DEFAULT_CITRINATION_CREDENTIALS_FILE ) : profile_api_key , profile_site = get_credentials_from_file ( cred_file ) if api_key is None : api_key = os . environ . get ( citr_env_vars . CITRINATION_API_KEY ) if api_key is None or len ( api_key ) == 0 : api_key = profile_api_key if site is None : site = os . environ . get ( citr_env_vars . CITRINATION_SITE ) if site is None or len ( site ) == 0 : site = profile_site if site is None : site = "https://citrination.com" return api_key , site
3893	def _get_parser ( extra_args ) : parser = argparse . ArgumentParser ( formatter_class = argparse . ArgumentDefaultsHelpFormatter , ) dirs = appdirs . AppDirs ( 'hangups' , 'hangups' ) default_token_path = os . path . join ( dirs . user_cache_dir , 'refresh_token.txt' ) parser . add_argument ( '--token-path' , default = default_token_path , help = 'path used to store OAuth refresh token' ) parser . add_argument ( '-d' , '--debug' , action = 'store_true' , help = 'log detailed debugging messages' ) for extra_arg in extra_args : parser . add_argument ( extra_arg , required = True ) return parser
9651	def check_shastore_version ( from_store , settings ) : sprint = settings [ "sprint" ] error = settings [ "error" ] sprint ( "checking .shastore version for potential incompatibilities" , level = "verbose" ) if not from_store or 'sake version' not in from_store : errmes = [ "Since you've used this project last, a new version of " , "sake was installed that introduced backwards incompatible" , " changes. Run 'sake clean', and rebuild before continuing\n" ] errmes = " " . join ( errmes ) error ( errmes ) sys . exit ( 1 )
8347	def _getAttrMap ( self ) : if not getattr ( self , 'attrMap' ) : self . attrMap = { } for ( key , value ) in self . attrs : self . attrMap [ key ] = value return self . attrMap
7286	def has_delete_permission ( self , request ) : return request . user . is_authenticated and request . user . is_active and request . user . is_superuser
10071	def pid ( self ) : pid = self . deposit_fetcher ( self . id , self ) return PersistentIdentifier . get ( pid . pid_type , pid . pid_value )
5684	def day_start_ut ( self , ut ) : old_tz = self . set_current_process_time_zone ( ) ut = time . mktime ( time . localtime ( ut ) [ : 3 ] + ( 12 , 00 , 0 , 0 , 0 , - 1 ) ) - 43200 set_process_timezone ( old_tz ) return ut
13566	def plot ( * args , ax = None , ** kwargs ) : if ax is None : fig , ax = _setup_axes ( ) pl = ax . plot ( * args , ** kwargs ) if _np . shape ( args ) [ 0 ] > 1 : if type ( args [ 1 ] ) is not str : min_x = min ( args [ 0 ] ) max_x = max ( args [ 0 ] ) ax . set_xlim ( ( min_x , max_x ) ) return pl
3153	def all ( self , list_id , ** queryparams ) : return self . _mc_client . _get ( url = self . _build_path ( list_id , 'segments' ) , ** queryparams )
9740	def get_2d_markers_linearized ( self , component_info = None , data = None , component_position = None , index = None ) : return self . _get_2d_markers ( data , component_info , component_position , index = index )
9603	def from_object ( cls , obj ) : return cls ( obj . get ( 'sessionId' , None ) , obj . get ( 'status' , 0 ) , obj . get ( 'value' , None ) )
4259	def read_markdown ( filename ) : global MD with open ( filename , 'r' , encoding = 'utf-8-sig' ) as f : text = f . read ( ) if MD is None : MD = Markdown ( extensions = [ 'markdown.extensions.meta' , 'markdown.extensions.tables' ] , output_format = 'html5' ) else : MD . reset ( ) MD . Meta = { } output = { 'description' : Markup ( MD . convert ( text ) ) } try : meta = MD . Meta . copy ( ) except AttributeError : pass else : output [ 'meta' ] = meta try : output [ 'title' ] = MD . Meta [ 'title' ] [ 0 ] except KeyError : pass return output
7787	def _try_backup_item ( self ) : if not self . _backup_state : return False item = self . cache . get_item ( self . address , self . _backup_state ) if item : self . _object_handler ( item . address , item . value , item . state ) return True else : False
2910	def _find_ancestor ( self , task_spec ) : if self . parent is None : return self if self . parent . task_spec == task_spec : return self . parent return self . parent . _find_ancestor ( task_spec )
10630	def clone ( self ) : result = copy . copy ( self ) result . _compound_mfrs = copy . deepcopy ( self . _compound_mfrs ) return result
6775	def force_stop_and_purge ( self ) : r = self . local_renderer self . stop ( ) with settings ( warn_only = True ) : r . sudo ( 'killall rabbitmq-server' ) with settings ( warn_only = True ) : r . sudo ( 'killall beam.smp' ) r . sudo ( 'rm -Rf /var/lib/rabbitmq/mnesia/*' )
3082	def _build_state_value ( request_handler , user ) : uri = request_handler . request . url token = xsrfutil . generate_token ( xsrf_secret_key ( ) , user . user_id ( ) , action_id = str ( uri ) ) return uri + ':' + token
2185	def tryload ( self , cfgstr = None , on_error = 'raise' ) : cfgstr = self . _rectify_cfgstr ( cfgstr ) if self . enabled : try : if self . verbose > 1 : self . log ( '[cacher] tryload fname={}' . format ( self . fname ) ) return self . load ( cfgstr ) except IOError : if self . verbose > 0 : self . log ( '[cacher] ... {} cache miss' . format ( self . fname ) ) except Exception : if self . verbose > 0 : self . log ( '[cacher] ... failed to load' ) if on_error == 'raise' : raise elif on_error == 'clear' : self . clear ( cfgstr ) return None else : raise KeyError ( 'Unknown method on_error={}' . format ( on_error ) ) else : if self . verbose > 1 : self . log ( '[cacher] ... cache disabled: fname={}' . format ( self . fname ) ) return None
13734	def register_range_type ( pgrange , pyrange , conn ) : register_adapter ( pyrange , partial ( adapt_range , pgrange ) ) register_range_caster ( pgrange , pyrange , * query_range_oids ( pgrange , conn ) , scope = conn )
2008	def _deserialize_int ( data , nbytes = 32 , padding = 0 ) : assert isinstance ( data , ( bytearray , Array ) ) value = ABI . _readBE ( data , nbytes , padding = True ) value = Operators . SEXTEND ( value , nbytes * 8 , ( nbytes + padding ) * 8 ) if not issymbolic ( value ) : if value & ( 1 << ( nbytes * 8 - 1 ) ) : value = - ( ( ( ~ value ) + 1 ) & ( ( 1 << ( nbytes * 8 ) ) - 1 ) ) return value
13262	def task ( func , ** config ) : if func . __name__ == func . __qualname__ : assert not func . __qualname__ in _task_list , "Can not define the same task \"{}\" twice" . format ( func . __qualname__ ) logger . debug ( "Found task %s" , func ) _task_list [ func . __qualname__ ] = Task ( plugin_class = None , func = func , config = config ) else : func . yaz_task_config = config return func
5182	def nodes ( self , unreported = 2 , with_status = False , ** kwargs ) : nodes = self . _query ( 'nodes' , ** kwargs ) now = datetime . datetime . utcnow ( ) if type ( nodes ) == dict : nodes = [ nodes , ] if with_status : latest_events = self . event_counts ( query = EqualsOperator ( "latest_report?" , True ) , summarize_by = 'certname' ) for node in nodes : node [ 'status_report' ] = None node [ 'events' ] = None if with_status : status = [ s for s in latest_events if s [ 'subject' ] [ 'title' ] == node [ 'certname' ] ] try : node [ 'status_report' ] = node [ 'latest_report_status' ] if status : node [ 'events' ] = status [ 0 ] except KeyError : if status : node [ 'events' ] = status = status [ 0 ] if status [ 'successes' ] > 0 : node [ 'status_report' ] = 'changed' if status [ 'noops' ] > 0 : node [ 'status_report' ] = 'noop' if status [ 'failures' ] > 0 : node [ 'status_report' ] = 'failed' else : node [ 'status_report' ] = 'unchanged' if node [ 'report_timestamp' ] is not None : try : last_report = json_to_datetime ( node [ 'report_timestamp' ] ) last_report = last_report . replace ( tzinfo = None ) unreported_border = now - timedelta ( hours = unreported ) if last_report < unreported_border : delta = ( now - last_report ) node [ 'unreported' ] = True node [ 'unreported_time' ] = '{0}d {1}h {2}m' . format ( delta . days , int ( delta . seconds / 3600 ) , int ( ( delta . seconds % 3600 ) / 60 ) ) except AttributeError : node [ 'unreported' ] = True if not node [ 'report_timestamp' ] : node [ 'unreported' ] = True yield Node ( self , name = node [ 'certname' ] , deactivated = node [ 'deactivated' ] , expired = node [ 'expired' ] , report_timestamp = node [ 'report_timestamp' ] , catalog_timestamp = node [ 'catalog_timestamp' ] , facts_timestamp = node [ 'facts_timestamp' ] , status_report = node [ 'status_report' ] , noop = node . get ( 'latest_report_noop' ) , noop_pending = node . get ( 'latest_report_noop_pending' ) , events = node [ 'events' ] , unreported = node . get ( 'unreported' ) , unreported_time = node . get ( 'unreported_time' ) , report_environment = node [ 'report_environment' ] , catalog_environment = node [ 'catalog_environment' ] , facts_environment = node [ 'facts_environment' ] , latest_report_hash = node . get ( 'latest_report_hash' ) , cached_catalog_status = node . get ( 'cached_catalog_status' ) )
2959	def _state_delete ( self ) : try : os . remove ( self . _state_file ) except OSError as err : if err . errno not in ( errno . EPERM , errno . ENOENT ) : raise try : os . rmdir ( self . _state_dir ) except OSError as err : if err . errno not in ( errno . ENOTEMPTY , errno . ENOENT ) : raise
5887	def extract ( self , url = None , raw_html = None ) : crawl_candidate = CrawlCandidate ( self . config , url , raw_html ) return self . __crawl ( crawl_candidate )
1273	def from_spec ( spec ) : exploration = util . get_object ( obj = spec , predefined_objects = tensorforce . core . explorations . explorations ) assert isinstance ( exploration , Exploration ) return exploration
9193	def _insert_file ( cursor , file , media_type ) : resource_hash = _get_file_sha1 ( file ) cursor . execute ( "SELECT fileid FROM files WHERE sha1 = %s" , ( resource_hash , ) ) try : fileid = cursor . fetchone ( ) [ 0 ] except ( IndexError , TypeError ) : cursor . execute ( "INSERT INTO files (file, media_type) " "VALUES (%s, %s)" "RETURNING fileid" , ( psycopg2 . Binary ( file . read ( ) ) , media_type , ) ) fileid = cursor . fetchone ( ) [ 0 ] return fileid , resource_hash
13654	def Integer ( name , base = 10 , encoding = None ) : def _match ( request , value ) : return name , query . Integer ( value , base = base , encoding = contentEncoding ( request . requestHeaders , encoding ) ) return _match
11574	def digital_message ( self , data ) : port = data [ 0 ] port_data = ( data [ self . MSB ] << 7 ) + data [ self . LSB ] pin = port * 8 for pin in range ( pin , min ( pin + 8 , self . total_pins_discovered ) ) : with self . pymata . data_lock : prev_data = self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] = port_data & 0x01 if prev_data != port_data & 0x01 : callback = self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_CALLBACK ] if callback : callback ( [ self . pymata . DIGITAL , pin , self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] ] ) latching_entry = self . digital_latch_table [ pin ] if latching_entry [ self . LATCH_STATE ] == self . LATCH_ARMED : if latching_entry [ self . LATCHED_THRESHOLD_TYPE ] == self . DIGITAL_LATCH_LOW : if ( port_data & 0x01 ) == 0 : if latching_entry [ self . DIGITAL_LATCH_CALLBACK ] is not None : self . digital_latch_table [ pin ] = [ 0 , 0 , 0 , 0 , None ] latching_entry [ self . DIGITAL_LATCH_CALLBACK ] ( [ self . pymata . OUTPUT | self . pymata . LATCH_MODE , pin , 0 , time . time ( ) ] ) else : updated_latch_entry = latching_entry updated_latch_entry [ self . LATCH_STATE ] = self . LATCH_LATCHED updated_latch_entry [ self . DIGITAL_LATCHED_DATA ] = self . DIGITAL_LATCH_LOW updated_latch_entry [ self . DIGITAL_TIME_STAMP ] = time . time ( ) else : pass elif latching_entry [ self . LATCHED_THRESHOLD_TYPE ] == self . DIGITAL_LATCH_HIGH : if port_data & 0x01 : if latching_entry [ self . DIGITAL_LATCH_CALLBACK ] is not None : self . digital_latch_table [ pin ] = [ 0 , 0 , 0 , 0 , None ] latching_entry [ self . DIGITAL_LATCH_CALLBACK ] ( [ self . pymata . OUTPUT | self . pymata . LATCH_MODE , pin , 1 , time . time ( ) ] ) else : updated_latch_entry = latching_entry updated_latch_entry [ self . LATCH_STATE ] = self . LATCH_LATCHED updated_latch_entry [ self . DIGITAL_LATCHED_DATA ] = self . DIGITAL_LATCH_HIGH updated_latch_entry [ self . DIGITAL_TIME_STAMP ] = time . time ( ) else : pass else : pass port_data >>= 1
3012	def locked_put ( self , credentials ) : filters = { self . key_name : self . key_value } query = self . session . query ( self . model_class ) . filter_by ( ** filters ) entity = query . first ( ) if not entity : entity = self . model_class ( ** filters ) setattr ( entity , self . property_name , credentials ) self . session . add ( entity )
5016	def transmit ( self , payload , ** kwargs ) : IntegratedChannelLearnerDataTransmissionAudit = apps . get_model ( app_label = kwargs . get ( 'app_label' , 'integrated_channel' ) , model_name = kwargs . get ( 'model_name' , 'LearnerDataTransmissionAudit' ) , ) for learner_data in payload . export ( ) : serialized_payload = learner_data . serialize ( enterprise_configuration = self . enterprise_configuration ) LOGGER . debug ( 'Attempting to transmit serialized payload: %s' , serialized_payload ) enterprise_enrollment_id = learner_data . enterprise_course_enrollment_id if learner_data . completed_timestamp is None : LOGGER . info ( 'Skipping in-progress enterprise enrollment {}' . format ( enterprise_enrollment_id ) ) continue previous_transmissions = IntegratedChannelLearnerDataTransmissionAudit . objects . filter ( enterprise_course_enrollment_id = enterprise_enrollment_id , error_message = '' ) if previous_transmissions . exists ( ) : LOGGER . info ( 'Skipping previously sent enterprise enrollment {}' . format ( enterprise_enrollment_id ) ) continue try : code , body = self . client . create_course_completion ( getattr ( learner_data , kwargs . get ( 'remote_user_id' ) ) , serialized_payload ) LOGGER . info ( 'Successfully sent completion status call for enterprise enrollment {}' . format ( enterprise_enrollment_id , ) ) except RequestException as request_exception : code = 500 body = str ( request_exception ) self . handle_transmission_error ( learner_data , request_exception ) learner_data . status = str ( code ) learner_data . error_message = body if code >= 400 else '' learner_data . save ( )
812	def _fixupRandomEncoderParams ( params , minVal , maxVal , minResolution ) : encodersDict = ( params [ "modelConfig" ] [ "modelParams" ] [ "sensorParams" ] [ "encoders" ] ) for encoder in encodersDict . itervalues ( ) : if encoder is not None : if encoder [ "type" ] == "RandomDistributedScalarEncoder" : resolution = max ( minResolution , ( maxVal - minVal ) / encoder . pop ( "numBuckets" ) ) encodersDict [ "c1" ] [ "resolution" ] = resolution
8820	def delete_network ( context , id ) : LOG . info ( "delete_network %s for tenant %s" % ( id , context . tenant_id ) ) with context . session . begin ( ) : net = db_api . network_find ( context = context , limit = None , sorts = [ 'id' ] , marker = None , page_reverse = False , id = id , scope = db_api . ONE ) if not net : raise n_exc . NetworkNotFound ( net_id = id ) if not context . is_admin : if STRATEGY . is_provider_network ( net . id ) : raise n_exc . NotAuthorized ( net_id = id ) if net . ports : raise n_exc . NetworkInUse ( net_id = id ) net_driver = registry . DRIVER_REGISTRY . get_driver ( net [ "network_plugin" ] ) net_driver . delete_network ( context , id ) for subnet in net [ "subnets" ] : subnets . _delete_subnet ( context , subnet ) db_api . network_delete ( context , net )
12087	def html_singleAll ( self , template = "basic" ) : for fname in smartSort ( self . cells ) : if template == "fixed" : self . html_single_fixed ( fname ) else : self . html_single_basic ( fname )
6792	def manage ( self , cmd , * args , ** kwargs ) : r = self . local_renderer environs = kwargs . pop ( 'environs' , '' ) . strip ( ) if environs : environs = ' ' . join ( 'export %s=%s;' % tuple ( _ . split ( '=' ) ) for _ in environs . split ( ',' ) ) environs = ' ' + environs + ' ' r . env . cmd = cmd r . env . SITE = r . genv . SITE or r . genv . default_site r . env . args = ' ' . join ( map ( str , args ) ) r . env . kwargs = ' ' . join ( ( '--%s' % _k if _v in ( True , 'True' ) else '--%s=%s' % ( _k , _v ) ) for _k , _v in kwargs . items ( ) ) r . env . environs = environs if self . is_local : r . env . project_dir = r . env . local_project_dir r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE};{environs} cd {project_dir}; {manage_cmd} {cmd} {args} {kwargs}' )
3370	def interface_to_str ( interface ) : if isinstance ( interface , ModuleType ) : interface = interface . __name__ return re . sub ( r"optlang.|.interface" , "" , interface )
10084	def discard ( self , pid = None ) : pid = pid or self . pid with db . session . begin_nested ( ) : before_record_update . send ( current_app . _get_current_object ( ) , record = self ) _ , record = self . fetch_published ( ) self . model . json = deepcopy ( record . model . json ) self . model . json [ '$schema' ] = self . build_deposit_schema ( record ) flag_modified ( self . model , 'json' ) db . session . merge ( self . model ) after_record_update . send ( current_app . _get_current_object ( ) , record = self ) return self . __class__ ( self . model . json , model = self . model )
12251	def get_all_keys ( self , * args , ** kwargs ) : if kwargs . pop ( 'force' , None ) : headers = kwargs . get ( 'headers' , args [ 0 ] if len ( args ) else None ) or dict ( ) headers [ 'force' ] = True kwargs [ 'headers' ] = headers return super ( Bucket , self ) . get_all_keys ( * args , ** kwargs )
10613	def H ( self , H ) : self . _H = H self . _T = self . _calculate_T ( H )
10138	def nearest ( self , ver ) : if not isinstance ( ver , Version ) : ver = Version ( ver ) if ver in OFFICIAL_VERSIONS : return ver versions = list ( OFFICIAL_VERSIONS ) versions . sort ( reverse = True ) best = None for candidate in versions : if candidate == ver : return candidate if ( best is None ) and ( candidate < ver ) : warnings . warn ( 'This version of hszinc does not yet ' 'support version %s, please seek a newer version ' 'or file a bug. Closest (older) version supported is %s.' % ( ver , candidate ) ) return candidate if candidate > ver : best = candidate assert best is not None warnings . warn ( 'This version of hszinc does not yet ' 'support version %s, please seek a newer version ' 'or file a bug. Closest (newer) version supported is %s.' % ( ver , best ) ) return best
3275	def handle_delete ( self ) : if "/by_tag/" not in self . path : raise DAVError ( HTTP_FORBIDDEN ) catType , tag , _rest = util . save_split ( self . path . strip ( "/" ) , "/" , 2 ) assert catType == "by_tag" assert tag in self . data [ "tags" ] self . data [ "tags" ] . remove ( tag ) return True
680	def getAllRecords ( self ) : values = [ ] numRecords = self . fields [ 0 ] . numRecords assert ( all ( field . numRecords == numRecords for field in self . fields ) ) for x in range ( numRecords ) : values . append ( self . getRecord ( x ) ) return values
8760	def get_subnets_count ( context , filters = None ) : LOG . info ( "get_subnets_count for tenant %s with filters %s" % ( context . tenant_id , filters ) ) return db_api . subnet_count_all ( context , ** filters )
6951	def jhk_to_sdssz ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSZ_JHK , SDSSZ_JH , SDSSZ_JK , SDSSZ_HK , SDSSZ_J , SDSSZ_H , SDSSZ_K )
10171	def post ( self , ** kwargs ) : data = request . get_json ( force = False ) if data is None : data = { } result = { } for query_name , config in data . items ( ) : if config is None or not isinstance ( config , dict ) or ( set ( config . keys ( ) ) != { 'stat' , 'params' } and set ( config . keys ( ) ) != { 'stat' } ) : raise InvalidRequestInputError ( 'Invalid Input. It should be of the form ' '{ STATISTIC_NAME: { "stat": STAT_TYPE, ' '"params": STAT_PARAMS \}}' ) stat = config [ 'stat' ] params = config . get ( 'params' , { } ) try : query_cfg = current_stats . queries [ stat ] except KeyError : raise UnknownQueryError ( stat ) permission = current_stats . permission_factory ( stat , params ) if permission is not None and not permission . can ( ) : message = ( 'You do not have a permission to query the ' 'statistic "{}" with those ' 'parameters' . format ( stat ) ) if current_user . is_authenticated : abort ( 403 , message ) abort ( 401 , message ) try : query = query_cfg . query_class ( ** query_cfg . query_config ) result [ query_name ] = query . run ( ** params ) except ValueError as e : raise InvalidRequestInputError ( e . args [ 0 ] ) except NotFoundError as e : return None return self . make_response ( result )
9149	def count_relations ( self ) -> int : if self . edge_model is ... : raise Bio2BELMissingEdgeModelError ( 'edge_edge model is undefined/count_bel_relations is not overridden' ) elif isinstance ( self . edge_model , list ) : return sum ( self . _count_model ( m ) for m in self . edge_model ) else : return self . _count_model ( self . edge_model )
2484	def create_checksum_node ( self , chksum ) : chksum_node = BNode ( ) type_triple = ( chksum_node , RDF . type , self . spdx_namespace . Checksum ) self . graph . add ( type_triple ) algorithm_triple = ( chksum_node , self . spdx_namespace . algorithm , Literal ( chksum . identifier ) ) self . graph . add ( algorithm_triple ) value_triple = ( chksum_node , self . spdx_namespace . checksumValue , Literal ( chksum . value ) ) self . graph . add ( value_triple ) return chksum_node
177	def concatenate ( self , other ) : if not isinstance ( other , LineString ) : other = LineString ( other ) return self . deepcopy ( coords = np . concatenate ( [ self . coords , other . coords ] , axis = 0 ) )
13913	def _InternalUnpackAny ( msg ) : type_url = msg . type_url db = symbol_database . Default ( ) if not type_url : return None type_name = type_url . split ( "/" ) [ - 1 ] descriptor = db . pool . FindMessageTypeByName ( type_name ) if descriptor is None : return None message_class = db . GetPrototype ( descriptor ) message = message_class ( ) message . ParseFromString ( msg . value ) return message
8154	def create ( self , name , overwrite = True ) : self . _name = name . rstrip ( ".db" ) from os import unlink if overwrite : try : unlink ( self . _name + ".db" ) except : pass self . _con = sqlite . connect ( self . _name + ".db" ) self . _cur = self . _con . cursor ( )
9342	def kill_all ( self ) : for pid in self . children : try : os . kill ( pid , signal . SIGTRAP ) except OSError : continue self . join ( )
7886	def emit_stanza ( self , element ) : if not self . _head_emitted : raise RuntimeError ( ".emit_head() must be called first." ) string = self . _emit_element ( element , level = 1 , declared_prefixes = self . _root_prefixes ) return remove_evil_characters ( string )
13751	def handle_data ( self , data ) : if data . strip ( ) : data = djeffify_string ( data ) self . djhtml += data
1121	def format ( self , o , context , maxlevels , level ) : return _safe_repr ( o , context , maxlevels , level )
13102	def get_template_uuid ( self ) : response = requests . get ( self . url + 'editor/scan/templates' , headers = self . headers , verify = False ) templates = json . loads ( response . text ) for template in templates [ 'templates' ] : if template [ 'name' ] == self . template_name : return template [ 'uuid' ]
12050	def getParent ( abfFname ) : child = os . path . abspath ( abfFname ) files = sorted ( glob . glob ( os . path . dirname ( child ) + "/*.*" ) ) parentID = abfFname for fname in files : if fname . endswith ( ".abf" ) and fname . replace ( ".abf" , ".TIF" ) in files : parentID = os . path . basename ( fname ) . replace ( ".abf" , "" ) if os . path . basename ( child ) in fname : break return parentID
5332	def get_panels ( config ) : task = TaskPanels ( config ) task . execute ( ) task = TaskPanelsMenu ( config ) task . execute ( ) logging . info ( "Panels creation finished!" )
8711	def __read_chunk ( self , buf ) : log . debug ( 'reading chunk' ) timeout_before = self . _port . timeout if SYSTEM != 'Windows' : if self . _port . timeout != MINIMAL_TIMEOUT : self . _port . timeout = MINIMAL_TIMEOUT end = time . time ( ) + timeout_before while len ( buf ) < 130 and time . time ( ) <= end : buf = buf + self . _port . read ( ) if buf [ 0 ] != BLOCK_START or len ( buf ) < 130 : log . debug ( 'buffer binary: %s ' , hexify ( buf ) ) raise Exception ( 'Bad blocksize or start byte' ) if SYSTEM != 'Windows' : self . _port . timeout = timeout_before chunk_size = ord ( buf [ 1 ] ) data = buf [ 2 : chunk_size + 2 ] buf = buf [ 130 : ] return ( data , buf )
9916	def validate_is_primary ( self , is_primary ) : if is_primary and not ( self . instance and self . instance . is_verified ) : raise serializers . ValidationError ( _ ( "Unverified email addresses may not be used as the " "primary address." ) ) return is_primary
10033	def join_phonemes ( * args ) : if len ( args ) == 1 : args = args [ 0 ] if len ( args ) == 2 : args += ( CODAS [ 0 ] , ) try : onset , nucleus , coda = args except ValueError : raise TypeError ( 'join_phonemes() takes at most 3 arguments' ) offset = ( ( ONSETS . index ( onset ) * NUM_NUCLEUSES + NUCLEUSES . index ( nucleus ) ) * NUM_CODAS + CODAS . index ( coda ) ) return unichr ( FIRST_HANGUL_OFFSET + offset )
11087	def sleep ( self , channel ) : self . log . info ( 'Sleeping in %s' , channel ) self . _bot . dispatcher . ignore ( channel ) self . send_message ( channel , 'Good night' )
4894	def _collect_grades_data ( self , enterprise_enrollment , course_details ) : if self . grades_api is None : self . grades_api = GradesApiClient ( self . user ) course_id = enterprise_enrollment . course_id username = enterprise_enrollment . enterprise_customer_user . user . username try : grades_data = self . grades_api . get_course_grade ( course_id , username ) except HttpNotFoundError as error : if hasattr ( error , 'content' ) : response_content = json . loads ( error . content ) if response_content . get ( 'error_code' , '' ) == 'user_not_enrolled' : LOGGER . info ( "User [%s] not enrolled in course [%s], enterprise enrollment [%d]" , username , course_id , enterprise_enrollment . pk ) return None , None , None LOGGER . error ( "No grades data found for [%d]: [%s], [%s]" , enterprise_enrollment . pk , course_id , username ) return None , None , None course_end_date = course_details . get ( 'end' ) if course_end_date is not None : course_end_date = parse_datetime ( course_end_date ) now = timezone . now ( ) is_passing = grades_data . get ( 'passed' ) if course_end_date is not None and course_end_date < now : completed_date = course_end_date grade = self . grade_passing if is_passing else self . grade_failing elif is_passing : completed_date = now grade = self . grade_passing else : completed_date = None grade = self . grade_incomplete return completed_date , grade , is_passing
5774	def ecdsa_sign ( private_key , data , hash_algorithm ) : if private_key . algorithm != 'ec' : raise ValueError ( 'The key specified is not an EC private key' ) return _sign ( private_key , data , hash_algorithm )
8152	def hex_to_rgb ( hex ) : hex = hex . lstrip ( "#" ) if len ( hex ) < 6 : hex += hex [ - 1 ] * ( 6 - len ( hex ) ) if len ( hex ) == 6 : r , g , b = hex [ 0 : 2 ] , hex [ 2 : 4 ] , hex [ 4 : ] r , g , b = [ int ( n , 16 ) / 255.0 for n in ( r , g , b ) ] a = 1.0 elif len ( hex ) == 8 : r , g , b , a = hex [ 0 : 2 ] , hex [ 2 : 4 ] , hex [ 4 : 6 ] , hex [ 6 : ] r , g , b , a = [ int ( n , 16 ) / 255.0 for n in ( r , g , b , a ) ] return r , g , b , a
413	def find_top_model ( self , sess , sort = None , model_name = 'model' , ** kwargs ) : kwargs . update ( { 'model_name' : model_name } ) self . _fill_project_info ( kwargs ) s = time . time ( ) d = self . db . Model . find_one ( filter = kwargs , sort = sort ) _temp_file_name = '_find_one_model_ztemp_file' if d is not None : params_id = d [ 'params_id' ] graphs = d [ 'architecture' ] _datetime = d [ 'time' ] exists_or_mkdir ( _temp_file_name , False ) with open ( os . path . join ( _temp_file_name , 'graph.pkl' ) , 'wb' ) as file : pickle . dump ( graphs , file , protocol = pickle . HIGHEST_PROTOCOL ) else : print ( "[Database] FAIL! Cannot find model: {}" . format ( kwargs ) ) return False try : params = self . _deserialization ( self . model_fs . get ( params_id ) . read ( ) ) np . savez ( os . path . join ( _temp_file_name , 'params.npz' ) , params = params ) network = load_graph_and_params ( name = _temp_file_name , sess = sess ) del_folder ( _temp_file_name ) pc = self . db . Model . find ( kwargs ) print ( "[Database] Find one model SUCCESS. kwargs:{} sort:{} save time:{} took: {}s" . format ( kwargs , sort , _datetime , round ( time . time ( ) - s , 2 ) ) ) for key in d : network . __dict__ . update ( { "_%s" % key : d [ key ] } ) params_id_list = pc . distinct ( 'params_id' ) n_params = len ( params_id_list ) if n_params != 1 : print ( " Note that there are {} models match the kwargs" . format ( n_params ) ) return network except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) return False
7874	def get_payload ( self , payload_class , payload_key = None , specialize = False ) : if self . _payload is None : self . decode_payload ( ) if payload_class is None : if self . _payload : payload = self . _payload [ 0 ] if specialize and isinstance ( payload , XMLPayload ) : klass = payload_class_for_element_name ( payload . element . tag ) if klass is not XMLPayload : payload = klass . from_xml ( payload . element ) self . _payload [ 0 ] = payload return payload else : return None elements = payload_class . _pyxmpp_payload_element_name for i , payload in enumerate ( self . _payload ) : if isinstance ( payload , XMLPayload ) : if payload_class is not XMLPayload : if payload . xml_element_name not in elements : continue payload = payload_class . from_xml ( payload . element ) elif not isinstance ( payload , payload_class ) : continue if payload_key is not None and payload_key != payload . handler_key ( ) : continue self . _payload [ i ] = payload return payload return None
617	def parseTimestamp ( s ) : s = s . strip ( ) for pattern in DATETIME_FORMATS : try : return datetime . datetime . strptime ( s , pattern ) except ValueError : pass raise ValueError ( 'The provided timestamp %s is malformed. The supported ' 'formats are: [%s]' % ( s , ', ' . join ( DATETIME_FORMATS ) ) )
1336	def name ( self ) : names = ( criterion . name ( ) for criterion in self . _criteria ) return '__' . join ( sorted ( names ) )
10577	def get_assay ( self ) : masses_sum = sum ( self . compound_masses ) return [ m / masses_sum for m in self . compound_masses ]
12452	def check_pre_requirements ( pre_requirements ) : pre_requirements = set ( pre_requirements or [ ] ) pre_requirements . add ( 'virtualenv' ) for requirement in pre_requirements : if not which ( requirement ) : print_error ( 'Requirement {0!r} is not found in system' . format ( requirement ) ) return False return True
12295	def annotate_metadata_dependencies ( repo ) : options = repo . options if 'dependencies' not in options : print ( "No dependencies" ) return [ ] repos = [ ] dependent_repos = options [ 'dependencies' ] for d in dependent_repos : if "/" not in d : print ( "Invalid dependency specification" ) ( username , reponame ) = d . split ( "/" ) try : repos . append ( repo . manager . lookup ( username , reponame ) ) except : print ( "Repository does not exist. Please create one" , d ) package = repo . package package [ 'dependencies' ] = [ ] for r in repos : package [ 'dependencies' ] . append ( { 'username' : r . username , 'reponame' : r . reponame , } )
1865	def PSHUFW ( cpu , op0 , op1 , op3 ) : size = op0 . size arg0 = op0 . read ( ) arg1 = op1 . read ( ) arg3 = Operators . ZEXTEND ( op3 . read ( ) , size ) assert size == 64 arg0 |= ( ( arg1 >> ( ( arg3 >> 0 ) & 3 * 16 ) ) & 0xffff ) arg0 |= ( ( arg1 >> ( ( arg3 >> 2 ) & 3 * 16 ) ) & 0xffff ) << 16 arg0 |= ( ( arg1 >> ( ( arg3 >> 4 ) & 3 * 16 ) ) & 0xffff ) << 32 arg0 |= ( ( arg1 >> ( ( arg3 >> 6 ) & 3 * 16 ) ) & 0xffff ) << 48 op0 . write ( arg0 )
1296	def demo_update ( self ) : fetches = self . demo_optimization_output self . monitored_session . run ( fetches = fetches )
6194	def datafile_from_hash ( hash_ , prefix , path ) : pattern = '%s_%s*.h*' % ( prefix , hash_ ) datafiles = list ( path . glob ( pattern ) ) if len ( datafiles ) == 0 : raise NoMatchError ( 'No matches for "%s"' % pattern ) if len ( datafiles ) > 1 : raise MultipleMatchesError ( 'More than 1 match for "%s"' % pattern ) return datafiles [ 0 ]
8906	def save_service ( self , service , overwrite = True ) : name = namesgenerator . get_sane_name ( service . name ) if not name : name = namesgenerator . get_random_name ( ) if self . collection . count_documents ( { 'name' : name } ) > 0 : name = namesgenerator . get_random_name ( retry = True ) if self . collection . count_documents ( { 'name' : name } ) > 0 : if overwrite : self . collection . delete_one ( { 'name' : name } ) else : raise Exception ( "service name already registered." ) self . collection . insert_one ( Service ( name = name , url = baseurl ( service . url ) , type = service . type , purl = service . purl , public = service . public , auth = service . auth , verify = service . verify ) ) return self . fetch_by_name ( name = name )
6584	def station_selection_menu ( self , error = None ) : self . screen . clear ( ) if error : self . screen . print_error ( "{}\n" . format ( error ) ) for i , station in enumerate ( self . stations ) : i = "{:>3}" . format ( i ) print ( "{}: {}" . format ( Colors . yellow ( i ) , station . name ) ) return self . stations [ self . screen . get_integer ( "Station: " ) ]
8836	def minus ( * args ) : if len ( args ) == 1 : return - to_numeric ( args [ 0 ] ) return to_numeric ( args [ 0 ] ) - to_numeric ( args [ 1 ] )
7109	def get_from_cache ( url : str , cache_dir : Path = None ) -> Path : cache_dir . mkdir ( parents = True , exist_ok = True ) filename = re . sub ( r'.+/' , '' , url ) cache_path = cache_dir / filename if cache_path . exists ( ) : return cache_path response = requests . head ( url ) if response . status_code != 200 : if "www.dropbox.com" in url : pass else : raise IOError ( "HEAD request failed for url {}" . format ( url ) ) if not cache_path . exists ( ) : fd , temp_filename = tempfile . mkstemp ( ) logger . info ( "%s not found in cache, downloading to %s" , url , temp_filename ) req = requests . get ( url , stream = True ) content_length = req . headers . get ( 'Content-Length' ) total = int ( content_length ) if content_length is not None else None progress = Tqdm . tqdm ( unit = "B" , total = total ) with open ( temp_filename , 'wb' ) as temp_file : for chunk in req . iter_content ( chunk_size = 1024 ) : if chunk : progress . update ( len ( chunk ) ) temp_file . write ( chunk ) progress . close ( ) logger . info ( "copying %s to cache at %s" , temp_filename , cache_path ) shutil . copyfile ( temp_filename , str ( cache_path ) ) logger . info ( "removing temp file %s" , temp_filename ) os . close ( fd ) os . remove ( temp_filename ) return cache_path
5379	def build_pipeline ( cls , project , zones , min_cores , min_ram , disk_size , boot_disk_size , preemptible , accelerator_type , accelerator_count , image , script_name , envs , inputs , outputs , pipeline_name ) : if min_cores is None : min_cores = job_model . DEFAULT_MIN_CORES if min_ram is None : min_ram = job_model . DEFAULT_MIN_RAM if disk_size is None : disk_size = job_model . DEFAULT_DISK_SIZE if boot_disk_size is None : boot_disk_size = job_model . DEFAULT_BOOT_DISK_SIZE if preemptible is None : preemptible = job_model . DEFAULT_PREEMPTIBLE docker_command = cls . _build_pipeline_docker_command ( script_name , inputs , outputs , envs ) input_envs = [ { 'name' : SCRIPT_VARNAME } ] + [ { 'name' : env . name } for env in envs if env . value ] input_files = [ cls . _build_pipeline_input_file_param ( var . name , var . docker_path ) for var in inputs if not var . recursive and var . value ] output_files = [ cls . _build_pipeline_file_param ( var . name , var . docker_path ) for var in outputs if not var . recursive and var . value ] return { 'ephemeralPipeline' : { 'projectId' : project , 'name' : pipeline_name , 'resources' : { 'minimumCpuCores' : min_cores , 'minimumRamGb' : min_ram , 'bootDiskSizeGb' : boot_disk_size , 'preemptible' : preemptible , 'zones' : google_base . get_zones ( zones ) , 'acceleratorType' : accelerator_type , 'acceleratorCount' : accelerator_count , 'disks' : [ { 'name' : 'datadisk' , 'autoDelete' : True , 'sizeGb' : disk_size , 'mountPoint' : providers_util . DATA_MOUNT_POINT , } ] , } , 'inputParameters' : input_envs + input_files , 'outputParameters' : output_files , 'docker' : { 'imageName' : image , 'cmd' : docker_command , } } }
5886	def close ( self ) : if self . fetcher is not None : self . shutdown_network ( ) self . finalizer . atexit = False
6563	def load_cnf ( fp ) : fp = iter ( fp ) csp = ConstraintSatisfactionProblem ( dimod . BINARY ) num_clauses = num_variables = 0 problem_pattern = re . compile ( _PROBLEM_REGEX ) for line in fp : matches = problem_pattern . findall ( line ) if matches : if len ( matches ) > 1 : raise ValueError nv , nc = matches [ 0 ] num_variables , num_clauses = int ( nv ) , int ( nc ) break clause_pattern = re . compile ( _CLAUSE_REGEX ) for line in fp : if clause_pattern . match ( line ) is not None : clause = [ int ( v ) for v in line . split ( ' ' ) [ : - 1 ] ] variables = [ abs ( v ) for v in clause ] f = _cnf_or ( clause ) csp . add_constraint ( f , variables ) for v in range ( 1 , num_variables + 1 ) : csp . add_variable ( v ) for v in csp . variables : if v > num_variables : msg = ( "given .cnf file's header defines variables [1, {}] and {} clauses " "but constraints a reference to variable {}" ) . format ( num_variables , num_clauses , v ) raise ValueError ( msg ) if len ( csp ) != num_clauses : msg = ( "given .cnf file's header defines {} " "clauses but the file contains {}" ) . format ( num_clauses , len ( csp ) ) raise ValueError ( msg ) return csp
8397	def gettrans ( t ) : obj = t if isinstance ( obj , str ) : name = '{}_trans' . format ( obj ) obj = globals ( ) [ name ] ( ) if callable ( obj ) : obj = obj ( ) if isinstance ( obj , type ) : obj = obj ( ) if not isinstance ( obj , trans ) : raise ValueError ( "Could not get transform object." ) return obj
9064	def fit ( self , verbose = True ) : if not self . _isfixed ( "logistic" ) : self . _maximize_scalar ( desc = "LMM" , rtol = 1e-6 , atol = 1e-6 , verbose = verbose ) if not self . _fix [ "beta" ] : self . _update_beta ( ) if not self . _fix [ "scale" ] : self . _update_scale ( )
9497	def parse_collection ( path , excludes = None ) : file = path / COLLECTION_FILENAME if not file . exists ( ) : raise MissingFile ( file ) id = _parse_document_id ( etree . parse ( file . open ( ) ) ) excludes = excludes or [ ] excludes . extend ( [ lambda filepath : filepath . name == COLLECTION_FILENAME , lambda filepath : filepath . is_dir ( ) , ] ) resources_paths = _find_resources ( path , excludes = excludes ) resources = tuple ( _resource_from_path ( res ) for res in resources_paths ) return Collection ( id , file , resources )
9199	def reversals ( series , left = False , right = False ) : series = iter ( series ) x_last , x = next ( series ) , next ( series ) d_last = ( x - x_last ) if left : yield x_last for x_next in series : if x_next == x : continue d_next = x_next - x if d_last * d_next < 0 : yield x x_last , x = x , x_next d_last = d_next if right : yield x_next
8451	def is_temple_project ( ) : if not os . path . exists ( temple . constants . TEMPLE_CONFIG_FILE ) : msg = 'No {} file found in repository.' . format ( temple . constants . TEMPLE_CONFIG_FILE ) raise temple . exceptions . InvalidTempleProjectError ( msg )
9723	async def save ( self , filename , overwrite = False ) : cmd = "save %s%s" % ( filename , " overwrite" if overwrite else "" ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
6700	def apt_key_exists ( keyid ) : gpg_cmd = 'gpg --ignore-time-conflict --no-options --no-default-keyring --keyring /etc/apt/trusted.gpg' with settings ( hide ( 'everything' ) , warn_only = True ) : res = run ( '%(gpg_cmd)s --fingerprint %(keyid)s' % locals ( ) ) return res . succeeded
7612	def get_clan_image ( self , obj : BaseAttrDict ) : try : badge_id = obj . clan . badge_id except AttributeError : try : badge_id = obj . badge_id except AttributeError : return 'https://i.imgur.com/Y3uXsgj.png' if badge_id is None : return 'https://i.imgur.com/Y3uXsgj.png' for i in self . constants . alliance_badges : if i . id == badge_id : return 'https://royaleapi.github.io/cr-api-assets/badges/' + i . name + '.png'
6213	def load ( self ) : self . path = self . find_scene ( self . meta . path ) if not self . path : raise ValueError ( "Scene '{}' not found" . format ( self . meta . path ) ) self . scene = Scene ( self . path ) if self . path . suffix == '.gltf' : self . load_gltf ( ) if self . path . suffix == '.glb' : self . load_glb ( ) self . meta . check_version ( ) self . meta . check_extensions ( self . supported_extensions ) self . load_images ( ) self . load_samplers ( ) self . load_textures ( ) self . load_materials ( ) self . load_meshes ( ) self . load_nodes ( ) self . scene . calc_scene_bbox ( ) self . scene . prepare ( ) return self . scene
2631	def _status ( self ) : job_id_list = ' ' . join ( self . resources . keys ( ) ) cmd = "condor_q {0} -af:jr JobStatus" . format ( job_id_list ) retcode , stdout , stderr = super ( ) . execute_wait ( cmd ) for line in stdout . strip ( ) . split ( '\n' ) : parts = line . split ( ) job_id = parts [ 0 ] status = translate_table . get ( parts [ 1 ] , 'UNKNOWN' ) self . resources [ job_id ] [ 'status' ] = status
7265	def run ( self , ctx ) : if ctx . reverse : self . engine . reverse ( ) if self . engine . empty : raise AssertionError ( 'grappa: no assertions to run' ) try : return self . run_assertions ( ctx ) except Exception as _err : if getattr ( _err , '__legit__' , False ) : raise _err return self . render_error ( ctx , _err )
1387	def set_execution_state ( self , execution_state ) : if not execution_state : self . execution_state = None self . cluster = None self . environ = None else : self . execution_state = execution_state cluster , environ = self . get_execution_state_dc_environ ( execution_state ) self . cluster = cluster self . environ = environ self . zone = cluster self . trigger_watches ( )
1608	def make_tuple ( stream , tuple_key , values , roots = None ) : component_name = stream . component_name stream_id = stream . id gen_task = roots [ 0 ] . taskid if roots is not None and len ( roots ) > 0 else None return HeronTuple ( id = str ( tuple_key ) , component = component_name , stream = stream_id , task = gen_task , values = values , creation_time = time . time ( ) , roots = roots )
1924	def binary_symbols ( binary ) : def substr_after ( string , delim ) : return string . partition ( delim ) [ 2 ] with open ( binary , 'rb' ) as f : elffile = ELFFile ( f ) for section in elffile . iter_sections ( ) : if not isinstance ( section , SymbolTableSection ) : continue symbols = [ sym . name for sym in section . iter_symbols ( ) if sym ] return [ substr_after ( name , PREPEND_SYM ) for name in symbols if name . startswith ( PREPEND_SYM ) ]
10898	def _draw ( self ) : if self . display : print ( self . _formatstr . format ( ** self . __dict__ ) , end = '' ) sys . stdout . flush ( )
13018	def configure ( self , argv = None ) : self . _setupOptions ( ) self . _parseOptions ( argv ) self . _setupLogging ( ) self . _setupModel ( ) self . dbsession . commit ( ) return self
8224	def _mouse_pointer_moved ( self , x , y ) : self . _namespace [ 'MOUSEX' ] = x self . _namespace [ 'MOUSEY' ] = y
3199	def delete ( self , workflow_id , email_id ) : self . workflow_id = workflow_id self . email_id = email_id return self . _mc_client . _delete ( url = self . _build_path ( workflow_id , 'emails' , email_id ) )
13363	def echo_via_pager ( text , color = None ) : color = resolve_color_default ( color ) if not isinstance ( text , string_types ) : text = text_type ( text ) from . _termui_impl import pager return pager ( text + '\n' , color )
5600	def for_web ( self , data ) : rgba = self . _prepare_array_for_png ( data ) data = ma . masked_where ( rgba == self . nodata , rgba ) return memory_file ( data , self . profile ( ) ) , 'image/png'
2919	def Serializable ( o ) : if isinstance ( o , ( str , dict , int ) ) : return o else : try : json . dumps ( o ) return o except Exception : LOG . debug ( "Got a non-serilizeable object: %s" % o ) return o . __repr__ ( )
7828	def _new_from_xml ( cls , xmlnode ) : label = from_utf8 ( xmlnode . prop ( "label" ) ) child = xmlnode . children value = None for child in xml_element_ns_iter ( xmlnode . children , DATAFORM_NS ) : if child . name == "value" : value = from_utf8 ( child . getContent ( ) ) break if value is None : raise BadRequestProtocolError ( "No value in <option/> element" ) return cls ( value , label )
4764	def is_equal_to ( self , other , ** kwargs ) : if self . _check_dict_like ( self . val , check_values = False , return_as_bool = True ) and self . _check_dict_like ( other , check_values = False , return_as_bool = True ) : if self . _dict_not_equal ( self . val , other , ignore = kwargs . get ( 'ignore' ) , include = kwargs . get ( 'include' ) ) : self . _dict_err ( self . val , other , ignore = kwargs . get ( 'ignore' ) , include = kwargs . get ( 'include' ) ) else : if self . val != other : self . _err ( 'Expected <%s> to be equal to <%s>, but was not.' % ( self . val , other ) ) return self
9579	def read_cell_array ( fd , endian , header ) : array = [ list ( ) for i in range ( header [ 'dims' ] [ 0 ] ) ] for row in range ( header [ 'dims' ] [ 0 ] ) : for col in range ( header [ 'dims' ] [ 1 ] ) : vheader , next_pos , fd_var = read_var_header ( fd , endian ) varray = read_var_array ( fd_var , endian , vheader ) array [ row ] . append ( varray ) fd . seek ( next_pos ) if header [ 'dims' ] [ 0 ] == 1 : return squeeze ( array [ 0 ] ) return squeeze ( array )
4592	def colors_no_palette ( colors = None , ** kwds ) : if isinstance ( colors , str ) : colors = _split_colors ( colors ) else : colors = to_triplets ( colors or ( ) ) colors = ( color ( c ) for c in colors or ( ) ) return palette . Palette ( colors , ** kwds )
2643	def filepath ( self ) : if hasattr ( self , 'local_path' ) : return self . local_path if self . scheme in [ 'ftp' , 'http' , 'https' , 'globus' ] : return self . filename elif self . scheme in [ 'file' ] : return self . path else : raise Exception ( 'Cannot return filepath for unknown scheme {}' . format ( self . scheme ) )
13354	def _pipepager ( text , cmd , color ) : import subprocess env = dict ( os . environ ) cmd_detail = cmd . rsplit ( '/' , 1 ) [ - 1 ] . split ( ) if color is None and cmd_detail [ 0 ] == 'less' : less_flags = os . environ . get ( 'LESS' , '' ) + ' ' . join ( cmd_detail [ 1 : ] ) if not less_flags : env [ 'LESS' ] = '-R' color = True elif 'r' in less_flags or 'R' in less_flags : color = True if not color : text = strip_ansi ( text ) c = subprocess . Popen ( cmd , shell = True , stdin = subprocess . PIPE , env = env ) encoding = get_best_encoding ( c . stdin ) try : c . stdin . write ( text . encode ( encoding , 'replace' ) ) c . stdin . close ( ) except ( IOError , KeyboardInterrupt ) : pass while True : try : c . wait ( ) except KeyboardInterrupt : pass else : break
12436	def traverse ( cls , request , params = None ) : result = cls . parse ( request . path ) if result is None : return cls , { } elif not result : raise http . exceptions . NotFound ( ) resource , data , rest = result if params : data . update ( params ) if resource is None : return cls , data if data . get ( 'path' ) is not None : request . path = data . pop ( 'path' ) elif rest is not None : request . path = rest result = resource . traverse ( request , params = data ) return result
1294	def tf_combined_loss ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : q_model_loss = self . fn_loss ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals , update = update , reference = reference ) demo_loss = self . fn_demo_loss ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , update = update , reference = reference ) return q_model_loss + self . supervised_weight * demo_loss
8050	def load_source ( self ) : if self . filename in self . STDIN_NAMES : self . filename = "stdin" if sys . version_info [ 0 ] < 3 : self . source = sys . stdin . read ( ) else : self . source = TextIOWrapper ( sys . stdin . buffer , errors = "ignore" ) . read ( ) else : handle = tokenize_open ( self . filename ) self . source = handle . read ( ) handle . close ( )
10415	def function_namespace_inclusion_builder ( func : str , namespace : Strings ) -> NodePredicate : if isinstance ( namespace , str ) : def function_namespaces_filter ( _ : BELGraph , node : BaseEntity ) -> bool : if func != node [ FUNCTION ] : return False return NAMESPACE in node and node [ NAMESPACE ] == namespace elif isinstance ( namespace , Iterable ) : namespaces = set ( namespace ) def function_namespaces_filter ( _ : BELGraph , node : BaseEntity ) -> bool : if func != node [ FUNCTION ] : return False return NAMESPACE in node and node [ NAMESPACE ] in namespaces else : raise ValueError ( 'Invalid type for argument: {}' . format ( namespace ) ) return function_namespaces_filter
1509	def stop_cluster ( cl_args ) : Log . info ( "Terminating cluster..." ) roles = read_and_parse_roles ( cl_args ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] dist_nodes = masters . union ( slaves ) if masters : try : single_master = list ( masters ) [ 0 ] jobs = get_jobs ( cl_args , single_master ) for job in jobs : job_id = job [ "ID" ] Log . info ( "Terminating job %s" % job_id ) delete_job ( cl_args , job_id , single_master ) except : Log . debug ( "Error stopping jobs" ) Log . debug ( sys . exc_info ( ) [ 0 ] ) for node in dist_nodes : Log . info ( "Terminating processes on %s" % node ) if not is_self ( node ) : cmd = "ps aux | grep heron-nomad | awk '{print \$2}' " "| xargs kill" cmd = ssh_remote_execute ( cmd , node , cl_args ) else : cmd = "ps aux | grep heron-nomad | awk '{print $2}' " "| xargs kill" Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) Log . info ( "Cleaning up directories on %s" % node ) cmd = "rm -rf /tmp/slave ; rm -rf /tmp/master" if not is_self ( node ) : cmd = ssh_remote_execute ( cmd , node , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) )
4943	def get_data_sharing_consent ( username , enterprise_customer_uuid , course_id = None , program_uuid = None ) : EnterpriseCustomer = apps . get_model ( 'enterprise' , 'EnterpriseCustomer' ) try : if course_id : return get_course_data_sharing_consent ( username , course_id , enterprise_customer_uuid ) return get_program_data_sharing_consent ( username , program_uuid , enterprise_customer_uuid ) except EnterpriseCustomer . DoesNotExist : return None
2554	def setdocument ( self , doc ) : if self . document != doc : self . document = doc for i in self . children : if not isinstance ( i , dom_tag ) : return i . setdocument ( doc )
9776	def outputs ( ctx ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) try : PolyaxonClient ( ) . job . download_outputs ( user , project_name , _job ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not download outputs for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( 'Files downloaded.' )
9985	def _reload ( self , module = None ) : if self . module is None : raise RuntimeError elif module is None : import importlib module = ModuleSource ( importlib . reload ( module ) ) elif module . name != self . module : raise RuntimeError if self . name in module . funcs : func = module . funcs [ self . name ] self . __init__ ( func = func ) else : self . __init__ ( func = NULL_FORMULA ) return self
8463	def _call_api ( self , verb , url , ** request_kwargs ) : api = 'https://api.github.com{}' . format ( url ) auth_headers = { 'Authorization' : 'token {}' . format ( self . api_token ) } headers = { ** auth_headers , ** request_kwargs . pop ( 'headers' , { } ) } return getattr ( requests , verb ) ( api , headers = headers , ** request_kwargs )
4023	def _get_localhost_ssh_port ( ) : for line in _get_vm_config ( ) : if line . startswith ( 'Forwarding' ) : spec = line . split ( '=' ) [ 1 ] . strip ( '"' ) name , protocol , host , host_port , target , target_port = spec . split ( ',' ) if name == 'ssh' and protocol == 'tcp' and target_port == '22' : return host_port raise ValueError ( 'Could not determine localhost port for SSH forwarding' )
6283	def set_default_viewport ( self ) : expected_height = int ( self . buffer_width / self . aspect_ratio ) blank_space = self . buffer_height - expected_height self . fbo . viewport = ( 0 , blank_space // 2 , self . buffer_width , expected_height )
4703	def compare ( buf_a , buf_b , ignore ) : for field in getattr ( buf_a , '_fields_' ) : name , types = field [ 0 ] , field [ 1 ] if name in ignore : continue val_a = getattr ( buf_a , name ) val_b = getattr ( buf_b , name ) if isinstance ( types , ( type ( Union ) , type ( Structure ) ) ) : if compare ( val_a , val_b , ignore ) : return 1 elif isinstance ( types , type ( Array ) ) : for i , _ in enumerate ( val_a ) : if isinstance ( types , ( type ( Union ) , type ( Structure ) ) ) : if compare ( val_a [ i ] , val_b [ i ] , ignore ) : return 1 else : if val_a [ i ] != val_b [ i ] : return 1 else : if val_a != val_b : return 1 return 0
1450	def get_all_file_state_managers ( conf ) : state_managers = [ ] state_locations = conf . get_state_locations_of_type ( "file" ) for location in state_locations : name = location [ 'name' ] rootpath = os . path . expanduser ( location [ 'rootpath' ] ) LOG . info ( "Connecting to file state with rootpath: " + rootpath ) state_manager = FileStateManager ( name , rootpath ) state_managers . append ( state_manager ) return state_managers
9587	def isarray ( array , test , dim = 2 ) : if dim > 1 : return all ( isarray ( array [ i ] , test , dim - 1 ) for i in range ( len ( array ) ) ) return all ( test ( i ) for i in array )
4628	def get_private ( self ) : encoded = "%s %d" % ( self . brainkey , self . sequence ) a = _bytes ( encoded ) s = hashlib . sha256 ( hashlib . sha512 ( a ) . digest ( ) ) . digest ( ) return PrivateKey ( hexlify ( s ) . decode ( "ascii" ) , prefix = self . prefix )
8814	def get_interfaces ( self ) : LOG . debug ( "Getting interfaces from Xapi" ) with self . sessioned ( ) as session : instances = self . get_instances ( session ) recs = session . xenapi . VIF . get_all_records ( ) interfaces = set ( ) for vif_ref , rec in recs . iteritems ( ) : vm = instances . get ( rec [ "VM" ] ) if not vm : continue device_id = vm . uuid interfaces . add ( VIF ( device_id , rec , vif_ref ) ) return interfaces
8475	def _getClassInstance ( path , args = None ) : if not path . endswith ( ".py" ) : return None if args is None : args = { } classname = AtomShieldsScanner . _getClassName ( path ) basename = os . path . basename ( path ) . replace ( ".py" , "" ) sys . path . append ( os . path . dirname ( path ) ) try : mod = __import__ ( basename , globals ( ) , locals ( ) , [ classname ] , - 1 ) class_ = getattr ( mod , classname ) instance = class_ ( ** args ) except Exception as e : AtomShieldsScanner . _debug ( "[!] %s" % e ) return None finally : sys . path . remove ( os . path . dirname ( path ) ) return instance
9132	def count ( cls , session : Optional [ Session ] = None ) -> int : if session is None : session = _make_session ( ) count = session . query ( cls ) . count ( ) session . close ( ) return count
5588	def calculate_slope_aspect ( elevation , xres , yres , z = 1.0 , scale = 1.0 ) : z = float ( z ) scale = float ( scale ) height , width = elevation . shape [ 0 ] - 2 , elevation . shape [ 1 ] - 2 window = [ z * elevation [ row : ( row + height ) , col : ( col + width ) ] for ( row , col ) in product ( range ( 3 ) , range ( 3 ) ) ] x = ( ( window [ 0 ] + window [ 3 ] + window [ 3 ] + window [ 6 ] ) - ( window [ 2 ] + window [ 5 ] + window [ 5 ] + window [ 8 ] ) ) / ( 8.0 * xres * scale ) y = ( ( window [ 6 ] + window [ 7 ] + window [ 7 ] + window [ 8 ] ) - ( window [ 0 ] + window [ 1 ] + window [ 1 ] + window [ 2 ] ) ) / ( 8.0 * yres * scale ) slope = math . pi / 2 - np . arctan ( np . sqrt ( x * x + y * y ) ) aspect = np . arctan2 ( x , y ) return slope , aspect
4036	def ss_wrap ( func ) : def wrapper ( self , * args , ** kwargs ) : if not self . savedsearch : self . savedsearch = SavedSearch ( self ) return func ( self , * args , ** kwargs ) return wrapper
8088	def fontsize ( self , fontsize = None ) : if fontsize is not None : self . _canvas . fontsize = fontsize else : return self . _canvas . fontsize
9972	def read_range ( filepath , range_expr , sheet = None , dict_generator = None ) : def default_generator ( cells ) : for row_ind , row in enumerate ( cells ) : for col_ind , cell in enumerate ( row ) : yield ( row_ind , col_ind ) , cell . value book = opxl . load_workbook ( filepath , data_only = True ) if _is_range_address ( range_expr ) : sheet_names = [ name . upper ( ) for name in book . sheetnames ] index = sheet_names . index ( sheet . upper ( ) ) cells = book . worksheets [ index ] [ range_expr ] else : cells = _get_namedrange ( book , range_expr , sheet ) if isinstance ( cells , opxl . cell . Cell ) : return cells . value if dict_generator is None : dict_generator = default_generator gen = dict_generator ( cells ) return { keyval [ 0 ] : keyval [ 1 ] for keyval in gen }
8856	def on_current_tab_changed ( self ) : self . menuEdit . clear ( ) self . menuModes . clear ( ) self . menuPanels . clear ( ) editor = self . tabWidget . current_widget ( ) self . menuEdit . setEnabled ( editor is not None ) self . menuModes . setEnabled ( editor is not None ) self . menuPanels . setEnabled ( editor is not None ) self . actionSave . setEnabled ( editor is not None ) self . actionSave_as . setEnabled ( editor is not None ) self . actionConfigure_run . setEnabled ( editor is not None ) self . actionRun . setEnabled ( editor is not None ) if editor is not None : self . setup_mnu_edit ( editor ) self . setup_mnu_modes ( editor ) self . setup_mnu_panels ( editor ) self . widgetOutline . set_editor ( editor ) self . _update_status_bar ( editor )
6218	def get_bbox ( self , primitive ) : accessor = primitive . attributes . get ( 'POSITION' ) return accessor . min , accessor . max
952	def trainTM ( sequence , timeSteps , noiseLevel ) : currentColumns = np . zeros ( tm . numberOfColumns ( ) , dtype = "uint32" ) predictedColumns = np . zeros ( tm . numberOfColumns ( ) , dtype = "uint32" ) ts = 0 for t in range ( timeSteps ) : tm . reset ( ) for k in range ( 4 ) : v = corruptVector ( sequence [ k ] [ : ] , noiseLevel , sparseCols ) tm . compute ( set ( v [ : ] . nonzero ( ) [ 0 ] . tolist ( ) ) , learn = True ) activeColumnsIndices = [ tm . columnForCell ( i ) for i in tm . getActiveCells ( ) ] predictedColumnIndices = [ tm . columnForCell ( i ) for i in tm . getPredictiveCells ( ) ] currentColumns = [ 1 if i in activeColumnsIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ] acc = accuracy ( currentColumns , predictedColumns ) x . append ( ts ) y . append ( acc ) ts += 1 predictedColumns = [ 1 if i in predictedColumnIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ]
3717	def economic_status ( CASRN , Method = None , AvailableMethods = False ) : load_economic_data ( ) CASi = CAS2int ( CASRN ) def list_methods ( ) : methods = [ ] methods . append ( 'Combined' ) if CASRN in _EPACDRDict : methods . append ( EPACDR ) if CASRN in _ECHATonnageDict : methods . append ( ECHA ) if CASi in HPV_data . index : methods . append ( OECD ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == EPACDR : status = 'US public: ' + str ( _EPACDRDict [ CASRN ] ) elif Method == ECHA : status = _ECHATonnageDict [ CASRN ] elif Method == OECD : status = 'OECD HPV Chemicals' elif Method == 'Combined' : status = [ ] if CASRN in _EPACDRDict : status += [ 'US public: ' + str ( _EPACDRDict [ CASRN ] ) ] if CASRN in _ECHATonnageDict : status += _ECHATonnageDict [ CASRN ] if CASi in HPV_data . index : status += [ 'OECD HPV Chemicals' ] elif Method == NONE : status = None else : raise Exception ( 'Failure in in function' ) return status
563	def invariant ( self ) : assert isinstance ( self . description , str ) assert isinstance ( self . singleNodeOnly , bool ) assert isinstance ( self . inputs , dict ) assert isinstance ( self . outputs , dict ) assert isinstance ( self . parameters , dict ) assert isinstance ( self . commands , dict ) hasDefaultInput = False for k , v in self . inputs . items ( ) : assert isinstance ( k , str ) assert isinstance ( v , InputSpec ) v . invariant ( ) if v . isDefaultInput : assert not hasDefaultInput hasDefaultInput = True hasDefaultOutput = False for k , v in self . outputs . items ( ) : assert isinstance ( k , str ) assert isinstance ( v , OutputSpec ) v . invariant ( ) if v . isDefaultOutput : assert not hasDefaultOutput hasDefaultOutput = True for k , v in self . parameters . items ( ) : assert isinstance ( k , str ) assert isinstance ( v , ParameterSpec ) v . invariant ( ) for k , v in self . commands . items ( ) : assert isinstance ( k , str ) assert isinstance ( v , CommandSpec ) v . invariant ( )
1837	def JGE ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , ( cpu . SF == cpu . OF ) , target . read ( ) , cpu . PC )
10225	def get_correlation_graph ( graph : BELGraph ) -> Graph : result = Graph ( ) for u , v , d in graph . edges ( data = True ) : if d [ RELATION ] not in CORRELATIVE_RELATIONS : continue if not result . has_edge ( u , v ) : result . add_edge ( u , v , ** { d [ RELATION ] : True } ) elif d [ RELATION ] not in result [ u ] [ v ] : log . log ( 5 , 'broken correlation relation for %s, %s' , u , v ) result [ u ] [ v ] [ d [ RELATION ] ] = True result [ v ] [ u ] [ d [ RELATION ] ] = True return result
2516	def p_file_notice ( self , f_term , predicate ) : try : for _ , _ , notice in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_notice ( self . doc , six . text_type ( notice ) ) except CardinalityError : self . more_than_one_error ( 'file notice' )
2146	def _separate ( self , kwargs ) : self . _pop_none ( kwargs ) result = { } for field in Resource . config_fields : if field in kwargs : result [ field ] = kwargs . pop ( field ) if field in Resource . json_fields : if not isinstance ( result [ field ] , six . string_types ) : continue try : data = json . loads ( result [ field ] ) result [ field ] = data except ValueError : raise exc . TowerCLIError ( 'Provided json file format ' 'invalid. Please recheck.' ) return result
9390	def get_aggregation_timestamp ( self , timestamp , granularity = 'second' ) : if granularity is None or granularity . lower ( ) == 'none' : return int ( timestamp ) , 1 elif granularity == 'hour' : return ( int ( timestamp ) / ( 3600 * 1000 ) ) * 3600 * 1000 , 3600 elif granularity == 'minute' : return ( int ( timestamp ) / ( 60 * 1000 ) ) * 60 * 1000 , 60 else : return ( int ( timestamp ) / 1000 ) * 1000 , 1
1852	def SAR ( cpu , dest , src ) : OperandSize = dest . size countMask = { 8 : 0x1f , 16 : 0x1f , 32 : 0x1f , 64 : 0x3f } [ OperandSize ] count = src . read ( ) & countMask value = dest . read ( ) res = Operators . SAR ( OperandSize , value , Operators . ZEXTEND ( count , OperandSize ) ) dest . write ( res ) SIGN_MASK = ( 1 << ( OperandSize - 1 ) ) if issymbolic ( count ) : cpu . CF = Operators . ITE ( Operators . AND ( count != 0 , count <= OperandSize ) , ( ( value >> Operators . ZEXTEND ( count - 1 , OperandSize ) ) & 1 ) != 0 , cpu . CF ) else : if count != 0 : if count > OperandSize : count = OperandSize cpu . CF = Operators . EXTRACT ( value , count - 1 , 1 ) != 0 cpu . ZF = Operators . ITE ( count != 0 , res == 0 , cpu . ZF ) cpu . SF = Operators . ITE ( count != 0 , ( res & SIGN_MASK ) != 0 , cpu . SF ) cpu . OF = Operators . ITE ( count == 1 , False , cpu . OF ) cpu . PF = Operators . ITE ( count != 0 , cpu . _calculate_parity_flag ( res ) , cpu . PF )
11468	def rm ( self , filename ) : try : self . _ftp . delete ( filename ) except error_perm : try : current_folder = self . _ftp . pwd ( ) self . cd ( filename ) except error_perm : print ( '550 Delete operation failed %s ' 'does not exist!' % ( filename , ) ) else : self . cd ( current_folder ) print ( '550 Delete operation failed %s ' 'is a folder. Use rmdir function ' 'to delete it.' % ( filename , ) )
7560	def get_total ( tots , node ) : if ( node . is_leaf ( ) or node . is_root ( ) ) : return 0 else : if len ( node . children ) > 2 : down_r = node . children [ 0 ] down_l = node . children [ 1 ] for child in node . children [ 2 : ] : down_l += child else : down_r , down_l = node . children lendr = sum ( 1 for i in down_r . iter_leaves ( ) ) lendl = sum ( 1 for i in down_l . iter_leaves ( ) ) up_r = node . get_sisters ( ) [ 0 ] lenur = sum ( 1 for i in up_r . iter_leaves ( ) ) lenul = tots - ( lendr + lendl + lenur ) return lendr * lendl * lenur * lenul
6214	def load_gltf ( self ) : with open ( self . path ) as fd : self . meta = GLTFMeta ( self . path , json . load ( fd ) )
8720	def operation_upload ( uploader , sources , verify , do_compile , do_file , do_restart ) : sources , destinations = destination_from_source ( sources ) if len ( destinations ) == len ( sources ) : if uploader . prepare ( ) : for filename , dst in zip ( sources , destinations ) : if do_compile : uploader . file_remove ( os . path . splitext ( dst ) [ 0 ] + '.lc' ) uploader . write_file ( filename , dst , verify ) if do_compile and dst != 'init.lua' : uploader . file_compile ( dst ) uploader . file_remove ( dst ) if do_file : uploader . file_do ( os . path . splitext ( dst ) [ 0 ] + '.lc' ) elif do_file : uploader . file_do ( dst ) else : raise Exception ( 'Error preparing nodemcu for reception' ) else : raise Exception ( 'You must specify a destination filename for each file you want to upload.' ) if do_restart : uploader . node_restart ( ) log . info ( 'All done!' )
5604	def _get_warped_array ( input_file = None , indexes = None , dst_bounds = None , dst_shape = None , dst_crs = None , resampling = None , src_nodata = None , dst_nodata = None ) : try : return _rasterio_read ( input_file = input_file , indexes = indexes , dst_bounds = dst_bounds , dst_shape = dst_shape , dst_crs = dst_crs , resampling = resampling , src_nodata = src_nodata , dst_nodata = dst_nodata ) except Exception as e : logger . exception ( "error while reading file %s: %s" , input_file , e ) raise
1664	def CheckMakePairUsesDeduction ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] match = _RE_PATTERN_EXPLICIT_MAKEPAIR . search ( line ) if match : error ( filename , linenum , 'build/explicit_make_pair' , 4 , 'For C++11-compatibility, omit template arguments from make_pair' ' OR use pair directly OR if appropriate, construct a pair directly' )
4607	def blacklist ( self , account ) : assert callable ( self . blockchain . account_whitelist ) return self . blockchain . account_whitelist ( account , lists = [ "black" ] , account = self )
9044	def listen ( self , func ) : self . _C0 . listen ( func ) self . _C1 . listen ( func )
4224	def _load_keyring_class ( keyring_name ) : module_name , sep , class_name = keyring_name . rpartition ( '.' ) __import__ ( module_name ) module = sys . modules [ module_name ] return getattr ( module , class_name )
13443	def path ( self , a_hash , b_hash ) : def _path ( a , b ) : if a is b : return [ a ] else : assert len ( a . children ) == 1 return [ a ] + _path ( a . children [ 0 ] , b ) a = self . nodes [ a_hash ] b = self . nodes [ b_hash ] return _path ( a , b ) [ 1 : ]
4583	def convert_mode ( image , mode = 'RGB' ) : deprecated . deprecated ( 'util.gif.convert_model' ) return image if ( image . mode == mode ) else image . convert ( mode = mode )
12646	def set_aad_metadata ( uri , resource , client ) : set_config_value ( 'authority_uri' , uri ) set_config_value ( 'aad_resource' , resource ) set_config_value ( 'aad_client' , client )
10774	def get_settings_path ( settings_module ) : cwd = os . getcwd ( ) settings_filename = '%s.py' % ( settings_module . split ( '.' ) [ - 1 ] ) while cwd : if settings_filename in os . listdir ( cwd ) : break cwd = os . path . split ( cwd ) [ 0 ] if os . name == 'nt' and NT_ROOT . match ( cwd ) : return None elif cwd == '/' : return None return cwd
5526	def grab ( bbox = None , childprocess = None , backend = None ) : if childprocess is None : childprocess = childprocess_default_value ( ) return _grab ( to_file = False , childprocess = childprocess , backend = backend , bbox = bbox )
4404	def lock ( self , name , timeout = None , sleep = 0.1 ) : return Lock ( self , name , timeout = timeout , sleep = sleep )
7460	def save_json2 ( data ) : datadict = OrderedDict ( [ ( "outfiles" , data . __dict__ [ "outfiles" ] ) , ( "stats_files" , dict ( data . __dict__ [ "stats_files" ] ) ) , ( "stats_dfs" , data . __dict__ [ "stats_dfs" ] ) ] )
6440	def dist_euclidean ( src , tar , qval = 2 , alphabet = None ) : return Euclidean ( ) . dist ( src , tar , qval , alphabet )
4749	def get_parm ( self , key ) : if key in self . __parm . keys ( ) : return self . __parm [ key ] return None
11830	def child_node ( self , problem , action ) : "Fig. 3.10" next = problem . result ( self . state , action ) return Node ( next , self , action , problem . path_cost ( self . path_cost , self . state , action , next ) )
6774	def deploy ( self , site = None ) : r = self . local_renderer self . deploy_logrotate ( ) cron_crontabs = [ ] for _site , site_data in self . iter_sites ( site = site ) : r . env . cron_stdout_log = r . format ( r . env . stdout_log_template ) r . env . cron_stderr_log = r . format ( r . env . stderr_log_template ) r . sudo ( 'touch {cron_stdout_log}' ) r . sudo ( 'touch {cron_stderr_log}' ) r . sudo ( 'sudo chown {user}:{user} {cron_stdout_log}' ) r . sudo ( 'sudo chown {user}:{user} {cron_stderr_log}' ) if self . verbose : print ( 'site:' , site , file = sys . stderr ) print ( 'env.crontabs_selected:' , self . env . crontabs_selected , file = sys . stderr ) for selected_crontab in self . env . crontabs_selected : lines = self . env . crontabs_available . get ( selected_crontab , [ ] ) if self . verbose : print ( 'lines:' , lines , file = sys . stderr ) for line in lines : cron_crontabs . append ( r . format ( line ) ) if not cron_crontabs : return cron_crontabs = self . env . crontab_headers + cron_crontabs cron_crontabs . append ( '\n' ) r . env . crontabs_rendered = '\n' . join ( cron_crontabs ) fn = self . write_to_file ( content = r . env . crontabs_rendered ) print ( 'fn:' , fn ) r . env . put_remote_path = r . put ( local_path = fn ) if isinstance ( r . env . put_remote_path , ( tuple , list ) ) : r . env . put_remote_path = r . env . put_remote_path [ 0 ] r . sudo ( 'crontab -u {cron_user} {put_remote_path}' )
10652	def prepare_to_run ( self , clock , period_count ) : for c in self . components : c . prepare_to_run ( clock , period_count ) for a in self . activities : a . prepare_to_run ( clock , period_count )
2190	def _product_file_hash ( self , product = None ) : if self . hasher is None : return None else : products = self . _rectify_products ( product ) product_file_hash = [ util_hash . hash_file ( p , hasher = self . hasher , base = 'hex' ) for p in products ] return product_file_hash
8908	def fetch_by_name ( self , name ) : service = self . collection . find_one ( { 'name' : name } ) if not service : raise ServiceNotFound return Service ( service )
7006	def plot_training_results ( classifier , classlabels , outfile ) : if isinstance ( classifier , str ) and os . path . exists ( classifier ) : with open ( classifier , 'rb' ) as infd : clfdict = pickle . load ( infd ) elif isinstance ( classifier , dict ) : clfdict = classifier else : LOGERROR ( "can't figure out the input classifier arg" ) return None confmatrix = clfdict [ 'best_confmatrix' ] overall_feature_importances = clfdict [ 'best_classifier' ] . feature_importances_ feature_importances_per_tree = np . array ( [ tree . feature_importances_ for tree in clfdict [ 'best_classifier' ] . estimators_ ] ) stdev_feature_importances = np . std ( feature_importances_per_tree , axis = 0 ) feature_names = np . array ( clfdict [ 'feature_names' ] ) plt . figure ( figsize = ( 6.4 * 3.0 , 4.8 ) ) plt . subplot ( 121 ) classes = np . array ( classlabels ) plt . imshow ( confmatrix , interpolation = 'nearest' , cmap = plt . cm . Blues ) tick_marks = np . arange ( len ( classes ) ) plt . xticks ( tick_marks , classes ) plt . yticks ( tick_marks , classes ) plt . title ( 'evaluation set confusion matrix' ) plt . ylabel ( 'predicted class' ) plt . xlabel ( 'actual class' ) thresh = confmatrix . max ( ) / 2. for i , j in itertools . product ( range ( confmatrix . shape [ 0 ] ) , range ( confmatrix . shape [ 1 ] ) ) : plt . text ( j , i , confmatrix [ i , j ] , horizontalalignment = "center" , color = "white" if confmatrix [ i , j ] > thresh else "black" ) plt . subplot ( 122 ) features = np . array ( feature_names ) sorted_ind = np . argsort ( overall_feature_importances ) [ : : - 1 ] features = features [ sorted_ind ] feature_names = feature_names [ sorted_ind ] overall_feature_importances = overall_feature_importances [ sorted_ind ] stdev_feature_importances = stdev_feature_importances [ sorted_ind ] plt . bar ( np . arange ( 0 , features . size ) , overall_feature_importances , yerr = stdev_feature_importances , width = 0.8 , color = 'grey' ) plt . xticks ( np . arange ( 0 , features . size ) , features , rotation = 90 ) plt . yticks ( [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9 , 1.0 ] ) plt . xlim ( - 0.75 , features . size - 1.0 + 0.75 ) plt . ylim ( 0.0 , 0.9 ) plt . ylabel ( 'relative importance' ) plt . title ( 'relative importance of features' ) plt . subplots_adjust ( wspace = 0.1 ) plt . savefig ( outfile , bbox_inches = 'tight' , dpi = 100 ) plt . close ( 'all' ) return outfile
9152	def _convert_vpathlist ( input_obj ) : vpl = pgmagick . VPathList ( ) for obj in input_obj : obj = pgmagick . PathMovetoAbs ( pgmagick . Coordinate ( obj [ 0 ] , obj [ 1 ] ) ) vpl . append ( obj ) return vpl
4887	def get_course_final_price ( self , mode , currency = '$' , enterprise_catalog_uuid = None ) : try : price_details = self . client . baskets . calculate . get ( sku = [ mode [ 'sku' ] ] , username = self . user . username , catalog = enterprise_catalog_uuid , ) except ( SlumberBaseException , ConnectionError , Timeout ) as exc : LOGGER . exception ( 'Failed to get price details for sku %s due to: %s' , mode [ 'sku' ] , str ( exc ) ) price_details = { } price = price_details . get ( 'total_incl_tax' , mode [ 'min_price' ] ) if price != mode [ 'min_price' ] : return format_price ( price , currency ) return mode [ 'original_price' ]
9563	def _as_dict ( self , r ) : d = dict ( ) for i , f in enumerate ( self . _field_names ) : d [ f ] = r [ i ] if i < len ( r ) else None return d
9024	def write ( self , string ) : bytes_ = string . encode ( self . _encoding ) self . _file . write ( bytes_ )
7721	def get_history ( self ) : for child in xml_element_iter ( self . xmlnode . children ) : if get_node_ns_uri ( child ) == MUC_NS and child . name == "history" : maxchars = from_utf8 ( child . prop ( "maxchars" ) ) if maxchars is not None : maxchars = int ( maxchars ) maxstanzas = from_utf8 ( child . prop ( "maxstanzas" ) ) if maxstanzas is not None : maxstanzas = int ( maxstanzas ) maxseconds = from_utf8 ( child . prop ( "maxseconds" ) ) if maxseconds is not None : maxseconds = int ( maxseconds ) since = None return HistoryParameters ( maxchars , maxstanzas , maxseconds , since )
12967	def random ( self , cascadeFetch = False ) : matchedKeys = list ( self . getPrimaryKeys ( ) ) obj = None while matchedKeys and not obj : key = matchedKeys . pop ( random . randint ( 0 , len ( matchedKeys ) - 1 ) ) obj = self . get ( key , cascadeFetch = cascadeFetch ) return obj
8456	def up_to_date ( version = None ) : temple . check . in_git_repo ( ) temple . check . is_temple_project ( ) temple_config = temple . utils . read_temple_config ( ) old_template_version = temple_config [ '_version' ] new_template_version = version or _get_latest_template_version ( temple_config [ '_template' ] ) return new_template_version == old_template_version
11749	def attach_bundle ( self , bundle ) : if not isinstance ( bundle , BlueprintBundle ) : raise IncompatibleBundle ( 'BlueprintBundle object passed to attach_bundle must be of type {0}' . format ( BlueprintBundle ) ) elif len ( bundle . blueprints ) == 0 : raise MissingBlueprints ( "Bundles must contain at least one flask.Blueprint" ) elif self . _bundle_exists ( bundle . path ) : raise ConflictingPath ( "Duplicate bundle path {0}" . format ( bundle . path ) ) elif self . _journey_path == bundle . path == '/' : raise ConflictingPath ( "Bundle path and Journey path cannot both be {0}" . format ( bundle . path ) ) self . _attached_bundles . append ( bundle )
2296	def featurize_row ( self , x , y ) : x = x . ravel ( ) y = y . ravel ( ) b = np . ones ( x . shape ) dx = np . cos ( np . dot ( self . W2 , np . vstack ( ( x , b ) ) ) ) . mean ( 1 ) dy = np . cos ( np . dot ( self . W2 , np . vstack ( ( y , b ) ) ) ) . mean ( 1 ) if ( sum ( dx ) > sum ( dy ) ) : return np . hstack ( ( dx , dy , np . cos ( np . dot ( self . W , np . vstack ( ( x , y , b ) ) ) ) . mean ( 1 ) ) ) else : return np . hstack ( ( dx , dy , np . cos ( np . dot ( self . W , np . vstack ( ( y , x , b ) ) ) ) . mean ( 1 ) ) )
13321	def add_active_module ( module ) : modules = set ( get_active_modules ( ) ) modules . add ( module ) new_modules_path = os . pathsep . join ( [ m . path for m in modules ] ) os . environ [ 'CPENV_ACTIVE_MODULES' ] = str ( new_modules_path )
3210	def get ( self , key , delete_if_expired = True ) : self . _update_cache_stats ( key , None ) if key in self . _CACHE : ( expiration , obj ) = self . _CACHE [ key ] if expiration > self . _now ( ) : self . _update_cache_stats ( key , 'hit' ) return obj else : if delete_if_expired : self . delete ( key ) self . _update_cache_stats ( key , 'expired' ) return None self . _update_cache_stats ( key , 'miss' ) return None
12174	def genIndex ( folder , forceIDs = [ ] ) : if not os . path . exists ( folder + "/swhlab4/" ) : print ( " !! cannot index if no /swhlab4/" ) return timestart = cm . timethis ( ) files = glob . glob ( folder + "/*.*" ) files . extend ( glob . glob ( folder + "/swhlab4/*.*" ) ) print ( " -- indexing glob took %.02f ms" % ( cm . timethis ( timestart ) * 1000 ) ) files . extend ( genPNGs ( folder , files ) ) files = sorted ( files ) timestart = cm . timethis ( ) d = cm . getIDfileDict ( files ) print ( " -- filedict length:" , len ( d ) ) print ( " -- generating ID dict took %.02f ms" % ( cm . timethis ( timestart ) * 1000 ) ) groups = cm . getABFgroups ( files ) print ( " -- groups length:" , len ( groups ) ) for ID in sorted ( list ( groups . keys ( ) ) ) : overwrite = False for abfID in groups [ ID ] : if abfID in forceIDs : overwrite = True try : htmlABF ( ID , groups [ ID ] , d , folder , overwrite ) except : print ( "~~ HTML GENERATION FAILED!!!" ) menu = expMenu ( groups , folder ) makeSplash ( menu , folder ) makeMenu ( menu , folder ) htmlFrames ( d , folder ) makeMenu ( menu , folder ) makeSplash ( menu , folder )
12663	def load_mask_data ( image , allow_empty = True ) : mask = load_mask ( image , allow_empty = allow_empty ) return get_img_data ( mask ) , mask . get_affine ( )
8719	def backup ( self , path ) : log . info ( 'Backing up in ' + path ) files = self . file_list ( ) self . prepare ( ) for f in files : self . read_file ( f [ 0 ] , os . path . join ( path , f [ 0 ] ) )
3438	def merge ( self , right , prefix_existing = None , inplace = True , objective = 'left' ) : if inplace : new_model = self else : new_model = self . copy ( ) new_model . id = '{}_{}' . format ( self . id , right . id ) new_reactions = deepcopy ( right . reactions ) if prefix_existing is not None : existing = new_reactions . query ( lambda rxn : rxn . id in self . reactions ) for reaction in existing : reaction . id = '{}{}' . format ( prefix_existing , reaction . id ) new_model . add_reactions ( new_reactions ) interface = new_model . problem new_vars = [ interface . Variable . clone ( v ) for v in right . variables if v . name not in new_model . variables ] new_model . add_cons_vars ( new_vars ) new_cons = [ interface . Constraint . clone ( c , model = new_model . solver ) for c in right . constraints if c . name not in new_model . constraints ] new_model . add_cons_vars ( new_cons , sloppy = True ) new_model . objective = dict ( left = self . objective , right = right . objective , sum = self . objective . expression + right . objective . expression ) [ objective ] return new_model
8618	def find_item_by_name ( list_ , namegetter , name ) : matching_items = [ i for i in list_ if namegetter ( i ) == name ] if not matching_items : prog = re . compile ( re . escape ( name ) + '$' , re . IGNORECASE ) matching_items = [ i for i in list_ if prog . match ( namegetter ( i ) ) ] if not matching_items : prog = re . compile ( re . escape ( name ) ) matching_items = [ i for i in list_ if prog . match ( namegetter ( i ) ) ] if not matching_items : prog = re . compile ( re . escape ( name ) , re . IGNORECASE ) matching_items = [ i for i in list_ if prog . match ( namegetter ( i ) ) ] if not matching_items : prog = re . compile ( re . escape ( name ) ) matching_items = [ i for i in list_ if prog . search ( namegetter ( i ) ) ] if not matching_items : prog = re . compile ( re . escape ( name ) , re . IGNORECASE ) matching_items = [ i for i in list_ if prog . search ( namegetter ( i ) ) ] return matching_items
5132	def generate_transition_matrix ( g , seed = None ) : g = _test_graph ( g ) if isinstance ( seed , numbers . Integral ) : np . random . seed ( seed ) nV = g . number_of_nodes ( ) mat = np . zeros ( ( nV , nV ) ) for v in g . nodes ( ) : ind = [ e [ 1 ] for e in sorted ( g . out_edges ( v ) ) ] deg = len ( ind ) if deg == 1 : mat [ v , ind ] = 1 elif deg > 1 : probs = np . ceil ( np . random . rand ( deg ) * 100 ) / 100. if np . isclose ( np . sum ( probs ) , 0 ) : probs [ np . random . randint ( deg ) ] = 1 mat [ v , ind ] = probs / np . sum ( probs ) return mat
142	def from_shapely ( polygon_shapely , label = None ) : import shapely . geometry ia . do_assert ( isinstance ( polygon_shapely , shapely . geometry . Polygon ) ) if polygon_shapely . exterior is None or len ( polygon_shapely . exterior . coords ) == 0 : return Polygon ( [ ] , label = label ) exterior = np . float32 ( [ [ x , y ] for ( x , y ) in polygon_shapely . exterior . coords ] ) return Polygon ( exterior , label = label )
403	def ramp ( x , v_min = 0 , v_max = 1 , name = None ) : return tf . clip_by_value ( x , clip_value_min = v_min , clip_value_max = v_max , name = name )
5937	def transform_args ( self , * args , ** kwargs ) : options = [ ] for option , value in kwargs . items ( ) : if not option . startswith ( '-' ) : if len ( option ) == 1 : option = '-' + option else : option = '--' + option if value is True : options . append ( option ) continue elif value is False : raise ValueError ( 'A False value is ambiguous for option {0!r}' . format ( option ) ) if option [ : 2 ] == '--' : options . append ( option + '=' + str ( value ) ) else : options . extend ( ( option , str ( value ) ) ) return options + list ( args )
9404	def _get_function_ptr ( self , name ) : func = _make_function_ptr_instance self . _function_ptrs . setdefault ( name , func ( self , name ) ) return self . _function_ptrs [ name ]
7745	def _loop_timeout_cb ( self , main_loop ) : self . _anything_done = True logger . debug ( "_loop_timeout_cb() called" ) main_loop . quit ( )
4328	def echo ( self , gain_in = 0.8 , gain_out = 0.9 , n_echos = 1 , delays = [ 60 ] , decays = [ 0.4 ] ) : if not is_number ( gain_in ) or gain_in <= 0 or gain_in > 1 : raise ValueError ( "gain_in must be a number between 0 and 1." ) if not is_number ( gain_out ) or gain_out <= 0 or gain_out > 1 : raise ValueError ( "gain_out must be a number between 0 and 1." ) if not isinstance ( n_echos , int ) or n_echos <= 0 : raise ValueError ( "n_echos must be a positive integer." ) if not isinstance ( delays , list ) : raise ValueError ( "delays must be a list" ) if len ( delays ) != n_echos : raise ValueError ( "the length of delays must equal n_echos" ) if any ( ( not is_number ( p ) or p <= 0 ) for p in delays ) : raise ValueError ( "the elements of delays must be numbers > 0" ) if not isinstance ( decays , list ) : raise ValueError ( "decays must be a list" ) if len ( decays ) != n_echos : raise ValueError ( "the length of decays must equal n_echos" ) if any ( ( not is_number ( p ) or p <= 0 or p > 1 ) for p in decays ) : raise ValueError ( "the elements of decays must be between 0 and 1" ) effect_args = [ 'echo' , '{:f}' . format ( gain_in ) , '{:f}' . format ( gain_out ) ] for i in range ( n_echos ) : effect_args . extend ( [ '{}' . format ( delays [ i ] ) , '{}' . format ( decays [ i ] ) ] ) self . effects . extend ( effect_args ) self . effects_log . append ( 'echo' ) return self
9909	def set_primary ( self ) : query = EmailAddress . objects . filter ( is_primary = True , user = self . user ) query = query . exclude ( pk = self . pk ) with transaction . atomic ( ) : query . update ( is_primary = False ) self . is_primary = True self . save ( ) logger . info ( "Set %s as the primary email address for %s." , self . email , self . user , )
10067	def json_files_serializer ( objs , status = None ) : files = [ file_serializer ( obj ) for obj in objs ] return make_response ( json . dumps ( files ) , status )
4614	def awaitTxConfirmation ( self , transaction , limit = 10 ) : counter = 10 for block in self . blocks ( ) : counter += 1 for tx in block [ "transactions" ] : if sorted ( tx [ "signatures" ] ) == sorted ( transaction [ "signatures" ] ) : return tx if counter > limit : raise Exception ( "The operation has not been added after 10 blocks!" )
12511	def load_nipy_img ( nii_file ) : import nipy if not os . path . exists ( nii_file ) : raise FileNotFound ( nii_file ) try : return nipy . load_image ( nii_file ) except Exception as exc : raise Exception ( 'Reading file {0}.' . format ( repr_imgs ( nii_file ) ) ) from exc
6057	def resized_array_2d_from_array_2d_and_resized_shape ( array_2d , resized_shape , origin = ( - 1 , - 1 ) , pad_value = 0.0 ) : y_is_even = int ( array_2d . shape [ 0 ] ) % 2 == 0 x_is_even = int ( array_2d . shape [ 1 ] ) % 2 == 0 if origin is ( - 1 , - 1 ) : if y_is_even : y_centre = int ( array_2d . shape [ 0 ] / 2 ) elif not y_is_even : y_centre = int ( array_2d . shape [ 0 ] / 2 ) if x_is_even : x_centre = int ( array_2d . shape [ 1 ] / 2 ) elif not x_is_even : x_centre = int ( array_2d . shape [ 1 ] / 2 ) origin = ( y_centre , x_centre ) resized_array = np . zeros ( shape = resized_shape ) if y_is_even : y_min = origin [ 0 ] - int ( resized_shape [ 0 ] / 2 ) y_max = origin [ 0 ] + int ( ( resized_shape [ 0 ] / 2 ) ) + 1 elif not y_is_even : y_min = origin [ 0 ] - int ( resized_shape [ 0 ] / 2 ) y_max = origin [ 0 ] + int ( ( resized_shape [ 0 ] / 2 ) ) + 1 if x_is_even : x_min = origin [ 1 ] - int ( resized_shape [ 1 ] / 2 ) x_max = origin [ 1 ] + int ( ( resized_shape [ 1 ] / 2 ) ) + 1 elif not x_is_even : x_min = origin [ 1 ] - int ( resized_shape [ 1 ] / 2 ) x_max = origin [ 1 ] + int ( ( resized_shape [ 1 ] / 2 ) ) + 1 for y_resized , y in enumerate ( range ( y_min , y_max ) ) : for x_resized , x in enumerate ( range ( x_min , x_max ) ) : if y >= 0 and y < array_2d . shape [ 0 ] and x >= 0 and x < array_2d . shape [ 1 ] : if y_resized >= 0 and y_resized < resized_shape [ 0 ] and x_resized >= 0 and x_resized < resized_shape [ 1 ] : resized_array [ y_resized , x_resized ] = array_2d [ y , x ] else : if y_resized >= 0 and y_resized < resized_shape [ 0 ] and x_resized >= 0 and x_resized < resized_shape [ 1 ] : resized_array [ y_resized , x_resized ] = pad_value return resized_array
7531	def _plot_dag ( dag , results , snames ) : try : import matplotlib . pyplot as plt from matplotlib . dates import date2num from matplotlib . cm import gist_rainbow plt . figure ( "dag_layout" , figsize = ( 10 , 10 ) ) nx . draw ( dag , pos = nx . spring_layout ( dag ) , node_color = 'pink' , with_labels = True ) plt . savefig ( "./dag_layout.png" , bbox_inches = 'tight' , dpi = 200 ) pos = { } colors = { } for node in dag : mtd = results [ node ] . metadata start = date2num ( mtd . started ) _ , _ , sname = node . split ( "-" , 2 ) sid = snames . index ( sname ) pos [ node ] = ( start + sid , start * 1e6 ) colors [ node ] = mtd . engine_id plt . figure ( "dag_starttimes" , figsize = ( 10 , 16 ) ) nx . draw ( dag , pos , node_list = colors . keys ( ) , node_color = colors . values ( ) , cmap = gist_rainbow , with_labels = True ) plt . savefig ( "./dag_starttimes.png" , bbox_inches = 'tight' , dpi = 200 ) except Exception as inst : LOGGER . warning ( inst )
8052	def parse_theme ( self , xml ) : kt = KulerTheme ( ) kt . author = xml . getElementsByTagName ( "author" ) [ 0 ] kt . author = kt . author . childNodes [ 1 ] . childNodes [ 0 ] . nodeValue kt . id = int ( self . parse_tag ( xml , "id" ) ) kt . label = self . parse_tag ( xml , "label" ) mode = self . parse_tag ( xml , "mode" ) for swatch in xml . getElementsByTagName ( "swatch" ) : c1 = float ( self . parse_tag ( swatch , "c1" ) ) c2 = float ( self . parse_tag ( swatch , "c2" ) ) c3 = float ( self . parse_tag ( swatch , "c3" ) ) c4 = float ( self . parse_tag ( swatch , "c4" ) ) if mode == "rgb" : kt . append ( ( c1 , c2 , c3 ) ) if mode == "cmyk" : kt . append ( cmyk_to_rgb ( c1 , c2 , c3 , c4 ) ) if mode == "hsv" : kt . append ( colorsys . hsv_to_rgb ( c1 , c2 , c3 ) ) if mode == "hex" : kt . append ( hex_to_rgb ( c1 ) ) if mode == "lab" : kt . append ( lab_to_rgb ( c1 , c2 , c3 ) ) if self . _cache . exists ( self . id_string + str ( kt . id ) ) : xml = self . _cache . read ( self . id_string + str ( kt . id ) ) xml = minidom . parseString ( xml ) for tags in xml . getElementsByTagName ( "tag" ) : tags = self . parse_tag ( tags , "label" ) tags = tags . split ( " " ) kt . tags . extend ( tags ) return kt
4031	def _decrypt ( self , value , encrypted_value ) : if sys . platform == 'win32' : return self . _decrypt_windows_chrome ( value , encrypted_value ) if value or ( encrypted_value [ : 3 ] != b'v10' ) : return value encrypted_value = encrypted_value [ 3 : ] encrypted_value_half_len = int ( len ( encrypted_value ) / 2 ) cipher = pyaes . Decrypter ( pyaes . AESModeOfOperationCBC ( self . key , self . iv ) ) decrypted = cipher . feed ( encrypted_value [ : encrypted_value_half_len ] ) decrypted += cipher . feed ( encrypted_value [ encrypted_value_half_len : ] ) decrypted += cipher . feed ( ) return decrypted . decode ( "utf-8" )
1775	def CPUID ( cpu ) : conf = { 0x0 : ( 0x0000000d , 0x756e6547 , 0x6c65746e , 0x49656e69 ) , 0x1 : ( 0x000306c3 , 0x05100800 , 0x7ffafbff , 0xbfebfbff ) , 0x2 : ( 0x76035a01 , 0x00f0b5ff , 0x00000000 , 0x00c10000 ) , 0x4 : { 0x0 : ( 0x1c004121 , 0x01c0003f , 0x0000003f , 0x00000000 ) , 0x1 : ( 0x1c004122 , 0x01c0003f , 0x0000003f , 0x00000000 ) , 0x2 : ( 0x1c004143 , 0x01c0003f , 0x000001ff , 0x00000000 ) , 0x3 : ( 0x1c03c163 , 0x03c0003f , 0x00000fff , 0x00000006 ) } , 0x7 : ( 0x00000000 , 0x00000000 , 0x00000000 , 0x00000000 ) , 0x8 : ( 0x00000000 , 0x00000000 , 0x00000000 , 0x00000000 ) , 0xb : { 0x0 : ( 0x00000001 , 0x00000002 , 0x00000100 , 0x00000005 ) , 0x1 : ( 0x00000004 , 0x00000004 , 0x00000201 , 0x00000003 ) } , 0xd : { 0x0 : ( 0x00000000 , 0x00000000 , 0x00000000 , 0x00000000 ) , 0x1 : ( 0x00000000 , 0x00000000 , 0x00000000 , 0x00000000 ) } , } if cpu . EAX not in conf : logger . warning ( 'CPUID with EAX=%x not implemented @ %x' , cpu . EAX , cpu . PC ) cpu . EAX , cpu . EBX , cpu . ECX , cpu . EDX = 0 , 0 , 0 , 0 return if isinstance ( conf [ cpu . EAX ] , tuple ) : cpu . EAX , cpu . EBX , cpu . ECX , cpu . EDX = conf [ cpu . EAX ] return if cpu . ECX not in conf [ cpu . EAX ] : logger . warning ( 'CPUID with EAX=%x ECX=%x not implemented' , cpu . EAX , cpu . ECX ) cpu . EAX , cpu . EBX , cpu . ECX , cpu . EDX = 0 , 0 , 0 , 0 return cpu . EAX , cpu . EBX , cpu . ECX , cpu . EDX = conf [ cpu . EAX ] [ cpu . ECX ]
2764	def get_all_snapshots ( self ) : data = self . get_data ( "snapshots/" ) return [ Snapshot ( token = self . token , ** snapshot ) for snapshot in data [ 'snapshots' ] ]
3764	def Parachor ( MW , rhol , rhog , sigma ) : r rhol , rhog = rhol * 1000. , rhog * 1000. return sigma ** 0.25 * MW / ( rhol - rhog )
7716	def remove_item ( self , jid , callback = None , error_callback = None ) : item = self . roster [ jid ] if jid not in self . roster : raise KeyError ( jid ) item = RosterItem ( jid , subscription = "remove" ) self . _roster_set ( item , callback , error_callback )
2377	def list_rules ( self ) : for rule in sorted ( self . all_rules , key = lambda rule : rule . name ) : print ( rule ) if self . args . verbose : for line in rule . doc . split ( "\n" ) : print ( " " , line )
2246	def symlink ( real_path , link_path , overwrite = False , verbose = 0 ) : path = normpath ( real_path ) link = normpath ( link_path ) if not os . path . isabs ( path ) : if _can_symlink ( ) : path = os . path . relpath ( path , os . path . dirname ( link ) ) else : path = os . path . abspath ( path ) if verbose : print ( 'Symlink: {path} -> {link}' . format ( path = path , link = link ) ) if islink ( link ) : if verbose : print ( '... already exists' ) pointed = _readlink ( link ) if pointed == path : if verbose > 1 : print ( '... and points to the right place' ) return link if verbose > 1 : if not exists ( link ) : print ( '... but it is broken and points somewhere else: {}' . format ( pointed ) ) else : print ( '... but it points somewhere else: {}' . format ( pointed ) ) if overwrite : util_io . delete ( link , verbose = verbose > 1 ) elif exists ( link ) : if _win32_links is None : if verbose : print ( '... already exists, but its a file. This will error.' ) raise FileExistsError ( 'cannot overwrite a physical path: "{}"' . format ( path ) ) else : if verbose : print ( '... already exists, and is either a file or hard link. ' 'Assuming it is a hard link. ' 'On non-win32 systems this would error.' ) if _win32_links is None : os . symlink ( path , link ) else : _win32_links . _symlink ( path , link , overwrite = overwrite , verbose = verbose ) return link
502	def _labelToCategoryNumber ( self , label ) : if label not in self . saved_categories : self . saved_categories . append ( label ) return pow ( 2 , self . saved_categories . index ( label ) )
10594	def Nu_x ( self , L , theta , Ts , ** statef ) : Tf = statef [ 'T' ] thetar = radians ( theta ) if self . _isgas : self . Tr = Ts - 0.38 * ( Ts - Tf ) beta = self . _fluid . beta ( T = Tf ) else : self . Tr = Ts - 0.5 * ( Ts - Tf ) beta = self . _fluid . beta ( T = self . Tr ) if Ts > Tf : if 0.0 < theta < 45.0 : g = const . g * cos ( thetar ) else : g = const . g else : if - 45.0 < theta < 0.0 : g = const . g * cos ( thetar ) else : g = const . g nu = self . _fluid . nu ( T = self . Tr ) alpha = self . _fluid . alpha ( T = self . Tr ) Gr = dq . Gr ( L , Ts , Tf , beta , nu , g ) Pr = dq . Pr ( nu , alpha ) Ra = Gr * Pr eq = [ self . equation_dict [ r ] for r in self . regions if r . contains_point ( theta , Ra ) ] [ 0 ] return eq ( self , Ra , Pr )
6615	def receive_all ( self ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return return self . dropbox . receive ( )
9292	def python_value ( self , value ) : value = super ( OrderedUUIDField , self ) . python_value ( value ) u = binascii . b2a_hex ( value ) value = u [ 8 : 16 ] + u [ 4 : 8 ] + u [ 0 : 4 ] + u [ 16 : 22 ] + u [ 22 : 32 ] return UUID ( value . decode ( ) )
4513	def drawCircle ( self , x0 , y0 , r , color = None ) : md . draw_circle ( self . set , x0 , y0 , r , color )
7075	def run_periodfinding ( simbasedir , pfmethods = ( 'gls' , 'pdm' , 'bls' ) , pfkwargs = ( { } , { } , { 'startp' : 1.0 , 'maxtransitduration' : 0.3 } ) , getblssnr = False , sigclip = 5.0 , nperiodworkers = 10 , ncontrolworkers = 4 , liststartindex = None , listmaxobjects = None ) : with open ( os . path . join ( simbasedir , 'fakelcs-info.pkl' ) , 'rb' ) as infd : siminfo = pickle . load ( infd ) lcfpaths = siminfo [ 'lcfpath' ] pfdir = os . path . join ( simbasedir , 'periodfinding' ) timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] fakelc_formatkey = 'fake-%s' % siminfo [ 'lcformat' ] lcproc . register_lcformat ( fakelc_formatkey , '*-fakelc.pkl' , timecols , magcols , errcols , 'astrobase.lcproc' , '_read_pklc' , magsarefluxes = siminfo [ 'magsarefluxes' ] ) if liststartindex : lcfpaths = lcfpaths [ liststartindex : ] if listmaxobjects : lcfpaths = lcfpaths [ : listmaxobjects ] pfinfo = periodsearch . parallel_pf ( lcfpaths , pfdir , lcformat = fakelc_formatkey , pfmethods = pfmethods , pfkwargs = pfkwargs , getblssnr = getblssnr , sigclip = sigclip , nperiodworkers = nperiodworkers , ncontrolworkers = ncontrolworkers ) with open ( os . path . join ( simbasedir , 'fakelc-periodsearch.pkl' ) , 'wb' ) as outfd : pickle . dump ( pfinfo , outfd , pickle . HIGHEST_PROTOCOL ) return os . path . join ( simbasedir , 'fakelc-periodsearch.pkl' )
4529	def set_device_brightness ( self , val ) : self . _chipset_brightness = ( val >> 3 ) self . _brightness_list = [ 0xE0 + self . _chipset_brightness ] * self . numLEDs self . _packet [ self . _start_frame : self . _pixel_stop : 4 ] = ( self . _brightness_list )
1501	def template_slave_hcl ( cl_args , masters ) : slave_config_template = "%s/standalone/templates/slave.template.hcl" % cl_args [ "config_path" ] slave_config_actual = "%s/standalone/resources/slave.hcl" % cl_args [ "config_path" ] masters_in_quotes = [ '"%s"' % master for master in masters ] template_file ( slave_config_template , slave_config_actual , { "<nomad_masters:master_port>" : ", " . join ( masters_in_quotes ) } )
9105	def dropbox_factory ( request ) : try : return request . registry . settings [ 'dropbox_container' ] . get_dropbox ( request . matchdict [ 'drop_id' ] ) except KeyError : raise HTTPNotFound ( 'no such dropbox' )
13668	def slinky ( filename , seconds_available , bucket_name , aws_key , aws_secret ) : if not os . environ . get ( 'AWS_ACCESS_KEY_ID' ) and os . environ . get ( 'AWS_SECRET_ACCESS_KEY' ) : print 'Need to set environment variables for AWS access and create a slinky bucket.' exit ( ) print create_temp_s3_link ( filename , seconds_available , bucket_name )
3719	def conductivity ( CASRN = None , AvailableMethods = False , Method = None , full_info = True ) : r def list_methods ( ) : methods = [ ] if CASRN in Lange_cond_pure . index : methods . append ( LANGE_COND ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == LANGE_COND : kappa = float ( Lange_cond_pure . at [ CASRN , 'Conductivity' ] ) if full_info : T = float ( Lange_cond_pure . at [ CASRN , 'T' ] ) elif Method == NONE : kappa , T = None , None else : raise Exception ( 'Failure in in function' ) if full_info : return kappa , T else : return kappa
1581	def create_packet ( header , data ) : packet = IncomingPacket ( ) packet . header = header packet . data = data if len ( header ) == HeronProtocol . HEADER_SIZE : packet . is_header_read = True if len ( data ) == packet . get_datasize ( ) : packet . is_complete = True return packet
12024	def adopt ( self , old_parent , new_parent ) : try : old_id = old_parent [ 'attributes' ] [ 'ID' ] except TypeError : try : old_id = self . lines [ old_parent ] [ 'attributes' ] [ 'ID' ] except TypeError : old_id = old_parent old_feature = self . features [ old_id ] old_indexes = [ ld [ 'line_index' ] for ld in old_feature ] try : new_id = new_parent [ 'attributes' ] [ 'ID' ] except TypeError : try : new_id = self . lines [ new_parent ] [ 'attributes' ] [ 'ID' ] except TypeError : new_id = new_parent new_feature = self . features [ new_id ] new_indexes = [ ld [ 'line_index' ] for ld in new_feature ] children = old_feature [ 0 ] [ 'children' ] new_parent_children_set = set ( [ ld [ 'line_index' ] for ld in new_feature [ 0 ] [ 'children' ] ] ) for child in children : if child [ 'line_index' ] not in new_parent_children_set : new_parent_children_set . add ( child [ 'line_index' ] ) for new_ld in new_feature : new_ld [ 'children' ] . append ( child ) child [ 'parents' ] . append ( new_feature ) child [ 'attributes' ] [ 'Parent' ] . append ( new_id ) child [ 'parents' ] = [ f for f in child [ 'parents' ] if f [ 0 ] [ 'attributes' ] [ 'ID' ] != old_id ] child [ 'attributes' ] [ 'Parent' ] = [ d for d in child [ 'attributes' ] [ 'Parent' ] if d != old_id ] for old_ld in old_feature : old_ld [ 'children' ] = [ ] return children
3178	def get ( self , list_id , merge_id ) : self . list_id = list_id self . merge_id = merge_id return self . _mc_client . _get ( url = self . _build_path ( list_id , 'merge-fields' , merge_id ) )
13159	def update ( cls , cur , table : str , values : dict , where_keys : list ) -> tuple : keys = cls . _COMMA . join ( values . keys ( ) ) value_place_holder = cls . _PLACEHOLDER * len ( values ) where_clause , where_values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _update_string . format ( table , keys , value_place_holder [ : - 1 ] , where_clause ) yield from cur . execute ( query , ( tuple ( values . values ( ) ) + where_values ) ) return ( yield from cur . fetchall ( ) )
1543	def get_clusters ( ) : instance = tornado . ioloop . IOLoop . instance ( ) try : return instance . run_sync ( lambda : API . get_clusters ( ) ) except Exception : Log . debug ( traceback . format_exc ( ) ) raise
2158	def _echo_method ( self , method ) : @ functools . wraps ( method ) def func ( * args , ** kwargs ) : if getattr ( method , 'deprecated' , False ) : debug . log ( 'This method is deprecated in Tower 3.0.' , header = 'warning' ) result = method ( * args , ** kwargs ) color_info = { } if isinstance ( result , dict ) and 'changed' in result : if result [ 'changed' ] : color_info [ 'fg' ] = 'yellow' else : color_info [ 'fg' ] = 'green' format = getattr ( self , '_format_%s' % ( getattr ( method , 'format_freezer' , None ) or settings . format ) ) output = format ( result ) secho ( output , ** color_info ) return func
6931	def xmatch_cpdir_external_catalogs ( cpdir , xmatchpkl , cpfileglob = 'checkplot-*.pkl*' , xmatchradiusarcsec = 2.0 , updateexisting = True , resultstodir = None ) : cplist = glob . glob ( os . path . join ( cpdir , cpfileglob ) ) return xmatch_cplist_external_catalogs ( cplist , xmatchpkl , xmatchradiusarcsec = xmatchradiusarcsec , updateexisting = updateexisting , resultstodir = resultstodir )
10905	def trisect_image ( imshape , edgepts = 'calc' ) : im_x , im_y = np . meshgrid ( np . arange ( imshape [ 0 ] ) , np . arange ( imshape [ 1 ] ) , indexing = 'ij' ) if np . size ( edgepts ) == 1 : f = np . sqrt ( 2. / 3. ) if edgepts == 'calc' else edgepts lower_edge = ( imshape [ 0 ] * ( 1 - f ) , imshape [ 1 ] * f ) upper_edge = ( imshape [ 0 ] * f , imshape [ 1 ] * ( 1 - f ) ) else : upper_edge , lower_edge = edgepts lower_slope = lower_edge [ 1 ] / max ( float ( imshape [ 0 ] - lower_edge [ 0 ] ) , 1e-9 ) upper_slope = ( imshape [ 1 ] - upper_edge [ 1 ] ) / float ( upper_edge [ 0 ] ) lower_intercept = - lower_slope * lower_edge [ 0 ] upper_intercept = upper_edge [ 1 ] lower_mask = im_y < ( im_x * lower_slope + lower_intercept ) upper_mask = im_y > ( im_x * upper_slope + upper_intercept ) center_mask = - ( lower_mask | upper_mask ) return upper_mask , center_mask , lower_mask
3500	def assess_component ( model , reaction , side , flux_coefficient_cutoff = 0.001 , solver = None ) : reaction = model . reactions . get_by_any ( reaction ) [ 0 ] result_key = dict ( reactants = 'produced' , products = 'capacity' ) [ side ] get_components = attrgetter ( side ) with model as m : m . objective = reaction if _optimize_or_value ( m , solver = solver ) >= flux_coefficient_cutoff : return True simulation_results = { } demand_reactions = { } for component in get_components ( reaction ) : coeff = reaction . metabolites [ component ] demand = m . add_boundary ( component , type = 'demand' ) demand . metabolites [ component ] = coeff demand_reactions [ demand ] = ( component , coeff ) joint_demand = Reaction ( "joint_demand" ) for demand_reaction in demand_reactions : joint_demand += demand_reaction m . add_reactions ( [ joint_demand ] ) m . objective = joint_demand if _optimize_or_value ( m , solver = solver ) >= flux_coefficient_cutoff : return True for demand_reaction , ( component , coeff ) in iteritems ( demand_reactions ) : with m : m . objective = demand_reaction flux = _optimize_or_value ( m , solver = solver ) if flux_coefficient_cutoff > flux : simulation_results . update ( { component : { 'required' : flux_coefficient_cutoff / abs ( coeff ) , result_key : flux / abs ( coeff ) } } ) if len ( simulation_results ) == 0 : simulation_results = False return simulation_results
6082	def deflections_of_galaxies_from_grid ( grid , galaxies ) : if len ( galaxies ) > 0 : deflections = sum ( map ( lambda galaxy : galaxy . deflections_from_grid ( grid ) , galaxies ) ) else : deflections = np . full ( ( grid . shape [ 0 ] , 2 ) , 0.0 ) if isinstance ( grid , grids . SubGrid ) : return np . asarray ( [ grid . regular_data_1d_from_sub_data_1d ( deflections [ : , 0 ] ) , grid . regular_data_1d_from_sub_data_1d ( deflections [ : , 1 ] ) ] ) . T return deflections
9370	def person_inn ( ) : mask11 = [ 7 , 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] mask12 = [ 3 , 7 , 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] inn = [ random . randint ( 1 , 9 ) for _ in range ( 12 ) ] weighted11 = [ v * mask11 [ i ] for i , v in enumerate ( inn [ : - 2 ] ) ] inn [ 10 ] = sum ( weighted11 ) % 11 % 10 weighted12 = [ v * mask12 [ i ] for i , v in enumerate ( inn [ : - 1 ] ) ] inn [ 11 ] = sum ( weighted12 ) % 11 % 10 return "" . join ( map ( str , inn ) )
12963	def getPrimaryKeys ( self , sortByAge = False ) : conn = self . _get_connection ( ) numFilters = len ( self . filters ) numNotFilters = len ( self . notFilters ) if numFilters + numNotFilters == 0 : conn = self . _get_connection ( ) matchedKeys = conn . smembers ( self . _get_ids_key ( ) ) elif numNotFilters == 0 : if numFilters == 1 : ( filterFieldName , filterValue ) = self . filters [ 0 ] matchedKeys = conn . smembers ( self . _get_key_for_index ( filterFieldName , filterValue ) ) else : indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] matchedKeys = conn . sinter ( indexKeys ) else : notIndexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . notFilters ] if numFilters == 0 : matchedKeys = conn . sdiff ( self . _get_ids_key ( ) , * notIndexKeys ) else : indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] tempKey = self . _getTempKey ( ) pipeline = conn . pipeline ( ) pipeline . sinterstore ( tempKey , * indexKeys ) pipeline . sdiff ( tempKey , * notIndexKeys ) pipeline . delete ( tempKey ) matchedKeys = pipeline . execute ( ) [ 1 ] matchedKeys = [ int ( _key ) for _key in matchedKeys ] if sortByAge is False : return list ( matchedKeys ) else : matchedKeys = list ( matchedKeys ) matchedKeys . sort ( ) return matchedKeys
8953	def load ( ) : cfg = Bunch ( DEFAULTS ) cfg . project_root = get_project_root ( ) if not cfg . project_root : raise RuntimeError ( "No tasks module is imported, cannot determine project root" ) cfg . rootjoin = lambda * names : os . path . join ( cfg . project_root , * names ) cfg . srcjoin = lambda * names : cfg . rootjoin ( cfg . srcdir , * names ) cfg . testjoin = lambda * names : cfg . rootjoin ( cfg . testdir , * names ) cfg . cwd = os . getcwd ( ) os . chdir ( cfg . project_root ) if cfg . project_root not in sys . path : sys . path . append ( cfg . project_root ) try : from setup import project except ImportError : from setup import setup_args as project cfg . project = Bunch ( project ) return cfg
10523	def getallitem ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) object_handle . Press ( ) self . wait ( 1 ) child = None try : if not object_handle . AXChildren : raise LdtpServerException ( u"Unable to find menu" ) children = object_handle . AXChildren [ 0 ] if not children : raise LdtpServerException ( u"Unable to find menu" ) children = children . AXChildren items = [ ] for child in children : label = self . _get_title ( child ) if label : items . append ( label ) finally : if child : child . Cancel ( ) return items
12606	def search_unique ( table , sample , unique_fields = None ) : if unique_fields is None : unique_fields = list ( sample . keys ( ) ) query = _query_data ( sample , field_names = unique_fields , operators = '__eq__' ) items = table . search ( query ) if len ( items ) == 1 : return items [ 0 ] if len ( items ) == 0 : return None raise MoreThanOneItemError ( 'Expected to find zero or one items, but found ' '{} items.' . format ( len ( items ) ) )
5706	def clean ( self ) : cleaned_data = super ( AuthForm , self ) . clean ( ) user = self . get_user ( ) if self . staff_only and ( not user or not user . is_staff ) : raise forms . ValidationError ( 'Sorry, only staff are allowed.' ) if self . superusers_only and ( not user or not user . is_superuser ) : raise forms . ValidationError ( 'Sorry, only superusers are allowed.' ) return cleaned_data
6699	def get_selections ( ) : with settings ( hide ( 'stdout' ) ) : res = run_as_root ( 'dpkg --get-selections' ) selections = dict ( ) for line in res . splitlines ( ) : package , status = line . split ( ) selections . setdefault ( status , list ( ) ) . append ( package ) return selections
7353	def NetMHC ( alleles , default_peptide_lengths = [ 9 ] , program_name = "netMHC" ) : with open ( os . devnull , 'w' ) as devnull : help_output = check_output ( [ program_name , "-h" ] , stderr = devnull ) help_output_str = help_output . decode ( "ascii" , "ignore" ) substring_to_netmhc_class = { "-listMHC" : NetMHC4 , "--Alleles" : NetMHC3 , } successes = [ ] for substring , netmhc_class in substring_to_netmhc_class . items ( ) : if substring in help_output_str : successes . append ( netmhc_class ) if len ( successes ) > 1 : raise SystemError ( "Command %s is valid for multiple NetMHC versions. " "This is likely an mhctools bug." % program_name ) if len ( successes ) == 0 : raise SystemError ( "Command %s is not a valid way of calling any NetMHC software." % program_name ) netmhc_class = successes [ 0 ] return netmhc_class ( alleles = alleles , default_peptide_lengths = default_peptide_lengths , program_name = program_name )
9533	def unsign ( self , signed_value , ttl = None ) : h_size , d_size = struct . calcsize ( '>cQ' ) , self . digest . digest_size fmt = '>cQ%ds%ds' % ( len ( signed_value ) - h_size - d_size , d_size ) try : version , timestamp , value , sig = struct . unpack ( fmt , signed_value ) except struct . error : raise BadSignature ( 'Signature is not valid' ) if version != self . version : raise BadSignature ( 'Signature version not supported' ) if ttl is not None : if isinstance ( ttl , datetime . timedelta ) : ttl = ttl . total_seconds ( ) age = abs ( time . time ( ) - timestamp ) if age > ttl + _MAX_CLOCK_SKEW : raise SignatureExpired ( 'Signature age %s > %s seconds' % ( age , ttl ) ) try : self . signature ( signed_value [ : - d_size ] ) . verify ( sig ) except InvalidSignature : raise BadSignature ( 'Signature "%s" does not match' % binascii . b2a_base64 ( sig ) ) return value
808	def handleLogOutput ( self , output ) : if self . _tapFileOut is not None : for k in range ( len ( output ) ) : print >> self . _tapFileOut , output [ k ] , print >> self . _tapFileOut
12768	def load_skeleton ( self , filename , pid_params = None ) : self . skeleton = skeleton . Skeleton ( self ) self . skeleton . load ( filename , color = ( 0.3 , 0.5 , 0.9 , 0.8 ) ) if pid_params : self . skeleton . set_pid_params ( ** pid_params ) self . skeleton . erp = 0.1 self . skeleton . cfm = 0
11121	def get_file_relative_path_by_name ( self , name , skip = 0 ) : if skip is None : paths = [ ] else : paths = None for path , info in self . walk_files_info ( ) : _ , n = os . path . split ( path ) if n == name : if skip is None : paths . append ( path ) elif skip > 0 : skip -= 1 else : paths = path break return paths
13687	def assert_equal_files ( self , obtained_fn , expected_fn , fix_callback = lambda x : x , binary = False , encoding = None ) : import os from zerotk . easyfs import GetFileContents , GetFileLines __tracebackhide__ = True import io def FindFile ( filename ) : data_filename = self . get_filename ( filename ) if os . path . isfile ( data_filename ) : return data_filename if os . path . isfile ( filename ) : return filename from . _exceptions import MultipleFilesNotFound raise MultipleFilesNotFound ( [ filename , data_filename ] ) obtained_fn = FindFile ( obtained_fn ) expected_fn = FindFile ( expected_fn ) if binary : obtained_lines = GetFileContents ( obtained_fn , binary = True ) expected_lines = GetFileContents ( expected_fn , binary = True ) assert obtained_lines == expected_lines else : obtained_lines = fix_callback ( GetFileLines ( obtained_fn , encoding = encoding ) ) expected_lines = GetFileLines ( expected_fn , encoding = encoding ) if obtained_lines != expected_lines : html_fn = os . path . splitext ( obtained_fn ) [ 0 ] + '.diff.html' html_diff = self . _generate_html_diff ( expected_fn , expected_lines , obtained_fn , obtained_lines ) with io . open ( html_fn , 'w' ) as f : f . write ( html_diff ) import difflib diff = [ 'FILES DIFFER:' , obtained_fn , expected_fn ] diff += [ 'HTML DIFF: %s' % html_fn ] diff += difflib . context_diff ( obtained_lines , expected_lines ) raise AssertionError ( '\n' . join ( diff ) + '\n' )
2795	def create ( self ) : params = { 'name' : self . name , 'region' : self . region , 'url' : self . url , 'distribution' : self . distribution , 'description' : self . description , 'tags' : self . tags } data = self . get_data ( 'images' , type = POST , params = params ) if data : for attr in data [ 'image' ] . keys ( ) : setattr ( self , attr , data [ 'image' ] [ attr ] ) return self
5738	def enqueue ( self , f , * args , ** kwargs ) : task = Task ( uuid4 ( ) . hex , f , args , kwargs ) self . storage . put_task ( task ) return self . enqueue_task ( task )
10182	def _aggregations_process ( aggregation_types = None , start_date = None , end_date = None , update_bookmark = False , eager = False ) : aggregation_types = ( aggregation_types or list ( current_stats . enabled_aggregations ) ) if eager : aggregate_events . apply ( ( aggregation_types , ) , dict ( start_date = start_date , end_date = end_date , update_bookmark = update_bookmark ) , throw = True ) click . secho ( 'Aggregations processed successfully.' , fg = 'green' ) else : aggregate_events . delay ( aggregation_types , start_date = start_date , end_date = end_date ) click . secho ( 'Aggregations processing task sent...' , fg = 'yellow' )
4752	def extract_hook_names ( ent ) : hnames = [ ] for hook in ent [ "hooks" ] [ "enter" ] + ent [ "hooks" ] [ "exit" ] : hname = os . path . basename ( hook [ "fpath_orig" ] ) hname = os . path . splitext ( hname ) [ 0 ] hname = hname . strip ( ) hname = hname . replace ( "_enter" , "" ) hname = hname . replace ( "_exit" , "" ) if hname in hnames : continue hnames . append ( hname ) hnames . sort ( ) return hnames
12173	def htmlABF ( ID , group , d , folder , overwrite = False ) : fname = folder + "/swhlab4/%s_index.html" % ID if overwrite is False and os . path . exists ( fname ) : return html = TEMPLATES [ 'abf' ] html = html . replace ( "~ID~" , ID ) html = html . replace ( "~CONTENT~" , htmlABFcontent ( ID , group , d ) ) print ( " <- writing [%s]" % os . path . basename ( fname ) ) with open ( fname , 'w' ) as f : f . write ( html ) return
537	def run ( self ) : descriptionPyModule = helpers . loadExperimentDescriptionScriptFromDir ( self . _experimentDir ) expIface = helpers . getExperimentDescriptionInterfaceFromModule ( descriptionPyModule ) expIface . normalizeStreamSources ( ) modelDescription = expIface . getModelDescription ( ) self . _modelControl = expIface . getModelControl ( ) streamDef = self . _modelControl [ 'dataset' ] from nupic . data . stream_reader import StreamReader readTimeout = 0 self . _inputSource = StreamReader ( streamDef , isBlocking = False , maxTimeout = readTimeout ) fieldStats = self . _getFieldStats ( ) self . _model = ModelFactory . create ( modelDescription ) self . _model . setFieldStatistics ( fieldStats ) self . _model . enableLearning ( ) self . _model . enableInference ( self . _modelControl . get ( "inferenceArgs" , None ) ) self . __metricMgr = MetricsManager ( self . _modelControl . get ( 'metrics' , None ) , self . _model . getFieldInfo ( ) , self . _model . getInferenceType ( ) ) self . __loggedMetricPatterns = self . _modelControl . get ( "loggedMetrics" , [ ] ) self . _optimizedMetricLabel = self . __getOptimizedMetricLabel ( ) self . _reportMetricLabels = matchPatterns ( self . _reportKeyPatterns , self . _getMetricLabels ( ) ) self . _periodic = self . _initPeriodicActivities ( ) numIters = self . _modelControl . get ( 'iterationCount' , - 1 ) learningOffAt = None iterationCountInferOnly = self . _modelControl . get ( 'iterationCountInferOnly' , 0 ) if iterationCountInferOnly == - 1 : self . _model . disableLearning ( ) elif iterationCountInferOnly > 0 : assert numIters > iterationCountInferOnly , "when iterationCountInferOnly " "is specified, iterationCount must be greater than " "iterationCountInferOnly." learningOffAt = numIters - iterationCountInferOnly self . __runTaskMainLoop ( numIters , learningOffAt = learningOffAt ) self . _finalize ( ) return ( self . _cmpReason , None )
431	def save_image ( image , image_path = '_temp.png' ) : try : imageio . imwrite ( image_path , image ) except Exception : imageio . imwrite ( image_path , image [ : , : , 0 ] )
5129	def size ( self , s ) : leader = self . find ( s ) return self . _size [ leader ]
8848	def mousePressEvent ( self , e ) : super ( PyInteractiveConsole , self ) . mousePressEvent ( e ) cursor = self . cursorForPosition ( e . pos ( ) ) p = cursor . positionInBlock ( ) usd = cursor . block ( ) . userData ( ) if usd and usd . start_pos_in_block <= p <= usd . end_pos_in_block : if e . button ( ) == QtCore . Qt . LeftButton : self . open_file_requested . emit ( usd . filename , usd . line )
12175	def plotAllSweeps ( abfFile ) : r = io . AxonIO ( filename = abfFile ) bl = r . read_block ( lazy = False , cascade = True ) print ( abfFile + "\nplotting %d sweeps..." % len ( bl . segments ) ) plt . figure ( figsize = ( 12 , 10 ) ) plt . title ( abfFile ) for sweep in range ( len ( bl . segments ) ) : trace = bl . segments [ sweep ] . analogsignals [ 0 ] plt . plot ( trace . times - trace . times [ 0 ] , trace . magnitude , alpha = .5 ) plt . ylabel ( trace . dimensionality ) plt . xlabel ( "seconds" ) plt . show ( ) plt . close ( )
13045	def f_i18n_iso ( isocode , lang = "eng" ) : if lang not in flask_nemo . _data . AVAILABLE_TRANSLATIONS : lang = "eng" try : return flask_nemo . _data . ISOCODES [ isocode ] [ lang ] except KeyError : return "Unknown"
2710	def make_sentence ( sent_text ) : lex = [ ] idx = 0 for word in sent_text : if len ( word ) > 0 : if ( idx > 0 ) and not ( word [ 0 ] in ",.:;!?-\"'" ) : lex . append ( " " ) lex . append ( word ) idx += 1 return "" . join ( lex )
13209	def _prep_snippet_for_pandoc ( self , latex_text ) : replace_cite = CitationLinker ( self . bib_db ) latex_text = replace_cite ( latex_text ) return latex_text
11785	def attrnum ( self , attr ) : "Returns the number used for attr, which can be a name, or -n .. n-1." if attr < 0 : return len ( self . attrs ) + attr elif isinstance ( attr , str ) : return self . attrnames . index ( attr ) else : return attr
12253	def _delete_key_internal ( self , * args , ** kwargs ) : mimicdb . backend . srem ( tpl . bucket % self . name , args [ 0 ] ) mimicdb . backend . delete ( tpl . key % ( self . name , args [ 0 ] ) ) return super ( Bucket , self ) . _delete_key_internal ( * args , ** kwargs )
12116	def loadResults ( resultsFile ) : with open ( resultsFile ) as f : raw = f . read ( ) . split ( "\n" ) foldersByDay = { } for line in raw : folder = line . split ( '"' ) [ 1 ] + "\\" line = [ ] + line . split ( '"' ) [ 2 ] . split ( ", " ) for day in line [ 1 : ] : if not day in foldersByDay : foldersByDay [ day ] = [ ] foldersByDay [ day ] = foldersByDay [ day ] + [ folder ] nActiveDays = len ( foldersByDay ) dayFirst = sorted ( foldersByDay . keys ( ) ) [ 0 ] dayLast = sorted ( foldersByDay . keys ( ) ) [ - 1 ] dayFirst = datetime . datetime . strptime ( dayFirst , "%Y-%m-%d" ) dayLast = datetime . datetime . strptime ( dayLast , "%Y-%m-%d" ) nDays = ( dayLast - dayFirst ) . days + 1 emptyDays = 0 for deltaDays in range ( nDays ) : day = dayFirst + datetime . timedelta ( days = deltaDays ) stamp = datetime . datetime . strftime ( day , "%Y-%m-%d" ) if not stamp in foldersByDay : foldersByDay [ stamp ] = [ ] emptyDays += 1 percActive = nActiveDays / nDays * 100 print ( "%d of %d days were active (%.02f%%)" % ( nActiveDays , nDays , percActive ) ) return foldersByDay
9577	def read_var_header ( fd , endian ) : mtpn , num_bytes = unpack ( endian , 'II' , fd . read ( 8 ) ) next_pos = fd . tell ( ) + num_bytes if mtpn == etypes [ 'miCOMPRESSED' ] [ 'n' ] : data = fd . read ( num_bytes ) dcor = zlib . decompressobj ( ) fd_var = BytesIO ( dcor . decompress ( data ) ) del data fd = fd_var if dcor . flush ( ) != b'' : raise ParseError ( 'Error in compressed data.' ) mtpn , num_bytes = unpack ( endian , 'II' , fd . read ( 8 ) ) if mtpn != etypes [ 'miMATRIX' ] [ 'n' ] : raise ParseError ( 'Expecting miMATRIX type number {}, ' 'got {}' . format ( etypes [ 'miMATRIX' ] [ 'n' ] , mtpn ) ) header = read_header ( fd , endian ) return header , next_pos , fd
4219	def get_preferred_collection ( self ) : bus = secretstorage . dbus_init ( ) try : if hasattr ( self , 'preferred_collection' ) : collection = secretstorage . Collection ( bus , self . preferred_collection ) else : collection = secretstorage . get_default_collection ( bus ) except exceptions . SecretStorageException as e : raise InitError ( "Failed to create the collection: %s." % e ) if collection . is_locked ( ) : collection . unlock ( ) if collection . is_locked ( ) : raise KeyringLocked ( "Failed to unlock the collection!" ) return collection
5605	def write_raster_window ( in_tile = None , in_data = None , out_profile = None , out_tile = None , out_path = None , tags = None , bucket_resource = None ) : if not isinstance ( out_path , str ) : raise TypeError ( "out_path must be a string" ) logger . debug ( "write %s" , out_path ) if out_path == "memoryfile" : raise DeprecationWarning ( "Writing to memoryfile with write_raster_window() is deprecated. " "Please use RasterWindowMemoryFile." ) out_tile = in_tile if out_tile is None else out_tile _validate_write_window_params ( in_tile , out_tile , in_data , out_profile ) window_data = extract_from_array ( in_raster = in_data , in_affine = in_tile . affine , out_tile = out_tile ) if in_tile != out_tile else in_data if "affine" in out_profile : out_profile [ "transform" ] = out_profile . pop ( "affine" ) if window_data . all ( ) is not ma . masked : try : if out_path . startswith ( "s3://" ) : with RasterWindowMemoryFile ( in_tile = out_tile , in_data = window_data , out_profile = out_profile , out_tile = out_tile , tags = tags ) as memfile : logger . debug ( ( out_tile . id , "upload tile" , out_path ) ) bucket_resource . put_object ( Key = "/" . join ( out_path . split ( "/" ) [ 3 : ] ) , Body = memfile ) else : with rasterio . open ( out_path , 'w' , ** out_profile ) as dst : logger . debug ( ( out_tile . id , "write tile" , out_path ) ) dst . write ( window_data . astype ( out_profile [ "dtype" ] , copy = False ) ) _write_tags ( dst , tags ) except Exception as e : logger . exception ( "error while writing file %s: %s" , out_path , e ) raise else : logger . debug ( ( out_tile . id , "array window empty" , out_path ) )
1221	def process ( self , tensor ) : for processor in self . preprocessors : tensor = processor . process ( tensor = tensor ) return tensor
7240	def window_cover ( self , window_shape , pad = True ) : size_y , size_x = window_shape [ 0 ] , window_shape [ 1 ] _ndepth , _nheight , _nwidth = self . shape nheight , _m = divmod ( _nheight , size_y ) nwidth , _n = divmod ( _nwidth , size_x ) img = self if pad is True : new_height , new_width = _nheight , _nwidth if _m != 0 : new_height = ( nheight + 1 ) * size_y if _n != 0 : new_width = ( nwidth + 1 ) * size_x if ( new_height , new_width ) != ( _nheight , _nwidth ) : bounds = box ( 0 , 0 , new_width , new_height ) geom = ops . transform ( self . __geo_transform__ . fwd , bounds ) img = self [ geom ] row_lims = range ( 0 , img . shape [ 1 ] , size_y ) col_lims = range ( 0 , img . shape [ 2 ] , size_x ) for maxy , maxx in product ( row_lims , col_lims ) : reg = img [ : , maxy : ( maxy + size_y ) , maxx : ( maxx + size_x ) ] if pad is False : if reg . shape [ 1 : ] == window_shape : yield reg else : yield reg
1264	def sanity_check_states ( states_spec ) : states = copy . deepcopy ( states_spec ) is_unique = ( 'shape' in states ) if is_unique : states = dict ( state = states ) for name , state in states . items ( ) : if isinstance ( state [ 'shape' ] , int ) : state [ 'shape' ] = ( state [ 'shape' ] , ) if 'type' not in state : state [ 'type' ] = 'float' return states , is_unique
4421	async def set_pause ( self , pause : bool ) : await self . _lavalink . ws . send ( op = 'pause' , guildId = self . guild_id , pause = pause ) self . paused = pause
2116	def convert ( self , value , param , ctx ) : if not isinstance ( value , str ) : return value if isinstance ( value , six . binary_type ) : value = value . decode ( 'UTF-8' ) if value . startswith ( '@' ) : filename = os . path . expanduser ( value [ 1 : ] ) file_obj = super ( Variables , self ) . convert ( filename , param , ctx ) if hasattr ( file_obj , 'read' ) : return file_obj . read ( ) return file_obj return value
2664	def status ( self ) : status = [ ] if self . provider : status = self . provider . status ( self . blocks . values ( ) ) return status
13266	def xml_to_json ( root ) : j = { } if len ( root ) == 0 : return _maybe_intify ( root . text ) if len ( root ) == 1 and root [ 0 ] . tag . startswith ( '{' + NS_GML ) : return gml_to_geojson ( root [ 0 ] ) if root . tag == 'open511' : j [ 'meta' ] = { 'version' : root . get ( 'version' ) } for elem in root : name = elem . tag if name == 'link' and elem . get ( 'rel' ) : name = elem . get ( 'rel' ) + '_url' if name == 'self_url' : name = 'url' if root . tag == 'open511' : j [ 'meta' ] [ name ] = elem . get ( 'href' ) continue elif name . startswith ( '{' + NS_PROTECTED ) : name = '!' + name [ name . index ( '}' ) + 1 : ] elif name [ 0 ] == '{' : name = '+' + name [ name . index ( '}' ) + 1 : ] if name in j : continue elif elem . tag == 'link' and not elem . text : j [ name ] = elem . get ( 'href' ) elif len ( elem ) : if name == 'grouped_events' : j [ name ] = [ xml_link_to_json ( child , to_dict = False ) for child in elem ] elif name in ( 'attachments' , 'media_files' ) : j [ name ] = [ xml_link_to_json ( child , to_dict = True ) for child in elem ] elif all ( ( name == pluralize ( child . tag ) for child in elem ) ) : j [ name ] = [ xml_to_json ( child ) for child in elem ] else : j [ name ] = xml_to_json ( elem ) else : if root . tag == 'open511' and name . endswith ( 's' ) and not elem . text : j [ name ] = [ ] else : j [ name ] = _maybe_intify ( elem . text ) return j
6790	def createsuperuser ( self , username = 'admin' , email = None , password = None , site = None ) : r = self . local_renderer site = site or self . genv . SITE self . set_site_specifics ( site ) options = [ '--username=%s' % username ] if email : options . append ( '--email=%s' % email ) if password : options . append ( '--password=%s' % password ) r . env . options_str = ' ' . join ( options ) if self . is_local : r . env . project_dir = r . env . local_project_dir r . genv . SITE = r . genv . SITE or site r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project_dir}; {manage_cmd} {createsuperuser_cmd} {options_str}' )
1802	def LEA ( cpu , dest , src ) : dest . write ( Operators . EXTRACT ( src . address ( ) , 0 , dest . size ) )
1177	def search ( self , string , pos = 0 , endpos = sys . maxint ) : state = _State ( string , pos , endpos , self . flags ) if state . search ( self . _code ) : return SRE_Match ( self , state ) else : return None
8250	def blend ( self , clr , factor = 0.5 ) : r = self . r * ( 1 - factor ) + clr . r * factor g = self . g * ( 1 - factor ) + clr . g * factor b = self . b * ( 1 - factor ) + clr . b * factor a = self . a * ( 1 - factor ) + clr . a * factor return Color ( r , g , b , a , mode = "rgb" )
2461	def set_file_name ( self , doc , name ) : if self . has_package ( doc ) : doc . package . files . append ( file . File ( name ) ) self . reset_file_stat ( ) return True else : raise OrderError ( 'File::Name' )
4494	def upload ( args ) : osf = _setup_osf ( args ) if osf . username is None or osf . password is None : sys . exit ( 'To upload a file you need to provide a username and' ' password.' ) project = osf . project ( args . project ) storage , remote_path = split_storage ( args . destination ) store = project . storage ( storage ) if args . recursive : if not os . path . isdir ( args . source ) : raise RuntimeError ( "Expected source ({}) to be a directory when " "using recursive mode." . format ( args . source ) ) _ , dir_name = os . path . split ( args . source ) for root , _ , files in os . walk ( args . source ) : subdir_path = os . path . relpath ( root , args . source ) for fname in files : local_path = os . path . join ( root , fname ) with open ( local_path , 'rb' ) as fp : name = os . path . join ( remote_path , dir_name , subdir_path , fname ) store . create_file ( name , fp , force = args . force , update = args . update ) else : with open ( args . source , 'rb' ) as fp : store . create_file ( remote_path , fp , force = args . force , update = args . update )
1474	def _get_tmaster_processes ( self ) : retval = { } tmaster_cmd_lst = [ self . tmaster_binary , '--topology_name=%s' % self . topology_name , '--topology_id=%s' % self . topology_id , '--zkhostportlist=%s' % self . state_manager_connection , '--zkroot=%s' % self . state_manager_root , '--myhost=%s' % self . master_host , '--master_port=%s' % str ( self . master_port ) , '--controller_port=%s' % str ( self . tmaster_controller_port ) , '--stats_port=%s' % str ( self . tmaster_stats_port ) , '--config_file=%s' % self . heron_internals_config_file , '--override_config_file=%s' % self . override_config_file , '--metrics_sinks_yaml=%s' % self . metrics_sinks_config_file , '--metricsmgr_port=%s' % str ( self . metrics_manager_port ) , '--ckptmgr_port=%s' % str ( self . checkpoint_manager_port ) ] tmaster_env = self . shell_env . copy ( ) if self . shell_env is not None else { } tmaster_cmd = Command ( tmaster_cmd_lst , tmaster_env ) if os . environ . get ( 'ENABLE_HEAPCHECK' ) is not None : tmaster_cmd . env . update ( { 'LD_PRELOAD' : "/usr/lib/libtcmalloc.so" , 'HEAPCHECK' : "normal" } ) retval [ "heron-tmaster" ] = tmaster_cmd if self . metricscache_manager_mode . lower ( ) != "disabled" : retval [ "heron-metricscache" ] = self . _get_metrics_cache_cmd ( ) if self . health_manager_mode . lower ( ) != "disabled" : retval [ "heron-healthmgr" ] = self . _get_healthmgr_cmd ( ) retval [ self . metricsmgr_ids [ 0 ] ] = self . _get_metricsmgr_cmd ( self . metricsmgr_ids [ 0 ] , self . metrics_sinks_config_file , self . metrics_manager_port ) if self . is_stateful_topology : retval . update ( self . _get_ckptmgr_process ( ) ) return retval
781	def _startJobWithRetries ( self , jobID ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET status=%%s, ' ' _eng_cjm_conn_id=%%s, ' ' start_time=UTC_TIMESTAMP(), ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE (job_id=%%s AND status=%%s)' % ( self . jobsTableName , ) sqlParams = [ self . STATUS_RUNNING , self . _connectionID , jobID , self . STATUS_NOTSTARTED ] numRowsUpdated = conn . cursor . execute ( query , sqlParams ) if numRowsUpdated != 1 : self . _logger . warn ( 'jobStartNext: numRowsUpdated=%r instead of 1; ' 'likely side-effect of transient connection ' 'failure' , numRowsUpdated ) return
8136	def duplicate ( self ) : i = self . canvas . layer ( self . img . copy ( ) , self . x , self . y , self . name ) clone = self . canvas . layers [ i ] clone . alpha = self . alpha clone . blend = self . blend
7683	def display_multi ( annotations , fig_kw = None , meta = True , ** kwargs ) : if fig_kw is None : fig_kw = dict ( ) fig_kw . setdefault ( 'sharex' , True ) fig_kw . setdefault ( 'squeeze' , True ) display_annotations = [ ] for ann in annotations : for namespace in VIZ_MAPPING : if can_convert ( ann , namespace ) : display_annotations . append ( ann ) break if not len ( display_annotations ) : raise ParameterError ( 'No displayable annotations found' ) fig , axs = plt . subplots ( nrows = len ( display_annotations ) , ncols = 1 , ** fig_kw ) if len ( display_annotations ) == 1 : axs = [ axs ] for ann , ax in zip ( display_annotations , axs ) : kwargs [ 'ax' ] = ax display ( ann , meta = meta , ** kwargs ) return fig , axs
7408	def worker ( self ) : fullseqs = self . sample_loci ( ) liters = itertools . product ( * self . imap . values ( ) ) hashval = uuid . uuid4 ( ) . hex weights = [ ] for ridx , lidx in enumerate ( liters ) : a , b , c , d = lidx sub = { } for i in lidx : if self . rmap [ i ] == "p1" : sub [ "A" ] = fullseqs [ i ] elif self . rmap [ i ] == "p2" : sub [ "B" ] = fullseqs [ i ] elif self . rmap [ i ] == "p3" : sub [ "C" ] = fullseqs [ i ] else : sub [ "D" ] = fullseqs [ i ] nex = [ ] for tax in list ( "ABCD" ) : nex . append ( ">{} {}" . format ( tax , sub [ tax ] ) ) nsites , nvar = count_var ( nex ) if nvar > self . minsnps : nexus = "{} {}\n" . format ( 4 , len ( fullseqs [ a ] ) ) + "\n" . join ( nex ) treeorder = self . run_tree_inference ( nexus , "{}.{}" . format ( hashval , ridx ) ) weights . append ( treeorder ) rfiles = glob . glob ( os . path . join ( tempfile . tempdir , "*{}*" . format ( hashval ) ) ) for rfile in rfiles : if os . path . exists ( rfile ) : os . remove ( rfile ) trees = [ "ABCD" , "ACBD" , "ADBC" ] wdict = { i : float ( weights . count ( i ) ) / len ( weights ) for i in trees } return wdict
2212	def touch ( fpath , mode = 0o666 , dir_fd = None , verbose = 0 , ** kwargs ) : if verbose : print ( 'Touching file {}' . format ( fpath ) ) if six . PY2 : with open ( fpath , 'a' ) : os . utime ( fpath , None ) else : flags = os . O_CREAT | os . O_APPEND with os . fdopen ( os . open ( fpath , flags = flags , mode = mode , dir_fd = dir_fd ) ) as f : os . utime ( f . fileno ( ) if os . utime in os . supports_fd else fpath , dir_fd = None if os . supports_fd else dir_fd , ** kwargs ) return fpath
5504	def relative_datetime ( self ) : now = datetime . now ( timezone . utc ) tense = "from now" if self . created_at > now else "ago" return "{0} {1}" . format ( humanize . naturaldelta ( now - self . created_at ) , tense )
11839	def set_board ( self , board = None ) : "Set the board, and find all the words in it." if board is None : board = random_boggle ( ) self . board = board self . neighbors = boggle_neighbors ( len ( board ) ) self . found = { } for i in range ( len ( board ) ) : lo , hi = self . wordlist . bounds [ board [ i ] ] self . find ( lo , hi , i , [ ] , '' ) return self
1107	def get_opcodes ( self ) : if self . opcodes is not None : return self . opcodes i = j = 0 self . opcodes = answer = [ ] for ai , bj , size in self . get_matching_blocks ( ) : tag = '' if i < ai and j < bj : tag = 'replace' elif i < ai : tag = 'delete' elif j < bj : tag = 'insert' if tag : answer . append ( ( tag , i , ai , j , bj ) ) i , j = ai + size , bj + size if size : answer . append ( ( 'equal' , ai , i , bj , j ) ) return answer
82	def SaltAndPepper ( p = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return ReplaceElementwise ( mask = p , replacement = iap . Beta ( 0.5 , 0.5 ) * 255 , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
4896	def get_enterprise_sso_uid ( self , obj ) : enterprise_learner = EnterpriseCustomerUser . objects . filter ( user_id = obj . id ) . first ( ) return enterprise_learner and enterprise_learner . get_remote_id ( )
2162	def update ( self , inventory_source , monitor = False , wait = False , timeout = None , ** kwargs ) : debug . log ( 'Asking whether the inventory source can be updated.' , header = 'details' ) r = client . get ( '%s%d/update/' % ( self . endpoint , inventory_source ) ) if not r . json ( ) [ 'can_update' ] : raise exc . BadRequest ( 'Tower says it cannot run an update against this inventory source.' ) debug . log ( 'Updating the inventory source.' , header = 'details' ) r = client . post ( '%s%d/update/' % ( self . endpoint , inventory_source ) , data = { } ) inventory_update_id = r . json ( ) [ 'inventory_update' ] if monitor or wait : if monitor : result = self . monitor ( inventory_update_id , parent_pk = inventory_source , timeout = timeout ) elif wait : result = self . wait ( inventory_update_id , parent_pk = inventory_source , timeout = timeout ) inventory = client . get ( '/inventory_sources/%d/' % result [ 'inventory_source' ] ) . json ( ) [ 'inventory' ] result [ 'inventory' ] = int ( inventory ) return result return { 'id' : inventory_update_id , 'status' : 'ok' }
10686	def Cp ( self , phase , T ) : if phase not in self . _phases : raise Exception ( "The phase '%s' was not found in compound '%s'." % ( phase , self . formula ) ) return self . _phases [ phase ] . Cp ( T )
12787	def check_file ( self , filename ) : if not exists ( filename ) : return False new_config = ConfigResolverBase ( ) new_config . read ( filename ) if self . version and not new_config . has_option ( 'meta' , 'version' ) : raise NoVersionError ( "The config option 'meta.version' is missing in {}. The " "application expects version {}!" . format ( filename , self . version ) ) elif not self . version and new_config . has_option ( 'meta' , 'version' ) : self . version = StrictVersion ( new_config . get ( 'meta' , 'version' ) ) self . _log . info ( '%r contains a version number, but the config ' 'instance was not created with a version ' 'restriction. Will set version number to "%s" to ' 'prevent accidents!' , filename , self . version ) elif self . version : file_version = new_config . get ( 'meta' , 'version' ) major , minor , _ = StrictVersion ( file_version ) . version expected_major , expected_minor , _ = self . version . version if expected_major != major : self . _log . error ( 'Invalid major version number in %r. Expected %r, got %r!' , abspath ( filename ) , str ( self . version ) , file_version ) return False if expected_minor != minor : self . _log . warning ( 'Mismatching minor version number in %r. ' 'Expected %r, got %r!' , abspath ( filename ) , str ( self . version ) , file_version ) return True return True
11918	def get_dataframe ( self ) : assert self . dataframe is not None , ( "'%s' should either include a `dataframe` attribute, " "or override the `get_dataframe()` method." % self . __class__ . __name__ ) dataframe = self . dataframe return dataframe
4985	def get ( self , request , enterprise_uuid , program_uuid ) : verify_edx_resources ( ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) context_data = get_global_context ( request , enterprise_customer ) program_details , error_code = self . get_program_details ( request , program_uuid , enterprise_customer ) if error_code : return render ( request , ENTERPRISE_GENERAL_ERROR_PAGE , context = context_data , status = 404 , ) if program_details [ 'certificate_eligible_for_program' ] : return redirect ( LMS_PROGRAMS_DASHBOARD_URL . format ( uuid = program_uuid ) ) course_run_ids = [ ] for course in program_details [ 'courses' ] : for course_run in course [ 'course_runs' ] : course_run_ids . append ( course_run [ 'key' ] ) embargo_url = EmbargoApiClient . redirect_if_blocked ( course_run_ids , request . user , get_ip ( request ) , request . path ) if embargo_url : return redirect ( embargo_url ) return self . get_enterprise_program_enrollment_page ( request , enterprise_customer , program_details )
7033	def get_new_apikey ( lcc_server ) : USERHOME = os . path . expanduser ( '~' ) APIKEYFILE = os . path . join ( USERHOME , '.astrobase' , 'lccs' , 'apikey-%s' % lcc_server . replace ( 'https://' , 'https-' ) . replace ( 'http://' , 'http-' ) ) url = '%s/api/key' % lcc_server resp = urlopen ( url ) if resp . code == 200 : respdict = json . loads ( resp . read ( ) ) else : LOGERROR ( 'could not fetch the API key from LCC-Server at: %s' % lcc_server ) LOGERROR ( 'the HTTP status code was: %s' % resp . status_code ) return None apikey = respdict [ 'result' ] [ 'apikey' ] expires = respdict [ 'result' ] [ 'expires' ] if not os . path . exists ( os . path . dirname ( APIKEYFILE ) ) : os . makedirs ( os . path . dirname ( APIKEYFILE ) ) with open ( APIKEYFILE , 'w' ) as outfd : outfd . write ( '%s %s\n' % ( apikey , expires ) ) os . chmod ( APIKEYFILE , 0o100600 ) LOGINFO ( 'key fetched successfully from: %s. expires on: %s' % ( lcc_server , expires ) ) LOGINFO ( 'written to: %s' % APIKEYFILE ) return apikey , expires
12836	def render_vars ( self ) : return { 'records' : [ { 'message' : record . getMessage ( ) , 'time' : dt . datetime . fromtimestamp ( record . created ) . strftime ( '%H:%M:%S' ) , } for record in self . handler . records ] }
9017	def _pattern ( self , base ) : rows = self . _rows ( base . get ( ROWS , [ ] ) ) self . _finish_inheritance ( ) self . _finish_instructions ( ) self . _connect_rows ( base . get ( CONNECTIONS , [ ] ) ) id_ = self . _to_id ( base [ ID ] ) name = base [ NAME ] return self . new_pattern ( id_ , name , rows )
1211	def run ( self ) : if not self . state . document . settings . file_insertion_enabled : raise self . warning ( '"%s" directive disabled.' % self . name ) source = self . state_machine . input_lines . source ( self . lineno - self . state_machine . input_offset - 1 ) source_dir = os . path . dirname ( os . path . abspath ( source ) ) path = rst . directives . path ( self . arguments [ 0 ] ) path = os . path . normpath ( os . path . join ( source_dir , path ) ) path = utils . relative_path ( None , path ) path = nodes . reprunicode ( path ) encoding = self . options . get ( 'encoding' , self . state . document . settings . input_encoding ) e_handler = self . state . document . settings . input_encoding_error_handler tab_width = self . options . get ( 'tab-width' , self . state . document . settings . tab_width ) try : self . state . document . settings . record_dependencies . add ( path ) include_file = io . FileInput ( source_path = path , encoding = encoding , error_handler = e_handler ) except UnicodeEncodeError as error : raise self . severe ( 'Problems with "%s" directive path:\n' 'Cannot encode input file path "%s" ' '(wrong locale?).' % ( self . name , SafeString ( path ) ) ) except IOError as error : raise self . severe ( 'Problems with "%s" directive path:\n%s.' % ( self . name , ErrorString ( error ) ) ) try : rawtext = include_file . read ( ) except UnicodeError as error : raise self . severe ( 'Problem with "%s" directive:\n%s' % ( self . name , ErrorString ( error ) ) ) config = self . state . document . settings . env . config converter = M2R ( no_underscore_emphasis = config . no_underscore_emphasis ) include_lines = statemachine . string2lines ( converter ( rawtext ) , tab_width , convert_whitespace = True ) self . state_machine . insert_input ( include_lines , path ) return [ ]
10705	def get_locations ( ) : arequest = requests . get ( LOCATIONS_URL , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
10651	def get_activity ( self , name ) : return [ a for a in self . activities if a . name == name ] [ 0 ]
351	def download_file_from_google_drive ( ID , destination ) : def save_response_content ( response , destination , chunk_size = 32 * 1024 ) : total_size = int ( response . headers . get ( 'content-length' , 0 ) ) with open ( destination , "wb" ) as f : for chunk in tqdm ( response . iter_content ( chunk_size ) , total = total_size , unit = 'B' , unit_scale = True , desc = destination ) : if chunk : f . write ( chunk ) def get_confirm_token ( response ) : for key , value in response . cookies . items ( ) : if key . startswith ( 'download_warning' ) : return value return None URL = "https://docs.google.com/uc?export=download" session = requests . Session ( ) response = session . get ( URL , params = { 'id' : ID } , stream = True ) token = get_confirm_token ( response ) if token : params = { 'id' : ID , 'confirm' : token } response = session . get ( URL , params = params , stream = True ) save_response_content ( response , destination )
9525	def sort_by_name ( infile , outfile ) : seqs = { } file_to_dict ( infile , seqs ) fout = utils . open_file_write ( outfile ) for name in sorted ( seqs ) : print ( seqs [ name ] , file = fout ) utils . close ( fout )
2689	def _CCompiler_spawn_silent ( cmd , dry_run = None ) : proc = Popen ( cmd , stdout = PIPE , stderr = PIPE ) out , err = proc . communicate ( ) if proc . returncode : raise DistutilsExecError ( err )
11367	def locate ( pattern , root = os . curdir ) : for path , dummy , files in os . walk ( os . path . abspath ( root ) ) : for filename in fnmatch . filter ( files , pattern ) : yield os . path . join ( path , filename )
8926	def bump ( ctx , verbose = False , pypi = False ) : cfg = config . load ( ) scm = scm_provider ( cfg . project_root , commit = False , ctx = ctx ) if not scm . workdir_is_clean ( ) : notify . warning ( "You have uncommitted changes, will create a time-stamped version!" ) pep440 = scm . pep440_dev_version ( verbose = verbose , non_local = pypi ) setup_cfg = cfg . rootjoin ( 'setup.cfg' ) if not pep440 : notify . info ( "Working directory contains a release version!" ) elif os . path . exists ( setup_cfg ) : with io . open ( setup_cfg , encoding = 'utf-8' ) as handle : data = handle . readlines ( ) changed = False for i , line in enumerate ( data ) : if re . match ( r"#? *tag_build *= *.*" , line ) : verb , _ = data [ i ] . split ( '=' , 1 ) data [ i ] = '{}= {}\n' . format ( verb , pep440 ) changed = True if changed : notify . info ( "Rewriting 'setup.cfg'..." ) with io . open ( setup_cfg , 'w' , encoding = 'utf-8' ) as handle : handle . write ( '' . join ( data ) ) else : notify . warning ( "No 'tag_build' setting found in 'setup.cfg'!" ) else : notify . warning ( "Cannot rewrite 'setup.cfg', none found!" ) if os . path . exists ( setup_cfg ) : egg_info = shell . capture ( "python setup.py egg_info" , echo = True if verbose else None ) for line in egg_info . splitlines ( ) : if line . endswith ( 'PKG-INFO' ) : pkg_info_file = line . split ( None , 1 ) [ 1 ] with io . open ( pkg_info_file , encoding = 'utf-8' ) as handle : notify . info ( '\n' . join ( i for i in handle . readlines ( ) if i . startswith ( 'Version:' ) ) . strip ( ) ) ctx . run ( "python setup.py -q develop" , echo = True if verbose else None )
3827	async def get_suggested_entities ( self , get_suggested_entities_request ) : response = hangouts_pb2 . GetSuggestedEntitiesResponse ( ) await self . _pb_request ( 'contacts/getsuggestedentities' , get_suggested_entities_request , response ) return response
3824	async def get_entity_by_id ( self , get_entity_by_id_request ) : response = hangouts_pb2 . GetEntityByIdResponse ( ) await self . _pb_request ( 'contacts/getentitybyid' , get_entity_by_id_request , response ) return response
8633	def place_project_bid ( session , project_id , bidder_id , description , amount , period , milestone_percentage ) : bid_data = { 'project_id' : project_id , 'bidder_id' : bidder_id , 'description' : description , 'amount' : amount , 'period' : period , 'milestone_percentage' : milestone_percentage , } response = make_post_request ( session , 'bids' , json_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : bid_data = json_data [ 'result' ] return Bid ( bid_data ) else : raise BidNotPlacedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
12122	def filter_gaussian ( self , sigmaMs = 100 , applyFiltered = False , applyBaseline = False ) : if sigmaMs == 0 : return self . dataY filtered = cm . filter_gaussian ( self . dataY , sigmaMs ) if applyBaseline : self . dataY = self . dataY - filtered elif applyFiltered : self . dataY = filtered else : return filtered
12146	def analyzeSingle ( abfFname ) : assert os . path . exists ( abfFname ) and abfFname . endswith ( ".abf" ) ABFfolder , ABFfname = os . path . split ( abfFname ) abfID = os . path . splitext ( ABFfname ) [ 0 ] IN = INDEX ( ABFfolder ) IN . analyzeABF ( abfID ) IN . scan ( ) IN . html_single_basic ( [ abfID ] , overwrite = True ) IN . html_single_plot ( [ abfID ] , overwrite = True ) IN . scan ( ) IN . html_index ( ) return
11486	def _descend_folder_for_id ( parsed_path , folder_id ) : if len ( parsed_path ) == 0 : return folder_id session . token = verify_credentials ( ) base_folder = session . communicator . folder_get ( session . token , folder_id ) cur_folder_id = - 1 for path_part in parsed_path : cur_folder_id = base_folder [ 'folder_id' ] cur_children = session . communicator . folder_children ( session . token , cur_folder_id ) for inner_folder in cur_children [ 'folders' ] : if inner_folder [ 'name' ] == path_part : base_folder = session . communicator . folder_get ( session . token , inner_folder [ 'folder_id' ] ) cur_folder_id = base_folder [ 'folder_id' ] break else : return - 1 return cur_folder_id
3303	def _read_config_file ( config_file , verbose ) : config_file = os . path . abspath ( config_file ) if not os . path . exists ( config_file ) : raise RuntimeError ( "Couldn't open configuration file '{}'." . format ( config_file ) ) if config_file . endswith ( ".json" ) : with io . open ( config_file , mode = "r" , encoding = "utf-8" ) as json_file : minified = jsmin ( json_file . read ( ) ) conf = json . loads ( minified ) elif config_file . endswith ( ".yaml" ) : with io . open ( config_file , mode = "r" , encoding = "utf-8" ) as yaml_file : conf = yaml . safe_load ( yaml_file ) else : try : import imp conf = { } configmodule = imp . load_source ( "configuration_module" , config_file ) for k , v in vars ( configmodule ) . items ( ) : if k . startswith ( "__" ) : continue elif isfunction ( v ) : continue conf [ k ] = v except Exception : exc_type , exc_value = sys . exc_info ( ) [ : 2 ] exc_info_list = traceback . format_exception_only ( exc_type , exc_value ) exc_text = "\n" . join ( exc_info_list ) print ( "Failed to read configuration file: " + config_file + "\nDue to " + exc_text , file = sys . stderr , ) raise conf [ "_config_file" ] = config_file return conf
8586	def get_attached_cdrom ( self , datacenter_id , server_id , cdrom_id ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/cdroms/%s' % ( datacenter_id , server_id , cdrom_id ) ) return response
7979	def _try_auth ( self ) : if self . authenticated : self . __logger . debug ( "try_auth: already authenticated" ) return self . __logger . debug ( "trying auth: %r" % ( self . _auth_methods_left , ) ) if not self . _auth_methods_left : raise LegacyAuthenticationError ( "No allowed authentication methods available" ) method = self . _auth_methods_left [ 0 ] if method . startswith ( "sasl:" ) : return ClientStream . _try_auth ( self ) elif method not in ( "plain" , "digest" ) : self . _auth_methods_left . pop ( 0 ) self . __logger . debug ( "Skipping unknown auth method: %s" % method ) return self . _try_auth ( ) elif self . available_auth_methods is not None : if method in self . available_auth_methods : self . _auth_methods_left . pop ( 0 ) self . auth_method_used = method if method == "digest" : self . _digest_auth_stage2 ( self . auth_stanza ) else : self . _plain_auth_stage2 ( self . auth_stanza ) self . auth_stanza = None return else : self . __logger . debug ( "Skipping unavailable auth method: %s" % method ) else : self . _auth_stage1 ( )
4801	def is_named ( self , filename ) : self . is_file ( ) if not isinstance ( filename , str_types ) : raise TypeError ( 'given filename arg must be a path' ) val_filename = os . path . basename ( os . path . abspath ( self . val ) ) if val_filename != filename : self . _err ( 'Expected filename <%s> to be equal to <%s>, but was not.' % ( val_filename , filename ) ) return self
3159	def get_metadata ( self ) : try : r = requests . get ( 'https://login.mailchimp.com/oauth2/metadata' , auth = self ) except requests . exceptions . RequestException as e : raise e else : r . raise_for_status ( ) output = r . json ( ) if 'error' in output : raise requests . exceptions . RequestException ( output [ 'error' ] ) return output
9644	def _flatten ( iterable ) : for i in iterable : if isinstance ( i , Iterable ) and not isinstance ( i , string_types ) : for sub_i in _flatten ( i ) : yield sub_i else : yield i
8352	def handle_charref ( self , ref ) : "Handle character references as data." if self . convertEntities : data = unichr ( int ( ref ) ) else : data = '&#%s;' % ref self . handle_data ( data )
1575	def hex_escape ( bin_str ) : printable = string . ascii_letters + string . digits + string . punctuation + ' ' return '' . join ( ch if ch in printable else r'0x{0:02x}' . format ( ord ( ch ) ) for ch in bin_str )
1002	def _updateAvgLearnedSeqLength ( self , prevSeqLength ) : if self . lrnIterationIdx < 100 : alpha = 0.5 else : alpha = 0.1 self . avgLearnedSeqLength = ( ( 1.0 - alpha ) * self . avgLearnedSeqLength + ( alpha * prevSeqLength ) )
5345	def compose_git ( projects , data ) : for p in [ project for project in data if len ( data [ project ] [ 'source_repo' ] ) > 0 ] : repos = [ ] for url in data [ p ] [ 'source_repo' ] : if len ( url [ 'url' ] . split ( ) ) > 1 : repo = url [ 'url' ] . split ( ) [ 1 ] . replace ( '/c/' , '/gitroot/' ) else : repo = url [ 'url' ] . replace ( '/c/' , '/gitroot/' ) if repo not in repos : repos . append ( repo ) projects [ p ] [ 'git' ] = repos return projects
9922	def create ( self , validated_data ) : email = validated_data . pop ( "email" ) password = validated_data . pop ( "password" ) user = get_user_model ( ) ( ** validated_data ) user . set_password ( password ) user . email = email email_query = models . EmailAddress . objects . filter ( email = email ) if email_query . exists ( ) : existing_email = email_query . get ( ) existing_email . send_duplicate_notification ( ) else : user . save ( ) email_instance = models . EmailAddress . objects . create ( email = email , user = user ) email_instance . send_confirmation ( ) signals . user_registered . send ( sender = self . __class__ , user = user ) return user
13430	def create_site ( self , params = { } ) : url = "/2/sites/" body = params data = self . _post_resource ( url , body ) return self . site_from_json ( data [ "site" ] )
6257	def find ( self , path : Path ) : if getattr ( self , 'settings_attr' , None ) : self . paths = getattr ( settings , self . settings_attr ) path_found = None for entry in self . paths : abspath = entry / path if abspath . exists ( ) : path_found = abspath return path_found
9272	def filter_due_tag ( self , all_tags ) : filtered_tags = [ ] tag = self . options . due_tag tag_names = [ t [ "name" ] for t in all_tags ] try : idx = tag_names . index ( tag ) except ValueError : self . warn_if_tag_not_found ( tag , "due-tag" ) return copy . deepcopy ( all_tags ) due_tag = all_tags [ idx ] due_date = self . get_time_of_tag ( due_tag ) for t in all_tags : tag_date = self . get_time_of_tag ( t ) if tag_date <= due_date : filtered_tags . append ( t ) return filtered_tags
716	def __loadHyperSearchJobID ( cls , permWorkDir , outputLabel ) : filePath = cls . __getHyperSearchJobIDFilePath ( permWorkDir = permWorkDir , outputLabel = outputLabel ) jobID = None with open ( filePath , "r" ) as jobIdPickleFile : jobInfo = pickle . load ( jobIdPickleFile ) jobID = jobInfo [ "hyperSearchJobID" ] return jobID
10870	def calc_pts_lag ( npts = 20 ) : scl = { 15 : 0.072144 , 20 : 0.051532 , 25 : 0.043266 } [ npts ] pts0 , wts0 = np . polynomial . laguerre . laggauss ( npts ) pts = np . sinh ( pts0 * scl ) wts = scl * wts0 * np . cosh ( pts0 * scl ) * np . exp ( pts0 ) return pts , wts
2331	def normal_noise ( points ) : return np . random . rand ( 1 ) * np . random . randn ( points , 1 ) + random . sample ( [ 2 , - 2 ] , 1 )
5908	def make_ndx_captured ( ** kwargs ) : kwargs [ 'stdout' ] = False user_input = kwargs . pop ( 'input' , [ ] ) user_input = [ cmd for cmd in user_input if cmd != 'q' ] kwargs [ 'input' ] = user_input + [ '' , 'q' ] return gromacs . make_ndx ( ** kwargs )
10137	def _assert_version ( self , version ) : if self . nearest_version < version : if self . _version_given : raise ValueError ( 'Data type requires version %s' % version ) else : self . _version = version
11604	def check_ranges ( cls , ranges , length ) : result = [ ] for start , end in ranges : if isinstance ( start , int ) or isinstance ( end , int ) : if isinstance ( start , int ) and not ( 0 <= start < length ) : continue elif isinstance ( start , int ) and isinstance ( end , int ) and not ( start <= end ) : continue elif start is None and end == 0 : continue result . append ( ( start , end ) ) return result
4576	def color_cmp ( a , b ) : if a == b : return 0 a , b = rgb_to_hsv ( a ) , rgb_to_hsv ( b ) return - 1 if a < b else 1
12731	def create_body ( self , shape , name = None , ** kwargs ) : shape = shape . lower ( ) if name is None : for i in range ( 1 + len ( self . _bodies ) ) : name = '{}{}' . format ( shape , i ) if name not in self . _bodies : break self . _bodies [ name ] = Body . build ( shape , name , self , ** kwargs ) return self . _bodies [ name ]
4939	def get_link_by_email ( self , user_email ) : try : user = User . objects . get ( email = user_email ) try : return self . get ( user_id = user . id ) except EnterpriseCustomerUser . DoesNotExist : pass except User . DoesNotExist : pass try : return PendingEnterpriseCustomerUser . objects . get ( user_email = user_email ) except PendingEnterpriseCustomerUser . DoesNotExist : pass return None
7293	def get_form ( self ) : self . set_fields ( ) if self . post_data_dict is not None : self . set_post_data ( ) return self . form
5980	def bin_up_mask_2d ( mask_2d , bin_up_factor ) : padded_array_2d = array_util . pad_2d_array_for_binning_up_with_bin_up_factor ( array_2d = mask_2d , bin_up_factor = bin_up_factor , pad_value = True ) binned_array_2d = np . zeros ( shape = ( padded_array_2d . shape [ 0 ] // bin_up_factor , padded_array_2d . shape [ 1 ] // bin_up_factor ) ) for y in range ( binned_array_2d . shape [ 0 ] ) : for x in range ( binned_array_2d . shape [ 1 ] ) : value = True for y1 in range ( bin_up_factor ) : for x1 in range ( bin_up_factor ) : padded_y = y * bin_up_factor + y1 padded_x = x * bin_up_factor + x1 if padded_array_2d [ padded_y , padded_x ] == False : value = False binned_array_2d [ y , x ] = value return binned_array_2d
7725	def __from_xmlnode ( self , xmlnode ) : actor = None reason = None n = xmlnode . children while n : ns = n . ns ( ) if ns and ns . getContent ( ) != MUC_USER_NS : continue if n . name == "actor" : actor = n . getContent ( ) if n . name == "reason" : reason = n . getContent ( ) n = n . next self . __init ( from_utf8 ( xmlnode . prop ( "affiliation" ) ) , from_utf8 ( xmlnode . prop ( "role" ) ) , from_utf8 ( xmlnode . prop ( "jid" ) ) , from_utf8 ( xmlnode . prop ( "nick" ) ) , from_utf8 ( actor ) , from_utf8 ( reason ) , )
5621	def tile_to_zoom_level ( tile , dst_pyramid = None , matching_method = "gdal" , precision = 8 ) : def width_height ( bounds ) : try : l , b , r , t = reproject_geometry ( box ( * bounds ) , src_crs = tile . crs , dst_crs = dst_pyramid . crs ) . bounds except ValueError : raise TopologicalError ( "bounds cannot be translated into target CRS" ) return r - l , t - b if tile . tp . crs == dst_pyramid . crs : return tile . zoom else : if matching_method == "gdal" : transform , width , height = calculate_default_transform ( tile . tp . crs , dst_pyramid . crs , tile . width , tile . height , * tile . bounds ) tile_resolution = round ( transform [ 0 ] , precision ) elif matching_method == "min" : l , b , r , t = tile . bounds x = tile . pixel_x_size y = tile . pixel_y_size res = [ ] for bounds in [ ( l , t - y , l + x , t ) , ( l , b , l + x , b + y ) , ( r - x , b , r , b + y ) , ( r - x , t - y , r , t ) ] : try : w , h = width_height ( bounds ) res . extend ( [ w , h ] ) except TopologicalError : logger . debug ( "pixel outside of destination pyramid" ) if res : tile_resolution = round ( min ( res ) , precision ) else : raise TopologicalError ( "tile outside of destination pyramid" ) else : raise ValueError ( "invalid method given: %s" , matching_method ) logger . debug ( "we are looking for a zoom level interpolating to %s resolution" , tile_resolution ) zoom = 0 while True : td_resolution = round ( dst_pyramid . pixel_x_size ( zoom ) , precision ) if td_resolution <= tile_resolution : break zoom += 1 logger . debug ( "target zoom for %s: %s (%s)" , tile_resolution , zoom , td_resolution ) return zoom
4183	def window_blackman_harris ( N ) : r a0 = 0.35875 a1 = 0.48829 a2 = 0.14128 a3 = 0.01168 return _coeff4 ( N , a0 , a1 , a2 , a3 )
6062	def mass_within_circle_in_units ( self , radius : dim . Length , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : self . check_units_of_radius_and_critical_surface_density ( radius = radius , critical_surface_density = critical_surface_density ) profile = self . new_profile_with_units_converted ( unit_length = radius . unit_length , unit_mass = 'angular' , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) mass_angular = dim . Mass ( value = quad ( profile . mass_integral , a = 0.0 , b = radius , args = ( 1.0 , ) ) [ 0 ] , unit_mass = 'angular' ) return mass_angular . convert ( unit_mass = unit_mass , critical_surface_density = critical_surface_density )
8104	def update ( self ) : try : self . manager . handle ( self . socket . recv ( 1024 ) ) except socket . error : pass
5734	def _get_result_msg_and_payload ( result , stream ) : groups = _GDB_MI_RESULT_RE . match ( result ) . groups ( ) token = int ( groups [ 0 ] ) if groups [ 0 ] != "" else None message = groups [ 1 ] if groups [ 2 ] is None : payload = None else : stream . advance_past_chars ( [ "," ] ) payload = _parse_dict ( stream ) return token , message , payload
8802	def do_notify ( context , event_type , payload ) : LOG . debug ( 'IP_BILL: notifying {}' . format ( payload ) ) notifier = n_rpc . get_notifier ( 'network' ) notifier . info ( context , event_type , payload )
9611	def _request ( self , method , url , body ) : if method != 'POST' and method != 'PUT' : body = None s = Session ( ) LOGGER . debug ( 'Method: {0}, Url: {1}, Body: {2}.' . format ( method , url , body ) ) req = Request ( method , url , json = body ) prepped = s . prepare_request ( req ) res = s . send ( prepped , timeout = self . _timeout or None ) res . raise_for_status ( ) return res . json ( )
450	def compute_alpha ( x ) : threshold = _compute_threshold ( x ) alpha1_temp1 = tf . where ( tf . greater ( x , threshold ) , x , tf . zeros_like ( x , tf . float32 ) ) alpha1_temp2 = tf . where ( tf . less ( x , - threshold ) , x , tf . zeros_like ( x , tf . float32 ) ) alpha_array = tf . add ( alpha1_temp1 , alpha1_temp2 , name = None ) alpha_array_abs = tf . abs ( alpha_array ) alpha_array_abs1 = tf . where ( tf . greater ( alpha_array_abs , 0 ) , tf . ones_like ( alpha_array_abs , tf . float32 ) , tf . zeros_like ( alpha_array_abs , tf . float32 ) ) alpha_sum = tf . reduce_sum ( alpha_array_abs ) n = tf . reduce_sum ( alpha_array_abs1 ) alpha = tf . div ( alpha_sum , n ) return alpha
10443	def getobjectinfo ( self , window_name , object_name ) : try : obj_info = self . _get_object_map ( window_name , object_name , wait_for_object = False ) except atomac . _a11y . ErrorInvalidUIElement : self . _windows = { } obj_info = self . _get_object_map ( window_name , object_name , wait_for_object = False ) props = [ ] if obj_info : for obj_prop in obj_info . keys ( ) : if not obj_info [ obj_prop ] or obj_prop == "obj" : continue props . append ( obj_prop ) return props
4888	def update_enterprise_courses ( self , enterprise_customer , course_container_key = 'results' , ** kwargs ) : enterprise_context = { 'tpa_hint' : enterprise_customer and enterprise_customer . identity_provider , 'enterprise_id' : enterprise_customer and str ( enterprise_customer . uuid ) , } enterprise_context . update ( ** kwargs ) courses = [ ] for course in self . data [ course_container_key ] : courses . append ( self . update_course ( course , enterprise_customer , enterprise_context ) ) self . data [ course_container_key ] = courses
11699	def serve ( self , sock , request_handler , error_handler , debug = False , request_timeout = 60 , ssl = None , request_max_size = None , reuse_port = False , loop = None , protocol = HttpProtocol , backlog = 100 , ** kwargs ) : if debug : loop . set_debug ( debug ) server = partial ( protocol , loop = loop , connections = self . connections , signal = self . signal , request_handler = request_handler , error_handler = error_handler , request_timeout = request_timeout , request_max_size = request_max_size , ) server_coroutine = loop . create_server ( server , host = None , port = None , ssl = ssl , reuse_port = reuse_port , sock = sock , backlog = backlog ) loop . call_soon ( partial ( update_current_time , loop ) ) return server_coroutine
13701	def _before ( self ) : if request . path in self . excluded_routes : request . _tracy_exclude = True return request . _tracy_start_time = monotonic ( ) client = request . headers . get ( trace_header_client , None ) require_client = current_app . config . get ( "TRACY_REQUIRE_CLIENT" , False ) if client is None and require_client : abort ( 400 , "Missing %s header" % trace_header_client ) request . _tracy_client = client request . _tracy_id = request . headers . get ( trace_header_id , new_id ( ) )
3350	def _generate_index ( self ) : self . _dict = { v . id : k for k , v in enumerate ( self ) }
7649	def _open ( name_or_fdesc , mode = 'r' , fmt = 'auto' ) : open_map = { 'jams' : open , 'json' : open , 'jamz' : gzip . open , 'gz' : gzip . open } if hasattr ( name_or_fdesc , 'read' ) or hasattr ( name_or_fdesc , 'write' ) : yield name_or_fdesc elif isinstance ( name_or_fdesc , six . string_types ) : if fmt == 'auto' : _ , ext = os . path . splitext ( name_or_fdesc ) ext = ext [ 1 : ] else : ext = fmt try : ext = ext . lower ( ) if ext in [ 'jamz' , 'gz' ] and 't' not in mode : mode = '{:s}t' . format ( mode ) with open_map [ ext ] ( name_or_fdesc , mode = mode ) as fdesc : yield fdesc except KeyError : raise ParameterError ( 'Unknown JAMS extension ' 'format: "{:s}"' . format ( ext ) ) else : raise ParameterError ( 'Invalid filename or ' 'descriptor: {}' . format ( name_or_fdesc ) )
12484	def get_subdict ( adict , path , sep = os . sep ) : return reduce ( adict . __class__ . get , [ p for p in op . split ( sep ) if p ] , adict )
3087	def _get_entity ( self ) : if self . _is_ndb ( ) : return self . _model . get_by_id ( self . _key_name ) else : return self . _model . get_by_key_name ( self . _key_name )
7749	def process_iq ( self , stanza ) : typ = stanza . stanza_type if typ in ( "result" , "error" ) : return self . _process_iq_response ( stanza ) if typ not in ( "get" , "set" ) : raise BadRequestProtocolError ( "Bad <iq/> type" ) logger . debug ( "Handling <iq type='{0}'> stanza: {1!r}" . format ( stanza , typ ) ) payload = stanza . get_payload ( None ) logger . debug ( " payload: {0!r}" . format ( payload ) ) if not payload : raise BadRequestProtocolError ( "<iq/> stanza with no child element" ) handler = self . _get_iq_handler ( typ , payload ) if not handler : payload = stanza . get_payload ( None , specialize = True ) logger . debug ( " specialized payload: {0!r}" . format ( payload ) ) if not isinstance ( payload , XMLPayload ) : handler = self . _get_iq_handler ( typ , payload ) if handler : response = handler ( stanza ) self . _process_handler_result ( response ) return True else : raise ServiceUnavailableProtocolError ( "Not implemented" )
12704	def make_quaternion ( theta , * axis ) : x , y , z = axis r = np . sqrt ( x * x + y * y + z * z ) st = np . sin ( theta / 2. ) ct = np . cos ( theta / 2. ) return [ x * st / r , y * st / r , z * st / r , ct ]
1432	def is_grouping_sane ( cls , gtype ) : if gtype == cls . SHUFFLE or gtype == cls . ALL or gtype == cls . LOWEST or gtype == cls . NONE : return True elif isinstance ( gtype , cls . FIELDS ) : return gtype . gtype == topology_pb2 . Grouping . Value ( "FIELDS" ) and gtype . fields is not None elif isinstance ( gtype , cls . CUSTOM ) : return gtype . gtype == topology_pb2 . Grouping . Value ( "CUSTOM" ) and gtype . python_serialized is not None else : return False
5851	def get_dataset_files ( self , dataset_id , glob = "." , is_dir = False , version_number = None ) : if version_number is None : latest = True else : latest = False data = { "download_request" : { "glob" : glob , "isDir" : is_dir , "latest" : latest } } failure_message = "Failed to get matched files in dataset {}" . format ( dataset_id ) versions = self . _get_success_json ( self . _post_json ( routes . matched_files ( dataset_id ) , data , failure_message = failure_message ) ) [ 'versions' ] if version_number is None : version = versions [ 0 ] else : try : version = list ( filter ( lambda v : v [ 'number' ] == version_number , versions ) ) [ 0 ] except IndexError : raise ResourceNotFoundException ( ) return list ( map ( lambda f : DatasetFile ( path = f [ 'filename' ] , url = f [ 'url' ] ) , version [ 'files' ] ) )
4104	def CORRELATION ( x , y = None , maxlags = None , norm = 'unbiased' ) : r assert norm in [ 'unbiased' , 'biased' , 'coeff' , None ] x = np . array ( x ) if y is None : y = x else : y = np . array ( y ) N = max ( len ( x ) , len ( y ) ) if len ( x ) < N : x = y . copy ( ) x . resize ( N ) if len ( y ) < N : y = y . copy ( ) y . resize ( N ) if maxlags is None : maxlags = N - 1 assert maxlags < N , 'lag must be less than len(x)' realdata = np . isrealobj ( x ) and np . isrealobj ( y ) if realdata == True : r = np . zeros ( maxlags , dtype = float ) else : r = np . zeros ( maxlags , dtype = complex ) if norm == 'coeff' : rmsx = pylab_rms_flat ( x ) rmsy = pylab_rms_flat ( y ) for k in range ( 0 , maxlags + 1 ) : nk = N - k - 1 if realdata == True : sum = 0 for j in range ( 0 , nk + 1 ) : sum = sum + x [ j + k ] * y [ j ] else : sum = 0. + 0j for j in range ( 0 , nk + 1 ) : sum = sum + x [ j + k ] * y [ j ] . conjugate ( ) if k == 0 : if norm in [ 'biased' , 'unbiased' ] : r0 = sum / float ( N ) elif norm is None : r0 = sum else : r0 = 1. else : if norm == 'unbiased' : r [ k - 1 ] = sum / float ( N - k ) elif norm == 'biased' : r [ k - 1 ] = sum / float ( N ) elif norm is None : r [ k - 1 ] = sum elif norm == 'coeff' : r [ k - 1 ] = sum / ( rmsx * rmsy ) / float ( N ) r = np . insert ( r , 0 , r0 ) return r
903	def _calcSkipRecords ( numIngested , windowSize , learningPeriod ) : numShiftedOut = max ( 0 , numIngested - windowSize ) return min ( numIngested , max ( 0 , learningPeriod - numShiftedOut ) )
4932	def transform_courserun_schedule ( self , content_metadata_item ) : start = content_metadata_item . get ( 'start' ) or UNIX_MIN_DATE_STRING end = content_metadata_item . get ( 'end' ) or UNIX_MAX_DATE_STRING return [ { 'startDate' : parse_datetime_to_epoch_millis ( start ) , 'endDate' : parse_datetime_to_epoch_millis ( end ) , 'active' : current_time_is_in_interval ( start , end ) } ]
3106	def code_challenge ( verifier ) : digest = hashlib . sha256 ( verifier ) . digest ( ) return base64 . urlsafe_b64encode ( digest ) . rstrip ( b'=' )
10102	def _make_file_dict ( self , f ) : if isinstance ( f , dict ) : file_obj = f [ 'file' ] if 'filename' in f : file_name = f [ 'filename' ] else : file_name = file_obj . name else : file_obj = f file_name = f . name b64_data = base64 . b64encode ( file_obj . read ( ) ) return { 'id' : file_name , 'data' : b64_data . decode ( ) if six . PY3 else b64_data , }
5857	def get_available_columns ( self , dataset_ids ) : if not isinstance ( dataset_ids , list ) : dataset_ids = [ dataset_ids ] data = { "dataset_ids" : dataset_ids } failure_message = "Failed to get available columns in dataset(s) {}" . format ( dataset_ids ) return self . _get_success_json ( self . _post_json ( 'v1/datasets/get-available-columns' , data , failure_message = failure_message ) ) [ 'data' ]
3196	def delete_permanent ( self , list_id , subscriber_hash ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash return self . _mc_client . _post ( url = self . _build_path ( list_id , 'members' , subscriber_hash , 'actions' , 'delete-permanent' ) )
11720	def get_directory ( self , path_to_directory , timeout = 30 , backoff = 0.4 , max_wait = 4 ) : response = None started_at = None time_elapsed = 0 i = 0 while time_elapsed < timeout : response = self . _get ( '{0}.zip' . format ( path_to_directory ) ) if response : break else : if started_at is None : started_at = time . time ( ) time . sleep ( min ( backoff * ( 2 ** i ) , max_wait ) ) i += 1 time_elapsed = time . time ( ) - started_at return response
9056	def sample ( self , random_state = None ) : r from numpy_sugar import epsilon from numpy_sugar . linalg import sum2diag from numpy_sugar . random import multivariate_normal if random_state is None : random_state = RandomState ( ) m = self . _mean . value ( ) K = self . _cov . value ( ) . copy ( ) sum2diag ( K , + epsilon . small , out = K ) return self . _lik . sample ( multivariate_normal ( m , K , random_state ) , random_state )
8091	def graph_background ( s ) : if s . background == None : s . _ctx . background ( None ) else : s . _ctx . background ( s . background ) if s . depth : try : clr = colors . color ( s . background ) . darker ( 0.2 ) p = s . _ctx . rect ( 0 , 0 , s . _ctx . WIDTH , s . _ctx . HEIGHT , draw = False ) colors . gradientfill ( p , clr , clr . lighter ( 0.35 ) ) colors . shadow ( dx = 0 , dy = 0 , blur = 2 , alpha = 0.935 , clr = s . background ) except : pass
10311	def prepare_c3 ( data : Union [ List [ Tuple [ str , int ] ] , Mapping [ str , int ] ] , y_axis_label : str = 'y' , x_axis_label : str = 'x' , ) -> str : if not isinstance ( data , list ) : data = sorted ( data . items ( ) , key = itemgetter ( 1 ) , reverse = True ) try : labels , values = zip ( * data ) except ValueError : log . info ( f'no values found for {x_axis_label}, {y_axis_label}' ) labels , values = [ ] , [ ] return json . dumps ( [ [ x_axis_label ] + list ( labels ) , [ y_axis_label ] + list ( values ) , ] )
9244	def set_date_from_event ( self , event , issue ) : if not event . get ( 'commit_id' , None ) : issue [ 'actual_date' ] = timestring_to_datetime ( issue [ 'closed_at' ] ) return try : commit = self . fetcher . fetch_commit ( event ) issue [ 'actual_date' ] = timestring_to_datetime ( commit [ 'author' ] [ 'date' ] ) except ValueError : print ( "WARNING: Can't fetch commit {0}. " "It is probably referenced from another repo." . format ( event [ 'commit_id' ] ) ) issue [ 'actual_date' ] = timestring_to_datetime ( issue [ 'closed_at' ] )
12275	def iso_reference_isvalid ( ref ) : ref = str ( ref ) cs_source = ref [ 4 : ] + ref [ : 4 ] return ( iso_reference_str2int ( cs_source ) % 97 ) == 1
13628	def parse ( expected , query ) : return dict ( ( key , parser ( query . get ( key , [ ] ) ) ) for key , parser in expected . items ( ) )
11868	def strip_minidom_whitespace ( node ) : for child in node . childNodes : if child . nodeType == Node . TEXT_NODE : if child . nodeValue : child . nodeValue = child . nodeValue . strip ( ) elif child . nodeType == Node . ELEMENT_NODE : strip_minidom_whitespace ( child )
4746	def pkill ( ) : if env ( ) : return 1 cmd = [ "ps -aux | grep fio | grep -v grep" ] status , _ , _ = cij . ssh . command ( cmd , shell = True , echo = False ) if not status : status , _ , _ = cij . ssh . command ( [ "pkill -f fio" ] , shell = True ) if status : return 1 return 0
3821	async def delete_conversation ( self , delete_conversation_request ) : response = hangouts_pb2 . DeleteConversationResponse ( ) await self . _pb_request ( 'conversations/deleteconversation' , delete_conversation_request , response ) return response
8092	def node ( s , node , alpha = 1.0 ) : if s . depth : try : colors . shadow ( dx = 5 , dy = 5 , blur = 10 , alpha = 0.5 * alpha ) except : pass s . _ctx . nofill ( ) s . _ctx . nostroke ( ) if s . fill : s . _ctx . fill ( s . fill . r , s . fill . g , s . fill . b , s . fill . a * alpha ) if s . stroke : s . _ctx . strokewidth ( s . strokewidth ) s . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * alpha * 3 ) r = node . r s . _ctx . oval ( node . x - r , node . y - r , r * 2 , r * 2 )
12264	def _send_file_internal ( self , * args , ** kwargs ) : super ( Key , self ) . _send_file_internal ( * args , ** kwargs ) mimicdb . backend . sadd ( tpl . bucket % self . bucket . name , self . name ) mimicdb . backend . hmset ( tpl . key % ( self . bucket . name , self . name ) , dict ( size = self . size , md5 = self . md5 ) )
9564	def create_validator ( ) : field_names = ( 'study_id' , 'patient_id' , 'gender' , 'age_years' , 'age_months' , 'date_inclusion' ) validator = CSVValidator ( field_names ) validator . add_header_check ( 'EX1' , 'bad header' ) validator . add_record_length_check ( 'EX2' , 'unexpected record length' ) validator . add_value_check ( 'study_id' , int , 'EX3' , 'study id must be an integer' ) validator . add_value_check ( 'patient_id' , int , 'EX4' , 'patient id must be an integer' ) validator . add_value_check ( 'gender' , enumeration ( 'M' , 'F' ) , 'EX5' , 'invalid gender' ) validator . add_value_check ( 'age_years' , number_range_inclusive ( 0 , 120 , int ) , 'EX6' , 'invalid age in years' ) validator . add_value_check ( 'date_inclusion' , datetime_string ( '%Y-%m-%d' ) , 'EX7' , 'invalid date' ) def check_age_variables ( r ) : age_years = int ( r [ 'age_years' ] ) age_months = int ( r [ 'age_months' ] ) valid = ( age_months >= age_years * 12 and age_months % age_years < 12 ) if not valid : raise RecordError ( 'EX8' , 'invalid age variables' ) validator . add_record_check ( check_age_variables ) return validator
11594	def _rc_rename ( self , src , dst ) : if src == dst : return self . rename ( src + "{" + src + "}" , src ) if not self . exists ( src ) : return self . rename ( src + "{" + src + "}" , src ) self . delete ( dst ) ktype = self . type ( src ) kttl = self . ttl ( src ) if ktype == b ( 'none' ) : return False if ktype == b ( 'string' ) : self . set ( dst , self . get ( src ) ) elif ktype == b ( 'hash' ) : self . hmset ( dst , self . hgetall ( src ) ) elif ktype == b ( 'list' ) : for k in self . lrange ( src , 0 , - 1 ) : self . rpush ( dst , k ) elif ktype == b ( 'set' ) : for k in self . smembers ( src ) : self . sadd ( dst , k ) elif ktype == b ( 'zset' ) : for k , v in self . zrange ( src , 0 , - 1 , withscores = True ) : self . zadd ( dst , v , k ) kttl = - 1 if kttl is None or kttl < 0 else int ( kttl ) if kttl != - 1 : self . expire ( dst , kttl ) return self . delete ( src )
442	def get_all_params ( self , session = None ) : _params = [ ] for p in self . all_params : if session is None : _params . append ( p . eval ( ) ) else : _params . append ( session . run ( p ) ) return _params
6582	def play_station ( self , station ) : for song in iterate_forever ( station . get_playlist ) : try : self . play ( song ) except StopIteration : self . stop ( ) return
6855	def ismounted ( device ) : with settings ( hide ( 'running' , 'stdout' ) ) : res = run_as_root ( 'mount' ) for line in res . splitlines ( ) : fields = line . split ( ) if fields [ 0 ] == device : return True with settings ( hide ( 'running' , 'stdout' ) ) : res = run_as_root ( 'swapon -s' ) for line in res . splitlines ( ) : fields = line . split ( ) if fields [ 0 ] == device : return True return False
1781	def AAM ( cpu , imm = None ) : if imm is None : imm = 10 else : imm = imm . read ( ) cpu . AH = Operators . UDIV ( cpu . AL , imm ) cpu . AL = Operators . UREM ( cpu . AL , imm ) cpu . _calculate_logic_flags ( 8 , cpu . AL )
11768	def weighted_sampler ( seq , weights ) : "Return a random-sample function that picks from seq weighted by weights." totals = [ ] for w in weights : totals . append ( w + totals [ - 1 ] if totals else w ) return lambda : seq [ bisect . bisect ( totals , random . uniform ( 0 , totals [ - 1 ] ) ) ]
13424	def get_message ( self , message_id ) : url = "/2/messages/%s" % message_id return self . message_from_json ( self . _get_resource ( url ) [ "message" ] )
12447	def render_to_string ( self ) : values = '' for key , value in self . items ( ) : values += '{}={};' . format ( key , value ) return values
884	def reset ( self ) : self . activeCells = [ ] self . winnerCells = [ ] self . activeSegments = [ ] self . matchingSegments = [ ]
2580	def checkpoint ( self , tasks = None ) : with self . checkpoint_lock : checkpoint_queue = None if tasks : checkpoint_queue = tasks else : checkpoint_queue = self . tasks checkpoint_dir = '{0}/checkpoint' . format ( self . run_dir ) checkpoint_dfk = checkpoint_dir + '/dfk.pkl' checkpoint_tasks = checkpoint_dir + '/tasks.pkl' if not os . path . exists ( checkpoint_dir ) : try : os . makedirs ( checkpoint_dir ) except FileExistsError : pass with open ( checkpoint_dfk , 'wb' ) as f : state = { 'rundir' : self . run_dir , 'task_count' : self . task_count } pickle . dump ( state , f ) count = 0 with open ( checkpoint_tasks , 'ab' ) as f : for task_id in checkpoint_queue : if not self . tasks [ task_id ] [ 'checkpoint' ] and self . tasks [ task_id ] [ 'app_fu' ] . done ( ) and self . tasks [ task_id ] [ 'app_fu' ] . exception ( ) is None : hashsum = self . tasks [ task_id ] [ 'hashsum' ] if not hashsum : continue t = { 'hash' : hashsum , 'exception' : None , 'result' : None } try : r = self . memoizer . hash_lookup ( hashsum ) . result ( ) except Exception as e : t [ 'exception' ] = e else : t [ 'result' ] = r pickle . dump ( t , f ) count += 1 self . tasks [ task_id ] [ 'checkpoint' ] = True logger . debug ( "Task {} checkpointed" . format ( task_id ) ) self . checkpointed_tasks += count if count == 0 : if self . checkpointed_tasks == 0 : logger . warn ( "No tasks checkpointed so far in this run. Please ensure caching is enabled" ) else : logger . debug ( "No tasks checkpointed in this pass." ) else : logger . info ( "Done checkpointing {} tasks" . format ( count ) ) return checkpoint_dir
1582	def read ( self , dispatcher ) : try : if not self . is_header_read : to_read = HeronProtocol . HEADER_SIZE - len ( self . header ) self . header += dispatcher . recv ( to_read ) if len ( self . header ) == HeronProtocol . HEADER_SIZE : self . is_header_read = True else : Log . debug ( "Header read incomplete; read %d bytes of header" % len ( self . header ) ) return if self . is_header_read and not self . is_complete : to_read = self . get_datasize ( ) - len ( self . data ) self . data += dispatcher . recv ( to_read ) if len ( self . data ) == self . get_datasize ( ) : self . is_complete = True except socket . error as e : if e . errno == socket . errno . EAGAIN or e . errno == socket . errno . EWOULDBLOCK : Log . debug ( "Try again error" ) else : Log . debug ( "Fatal error when reading IncomingPacket" ) raise RuntimeError ( "Fatal error occured in IncomingPacket.read()" )
6877	def _gunzip_sqlitecurve ( sqlitecurve ) : cmd = 'gunzip -k %s' % sqlitecurve try : subprocess . check_output ( cmd , shell = True ) return sqlitecurve . replace ( '.gz' , '' ) except subprocess . CalledProcessError : return None
5327	def sha_github_file ( cls , config , repo_file , repository_api , repository_branch ) : repo_file_sha = None cfg = config . get_conf ( ) github_token = cfg [ 'sortinghat' ] [ 'identities_api_token' ] headers = { "Authorization" : "token " + github_token } url_dir = repository_api + "/git/trees/" + repository_branch logger . debug ( "Gettting sha data from tree: %s" , url_dir ) raw_repo_file_info = requests . get ( url_dir , headers = headers ) raw_repo_file_info . raise_for_status ( ) for rfile in raw_repo_file_info . json ( ) [ 'tree' ] : if rfile [ 'path' ] == repo_file : logger . debug ( "SHA found: %s, " , rfile [ "sha" ] ) repo_file_sha = rfile [ "sha" ] break return repo_file_sha
5445	def _parse_image_uri ( self , raw_uri ) : docker_uri = os . path . join ( self . _relative_path , raw_uri . replace ( 'https://' , 'https/' , 1 ) ) return docker_uri
8522	def add_int ( self , name , min , max , warp = None ) : min , max = map ( int , ( min , max ) ) if max < min : raise ValueError ( 'variable %s: max < min error' % name ) if warp not in ( None , 'log' ) : raise ValueError ( 'variable %s: warp=%s is not supported. use ' 'None or "log",' % ( name , warp ) ) if min <= 0 and warp == 'log' : raise ValueError ( 'variable %s: log-warping requires min > 0' ) self . variables [ name ] = IntVariable ( name , min , max , warp )
9955	def custom_showtraceback ( self , exc_tuple = None , filename = None , tb_offset = None , exception_only = False , running_compiled_code = False , ) : self . default_showtraceback ( exc_tuple , filename , tb_offset , exception_only = True , running_compiled_code = running_compiled_code , )
11650	def fit ( self , X , y = None ) : if is_integer ( X ) : dim = X else : X = as_features ( X ) dim = X . dim M = self . smoothness inds = np . mgrid [ ( slice ( M + 1 ) , ) * dim ] . reshape ( dim , ( M + 1 ) ** dim ) . T self . inds_ = inds [ ( inds ** 2 ) . sum ( axis = 1 ) <= M ** 2 ] return self
186	def copy ( self , coords = None , label = None ) : return LineString ( coords = self . coords if coords is None else coords , label = self . label if label is None else label )
12959	def _peekNextID ( self , conn = None ) : if conn is None : conn = self . _get_connection ( ) return to_unicode ( conn . get ( self . _get_next_id_key ( ) ) or 0 )
10998	def moment ( p , v , order = 1 ) : if order == 1 : return ( v * p ) . sum ( ) elif order == 2 : return np . sqrt ( ( ( v ** 2 ) * p ) . sum ( ) - ( v * p ) . sum ( ) ** 2 )
10509	def stoplog ( self ) : if self . _file_logger : self . logger . removeHandler ( _file_logger ) self . _file_logger = None return 1
3081	def xsrf_secret_key ( ) : secret = memcache . get ( XSRF_MEMCACHE_ID , namespace = OAUTH2CLIENT_NAMESPACE ) if not secret : model = SiteXsrfSecretKey . get_or_insert ( key_name = 'site' ) if not model . secret : model . secret = _generate_new_xsrf_secret_key ( ) model . put ( ) secret = model . secret memcache . add ( XSRF_MEMCACHE_ID , secret , namespace = OAUTH2CLIENT_NAMESPACE ) return str ( secret )
9025	def kivy_svg ( self ) : from kivy . graphics . svg import Svg path = self . temporary_path ( ".svg" ) try : return Svg ( path ) finally : remove_file ( path )
10771	def contour ( self , level ) : if not isinstance ( level , numbers . Number ) : raise TypeError ( ( "'_level' must be of type 'numbers.Number' but is " "'{:s}'" ) . format ( type ( level ) ) ) vertices = self . _contour_generator . create_contour ( level ) return self . formatter ( level , vertices )
6988	def serial_varfeatures ( lclist , outdir , maxobjects = None , timecols = None , magcols = None , errcols = None , mindet = 1000 , lcformat = 'hat-sql' , lcformatdir = None ) : if maxobjects : lclist = lclist [ : maxobjects ] tasks = [ ( x , outdir , timecols , magcols , errcols , mindet , lcformat , lcformatdir ) for x in lclist ] for task in tqdm ( tasks ) : result = _varfeatures_worker ( task ) return result
9340	def flatten_dtype ( dtype , _next = None ) : types = [ ] if _next is None : _next = [ 0 , '' ] primary = True else : primary = False prefix = _next [ 1 ] if dtype . names is None : for i in numpy . ndindex ( dtype . shape ) : if dtype . base == dtype : types . append ( ( '%s%s' % ( prefix , simplerepr ( i ) ) , dtype ) ) _next [ 0 ] += 1 else : _next [ 1 ] = '%s%s' % ( prefix , simplerepr ( i ) ) types . extend ( flatten_dtype ( dtype . base , _next ) ) else : for field in dtype . names : typ_fields = dtype . fields [ field ] if len ( prefix ) > 0 : _next [ 1 ] = prefix + '.' + field else : _next [ 1 ] = '' + field flat_dt = flatten_dtype ( typ_fields [ 0 ] , _next ) types . extend ( flat_dt ) _next [ 1 ] = prefix if primary : return numpy . dtype ( types ) else : return types
691	def _setTypes ( self , encoderSpec ) : if self . encoderType is None : if self . dataType in [ 'int' , 'float' ] : self . encoderType = 'adaptiveScalar' elif self . dataType == 'string' : self . encoderType = 'category' elif self . dataType in [ 'date' , 'datetime' ] : self . encoderType = 'date' if self . dataType is None : if self . encoderType in [ 'scalar' , 'adaptiveScalar' ] : self . dataType = 'float' elif self . encoderType in [ 'category' , 'enumeration' ] : self . dataType = 'string' elif self . encoderType in [ 'date' , 'datetime' ] : self . dataType = 'datetime'
8632	def search_projects ( session , query , search_filter = None , project_details = None , user_details = None , limit = 10 , offset = 0 , active_only = None ) : search_data = { 'query' : query , 'limit' : limit , 'offset' : offset , } if search_filter : search_data . update ( search_filter ) if project_details : search_data . update ( project_details ) if user_details : search_data . update ( user_details ) endpoint = 'projects/{}' . format ( 'active' if active_only else 'all' ) response = make_get_request ( session , endpoint , params_data = search_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
6682	def remove ( self , path , recursive = False , use_sudo = False ) : func = use_sudo and run_as_root or self . run options = '-r ' if recursive else '' func ( '/bin/rm {0}{1}' . format ( options , quote ( path ) ) )
2487	def create_conjunction_node ( self , conjunction ) : node = BNode ( ) type_triple = ( node , RDF . type , self . spdx_namespace . ConjunctiveLicenseSet ) self . graph . add ( type_triple ) licenses = self . licenses_from_tree ( conjunction ) for lic in licenses : member_triple = ( node , self . spdx_namespace . member , lic ) self . graph . add ( member_triple ) return node
13605	def url_correct ( self , point , auth = None , export = None ) : newUrl = self . __url + point + '.json' if auth or export : newUrl += "?" if auth : newUrl += ( "auth=" + auth ) if export : if not newUrl . endswith ( '?' ) : newUrl += "&" newUrl += "format=export" return newUrl
5577	def load_input_reader ( input_params , readonly = False ) : logger . debug ( "find input reader with params %s" , input_params ) if not isinstance ( input_params , dict ) : raise TypeError ( "input_params must be a dictionary" ) if "abstract" in input_params : driver_name = input_params [ "abstract" ] [ "format" ] elif "path" in input_params : if os . path . splitext ( input_params [ "path" ] ) [ 1 ] : input_file = input_params [ "path" ] driver_name = driver_from_file ( input_file ) else : logger . debug ( "%s is a directory" , input_params [ "path" ] ) driver_name = "TileDirectory" else : raise MapcheteDriverError ( "invalid input parameters %s" % input_params ) for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : driver_ = v . load ( ) if hasattr ( driver_ , "METADATA" ) and ( driver_ . METADATA [ "driver_name" ] == driver_name ) : return v . load ( ) . InputData ( input_params , readonly = readonly ) raise MapcheteDriverError ( "no loader for driver '%s' could be found." % driver_name )
4566	def euclidean ( c1 , c2 ) : diffs = ( ( i - j ) for i , j in zip ( c1 , c2 ) ) return sum ( x * x for x in diffs )
4011	def get_dusty_containers ( services , include_exited = False ) : client = get_docker_client ( ) if services : containers = [ get_container_for_app_or_service ( service , include_exited = include_exited ) for service in services ] return [ container for container in containers if container ] else : return [ container for container in client . containers ( all = include_exited ) if any ( name . startswith ( '/dusty' ) for name in container . get ( 'Names' , [ ] ) ) ]
4967	def _validate_course ( self ) : course_details = self . cleaned_data . get ( self . Fields . COURSE ) if course_details : course_mode = self . cleaned_data . get ( self . Fields . COURSE_MODE ) if not course_mode : raise ValidationError ( ValidationMessages . COURSE_WITHOUT_COURSE_MODE ) valid_course_modes = course_details [ "course_modes" ] if all ( course_mode != mode [ "slug" ] for mode in valid_course_modes ) : error = ValidationError ( ValidationMessages . COURSE_MODE_INVALID_FOR_COURSE . format ( course_mode = course_mode , course_id = course_details [ "course_id" ] , ) ) raise ValidationError ( { self . Fields . COURSE_MODE : error } )
7155	def raw ( prompt , * args , ** kwargs ) : go_back = kwargs . get ( 'go_back' , '<' ) type_ = kwargs . get ( 'type' , str ) default = kwargs . get ( 'default' , '' ) with stdout_redirected ( sys . stderr ) : while True : try : if kwargs . get ( 'secret' , False ) : answer = getpass . getpass ( prompt ) elif sys . version_info < ( 3 , 0 ) : answer = raw_input ( prompt ) else : answer = input ( prompt ) if not answer : answer = default if answer == go_back : raise QuestionnaireGoBack return type_ ( answer ) except ValueError : eprint ( '\n`{}` is not a valid `{}`\n' . format ( answer , type_ ) )
336	def run_model ( model , returns_train , returns_test = None , bmark = None , samples = 500 , ppc = False , progressbar = True ) : if model == 'alpha_beta' : model , trace = model_returns_t_alpha_beta ( returns_train , bmark , samples , progressbar = progressbar ) elif model == 't' : model , trace = model_returns_t ( returns_train , samples , progressbar = progressbar ) elif model == 'normal' : model , trace = model_returns_normal ( returns_train , samples , progressbar = progressbar ) elif model == 'best' : model , trace = model_best ( returns_train , returns_test , samples = samples , progressbar = progressbar ) else : raise NotImplementedError ( 'Model {} not found.' 'Use alpha_beta, t, normal, or best.' . format ( model ) ) if ppc : ppc_samples = pm . sample_ppc ( trace , samples = samples , model = model , size = len ( returns_test ) , progressbar = progressbar ) return trace , ppc_samples [ 'returns' ] return trace
6636	def publish ( self , registry = None ) : if ( registry is None ) or ( registry == registry_access . Registry_Base_URL ) : if 'private' in self . description and self . description [ 'private' ] : return "this %s is private and cannot be published" % ( self . description_filename . split ( '.' ) [ 0 ] ) upload_archive = os . path . join ( self . path , 'upload.tar.gz' ) fsutils . rmF ( upload_archive ) fd = os . open ( upload_archive , os . O_CREAT | os . O_EXCL | os . O_RDWR | getattr ( os , "O_BINARY" , 0 ) ) with os . fdopen ( fd , 'rb+' ) as tar_file : tar_file . truncate ( ) self . generateTarball ( tar_file ) logger . debug ( 'generated tar file of length %s' , tar_file . tell ( ) ) tar_file . seek ( 0 ) shasum = hashlib . sha256 ( ) while True : chunk = tar_file . read ( 1000 ) if not chunk : break shasum . update ( chunk ) logger . debug ( 'generated tar file has hash %s' , shasum . hexdigest ( ) ) tar_file . seek ( 0 ) with self . findAndOpenReadme ( ) as readme_file_wrapper : if not readme_file_wrapper : logger . warning ( "no readme.md file detected" ) with open ( self . getDescriptionFile ( ) , 'r' ) as description_file : return registry_access . publish ( self . getRegistryNamespace ( ) , self . getName ( ) , self . getVersion ( ) , description_file , tar_file , readme_file_wrapper . file , readme_file_wrapper . extension ( ) . lower ( ) , registry = registry )
4051	def file ( self , item , ** kwargs ) : query_string = "/{t}/{u}/items/{i}/file" . format ( u = self . library_id , t = self . library_type , i = item . upper ( ) ) return self . _build_query ( query_string , no_params = True )
12412	def write ( self , chunk , serialize = False , format = None ) : self . require_not_closed ( ) if chunk is None : return if serialize or format is not None : self . serialize ( chunk , format = format ) return if type ( chunk ) is six . binary_type : self . _length += len ( chunk ) self . _stream . write ( chunk ) elif isinstance ( chunk , six . string_types ) : encoding = self . encoding if encoding is not None : chunk = chunk . encode ( encoding ) else : raise exceptions . InvalidOperation ( 'Attempting to write textual data without an encoding.' ) self . _length += len ( chunk ) self . _stream . write ( chunk ) elif isinstance ( chunk , collections . Iterable ) : for section in chunk : self . write ( section ) else : raise exceptions . InvalidOperation ( 'Attempting to write something not recognized.' )
10318	def _microcanonical_average_spanning_cluster ( has_spanning_cluster , alpha ) : r ret = dict ( ) runs = has_spanning_cluster . size k = has_spanning_cluster . sum ( dtype = np . float ) ret [ 'spanning_cluster' ] = ( ( k + 1 ) / ( runs + 2 ) ) ret [ 'spanning_cluster_ci' ] = scipy . stats . beta . ppf ( [ alpha / 2 , 1 - alpha / 2 ] , k + 1 , runs - k + 1 ) return ret
168	def is_partly_within_image ( self , image , default = False ) : if len ( self . coords ) == 0 : return default mask = self . get_pointwise_inside_image_mask ( image ) if np . any ( mask ) : return True return len ( self . clip_out_of_image ( image ) ) > 0
2911	def _find_ancestor_from_name ( self , name ) : if self . parent is None : return None if self . parent . get_name ( ) == name : return self . parent return self . parent . _find_ancestor_from_name ( name )
7503	def _byteify ( data , ignore_dicts = False ) : if isinstance ( data , unicode ) : return data . encode ( "utf-8" ) if isinstance ( data , list ) : return [ _byteify ( item , ignore_dicts = True ) for item in data ] if isinstance ( data , dict ) and not ignore_dicts : return { _byteify ( key , ignore_dicts = True ) : _byteify ( value , ignore_dicts = True ) for key , value in data . iteritems ( ) } return data
8603	def create_user ( self , user ) : data = self . _create_user_dict ( user = user ) response = self . _perform_request ( url = '/um/users' , method = 'POST' , data = json . dumps ( data ) ) return response
3113	def _validate ( self , value ) : _LOGGER . info ( 'validate: Got type %s' , type ( value ) ) if value is not None and not isinstance ( value , client . Flow ) : raise TypeError ( 'Property {0} must be convertible to a flow ' 'instance; received: {1}.' . format ( self . _name , value ) )
738	def compute ( self , activeColumns , learn = True ) : bottomUpInput = numpy . zeros ( self . numberOfCols , dtype = dtype ) bottomUpInput [ list ( activeColumns ) ] = 1 super ( TemporalMemoryShim , self ) . compute ( bottomUpInput , enableLearn = learn , enableInference = True ) predictedState = self . getPredictedState ( ) self . predictiveCells = set ( numpy . flatnonzero ( predictedState ) )
3371	def get_solver_name ( mip = False , qp = False ) : if len ( solvers ) == 0 : raise SolverNotFound ( "no solvers installed" ) mip_order = [ "gurobi" , "cplex" , "glpk" ] lp_order = [ "glpk" , "cplex" , "gurobi" ] qp_order = [ "gurobi" , "cplex" ] if mip is False and qp is False : for solver_name in lp_order : if solver_name in solvers : return solver_name return list ( solvers ) [ 0 ] elif qp : for solver_name in qp_order : if solver_name in solvers : return solver_name raise SolverNotFound ( "no qp-capable solver found" ) else : for solver_name in mip_order : if solver_name in solvers : return solver_name raise SolverNotFound ( "no mip-capable solver found" )
5856	def create_dataset_version ( self , dataset_id ) : failure_message = "Failed to create dataset version for dataset {}" . format ( dataset_id ) number = self . _get_success_json ( self . _post_json ( routes . create_dataset_version ( dataset_id ) , data = { } , failure_message = failure_message ) ) [ 'dataset_scoped_id' ] return DatasetVersion ( number = number )
7540	def basecaller ( arrayed , mindepth_majrule , mindepth_statistical , estH , estE ) : cons = np . zeros ( arrayed . shape [ 1 ] , dtype = np . uint8 ) cons . fill ( 78 ) arr = arrayed . view ( np . uint8 ) for col in xrange ( arr . shape [ 1 ] ) : carr = arr [ : , col ] mask = carr == 45 mask += carr == 78 marr = carr [ ~ mask ] if not marr . shape [ 0 ] : cons [ col ] = 78 elif np . all ( marr == marr [ 0 ] ) : cons [ col ] = marr [ 0 ] else : counts = np . bincount ( marr ) pbase = np . argmax ( counts ) nump = counts [ pbase ] counts [ pbase ] = 0 qbase = np . argmax ( counts ) numq = counts [ qbase ] counts [ qbase ] = 0 rbase = np . argmax ( counts ) numr = counts [ rbase ] bidepth = nump + numq if bidepth < mindepth_majrule : cons [ col ] = 78 else : if bidepth > 500 : base1 = int ( 500 * ( nump / float ( bidepth ) ) ) base2 = int ( 500 * ( numq / float ( bidepth ) ) ) else : base1 = nump base2 = numq if bidepth >= mindepth_statistical : ishet , prob = get_binom ( base1 , base2 , estE , estH ) if prob < 0.95 : cons [ col ] = 78 else : if ishet : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase else : if nump == numq : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase return cons . view ( "S1" )
10386	def match_simple_metapath ( graph , node , simple_metapath ) : if 0 == len ( simple_metapath ) : yield node , else : for neighbor in graph . edges [ node ] : if graph . nodes [ neighbor ] [ FUNCTION ] == simple_metapath [ 0 ] : for path in match_simple_metapath ( graph , neighbor , simple_metapath [ 1 : ] ) : if node not in path : yield ( node , ) + path
10879	def wrap_and_calc_psf ( xpts , ypts , zpts , func , ** kwargs ) : for t in [ xpts , ypts , zpts ] : if len ( t . shape ) != 1 : raise ValueError ( 'xpts,ypts,zpts must be 1D.' ) dx = 1 if xpts [ 0 ] == 0 else 0 dy = 1 if ypts [ 0 ] == 0 else 0 xg , yg , zg = np . meshgrid ( xpts , ypts , zpts , indexing = 'ij' ) xs , ys , zs = [ pts . size for pts in [ xpts , ypts , zpts ] ] to_return = np . zeros ( [ 2 * xs - dx , 2 * ys - dy , zs ] ) up_corner_psf = func ( xg , yg , zg , ** kwargs ) to_return [ xs - dx : , ys - dy : , : ] = up_corner_psf . copy ( ) if dx == 0 : to_return [ : xs - dx , ys - dy : , : ] = up_corner_psf [ : : - 1 , : , : ] . copy ( ) else : to_return [ : xs - dx , ys - dy : , : ] = up_corner_psf [ - 1 : 0 : - 1 , : , : ] . copy ( ) if dy == 0 : to_return [ xs - dx : , : ys - dy , : ] = up_corner_psf [ : , : : - 1 , : ] . copy ( ) else : to_return [ xs - dx : , : ys - dy , : ] = up_corner_psf [ : , - 1 : 0 : - 1 , : ] . copy ( ) if ( dx == 0 ) and ( dy == 0 ) : to_return [ : xs - dx , : ys - dy , : ] = up_corner_psf [ : : - 1 , : : - 1 , : ] . copy ( ) elif ( dx == 0 ) and ( dy != 0 ) : to_return [ : xs - dx , : ys - dy , : ] = up_corner_psf [ : : - 1 , - 1 : 0 : - 1 , : ] . copy ( ) elif ( dy == 0 ) and ( dx != 0 ) : to_return [ : xs - dx , : ys - dy , : ] = up_corner_psf [ - 1 : 0 : - 1 , : : - 1 , : ] . copy ( ) else : to_return [ : xs - dx , : ys - dy , : ] = up_corner_psf [ - 1 : 0 : - 1 , - 1 : 0 : - 1 , : ] . copy ( ) return to_return
9189	def get_api_keys ( request ) : with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( ) api_keys = [ x [ 0 ] for x in cursor . fetchall ( ) ] return api_keys
11426	def record_strip_empty_volatile_subfields ( rec ) : for tag in rec . keys ( ) : for field in rec [ tag ] : field [ 0 ] [ : ] = [ subfield for subfield in field [ 0 ] if subfield [ 1 ] [ : 9 ] != "VOLATILE:" ]
3660	def _coeff_ind_from_T ( self , T ) : if self . n == 1 : return 0 for i in range ( self . n ) : if T <= self . Ts [ i + 1 ] : return i return self . n - 1
7067	def gcs_put_file ( local_file , bucketname , service_account_json = None , client = None , raiseonfail = False ) : if not client : if ( service_account_json is not None and os . path . exists ( service_account_json ) ) : client = storage . Client . from_service_account_json ( service_account_json ) else : client = storage . Client ( ) try : bucket = client . get_bucket ( bucketname ) remote_blob = bucket . blob ( local_file ) remote_blob . upload_from_filename ( local_file ) return 'gs://%s/%s' % ( bucketname , local_file . lstrip ( '/' ) ) except Exception as e : LOGEXCEPTION ( 'could not upload %s to bucket %s' % ( local_file , bucket ) ) if raiseonfail : raise return None
794	def getActiveJobCountForClientInfo ( self , clientInfo ) : with ConnectionFactory . get ( ) as conn : query = 'SELECT count(job_id) ' 'FROM %s ' 'WHERE client_info = %%s ' ' AND status != %%s' % self . jobsTableName conn . cursor . execute ( query , [ clientInfo , self . STATUS_COMPLETED ] ) activeJobCount = conn . cursor . fetchone ( ) [ 0 ] return activeJobCount
4095	def AICc ( N , rho , k , norm = True ) : r from numpy import log , array p = k res = log ( rho ) + 2. * ( p + 1 ) / ( N - p - 2 ) return res
2483	def write_document ( document , out , validate = True ) : if validate : messages = [ ] messages = document . validate ( messages ) if messages : raise InvalidDocumentError ( messages ) writer = Writer ( document , out ) writer . write ( )
12100	def _append_log ( self , specs ) : self . _spec_log += specs log_path = os . path . join ( self . root_directory , ( "%s.log" % self . batch_name ) ) core . Log . write_log ( log_path , [ spec for ( _ , spec ) in specs ] , allow_append = True )
12642	def get_config_bool ( name ) : cli_config = CLIConfig ( SF_CLI_CONFIG_DIR , SF_CLI_ENV_VAR_PREFIX ) return cli_config . getboolean ( 'servicefabric' , name , False )
11111	def synchronize ( self , verbose = False ) : if self . __path is None : return for dirPath in sorted ( list ( self . walk_directories_relative_path ( ) ) ) : realPath = os . path . join ( self . __path , dirPath ) if os . path . isdir ( realPath ) : continue if verbose : warnings . warn ( "%s directory is missing" % realPath ) keys = dirPath . split ( os . sep ) dirInfoDict = self for idx in range ( len ( keys ) - 1 ) : dirs = dict . get ( dirInfoDict , 'directories' , None ) if dirs is None : break dirInfoDict = dict . get ( dirs , keys [ idx ] , None ) if dirInfoDict is None : break if dirInfoDict is not None : dirs = dict . get ( dirInfoDict , 'directories' , None ) if dirs is not None : dict . pop ( dirs , keys [ - 1 ] , None ) for filePath in sorted ( list ( self . walk_files_relative_path ( ) ) ) : realPath = os . path . join ( self . __path , filePath ) if os . path . isfile ( realPath ) : continue if verbose : warnings . warn ( "%s file is missing" % realPath ) keys = filePath . split ( os . sep ) dirInfoDict = self for idx in range ( len ( keys ) - 1 ) : dirs = dict . get ( dirInfoDict , 'directories' , None ) if dirs is None : break dirInfoDict = dict . get ( dirs , keys [ idx ] , None ) if dirInfoDict is None : break if dirInfoDict is not None : files = dict . get ( dirInfoDict , 'files' , None ) if files is not None : dict . pop ( files , keys [ - 1 ] , None )
9976	def alter_freevars ( func , globals_ = None , ** vars ) : if globals_ is None : globals_ = func . __globals__ frees = tuple ( vars . keys ( ) ) oldlocs = func . __code__ . co_names newlocs = tuple ( name for name in oldlocs if name not in frees ) code = _alter_code ( func . __code__ , co_freevars = frees , co_names = newlocs , co_flags = func . __code__ . co_flags | inspect . CO_NESTED ) closure = _create_closure ( * vars . values ( ) ) return FunctionType ( code , globals_ , closure = closure )
5322	def get_humidity ( self , sensors = None ) : _sensors = sensors if _sensors is None : _sensors = list ( range ( 0 , self . _sensor_count ) ) if not set ( _sensors ) . issubset ( list ( range ( 0 , self . _sensor_count ) ) ) : raise ValueError ( 'Some or all of the sensors in the list %s are out of range ' 'given a sensor_count of %d. Valid range: %s' % ( _sensors , self . _sensor_count , list ( range ( 0 , self . _sensor_count ) ) , ) ) data = self . get_data ( ) data = data [ 'humidity_data' ] results = { } for sensor in _sensors : offset = self . lookup_humidity_offset ( sensor ) if offset is None : continue humidity = ( struct . unpack_from ( '>H' , data , offset ) [ 0 ] * 32 ) / 1000.0 results [ sensor ] = { 'ports' : self . get_ports ( ) , 'bus' : self . get_bus ( ) , 'sensor' : sensor , 'humidity_pc' : humidity , } return results
9541	def datetime_range_inclusive ( min , max , format ) : dmin = datetime . strptime ( min , format ) dmax = datetime . strptime ( max , format ) def checker ( v ) : dv = datetime . strptime ( v , format ) if dv < dmin or dv > dmax : raise ValueError ( v ) return checker
5269	def _label_generalized ( self , node ) : if node . is_leaf ( ) : x = { self . _get_word_start_index ( node . idx ) } else : x = { n for ns in node . transition_links for n in ns [ 0 ] . generalized_idxs } node . generalized_idxs = x
11723	def init_app ( self , app , ** kwargs ) : self . init_config ( app ) self . limiter = Limiter ( app , key_func = get_ipaddr ) if app . config [ 'APP_ENABLE_SECURE_HEADERS' ] : self . talisman = Talisman ( app , ** app . config . get ( 'APP_DEFAULT_SECURE_HEADERS' , { } ) ) if app . config [ 'APP_HEALTH_BLUEPRINT_ENABLED' ] : blueprint = Blueprint ( 'invenio_app_ping' , __name__ ) @ blueprint . route ( '/ping' ) def ping ( ) : return 'OK' ping . talisman_view_options = { 'force_https' : False } app . register_blueprint ( blueprint ) requestid_header = app . config . get ( 'APP_REQUESTID_HEADER' ) if requestid_header : @ app . before_request def set_request_id ( ) : request_id = request . headers . get ( requestid_header ) if request_id : g . request_id = request_id [ : 200 ] try : from flask_debugtoolbar import DebugToolbarExtension app . extensions [ 'flask-debugtoolbar' ] = DebugToolbarExtension ( app ) except ImportError : app . logger . debug ( 'Flask-DebugToolbar extension not installed.' ) app . extensions [ 'invenio-app' ] = self
6502	def strings_in_dictionary ( dictionary ) : strings = [ value for value in six . itervalues ( dictionary ) if not isinstance ( value , dict ) ] for child_dict in [ dv for dv in six . itervalues ( dictionary ) if isinstance ( dv , dict ) ] : strings . extend ( SearchResultProcessor . strings_in_dictionary ( child_dict ) ) return strings
2954	def initialize ( self , containers ) : self . _containers = deepcopy ( containers ) self . __write ( containers , initialize = True )
2159	def _format_yaml ( self , payload ) : return parser . ordered_dump ( payload , Dumper = yaml . SafeDumper , default_flow_style = False )
11039	def read ( self , path , ** params ) : d = self . request ( 'GET' , '/v1/' + path , params = params ) return d . addCallback ( self . _handle_response )
12716	def position_rates ( self ) : return [ self . ode_obj . getPositionRate ( i ) for i in range ( self . LDOF ) ]
1400	def extract_logical_plan ( self , topology ) : logicalPlan = { "spouts" : { } , "bolts" : { } , } for spout in topology . spouts ( ) : spoutName = spout . comp . name spoutType = "default" spoutSource = "NA" spoutVersion = "NA" spoutConfigs = spout . comp . config . kvs for kvs in spoutConfigs : if kvs . key == "spout.type" : spoutType = javaobj . loads ( kvs . serialized_value ) elif kvs . key == "spout.source" : spoutSource = javaobj . loads ( kvs . serialized_value ) elif kvs . key == "spout.version" : spoutVersion = javaobj . loads ( kvs . serialized_value ) spoutPlan = { "config" : convert_pb_kvs ( spoutConfigs , include_non_primitives = False ) , "type" : spoutType , "source" : spoutSource , "version" : spoutVersion , "outputs" : [ ] } for outputStream in list ( spout . outputs ) : spoutPlan [ "outputs" ] . append ( { "stream_name" : outputStream . stream . id } ) logicalPlan [ "spouts" ] [ spoutName ] = spoutPlan for bolt in topology . bolts ( ) : boltName = bolt . comp . name boltPlan = { "config" : convert_pb_kvs ( bolt . comp . config . kvs , include_non_primitives = False ) , "outputs" : [ ] , "inputs" : [ ] } for outputStream in list ( bolt . outputs ) : boltPlan [ "outputs" ] . append ( { "stream_name" : outputStream . stream . id } ) for inputStream in list ( bolt . inputs ) : boltPlan [ "inputs" ] . append ( { "stream_name" : inputStream . stream . id , "component_name" : inputStream . stream . component_name , "grouping" : topology_pb2 . Grouping . Name ( inputStream . gtype ) } ) logicalPlan [ "bolts" ] [ boltName ] = boltPlan return logicalPlan
7527	def align_and_parse ( handle , max_internal_indels = 5 , is_gbs = False ) : try : with open ( handle , 'rb' ) as infile : clusts = infile . read ( ) . split ( "//\n//\n" ) clusts = [ i for i in clusts if i ] if not clusts : raise IPyradError except ( IOError , IPyradError ) : LOGGER . debug ( "skipping empty chunk - {}" . format ( handle ) ) return 0 highindels = 0 try : aligned = persistent_popen_align3 ( clusts , 200 , is_gbs ) except Exception as inst : LOGGER . debug ( "Error in handle - {} - {}" . format ( handle , inst ) ) aligned = [ ] refined = [ ] for clust in aligned : filtered = aligned_indel_filter ( clust , max_internal_indels ) if not filtered : refined . append ( clust ) else : highindels += 1 if refined : outhandle = handle . rsplit ( "." , 1 ) [ 0 ] + ".aligned" with open ( outhandle , 'wb' ) as outfile : outfile . write ( "\n//\n//\n" . join ( refined ) + "\n" ) log_level = logging . getLevelName ( LOGGER . getEffectiveLevel ( ) ) if not log_level == "DEBUG" : os . remove ( handle ) return highindels
13876	def CopyFilesX ( file_mapping ) : files = [ ] for i_target_path , i_source_path_mask in file_mapping : tree_recurse , flat_recurse , dirname , in_filters , out_filters = ExtendedPathMask . Split ( i_source_path_mask ) _AssertIsLocal ( dirname ) filenames = FindFiles ( dirname , in_filters , out_filters , tree_recurse ) for i_source_filename in filenames : if os . path . isdir ( i_source_filename ) : continue i_target_filename = i_source_filename [ len ( dirname ) + 1 : ] if flat_recurse : i_target_filename = os . path . basename ( i_target_filename ) i_target_filename = os . path . join ( i_target_path , i_target_filename ) files . append ( ( StandardizePath ( i_source_filename ) , StandardizePath ( i_target_filename ) ) ) for i_source_filename , i_target_filename in files : target_dir = os . path . dirname ( i_target_filename ) CreateDirectory ( target_dir ) CopyFile ( i_source_filename , i_target_filename ) return files
7001	def parallel_pf ( lclist , outdir , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , pfmethods = ( 'gls' , 'pdm' , 'mav' , 'win' ) , pfkwargs = ( { } , { } , { } , { } ) , sigclip = 10.0 , getblssnr = False , nperiodworkers = NCPUS , ncontrolworkers = 1 , liststartindex = None , listmaxobjects = None , minobservations = 500 , excludeprocessed = True ) : if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if ( liststartindex is not None ) and ( listmaxobjects is None ) : lclist = lclist [ liststartindex : ] elif ( liststartindex is None ) and ( listmaxobjects is not None ) : lclist = lclist [ : listmaxobjects ] elif ( liststartindex is not None ) and ( listmaxobjects is not None ) : lclist = lclist [ liststartindex : liststartindex + listmaxobjects ] tasklist = [ ( x , outdir , timecols , magcols , errcols , lcformat , lcformatdir , pfmethods , pfkwargs , getblssnr , sigclip , nperiodworkers , minobservations , excludeprocessed ) for x in lclist ] with ProcessPoolExecutor ( max_workers = ncontrolworkers ) as executor : resultfutures = executor . map ( _runpf_worker , tasklist ) results = [ x for x in resultfutures ] return results
8962	def freeze ( ctx , local = False ) : cmd = 'pip --disable-pip-version-check freeze{}' . format ( ' --local' if local else '' ) frozen = ctx . run ( cmd , hide = 'out' ) . stdout . replace ( '\x1b' , '#' ) with io . open ( 'frozen-requirements.txt' , 'w' , encoding = 'ascii' ) as out : out . write ( "# Requirements frozen by 'pip freeze' on {}\n" . format ( isodate ( ) ) ) out . write ( frozen ) notify . info ( "Frozen {} requirements." . format ( len ( frozen . splitlines ( ) ) , ) )
1236	def get_named_tensor ( self , name ) : if name in self . named_tensors : return True , self . named_tensors [ name ] else : return False , None
4247	def id_by_addr ( self , addr ) : if self . _databaseType in ( const . PROXY_EDITION , const . NETSPEED_EDITION_REV1 , const . NETSPEED_EDITION_REV1_V6 ) : raise GeoIPError ( 'Invalid database type; this database is not supported' ) ipv = 6 if addr . find ( ':' ) >= 0 else 4 if ipv == 4 and self . _databaseType not in ( const . COUNTRY_EDITION , const . NETSPEED_EDITION ) : raise GeoIPError ( 'Invalid database type; this database supports IPv6 addresses, not IPv4' ) if ipv == 6 and self . _databaseType != const . COUNTRY_EDITION_V6 : raise GeoIPError ( 'Invalid database type; this database supports IPv4 addresses, not IPv6' ) ipnum = util . ip2long ( addr ) return self . _seek_country ( ipnum ) - const . COUNTRY_BEGIN
12736	def are_connected ( self , body_a , body_b ) : return bool ( ode . areConnected ( self . get_body ( body_a ) . ode_body , self . get_body ( body_b ) . ode_body ) )
1231	def tf_import_experience ( self , states , internals , actions , terminal , reward ) : return self . memory . store ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
7345	async def call_on_response ( self , data ) : since_id = self . kwargs . get ( self . param , 0 ) + 1 if self . fill_gaps : if data [ - 1 ] [ 'id' ] != since_id : max_id = data [ - 1 ] [ 'id' ] - 1 responses = with_max_id ( self . request ( ** self . kwargs , max_id = max_id ) ) async for tweets in responses : data . extend ( tweets ) if data [ - 1 ] [ 'id' ] == self . last_id : data = data [ : - 1 ] if not data and not self . force : raise StopAsyncIteration await self . set_param ( data )
3715	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : Vms = [ i ( T , P ) for i in self . VolumeSolids ] return mixing_simple ( zs , Vms ) else : raise Exception ( 'Method not valid' )
12744	def get_internal_urls ( self ) : internal_urls = self . get_subfields ( "856" , "u" , i1 = "4" , i2 = "0" ) internal_urls . extend ( self . get_subfields ( "998" , "a" ) ) internal_urls . extend ( self . get_subfields ( "URL" , "u" ) ) return map ( lambda x : x . replace ( "&amp;" , "&" ) , internal_urls )
10695	def yiq_to_rgb ( yiq ) : y , i , q = yiq r = y + ( 0.956 * i ) + ( 0.621 * q ) g = y - ( 0.272 * i ) - ( 0.647 * q ) b = y - ( 1.108 * i ) + ( 1.705 * q ) r = 1 if r > 1 else max ( 0 , r ) g = 1 if g > 1 else max ( 0 , g ) b = 1 if b > 1 else max ( 0 , b ) return round ( r * 255 , 3 ) , round ( g * 255 , 3 ) , round ( b * 255 , 3 )
4680	def getAccountsFromPublicKey ( self , pub ) : names = self . rpc . get_key_references ( [ str ( pub ) ] ) [ 0 ] for name in names : yield name
7767	def _stream_authenticated ( self , event ) : with self . lock : if event . stream != self . stream : return self . me = event . stream . me self . peer = event . stream . peer handlers = self . _base_handlers [ : ] handlers += self . handlers + [ self ] self . setup_stanza_handlers ( handlers , "post-auth" )
8379	def copy ( self , graph ) : e = events ( graph , self . _ctx ) e . clicked = self . clicked return e
11405	def record_drop_duplicate_fields ( record ) : out = { } position = 0 tags = sorted ( record . keys ( ) ) for tag in tags : fields = record [ tag ] out [ tag ] = [ ] current_fields = set ( ) for full_field in fields : field = ( tuple ( full_field [ 0 ] ) , ) + full_field [ 1 : 4 ] if field not in current_fields : current_fields . add ( field ) position += 1 out [ tag ] . append ( full_field [ : 4 ] + ( position , ) ) return out
6665	def verify_certificate_chain ( self , base = None , crt = None , csr = None , key = None ) : from burlap . common import get_verbose , print_fail , print_success r = self . local_renderer if base : crt = base + '.crt' csr = base + '.csr' key = base + '.key' else : assert crt and csr and key , 'If base not provided, crt and csr and key must be given.' assert os . path . isfile ( crt ) assert os . path . isfile ( csr ) assert os . path . isfile ( key ) csr_md5 = r . local ( 'openssl req -noout -modulus -in %s | openssl md5' % csr , capture = True ) key_md5 = r . local ( 'openssl rsa -noout -modulus -in %s | openssl md5' % key , capture = True ) crt_md5 = r . local ( 'openssl x509 -noout -modulus -in %s | openssl md5' % crt , capture = True ) match = crt_md5 == csr_md5 == key_md5 if self . verbose or not match : print ( 'crt:' , crt_md5 ) print ( 'csr:' , csr_md5 ) print ( 'key:' , key_md5 ) if match : print_success ( 'Files look good!' ) else : print_fail ( 'Files no not match!' ) raise Exception ( 'Files no not match!' )
13558	def get_all_images_count ( self ) : self_imgs = self . image_set . count ( ) update_ids = self . update_set . values_list ( 'id' , flat = True ) u_images = UpdateImage . objects . filter ( update__id__in = update_ids ) . count ( ) count = self_imgs + u_images return count
9415	def from_value ( cls , value ) : instance = OctaveUserClass . __new__ ( cls ) instance . _address = '%s_%s' % ( instance . _name , id ( instance ) ) instance . _ref ( ) . push ( instance . _address , value ) return instance
13633	def _negotiateHandler ( self , request ) : accept = _parseAccept ( request . requestHeaders . getRawHeaders ( 'Accept' ) ) for contentType in accept . keys ( ) : handler = self . _acceptHandlers . get ( contentType . lower ( ) ) if handler is not None : return handler , handler . contentType if self . _fallback : handler = self . _handlers [ 0 ] return handler , handler . contentType return NotAcceptable ( ) , None
342	def create_distributed_session ( task_spec = None , checkpoint_dir = None , scaffold = None , hooks = None , chief_only_hooks = None , save_checkpoint_secs = 600 , save_summaries_steps = object ( ) , save_summaries_secs = object ( ) , config = None , stop_grace_period_secs = 120 , log_step_count_steps = 100 ) : target = task_spec . target ( ) if task_spec is not None else None is_chief = task_spec . is_master ( ) if task_spec is not None else True return tf . train . MonitoredTrainingSession ( master = target , is_chief = is_chief , checkpoint_dir = checkpoint_dir , scaffold = scaffold , save_checkpoint_secs = save_checkpoint_secs , save_summaries_steps = save_summaries_steps , save_summaries_secs = save_summaries_secs , log_step_count_steps = log_step_count_steps , stop_grace_period_secs = stop_grace_period_secs , config = config , hooks = hooks , chief_only_hooks = chief_only_hooks )
12099	def get_root_directory ( self , timestamp = None ) : if timestamp is None : timestamp = self . timestamp if self . timestamp_format is not None : root_name = ( time . strftime ( self . timestamp_format , timestamp ) + '-' + self . batch_name ) else : root_name = self . batch_name path = os . path . join ( self . output_directory , * ( self . subdir + [ root_name ] ) ) return os . path . abspath ( path )
8759	def get_subnets ( context , limit = None , page_reverse = False , sorts = [ 'id' ] , marker = None , filters = None , fields = None ) : LOG . info ( "get_subnets for tenant %s with filters %s fields %s" % ( context . tenant_id , filters , fields ) ) filters = filters or { } subnets = db_api . subnet_find ( context , limit = limit , page_reverse = page_reverse , sorts = sorts , marker_obj = marker , join_dns = True , join_routes = True , join_pool = True , ** filters ) for subnet in subnets : cache = subnet . get ( "_allocation_pool_cache" ) if not cache : db_api . subnet_update_set_alloc_pool_cache ( context , subnet , subnet . allocation_pools ) return v . _make_subnets_list ( subnets , fields = fields )
2353	def wait_for_region_to_load ( self ) : self . wait . until ( lambda _ : self . loaded ) self . pm . hook . pypom_after_wait_for_region_to_load ( region = self ) return self
6042	def sparse_to_unmasked_sparse ( self ) : return mapping_util . sparse_to_unmasked_sparse_from_mask_and_pixel_centres ( total_sparse_pixels = self . total_sparse_pixels , mask = self . regular_grid . mask , unmasked_sparse_grid_pixel_centres = self . unmasked_sparse_grid_pixel_centres ) . astype ( 'int' )
6645	def _mergeDictionaries ( * args ) : result = type ( args [ 0 ] ) ( ) for k , v in itertools . chain ( * [ x . items ( ) for x in args ] ) : if not k in result : result [ k ] = v elif isinstance ( result [ k ] , dict ) and isinstance ( v , dict ) : result [ k ] = _mergeDictionaries ( result [ k ] , v ) return result
1367	def start_connect ( self ) : Log . debug ( "In start_connect() of %s" % self . _get_classname ( ) ) self . create_socket ( socket . AF_INET , socket . SOCK_STREAM ) self . _connecting = True self . connect ( self . endpoint )
5485	def _eval_arg_type ( arg_type , T = Any , arg = None , sig = None ) : try : T = eval ( arg_type ) except Exception as e : raise ValueError ( 'The type of {0} could not be evaluated in {1} for {2}: {3}' . format ( arg_type , arg , sig , text_type ( e ) ) ) else : if type ( T ) not in ( type , Type ) : raise TypeError ( '{0} is not a valid type in {1} for {2}' . format ( repr ( T ) , arg , sig ) ) return T
10040	def deposit_fetcher ( record_uuid , data ) : return FetchedPID ( provider = DepositProvider , pid_type = DepositProvider . pid_type , pid_value = str ( data [ '_deposit' ] [ 'id' ] ) , )
11664	def start ( self , total ) : self . logger . info ( json . dumps ( [ 'START' , self . name , total ] ) )
6824	def restart ( self ) : n = 60 sleep_n = int ( self . env . max_restart_wait_minutes / 10. * 60 ) for _ in xrange ( n ) : self . stop ( ) if self . dryrun or not self . is_running ( ) : break print ( 'Waiting for supervisor to stop (%i of %i)...' % ( _ , n ) ) time . sleep ( sleep_n ) self . start ( ) for _ in xrange ( n ) : if self . dryrun or self . is_running ( ) : return print ( 'Waiting for supervisor to start (%i of %i)...' % ( _ , n ) ) time . sleep ( sleep_n ) raise Exception ( 'Failed to restart service %s!' % self . name )
1938	def get_func_argument_types ( self , hsh : bytes ) : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) sig = self . _function_signatures_by_selector . get ( hsh ) return '()' if sig is None else sig [ sig . find ( '(' ) : ]
3720	def ionic_strength ( mis , zis ) : r return 0.5 * sum ( [ mi * zi * zi for mi , zi in zip ( mis , zis ) ] )
8805	def build_full_day_ips ( query , period_start , period_end ) : ip_list = query . filter ( models . IPAddress . version == 4L ) . filter ( models . IPAddress . network_id == PUBLIC_NETWORK_ID ) . filter ( models . IPAddress . used_by_tenant_id is not None ) . filter ( models . IPAddress . allocated_at != null ( ) ) . filter ( models . IPAddress . allocated_at < period_start ) . filter ( or_ ( models . IPAddress . _deallocated is False , models . IPAddress . deallocated_at == null ( ) , models . IPAddress . deallocated_at >= period_end ) ) . all ( ) return ip_list
11014	def set_real_value_class ( self ) : if self . value_class is not None and isinstance ( self . value_class , str ) : module_name , dot , class_name = self . value_class . rpartition ( "." ) module = __import__ ( module_name , fromlist = [ class_name ] ) self . value_class = getattr ( module , class_name ) self . _initialized = True
11858	def make_factor ( var , e , bn ) : node = bn . variable_node ( var ) vars = [ X for X in [ var ] + node . parents if X not in e ] cpt = dict ( ( event_values ( e1 , vars ) , node . p ( e1 [ var ] , e1 ) ) for e1 in all_events ( vars , bn , e ) ) return Factor ( vars , cpt )
2552	def attr ( * args , ** kwargs ) : ctx = dom_tag . _with_contexts [ _get_thread_context ( ) ] if ctx and ctx [ - 1 ] : dicts = args + ( kwargs , ) for d in dicts : for attr , value in d . items ( ) : ctx [ - 1 ] . tag . set_attribute ( * dom_tag . clean_pair ( attr , value ) ) else : raise ValueError ( 'not in a tag context' )
13860	def is_date_type ( cls ) : if not isinstance ( cls , type ) : return False return issubclass ( cls , date ) and not issubclass ( cls , datetime )
6519	def is_excluded_dir ( self , path ) : if self . is_excluded ( path ) : return True return matches_masks ( path . name , ALWAYS_EXCLUDED_DIRS )
11431	def _compare_fields ( field1 , field2 , strict = True ) : if strict : return field1 [ : 4 ] == field2 [ : 4 ] else : if field1 [ 1 : 4 ] != field2 [ 1 : 4 ] : return False else : return set ( field1 [ 0 ] ) == set ( field2 [ 0 ] )
12676	def _escape_char ( c , escape_char = ESCAPE_CHAR ) : buf = [ ] for byte in c . encode ( 'utf8' ) : buf . append ( escape_char ) buf . append ( '%X' % _ord ( byte ) ) return '' . join ( buf )
11298	def get_all_text ( node ) : if node . nodeType == node . TEXT_NODE : return node . data else : text_string = "" for child_node in node . childNodes : text_string += get_all_text ( child_node ) return text_string
3769	def none_and_length_check ( all_inputs , length = None ) : r if not length : length = len ( all_inputs [ 0 ] ) for things in all_inputs : if None in things or len ( things ) != length : return False return True
12626	def recursive_find_search ( folder_path , regex = '' ) : outlist = [ ] for root , dirs , files in os . walk ( folder_path ) : outlist . extend ( [ op . join ( root , f ) for f in files if re . search ( regex , f ) ] ) return outlist
11228	def before ( self , dt , inc = False ) : if self . _cache_complete : gen = self . _cache else : gen = self last = None if inc : for i in gen : if i > dt : break last = i else : for i in gen : if i >= dt : break last = i return last
12861	def add_months ( self , month_int ) : month_int += self . month while month_int > 12 : self = BusinessDate . add_years ( self , 1 ) month_int -= 12 while month_int < 1 : self = BusinessDate . add_years ( self , - 1 ) month_int += 12 l = monthrange ( self . year , month_int ) [ 1 ] return BusinessDate . from_ymd ( self . year , month_int , min ( l , self . day ) )
5257	def block_to_fork ( block_number ) : forks_by_block = { 0 : "frontier" , 1150000 : "homestead" , 2463000 : "tangerine_whistle" , 2675000 : "spurious_dragon" , 4370000 : "byzantium" , 7280000 : "petersburg" , 9999999 : "serenity" } fork_names = list ( forks_by_block . values ( ) ) fork_blocks = list ( forks_by_block . keys ( ) ) return fork_names [ bisect ( fork_blocks , block_number ) - 1 ]
10982	def locate_spheres ( image , feature_rad , dofilter = False , order = ( 3 , 3 , 3 ) , trim_edge = True , ** kwargs ) : m = models . SmoothFieldModel ( ) I = ilms . LegendrePoly2P1D ( order = order , constval = image . get_image ( ) . mean ( ) ) s = states . ImageState ( image , [ I ] , pad = 0 , mdl = m ) if dofilter : opt . do_levmarq ( s , s . params ) pos = addsub . feature_guess ( s , feature_rad , trim_edge = trim_edge , ** kwargs ) [ 0 ] return pos
3292	def set_property_value ( self , name , value , dry_run = False ) : assert value is None or xml_tools . is_etree_element ( value ) if name in _lockPropertyNames : raise DAVError ( HTTP_FORBIDDEN , err_condition = PRECONDITION_CODE_ProtectedProperty ) config = self . environ [ "wsgidav.config" ] mutableLiveProps = config . get ( "mutable_live_props" , [ ] ) if ( name . startswith ( "{DAV:}" ) and name in _standardLivePropNames and name in mutableLiveProps ) : if name in ( "{DAV:}getlastmodified" , "{DAV:}last_modified" ) : try : return self . set_last_modified ( self . path , value . text , dry_run ) except Exception : _logger . warning ( "Provider does not support set_last_modified on {}." . format ( self . path ) ) raise DAVError ( HTTP_FORBIDDEN ) if name . startswith ( "{urn:schemas-microsoft-com:}" ) : agent = self . environ . get ( "HTTP_USER_AGENT" , "None" ) win32_emu = config . get ( "hotfixes" , { } ) . get ( "emulate_win32_lastmod" , False ) if win32_emu and "MiniRedir/6.1" not in agent : if "Win32LastModifiedTime" in name : return self . set_last_modified ( self . path , value . text , dry_run ) elif "Win32FileAttributes" in name : return True elif "Win32CreationTime" in name : return True elif "Win32LastAccessTime" in name : return True pm = self . provider . prop_manager if pm and not name . startswith ( "{DAV:}" ) : refUrl = self . get_ref_url ( ) if value is None : return pm . remove_property ( refUrl , name , dry_run , self . environ ) else : value = etree . tostring ( value ) return pm . write_property ( refUrl , name , value , dry_run , self . environ ) raise DAVError ( HTTP_FORBIDDEN )
11049	def _handle_field_value ( self , field , value ) : if field == 'event' : self . _event = value elif field == 'data' : self . _data_lines . append ( value ) elif field == 'id' : pass elif field == 'retry' : pass
9252	def generate_unreleased_section ( self ) : if not self . filtered_tags : return "" now = datetime . datetime . utcnow ( ) now = now . replace ( tzinfo = dateutil . tz . tzutc ( ) ) head_tag = { "name" : self . options . unreleased_label } self . tag_times_dict [ head_tag [ "name" ] ] = now unreleased_log = self . generate_log_between_tags ( self . filtered_tags [ 0 ] , head_tag ) return unreleased_log
6495	def log_indexing_error ( cls , indexing_errors ) : indexing_errors_log = [ ] for indexing_error in indexing_errors : indexing_errors_log . append ( str ( indexing_error ) ) raise exceptions . ElasticsearchException ( ', ' . join ( indexing_errors_log ) )
12052	def getNotesForABF ( abfFile ) : parent = getParent ( abfFile ) parent = os . path . basename ( parent ) . replace ( ".abf" , "" ) expFile = os . path . dirname ( abfFile ) + "/experiment.txt" if not os . path . exists ( expFile ) : return "no experiment file" with open ( expFile ) as f : raw = f . readlines ( ) for line in raw : if line [ 0 ] == '~' : line = line [ 1 : ] . strip ( ) if line . startswith ( parent ) : while "\t\t" in line : line = line . replace ( "\t\t" , "\t" ) line = line . replace ( "\t" , "\n" ) return line return "experiment.txt found, but didn't contain %s" % parent
7827	def stream_element_handler ( element_name , usage_restriction = None ) : def decorator ( func ) : func . _pyxmpp_stream_element_handled = element_name func . _pyxmpp_usage_restriction = usage_restriction return func return decorator
7924	def is_ipv6_available ( ) : try : socket . socket ( socket . AF_INET6 ) . close ( ) except ( socket . error , AttributeError ) : return False return True
134	def extract_from_image ( self , image ) : ia . do_assert ( image . ndim in [ 2 , 3 ] ) if len ( self . exterior ) <= 2 : raise Exception ( "Polygon must be made up of at least 3 points to extract its area from an image." ) bb = self . to_bounding_box ( ) bb_area = bb . extract_from_image ( image ) if self . is_out_of_image ( image , fully = True , partly = False ) : return bb_area xx = self . xx_int yy = self . yy_int xx_mask = xx - np . min ( xx ) yy_mask = yy - np . min ( yy ) height_mask = np . max ( yy_mask ) width_mask = np . max ( xx_mask ) rr_face , cc_face = skimage . draw . polygon ( yy_mask , xx_mask , shape = ( height_mask , width_mask ) ) mask = np . zeros ( ( height_mask , width_mask ) , dtype = np . bool ) mask [ rr_face , cc_face ] = True if image . ndim == 3 : mask = np . tile ( mask [ : , : , np . newaxis ] , ( 1 , 1 , image . shape [ 2 ] ) ) return bb_area * mask
2822	def convert_relu ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting relu ...' ) if names == 'short' : tf_name = 'RELU' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) relu = keras . layers . Activation ( 'relu' , name = tf_name ) layers [ scope_name ] = relu ( layers [ inputs [ 0 ] ] )
834	def run ( self ) : print "-" * 80 + "Computing the SDR" + "-" * 80 self . sp . compute ( self . inputArray , True , self . activeArray ) print self . activeArray . nonzero ( )
3478	def build_reaction_from_string ( self , reaction_str , verbose = True , fwd_arrow = None , rev_arrow = None , reversible_arrow = None , term_split = "+" ) : forward_arrow_finder = _forward_arrow_finder if fwd_arrow is None else re . compile ( re . escape ( fwd_arrow ) ) reverse_arrow_finder = _reverse_arrow_finder if rev_arrow is None else re . compile ( re . escape ( rev_arrow ) ) reversible_arrow_finder = _reversible_arrow_finder if reversible_arrow is None else re . compile ( re . escape ( reversible_arrow ) ) if self . _model is None : warn ( "no model found" ) model = None else : model = self . _model found_compartments = compartment_finder . findall ( reaction_str ) if len ( found_compartments ) == 1 : compartment = found_compartments [ 0 ] reaction_str = compartment_finder . sub ( "" , reaction_str ) else : compartment = "" arrow_match = reversible_arrow_finder . search ( reaction_str ) if arrow_match is not None : self . lower_bound = - 1000 self . upper_bound = 1000 else : arrow_match = forward_arrow_finder . search ( reaction_str ) if arrow_match is not None : self . upper_bound = 1000 self . lower_bound = 0 else : arrow_match = reverse_arrow_finder . search ( reaction_str ) if arrow_match is None : raise ValueError ( "no suitable arrow found in '%s'" % reaction_str ) else : self . upper_bound = 0 self . lower_bound = - 1000 reactant_str = reaction_str [ : arrow_match . start ( ) ] . strip ( ) product_str = reaction_str [ arrow_match . end ( ) : ] . strip ( ) self . subtract_metabolites ( self . metabolites , combine = True ) for substr , factor in ( ( reactant_str , - 1 ) , ( product_str , 1 ) ) : if len ( substr ) == 0 : continue for term in substr . split ( term_split ) : term = term . strip ( ) if term . lower ( ) == "nothing" : continue if " " in term : num_str , met_id = term . split ( ) num = float ( num_str . lstrip ( "(" ) . rstrip ( ")" ) ) * factor else : met_id = term num = factor met_id += compartment try : met = model . metabolites . get_by_id ( met_id ) except KeyError : if verbose : print ( "unknown metabolite '%s' created" % met_id ) met = Metabolite ( met_id ) self . add_metabolites ( { met : num } )
12168	def _dispatch_function ( self , event , listener , * args , ** kwargs ) : try : return listener ( * args , ** kwargs ) except Exception as exc : if event == self . LISTENER_ERROR_EVENT : raise return self . emit ( self . LISTENER_ERROR_EVENT , event , listener , exc )
13582	def format_to_csv ( filename , skiprows = 0 , delimiter = "" ) : if not delimiter : delimiter = "\t" input_file = open ( filename , "r" ) if skiprows : [ input_file . readline ( ) for _ in range ( skiprows ) ] new_filename = os . path . splitext ( filename ) [ 0 ] + ".csv" output_file = open ( new_filename , "w" ) header = input_file . readline ( ) . split ( ) reader = csv . DictReader ( input_file , fieldnames = header , delimiter = delimiter ) writer = csv . DictWriter ( output_file , fieldnames = header , delimiter = "," ) writer . writerow ( dict ( ( x , x ) for x in header ) ) for line in reader : if None in line : del line [ None ] writer . writerow ( line ) input_file . close ( ) output_file . close ( ) print "Saved %s." % new_filename
6295	def index_buffer ( self , buffer , index_element_size = 4 ) : if not type ( buffer ) in [ moderngl . Buffer , numpy . ndarray , bytes ] : raise VAOError ( "buffer parameter must be a moderngl.Buffer, numpy.ndarray or bytes instance" ) if isinstance ( buffer , numpy . ndarray ) : buffer = self . ctx . buffer ( buffer . tobytes ( ) ) if isinstance ( buffer , bytes ) : buffer = self . ctx . buffer ( data = buffer ) self . _index_buffer = buffer self . _index_element_size = index_element_size
9400	def _feval ( self , func_name , func_args = ( ) , dname = '' , nout = 0 , timeout = None , stream_handler = None , store_as = '' , plot_dir = None ) : engine = self . _engine if engine is None : raise Oct2PyError ( 'Session is closed' ) out_file = osp . join ( self . temp_dir , 'writer.mat' ) out_file = out_file . replace ( osp . sep , '/' ) in_file = osp . join ( self . temp_dir , 'reader.mat' ) in_file = in_file . replace ( osp . sep , '/' ) func_args = list ( func_args ) ref_indices = [ ] for ( i , value ) in enumerate ( func_args ) : if isinstance ( value , OctavePtr ) : ref_indices . append ( i + 1 ) func_args [ i ] = value . address ref_indices = np . array ( ref_indices ) req = dict ( func_name = func_name , func_args = tuple ( func_args ) , dname = dname or '' , nout = nout , store_as = store_as or '' , ref_indices = ref_indices ) write_file ( req , out_file , oned_as = self . _oned_as , convert_to_float = self . convert_to_float ) engine . stream_handler = stream_handler or self . logger . info if timeout is None : timeout = self . timeout try : engine . eval ( '_pyeval("%s", "%s");' % ( out_file , in_file ) , timeout = timeout ) except KeyboardInterrupt as e : stream_handler ( engine . repl . interrupt ( ) ) raise except TIMEOUT : stream_handler ( engine . repl . interrupt ( ) ) raise Oct2PyError ( 'Timed out, interrupting' ) except EOF : stream_handler ( engine . repl . child . before ) self . restart ( ) raise Oct2PyError ( 'Session died, restarting' ) resp = read_file ( in_file , self ) if resp [ 'err' ] : msg = self . _parse_error ( resp [ 'err' ] ) raise Oct2PyError ( msg ) result = resp [ 'result' ] . ravel ( ) . tolist ( ) if isinstance ( result , list ) and len ( result ) == 1 : result = result [ 0 ] if ( isinstance ( result , Cell ) and result . size == 1 and isinstance ( result [ 0 ] , string_types ) and result [ 0 ] == '__no_value__' ) : result = None if plot_dir : self . _engine . make_figures ( plot_dir ) return result
1411	def filter_spouts ( table , header ) : spouts_info = [ ] for row in table : if row [ 0 ] == 'spout' : spouts_info . append ( row ) return spouts_info , header
13526	def to_message ( self ) : from . messages import ack return ack . Acknowledgement ( self . code , self . args [ 0 ] if len ( self . args ) > 0 else '' )
8014	async def send_upstream ( self , message , stream_name = None ) : if stream_name is None : for steam_queue in self . application_streams . values ( ) : await steam_queue . put ( message ) return steam_queue = self . application_streams . get ( stream_name ) if steam_queue is None : raise ValueError ( "Invalid multiplexed frame received (stream not mapped)" ) await steam_queue . put ( message )
1896	def _is_sat ( self ) -> bool : logger . debug ( "Solver.check() " ) start = time . time ( ) self . _send ( '(check-sat)' ) status = self . _recv ( ) logger . debug ( "Check took %s seconds (%s)" , time . time ( ) - start , status ) if status not in ( 'sat' , 'unsat' , 'unknown' ) : raise SolverError ( status ) if consider_unknown_as_unsat : if status == 'unknown' : logger . info ( 'Found an unknown core, probably a solver timeout' ) status = 'unsat' if status == 'unknown' : raise SolverUnknown ( status ) return status == 'sat'
1048	def print_tb ( tb , limit = None , file = None ) : if file is None : file = sys . stderr if limit is None : if hasattr ( sys , 'tracebacklimit' ) : limit = sys . tracebacklimit n = 0 while tb is not None and ( limit is None or n < limit ) : f = tb . tb_frame lineno = tb . tb_lineno co = f . f_code filename = co . co_filename name = co . co_name _print ( file , ' File "%s", line %d, in %s' % ( filename , lineno , name ) ) linecache . checkcache ( filename ) line = linecache . getline ( filename , lineno , f . f_globals ) if line : _print ( file , ' ' + line . strip ( ) ) tb = tb . tb_next n = n + 1
3561	def advertised ( self ) : uuids = [ ] try : uuids = self . _props . Get ( _INTERFACE , 'UUIDs' ) except dbus . exceptions . DBusException as ex : if ex . get_dbus_name ( ) != 'org.freedesktop.DBus.Error.InvalidArgs' : raise ex return [ uuid . UUID ( str ( x ) ) for x in uuids ]
31	def adjust_shape ( placeholder , data ) : if not isinstance ( data , np . ndarray ) and not isinstance ( data , list ) : return data if isinstance ( data , list ) : data = np . array ( data ) placeholder_shape = [ x or - 1 for x in placeholder . shape . as_list ( ) ] assert _check_shape ( placeholder_shape , data . shape ) , 'Shape of data {} is not compatible with shape of the placeholder {}' . format ( data . shape , placeholder_shape ) return np . reshape ( data , placeholder_shape )
6838	def distrib_id ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : kernel = ( run ( 'uname -s' ) or '' ) . strip ( ) . lower ( ) if kernel == LINUX : if is_file ( '/usr/bin/lsb_release' ) : id_ = run ( 'lsb_release --id --short' ) . strip ( ) . lower ( ) if id in [ 'arch' , 'archlinux' ] : id_ = ARCH return id_ else : if is_file ( '/etc/debian_version' ) : return DEBIAN elif is_file ( '/etc/fedora-release' ) : return FEDORA elif is_file ( '/etc/arch-release' ) : return ARCH elif is_file ( '/etc/redhat-release' ) : release = run ( 'cat /etc/redhat-release' ) if release . startswith ( 'Red Hat Enterprise Linux' ) : return REDHAT elif release . startswith ( 'CentOS' ) : return CENTOS elif release . startswith ( 'Scientific Linux' ) : return SLES elif is_file ( '/etc/gentoo-release' ) : return GENTOO elif kernel == SUNOS : return SUNOS
13802	def _auth ( self , client_id , key , method , callback ) : available = auth_methods . keys ( ) if method not in available : raise Proauth2Error ( 'invalid_request' , 'unsupported authentication method: %s' 'available methods: %s' % ( method , '\n' . join ( available ) ) ) client = yield Task ( self . data_store . fetch , 'applications' , client_id = client_id ) if not client : raise Proauth2Error ( 'access_denied' ) if not auth_methods [ method ] ( key , client [ 'client_secret' ] ) : raise Proauth2Error ( 'access_denied' ) callback ( )
12164	def add_listener ( self , event , listener ) : self . emit ( 'new_listener' , event , listener ) self . _listeners [ event ] . append ( listener ) self . _check_limit ( event ) return self
10353	def write_boilerplate ( name : str , version : Optional [ str ] = None , description : Optional [ str ] = None , authors : Optional [ str ] = None , contact : Optional [ str ] = None , copyright : Optional [ str ] = None , licenses : Optional [ str ] = None , disclaimer : Optional [ str ] = None , namespace_url : Optional [ Mapping [ str , str ] ] = None , namespace_patterns : Optional [ Mapping [ str , str ] ] = None , annotation_url : Optional [ Mapping [ str , str ] ] = None , annotation_patterns : Optional [ Mapping [ str , str ] ] = None , annotation_list : Optional [ Mapping [ str , Set [ str ] ] ] = None , pmids : Optional [ Iterable [ Union [ str , int ] ] ] = None , entrez_ids : Optional [ Iterable [ Union [ str , int ] ] ] = None , file : Optional [ TextIO ] = None , ) -> None : lines = make_knowledge_header ( name = name , version = version or '1.0.0' , description = description , authors = authors , contact = contact , copyright = copyright , licenses = licenses , disclaimer = disclaimer , namespace_url = namespace_url , namespace_patterns = namespace_patterns , annotation_url = annotation_url , annotation_patterns = annotation_patterns , annotation_list = annotation_list , ) for line in lines : print ( line , file = file ) if pmids is not None : for line in make_pubmed_abstract_group ( pmids ) : print ( line , file = file ) if entrez_ids is not None : for line in make_pubmed_gene_group ( entrez_ids ) : print ( line , file = file )
704	def _okToExit ( self ) : print >> sys . stderr , "reporter:status:In hypersearchV2: _okToExit" if not self . _jobCancelled : ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( matured = False ) if len ( modelIds ) > 0 : self . logger . info ( "Ready to end hyperseach, but not all models have " "matured yet. Sleeping a bit to wait for all models " "to mature." ) time . sleep ( 5.0 * random . random ( ) ) return False ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( completed = False ) for modelId in modelIds : self . logger . info ( "Stopping model %d because the search has ended" % ( modelId ) ) self . _cjDAO . modelSetFields ( modelId , dict ( engStop = ClientJobsDAO . STOP_REASON_STOPPED ) , ignoreUnchanged = True ) self . _hsStatePeriodicUpdate ( ) pctFieldContributions , absFieldContributions = self . _hsState . getFieldContributions ( ) jobResultsStr = self . _cjDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] if jobResultsStr is not None : jobResults = json . loads ( jobResultsStr ) else : jobResults = { } if pctFieldContributions != jobResults . get ( 'fieldContributions' , None ) : jobResults [ 'fieldContributions' ] = pctFieldContributions jobResults [ 'absoluteFieldContributions' ] = absFieldContributions isUpdated = self . _cjDAO . jobSetFieldIfEqual ( self . _jobID , fieldName = 'results' , curValue = jobResultsStr , newValue = json . dumps ( jobResults ) ) if isUpdated : self . logger . info ( 'Successfully updated the field contributions:%s' , pctFieldContributions ) else : self . logger . info ( 'Failed updating the field contributions, ' 'another hypersearch worker must have updated it' ) return True
7354	def predict_peptides ( self , peptides ) : from mhcflurry . encodable_sequences import EncodableSequences binding_predictions = [ ] encodable_sequences = EncodableSequences . create ( peptides ) for allele in self . alleles : predictions_df = self . predictor . predict_to_dataframe ( encodable_sequences , allele = allele ) for ( _ , row ) in predictions_df . iterrows ( ) : binding_prediction = BindingPrediction ( allele = allele , peptide = row . peptide , affinity = row . prediction , percentile_rank = ( row . prediction_percentile if 'prediction_percentile' in row else nan ) , prediction_method_name = "mhcflurry" ) binding_predictions . append ( binding_prediction ) return BindingPredictionCollection ( binding_predictions )
10099	def snippets ( self , timeout = None ) : return self . _api_request ( self . SNIPPETS_ENDPOINT , self . HTTP_GET , timeout = timeout )
12921	def refetch ( self ) : if len ( self ) == 0 : return IRQueryableList ( ) mdl = self . getModel ( ) pks = [ item . _id for item in self if item . _id ] return mdl . objects . getMultiple ( pks )
11346	def handle_endtag ( self , tag ) : if tag in self . mathml_elements : self . fed . append ( "</{0}>" . format ( tag ) )
8115	def distance ( x0 , y0 , x1 , y1 ) : return sqrt ( pow ( x1 - x0 , 2 ) + pow ( y1 - y0 , 2 ) )
4918	def course_detail ( self , request , pk , course_key ) : enterprise_customer_catalog = self . get_object ( ) course = enterprise_customer_catalog . get_course ( course_key ) if not course : raise Http404 context = self . get_serializer_context ( ) context [ 'enterprise_customer_catalog' ] = enterprise_customer_catalog serializer = serializers . CourseDetailSerializer ( course , context = context ) return Response ( serializer . data )
11983	async def upload_file ( self , bucket , file , uploadpath = None , key = None , ContentType = None , ** kw ) : is_filename = False if hasattr ( file , 'read' ) : if hasattr ( file , 'seek' ) : file . seek ( 0 ) file = file . read ( ) size = len ( file ) elif key : size = len ( file ) else : is_filename = True size = os . stat ( file ) . st_size key = os . path . basename ( file ) assert key , 'key not available' if not ContentType : ContentType , _ = mimetypes . guess_type ( key ) if uploadpath : if not uploadpath . endswith ( '/' ) : uploadpath = '%s/' % uploadpath key = '%s%s' % ( uploadpath , key ) params = dict ( Bucket = bucket , Key = key ) if not ContentType : ContentType = 'application/octet-stream' params [ 'ContentType' ] = ContentType if size > MULTI_PART_SIZE and is_filename : resp = await _multipart ( self , file , params ) elif is_filename : with open ( file , 'rb' ) as fp : params [ 'Body' ] = fp . read ( ) resp = await self . put_object ( ** params ) else : params [ 'Body' ] = file resp = await self . put_object ( ** params ) if 'Key' not in resp : resp [ 'Key' ] = key if 'Bucket' not in resp : resp [ 'Bucket' ] = bucket return resp
10135	def dump ( grids , mode = MODE_ZINC ) : if isinstance ( grids , Grid ) : return dump_grid ( grids , mode = mode ) _dump = functools . partial ( dump_grid , mode = mode ) if mode == MODE_ZINC : return '\n' . join ( map ( _dump , grids ) ) elif mode == MODE_JSON : return '[%s]' % ',' . join ( map ( _dump , grids ) ) else : raise NotImplementedError ( 'Format not implemented: %s' % mode )
1834	def JECXZ ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . ECX == 0 , target . read ( ) , cpu . PC )
11040	def write ( self , path , ** data ) : d = self . request ( 'PUT' , '/v1/' + path , json = data ) return d . addCallback ( self . _handle_response , check_cas = True )
1750	def mappings ( self ) : result = [ ] for m in self . maps : if isinstance ( m , AnonMap ) : result . append ( ( m . start , m . end , m . perms , 0 , '' ) ) elif isinstance ( m , FileMap ) : result . append ( ( m . start , m . end , m . perms , m . _offset , m . _filename ) ) else : result . append ( ( m . start , m . end , m . perms , 0 , m . name ) ) return sorted ( result )
869	def clear ( cls , persistent = False ) : if persistent : try : os . unlink ( cls . getPath ( ) ) except OSError , e : if e . errno != errno . ENOENT : _getLogger ( ) . exception ( "Error %s while trying to remove dynamic " "configuration file: %s" , e . errno , cls . getPath ( ) ) raise cls . _path = None
12032	def average ( self , t1 = 0 , t2 = None , setsweep = False ) : if setsweep : self . setsweep ( setsweep ) if t2 is None or t2 > self . sweepLength : t2 = self . sweepLength self . log . debug ( "resetting t2 to [%f]" , t2 ) t1 = max ( t1 , 0 ) if t1 > t2 : self . log . error ( "t1 cannot be larger than t2" ) return False I1 , I2 = int ( t1 * self . pointsPerSec ) , int ( t2 * self . pointsPerSec ) if I1 == I2 : return np . nan return np . average ( self . sweepY [ I1 : I2 ] )
8717	def file_compile ( self , path ) : log . info ( 'Compile ' + path ) cmd = 'node.compile("%s")' % path res = self . __exchange ( cmd ) log . info ( res ) return res
8636	def get_milestone_by_id ( session , milestone_id , user_details = None ) : endpoint = 'milestones/{}' . format ( milestone_id ) response = make_get_request ( session , endpoint , params_data = user_details ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MilestonesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
5608	def bounds_to_ranges ( out_bounds = None , in_affine = None , in_shape = None ) : return itertools . chain ( * from_bounds ( * out_bounds , transform = in_affine , height = in_shape [ - 2 ] , width = in_shape [ - 1 ] ) . round_lengths ( pixel_precision = 0 ) . round_offsets ( pixel_precision = 0 ) . toranges ( ) )
8800	def run_migrations_offline ( ) : context . configure ( url = neutron_config . database . connection ) with context . begin_transaction ( ) : context . run_migrations ( )
3195	def delete ( self , list_id , subscriber_hash ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash return self . _mc_client . _delete ( url = self . _build_path ( list_id , 'members' , subscriber_hash ) )
9781	def get ( ctx ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) try : response = PolyaxonClient ( ) . build_job . get_build ( user , project_name , _build ) cache . cache ( config_manager = BuildJobManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get build job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_build_details ( response )
8067	def loadGrammar ( self , grammar , searchpaths = None ) : self . grammar = self . _load ( grammar , searchpaths = searchpaths ) self . refs = { } for ref in self . grammar . getElementsByTagName ( "ref" ) : self . refs [ ref . attributes [ "id" ] . value ] = ref
3607	def put ( self , url , name , data , params = None , headers = None , connection = None ) : assert name , 'Snapshot name must be specified' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) data = json . dumps ( data , cls = JSONEncoder ) return make_put_request ( endpoint , data , params , headers , connection = connection )
2531	def parse_doc_fields ( self , doc_term ) : try : self . builder . set_doc_spdx_id ( self . doc , doc_term ) except SPDXValueError : self . value_error ( 'DOC_SPDX_ID_VALUE' , doc_term ) try : if doc_term . count ( '#' , 0 , len ( doc_term ) ) <= 1 : doc_namespace = doc_term . split ( '#' ) [ 0 ] self . builder . set_doc_namespace ( self . doc , doc_namespace ) else : self . value_error ( 'DOC_NAMESPACE_VALUE' , doc_term ) except SPDXValueError : self . value_error ( 'DOC_NAMESPACE_VALUE' , doc_term ) for _s , _p , o in self . graph . triples ( ( doc_term , self . spdx_namespace [ 'specVersion' ] , None ) ) : try : self . builder . set_doc_version ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'DOC_VERS_VALUE' , o ) except CardinalityError : self . more_than_one_error ( 'specVersion' ) break for _s , _p , o in self . graph . triples ( ( doc_term , self . spdx_namespace [ 'dataLicense' ] , None ) ) : try : self . builder . set_doc_data_lic ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'DOC_D_LICS' , o ) except CardinalityError : self . more_than_one_error ( 'dataLicense' ) break for _s , _p , o in self . graph . triples ( ( doc_term , self . spdx_namespace [ 'name' ] , None ) ) : try : self . builder . set_doc_name ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'name' ) break for _s , _p , o in self . graph . triples ( ( doc_term , RDFS . comment , None ) ) : try : self . builder . set_doc_comment ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'Document comment' ) break
9674	def get_days_span ( self , month_index ) : is_first_month = month_index == 0 is_last_month = month_index == self . __len__ ( ) - 1 y = int ( self . start_date . year + ( self . start_date . month + month_index ) / 13 ) m = int ( ( self . start_date . month + month_index ) % 12 or 12 ) total = calendar . monthrange ( y , m ) [ 1 ] if is_first_month and is_last_month : return ( self . end_date - self . start_date ) . days + 1 else : if is_first_month : return total - self . start_date . day + 1 elif is_last_month : return self . end_date . day else : return total
7094	def init_options ( self ) : self . options = GoogleMapOptions ( ) d = self . declaration self . set_map_type ( d . map_type ) if d . ambient_mode : self . set_ambient_mode ( d . ambient_mode ) if ( d . camera_position or d . camera_zoom or d . camera_tilt or d . camera_bearing ) : self . update_camera ( ) if d . map_bounds : self . set_map_bounds ( d . map_bounds ) if not d . show_compass : self . set_show_compass ( d . show_compass ) if not d . show_zoom_controls : self . set_show_zoom_controls ( d . show_zoom_controls ) if not d . show_toolbar : self . set_show_toolbar ( d . show_toolbar ) if d . lite_mode : self . set_lite_mode ( d . lite_mode ) if not d . rotate_gestures : self . set_rotate_gestures ( d . rotate_gestures ) if not d . scroll_gestures : self . set_scroll_gestures ( d . scroll_gestures ) if not d . tilt_gestures : self . set_tilt_gestures ( d . tilt_gestures ) if not d . zoom_gestures : self . set_zoom_gestures ( d . zoom_gestures ) if d . min_zoom : self . set_min_zoom ( d . min_zoom ) if d . max_zoom : self . set_max_zoom ( d . max_zoom )
12906	def objHasUnsavedChanges ( self ) : if not self . obj : return False return self . obj . hasUnsavedChanges ( cascadeObjects = True )
5434	def parse_tasks_file_header ( header , input_file_param_util , output_file_param_util ) : job_params = [ ] for col in header : col_type = '--env' col_value = col if col . startswith ( '-' ) : col_type , col_value = split_pair ( col , ' ' , 1 ) if col_type == '--env' : job_params . append ( job_model . EnvParam ( col_value ) ) elif col_type == '--label' : job_params . append ( job_model . LabelParam ( col_value ) ) elif col_type == '--input' or col_type == '--input-recursive' : name = input_file_param_util . get_variable_name ( col_value ) job_params . append ( job_model . InputFileParam ( name , recursive = ( col_type . endswith ( 'recursive' ) ) ) ) elif col_type == '--output' or col_type == '--output-recursive' : name = output_file_param_util . get_variable_name ( col_value ) job_params . append ( job_model . OutputFileParam ( name , recursive = ( col_type . endswith ( 'recursive' ) ) ) ) else : raise ValueError ( 'Unrecognized column header: %s' % col ) return job_params
8236	def left_complement ( clr ) : left = split_complementary ( clr ) [ 1 ] colors = complementary ( clr ) colors [ 3 ] . h = left . h colors [ 4 ] . h = left . h colors [ 5 ] . h = left . h colors = colorlist ( colors [ 0 ] , colors [ 2 ] , colors [ 1 ] , colors [ 3 ] , colors [ 4 ] , colors [ 5 ] ) return colors
9196	def includeme ( config ) : global cache_manager settings = config . registry . settings cache_manager = CacheManager ( ** parse_cache_config_options ( settings ) )
10202	def register_aggregations ( ) : return [ dict ( aggregation_name = 'file-download-agg' , templates = 'invenio_stats.contrib.aggregations.aggr_file_download' , aggregator_class = StatAggregator , aggregator_config = dict ( client = current_search_client , event = 'file-download' , aggregation_field = 'unique_id' , aggregation_interval = 'day' , copy_fields = dict ( file_key = 'file_key' , bucket_id = 'bucket_id' , file_id = 'file_id' , ) , metric_aggregation_fields = { 'unique_count' : ( 'cardinality' , 'unique_session_id' , { 'precision_threshold' : 1000 } ) , 'volume' : ( 'sum' , 'size' , { } ) , } , ) ) , dict ( aggregation_name = 'record-view-agg' , templates = 'invenio_stats.contrib.aggregations.aggr_record_view' , aggregator_class = StatAggregator , aggregator_config = dict ( client = current_search_client , event = 'record-view' , aggregation_field = 'unique_id' , aggregation_interval = 'day' , copy_fields = dict ( record_id = 'record_id' , pid_type = 'pid_type' , pid_value = 'pid_value' , ) , metric_aggregation_fields = { 'unique_count' : ( 'cardinality' , 'unique_session_id' , { 'precision_threshold' : 1000 } ) , } , ) ) ]
2953	def container_id ( self , name ) : container = self . _containers . get ( name , None ) if not container is None : return container . get ( 'id' , None ) return None
11482	def _upload_folder_recursive ( local_folder , parent_folder_id , leaf_folders_as_items = False , reuse_existing = False ) : if leaf_folders_as_items and _has_only_files ( local_folder ) : print ( 'Creating item from {0}' . format ( local_folder ) ) _upload_folder_as_item ( local_folder , parent_folder_id , reuse_existing ) return else : print ( 'Creating folder from {0}' . format ( local_folder ) ) new_folder_id = _create_or_reuse_folder ( local_folder , parent_folder_id , reuse_existing ) for entry in sorted ( os . listdir ( local_folder ) ) : full_entry = os . path . join ( local_folder , entry ) if os . path . islink ( full_entry ) : continue elif os . path . isdir ( full_entry ) : _upload_folder_recursive ( full_entry , new_folder_id , leaf_folders_as_items , reuse_existing ) else : print ( 'Uploading item from {0}' . format ( full_entry ) ) _upload_as_item ( entry , new_folder_id , full_entry , reuse_existing )
10953	def set_model ( self , mdl ) : self . mdl = mdl self . mdl . check_inputs ( self . comps ) for c in self . comps : setattr ( self , '_comp_' + c . category , c )
446	def batch_with_dynamic_pad ( images_and_captions , batch_size , queue_capacity , add_summaries = True ) : enqueue_list = [ ] for image , caption in images_and_captions : caption_length = tf . shape ( caption ) [ 0 ] input_length = tf . expand_dims ( tf . subtract ( caption_length , 1 ) , 0 ) input_seq = tf . slice ( caption , [ 0 ] , input_length ) target_seq = tf . slice ( caption , [ 1 ] , input_length ) indicator = tf . ones ( input_length , dtype = tf . int32 ) enqueue_list . append ( [ image , input_seq , target_seq , indicator ] ) images , input_seqs , target_seqs , mask = tf . train . batch_join ( enqueue_list , batch_size = batch_size , capacity = queue_capacity , dynamic_pad = True , name = "batch_and_pad" ) if add_summaries : lengths = tf . add ( tf . reduce_sum ( mask , 1 ) , 1 ) tf . summary . scalar ( "caption_length/batch_min" , tf . reduce_min ( lengths ) ) tf . summary . scalar ( "caption_length/batch_max" , tf . reduce_max ( lengths ) ) tf . summary . scalar ( "caption_length/batch_mean" , tf . reduce_mean ( lengths ) ) return images , input_seqs , target_seqs , mask
9325	def _validate_server ( self ) : if not self . _title : msg = "No 'title' in Server Discovery for request '{}'" raise ValidationError ( msg . format ( self . url ) )
10922	def do_levmarq_n_directions ( s , directions , max_iter = 2 , run_length = 2 , damping = 1e-3 , collect_stats = False , marquardt_damping = True , ** kwargs ) : normals = np . array ( [ d / np . sqrt ( np . dot ( d , d ) ) for d in directions ] ) if np . isnan ( normals ) . any ( ) : raise ValueError ( '`directions` must not be 0s or contain nan' ) obj = OptState ( s , normals ) lo = LMOptObj ( obj , max_iter = max_iter , run_length = run_length , damping = damping , marquardt_damping = marquardt_damping , ** kwargs ) lo . do_run_1 ( ) if collect_stats : return lo . get_termination_stats ( )
7823	def _final_challenge ( self , challenge ) : if self . _finished : return Failure ( "extra-challenge" ) match = SERVER_FINAL_MESSAGE_RE . match ( challenge ) if not match : logger . debug ( "Bad final message syntax: {0!r}" . format ( challenge ) ) return Failure ( "bad-challenge" ) error = match . group ( "error" ) if error : logger . debug ( "Server returned SCRAM error: {0!r}" . format ( error ) ) return Failure ( u"scram-" + error . decode ( "utf-8" ) ) verifier = match . group ( "verifier" ) if not verifier : logger . debug ( "No verifier value in the final message" ) return Failure ( "bad-succes" ) server_key = self . HMAC ( self . _salted_password , b"Server Key" ) server_signature = self . HMAC ( server_key , self . _auth_message ) if server_signature != a2b_base64 ( verifier ) : logger . debug ( "Server verifier does not match" ) return Failure ( "bad-succes" ) self . _finished = True return Response ( None )
8588	def start_server ( self , datacenter_id , server_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/start' % ( datacenter_id , server_id ) , method = 'POST-ACTION' ) return response
8562	def list_loadbalancers ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
11016	def deploy ( context ) : config = context . obj header ( 'Generating HTML...' ) pelican ( config , '--verbose' , production = True ) header ( 'Removing unnecessary output...' ) unnecessary_paths = [ 'author' , 'category' , 'tag' , 'feeds' , 'tags.html' , 'authors.html' , 'categories.html' , 'archives.html' , ] for path in unnecessary_paths : remove_path ( os . path . join ( config [ 'OUTPUT_DIR' ] , path ) ) if os . environ . get ( 'TRAVIS' ) : header ( 'Setting up Git...' ) run ( 'git config user.name ' + run ( 'git show --format="%cN" -s' , capture = True ) ) run ( 'git config user.email ' + run ( 'git show --format="%cE" -s' , capture = True ) ) github_token = os . environ . get ( 'GITHUB_TOKEN' ) repo_slug = os . environ . get ( 'TRAVIS_REPO_SLUG' ) origin = 'https://{}@github.com/{}.git' . format ( github_token , repo_slug ) run ( 'git remote set-url origin ' + origin ) header ( 'Rewriting gh-pages branch...' ) run ( 'ghp-import -m "{message}" {dir}' . format ( message = 'Deploying {}' . format ( choose_commit_emoji ( ) ) , dir = config [ 'OUTPUT_DIR' ] , ) ) header ( 'Pushing to GitHub...' ) run ( 'git push origin gh-pages:gh-pages --force' )
125	def Positive ( other_param , mode = "invert" , reroll_count_max = 2 ) : return ForceSign ( other_param = other_param , positive = True , mode = mode , reroll_count_max = reroll_count_max )
3527	def piwik ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return PiwikNode ( )
2736	def create ( self , * args , ** kwargs ) : data = self . get_data ( 'floating_ips/' , type = POST , params = { 'droplet_id' : self . droplet_id } ) if data : self . ip = data [ 'floating_ip' ] [ 'ip' ] self . region = data [ 'floating_ip' ] [ 'region' ] return self
9369	def legal_ogrn ( ) : ogrn = "" . join ( map ( str , [ random . randint ( 1 , 9 ) for _ in range ( 12 ) ] ) ) ogrn += str ( ( int ( ogrn ) % 11 % 10 ) ) return ogrn
12308	def get_files_to_commit ( autooptions ) : workingdir = autooptions [ 'working-directory' ] includes = autooptions [ 'track' ] [ 'includes' ] excludes = autooptions [ 'track' ] [ 'excludes' ] includes = r'|' . join ( [ fnmatch . translate ( x ) for x in includes ] ) excludes = r'|' . join ( [ fnmatch . translate ( x ) for x in excludes ] ) or r'$.' matched_files = [ ] for root , dirs , files in os . walk ( workingdir ) : dirs [ : ] = [ d for d in dirs if not re . match ( excludes , d ) ] files = [ f for f in files if not re . match ( excludes , f ) ] files = [ f for f in files if re . match ( includes , f ) ] files = [ os . path . join ( root , f ) for f in files ] matched_files . extend ( files ) return matched_files
836	def _removeRows ( self , rowsToRemove ) : removalArray = numpy . array ( rowsToRemove ) self . _categoryList = numpy . delete ( numpy . array ( self . _categoryList ) , removalArray ) . tolist ( ) if self . fixedCapacity : self . _categoryRecencyList = numpy . delete ( numpy . array ( self . _categoryRecencyList ) , removalArray ) . tolist ( ) for row in reversed ( rowsToRemove ) : self . _partitionIdList . pop ( row ) self . _rebuildPartitionIdMap ( self . _partitionIdList ) if self . useSparseMemory : for rowIndex in rowsToRemove [ : : - 1 ] : self . _Memory . deleteRow ( rowIndex ) else : self . _M = numpy . delete ( self . _M , removalArray , 0 ) numRemoved = len ( rowsToRemove ) numRowsExpected = self . _numPatterns - numRemoved if self . useSparseMemory : if self . _Memory is not None : assert self . _Memory . nRows ( ) == numRowsExpected else : assert self . _M . shape [ 0 ] == numRowsExpected assert len ( self . _categoryList ) == numRowsExpected self . _numPatterns -= numRemoved return numRemoved
11954	def upload_gif ( gif ) : client_id = os . environ . get ( 'IMGUR_API_ID' ) client_secret = os . environ . get ( 'IMGUR_API_SECRET' ) if client_id is None or client_secret is None : click . echo ( 'Cannot upload - could not find IMGUR_API_ID or IMGUR_API_SECRET environment variables' ) return client = ImgurClient ( client_id , client_secret ) click . echo ( 'Uploading file {}' . format ( click . format_filename ( gif ) ) ) response = client . upload_from_path ( gif ) click . echo ( 'File uploaded - see your gif at {}' . format ( response [ 'link' ] ) )
11504	def delete_folder ( self , token , folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = folder_id response = self . request ( 'midas.folder.delete' , parameters ) return response
5090	def export_as_csv_action ( description = "Export selected objects as CSV file" , fields = None , header = True ) : def export_as_csv ( modeladmin , request , queryset ) : opts = modeladmin . model . _meta if not fields : field_names = [ field . name for field in opts . fields ] else : field_names = fields response = HttpResponse ( content_type = "text/csv" ) response [ "Content-Disposition" ] = "attachment; filename={filename}.csv" . format ( filename = str ( opts ) . replace ( "." , "_" ) ) writer = unicodecsv . writer ( response , encoding = "utf-8" ) if header : writer . writerow ( field_names ) for obj in queryset : row = [ ] for field_name in field_names : field = getattr ( obj , field_name ) if callable ( field ) : value = field ( ) else : value = field if value is None : row . append ( "[Not Set]" ) elif not value and isinstance ( value , string_types ) : row . append ( "[Empty]" ) else : row . append ( value ) writer . writerow ( row ) return response export_as_csv . short_description = description return export_as_csv
7656	def validate ( self , strict = True ) : valid = True try : jsonschema . validate ( self . __json__ , self . __schema__ ) except jsonschema . ValidationError as invalid : if strict : raise SchemaError ( str ( invalid ) ) else : warnings . warn ( str ( invalid ) ) valid = False return valid
6135	def _fix_docs ( this_abc , child_class ) : if sys . version_info >= ( 3 , 5 ) : return child_class if not issubclass ( child_class , this_abc ) : raise KappaError ( 'Cannot fix docs of class that is not decendent.' ) for name , child_func in vars ( child_class ) . items ( ) : if callable ( child_func ) and not child_func . __doc__ : if name in this_abc . __abstractmethods__ : parent_func = getattr ( this_abc , name ) child_func . __doc__ = parent_func . __doc__ return child_class
5878	def get_video ( self , node ) : video = Video ( ) video . _embed_code = self . get_embed_code ( node ) video . _embed_type = self . get_embed_type ( node ) video . _width = self . get_width ( node ) video . _height = self . get_height ( node ) video . _src = self . get_src ( node ) video . _provider = self . get_provider ( video . src ) return video
7617	def coerce_annotation ( ann , namespace ) : ann = convert ( ann , namespace ) ann . validate ( strict = True ) return ann
12350	def get ( self , id ) : info = self . _get_droplet_info ( id ) return DropletActions ( self . api , self , ** info )
13686	def embed_data ( request ) : result = _EmbedDataFixture ( request ) result . delete_data_dir ( ) result . create_data_dir ( ) yield result result . delete_data_dir ( )
8552	def update_image ( self , image_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/images/' + image_id , method = 'PATCH' , data = json . dumps ( data ) ) return response
13713	def run ( self ) : self . log . debug ( 'consumer is running...' ) self . running = True while self . running : self . upload ( ) self . log . debug ( 'consumer exited.' )
8595	def create_group ( self , group ) : data = json . dumps ( self . _create_group_dict ( group ) ) response = self . _perform_request ( url = '/um/groups' , method = 'POST' , data = data ) return response
6320	def create_entrypoint ( self ) : with open ( os . path . join ( self . template_dir , 'manage.py' ) , 'r' ) as fd : data = fd . read ( ) . format ( project_name = self . project_name ) with open ( 'manage.py' , 'w' ) as fd : fd . write ( data ) os . chmod ( 'manage.py' , 0o777 )
6613	def receive_finished ( self ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return return self . dropbox . poll ( )
8762	def _perform_async_update_rule ( context , id , db_sg_group , rule_id , action ) : rpc_reply = None sg_rpc = sg_rpc_api . QuarkSGAsyncProcessClient ( ) ports = db_api . sg_gather_associated_ports ( context , db_sg_group ) if len ( ports ) > 0 : rpc_reply = sg_rpc . start_update ( context , id , rule_id , action ) if rpc_reply : job_id = rpc_reply [ 'job_id' ] job_api . add_job_to_context ( context , job_id ) else : LOG . error ( "Async update failed. Is the worker running?" )
11940	def mark_read ( user , message ) : BackendClass = stored_messages_settings . STORAGE_BACKEND backend = BackendClass ( ) backend . inbox_delete ( user , message )
7517	def snpcount_numba ( superints , snpsarr ) : for iloc in xrange ( superints . shape [ 0 ] ) : for site in xrange ( superints . shape [ 2 ] ) : catg = np . zeros ( 4 , dtype = np . int16 ) ncol = superints [ iloc , : , site ] for idx in range ( ncol . shape [ 0 ] ) : if ncol [ idx ] == 67 : catg [ 0 ] += 1 elif ncol [ idx ] == 65 : catg [ 1 ] += 1 elif ncol [ idx ] == 84 : catg [ 2 ] += 1 elif ncol [ idx ] == 71 : catg [ 3 ] += 1 elif ncol [ idx ] == 82 : catg [ 1 ] += 1 catg [ 3 ] += 1 elif ncol [ idx ] == 75 : catg [ 2 ] += 1 catg [ 3 ] += 1 elif ncol [ idx ] == 83 : catg [ 0 ] += 1 catg [ 3 ] += 1 elif ncol [ idx ] == 89 : catg [ 0 ] += 1 catg [ 2 ] += 1 elif ncol [ idx ] == 87 : catg [ 1 ] += 1 catg [ 2 ] += 1 elif ncol [ idx ] == 77 : catg [ 0 ] += 1 catg [ 1 ] += 1 catg . sort ( ) if not catg [ 2 ] : pass else : if catg [ 2 ] > 1 : snpsarr [ iloc , site , 1 ] = True else : snpsarr [ iloc , site , 0 ] = True return snpsarr
11474	def login ( email = None , password = None , api_key = None , application = 'Default' , url = None , verify_ssl_certificate = True ) : try : input_ = raw_input except NameError : input_ = input if url is None : url = input_ ( 'Server URL: ' ) url = url . rstrip ( '/' ) if session . communicator is None : session . communicator = Communicator ( url ) else : session . communicator . url = url session . communicator . verify_ssl_certificate = verify_ssl_certificate if email is None : email = input_ ( 'Email: ' ) session . email = email if api_key is None : if password is None : password = getpass . getpass ( ) session . api_key = session . communicator . get_default_api_key ( session . email , password ) session . application = 'Default' else : session . api_key = api_key session . application = application return renew_token ( )
12192	def _instruction_list ( self , filters ) : return '\n\n' . join ( [ self . INSTRUCTIONS . strip ( ) , '*Supported methods:*' , 'If you send "@{}: help" to me I reply with these ' 'instructions.' . format ( self . user ) , 'If you send "@{}: version" to me I reply with my current ' 'version.' . format ( self . user ) , ] + [ filter . description ( ) for filter in filters ] )
2005	def _serialize_uint ( value , size = 32 , padding = 0 ) : if size <= 0 or size > 32 : raise ValueError from . account import EVMAccount if not isinstance ( value , ( int , BitVec , EVMAccount ) ) : raise ValueError if issymbolic ( value ) : bytes = ArrayVariable ( index_bits = 256 , index_max = 32 , value_bits = 8 , name = 'temp{}' . format ( uuid . uuid1 ( ) ) ) if value . size <= size * 8 : value = Operators . ZEXTEND ( value , size * 8 ) else : value = Operators . EXTRACT ( value , 0 , size * 8 ) bytes = ArrayProxy ( bytes . write_BE ( padding , value , size ) ) else : value = int ( value ) bytes = bytearray ( ) for _ in range ( padding ) : bytes . append ( 0 ) for position in reversed ( range ( size ) ) : bytes . append ( Operators . EXTRACT ( value , position * 8 , 8 ) ) assert len ( bytes ) == size + padding return bytes
7820	def flush ( self , dispatch = True ) : if dispatch : while True : event = self . dispatch ( False ) if event in ( None , QUIT ) : return event else : while True : try : self . queue . get ( False ) except Queue . Empty : return None
2257	def allsame ( iterable , eq = operator . eq ) : iter_ = iter ( iterable ) try : first = next ( iter_ ) except StopIteration : return True return all ( eq ( first , item ) for item in iter_ )
8164	def inheritFromContext ( self , ignore = ( ) ) : for canvas_attr , grob_attr in STATES . items ( ) : if canvas_attr in ignore : continue setattr ( self , grob_attr , getattr ( self . _bot . _canvas , canvas_attr ) )
1437	def update_reduced_metric ( self , name , value , key = None ) : if name not in self . metrics : Log . error ( "In update_reduced_metric(): %s is not registered in the metric" , name ) if key is None and isinstance ( self . metrics [ name ] , ReducedMetric ) : self . metrics [ name ] . update ( value ) elif key is not None and isinstance ( self . metrics [ name ] , MultiReducedMetric ) : self . metrics [ name ] . update ( key , value ) else : Log . error ( "In update_count(): %s is registered but not supported with this method" , name )
1190	def filter ( names , pat ) : import os result = [ ] try : re_pat = _cache [ pat ] except KeyError : res = translate ( pat ) if len ( _cache ) >= _MAXCACHE : globals ( ) [ '_cache' ] = { } _cache [ pat ] = re_pat = re . compile ( res ) match = re_pat . match if 1 : for name in names : if match ( name ) : result . append ( name ) else : for name in names : if match ( os . path . normcase ( name ) ) : result . append ( name ) return result
4817	def _dictfetchall ( self , cursor ) : columns = [ col [ 0 ] for col in cursor . description ] return [ dict ( zip ( columns , row ) ) for row in cursor . fetchall ( ) ]
6470	def consume_line ( self , line ) : data = RE_VALUE_KEY . split ( line . strip ( ) , 1 ) if len ( data ) == 1 : return float ( data [ 0 ] ) , None else : return float ( data [ 0 ] ) , data [ 1 ] . strip ( )
2229	def hash_file ( fpath , blocksize = 65536 , stride = 1 , hasher = NoParam , hashlen = NoParam , base = NoParam ) : base = _rectify_base ( base ) hashlen = _rectify_hashlen ( hashlen ) hasher = _rectify_hasher ( hasher ) ( ) with open ( fpath , 'rb' ) as file : buf = file . read ( blocksize ) if stride > 1 : while len ( buf ) > 0 : hasher . update ( buf ) file . seek ( blocksize * ( stride - 1 ) , 1 ) buf = file . read ( blocksize ) else : while len ( buf ) > 0 : hasher . update ( buf ) buf = file . read ( blocksize ) text = _digest_hasher ( hasher , hashlen , base ) return text
5048	def delete ( self , request , customer_uuid ) : enterprise_customer = EnterpriseCustomer . objects . get ( uuid = customer_uuid ) email_to_unlink = request . GET [ "unlink_email" ] try : EnterpriseCustomerUser . objects . unlink_user ( enterprise_customer = enterprise_customer , user_email = email_to_unlink ) except ( EnterpriseCustomerUser . DoesNotExist , PendingEnterpriseCustomerUser . DoesNotExist ) : message = _ ( "Email {email} is not associated with Enterprise " "Customer {ec_name}" ) . format ( email = email_to_unlink , ec_name = enterprise_customer . name ) return HttpResponse ( message , content_type = "application/json" , status = 404 ) return HttpResponse ( json . dumps ( { } ) , content_type = "application/json" )
2103	def _produce_raw_method ( self ) : def method ( res_self , ** kwargs ) : obj_pk = kwargs . get ( method . _res_name ) other_obj_pk = kwargs . get ( method . _other_name ) internal_method = getattr ( res_self , method . _internal_name ) return internal_method ( method . _relationship , obj_pk , other_obj_pk ) return method
9701	def send ( self , msg ) : slipDriver = sliplib . Driver ( ) slipData = slipDriver . send ( msg ) res = self . _serialPort . write ( slipData ) return res
10489	def _getBundleId ( self ) : ra = AppKit . NSRunningApplication app = ra . runningApplicationWithProcessIdentifier_ ( self . _getPid ( ) ) return app . bundleIdentifier ( )
546	def _writePrediction ( self , result ) : self . __predictionCache . append ( result ) if self . _isBestModel : self . __flushPredictionCache ( )
2711	def json_iter ( path ) : with open ( path , 'r' ) as f : for line in f . readlines ( ) : yield json . loads ( line )
13693	def register ( self , service , name = '' ) : try : is_model = issubclass ( service , orb . Model ) except StandardError : is_model = False if is_model : self . services [ service . schema ( ) . dbname ( ) ] = ( ModelService , service ) else : super ( OrbApiFactory , self ) . register ( service , name = name )
2248	def memoize ( func ) : cache = { } @ functools . wraps ( func ) def memoizer ( * args , ** kwargs ) : key = _make_signature_key ( args , kwargs ) if key not in cache : cache [ key ] = func ( * args , ** kwargs ) return cache [ key ] memoizer . cache = cache return memoizer
8852	def on_new ( self ) : interpreter , pyserver , args = self . _get_backend_parameters ( ) self . setup_editor ( self . tabWidget . create_new_document ( extension = '.py' , interpreter = interpreter , server_script = pyserver , args = args ) ) self . actionRun . setDisabled ( True ) self . actionConfigure_run . setDisabled ( True )
11494	def get_default_api_key ( self , email , password ) : parameters = dict ( ) parameters [ 'email' ] = email parameters [ 'password' ] = password response = self . request ( 'midas.user.apikey.default' , parameters ) return response [ 'apikey' ]
13143	def _transform_triple_numpy ( x ) : return np . array ( [ x . head , x . relation , x . tail ] , dtype = np . int64 )
10045	def create ( cls , object_type = None , object_uuid = None , ** kwargs ) : assert 'pid_value' in kwargs kwargs . setdefault ( 'status' , cls . default_status ) return super ( DepositProvider , cls ) . create ( object_type = object_type , object_uuid = object_uuid , ** kwargs )
3025	def _save_private_file ( filename , json_contents ) : temp_filename = tempfile . mktemp ( ) file_desc = os . open ( temp_filename , os . O_WRONLY | os . O_CREAT , 0o600 ) with os . fdopen ( file_desc , 'w' ) as file_handle : json . dump ( json_contents , file_handle , sort_keys = True , indent = 2 , separators = ( ',' , ': ' ) ) shutil . move ( temp_filename , filename )
6985	def parallel_timebin ( lclist , binsizesec , maxobjects = None , outdir = None , lcformat = 'hat-sql' , lcformatdir = None , timecols = None , magcols = None , errcols = None , minbinelems = 7 , nworkers = NCPUS , maxworkertasks = 1000 ) : if outdir and not os . path . exists ( outdir ) : os . mkdir ( outdir ) if maxobjects is not None : lclist = lclist [ : maxobjects ] tasks = [ ( x , binsizesec , { 'outdir' : outdir , 'lcformat' : lcformat , 'lcformatdir' : lcformatdir , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'minbinelems' : minbinelems } ) for x in lclist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( timebinlc_worker , tasks ) pool . close ( ) pool . join ( ) resdict = { os . path . basename ( x ) : y for ( x , y ) in zip ( lclist , results ) } return resdict
7753	def route_stanza ( self , stanza ) : if stanza . stanza_type not in ( "error" , "result" ) : response = stanza . make_error_response ( u"recipient-unavailable" ) self . send ( response ) return True
2773	def create ( self , * args , ** kwargs ) : rules_dict = [ rule . __dict__ for rule in self . forwarding_rules ] params = { 'name' : self . name , 'region' : self . region , 'forwarding_rules' : rules_dict , 'redirect_http_to_https' : self . redirect_http_to_https } if self . droplet_ids and self . tag : raise ValueError ( 'droplet_ids and tag are mutually exclusive args' ) elif self . tag : params [ 'tag' ] = self . tag else : params [ 'droplet_ids' ] = self . droplet_ids if self . algorithm : params [ 'algorithm' ] = self . algorithm if self . health_check : params [ 'health_check' ] = self . health_check . __dict__ if self . sticky_sessions : params [ 'sticky_sessions' ] = self . sticky_sessions . __dict__ data = self . get_data ( 'load_balancers/' , type = POST , params = params ) if data : self . id = data [ 'load_balancer' ] [ 'id' ] self . ip = data [ 'load_balancer' ] [ 'ip' ] self . algorithm = data [ 'load_balancer' ] [ 'algorithm' ] self . health_check = HealthCheck ( ** data [ 'load_balancer' ] [ 'health_check' ] ) self . sticky_sessions = StickySesions ( ** data [ 'load_balancer' ] [ 'sticky_sessions' ] ) self . droplet_ids = data [ 'load_balancer' ] [ 'droplet_ids' ] self . status = data [ 'load_balancer' ] [ 'status' ] self . created_at = data [ 'load_balancer' ] [ 'created_at' ] return self
10939	def calc_accel_correction ( self , damped_JTJ , delta0 ) : _ = self . update_function ( self . param_vals ) rm0 = self . calc_residuals ( ) . copy ( ) _ = self . update_function ( self . param_vals + delta0 ) rm1 = self . calc_residuals ( ) . copy ( ) _ = self . update_function ( self . param_vals - delta0 ) rm2 = self . calc_residuals ( ) . copy ( ) der2 = ( rm2 + rm1 - 2 * rm0 ) corr , res , rank , s = np . linalg . lstsq ( damped_JTJ , np . dot ( self . J , der2 ) , rcond = self . min_eigval ) corr *= - 0.5 return corr
8834	def less ( a , b , * args ) : types = set ( [ type ( a ) , type ( b ) ] ) if float in types or int in types : try : a , b = float ( a ) , float ( b ) except TypeError : return False return a < b and ( not args or less ( b , * args ) )
627	def _hashCoordinate ( coordinate ) : coordinateStr = "," . join ( str ( v ) for v in coordinate ) hash = int ( int ( hashlib . md5 ( coordinateStr ) . hexdigest ( ) , 16 ) % ( 2 ** 64 ) ) return hash
1462	def new_source ( self , source ) : source_streamlet = None if callable ( source ) : source_streamlet = SupplierStreamlet ( source ) elif isinstance ( source , Generator ) : source_streamlet = GeneratorStreamlet ( source ) else : raise RuntimeError ( "Builder's new source has to be either a Generator or a function" ) self . _sources . append ( source_streamlet ) return source_streamlet
11267	def walk ( prev , inital_path , * args , ** kw ) : for dir_path , dir_names , filenames in os . walk ( inital_path ) : for filename in filenames : yield os . path . join ( dir_path , filename )
4727	def s20_to_gen ( self , pugrp , punit , chunk , sectr ) : cmd = [ "nvm_addr s20_to_gen" , self . envs [ "DEV_PATH" ] , "%d %d %d %d" % ( pugrp , punit , chunk , sectr ) ] status , stdout , _ = cij . ssh . command ( cmd , shell = True ) if status : raise RuntimeError ( "cij.liblight.s20_to_gen: cmd fail" ) return int ( re . findall ( r"val: ([0-9a-fx]+)" , stdout ) [ 0 ] , 16 )
4593	def receive ( self , msg ) : if self . edit_queue : self . edit_queue . put_edit ( self . _set , msg ) else : self . _set ( msg )
2813	def convert_unsqueeze ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting unsqueeze ...' ) if names == 'short' : tf_name = 'UNSQ' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) def target_layer ( x ) : import keras return keras . backend . expand_dims ( x ) lambda_layer = keras . layers . Lambda ( target_layer , name = tf_name + 'E' ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
8066	def get_source ( self , doc ) : start_iter = doc . get_start_iter ( ) end_iter = doc . get_end_iter ( ) source = doc . get_text ( start_iter , end_iter , False ) return source
1218	def save ( self , sess , save_path , timestep = None ) : if self . _saver is None : raise TensorForceError ( "register_saver_ops should be called before save" ) return self . _saver . save ( sess = sess , save_path = save_path , global_step = timestep , write_meta_graph = False , write_state = True , )
7415	def copy ( self ) : cp = copy . deepcopy ( self ) cp . genotypes = allel . GenotypeArray ( self . genotypes , copy = True ) return cp
12257	def smooth ( x , rho , penalty , axis = 0 , newshape = None ) : orig_shape = x . shape if newshape is not None : x = x . reshape ( newshape ) n = x . shape [ axis ] lap_op = spdiags ( [ ( 2 + rho / penalty ) * np . ones ( n ) , - 1 * np . ones ( n ) , - 1 * np . ones ( n ) ] , [ 0 , - 1 , 1 ] , n , n , format = 'csc' ) A = penalty * lap_op b = rho * np . rollaxis ( x , axis , 0 ) return np . rollaxis ( spsolve ( A , b ) , axis , 0 ) . reshape ( orig_shape )
62	def is_fully_within_image ( self , image ) : shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] return self . x1 >= 0 and self . x2 < width and self . y1 >= 0 and self . y2 < height
13645	def parse ( parser , argv = None , settings_key = 'settings' , no_args_func = None ) : argv = argv or sys . argv commands = command_list ( ) if type ( argv ) not in [ list , tuple ] : raise TypeError ( "argv only can be list or tuple" ) if len ( argv ) >= 2 and argv [ 1 ] in commands : sub_parsers = parser . add_subparsers ( ) class_name = argv [ 1 ] . capitalize ( ) + 'Component' from cliez . conf import ( COMPONENT_ROOT , LOGGING_CONFIG , EPILOG , GENERAL_ARGUMENTS ) sys . path . insert ( 0 , os . path . dirname ( COMPONENT_ROOT ) ) mod = importlib . import_module ( '{}.components.{}' . format ( os . path . basename ( COMPONENT_ROOT ) , argv [ 1 ] ) ) klass = getattr ( mod , class_name ) sub_parser = append_arguments ( klass , sub_parsers , EPILOG , GENERAL_ARGUMENTS ) options = parser . parse_args ( argv [ 1 : ] ) settings = Settings . bind ( getattr ( options , settings_key ) ) if settings_key and hasattr ( options , settings_key ) else None obj = klass ( parser , sub_parser , options , settings ) logger_level = logging . CRITICAL if hasattr ( options , 'verbose' ) : if options . verbose == 1 : logger_level = logging . ERROR elif options . verbose == 2 : logger_level = logging . WARNING elif options . verbose == 3 : logger_level = logging . INFO obj . logger . setLevel ( logging . INFO ) pass if hasattr ( options , 'debug' ) and options . debug : logger_level = logging . DEBUG try : import http . client as http_client http_client . HTTPConnection . debuglevel = 1 except Exception : pass pass loggers = LOGGING_CONFIG [ 'loggers' ] for k , v in loggers . items ( ) : v . setdefault ( 'level' , logger_level ) if logger_level in [ logging . INFO , logging . DEBUG ] : v [ 'handlers' ] = [ 'stdout' ] pass logging_config . dictConfig ( LOGGING_CONFIG ) obj . run ( options ) return obj if not parser . description and len ( commands ) : sub_parsers = parser . add_subparsers ( ) [ sub_parsers . add_parser ( v ) for v in commands ] pass pass options = parser . parse_args ( argv [ 1 : ] ) if no_args_func and callable ( no_args_func ) : return no_args_func ( options ) else : parser . _print_message ( "nothing to do...\n" ) pass
12987	def keep_kwargs_partial ( func , * args , ** keywords ) : def newfunc ( * fargs , ** fkeywords ) : newkeywords = fkeywords . copy ( ) newkeywords . update ( keywords ) return func ( * ( args + fargs ) , ** newkeywords ) newfunc . func = func newfunc . args = args newfunc . keywords = keywords return newfunc
8907	def list_services ( self ) : my_services = [ ] for service in self . collection . find ( ) . sort ( 'name' , pymongo . ASCENDING ) : my_services . append ( Service ( service ) ) return my_services
10918	def find_best_step ( err_vals ) : if np . all ( np . isnan ( err_vals ) ) : raise ValueError ( 'All err_vals are nans!' ) return np . nanargmin ( err_vals )
7008	def _fourier_chisq ( fourierparams , phase , mags , errs ) : f = _fourier_func ( fourierparams , phase , mags ) chisq = npsum ( ( ( mags - f ) * ( mags - f ) ) / ( errs * errs ) ) return chisq
8487	def init ( self , hosts = None , cacert = None , client_cert = None , client_key = None ) : try : import etcd self . module = etcd except ImportError : pass if not self . module : return self . _parse_jetconfig ( ) hosts = env ( 'PYCONFIG_ETCD_HOSTS' , hosts ) protocol = env ( 'PYCONFIG_ETCD_PROTOCOL' , None ) cacert = env ( 'PYCONFIG_ETCD_CACERT' , cacert ) client_cert = env ( 'PYCONFIG_ETCD_CERT' , client_cert ) client_key = env ( 'PYCONFIG_ETCD_KEY' , client_key ) username = None password = None auth = env ( 'PYCONFIG_ETCD_AUTH' , None ) if auth : auth = auth . split ( ':' ) auth . append ( '' ) username = auth [ 0 ] password = auth [ 1 ] hosts = self . _parse_hosts ( hosts ) if hosts is None : return kw = { } kw [ 'allow_reconnect' ] = True if protocol : kw [ 'protocol' ] = protocol if username : kw [ 'username' ] = username if password : kw [ 'password' ] = password if cacert : kw [ 'ca_cert' ] = os . path . abspath ( cacert ) if client_cert and client_key : kw [ 'cert' ] = ( ( os . path . abspath ( client_cert ) , os . path . abspath ( client_key ) ) ) elif client_cert : kw [ 'cert' ] = os . path . abspath ( client_cert ) if cacert or client_cert or client_key : kw [ 'protocol' ] = 'https' self . client = self . module . Client ( hosts , ** kw )
3172	def get ( self , store_id , customer_id , ** queryparams ) : self . store_id = store_id self . customer_id = customer_id return self . _mc_client . _get ( url = self . _build_path ( store_id , 'customers' , customer_id ) , ** queryparams )
1483	def launch ( self ) : with self . process_lock : current_commands = dict ( map ( ( lambda process : ( process . name , process . command ) ) , self . processes_to_monitor . values ( ) ) ) updated_commands = self . get_commands_to_run ( ) commands_to_kill , commands_to_keep , commands_to_start = self . get_command_changes ( current_commands , updated_commands ) Log . info ( "current commands: %s" % sorted ( current_commands . keys ( ) ) ) Log . info ( "new commands : %s" % sorted ( updated_commands . keys ( ) ) ) Log . info ( "commands_to_kill: %s" % sorted ( commands_to_kill . keys ( ) ) ) Log . info ( "commands_to_keep: %s" % sorted ( commands_to_keep . keys ( ) ) ) Log . info ( "commands_to_start: %s" % sorted ( commands_to_start . keys ( ) ) ) self . _kill_processes ( commands_to_kill ) self . _start_processes ( commands_to_start ) Log . info ( "Launch complete - processes killed=%s kept=%s started=%s monitored=%s" % ( len ( commands_to_kill ) , len ( commands_to_keep ) , len ( commands_to_start ) , len ( self . processes_to_monitor ) ) )
222	def is_not_modified ( self , response_headers : Headers , request_headers : Headers ) -> bool : try : if_none_match = request_headers [ "if-none-match" ] etag = response_headers [ "etag" ] if if_none_match == etag : return True except KeyError : pass try : if_modified_since = parsedate ( request_headers [ "if-modified-since" ] ) last_modified = parsedate ( response_headers [ "last-modified" ] ) if ( if_modified_since is not None and last_modified is not None and if_modified_since >= last_modified ) : return True except KeyError : pass return False
10711	def models_preparing ( app ) : def wrapper ( resource , parent ) : if isinstance ( resource , DeclarativeMeta ) : resource = ListResource ( resource ) if not getattr ( resource , '__parent__' , None ) : resource . __parent__ = parent return resource resources_preparing_factory ( app , wrapper )
12301	def instantiate ( repo , validator_name = None , filename = None , rulesfiles = None ) : default_validators = repo . options . get ( 'validator' , { } ) validators = { } if validator_name is not None : if validator_name in default_validators : validators = { validator_name : default_validators [ validator_name ] } else : validators = { validator_name : { 'files' : [ ] , 'rules' : { } , 'rules-files' : [ ] } } else : validators = default_validators if filename is not None : matching_files = repo . find_matching_files ( [ filename ] ) if len ( matching_files ) == 0 : print ( "Filename could not be found" , filename ) raise Exception ( "Invalid filename pattern" ) for v in validators : validators [ v ] [ 'files' ] = matching_files else : for v in validators : if 'files' not in validators [ v ] : validators [ v ] [ 'files' ] = [ ] elif len ( validators [ v ] [ 'files' ] ) > 0 : matching_files = repo . find_matching_files ( validators [ v ] [ 'files' ] ) validators [ v ] [ 'files' ] = matching_files if rulesfiles is not None : matching_files = repo . find_matching_files ( [ rulesfiles ] ) if len ( matching_files ) == 0 : print ( "Could not find matching rules files ({}) for {}" . format ( rulesfiles , v ) ) raise Exception ( "Invalid rules" ) for v in validators : validators [ v ] [ 'rules-files' ] = matching_files else : for v in validators : if 'rules-files' not in validators [ v ] : validators [ v ] [ 'rules-files' ] = [ ] else : rulesfiles = validators [ v ] [ 'rules-files' ] matching_files = repo . find_matching_files ( rulesfiles ) validators [ v ] [ 'rules-files' ] = matching_files return validators
10747	def fetch ( self , url , path , filename ) : logger . debug ( 'initializing download in ' , url ) remote_file_size = self . get_remote_file_size ( url ) if exists ( join ( path , filename ) ) : size = getsize ( join ( path , filename ) ) if size == remote_file_size : logger . error ( '%s already exists on your system' % filename ) print ( '%s already exists on your system' % filename ) return [ join ( path , filename ) , size ] logger . debug ( 'Downloading: %s' % filename ) print ( 'Downloading: %s' % filename ) fetch ( url , path ) print ( 'stored at %s' % path ) logger . debug ( 'stored at %s' % path ) return [ join ( path , filename ) , remote_file_size ]
1319	def IsTopLevel ( self ) -> bool : handle = self . NativeWindowHandle if handle : return GetAncestor ( handle , GAFlag . Root ) == handle return False
8282	def _linelength ( self , x0 , y0 , x1 , y1 ) : a = pow ( abs ( x0 - x1 ) , 2 ) b = pow ( abs ( y0 - y1 ) , 2 ) return sqrt ( a + b )
4904	def populate_data_sharing_consent ( apps , schema_editor ) : DataSharingConsent = apps . get_model ( 'consent' , 'DataSharingConsent' ) EnterpriseCourseEnrollment = apps . get_model ( 'enterprise' , 'EnterpriseCourseEnrollment' ) User = apps . get_model ( 'auth' , 'User' ) for enrollment in EnterpriseCourseEnrollment . objects . all ( ) : user = User . objects . get ( pk = enrollment . enterprise_customer_user . user_id ) data_sharing_consent , __ = DataSharingConsent . objects . get_or_create ( username = user . username , enterprise_customer = enrollment . enterprise_customer_user . enterprise_customer , course_id = enrollment . course_id , ) if enrollment . consent_granted is not None : data_sharing_consent . granted = enrollment . consent_granted else : consent_state = enrollment . enterprise_customer_user . data_sharing_consent . first ( ) if consent_state is not None : data_sharing_consent . granted = consent_state . state in [ 'enabled' , 'external' ] else : data_sharing_consent . granted = False data_sharing_consent . save ( )
1265	def sanity_check_actions ( actions_spec ) : actions = copy . deepcopy ( actions_spec ) is_unique = ( 'type' in actions ) if is_unique : actions = dict ( action = actions ) for name , action in actions . items ( ) : if 'type' not in action : action [ 'type' ] = 'int' if action [ 'type' ] == 'int' : if 'num_actions' not in action : raise TensorForceError ( "Action requires value 'num_actions' set!" ) elif action [ 'type' ] == 'float' : if ( 'min_value' in action ) != ( 'max_value' in action ) : raise TensorForceError ( "Action requires both values 'min_value' and 'max_value' set!" ) if 'shape' not in action : action [ 'shape' ] = ( ) if isinstance ( action [ 'shape' ] , int ) : action [ 'shape' ] = ( action [ 'shape' ] , ) return actions , is_unique
10101	def create_snippet ( self , name , body , timeout = None ) : payload = { 'name' : name , 'body' : body } return self . _api_request ( self . SNIPPETS_ENDPOINT , self . HTTP_POST , payload = payload , timeout = timeout )
11409	def record_add_fields ( rec , tag , fields , field_position_local = None , field_position_global = None ) : if field_position_local is None and field_position_global is None : for field in fields : record_add_field ( rec , tag , ind1 = field [ 1 ] , ind2 = field [ 2 ] , subfields = field [ 0 ] , controlfield_value = field [ 3 ] ) else : fields . reverse ( ) for field in fields : record_add_field ( rec , tag , ind1 = field [ 1 ] , ind2 = field [ 2 ] , subfields = field [ 0 ] , controlfield_value = field [ 3 ] , field_position_local = field_position_local , field_position_global = field_position_global ) return field_position_local
710	def _backupFile ( filePath ) : assert os . path . exists ( filePath ) stampNum = 0 ( prefix , suffix ) = os . path . splitext ( filePath ) while True : backupPath = "%s.%d%s" % ( prefix , stampNum , suffix ) stampNum += 1 if not os . path . exists ( backupPath ) : break shutil . copyfile ( filePath , backupPath ) return backupPath
599	def computeRawAnomalyScore ( activeColumns , prevPredictedColumns ) : nActiveColumns = len ( activeColumns ) if nActiveColumns > 0 : score = numpy . in1d ( activeColumns , prevPredictedColumns ) . sum ( ) score = ( nActiveColumns - score ) / float ( nActiveColumns ) else : score = 0.0 return score
11996	def _set_options ( self , options ) : if not options : return self . options . copy ( ) options = options . copy ( ) if 'magic' in options : self . set_magic ( options [ 'magic' ] ) del ( options [ 'magic' ] ) if 'flags' in options : flags = options [ 'flags' ] del ( options [ 'flags' ] ) for key , value in flags . iteritems ( ) : if not isinstance ( value , bool ) : raise TypeError ( 'Invalid flag type for: %s' % key ) else : flags = self . options [ 'flags' ] if 'info' in options : del ( options [ 'info' ] ) for key , value in options . iteritems ( ) : if not isinstance ( value , int ) : raise TypeError ( 'Invalid option type for: %s' % key ) if value < 0 or value > 255 : raise ValueError ( 'Option value out of range for: %s' % key ) new_options = self . options . copy ( ) new_options . update ( options ) new_options [ 'flags' ] . update ( flags ) return new_options
6766	def interfaces ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : if is_file ( '/usr/sbin/dladm' ) : res = run ( '/usr/sbin/dladm show-link' ) else : res = sudo ( '/sbin/ifconfig -s' ) return [ line . split ( ' ' ) [ 0 ] for line in res . splitlines ( ) [ 1 : ] ]
8288	def get_child_by_name ( parent , name ) : def iterate_children ( widget , name ) : if widget . get_name ( ) == name : return widget try : for w in widget . get_children ( ) : result = iterate_children ( w , name ) if result is not None : return result else : continue except AttributeError : pass return iterate_children ( parent , name )
6677	def md5sum ( self , filename , use_sudo = False ) : func = use_sudo and run_as_root or self . run with self . settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : if exists ( u'/usr/bin/md5sum' ) : res = func ( u'/usr/bin/md5sum %(filename)s' % locals ( ) ) elif exists ( u'/sbin/md5' ) : res = func ( u'/sbin/md5 -r %(filename)s' % locals ( ) ) elif exists ( u'/opt/local/gnu/bin/md5sum' ) : res = func ( u'/opt/local/gnu/bin/md5sum %(filename)s' % locals ( ) ) elif exists ( u'/opt/local/bin/md5sum' ) : res = func ( u'/opt/local/bin/md5sum %(filename)s' % locals ( ) ) else : md5sum = func ( u'which md5sum' ) md5 = func ( u'which md5' ) if exists ( md5sum ) : res = func ( '%(md5sum)s %(filename)s' % locals ( ) ) elif exists ( md5 ) : res = func ( '%(md5)s %(filename)s' % locals ( ) ) else : abort ( 'No MD5 utility was found on this system.' ) if res . succeeded : _md5sum = res else : warn ( res ) _md5sum = None if isinstance ( _md5sum , six . string_types ) : _md5sum = _md5sum . strip ( ) . split ( '\n' ) [ - 1 ] . split ( ) [ 0 ] return _md5sum
4781	def is_between ( self , low , high ) : val_type = type ( self . val ) self . _validate_between_args ( val_type , low , high ) if self . val < low or self . val > high : if val_type is datetime . datetime : self . _err ( 'Expected <%s> to be between <%s> and <%s>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , low . strftime ( '%Y-%m-%d %H:%M:%S' ) , high . strftime ( '%Y-%m-%d %H:%M:%S' ) ) ) else : self . _err ( 'Expected <%s> to be between <%s> and <%s>, but was not.' % ( self . val , low , high ) ) return self
6587	def iterate_forever ( func , * args , ** kwargs ) : output = func ( * args , ** kwargs ) while True : try : playlist_item = next ( output ) playlist_item . prepare_playback ( ) yield playlist_item except StopIteration : output = func ( * args , ** kwargs )
2646	def python_app ( function = None , data_flow_kernel = None , walltime = 60 , cache = False , executors = 'all' ) : from parsl . app . python import PythonApp def decorator ( func ) : def wrapper ( f ) : return PythonApp ( f , data_flow_kernel = data_flow_kernel , walltime = walltime , cache = cache , executors = executors ) return wrapper ( func ) if function is not None : return decorator ( function ) return decorator
6429	def stem ( self , word ) : lowered = word . lower ( ) if lowered [ - 3 : ] == 'ies' and lowered [ - 4 : - 3 ] not in { 'e' , 'a' } : return word [ : - 3 ] + ( 'Y' if word [ - 1 : ] . isupper ( ) else 'y' ) if lowered [ - 2 : ] == 'es' and lowered [ - 3 : - 2 ] not in { 'a' , 'e' , 'o' } : return word [ : - 1 ] if lowered [ - 1 : ] == 's' and lowered [ - 2 : - 1 ] not in { 'u' , 's' } : return word [ : - 1 ] return word
11922	def paginate_dataframe ( self , dataframe ) : if self . paginator is None : return None return self . paginator . paginate_dataframe ( dataframe , self . request , view = self )
13759	def _create_api_uri ( self , * parts ) : return urljoin ( self . API_URI , '/' . join ( map ( quote , parts ) ) )
10569	def filter_local_songs ( filepaths , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False ) : matched_songs = [ ] filtered_songs = [ ] for filepath in filepaths : try : song = _get_mutagen_metadata ( filepath ) except mutagen . MutagenError : filtered_songs . append ( filepath ) else : if include_filters or exclude_filters : if _check_filters ( song , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) : matched_songs . append ( filepath ) else : filtered_songs . append ( filepath ) else : matched_songs . append ( filepath ) return matched_songs , filtered_songs
6617	def expand_path_cfg ( path_cfg , alias_dict = { } , overriding_kargs = { } ) : if isinstance ( path_cfg , str ) : return _expand_str ( path_cfg , alias_dict , overriding_kargs ) if isinstance ( path_cfg , dict ) : return _expand_dict ( path_cfg , alias_dict ) return _expand_tuple ( path_cfg , alias_dict , overriding_kargs )
7833	def add_item ( self , fields = None ) : item = Item ( fields ) self . items . append ( item ) return item
6146	def IIR_bsf ( f_pass1 , f_stop1 , f_stop2 , f_pass2 , Ripple_pass , Atten_stop , fs = 1.00 , ftype = 'butter' ) : b , a = signal . iirdesign ( [ 2 * float ( f_pass1 ) / fs , 2 * float ( f_pass2 ) / fs ] , [ 2 * float ( f_stop1 ) / fs , 2 * float ( f_stop2 ) / fs ] , Ripple_pass , Atten_stop , ftype = ftype , output = 'ba' ) sos = signal . iirdesign ( [ 2 * float ( f_pass1 ) / fs , 2 * float ( f_pass2 ) / fs ] , [ 2 * float ( f_stop1 ) / fs , 2 * float ( f_stop2 ) / fs ] , Ripple_pass , Atten_stop , ftype = ftype , output = 'sos' ) tag = 'IIR ' + ftype + ' order' print ( '%s = %d.' % ( tag , len ( a ) - 1 ) ) return b , a , sos
74	def EdgeDetect ( alpha = 0 , name = None , deterministic = False , random_state = None ) : alpha_param = iap . handle_continuous_param ( alpha , "alpha" , value_range = ( 0 , 1.0 ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( _image , nb_channels , random_state_func ) : alpha_sample = alpha_param . draw_sample ( random_state = random_state_func ) ia . do_assert ( 0 <= alpha_sample <= 1.0 ) matrix_nochange = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) matrix_effect = np . array ( [ [ 0 , 1 , 0 ] , [ 1 , - 4 , 1 ] , [ 0 , 1 , 0 ] ] , dtype = np . float32 ) matrix = ( 1 - alpha_sample ) * matrix_nochange + alpha_sample * matrix_effect return [ matrix ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
13655	def _matchRoute ( components , request , segments , partialMatching ) : if len ( components ) == 1 and isinstance ( components [ 0 ] , bytes ) : components = components [ 0 ] if components [ : 1 ] == '/' : components = components [ 1 : ] components = components . split ( '/' ) results = OrderedDict ( ) NO_MATCH = None , segments remaining = list ( segments ) if len ( segments ) == len ( components ) == 0 : return results , remaining for us , them in izip_longest ( components , segments ) : if us is None : if partialMatching : break else : return NO_MATCH elif them is None : return NO_MATCH if callable ( us ) : name , match = us ( request , them ) if match is None : return NO_MATCH results [ name ] = match elif us != them : return NO_MATCH remaining . pop ( 0 ) return results , remaining
4119	def twosided_2_onesided ( data ) : assert len ( data ) % 2 == 0 N = len ( data ) psd = np . array ( data [ 0 : N // 2 + 1 ] ) * 2. psd [ 0 ] /= 2. psd [ - 1 ] = data [ - 1 ] return psd
7012	def lcdict_to_pickle ( lcdict , outfile = None ) : if not outfile and lcdict [ 'objectid' ] : outfile = '%s-hplc.pkl' % lcdict [ 'objectid' ] elif not outfile and not lcdict [ 'objectid' ] : outfile = 'hplc.pkl' with open ( outfile , 'wb' ) as outfd : pickle . dump ( lcdict , outfd , protocol = pickle . HIGHEST_PROTOCOL ) if os . path . exists ( outfile ) : LOGINFO ( 'lcdict for object: %s -> %s OK' % ( lcdict [ 'objectid' ] , outfile ) ) return outfile else : LOGERROR ( 'could not make a pickle for this lcdict!' ) return None
13049	def check_service ( service ) : service . add_tag ( 'header_scan' ) http = False try : result = requests . head ( 'http://{}:{}' . format ( service . address , service . port ) , timeout = 1 ) print_success ( "Found http service on {}:{}" . format ( service . address , service . port ) ) service . add_tag ( 'http' ) http = True try : service . banner = result . headers [ 'Server' ] except KeyError : pass except ( ConnectionError , ConnectTimeout , ReadTimeout , Error ) : pass if not http : try : result = requests . head ( 'https://{}:{}' . format ( service . address , service . port ) , verify = False , timeout = 3 ) service . add_tag ( 'https' ) print_success ( "Found https service on {}:{}" . format ( service . address , service . port ) ) try : service . banner = result . headers [ 'Server' ] except KeyError : pass except ( ConnectionError , ConnectTimeout , ReadTimeout , Error ) : pass service . save ( )
13316	def create ( name_or_path = None , config = None ) : if utils . is_system_path ( name_or_path ) : path = unipath ( name_or_path ) else : path = unipath ( get_home_path ( ) , name_or_path ) if os . path . exists ( path ) : raise OSError ( '{} already exists' . format ( path ) ) env = VirtualEnvironment ( path ) utils . ensure_path_exists ( env . path ) if config : if utils . is_git_repo ( config ) : Git ( '' ) . clone ( config , env . path ) else : shutil . copy2 ( config , env . config_path ) else : with open ( env . config_path , 'w' ) as f : f . write ( defaults . environment_config ) utils . ensure_path_exists ( env . hook_path ) utils . ensure_path_exists ( env . modules_path ) env . run_hook ( 'precreate' ) virtualenv . create_environment ( env . path ) if not utils . is_home_environment ( env . path ) : EnvironmentCache . add ( env ) EnvironmentCache . save ( ) try : env . update ( ) except : utils . rmtree ( path ) logger . debug ( 'Failed to update, rolling back...' ) raise else : env . run_hook ( 'postcreate' ) return env
5009	def create_course_completion ( self , user_id , payload ) : url = self . enterprise_configuration . sapsf_base_url + self . global_sap_config . completion_status_api_path return self . _call_post_with_user_override ( user_id , url , payload )
6296	def instance ( self , program : moderngl . Program ) -> moderngl . VertexArray : vao = self . vaos . get ( program . glo ) if vao : return vao program_attributes = [ name for name , attr in program . _members . items ( ) if isinstance ( attr , moderngl . Attribute ) ] for attrib_name in program_attributes : if attrib_name . startswith ( 'gl_' ) : continue if not sum ( buffer . has_attribute ( attrib_name ) for buffer in self . buffers ) : raise VAOError ( "VAO {} doesn't have attribute {} for program {}" . format ( self . name , attrib_name , program . name ) ) vao_content = [ ] for buffer in self . buffers : content = buffer . content ( program_attributes ) if content : vao_content . append ( content ) if program_attributes : for attrib_name in program_attributes : if attrib_name . startswith ( 'gl_' ) : continue raise VAOError ( "Did not find a buffer mapping for {}" . format ( [ n for n in program_attributes ] ) ) if self . _index_buffer : vao = context . ctx ( ) . vertex_array ( program , vao_content , self . _index_buffer , self . _index_element_size ) else : vao = context . ctx ( ) . vertex_array ( program , vao_content ) self . vaos [ program . glo ] = vao return vao
10065	def json_serializer ( pid , data , * args ) : if data is not None : response = Response ( json . dumps ( data . dumps ( ) ) , mimetype = 'application/json' ) else : response = Response ( mimetype = 'application/json' ) return response
11344	def request ( self , url , method = "GET" , data = None , params = None , retry = True ) : headers = config . REQUEST_HEADERS if params and self . _session_id : params [ 'sessionid' ] = self . _session_id if method == "GET" : response = requests . get ( url , headers = headers , params = params ) elif method == "POST" : response = requests . post ( url , headers = headers , params = params , data = data ) if response . status_code == 401 and retry : _LOGGER . warn ( "NuHeat APIrequest unauthorized [401]. Try to re-authenticate." ) self . _session_id = None self . authenticate ( ) return self . request ( url , method = method , data = data , params = params , retry = False ) response . raise_for_status ( ) try : return response . json ( ) except ValueError : return response
13587	def add_formatted_field ( cls , field , format_string , title = '' ) : global klass_count klass_count += 1 fn_name = 'dyn_fn_%d' % klass_count cls . list_display . append ( fn_name ) if not title : title = field . capitalize ( ) _format_string = format_string def _ref ( self , obj ) : return _format_string % getattr ( obj , field ) _ref . short_description = title _ref . allow_tags = True _ref . admin_order_field = field setattr ( cls , fn_name , _ref )
3974	def _get_compose_volumes ( app_name , assembled_specs ) : volumes = [ ] volumes . append ( _get_cp_volume_mount ( app_name ) ) volumes += get_app_volume_mounts ( app_name , assembled_specs ) return volumes
4027	def create_local_copy ( cookie_file ) : if isinstance ( cookie_file , list ) : cookie_file = cookie_file [ 0 ] if os . path . exists ( cookie_file ) : tmp_cookie_file = tempfile . NamedTemporaryFile ( suffix = '.sqlite' ) . name open ( tmp_cookie_file , 'wb' ) . write ( open ( cookie_file , 'rb' ) . read ( ) ) return tmp_cookie_file else : raise BrowserCookieError ( 'Can not find cookie file at: ' + cookie_file )
13078	def make_cache_keys ( self , endpoint , kwargs ) : keys = sorted ( kwargs . keys ( ) ) i18n_cache_key = endpoint + "|" + "|" . join ( [ kwargs [ k ] for k in keys ] ) if "lang" in keys : cache_key = endpoint + "|" + "|" . join ( [ kwargs [ k ] for k in keys if k != "lang" ] ) else : cache_key = i18n_cache_key return i18n_cache_key , cache_key
4220	def backends ( cls ) : allowed = ( keyring for keyring in filter ( backend . _limit , backend . get_all_keyring ( ) ) if not isinstance ( keyring , ChainerBackend ) and keyring . priority > 0 ) return sorted ( allowed , key = backend . by_priority , reverse = True )
9185	def _reassemble_binder ( id , tree , metadata ) : binder = cnxepub . Binder ( id , metadata = metadata ) for item in tree [ 'contents' ] : node = _node_to_model ( item , parent = binder ) if node . metadata [ 'title' ] != item [ 'title' ] : binder . set_title_for_node ( node , item [ 'title' ] ) return binder
8627	def get_users ( session , query ) : response = make_get_request ( session , 'users' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise UsersNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
9551	def _init_unique_sets ( self ) : ks = dict ( ) for t in self . _unique_checks : key = t [ 0 ] ks [ key ] = set ( ) return ks
3272	def _init ( self ) : self . provider . _count_get_resource_inst_init += 1 tableName , primKey = self . provider . _split_path ( self . path ) display_type = "Unknown" displayTypeComment = "" contentType = "text/html" if tableName is None : display_type = "Database" elif primKey is None : display_type = "Database Table" else : contentType = "text/csv" if primKey == "_ENTIRE_CONTENTS" : display_type = "Database Table Contents" displayTypeComment = "CSV Representation of Table Contents" else : display_type = "Database Record" displayTypeComment = "Attributes available as properties" is_collection = primKey is None self . _cache = { "content_length" : None , "contentType" : contentType , "created" : time . time ( ) , "display_name" : self . name , "etag" : hashlib . md5 ( ) . update ( self . path ) . hexdigest ( ) , "modified" : None , "support_ranges" : False , "display_info" : { "type" : display_type , "typeComment" : displayTypeComment } , } if not is_collection : self . _cache [ "modified" ] = time . time ( ) _logger . debug ( "- % self . provider . _count_initConnection )
12936	def add ( self , * names ) : def decorator ( blok ) : for name in names or ( blok . __name__ , ) : self [ name ] = blok return blok return decorator
1887	def emulate ( self , instruction ) : while True : self . reset ( ) for base in self . _should_be_mapped : size , perms = self . _should_be_mapped [ base ] self . _emu . mem_map ( base , size , perms ) for address , values in self . _should_be_written . items ( ) : for offset , byte in enumerate ( values , start = address ) : if issymbolic ( byte ) : from . . native . cpu . abstractcpu import ConcretizeMemory raise ConcretizeMemory ( self . _cpu . memory , offset , 8 , "Concretizing for emulation" ) self . _emu . mem_write ( address , b'' . join ( values ) ) self . _should_try_again = False self . _step ( instruction ) if not self . _should_try_again : break
3860	def _wrap_event ( event_ ) : cls = conversation_event . ConversationEvent if event_ . HasField ( 'chat_message' ) : cls = conversation_event . ChatMessageEvent elif event_ . HasField ( 'otr_modification' ) : cls = conversation_event . OTREvent elif event_ . HasField ( 'conversation_rename' ) : cls = conversation_event . RenameEvent elif event_ . HasField ( 'membership_change' ) : cls = conversation_event . MembershipChangeEvent elif event_ . HasField ( 'hangout_event' ) : cls = conversation_event . HangoutEvent elif event_ . HasField ( 'group_link_sharing_modification' ) : cls = conversation_event . GroupLinkSharingModificationEvent return cls ( event_ )
13830	def _url ( self ) : if self . _api_arg : mypart = str ( self . _api_arg ) else : mypart = self . _name if self . _parent : return '/' . join ( filter ( None , [ self . _parent . _url , mypart ] ) ) else : return mypart
6573	def formatter ( self , api_client , data , newval ) : url_map = data . get ( "audioUrlMap" ) audio_url = data . get ( "audioUrl" ) if audio_url and not url_map : url_map = { BaseAPIClient . HIGH_AUDIO_QUALITY : { "audioUrl" : audio_url , "bitrate" : 64 , "encoding" : "aacplus" , } } elif not url_map : return None valid_audio_formats = [ BaseAPIClient . HIGH_AUDIO_QUALITY , BaseAPIClient . MED_AUDIO_QUALITY , BaseAPIClient . LOW_AUDIO_QUALITY ] preferred_quality = api_client . default_audio_quality if preferred_quality in valid_audio_formats : i = valid_audio_formats . index ( preferred_quality ) valid_audio_formats = valid_audio_formats [ i : ] for quality in valid_audio_formats : audio_url = url_map . get ( quality ) if audio_url : return audio_url [ self . field ] return audio_url [ self . field ] if audio_url else None
56	def deepcopy ( self , keypoints = None , shape = None ) : if keypoints is None : keypoints = [ kp . deepcopy ( ) for kp in self . keypoints ] if shape is None : shape = tuple ( self . shape ) return KeypointsOnImage ( keypoints , shape )
846	def _getDistances ( self , inputPattern , partitionId = None ) : if not self . _finishedLearning : self . finishLearning ( ) self . _finishedLearning = True if self . _vt is not None and len ( self . _vt ) > 0 : inputPattern = numpy . dot ( self . _vt , inputPattern - self . _mean ) sparseInput = self . _sparsifyVector ( inputPattern ) dist = self . _calcDistance ( sparseInput ) if self . _specificIndexTraining : dist [ numpy . array ( self . _categoryList ) == - 1 ] = numpy . inf if partitionId is not None : dist [ self . _partitionIdMap . get ( partitionId , [ ] ) ] = numpy . inf return dist
2206	def compressuser ( path , home = '~' ) : path = normpath ( path ) userhome_dpath = userhome ( ) if path . startswith ( userhome_dpath ) : if len ( path ) == len ( userhome_dpath ) : path = home elif path [ len ( userhome_dpath ) ] == os . path . sep : path = home + path [ len ( userhome_dpath ) : ] return path
9416	def to_value ( cls , instance ) : if not isinstance ( instance , OctaveUserClass ) or not instance . _attrs : return dict ( ) dtype = [ ] values = [ ] for attr in instance . _attrs : dtype . append ( ( str ( attr ) , object ) ) values . append ( getattr ( instance , attr ) ) struct = np . array ( [ tuple ( values ) ] , dtype ) return MatlabObject ( struct , instance . _name )
7896	def change_nick ( self , new_nick ) : new_room_jid = JID ( self . room_jid . node , self . room_jid . domain , new_nick ) p = Presence ( to_jid = new_room_jid ) self . manager . stream . send ( p )
11320	def update_date_year ( self ) : dates = record_get_field_instances ( self . record , '260' ) for field in dates : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'c' : field [ 0 ] [ idx ] = ( 'c' , value [ : 4 ] ) elif key == 't' : del field [ 0 ] [ idx ] if not dates : published_years = record_get_field_values ( self . record , "773" , code = "y" ) if published_years : record_add_field ( self . record , "260" , subfields = [ ( "c" , published_years [ 0 ] [ : 4 ] ) ] ) else : other_years = record_get_field_values ( self . record , "269" , code = "c" ) if other_years : record_add_field ( self . record , "260" , subfields = [ ( "c" , other_years [ 0 ] [ : 4 ] ) ] )
5143	def search_for_comment ( self , lineno , default = None ) : if not self . index : self . make_index ( ) block = self . index . get ( lineno , None ) text = getattr ( block , 'text' , default ) return text
10158	def get_viewset_transition_action_mixin ( model , ** kwargs ) : instance = model ( ) class Mixin ( object ) : save_after_transition = True transitions = instance . get_all_status_transitions ( ) transition_names = set ( x . name for x in transitions ) for transition_name in transition_names : setattr ( Mixin , transition_name , get_transition_viewset_method ( transition_name , ** kwargs ) ) return Mixin
1169	def rlecode_hqx ( s ) : if not s : return '' result = [ ] prev = s [ 0 ] count = 1 if s [ - 1 ] == '!' : s = s [ 1 : ] + '?' else : s = s [ 1 : ] + '!' for c in s : if c == prev and count < 255 : count += 1 else : if count == 1 : if prev != '\x90' : result . append ( prev ) else : result += [ '\x90' , '\x00' ] elif count < 4 : if prev != '\x90' : result += [ prev ] * count else : result += [ '\x90' , '\x00' ] * count else : if prev != '\x90' : result += [ prev , '\x90' , chr ( count ) ] else : result += [ '\x90' , '\x00' , '\x90' , chr ( count ) ] count = 1 prev = c return '' . join ( result )
3626	def pad_to ( unpadded , target_len ) : under = target_len - len ( unpadded ) if under <= 0 : return unpadded return unpadded + ( ' ' * under )
9664	def construct_graph ( sakefile , settings ) : verbose = settings [ "verbose" ] sprint = settings [ "sprint" ] G = nx . DiGraph ( ) sprint ( "Going to construct Graph" , level = "verbose" ) for target in sakefile : if target == "all" : continue if "formula" not in sakefile [ target ] : for atomtarget in sakefile [ target ] : if atomtarget == "help" : continue sprint ( "Adding '{}'" . format ( atomtarget ) , level = "verbose" ) data_dict = sakefile [ target ] [ atomtarget ] data_dict [ "parent" ] = target G . add_node ( atomtarget , ** data_dict ) else : sprint ( "Adding '{}'" . format ( target ) , level = "verbose" ) G . add_node ( target , ** sakefile [ target ] ) sprint ( "Nodes are built\nBuilding connections" , level = "verbose" ) for node in G . nodes ( data = True ) : sprint ( "checking node {} for dependencies" . format ( node [ 0 ] ) , level = "verbose" ) for k , v in node [ 1 ] . items ( ) : if v is None : node [ 1 ] [ k ] = [ ] if "output" in node [ 1 ] : for index , out in enumerate ( node [ 1 ] [ 'output' ] ) : node [ 1 ] [ 'output' ] [ index ] = clean_path ( node [ 1 ] [ 'output' ] [ index ] ) if "dependencies" not in node [ 1 ] : continue sprint ( "it has dependencies" , level = "verbose" ) connects = [ ] for index , dep in enumerate ( node [ 1 ] [ 'dependencies' ] ) : dep = os . path . normpath ( dep ) shrt = "dependencies" node [ 1 ] [ 'dependencies' ] [ index ] = clean_path ( node [ 1 ] [ shrt ] [ index ] ) for node in G . nodes ( data = True ) : connects = [ ] if "dependencies" not in node [ 1 ] : continue for dep in node [ 1 ] [ 'dependencies' ] : matches = check_for_dep_in_outputs ( dep , verbose , G ) if not matches : continue for match in matches : sprint ( "Appending {} to matches" . format ( match ) , level = "verbose" ) connects . append ( match ) if connects : for connect in connects : G . add_edge ( connect , node [ 0 ] ) return G
5701	def _feed_calendar_span ( gtfs , stats ) : n_feeds = _n_gtfs_sources ( gtfs ) [ 0 ] max_start = None min_end = None if n_feeds > 1 : for i in range ( n_feeds ) : feed_key = "feed_" + str ( i ) + "_" start_key = feed_key + "calendar_start" end_key = feed_key + "calendar_end" calendar_span = gtfs . conn . cursor ( ) . execute ( 'SELECT min(date), max(date) FROM trips, days ' 'WHERE trips.trip_I = days.trip_I AND trip_id LIKE ?;' , ( feed_key + '%' , ) ) . fetchone ( ) stats [ start_key ] = calendar_span [ 0 ] stats [ end_key ] = calendar_span [ 1 ] if calendar_span [ 0 ] is not None and calendar_span [ 1 ] is not None : if not max_start and not min_end : max_start = calendar_span [ 0 ] min_end = calendar_span [ 1 ] else : if gtfs . get_day_start_ut ( calendar_span [ 0 ] ) > gtfs . get_day_start_ut ( max_start ) : max_start = calendar_span [ 0 ] if gtfs . get_day_start_ut ( calendar_span [ 1 ] ) < gtfs . get_day_start_ut ( min_end ) : min_end = calendar_span [ 1 ] stats [ "latest_feed_start_date" ] = max_start stats [ "earliest_feed_end_date" ] = min_end else : stats [ "latest_feed_start_date" ] = stats [ "start_date" ] stats [ "earliest_feed_end_date" ] = stats [ "end_date" ] return stats
6812	def deploy ( self ) : for service in self . genv . services : service = service . strip ( ) . upper ( ) funcs = common . service_deployers . get ( service ) if funcs : print ( 'Deploying service %s...' % ( service , ) ) for func in funcs : if not self . dryrun : func ( )
12577	def _mask_data ( self , data ) : self . _check_for_mask ( ) msk_data = self . mask . get_data ( ) if self . ndim == 3 : return data [ msk_data ] , np . where ( msk_data ) elif self . ndim == 4 : return _apply_mask_to_4d_data ( data , self . mask ) else : raise ValueError ( 'Cannot mask {} with {} dimensions using mask {}.' . format ( self , self . ndim , self . mask ) )
495	def _releaseConnection ( self , dbConn , cursor ) : self . _logger . debug ( "Releasing connection" ) cursor . close ( ) dbConn . close ( ) return
5188	def connect ( host = 'localhost' , port = 8080 , ssl_verify = False , ssl_key = None , ssl_cert = None , timeout = 10 , protocol = None , url_path = '/' , username = None , password = None , token = None ) : return BaseAPI ( host = host , port = port , timeout = timeout , ssl_verify = ssl_verify , ssl_key = ssl_key , ssl_cert = ssl_cert , protocol = protocol , url_path = url_path , username = username , password = password , token = token )
3887	def add_observer ( self , callback ) : if callback in self . _observers : raise ValueError ( '{} is already an observer of {}' . format ( callback , self ) ) self . _observers . append ( callback )
10840	def publish ( self ) : url = PATHS [ 'PUBLISH' ] % self . id return self . api . post ( url = url )
9079	def decode_timestamp ( data : str ) -> datetime . datetime : year = 2000 + int ( data [ 0 : 2 ] ) month = int ( data [ 2 : 4 ] ) day = int ( data [ 4 : 6 ] ) hour = int ( data [ 6 : 8 ] ) minute = int ( data [ 8 : 10 ] ) second = int ( data [ 10 : 12 ] ) if minute == 60 : minute = 0 hour += 1 return datetime . datetime ( year = year , month = month , day = day , hour = hour , minute = minute , second = second )
5949	def filename ( self , filename = None , ext = None , set_default = False , use_my_ext = False ) : if filename is None : if not hasattr ( self , '_filename' ) : self . _filename = None if self . _filename : filename = self . _filename else : raise ValueError ( "A file name is required because no default file name was defined." ) my_ext = None else : filename , my_ext = os . path . splitext ( filename ) if set_default : self . _filename = filename if my_ext and use_my_ext : ext = my_ext if ext is not None : if ext . startswith ( os . extsep ) : ext = ext [ 1 : ] if ext != "" : filename = filename + os . extsep + ext return filename
9150	def to_indra_statements ( self , * args , ** kwargs ) : graph = self . to_bel ( * args , ** kwargs ) return to_indra_statements ( graph )
88	def is_float_array ( val ) : return is_np_array ( val ) and issubclass ( val . dtype . type , np . floating )
7486	def run_cutadapt ( data , subsamples , lbview ) : start = time . time ( ) printstr = " processing reads | {} | s2 |" finished = 0 rawedits = { } subsamples . sort ( key = lambda x : x . stats . reads_raw , reverse = True ) LOGGER . info ( [ i . stats . reads_raw for i in subsamples ] ) if "pair" in data . paramsdict [ "datatype" ] : for sample in subsamples : rawedits [ sample . name ] = lbview . apply ( cutadaptit_pairs , * ( data , sample ) ) else : for sample in subsamples : rawedits [ sample . name ] = lbview . apply ( cutadaptit_single , * ( data , sample ) ) while 1 : finished = sum ( [ i . ready ( ) for i in rawedits . values ( ) ] ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( rawedits ) , finished , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if finished == len ( rawedits ) : print ( "" ) break for async in rawedits : if rawedits [ async ] . successful ( ) : res = rawedits [ async ] . result ( ) if "pair" not in data . paramsdict [ "datatype" ] : parse_single_results ( data , data . samples [ async ] , res ) else : parse_pair_results ( data , data . samples [ async ] , res ) else : print ( " found an error in step2; see ipyrad_log.txt" ) LOGGER . error ( "error in run_cutadapt(): %s" , rawedits [ async ] . exception ( ) )
1354	def get_argument_cluster ( self ) : try : return self . get_argument ( constants . PARAM_CLUSTER ) except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
7080	def tic_objectsearch ( objectid , idcol_to_use = "ID" , apiversion = 'v0' , forcefetch = False , cachedir = '~/.astrobase/mast-cache' , verbose = True , timeout = 90.0 , refresh = 5.0 , maxtimeout = 180.0 , maxtries = 3 , jitter = 5.0 , raiseonfail = False ) : params = { 'columns' : '*' , 'filters' : [ { "paramName" : idcol_to_use , "values" : [ str ( objectid ) ] } ] } service = 'Mast.Catalogs.Filtered.Tic' return mast_query ( service , params , jitter = jitter , apiversion = apiversion , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , raiseonfail = raiseonfail )
4414	def add_next ( self , requester : int , track : dict ) : self . queue . insert ( 0 , AudioTrack ( ) . build ( track , requester ) )
7500	def get_spans ( maparr , spans ) : bidx = 1 spans = np . zeros ( ( maparr [ - 1 , 0 ] , 2 ) , np . uint64 ) for idx in xrange ( 1 , maparr . shape [ 0 ] ) : cur = maparr [ idx , 0 ] if cur != bidx : idy = idx + 1 spans [ cur - 2 , 1 ] = idx spans [ cur - 1 , 0 ] = idx bidx = cur spans [ - 1 , 1 ] = maparr [ - 1 , - 1 ] return spans
2850	def _mpsse_enable ( self ) : self . _check ( ftdi . set_bitmode , 0 , 0 ) self . _check ( ftdi . set_bitmode , 0 , 2 )
10390	def workflow ( graph : BELGraph , node : BaseEntity , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , minimum_nodes : int = 1 , ) -> List [ 'Runner' ] : subgraph = generate_mechanism ( graph , node , key = key ) if subgraph . number_of_nodes ( ) <= minimum_nodes : return [ ] runners = multirun ( subgraph , node , key = key , tag = tag , default_score = default_score , runs = runs ) return list ( runners )
9329	def post ( self , url , headers = None , params = None , ** kwargs ) : if len ( kwargs ) > 1 : raise InvalidArgumentsError ( "Too many extra args ({} > 1)" . format ( len ( kwargs ) ) ) if kwargs : kwarg = next ( iter ( kwargs ) ) if kwarg not in ( "json" , "data" ) : raise InvalidArgumentsError ( "Invalid kwarg: " + kwarg ) resp = self . session . post ( url , headers = headers , params = params , ** kwargs ) resp . raise_for_status ( ) return _to_json ( resp )
4472	def transform ( self , jam ) : for state in self . states ( jam ) : yield self . _transform ( jam , state )
8144	def flip ( self , axis = HORIZONTAL ) : if axis == HORIZONTAL : self . img = self . img . transpose ( Image . FLIP_LEFT_RIGHT ) if axis == VERTICAL : self . img = self . img . transpose ( Image . FLIP_TOP_BOTTOM )
347	def load_imdb_dataset ( path = 'data' , nb_words = None , skip_top = 0 , maxlen = None , test_split = 0.2 , seed = 113 , start_char = 1 , oov_char = 2 , index_from = 3 ) : path = os . path . join ( path , 'imdb' ) filename = "imdb.pkl" url = 'https://s3.amazonaws.com/text-datasets/' maybe_download_and_extract ( filename , path , url ) if filename . endswith ( ".gz" ) : f = gzip . open ( os . path . join ( path , filename ) , 'rb' ) else : f = open ( os . path . join ( path , filename ) , 'rb' ) X , labels = cPickle . load ( f ) f . close ( ) np . random . seed ( seed ) np . random . shuffle ( X ) np . random . seed ( seed ) np . random . shuffle ( labels ) if start_char is not None : X = [ [ start_char ] + [ w + index_from for w in x ] for x in X ] elif index_from : X = [ [ w + index_from for w in x ] for x in X ] if maxlen : new_X = [ ] new_labels = [ ] for x , y in zip ( X , labels ) : if len ( x ) < maxlen : new_X . append ( x ) new_labels . append ( y ) X = new_X labels = new_labels if not X : raise Exception ( 'After filtering for sequences shorter than maxlen=' + str ( maxlen ) + ', no sequence was kept. ' 'Increase maxlen.' ) if not nb_words : nb_words = max ( [ max ( x ) for x in X ] ) if oov_char is not None : X = [ [ oov_char if ( w >= nb_words or w < skip_top ) else w for w in x ] for x in X ] else : nX = [ ] for x in X : nx = [ ] for w in x : if ( w >= nb_words or w < skip_top ) : nx . append ( w ) nX . append ( nx ) X = nX X_train = np . array ( X [ : int ( len ( X ) * ( 1 - test_split ) ) ] ) y_train = np . array ( labels [ : int ( len ( X ) * ( 1 - test_split ) ) ] ) X_test = np . array ( X [ int ( len ( X ) * ( 1 - test_split ) ) : ] ) y_test = np . array ( labels [ int ( len ( X ) * ( 1 - test_split ) ) : ] ) return X_train , y_train , X_test , y_test
6572	def register_simple_chooser ( self , model , ** kwargs ) : name = '{}Chooser' . format ( model . _meta . object_name ) attrs = { 'model' : model } attrs . update ( kwargs ) chooser = type ( name , ( Chooser , ) , attrs ) self . register_chooser ( chooser ) return model
13269	def deparagraph ( element , doc ) : if isinstance ( element , Para ) : if element . next is not None : return element elif element . prev is not None : return element return Plain ( * element . content )
5982	def output_figure ( array , as_subplot , output_path , output_filename , output_format ) : if not as_subplot : if output_format is 'show' : plt . show ( ) elif output_format is 'png' : plt . savefig ( output_path + output_filename + '.png' , bbox_inches = 'tight' ) elif output_format is 'fits' : array_util . numpy_array_2d_to_fits ( array_2d = array , file_path = output_path + output_filename + '.fits' , overwrite = True )
4313	def silent ( input_filepath , threshold = 0.001 ) : validate_input_file ( input_filepath ) stat_dictionary = stat ( input_filepath ) mean_norm = stat_dictionary [ 'Mean norm' ] if mean_norm is not float ( 'nan' ) : if mean_norm >= threshold : return False else : return True else : return True
1910	def run ( self , procs = 1 , timeout = None , should_profile = False ) : assert not self . running , "Manticore is already running." self . _start_run ( ) self . _last_run_stats [ 'time_started' ] = time . time ( ) with self . shutdown_timeout ( timeout ) : self . _start_workers ( procs , profiling = should_profile ) self . _join_workers ( ) self . _finish_run ( profiling = should_profile )
2623	def shut_down_instance ( self , instances = None ) : if instances and len ( self . instances ) > 0 : print ( instances ) try : print ( [ i . id for i in instances ] ) except Exception as e : print ( e ) term = self . client . terminate_instances ( InstanceIds = instances ) logger . info ( "Shut down {} instances (ids:{}" . format ( len ( instances ) , str ( instances ) ) ) elif len ( self . instances ) > 0 : instance = self . instances . pop ( ) term = self . client . terminate_instances ( InstanceIds = [ instance ] ) logger . info ( "Shut down 1 instance (id:{})" . format ( instance ) ) else : logger . warn ( "No Instances to shut down.\n" ) return - 1 self . get_instance_state ( ) return term
5814	def _try_decode ( byte_string ) : try : return str_cls ( byte_string , _encoding ) except ( UnicodeDecodeError ) : for encoding in _fallback_encodings : try : return str_cls ( byte_string , encoding , errors = 'strict' ) except ( UnicodeDecodeError ) : pass return str_cls ( byte_string , errors = 'replace' )
4327	def downsample ( self , factor = 2 ) : if not isinstance ( factor , int ) or factor < 1 : raise ValueError ( 'factor must be a positive integer.' ) effect_args = [ 'downsample' , '{}' . format ( factor ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'downsample' ) return self
5111	def _get_queues ( g , queues , edge , edge_type ) : INT = numbers . Integral if isinstance ( queues , INT ) : queues = [ queues ] elif queues is None : if edge is not None : if isinstance ( edge , tuple ) : if isinstance ( edge [ 0 ] , INT ) and isinstance ( edge [ 1 ] , INT ) : queues = [ g . edge_index [ edge ] ] elif isinstance ( edge [ 0 ] , collections . Iterable ) : if np . array ( [ len ( e ) == 2 for e in edge ] ) . all ( ) : queues = [ g . edge_index [ e ] for e in edge ] else : queues = [ g . edge_index [ edge ] ] elif edge_type is not None : if isinstance ( edge_type , collections . Iterable ) : edge_type = set ( edge_type ) else : edge_type = set ( [ edge_type ] ) tmp = [ ] for e in g . edges ( ) : if g . ep ( e , 'edge_type' ) in edge_type : tmp . append ( g . edge_index [ e ] ) queues = np . array ( tmp , int ) if queues is None : queues = range ( g . number_of_edges ( ) ) return queues
1285	def footnote_item ( self , key , text ) : back = ( '<a href="#fnref-%s" class="footnote">&#8617;</a>' ) % escape ( key ) text = text . rstrip ( ) if text . endswith ( '</p>' ) : text = re . sub ( r'<\/p>$' , r'%s</p>' % back , text ) else : text = '%s<p>%s</p>' % ( text , back ) html = '<li id="fn-%s">%s</li>\n' % ( escape ( key ) , text ) return html
8041	def is_public ( self ) : return ( not self . name . startswith ( "_" ) and self . parent . is_class and self . parent . is_public )
13536	def prune_list ( self ) : targets = self . descendents_root ( ) try : targets . remove ( self . graph . root ) except ValueError : pass targets . append ( self ) return targets
7124	def validate_date ( date_text ) : try : if int ( date_text ) < 0 : return True except ValueError : pass try : datetime . strptime ( date_text , '%Y-%m-%d' ) return True except ValueError : pass raise ValueError ( 'Dates must be negative integers or YYYY-MM-DD in the past.' )
3559	def find_service ( self , uuid ) : for service in self . list_services ( ) : if service . uuid == uuid : return service return None
4753	def tcase_parse_descr ( tcase ) : descr_short = "SHORT" descr_long = "LONG" try : comment = tcase_comment ( tcase ) except ( IOError , OSError , ValueError ) as exc : comment = [ ] cij . err ( "tcase_parse_descr: failed: %r, tcase: %r" % ( exc , tcase ) ) comment = [ l for l in comment if l . strip ( ) ] for line_number , line in enumerate ( comment ) : if line . startswith ( "#" ) : comment [ line_number ] = line [ 1 : ] if comment : descr_short = comment [ 0 ] if len ( comment ) > 1 : descr_long = "\n" . join ( comment [ 1 : ] ) return descr_short , descr_long
2807	def convert_gemm ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting Linear ...' ) if names == 'short' : tf_name = 'FC' + random_string ( 6 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) bias_name = '{0}.bias' . format ( w_name ) weights_name = '{0}.weight' . format ( w_name ) W = weights [ weights_name ] . numpy ( ) . transpose ( ) input_channels , output_channels = W . shape keras_weights = [ W ] has_bias = False if bias_name in weights : bias = weights [ bias_name ] . numpy ( ) keras_weights = [ W , bias ] has_bias = True dense = keras . layers . Dense ( output_channels , weights = keras_weights , use_bias = has_bias , name = tf_name , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , ) layers [ scope_name ] = dense ( layers [ inputs [ 0 ] ] )
6088	def scaled_noise_map_from_hyper_galaxies_and_contribution_maps ( contribution_maps , hyper_galaxies , noise_map ) : scaled_noise_maps = list ( map ( lambda hyper_galaxy , contribution_map : hyper_galaxy . hyper_noise_from_contributions ( noise_map = noise_map , contributions = contribution_map ) , hyper_galaxies , contribution_maps ) ) return noise_map + sum ( scaled_noise_maps )
6800	def get_free_space ( self ) : cmd = "df -k | grep -vE '^Filesystem|tmpfs|cdrom|none|udev|cgroup' | awk '{ print($1 \" \" $4 }'" lines = [ _ for _ in self . run ( cmd ) . strip ( ) . split ( '\n' ) if _ . startswith ( '/' ) ] assert len ( lines ) == 1 , 'Ambiguous devices: %s' % str ( lines ) device , kb = lines [ 0 ] . split ( ' ' ) free_space = int ( kb ) * 1024 self . vprint ( 'free_space (bytes):' , free_space ) return free_space
5320	def find_ports ( device ) : bus_id = device . bus dev_id = device . address for dirent in os . listdir ( USB_SYS_PREFIX ) : matches = re . match ( USB_PORTS_STR + '$' , dirent ) if matches : bus_str = readattr ( dirent , 'busnum' ) if bus_str : busnum = float ( bus_str ) else : busnum = None dev_str = readattr ( dirent , 'devnum' ) if dev_str : devnum = float ( dev_str ) else : devnum = None if busnum == bus_id and devnum == dev_id : return str ( matches . groups ( ) [ 1 ] )
2830	def convert_upsample ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting upsample...' ) if params [ 'mode' ] != 'nearest' : raise AssertionError ( 'Cannot convert non-nearest upsampling' ) if names == 'short' : tf_name = 'UPSL' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'height_scale' in params : scale = ( params [ 'height_scale' ] , params [ 'width_scale' ] ) elif len ( inputs ) == 2 : scale = layers [ inputs [ - 1 ] + '_np' ] [ - 2 : ] upsampling = keras . layers . UpSampling2D ( size = scale , name = tf_name ) layers [ scope_name ] = upsampling ( layers [ inputs [ 0 ] ] )
13133	def parse_domain_users ( domain_users_file , domain_groups_file ) : with open ( domain_users_file ) as f : users = json . loads ( f . read ( ) ) domain_groups = { } if domain_groups_file : with open ( domain_groups_file ) as f : groups = json . loads ( f . read ( ) ) for group in groups : sid = get_field ( group , 'objectSid' ) domain_groups [ int ( sid . split ( '-' ) [ - 1 ] ) ] = get_field ( group , 'cn' ) user_search = UserSearch ( ) count = 0 total = len ( users ) print_notification ( "Importing {} users" . format ( total ) ) for entry in users : result = parse_user ( entry , domain_groups ) user = user_search . id_to_object ( result [ 'username' ] ) user . name = result [ 'name' ] user . domain . append ( result [ 'domain' ] ) user . description = result [ 'description' ] user . groups . extend ( result [ 'groups' ] ) user . flags . extend ( result [ 'flags' ] ) user . sid = result [ 'sid' ] user . add_tag ( "domaindump" ) user . save ( ) count += 1 sys . stdout . write ( '\r' ) sys . stdout . write ( "[{}/{}]" . format ( count , total ) ) sys . stdout . flush ( ) sys . stdout . write ( '\r' ) return count
1179	def split ( self , string , maxsplit = 0 ) : splitlist = [ ] state = _State ( string , 0 , sys . maxint , self . flags ) n = 0 last = state . start while not maxsplit or n < maxsplit : state . reset ( ) state . string_position = state . start if not state . search ( self . _code ) : break if state . start == state . string_position : if last == state . end : break state . start += 1 continue splitlist . append ( string [ last : state . start ] ) if self . groups : match = SRE_Match ( self , state ) splitlist += ( list ( match . groups ( None ) ) ) n += 1 last = state . start = state . string_position splitlist . append ( string [ last : state . end ] ) return splitlist
1022	def createTMs ( includeCPP = True , includePy = True , numCols = 100 , cellsPerCol = 4 , activationThreshold = 3 , minThreshold = 3 , newSynapseCount = 3 , initialPerm = 0.6 , permanenceInc = 0.1 , permanenceDec = 0.0 , globalDecay = 0.0 , pamLength = 0 , checkSynapseConsistency = True , maxInfBacktrack = 0 , maxLrnBacktrack = 0 , ** kwargs ) : connectedPerm = 0.5 tms = dict ( ) if includeCPP : if VERBOSITY >= 2 : print "Creating BacktrackingTMCPP instance" cpp_tm = BacktrackingTMCPP ( numberOfCols = numCols , cellsPerColumn = cellsPerCol , initialPerm = initialPerm , connectedPerm = connectedPerm , minThreshold = minThreshold , newSynapseCount = newSynapseCount , permanenceInc = permanenceInc , permanenceDec = permanenceDec , activationThreshold = activationThreshold , globalDecay = globalDecay , burnIn = 1 , seed = SEED , verbosity = VERBOSITY , checkSynapseConsistency = checkSynapseConsistency , collectStats = True , pamLength = pamLength , maxInfBacktrack = maxInfBacktrack , maxLrnBacktrack = maxLrnBacktrack , ) cpp_tm . retrieveLearningStates = True tms [ 'CPP' ] = cpp_tm if includePy : if VERBOSITY >= 2 : print "Creating PY TM instance" py_tm = BacktrackingTM ( numberOfCols = numCols , cellsPerColumn = cellsPerCol , initialPerm = initialPerm , connectedPerm = connectedPerm , minThreshold = minThreshold , newSynapseCount = newSynapseCount , permanenceInc = permanenceInc , permanenceDec = permanenceDec , activationThreshold = activationThreshold , globalDecay = globalDecay , burnIn = 1 , seed = SEED , verbosity = VERBOSITY , collectStats = True , pamLength = pamLength , maxInfBacktrack = maxInfBacktrack , maxLrnBacktrack = maxLrnBacktrack , ) tms [ 'PY ' ] = py_tm return tms
108	def draw_grid ( images , rows = None , cols = None ) : nb_images = len ( images ) do_assert ( nb_images > 0 ) if is_np_array ( images ) : do_assert ( images . ndim == 4 ) else : do_assert ( is_iterable ( images ) and is_np_array ( images [ 0 ] ) and images [ 0 ] . ndim == 3 ) dts = [ image . dtype . name for image in images ] nb_dtypes = len ( set ( dts ) ) do_assert ( nb_dtypes == 1 , ( "All images provided to draw_grid() must have the same dtype, " + "found %d dtypes (%s)" ) % ( nb_dtypes , ", " . join ( dts ) ) ) cell_height = max ( [ image . shape [ 0 ] for image in images ] ) cell_width = max ( [ image . shape [ 1 ] for image in images ] ) channels = set ( [ image . shape [ 2 ] for image in images ] ) do_assert ( len ( channels ) == 1 , "All images are expected to have the same number of channels, " + "but got channel set %s with length %d instead." % ( str ( channels ) , len ( channels ) ) ) nb_channels = list ( channels ) [ 0 ] if rows is None and cols is None : rows = cols = int ( math . ceil ( math . sqrt ( nb_images ) ) ) elif rows is not None : cols = int ( math . ceil ( nb_images / rows ) ) elif cols is not None : rows = int ( math . ceil ( nb_images / cols ) ) do_assert ( rows * cols >= nb_images ) width = cell_width * cols height = cell_height * rows dt = images . dtype if is_np_array ( images ) else images [ 0 ] . dtype grid = np . zeros ( ( height , width , nb_channels ) , dtype = dt ) cell_idx = 0 for row_idx in sm . xrange ( rows ) : for col_idx in sm . xrange ( cols ) : if cell_idx < nb_images : image = images [ cell_idx ] cell_y1 = cell_height * row_idx cell_y2 = cell_y1 + image . shape [ 0 ] cell_x1 = cell_width * col_idx cell_x2 = cell_x1 + image . shape [ 1 ] grid [ cell_y1 : cell_y2 , cell_x1 : cell_x2 , : ] = image cell_idx += 1 return grid
12148	def convertImages ( self ) : exts = [ '.jpg' , '.png' ] for fname in [ x for x in self . files1 if cm . ext ( x ) in exts ] : ID = "UNKNOWN" if len ( fname ) > 8 and fname [ : 8 ] in self . IDs : ID = fname [ : 8 ] fname2 = ID + "_jpg_" + fname if not fname2 in self . files2 : self . log . info ( "copying over [%s]" % fname2 ) shutil . copy ( os . path . join ( self . folder1 , fname ) , os . path . join ( self . folder2 , fname2 ) ) if not fname [ : 8 ] + ".abf" in self . files1 : self . log . error ( "orphan image: %s" , fname ) exts = [ '.tif' , '.tiff' ] for fname in [ x for x in self . files1 if cm . ext ( x ) in exts ] : ID = "UNKNOWN" if len ( fname ) > 8 and fname [ : 8 ] in self . IDs : ID = fname [ : 8 ] fname2 = ID + "_tif_" + fname + ".jpg" if not fname2 in self . files2 : self . log . info ( "converting micrograph [%s]" % fname2 ) imaging . TIF_to_jpg ( os . path . join ( self . folder1 , fname ) , saveAs = os . path . join ( self . folder2 , fname2 ) ) if not fname [ : 8 ] + ".abf" in self . files1 : self . log . error ( "orphan image: %s" , fname )
1039	def column ( self ) : line , column = self . source_buffer . decompose_position ( self . begin_pos ) return column
8724	def from_timestamp ( ts ) : return datetime . datetime . utcfromtimestamp ( ts ) . replace ( tzinfo = pytz . utc )
6884	def find_lc_timegroups ( lctimes , mingap = 4.0 ) : lc_time_diffs = [ ( lctimes [ x ] - lctimes [ x - 1 ] ) for x in range ( 1 , len ( lctimes ) ) ] lc_time_diffs = np . array ( lc_time_diffs ) group_start_indices = np . where ( lc_time_diffs > mingap ) [ 0 ] if len ( group_start_indices ) > 0 : group_indices = [ ] for i , gindex in enumerate ( group_start_indices ) : if i == 0 : group_indices . append ( slice ( 0 , gindex + 1 ) ) else : group_indices . append ( slice ( group_start_indices [ i - 1 ] + 1 , gindex + 1 ) ) group_indices . append ( slice ( group_start_indices [ - 1 ] + 1 , len ( lctimes ) ) ) else : group_indices = [ slice ( 0 , len ( lctimes ) ) ] return len ( group_indices ) , group_indices
10671	def _finalise_result_ ( compound , value , mass ) : result = value / 3.6E6 result = result / compound . molar_mass result = result * mass return result
250	def adjust_returns_for_slippage ( returns , positions , transactions , slippage_bps ) : slippage = 0.0001 * slippage_bps portfolio_value = positions . sum ( axis = 1 ) pnl = portfolio_value * returns traded_value = get_txn_vol ( transactions ) . txn_volume slippage_dollars = traded_value * slippage adjusted_pnl = pnl . add ( - slippage_dollars , fill_value = 0 ) adjusted_returns = returns * adjusted_pnl / pnl return adjusted_returns
13353	def status_job ( self , fn = None , name = None , timeout = 3 ) : if fn is None : def decorator ( fn ) : self . add_status_job ( fn , name , timeout ) return decorator else : self . add_status_job ( fn , name , timeout )
3296	def ref_url_to_path ( self , ref_url ) : return "/" + compat . unquote ( util . lstripstr ( ref_url , self . share_path ) ) . lstrip ( "/" )
2545	def add_review_comment ( self , doc , comment ) : if len ( doc . reviews ) != 0 : if not self . review_comment_set : self . review_comment_set = True doc . reviews [ - 1 ] . comment = comment return True else : raise CardinalityError ( 'ReviewComment' ) else : raise OrderError ( 'ReviewComment' )
3963	def start_local_env ( recreate_containers ) : assembled_spec = spec_assembler . get_assembled_specs ( ) required_absent_assets = virtualbox . required_absent_assets ( assembled_spec ) if required_absent_assets : raise RuntimeError ( 'Assets {} are specified as required but are not set. Set them with `dusty assets set`' . format ( required_absent_assets ) ) docker_ip = virtualbox . get_docker_vm_ip ( ) if os . path . exists ( constants . COMPOSEFILE_PATH ) : try : stop_apps_or_services ( rm_containers = recreate_containers ) except CalledProcessError as e : log_to_client ( "WARNING: docker-compose stop failed" ) log_to_client ( str ( e ) ) daemon_warnings . clear_namespace ( 'disk' ) df_info = virtualbox . get_docker_vm_disk_info ( as_dict = True ) if 'M' in df_info [ 'free' ] or 'K' in df_info [ 'free' ] : warning_msg = 'VM is low on disk. Available disk: {}' . format ( df_info [ 'free' ] ) daemon_warnings . warn ( 'disk' , warning_msg ) log_to_client ( warning_msg ) log_to_client ( "Compiling together the assembled specs" ) active_repos = spec_assembler . get_all_repos ( active_only = True , include_specs_repo = False ) log_to_client ( "Compiling the port specs" ) port_spec = port_spec_compiler . get_port_spec_document ( assembled_spec , docker_ip ) log_to_client ( "Compiling the nginx config" ) docker_bridge_ip = virtualbox . get_docker_bridge_ip ( ) nginx_config = nginx_compiler . get_nginx_configuration_spec ( port_spec , docker_bridge_ip ) log_to_client ( "Creating setup and script bash files" ) make_up_command_files ( assembled_spec , port_spec ) log_to_client ( "Compiling docker-compose config" ) compose_config = compose_compiler . get_compose_dict ( assembled_spec , port_spec ) log_to_client ( "Saving port forwarding to hosts file" ) hosts . update_hosts_file_from_port_spec ( port_spec ) log_to_client ( "Configuring NFS" ) nfs . configure_nfs ( ) log_to_client ( "Saving updated nginx config to the VM" ) nginx . update_nginx_from_config ( nginx_config ) log_to_client ( "Saving Docker Compose config and starting all containers" ) compose . update_running_containers_from_spec ( compose_config , recreate_containers = recreate_containers ) log_to_client ( "Your local environment is now started!" )
10410	def finalize_canonical_averages ( number_of_nodes , ps , canonical_averages , alpha , ) : spanning_cluster = ( ( 'percolation_probability_mean' in canonical_averages . dtype . names ) and 'percolation_probability_m2' in canonical_averages . dtype . names ) ret = np . empty_like ( canonical_averages , dtype = finalized_canonical_averages_dtype ( spanning_cluster = spanning_cluster ) , ) n = canonical_averages [ 'number_of_runs' ] sqrt_n = np . sqrt ( canonical_averages [ 'number_of_runs' ] ) ret [ 'number_of_runs' ] = n ret [ 'p' ] = ps ret [ 'alpha' ] = alpha def _transform ( original_key , final_key = None , normalize = False , transpose = False , ) : if final_key is None : final_key = original_key keys_mean = [ '{}_mean' . format ( key ) for key in [ original_key , final_key ] ] keys_std = [ '{}_m2' . format ( original_key ) , '{}_std' . format ( final_key ) , ] key_ci = '{}_ci' . format ( final_key ) ret [ keys_mean [ 1 ] ] = canonical_averages [ keys_mean [ 0 ] ] if normalize : ret [ keys_mean [ 1 ] ] /= number_of_nodes array = canonical_averages [ keys_std [ 0 ] ] result = np . sqrt ( ( array . T if transpose else array ) / ( n - 1 ) ) ret [ keys_std [ 1 ] ] = ( result . T if transpose else result ) if normalize : ret [ keys_std [ 1 ] ] /= number_of_nodes array = ret [ keys_std [ 1 ] ] scale = ( array . T if transpose else array ) / sqrt_n array = ret [ keys_mean [ 1 ] ] mean = ( array . T if transpose else array ) result = scipy . stats . t . interval ( 1 - alpha , df = n - 1 , loc = mean , scale = scale , ) ( ret [ key_ci ] [ ... , 0 ] , ret [ key_ci ] [ ... , 1 ] ) = ( [ my_array . T for my_array in result ] if transpose else result ) if spanning_cluster : _transform ( 'percolation_probability' ) _transform ( 'max_cluster_size' , 'percolation_strength' , normalize = True ) _transform ( 'moments' , normalize = True , transpose = True ) return ret
11417	def record_modify_controlfield ( rec , tag , controlfield_value , field_position_global = None , field_position_local = None ) : field = record_get_field ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) new_field = ( field [ 0 ] , field [ 1 ] , field [ 2 ] , controlfield_value , field [ 4 ] ) record_replace_field ( rec , tag , new_field , field_position_global = field_position_global , field_position_local = field_position_local )
7258	def search_address ( self , address , filters = None , startDate = None , endDate = None , types = None ) : lat , lng = self . get_address_coords ( address ) return self . search_point ( lat , lng , filters = filters , startDate = startDate , endDate = endDate , types = types )
2455	def set_pkg_licenses_concluded ( self , doc , licenses ) : self . assert_package_exists ( ) if not self . package_conc_lics_set : self . package_conc_lics_set = True if validations . validate_lics_conc ( licenses ) : doc . package . conc_lics = licenses return True else : raise SPDXValueError ( 'Package::ConcludedLicenses' ) else : raise CardinalityError ( 'Package::ConcludedLicenses' )
13575	def select ( course = False , tid = None , auto = False ) : if course : update ( course = True ) course = None try : course = Course . get_selected ( ) except NoCourseSelected : pass ret = { } if not tid : ret = Menu . launch ( "Select a course" , Course . select ( ) . execute ( ) , course ) else : ret [ "item" ] = Course . get ( Course . tid == tid ) if "item" in ret : ret [ "item" ] . set_select ( ) update ( ) if ret [ "item" ] . path == "" : select_a_path ( auto = auto ) skip ( ) return else : print ( "You can select the course with `tmc select --course`" ) return else : selected = None try : selected = Exercise . get_selected ( ) except NoExerciseSelected : pass ret = { } if not tid : ret = Menu . launch ( "Select an exercise" , Course . get_selected ( ) . exercises , selected ) else : ret [ "item" ] = Exercise . byid ( tid ) if "item" in ret : ret [ "item" ] . set_select ( ) print ( "Selected {}" . format ( ret [ "item" ] ) )
12190	def _format_message ( self , channel , text ) : payload = { 'type' : 'message' , 'id' : next ( self . _msg_ids ) } payload . update ( channel = channel , text = text ) return json . dumps ( payload )
4270	def get_iptc_data ( filename ) : logger = logging . getLogger ( __name__ ) iptc_data = { } raw_iptc = { } try : img = _read_image ( filename ) raw_iptc = IptcImagePlugin . getiptcinfo ( img ) except SyntaxError : logger . info ( 'IPTC Error in %s' , filename ) if raw_iptc and ( 2 , 5 ) in raw_iptc : iptc_data [ "title" ] = raw_iptc [ ( 2 , 5 ) ] . decode ( 'utf-8' , errors = 'replace' ) if raw_iptc and ( 2 , 120 ) in raw_iptc : iptc_data [ "description" ] = raw_iptc [ ( 2 , 120 ) ] . decode ( 'utf-8' , errors = 'replace' ) if raw_iptc and ( 2 , 105 ) in raw_iptc : iptc_data [ "headline" ] = raw_iptc [ ( 2 , 105 ) ] . decode ( 'utf-8' , errors = 'replace' ) return iptc_data
13382	def dict_to_env ( d , pathsep = os . pathsep ) : out_env = { } for k , v in d . iteritems ( ) : if isinstance ( v , list ) : out_env [ k ] = pathsep . join ( v ) elif isinstance ( v , string_types ) : out_env [ k ] = v else : raise TypeError ( '{} not a valid env var type' . format ( type ( v ) ) ) return out_env
8799	def update_group_states_for_vifs ( self , vifs , ack ) : vif_keys = [ self . vif_key ( vif . device_id , vif . mac_address ) for vif in vifs ] self . set_fields ( vif_keys , SECURITY_GROUP_ACK , ack )
9818	def upgrade ( self ) : if not self . is_valid : raise PolyaxonDeploymentConfigError ( 'Deployment type `{}` not supported' . format ( self . deployment_type ) ) if self . is_kubernetes : self . upgrade_on_kubernetes ( ) elif self . is_docker_compose : self . upgrade_on_docker_compose ( ) elif self . is_docker : self . upgrade_on_docker ( ) elif self . is_heroku : self . upgrade_on_heroku ( )
379	def featurewise_norm ( x , mean = None , std = None , epsilon = 1e-7 ) : if mean : x = x - mean if std : x = x / ( std + epsilon ) return x
3179	def get ( self , batch_webhook_id , ** queryparams ) : self . batch_webhook_id = batch_webhook_id return self . _mc_client . _get ( url = self . _build_path ( batch_webhook_id ) , ** queryparams )
9251	def generate_log_for_all_tags ( self ) : if self . options . verbose : print ( "Generating log..." ) self . issues2 = copy . deepcopy ( self . issues ) log1 = "" if self . options . with_unreleased : log1 = self . generate_unreleased_section ( ) log = "" for index in range ( len ( self . filtered_tags ) - 1 ) : log += self . do_generate_log_for_all_tags_part1 ( log , index ) if self . options . tag_separator and log1 : log = log1 + self . options . tag_separator + log else : log = log1 + log if len ( self . filtered_tags ) != 0 : log += self . do_generate_log_for_all_tags_part2 ( log ) return log
8386	def amend_filename ( filename , amend ) : base , ext = os . path . splitext ( filename ) amended_name = base + amend + ext return amended_name
3062	def string_to_scopes ( scopes ) : if not scopes : return [ ] elif isinstance ( scopes , six . string_types ) : return scopes . split ( ' ' ) else : return scopes
10248	def update_context ( universe : BELGraph , graph : BELGraph ) : for namespace in get_namespaces ( graph ) : if namespace in universe . namespace_url : graph . namespace_url [ namespace ] = universe . namespace_url [ namespace ] elif namespace in universe . namespace_pattern : graph . namespace_pattern [ namespace ] = universe . namespace_pattern [ namespace ] else : log . warning ( 'namespace: %s missing from universe' , namespace ) for annotation in get_annotations ( graph ) : if annotation in universe . annotation_url : graph . annotation_url [ annotation ] = universe . annotation_url [ annotation ] elif annotation in universe . annotation_pattern : graph . annotation_pattern [ annotation ] = universe . annotation_pattern [ annotation ] elif annotation in universe . annotation_list : graph . annotation_list [ annotation ] = universe . annotation_list [ annotation ] else : log . warning ( 'annotation: %s missing from universe' , annotation )
12895	def get_modes ( self ) : if not self . __modes : self . __modes = yield from self . handle_list ( self . API . get ( 'valid_modes' ) ) return self . __modes
2549	def include ( f ) : fl = open ( f , 'r' ) data = fl . read ( ) fl . close ( ) return raw ( data )
13532	def ancestors_root ( self ) : if self . is_root ( ) : return [ ] ancestors = set ( [ ] ) self . _depth_ascend ( self , ancestors , True ) try : ancestors . remove ( self ) except KeyError : pass return list ( ancestors )
112	def is_activated ( self , images , augmenter , parents , default ) : if self . activator is None : return default else : return self . activator ( images , augmenter , parents , default )
227	def get_max_median_position_concentration ( positions ) : expos = get_percent_alloc ( positions ) expos = expos . drop ( 'cash' , axis = 1 ) longs = expos . where ( expos . applymap ( lambda x : x > 0 ) ) shorts = expos . where ( expos . applymap ( lambda x : x < 0 ) ) alloc_summary = pd . DataFrame ( ) alloc_summary [ 'max_long' ] = longs . max ( axis = 1 ) alloc_summary [ 'median_long' ] = longs . median ( axis = 1 ) alloc_summary [ 'median_short' ] = shorts . median ( axis = 1 ) alloc_summary [ 'max_short' ] = shorts . min ( axis = 1 ) return alloc_summary
11265	def readline ( prev , filename = None , mode = 'r' , trim = str . rstrip , start = 1 , end = sys . maxsize ) : if prev is None : if filename is None : raise Exception ( 'No input available for readline.' ) elif is_str_type ( filename ) : file_list = [ filename , ] else : file_list = filename else : file_list = prev for fn in file_list : if isinstance ( fn , file_type ) : fd = fn else : fd = open ( fn , mode ) try : if start <= 1 and end == sys . maxsize : for line in fd : yield trim ( line ) else : for line_no , line in enumerate ( fd , 1 ) : if line_no < start : continue yield trim ( line ) if line_no >= end : break finally : if fd != fn : fd . close ( )
10757	def writable_path ( path ) : if os . path . exists ( path ) : return os . access ( path , os . W_OK ) try : with open ( path , 'w' ) : pass except ( OSError , IOError ) : return False else : os . remove ( path ) return True
11915	def render ( self , template , ** data ) : dct = self . global_data . copy ( ) dct . update ( data ) try : html = self . env . get_template ( template ) . render ( ** dct ) except TemplateNotFound : raise JinjaTemplateNotFound return html
3266	def prepare_upload_bundle ( name , data ) : fd , path = mkstemp ( ) zip_file = ZipFile ( path , 'w' ) for ext , stream in data . items ( ) : fname = "%s.%s" % ( name , ext ) if ( isinstance ( stream , basestring ) ) : zip_file . write ( stream , fname ) else : zip_file . writestr ( fname , stream . read ( ) ) zip_file . close ( ) os . close ( fd ) return path
1902	def summarized_name ( self , name ) : components = name . split ( '.' ) prefix = '.' . join ( c [ 0 ] for c in components [ : - 1 ] ) return f'{prefix}.{components[-1]}'
12572	def put ( self , key , value , attrs = None , format = None , append = False , ** kwargs ) : if not isinstance ( value , np . ndarray ) : super ( NumpyHDFStore , self ) . put ( key , value , format , append , ** kwargs ) else : group = self . get_node ( key ) if group is not None and not append : self . _handle . removeNode ( group , recursive = True ) group = None if group is None : paths = key . split ( '/' ) path = '/' for p in paths : if not len ( p ) : continue new_path = path if not path . endswith ( '/' ) : new_path += '/' new_path += p group = self . get_node ( new_path ) if group is None : group = self . _handle . createGroup ( path , p ) path = new_path ds_name = kwargs . get ( 'ds_name' , self . _array_dsname ) ds = self . _handle . createArray ( group , ds_name , value ) if attrs is not None : for key in attrs : setattr ( ds . attrs , key , attrs [ key ] ) self . _handle . flush ( ) return ds
2255	def unique_flags ( items , key = None ) : len_ = len ( items ) if key is None : item_to_index = dict ( zip ( reversed ( items ) , reversed ( range ( len_ ) ) ) ) indices = item_to_index . values ( ) else : indices = argunique ( items , key = key ) flags = boolmask ( indices , len_ ) return flags
8075	def rect ( self , x , y , width , height , roundness = 0.0 , draw = True , ** kwargs ) : path = self . BezierPath ( ** kwargs ) path . rect ( x , y , width , height , roundness , self . rectmode ) if draw : path . draw ( ) return path
8641	def highlight_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'highlight' } endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotHighlightedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
13280	def child_end_handler ( self , scache ) : desc = self . desc desc_level = scache . desc_level breadth = desc_level . __len__ ( ) desc [ 'breadth' ] = breadth desc [ 'breadth_path' ] . append ( breadth ) desc_level . append ( desc )
10131	def _unescape ( s , uri = False ) : out = '' while len ( s ) > 0 : c = s [ 0 ] if c == '\\' : esc_c = s [ 1 ] if esc_c in ( 'u' , 'U' ) : out += six . unichr ( int ( s [ 2 : 6 ] , base = 16 ) ) s = s [ 6 : ] continue else : if esc_c == 'b' : out += '\b' elif esc_c == 'f' : out += '\f' elif esc_c == 'n' : out += '\n' elif esc_c == 'r' : out += '\r' elif esc_c == 't' : out += '\t' else : if uri and ( esc_c == '#' ) : out += '\\' out += esc_c s = s [ 2 : ] continue else : out += c s = s [ 1 : ] return out
12848	def add_safety_checks ( meta , members ) : for member_name , member_value in members . items ( ) : members [ member_name ] = meta . add_safety_check ( member_name , member_value )
8076	def rectmode ( self , mode = None ) : if mode in ( self . CORNER , self . CENTER , self . CORNERS ) : self . rectmode = mode return self . rectmode elif mode is None : return self . rectmode else : raise ShoebotError ( _ ( "rectmode: invalid input" ) )
2613	def pack_apply_message ( f , args , kwargs , buffer_threshold = MAX_BYTES , item_threshold = MAX_ITEMS ) : arg_bufs = list ( chain . from_iterable ( serialize_object ( arg , buffer_threshold , item_threshold ) for arg in args ) ) kw_keys = sorted ( kwargs . keys ( ) ) kwarg_bufs = list ( chain . from_iterable ( serialize_object ( kwargs [ key ] , buffer_threshold , item_threshold ) for key in kw_keys ) ) info = dict ( nargs = len ( args ) , narg_bufs = len ( arg_bufs ) , kw_keys = kw_keys ) msg = [ pickle . dumps ( can ( f ) , PICKLE_PROTOCOL ) ] msg . append ( pickle . dumps ( info , PICKLE_PROTOCOL ) ) msg . extend ( arg_bufs ) msg . extend ( kwarg_bufs ) return msg
12742	def get_ISBNs ( self ) : invalid_isbns = set ( self . get_invalid_ISBNs ( ) ) valid_isbns = [ self . _clean_isbn ( isbn ) for isbn in self [ "020a" ] if self . _clean_isbn ( isbn ) not in invalid_isbns ] if valid_isbns : return valid_isbns return [ self . _clean_isbn ( isbn ) for isbn in self [ "901i" ] ]
10359	def shuffle_relations ( graph : BELGraph , percentage : Optional [ str ] = None ) -> BELGraph : percentage = percentage or 0.3 assert 0 < percentage <= 1 n = graph . number_of_edges ( ) swaps = int ( percentage * n * ( n - 1 ) / 2 ) result : BELGraph = graph . copy ( ) edges = result . edges ( keys = True ) for _ in range ( swaps ) : ( s1 , t1 , k1 ) , ( s2 , t2 , k2 ) = random . sample ( edges , 2 ) result [ s1 ] [ t1 ] [ k1 ] , result [ s2 ] [ t2 ] [ k2 ] = result [ s2 ] [ t2 ] [ k2 ] , result [ s1 ] [ t1 ] [ k1 ] return result
9147	def web ( connection , host , port ) : from bio2bel . web . application import create_application app = create_application ( connection = connection ) app . run ( host = host , port = port )
6690	def repolist ( status = '' , media = None ) : manager = MANAGER with settings ( hide ( 'running' , 'stdout' ) ) : if media : repos = run_as_root ( "%(manager)s repolist %(status)s | sed '$d' | sed -n '/repo id/,$p'" % locals ( ) ) else : repos = run_as_root ( "%(manager)s repolist %(status)s | sed '/Media\\|Debug/d' | sed '$d' | sed -n '/repo id/,$p'" % locals ( ) ) return [ line . split ( ' ' ) [ 0 ] for line in repos . splitlines ( ) [ 1 : ] ]
4102	def generate_gallery_rst ( app ) : try : plot_gallery = eval ( app . builder . config . plot_gallery ) except TypeError : plot_gallery = bool ( app . builder . config . plot_gallery ) gallery_conf . update ( app . config . sphinx_gallery_conf ) gallery_conf . update ( plot_gallery = plot_gallery ) gallery_conf . update ( abort_on_example_error = app . builder . config . abort_on_example_error ) app . config . sphinx_gallery_conf = gallery_conf app . config . html_static_path . append ( glr_path_static ( ) ) clean_gallery_out ( app . builder . outdir ) examples_dirs = gallery_conf [ 'examples_dirs' ] gallery_dirs = gallery_conf [ 'gallery_dirs' ] if not isinstance ( examples_dirs , list ) : examples_dirs = [ examples_dirs ] if not isinstance ( gallery_dirs , list ) : gallery_dirs = [ gallery_dirs ] mod_examples_dir = os . path . relpath ( gallery_conf [ 'mod_example_dir' ] , app . builder . srcdir ) seen_backrefs = set ( ) for examples_dir , gallery_dir in zip ( examples_dirs , gallery_dirs ) : examples_dir = os . path . relpath ( examples_dir , app . builder . srcdir ) gallery_dir = os . path . relpath ( gallery_dir , app . builder . srcdir ) for workdir in [ examples_dir , gallery_dir , mod_examples_dir ] : if not os . path . exists ( workdir ) : os . makedirs ( workdir ) fhindex = open ( os . path . join ( gallery_dir , 'index.rst' ) , 'w' ) fhindex . write ( generate_dir_rst ( examples_dir , gallery_dir , gallery_conf , seen_backrefs ) ) for directory in sorted ( os . listdir ( examples_dir ) ) : if os . path . isdir ( os . path . join ( examples_dir , directory ) ) : src_dir = os . path . join ( examples_dir , directory ) target_dir = os . path . join ( gallery_dir , directory ) fhindex . write ( generate_dir_rst ( src_dir , target_dir , gallery_conf , seen_backrefs ) ) fhindex . flush ( )
9104	def dropbox_post_factory ( request ) : try : max_age = int ( request . registry . settings . get ( 'post_token_max_age_seconds' ) ) except Exception : max_age = 300 try : drop_id = parse_post_token ( token = request . matchdict [ 'token' ] , secret = request . registry . settings [ 'post_secret' ] , max_age = max_age ) except SignatureExpired : raise HTTPGone ( 'dropbox expired' ) except Exception : raise HTTPNotFound ( 'no such dropbox' ) dropbox = request . registry . settings [ 'dropbox_container' ] . get_dropbox ( drop_id ) if dropbox . status_int >= 20 : raise HTTPGone ( 'dropbox already in processing, no longer accepts data' ) return dropbox
45	def compute_geometric_median ( X , eps = 1e-5 ) : y = np . mean ( X , 0 ) while True : D = scipy . spatial . distance . cdist ( X , [ y ] ) nonzeros = ( D != 0 ) [ : , 0 ] Dinv = 1 / D [ nonzeros ] Dinvs = np . sum ( Dinv ) W = Dinv / Dinvs T = np . sum ( W * X [ nonzeros ] , 0 ) num_zeros = len ( X ) - np . sum ( nonzeros ) if num_zeros == 0 : y1 = T elif num_zeros == len ( X ) : return y else : R = ( T - y ) * Dinvs r = np . linalg . norm ( R ) rinv = 0 if r == 0 else num_zeros / r y1 = max ( 0 , 1 - rinv ) * T + min ( 1 , rinv ) * y if scipy . spatial . distance . euclidean ( y , y1 ) < eps : return y1 y = y1
4190	def window_poisson_hanning ( N , alpha = 2 ) : r w1 = window_hann ( N ) w2 = window_poisson ( N , alpha = alpha ) return w1 * w2
10829	def accept ( self ) : with db . session . begin_nested ( ) : self . state = MembershipState . ACTIVE db . session . merge ( self )
8704	def read_file ( self , filename , destination = '' ) : if not destination : destination = filename log . info ( 'Transferring %s to %s' , filename , destination ) data = self . download_file ( filename ) log . info ( destination ) if not os . path . exists ( os . path . dirname ( destination ) ) : try : os . makedirs ( os . path . dirname ( destination ) ) except OSError as e : if e . errno != errno . EEXIST : raise with open ( destination , 'w' ) as fil : fil . write ( data )
12726	def stop_cfms ( self , stop_cfms ) : _set_params ( self . ode_obj , 'StopCFM' , stop_cfms , self . ADOF + self . LDOF )
13048	def f_annotation_filter ( annotations , type_uri , number ) : filtered = [ annotation for annotation in annotations if annotation . type_uri == type_uri ] number = min ( [ len ( filtered ) , number ] ) if number == 0 : return None else : return filtered [ number - 1 ]
7999	def auth_properties ( self ) : props = dict ( self . settings [ "extra_auth_properties" ] ) if self . transport : props . update ( self . transport . auth_properties ) props [ "local-jid" ] = self . me props [ "service-type" ] = "xmpp" return props
496	def _classifyState ( self , state ) : if state . ROWID < self . getParameter ( 'trainRecords' ) : if not state . setByUser : state . anomalyLabel = [ ] self . _deleteRecordsFromKNN ( [ state ] ) return label = KNNAnomalyClassifierRegion . AUTO_THRESHOLD_CLASSIFIED_LABEL autoLabel = label + KNNAnomalyClassifierRegion . AUTO_TAG newCategory = self . _recomputeRecordFromKNN ( state ) labelList = self . _categoryToLabelList ( newCategory ) if state . setByUser : if label in state . anomalyLabel : state . anomalyLabel . remove ( label ) if autoLabel in state . anomalyLabel : state . anomalyLabel . remove ( autoLabel ) labelList . extend ( state . anomalyLabel ) if state . anomalyScore >= self . getParameter ( 'anomalyThreshold' ) : labelList . append ( label ) elif label in labelList : ind = labelList . index ( label ) labelList [ ind ] = autoLabel labelList = list ( set ( labelList ) ) if label in labelList and autoLabel in labelList : labelList . remove ( autoLabel ) if state . anomalyLabel == labelList : return state . anomalyLabel = labelList if state . anomalyLabel == [ ] : self . _deleteRecordsFromKNN ( [ state ] ) else : self . _addRecordToKNN ( state )
7089	def jd_corr ( jd , ra , dec , obslon = None , obslat = None , obsalt = None , jd_type = 'bjd' ) : if not HAVEKERNEL : LOGERROR ( 'no JPL kernel available, can\'t continue!' ) return rarad = np . radians ( ra ) decrad = np . radians ( dec ) cosra = np . cos ( rarad ) sinra = np . sin ( rarad ) cosdec = np . cos ( decrad ) sindec = np . sin ( decrad ) src_unitvector = np . array ( [ cosdec * cosra , cosdec * sinra , sindec ] ) if ( obslon is None ) or ( obslat is None ) or ( obsalt is None ) : t = astime . Time ( jd , scale = 'utc' , format = 'jd' ) else : t = astime . Time ( jd , scale = 'utc' , format = 'jd' , location = ( '%.5fd' % obslon , '%.5fd' % obslat , obsalt ) ) barycenter_earthmoon = jplkernel [ 0 , 3 ] . compute ( t . tdb . jd ) moonvector = ( jplkernel [ 3 , 301 ] . compute ( t . tdb . jd ) - jplkernel [ 3 , 399 ] . compute ( t . tdb . jd ) ) pos_earth = ( barycenter_earthmoon - moonvector * 1.0 / ( 1.0 + EMRAT ) ) if jd_type == 'bjd' : correction_seconds = np . dot ( pos_earth . T , src_unitvector ) / CLIGHT_KPS correction_days = correction_seconds / SEC_P_DAY elif jd_type == 'hjd' : pos_sun = jplkernel [ 0 , 10 ] . compute ( t . tdb . jd ) sun_earth_vec = pos_earth - pos_sun correction_seconds = np . dot ( sun_earth_vec . T , src_unitvector ) / CLIGHT_KPS correction_days = correction_seconds / SEC_P_DAY new_jd = t . tdb . jd + correction_days return new_jd
506	def getLabels ( self , start = None , end = None ) : if len ( self . _recordsCache ) == 0 : return { 'isProcessing' : False , 'recordLabels' : [ ] } try : start = int ( start ) except Exception : start = 0 try : end = int ( end ) except Exception : end = self . _recordsCache [ - 1 ] . ROWID if end <= start : raise HTMPredictionModelInvalidRangeError ( "Invalid supplied range for 'getLabels'." , debugInfo = { 'requestRange' : { 'startRecordID' : start , 'endRecordID' : end } , 'numRecordsStored' : len ( self . _recordsCache ) } ) results = { 'isProcessing' : False , 'recordLabels' : [ ] } ROWIDX = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) validIdx = numpy . where ( ( ROWIDX >= start ) & ( ROWIDX < end ) ) [ 0 ] . tolist ( ) categories = self . _knnclassifier . getCategoryList ( ) for idx in validIdx : row = dict ( ROWID = int ( ROWIDX [ idx ] ) , labels = self . _categoryToLabelList ( categories [ idx ] ) ) results [ 'recordLabels' ] . append ( row ) return results
13729	def balance_over_time ( address ) : forged_blocks = None txhistory = Address . transactions ( address ) delegates = Delegate . delegates ( ) for i in delegates : if address == i . address : forged_blocks = Delegate . blocks ( i . pubkey ) balance_over_time = [ ] balance = 0 block = 0 Balance = namedtuple ( 'balance' , 'timestamp amount' ) for tx in txhistory : if forged_blocks : while forged_blocks [ block ] . timestamp <= tx . timestamp : balance += ( forged_blocks [ block ] . reward + forged_blocks [ block ] . totalFee ) balance_over_time . append ( Balance ( timestamp = forged_blocks [ block ] . timestamp , amount = balance ) ) block += 1 if tx . senderId == address : balance -= ( tx . amount + tx . fee ) res = Balance ( timestamp = tx . timestamp , amount = balance ) balance_over_time . append ( res ) if tx . recipientId == address : balance += tx . amount res = Balance ( timestamp = tx . timestamp , amount = balance ) balance_over_time . append ( res ) if forged_blocks and block <= len ( forged_blocks ) - 1 : if forged_blocks [ block ] . timestamp > txhistory [ - 1 ] . timestamp : for i in forged_blocks [ block : ] : balance += ( i . reward + i . totalFee ) res = Balance ( timestamp = i . timestamp , amount = balance ) balance_over_time . append ( res ) return balance_over_time
1346	def gradient ( self , image , label ) : _ , gradient = self . predictions_and_gradient ( image , label ) return gradient
3129	def get_subscriber_hash ( member_email ) : check_email ( member_email ) member_email = member_email . lower ( ) . encode ( ) m = hashlib . md5 ( member_email ) return m . hexdigest ( )
8294	def clique ( graph , id ) : clique = [ id ] for n in graph . nodes : friend = True for id in clique : if n . id == id or graph . edge ( n . id , id ) == None : friend = False break if friend : clique . append ( n . id ) return clique
6856	def query ( query , use_sudo = True , ** kwargs ) : func = use_sudo and run_as_root or run user = kwargs . get ( 'mysql_user' ) or env . get ( 'mysql_user' ) password = kwargs . get ( 'mysql_password' ) or env . get ( 'mysql_password' ) options = [ '--batch' , '--raw' , '--skip-column-names' , ] if user : options . append ( '--user=%s' % quote ( user ) ) if password : options . append ( '--password=%s' % quote ( password ) ) options = ' ' . join ( options ) return func ( 'mysql %(options)s --execute=%(query)s' % { 'options' : options , 'query' : quote ( query ) , } )
13856	def getbalance ( self , url = 'http://services.ambientmobile.co.za/credits' ) : postXMLList = [ ] postXMLList . append ( "<api-key>%s</api-key>" % self . api_key ) postXMLList . append ( "<password>%s</password>" % self . password ) postXML = '<sms>%s</sms>' % "" . join ( postXMLList ) result = self . curl ( url , postXML ) if result . get ( "credits" , None ) : return result [ "credits" ] else : raise AmbientSMSError ( result [ "status" ] )
5695	def import_ ( self , conn ) : if self . print_progress : print ( 'Beginning' , self . __class__ . __name__ ) self . _conn = conn self . create_table ( conn ) if self . mode in ( 'all' , 'import' ) and self . fname and self . exists ( ) and self . table not in ignore_tables : self . insert_data ( conn ) if self . mode in ( 'all' , 'index' ) and hasattr ( self , 'index' ) : self . create_index ( conn ) if self . mode in ( 'all' , 'import' ) and hasattr ( self , 'post_import' ) : self . run_post_import ( conn ) conn . commit ( )
5046	def _enroll_users ( cls , request , enterprise_customer , emails , mode , course_id = None , program_details = None , notify = True ) : pending_messages = [ ] if course_id : succeeded , pending , failed = cls . enroll_users_in_course ( enterprise_customer = enterprise_customer , course_id = course_id , course_mode = mode , emails = emails , ) all_successes = succeeded + pending if notify : enterprise_customer . notify_enrolled_learners ( catalog_api_user = request . user , course_id = course_id , users = all_successes , ) if succeeded : pending_messages . append ( cls . get_success_enrollment_message ( succeeded , course_id ) ) if failed : pending_messages . append ( cls . get_failed_enrollment_message ( failed , course_id ) ) if pending : pending_messages . append ( cls . get_pending_enrollment_message ( pending , course_id ) ) if program_details : succeeded , pending , failed = cls . enroll_users_in_program ( enterprise_customer = enterprise_customer , program_details = program_details , course_mode = mode , emails = emails , ) all_successes = succeeded + pending if notify : cls . notify_program_learners ( enterprise_customer = enterprise_customer , program_details = program_details , users = all_successes ) program_identifier = program_details . get ( 'title' , program_details . get ( 'uuid' , _ ( 'the program' ) ) ) if succeeded : pending_messages . append ( cls . get_success_enrollment_message ( succeeded , program_identifier ) ) if failed : pending_messages . append ( cls . get_failed_enrollment_message ( failed , program_identifier ) ) if pending : pending_messages . append ( cls . get_pending_enrollment_message ( pending , program_identifier ) ) cls . send_messages ( request , pending_messages )
1783	def ADC ( cpu , dest , src ) : cpu . _ADD ( dest , src , carry = True )
2075	def score_models ( clf , X , y , encoder , runs = 1 ) : scores = [ ] X_test = None for _ in range ( runs ) : X_test = encoder ( ) . fit_transform ( X , y ) X_test = StandardScaler ( ) . fit_transform ( X_test ) scores . append ( cross_validate ( clf , X_test , y , n_jobs = 1 , cv = 5 ) [ 'test_score' ] ) gc . collect ( ) scores = [ y for z in [ x for x in scores ] for y in z ] return float ( np . mean ( scores ) ) , float ( np . std ( scores ) ) , scores , X_test . shape [ 1 ]
7732	def get_join_info ( self ) : x = self . get_muc_child ( ) if not x : return None if not isinstance ( x , MucX ) : return None return x
936	def writeBaseToProto ( self , proto ) : inferenceType = self . getInferenceType ( ) inferenceType = inferenceType [ : 1 ] . lower ( ) + inferenceType [ 1 : ] proto . inferenceType = inferenceType proto . numPredictions = self . _numPredictions proto . learningEnabled = self . __learningEnabled proto . inferenceEnabled = self . __inferenceEnabled proto . inferenceArgs = json . dumps ( self . __inferenceArgs )
7208	def task_ids ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get task IDs.' ) if self . batch_values : raise NotImplementedError ( "Query Each Workflow Id within the Batch Workflow for task IDs." ) wf = self . workflow . get ( self . id ) return [ task [ 'id' ] for task in wf [ 'tasks' ] ]
13660	def subroute ( self , * components ) : def _factory ( f ) : self . _addRoute ( f , subroute ( * components ) ) return f return _factory
3280	def resolve_provider ( self , path ) : share = None lower_path = path . lower ( ) for r in self . sorted_share_list : if r == "/" : share = r break elif lower_path == r or lower_path . startswith ( r + "/" ) : share = r break if share is None : return None , None return share , self . provider_map . get ( share )
3066	def _apply_user_agent ( headers , user_agent ) : if user_agent is not None : if 'user-agent' in headers : headers [ 'user-agent' ] = ( user_agent + ' ' + headers [ 'user-agent' ] ) else : headers [ 'user-agent' ] = user_agent return headers
11576	def sonar_data ( self , data ) : val = int ( ( data [ self . MSB ] << 7 ) + data [ self . LSB ] ) pin_number = data [ 0 ] with self . pymata . data_lock : sonar_pin_entry = self . active_sonar_map [ pin_number ] self . digital_response_table [ data [ self . RESPONSE_TABLE_MODE ] ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] = val if sonar_pin_entry [ 0 ] is not None : if sonar_pin_entry [ 1 ] != val : self . active_sonar_map [ pin_number ] [ 0 ] ( [ self . pymata . SONAR , pin_number , val ] ) sonar_pin_entry [ 1 ] = val self . active_sonar_map [ pin_number ] = sonar_pin_entry
4930	def transform_courserun_title ( self , content_metadata_item ) : title = content_metadata_item . get ( 'title' ) or '' course_run_start = content_metadata_item . get ( 'start' ) if course_run_start : if course_available_for_enrollment ( content_metadata_item ) : title += ' ({starts}: {:%B %Y})' . format ( parse_lms_api_datetime ( course_run_start ) , starts = _ ( 'Starts' ) ) else : title += ' ({:%B %Y} - {enrollment_closed})' . format ( parse_lms_api_datetime ( course_run_start ) , enrollment_closed = _ ( 'Enrollment Closed' ) ) title_with_locales = [ ] content_metadata_language_code = transform_language_code ( content_metadata_item . get ( 'content_language' , '' ) ) for locale in self . enterprise_configuration . get_locales ( default_locale = content_metadata_language_code ) : title_with_locales . append ( { 'locale' : locale , 'value' : title } ) return title_with_locales
12991	def create_hierarchy ( hierarchy , level ) : if level not in hierarchy : hierarchy [ level ] = OrderedDict ( ) return hierarchy [ level ]
5713	def is_safe_path ( path ) : contains_windows_var = lambda val : re . match ( r'%.+%' , val ) contains_posix_var = lambda val : re . match ( r'\$.+' , val ) unsafeness_conditions = [ os . path . isabs ( path ) , ( '..%s' % os . path . sep ) in path , path . startswith ( '~' ) , os . path . expandvars ( path ) != path , contains_windows_var ( path ) , contains_posix_var ( path ) , ] return not any ( unsafeness_conditions )
5458	def from_yaml ( cls , yaml_string ) : try : job = yaml . full_load ( yaml_string ) except AttributeError : job = yaml . load ( yaml_string ) dsub_version = job . get ( 'dsub-version' ) if not dsub_version : return cls . _from_yaml_v0 ( job ) job_metadata = { } for key in [ 'job-id' , 'job-name' , 'task-ids' , 'user-id' , 'dsub-version' , 'user-project' , 'script-name' ] : if job . get ( key ) is not None : job_metadata [ key ] = job . get ( key ) job_metadata [ 'create-time' ] = dsub_util . replace_timezone ( job . get ( 'create-time' ) , pytz . utc ) job_resources = Resources ( logging = job . get ( 'logging' ) ) job_params = { } job_params [ 'labels' ] = cls . _label_params_from_dict ( job . get ( 'labels' , { } ) ) job_params [ 'envs' ] = cls . _env_params_from_dict ( job . get ( 'envs' , { } ) ) job_params [ 'inputs' ] = cls . _input_file_params_from_dict ( job . get ( 'inputs' , { } ) , False ) job_params [ 'input-recursives' ] = cls . _input_file_params_from_dict ( job . get ( 'input-recursives' , { } ) , True ) job_params [ 'outputs' ] = cls . _output_file_params_from_dict ( job . get ( 'outputs' , { } ) , False ) job_params [ 'output-recursives' ] = cls . _output_file_params_from_dict ( job . get ( 'output-recursives' , { } ) , True ) job_params [ 'mounts' ] = cls . _mount_params_from_dict ( job . get ( 'mounts' , { } ) ) task_descriptors = [ ] for task in job . get ( 'tasks' , [ ] ) : task_metadata = { 'task-id' : task . get ( 'task-id' ) } create_time = task . get ( 'create-time' ) if create_time : task_metadata [ 'create-time' ] = dsub_util . replace_timezone ( create_time , pytz . utc ) if task . get ( 'task-attempt' ) is not None : task_metadata [ 'task-attempt' ] = task . get ( 'task-attempt' ) task_params = { } task_params [ 'labels' ] = cls . _label_params_from_dict ( task . get ( 'labels' , { } ) ) task_params [ 'envs' ] = cls . _env_params_from_dict ( task . get ( 'envs' , { } ) ) task_params [ 'inputs' ] = cls . _input_file_params_from_dict ( task . get ( 'inputs' , { } ) , False ) task_params [ 'input-recursives' ] = cls . _input_file_params_from_dict ( task . get ( 'input-recursives' , { } ) , True ) task_params [ 'outputs' ] = cls . _output_file_params_from_dict ( task . get ( 'outputs' , { } ) , False ) task_params [ 'output-recursives' ] = cls . _output_file_params_from_dict ( task . get ( 'output-recursives' , { } ) , True ) task_resources = Resources ( logging_path = task . get ( 'logging-path' ) ) task_descriptors . append ( TaskDescriptor ( task_metadata , task_params , task_resources ) ) return JobDescriptor ( job_metadata , job_params , job_resources , task_descriptors )
12702	def _get_params ( target , param , dof ) : return [ target . getParam ( getattr ( ode , 'Param{}{}' . format ( param , s ) ) ) for s in [ '' , '2' , '3' ] [ : dof ] ]
3185	def get ( self , store_id , product_id , image_id , ** queryparams ) : self . store_id = store_id self . product_id = product_id self . image_id = image_id return self . _mc_client . _post ( url = self . _build_path ( store_id , 'products' , product_id , 'images' , image_id ) , ** queryparams )
3143	def get ( self , file_id , ** queryparams ) : self . file_id = file_id return self . _mc_client . _get ( url = self . _build_path ( file_id ) , ** queryparams )
2120	def associate_success_node ( self , parent , child = None , ** kwargs ) : return self . _assoc_or_create ( 'success' , parent , child , ** kwargs )
11488	def _find_resource_id_from_path ( path ) : session . token = verify_credentials ( ) parsed_path = path . split ( '/' ) if parsed_path [ - 1 ] == '' : parsed_path . pop ( ) if path . startswith ( '/users/' ) : parsed_path . pop ( 0 ) parsed_path . pop ( 0 ) name = parsed_path . pop ( 0 ) firstname , lastname = name . split ( '_' ) end = parsed_path . pop ( ) user = session . communicator . get_user_by_name ( firstname , lastname ) leaf_folder_id = _descend_folder_for_id ( parsed_path , user [ 'folder_id' ] ) return _search_folder_for_item_or_folder ( end , leaf_folder_id ) elif path . startswith ( '/communities/' ) : print ( parsed_path ) parsed_path . pop ( 0 ) parsed_path . pop ( 0 ) community_name = parsed_path . pop ( 0 ) end = parsed_path . pop ( ) community = session . communicator . get_community_by_name ( community_name ) leaf_folder_id = _descend_folder_for_id ( parsed_path , community [ 'folder_id' ] ) return _search_folder_for_item_or_folder ( end , leaf_folder_id ) else : return False , - 1
5161	def __intermediate_addresses ( self , interface ) : address_list = self . get_copy ( interface , 'addresses' ) if not address_list : return [ { 'proto' : 'none' } ] result = [ ] static = { } dhcp = [ ] for address in address_list : family = address . get ( 'family' ) if address [ 'proto' ] == 'dhcp' : address [ 'proto' ] = 'dhcp' if family == 'ipv4' else 'dhcpv6' dhcp . append ( self . __intermediate_address ( address ) ) continue if 'gateway' in address : uci_key = 'gateway' if family == 'ipv4' else 'ip6gw' interface [ uci_key ] = address [ 'gateway' ] address_key = 'ipaddr' if family == 'ipv4' else 'ip6addr' static . setdefault ( address_key , [ ] ) static [ address_key ] . append ( '{address}/{mask}' . format ( ** address ) ) static . update ( self . __intermediate_address ( address ) ) if static : if len ( static . get ( 'ipaddr' , [ ] ) ) == 1 : network = ip_interface ( six . text_type ( static [ 'ipaddr' ] [ 0 ] ) ) static [ 'ipaddr' ] = str ( network . ip ) static [ 'netmask' ] = str ( network . netmask ) if len ( static . get ( 'ip6addr' , [ ] ) ) == 1 : static [ 'ip6addr' ] = static [ 'ip6addr' ] [ 0 ] result . append ( static ) if dhcp : result += dhcp return result
9027	def _width ( self ) : layout = self . _instruction . get ( GRID_LAYOUT ) if layout is not None : width = layout . get ( WIDTH ) if width is not None : return width return self . _instruction . number_of_consumed_meshes
6035	def map_function ( self , func , * arg_lists ) : return GridStack ( * [ func ( * args ) for args in zip ( self , * arg_lists ) ] )
6622	def configure ( self , component , all_dependencies ) : r = { } builddir = self . buildroot available_dependencies = OrderedDict ( ( k , v ) for k , v in all_dependencies . items ( ) if v ) self . set_toplevel_definitions = '' if self . build_info_include_file is None : self . build_info_include_file , build_info_definitions = self . getBuildInfo ( component . path , builddir ) self . set_toplevel_definitions += build_info_definitions if self . config_include_file is None : self . config_include_file , config_definitions , self . config_json_file = self . _getConfigData ( available_dependencies , component , builddir , self . build_info_include_file ) self . set_toplevel_definitions += config_definitions self . configured = True return { 'merged_config_include' : self . config_include_file , 'merged_config_json' : self . config_json_file , 'build_info_include' : self . build_info_include_file }
1691	def InnermostClass ( self ) : for i in range ( len ( self . stack ) , 0 , - 1 ) : classinfo = self . stack [ i - 1 ] if isinstance ( classinfo , _ClassInfo ) : return classinfo return None
9702	def checkTUN ( self ) : packet = self . _TUN . _tun . read ( self . _TUN . _tun . mtu ) return ( packet )
8235	def split_complementary ( clr ) : clr = color ( clr ) colors = colorlist ( clr ) clr = clr . complement colors . append ( clr . rotate_ryb ( - 30 ) . lighten ( 0.1 ) ) colors . append ( clr . rotate_ryb ( 30 ) . lighten ( 0.1 ) ) return colors
5173	def get_install_requires ( ) : requirements = [ ] for line in open ( 'requirements.txt' ) . readlines ( ) : if line . startswith ( '#' ) or line == '' or line . startswith ( 'http' ) or line . startswith ( 'git' ) : continue requirements . append ( line . replace ( '\n' , '' ) ) if sys . version_info . major < 3 : requirements . append ( 'py2-ipaddress' ) return requirements
10951	def update ( self , params , values ) : return super ( State , self ) . update ( params , values )
9042	def eigh ( self ) : from numpy . linalg import svd if self . _cache [ "eig" ] is not None : return self . _cache [ "eig" ] U , S = svd ( self . L ) [ : 2 ] S *= S S += self . _epsilon self . _cache [ "eig" ] = S , U return self . _cache [ "eig" ]
1538	def add_spout ( self , name , spout_cls , par , config = None , optional_outputs = None ) : spout_spec = spout_cls . spec ( name = name , par = par , config = config , optional_outputs = optional_outputs ) self . add_spec ( spout_spec ) return spout_spec
1672	def ProcessFile ( filename , vlevel , extra_check_functions = None ) : _SetVerboseLevel ( vlevel ) _BackupFilters ( ) if not ProcessConfigOverrides ( filename ) : _RestoreFilters ( ) return lf_lines = [ ] crlf_lines = [ ] try : if filename == '-' : lines = codecs . StreamReaderWriter ( sys . stdin , codecs . getreader ( 'utf8' ) , codecs . getwriter ( 'utf8' ) , 'replace' ) . read ( ) . split ( '\n' ) else : lines = codecs . open ( filename , 'r' , 'utf8' , 'replace' ) . read ( ) . split ( '\n' ) for linenum in range ( len ( lines ) - 1 ) : if lines [ linenum ] . endswith ( '\r' ) : lines [ linenum ] = lines [ linenum ] . rstrip ( '\r' ) crlf_lines . append ( linenum + 1 ) else : lf_lines . append ( linenum + 1 ) except IOError : _cpplint_state . PrintError ( "Skipping input '%s': Can't open for reading\n" % filename ) _RestoreFilters ( ) return file_extension = filename [ filename . rfind ( '.' ) + 1 : ] if filename != '-' and file_extension not in GetAllExtensions ( ) : bazel_gen_files = set ( [ "external/local_config_cc/libtool" , "external/local_config_cc/make_hashed_objlist.py" , "external/local_config_cc/wrapped_ar" , "external/local_config_cc/wrapped_clang" , "external/local_config_cc/xcrunwrapper.sh" , ] ) if not filename in bazel_gen_files : _cpplint_state . PrintError ( 'Ignoring %s; not a valid file name ' '(%s)\n' % ( filename , ', ' . join ( GetAllExtensions ( ) ) ) ) else : ProcessFileData ( filename , file_extension , lines , Error , extra_check_functions ) if lf_lines and crlf_lines : for linenum in crlf_lines : Error ( filename , linenum , 'whitespace/newline' , 1 , 'Unexpected \\r (^M) found; better to use only \\n' ) _RestoreFilters ( )
1488	def save_module ( self , obj ) : self . modules . add ( obj ) self . save_reduce ( subimport , ( obj . __name__ , ) , obj = obj )
9570	async def _get_response ( self , message ) : view = self . discovery_view ( message ) if not view : return if inspect . iscoroutinefunction ( view ) : response = await view ( message ) else : response = view ( message ) return self . prepare_response ( response , message )
4908	def _post ( self , url , data , scope ) : self . _create_session ( scope ) response = self . session . post ( url , data = data ) return response . status_code , response . text
78	def AdditivePoissonNoise ( lam = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : lam2 = iap . handle_continuous_param ( lam , "lam" , value_range = ( 0 , None ) , tuple_to_uniform = True , list_to_choice = True ) if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return AddElementwise ( iap . RandomSign ( iap . Poisson ( lam = lam2 ) ) , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
41	def update_priorities ( self , idxes , priorities ) : assert len ( idxes ) == len ( priorities ) for idx , priority in zip ( idxes , priorities ) : assert priority > 0 assert 0 <= idx < len ( self . _storage ) self . _it_sum [ idx ] = priority ** self . _alpha self . _it_min [ idx ] = priority ** self . _alpha self . _max_priority = max ( self . _max_priority , priority )
12061	def processArgs ( ) : if len ( sys . argv ) < 2 : print ( "\n\nERROR:" ) print ( "this script requires arguments!" ) print ( 'try "python command.py info"' ) return if sys . argv [ 1 ] == 'info' : print ( "import paths:\n " , "\n " . join ( sys . path ) ) print ( ) print ( "python version:" , sys . version ) print ( "SWHLab path:" , __file__ ) print ( "SWHLab version:" , swhlab . __version__ ) return if sys . argv [ 1 ] == 'glanceFolder' : abfFolder = swhlab . common . gui_getFolder ( ) if not abfFolder or not os . path . isdir ( abfFolder ) : print ( "bad path" ) return fnames = sorted ( glob . glob ( abfFolder + "/*.abf" ) ) outFolder = tempfile . gettempdir ( ) + "/swhlab/" if os . path . exists ( outFolder ) : shutil . rmtree ( outFolder ) os . mkdir ( outFolder ) outFile = outFolder + "/index.html" out = '<html><body>' out += '<h2>%s</h2>' % abfFolder for i , fname in enumerate ( fnames ) : print ( "\n\n### PROCESSING %d of %d" % ( i , len ( fnames ) ) ) saveAs = os . path . join ( os . path . dirname ( outFolder ) , os . path . basename ( fname ) ) + ".png" out += '<br><br><br><code>%s</code><br>' % os . path . abspath ( fname ) out += '<a href="%s"><img src="%s"></a><br>' % ( saveAs , saveAs ) swhlab . analysis . glance . processAbf ( fname , saveAs ) out += '</body></html>' with open ( outFile , 'w' ) as f : f . write ( out ) webbrowser . open_new_tab ( outFile ) return print ( "\n\nERROR:\nI'm not sure how to process these arguments!" ) print ( sys . argv )
1217	def register_saver_ops ( self ) : variables = self . get_savable_variables ( ) if variables is None or len ( variables ) == 0 : self . _saver = None return base_scope = self . _get_base_variable_scope ( ) variables_map = { strip_name_scope ( v . name , base_scope ) : v for v in variables } self . _saver = tf . train . Saver ( var_list = variables_map , reshape = False , sharded = False , max_to_keep = 5 , keep_checkpoint_every_n_hours = 10000.0 , name = None , restore_sequentially = False , saver_def = None , builder = None , defer_build = False , allow_empty = True , write_version = tf . train . SaverDef . V2 , pad_step_number = False , save_relative_paths = True )
1307	def IsDesktopLocked ( ) -> bool : isLocked = False desk = ctypes . windll . user32 . OpenDesktopW ( ctypes . c_wchar_p ( 'Default' ) , 0 , 0 , 0x0100 ) if desk : isLocked = not ctypes . windll . user32 . SwitchDesktop ( desk ) ctypes . windll . user32 . CloseDesktop ( desk ) return isLocked
1925	def get_group ( name : str ) -> _Group : global _groups if name in _groups : return _groups [ name ] group = _Group ( name ) _groups [ name ] = group return group
7645	def pitch_hz_to_contour ( annotation ) : annotation . namespace = 'pitch_contour' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = dict ( index = 0 , frequency = np . abs ( obs . value ) , voiced = obs . value > 0 ) ) return annotation
1373	def defaults_cluster_role_env ( cluster_role_env ) : if len ( cluster_role_env [ 1 ] ) == 0 and len ( cluster_role_env [ 2 ] ) == 0 : return ( cluster_role_env [ 0 ] , getpass . getuser ( ) , ENVIRON ) return ( cluster_role_env [ 0 ] , cluster_role_env [ 1 ] , cluster_role_env [ 2 ] )
701	def firstNonFullGeneration ( self , swarmId , minNumParticles ) : if not swarmId in self . _swarmNumParticlesPerGeneration : return None numPsPerGen = self . _swarmNumParticlesPerGeneration [ swarmId ] numPsPerGen = numpy . array ( numPsPerGen ) firstNonFull = numpy . where ( numPsPerGen < minNumParticles ) [ 0 ] if len ( firstNonFull ) == 0 : return len ( numPsPerGen ) else : return firstNonFull [ 0 ]
6796	def manage_async ( self , command = '' , name = 'process' , site = ALL , exclude_sites = '' , end_message = '' , recipients = '' ) : exclude_sites = exclude_sites . split ( ':' ) r = self . local_renderer for _site , site_data in self . iter_sites ( site = site , no_secure = True ) : if _site in exclude_sites : continue r . env . SITE = _site r . env . command = command r . env . end_email_command = '' r . env . recipients = recipients or '' r . env . end_email_command = '' if end_message : end_message = end_message + ' for ' + _site end_message = end_message . replace ( ' ' , '_' ) r . env . end_message = end_message r . env . end_email_command = r . format ( '{manage_cmd} send_mail --subject={end_message} --recipients={recipients}' ) r . env . name = name . format ( ** r . genv ) r . run ( 'screen -dmS {name} bash -c "export SITE={SITE}; ' 'export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} {command} --traceback; {end_email_command}"; sleep 3;' )
12710	def relative_offset_to_world ( self , offset ) : return np . array ( self . body_to_world ( offset * self . dimensions / 2 ) )
6120	def elliptical ( cls , shape , pixel_scale , major_axis_radius_arcsec , axis_ratio , phi , centre = ( 0. , 0. ) , invert = False ) : mask = mask_util . mask_elliptical_from_shape_pixel_scale_and_radius ( shape , pixel_scale , major_axis_radius_arcsec , axis_ratio , phi , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
13114	def resolve_domains ( domains , disable_zone = False ) : dnsresolver = dns . resolver . Resolver ( ) ips = [ ] for domain in domains : print_notification ( "Resolving {}" . format ( domain ) ) try : result = dnsresolver . query ( domain , 'A' ) for a in result . response . answer [ 0 ] : ips . append ( str ( a ) ) if not disable_zone : ips . extend ( zone_transfer ( str ( a ) , domain ) ) except dns . resolver . NXDOMAIN as e : print_error ( e ) return ips
2571	def send_message ( self ) : start = time . time ( ) message = None if not self . initialized : message = self . construct_start_message ( ) self . initialized = True else : message = self . construct_end_message ( ) self . send_UDP_message ( message ) end = time . time ( ) return end - start
6858	def database_exists ( name , ** kwargs ) : with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : res = query ( "SHOW DATABASES LIKE '%(name)s';" % { 'name' : name } , ** kwargs ) return res . succeeded and ( res == name )
141	def to_line_string ( self , closed = True ) : from imgaug . augmentables . lines import LineString if not closed or len ( self . exterior ) <= 1 : return LineString ( self . exterior , label = self . label ) return LineString ( np . concatenate ( [ self . exterior , self . exterior [ 0 : 1 , : ] ] , axis = 0 ) , label = self . label )
7718	def free ( self ) : if not self . borrowed : self . xmlnode . unlinkNode ( ) self . xmlnode . freeNode ( ) self . xmlnode = None
1763	def pop_bytes ( self , nbytes , force = False ) : data = self . read_bytes ( self . STACK , nbytes , force = force ) self . STACK += nbytes return data
13730	def value_to_bool ( config_val , evar ) : if not config_val : return False if config_val . strip ( ) . lower ( ) == 'true' : return True else : return False
1892	def _start_proc ( self ) : assert '_proc' not in dir ( self ) or self . _proc is None try : self . _proc = Popen ( shlex . split ( self . _command ) , stdin = PIPE , stdout = PIPE , bufsize = 0 , universal_newlines = True ) except OSError as e : print ( e , "Probably too many cached expressions? visitors._cache..." ) raise Z3NotFoundError for cfg in self . _init : self . _send ( cfg )
5938	def help ( self , long = False ) : print ( "\ncommand: {0!s}\n\n" . format ( self . command_name ) ) print ( self . __doc__ ) if long : print ( "\ncall method: command():\n" ) print ( self . __call__ . __doc__ )
7453	def collate_files ( data , sname , tmp1s , tmp2s ) : out1 = os . path . join ( data . dirs . fastqs , "{}_R1_.fastq.gz" . format ( sname ) ) out = io . BufferedWriter ( gzip . open ( out1 , 'w' ) ) cmd1 = [ 'cat' ] for tmpfile in tmp1s : cmd1 += [ tmpfile ] proc = sps . Popen ( [ 'which' , 'pigz' ] , stderr = sps . PIPE , stdout = sps . PIPE ) . communicate ( ) if proc [ 0 ] . strip ( ) : compress = [ "pigz" ] else : compress = [ "gzip" ] proc1 = sps . Popen ( cmd1 , stderr = sps . PIPE , stdout = sps . PIPE ) proc2 = sps . Popen ( compress , stdin = proc1 . stdout , stderr = sps . PIPE , stdout = out ) err = proc2 . communicate ( ) if proc2 . returncode : raise IPyradWarningExit ( "error in collate_files R1 %s" , err ) proc1 . stdout . close ( ) out . close ( ) for tmpfile in tmp1s : os . remove ( tmpfile ) if 'pair' in data . paramsdict [ "datatype" ] : out2 = os . path . join ( data . dirs . fastqs , "{}_R2_.fastq.gz" . format ( sname ) ) out = io . BufferedWriter ( gzip . open ( out2 , 'w' ) ) cmd1 = [ 'cat' ] for tmpfile in tmp2s : cmd1 += [ tmpfile ] proc1 = sps . Popen ( cmd1 , stderr = sps . PIPE , stdout = sps . PIPE ) proc2 = sps . Popen ( compress , stdin = proc1 . stdout , stderr = sps . PIPE , stdout = out ) err = proc2 . communicate ( ) if proc2 . returncode : raise IPyradWarningExit ( "error in collate_files R2 %s" , err ) proc1 . stdout . close ( ) out . close ( ) for tmpfile in tmp2s : os . remove ( tmpfile )
9228	def get_all_tags ( self ) : verbose = self . options . verbose gh = self . github user = self . options . user repo = self . options . project if verbose : print ( "Fetching tags..." ) tags = [ ] page = 1 while page > 0 : if verbose > 2 : print ( "." , end = "" ) rc , data = gh . repos [ user ] [ repo ] . tags . get ( page = page , per_page = PER_PAGE_NUMBER ) if rc == 200 : tags . extend ( data ) else : self . raise_GitHubError ( rc , data , gh . getheaders ( ) ) page = NextPage ( gh ) if verbose > 2 : print ( "." ) if len ( tags ) == 0 : if not self . options . quiet : print ( "Warning: Can't find any tags in repo. Make sure, that " "you push tags to remote repo via 'git push --tags'" ) exit ( ) if verbose > 1 : print ( "Found {} tag(s)" . format ( len ( tags ) ) ) return tags
4743	def exists ( ) : if env ( ) : cij . err ( "cij.nvm.exists: Invalid NVMe ENV." ) return 1 nvm = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) cmd = [ '[[ -b "%s" ]]' % nvm [ "DEV_PATH" ] ] rcode , _ , _ = cij . ssh . command ( cmd , shell = True , echo = False ) return rcode
2509	def parse_only_extr_license ( self , extr_lic ) : ident = self . get_extr_license_ident ( extr_lic ) text = self . get_extr_license_text ( extr_lic ) comment = self . get_extr_lics_comment ( extr_lic ) xrefs = self . get_extr_lics_xref ( extr_lic ) name = self . get_extr_lic_name ( extr_lic ) if not ident : return lic = document . ExtractedLicense ( ident ) if text is not None : lic . text = text if name is not None : lic . full_name = name if comment is not None : lic . comment = comment lic . cross_ref = map ( lambda x : six . text_type ( x ) , xrefs ) return lic
9885	def load_all_variables ( self ) : self . data = { } file_var_names = self . z_variable_info . keys ( ) dim_sizes = [ ] rec_nums = [ ] data_types = [ ] names = [ ] for i , name in enumerate ( file_var_names ) : dim_sizes . extend ( self . z_variable_info [ name ] [ 'dim_sizes' ] ) rec_nums . append ( self . z_variable_info [ name ] [ 'rec_num' ] ) data_types . append ( self . z_variable_info [ name ] [ 'data_type' ] ) names . append ( name . ljust ( 256 ) ) dim_sizes = np . array ( dim_sizes ) rec_nums = np . array ( rec_nums ) data_types = np . array ( data_types ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'real4' ] , fortran_cdf . get_multi_z_real4 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'float' ] , fortran_cdf . get_multi_z_real4 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'real8' ] , fortran_cdf . get_multi_z_real8 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'double' ] , fortran_cdf . get_multi_z_real8 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'int4' ] , fortran_cdf . get_multi_z_int4 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'uint4' ] , fortran_cdf . get_multi_z_int4 , data_offset = 2 ** 32 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'int2' ] , fortran_cdf . get_multi_z_int2 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'uint2' ] , fortran_cdf . get_multi_z_int2 , data_offset = 2 ** 16 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'int1' ] , fortran_cdf . get_multi_z_int1 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'uint1' ] , fortran_cdf . get_multi_z_int1 , data_offset = 2 ** 8 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'byte' ] , fortran_cdf . get_multi_z_int1 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'epoch' ] , fortran_cdf . get_multi_z_real8 , epoch = True ) self . _call_multi_fortran_z ( names , data_types , rec_nums , 2 * dim_sizes , self . cdf_data_types [ 'epoch16' ] , fortran_cdf . get_multi_z_epoch16 , epoch16 = True ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'TT2000' ] , fortran_cdf . get_multi_z_tt2000 , epoch = True ) self . data_loaded = True
10402	def calculate_score ( self , node : BaseEntity ) -> float : score = ( self . graph . nodes [ node ] [ self . tag ] if self . tag in self . graph . nodes [ node ] else self . default_score ) for predecessor , _ , d in self . graph . in_edges ( node , data = True ) : if d [ RELATION ] in CAUSAL_INCREASE_RELATIONS : score += self . graph . nodes [ predecessor ] [ self . tag ] elif d [ RELATION ] in CAUSAL_DECREASE_RELATIONS : score -= self . graph . nodes [ predecessor ] [ self . tag ] return score
2266	def map_vals ( func , dict_ ) : if not hasattr ( func , '__call__' ) : func = func . __getitem__ keyval_list = [ ( key , func ( val ) ) for key , val in six . iteritems ( dict_ ) ] dictclass = OrderedDict if isinstance ( dict_ , OrderedDict ) else dict newdict = dictclass ( keyval_list ) return newdict
3337	def join_uri ( uri , * segments ) : sub = "/" . join ( segments ) if not sub : return uri return uri . rstrip ( "/" ) + "/" + sub
6973	def epd_magseries ( times , mags , errs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd , magsarefluxes = False , epdsmooth_sigclip = 3.0 , epdsmooth_windowsize = 21 , epdsmooth_func = smooth_magseries_savgol , epdsmooth_extraparams = None ) : finind = np . isfinite ( times ) & np . isfinite ( mags ) & np . isfinite ( errs ) ftimes , fmags , ferrs = times [ : : ] [ finind ] , mags [ : : ] [ finind ] , errs [ : : ] [ finind ] ffsv , ffdv , ffkv , fxcc , fycc , fbgv , fbge , fiha , fizd = ( fsv [ : : ] [ finind ] , fdv [ : : ] [ finind ] , fkv [ : : ] [ finind ] , xcc [ : : ] [ finind ] , ycc [ : : ] [ finind ] , bgv [ : : ] [ finind ] , bge [ : : ] [ finind ] , iha [ : : ] [ finind ] , izd [ : : ] [ finind ] , ) stimes , smags , serrs , separams = sigclip_magseries_with_extparams ( times , mags , errs , [ fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ] , sigclip = epdsmooth_sigclip , magsarefluxes = magsarefluxes ) sfsv , sfdv , sfkv , sxcc , sycc , sbgv , sbge , siha , sizd = separams if isinstance ( epdsmooth_extraparams , dict ) : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize , ** epdsmooth_extraparams ) else : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize ) initcoeffs = np . zeros ( 22 ) leastsqfit = leastsq ( _epd_residual , initcoeffs , args = ( smoothedmags , sfsv , sfdv , sfkv , sxcc , sycc , sbgv , sbge , siha , sizd ) , full_output = True ) if leastsqfit [ - 1 ] in ( 1 , 2 , 3 , 4 ) : fitcoeffs = leastsqfit [ 0 ] epdfit = _epd_function ( fitcoeffs , ffsv , ffdv , ffkv , fxcc , fycc , fbgv , fbge , fiha , fizd ) epdmags = npmedian ( fmags ) + fmags - epdfit retdict = { 'times' : ftimes , 'mags' : epdmags , 'errs' : ferrs , 'fitcoeffs' : fitcoeffs , 'fitinfo' : leastsqfit , 'fitmags' : epdfit , 'mags_median' : npmedian ( epdmags ) , 'mags_mad' : npmedian ( npabs ( epdmags - npmedian ( epdmags ) ) ) } return retdict else : LOGERROR ( 'EPD fit did not converge' ) return None
11022	def get_node ( self , string_key ) : pos = self . get_node_pos ( string_key ) if pos is None : return None return self . ring [ self . _sorted_keys [ pos ] ]
225	async def send ( self , message : Message ) -> None : if self . application_state == WebSocketState . CONNECTING : message_type = message [ "type" ] assert message_type in { "websocket.accept" , "websocket.close" } if message_type == "websocket.close" : self . application_state = WebSocketState . DISCONNECTED else : self . application_state = WebSocketState . CONNECTED await self . _send ( message ) elif self . application_state == WebSocketState . CONNECTED : message_type = message [ "type" ] assert message_type in { "websocket.send" , "websocket.close" } if message_type == "websocket.close" : self . application_state = WebSocketState . DISCONNECTED await self . _send ( message ) else : raise RuntimeError ( 'Cannot call "send" once a close message has been sent.' )
8682	def purge ( self , force = False , key_type = None ) : self . _assert_valid_stash ( ) if not force : raise GhostError ( "The `force` flag must be provided to perform a stash purge. " "I mean, you don't really want to just delete everything " "without precautionary measures eh?" ) audit ( storage = self . _storage . db_path , action = 'PURGE' , message = json . dumps ( dict ( ) ) ) for key_name in self . list ( key_type = key_type ) : self . delete ( key_name )
9925	def get_queryset ( self ) : oldest = timezone . now ( ) - app_settings . PASSWORD_RESET_EXPIRATION queryset = super ( ValidPasswordResetTokenManager , self ) . get_queryset ( ) return queryset . filter ( created_at__gt = oldest )
9191	def _insert_metadata ( cursor , model , publisher , message ) : params = model . metadata . copy ( ) params [ 'publisher' ] = publisher params [ 'publication_message' ] = message params [ '_portal_type' ] = _model_to_portaltype ( model ) params [ 'summary' ] = str ( cnxepub . DocumentSummaryFormatter ( model ) ) for person_field in ATTRIBUTED_ROLE_KEYS : params [ person_field ] = [ parse_user_uri ( x [ 'id' ] ) for x in params . get ( person_field , [ ] ) ] params [ 'parent_ident_hash' ] = parse_parent_ident_hash ( model ) if model . ident_hash is not None : uuid , version = split_ident_hash ( model . ident_hash , split_version = True ) params [ '_uuid' ] = uuid params [ '_major_version' ] , params [ '_minor_version' ] = version cursor . execute ( "SELECT moduleid FROM latest_modules WHERE uuid = %s" , ( uuid , ) ) try : moduleid = cursor . fetchone ( ) [ 0 ] except TypeError : moduleid = None params [ '_moduleid' ] = moduleid cursor . execute ( "SELECT * from document_controls where uuid = %s" , ( uuid , ) ) try : cursor . fetchone ( ) [ 0 ] except TypeError : cursor . execute ( "INSERT INTO document_controls (uuid) VALUES (%s)" , ( uuid , ) ) created = model . metadata . get ( 'created' , None ) stmt = MODULE_INSERTION_TEMPLATE . format ( ** { '__uuid__' : "%(_uuid)s::uuid" , '__major_version__' : "%(_major_version)s" , '__minor_version__' : "%(_minor_version)s" , '__moduleid__' : moduleid is None and "DEFAULT" or "%(_moduleid)s" , '__created__' : created is None and "DEFAULT" or "%(created)s" , } ) else : created = model . metadata . get ( 'created' , None ) stmt = MODULE_INSERTION_TEMPLATE . format ( ** { '__uuid__' : "DEFAULT" , '__major_version__' : "DEFAULT" , '__minor_version__' : "DEFAULT" , '__moduleid__' : "DEFAULT" , '__created__' : created is None and "DEFAULT" or "%(created)s" , } ) cursor . execute ( stmt , params ) module_ident , ident_hash = cursor . fetchone ( ) _insert_optional_roles ( cursor , model , module_ident ) return module_ident , ident_hash
13711	def invalidate_ip ( self , ip ) : if self . _use_cache : key = self . _make_cache_key ( ip ) self . _cache . delete ( key , version = self . _cache_version )
8574	def create_nic ( self , datacenter_id , server_id , nic ) : data = json . dumps ( self . _create_nic_dict ( nic ) ) response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics' % ( datacenter_id , server_id ) , method = 'POST' , data = data ) return response
6281	def keyboard_event ( self , key , action , modifier ) : if key == self . keys . ESCAPE : self . close ( ) return if key == self . keys . SPACE and action == self . keys . ACTION_PRESS : self . timer . toggle_pause ( ) if key == self . keys . D : if action == self . keys . ACTION_PRESS : self . sys_camera . move_right ( True ) elif action == self . keys . ACTION_RELEASE : self . sys_camera . move_right ( False ) elif key == self . keys . A : if action == self . keys . ACTION_PRESS : self . sys_camera . move_left ( True ) elif action == self . keys . ACTION_RELEASE : self . sys_camera . move_left ( False ) elif key == self . keys . W : if action == self . keys . ACTION_PRESS : self . sys_camera . move_forward ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_forward ( False ) elif key == self . keys . S : if action == self . keys . ACTION_PRESS : self . sys_camera . move_backward ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_backward ( False ) elif key == self . keys . Q : if action == self . keys . ACTION_PRESS : self . sys_camera . move_down ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_down ( False ) elif key == self . keys . E : if action == self . keys . ACTION_PRESS : self . sys_camera . move_up ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_up ( False ) if key == self . keys . X and action == self . keys . ACTION_PRESS : screenshot . create ( ) if key == self . keys . R and action == self . keys . ACTION_PRESS : project . instance . reload_programs ( ) if key == self . keys . RIGHT and action == self . keys . ACTION_PRESS : self . timer . set_time ( self . timer . get_time ( ) + 10.0 ) if key == self . keys . LEFT and action == self . keys . ACTION_PRESS : self . timer . set_time ( self . timer . get_time ( ) - 10.0 ) self . timeline . key_event ( key , action , modifier )
10719	def x10_command ( self , house_code , unit_number , state ) : house_code = normalize_housecode ( house_code ) if unit_number is not None : unit_number = normalize_unitnumber ( unit_number ) return self . _x10_command ( house_code , unit_number , state )
3217	def get_subnets ( vpc , ** conn ) : subnets = describe_subnets ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , ** conn ) s_ids = [ ] for s in subnets : s_ids . append ( s [ "SubnetId" ] ) return s_ids
3991	def _nginx_stream_spec ( port_spec , bridge_ip ) : server_string_spec = "\t server {\n" server_string_spec += "\t \t {}\n" . format ( _nginx_listen_string ( port_spec ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_proxy_string ( port_spec , bridge_ip ) ) server_string_spec += "\t }\n" return server_string_spec
7467	def summarize_results ( self , individual_results = False ) : if ( not self . params . infer_delimit ) & ( not self . params . infer_sptree ) : if individual_results : return [ _parse_00 ( i ) for i in self . files . outfiles ] else : return pd . concat ( [ pd . read_csv ( i , sep = '\t' , index_col = 0 ) for i in self . files . mcmcfiles ] ) . describe ( ) . T if self . params . infer_delimit & ( not self . params . infer_sptree ) : return _parse_01 ( self . files . outfiles , individual = individual_results ) else : return "summary function not yet ready for this type of result"
5646	def createcolorbar ( cmap , norm ) : cax , kw = matplotlib . colorbar . make_axes ( matplotlib . pyplot . gca ( ) ) c = matplotlib . colorbar . ColorbarBase ( cax , cmap = cmap , norm = norm ) return c
11748	def _bundle_exists ( self , path ) : for attached_bundle in self . _attached_bundles : if path == attached_bundle . path : return True return False
10495	def clickMouseButtonRightWithMods ( self , coord , modifiers ) : modFlags = self . _pressModifiers ( modifiers ) self . _queueMouseButton ( coord , Quartz . kCGMouseButtonRight , modFlags ) self . _releaseModifiers ( modifiers ) self . _postQueuedEvents ( )
12121	def get_data_around ( self , timePoints , thisSweep = False , padding = 0.02 , msDeriv = 0 ) : if not np . array ( timePoints ) . shape : timePoints = [ float ( timePoints ) ] data = None for timePoint in timePoints : if thisSweep : sweep = self . currentSweep else : sweep = int ( timePoint / self . sweepInterval ) timePoint = timePoint - sweep * self . sweepInterval self . setSweep ( sweep ) if msDeriv : dx = int ( msDeriv * self . rate / 1000 ) newData = ( self . dataY [ dx : ] - self . dataY [ : - dx ] ) * self . rate / 1000 / dx else : newData = self . dataY padPoints = int ( padding * self . rate ) pad = np . empty ( padPoints ) * np . nan Ic = timePoint * self . rate newData = np . concatenate ( ( pad , pad , newData , pad , pad ) ) Ic += padPoints * 2 newData = newData [ Ic - padPoints : Ic + padPoints ] newData = newData [ : int ( padPoints * 2 ) ] if data is None : data = [ newData ] else : data = np . vstack ( ( data , newData ) ) return data
2589	def shutdown ( self , block = False ) : x = self . executor . shutdown ( wait = block ) logger . debug ( "Done with executor shutdown" ) return x
7688	def pitch_contour ( annotation , sr = 22050 , length = None , ** kwargs ) : times = defaultdict ( list ) freqs = defaultdict ( list ) for obs in annotation : times [ obs . value [ 'index' ] ] . append ( obs . time ) freqs [ obs . value [ 'index' ] ] . append ( obs . value [ 'frequency' ] * ( - 1 ) ** ( ~ obs . value [ 'voiced' ] ) ) y_out = 0.0 for ix in times : y_out = y_out + filter_kwargs ( mir_eval . sonify . pitch_contour , np . asarray ( times [ ix ] ) , np . asarray ( freqs [ ix ] ) , fs = sr , length = length , ** kwargs ) if length is None : length = len ( y_out ) return y_out
10162	def setup ( app ) : lexer = MarkdownLexer ( ) for alias in lexer . aliases : app . add_lexer ( alias , lexer ) return dict ( version = __version__ )
1777	def TEST ( cpu , src1 , src2 ) : temp = src1 . read ( ) & src2 . read ( ) cpu . SF = ( temp & ( 1 << ( src1 . size - 1 ) ) ) != 0 cpu . ZF = temp == 0 cpu . PF = cpu . _calculate_parity_flag ( temp ) cpu . CF = False cpu . OF = False
2009	def concretized_args ( ** policies ) : def concretizer ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : spec = inspect . getfullargspec ( func ) for arg , policy in policies . items ( ) : assert arg in spec . args , "Concretizer argument not found in wrapped function." index = spec . args . index ( arg ) if not issymbolic ( args [ index ] ) : continue if not policy : policy = 'SAMPLED' if policy == "ACCOUNTS" : value = args [ index ] world = args [ 0 ] . world cond = world . _constraint_to_accounts ( value , ty = 'both' , include_zero = True ) world . constraints . add ( cond ) policy = 'ALL' raise ConcretizeArgument ( index , policy = policy ) return func ( * args , ** kwargs ) wrapper . __signature__ = inspect . signature ( func ) return wrapper return concretizer
5321	def get_data ( self , reset_device = False ) : try : if reset_device : self . _device . reset ( ) for interface in [ 0 , 1 ] : if self . _device . is_kernel_driver_active ( interface ) : LOGGER . debug ( 'Detaching kernel driver for interface %d ' 'of %r on ports %r' , interface , self . _device , self . _ports ) self . _device . detach_kernel_driver ( interface ) self . _device . set_configuration ( ) usb . util . claim_interface ( self . _device , INTERFACE ) self . _control_transfer ( COMMANDS [ 'temp' ] ) self . _interrupt_read ( ) self . _control_transfer ( COMMANDS [ 'temp' ] ) temp_data = self . _interrupt_read ( ) if self . _device . product == 'TEMPer1F_H1_V1.4' : humidity_data = temp_data else : humidity_data = None data = { 'temp_data' : temp_data , 'humidity_data' : humidity_data } usb . util . dispose_resources ( self . _device ) return data except usb . USBError as err : if not reset_device : LOGGER . warning ( "Encountered %s, resetting %r and trying again." , err , self . _device ) return self . get_data ( True ) if "not permitted" in str ( err ) : raise Exception ( "Permission problem accessing USB. " "Maybe I need to run as root?" ) else : LOGGER . error ( err ) raise
8681	def delete ( self , key_name ) : self . _assert_valid_stash ( ) if key_name == 'stored_passphrase' : raise GhostError ( '`stored_passphrase` is a reserved ghost key name ' 'which cannot be deleted' ) if not self . get ( key_name ) : raise GhostError ( 'Key `{0}` not found' . format ( key_name ) ) key = self . _storage . get ( key_name ) if key . get ( 'lock' ) : raise GhostError ( 'Key `{0}` is locked and therefore cannot be deleted ' 'Please unlock the key and try again' . format ( key_name ) ) deleted = self . _storage . delete ( key_name ) audit ( storage = self . _storage . db_path , action = 'DELETE' , message = json . dumps ( dict ( key_name = key_name ) ) ) if not deleted : raise GhostError ( 'Failed to delete {0}' . format ( key_name ) )
4901	def get_course_enrollments ( self , enterprise_customer , days ) : return CourseEnrollment . objects . filter ( created__gt = datetime . datetime . now ( ) - datetime . timedelta ( days = days ) ) . filter ( user_id__in = enterprise_customer . enterprise_customer_users . values_list ( 'user_id' , flat = True ) )
2834	def platform_detect ( ) : pi = pi_version ( ) if pi is not None : return RASPBERRY_PI plat = platform . platform ( ) if plat . lower ( ) . find ( 'armv7l-with-debian' ) > - 1 : return BEAGLEBONE_BLACK elif plat . lower ( ) . find ( 'armv7l-with-ubuntu' ) > - 1 : return BEAGLEBONE_BLACK elif plat . lower ( ) . find ( 'armv7l-with-glibc2.4' ) > - 1 : return BEAGLEBONE_BLACK elif plat . lower ( ) . find ( 'tegra-aarch64-with-ubuntu' ) > - 1 : return JETSON_NANO try : import mraa if mraa . getPlatformName ( ) == 'MinnowBoard MAX' : return MINNOWBOARD except ImportError : pass return UNKNOWN
9575	def read_elements ( fd , endian , mtps , is_name = False ) : mtpn , num_bytes , data = read_element_tag ( fd , endian ) if mtps and mtpn not in [ etypes [ mtp ] [ 'n' ] for mtp in mtps ] : raise ParseError ( 'Got type {}, expected {}' . format ( mtpn , ' / ' . join ( '{} ({})' . format ( etypes [ mtp ] [ 'n' ] , mtp ) for mtp in mtps ) ) ) if not data : data = fd . read ( num_bytes ) mod8 = num_bytes % 8 if mod8 : fd . seek ( 8 - mod8 , 1 ) if is_name : fmt = 's' val = [ unpack ( endian , fmt , s ) for s in data . split ( b'\0' ) if s ] if len ( val ) == 0 : val = '' elif len ( val ) == 1 : val = asstr ( val [ 0 ] ) else : val = [ asstr ( s ) for s in val ] else : fmt = etypes [ inv_etypes [ mtpn ] ] [ 'fmt' ] val = unpack ( endian , fmt , data ) return val
12886	def get_fsapi_endpoint ( self ) : endpoint = yield from self . __session . get ( self . fsapi_device_url , timeout = self . timeout ) text = yield from endpoint . text ( encoding = 'utf-8' ) doc = objectify . fromstring ( text ) return doc . webfsapi . text
10794	def create_comparison_state ( image , position , radius = 5.0 , snr = 20 , method = 'constrained-cubic' , extrapad = 2 , zscale = 1.0 ) : image = common . pad ( image , extrapad , 0 ) s = init . create_single_particle_state ( imsize = np . array ( image . shape ) , sigma = 1.0 / snr , radius = radius , psfargs = { 'params' : np . array ( [ 2.0 , 1.0 , 3.0 ] ) , 'error' : 1e-6 , 'threads' : 2 } , objargs = { 'method' : method } , stateargs = { 'sigmapad' : False , 'pad' : 4 , 'zscale' : zscale } ) s . obj . pos [ 0 ] = position + s . pad + extrapad s . reset ( ) s . model_to_true_image ( ) timage = 1 - np . pad ( image , s . pad , mode = 'constant' , constant_values = 0 ) timage = s . psf . execute ( timage ) return s , timage [ s . inner ]
12245	def get_all_buckets ( self , * args , ** kwargs ) : if kwargs . pop ( 'force' , None ) : buckets = super ( S3Connection , self ) . get_all_buckets ( * args , ** kwargs ) for bucket in buckets : mimicdb . backend . sadd ( tpl . connection , bucket . name ) return buckets return [ Bucket ( self , bucket ) for bucket in mimicdb . backend . smembers ( tpl . connection ) ]
3940	async def listen ( self ) : retries = 0 need_new_sid = True while retries <= self . _max_retries : if retries > 0 : backoff_seconds = self . _retry_backoff_base ** retries logger . info ( 'Backing off for %s seconds' , backoff_seconds ) await asyncio . sleep ( backoff_seconds ) if need_new_sid : await self . _fetch_channel_sid ( ) need_new_sid = False self . _chunk_parser = ChunkParser ( ) try : await self . _longpoll_request ( ) except ChannelSessionError as err : logger . warning ( 'Long-polling interrupted: %s' , err ) need_new_sid = True except exceptions . NetworkError as err : logger . warning ( 'Long-polling request failed: %s' , err ) else : retries = 0 continue retries += 1 logger . info ( 'retry attempt count is now %s' , retries ) if self . _is_connected : self . _is_connected = False await self . on_disconnect . fire ( ) logger . error ( 'Ran out of retries for long-polling request' )
7516	def init_arrays ( data ) : co5 = h5py . File ( data . clust_database , 'r' ) io5 = h5py . File ( data . database , 'w' ) maxlen = data . _hackersonly [ "max_fragment_length" ] + 20 chunks = co5 [ "seqs" ] . attrs [ "chunksize" ] [ 0 ] nloci = co5 [ "seqs" ] . shape [ 0 ] snps = io5 . create_dataset ( "snps" , ( nloci , maxlen , 2 ) , dtype = np . bool , chunks = ( chunks , maxlen , 2 ) , compression = 'gzip' ) snps . attrs [ "chunksize" ] = chunks snps . attrs [ "names" ] = [ "-" , "*" ] filters = io5 . create_dataset ( "filters" , ( nloci , 6 ) , dtype = np . bool ) filters . attrs [ "filters" ] = [ "duplicates" , "max_indels" , "max_snps" , "max_shared_hets" , "min_samps" , "max_alleles" ] edges = io5 . create_dataset ( "edges" , ( nloci , 5 ) , dtype = np . uint16 , chunks = ( chunks , 5 ) , compression = "gzip" ) edges . attrs [ "chunksize" ] = chunks edges . attrs [ "names" ] = [ "R1_L" , "R1_R" , "R2_L" , "R2_R" , "sep" ] edges [ : , 4 ] = co5 [ "splits" ] [ : ] filters [ : , 0 ] = co5 [ "duplicates" ] [ : ] io5 . close ( ) co5 . close ( )
2748	def get_all_droplets ( self , tag_name = None ) : params = dict ( ) if tag_name : params [ "tag_name" ] = tag_name data = self . get_data ( "droplets/" , params = params ) droplets = list ( ) for jsoned in data [ 'droplets' ] : droplet = Droplet ( ** jsoned ) droplet . token = self . token for net in droplet . networks [ 'v4' ] : if net [ 'type' ] == 'private' : droplet . private_ip_address = net [ 'ip_address' ] if net [ 'type' ] == 'public' : droplet . ip_address = net [ 'ip_address' ] if droplet . networks [ 'v6' ] : droplet . ip_v6_address = droplet . networks [ 'v6' ] [ 0 ] [ 'ip_address' ] if "backups" in droplet . features : droplet . backups = True else : droplet . backups = False if "ipv6" in droplet . features : droplet . ipv6 = True else : droplet . ipv6 = False if "private_networking" in droplet . features : droplet . private_networking = True else : droplet . private_networking = False droplets . append ( droplet ) return droplets
5405	def _build_user_environment ( self , envs , inputs , outputs , mounts ) : envs = { env . name : env . value for env in envs } envs . update ( providers_util . get_file_environment_variables ( inputs ) ) envs . update ( providers_util . get_file_environment_variables ( outputs ) ) envs . update ( providers_util . get_file_environment_variables ( mounts ) ) return envs
5056	def get_catalog_admin_url_template ( mode = 'change' ) : api_base_url = getattr ( settings , "COURSE_CATALOG_API_URL" , "" ) match = re . match ( r"^(?P<fqdn>(?:https?://)?[^/]+)" , api_base_url ) if not match : return "" if mode == 'change' : return match . group ( "fqdn" ) . rstrip ( "/" ) + "/admin/catalogs/catalog/{catalog_id}/change/" elif mode == 'add' : return match . group ( "fqdn" ) . rstrip ( "/" ) + "/admin/catalogs/catalog/add/"
9837	def __object ( self ) : self . __consume ( ) classid = self . __consume ( ) . text word = self . __consume ( ) . text if word != "class" : raise DXParseError ( "reserved word %s should have been 'class'." % word ) if self . currentobject : self . objects . append ( self . currentobject ) classtype = self . __consume ( ) . text self . currentobject = DXInitObject ( classtype = classtype , classid = classid ) self . use_parser ( classtype )
6326	def corpus_importer ( self , corpus , n_val = 1 , bos = '_START_' , eos = '_END_' ) : r if not corpus or not isinstance ( corpus , Corpus ) : raise TypeError ( 'Corpus argument of the Corpus class required.' ) sentences = corpus . sents ( ) for sent in sentences : ngs = Counter ( sent ) for key in ngs . keys ( ) : self . _add_to_ngcorpus ( self . ngcorpus , [ key ] , ngs [ key ] ) if n_val > 1 : if bos and bos != '' : sent = [ bos ] + sent if eos and eos != '' : sent += [ eos ] for i in range ( 2 , n_val + 1 ) : for j in range ( len ( sent ) - i + 1 ) : self . _add_to_ngcorpus ( self . ngcorpus , sent [ j : j + i ] , 1 )
5340	def __remove_dashboard_menu ( self , kibiter_major ) : logger . info ( "Removing old dashboard menu, if any" ) if kibiter_major == "6" : metadashboard = ".kibana/doc/metadashboard" else : metadashboard = ".kibana/metadashboard/main" menu_url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , metadashboard ) self . grimoire_con . delete ( menu_url )
9766	def clean_outputs ( fn ) : @ wraps ( fn ) def clean_outputs_wrapper ( * args , ** kwargs ) : try : return fn ( * args , ** kwargs ) except SystemExit as e : sys . stdout = StringIO ( ) sys . exit ( e . code ) except Exception as e : sys . stdout = StringIO ( ) raise e return clean_outputs_wrapper
8277	def fseq ( self , client , message ) : client . last_frame = client . current_frame client . current_frame = message [ 3 ]
5359	def execute_nonstop_tasks ( self , tasks_cls ) : self . execute_batch_tasks ( tasks_cls , self . conf [ 'sortinghat' ] [ 'sleep_for' ] , self . conf [ 'general' ] [ 'min_update_delay' ] , False )
12824	def _exec ( self , globals_dict = None ) : globals_dict = globals_dict or { } globals_dict . setdefault ( '__builtins__' , { } ) exec ( self . _code , globals_dict ) return globals_dict
5055	def get_idp_choices ( ) : try : from third_party_auth . provider import Registry except ImportError as exception : LOGGER . warning ( "Could not import Registry from third_party_auth.provider" ) LOGGER . warning ( exception ) Registry = None first = [ ( "" , "-" * 7 ) ] if Registry : return first + [ ( idp . provider_id , idp . name ) for idp in Registry . enabled ( ) ] return None
12344	def well_images ( self , well_row , well_column ) : return list ( i for i in self . images if attribute ( i , 'u' ) == well_column and attribute ( i , 'v' ) == well_row )
10559	def convert_cygwin_path ( path ) : try : win_path = subprocess . check_output ( [ "cygpath" , "-aw" , path ] , universal_newlines = True ) . strip ( ) except ( FileNotFoundError , subprocess . CalledProcessError ) : logger . exception ( "Call to cygpath failed." ) raise return win_path
2297	def fit ( self , x , y ) : train = np . vstack ( ( np . array ( [ self . featurize_row ( row . iloc [ 0 ] , row . iloc [ 1 ] ) for idx , row in x . iterrows ( ) ] ) , np . array ( [ self . featurize_row ( row . iloc [ 1 ] , row . iloc [ 0 ] ) for idx , row in x . iterrows ( ) ] ) ) ) labels = np . vstack ( ( y , - y ) ) . ravel ( ) verbose = 1 if self . verbose else 0 self . clf = CLF ( verbose = verbose , min_samples_leaf = self . L , n_estimators = self . E , max_depth = self . max_depth , n_jobs = self . n_jobs ) . fit ( train , labels )
5780	def _obtain_credentials ( self ) : protocol_values = { 'SSLv3' : Secur32Const . SP_PROT_SSL3_CLIENT , 'TLSv1' : Secur32Const . SP_PROT_TLS1_CLIENT , 'TLSv1.1' : Secur32Const . SP_PROT_TLS1_1_CLIENT , 'TLSv1.2' : Secur32Const . SP_PROT_TLS1_2_CLIENT , } protocol_bit_mask = 0 for key , value in protocol_values . items ( ) : if key in self . _protocols : protocol_bit_mask |= value algs = [ Secur32Const . CALG_AES_128 , Secur32Const . CALG_AES_256 , Secur32Const . CALG_3DES , Secur32Const . CALG_SHA1 , Secur32Const . CALG_ECDHE , Secur32Const . CALG_DH_EPHEM , Secur32Const . CALG_RSA_KEYX , Secur32Const . CALG_RSA_SIGN , Secur32Const . CALG_ECDSA , Secur32Const . CALG_DSS_SIGN , ] if 'TLSv1.2' in self . _protocols : algs . extend ( [ Secur32Const . CALG_SHA512 , Secur32Const . CALG_SHA384 , Secur32Const . CALG_SHA256 , ] ) alg_array = new ( secur32 , 'ALG_ID[%s]' % len ( algs ) ) for index , alg in enumerate ( algs ) : alg_array [ index ] = alg flags = Secur32Const . SCH_USE_STRONG_CRYPTO | Secur32Const . SCH_CRED_NO_DEFAULT_CREDS if not self . _manual_validation and not self . _extra_trust_roots : flags |= Secur32Const . SCH_CRED_AUTO_CRED_VALIDATION else : flags |= Secur32Const . SCH_CRED_MANUAL_CRED_VALIDATION schannel_cred_pointer = struct ( secur32 , 'SCHANNEL_CRED' ) schannel_cred = unwrap ( schannel_cred_pointer ) schannel_cred . dwVersion = Secur32Const . SCHANNEL_CRED_VERSION schannel_cred . cCreds = 0 schannel_cred . paCred = null ( ) schannel_cred . hRootStore = null ( ) schannel_cred . cMappers = 0 schannel_cred . aphMappers = null ( ) schannel_cred . cSupportedAlgs = len ( alg_array ) schannel_cred . palgSupportedAlgs = alg_array schannel_cred . grbitEnabledProtocols = protocol_bit_mask schannel_cred . dwMinimumCipherStrength = 0 schannel_cred . dwMaximumCipherStrength = 0 schannel_cred . dwSessionLifespan = 0 schannel_cred . dwFlags = flags schannel_cred . dwCredFormat = 0 cred_handle_pointer = new ( secur32 , 'CredHandle *' ) result = secur32 . AcquireCredentialsHandleW ( null ( ) , Secur32Const . UNISP_NAME , Secur32Const . SECPKG_CRED_OUTBOUND , null ( ) , schannel_cred_pointer , null ( ) , null ( ) , cred_handle_pointer , null ( ) ) handle_error ( result ) self . _credentials_handle = cred_handle_pointer
7693	def _check_authorization ( self , properties , stream ) : authzid = properties . get ( "authzid" ) if not authzid : return True try : jid = JID ( authzid ) except ValueError : return False if "username" not in properties : result = False elif jid . local != properties [ "username" ] : result = False elif jid . domain != stream . me . domain : result = False elif jid . resource : result = False else : result = True return result
9959	def restore_python ( self ) : orig = self . orig_settings sys . setrecursionlimit ( orig [ "sys.recursionlimit" ] ) if "sys.tracebacklimit" in orig : sys . tracebacklimit = orig [ "sys.tracebacklimit" ] else : if hasattr ( sys , "tracebacklimit" ) : del sys . tracebacklimit if "showwarning" in orig : warnings . showwarning = orig [ "showwarning" ] orig . clear ( ) threading . stack_size ( )
7347	async def get_oauth_verifier ( oauth_token ) : url = "https://api.twitter.com/oauth/authorize?oauth_token=" + oauth_token try : browser = webbrowser . open ( url ) await asyncio . sleep ( 2 ) if not browser : raise RuntimeError except RuntimeError : print ( "could not open a browser\ngo here to enter your PIN: " + url ) verifier = input ( "\nEnter your PIN: " ) return verifier
9924	def create ( self , * args , ** kwargs ) : is_primary = kwargs . pop ( "is_primary" , False ) with transaction . atomic ( ) : email = super ( EmailAddressManager , self ) . create ( * args , ** kwargs ) if is_primary : email . set_primary ( ) return email
1528	def pick_unused_port ( self ) : s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) s . bind ( ( '127.0.0.1' , 0 ) ) _ , port = s . getsockname ( ) s . close ( ) return port
6055	def sparse_grid_from_unmasked_sparse_grid ( unmasked_sparse_grid , sparse_to_unmasked_sparse ) : total_pix_pixels = sparse_to_unmasked_sparse . shape [ 0 ] pix_grid = np . zeros ( ( total_pix_pixels , 2 ) ) for pixel_index in range ( total_pix_pixels ) : pix_grid [ pixel_index , 0 ] = unmasked_sparse_grid [ sparse_to_unmasked_sparse [ pixel_index ] , 0 ] pix_grid [ pixel_index , 1 ] = unmasked_sparse_grid [ sparse_to_unmasked_sparse [ pixel_index ] , 1 ] return pix_grid
8988	def last_consumed_mesh ( self ) : for instruction in reversed ( self . instructions ) : if instruction . consumes_meshes ( ) : return instruction . last_consumed_mesh raise IndexError ( "{} consumes no meshes" . format ( self ) )
4563	def to_type_constructor ( value , python_path = None ) : if not value : return value if callable ( value ) : return { 'datatype' : value } value = to_type ( value ) typename = value . get ( 'typename' ) if typename : r = aliases . resolve ( typename ) try : value [ 'datatype' ] = importer . import_symbol ( r , python_path = python_path ) del value [ 'typename' ] except Exception as e : value [ '_exception' ] = e return value
11999	def _decode ( self , data , algorithm , key = None ) : if algorithm [ 'type' ] == 'hmac' : verify_signature = data [ - algorithm [ 'hash_size' ] : ] data = data [ : - algorithm [ 'hash_size' ] ] signature = self . _hmac_generate ( data , algorithm , key ) if not const_equal ( verify_signature , signature ) : raise Exception ( 'Invalid signature' ) return data elif algorithm [ 'type' ] == 'aes' : return self . _aes_decrypt ( data , algorithm , key ) elif algorithm [ 'type' ] == 'no-serialization' : return data elif algorithm [ 'type' ] == 'json' : return json . loads ( data ) elif algorithm [ 'type' ] == 'no-compression' : return data elif algorithm [ 'type' ] == 'gzip' : return self . _zlib_decompress ( data , algorithm ) else : raise Exception ( 'Algorithm not supported: %s' % algorithm [ 'type' ] )
942	def _runExperimentImpl ( options , model = None ) : json_helpers . validate ( options . privateOptions , schemaDict = g_parsedPrivateCommandLineOptionsSchema ) experimentDir = options . experimentDir descriptionPyModule = helpers . loadExperimentDescriptionScriptFromDir ( experimentDir ) expIface = helpers . getExperimentDescriptionInterfaceFromModule ( descriptionPyModule ) if options . privateOptions [ 'listAvailableCheckpoints' ] : _printAvailableCheckpoints ( experimentDir ) return None experimentTasks = expIface . getModelControl ( ) . get ( 'tasks' , [ ] ) if ( len ( experimentTasks ) == 0 and expIface . getModelControl ( ) [ 'environment' ] == OpfEnvironment . Nupic ) : expIface . convertNupicEnvToOPF ( ) experimentTasks = expIface . getModelControl ( ) . get ( 'tasks' , [ ] ) expIface . normalizeStreamSources ( ) newSerialization = options . privateOptions [ 'newSerialization' ] if options . privateOptions [ 'listTasks' ] : print "Available tasks:" for label in [ t [ 'taskLabel' ] for t in experimentTasks ] : print "\t" , label return None if options . privateOptions [ 'runCheckpointName' ] : assert model is None checkpointName = options . privateOptions [ 'runCheckpointName' ] model = ModelFactory . loadFromCheckpoint ( savedModelDir = _getModelCheckpointDir ( experimentDir , checkpointName ) , newSerialization = newSerialization ) elif model is not None : print "Skipping creation of OPFExperiment instance: caller provided his own" else : modelDescription = expIface . getModelDescription ( ) model = ModelFactory . create ( modelDescription ) if options . privateOptions [ 'createCheckpointName' ] : checkpointName = options . privateOptions [ 'createCheckpointName' ] _saveModel ( model = model , experimentDir = experimentDir , checkpointLabel = checkpointName , newSerialization = newSerialization ) return model taskIndexList = range ( len ( experimentTasks ) ) customTaskExecutionLabelsList = options . privateOptions [ 'taskLabels' ] if customTaskExecutionLabelsList : taskLabelsList = [ t [ 'taskLabel' ] for t in experimentTasks ] taskLabelsSet = set ( taskLabelsList ) customTaskExecutionLabelsSet = set ( customTaskExecutionLabelsList ) assert customTaskExecutionLabelsSet . issubset ( taskLabelsSet ) , ( "Some custom-provided task execution labels don't correspond " "to actual task labels: mismatched labels: %r; actual task " "labels: %r." ) % ( customTaskExecutionLabelsSet - taskLabelsSet , customTaskExecutionLabelsList ) taskIndexList = [ taskLabelsList . index ( label ) for label in customTaskExecutionLabelsList ] print "#### Executing custom task list: %r" % [ taskLabelsList [ i ] for i in taskIndexList ] for taskIndex in taskIndexList : task = experimentTasks [ taskIndex ] taskRunner = _TaskRunner ( model = model , task = task , cmdOptions = options ) taskRunner . run ( ) del taskRunner if options . privateOptions [ 'checkpointModel' ] : _saveModel ( model = model , experimentDir = experimentDir , checkpointLabel = task [ 'taskLabel' ] , newSerialization = newSerialization ) return model
12272	def int2fin_reference ( n ) : checksum = 10 - ( sum ( [ int ( c ) * i for c , i in zip ( str ( n ) [ : : - 1 ] , it . cycle ( ( 7 , 3 , 1 ) ) ) ] ) % 10 ) if checksum == 10 : checksum = 0 return "%s%s" % ( n , checksum )
10919	def do_levmarq ( s , param_names , damping = 0.1 , decrease_damp_factor = 10. , run_length = 6 , eig_update = True , collect_stats = False , rz_order = 0 , run_type = 2 , ** kwargs ) : if rz_order > 0 : aug = AugmentedState ( s , param_names , rz_order = rz_order ) lm = LMAugmentedState ( aug , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , eig_update = eig_update , ** kwargs ) else : lm = LMGlobals ( s , param_names , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , eig_update = eig_update , ** kwargs ) if run_type == 2 : lm . do_run_2 ( ) elif run_type == 1 : lm . do_run_1 ( ) else : raise ValueError ( 'run_type=1,2 only' ) if collect_stats : return lm . get_termination_stats ( )
686	def getEncoding ( self , n ) : assert ( all ( field . numEncodings > n for field in self . fields ) ) encoding = np . concatenate ( [ field . encodings [ n ] for field in self . fields ] ) return encoding
13250	def get_authoryear_from_entry ( entry , paren = False ) : def _format_last ( person ) : return ' ' . join ( [ n . strip ( '{}' ) for n in person . last_names ] ) if len ( entry . persons [ 'author' ] ) > 0 : persons = entry . persons [ 'author' ] elif len ( entry . persons [ 'editor' ] ) > 0 : persons = entry . persons [ 'editor' ] else : raise AuthorYearError try : year = entry . fields [ 'year' ] except KeyError : raise AuthorYearError if paren and len ( persons ) == 1 : template = '{author} ({year})' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year ) elif not paren and len ( persons ) == 1 : template = '{author} {year}' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year ) elif paren and len ( persons ) == 2 : template = '{author1} and {author2} ({year})' return template . format ( author1 = _format_last ( persons [ 0 ] ) , author2 = _format_last ( persons [ 1 ] ) , year = year ) elif not paren and len ( persons ) == 2 : template = '{author1} and {author2} {year}' return template . format ( author1 = _format_last ( persons [ 0 ] ) , author2 = _format_last ( persons [ 1 ] ) , year = year ) elif not paren and len ( persons ) > 2 : template = '{author} et al {year}' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year ) elif paren and len ( persons ) > 2 : template = '{author} et al ({year})' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year )
7578	def _get_evanno_table ( self , kpops , max_var_multiple , quiet ) : kpops = sorted ( kpops ) replnliks = [ ] for kpop in kpops : reps , excluded = _concat_reps ( self , kpop , max_var_multiple , quiet ) if excluded : if not quiet : sys . stderr . write ( "[K{}] {} reps excluded (not converged) see 'max_var_multiple'.\n" . format ( kpop , excluded ) ) if reps : ninds = reps [ 0 ] . inds nreps = len ( reps ) else : ninds = nreps = 0 if not reps : print "no result files found" replnliks . append ( [ i . est_lnlik for i in reps ] ) if len ( replnliks ) > 1 : lnmean = [ np . mean ( i ) for i in replnliks ] lnstds = [ np . std ( i , ddof = 1 ) for i in replnliks ] else : lnmean = replnliks lnstds = np . nan tab = pd . DataFrame ( index = kpops , data = { "Nreps" : [ len ( i ) for i in replnliks ] , "lnPK" : [ 0 ] * len ( kpops ) , "lnPPK" : [ 0 ] * len ( kpops ) , "deltaK" : [ 0 ] * len ( kpops ) , "estLnProbMean" : lnmean , "estLnProbStdev" : lnstds , } ) for kpop in kpops [ 1 : ] : tab . loc [ kpop , "lnPK" ] = tab . loc [ kpop , "estLnProbMean" ] - tab . loc [ kpop - 1 , "estLnProbMean" ] for kpop in kpops [ 1 : - 1 ] : tab . loc [ kpop , "lnPPK" ] = abs ( tab . loc [ kpop + 1 , "lnPK" ] - tab . loc [ kpop , "lnPK" ] ) tab . loc [ kpop , "deltaK" ] = ( abs ( tab . loc [ kpop + 1 , "estLnProbMean" ] - 2.0 * tab . loc [ kpop , "estLnProbMean" ] + tab . loc [ kpop - 1 , "estLnProbMean" ] ) / tab . loc [ kpop , "estLnProbStdev" ] ) return tab
6203	def em_rates_from_E_DA_mix ( em_rates_tot , E_values ) : em_rates_d , em_rates_a = [ ] , [ ] for em_rate_tot , E_value in zip ( em_rates_tot , E_values ) : em_rate_di , em_rate_ai = em_rates_from_E_DA ( em_rate_tot , E_value ) em_rates_d . append ( em_rate_di ) em_rates_a . append ( em_rate_ai ) return em_rates_d , em_rates_a
11420	def record_xml_output ( rec , tags = None , order_fn = None ) : if tags is None : tags = [ ] if isinstance ( tags , str ) : tags = [ tags ] if tags and '001' not in tags : tags . append ( '001' ) marcxml = [ '<record>' ] fields = [ ] if rec is not None : for tag in rec : if not tags or tag in tags : for field in rec [ tag ] : fields . append ( ( tag , field ) ) if order_fn is None : record_order_fields ( fields ) else : record_order_fields ( fields , order_fn ) for field in fields : marcxml . append ( field_xml_output ( field [ 1 ] , field [ 0 ] ) ) marcxml . append ( '</record>' ) return '\n' . join ( marcxml )
4800	def is_directory ( self ) : self . exists ( ) if not os . path . isdir ( self . val ) : self . _err ( 'Expected <%s> to be a directory, but was not.' % self . val ) return self
12455	def error_handler ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : try : return func ( * args , ** kwargs ) except BaseException as err : if BOOTSTRAPPER_TEST_KEY in os . environ : raise if ERROR_HANDLER_DISABLED : return True return save_traceback ( err ) return wrapper
7809	def from_ssl_socket ( cls , ssl_socket ) : try : data = ssl_socket . getpeercert ( True ) except AttributeError : data = None if not data : logger . debug ( "No certificate infromation" ) return cls ( ) result = cls . from_der_data ( data ) result . validated = bool ( ssl_socket . getpeercert ( ) ) return result
6740	def get_packager ( ) : import warnings warnings . filterwarnings ( "ignore" , category = DeprecationWarning ) common_packager = get_rc ( 'common_packager' ) if common_packager : return common_packager with settings ( warn_only = True ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : ret = _run ( 'cat /etc/fedora-release' ) if ret . succeeded : common_packager = YUM else : ret = _run ( 'cat /etc/lsb-release' ) if ret . succeeded : common_packager = APT else : for pn in PACKAGERS : ret = _run ( 'which %s' % pn ) if ret . succeeded : common_packager = pn break if not common_packager : raise Exception ( 'Unable to determine packager.' ) set_rc ( 'common_packager' , common_packager ) return common_packager
4105	def xcorr ( x , y = None , maxlags = None , norm = 'biased' ) : N = len ( x ) if y is None : y = x assert len ( x ) == len ( y ) , 'x and y must have the same length. Add zeros if needed' if maxlags is None : maxlags = N - 1 lags = np . arange ( 0 , 2 * N - 1 ) else : assert maxlags <= N , 'maxlags must be less than data length' lags = np . arange ( N - maxlags - 1 , N + maxlags ) res = np . correlate ( x , y , mode = 'full' ) if norm == 'biased' : Nf = float ( N ) res = res [ lags ] / float ( N ) elif norm == 'unbiased' : res = res [ lags ] / ( float ( N ) - abs ( np . arange ( - N + 1 , N ) ) ) [ lags ] elif norm == 'coeff' : Nf = float ( N ) rms = pylab_rms_flat ( x ) * pylab_rms_flat ( y ) res = res [ lags ] / rms / Nf else : res = res [ lags ] lags = np . arange ( - maxlags , maxlags + 1 ) return res , lags
10375	def calculate_concordance_helper ( graph : BELGraph , key : str , cutoff : Optional [ float ] = None , ) -> Tuple [ int , int , int , int ] : scores = defaultdict ( int ) for u , v , k , d in graph . edges ( keys = True , data = True ) : c = edge_concords ( graph , u , v , k , d , key , cutoff = cutoff ) scores [ c ] += 1 return ( scores [ Concordance . correct ] , scores [ Concordance . incorrect ] , scores [ Concordance . ambiguous ] , scores [ Concordance . unassigned ] , )
9016	def instruction_in_row ( self , row , specification ) : whole_instruction_ = self . _as_instruction ( specification ) return self . _spec . new_instruction_in_row ( row , whole_instruction_ )
3763	def Pt ( CASRN , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in Staveley_data . index and not np . isnan ( Staveley_data . at [ CASRN , 'Pt' ] ) : methods . append ( STAVELEY ) if Tt ( CASRN ) and VaporPressure ( CASRN = CASRN ) . T_dependent_property ( T = Tt ( CASRN ) ) : methods . append ( DEFINITION ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == STAVELEY : Pt = Staveley_data . at [ CASRN , 'Pt' ] elif Method == DEFINITION : Pt = VaporPressure ( CASRN = CASRN ) . T_dependent_property ( T = Tt ( CASRN ) ) elif Method == NONE : Pt = None else : raise Exception ( 'Failure in in function' ) return Pt
12867	def cleanup ( self , app ) : if hasattr ( self . database . obj , 'close_all' ) : self . database . close_all ( )
12944	def save ( self , cascadeSave = True ) : saver = IndexedRedisSave ( self . __class__ ) return saver . save ( self , cascadeSave = cascadeSave )
13691	def status ( self ) : peer = random . choice ( self . PEERS ) formatted_peer = 'http://{}:4001' . format ( peer ) peerdata = requests . get ( url = formatted_peer + '/api/peers/' ) . json ( ) [ 'peers' ] peers_status = { } networkheight = max ( [ x [ 'height' ] for x in peerdata ] ) for i in peerdata : if 'http://{}:4001' . format ( i [ 'ip' ] ) in self . PEERS : peers_status . update ( { i [ 'ip' ] : { 'height' : i [ 'height' ] , 'status' : i [ 'status' ] , 'version' : i [ 'version' ] , 'delay' : i [ 'delay' ] , } } ) return { 'network_height' : networkheight , 'peer_status' : peers_status }
2237	def import_module_from_path ( modpath , index = - 1 ) : import os if not os . path . exists ( modpath ) : import re import zipimport pat = '(.zip[' + re . escape ( os . path . sep ) + '/:])' parts = re . split ( pat , modpath , flags = re . IGNORECASE ) if len ( parts ) > 2 : archivepath = '' . join ( parts [ : - 1 ] ) [ : - 1 ] internal = parts [ - 1 ] modname = os . path . splitext ( internal ) [ 0 ] modname = os . path . normpath ( modname ) if os . path . exists ( archivepath ) : zimp_file = zipimport . zipimporter ( archivepath ) module = zimp_file . load_module ( modname ) return module raise IOError ( 'modpath={} does not exist' . format ( modpath ) ) else : module = _custom_import_modpath ( modpath ) return module
399	def binary_cross_entropy ( output , target , epsilon = 1e-8 , name = 'bce_loss' ) : return tf . reduce_mean ( tf . reduce_sum ( - ( target * tf . log ( output + epsilon ) + ( 1. - target ) * tf . log ( 1. - output + epsilon ) ) , axis = 1 ) , name = name )
13423	def line ( self , line ) : return [ x for x in re . split ( self . delimiter , line . rstrip ( ) ) if x != '' ]
12082	def clampfit_rename ( path , char ) : assert len ( char ) == 1 and type ( char ) == str , "replacement character must be a single character" assert os . path . exists ( path ) , "path doesn't exist" files = sorted ( os . listdir ( path ) ) files = [ x for x in files if len ( x ) > 18 and x [ 4 ] + x [ 7 ] + x [ 10 ] == ' ' ] for fname in files : fname2 = list ( fname ) fname2 [ 11 ] = char fname2 = "" . join ( fname2 ) if fname == fname2 : print ( fname , "==" , fname2 ) else : print ( fname , "->" , fname2 ) return
11287	def execute ( self , arg_str = '' , ** kwargs ) : cmd = "{} {} {}" . format ( self . cmd_prefix , self . script , arg_str ) expected_ret_code = kwargs . pop ( 'code' , 0 ) environ = self . environ for k in list ( kwargs . keys ( ) ) : if k . isupper ( ) : environ [ k ] = kwargs . pop ( k ) kwargs . setdefault ( "stderr" , subprocess . STDOUT ) kwargs [ "shell" ] = True kwargs [ "stdout" ] = subprocess . PIPE kwargs [ "cwd" ] = self . cwd kwargs [ "env" ] = environ process = None self . buf = deque ( maxlen = self . bufsize ) try : process = subprocess . Popen ( cmd , ** kwargs ) for line in iter ( process . stdout . readline , b"" ) : line = line . decode ( self . encoding ) self . buf . append ( line . rstrip ( ) ) yield line process . wait ( ) if process . returncode != expected_ret_code : if process . returncode > 0 : raise RuntimeError ( "{} returned {} with output: {}" . format ( cmd , process . returncode , self . output ) ) except subprocess . CalledProcessError as e : if e . returncode != expected_ret_code : raise RuntimeError ( "{} returned {} with output: {}" . format ( cmd , e . returncode , self . output ) ) finally : if process : process . stdout . close ( )
2322	def read_causal_pairs ( filename , scale = True , ** kwargs ) : def convert_row ( row , scale ) : a = row [ "A" ] . split ( " " ) b = row [ "B" ] . split ( " " ) if a [ 0 ] == "" : a . pop ( 0 ) b . pop ( 0 ) if a [ - 1 ] == "" : a . pop ( - 1 ) b . pop ( - 1 ) a = array ( [ float ( i ) for i in a ] ) b = array ( [ float ( i ) for i in b ] ) if scale : a = scaler ( a ) b = scaler ( b ) return row [ 'SampleID' ] , a , b if isinstance ( filename , str ) : data = read_csv ( filename , ** kwargs ) elif isinstance ( filename , DataFrame ) : data = filename else : raise TypeError ( "Type not supported." ) conv_data = [ ] for idx , row in data . iterrows ( ) : conv_data . append ( convert_row ( row , scale ) ) df = DataFrame ( conv_data , columns = [ 'SampleID' , 'A' , 'B' ] ) df = df . set_index ( "SampleID" ) return df
10513	def registerkbevent ( self , keys , modifiers , fn_name , * args ) : event_name = "kbevent%s%s" % ( keys , modifiers ) self . _pollEvents . _callback [ event_name ] = [ event_name , fn_name , args ] return self . _remote_registerkbevent ( keys , modifiers )
1869	def MOVZX ( cpu , op0 , op1 ) : op0 . write ( Operators . ZEXTEND ( op1 . read ( ) , op0 . size ) )
684	def getTotaln ( self ) : n = sum ( [ field . n for field in self . fields ] ) return n
10080	def _publish_edited ( self ) : record_pid , record = self . fetch_published ( ) if record . revision_id == self [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] : data = dict ( self . dumps ( ) ) else : data = self . merge_with_published ( ) data [ '$schema' ] = self . record_schema data [ '_deposit' ] = self [ '_deposit' ] record = record . __class__ ( data , model = record . model ) return record
1195	def calculate_transitive_deps ( modname , script , gopath ) : deps = set ( ) def calc ( modname , script ) : if modname in deps : return deps . add ( modname ) for imp in collect_imports ( modname , script , gopath ) : if imp . is_native : deps . add ( imp . name ) continue parts = imp . name . split ( '.' ) calc ( imp . name , imp . script ) if len ( parts ) == 1 : continue package_dir , filename = os . path . split ( imp . script ) if filename == '__init__.py' : package_dir = os . path . dirname ( package_dir ) for i in xrange ( len ( parts ) - 1 , 0 , - 1 ) : modname = '.' . join ( parts [ : i ] ) script = os . path . join ( package_dir , '__init__.py' ) calc ( modname , script ) package_dir = os . path . dirname ( package_dir ) calc ( modname , script ) deps . remove ( modname ) return deps
11450	def get_date ( self , filename ) : try : self . document = parse ( filename ) return self . _get_date ( ) except DateNotFoundException : print ( "Date problem found in {0}" . format ( filename ) ) return datetime . datetime . strftime ( datetime . datetime . now ( ) , "%Y-%m-%d" )
343	def validation_metrics ( self ) : if ( self . _validation_iterator is None ) or ( self . _validation_metrics is None ) : raise AttributeError ( 'Validation is not setup.' ) n = 0.0 metric_sums = [ 0.0 ] * len ( self . _validation_metrics ) self . _sess . run ( self . _validation_iterator . initializer ) while True : try : metrics = self . _sess . run ( self . _validation_metrics ) for i , m in enumerate ( metrics ) : metric_sums [ i ] += m n += 1.0 except tf . errors . OutOfRangeError : break for i , m in enumerate ( metric_sums ) : metric_sums [ i ] = metric_sums [ i ] / n return zip ( self . _validation_metrics , metric_sums )
6324	def train ( self , text ) : r text = text_type ( text ) if '\x00' in text : text = text . replace ( '\x00' , ' ' ) counts = Counter ( text ) counts [ '\x00' ] = 1 tot_letters = sum ( counts . values ( ) ) tot = 0 self . _probs = { } prev = Fraction ( 0 ) for char , count in sorted ( counts . items ( ) , key = lambda x : ( x [ 1 ] , x [ 0 ] ) , reverse = True ) : follow = Fraction ( tot + count , tot_letters ) self . _probs [ char ] = ( prev , follow ) prev = follow tot = tot + count
10442	def getobjectlist ( self , window_name ) : try : window_handle , name , app = self . _get_window_handle ( window_name , True ) object_list = self . _get_appmap ( window_handle , name , True ) except atomac . _a11y . ErrorInvalidUIElement : self . _windows = { } window_handle , name , app = self . _get_window_handle ( window_name , True ) object_list = self . _get_appmap ( window_handle , name , True ) return object_list . keys ( )
13650	def get_fuel_price_trends ( self , latitude : float , longitude : float , fuel_types : List [ str ] ) -> PriceTrends : response = requests . post ( '{}/prices/trends/' . format ( API_URL_BASE ) , json = { 'location' : { 'latitude' : latitude , 'longitude' : longitude , } , 'fueltypes' : [ { 'code' : type } for type in fuel_types ] , } , headers = self . _get_headers ( ) , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) data = response . json ( ) return PriceTrends ( variances = [ Variance . deserialize ( variance ) for variance in data [ 'Variances' ] ] , average_prices = [ AveragePrice . deserialize ( avg_price ) for avg_price in data [ 'AveragePrices' ] ] )
2634	def status ( self ) : if self . provider : status = self . provider . status ( self . engines ) else : status = [ ] return status
9553	def _apply_header_checks ( self , i , r , summarize = False , context = None ) : for code , message in self . _header_checks : if tuple ( r ) != self . _field_names : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = tuple ( r ) p [ 'missing' ] = set ( self . _field_names ) - set ( r ) p [ 'unexpected' ] = set ( r ) - set ( self . _field_names ) if context is not None : p [ 'context' ] = context yield p
5244	def missing_info ( ** kwargs ) -> str : func = kwargs . pop ( 'func' , 'unknown' ) if 'ticker' in kwargs : kwargs [ 'ticker' ] = kwargs [ 'ticker' ] . replace ( '/' , '_' ) info = utils . to_str ( kwargs , fmt = '{value}' , sep = '/' ) [ 1 : - 1 ] return f'{func}/{info}'
1355	def get_argument_role ( self ) : try : return self . get_argument ( constants . PARAM_ROLE , default = None ) except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
3239	def get_group ( group_name , users = True , client = None , ** kwargs ) : result = client . get_group ( GroupName = group_name , ** kwargs ) if users : if result . get ( 'IsTruncated' ) : kwargs_to_send = { 'GroupName' : group_name } kwargs_to_send . update ( kwargs ) user_list = result [ 'Users' ] kwargs_to_send [ 'Marker' ] = result [ 'Marker' ] result [ 'Users' ] = user_list + _get_users_for_group ( client , ** kwargs_to_send ) else : result . pop ( 'Users' , None ) result . pop ( 'IsTruncated' , None ) result . pop ( 'Marker' , None ) return result
4353	def socketio_manage ( environ , namespaces , request = None , error_handler = None , json_loads = None , json_dumps = None ) : socket = environ [ 'socketio' ] socket . _set_environ ( environ ) socket . _set_namespaces ( namespaces ) if request : socket . _set_request ( request ) if error_handler : socket . _set_error_handler ( error_handler ) if json_loads : socket . _set_json_loads ( json_loads ) if json_dumps : socket . _set_json_dumps ( json_dumps ) receiver_loop = socket . _spawn_receiver_loop ( ) gevent . joinall ( [ receiver_loop ] ) return
970	def _getEphemeralMembers ( self ) : e = BacktrackingTM . _getEphemeralMembers ( self ) if self . makeCells4Ephemeral : e . extend ( [ 'cells4' ] ) return e
4761	def assert_that ( val , description = '' ) : global _soft_ctx if _soft_ctx : return AssertionBuilder ( val , description , 'soft' ) return AssertionBuilder ( val , description )
12684	def query ( self , input = '' , params = { } ) : payload = { 'input' : input , 'appid' : self . appid } for key , value in params . items ( ) : if isinstance ( value , ( list , tuple ) ) : payload [ key ] = ',' . join ( value ) else : payload [ key ] = value try : r = requests . get ( "http://api.wolframalpha.com/v2/query" , params = payload ) if r . status_code != 200 : raise Exception ( 'Invalid response status code: %s' % ( r . status_code ) ) if r . encoding != 'utf-8' : raise Exception ( 'Invalid encoding: %s' % ( r . encoding ) ) except Exception , e : return Result ( error = e ) return Result ( xml = r . text )
9812	def revoke ( username ) : try : PolyaxonClient ( ) . user . revoke_superuser ( username ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not revoke superuser role from user `{}`.' . format ( username ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Superuser role was revoked successfully from user `{}`." . format ( username ) )
5719	def _convert_path ( path , name ) : table = os . path . splitext ( path ) [ 0 ] table = table . replace ( os . path . sep , '__' ) if name is not None : table = ' ' . join ( [ table , name ] ) table = re . sub ( '[^0-9a-zA-Z_]+' , '_' , table ) table = table . lower ( ) return table
9002	def _register_instruction_in_defs ( self , instruction ) : type_ = instruction . type color_ = instruction . color instruction_to_svg_dict = self . _instruction_to_svg . instruction_to_svg_dict instruction_id = "{}:{}" . format ( type_ , color_ ) defs_id = instruction_id + ":defs" if instruction_id not in self . _instruction_type_color_to_symbol : svg_dict = instruction_to_svg_dict ( instruction ) self . _compute_scale ( instruction_id , svg_dict ) symbol = self . _make_definition ( svg_dict , instruction_id ) self . _instruction_type_color_to_symbol [ defs_id ] = symbol [ DEFINITION_HOLDER ] . pop ( "defs" , { } ) self . _instruction_type_color_to_symbol [ instruction_id ] = symbol return instruction_id
4307	def _validate_file_formats ( input_filepath_list , combine_type ) : _validate_sample_rates ( input_filepath_list , combine_type ) if combine_type == 'concatenate' : _validate_num_channels ( input_filepath_list , combine_type )
6624	def availableVersions ( self ) : r = [ ] for t in self . _getTags ( ) : logger . debug ( "available version tag: %s" , t ) if not len ( t [ 0 ] . strip ( ) ) : continue try : r . append ( GithubComponentVersion ( t [ 0 ] , t [ 0 ] , url = t [ 1 ] , name = self . name , cache_key = None ) ) except ValueError : logger . debug ( 'invalid version tag: %s' , t ) return r
809	def _storeSample ( self , inputVector , trueCatIndex , partition = 0 ) : if self . _samples is None : self . _samples = numpy . zeros ( ( 0 , len ( inputVector ) ) , dtype = RealNumpyDType ) assert self . _labels is None self . _labels = [ ] self . _samples = numpy . concatenate ( ( self . _samples , numpy . atleast_2d ( inputVector ) ) , axis = 0 ) self . _labels += [ trueCatIndex ] if self . _partitions is None : self . _partitions = [ ] if partition is None : partition = 0 self . _partitions += [ partition ]
7889	def update_presence ( self , presence ) : self . presence = MucPresence ( presence ) t = presence . get_type ( ) if t == "unavailable" : self . role = "none" self . affiliation = "none" self . room_jid = self . presence . get_from ( ) self . nick = self . room_jid . resource mc = self . presence . get_muc_child ( ) if isinstance ( mc , MucUserX ) : items = mc . get_items ( ) for item in items : if not isinstance ( item , MucItem ) : continue if item . role : self . role = item . role if item . affiliation : self . affiliation = item . affiliation if item . jid : self . real_jid = item . jid if item . nick : self . new_nick = item . nick break
8254	def _context ( self ) : tags1 = None for clr in self : overlap = [ ] if clr . is_black : name = "black" elif clr . is_white : name = "white" elif clr . is_grey : name = "grey" else : name = clr . nearest_hue ( primary = True ) if name == "orange" and clr . brightness < 0.6 : name = "brown" tags2 = context [ name ] if tags1 is None : tags1 = tags2 else : for tag in tags2 : if tag in tags1 : if tag not in overlap : overlap . append ( tag ) tags1 = overlap overlap . sort ( ) return overlap
2686	def curated ( name ) : return cached_download ( 'https://docs.mikeboers.com/pyav/samples/' + name , os . path . join ( 'pyav-curated' , name . replace ( '/' , os . path . sep ) ) )
991	def copy ( reader , writer , start , stop , insertLocation = None , tsCol = None ) : assert stop >= start startRows = [ ] copyRows = [ ] ts = None inc = None if tsCol is None : tsCol = reader . getTimestampFieldIdx ( ) for i , row in enumerate ( reader ) : if ts is None : ts = row [ tsCol ] elif inc is None : inc = row [ tsCol ] - ts if i >= start and i <= stop : copyRows . append ( row ) startRows . append ( row ) if insertLocation is None : insertLocation = stop + 1 startRows [ insertLocation : insertLocation ] = copyRows for row in startRows : row [ tsCol ] = ts writer . appendRecord ( row ) ts += inc
1530	def monitor ( self ) : def trigger_watches_based_on_files ( watchers , path , directory , ProtoClass ) : for topology , callbacks in watchers . items ( ) : file_path = os . path . join ( path , topology ) data = "" if os . path . exists ( file_path ) : with open ( os . path . join ( path , topology ) ) as f : data = f . read ( ) if topology not in directory or data != directory [ topology ] : proto_object = ProtoClass ( ) proto_object . ParseFromString ( data ) for callback in callbacks : callback ( proto_object ) directory [ topology ] = data while not self . monitoring_thread_stop_signal : topologies_path = self . get_topologies_path ( ) topologies = [ ] if os . path . isdir ( topologies_path ) : topologies = list ( filter ( lambda f : os . path . isfile ( os . path . join ( topologies_path , f ) ) , os . listdir ( topologies_path ) ) ) if set ( topologies ) != set ( self . topologies_directory ) : for callback in self . topologies_watchers : callback ( topologies ) self . topologies_directory = topologies trigger_watches_based_on_files ( self . topology_watchers , topologies_path , self . topologies_directory , Topology ) execution_state_path = os . path . dirname ( self . get_execution_state_path ( "" ) ) trigger_watches_based_on_files ( self . execution_state_watchers , execution_state_path , self . execution_state_directory , ExecutionState ) packing_plan_path = os . path . dirname ( self . get_packing_plan_path ( "" ) ) trigger_watches_based_on_files ( self . packing_plan_watchers , packing_plan_path , self . packing_plan_directory , PackingPlan ) pplan_path = os . path . dirname ( self . get_pplan_path ( "" ) ) trigger_watches_based_on_files ( self . pplan_watchers , pplan_path , self . pplan_directory , PhysicalPlan ) tmaster_path = os . path . dirname ( self . get_tmaster_path ( "" ) ) trigger_watches_based_on_files ( self . tmaster_watchers , tmaster_path , self . tmaster_directory , TMasterLocation ) scheduler_location_path = os . path . dirname ( self . get_scheduler_location_path ( "" ) ) trigger_watches_based_on_files ( self . scheduler_location_watchers , scheduler_location_path , self . scheduler_location_directory , SchedulerLocation ) self . event . wait ( timeout = 5 )
9982	def remove_decorator ( source : str ) : lines = source . splitlines ( ) atok = asttokens . ASTTokens ( source , parse = True ) for node in ast . walk ( atok . tree ) : if isinstance ( node , ast . FunctionDef ) : break if node . decorator_list : deco_first = node . decorator_list [ 0 ] deco_last = node . decorator_list [ - 1 ] line_first = atok . tokens [ deco_first . first_token . index - 1 ] . start [ 0 ] line_last = atok . tokens [ deco_last . last_token . index + 1 ] . start [ 0 ] lines = lines [ : line_first - 1 ] + lines [ line_last : ] return "\n" . join ( lines ) + "\n"
10064	def process_schema ( value ) : schemas = current_app . extensions [ 'invenio-jsonschemas' ] . schemas try : return schemas [ value ] except KeyError : raise click . BadParameter ( 'Unknown schema {0}. Please use one of:\n {1}' . format ( value , '\n' . join ( schemas . keys ( ) ) ) )
12017	def _dump_field ( self , fd ) : v = { } v [ 'label' ] = Pbd . LABELS [ fd . label ] v [ 'type' ] = fd . type_name if len ( fd . type_name ) > 0 else Pbd . TYPES [ fd . type ] v [ 'name' ] = fd . name v [ 'number' ] = fd . number v [ 'default' ] = '[default = {}]' . format ( fd . default_value ) if len ( fd . default_value ) > 0 else '' f = '{label} {type} {name} = {number} {default};' . format ( ** v ) f = ' ' . join ( f . split ( ) ) self . _print ( f ) if len ( fd . type_name ) > 0 : self . uses . append ( fd . type_name )
8175	def update ( self , shuffled = True , cohesion = 100 , separation = 10 , alignment = 5 , goal = 20 , limit = 30 ) : from random import shuffle if shuffled : shuffle ( self ) m1 = 1.0 m2 = 1.0 m3 = 1.0 m4 = 1.0 if not self . scattered and _ctx . random ( ) < self . _scatter : self . scattered = True if self . scattered : m1 = - m1 m3 *= 0.25 self . _scatter_i += 1 if self . _scatter_i >= self . _scatter_t : self . scattered = False self . _scatter_i = 0 if not self . has_goal : m4 = 0 if self . flee : m4 = - m4 for b in self : if b . is_perching : if b . _perch_t > 0 : b . _perch_t -= 1 continue else : b . is_perching = False vx1 , vy1 , vz1 = b . cohesion ( cohesion ) vx2 , vy2 , vz2 = b . separation ( separation ) vx3 , vy3 , vz3 = b . alignment ( alignment ) vx4 , vy4 , vz4 = b . goal ( self . _gx , self . _gy , self . _gz , goal ) b . vx += m1 * vx1 + m2 * vx2 + m3 * vx3 + m4 * vx4 b . vy += m1 * vy1 + m2 * vy2 + m3 * vy3 + m4 * vy4 b . vz += m1 * vz1 + m2 * vz2 + m3 * vz3 + m4 * vz4 b . limit ( limit ) b . x += b . vx b . y += b . vy b . z += b . vz self . constrain ( )
8601	def delete_share ( self , group_id , resource_id ) : response = self . _perform_request ( url = '/um/groups/%s/shares/%s' % ( group_id , resource_id ) , method = 'DELETE' ) return response
929	def _getFuncPtrAndParams ( self , funcName ) : params = None if isinstance ( funcName , basestring ) : if funcName == 'sum' : fp = _aggr_sum elif funcName == 'first' : fp = _aggr_first elif funcName == 'last' : fp = _aggr_last elif funcName == 'mean' : fp = _aggr_mean elif funcName == 'max' : fp = max elif funcName == 'min' : fp = min elif funcName == 'mode' : fp = _aggr_mode elif funcName . startswith ( 'wmean:' ) : fp = _aggr_weighted_mean paramsName = funcName [ 6 : ] params = [ f [ 0 ] for f in self . _inputFields ] . index ( paramsName ) else : fp = funcName return ( fp , params )
2215	def _join_itemstrs ( itemstrs , itemsep , newlines , _leaf_info , nobraces , trailing_sep , compact_brace , lbr , rbr ) : use_newline = newlines > 0 if newlines < 0 : use_newline = ( - newlines ) < _leaf_info [ 'max_height' ] if use_newline : sep = ',\n' if nobraces : body_str = sep . join ( itemstrs ) if trailing_sep and len ( itemstrs ) > 0 : body_str += ',' retstr = body_str else : if compact_brace : indented = itemstrs else : import ubelt as ub prefix = ' ' * 4 indented = [ ub . indent ( s , prefix ) for s in itemstrs ] body_str = sep . join ( indented ) if trailing_sep and len ( itemstrs ) > 0 : body_str += ',' if compact_brace : braced_body_str = ( lbr + body_str . replace ( '\n' , '\n ' ) + rbr ) else : braced_body_str = ( lbr + '\n' + body_str + '\n' + rbr ) retstr = braced_body_str else : sep = ',' + itemsep body_str = sep . join ( itemstrs ) if trailing_sep and len ( itemstrs ) > 0 : body_str += ',' retstr = ( lbr + body_str + rbr ) return retstr
3186	def create ( self , conversation_id , data ) : self . conversation_id = conversation_id if 'from_email' not in data : raise KeyError ( 'The conversation message must have a from_email' ) check_email ( data [ 'from_email' ] ) if 'read' not in data : raise KeyError ( 'The conversation message must have a read' ) if data [ 'read' ] not in [ True , False ] : raise TypeError ( 'The conversation message read must be True or False' ) response = self . _mc_client . _post ( url = self . _build_path ( conversation_id , 'messages' ) , data = data ) if response is not None : self . message_id = response [ 'id' ] else : self . message_id = None return response
4482	def storages ( self ) : stores = self . _json ( self . _get ( self . _storages_url ) , 200 ) stores = stores [ 'data' ] for store in stores : yield Storage ( store , self . session )
3047	def _do_retrieve_scopes ( self , http , token ) : logger . info ( 'Refreshing scopes' ) query_params = { 'access_token' : token , 'fields' : 'scope' } token_info_uri = _helpers . update_query_params ( self . token_info_uri , query_params ) resp , content = transport . request ( http , token_info_uri ) content = _helpers . _from_bytes ( content ) if resp . status == http_client . OK : d = json . loads ( content ) self . scopes = set ( _helpers . string_to_scopes ( d . get ( 'scope' , '' ) ) ) else : error_msg = 'Invalid response {0}.' . format ( resp . status ) try : d = json . loads ( content ) if 'error_description' in d : error_msg = d [ 'error_description' ] except ( TypeError , ValueError ) : pass raise Error ( error_msg )
8406	def zero_range ( x , tol = np . finfo ( float ) . eps * 100 ) : try : if len ( x ) == 1 : return True except TypeError : return True if len ( x ) != 2 : raise ValueError ( 'x must be length 1 or 2' ) x = tuple ( x ) if isinstance ( x [ 0 ] , ( pd . Timestamp , datetime . datetime ) ) : x = date2num ( x ) elif isinstance ( x [ 0 ] , np . datetime64 ) : return x [ 0 ] == x [ 1 ] elif isinstance ( x [ 0 ] , ( pd . Timedelta , datetime . timedelta ) ) : x = x [ 0 ] . total_seconds ( ) , x [ 1 ] . total_seconds ( ) elif isinstance ( x [ 0 ] , np . timedelta64 ) : return x [ 0 ] == x [ 1 ] elif not isinstance ( x [ 0 ] , ( float , int , np . number ) ) : raise TypeError ( "zero_range objects cannot work with objects " "of type '{}'" . format ( type ( x [ 0 ] ) ) ) if any ( np . isnan ( x ) ) : return np . nan if x [ 0 ] == x [ 1 ] : return True if all ( np . isinf ( x ) ) : return False m = np . abs ( x ) . min ( ) if m == 0 : return False return np . abs ( ( x [ 0 ] - x [ 1 ] ) / m ) < tol
727	def numbersForBit ( self , bit ) : if bit >= self . _n : raise IndexError ( "Invalid bit" ) numbers = set ( ) for index , pattern in self . _patterns . iteritems ( ) : if bit in pattern : numbers . add ( index ) return numbers
3152	def delete ( self , list_id , webhook_id ) : self . list_id = list_id self . webhook_id = webhook_id return self . _mc_client . _delete ( url = self . _build_path ( list_id , 'webhooks' , webhook_id ) )
12501	def _smooth_data_array ( arr , affine , fwhm , copy = True ) : if arr . dtype . kind == 'i' : if arr . dtype == np . int64 : arr = arr . astype ( np . float64 ) else : arr = arr . astype ( np . float32 ) if copy : arr = arr . copy ( ) arr [ np . logical_not ( np . isfinite ( arr ) ) ] = 0 try : affine = affine [ : 3 , : 3 ] fwhm_sigma_ratio = np . sqrt ( 8 * np . log ( 2 ) ) vox_size = np . sqrt ( np . sum ( affine ** 2 , axis = 0 ) ) sigma = fwhm / ( fwhm_sigma_ratio * vox_size ) for n , s in enumerate ( sigma ) : ndimage . gaussian_filter1d ( arr , s , output = arr , axis = n ) except : raise ValueError ( 'Error smoothing the array.' ) else : return arr
9946	def cur_space ( self , name = None ) : if name is None : return self . _impl . model . currentspace . interface else : self . _impl . model . currentspace = self . _impl . spaces [ name ] return self . cur_space ( )
4256	def get_compressed_filename ( self , filename ) : if not os . path . splitext ( filename ) [ 1 ] [ 1 : ] in self . suffixes_to_compress : return False file_stats = None compressed_stats = None compressed_filename = '{}.{}' . format ( filename , self . suffix ) try : file_stats = os . stat ( filename ) compressed_stats = os . stat ( compressed_filename ) except OSError : pass if file_stats and compressed_stats : return ( compressed_filename if file_stats . st_mtime > compressed_stats . st_mtime else False ) else : return compressed_filename
7216	def register ( self , task_json = None , json_filename = None ) : if not task_json and not json_filename : raise Exception ( "Both task json and filename can't be none." ) if task_json and json_filename : raise Exception ( "Both task json and filename can't be provided." ) if json_filename : task_json = json . load ( open ( json_filename , 'r' ) ) r = self . gbdx_connection . post ( self . _base_url , json = task_json ) raise_for_status ( r ) return r . text
7530	def build_dag ( data , samples ) : snames = [ i . name for i in samples ] dag = nx . DiGraph ( ) joborder = JOBORDER [ data . paramsdict [ "assembly_method" ] ] for sname in snames : for func in joborder : dag . add_node ( "{}-{}-{}" . format ( func , 0 , sname ) ) for chunk in xrange ( 10 ) : dag . add_node ( "{}-{}-{}" . format ( "muscle_align" , chunk , sname ) ) dag . add_node ( "{}-{}-{}" . format ( "reconcat" , 0 , sname ) ) for sname in snames : for sname2 in snames : dag . add_edge ( "{}-{}-{}" . format ( joborder [ 0 ] , 0 , sname2 ) , "{}-{}-{}" . format ( joborder [ 1 ] , 0 , sname ) ) for idx in xrange ( 2 , len ( joborder ) ) : dag . add_edge ( "{}-{}-{}" . format ( joborder [ idx - 1 ] , 0 , sname ) , "{}-{}-{}" . format ( joborder [ idx ] , 0 , sname ) ) for sname2 in snames : for chunk in range ( 10 ) : dag . add_edge ( "{}-{}-{}" . format ( "muscle_chunker" , 0 , sname2 ) , "{}-{}-{}" . format ( "muscle_align" , chunk , sname ) ) dag . add_edge ( "{}-{}-{}" . format ( "muscle_align" , chunk , sname ) , "{}-{}-{}" . format ( "reconcat" , 0 , sname ) ) return dag , joborder
13509	def sloccount ( ) : setup = options . get ( 'setup' ) packages = options . get ( 'packages' ) if setup else None if packages : dirs = [ x for x in packages if '.' not in x ] else : dirs = [ '.' ] ls = [ ] for d in dirs : ls += list ( path ( d ) . walkfiles ( ) ) files = ' ' . join ( ls ) param = options . paved . pycheck . sloccount . param sh ( 'sloccount {param} {files} | tee sloccount.sc' . format ( param = param , files = files ) )
11303	def invalidate_stored_oembeds ( self , sender , instance , created , ** kwargs ) : ctype = ContentType . objects . get_for_model ( instance ) StoredOEmbed . objects . filter ( object_id = instance . pk , content_type = ctype ) . delete ( )
10279	def neurommsig_topology ( graph : BELGraph , nodes : List [ BaseEntity ] ) -> float : nodes = list ( nodes ) number_nodes = len ( nodes ) if number_nodes <= 1 : return 0.0 unnormalized_sum = sum ( u in graph [ v ] for u , v in itt . product ( nodes , repeat = 2 ) if v in graph and u != v ) return unnormalized_sum / ( number_nodes * ( number_nodes - 1.0 ) )
1189	def _randbelow ( self , n ) : k = _int_bit_length ( n ) r = self . getrandbits ( k ) while r >= n : r = self . getrandbits ( k ) return r
10861	def param_particle_pos ( self , ind ) : ind = self . _vps ( listify ( ind ) ) return [ self . _i2p ( i , j ) for i in ind for j in [ 'z' , 'y' , 'x' ] ]
5501	def remove_tweets ( self , url ) : try : del self . cache [ url ] self . mark_updated ( ) return True except KeyError : return False
10845	def shuffle ( self , count = None , utc = None ) : url = PATHS [ 'SHUFFLE' ] % self . profile_id post_data = '' if count : post_data += 'count=%s&' % count if utc : post_data += 'utc=%s' % utc return self . api . post ( url = url , data = post_data )
4613	def blocks ( self , start = None , stop = None ) : self . block_interval = self . get_block_interval ( ) if not start : start = self . get_current_block_num ( ) while True : if stop : head_block = stop else : head_block = self . get_current_block_num ( ) for blocknum in range ( start , head_block + 1 ) : block = self . wait_for_and_get_block ( blocknum ) block . update ( { "block_num" : blocknum } ) yield block start = head_block + 1 if stop and start > stop : return time . sleep ( self . block_interval )
13778	def FindFileContainingSymbol ( self , symbol ) : symbol = _NormalizeFullyQualifiedName ( symbol ) try : return self . _descriptors [ symbol ] . file except KeyError : pass try : return self . _enum_descriptors [ symbol ] . file except KeyError : pass try : file_proto = self . _internal_db . FindFileContainingSymbol ( symbol ) except KeyError as error : if self . _descriptor_db : file_proto = self . _descriptor_db . FindFileContainingSymbol ( symbol ) else : raise error if not file_proto : raise KeyError ( 'Cannot find a file containing %s' % symbol ) return self . _ConvertFileProtoToFileDescriptor ( file_proto )
8157	def close ( self ) : self . _con . commit ( ) self . _cur . close ( ) self . _con . close ( )
8168	def run ( self ) : with LiveExecution . lock : if self . edited_source : success , ex = self . run_tenuous ( ) if success : return self . do_exec ( self . known_good , self . ns )
3078	def email ( self ) : if not self . credentials : return None try : return self . credentials . id_token [ 'email' ] except KeyError : current_app . logger . error ( 'Invalid id_token {0}' . format ( self . credentials . id_token ) )
3866	async def rename ( self , name ) : await self . _client . rename_conversation ( hangouts_pb2 . RenameConversationRequest ( request_header = self . _client . get_request_header ( ) , new_name = name , event_request_header = self . _get_event_request_header ( ) , ) )
1760	def read_bytes ( self , where , size , force = False ) : result = [ ] for i in range ( size ) : result . append ( Operators . CHR ( self . read_int ( where + i , 8 , force ) ) ) return result
7832	def _new_from_xml ( cls , xmlnode ) : child = xmlnode . children fields = [ ] while child : if child . type != "element" or child . ns ( ) . content != DATAFORM_NS : pass elif child . name == "field" : fields . append ( Field . _new_from_xml ( child ) ) child = child . next return cls ( fields )
13549	def pip_install ( * args ) : download_cache = ( '--download-cache=%s ' % options . paved . pip . download_cache ) if options . paved . pip . download_cache else '' shv ( 'pip install %s%s' % ( download_cache , ' ' . join ( args ) ) )
5145	def _merge_config ( self , config , templates ) : if not templates : return config if not isinstance ( templates , list ) : raise TypeError ( 'templates argument must be an instance of list' ) result = { } config_list = templates + [ config ] for merging in config_list : result = merge_config ( result , self . _load ( merging ) , self . list_identifiers ) return result
2414	def write_creation_info ( creation_info , out ) : out . write ( '# Creation Info\n\n' ) for creator in sorted ( creation_info . creators ) : write_value ( 'Creator' , creator , out ) write_value ( 'Created' , creation_info . created_iso_format , out ) if creation_info . has_comment : write_text_value ( 'CreatorComment' , creation_info . comment , out )
5643	def get_min_visit_time ( self ) : if not self . visit_events : return float ( 'inf' ) else : return min ( self . visit_events , key = lambda event : event . arr_time_ut ) . arr_time_ut
543	def _getFieldStats ( self ) : fieldStats = dict ( ) fieldNames = self . _inputSource . getFieldNames ( ) for field in fieldNames : curStats = dict ( ) curStats [ 'min' ] = self . _inputSource . getFieldMin ( field ) curStats [ 'max' ] = self . _inputSource . getFieldMax ( field ) fieldStats [ field ] = curStats return fieldStats
2702	def enumerate_chunks ( phrase , spacy_nlp ) : if ( len ( phrase ) > 1 ) : found = False text = " " . join ( [ rl . text for rl in phrase ] ) doc = spacy_nlp ( text . strip ( ) , parse = True ) for np in doc . noun_chunks : if np . text != text : found = True yield np . text , find_chunk ( phrase , np . text . split ( " " ) ) if not found and all ( [ rl . pos [ 0 ] != "v" for rl in phrase ] ) : yield text , phrase
6569	def signature_matches ( func , args = ( ) , kwargs = { } ) : try : sig = inspect . signature ( func ) sig . bind ( * args , ** kwargs ) except TypeError : return False else : return True
11259	def grep ( prev , pattern , * args , ** kw ) : inv = False if 'inv' not in kw else kw . pop ( 'inv' ) pattern_obj = re . compile ( pattern , * args , ** kw ) for data in prev : if bool ( inv ) ^ bool ( pattern_obj . match ( data ) ) : yield data
8829	def security_group_rule_update ( context , rule , ** kwargs ) : rule . update ( kwargs ) context . session . add ( rule ) return rule
7441	def set_params ( self , param , newvalue ) : legacy_params = [ "edit_cutsites" , "trim_overhang" ] current_params = self . paramsdict . keys ( ) allowed_params = current_params + legacy_params if not param in allowed_params : raise IPyradParamsError ( "Parameter key not recognized: {}" . format ( param ) ) param = str ( param ) if len ( param ) < 3 : param = self . paramsdict . keys ( ) [ int ( param ) ] try : self = _paramschecker ( self , param , newvalue ) except Exception as inst : raise IPyradWarningExit ( BAD_PARAMETER . format ( param , inst , newvalue ) )
0	def save_act ( self , path = None ) : if path is None : path = os . path . join ( logger . get_dir ( ) , "model.pkl" ) with tempfile . TemporaryDirectory ( ) as td : save_variables ( os . path . join ( td , "model" ) ) arc_name = os . path . join ( td , "packed.zip" ) with zipfile . ZipFile ( arc_name , 'w' ) as zipf : for root , dirs , files in os . walk ( td ) : for fname in files : file_path = os . path . join ( root , fname ) if file_path != arc_name : zipf . write ( file_path , os . path . relpath ( file_path , td ) ) with open ( arc_name , "rb" ) as f : model_data = f . read ( ) with open ( path , "wb" ) as f : cloudpickle . dump ( ( model_data , self . _act_params ) , f )
851	def rewind ( self ) : super ( FileRecordStream , self ) . rewind ( ) self . close ( ) self . _file = open ( self . _filename , self . _mode ) self . _reader = csv . reader ( self . _file , dialect = "excel" ) self . _reader . next ( ) self . _reader . next ( ) self . _reader . next ( ) self . _recordCount = 0
4515	def drawLine ( self , x0 , y0 , x1 , y1 , color = None , colorFunc = None , aa = False ) : md . draw_line ( self . set , x0 , y0 , x1 , y1 , color , colorFunc , aa )
9871	def write ( self , data , sections = None ) : if self . error [ 0 ] : self . status = self . error [ 0 ] data = b ( self . error [ 1 ] ) if not self . headers_sent : self . send_headers ( data , sections ) if self . request_method != 'HEAD' : try : if self . chunked : self . conn . sendall ( b ( '%x\r\n%s\r\n' % ( len ( data ) , data ) ) ) else : self . conn . sendall ( data ) except socket . timeout : self . closeConnection = True except socket . error : self . closeConnection = True
7568	def comp ( seq ) : return seq . replace ( "A" , 't' ) . replace ( 'T' , 'a' ) . replace ( 'C' , 'g' ) . replace ( 'G' , 'c' ) . replace ( 'n' , 'Z' ) . upper ( ) . replace ( "Z" , "n" )
1702	def outer_right_join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . OUTER_RIGHT , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
5642	def compute_pseudo_connections ( transit_connections , start_time_dep , end_time_dep , transfer_margin , walk_network , walk_speed ) : pseudo_connection_set = set ( ) for c in transit_connections : if start_time_dep <= c . departure_time <= end_time_dep : walk_arr_stop = c . departure_stop walk_arr_time = c . departure_time - transfer_margin for _ , walk_dep_stop , data in walk_network . edges ( nbunch = [ walk_arr_stop ] , data = True ) : walk_dep_time = walk_arr_time - data [ 'd_walk' ] / float ( walk_speed ) if walk_dep_time > end_time_dep or walk_dep_time < start_time_dep : continue pseudo_connection = Connection ( walk_dep_stop , walk_arr_stop , walk_dep_time , walk_arr_time , Connection . WALK_TRIP_ID , Connection . WALK_SEQ , is_walk = True ) pseudo_connection_set . add ( pseudo_connection ) return pseudo_connection_set
8299	def decodeOSC ( data ) : table = { "i" : readInt , "f" : readFloat , "s" : readString , "b" : readBlob } decoded = [ ] address , rest = readString ( data ) typetags = "" if address == "#bundle" : time , rest = readLong ( rest ) while len ( rest ) > 0 : length , rest = readInt ( rest ) decoded . append ( decodeOSC ( rest [ : length ] ) ) rest = rest [ length : ] elif len ( rest ) > 0 : typetags , rest = readString ( rest ) decoded . append ( address ) decoded . append ( typetags ) if typetags [ 0 ] == "," : for tag in typetags [ 1 : ] : value , rest = table [ tag ] ( rest ) decoded . append ( value ) else : print "Oops, typetag lacks the magic ," return decoded
9315	def _ensure_datetime_to_string ( maybe_dttm ) : if isinstance ( maybe_dttm , datetime . datetime ) : maybe_dttm = _format_datetime ( maybe_dttm ) return maybe_dttm
11677	def transform ( self , X ) : X = as_features ( X ) return np . vstack ( [ np . mean ( bag , axis = 0 ) for bag in X ] )
11328	def select ( options = None ) : if not options : return None width = len ( str ( len ( options ) ) ) for x , option in enumerate ( options ) : sys . stdout . write ( '{:{width}}) {}\n' . format ( x + 1 , option , width = width ) ) sys . stdout . write ( '{:>{width}} ' . format ( '#?' , width = width + 1 ) ) sys . stdout . flush ( ) if sys . stdin . isatty ( ) : try : response = raw_input ( ) . strip ( ) except ( EOFError , KeyboardInterrupt ) : response = '' else : sys . stdin = open ( "/dev/tty" ) try : response = '' while True : response += sys . stdin . read ( 1 ) if response . endswith ( '\n' ) : break except ( EOFError , KeyboardInterrupt ) : sys . stdout . flush ( ) pass try : response = int ( response ) - 1 except ValueError : return None if response < 0 or response >= len ( options ) : return None return options [ response ]
4008	def _increase_file_handle_limit ( ) : logging . info ( 'Increasing file handle limit to {}' . format ( constants . FILE_HANDLE_LIMIT ) ) resource . setrlimit ( resource . RLIMIT_NOFILE , ( constants . FILE_HANDLE_LIMIT , resource . RLIM_INFINITY ) )
12968	def delete ( self ) : if self . filters or self . notFilters : return self . mdl . deleter . deleteMultiple ( self . allOnlyIndexedFields ( ) ) return self . mdl . deleter . destroyModel ( )
7524	def _collapse_outgroup ( tree , taxdicts ) : outg = taxdicts [ 0 ] [ "p4" ] if not all ( [ i [ "p4" ] == outg for i in taxdicts ] ) : raise Exception ( "no good" ) tre = ete . Tree ( tree . write ( format = 1 ) ) alltax = [ i for i in tre . get_leaf_names ( ) if i not in outg ] alltax += [ outg [ 0 ] ] tre . prune ( alltax ) tre . search_nodes ( name = outg [ 0 ] ) [ 0 ] . name = "outgroup" tre . ladderize ( ) taxd = copy . deepcopy ( taxdicts ) newtaxdicts = [ ] for test in taxd : test [ "p4" ] = [ "outgroup" ] newtaxdicts . append ( test ) return tre , newtaxdicts
3063	def parse_unique_urlencoded ( content ) : urlencoded_params = urllib . parse . parse_qs ( content ) params = { } for key , value in six . iteritems ( urlencoded_params ) : if len ( value ) != 1 : msg = ( 'URL-encoded content contains a repeated value:' '%s -> %s' % ( key , ', ' . join ( value ) ) ) raise ValueError ( msg ) params [ key ] = value [ 0 ] return params
8610	def get_resource ( self , resource_type , resource_id , depth = 1 ) : response = self . _perform_request ( '/um/resources/%s/%s?depth=%s' % ( resource_type , resource_id , str ( depth ) ) ) return response
3209	def get_load_balancer ( load_balancer , flags = FLAGS . ALL ^ FLAGS . POLICY_TYPES , ** conn ) : try : basestring except NameError as _ : basestring = str if isinstance ( load_balancer , basestring ) : load_balancer = dict ( LoadBalancerName = load_balancer ) return registry . build_out ( flags , start_with = load_balancer , pass_datastructure = True , ** conn )
1054	def seed ( self , a = None ) : super ( Random , self ) . seed ( a ) self . gauss_next = None
11726	def format_seconds ( self , n_seconds ) : func = self . ok if n_seconds >= 60 : n_minutes , n_seconds = divmod ( n_seconds , 60 ) return "%s minutes %s seconds" % ( func ( "%d" % n_minutes ) , func ( "%.3f" % n_seconds ) ) else : return "%s seconds" % ( func ( "%.3f" % n_seconds ) )
494	def acquireConnection ( self ) : self . _logger . debug ( "Acquiring connection" ) dbConn = SteadyDB . connect ( ** _getCommonSteadyDBArgsDict ( ) ) connWrap = ConnectionWrapper ( dbConn = dbConn , cursor = dbConn . cursor ( ) , releaser = self . _releaseConnection , logger = self . _logger ) return connWrap
2907	def _is_descendant_of ( self , parent ) : if self . parent is None : return False if self . parent == parent : return True return self . parent . _is_descendant_of ( parent )
2604	def close ( self ) : if self . reuse : logger . debug ( "Ipcontroller not shutting down: reuse enabled" ) return if self . mode == "manual" : logger . debug ( "Ipcontroller not shutting down: Manual mode" ) return try : pgid = os . getpgid ( self . proc . pid ) os . killpg ( pgid , signal . SIGTERM ) time . sleep ( 0.2 ) os . killpg ( pgid , signal . SIGKILL ) try : self . proc . wait ( timeout = 1 ) x = self . proc . returncode if x == 0 : logger . debug ( "Controller exited with {0}" . format ( x ) ) else : logger . error ( "Controller exited with {0}. May require manual cleanup" . format ( x ) ) except subprocess . TimeoutExpired : logger . warn ( "Ipcontroller process:{0} cleanup failed. May require manual cleanup" . format ( self . proc . pid ) ) except Exception as e : logger . warn ( "Failed to kill the ipcontroller process[{0}]: {1}" . format ( self . proc . pid , e ) )
5005	def get_enterprise_customer_for_running_pipeline ( request , pipeline ) : sso_provider_id = request . GET . get ( 'tpa_hint' ) if pipeline : sso_provider_id = Registry . get_from_pipeline ( pipeline ) . provider_id return get_enterprise_customer_for_sso ( sso_provider_id )
12681	def copy_attributes ( source , destination , ignore_patterns = [ ] ) : for attr in _wildcard_filter ( dir ( source ) , * ignore_patterns ) : setattr ( destination , attr , getattr ( source , attr ) )
9366	def account_number ( ) : account = [ random . randint ( 1 , 9 ) for _ in range ( 20 ) ] return "" . join ( map ( str , account ) )
7069	def get_varfeatures ( simbasedir , mindet = 1000 , nworkers = None ) : with open ( os . path . join ( simbasedir , 'fakelcs-info.pkl' ) , 'rb' ) as infd : siminfo = pickle . load ( infd ) lcfpaths = siminfo [ 'lcfpath' ] varfeaturedir = os . path . join ( simbasedir , 'varfeatures' ) timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] fakelc_formatkey = 'fake-%s' % siminfo [ 'lcformat' ] lcproc . register_lcformat ( fakelc_formatkey , '*-fakelc.pkl' , timecols , magcols , errcols , 'astrobase.lcproc' , '_read_pklc' , magsarefluxes = siminfo [ 'magsarefluxes' ] ) varinfo = lcvfeatures . parallel_varfeatures ( lcfpaths , varfeaturedir , lcformat = fakelc_formatkey , mindet = mindet , nworkers = nworkers ) with open ( os . path . join ( simbasedir , 'fakelc-varfeatures.pkl' ) , 'wb' ) as outfd : pickle . dump ( varinfo , outfd , pickle . HIGHEST_PROTOCOL ) return os . path . join ( simbasedir , 'fakelc-varfeatures.pkl' )
12649	def is_valid_regex ( string ) : try : re . compile ( string ) is_valid = True except re . error : is_valid = False return is_valid
1312	def HardwareInput ( uMsg : int , param : int = 0 ) -> INPUT : return _CreateInput ( HARDWAREINPUT ( uMsg , param & 0xFFFF , param >> 16 & 0xFFFF ) )
6603	def package_fullpath ( self , package_index ) : ret = os . path . join ( self . path , self . package_relpath ( package_index ) ) return ret
1139	def wrap ( text , width = 70 , ** kwargs ) : w = TextWrapper ( width = width , ** kwargs ) return w . wrap ( text )
3741	def omega_mixture ( omegas , zs , CASRNs = None , Method = None , AvailableMethods = False ) : r def list_methods ( ) : methods = [ ] if none_and_length_check ( [ zs , omegas ] ) : methods . append ( 'SIMPLE' ) methods . append ( 'NONE' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'SIMPLE' : _omega = mixing_simple ( zs , omegas ) elif Method == 'NONE' : _omega = None else : raise Exception ( 'Failure in in function' ) return _omega
12453	def config_to_args ( config ) : result = [ ] for key , value in iteritems ( config ) : if value is False : continue key = '--{0}' . format ( key . replace ( '_' , '-' ) ) if isinstance ( value , ( list , set , tuple ) ) : for item in value : result . extend ( ( key , smart_str ( item ) ) ) elif value is not True : result . extend ( ( key , smart_str ( value ) ) ) else : result . append ( key ) return tuple ( result )
5236	def file_modified_time ( file_name ) -> pd . Timestamp : return pd . to_datetime ( time . ctime ( os . path . getmtime ( filename = file_name ) ) )
2780	def get_object ( cls , api_token , domain , record_id ) : record = cls ( token = api_token , domain = domain , id = record_id ) record . load ( ) return record
6021	def from_fits_renormalized ( cls , file_path , hdu , pixel_scale ) : psf = PSF . from_fits_with_scale ( file_path , hdu , pixel_scale ) psf [ : , : ] = np . divide ( psf , np . sum ( psf ) ) return psf
10405	def bond_canonical_statistics ( microcanonical_statistics , convolution_factors , ** kwargs ) : spanning_cluster = ( 'has_spanning_cluster' in microcanonical_statistics . dtype . names ) ret = np . empty ( 1 , dtype = canonical_statistics_dtype ( spanning_cluster ) ) if spanning_cluster : ret [ 'percolation_probability' ] = np . sum ( convolution_factors * microcanonical_statistics [ 'has_spanning_cluster' ] ) ret [ 'max_cluster_size' ] = np . sum ( convolution_factors * microcanonical_statistics [ 'max_cluster_size' ] ) ret [ 'moments' ] = np . sum ( convolution_factors [ : , np . newaxis ] * microcanonical_statistics [ 'moments' ] , axis = 0 , ) return ret
6531	def get_user_config ( project_path , use_cache = True ) : if sys . platform == 'win32' : user_config = os . path . expanduser ( r'~\\tidypy' ) else : user_config = os . path . join ( os . getenv ( 'XDG_CONFIG_HOME' ) or os . path . expanduser ( '~/.config' ) , 'tidypy' ) if os . path . exists ( user_config ) : with open ( user_config , 'r' ) as config_file : config = pytoml . load ( config_file ) . get ( 'tidypy' , { } ) config = merge_dict ( get_default_config ( ) , config ) config = process_extensions ( config , project_path , use_cache = use_cache ) return config return None
11877	def format ( self , record ) : try : record . message = record . getMessage ( ) except TypeError : if record . args : if isinstance ( record . args , collections . Mapping ) : record . message = record . msg . format ( ** record . args ) else : record . message = record . msg . format ( record . args ) self . _fmt = self . getfmt ( record . levelname ) if self . usesTime ( ) : record . asctime = self . formatTime ( record , self . datefmt ) s = self . _fmt . format ( ** record . __dict__ ) if record . exc_info : if not record . exc_text : record . exc_text = self . formatException ( record . exc_info ) if record . exc_text : if s [ - 1 : ] != '\n' : s += '\n' try : s = s + record . exc_text except UnicodeError : s = s + record . exc_text . decode ( sys . getfilesystemencoding ( ) , 'replace' ) return s
3351	def get_by_any ( self , iterable ) : def get_item ( item ) : if isinstance ( item , int ) : return self [ item ] elif isinstance ( item , string_types ) : return self . get_by_id ( item ) elif item in self : return item else : raise TypeError ( "item in iterable cannot be '%s'" % type ( item ) ) if not isinstance ( iterable , list ) : iterable = [ iterable ] return [ get_item ( item ) for item in iterable ]
524	def _inhibitColumns ( self , overlaps ) : if ( self . _localAreaDensity > 0 ) : density = self . _localAreaDensity else : inhibitionArea = ( ( 2 * self . _inhibitionRadius + 1 ) ** self . _columnDimensions . size ) inhibitionArea = min ( self . _numColumns , inhibitionArea ) density = float ( self . _numActiveColumnsPerInhArea ) / inhibitionArea density = min ( density , 0.5 ) if self . _globalInhibition or self . _inhibitionRadius > max ( self . _columnDimensions ) : return self . _inhibitColumnsGlobal ( overlaps , density ) else : return self . _inhibitColumnsLocal ( overlaps , density )
8547	def get_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule_id ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/nics/%s/firewallrules/%s' % ( datacenter_id , server_id , nic_id , firewall_rule_id ) ) return response
605	def getVersion ( ) : with open ( os . path . join ( REPO_DIR , "VERSION" ) , "r" ) as versionFile : return versionFile . read ( ) . strip ( )
6634	def displayOutdated ( modules , dependency_specs , use_colours ) : if use_colours : DIM = colorama . Style . DIM NORMAL = colorama . Style . NORMAL BRIGHT = colorama . Style . BRIGHT YELLOW = colorama . Fore . YELLOW RED = colorama . Fore . RED GREEN = colorama . Fore . GREEN RESET = colorama . Style . RESET_ALL else : DIM = BRIGHT = YELLOW = RED = GREEN = RESET = u'' status = 0 from yotta . lib import access from yotta . lib import access_common from yotta . lib import sourceparse for name , m in modules . items ( ) : if m . isTestDependency ( ) : continue try : latest_v = access . latestSuitableVersion ( name , '*' , registry = 'modules' , quiet = True ) except access_common . Unavailable as e : latest_v = None if not m : m_version = u' ' + RESET + BRIGHT + RED + u"missing" + RESET else : m_version = DIM + u'@%s' % ( m . version ) if not latest_v : print ( u'%s%s%s%s not available from the registry%s' % ( RED , name , m_version , NORMAL , RESET ) ) status = 2 continue elif not m or m . version < latest_v : update_prevented_by = '' if m : specs_preventing_update = [ x for x in dependency_specs if x . name == name and not sourceparse . parseSourceURL ( x . nonShrinkwrappedVersionReq ( ) ) . semanticSpecMatches ( latest_v ) ] shrinkwrap_prevents_update = [ x for x in dependency_specs if x . name == name and x . isShrinkwrapped ( ) and not sourceparse . parseSourceURL ( x . versionReq ( ) ) . semanticSpecMatches ( latest_v ) ] if len ( specs_preventing_update ) : update_prevented_by = ' (update prevented by specifications: %s)' % ( ', ' . join ( [ '%s from %s' % ( x . version_req , x . specifying_module ) for x in specs_preventing_update ] ) ) if len ( shrinkwrap_prevents_update ) : update_prevented_by += ' yotta-shrinkwrap.json prevents update' if m . version . major ( ) < latest_v . major ( ) : colour = GREEN elif m . version . minor ( ) < latest_v . minor ( ) : colour = YELLOW else : colour = RED else : colour = RED print ( u'%s%s%s latest: %s%s%s%s' % ( name , m_version , RESET , colour , latest_v . version , update_prevented_by , RESET ) ) if not status : status = 1 return status
3045	def _refresh ( self , http ) : if not self . store : self . _do_refresh_request ( http ) else : self . store . acquire_lock ( ) try : new_cred = self . store . locked_get ( ) if ( new_cred and not new_cred . invalid and new_cred . access_token != self . access_token and not new_cred . access_token_expired ) : logger . info ( 'Updated access_token read from Storage' ) self . _updateFromCredential ( new_cred ) else : self . _do_refresh_request ( http ) finally : self . store . release_lock ( )
6880	def _parse_csv_header ( header ) : headerlines = header . split ( '\n' ) headerlines = [ x . lstrip ( '# ' ) for x in headerlines ] objectstart = headerlines . index ( 'OBJECT' ) metadatastart = headerlines . index ( 'METADATA' ) camfilterstart = headerlines . index ( 'CAMFILTERS' ) photaperturestart = headerlines . index ( 'PHOTAPERTURES' ) columnstart = headerlines . index ( 'COLUMNS' ) lcstart = headerlines . index ( 'LIGHTCURVE' ) objectinfo = headerlines [ objectstart + 1 : metadatastart - 1 ] metadatainfo = headerlines [ metadatastart + 1 : camfilterstart - 1 ] camfilterinfo = headerlines [ camfilterstart + 1 : photaperturestart - 1 ] photapertureinfo = headerlines [ photaperturestart + 1 : columnstart - 1 ] columninfo = headerlines [ columnstart + 1 : lcstart - 1 ] metadict = { 'objectinfo' : { } } objectinfo = [ x . split ( ';' ) for x in objectinfo ] for elem in objectinfo : for kvelem in elem : key , val = kvelem . split ( ' = ' , 1 ) metadict [ 'objectinfo' ] [ key . strip ( ) ] = ( _smartcast ( val , METAKEYS [ key . strip ( ) ] ) ) metadict [ 'objectid' ] = metadict [ 'objectinfo' ] [ 'objectid' ] [ : ] del metadict [ 'objectinfo' ] [ 'objectid' ] metadatainfo = [ x . split ( ';' ) for x in metadatainfo ] for elem in metadatainfo : for kvelem in elem : try : key , val = kvelem . split ( ' = ' , 1 ) if key . strip ( ) == 'lcbestaperture' : val = json . loads ( val ) if key . strip ( ) in ( 'datarelease' , 'lcversion' ) : val = int ( val ) if key . strip ( ) == 'lastupdated' : val = float ( val ) metadict [ key . strip ( ) ] = val except Exception as e : LOGWARNING ( 'could not understand header element "%s",' ' skipped.' % kvelem ) metadict [ 'filters' ] = [ ] for row in camfilterinfo : filterid , filtername , filterdesc = row . split ( ' - ' ) metadict [ 'filters' ] . append ( ( int ( filterid ) , filtername , filterdesc ) ) metadict [ 'lcapertures' ] = { } for row in photapertureinfo : apnum , appix = row . split ( ' - ' ) appix = float ( appix . rstrip ( ' px' ) ) metadict [ 'lcapertures' ] [ apnum . strip ( ) ] = appix metadict [ 'columns' ] = [ ] for row in columninfo : colnum , colname , coldesc = row . split ( ' - ' ) metadict [ 'columns' ] . append ( colname ) return metadict
1694	def flat_map ( self , flatmap_function ) : from heronpy . streamlet . impl . flatmapbolt import FlatMapStreamlet fm_streamlet = FlatMapStreamlet ( flatmap_function , self ) self . _add_child ( fm_streamlet ) return fm_streamlet
7297	def get_attrs ( model_field , disabled = False ) : attrs = { } attrs [ 'class' ] = 'span6 xlarge' if disabled or isinstance ( model_field , ObjectIdField ) : attrs [ 'class' ] += ' disabled' attrs [ 'readonly' ] = 'readonly' return attrs
6426	def sim_sift4 ( src , tar , max_offset = 5 , max_distance = 0 ) : return Sift4 ( ) . sim ( src , tar , max_offset , max_distance )
8210	def contains_point ( self , x , y , d = 2 ) : if self . path != None and len ( self . path ) > 1 and self . path . contains ( x , y ) : if not self . path . contains ( x + d , y ) or not self . path . contains ( x , y + d ) or not self . path . contains ( x - d , y ) or not self . path . contains ( x , y - d ) or not self . path . contains ( x + d , y + d ) or not self . path . contains ( x - d , y - d ) or not self . path . contains ( x + d , y - d ) or not self . path . contains ( x - d , y + d ) : return True return False
7536	def derep_concat_split ( data , sample , nthreads , force ) : LOGGER . info ( "INSIDE derep %s" , sample . name ) mergefile = os . path . join ( data . dirs . edits , sample . name + "_merged_.fastq" ) if not force : if not os . path . exists ( mergefile ) : sample . files . edits = concat_multiple_edits ( data , sample ) else : LOGGER . info ( "skipped concat_multiple_edits: {} exists" . format ( mergefile ) ) else : sample . files . edits = concat_multiple_edits ( data , sample ) if 'pair' in data . paramsdict [ 'datatype' ] : if "reference" in data . paramsdict [ "assembly_method" ] : nmerged = merge_pairs ( data , sample . files . edits , mergefile , 0 , 0 ) else : nmerged = merge_pairs ( data , sample . files . edits , mergefile , 1 , 1 ) sample . files . edits = [ ( mergefile , ) ] sample . stats . reads_merged = nmerged if "3rad" in data . paramsdict [ "datatype" ] : declone_3rad ( data , sample ) derep_and_sort ( data , os . path . join ( data . dirs . edits , sample . name + "_declone.fastq" ) , os . path . join ( data . dirs . edits , sample . name + "_derep.fastq" ) , nthreads ) else : derep_and_sort ( data , sample . files . edits [ 0 ] [ 0 ] , os . path . join ( data . dirs . edits , sample . name + "_derep.fastq" ) , nthreads )
9694	def replace ( self , ** k ) : if self . date != 'infinity' : return Date ( self . date . replace ( ** k ) ) else : return Date ( 'infinity' )
11218	def encode ( self ) -> str : payload = { } payload . update ( self . registered_claims ) payload . update ( self . payload ) return encode ( self . secret , payload , self . alg , self . header )
7477	def count_seeds ( usort ) : with open ( usort , 'r' ) as insort : cmd1 = [ "cut" , "-f" , "2" ] cmd2 = [ "uniq" ] cmd3 = [ "wc" ] proc1 = sps . Popen ( cmd1 , stdin = insort , stdout = sps . PIPE , close_fds = True ) proc2 = sps . Popen ( cmd2 , stdin = proc1 . stdout , stdout = sps . PIPE , close_fds = True ) proc3 = sps . Popen ( cmd3 , stdin = proc2 . stdout , stdout = sps . PIPE , close_fds = True ) res = proc3 . communicate ( ) nseeds = int ( res [ 0 ] . split ( ) [ 0 ] ) proc1 . stdout . close ( ) proc2 . stdout . close ( ) proc3 . stdout . close ( ) return nseeds
7585	def dstat ( inarr , taxdict , mindict = 1 , nboots = 1000 , name = 0 ) : if isinstance ( inarr , list ) : arr , _ = _loci_to_arr ( inarr , taxdict , mindict ) if arr . shape [ 1 ] == 4 : res , boots = _get_signif_4 ( arr , nboots ) res = pd . DataFrame ( res , columns = [ name ] , index = [ "Dstat" , "bootmean" , "bootstd" , "Z" , "ABBA" , "BABA" , "nloci" ] ) else : res , boots = _get_signif_5 ( arr , nboots ) res = pd . DataFrame ( res , index = [ "p3" , "p4" , "shared" ] , columns = [ "Dstat" , "bootmean" , "bootstd" , "Z" , "ABxxA" , "BAxxA" , "nloci" ] ) return res . T , boots
9030	def _expand_consumed_mesh ( self , mesh , mesh_index , row_position , passed ) : if not mesh . is_produced ( ) : return row = mesh . producing_row position = Point ( row_position . x + mesh . index_in_producing_row - mesh_index , row_position . y - INSTRUCTION_HEIGHT ) self . _expand ( row , position , passed )
10831	def get ( cls , group , admin ) : try : ga = cls . query . filter_by ( group = group , admin_id = admin . get_id ( ) , admin_type = resolve_admin_type ( admin ) ) . one ( ) return ga except Exception : return None
6130	def get ( self , * args , ** kwargs ) : try : req_func = self . session . get if self . session else requests . get req = req_func ( * args , ** kwargs ) req . raise_for_status ( ) self . failed_last = False return req except requests . exceptions . RequestException as e : self . log_error ( e ) for i in range ( 1 , self . num_retries ) : sleep_time = self . retry_rate * i self . log_function ( "Retrying in %s seconds" % sleep_time ) self . _sleep ( sleep_time ) try : req = requests . get ( * args , ** kwargs ) req . raise_for_status ( ) self . log_function ( "New request successful" ) return req except requests . exceptions . RequestException : self . log_function ( "New request failed" ) if not self . failed_last : self . failed_last = True raise ApiError ( e ) else : raise FatalApiError ( e )
13478	def _sentence_to_interstitial_spacing ( self ) : not_sentence_end_chars = [ ' ' ] abbreviations = [ 'i.e.' , 'e.g.' , ' v.' , ' w.' , ' wh.' ] titles = [ 'Prof.' , 'Mr.' , 'Mrs.' , 'Messrs.' , 'Mmes.' , 'Msgr.' , 'Ms.' , 'Fr.' , 'Rev.' , 'St.' , 'Dr.' , 'Lieut.' , 'Lt.' , 'Capt.' , 'Cptn.' , 'Sgt.' , 'Sjt.' , 'Gen.' , 'Hon.' , 'Cpl.' , 'L-Cpl.' , 'Pvt.' , 'Dvr.' , 'Gnr.' , 'Spr.' , 'Col.' , 'Lt-Col' , 'Lt-Gen.' , 'Mx.' ] for abbrev in abbreviations : for x in not_sentence_end_chars : self . _str_replacement ( abbrev + x , abbrev + '\ ' ) for title in titles : for x in not_sentence_end_chars : self . _str_replacement ( title + x , title + '~' )
2347	def phrase_to_filename ( self , phrase ) : name = re . sub ( r"[^\w\s\.]" , '' , phrase . strip ( ) . lower ( ) ) name = re . sub ( r"\s+" , '_' , name ) return name + '.png'
4388	def adsPortCloseEx ( port ) : port_close_ex = _adsDLL . AdsPortCloseEx port_close_ex . restype = ctypes . c_long error_code = port_close_ex ( port ) if error_code : raise ADSError ( error_code )
14	def obs_space_info ( obs_space ) : if isinstance ( obs_space , gym . spaces . Dict ) : assert isinstance ( obs_space . spaces , OrderedDict ) subspaces = obs_space . spaces else : subspaces = { None : obs_space } keys = [ ] shapes = { } dtypes = { } for key , box in subspaces . items ( ) : keys . append ( key ) shapes [ key ] = box . shape dtypes [ key ] = box . dtype return keys , shapes , dtypes
2877	def one ( nodes , or_none = False ) : if not nodes and or_none : return None assert len ( nodes ) == 1 , 'Expected 1 result. Received %d results.' % ( len ( nodes ) ) return nodes [ 0 ]
10784	def should_particle_exist ( absent_err , present_err , absent_d , present_d , im_change_frac = 0.2 , min_derr = 0.1 ) : delta_im = np . ravel ( present_d - absent_d ) im_change = np . dot ( delta_im , delta_im ) err_cutoff = max ( [ im_change_frac * im_change , min_derr ] ) return ( absent_err - present_err ) >= err_cutoff
12243	def dixon_price ( theta ) : x , y = theta obj = ( x - 1 ) ** 2 + 2 * ( 2 * y ** 2 - x ) ** 2 grad = np . array ( [ 2 * x - 2 - 4 * ( 2 * y ** 2 - x ) , 16 * ( 2 * y ** 2 - x ) * y , ] ) return obj , grad
13144	def pack_triples_numpy ( triples ) : if len ( triples ) == 0 : return np . array ( [ ] , dtype = np . int64 ) return np . stack ( list ( map ( _transform_triple_numpy , triples ) ) , axis = 0 )
441	def count_params ( self ) : n_params = 0 for _i , p in enumerate ( self . all_params ) : n = 1 for s in p . get_shape ( ) : try : s = int ( s ) except Exception : s = 1 if s : n = n * s n_params = n_params + n return n_params
3014	def _to_json ( self , strip , to_serialize = None ) : if to_serialize is None : to_serialize = copy . copy ( self . __dict__ ) pkcs12_val = to_serialize . get ( _PKCS12_KEY ) if pkcs12_val is not None : to_serialize [ _PKCS12_KEY ] = base64 . b64encode ( pkcs12_val ) return super ( ServiceAccountCredentials , self ) . _to_json ( strip , to_serialize = to_serialize )
3071	def _get_flow_for_token ( csrf_token ) : flow_pickle = session . pop ( _FLOW_KEY . format ( csrf_token ) , None ) if flow_pickle is None : return None else : return pickle . loads ( flow_pickle )
7127	def find_and_patch_entry ( soup , entry ) : link = soup . find ( "a" , { "class" : "headerlink" } , href = "#" + entry . anchor ) tag = soup . new_tag ( "a" ) tag [ "name" ] = APPLE_REF_TEMPLATE . format ( entry . type , entry . name ) if link : link . parent . insert ( 0 , tag ) return True elif entry . anchor . startswith ( "module-" ) : soup . h1 . parent . insert ( 0 , tag ) return True else : return False
4782	def is_close_to ( self , other , tolerance ) : self . _validate_close_to_args ( self . val , other , tolerance ) if self . val < ( other - tolerance ) or self . val > ( other + tolerance ) : if type ( self . val ) is datetime . datetime : tolerance_seconds = tolerance . days * 86400 + tolerance . seconds + tolerance . microseconds / 1000000 h , rem = divmod ( tolerance_seconds , 3600 ) m , s = divmod ( rem , 60 ) self . _err ( 'Expected <%s> to be close to <%s> within tolerance <%d:%02d:%02d>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) , h , m , s ) ) else : self . _err ( 'Expected <%s> to be close to <%s> within tolerance <%s>, but was not.' % ( self . val , other , tolerance ) ) return self
5696	def copy ( cls , conn , ** where ) : cur = conn . cursor ( ) if where and cls . copy_where : copy_where = cls . copy_where . format ( ** where ) else : copy_where = '' cur . execute ( 'INSERT INTO %s ' 'SELECT * FROM source.%s %s' % ( cls . table , cls . table , copy_where ) )
66	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , alpha = 1.0 , size = 1 , copy = True , raise_if_out_of_image = False , thickness = None ) : if thickness is not None : ia . warn_deprecated ( "Usage of argument 'thickness' in BoundingBox.draw_on_image() " "is deprecated. The argument was renamed to 'size'." ) size = thickness if raise_if_out_of_image and self . is_out_of_image ( image ) : raise Exception ( "Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s." % ( self . x1 , self . y1 , self . x2 , self . y2 , image . shape ) ) result = np . copy ( image ) if copy else image if isinstance ( color , ( tuple , list ) ) : color = np . uint8 ( color ) for i in range ( size ) : y1 , y2 , x1 , x2 = self . y1_int , self . y2_int , self . x1_int , self . x2_int if self . is_fully_within_image ( image ) : y1 = np . clip ( y1 , 0 , image . shape [ 0 ] - 1 ) y2 = np . clip ( y2 , 0 , image . shape [ 0 ] - 1 ) x1 = np . clip ( x1 , 0 , image . shape [ 1 ] - 1 ) x2 = np . clip ( x2 , 0 , image . shape [ 1 ] - 1 ) y = [ y1 - i , y1 - i , y2 + i , y2 + i ] x = [ x1 - i , x2 + i , x2 + i , x1 - i ] rr , cc = skimage . draw . polygon_perimeter ( y , x , shape = result . shape ) if alpha >= 0.99 : result [ rr , cc , : ] = color else : if ia . is_float_array ( result ) : result [ rr , cc , : ] = ( 1 - alpha ) * result [ rr , cc , : ] + alpha * color result = np . clip ( result , 0 , 255 ) else : input_dtype = result . dtype result = result . astype ( np . float32 ) result [ rr , cc , : ] = ( 1 - alpha ) * result [ rr , cc , : ] + alpha * color result = np . clip ( result , 0 , 255 ) . astype ( input_dtype ) return result
2082	def lookup_stdout ( self , pk = None , start_line = None , end_line = None , full = True ) : uj_res = get_resource ( 'unified_job' ) query_params = ( ( 'unified_job_node__workflow_job' , pk ) , ( 'order_by' , 'finished' ) , ( 'status__in' , 'successful,failed,error' ) ) jobs_list = uj_res . list ( all_pages = True , query = query_params ) if jobs_list [ 'count' ] == 0 : return '' return_content = ResSubcommand ( uj_res ) . _format_human ( jobs_list ) lines = return_content . split ( '\n' ) if not full : lines = lines [ : - 1 ] N = len ( lines ) start_range = start_line if start_line is None : start_range = 0 elif start_line > N : start_range = N end_range = end_line if end_line is None or end_line > N : end_range = N lines = lines [ start_range : end_range ] return_content = '\n' . join ( lines ) if len ( lines ) > 0 : return_content += '\n' return return_content
11167	def keys ( self ) : return self . options . keys ( ) + [ p . name for p in self . positional_args ]
11849	def percept ( self , agent ) : "By default, agent perceives things within a default radius." return [ self . thing_percept ( thing , agent ) for thing in self . things_near ( agent . location ) ]
7643	def convert ( annotation , target_namespace ) : annotation . validate ( strict = True ) if annotation . namespace == target_namespace : return annotation if target_namespace in __CONVERSION__ : annotation = deepcopy ( annotation ) for source in __CONVERSION__ [ target_namespace ] : if annotation . search ( namespace = source ) : return __CONVERSION__ [ target_namespace ] [ source ] ( annotation ) raise NamespaceError ( 'Unable to convert annotation from namespace=' '"{0}" to "{1}"' . format ( annotation . namespace , target_namespace ) )
7329	async def request ( self , method , url , future , headers = None , session = None , encoding = None , ** kwargs ) : await self . setup req_kwargs = await self . headers . prepare_request ( method = method , url = url , headers = headers , proxy = self . proxy , ** kwargs ) if encoding is None : encoding = self . encoding session = session if ( session is not None ) else self . _session logger . debug ( "making request with parameters: %s" % req_kwargs ) async with session . request ( ** req_kwargs ) as response : if response . status < 400 : data = await data_processing . read ( response , self . _loads , encoding = encoding ) future . set_result ( data_processing . PeonyResponse ( data = data , headers = response . headers , url = response . url , request = req_kwargs ) ) else : await exceptions . throw ( response , loads = self . _loads , encoding = encoding , url = url )
1755	def read_register ( self , register ) : self . _publish ( 'will_read_register' , register ) value = self . _regfile . read ( register ) self . _publish ( 'did_read_register' , register , value ) return value
11149	def rename ( self , key : Any , new_key : Any ) : if new_key == key : return required_locks = [ self . _key_locks [ key ] , self . _key_locks [ new_key ] ] ordered_required_locks = sorted ( required_locks , key = lambda x : id ( x ) ) for lock in ordered_required_locks : lock . acquire ( ) try : if key not in self . _data : raise KeyError ( "Attribute to rename \"%s\" does not exist" % key ) self . _data [ new_key ] = self [ key ] del self . _data [ key ] finally : for lock in required_locks : lock . release ( )
10556	def update_helping_material ( helpingmaterial ) : try : helpingmaterial_id = helpingmaterial . id helpingmaterial = _forbidden_attributes ( helpingmaterial ) res = _pybossa_req ( 'put' , 'helpingmaterial' , helpingmaterial_id , payload = helpingmaterial . data ) if res . get ( 'id' ) : return HelpingMaterial ( res ) else : return res except : raise
13501	def update_time ( sender , ** kwargs ) : comment = kwargs [ 'instance' ] if comment . content_type . app_label == "happenings" and comment . content_type . name == "Update" : from . models import Update item = Update . objects . get ( id = comment . object_pk ) item . save ( )
7590	def fields_checker ( fields ) : if isinstance ( fields , int ) : fields = str ( fields ) if isinstance ( fields , str ) : if "," in fields : fields = [ str ( i ) for i in fields . split ( "," ) ] else : fields = [ str ( fields ) ] elif isinstance ( fields , ( tuple , list ) ) : fields = [ str ( i ) for i in fields ] else : raise IPyradWarningExit ( "fields not properly formatted" ) fields = [ i for i in fields if i != '0' ] return fields
8096	def edge_label ( s , edge , alpha = 1.0 ) : if s . text and edge . label != "" : s . _ctx . nostroke ( ) s . _ctx . fill ( s . text . r , s . text . g , s . text . b , s . text . a * alpha * 0.75 ) s . _ctx . lineheight ( 1 ) s . _ctx . font ( s . font ) s . _ctx . fontsize ( s . fontsize * 0.75 ) try : p = edge . _textpath except : try : txt = unicode ( edge . label ) except : try : txt = edge . label . decode ( "utf-8" ) except : pass edge . _textpath = s . _ctx . textpath ( txt , s . _ctx . textwidth ( " " ) , 0 , width = s . textwidth ) p = edge . _textpath a = degrees ( atan2 ( edge . node2 . y - edge . node1 . y , edge . node2 . x - edge . node1 . x ) ) d = sqrt ( ( edge . node2 . x - edge . node1 . x ) ** 2 + ( edge . node2 . y - edge . node1 . y ) ** 2 ) d = abs ( d - s . _ctx . textwidth ( edge . label ) ) * 0.5 s . _ctx . push ( ) s . _ctx . transform ( CORNER ) s . _ctx . translate ( edge . node1 . x , edge . node1 . y ) s . _ctx . rotate ( - a ) s . _ctx . translate ( d , s . fontsize * 1.0 ) s . _ctx . scale ( alpha ) if 90 < a % 360 < 270 : s . _ctx . translate ( s . _ctx . textwidth ( edge . label ) , - s . fontsize * 2.0 ) s . _ctx . transform ( CENTER ) s . _ctx . rotate ( 180 ) s . _ctx . transform ( CORNER ) s . _ctx . drawpath ( p . copy ( ) ) s . _ctx . pop ( )
3091	def locked_delete ( self ) : if self . _cache : self . _cache . delete ( self . _key_name ) self . _delete_entity ( )
12195	def get_app_locations ( ) : return [ os . path . dirname ( os . path . normpath ( import_module ( app_name ) . __file__ ) ) for app_name in PROJECT_APPS ]
13688	def _generate_html_diff ( self , expected_fn , expected_lines , obtained_fn , obtained_lines ) : import difflib differ = difflib . HtmlDiff ( ) return differ . make_file ( fromlines = expected_lines , fromdesc = expected_fn , tolines = obtained_lines , todesc = obtained_fn , )
10348	def export_namespace ( graph , namespace , directory = None , cacheable = False ) : directory = os . getcwd ( ) if directory is None else directory path = os . path . join ( directory , '{}.belns' . format ( namespace ) ) with open ( path , 'w' ) as file : log . info ( 'Outputting to %s' , path ) right_names = get_names_by_namespace ( graph , namespace ) log . info ( 'Graph has %d correct names in %s' , len ( right_names ) , namespace ) wrong_names = get_incorrect_names_by_namespace ( graph , namespace ) log . info ( 'Graph has %d incorrect names in %s' , len ( right_names ) , namespace ) undefined_ns_names = get_undefined_namespace_names ( graph , namespace ) log . info ( 'Graph has %d names in missing namespace %s' , len ( right_names ) , namespace ) names = ( right_names | wrong_names | undefined_ns_names ) if 0 == len ( names ) : log . warning ( '%s is empty' , namespace ) write_namespace ( namespace_name = namespace , namespace_keyword = namespace , namespace_domain = 'Other' , author_name = graph . authors , author_contact = graph . contact , citation_name = graph . name , values = names , cacheable = cacheable , file = file )
3302	def _get_checked_path ( path , config , must_exist = True , allow_none = True ) : if path in ( None , "" ) : if allow_none : return None raise ValueError ( "Invalid path {!r}" . format ( path ) ) config_file = config . get ( "_config_file" ) if config_file and not os . path . isabs ( path ) : path = os . path . normpath ( os . path . join ( os . path . dirname ( config_file ) , path ) ) else : path = os . path . abspath ( path ) if must_exist and not os . path . exists ( path ) : raise ValueError ( "Invalid path {!r}" . format ( path ) ) return path
1696	def clone ( self , num_clones ) : retval = [ ] for i in range ( num_clones ) : retval . append ( self . repartition ( self . get_num_partitions ( ) ) ) return retval
9448	def hangup_call ( self , call_params ) : path = '/' + self . api_version + '/HangupCall/' method = 'POST' return self . request ( path , method , call_params )
13157	def count ( cls , cur , table : str , where_keys : list = None ) : if where_keys : where_clause , values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _count_query_where . format ( table , where_clause ) q , t = query , values else : query = cls . _count_query . format ( table ) q , t = query , ( ) yield from cur . execute ( q , t ) result = yield from cur . fetchone ( ) return int ( result [ 0 ] )
3692	def a_alpha_and_derivatives ( self , T , full = True , quick = True ) : r return TWU_a_alpha_common ( T , self . Tc , self . omega , self . a , full = full , quick = quick , method = 'SRK' )
12004	def _read_header ( self , data ) : version = self . _read_version ( data ) version_info = self . _get_version_info ( version ) header_data = data [ : version_info [ 'header_size' ] ] header = version_info [ 'header' ] header = header . _make ( unpack ( version_info [ 'header_format' ] , header_data ) ) header = dict ( header . _asdict ( ) ) flags = list ( "{0:0>8b}" . format ( header [ 'flags' ] ) ) flags = dict ( version_info [ 'flags' ] . _make ( flags ) . _asdict ( ) ) flags = dict ( ( i , bool ( int ( j ) ) ) for i , j in flags . iteritems ( ) ) header [ 'flags' ] = flags timestamp = None if flags [ 'timestamp' ] : ts_start = version_info [ 'header_size' ] ts_end = ts_start + version_info [ 'timestamp_size' ] timestamp_data = data [ ts_start : ts_end ] timestamp = unpack ( version_info [ 'timestamp_format' ] , timestamp_data ) [ 0 ] header [ 'info' ] = { 'timestamp' : timestamp } return header
8390	def validate ( self ) : with open ( self . filename , "rb" ) as f : text = f . read ( ) start_last_line = text . rfind ( b"\n" , 0 , - 1 ) if start_last_line == - 1 : return False original_text = text [ : start_last_line + 1 ] last_line = text [ start_last_line + 1 : ] expected_hash = hashlib . sha1 ( original_text ) . hexdigest ( ) . encode ( 'utf8' ) match = re . search ( b"[0-9a-f]{40}" , last_line ) if not match : return False actual_hash = match . group ( 0 ) return actual_hash == expected_hash
6184	def get_git_version ( git_path = None ) : if git_path is None : git_path = GIT_PATH git_version = check_output ( [ git_path , "--version" ] ) . split ( ) [ 2 ] return git_version
7472	def get_nloci ( data ) : bseeds = os . path . join ( data . dirs . across , data . name + ".tmparrs.h5" ) with h5py . File ( bseeds ) as io5 : return io5 [ "seedsarr" ] . shape [ 0 ]
4411	def store ( self , key : object , value : object ) : self . _user_data . update ( { key : value } )
1833	def JCXZ ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CX == 0 , target . read ( ) , cpu . PC )
2950	def _start ( self , my_task , force = False ) : if my_task . _has_state ( Task . COMPLETED ) : return True , None if my_task . _has_state ( Task . READY ) : return True , None if self . split_task is None : return self . _check_threshold_unstructured ( my_task , force ) return self . _check_threshold_structured ( my_task , force )
6181	def load_PSFLab_file ( fname ) : if os . path . exists ( fname ) or os . path . exists ( fname + '.mat' ) : return loadmat ( fname ) [ 'data' ] else : raise IOError ( "Can't find PSF file '%s'" % fname )
1412	def _get_topologies_with_watch ( self , callback , isWatching ) : path = self . get_topologies_path ( ) if isWatching : LOG . info ( "Adding children watch for path: " + path ) @ self . client . ChildrenWatch ( path ) def watch_topologies ( topologies ) : callback ( topologies ) return isWatching
2303	def create_graph_from_data ( self , data ) : self . arguments [ '{SCORE}' ] = self . scores [ self . score ] self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) results = self . _run_gies ( data , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
2344	def create_graph_from_data ( self , data , ** kwargs ) : self . arguments [ '{SCORE}' ] = self . scores [ self . score ] self . arguments [ '{CUTOFF}' ] = str ( self . cutoff ) self . arguments [ '{VARSEL}' ] = str ( self . variablesel ) . upper ( ) self . arguments [ '{SELMETHOD}' ] = self . var_selection [ self . selmethod ] self . arguments [ '{PRUNING}' ] = str ( self . pruning ) . upper ( ) self . arguments [ '{PRUNMETHOD}' ] = self . var_selection [ self . prunmethod ] self . arguments [ '{NJOBS}' ] = str ( self . nb_jobs ) self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) results = self . _run_cam ( data , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
2285	def predict ( self , df_data , graph = None , ** kwargs ) : if graph is None : return self . create_graph_from_data ( df_data , ** kwargs ) elif isinstance ( graph , nx . DiGraph ) : return self . orient_directed_graph ( df_data , graph , ** kwargs ) elif isinstance ( graph , nx . Graph ) : return self . orient_undirected_graph ( df_data , graph , ** kwargs ) else : print ( 'Unknown Graph type' ) raise ValueError
2278	def parse ( config ) : if not isinstance ( config , basestring ) : raise TypeError ( "Contains input must be a simple string" ) validator = ContainsValidator ( ) validator . contains_string = config return validator
8861	def defined_names ( request_data ) : global _old_definitions ret_val = [ ] path = request_data [ 'path' ] toplvl_definitions = jedi . names ( request_data [ 'code' ] , path , 'utf-8' ) for d in toplvl_definitions : definition = _extract_def ( d , path ) if d . type != 'import' : ret_val . append ( definition ) ret_val = [ d . to_dict ( ) for d in ret_val ] return ret_val
13819	def _ConvertValueMessage ( value , message ) : if isinstance ( value , dict ) : _ConvertStructMessage ( value , message . struct_value ) elif isinstance ( value , list ) : _ConvertListValueMessage ( value , message . list_value ) elif value is None : message . null_value = 0 elif isinstance ( value , bool ) : message . bool_value = value elif isinstance ( value , six . string_types ) : message . string_value = value elif isinstance ( value , _INT_OR_FLOAT ) : message . number_value = value else : raise ParseError ( 'Unexpected type for Value message.' )
4788	def is_alpha ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if not self . val . isalpha ( ) : self . _err ( 'Expected <%s> to contain only alphabetic chars, but did not.' % self . val ) return self
3167	def pause ( self , campaign_id ) : self . campaign_id = campaign_id return self . _mc_client . _post ( url = self . _build_path ( campaign_id , 'actions/pause' ) )
12316	def init ( self , username , reponame , force , backend = None ) : key = self . key ( username , reponame ) server_repodir = self . server_rootdir ( username , reponame , create = False ) if os . path . exists ( server_repodir ) and not force : raise RepositoryExists ( ) if os . path . exists ( server_repodir ) : shutil . rmtree ( server_repodir ) os . makedirs ( server_repodir ) with cd ( server_repodir ) : git . init ( "." , "--bare" ) if backend is not None : backend . init_repo ( server_repodir ) repodir = self . rootdir ( username , reponame , create = False ) if os . path . exists ( repodir ) and not force : raise Exception ( "Local repo already exists" ) if os . path . exists ( repodir ) : shutil . rmtree ( repodir ) os . makedirs ( repodir ) with cd ( os . path . dirname ( repodir ) ) : git . clone ( server_repodir , '--no-hardlinks' ) url = server_repodir if backend is not None : url = backend . url ( username , reponame ) repo = Repo ( username , reponame ) repo . manager = self repo . remoteurl = url repo . rootdir = self . rootdir ( username , reponame ) self . add ( repo ) return repo
8290	def _description ( self ) : meta = self . find ( "meta" , { "name" : "description" } ) if isinstance ( meta , dict ) and meta . has_key ( "content" ) : return meta [ "content" ] else : return u""
6723	def get_or_create_ec2_key_pair ( name = None , verbose = 1 ) : verbose = int ( verbose ) name = name or env . vm_ec2_keypair_name pem_path = 'roles/%s/%s.pem' % ( env . ROLE , name ) conn = get_ec2_connection ( ) kp = conn . get_key_pair ( name ) if kp : print ( 'Key pair %s already exists.' % name ) else : kp = conn . create_key_pair ( name ) open ( pem_path , 'wb' ) . write ( kp . material ) os . system ( 'chmod 600 %s' % pem_path ) print ( 'Key pair %s created.' % name ) return pem_path
4684	def getPublicKeys ( self , current = False ) : pubkeys = self . store . getPublicKeys ( ) if not current : return pubkeys pubs = [ ] for pubkey in pubkeys : if pubkey [ : len ( self . prefix ) ] == self . prefix : pubs . append ( pubkey ) return pubs
849	def getOutputElementCount ( self , name ) : if name == "resetOut" : print ( "WARNING: getOutputElementCount should not have been called with " "resetOut" ) return 1 elif name == "sequenceIdOut" : print ( "WARNING: getOutputElementCount should not have been called with " "sequenceIdOut" ) return 1 elif name == "dataOut" : if self . encoder is None : raise Exception ( "NuPIC requested output element count for 'dataOut' " "on a RecordSensor node, but the encoder has not " "been set" ) return self . encoder . getWidth ( ) elif name == "sourceOut" : if self . encoder is None : raise Exception ( "NuPIC requested output element count for 'sourceOut' " "on a RecordSensor node, " "but the encoder has not been set" ) return len ( self . encoder . getDescription ( ) ) elif name == "bucketIdxOut" : return 1 elif name == "actValueOut" : return 1 elif name == "categoryOut" : return self . numCategories elif name == 'spatialTopDownOut' or name == 'temporalTopDownOut' : if self . encoder is None : raise Exception ( "NuPIC requested output element count for 'sourceOut' " "on a RecordSensor node, " "but the encoder has not been set" ) return len ( self . encoder . getDescription ( ) ) else : raise Exception ( "Unknown output %s" % name )
11942	def stored_messages_archive ( context , num_elements = 10 ) : if "user" in context : user = context [ "user" ] if user . is_authenticated ( ) : qs = MessageArchive . objects . select_related ( "message" ) . filter ( user = user ) return { "messages" : qs [ : num_elements ] , "count" : qs . count ( ) , }
10988	def _translate_particles ( s , max_mem = 1e9 , desc = '' , min_rad = 'calc' , max_rad = 'calc' , invert = 'guess' , rz_order = 0 , do_polish = True ) : if desc is not None : desc_trans = desc + 'translate-particles' desc_burn = desc + 'addsub_burn' desc_polish = desc + 'addsub_polish' else : desc_trans , desc_burn , desc_polish = [ None ] * 3 RLOG . info ( 'Translate Particles:' ) opt . burn ( s , mode = 'do-particles' , n_loop = 4 , fractol = 0.1 , desc = desc_trans , max_mem = max_mem , include_rad = False , dowarn = False ) opt . burn ( s , mode = 'do-particles' , n_loop = 4 , fractol = 0.05 , desc = desc_trans , max_mem = max_mem , include_rad = True , dowarn = False ) RLOG . info ( 'Start add-subtract' ) addsub . add_subtract ( s , tries = 30 , min_rad = min_rad , max_rad = max_rad , invert = invert ) if desc is not None : states . save ( s , desc = desc + 'translate-addsub' ) if do_polish : RLOG . info ( 'Final Burn:' ) opt . burn ( s , mode = 'burn' , n_loop = 3 , fractol = 3e-4 , desc = desc_burn , max_mem = max_mem , rz_order = rz_order , dowarn = False ) RLOG . info ( 'Final Polish:' ) d = opt . burn ( s , mode = 'polish' , n_loop = 4 , fractol = 3e-4 , desc = desc_polish , max_mem = max_mem , rz_order = rz_order , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' )
1632	def CheckForBadCharacters ( filename , lines , error ) : for linenum , line in enumerate ( lines ) : if unicode_escape_decode ( '\ufffd' ) in line : error ( filename , linenum , 'readability/utf8' , 5 , 'Line contains invalid UTF-8 (or Unicode replacement character).' ) if '\0' in line : error ( filename , linenum , 'readability/nul' , 5 , 'Line contains NUL byte.' )
13293	def ensure_pandoc ( func ) : logger = logging . getLogger ( __name__ ) @ functools . wraps ( func ) def _install_and_run ( * args , ** kwargs ) : try : result = func ( * args , ** kwargs ) except OSError : message = "pandoc needed but not found. Now installing it for you." logger . warning ( message ) pypandoc . download_pandoc ( version = '1.19.1' ) logger . debug ( "pandoc download complete" ) result = func ( * args , ** kwargs ) return result return _install_and_run
1962	def sys_rt_sigaction ( self , signum , act , oldact ) : return self . sys_sigaction ( signum , act , oldact )
3464	def reverse_id ( self ) : return '_' . join ( ( self . id , 'reverse' , hashlib . md5 ( self . id . encode ( 'utf-8' ) ) . hexdigest ( ) [ 0 : 5 ] ) )
5669	def combined_stop_to_stop_transit_network ( gtfs , start_time_ut = None , end_time_ut = None ) : multi_di_graph = networkx . MultiDiGraph ( ) for route_type in route_types . TRANSIT_ROUTE_TYPES : graph = stop_to_stop_network_for_route_type ( gtfs , route_type , start_time_ut = start_time_ut , end_time_ut = end_time_ut ) for from_node , to_node , data in graph . edges ( data = True ) : data [ 'route_type' ] = route_type multi_di_graph . add_edges_from ( graph . edges ( data = True ) ) multi_di_graph . add_nodes_from ( graph . nodes ( data = True ) ) return multi_di_graph
2673	def invoke ( src , event_file = 'event.json' , config_file = 'config.yaml' , profile_name = None , verbose = False , ) : path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) if profile_name : os . environ [ 'AWS_PROFILE' ] = profile_name env_vars = cfg . get ( 'environment_variables' ) if env_vars : for key , value in env_vars . items ( ) : os . environ [ key ] = get_environment_variable_value ( value ) path_to_event_file = os . path . join ( src , event_file ) event = read ( path_to_event_file , loader = json . loads ) try : sys . path . index ( src ) except ValueError : sys . path . append ( src ) handler = cfg . get ( 'handler' ) fn = get_callable_handler_function ( src , handler ) timeout = cfg . get ( 'timeout' ) if timeout : context = LambdaContext ( cfg . get ( 'function_name' ) , timeout ) else : context = LambdaContext ( cfg . get ( 'function_name' ) ) start = time . time ( ) results = fn ( event , context ) end = time . time ( ) print ( '{0}' . format ( results ) ) if verbose : print ( '\nexecution time: {:.8f}s\nfunction execution ' 'timeout: {:2}s' . format ( end - start , cfg . get ( 'timeout' , 15 ) ) )
7399	def swap ( self , qs ) : try : replacement = qs [ 0 ] except IndexError : return if not self . _valid_ordering_reference ( replacement ) : raise ValueError ( "%r can only be swapped with instances of %r which %s equals %r." % ( self , self . __class__ , self . order_with_respect_to , self . _get_order_with_respect_to ( ) ) ) self . order , replacement . order = replacement . order , self . order self . save ( ) replacement . save ( )
9313	def sign_sha256 ( key , msg ) : if isinstance ( msg , text_type ) : msg = msg . encode ( 'utf-8' ) return hmac . new ( key , msg , hashlib . sha256 ) . digest ( )
6583	def _post_start ( self ) : flags = fcntl . fcntl ( self . _process . stdout , fcntl . F_GETFL ) fcntl . fcntl ( self . _process . stdout , fcntl . F_SETFL , flags | os . O_NONBLOCK )
9820	def project ( ctx , project ) : if ctx . invoked_subcommand not in [ 'create' , 'list' ] : ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project
596	def _buildArgs ( f , self = None , kwargs = { } ) : argTuples = getArgumentDescriptions ( f ) argTuples = argTuples [ 1 : ] init = TMRegion . __init__ ourArgNames = [ t [ 0 ] for t in getArgumentDescriptions ( init ) ] ourArgNames += [ 'numberOfCols' , ] for argTuple in argTuples [ : ] : if argTuple [ 0 ] in ourArgNames : argTuples . remove ( argTuple ) if self : for argTuple in argTuples : argName = argTuple [ 0 ] if argName in kwargs : argValue = kwargs . pop ( argName ) else : if len ( argTuple ) == 2 : raise TypeError ( "Must provide '%s'" % argName ) argValue = argTuple [ 2 ] setattr ( self , argName , argValue ) return argTuples
9226	def post_parse ( self ) : if self . result : out = [ ] for pu in self . result : try : out . append ( pu . parse ( self . scope ) ) except SyntaxError as e : self . handle_error ( e , 0 ) self . result = list ( utility . flatten ( out ) )
6673	def is_link ( self , path , use_sudo = False ) : func = use_sudo and _sudo or _run with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : return func ( '[ -L "%(path)s" ]' % locals ( ) ) . succeeded
7713	def handle_roster_push ( self , stanza ) : if self . server is None and stanza . from_jid : logger . debug ( u"Server address not known, cannot verify roster push" " from {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) if self . server and stanza . from_jid and stanza . from_jid != self . server : logger . debug ( u"Roster push from invalid source: {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) payload = stanza . get_payload ( RosterPayload ) if len ( payload ) != 1 : logger . warning ( "Bad roster push received ({0} items)" . format ( len ( payload ) ) ) return stanza . make_error_response ( u"bad-request" ) if self . roster is None : logger . debug ( "Dropping roster push - no roster here" ) return True item = payload [ 0 ] item . verify_roster_push ( True ) old_item = self . roster . get ( item . jid ) if item . subscription == "remove" : if old_item : self . roster . remove_item ( item . jid ) else : self . roster . add_item ( item , replace = True ) self . _event_queue . put ( RosterUpdatedEvent ( self , old_item , item ) ) return stanza . make_result_response ( )
274	def get_symbol_rets ( symbol , start = None , end = None ) : return SETTINGS [ 'returns_func' ] ( symbol , start = start , end = end )
416	def find_top_dataset ( self , dataset_name = None , sort = None , ** kwargs ) : self . _fill_project_info ( kwargs ) if dataset_name is None : raise Exception ( "dataset_name is None, please give a dataset name" ) kwargs . update ( { 'dataset_name' : dataset_name } ) s = time . time ( ) d = self . db . Dataset . find_one ( filter = kwargs , sort = sort ) if d is not None : dataset_id = d [ 'dataset_id' ] else : print ( "[Database] FAIL! Cannot find dataset: {}" . format ( kwargs ) ) return False try : dataset = self . _deserialization ( self . dataset_fs . get ( dataset_id ) . read ( ) ) pc = self . db . Dataset . find ( kwargs ) print ( "[Database] Find one dataset SUCCESS, {} took: {}s" . format ( kwargs , round ( time . time ( ) - s , 2 ) ) ) dataset_id_list = pc . distinct ( 'dataset_id' ) n_dataset = len ( dataset_id_list ) if n_dataset != 1 : print ( " Note that there are {} datasets match the requirement" . format ( n_dataset ) ) return dataset except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) return False
514	def _avgColumnsPerInput ( self ) : numDim = max ( self . _columnDimensions . size , self . _inputDimensions . size ) colDim = numpy . ones ( numDim ) colDim [ : self . _columnDimensions . size ] = self . _columnDimensions inputDim = numpy . ones ( numDim ) inputDim [ : self . _inputDimensions . size ] = self . _inputDimensions columnsPerInput = colDim . astype ( realDType ) / inputDim return numpy . average ( columnsPerInput )
2208	def ensuredir ( dpath , mode = 0o1777 , verbose = None ) : r if verbose is None : verbose = 0 if isinstance ( dpath , ( list , tuple ) ) : dpath = join ( * dpath ) if not exists ( dpath ) : if verbose : print ( 'Ensuring new directory (%r)' % dpath ) if sys . version_info . major == 2 : os . makedirs ( normpath ( dpath ) , mode = mode ) else : os . makedirs ( normpath ( dpath ) , mode = mode , exist_ok = True ) else : if verbose : print ( 'Ensuring existing directory (%r)' % dpath ) return dpath
1570	def submit_fatjar ( cl_args , unknown_args , tmp_dir ) : topology_file = cl_args [ 'topology-file-name' ] main_class = cl_args [ 'topology-class-name' ] res = execute . heron_class ( class_name = main_class , lib_jars = config . get_heron_libs ( jars . topology_jars ( ) ) , extra_jars = [ topology_file ] , args = tuple ( unknown_args ) , java_defines = cl_args [ 'topology_main_jvm_property' ] ) result . render ( res ) if not result . is_successful ( res ) : err_context = ( "Failed to create topology definition " "file when executing class '%s' of file '%s'" ) % ( main_class , topology_file ) res . add_context ( err_context ) return res results = launch_topologies ( cl_args , topology_file , tmp_dir ) return results
6742	def render_to_string ( template , extra = None ) : from jinja2 import Template extra = extra or { } final_fqfn = find_template ( template ) assert final_fqfn , 'Template not found: %s' % template template_content = open ( final_fqfn , 'r' ) . read ( ) t = Template ( template_content ) if extra : context = env . copy ( ) context . update ( extra ) else : context = env rendered_content = t . render ( ** context ) rendered_content = rendered_content . replace ( '&quot;' , '"' ) return rendered_content
9476	def add_edge ( self , n1_label , n2_label , directed = False ) : n1 = self . add_node ( n1_label ) n2 = self . add_node ( n2_label ) e = Edge ( n1 , n2 , directed ) self . _edges . append ( e ) return e
9519	def interleave ( infile_1 , infile_2 , outfile , suffix1 = None , suffix2 = None ) : seq_reader_1 = sequences . file_reader ( infile_1 ) seq_reader_2 = sequences . file_reader ( infile_2 ) f_out = utils . open_file_write ( outfile ) for seq_1 in seq_reader_1 : try : seq_2 = next ( seq_reader_2 ) except : utils . close ( f_out ) raise Error ( 'Error getting mate for sequence' , seq_1 . id , ' ... cannot continue' ) if suffix1 is not None and not seq_1 . id . endswith ( suffix1 ) : seq_1 . id += suffix1 if suffix2 is not None and not seq_2 . id . endswith ( suffix2 ) : seq_2 . id += suffix2 print ( seq_1 , file = f_out ) print ( seq_2 , file = f_out ) try : seq_2 = next ( seq_reader_2 ) except : seq_2 = None if seq_2 is not None : utils . close ( f_out ) raise Error ( 'Error getting mate for sequence' , seq_2 . id , ' ... cannot continue' ) utils . close ( f_out )
1442	def next_tuple ( self , latency_in_ns ) : self . update_reduced_metric ( self . NEXT_TUPLE_LATENCY , latency_in_ns ) self . update_count ( self . NEXT_TUPLE_COUNT )
8252	def image_to_rgb ( self , path , n = 10 ) : from PIL import Image img = Image . open ( path ) p = img . getdata ( ) f = lambda p : choice ( p ) for i in _range ( n ) : rgba = f ( p ) rgba = _list ( rgba ) if len ( rgba ) == 3 : rgba . append ( 255 ) r , g , b , a = [ v / 255.0 for v in rgba ] clr = color ( r , g , b , a , mode = "rgb" ) self . append ( clr )
5409	def _validate_ram ( ram_in_mb ) : return int ( GoogleV2CustomMachine . _MEMORY_MULTIPLE * math . ceil ( ram_in_mb / GoogleV2CustomMachine . _MEMORY_MULTIPLE ) )
946	def _isCheckpointDir ( checkpointDir ) : lastSegment = os . path . split ( checkpointDir ) [ 1 ] if lastSegment [ 0 ] == '.' : return False if not checkpointDir . endswith ( g_defaultCheckpointExtension ) : return False if not os . path . isdir ( checkpointDir ) : return False return True
1584	def yaml_config_reader ( config_path ) : if not config_path . endswith ( ".yaml" ) : raise ValueError ( "Config file not yaml" ) with open ( config_path , 'r' ) as f : config = yaml . load ( f ) return config
17	def _subproc_worker ( pipe , parent_pipe , env_fn_wrapper , obs_bufs , obs_shapes , obs_dtypes , keys ) : def _write_obs ( maybe_dict_obs ) : flatdict = obs_to_dict ( maybe_dict_obs ) for k in keys : dst = obs_bufs [ k ] . get_obj ( ) dst_np = np . frombuffer ( dst , dtype = obs_dtypes [ k ] ) . reshape ( obs_shapes [ k ] ) np . copyto ( dst_np , flatdict [ k ] ) env = env_fn_wrapper . x ( ) parent_pipe . close ( ) try : while True : cmd , data = pipe . recv ( ) if cmd == 'reset' : pipe . send ( _write_obs ( env . reset ( ) ) ) elif cmd == 'step' : obs , reward , done , info = env . step ( data ) if done : obs = env . reset ( ) pipe . send ( ( _write_obs ( obs ) , reward , done , info ) ) elif cmd == 'render' : pipe . send ( env . render ( mode = 'rgb_array' ) ) elif cmd == 'close' : pipe . send ( None ) break else : raise RuntimeError ( 'Got unrecognized cmd %s' % cmd ) except KeyboardInterrupt : print ( 'ShmemVecEnv worker: got KeyboardInterrupt' ) finally : env . close ( )
7635	def import_lab ( namespace , filename , infer_duration = True , ** parse_options ) : r annotation = core . Annotation ( namespace ) parse_options . setdefault ( 'sep' , r'\s+' ) parse_options . setdefault ( 'engine' , 'python' ) parse_options . setdefault ( 'header' , None ) parse_options . setdefault ( 'index_col' , False ) parse_options . setdefault ( 'names' , range ( 20 ) ) data = pd . read_csv ( filename , ** parse_options ) data = data . dropna ( how = 'all' , axis = 1 ) if len ( data . columns ) == 2 : data . insert ( 1 , 'duration' , 0 ) if infer_duration : data [ 'duration' ] [ : - 1 ] = data . loc [ : , 0 ] . diff ( ) [ 1 : ] . values else : if infer_duration : data . loc [ : , 1 ] -= data [ 0 ] for row in data . itertuples ( ) : time , duration = row [ 1 : 3 ] value = [ x for x in row [ 3 : ] if x is not None ] [ - 1 ] annotation . append ( time = time , duration = duration , confidence = 1.0 , value = value ) return annotation
6713	def install_setuptools ( python_cmd = 'python' , use_sudo = True ) : setuptools_version = package_version ( 'setuptools' , python_cmd ) distribute_version = package_version ( 'distribute' , python_cmd ) if setuptools_version is None : _install_from_scratch ( python_cmd , use_sudo ) else : if distribute_version is None : _upgrade_from_setuptools ( python_cmd , use_sudo ) else : _upgrade_from_distribute ( python_cmd , use_sudo )
321	def get_max_drawdown ( returns ) : returns = returns . copy ( ) df_cum = cum_returns ( returns , 1.0 ) running_max = np . maximum . accumulate ( df_cum ) underwater = df_cum / running_max - 1 return get_max_drawdown_underwater ( underwater )
63	def is_partly_within_image ( self , image ) : shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] eps = np . finfo ( np . float32 ) . eps img_bb = BoundingBox ( x1 = 0 , x2 = width - eps , y1 = 0 , y2 = height - eps ) return self . intersection ( img_bb ) is not None
8828	def sg_gather_associated_ports ( context , group ) : if not group : return None if not hasattr ( group , "ports" ) or len ( group . ports ) <= 0 : return [ ] return group . ports
12187	async def handle_message ( self , message , filters ) : data = self . _unpack_message ( message ) logger . debug ( data ) if data . get ( 'type' ) == 'error' : raise SlackApiError ( data . get ( 'error' , { } ) . get ( 'msg' , str ( data ) ) ) elif self . message_is_to_me ( data ) : text = data [ 'text' ] [ len ( self . address_as ) : ] . strip ( ) if text == 'help' : return self . _respond ( channel = data [ 'channel' ] , text = self . _instruction_list ( filters ) , ) elif text == 'version' : return self . _respond ( channel = data [ 'channel' ] , text = self . VERSION , ) for _filter in filters : if _filter . matches ( data ) : logger . debug ( 'Response triggered' ) async for response in _filter : self . _respond ( channel = data [ 'channel' ] , text = response )
2309	def predict ( self , data , graph = None , nruns = 6 , njobs = None , gpus = 0 , verbose = None , plot = False , plot_generated_pair = False , return_list_results = False ) : verbose , njobs = SETTINGS . get_default ( ( 'verbose' , verbose ) , ( 'nb_jobs' , njobs ) ) if njobs != 1 : list_out = Parallel ( n_jobs = njobs ) ( delayed ( run_SAM ) ( data , skeleton = graph , lr_gen = self . lr , lr_disc = self . dlr , regul_param = self . l1 , nh = self . nh , dnh = self . dnh , gpu = bool ( gpus ) , train_epochs = self . train , test_epochs = self . test , batch_size = self . batchsize , plot = plot , verbose = verbose , gpu_no = idx % max ( gpus , 1 ) ) for idx in range ( nruns ) ) else : list_out = [ run_SAM ( data , skeleton = graph , lr_gen = self . lr , lr_disc = self . dlr , regul_param = self . l1 , nh = self . nh , dnh = self . dnh , gpu = bool ( gpus ) , train_epochs = self . train , test_epochs = self . test , batch_size = self . batchsize , plot = plot , verbose = verbose , gpu_no = 0 ) for idx in range ( nruns ) ] if return_list_results : return list_out else : W = list_out [ 0 ] for w in list_out [ 1 : ] : W += w W /= nruns return nx . relabel_nodes ( nx . DiGraph ( W ) , { idx : i for idx , i in enumerate ( data . columns ) } )
975	def _createBucket ( self , index ) : if index < self . minIndex : if index == self . minIndex - 1 : self . bucketMap [ index ] = self . _newRepresentation ( self . minIndex , index ) self . minIndex = index else : self . _createBucket ( index + 1 ) self . _createBucket ( index ) else : if index == self . maxIndex + 1 : self . bucketMap [ index ] = self . _newRepresentation ( self . maxIndex , index ) self . maxIndex = index else : self . _createBucket ( index - 1 ) self . _createBucket ( index )
10454	def stateenabled ( self , window_name , object_name ) : try : object_handle = self . _get_object_handle ( window_name , object_name ) if object_handle . AXEnabled : return 1 except LdtpServerException : pass return 0
833	def createInput ( self ) : print "-" * 70 + "Creating a random input vector" + "-" * 70 self . inputArray [ 0 : ] = 0 for i in range ( self . inputSize ) : self . inputArray [ i ] = random . randrange ( 2 )
13805	def merge_ordered ( ordereds : typing . Iterable [ typing . Any ] ) -> typing . Iterable [ typing . Any ] : seen_set = set ( ) add_seen = seen_set . add return reversed ( tuple ( map ( lambda obj : add_seen ( obj ) or obj , filterfalse ( seen_set . __contains__ , chain . from_iterable ( map ( reversed , reversed ( ordereds ) ) ) , ) , ) ) )
813	def read ( cls , proto ) : tm = super ( TemporalMemoryMonitorMixin , cls ) . read ( proto ) tm . mmName = None tm . _mmTraces = None tm . _mmData = None tm . mmClearHistory ( ) tm . _mmResetActive = True return tm
2980	def cmd_logs ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) puts ( b . logs ( opts . container ) . decode ( encoding = 'UTF-8' ) )
4591	def to_triplets ( colors ) : try : colors [ 0 ] [ 0 ] return colors except : pass extra = len ( colors ) % 3 if extra : colors = colors [ : - extra ] return list ( zip ( * [ iter ( colors ) ] * 3 ) )
8745	def get_floatingips ( context , filters = None , fields = None , sorts = [ 'id' ] , limit = None , marker = None , page_reverse = False ) : LOG . info ( 'get_floatingips for tenant %s filters %s fields %s' % ( context . tenant_id , filters , fields ) ) floating_ips = _get_ips_by_type ( context , ip_types . FLOATING , filters = filters , fields = fields ) return [ v . _make_floating_ip_dict ( flip ) for flip in floating_ips ]
6074	def mass_within_ellipse_in_units ( self , major_axis , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : if self . has_mass_profile : return sum ( map ( lambda p : p . mass_within_ellipse_in_units ( major_axis = major_axis , unit_mass = unit_mass , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) , self . mass_profiles ) ) else : return None
7191	def _load_info ( self ) : url = '%s/prefix?duration=36000' % self . base_url r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( )
5970	def MD_restrained ( dirname = 'MD_POSRES' , ** kwargs ) : logger . info ( "[{dirname!s}] Setting up MD with position restraints..." . format ( ** vars ( ) ) ) kwargs . setdefault ( 'struct' , 'em/em.pdb' ) kwargs . setdefault ( 'qname' , 'PR_GMX' ) kwargs . setdefault ( 'define' , '-DPOSRES' ) kwargs . setdefault ( 'nstxout' , '50000' ) kwargs . setdefault ( 'nstvout' , '50000' ) kwargs . setdefault ( 'nstfout' , '0' ) kwargs . setdefault ( 'nstlog' , '500' ) kwargs . setdefault ( 'nstenergy' , '2500' ) kwargs . setdefault ( 'nstxtcout' , '5000' ) kwargs . setdefault ( 'refcoord_scaling' , 'com' ) kwargs . setdefault ( 'Pcoupl' , "Berendsen" ) new_kwargs = _setup_MD ( dirname , ** kwargs ) new_kwargs . pop ( 'define' , None ) new_kwargs . pop ( 'refcoord_scaling' , None ) new_kwargs . pop ( 'Pcoupl' , None ) return new_kwargs
11676	def fit ( self , X , y = None ) : self . features_ = as_features ( X , stack = True , bare = True ) return self
1917	def fork ( self , state , expression , policy = 'ALL' , setstate = None ) : assert isinstance ( expression , Expression ) if setstate is None : setstate = lambda x , y : None solutions = state . concretize ( expression , policy ) if not solutions : raise ExecutorError ( "Forking on unfeasible constraint set" ) if len ( solutions ) == 1 : setstate ( state , solutions [ 0 ] ) return state logger . info ( "Forking. Policy: %s. Values: %s" , policy , ', ' . join ( f'0x{sol:x}' for sol in solutions ) ) self . _publish ( 'will_fork_state' , state , expression , solutions , policy ) children = [ ] for new_value in solutions : with state as new_state : new_state . constrain ( expression == new_value ) setstate ( new_state , new_value ) self . _publish ( 'did_fork_state' , new_state , expression , new_value , policy ) state_id = self . enqueue ( new_state ) children . append ( state_id ) logger . info ( "Forking current state into states %r" , children ) return None
13106	def add_tag ( self , tag ) : self . tags = list ( set ( self . tags or [ ] ) | set ( [ tag ] ) )
9637	def format ( self , record ) : data = record . _raw . copy ( ) data [ 'time' ] = data [ 'time' ] . isoformat ( ) if data . get ( 'traceback' ) : data [ 'traceback' ] = self . formatException ( data [ 'traceback' ] ) return json . dumps ( data )
12263	def _load_meta ( self , size , md5 ) : if not hasattr ( self , 'local_hashes' ) : self . local_hashes = { } self . size = int ( size ) if ( re . match ( '^[a-fA-F0-9]{32}$' , md5 ) ) : self . md5 = md5
8923	def baseurl ( url ) : parsed_url = urlparse . urlparse ( url ) if not parsed_url . netloc or parsed_url . scheme not in ( "http" , "https" ) : raise ValueError ( 'bad url' ) service_url = "%s://%s%s" % ( parsed_url . scheme , parsed_url . netloc , parsed_url . path . strip ( ) ) return service_url
6005	def generate_poisson_noise ( image , exposure_time_map , seed = - 1 ) : setup_random_seed ( seed ) image_counts = np . multiply ( image , exposure_time_map ) return image - np . divide ( np . random . poisson ( image_counts , image . shape ) , exposure_time_map )
11773	def information_content ( values ) : "Number of bits to represent the probability distribution in values." probabilities = normalize ( removeall ( 0 , values ) ) return sum ( - p * log2 ( p ) for p in probabilities )
1808	def SETC ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . CF , 1 , 0 ) )
9041	def as_instruction ( self , specification ) : instruction = self . _instruction_class ( specification ) type_ = instruction . type if type_ in self . _type_to_instruction : instruction . inherit_from ( self . _type_to_instruction [ type_ ] ) return instruction
8395	def show_help ( ) : print ( ) for cmd in [ write_main , check_main , list_main ] : print ( cmd . __doc__ . lstrip ( "\n" ) )
8296	def render ( self , size , frame , drawqueue ) : r_context = self . create_rcontext ( size , frame ) drawqueue . render ( r_context ) self . rendering_finished ( size , frame , r_context ) return r_context
12184	def _add_parsley_ns ( cls , namespace_dict ) : namespace_dict . update ( { 'parslepy' : cls . LOCAL_NAMESPACE , 'parsley' : cls . LOCAL_NAMESPACE , } ) return namespace_dict
11732	def registerGoodClass ( self , class_ ) : self . _valid_classes . append ( class_ ) for name , cls in class_members ( class_ ) : if self . isValidClass ( cls ) : self . registerGoodClass ( cls )
11549	def main ( ) : usage = "Usage: %prog PATH_TO_PACKAGE" parser = optparse . OptionParser ( usage = usage ) parser . add_option ( "-v" , "--verbose" , action = "store_true" , dest = "verbose" , default = False , help = "Show debug output" ) parser . add_option ( "-d" , "--output-dir" , action = "store" , type = "string" , dest = "output_dir" , default = '' , help = "" ) parser . add_option ( "-t" , "--test-args" , action = "store" , type = "string" , dest = "test_args" , default = '' , help = ( "Pass argument on to bin/test. Quote the argument, " + "for instance \"-t '-m somemodule'\"." ) ) ( options , args ) = parser . parse_args ( ) if options . verbose : log_level = logging . DEBUG else : log_level = logging . INFO logging . basicConfig ( level = log_level , format = "%(levelname)s: %(message)s" ) curdir = os . getcwd ( ) testbinary = os . path . join ( curdir , 'bin' , 'test' ) if not os . path . exists ( testbinary ) : raise RuntimeError ( "Test command doesn't exist: %s" % testbinary ) coveragebinary = os . path . join ( curdir , 'bin' , 'coverage' ) if not os . path . exists ( coveragebinary ) : logger . debug ( "Trying globally installed coverage command." ) coveragebinary = 'coverage' logger . info ( "Running tests in coverage mode (can take a long time)" ) parts = [ coveragebinary , 'run' , testbinary ] if options . test_args : parts . append ( options . test_args ) system ( " " . join ( parts ) ) logger . debug ( "Creating coverage reports..." ) if options . output_dir : coverage_dir = options . output_dir open_in_browser = False else : coverage_dir = 'htmlcov' open_in_browser = True system ( "%s html --directory=%s" % ( coveragebinary , coverage_dir ) ) logger . info ( "Wrote coverage files to %s" , coverage_dir ) if open_in_browser : index_file = os . path . abspath ( os . path . join ( coverage_dir , 'index.html' ) ) logger . debug ( "About to open %s in your webbrowser." , index_file ) webbrowser . open ( 'file://' + index_file ) logger . info ( "Opened reports in your browser." )
5721	def _convert_schemas ( mapping , schemas ) : schemas = deepcopy ( schemas ) for schema in schemas : for fk in schema . get ( 'foreignKeys' , [ ] ) : resource = fk [ 'reference' ] [ 'resource' ] if resource != 'self' : if resource not in mapping : message = 'Not resource "%s" for foreign key "%s"' message = message % ( resource , fk ) raise ValueError ( message ) fk [ 'reference' ] [ 'resource' ] = mapping [ resource ] return schemas
3064	def update_query_params ( uri , params ) : parts = urllib . parse . urlparse ( uri ) query_params = parse_unique_urlencoded ( parts . query ) query_params . update ( params ) new_query = urllib . parse . urlencode ( query_params ) new_parts = parts . _replace ( query = new_query ) return urllib . parse . urlunparse ( new_parts )
5781	def _create_buffers ( self , number ) : buffers = new ( secur32 , 'SecBuffer[%d]' % number ) for index in range ( 0 , number ) : buffers [ index ] . cbBuffer = 0 buffers [ index ] . BufferType = Secur32Const . SECBUFFER_EMPTY buffers [ index ] . pvBuffer = null ( ) sec_buffer_desc_pointer = struct ( secur32 , 'SecBufferDesc' ) sec_buffer_desc = unwrap ( sec_buffer_desc_pointer ) sec_buffer_desc . ulVersion = Secur32Const . SECBUFFER_VERSION sec_buffer_desc . cBuffers = number sec_buffer_desc . pBuffers = buffers return ( sec_buffer_desc_pointer , buffers )
12904	def toIndex ( self , value ) : if self . _isIrNull ( value ) : ret = IR_NULL_STR else : ret = self . _toIndex ( value ) if self . isIndexHashed is False : return ret return md5 ( tobytes ( ret ) ) . hexdigest ( )
3234	def list_targets_by_rule ( client = None , ** kwargs ) : result = client . list_targets_by_rule ( ** kwargs ) if not result . get ( "Targets" ) : result . update ( { "Targets" : [ ] } ) return result
7059	def s3_delete_file ( bucket , filename , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 's3' ) try : resp = client . delete_object ( Bucket = bucket , Key = filename ) if not resp : LOGERROR ( 'could not delete file %s from bucket %s' % ( filename , bucket ) ) else : return resp [ 'DeleteMarker' ] except Exception as e : LOGEXCEPTION ( 'could not delete file %s from bucket %s' % ( filename , bucket ) ) if raiseonfail : raise return None
2329	def create_graph_from_data ( self , data ) : self . arguments [ '{SCORE}' ] = self . score self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) self . arguments [ '{BETA}' ] = str ( self . beta ) self . arguments [ '{OPTIM}' ] = str ( self . optim ) . upper ( ) self . arguments [ '{ALPHA}' ] = str ( self . alpha ) results = self . _run_bnlearn ( data , verbose = self . verbose ) graph = nx . DiGraph ( ) graph . add_edges_from ( results ) return graph
11103	def backup ( self , dst = None , ignore = None , ignore_ext = None , ignore_pattern = None , ignore_size_smaller_than = None , ignore_size_larger_than = None , case_sensitive = False ) : def preprocess_arg ( arg ) : if arg is None : return [ ] if isinstance ( arg , ( tuple , list ) ) : return list ( arg ) else : return [ arg , ] self . assert_is_dir_and_exists ( ) ignore = preprocess_arg ( ignore ) for i in ignore : if i . startswith ( "/" ) or i . startswith ( "\\" ) : raise ValueError ignore_ext = preprocess_arg ( ignore_ext ) for ext in ignore_ext : if not ext . startswith ( "." ) : raise ValueError ignore_pattern = preprocess_arg ( ignore_pattern ) if case_sensitive : pass else : ignore = [ i . lower ( ) for i in ignore ] ignore_ext = [ i . lower ( ) for i in ignore_ext ] ignore_pattern = [ i . lower ( ) for i in ignore_pattern ] def filters ( p ) : relpath = p . relative_to ( self ) . abspath if not case_sensitive : relpath = relpath . lower ( ) for i in ignore : if relpath . startswith ( i ) : return False if case_sensitive : ext = p . ext else : ext = p . ext . lower ( ) if ext in ignore_ext : return False for pattern in ignore_pattern : if pattern in relpath : return False if ignore_size_smaller_than : if p . size < ignore_size_smaller_than : return False if ignore_size_larger_than : if p . size > ignore_size_larger_than : return False return True self . make_zip_archive ( dst = dst , filters = filters , compress = True , overwrite = False , verbose = True , )
744	def requireAnomalyModel ( func ) : @ wraps ( func ) def _decorator ( self , * args , ** kwargs ) : if not self . getInferenceType ( ) == InferenceType . TemporalAnomaly : raise RuntimeError ( "Method required a TemporalAnomaly model." ) if self . _getAnomalyClassifier ( ) is None : raise RuntimeError ( "Model does not support this command. Model must" "be an active anomalyDetector model." ) return func ( self , * args , ** kwargs ) return _decorator
624	def neighborhood ( centerIndex , radius , dimensions ) : centerPosition = coordinatesFromIndex ( centerIndex , dimensions ) intervals = [ ] for i , dimension in enumerate ( dimensions ) : left = max ( 0 , centerPosition [ i ] - radius ) right = min ( dimension - 1 , centerPosition [ i ] + radius ) intervals . append ( xrange ( left , right + 1 ) ) coords = numpy . array ( list ( itertools . product ( * intervals ) ) ) return numpy . ravel_multi_index ( coords . T , dimensions )
2356	def is_element_present ( self , strategy , locator ) : return self . driver_adapter . is_element_present ( strategy , locator , root = self . root )
6144	def DSP_callback_tic ( self ) : if self . Tcapture > 0 : self . DSP_tic . append ( time . time ( ) - self . start_time )
11060	def stop ( self ) : if self . webserver is not None : self . webserver . stop ( ) if not self . test_mode : self . plugins . save_state ( )
267	def vectorize ( func ) : def wrapper ( df , * args , ** kwargs ) : if df . ndim == 1 : return func ( df , * args , ** kwargs ) elif df . ndim == 2 : return df . apply ( func , * args , ** kwargs ) return wrapper
11583	def image_urls ( self ) : all_image_urls = self . finder_image_urls [ : ] for image_url in self . extender_image_urls : if image_url not in all_image_urls : all_image_urls . append ( image_url ) return all_image_urls
11991	def decode_html_entities ( html ) : if not html : return html for entity , char in six . iteritems ( html_entity_map ) : html = html . replace ( entity , char ) return html
1138	def _splitext ( p , sep , altsep , extsep ) : sepIndex = p . rfind ( sep ) if altsep : altsepIndex = p . rfind ( altsep ) sepIndex = max ( sepIndex , altsepIndex ) dotIndex = p . rfind ( extsep ) if dotIndex > sepIndex : filenameIndex = sepIndex + 1 while filenameIndex < dotIndex : if p [ filenameIndex ] != extsep : return p [ : dotIndex ] , p [ dotIndex : ] filenameIndex += 1 return p , ''
10677	def Cp ( self , T ) : result = 0.0 for c , e in zip ( self . _coefficients , self . _exponents ) : result += c * T ** e return result
2137	def create ( self , fail_on_found = False , force_on_exists = False , ** kwargs ) : if kwargs . get ( 'parent' , None ) : parent_data = self . set_child_endpoint ( parent = kwargs [ 'parent' ] , inventory = kwargs . get ( 'inventory' , None ) ) kwargs [ 'inventory' ] = parent_data [ 'inventory' ] elif 'inventory' not in kwargs : raise exc . UsageError ( 'To create a group, you must provide a parent inventory or parent group.' ) return super ( Resource , self ) . create ( fail_on_found = fail_on_found , force_on_exists = force_on_exists , ** kwargs )
3723	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : sigmas = [ i ( T ) for i in self . SurfaceTensions ] return mixing_simple ( zs , sigmas ) elif method == DIGUILIOTEJA : return Diguilio_Teja ( T = T , xs = zs , sigmas_Tb = self . sigmas_Tb , Tbs = self . Tbs , Tcs = self . Tcs ) elif method == WINTERFELDSCRIVENDAVIS : sigmas = [ i ( T ) for i in self . SurfaceTensions ] rhoms = [ 1. / i ( T , P ) for i in self . VolumeLiquids ] return Winterfeld_Scriven_Davis ( zs , sigmas , rhoms ) else : raise Exception ( 'Method not valid' )
6123	def instance_for_arguments ( self , arguments ) : profiles = { ** { key : value . instance_for_arguments ( arguments ) for key , value in self . profile_prior_model_dict . items ( ) } , ** self . constant_profile_dict } try : redshift = self . redshift . instance_for_arguments ( arguments ) except AttributeError : redshift = self . redshift pixelization = self . pixelization . instance_for_arguments ( arguments ) if isinstance ( self . pixelization , pm . PriorModel ) else self . pixelization regularization = self . regularization . instance_for_arguments ( arguments ) if isinstance ( self . regularization , pm . PriorModel ) else self . regularization hyper_galaxy = self . hyper_galaxy . instance_for_arguments ( arguments ) if isinstance ( self . hyper_galaxy , pm . PriorModel ) else self . hyper_galaxy return galaxy . Galaxy ( redshift = redshift , pixelization = pixelization , regularization = regularization , hyper_galaxy = hyper_galaxy , ** profiles )
384	def parse_darknet_ann_str_to_list ( annotations ) : r annotations = annotations . split ( "\n" ) ann = [ ] for a in annotations : a = a . split ( ) if len ( a ) == 5 : for i , _v in enumerate ( a ) : if i == 0 : a [ i ] = int ( a [ i ] ) else : a [ i ] = float ( a [ i ] ) ann . append ( a ) return ann
12300	def search ( self , what , name = None , version = None ) : filtered = { } if what is None : whats = list ( self . plugins . keys ( ) ) elif what is not None : if what not in self . plugins : raise Exception ( "Unknown class of plugins" ) whats = [ what ] for what in whats : if what not in filtered : filtered [ what ] = [ ] for key in self . plugins [ what ] . keys ( ) : ( k_name , k_version ) = key if name is not None and k_name != name : continue if version is not None and k_version != version : continue if self . plugins [ what ] [ key ] . enable == 'n' : continue filtered [ what ] . append ( key ) return filtered
171	def draw_lines_heatmap_array ( self , image_shape , alpha = 1.0 , size = 1 , antialiased = True , raise_if_out_of_image = False ) : assert len ( image_shape ) == 2 or ( len ( image_shape ) == 3 and image_shape [ - 1 ] == 1 ) , ( "Expected (H,W) or (H,W,1) as image_shape, got %s." % ( image_shape , ) ) arr = self . draw_lines_on_image ( np . zeros ( image_shape , dtype = np . uint8 ) , color = 255 , alpha = alpha , size = size , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) return arr . astype ( np . float32 ) / 255.0
9242	def detect_actual_closed_dates ( self , issues , kind ) : if self . options . verbose : print ( "Fetching closed dates for {} {}..." . format ( len ( issues ) , kind ) ) all_issues = copy . deepcopy ( issues ) for issue in all_issues : if self . options . verbose > 2 : print ( "." , end = "" ) if not issues . index ( issue ) % 30 : print ( "" ) self . find_closed_date_by_commit ( issue ) if not issue . get ( 'actual_date' , False ) : if issue . get ( 'closed_at' , False ) : print ( "Skipping closed non-merged issue: #{0} {1}" . format ( issue [ "number" ] , issue [ "title" ] ) ) all_issues . remove ( issue ) if self . options . verbose > 2 : print ( "." ) return all_issues
1146	def copy ( x ) : cls = type ( x ) copier = _copy_dispatch . get ( cls ) if copier : return copier ( x ) copier = getattr ( cls , "__copy__" , None ) if copier : return copier ( x ) reductor = dispatch_table . get ( cls ) if reductor : rv = reductor ( x ) else : reductor = getattr ( x , "__reduce_ex__" , None ) if reductor : rv = reductor ( 2 ) else : reductor = getattr ( x , "__reduce__" , None ) if reductor : rv = reductor ( ) else : raise Error ( "un(shallow)copyable object of type %s" % cls ) return _reconstruct ( x , rv , 0 )
2217	def _list_itemstrs ( list_ , ** kwargs ) : items = list ( list_ ) kwargs [ '_return_info' ] = True _tups = [ repr2 ( item , ** kwargs ) for item in items ] itemstrs = [ t [ 0 ] for t in _tups ] max_height = max ( [ t [ 1 ] [ 'max_height' ] for t in _tups ] ) if _tups else 0 _leaf_info = { 'max_height' : max_height + 1 , } sort = kwargs . get ( 'sort' , None ) if sort is None : sort = isinstance ( list_ , ( set , frozenset ) ) if sort : itemstrs = _sort_itemstrs ( items , itemstrs ) return itemstrs , _leaf_info
13568	def selected_course ( func ) : @ wraps ( func ) def inner ( * args , ** kwargs ) : course = Course . get_selected ( ) return func ( course , * args , ** kwargs ) return inner
7743	def _prepare_io_handler_cb ( self , handler ) : self . _anything_done = True logger . debug ( "_prepar_io_handler_cb called for {0!r}" . format ( handler ) ) self . _configure_io_handler ( handler ) self . _prepare_sources . pop ( handler , None ) return False
2369	def keywords ( self ) : for table in self . tables : if isinstance ( table , KeywordTable ) : for keyword in table . keywords : yield keyword
3586	def add ( self , cbobject , metadata ) : with self . _lock : if cbobject not in self . _metadata : self . _metadata [ cbobject ] = metadata return self . _metadata [ cbobject ]
4387	def adsAddRoute ( net_id , ip_address ) : add_route = _adsDLL . AdsAddRoute add_route . restype = ctypes . c_long ip_address_p = ctypes . c_char_p ( ip_address . encode ( "utf-8" ) ) error_code = add_route ( net_id , ip_address_p ) if error_code : raise ADSError ( error_code )
10027	def delete_unused_versions ( self , versions_to_keep = 10 ) : environments = self . ebs . describe_environments ( application_name = self . app_name , include_deleted = False ) environments = environments [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ] versions_in_use = [ ] for env in environments : versions_in_use . append ( env [ 'VersionLabel' ] ) versions = self . ebs . describe_application_versions ( application_name = self . app_name ) versions = versions [ 'DescribeApplicationVersionsResponse' ] [ 'DescribeApplicationVersionsResult' ] [ 'ApplicationVersions' ] versions = sorted ( versions , reverse = True , key = functools . cmp_to_key ( lambda x , y : ( x [ 'DateCreated' ] > y [ 'DateCreated' ] ) - ( x [ 'DateCreated' ] < y [ 'DateCreated' ] ) ) ) for version in versions [ versions_to_keep : ] : if version [ 'VersionLabel' ] in versions_in_use : out ( "Not deleting " + version [ "VersionLabel" ] + " because it is in use" ) else : out ( "Deleting unused version: " + version [ "VersionLabel" ] ) self . ebs . delete_application_version ( application_name = self . app_name , version_label = version [ 'VersionLabel' ] ) sleep ( 2 )
4366	def process_event ( self , packet ) : args = packet [ 'args' ] name = packet [ 'name' ] if not allowed_event_name_regex . match ( name ) : self . error ( "unallowed_event_name" , "name must only contains alpha numerical characters" ) return method_name = 'on_' + name . replace ( ' ' , '_' ) return self . call_method_with_acl ( method_name , packet , * args )
9279	def from_decimal ( number , width = 1 ) : text = [ ] if not isinstance ( number , int_type ) : raise TypeError ( "Expected number to be int, got %s" , type ( number ) ) elif not isinstance ( width , int_type ) : raise TypeError ( "Expected width to be int, got %s" , type ( number ) ) elif number < 0 : raise ValueError ( "Expected number to be positive integer" ) elif number > 0 : max_n = ceil ( log ( number ) / log ( 91 ) ) for n in _range ( int ( max_n ) , - 1 , - 1 ) : quotient , number = divmod ( number , 91 ** n ) text . append ( chr ( 33 + quotient ) ) return "" . join ( text ) . lstrip ( '!' ) . rjust ( max ( 1 , width ) , '!' )
4590	def serpentine_y ( x , y , matrix ) : if x % 2 : return x , matrix . rows - 1 - y return x , y
9	def encode_observation ( ob_space , placeholder ) : if isinstance ( ob_space , Discrete ) : return tf . to_float ( tf . one_hot ( placeholder , ob_space . n ) ) elif isinstance ( ob_space , Box ) : return tf . to_float ( placeholder ) elif isinstance ( ob_space , MultiDiscrete ) : placeholder = tf . cast ( placeholder , tf . int32 ) one_hots = [ tf . to_float ( tf . one_hot ( placeholder [ ... , i ] , ob_space . nvec [ i ] ) ) for i in range ( placeholder . shape [ - 1 ] ) ] return tf . concat ( one_hots , axis = - 1 ) else : raise NotImplementedError
10923	def finish ( s , desc = 'finish' , n_loop = 4 , max_mem = 1e9 , separate_psf = True , fractol = 1e-7 , errtol = 1e-3 , dowarn = True ) : values = [ np . copy ( s . state [ s . params ] ) ] remove_params = s . get ( 'psf' ) . params if separate_psf else None global_params = name_globals ( s , remove_params = remove_params ) gs = np . floor ( max_mem / s . residuals . nbytes ) . astype ( 'int' ) groups = [ global_params [ a : a + gs ] for a in range ( 0 , len ( global_params ) , gs ) ] CLOG . info ( 'Start ``finish``:\t{}' . format ( s . error ) ) for a in range ( n_loop ) : start_err = s . error for g in groups : do_levmarq ( s , g , damping = 0.1 , decrease_damp_factor = 20. , max_iter = 1 , max_mem = max_mem , eig_update = False ) if separate_psf : do_levmarq ( s , remove_params , max_mem = max_mem , max_iter = 4 , eig_update = False ) CLOG . info ( 'Globals, loop {}:\t{}' . format ( a , s . error ) ) if desc is not None : states . save ( s , desc = desc ) do_levmarq_all_particle_groups ( s , max_iter = 1 , max_mem = max_mem ) CLOG . info ( 'Particles, loop {}:\t{}' . format ( a , s . error ) ) if desc is not None : states . save ( s , desc = desc ) values . append ( np . copy ( s . state [ s . params ] ) ) new_err = s . error derr = start_err - new_err dobreak = ( derr / new_err < fractol ) or ( derr < errtol ) if dobreak : break if dowarn and ( not dobreak ) : CLOG . warn ( 'finish() did not converge; consider re-running' ) return { 'converged' : dobreak , 'loop_values' : np . array ( values ) }
3782	def set_user_methods_P ( self , user_methods_P , forced_P = False ) : r if isinstance ( user_methods_P , str ) : user_methods_P = [ user_methods_P ] self . user_methods_P = user_methods_P self . forced_P = forced_P if set ( self . user_methods_P ) . difference ( self . all_methods_P ) : raise Exception ( "One of the given methods is not available for this chemical" ) if not self . user_methods_P and self . forced : raise Exception ( 'Only user specified methods are considered when forced is True, but no methods were provided' ) self . method_P = None self . sorted_valid_methods_P = [ ] self . TP_cached = None
3812	async def set_active ( self ) : is_active = ( self . _active_client_state == hangouts_pb2 . ACTIVE_CLIENT_STATE_IS_ACTIVE ) timed_out = ( time . time ( ) - self . _last_active_secs > SETACTIVECLIENT_LIMIT_SECS ) if not is_active or timed_out : self . _active_client_state = ( hangouts_pb2 . ACTIVE_CLIENT_STATE_IS_ACTIVE ) self . _last_active_secs = time . time ( ) if self . _email is None : try : get_self_info_request = hangouts_pb2 . GetSelfInfoRequest ( request_header = self . get_request_header ( ) , ) get_self_info_response = await self . get_self_info ( get_self_info_request ) except exceptions . NetworkError as e : logger . warning ( 'Failed to find email address: {}' . format ( e ) ) return self . _email = ( get_self_info_response . self_entity . properties . email [ 0 ] ) if self . _client_id is None : logger . info ( 'Cannot set active client until client_id is received' ) return try : set_active_request = hangouts_pb2 . SetActiveClientRequest ( request_header = self . get_request_header ( ) , is_active = True , full_jid = "{}/{}" . format ( self . _email , self . _client_id ) , timeout_secs = ACTIVE_TIMEOUT_SECS , ) await self . set_active_client ( set_active_request ) except exceptions . NetworkError as e : logger . warning ( 'Failed to set active client: {}' . format ( e ) ) else : logger . info ( 'Set active client for {} seconds' . format ( ACTIVE_TIMEOUT_SECS ) )
3049	def _get_implicit_credentials ( cls ) : environ_checkers = [ cls . _implicit_credentials_from_files , cls . _implicit_credentials_from_gae , cls . _implicit_credentials_from_gce , ] for checker in environ_checkers : credentials = checker ( ) if credentials is not None : return credentials raise ApplicationDefaultCredentialsError ( ADC_HELP_MSG )
8778	def _check_collisions ( self , new_range , existing_ranges ) : def _contains ( num , r1 ) : return ( num >= r1 [ 0 ] and num <= r1 [ 1 ] ) def _is_overlap ( r1 , r2 ) : return ( _contains ( r1 [ 0 ] , r2 ) or _contains ( r1 [ 1 ] , r2 ) or _contains ( r2 [ 0 ] , r1 ) or _contains ( r2 [ 1 ] , r1 ) ) for existing_range in existing_ranges : if _is_overlap ( new_range , existing_range ) : return True return False
12135	def directory ( cls , directory , root = None , extension = None , ** kwargs ) : root = os . getcwd ( ) if root is None else root suffix = '' if extension is None else '.' + extension . rsplit ( '.' ) [ - 1 ] pattern = directory + os . sep + '*' + suffix key = os . path . join ( root , directory , '*' ) . rsplit ( os . sep ) [ - 2 ] format_parse = list ( string . Formatter ( ) . parse ( key ) ) if not all ( [ el is None for el in zip ( * format_parse ) [ 1 ] ] ) : raise Exception ( 'Directory cannot contain format field specifications' ) return cls ( key , pattern , root , ** kwargs )
4601	def main ( ) : if not _curses : if os . name == 'nt' : raise ValueError ( 'curses is not supported under Windows' ) raise ValueError ( 'Your platform does not support curses.' ) try : driver = next ( iter ( Curses . DRIVERS ) ) except : raise ValueError ( 'No Curses driver in project' ) _curses . wrapper ( driver . run_in_curses )
4978	def get_course_or_program_context ( self , enterprise_customer , course_id = None , program_uuid = None ) : context_data = { } if course_id : context_data . update ( { 'course_id' : course_id , 'course_specific' : True } ) if not self . preview_mode : try : catalog_api_client = CourseCatalogApiServiceClient ( enterprise_customer . site ) except ImproperlyConfigured : raise Http404 course_run_details = catalog_api_client . get_course_run ( course_id ) course_start_date = '' if course_run_details [ 'start' ] : course_start_date = parse ( course_run_details [ 'start' ] ) . strftime ( '%B %d, %Y' ) context_data . update ( { 'course_title' : course_run_details [ 'title' ] , 'course_start_date' : course_start_date , } ) else : context_data . update ( { 'course_title' : 'Demo Course' , 'course_start_date' : datetime . datetime . now ( ) . strftime ( '%B %d, %Y' ) , } ) else : context_data . update ( { 'program_uuid' : program_uuid , 'program_specific' : True , } ) return context_data
4864	def get_groups ( self , obj ) : if obj . user : return [ group . name for group in obj . user . groups . filter ( name__in = ENTERPRISE_PERMISSION_GROUPS ) ] return [ ]
10824	def query_by_group ( cls , group_or_id , with_invitations = False , ** kwargs ) : if isinstance ( group_or_id , Group ) : id_group = group_or_id . id else : id_group = group_or_id if not with_invitations : return cls . _filter ( cls . query . filter_by ( id_group = id_group ) , ** kwargs ) else : return cls . query . filter ( Membership . id_group == id_group , db . or_ ( Membership . state == MembershipState . PENDING_USER , Membership . state == MembershipState . ACTIVE ) )
5310	def translate_rgb_to_ansi_code ( red , green , blue , offset , colormode ) : if colormode == terminal . NO_COLORS : return '' , '' if colormode == terminal . ANSI_8_COLORS or colormode == terminal . ANSI_16_COLORS : color_code = ansi . rgb_to_ansi16 ( red , green , blue ) start_code = ansi . ANSI_ESCAPE_CODE . format ( code = color_code + offset - ansi . FOREGROUND_COLOR_OFFSET ) end_code = ansi . ANSI_ESCAPE_CODE . format ( code = offset + ansi . COLOR_CLOSE_OFFSET ) return start_code , end_code if colormode == terminal . ANSI_256_COLORS : color_code = ansi . rgb_to_ansi256 ( red , green , blue ) start_code = ansi . ANSI_ESCAPE_CODE . format ( code = '{base};5;{code}' . format ( base = 8 + offset , code = color_code ) ) end_code = ansi . ANSI_ESCAPE_CODE . format ( code = offset + ansi . COLOR_CLOSE_OFFSET ) return start_code , end_code if colormode == terminal . TRUE_COLORS : start_code = ansi . ANSI_ESCAPE_CODE . format ( code = '{base};2;{red};{green};{blue}' . format ( base = 8 + offset , red = red , green = green , blue = blue ) ) end_code = ansi . ANSI_ESCAPE_CODE . format ( code = offset + ansi . COLOR_CLOSE_OFFSET ) return start_code , end_code raise ColorfulError ( 'invalid color mode "{0}"' . format ( colormode ) )
12542	def get_attributes ( self , attributes , default = '' ) : if isinstance ( attributes , str ) : attributes = [ attributes ] attrs = [ getattr ( self , attr , default ) for attr in attributes ] if len ( attrs ) == 1 : return attrs [ 0 ] return tuple ( attrs )
373	def brightness ( x , gamma = 1 , gain = 1 , is_random = False ) : if is_random : gamma = np . random . uniform ( 1 - gamma , 1 + gamma ) x = exposure . adjust_gamma ( x , gamma , gain ) return x
12028	def headerHTML ( header , fname ) : html = "<html><body><code>" html += "<h2>%s</h2>" % ( fname ) html += pprint . pformat ( header , indent = 1 ) html = html . replace ( "\n" , '<br>' ) . replace ( " " , "&nbsp;" ) html = html . replace ( r"\x00" , "" ) html += "</code></body></html>" print ( "saving header file:" , fname ) f = open ( fname , 'w' ) f . write ( html ) f . close ( ) webbrowser . open ( fname )
2318	def predict ( self , data , alpha = 0.01 , max_iter = 2000 , ** kwargs ) : edge_model = GraphLasso ( alpha = alpha , max_iter = max_iter ) edge_model . fit ( data . values ) return nx . relabel_nodes ( nx . DiGraph ( edge_model . get_precision ( ) ) , { idx : i for idx , i in enumerate ( data . columns ) } )
9580	def read_struct_array ( fd , endian , header ) : field_name_length = read_elements ( fd , endian , [ 'miINT32' ] ) if field_name_length > 32 : raise ParseError ( 'Unexpected field name length: {}' . format ( field_name_length ) ) fields = read_elements ( fd , endian , [ 'miINT8' ] , is_name = True ) if isinstance ( fields , basestring ) : fields = [ fields ] empty = lambda : [ list ( ) for i in range ( header [ 'dims' ] [ 0 ] ) ] array = { } for row in range ( header [ 'dims' ] [ 0 ] ) : for col in range ( header [ 'dims' ] [ 1 ] ) : for field in fields : vheader , next_pos , fd_var = read_var_header ( fd , endian ) data = read_var_array ( fd_var , endian , vheader ) if field not in array : array [ field ] = empty ( ) array [ field ] [ row ] . append ( data ) fd . seek ( next_pos ) for field in fields : rows = array [ field ] for i in range ( header [ 'dims' ] [ 0 ] ) : rows [ i ] = squeeze ( rows [ i ] ) array [ field ] = squeeze ( array [ field ] ) return array
1314	def ControlFromPoint2 ( x : int , y : int ) -> Control : return Control . CreateControlFromElement ( _AutomationClient . instance ( ) . IUIAutomation . ElementFromHandle ( WindowFromPoint ( x , y ) ) )
6227	def _translate_string ( self , data , length ) : for index , char in enumerate ( data ) : if index == length : break yield self . _meta . characters - 1 - self . _ct [ char ]
8013	async def _create_upstream_applications ( self ) : loop = asyncio . get_event_loop ( ) for steam_name , ApplicationsCls in self . applications . items ( ) : application = ApplicationsCls ( self . scope ) upstream_queue = asyncio . Queue ( ) self . application_streams [ steam_name ] = upstream_queue self . application_futures [ steam_name ] = loop . create_task ( application ( upstream_queue . get , partial ( self . dispatch_downstream , steam_name = steam_name ) ) )
824	def mostLikely ( self , pred ) : if len ( pred ) == 1 : return pred . keys ( ) [ 0 ] mostLikelyOutcome = None maxProbability = 0 for prediction , probability in pred . items ( ) : if probability > maxProbability : mostLikelyOutcome = prediction maxProbability = probability return mostLikelyOutcome
7074	def variable_index_gridsearch_magbin ( simbasedir , stetson_stdev_range = ( 1.0 , 20.0 ) , inveta_stdev_range = ( 1.0 , 20.0 ) , iqr_stdev_range = ( 1.0 , 20.0 ) , ngridpoints = 32 , ngridworkers = None ) : outdir = os . path . join ( simbasedir , 'recvar-threshold-pkls' ) if not os . path . exists ( outdir ) : os . mkdir ( outdir ) with open ( os . path . join ( simbasedir , 'fakelcs-info.pkl' ) , 'rb' ) as infd : siminfo = pickle . load ( infd ) timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] magbinmedians = siminfo [ 'magrms' ] [ magcols [ 0 ] ] [ 'binned_sdssr_median' ] stetson_grid = np . linspace ( stetson_stdev_range [ 0 ] , stetson_stdev_range [ 1 ] , num = ngridpoints ) inveta_grid = np . linspace ( inveta_stdev_range [ 0 ] , inveta_stdev_range [ 1 ] , num = ngridpoints ) iqr_grid = np . linspace ( iqr_stdev_range [ 0 ] , iqr_stdev_range [ 1 ] , num = ngridpoints ) stet_inveta_iqr_grid = [ ] for stet in stetson_grid : for inveta in inveta_grid : for iqr in iqr_grid : grid_point = [ stet , inveta , iqr ] stet_inveta_iqr_grid . append ( grid_point ) grid_results = { 'stetson_grid' : stetson_grid , 'inveta_grid' : inveta_grid , 'iqr_grid' : iqr_grid , 'stet_inveta_iqr_grid' : stet_inveta_iqr_grid , 'magbinmedians' : magbinmedians , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'simbasedir' : os . path . abspath ( simbasedir ) , 'recovery' : [ ] } pool = mp . Pool ( ngridworkers ) for magbinmedian in magbinmedians : LOGINFO ( 'running stetson J-inveta grid-search ' 'for magbinmedian = %.3f...' % magbinmedian ) tasks = [ ( simbasedir , gp , magbinmedian ) for gp in stet_inveta_iqr_grid ] thisbin_results = pool . map ( magbin_varind_gridsearch_worker , tasks ) grid_results [ 'recovery' ] . append ( thisbin_results ) pool . close ( ) pool . join ( ) LOGINFO ( 'done.' ) with open ( os . path . join ( simbasedir , 'fakevar-recovery-per-magbin.pkl' ) , 'wb' ) as outfd : pickle . dump ( grid_results , outfd , pickle . HIGHEST_PROTOCOL ) return grid_results
3301	def element_content_as_string ( element ) : if len ( element ) == 0 : return element . text or "" stream = compat . StringIO ( ) for childnode in element : stream . write ( xml_to_bytes ( childnode , pretty_print = False ) + "\n" ) s = stream . getvalue ( ) stream . close ( ) return s
6138	def set_default_sim_param ( self , * args , ** kwargs ) : if len ( args ) is 1 and isinstance ( args [ 0 ] , SimulationParameter ) : self . __default_param = args [ 0 ] else : self . __default_param = SimulationParameter ( * args , ** kwargs ) return
6811	def pre_deploy ( self ) : for service in self . genv . services : service = service . strip ( ) . upper ( ) funcs = common . service_pre_deployers . get ( service ) if funcs : print ( 'Running pre-deployments for service %s...' % ( service , ) ) for func in funcs : func ( )
9014	def _fill_pattern_collection ( self , pattern_collection , values ) : pattern = values . get ( PATTERNS , [ ] ) for pattern_to_parse in pattern : parsed_pattern = self . _pattern ( pattern_to_parse ) pattern_collection . append ( parsed_pattern )
4261	def _restore_cache ( gallery ) : cachePath = os . path . join ( gallery . settings [ "destination" ] , ".exif_cache" ) try : if os . path . exists ( cachePath ) : with open ( cachePath , "rb" ) as cacheFile : gallery . exifCache = pickle . load ( cacheFile ) logger . debug ( "Loaded cache with %d entries" , len ( gallery . exifCache ) ) else : gallery . exifCache = { } except Exception as e : logger . warn ( "Could not load cache: %s" , e ) gallery . exifCache = { }
4096	def KIC ( N , rho , k ) : r from numpy import log , array res = log ( rho ) + 3. * ( k + 1. ) / float ( N ) return res
8757	def _validate_subnet_cidr ( context , network_id , new_subnet_cidr ) : if neutron_cfg . cfg . CONF . allow_overlapping_ips : return try : new_subnet_ipset = netaddr . IPSet ( [ new_subnet_cidr ] ) except TypeError : LOG . exception ( "Invalid or missing cidr: %s" % new_subnet_cidr ) raise n_exc . BadRequest ( resource = "subnet" , msg = "Invalid or missing cidr" ) filters = { 'network_id' : network_id , 'shared' : [ False ] } subnet_list = db_api . subnet_find ( context = context . elevated ( ) , ** filters ) for subnet in subnet_list : if ( netaddr . IPSet ( [ subnet . cidr ] ) & new_subnet_ipset ) : err_msg = ( _ ( "Requested subnet with cidr: %(cidr)s for " "network: %(network_id)s overlaps with another " "subnet" ) % { 'cidr' : new_subnet_cidr , 'network_id' : network_id } ) LOG . error ( _ ( "Validation for CIDR: %(new_cidr)s failed - " "overlaps with subnet %(subnet_id)s " "(CIDR: %(cidr)s)" ) , { 'new_cidr' : new_subnet_cidr , 'subnet_id' : subnet . id , 'cidr' : subnet . cidr } ) raise n_exc . InvalidInput ( error_message = err_msg )
12602	def col_values ( df , col_name ) : _check_cols ( df , [ col_name ] ) if 'O' in df [ col_name ] or pd . np . issubdtype ( df [ col_name ] . dtype , str ) : return [ nom . lower ( ) for nom in df [ pd . notnull ( df ) ] [ col_name ] if not pd . isnull ( nom ) ] else : return [ nom for nom in df [ pd . notnull ( df ) ] [ col_name ] if not pd . isnull ( nom ) ]
3368	def _valid_atoms ( model , expression ) : atoms = expression . atoms ( optlang . interface . Variable ) return all ( a . problem is model . solver for a in atoms )
13352	def monitor ( self , sleep = 5 ) : manager = FileModificationObjectManager ( ) timestamps = { } filebodies = { } for file in self . f_repository : timestamps [ file ] = self . _get_mtime ( file ) filebodies [ file ] = open ( file ) . read ( ) while True : for file in self . f_repository : mtime = timestamps [ file ] fbody = filebodies [ file ] modified = self . _check_modify ( file , mtime , fbody ) if not modified : continue new_mtime = self . _get_mtime ( file ) new_fbody = open ( file ) . read ( ) obj = FileModificationObject ( file , ( mtime , new_mtime ) , ( fbody , new_fbody ) ) timestamps [ file ] = new_mtime filebodies [ file ] = new_fbody manager . add_object ( obj ) yield obj time . sleep ( sleep )
9006	def to_svg ( self , converter = None ) : if converter is None : from knittingpattern . convert . InstructionSVGCache import default_svg_cache converter = default_svg_cache ( ) return converter . to_svg ( self )
9791	def find_matching ( cls , path , patterns ) : for pattern in patterns : if pattern . match ( path ) : yield pattern
7774	def _compute_response ( urp_hash , nonce , cnonce , nonce_count , authzid , digest_uri ) : logger . debug ( "_compute_response{0!r}" . format ( ( urp_hash , nonce , cnonce , nonce_count , authzid , digest_uri ) ) ) if authzid : a1 = b":" . join ( ( urp_hash , nonce , cnonce , authzid ) ) else : a1 = b":" . join ( ( urp_hash , nonce , cnonce ) ) a2 = b"AUTHENTICATE:" + digest_uri return b2a_hex ( _kd_value ( b2a_hex ( _h_value ( a1 ) ) , b":" . join ( ( nonce , nonce_count , cnonce , b"auth" , b2a_hex ( _h_value ( a2 ) ) ) ) ) )
6816	def optimize_wsgi_processes ( self ) : r = self . local_renderer r . env . wsgi_server_memory_gb = 8 verbose = self . verbose all_sites = list ( self . iter_sites ( site = ALL , setter = self . set_site_specifics ) )
7676	def intervals ( annotation , ** kwargs ) : times , labels = annotation . to_interval_values ( ) return mir_eval . display . labeled_intervals ( times , labels , ** kwargs )
7182	def parse_signature_type_comment ( type_comment ) : try : result = ast3 . parse ( type_comment , '<func_type>' , 'func_type' ) except SyntaxError : raise ValueError ( f"invalid function signature type comment: {type_comment!r}" ) assert isinstance ( result , ast3 . FunctionType ) if len ( result . argtypes ) == 1 : argtypes = result . argtypes [ 0 ] else : argtypes = result . argtypes return argtypes , result . returns
1205	def target_optimizer_arguments ( self ) : variables = self . target_network . get_variables ( ) + [ variable for name in sorted ( self . target_distributions ) for variable in self . target_distributions [ name ] . get_variables ( ) ] source_variables = self . network . get_variables ( ) + [ variable for name in sorted ( self . distributions ) for variable in self . distributions [ name ] . get_variables ( ) ] arguments = dict ( time = self . global_timestep , variables = variables , source_variables = source_variables ) if self . global_model is not None : arguments [ 'global_variables' ] = self . global_model . target_network . get_variables ( ) + [ variable for name in sorted ( self . global_model . target_distributions ) for variable in self . global_model . target_distributions [ name ] . get_variables ( ) ] return arguments
3228	def gce_list_aggregated ( service = None , key_name = 'name' , ** kwargs ) : resp_list = [ ] req = service . aggregatedList ( ** kwargs ) while req is not None : resp = req . execute ( ) for location , item in resp [ 'items' ] . items ( ) : if key_name in item : resp_list . extend ( item [ key_name ] ) req = service . aggregatedList_next ( previous_request = req , previous_response = resp ) return resp_list
7858	def make_error_response ( self , cond ) : if self . stanza_type in ( "result" , "error" ) : raise ValueError ( "Errors may not be generated for" " 'result' and 'error' iq" ) stanza = Iq ( stanza_type = "error" , from_jid = self . to_jid , to_jid = self . from_jid , stanza_id = self . stanza_id , error_cond = cond ) if self . _payload is None : self . decode_payload ( ) for payload in self . _payload : Stanza . add_payload ( stanza , payload ) return stanza
5339	def __create_dashboard_menu ( self , dash_menu , kibiter_major ) : logger . info ( "Adding dashboard menu" ) if kibiter_major == "6" : menu_resource = ".kibana/doc/metadashboard" mapping_resource = ".kibana/_mapping/doc" mapping = { "dynamic" : "true" } menu = { 'metadashboard' : dash_menu } else : menu_resource = ".kibana/metadashboard/main" mapping_resource = ".kibana/_mapping/metadashboard" mapping = { "dynamic" : "true" } menu = dash_menu menu_url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , menu_resource ) mapping_url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , mapping_resource ) logger . debug ( "Adding mapping for metadashboard" ) res = self . grimoire_con . put ( mapping_url , data = json . dumps ( mapping ) , headers = ES6_HEADER ) try : res . raise_for_status ( ) except requests . exceptions . HTTPError : logger . error ( "Couldn't create mapping for Kibiter menu." ) res = self . grimoire_con . post ( menu_url , data = json . dumps ( menu ) , headers = ES6_HEADER ) try : res . raise_for_status ( ) except requests . exceptions . HTTPError : logger . error ( "Couldn't create Kibiter menu." ) logger . error ( res . json ( ) ) raise
2496	def create_package_node ( self , package ) : package_node = BNode ( ) type_triple = ( package_node , RDF . type , self . spdx_namespace . Package ) self . graph . add ( type_triple ) self . handle_pkg_optional_fields ( package , package_node ) name_triple = ( package_node , self . spdx_namespace . name , Literal ( package . name ) ) self . graph . add ( name_triple ) down_loc_node = ( package_node , self . spdx_namespace . downloadLocation , self . to_special_value ( package . download_location ) ) self . graph . add ( down_loc_node ) verif_node = self . package_verif_node ( package ) verif_triple = ( package_node , self . spdx_namespace . packageVerificationCode , verif_node ) self . graph . add ( verif_triple ) conc_lic_node = self . license_or_special ( package . conc_lics ) conc_lic_triple = ( package_node , self . spdx_namespace . licenseConcluded , conc_lic_node ) self . graph . add ( conc_lic_triple ) decl_lic_node = self . license_or_special ( package . license_declared ) decl_lic_triple = ( package_node , self . spdx_namespace . licenseDeclared , decl_lic_node ) self . graph . add ( decl_lic_triple ) licenses_from_files_nodes = map ( lambda el : self . license_or_special ( el ) , package . licenses_from_files ) lic_from_files_predicate = self . spdx_namespace . licenseInfoFromFiles lic_from_files_triples = [ ( package_node , lic_from_files_predicate , node ) for node in licenses_from_files_nodes ] for triple in lic_from_files_triples : self . graph . add ( triple ) cr_text_node = self . to_special_value ( package . cr_text ) cr_text_triple = ( package_node , self . spdx_namespace . copyrightText , cr_text_node ) self . graph . add ( cr_text_triple ) self . handle_package_has_file ( package , package_node ) return package_node
2946	def get_ready_user_tasks ( self ) : return [ t for t in self . get_tasks ( Task . READY ) if not self . _is_engine_task ( t . task_spec ) ]
5399	def get_filtered_normalized_events ( self ) : user_image = google_v2_operations . get_action_image ( self . _op , _ACTION_USER_COMMAND ) need_ok = google_v2_operations . is_success ( self . _op ) events = { } for event in google_v2_operations . get_events ( self . _op ) : if self . _filter ( event ) : continue mapped , match = self . _map ( event ) name = mapped [ 'name' ] if name == 'ok' : if not need_ok or 'ok' in events : continue if name == 'pulling-image' : if match . group ( 1 ) != user_image : continue events [ name ] = mapped return sorted ( events . values ( ) , key = operator . itemgetter ( 'start-time' ) )
11670	def _get_Ks ( self ) : "Ks as an array and type-checked." Ks = as_integer_type ( self . Ks ) if Ks . ndim != 1 : raise TypeError ( "Ks should be 1-dim, got shape {}" . format ( Ks . shape ) ) if Ks . min ( ) < 1 : raise ValueError ( "Ks should be positive; got {}" . format ( Ks . min ( ) ) ) return Ks
13014	def remove_namespace ( doc , namespace ) : ns = u'{%s}' % namespace nsl = len ( ns ) for elem in doc . getiterator ( ) : if elem . tag . startswith ( ns ) : elem . tag = elem . tag [ nsl : ] elem . attrib [ 'oxmlns' ] = namespace
6260	def calc_global_bbox ( self , view_matrix , bbox_min , bbox_max ) : if self . matrix is not None : view_matrix = matrix44 . multiply ( self . matrix , view_matrix ) if self . mesh : bbox_min , bbox_max = self . mesh . calc_global_bbox ( view_matrix , bbox_min , bbox_max ) for child in self . children : bbox_min , bbox_max = child . calc_global_bbox ( view_matrix , bbox_min , bbox_max ) return bbox_min , bbox_max
7945	def _continue_connect ( self ) : try : self . _socket . connect ( self . _dst_addr ) except socket . error , err : logger . debug ( "Connect error: {0}" . format ( err ) ) if err . args [ 0 ] == errno . EISCONN : pass elif err . args [ 0 ] in BLOCKING_ERRORS : return None elif self . _dst_addrs : self . _set_state ( "connect" ) return None elif self . _dst_nameports : self . _set_state ( "resolve-hostname" ) return None else : self . _socket . close ( ) self . _socket = None self . _set_state ( "aborted" ) raise self . _connected ( )
12761	def process_data ( self ) : self . visibility = self . data [ : , : , 3 ] self . positions = self . data [ : , : , : 3 ] self . velocities = np . zeros_like ( self . positions ) + 1000 for frame_no in range ( 1 , len ( self . data ) - 1 ) : prev = self . data [ frame_no - 1 ] next = self . data [ frame_no + 1 ] for c in range ( self . num_markers ) : if - 1 < prev [ c , 3 ] < 100 and - 1 < next [ c , 3 ] < 100 : self . velocities [ frame_no , c ] = ( next [ c , : 3 ] - prev [ c , : 3 ] ) / ( 2 * self . world . dt ) self . cfms = np . zeros_like ( self . visibility ) + self . DEFAULT_CFM
2416	def write_annotation ( annotation , out ) : out . write ( '# Annotation\n\n' ) write_value ( 'Annotator' , annotation . annotator , out ) write_value ( 'AnnotationDate' , annotation . annotation_date_iso_format , out ) if annotation . has_comment : write_text_value ( 'AnnotationComment' , annotation . comment , out ) write_value ( 'AnnotationType' , annotation . annotation_type , out ) write_value ( 'SPDXREF' , annotation . spdx_id , out )
12913	def extend ( self , item ) : if self . meta_type == 'dict' : raise AssertionError ( 'Cannot extend to object of `dict` base type!' ) if self . meta_type == 'list' : self . _list . extend ( item ) return
4106	def MINEIGVAL ( T0 , T , TOL ) : M = len ( T ) eigval = 10 eigvalold = 1 eigvec = numpy . zeros ( M + 1 , dtype = complex ) for k in range ( 0 , M + 1 ) : eigvec [ k ] = 1 + 0j it = 0 maxit = 15 while abs ( eigvalold - eigval ) > TOL * eigvalold and it < maxit : it = it + 1 eigvalold = eigval eig = toeplitz . HERMTOEP ( T0 , T , eigvec ) SUM = 0 save = 0. + 0j for k in range ( 0 , M + 1 ) : SUM = SUM + eig [ k ] . real ** 2 + eig [ k ] . imag ** 2 save = save + eig [ k ] * eigvec [ k ] . conjugate ( ) SUM = 1. / SUM eigval = save . real * SUM for k in range ( 0 , M + 1 ) : eigvec [ k ] = SUM * eig [ k ] if it == maxit : print ( 'warning reached max number of iteration (%s)' % maxit ) return eigval , eigvec
13709	def is_threat ( self , result = None , harmless_age = None , threat_score = None , threat_type = None ) : harmless_age = harmless_age if harmless_age is not None else settings . CACHED_HTTPBL_HARMLESS_AGE threat_score = threat_score if threat_score is not None else settings . CACHED_HTTPBL_THREAT_SCORE threat_type = threat_type if threat_type is not None else - 1 result = result if result is not None else self . _last_result threat = False if result is not None : if result [ 'age' ] < harmless_age and result [ 'threat' ] > threat_score : threat = True if threat_type > - 1 : if result [ 'type' ] & threat_type : threat = True else : threat = False return threat
8318	def connect_table ( self , table , chunk , markup ) : k = markup . find ( chunk ) i = markup . rfind ( "\n=" , 0 , k ) j = markup . find ( "\n" , i + 1 ) paragraph_title = markup [ i : j ] . strip ( ) . strip ( "= " ) for paragraph in self . paragraphs : if paragraph . title == paragraph_title : paragraph . tables . append ( table ) table . paragraph = paragraph
6411	def heronian_mean ( nums ) : r mag = len ( nums ) rolling_sum = 0 for i in range ( mag ) : for j in range ( i , mag ) : if nums [ i ] == nums [ j ] : rolling_sum += nums [ i ] else : rolling_sum += ( nums [ i ] * nums [ j ] ) ** 0.5 return rolling_sum * 2 / ( mag * ( mag + 1 ) )
12435	def parse ( cls , path ) : for resource , pattern in cls . meta . patterns : match = re . match ( pattern , path ) if match is not None : return resource , match . groupdict ( ) , match . string [ match . end ( ) : ] return None if not cls . meta . patterns else False
1113	def _split_line ( self , data_list , line_num , text ) : if not line_num : data_list . append ( ( line_num , text ) ) return size = len ( text ) max = self . _wrapcolumn if ( size <= max ) or ( ( size - ( text . count ( '\0' ) * 3 ) ) <= max ) : data_list . append ( ( line_num , text ) ) return i = 0 n = 0 mark = '' while n < max and i < size : if text [ i ] == '\0' : i += 1 mark = text [ i ] i += 1 elif text [ i ] == '\1' : i += 1 mark = '' else : i += 1 n += 1 line1 = text [ : i ] line2 = text [ i : ] if mark : line1 = line1 + '\1' line2 = '\0' + mark + line2 data_list . append ( ( line_num , line1 ) ) self . _split_line ( data_list , '>' , line2 )
11138	def __clean_before_after ( self , stateBefore , stateAfter , keepNoneEmptyDirectory = True ) : errors = [ ] afterDict = { } [ afterDict . setdefault ( list ( aitem ) [ 0 ] , [ ] ) . append ( aitem ) for aitem in stateAfter ] for bitem in reversed ( stateBefore ) : relaPath = list ( bitem ) [ 0 ] basename = os . path . basename ( relaPath ) btype = bitem [ relaPath ] [ 'type' ] alist = afterDict . get ( relaPath , [ ] ) aitem = [ a for a in alist if a [ relaPath ] [ 'type' ] == btype ] if len ( aitem ) > 1 : errors . append ( "Multiple '%s' of type '%s' where found in '%s', this should never had happened. Please report issue" % ( basename , btype , relaPath ) ) continue if not len ( aitem ) : removeDirs = [ ] removeFiles = [ ] if btype == 'dir' : if not len ( relaPath ) : errors . append ( "Removing main repository directory is not allowed" ) continue removeDirs . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __dirInfo ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __dirLock ) ) elif btype == 'file' : removeFiles . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileInfo % basename ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileLock % basename ) ) else : removeDirs . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileInfo % basename ) ) for fpath in removeFiles : if os . path . isfile ( fpath ) : try : os . remove ( fpath ) except Exception as err : errors . append ( "Unable to clean file '%s' (%s)" % ( fpath , str ( err ) ) ) for dpath in removeDirs : if os . path . isdir ( dpath ) : if keepNoneEmptyDirectory or not len ( os . listdir ( dpath ) ) : try : shutil . rmtree ( dpath ) except Exception as err : errors . append ( "Unable to clean directory '%s' (%s)" % ( fpath , str ( err ) ) ) return len ( errors ) == 0 , errors
4558	def make_segments ( strip , length ) : if len ( strip ) % length : raise ValueError ( 'The length of strip must be a multiple of length' ) s = [ ] try : while True : s . append ( s [ - 1 ] . next ( length ) if s else Segment ( strip , length ) ) except ValueError : return s
2771	def get_object ( cls , api_token , id ) : load_balancer = cls ( token = api_token , id = id ) load_balancer . load ( ) return load_balancer
5439	def _interval_to_seconds ( interval , valid_units = 'smhdw' ) : if not interval : return None try : last_char = interval [ - 1 ] if last_char == 's' and 's' in valid_units : return str ( float ( interval [ : - 1 ] ) ) + 's' elif last_char == 'm' and 'm' in valid_units : return str ( float ( interval [ : - 1 ] ) * 60 ) + 's' elif last_char == 'h' and 'h' in valid_units : return str ( float ( interval [ : - 1 ] ) * 60 * 60 ) + 's' elif last_char == 'd' and 'd' in valid_units : return str ( float ( interval [ : - 1 ] ) * 60 * 60 * 24 ) + 's' elif last_char == 'w' and 'w' in valid_units : return str ( float ( interval [ : - 1 ] ) * 60 * 60 * 24 * 7 ) + 's' else : raise ValueError ( 'Unsupported units in interval string %s: %s' % ( interval , last_char ) ) except ( ValueError , OverflowError ) as e : raise ValueError ( 'Unable to parse interval string %s: %s' % ( interval , e ) )
3051	def FromResponse ( cls , response ) : kwargs = { 'device_code' : response [ 'device_code' ] , 'user_code' : response [ 'user_code' ] , } verification_url = response . get ( 'verification_url' , response . get ( 'verification_uri' ) ) if verification_url is None : raise OAuth2DeviceCodeError ( 'No verification_url provided in server response' ) kwargs [ 'verification_url' ] = verification_url kwargs . update ( { 'interval' : response . get ( 'interval' ) , 'user_code_expiry' : None , } ) if 'expires_in' in response : kwargs [ 'user_code_expiry' ] = ( _UTCNOW ( ) + datetime . timedelta ( seconds = int ( response [ 'expires_in' ] ) ) ) return cls ( ** kwargs )
5805	def get_dh_params_length ( server_handshake_bytes ) : output = None dh_params_bytes = None for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type == b'\x0c' : dh_params_bytes = message_data break if dh_params_bytes : break if dh_params_bytes : output = int_from_bytes ( dh_params_bytes [ 0 : 2 ] ) * 8 return output
1267	def _fly ( self , board , layers , things , the_plot ) : if ( self . character in the_plot [ 'bunker_hitters' ] or self . character in the_plot [ 'marauder_hitters' ] ) : return self . _teleport ( ( - 1 , - 1 ) ) self . _north ( board , the_plot )
8367	def output_closure ( self , target , file_number = None ) : def output_context ( ctx ) : target_ctx = target target_ctx . set_source_surface ( ctx . get_target ( ) ) target_ctx . paint ( ) return target_ctx def output_surface ( ctx ) : target_ctx = cairo . Context ( target ) target_ctx . set_source_surface ( ctx . get_target ( ) ) target_ctx . paint ( ) return target_ctx def output_file ( ctx ) : root , extension = os . path . splitext ( target ) if file_number : filename = '%s_%04d%s' % ( root , file_number , extension ) else : filename = target extension = extension . lower ( ) if extension == '.png' : surface = ctx . get_target ( ) surface . write_to_png ( target ) elif extension == '.pdf' : target_ctx = cairo . Context ( cairo . PDFSurface ( filename , * self . size_or_default ( ) ) ) target_ctx . set_source_surface ( ctx . get_target ( ) ) target_ctx . paint ( ) elif extension in ( '.ps' , '.eps' ) : target_ctx = cairo . Context ( cairo . PSSurface ( filename , * self . size_or_default ( ) ) ) if extension == '.eps' : target_ctx . set_eps ( extension = '.eps' ) target_ctx . set_source_surface ( ctx . get_target ( ) ) target_ctx . paint ( ) elif extension == '.svg' : target_ctx = cairo . Context ( cairo . SVGSurface ( filename , * self . size_or_default ( ) ) ) target_ctx . set_source_surface ( ctx . get_target ( ) ) target_ctx . paint ( ) return filename if isinstance ( target , cairo . Context ) : return output_context elif isinstance ( target , cairo . Surface ) : return output_surface else : return output_file
13099	def getAnnotations ( self , targets , wildcard = "." , include = None , exclude = None , limit = None , start = 1 , expand = False , ** kwargs ) : return 0 , [ ]
4393	def adsSyncWriteReqEx ( port , address , index_group , index_offset , value , plc_data_type ) : sync_write_request = _adsDLL . AdsSyncWriteReqEx ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) index_group_c = ctypes . c_ulong ( index_group ) index_offset_c = ctypes . c_ulong ( index_offset ) if plc_data_type == PLCTYPE_STRING : data = ctypes . c_char_p ( value . encode ( "utf-8" ) ) data_pointer = data data_length = len ( data_pointer . value ) + 1 else : if type ( plc_data_type ) . __name__ == "PyCArrayType" : data = plc_data_type ( * value ) else : data = plc_data_type ( value ) data_pointer = ctypes . pointer ( data ) data_length = ctypes . sizeof ( data ) error_code = sync_write_request ( port , ams_address_pointer , index_group_c , index_offset_c , data_length , data_pointer , ) if error_code : raise ADSError ( error_code )
3902	def _input_filter ( self , keys , _ ) : if keys == [ self . _keys [ 'menu' ] ] : if self . _urwid_loop . widget == self . _tabbed_window : self . _show_menu ( ) else : self . _hide_menu ( ) elif keys == [ self . _keys [ 'quit' ] ] : self . _coroutine_queue . put ( self . _client . disconnect ( ) ) else : return keys
8577	def get_server ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s?depth=%s' % ( datacenter_id , server_id , str ( depth ) ) ) return response
7210	def stdout ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get stdout.' ) if self . batch_values : raise NotImplementedError ( "Query Each Workflow Id within the Batch Workflow for stdout." ) wf = self . workflow . get ( self . id ) stdout_list = [ ] for task in wf [ 'tasks' ] : stdout_list . append ( { 'id' : task [ 'id' ] , 'taskType' : task [ 'taskType' ] , 'name' : task [ 'name' ] , 'stdout' : self . workflow . get_stdout ( self . id , task [ 'id' ] ) } ) return stdout_list
10289	def enrich_reactions ( graph : BELGraph ) : nodes = list ( get_nodes_by_function ( graph , REACTION ) ) for u in nodes : for v in u . reactants : graph . add_has_reactant ( u , v ) for v in u . products : graph . add_has_product ( u , v )
10572	def walk_depth ( path , max_depth = float ( 'inf' ) ) : start_level = os . path . abspath ( path ) . count ( os . path . sep ) for dir_entry in os . walk ( path ) : root , dirs , _ = dir_entry level = root . count ( os . path . sep ) - start_level yield dir_entry if level >= max_depth : dirs [ : ] = [ ]
13020	def _execute ( self , query , commit = False , working_columns = None ) : log . debug ( "RawlBase._execute()" ) result = [ ] if working_columns is None : working_columns = self . columns with RawlConnection ( self . dsn ) as conn : query_id = random . randrange ( 9999 ) curs = conn . cursor ( ) try : log . debug ( "Executing(%s): %s" % ( query_id , query . as_string ( curs ) ) ) except : log . exception ( "LOGGING EXCEPTION LOL" ) curs . execute ( query ) log . debug ( "Executed" ) if commit == True : log . debug ( "COMMIT(%s)" % query_id ) conn . commit ( ) log . debug ( "curs.rowcount: %s" % curs . rowcount ) if curs . rowcount > 0 : result_rows = curs . fetchall ( ) for row in result_rows : i = 0 row_dict = { } for col in working_columns : try : col = col . replace ( '.' , '_' ) row_dict [ col ] = row [ i ] except IndexError : pass i += 1 log . debug ( "Appending dict to result: %s" % row_dict ) rr = RawlResult ( working_columns , row_dict ) result . append ( rr ) curs . close ( ) return result
6589	def open ( self ) : self . workingArea . open ( ) self . runid_pkgidx_map = { } self . runid_to_return = deque ( )
4524	def save ( self , project_file = '' ) : self . _request_project_file ( project_file ) data_file . dump ( self . desc . as_dict ( ) , self . project_file )
7366	def run_multiple_commands_redirect_stdout ( multiple_args_dict , print_commands = True , process_limit = - 1 , polling_freq = 0.5 , ** kwargs ) : assert len ( multiple_args_dict ) > 0 assert all ( len ( args ) > 0 for args in multiple_args_dict . values ( ) ) assert all ( hasattr ( f , 'name' ) for f in multiple_args_dict . keys ( ) ) if process_limit < 0 : logger . debug ( "Using %d processes" % cpu_count ( ) ) process_limit = cpu_count ( ) start_time = time . time ( ) processes = Queue ( maxsize = process_limit ) def add_to_queue ( process ) : process . start ( ) if print_commands : handler = logging . FileHandler ( process . redirect_stdout_file . name ) handler . setLevel ( logging . DEBUG ) logger . addHandler ( handler ) logger . debug ( " " . join ( process . args ) ) logger . removeHandler ( handler ) processes . put ( process ) for f , args in multiple_args_dict . items ( ) : p = AsyncProcess ( args , redirect_stdout_file = f , ** kwargs ) if not processes . full ( ) : add_to_queue ( p ) else : while processes . full ( ) : to_remove = [ ] for possibly_done in processes . queue : if possibly_done . poll ( ) is not None : possibly_done . wait ( ) to_remove . append ( possibly_done ) if to_remove : for process_to_remove in to_remove : processes . queue . remove ( process_to_remove ) break time . sleep ( polling_freq ) add_to_queue ( p ) while not processes . empty ( ) : processes . get ( ) . wait ( ) elapsed_time = time . time ( ) - start_time logger . info ( "Ran %d commands in %0.4f seconds" , len ( multiple_args_dict ) , elapsed_time )
3241	def _get_base ( server_certificate , ** conn ) : server_certificate [ '_version' ] = 1 cert_details = get_server_certificate_api ( server_certificate [ 'ServerCertificateName' ] , ** conn ) if cert_details : server_certificate . update ( cert_details [ 'ServerCertificateMetadata' ] ) server_certificate [ 'CertificateBody' ] = cert_details [ 'CertificateBody' ] server_certificate [ 'CertificateChain' ] = cert_details . get ( 'CertificateChain' , None ) server_certificate [ 'UploadDate' ] = get_iso_string ( server_certificate [ 'UploadDate' ] ) server_certificate [ 'Expiration' ] = get_iso_string ( server_certificate [ 'Expiration' ] ) return server_certificate
201	def pad_to_aspect_ratio ( self , aspect_ratio , mode = "constant" , cval = 0.0 , return_pad_amounts = False ) : arr_padded , pad_amounts = ia . pad_to_aspect_ratio ( self . arr , aspect_ratio = aspect_ratio , mode = mode , cval = cval , return_pad_amounts = True ) segmap = SegmentationMapOnImage ( arr_padded , shape = self . shape ) segmap . input_was = self . input_was if return_pad_amounts : return segmap , pad_amounts else : return segmap
6240	def render_lights_debug ( self , camera_matrix , projection ) : self . ctx . enable ( moderngl . BLEND ) self . ctx . blend_func = moderngl . SRC_ALPHA , moderngl . ONE_MINUS_SRC_ALPHA for light in self . point_lights : m_mv = matrix44 . multiply ( light . matrix , camera_matrix ) light_size = light . radius self . debug_shader [ "m_proj" ] . write ( projection . tobytes ( ) ) self . debug_shader [ "m_mv" ] . write ( m_mv . astype ( 'f4' ) . tobytes ( ) ) self . debug_shader [ "size" ] . value = light_size self . unit_cube . render ( self . debug_shader , mode = moderngl . LINE_STRIP ) self . ctx . disable ( moderngl . BLEND )
11388	def parse ( self ) : if self . parsed : return self . callbacks = { } regex = re . compile ( "^{}_?" . format ( self . function_name ) , flags = re . I ) mains = set ( ) body = self . body ast_tree = ast . parse ( self . body , self . path ) for n in ast_tree . body : if hasattr ( n , 'name' ) : if regex . match ( n . name ) : mains . add ( n . name ) if hasattr ( n , 'value' ) : ns = n . value if hasattr ( ns , 'id' ) : if regex . match ( ns . id ) : mains . add ( ns . id ) if hasattr ( n , 'targets' ) : ns = n . targets [ 0 ] if hasattr ( ns , 'id' ) : if regex . match ( ns . id ) : mains . add ( ns . id ) if hasattr ( n , 'names' ) : for ns in n . names : if hasattr ( ns , 'name' ) : if regex . match ( ns . name ) : mains . add ( ns . name ) if getattr ( ns , 'asname' , None ) : if regex . match ( ns . asname ) : mains . add ( ns . asname ) if len ( mains ) > 0 : module = self . module for function_name in mains : cb = getattr ( module , function_name , None ) if cb and callable ( cb ) : self . callbacks [ function_name ] = cb else : raise ParseError ( "no main function found" ) self . parsed = True return len ( self . callbacks ) > 0
7970	def _add_timeout_handler ( self , handler ) : self . timeout_handlers . append ( handler ) if self . event_thread is None : return self . _run_timeout_threads ( handler )
4649	def json ( self ) : if not self . _is_constructed ( ) or self . _is_require_reconstruction ( ) : self . constructTx ( ) return dict ( self )
3842	async def sync_recent_conversations ( self , sync_recent_conversations_request ) : response = hangouts_pb2 . SyncRecentConversationsResponse ( ) await self . _pb_request ( 'conversations/syncrecentconversations' , sync_recent_conversations_request , response ) return response
4196	def TOEPLITZ ( T0 , TC , TR , Z ) : assert len ( TC ) > 0 assert len ( TC ) == len ( TR ) M = len ( TC ) X = numpy . zeros ( M + 1 , dtype = complex ) A = numpy . zeros ( M , dtype = complex ) B = numpy . zeros ( M , dtype = complex ) P = T0 if P == 0 : raise ValueError ( "P must be different from zero" ) if P == 0 : raise ValueError ( "P must be different from zero" ) X [ 0 ] = Z [ 0 ] / T0 for k in range ( 0 , M ) : save1 = TC [ k ] save2 = TR [ k ] beta = X [ 0 ] * TC [ k ] if k == 0 : temp1 = - save1 / P temp2 = - save2 / P else : for j in range ( 0 , k ) : save1 = save1 + A [ j ] * TC [ k - j - 1 ] save2 = save2 + B [ j ] * TR [ k - j - 1 ] beta = beta + X [ j + 1 ] * TC [ k - j - 1 ] temp1 = - save1 / P temp2 = - save2 / P P = P * ( 1. - ( temp1 * temp2 ) ) if P <= 0 : raise ValueError ( "singular matrix" ) A [ k ] = temp1 B [ k ] = temp2 alpha = ( Z [ k + 1 ] - beta ) / P if k == 0 : X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * B [ k - j ] continue for j in range ( 0 , k ) : kj = k - j - 1 save1 = A [ j ] A [ j ] = save1 + temp1 * B [ kj ] B [ kj ] = B [ kj ] + temp2 * save1 X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * B [ k - j ] return X
9489	def generate_bytecode_from_obb ( obb : object , previous : bytes ) -> bytes : if isinstance ( obb , pyte . superclasses . _PyteOp ) : return obb . to_bytes ( previous ) elif isinstance ( obb , ( pyte . superclasses . _PyteAugmentedComparator , pyte . superclasses . _PyteAugmentedValidator . _FakeMathematicalOP ) ) : return obb . to_bytes ( previous ) elif isinstance ( obb , pyte . superclasses . _PyteAugmentedValidator ) : obb . validate ( ) return obb . to_load ( ) elif isinstance ( obb , int ) : return obb . to_bytes ( ( obb . bit_length ( ) + 7 ) // 8 , byteorder = "little" ) or b'' elif isinstance ( obb , bytes ) : return obb else : raise TypeError ( "`{}` was not a valid bytecode-encodable item" . format ( obb ) )
7637	def smkdirs ( dpath , mode = 0o777 ) : if not os . path . exists ( dpath ) : os . makedirs ( dpath , mode = mode )
3744	def _round_whole_even ( i ) : r if i % .5 == 0 : if ( i + 0.5 ) % 2 == 0 : i = i + 0.5 else : i = i - 0.5 else : i = round ( i , 0 ) return int ( i )
1358	def get_argument_component ( self ) : try : component = self . get_argument ( constants . PARAM_COMPONENT ) return component except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
8934	def auto_detect ( workdir ) : if os . path . isdir ( os . path . join ( workdir , '.git' ) ) and os . path . isfile ( os . path . join ( workdir , '.git' , 'HEAD' ) ) : return 'git' return 'unknown'
1370	def get_subparser ( parser , command ) : subparsers_actions = [ action for action in parser . _actions if isinstance ( action , argparse . _SubParsersAction ) ] for subparsers_action in subparsers_actions : for choice , subparser in subparsers_action . choices . items ( ) : if choice == command : return subparser return None
10005	def clear_descendants ( self , source , clear_source = True ) : removed = self . cellgraph . clear_descendants ( source , clear_source ) for node in removed : del node [ OBJ ] . data [ node [ KEY ] ]
6500	def perform_search ( search_term , user = None , size = 10 , from_ = 0 , course_id = None ) : ( field_dictionary , filter_dictionary , exclude_dictionary ) = SearchFilterGenerator . generate_field_filters ( user = user , course_id = course_id ) searcher = SearchEngine . get_search_engine ( getattr ( settings , "COURSEWARE_INDEX_NAME" , "courseware_index" ) ) if not searcher : raise NoSearchEngineError ( "No search engine specified in settings.SEARCH_ENGINE" ) results = searcher . search_string ( search_term , field_dictionary = field_dictionary , filter_dictionary = filter_dictionary , exclude_dictionary = exclude_dictionary , size = size , from_ = from_ , doc_type = "courseware_content" , ) for result in results [ "results" ] : result [ "data" ] = SearchResultProcessor . process_result ( result [ "data" ] , search_term , user ) results [ "access_denied_count" ] = len ( [ r for r in results [ "results" ] if r [ "data" ] is None ] ) results [ "results" ] = [ r for r in results [ "results" ] if r [ "data" ] is not None ] return results
4963	def clean_course ( self ) : course_id = self . cleaned_data [ self . Fields . COURSE ] . strip ( ) if not course_id : return None try : client = EnrollmentApiClient ( ) return client . get_course_details ( course_id ) except ( HttpClientError , HttpServerError ) : raise ValidationError ( ValidationMessages . INVALID_COURSE_ID . format ( course_id = course_id ) )
101	def draw_text ( img , y , x , text , color = ( 0 , 255 , 0 ) , size = 25 ) : do_assert ( img . dtype in [ np . uint8 , np . float32 ] ) input_dtype = img . dtype if img . dtype == np . float32 : img = img . astype ( np . uint8 ) img = PIL_Image . fromarray ( img ) font = PIL_ImageFont . truetype ( DEFAULT_FONT_FP , size ) context = PIL_ImageDraw . Draw ( img ) context . text ( ( x , y ) , text , fill = tuple ( color ) , font = font ) img_np = np . asarray ( img ) if not img_np . flags [ "WRITEABLE" ] : try : img_np . setflags ( write = True ) except ValueError as ex : if "cannot set WRITEABLE flag to True of this array" in str ( ex ) : img_np = np . copy ( img_np ) if img_np . dtype != input_dtype : img_np = img_np . astype ( input_dtype ) return img_np
12373	def get_first ( ) : client = po . connect ( ) all_droplets = client . droplets . list ( ) id = all_droplets [ 0 ] [ 'id' ] return client . droplets . get ( id )
1765	def pop_int ( self , force = False ) : value = self . read_int ( self . STACK , force = force ) self . STACK += self . address_bit_size // 8 return value
13853	def run ( self ) : if not self . device : return try : data = "" while ( self . do_run ) : try : if ( self . device . inWaiting ( ) > 1 ) : l = self . device . readline ( ) [ : - 2 ] l = l . decode ( "UTF-8" ) if ( l == "[" ) : data = "[" elif ( l == "]" ) and ( len ( data ) > 4 ) and ( data [ 0 ] == "[" ) : data = data + "]" self . store . register_json ( data ) self . age ( ) elif ( l [ 0 : 3 ] == " {" ) : data = data + " " + l else : sleep ( 1 ) self . age ( ) except ( UnicodeDecodeError , ValueError ) : data = "" self . age ( ) except serial . serialutil . SerialException : print ( "Could not connect to the serial line at " + self . device_name )
2238	def _extension_module_tags ( ) : import sysconfig tags = [ ] if six . PY2 : multiarch = sysconfig . get_config_var ( 'MULTIARCH' ) if multiarch is not None : tags . append ( multiarch ) else : tags . append ( sysconfig . get_config_var ( 'SOABI' ) ) tags . append ( 'abi3' ) tags = [ t for t in tags if t ] return tags
10789	def guess_invert ( st ) : pos = st . obj_get_positions ( ) pxinds_ar = np . round ( pos ) . astype ( 'int' ) inim = st . ishape . translate ( - st . pad ) . contains ( pxinds_ar ) pxinds_tuple = tuple ( pxinds_ar [ inim ] . T ) pxvals = st . data [ pxinds_tuple ] invert = np . median ( pxvals ) < np . median ( st . data ) return invert
4278	def process_dir ( self , album , force = False ) : for f in album : if isfile ( f . dst_path ) and not force : self . logger . info ( "%s exists - skipping" , f . filename ) self . stats [ f . type + '_skipped' ] += 1 else : self . stats [ f . type ] += 1 yield ( f . type , f . path , f . filename , f . src_path , album . dst_path , self . settings )
12877	def many_until ( these , term ) : results = [ ] while True : stop , result = choice ( _tag ( True , term ) , _tag ( False , these ) ) if stop : return results , result else : results . append ( result )
5766	def _setup_evp_encrypt_decrypt ( cipher , data ) : evp_cipher = { 'aes128' : libcrypto . EVP_aes_128_cbc , 'aes192' : libcrypto . EVP_aes_192_cbc , 'aes256' : libcrypto . EVP_aes_256_cbc , 'rc2' : libcrypto . EVP_rc2_cbc , 'rc4' : libcrypto . EVP_rc4 , 'des' : libcrypto . EVP_des_cbc , 'tripledes_2key' : libcrypto . EVP_des_ede_cbc , 'tripledes_3key' : libcrypto . EVP_des_ede3_cbc , } [ cipher ] ( ) if cipher == 'rc4' : buffer_size = len ( data ) else : block_size = { 'aes128' : 16 , 'aes192' : 16 , 'aes256' : 16 , 'rc2' : 8 , 'des' : 8 , 'tripledes_2key' : 8 , 'tripledes_3key' : 8 , } [ cipher ] buffer_size = block_size * int ( math . ceil ( len ( data ) / block_size ) ) return ( evp_cipher , buffer_size )
9983	def replace_funcname ( source : str , name : str ) : lines = source . splitlines ( ) atok = asttokens . ASTTokens ( source , parse = True ) for node in ast . walk ( atok . tree ) : if isinstance ( node , ast . FunctionDef ) : break i = node . first_token . index for i in range ( node . first_token . index , node . last_token . index ) : if ( atok . tokens [ i ] . type == token . NAME and atok . tokens [ i ] . string == "def" ) : break lineno , col_begin = atok . tokens [ i + 1 ] . start lineno_end , col_end = atok . tokens [ i + 1 ] . end assert lineno == lineno_end lines [ lineno - 1 ] = ( lines [ lineno - 1 ] [ : col_begin ] + name + lines [ lineno - 1 ] [ col_end : ] ) return "\n" . join ( lines ) + "\n"
3442	def to_json ( model , sort = False , ** kwargs ) : obj = model_to_dict ( model , sort = sort ) obj [ u"version" ] = JSON_SPEC return json . dumps ( obj , allow_nan = False , ** kwargs )
1444	def deserialize_data_tuple ( self , stream_id , source_component , latency_in_ns ) : self . update_count ( self . TUPLE_DESERIALIZATION_TIME_NS , incr_by = latency_in_ns , key = stream_id ) global_stream_id = source_component + "/" + stream_id self . update_count ( self . TUPLE_DESERIALIZATION_TIME_NS , incr_by = latency_in_ns , key = global_stream_id )
11678	def run ( self ) : logger . info ( u'Started listening' ) while not self . _stop : xml = self . _readxml ( ) if xml is None : break if not self . modelize : logger . info ( u'Raw xml: %s' % xml ) self . results . put ( xml ) continue if xml . tag == 'RECOGOUT' : sentence = Sentence . from_shypo ( xml . find ( 'SHYPO' ) , self . encoding ) logger . info ( u'Modelized recognition: %r' % sentence ) self . results . put ( sentence ) else : logger . info ( u'Unmodelized xml: %s' % xml ) self . results . put ( xml ) logger . info ( u'Stopped listening' )
3046	def _do_refresh_request ( self , http ) : body = self . _generate_refresh_request_body ( ) headers = self . _generate_refresh_request_headers ( ) logger . info ( 'Refreshing access_token' ) resp , content = transport . request ( http , self . token_uri , method = 'POST' , body = body , headers = headers ) content = _helpers . _from_bytes ( content ) if resp . status == http_client . OK : d = json . loads ( content ) self . token_response = d self . access_token = d [ 'access_token' ] self . refresh_token = d . get ( 'refresh_token' , self . refresh_token ) if 'expires_in' in d : delta = datetime . timedelta ( seconds = int ( d [ 'expires_in' ] ) ) self . token_expiry = delta + _UTCNOW ( ) else : self . token_expiry = None if 'id_token' in d : self . id_token = _extract_id_token ( d [ 'id_token' ] ) self . id_token_jwt = d [ 'id_token' ] else : self . id_token = None self . id_token_jwt = None self . invalid = False if self . store : self . store . locked_put ( self ) else : logger . info ( 'Failed to retrieve access token: %s' , content ) error_msg = 'Invalid response {0}.' . format ( resp . status ) try : d = json . loads ( content ) if 'error' in d : error_msg = d [ 'error' ] if 'error_description' in d : error_msg += ': ' + d [ 'error_description' ] self . invalid = True if self . store is not None : self . store . locked_put ( self ) except ( TypeError , ValueError ) : pass raise HttpAccessTokenRefreshError ( error_msg , status = resp . status )
10910	def name_globals ( s , remove_params = None ) : all_params = s . params for p in s . param_particle ( np . arange ( s . obj_get_positions ( ) . shape [ 0 ] ) ) : all_params . remove ( p ) if remove_params is not None : for p in set ( remove_params ) : all_params . remove ( p ) return all_params
13223	def dinner ( self , message = "Dinner is served" , shout : bool = False ) : return self . helper . output ( message , shout )
8880	def predict_proba ( self , X ) : check_is_fitted ( self , [ 'tree' ] ) X = check_array ( X ) return self . tree . query ( X ) [ 0 ] . flatten ( )
6025	def geometry_from_grid ( self , grid , buffer = 1e-8 ) : y_min = np . min ( grid [ : , 0 ] ) - buffer y_max = np . max ( grid [ : , 0 ] ) + buffer x_min = np . min ( grid [ : , 1 ] ) - buffer x_max = np . max ( grid [ : , 1 ] ) + buffer pixel_scales = ( float ( ( y_max - y_min ) / self . shape [ 0 ] ) , float ( ( x_max - x_min ) / self . shape [ 1 ] ) ) origin = ( ( y_max + y_min ) / 2.0 , ( x_max + x_min ) / 2.0 ) pixel_neighbors , pixel_neighbors_size = self . neighbors_from_pixelization ( ) return self . Geometry ( shape = self . shape , pixel_scales = pixel_scales , origin = origin , pixel_neighbors = pixel_neighbors , pixel_neighbors_size = pixel_neighbors_size )
5079	def strip_html_tags ( text , allowed_tags = None ) : if text is None : return if allowed_tags is None : allowed_tags = ALLOWED_TAGS return bleach . clean ( text , tags = allowed_tags , attributes = [ 'id' , 'class' , 'style' , 'href' , 'title' ] , strip = True )
11990	def const_equal ( str_a , str_b ) : if len ( str_a ) != len ( str_b ) : return False result = True for i in range ( len ( str_a ) ) : result &= ( str_a [ i ] == str_b [ i ] ) return result
10492	def doubleClickDragMouseButtonLeft ( self , coord , dest_coord , interval = 0.5 ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , dest_coord = dest_coord ) self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , dest_coord = dest_coord , clickCount = 2 ) self . _postQueuedEvents ( interval = interval )
3367	def linear_reaction_coefficients ( model , reactions = None ) : linear_coefficients = { } reactions = model . reactions if not reactions else reactions try : objective_expression = model . solver . objective . expression coefficients = objective_expression . as_coefficients_dict ( ) except AttributeError : return linear_coefficients for rxn in reactions : forward_coefficient = coefficients . get ( rxn . forward_variable , 0 ) reverse_coefficient = coefficients . get ( rxn . reverse_variable , 0 ) if forward_coefficient != 0 : if forward_coefficient == - reverse_coefficient : linear_coefficients [ rxn ] = float ( forward_coefficient ) return linear_coefficients
5537	def get_raw_output ( self , tile , _baselevel_readonly = False ) : if not isinstance ( tile , ( BufferedTile , tuple ) ) : raise TypeError ( "'tile' must be a tuple or BufferedTile" ) if isinstance ( tile , tuple ) : tile = self . config . output_pyramid . tile ( * tile ) if _baselevel_readonly : tile = self . config . baselevels [ "tile_pyramid" ] . tile ( * tile . id ) if tile . zoom not in self . config . zoom_levels : return self . config . output . empty ( tile ) if tile . crs != self . config . process_pyramid . crs : raise NotImplementedError ( "reprojection between processes not yet implemented" ) if self . config . mode == "memory" : process_tile = self . config . process_pyramid . intersecting ( tile ) [ 0 ] return self . _extract ( in_tile = process_tile , in_data = self . _execute_using_cache ( process_tile ) , out_tile = tile ) process_tile = self . config . process_pyramid . intersecting ( tile ) [ 0 ] if tile . pixelbuffer > self . config . output . pixelbuffer : output_tiles = list ( self . config . output_pyramid . tiles_from_bounds ( tile . bounds , tile . zoom ) ) else : output_tiles = self . config . output_pyramid . intersecting ( tile ) if self . config . mode == "readonly" or _baselevel_readonly : if self . config . output . tiles_exist ( process_tile ) : return self . _read_existing_output ( tile , output_tiles ) else : return self . config . output . empty ( tile ) elif self . config . mode == "continue" and not _baselevel_readonly : if self . config . output . tiles_exist ( process_tile ) : return self . _read_existing_output ( tile , output_tiles ) else : return self . _process_and_overwrite_output ( tile , process_tile ) elif self . config . mode == "overwrite" and not _baselevel_readonly : return self . _process_and_overwrite_output ( tile , process_tile )
2775	def add_droplets ( self , droplet_ids ) : return self . get_data ( "load_balancers/%s/droplets/" % self . id , type = POST , params = { "droplet_ids" : droplet_ids } )
12180	def get_author_and_version ( package ) : init_py = open ( os . path . join ( package , '__init__.py' ) ) . read ( ) author = re . search ( "__author__ = ['\"]([^'\"]+)['\"]" , init_py ) . group ( 1 ) version = re . search ( "__version__ = ['\"]([^'\"]+)['\"]" , init_py ) . group ( 1 ) return author , version
8598	def list_shares ( self , group_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s/shares?depth=%s' % ( group_id , str ( depth ) ) ) return response
3714	def calculate ( self , T , method ) : r if method == CRC_INORG_S : Vms = self . CRC_INORG_S_Vm elif method in self . tabular_data : Vms = self . interpolate ( T , method ) return Vms
9610	def execute ( self , command , data = { } ) : method , uri = command try : path = self . _formatter . format_map ( uri , data ) body = self . _formatter . get_unused_kwargs ( ) url = "{0}{1}" . format ( self . _url , path ) return self . _request ( method , url , body ) except KeyError as err : LOGGER . debug ( 'Endpoint {0} is missing argument {1}' . format ( uri , err ) ) raise
3936	def set ( self , refresh_token ) : logger . info ( 'Saving refresh_token to %s' , repr ( self . _filename ) ) try : with open ( self . _filename , 'w' ) as f : f . write ( refresh_token ) except IOError as e : logger . warning ( 'Failed to save refresh_token: %s' , e )
11707	def generate_gamete ( self , egg_or_sperm_word ) : p_rate_of_mutation = [ 0.9 , 0.1 ] should_use_mutant_pool = ( npchoice ( [ 0 , 1 ] , 1 , p = p_rate_of_mutation ) [ 0 ] == 1 ) if should_use_mutant_pool : pool = tokens . secondary_tokens else : pool = tokens . primary_tokens return get_matches ( egg_or_sperm_word , pool , 23 )
6944	def jhk_to_vmag ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , VJHK , VJH , VJK , VHK , VJ , VH , VK )
1480	def _start_processes ( self , commands ) : Log . info ( "Start processes" ) processes_to_monitor = { } for ( name , command ) in commands . items ( ) : p = self . _run_process ( name , command ) processes_to_monitor [ p . pid ] = ProcessInfo ( p , name , command ) log_pid_for_process ( name , p . pid ) with self . process_lock : self . processes_to_monitor . update ( processes_to_monitor )
510	def _updateMinDutyCycles ( self ) : if self . _globalInhibition or self . _inhibitionRadius > self . _numInputs : self . _updateMinDutyCyclesGlobal ( ) else : self . _updateMinDutyCyclesLocal ( )
12171	def count ( self , event ) : return len ( self . _listeners [ event ] ) + len ( self . _once [ event ] )
1899	def can_be_true ( self , constraints , expression ) : if isinstance ( expression , bool ) : if not expression : return expression else : self . _reset ( constraints ) return self . _is_sat ( ) assert isinstance ( expression , Bool ) with constraints as temp_cs : temp_cs . add ( expression ) self . _reset ( temp_cs . to_string ( related_to = expression ) ) return self . _is_sat ( )
10849	def set_verbosity ( self , verbosity = 'vvv' , handlers = None ) : self . verbosity = sanitize ( verbosity ) self . set_level ( v2l [ verbosity ] , handlers = handlers ) self . set_formatter ( v2f [ verbosity ] , handlers = handlers )
7922	def __prepare_resource ( data ) : if not data : return None data = unicode ( data ) try : resource = RESOURCEPREP . prepare ( data ) except StringprepError , err : raise JIDError ( u"Local part invalid: {0}" . format ( err ) ) if len ( resource . encode ( "utf-8" ) ) > 1023 : raise JIDError ( "Resource name too long" ) return resource
1182	def fast_search ( self , pattern_codes ) : flags = pattern_codes [ 2 ] prefix_len = pattern_codes [ 5 ] prefix_skip = pattern_codes [ 6 ] prefix = pattern_codes [ 7 : 7 + prefix_len ] overlap = pattern_codes [ 7 + prefix_len - 1 : pattern_codes [ 1 ] + 1 ] pattern_codes = pattern_codes [ pattern_codes [ 1 ] + 1 : ] i = 0 string_position = self . string_position while string_position < self . end : while True : if ord ( self . string [ string_position ] ) != prefix [ i ] : if i == 0 : break else : i = overlap [ i ] else : i += 1 if i == prefix_len : self . start = string_position + 1 - prefix_len self . string_position = string_position + 1 - prefix_len + prefix_skip if flags & SRE_INFO_LITERAL : return True if self . match ( pattern_codes [ 2 * prefix_skip : ] ) : return True i = overlap [ i ] break string_position += 1 return False
9849	def _load_plt ( self , filename ) : g = gOpenMol . Plt ( ) g . read ( filename ) grid , edges = g . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
10010	def get_command_names ( ) : ret = [ ] for f in os . listdir ( COMMAND_MODULE_PATH ) : if os . path . isfile ( os . path . join ( COMMAND_MODULE_PATH , f ) ) and f . endswith ( COMMAND_MODULE_SUFFIX ) : ret . append ( f [ : - len ( COMMAND_MODULE_SUFFIX ) ] ) return ret
8549	def create_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule ) : properties = { "name" : firewall_rule . name } if firewall_rule . protocol : properties [ 'protocol' ] = firewall_rule . protocol if firewall_rule . source_mac : properties [ 'sourceMac' ] = firewall_rule . source_mac if firewall_rule . source_ip : properties [ 'sourceIp' ] = firewall_rule . source_ip if firewall_rule . target_ip : properties [ 'targetIp' ] = firewall_rule . target_ip if firewall_rule . port_range_start : properties [ 'portRangeStart' ] = firewall_rule . port_range_start if firewall_rule . port_range_end : properties [ 'portRangeEnd' ] = firewall_rule . port_range_end if firewall_rule . icmp_type : properties [ 'icmpType' ] = firewall_rule . icmp_type if firewall_rule . icmp_code : properties [ 'icmpCode' ] = firewall_rule . icmp_code data = { "properties" : properties } response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s/firewallrules' % ( datacenter_id , server_id , nic_id ) , method = 'POST' , data = json . dumps ( data ) ) return response
1770	def backup_emulate ( self , insn ) : if not hasattr ( self , 'backup_emu' ) : self . backup_emu = UnicornEmulator ( self ) try : self . backup_emu . emulate ( insn ) except unicorn . UcError as e : if e . errno == unicorn . UC_ERR_INSN_INVALID : text_bytes = ' ' . join ( '%02x' % x for x in insn . bytes ) logger . error ( "Unimplemented instruction: 0x%016x:\t%s\t%s\t%s" , insn . address , text_bytes , insn . mnemonic , insn . op_str ) raise InstructionEmulationError ( str ( e ) ) finally : del self . backup_emu
5983	def output_subplot_array ( output_path , output_filename , output_format ) : if output_format is 'show' : plt . show ( ) elif output_format is 'png' : plt . savefig ( output_path + output_filename + '.png' , bbox_inches = 'tight' ) elif output_format is 'fits' : raise exc . PlottingException ( 'You cannot output a subplots with format .fits' )
5286	def post ( self , request , * args , ** kwargs ) : formset = self . construct_formset ( ) if formset . is_valid ( ) : return self . formset_valid ( formset ) else : return self . formset_invalid ( formset )
7528	def aligned_indel_filter ( clust , max_internal_indels ) : lclust = clust . split ( ) try : seq1 = [ i . split ( "nnnn" ) [ 0 ] for i in lclust [ 1 : : 2 ] ] seq2 = [ i . split ( "nnnn" ) [ 1 ] for i in lclust [ 1 : : 2 ] ] intindels1 = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq1 ] intindels2 = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq2 ] intindels = intindels1 + intindels2 if max ( intindels ) > max_internal_indels : return 1 except IndexError : seq1 = lclust [ 1 : : 2 ] intindels = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq1 ] if max ( intindels ) > max_internal_indels : return 1 return 0
483	def getSwarmModelParams ( modelID ) : cjDAO = ClientJobsDAO . get ( ) ( jobID , description ) = cjDAO . modelsGetFields ( modelID , [ "jobId" , "genDescription" ] ) ( baseDescription , ) = cjDAO . jobGetFields ( jobID , [ "genBaseDescription" ] ) descriptionDirectory = tempfile . mkdtemp ( ) try : baseDescriptionFilePath = os . path . join ( descriptionDirectory , "base.py" ) with open ( baseDescriptionFilePath , mode = "wb" ) as f : f . write ( baseDescription ) descriptionFilePath = os . path . join ( descriptionDirectory , "description.py" ) with open ( descriptionFilePath , mode = "wb" ) as f : f . write ( description ) expIface = helpers . getExperimentDescriptionInterfaceFromModule ( helpers . loadExperimentDescriptionScriptFromDir ( descriptionDirectory ) ) return json . dumps ( dict ( modelConfig = expIface . getModelDescription ( ) , inferenceArgs = expIface . getModelControl ( ) . get ( "inferenceArgs" , None ) ) ) finally : shutil . rmtree ( descriptionDirectory , ignore_errors = True )
1669	def FlagCxx14Features ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] include = Match ( r'\s*#\s*include\s+[<"]([^<"]+)[">]' , line ) if include and include . group ( 1 ) in ( 'scoped_allocator' , 'shared_mutex' ) : error ( filename , linenum , 'build/c++14' , 5 , ( '<%s> is an unapproved C++14 header.' ) % include . group ( 1 ) )
1542	def queries_map ( ) : qs = _all_metric_queries ( ) return dict ( zip ( qs [ 0 ] , qs [ 1 ] ) + zip ( qs [ 2 ] , qs [ 3 ] ) )
7343	def clone_with_updates ( self , ** kwargs ) : fields_dict = self . to_dict ( ) fields_dict . update ( kwargs ) return BindingPrediction ( ** fields_dict )
8761	def delete_subnet ( context , id ) : LOG . info ( "delete_subnet %s for tenant %s" % ( id , context . tenant_id ) ) with context . session . begin ( ) : subnet = db_api . subnet_find ( context , id = id , scope = db_api . ONE ) if not subnet : raise n_exc . SubnetNotFound ( subnet_id = id ) if not context . is_admin : if STRATEGY . is_provider_network ( subnet . network_id ) : if subnet . tenant_id == context . tenant_id : raise n_exc . NotAuthorized ( subnet_id = id ) else : raise n_exc . SubnetNotFound ( subnet_id = id ) _delete_subnet ( context , subnet )
3697	def Hfus ( T = 298.15 , P = 101325 , MW = None , AvailableMethods = False , Method = None , CASRN = '' ) : def list_methods ( ) : methods = [ ] if CASRN in CRCHfus_data . index : methods . append ( 'CRC, at melting point' ) methods . append ( 'None' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'CRC, at melting point' : _Hfus = CRCHfus_data . at [ CASRN , 'Hfus' ] elif Method == 'None' or not MW : _Hfus = None else : raise Exception ( 'Failure in in function' ) _Hfus = property_molar_to_mass ( _Hfus , MW ) return _Hfus
9032	def _place_row ( self , row , position ) : self . _rows_in_grid [ row ] = RowInGrid ( row , position )
7956	def _initiate_starttls ( self , ** kwargs ) : if self . _tls_state == "connected" : raise RuntimeError ( "Already TLS-connected" ) kwargs [ "do_handshake_on_connect" ] = False logger . debug ( "Wrapping the socket into ssl" ) self . _socket = ssl . wrap_socket ( self . _socket , ** kwargs ) self . _set_state ( "tls-handshake" ) self . _continue_tls_handshake ( )
2778	def remove_forwarding_rules ( self , forwarding_rules ) : rules_dict = [ rule . __dict__ for rule in forwarding_rules ] return self . get_data ( "load_balancers/%s/forwarding_rules/" % self . id , type = DELETE , params = { "forwarding_rules" : rules_dict } )
1774	def invalidate_cache ( cpu , address , size ) : cache = cpu . instruction_cache for offset in range ( size ) : if address + offset in cache : del cache [ address + offset ]
8616	def _b ( s , encoding = 'utf-8' ) : if six . PY2 : if isinstance ( s , str ) : return s elif isinstance ( s , unicode ) : return s . encode ( encoding ) else : if isinstance ( s , bytes ) : return s elif isinstance ( s , str ) : return s . encode ( encoding ) raise TypeError ( "Invalid argument %r for _b()" % ( s , ) )
2094	def stdout ( self , pk , start_line = None , end_line = None , outfile = sys . stdout , ** kwargs ) : if self . unified_job_type != self . endpoint : unified_job = self . last_job_data ( pk , ** kwargs ) pk = unified_job [ 'id' ] elif not pk : unified_job = self . get ( ** kwargs ) pk = unified_job [ 'id' ] content = self . lookup_stdout ( pk , start_line , end_line ) opened = False if isinstance ( outfile , six . string_types ) : outfile = open ( outfile , 'w' ) opened = True if len ( content ) > 0 : click . echo ( content , nl = 1 , file = outfile ) if opened : outfile . close ( ) return { "changed" : False }
8478	def install ( ) : cmd = CommandHelper ( ) cmd . install ( "npm" ) cmd = CommandHelper ( ) cmd . install ( "nodejs-legacy" ) cmd = CommandHelper ( ) cmd . command = "npm install -g retire" cmd . execute ( ) if cmd . errors : from termcolor import colored print colored ( cmd . errors , "red" ) else : print cmd . output
9157	def stroke_linejoin ( self , linejoin ) : linejoin = getattr ( pgmagick . LineJoin , "%sJoin" % linejoin . title ( ) ) linejoin = pgmagick . DrawableStrokeLineJoin ( linejoin ) self . drawer . append ( linejoin )
7595	def get_clan ( self , * tags : crtag , ** params : keys ) : url = self . api . CLAN + '/' + ',' . join ( tags ) return self . _get_model ( url , FullClan , ** params )
1795	def SBB ( cpu , dest , src ) : cpu . _SUB ( dest , src , carry = True )
2870	def setup ( self , pin , mode ) : self . mraa_gpio . Gpio . dir ( self . mraa_gpio . Gpio ( pin ) , self . _dir_mapping [ mode ] )
11937	def create_message ( self , level , msg_text , extra_tags = '' , date = None , url = None ) : if not date : now = timezone . now ( ) else : now = date r = now . isoformat ( ) if now . microsecond : r = r [ : 23 ] + r [ 26 : ] if r . endswith ( '+00:00' ) : r = r [ : - 6 ] + 'Z' fingerprint = r + msg_text msg_id = hashlib . sha256 ( fingerprint . encode ( 'ascii' , 'ignore' ) ) . hexdigest ( ) return Message ( id = msg_id , message = msg_text , level = level , tags = extra_tags , date = r , url = url )
3972	def _composed_service_dict ( service_spec ) : compose_dict = service_spec . plain_dict ( ) _apply_env_overrides ( env_overrides_for_app_or_service ( service_spec . name ) , compose_dict ) compose_dict . setdefault ( 'volumes' , [ ] ) . append ( _get_cp_volume_mount ( service_spec . name ) ) compose_dict [ 'container_name' ] = "dusty_{}_1" . format ( service_spec . name ) return compose_dict
7135	def filter_dict ( d , exclude ) : ret = { } for key , value in d . items ( ) : if key not in exclude : ret . update ( { key : value } ) return ret
8629	def create_hireme_project ( session , title , description , currency , budget , jobs , hireme_initial_bid ) : jobs . append ( create_job_object ( id = 417 ) ) project_data = { 'title' : title , 'description' : description , 'currency' : currency , 'budget' : budget , 'jobs' : jobs , 'hireme' : True , 'hireme_initial_bid' : hireme_initial_bid } response = make_post_request ( session , 'projects' , json_data = project_data ) json_data = response . json ( ) if response . status_code == 200 : project_data = json_data [ 'result' ] p = Project ( project_data ) p . url = urljoin ( session . url , 'projects/%s' % p . seo_url ) return p else : raise ProjectNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] , )
10733	def to_bool ( option , value ) : if type ( value ) is str : if value . lower ( ) == 'true' : value = True elif value . lower ( ) == 'false' : value = False return ( option , value )
2637	def parent_callback ( self , parent_fu ) : if parent_fu . done ( ) is True : e = parent_fu . _exception if e : super ( ) . set_exception ( e ) else : super ( ) . set_result ( self . file_obj ) return
5544	def clip_array_with_vector ( array , array_affine , geometries , inverted = False , clip_buffer = 0 ) : buffered_geometries = [ ] for feature in geometries : feature_geom = to_shape ( feature [ "geometry" ] ) if feature_geom . is_empty : continue if feature_geom . geom_type == "GeometryCollection" : buffered_geom = unary_union ( [ g . buffer ( clip_buffer ) for g in feature_geom ] ) else : buffered_geom = feature_geom . buffer ( clip_buffer ) if not buffered_geom . is_empty : buffered_geometries . append ( buffered_geom ) if buffered_geometries : if array . ndim == 2 : return ma . masked_array ( array , geometry_mask ( buffered_geometries , array . shape , array_affine , invert = inverted ) ) elif array . ndim == 3 : mask = geometry_mask ( buffered_geometries , ( array . shape [ 1 ] , array . shape [ 2 ] ) , array_affine , invert = inverted ) return ma . masked_array ( array , mask = np . stack ( ( mask for band in array ) ) ) else : fill = False if inverted else True return ma . masked_array ( array , mask = np . full ( array . shape , fill , dtype = bool ) )
4525	def get ( self , position = 0 ) : n = len ( self ) if n == 1 : return self [ 0 ] pos = position if self . length and self . autoscale : pos *= len ( self ) pos /= self . length pos *= self . scale pos += self . offset if not self . continuous : if not self . serpentine : return self [ int ( pos % n ) ] m = ( 2 * n ) - 2 pos %= m if pos < n : return self [ int ( pos ) ] else : return self [ int ( m - pos ) ] if self . serpentine : pos %= ( 2 * n ) if pos > n : pos = ( 2 * n ) - pos else : pos %= n pos *= n - 1 pos /= n index = int ( pos ) fade = pos - index if not fade : return self [ index ] r1 , g1 , b1 = self [ index ] r2 , g2 , b2 = self [ ( index + 1 ) % len ( self ) ] dr , dg , db = r2 - r1 , g2 - g1 , b2 - b1 return r1 + fade * dr , g1 + fade * dg , b1 + fade * db
4211	def compatible_staticpath ( path ) : if VERSION >= ( 1 , 10 ) : return path try : from django . templatetags . static import static return static ( path ) except ImportError : pass try : return '%s/%s' % ( settings . STATIC_URL . rstrip ( '/' ) , path ) except AttributeError : pass try : return '%s/%s' % ( settings . PAGEDOWN_URL . rstrip ( '/' ) , path ) except AttributeError : pass return '%s/%s' % ( settings . MEDIA_URL . rstrip ( '/' ) , path )
2156	def launch ( self , monitor = False , wait = False , timeout = None , ** kwargs ) : r = client . get ( '/' ) if 'ad_hoc_commands' not in r . json ( ) : raise exc . TowerCLIError ( 'Your host is running an outdated version' 'of Ansible Tower that can not run ' 'ad-hoc commands (2.2 or earlier)' ) self . _pop_none ( kwargs ) debug . log ( 'Launching the ad-hoc command.' , header = 'details' ) result = client . post ( self . endpoint , data = kwargs ) command = result . json ( ) command_id = command [ 'id' ] if monitor : return self . monitor ( command_id , timeout = timeout ) elif wait : return self . wait ( command_id , timeout = timeout ) answer = OrderedDict ( ( ( 'changed' , True ) , ( 'id' , command_id ) , ) ) answer . update ( result . json ( ) ) return answer
9998	def cellsiter_to_dataframe ( cellsiter , args , drop_allna = True ) : from modelx . core . cells import shareable_parameters if len ( args ) : indexes = shareable_parameters ( cellsiter ) else : indexes = get_all_params ( cellsiter . values ( ) ) result = None for cells in cellsiter . values ( ) : df = cells_to_dataframe ( cells , args ) if drop_allna and df . isnull ( ) . all ( ) . all ( ) : continue if df . index . names != [ None ] : if isinstance ( df . index , pd . MultiIndex ) : if _pd_ver < ( 0 , 20 ) : df = _reset_naindex ( df ) df = df . reset_index ( ) missing_params = set ( indexes ) - set ( df ) for params in missing_params : df [ params ] = np . nan if result is None : result = df else : try : result = pd . merge ( result , df , how = "outer" ) except MergeError : result = pd . concat ( [ result , df ] , axis = 1 ) except ValueError : cols = set ( result . columns ) & set ( df . columns ) for col in cols : if ( len ( [ str ( frame [ col ] . dtype ) for frame in ( result , df ) if str ( frame [ col ] . dtype ) == "object" ] ) == 1 ) : if str ( result [ col ] . dtype ) == "object" : frame = df else : frame = result frame [ [ col ] ] = frame [ col ] . astype ( "object" ) result = pd . merge ( result , df , how = "outer" ) if result is None : return pd . DataFrame ( ) else : return result . set_index ( indexes ) if indexes else result
1207	def setup ( app ) : global _is_sphinx _is_sphinx = True app . add_config_value ( 'no_underscore_emphasis' , False , 'env' ) app . add_source_parser ( '.md' , M2RParser ) app . add_directive ( 'mdinclude' , MdInclude )
3844	def from_participantid ( participant_id ) : return user . UserID ( chat_id = participant_id . chat_id , gaia_id = participant_id . gaia_id )
5159	def _add_tc_script ( self ) : context = dict ( tc_options = self . config . get ( 'tc_options' , [ ] ) ) contents = self . _render_template ( 'tc_script.sh' , context ) self . config . setdefault ( 'files' , [ ] ) self . _add_unique_file ( { "path" : "/tc_script.sh" , "contents" : contents , "mode" : "755" } )
10587	def get_account_descendants ( self , account ) : result = [ ] for child in account . accounts : self . _get_account_and_descendants_ ( child , result ) return result
1897	def _assert ( self , expression : Bool ) : assert isinstance ( expression , Bool ) smtlib = translate_to_smtlib ( expression ) self . _send ( '(assert %s)' % smtlib )
9207	def add_prefix ( multicodec , bytes_ ) : prefix = get_prefix ( multicodec ) return b'' . join ( [ prefix , bytes_ ] )
9824	def update ( ctx , name , description , tags , private ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description if private is not None : update_dict [ 'is_public' ] = not private tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the project.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . project . update_project ( user , project_name , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Project updated." ) get_project_details ( response )
11757	def pl_true ( exp , model = { } ) : op , args = exp . op , exp . args if exp == TRUE : return True elif exp == FALSE : return False elif is_prop_symbol ( op ) : return model . get ( exp ) elif op == '~' : p = pl_true ( args [ 0 ] , model ) if p is None : return None else : return not p elif op == '|' : result = False for arg in args : p = pl_true ( arg , model ) if p is True : return True if p is None : result = None return result elif op == '&' : result = True for arg in args : p = pl_true ( arg , model ) if p is False : return False if p is None : result = None return result p , q = args if op == '>>' : return pl_true ( ~ p | q , model ) elif op == '<<' : return pl_true ( p | ~ q , model ) pt = pl_true ( p , model ) if pt is None : return None qt = pl_true ( q , model ) if qt is None : return None if op == '<=>' : return pt == qt elif op == '^' : return pt != qt else : raise ValueError , "illegal operator in logic expression" + str ( exp )
1571	def submit_tar ( cl_args , unknown_args , tmp_dir ) : topology_file = cl_args [ 'topology-file-name' ] java_defines = cl_args [ 'topology_main_jvm_property' ] main_class = cl_args [ 'topology-class-name' ] res = execute . heron_tar ( main_class , topology_file , tuple ( unknown_args ) , tmp_dir , java_defines ) result . render ( res ) if not result . is_successful ( res ) : err_context = ( "Failed to create topology definition " "file when executing class '%s' of file '%s'" ) % ( main_class , topology_file ) res . add_context ( err_context ) return res return launch_topologies ( cl_args , topology_file , tmp_dir )
4055	def everything ( self , query ) : try : items = [ ] items . extend ( query ) while self . links . get ( "next" ) : items . extend ( self . follow ( ) ) except TypeError : items = copy . deepcopy ( query ) while self . links . get ( "next" ) : items . entries . extend ( self . follow ( ) . entries ) return items
11948	def init ( base_level = DEFAULT_BASE_LOGGING_LEVEL , verbose_level = DEFAULT_VERBOSE_LOGGING_LEVEL , logging_config = None ) : if logging_config is None : logging_config = { } logging_config = logging_config or LOGGER log_file = LOGGER [ 'handlers' ] [ 'file' ] [ 'filename' ] log_dir = os . path . dirname ( os . path . expanduser ( log_file ) ) if os . path . isfile ( log_dir ) : sys . exit ( 'file {0} exists - log directory cannot be created ' 'there. please remove the file and try again.' . format ( log_dir ) ) try : if not os . path . exists ( log_dir ) and not len ( log_dir ) == 0 : os . makedirs ( log_dir ) dictconfig . dictConfig ( logging_config ) lgr = logging . getLogger ( 'user' ) lgr . setLevel ( base_level ) return lgr except ValueError as e : sys . exit ( 'could not initialize logger.' ' verify your logger config' ' and permissions to write to {0} ({1})' . format ( log_file , e ) )
3150	def get ( self , list_id , webhook_id ) : self . list_id = list_id self . webhook_id = webhook_id return self . _mc_client . _get ( url = self . _build_path ( list_id , 'webhooks' , webhook_id ) )
12071	def update ( self , tids , info ) : outputs_dir = os . path . join ( info [ 'root_directory' ] , 'streams' ) pattern = '%s_*_tid_*{tid}.o.{tid}*' % info [ 'batch_name' ] flist = os . listdir ( outputs_dir ) try : outputs = [ ] for tid in tids : matches = fnmatch . filter ( flist , pattern . format ( tid = tid ) ) if len ( matches ) != 1 : self . warning ( "No unique output file for tid %d" % tid ) contents = open ( os . path . join ( outputs_dir , matches [ 0 ] ) , 'r' ) . read ( ) outputs . append ( self . output_extractor ( contents ) ) self . _next_val = self . _update_state ( outputs ) self . trace . append ( ( outputs , self . _next_val ) ) except : self . warning ( "Cannot load required output files. Cannot continue." ) self . _next_val = StopIteration
10623	def extract ( self , other ) : if type ( other ) is float or type ( other ) is numpy . float64 or type ( other ) is numpy . float32 : return self . _extract_mass ( other ) elif self . _is_compound_mass_tuple ( other ) : return self . _extract_compound_mass ( other [ 0 ] , other [ 1 ] ) elif type ( other ) is str : return self . _extract_compound ( other ) elif type ( other ) is Material : return self . _extract_material ( other ) else : raise TypeError ( "Invalid extraction argument." )
3748	def calculate ( self , T , method ) : r if method == GHARAGHEIZI : mu = Gharagheizi_gas_viscosity ( T , self . Tc , self . Pc , self . MW ) elif method == COOLPROP : mu = CoolProp_T_dependent_property ( T , self . CASRN , 'V' , 'g' ) elif method == DIPPR_PERRY_8E : mu = EQ102 ( T , * self . Perrys2_312_coeffs ) elif method == VDI_PPDS : mu = horner ( self . VDI_PPDS_coeffs , T ) elif method == YOON_THODOS : mu = Yoon_Thodos ( T , self . Tc , self . Pc , self . MW ) elif method == STIEL_THODOS : mu = Stiel_Thodos ( T , self . Tc , self . Pc , self . MW ) elif method == LUCAS_GAS : mu = lucas_gas ( T , self . Tc , self . Pc , self . Zc , self . MW , self . dipole , CASRN = self . CASRN ) elif method in self . tabular_data : mu = self . interpolate ( T , method ) return mu
2205	def userhome ( username = None ) : if username is None : if 'HOME' in os . environ : userhome_dpath = os . environ [ 'HOME' ] else : if sys . platform . startswith ( 'win32' ) : if 'USERPROFILE' in os . environ : userhome_dpath = os . environ [ 'USERPROFILE' ] elif 'HOMEPATH' in os . environ : drive = os . environ . get ( 'HOMEDRIVE' , '' ) userhome_dpath = join ( drive , os . environ [ 'HOMEPATH' ] ) else : raise OSError ( "Cannot determine the user's home directory" ) else : import pwd userhome_dpath = pwd . getpwuid ( os . getuid ( ) ) . pw_dir else : if sys . platform . startswith ( 'win32' ) : c_users = dirname ( userhome ( ) ) userhome_dpath = join ( c_users , username ) if not exists ( userhome_dpath ) : raise KeyError ( 'Unknown user: {}' . format ( username ) ) else : import pwd try : pwent = pwd . getpwnam ( username ) except KeyError : raise KeyError ( 'Unknown user: {}' . format ( username ) ) userhome_dpath = pwent . pw_dir return userhome_dpath
1695	def filter ( self , filter_function ) : from heronpy . streamlet . impl . filterbolt import FilterStreamlet filter_streamlet = FilterStreamlet ( filter_function , self ) self . _add_child ( filter_streamlet ) return filter_streamlet
4287	def get_thumb ( settings , filename ) : path , filen = os . path . split ( filename ) name , ext = os . path . splitext ( filen ) if ext . lower ( ) in settings [ 'video_extensions' ] : ext = '.jpg' return join ( path , settings [ 'thumb_dir' ] , settings [ 'thumb_prefix' ] + name + settings [ 'thumb_suffix' ] + ext )
11744	def goto ( self , rules , symbol ) : return self . closure ( { rule . move_dot ( ) for rule in rules if not rule . at_end and rule . rhs [ rule . pos ] == symbol } , )
1556	def get_out_streamids ( self ) : if self . outputs is None : return set ( ) if not isinstance ( self . outputs , ( list , tuple ) ) : raise TypeError ( "Argument to outputs must be either list or tuple, given: %s" % str ( type ( self . outputs ) ) ) ret_lst = [ ] for output in self . outputs : if not isinstance ( output , ( str , Stream ) ) : raise TypeError ( "Outputs must be a list of strings or Streams, given: %s" % str ( output ) ) ret_lst . append ( Stream . DEFAULT_STREAM_ID if isinstance ( output , str ) else output . stream_id ) return set ( ret_lst )
3833	async def modify_otr_status ( self , modify_otr_status_request ) : response = hangouts_pb2 . ModifyOTRStatusResponse ( ) await self . _pb_request ( 'conversations/modifyotrstatus' , modify_otr_status_request , response ) return response
11284	def reduce ( func ) : def wrapper ( prev , * argv , ** kw ) : accum_value = None if 'init' not in kw else kw . pop ( 'init' ) if prev is None : raise TypeError ( 'A reducer must have input.' ) for i in prev : accum_value = func ( accum_value , i , * argv , ** kw ) yield accum_value return Pipe ( wrapper )
12120	def generate_colormap ( self , colormap = None , reverse = False ) : if colormap is None : colormap = pylab . cm . Dark2 self . cm = colormap self . colormap = [ ] for i in range ( self . sweeps ) : self . colormap . append ( colormap ( i / self . sweeps ) ) if reverse : self . colormap . reverse ( )
12817	def _length ( self ) : self . _build_chunk_headers ( ) length = 0 if self . _data : for field in self . _data : length += len ( self . _chunk_headers [ field ] ) length += len ( self . _data [ field ] ) length += 2 if self . _files : for field in self . _files : length += len ( self . _chunk_headers [ field ] ) length += self . _file_size ( field ) length += 2 length += len ( self . boundary ) length += 6 return length
11668	def quadratic ( Ks , dim , rhos , required = None ) : r N = rhos . shape [ 0 ] Ks = np . asarray ( Ks ) Bs = ( Ks - 1 ) / np . pi ** ( dim / 2 ) * gamma ( dim / 2 + 1 ) est = Bs / ( N - 1 ) * np . mean ( rhos ** ( - dim ) , axis = 0 ) return est
3193	def update ( self , list_id , subscriber_hash , data ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'members' , subscriber_hash ) , data = data )
9417	def to_pointer ( cls , instance ) : return OctavePtr ( instance . _ref , instance . _name , instance . _address )
4922	def retrieve ( self , request , pk = None ) : catalog_api = CourseCatalogApiClient ( request . user ) catalog = catalog_api . get_catalog ( pk ) self . ensure_data_exists ( request , catalog , error_message = ( "Unable to fetch API response for given catalog from endpoint '/catalog/{pk}/'. " "The resource you are looking for does not exist." . format ( pk = pk ) ) ) serializer = self . serializer_class ( catalog ) return Response ( serializer . data )
9981	def is_funcdef ( src ) : module_node = ast . parse ( dedent ( src ) ) if len ( module_node . body ) == 1 and isinstance ( module_node . body [ 0 ] , ast . FunctionDef ) : return True else : return False
4425	def remove ( self , guild_id ) : if guild_id in self . _players : self . _players [ guild_id ] . cleanup ( ) del self . _players [ guild_id ]
3412	def _fix_type ( value ) : if isinstance ( value , string_types ) : return str ( value ) if isinstance ( value , float_ ) : return float ( value ) if isinstance ( value , bool_ ) : return bool ( value ) if isinstance ( value , set ) : return list ( value ) if isinstance ( value , dict ) : return OrderedDict ( ( key , value [ key ] ) for key in sorted ( value ) ) if value . __class__ . __name__ == "Formula" : return str ( value ) if value is None : return "" return value
1384	def trigger_watches ( self ) : to_remove = [ ] for uid , callback in self . watches . items ( ) : try : callback ( self ) except Exception as e : Log . error ( "Caught exception while triggering callback: " + str ( e ) ) Log . debug ( traceback . format_exc ( ) ) to_remove . append ( uid ) for uid in to_remove : self . unregister_watch ( uid )
1995	def _named_stream ( self , name , binary = False ) : with self . _store . save_stream ( self . _named_key ( name ) , binary = binary ) as s : yield s
1932	def get_description ( self , name : str ) -> str : if name not in self . _vars : raise ConfigError ( f"{self.name}.{name} not defined." ) return self . _vars [ name ] . description
8132	def export ( self , filename ) : self . flatten ( ) self . layers [ 1 ] . img . save ( filename ) return filename
3648	def applyConsumable ( self , item_id , resource_id ) : method = 'POST' url = 'item/resource/%s' % resource_id data = { 'apply' : [ { 'id' : item_id } ] } self . __request__ ( method , url , data = json . dumps ( data ) )
13455	def _parse_args ( args ) : parser = argparse . ArgumentParser ( description = "Remove and/or rearrange " + "sections from each line of a file(s)." , usage = _usage ( ) [ len ( 'usage: ' ) : ] ) parser . add_argument ( '-b' , "--bytes" , action = 'store' , type = lst , default = [ ] , help = "Bytes to select" ) parser . add_argument ( '-c' , "--chars" , action = 'store' , type = lst , default = [ ] , help = "Character to select" ) parser . add_argument ( '-f' , "--fields" , action = 'store' , type = lst , default = [ ] , help = "Fields to select" ) parser . add_argument ( '-d' , "--delimiter" , action = 'store' , default = "\t" , help = "Sets field delimiter(default is TAB)" ) parser . add_argument ( '-e' , "--regex" , action = 'store_true' , help = 'Enable regular expressions to be used as input ' + 'delimiter' ) parser . add_argument ( '-s' , '--skip' , action = 'store_true' , help = "Skip lines that do not contain input delimiter." ) parser . add_argument ( '-S' , "--separator" , action = 'store' , default = "\t" , help = "Sets field separator for output." ) parser . add_argument ( 'file' , nargs = '*' , default = "-" , help = "File(s) to cut" ) return parser . parse_args ( args )
12597	def _check_xl_path ( xl_path : str ) : xl_path = op . abspath ( op . expanduser ( xl_path ) ) if not op . isfile ( xl_path ) : raise IOError ( "Could not find file in {}." . format ( xl_path ) ) return xl_path , _use_openpyxl_or_xlrf ( xl_path )
1413	def _get_packing_plan_with_watch ( self , topologyName , callback , isWatching ) : path = self . get_packing_plan_path ( topologyName ) if isWatching : LOG . info ( "Adding data watch for path: " + path ) @ self . client . DataWatch ( path ) def watch_packing_plan ( data , stats ) : if data : packing_plan = PackingPlan ( ) packing_plan . ParseFromString ( data ) callback ( packing_plan ) else : callback ( None ) return isWatching
11565	def stepper_config ( self , steps_per_revolution , stepper_pins ) : data = [ self . STEPPER_CONFIGURE , steps_per_revolution & 0x7f , ( steps_per_revolution >> 7 ) & 0x7f ] for pin in range ( len ( stepper_pins ) ) : data . append ( stepper_pins [ pin ] ) self . _command_handler . send_sysex ( self . _command_handler . STEPPER_DATA , data )
7226	def paint ( self ) : snippet = { 'line-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'line-color' : VectorStyle . get_style_value ( self . color ) , 'line-width' : VectorStyle . get_style_value ( self . width ) , } if self . translate : snippet [ 'line-translate' ] = self . translate if self . dasharray : snippet [ 'line-dasharray' ] = VectorStyle . get_style_value ( self . dasharray ) return snippet
2290	def create_graph_from_data ( self , data ) : warnings . warn ( "An exhaustive search of the causal structure of CGNN without" " skeleton is super-exponential in the number of variables." ) nb_vars = len ( list ( data . columns ) ) data = scale ( data . values ) . astype ( 'float32' ) candidates = [ np . reshape ( np . array ( i ) , ( nb_vars , nb_vars ) ) for i in itertools . product ( [ 0 , 1 ] , repeat = nb_vars * nb_vars ) if ( np . trace ( np . reshape ( np . array ( i ) , ( nb_vars , nb_vars ) ) ) == 0 and nx . is_directed_acyclic_graph ( nx . DiGraph ( np . reshape ( np . array ( i ) , ( nb_vars , nb_vars ) ) ) ) ) ] warnings . warn ( "A total of {} graphs will be evaluated." . format ( len ( candidates ) ) ) scores = [ parallel_graph_evaluation ( data , i , nh = self . nh , nb_runs = self . nb_runs , gpu = self . gpu , nb_jobs = self . nb_jobs , lr = self . lr , train_epochs = self . train_epochs , test_epochs = self . test_epochs , verbose = self . verbose ) for i in candidates ] final_candidate = candidates [ scores . index ( min ( scores ) ) ] output = np . zeros ( final_candidate . shape ) for ( i , j ) , x in np . ndenumerate ( final_candidate ) : if x > 0 : cand = final_candidate cand [ i , j ] = 0 output [ i , j ] = min ( scores ) - scores [ candidates . index ( cand ) ] return nx . DiGraph ( candidates [ output ] , { idx : i for idx , i in enumerate ( data . columns ) } )
8443	def ls ( github_user , template = None ) : temple . check . has_env_vars ( temple . constants . GITHUB_API_TOKEN_ENV_VAR ) if template : temple . check . is_git_ssh_path ( template ) search_q = 'user:{} filename:{} {}' . format ( github_user , temple . constants . TEMPLE_CONFIG_FILE , template ) else : search_q = 'user:{} cookiecutter.json in:path' . format ( github_user ) results = _code_search ( search_q , github_user ) return collections . OrderedDict ( sorted ( results . items ( ) ) )
7459	def paramname ( param = "" ) : try : name = pinfo [ str ( param ) ] [ 0 ] . strip ( ) . split ( " " ) [ 1 ] except ( KeyError , ValueError ) as err : print ( "\tKey name/number not recognized - " . format ( param ) , err ) raise return name
7393	def mods_genre ( self ) : type2genre = { 'conference' : 'conference publication' , 'book chapter' : 'bibliography' , 'unpublished' : 'article' } tp = str ( self . type ) . lower ( ) return type2genre . get ( tp , tp )
10283	def count_sources ( edge_iter : EdgeIterator ) -> Counter : return Counter ( u for u , _ , _ in edge_iter )
13749	def many_to_one ( clsname , ** kw ) : @ declared_attr def m2o ( cls ) : cls . _references ( ( cls . __name__ , clsname ) ) return relationship ( clsname , ** kw ) return m2o
4639	def set_shared_config ( cls , config ) : assert isinstance ( config , dict ) cls . _sharedInstance . config . update ( config ) if cls . _sharedInstance . instance : cls . _sharedInstance . instance = None
2184	def clear ( self , cfgstr = None ) : data_fpath = self . get_fpath ( cfgstr ) if self . verbose > 0 : self . log ( '[cacher] clear cache' ) if exists ( data_fpath ) : if self . verbose > 0 : self . log ( '[cacher] removing {}' . format ( data_fpath ) ) os . remove ( data_fpath ) meta_fpath = data_fpath + '.meta' if exists ( meta_fpath ) : os . remove ( meta_fpath ) else : if self . verbose > 0 : self . log ( '[cacher] ... nothing to clear' )
9373	def is_valid_url ( url ) : regex = re . compile ( r'^(?:http|ftp)s?://' r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+(?:[A-Z]{2,6}\.?|[A-Z0-9-]{2,}\.?)|' r'localhost|' r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})' r'(?::\d+)?' r'(?:/?|[/?]\S+)$' , re . IGNORECASE ) if regex . match ( url ) : logger . info ( "URL given as config" ) return True else : return False
9870	def build_environ ( self , sock_file , conn ) : request = self . read_request_line ( sock_file ) environ = self . base_environ . copy ( ) for k , v in self . read_headers ( sock_file ) . items ( ) : environ [ str ( 'HTTP_' + k ) ] = v environ [ 'REQUEST_METHOD' ] = request [ 'method' ] environ [ 'PATH_INFO' ] = request [ 'path' ] environ [ 'SERVER_PROTOCOL' ] = request [ 'protocol' ] environ [ 'SERVER_PORT' ] = str ( conn . server_port ) environ [ 'REMOTE_PORT' ] = str ( conn . client_port ) environ [ 'REMOTE_ADDR' ] = str ( conn . client_addr ) environ [ 'QUERY_STRING' ] = request [ 'query_string' ] if 'HTTP_CONTENT_LENGTH' in environ : environ [ 'CONTENT_LENGTH' ] = environ [ 'HTTP_CONTENT_LENGTH' ] if 'HTTP_CONTENT_TYPE' in environ : environ [ 'CONTENT_TYPE' ] = environ [ 'HTTP_CONTENT_TYPE' ] self . request_method = environ [ 'REQUEST_METHOD' ] if conn . ssl : environ [ 'wsgi.url_scheme' ] = 'https' environ [ 'HTTPS' ] = 'on' else : environ [ 'wsgi.url_scheme' ] = 'http' if environ . get ( 'HTTP_TRANSFER_ENCODING' , '' ) == 'chunked' : environ [ 'wsgi.input' ] = ChunkedReader ( sock_file ) else : environ [ 'wsgi.input' ] = sock_file return environ
5638	def _temporal_distance_cdf ( self ) : distance_split_points = set ( ) for block in self . _profile_blocks : if block . distance_start != float ( 'inf' ) : distance_split_points . add ( block . distance_end ) distance_split_points . add ( block . distance_start ) distance_split_points_ordered = numpy . array ( sorted ( list ( distance_split_points ) ) ) temporal_distance_split_widths = distance_split_points_ordered [ 1 : ] - distance_split_points_ordered [ : - 1 ] trip_counts = numpy . zeros ( len ( temporal_distance_split_widths ) ) delta_peaks = defaultdict ( lambda : 0 ) for block in self . _profile_blocks : if block . distance_start == block . distance_end : delta_peaks [ block . distance_end ] += block . width ( ) else : start_index = numpy . searchsorted ( distance_split_points_ordered , block . distance_end ) end_index = numpy . searchsorted ( distance_split_points_ordered , block . distance_start ) trip_counts [ start_index : end_index ] += 1 unnormalized_cdf = numpy . array ( [ 0 ] + list ( numpy . cumsum ( temporal_distance_split_widths * trip_counts ) ) ) if not ( numpy . isclose ( [ unnormalized_cdf [ - 1 ] ] , [ self . _end_time - self . _start_time - sum ( delta_peaks . values ( ) ) ] , atol = 1E-4 ) . all ( ) ) : print ( unnormalized_cdf [ - 1 ] , self . _end_time - self . _start_time - sum ( delta_peaks . values ( ) ) ) raise RuntimeError ( "Something went wrong with cdf computation!" ) if len ( delta_peaks ) > 0 : for peak in delta_peaks . keys ( ) : if peak == float ( 'inf' ) : continue index = numpy . nonzero ( distance_split_points_ordered == peak ) [ 0 ] [ 0 ] unnormalized_cdf = numpy . insert ( unnormalized_cdf , index , unnormalized_cdf [ index ] ) distance_split_points_ordered = numpy . insert ( distance_split_points_ordered , index , distance_split_points_ordered [ index ] ) unnormalized_cdf [ ( index + 1 ) : ] = unnormalized_cdf [ ( index + 1 ) : ] + delta_peaks [ peak ] norm_cdf = unnormalized_cdf / ( unnormalized_cdf [ - 1 ] + delta_peaks [ float ( 'inf' ) ] ) return distance_split_points_ordered , norm_cdf
170	def draw_mask ( self , image_shape , size_lines = 1 , size_points = 0 , raise_if_out_of_image = False ) : heatmap = self . draw_heatmap_array ( image_shape , alpha_lines = 1.0 , alpha_points = 1.0 , size_lines = size_lines , size_points = size_points , antialiased = False , raise_if_out_of_image = raise_if_out_of_image ) return heatmap > 0.5
12795	def _get_password_url ( self ) : password_url = None if self . _settings [ "user" ] or self . _settings [ "authorization" ] : if self . _settings [ "url" ] : password_url = self . _settings [ "url" ] elif self . _settings [ "base_url" ] : password_url = self . _settings [ "base_url" ] return password_url
9250	def filter_issues_for_tags ( self , newer_tag , older_tag ) : filtered_pull_requests = self . delete_by_time ( self . pull_requests , older_tag , newer_tag ) filtered_issues = self . delete_by_time ( self . issues , older_tag , newer_tag ) newer_tag_name = newer_tag [ "name" ] if newer_tag else None if self . options . filter_issues_by_milestone : filtered_issues = self . filter_by_milestone ( filtered_issues , newer_tag_name , self . issues ) filtered_pull_requests = self . filter_by_milestone ( filtered_pull_requests , newer_tag_name , self . pull_requests ) return filtered_issues , filtered_pull_requests
10972	def requests ( ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) memberships = Membership . query_requests ( current_user , eager = True ) . all ( ) return render_template ( 'invenio_groups/pending.html' , memberships = memberships , requests = True , page = page , per_page = per_page , )
13056	def _plugin_endpoint_rename ( fn_name , instance ) : if instance and instance . namespaced : fn_name = "r_{0}_{1}" . format ( instance . name , fn_name [ 2 : ] ) return fn_name
13758	def split_path ( path ) : result_parts = [ ] while path != "/" : parts = os . path . split ( path ) if parts [ 1 ] == path : result_parts . insert ( 0 , parts [ 1 ] ) break elif parts [ 0 ] == path : result_parts . insert ( 0 , parts [ 0 ] ) break else : path = parts [ 0 ] result_parts . insert ( 0 , parts [ 1 ] ) return result_parts
1622	def RemoveMultiLineCommentsFromRange ( lines , begin , end ) : for i in range ( begin , end ) : lines [ i ] = '/**/'
5616	def write_vector_window ( in_data = None , out_schema = None , out_tile = None , out_path = None , bucket_resource = None ) : try : os . remove ( out_path ) except OSError : pass out_features = [ ] for feature in in_data : try : for out_geom in multipart_to_singleparts ( clean_geometry_type ( to_shape ( feature [ "geometry" ] ) . intersection ( out_tile . bbox ) , out_schema [ "geometry" ] ) ) : out_features . append ( { "geometry" : mapping ( out_geom ) , "properties" : feature [ "properties" ] } ) except Exception as e : logger . warning ( "failed to prepare geometry for writing: %s" , e ) continue if out_features : try : if out_path . startswith ( "s3://" ) : with VectorWindowMemoryFile ( tile = out_tile , features = out_features , schema = out_schema , driver = "GeoJSON" ) as memfile : logger . debug ( ( out_tile . id , "upload tile" , out_path ) ) bucket_resource . put_object ( Key = "/" . join ( out_path . split ( "/" ) [ 3 : ] ) , Body = memfile ) else : with fiona . open ( out_path , 'w' , schema = out_schema , driver = "GeoJSON" , crs = out_tile . crs . to_dict ( ) ) as dst : logger . debug ( ( out_tile . id , "write tile" , out_path ) ) dst . writerecords ( out_features ) except Exception as e : logger . error ( "error while writing file %s: %s" , out_path , e ) raise else : logger . debug ( ( out_tile . id , "nothing to write" , out_path ) )
7079	def tic_xmatch ( ra , decl , radius_arcsec = 5.0 , apiversion = 'v0' , forcefetch = False , cachedir = '~/.astrobase/mast-cache' , verbose = True , timeout = 90.0 , refresh = 5.0 , maxtimeout = 180.0 , maxtries = 3 , jitter = 5.0 , raiseonfail = False ) : service = 'Mast.Tic.Crossmatch' xmatch_input = { 'fields' : [ { 'name' : 'ra' , 'type' : 'float' } , { 'name' : 'dec' , 'type' : 'float' } ] } xmatch_input [ 'data' ] = [ { 'ra' : x , 'dec' : y } for ( x , y ) in zip ( ra , decl ) ] params = { 'raColumn' : 'ra' , 'decColumn' : 'dec' , 'radius' : radius_arcsec / 3600.0 } return mast_query ( service , params , data = xmatch_input , jitter = jitter , apiversion = apiversion , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , raiseonfail = raiseonfail )
11247	def triangle_area ( point1 , point2 , point3 ) : a = point_distance ( point1 , point2 ) b = point_distance ( point1 , point3 ) c = point_distance ( point2 , point3 ) s = ( a + b + c ) / 2.0 return math . sqrt ( s * ( s - a ) * ( s - b ) * ( s - c ) )
2242	def split_modpath ( modpath , check = True ) : if six . PY2 : if modpath . endswith ( '.pyc' ) : modpath = modpath [ : - 1 ] modpath_ = abspath ( expanduser ( modpath ) ) if check : if not exists ( modpath_ ) : if not exists ( modpath ) : raise ValueError ( 'modpath={} does not exist' . format ( modpath ) ) raise ValueError ( 'modpath={} is not a module' . format ( modpath ) ) if isdir ( modpath_ ) and not exists ( join ( modpath , '__init__.py' ) ) : raise ValueError ( 'modpath={} is not a module' . format ( modpath ) ) full_dpath , fname_ext = split ( modpath_ ) _relmod_parts = [ fname_ext ] dpath = full_dpath while exists ( join ( dpath , '__init__.py' ) ) : dpath , dname = split ( dpath ) _relmod_parts . append ( dname ) relmod_parts = _relmod_parts [ : : - 1 ] rel_modpath = os . path . sep . join ( relmod_parts ) return dpath , rel_modpath
12855	def subtree ( events ) : stack = 0 for obj in events : if obj [ 'type' ] == ENTER : stack += 1 elif obj [ 'type' ] == EXIT : if stack == 0 : break stack -= 1 yield obj
953	def closenessScores ( self , expValues , actValues , ** kwargs ) : ratio = 1.0 esum = int ( expValues . sum ( ) ) asum = int ( actValues . sum ( ) ) if asum > esum : diff = asum - esum if diff < esum : ratio = 1 - diff / float ( esum ) else : ratio = 1 / float ( diff ) olap = expValues & actValues osum = int ( olap . sum ( ) ) if esum == 0 : r = 0.0 else : r = osum / float ( esum ) r = r * ratio return numpy . array ( [ r ] )
12673	def aggregate ( * args ) : if args and isinstance ( args [ 0 ] , dataframe . DataFrame ) : return args [ 0 ] . aggregate ( args [ 1 ] , args [ 2 ] , * args [ 3 : ] ) elif not args : raise ValueError ( "No arguments provided" ) else : return pipeable . Pipeable ( pipeable . PipingMethod . AGGREGATE , * args )
7342	def set_debug ( ) : logging . basicConfig ( level = logging . WARNING ) peony . logger . setLevel ( logging . DEBUG )
9864	def get_home ( self , home_id ) : if home_id not in self . _all_home_ids : _LOGGER . error ( "Could not find any Tibber home with id: %s" , home_id ) return None if home_id not in self . _homes . keys ( ) : self . _homes [ home_id ] = TibberHome ( home_id , self ) return self . _homes [ home_id ]
11978	def get_wildcard ( self ) : return _convert ( self . _ip , notation = NM_WILDCARD , inotation = IP_DOT , _check = False , _isnm = self . _isnm )
5960	def _tcorrel ( self , nstep = 100 , ** kwargs ) : t = self . array [ 0 , : : nstep ] r = gromacs . collections . Collection ( [ numkit . timeseries . tcorrel ( t , Y , nstep = 1 , ** kwargs ) for Y in self . array [ 1 : , : : nstep ] ] ) return r
3421	def model_to_pymatbridge ( model , variable_name = "model" , matlab = None ) : if scipy_sparse is None : raise ImportError ( "`model_to_pymatbridge` requires scipy!" ) if matlab is None : from IPython import get_ipython matlab = get_ipython ( ) . magics_manager . registry [ "MatlabMagics" ] . Matlab model_info = create_mat_dict ( model ) S = model_info [ "S" ] . todok ( ) model_info [ "S" ] = 0 temp_S_name = "cobra_pymatbridge_temp_" + uuid4 ( ) . hex _check ( matlab . set_variable ( variable_name , model_info ) ) _check ( matlab . set_variable ( temp_S_name , S ) ) _check ( matlab . run_code ( "%s.S = %s;" % ( variable_name , temp_S_name ) ) ) for i in model_info . keys ( ) : if i == "S" : continue _check ( matlab . run_code ( "{0}.{1} = {0}.{1}';" . format ( variable_name , i ) ) ) _check ( matlab . run_code ( "clear %s;" % temp_S_name ) )
8173	def _angle ( self ) : from math import atan , pi , degrees a = degrees ( atan ( self . vy / self . vx ) ) + 360 if self . vx < 0 : a += 180 return a
9962	def get_interfaces ( impls ) : if impls is None : return None elif isinstance ( impls , OrderMixin ) : result = OrderedDict ( ) for name in impls . order : result [ name ] = impls [ name ] . interface return result elif isinstance ( impls , Mapping ) : return { name : impls [ name ] . interface for name in impls } elif isinstance ( impls , Sequence ) : return [ impl . interface for impl in impls ] else : return impls . interface
12391	def indexesOptional ( f ) : stack = inspect . stack ( ) _NO_INDEX_CHECK_NEEDED . add ( '%s.%s.%s' % ( f . __module__ , stack [ 1 ] [ 3 ] , f . __name__ ) ) del stack return f
6820	def configure_modevasive ( self ) : r = self . local_renderer if r . env . modevasive_enabled : self . install_packages ( ) fn = r . render_to_file ( 'apache/apache_modevasive.template.conf' ) r . put ( local_path = fn , remote_path = '/etc/apache2/mods-available/mod-evasive.conf' , use_sudo = True ) r . put ( local_path = fn , remote_path = '/etc/apache2/mods-available/evasive.conf' , use_sudo = True ) self . enable_mod ( 'evasive' ) else : if self . last_manifest . modevasive_enabled : self . disable_mod ( 'evasive' )
10679	def S ( self , T ) : result = 0.0 if T < self . Tmax : lT = T else : lT = self . Tmax Tref = self . Tmin for c , e in zip ( self . _coefficients , self . _exponents ) : e_modified = e - 1.0 if e_modified == - 1.0 : result += c * math . log ( lT / Tref ) else : e_mod = e_modified + 1.0 result += c * ( lT ** e_mod - Tref ** e_mod ) / e_mod return result
13440	def unlock_file ( filename ) : lockfile = "%s.lock" % filename if isfile ( lockfile ) : os . remove ( lockfile ) return True else : return False
12230	def unpatch_locals ( depth = 3 ) : for name , locals_dict in traverse_local_prefs ( depth ) : if isinstance ( locals_dict [ name ] , PatchedLocal ) : locals_dict [ name ] = locals_dict [ name ] . val del get_frame_locals ( depth ) [ __PATCHED_LOCALS_SENTINEL ]
9086	def _sort ( self , concepts , sort = None , language = 'any' , reverse = False ) : sorted = copy . copy ( concepts ) if sort : sorted . sort ( key = methodcaller ( '_sortkey' , sort , language ) , reverse = reverse ) return sorted
3671	def identify_phase ( T , P , Tm = None , Tb = None , Tc = None , Psat = None ) : r if Tm and T <= Tm : return 's' elif Tc and T >= Tc : return 'g' elif Psat : if P <= Psat : return 'g' elif P > Psat : return 'l' elif Tb : if 9E4 < P < 1.1E5 : if T < Tb : return 'l' else : return 'g' elif P > 1.1E5 and T <= Tb : return 'l' else : return None else : return None
1104	def set_seq1 ( self , a ) : if a is self . a : return self . a = a self . matching_blocks = self . opcodes = None
13415	def addlabel ( ax = None , toplabel = None , xlabel = None , ylabel = None , zlabel = None , clabel = None , cb = None , windowlabel = None , fig = None , axes = None ) : if ( axes is None ) and ( ax is not None ) : axes = ax if ( windowlabel is not None ) and ( fig is not None ) : fig . canvas . set_window_title ( windowlabel ) if fig is None : fig = _plt . gcf ( ) if fig is not None and axes is None : axes = fig . get_axes ( ) if axes == [ ] : logger . error ( 'No axes found!' ) if axes is not None : if toplabel is not None : axes . set_title ( toplabel ) if xlabel is not None : axes . set_xlabel ( xlabel ) if ylabel is not None : axes . set_ylabel ( ylabel ) if zlabel is not None : axes . set_zlabel ( zlabel ) if ( clabel is not None ) or ( cb is not None ) : if ( clabel is not None ) and ( cb is not None ) : cb . set_label ( clabel ) else : if clabel is None : logger . error ( 'Missing colorbar label' ) else : logger . error ( 'Missing colorbar instance' )
10309	def barv ( d , plt , title = None , rotation = 'vertical' ) : labels = sorted ( d , key = d . get , reverse = True ) index = range ( len ( labels ) ) plt . xticks ( index , labels , rotation = rotation ) plt . bar ( index , [ d [ v ] for v in labels ] ) if title is not None : plt . title ( title )
756	def createExperimentInferenceDir ( cls , experimentDir ) : path = cls . getExperimentInferenceDirPath ( experimentDir ) cls . makeDirectory ( path ) return path
5602	def create_app ( mapchete_files = None , zoom = None , bounds = None , single_input_file = None , mode = "continue" , debug = None ) : from flask import Flask , render_template_string app = Flask ( __name__ ) mapchete_processes = { os . path . splitext ( os . path . basename ( mapchete_file ) ) [ 0 ] : mapchete . open ( mapchete_file , zoom = zoom , bounds = bounds , single_input_file = single_input_file , mode = mode , with_cache = True , debug = debug ) for mapchete_file in mapchete_files } mp = next ( iter ( mapchete_processes . values ( ) ) ) pyramid_type = mp . config . process_pyramid . grid pyramid_srid = mp . config . process_pyramid . crs . to_epsg ( ) process_bounds = "," . join ( [ str ( i ) for i in mp . config . bounds_at_zoom ( ) ] ) grid = "g" if pyramid_srid == 3857 else "WGS84" web_pyramid = BufferedTilePyramid ( pyramid_type ) @ app . route ( '/' , methods = [ 'GET' ] ) def index ( ) : return render_template_string ( pkgutil . get_data ( 'mapchete.static' , 'index.html' ) . decode ( "utf-8" ) , srid = pyramid_srid , process_bounds = process_bounds , is_mercator = ( pyramid_srid == 3857 ) , process_names = mapchete_processes . keys ( ) ) @ app . route ( "/" . join ( [ "" , "wmts_simple" , "1.0.0" , "<string:mp_name>" , "default" , grid , "<int:zoom>" , "<int:row>" , "<int:col>.<string:file_ext>" ] ) , methods = [ 'GET' ] ) def get ( mp_name , zoom , row , col , file_ext ) : logger . debug ( "received tile (%s, %s, %s) for process %s" , zoom , row , col , mp_name ) return _tile_response ( mapchete_processes [ mp_name ] , web_pyramid . tile ( zoom , row , col ) , debug ) return app
575	def loadJsonValueFromFile ( inputFilePath ) : with open ( inputFilePath ) as fileObj : value = json . load ( fileObj ) return value
13393	def notify_client ( notifier_uri , client_id , status_code , message = None ) : payload = { "client_id" : client_id , "result" : { "response" : { "status_code" : status_code } } } if message is not None : payload [ "result" ] [ "response" ] [ "message" ] = message response = requests . post ( notifier_uri , json = payload ) if response . status_code != 201 : sys . stderr . write ( "failed to notify client: {}\n" . format ( payload ) ) sys . stderr . flush ( )
8029	def sizeClassifier ( path , min_size = DEFAULTS [ 'min_size' ] ) : filestat = _stat ( path ) if stat . S_ISLNK ( filestat . st_mode ) : return if filestat . st_size < min_size : return return filestat . st_size
5088	def ecommerce_coupon_url ( self , instance ) : if not instance . entitlement_id : return "N/A" return format_html ( '<a href="{base_url}/coupons/{id}" target="_blank">View coupon "{id}" details</a>' , base_url = settings . ECOMMERCE_PUBLIC_URL_ROOT , id = instance . entitlement_id )
9393	def check_important_sub_metrics ( self , sub_metric ) : if not self . important_sub_metrics : return False if sub_metric in self . important_sub_metrics : return True items = sub_metric . split ( '.' ) if items [ - 1 ] in self . important_sub_metrics : return True return False
11931	def using ( context , alias ) : if alias == '' : yield context else : try : widgets = context . render_context [ WIDGET_CONTEXT_KEY ] except KeyError : raise template . TemplateSyntaxError ( 'No widget libraries loaded!' ) try : block_set = widgets [ alias ] except KeyError : raise template . TemplateSyntaxError ( 'No widget library loaded for alias: %r' % alias ) context . render_context . push ( ) context . render_context [ BLOCK_CONTEXT_KEY ] = block_set context . render_context [ WIDGET_CONTEXT_KEY ] = widgets yield context context . render_context . pop ( )
3767	def zs_to_ws ( zs , MWs ) : r Mavg = sum ( zi * MWi for zi , MWi in zip ( zs , MWs ) ) ws = [ zi * MWi / Mavg for zi , MWi in zip ( zs , MWs ) ] return ws
10396	def unscored_nodes_iter ( self ) -> BaseEntity : for node , data in self . graph . nodes ( data = True ) : if self . tag not in data : yield node
10448	def click ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) size = self . _getobjectsize ( object_handle ) self . _grabfocus ( object_handle ) self . wait ( 0.5 ) self . generatemouseevent ( size [ 0 ] + size [ 2 ] / 2 , size [ 1 ] + size [ 3 ] / 2 , "b1c" ) return 1
6200	def _sim_timestamps ( self , max_rate , bg_rate , emission , i_start , rs , ip_start = 0 , scale = 10 , sort = True ) : counts_chunk = sim_timetrace_bg ( emission , max_rate , bg_rate , self . t_step , rs = rs ) nrows = emission . shape [ 0 ] if bg_rate is not None : nrows += 1 assert counts_chunk . shape == ( nrows , emission . shape [ 1 ] ) max_counts = counts_chunk . max ( ) if max_counts == 0 : return np . array ( [ ] , dtype = np . int64 ) , np . array ( [ ] , dtype = np . int64 ) time_start = i_start * scale time_stop = time_start + counts_chunk . shape [ 1 ] * scale ts_range = np . arange ( time_start , time_stop , scale , dtype = 'int64' ) times_chunk_p = [ ] par_index_chunk_p = [ ] for ip , counts_chunk_ip in enumerate ( counts_chunk ) : times_c_ip = [ ] for v in range ( 1 , max_counts + 1 ) : times_c_ip . append ( ts_range [ counts_chunk_ip >= v ] ) t = np . hstack ( times_c_ip ) times_chunk_p . append ( t ) par_index_chunk_p . append ( np . full ( t . size , ip + ip_start , dtype = 'u1' ) ) times_chunk = np . hstack ( times_chunk_p ) par_index_chunk = np . hstack ( par_index_chunk_p ) if sort : index_sort = times_chunk . argsort ( kind = 'mergesort' ) times_chunk = times_chunk [ index_sort ] par_index_chunk = par_index_chunk [ index_sort ] return times_chunk , par_index_chunk
4656	def broadcast ( self ) : if not self . _is_signed ( ) : self . sign ( ) if "operations" not in self or not self [ "operations" ] : log . warning ( "No operations in transaction! Returning" ) return ret = self . json ( ) if self . blockchain . nobroadcast : log . warning ( "Not broadcasting anything!" ) self . clear ( ) return ret try : if self . blockchain . blocking : ret = self . blockchain . rpc . broadcast_transaction_synchronous ( ret , api = "network_broadcast" ) ret . update ( ** ret . get ( "trx" , { } ) ) else : self . blockchain . rpc . broadcast_transaction ( ret , api = "network_broadcast" ) except Exception as e : raise e finally : self . clear ( ) return ret
11876	def getch ( ) : try : termios . tcsetattr ( _fd , termios . TCSANOW , _new_settings ) ch = sys . stdin . read ( 1 ) finally : termios . tcsetattr ( _fd , termios . TCSADRAIN , _old_settings ) return ch
7741	def _configure_io_handler ( self , handler ) : if self . check_events ( ) : return if handler in self . _unprepared_handlers : old_fileno = self . _unprepared_handlers [ handler ] prepared = self . _prepare_io_handler ( handler ) else : old_fileno = None prepared = True fileno = handler . fileno ( ) if old_fileno is not None and fileno != old_fileno : tag = self . _io_sources . pop ( handler , None ) if tag is not None : glib . source_remove ( tag ) if not prepared : self . _unprepared_handlers [ handler ] = fileno if fileno is None : logger . debug ( " {0!r}.fileno() is None, not polling" . format ( handler ) ) return events = 0 if handler . is_readable ( ) : logger . debug ( " {0!r} readable" . format ( handler ) ) events |= glib . IO_IN | glib . IO_ERR if handler . is_writable ( ) : logger . debug ( " {0!r} writable" . format ( handler ) ) events |= glib . IO_OUT | glib . IO_HUP | glib . IO_ERR if events : logger . debug ( " registering {0!r} handler fileno {1} for" " events {2}" . format ( handler , fileno , events ) ) glib . io_add_watch ( fileno , events , self . _io_callback , handler )
13807	def __get_current_datetime ( self ) : self . wql_time = "SELECT LocalDateTime FROM Win32_OperatingSystem" self . current_time = self . query ( self . wql_time ) self . current_time_string = str ( self . current_time [ 0 ] . get ( 'LocalDateTime' ) . split ( '.' ) [ 0 ] ) self . current_time_format = datetime . datetime . strptime ( self . current_time_string , '%Y%m%d%H%M%S' ) return self . current_time_format
4820	def refresh_token ( func ) : @ wraps ( func ) def inner ( self , * args , ** kwargs ) : if self . token_expired ( ) : self . connect ( ) return func ( self , * args , ** kwargs ) return inner
3240	def get_group_policy_document ( group_name , policy_name , client = None , ** kwargs ) : return client . get_group_policy ( GroupName = group_name , PolicyName = policy_name , ** kwargs ) [ 'PolicyDocument' ]
10614	def T ( self , T ) : self . _T = T self . _H = self . _calculate_H ( T )
1212	def WorkerAgentGenerator ( agent_class ) : if isinstance ( agent_class , str ) : agent_class = AgentsDictionary . get ( agent_class ) if not agent_class and agent_class . find ( '.' ) != - 1 : module_name , function_name = agent_class . rsplit ( '.' , 1 ) module = importlib . import_module ( module_name ) agent_class = getattr ( module , function_name ) class WorkerAgent ( agent_class ) : def __init__ ( self , model = None , ** kwargs ) : self . model = model if not issubclass ( agent_class , LearningAgent ) : kwargs . pop ( "network" ) super ( WorkerAgent , self ) . __init__ ( ** kwargs ) def initialize_model ( self ) : return self . model return WorkerAgent
9811	def grant ( username ) : try : PolyaxonClient ( ) . user . grant_superuser ( username ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not grant superuser role to user `{}`.' . format ( username ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Superuser role was granted successfully to user `{}`." . format ( username ) )
8457	def _needs_new_cc_config_for_update ( old_template , old_version , new_template , new_version ) : if old_template != new_template : return True else : return _cookiecutter_configs_have_changed ( new_template , old_version , new_version )
4130	def _remove_bias ( x , axis ) : "Subtracts an estimate of the mean from signal x at axis" padded_slice = [ slice ( d ) for d in x . shape ] padded_slice [ axis ] = np . newaxis mn = np . mean ( x , axis = axis ) return x - mn [ tuple ( padded_slice ) ]
10119	def circle ( cls , center , radius , n_vertices = 50 , ** kwargs ) : return cls . regular_polygon ( center , radius , n_vertices , ** kwargs )
4449	def load_document ( self , id ) : fields = self . redis . hgetall ( id ) if six . PY3 : f2 = { to_string ( k ) : to_string ( v ) for k , v in fields . items ( ) } fields = f2 try : del fields [ 'id' ] except KeyError : pass return Document ( id = id , ** fields )
11500	def get_community_children ( self , community_id , token = None ) : parameters = dict ( ) parameters [ 'id' ] = community_id if token : parameters [ 'token' ] = token response = self . request ( 'midas.community.children' , parameters ) return response
3864	async def send_message ( self , segments , image_file = None , image_id = None , image_user_id = None ) : async with self . _send_message_lock : if image_file : try : uploaded_image = await self . _client . upload_image ( image_file , return_uploaded_image = True ) except exceptions . NetworkError as e : logger . warning ( 'Failed to upload image: {}' . format ( e ) ) raise image_id = uploaded_image . image_id try : request = hangouts_pb2 . SendChatMessageRequest ( request_header = self . _client . get_request_header ( ) , event_request_header = self . _get_event_request_header ( ) , message_content = hangouts_pb2 . MessageContent ( segment = [ seg . serialize ( ) for seg in segments ] , ) , ) if image_id is not None : request . existing_media . photo . photo_id = image_id if image_user_id is not None : request . existing_media . photo . user_id = image_user_id request . existing_media . photo . is_custom_user_id = True await self . _client . send_chat_message ( request ) except exceptions . NetworkError as e : logger . warning ( 'Failed to send message: {}' . format ( e ) ) raise
3662	def calculate_integral ( self , T1 , T2 , method ) : r if method == ZABRANSKY_SPLINE : return self . Zabransky_spline . calculate_integral ( T1 , T2 ) elif method == ZABRANSKY_SPLINE_C : return self . Zabransky_spline_iso . calculate_integral ( T1 , T2 ) elif method == ZABRANSKY_SPLINE_SAT : return self . Zabransky_spline_sat . calculate_integral ( T1 , T2 ) elif method == ZABRANSKY_QUASIPOLYNOMIAL : return self . Zabransky_quasipolynomial . calculate_integral ( T1 , T2 ) elif method == ZABRANSKY_QUASIPOLYNOMIAL_C : return self . Zabransky_quasipolynomial_iso . calculate_integral ( T1 , T2 ) elif method == ZABRANSKY_QUASIPOLYNOMIAL_SAT : return self . Zabransky_quasipolynomial_sat . calculate_integral ( T1 , T2 ) elif method == POLING_CONST : return ( T2 - T1 ) * self . POLING_constant elif method == CRCSTD : return ( T2 - T1 ) * self . CRCSTD_constant elif method == DADGOSTAR_SHAW : dH = ( Dadgostar_Shaw_integral ( T2 , self . similarity_variable ) - Dadgostar_Shaw_integral ( T1 , self . similarity_variable ) ) return property_mass_to_molar ( dH , self . MW ) elif method in self . tabular_data or method == COOLPROP or method in [ ROWLINSON_POLING , ROWLINSON_BONDI ] : return float ( quad ( self . calculate , T1 , T2 , args = ( method ) ) [ 0 ] ) else : raise Exception ( 'Method not valid' )
12362	def list ( self , url_components = ( ) ) : resp = self . get ( url_components ) return resp . get ( self . result_key , [ ] )
734	def _calculateError ( self , recordNum , bucketIdxList ) : error = dict ( ) targetDist = numpy . zeros ( self . _maxBucketIdx + 1 ) numCategories = len ( bucketIdxList ) for bucketIdx in bucketIdxList : targetDist [ bucketIdx ] = 1.0 / numCategories for ( learnRecordNum , learnPatternNZ ) in self . _patternNZHistory : nSteps = recordNum - learnRecordNum if nSteps in self . steps : predictDist = self . inferSingleStep ( learnPatternNZ , self . _weightMatrix [ nSteps ] ) error [ nSteps ] = targetDist - predictDist return error
1740	def maybe_download_and_extract ( ) : dest_directory = '.' filename = DATA_URL . split ( '/' ) [ - 1 ] filepath = os . path . join ( dest_directory , filename ) if not os . path . exists ( filepath ) : def _progress ( count , block_size , total_size ) : sys . stdout . write ( '\r>> Downloading %s %.1f%%' % ( filename , float ( count * block_size ) / float ( total_size ) * 100.0 ) ) sys . stdout . flush ( ) filepath , _ = urllib . request . urlretrieve ( DATA_URL , filepath , _progress ) print ( ) statinfo = os . stat ( filepath ) print ( 'Successfully downloaded' , filename , statinfo . st_size , 'bytes.' ) extracted_dir_path = os . path . join ( dest_directory , 'trees' ) if not os . path . exists ( extracted_dir_path ) : zip_ref = zipfile . ZipFile ( filepath , 'r' ) zip_ref . extractall ( dest_directory ) zip_ref . close ( )
257	def print_round_trip_stats ( round_trips , hide_pos = False ) : stats = gen_round_trip_stats ( round_trips ) print_table ( stats [ 'summary' ] , float_format = '{:.2f}' . format , name = 'Summary stats' ) print_table ( stats [ 'pnl' ] , float_format = '${:.2f}' . format , name = 'PnL stats' ) print_table ( stats [ 'duration' ] , float_format = '{:.2f}' . format , name = 'Duration stats' ) print_table ( stats [ 'returns' ] * 100 , float_format = '{:.2f}%' . format , name = 'Return stats' ) if not hide_pos : stats [ 'symbols' ] . columns = stats [ 'symbols' ] . columns . map ( format_asset ) print_table ( stats [ 'symbols' ] * 100 , float_format = '{:.2f}%' . format , name = 'Symbol stats' )
11464	def download ( self , source_file , target_folder = '' ) : current_folder = self . _ftp . pwd ( ) if not target_folder . startswith ( '/' ) : target_folder = join ( getcwd ( ) , target_folder ) folder = os . path . dirname ( source_file ) self . cd ( folder ) if folder . startswith ( "/" ) : folder = folder [ 1 : ] destination_folder = join ( target_folder , folder ) if not os . path . exists ( destination_folder ) : print ( "Creating folder" , destination_folder ) os . makedirs ( destination_folder ) source_file = os . path . basename ( source_file ) destination = join ( destination_folder , source_file ) try : with open ( destination , 'wb' ) as result : self . _ftp . retrbinary ( 'RETR %s' % ( source_file , ) , result . write ) except error_perm as e : print ( e ) remove ( join ( target_folder , source_file ) ) raise self . _ftp . cwd ( current_folder )
10615	def clone ( self ) : result = copy . copy ( self ) result . _compound_masses = copy . deepcopy ( self . _compound_masses ) return result
1026	def encode ( input , output , quotetabs , header = 0 ) : if b2a_qp is not None : data = input . read ( ) odata = b2a_qp ( data , quotetabs = quotetabs , header = header ) output . write ( odata ) return def write ( s , output = output , lineEnd = '\n' ) : if s and s [ - 1 : ] in ' \t' : output . write ( s [ : - 1 ] + quote ( s [ - 1 ] ) + lineEnd ) elif s == '.' : output . write ( quote ( s ) + lineEnd ) else : output . write ( s + lineEnd ) prevline = None while 1 : line = input . readline ( ) if not line : break outline = [ ] stripped = '' if line [ - 1 : ] == '\n' : line = line [ : - 1 ] stripped = '\n' for c in line : if needsquoting ( c , quotetabs , header ) : c = quote ( c ) if header and c == ' ' : outline . append ( '_' ) else : outline . append ( c ) if prevline is not None : write ( prevline ) thisline = EMPTYSTRING . join ( outline ) while len ( thisline ) > MAXLINESIZE : write ( thisline [ : MAXLINESIZE - 1 ] , lineEnd = '=\n' ) thisline = thisline [ MAXLINESIZE - 1 : ] prevline = thisline if prevline is not None : write ( prevline , lineEnd = stripped )
8215	def show_variables_window ( self ) : if self . var_window is None and self . bot . _vars : self . var_window = VarWindow ( self , self . bot , '%s variables' % ( self . title or 'Shoebot' ) ) self . var_window . window . connect ( "destroy" , self . var_window_closed )
11661	def transform ( self , X ) : self . _check_fitted ( ) X = as_features ( X , stack = True ) assignments = self . kmeans_fit_ . predict ( X . stacked_features ) return self . _group_assignments ( X , assignments )
7768	def _stream_authorized ( self , event ) : with self . lock : if event . stream != self . stream : return self . me = event . stream . me self . peer = event . stream . peer presence = self . settings [ u"initial_presence" ] if presence : self . send ( presence )
13911	def new_user ( yaml_path ) : print 'Retrieve API Key from https://www.shirts.io/accounts/api_console/' api_key = raw_input ( 'Shirts.io API Key: ' ) tokens = { 'api_key' : api_key , } yaml_file = open ( yaml_path , 'w+' ) yaml . dump ( tokens , yaml_file , indent = 2 ) yaml_file . close ( ) return tokens
3443	def save_json_model ( model , filename , sort = False , pretty = False , ** kwargs ) : obj = model_to_dict ( model , sort = sort ) obj [ u"version" ] = JSON_SPEC if pretty : dump_opts = { "indent" : 4 , "separators" : ( "," , ": " ) , "sort_keys" : True , "allow_nan" : False } else : dump_opts = { "indent" : 0 , "separators" : ( "," , ":" ) , "sort_keys" : False , "allow_nan" : False } dump_opts . update ( ** kwargs ) if isinstance ( filename , string_types ) : with open ( filename , "w" ) as file_handle : json . dump ( obj , file_handle , ** dump_opts ) else : json . dump ( obj , filename , ** dump_opts )
12738	def create_bodies ( self , translate = ( 0 , 1 , 0 ) , size = 0.1 ) : stack = [ ( 'root' , 0 , self . root [ 'position' ] + translate ) ] while stack : name , depth , end = stack . pop ( ) for child in self . hierarchy . get ( name , ( ) ) : stack . append ( ( child , depth + 1 , end + self . bones [ child ] . end ) ) if name not in self . bones : continue bone = self . bones [ name ] body = self . world . create_body ( 'box' , name = bone . name , density = self . density , lengths = ( size , size , bone . length ) ) body . color = self . color x , y , z = end - bone . direction * bone . length / 2 body . position = x , z , y u = bone . direction v = np . cross ( u , [ 0 , 1 , 0 ] ) l = np . linalg . norm ( v ) if l > 0 : v /= l rot = np . vstack ( [ np . cross ( u , v ) , v , u ] ) . T swizzle = [ [ 1 , 0 , 0 ] , [ 0 , 0 , 1 ] , [ 0 , - 1 , 0 ] ] body . rotation = np . dot ( swizzle , rot ) self . bodies . append ( body )
10090	def rst2node ( doc_name , data ) : if not data : return parser = docutils . parsers . rst . Parser ( ) document = docutils . utils . new_document ( '<%s>' % doc_name ) document . settings = docutils . frontend . OptionParser ( ) . get_default_values ( ) document . settings . tab_width = 4 document . settings . pep_references = False document . settings . rfc_references = False document . settings . env = Env ( ) parser . parse ( data , document ) if len ( document . children ) == 1 : return document . children [ 0 ] else : par = docutils . nodes . paragraph ( ) for child in document . children : par += child return par
10115	def parse ( grid_str , mode = MODE_ZINC , charset = 'utf-8' ) : if isinstance ( grid_str , six . binary_type ) : grid_str = grid_str . decode ( encoding = charset ) _parse = functools . partial ( parse_grid , mode = mode , charset = charset ) if mode == MODE_JSON : if isinstance ( grid_str , six . string_types ) : grid_data = json . loads ( grid_str ) else : grid_data = grid_str if isinstance ( grid_data , dict ) : return _parse ( grid_data ) else : return list ( map ( _parse , grid_data ) ) else : return list ( map ( _parse , GRID_SEP . split ( grid_str . rstrip ( ) ) ) )
8118	def invert ( self ) : m = self . matrix d = m [ 0 ] * m [ 4 ] - m [ 1 ] * m [ 3 ] self . matrix = [ m [ 4 ] / d , - m [ 1 ] / d , 0 , - m [ 3 ] / d , m [ 0 ] / d , 0 , ( m [ 3 ] * m [ 7 ] - m [ 4 ] * m [ 6 ] ) / d , - ( m [ 0 ] * m [ 7 ] - m [ 1 ] * m [ 6 ] ) / d , 1 ]
11511	def set_item_metadata ( self , token , item_id , element , value , qualifier = None ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'itemId' ] = item_id parameters [ 'element' ] = element parameters [ 'value' ] = value if qualifier : parameters [ 'qualifier' ] = qualifier response = self . request ( 'midas.item.setmetadata' , parameters ) return response
6535	def merge_dict ( dict1 , dict2 , merge_lists = False ) : merged = dict ( dict1 ) for key , value in iteritems ( dict2 ) : if isinstance ( merged . get ( key ) , dict ) : merged [ key ] = merge_dict ( merged [ key ] , value ) elif merge_lists and isinstance ( merged . get ( key ) , list ) : merged [ key ] = merge_list ( merged [ key ] , value ) else : merged [ key ] = value return merged
12319	def permalink ( self , repo , path ) : if not os . path . exists ( path ) : return ( None , None ) cwd = os . getcwd ( ) if os . path . isfile ( path ) : os . chdir ( os . path . dirname ( path ) ) rootdir = self . _run ( [ "rev-parse" , "--show-toplevel" ] ) if "fatal" in rootdir : return ( None , None ) os . chdir ( rootdir ) relpath = os . path . relpath ( path , rootdir ) sha1 = self . _run ( [ "log" , "-n" , "1" , "--format=format:%H" , relpath ] ) remoteurl = self . _run ( [ "config" , "--get" , "remote.origin.url" ] ) os . chdir ( cwd ) m = re . search ( '^git@([^:\/]+):([^/]+)/([^/]+)' , remoteurl ) if m is None : m = re . search ( '^https://([^:/]+)/([^/]+)/([^/]+)' , remoteurl ) if m is not None : domain = m . group ( 1 ) username = m . group ( 2 ) project = m . group ( 3 ) if project . endswith ( ".git" ) : project = project [ : - 4 ] permalink = "https://{}/{}/{}/blob/{}/{}" . format ( domain , username , project , sha1 , relpath ) return ( relpath , permalink ) else : return ( None , None )
11692	def set_fields ( self , changeset ) : self . id = int ( changeset . get ( 'id' ) ) self . user = changeset . get ( 'user' ) self . uid = changeset . get ( 'uid' ) self . editor = changeset . get ( 'created_by' , None ) self . review_requested = changeset . get ( 'review_requested' , False ) self . host = changeset . get ( 'host' , 'Not reported' ) self . bbox = changeset . get ( 'bbox' ) . wkt self . comment = changeset . get ( 'comment' , 'Not reported' ) self . source = changeset . get ( 'source' , 'Not reported' ) self . imagery_used = changeset . get ( 'imagery_used' , 'Not reported' ) self . date = datetime . strptime ( changeset . get ( 'created_at' ) , '%Y-%m-%dT%H:%M:%SZ' ) self . suspicion_reasons = [ ] self . is_suspect = False self . powerfull_editor = False
9908	def send_duplicate_notification ( self ) : email_utils . send_email ( from_email = settings . DEFAULT_FROM_EMAIL , recipient_list = [ self . email ] , subject = _ ( "Registration Attempt" ) , template_name = "rest_email_auth/emails/duplicate-email" , ) logger . info ( "Sent duplicate email notification to: %s" , self . email )
13140	def read ( self ) : if not self . __content__ : self . __retriever__ = self . __resolver__ . resolve ( self . uri ) self . __content__ , self . __mimetype__ = self . __retriever__ . read ( self . uri ) return self . __content__
10773	def add_node ( self , node , offset ) : width = self . end [ 0 ] - self . start [ 0 ] height = self . end [ 1 ] - self . start [ 1 ] node . x = self . start [ 0 ] + ( width * offset ) node . y = self . start [ 1 ] + ( height * offset ) self . nodes [ node . ID ] = node
2652	def execute_no_wait ( self , cmd , walltime = 2 , envs = { } ) : stdin , stdout , stderr = self . ssh_client . exec_command ( self . prepend_envs ( cmd , envs ) , bufsize = - 1 , timeout = walltime ) return None , stdout , stderr
1492	def register_timer_task_in_sec ( self , task , second ) : second_in_float = float ( second ) expiration = time . time ( ) + second_in_float heappush ( self . timer_tasks , ( expiration , task ) )
2713	def get_object ( cls , api_token , snapshot_id ) : snapshot = cls ( token = api_token , id = snapshot_id ) snapshot . load ( ) return snapshot
9850	def export ( self , filename , file_format = None , type = None , typequote = '"' ) : exporter = self . _get_exporter ( filename , file_format = file_format ) exporter ( filename , type = type , typequote = typequote )
12637	def get_groups_in_same_folder ( self , folder_depth = 3 ) : group_pairs = [ ] key_dicoms = list ( self . dicom_groups . keys ( ) ) idx = len ( key_dicoms ) while idx > 0 : group1 = key_dicoms . pop ( ) dir_group1 = get_folder_subpath ( group1 , folder_depth ) for group in key_dicoms : if group . startswith ( dir_group1 ) : group_pairs . append ( ( group1 , group ) ) idx -= 1 return group_pairs
615	def _generateInferenceArgs ( options , tokenReplacements ) : inferenceType = options [ 'inferenceType' ] optionInferenceArgs = options . get ( 'inferenceArgs' , None ) resultInferenceArgs = { } predictedField = _getPredictedField ( options ) [ 0 ] if inferenceType in ( InferenceType . TemporalNextStep , InferenceType . TemporalAnomaly ) : assert predictedField , "Inference Type '%s' needs a predictedField " "specified in the inferenceArgs dictionary" % inferenceType if optionInferenceArgs : if options [ 'dynamicPredictionSteps' ] : altOptionInferenceArgs = copy . deepcopy ( optionInferenceArgs ) altOptionInferenceArgs [ 'predictionSteps' ] = '$REPLACE_ME' resultInferenceArgs = pprint . pformat ( altOptionInferenceArgs ) resultInferenceArgs = resultInferenceArgs . replace ( "'$REPLACE_ME'" , '[predictionSteps]' ) else : resultInferenceArgs = pprint . pformat ( optionInferenceArgs ) tokenReplacements [ '\$INFERENCE_ARGS' ] = resultInferenceArgs tokenReplacements [ '\$PREDICTION_FIELD' ] = predictedField
7292	def set_post_data ( self ) : self . form . data = self . post_data_dict for field_key , field in self . form . fields . items ( ) : if has_digit ( field_key ) : base_key = make_key ( field_key , exclude_last_string = True ) for key in self . post_data_dict . keys ( ) : if base_key in key : self . form . fields . update ( { key : field } )
6521	def directories ( self , filters = None , containing = None ) : filters = compile_masks ( filters or [ r'.*' ] ) contains = compile_masks ( containing ) for dirname , files in iteritems ( self . _found ) : relpath = text_type ( Path ( dirname ) . relative_to ( self . base_path ) ) if matches_masks ( relpath , filters ) : if not contains or self . _contains ( files , contains ) : yield dirname
4238	def get_traffic_meter ( self ) : _LOGGER . info ( "Get traffic meter" ) def parse_text ( text ) : def tofloats ( lst ) : return ( float ( t ) for t in lst ) try : if "/" in text : return tuple ( tofloats ( text . split ( '/' ) ) ) elif ":" in text : hour , mins = tofloats ( text . split ( ':' ) ) return timedelta ( hours = hour , minutes = mins ) else : return float ( text ) except ValueError : return None success , response = self . _make_request ( SERVICE_DEVICE_CONFIG , "GetTrafficMeterStatistics" ) if not success : return None success , node = _find_node ( response . text , ".//GetTrafficMeterStatisticsResponse" ) if not success : return None return { t . tag : parse_text ( t . text ) for t in node }
2818	def convert_padding ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting padding...' ) if params [ 'mode' ] == 'constant' : if params [ 'value' ] != 0.0 : raise AssertionError ( 'Cannot convert non-zero padding' ) if names : tf_name = 'PADD' + random_string ( 4 ) else : tf_name = w_name + str ( random . random ( ) ) padding_name = tf_name padding_layer = keras . layers . ZeroPadding2D ( padding = ( ( params [ 'pads' ] [ 2 ] , params [ 'pads' ] [ 6 ] ) , ( params [ 'pads' ] [ 3 ] , params [ 'pads' ] [ 7 ] ) ) , name = padding_name ) layers [ scope_name ] = padding_layer ( layers [ inputs [ 0 ] ] ) elif params [ 'mode' ] == 'reflect' : def target_layer ( x , pads = params [ 'pads' ] ) : layer = tf . pad ( x , [ [ 0 , 0 ] , [ 0 , 0 ] , [ pads [ 2 ] , pads [ 6 ] ] , [ pads [ 3 ] , pads [ 7 ] ] ] , 'REFLECT' ) return layer lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
10104	def execute ( self , timeout = None ) : logger . debug ( ' > Batch API request (length %s)' % len ( self . _commands ) ) auth = self . _build_http_auth ( ) headers = self . _build_request_headers ( ) logger . debug ( '\tbatch headers: %s' % headers ) logger . debug ( '\tbatch command length: %s' % len ( self . _commands ) ) path = self . _build_request_path ( self . BATCH_ENDPOINT ) data = json . dumps ( self . _commands , cls = self . _json_encoder ) r = requests . post ( path , auth = auth , headers = headers , data = data , timeout = ( self . DEFAULT_TIMEOUT if timeout is None else timeout ) ) self . _commands = [ ] logger . debug ( '\tresponse code:%s' % r . status_code ) try : logger . debug ( '\tresponse: %s' % r . json ( ) ) except : logger . debug ( '\tresponse: %s' % r . content ) return r
4531	def construct ( cls , project , ** desc ) : return cls ( project . drivers , maker = project . maker , ** desc )
1886	def _hook_xfer_mem ( self , uc , access , address , size , value , data ) : assert access in ( UC_MEM_WRITE , UC_MEM_READ , UC_MEM_FETCH ) if access == UC_MEM_WRITE : self . _cpu . write_int ( address , value , size * 8 ) elif access == UC_MEM_READ : value = self . _cpu . read_bytes ( address , size ) if address in self . _should_be_written : return True self . _should_be_written [ address ] = value self . _should_try_again = True return False return True
7547	def make ( data , samples ) : outfile = open ( os . path . join ( data . dirs . outfiles , data . name + ".alleles" ) , 'w' ) lines = open ( os . path . join ( data . dirs . outfiles , data . name + ".loci" ) , 'r' ) longname = max ( len ( x ) for x in data . samples . keys ( ) ) name_padding = 5 writing = [ ] loc = 0 for line in lines : if ">" in line : name , seq = line . split ( " " ) [ 0 ] , line . split ( " " ) [ - 1 ] allele1 , allele2 = splitalleles ( seq . strip ( ) ) writing . append ( name + "_0" + " " * ( longname - len ( name ) - 2 + name_padding ) + allele1 ) writing . append ( name + "_1" + " " * ( longname - len ( name ) - 2 + name_padding ) + allele2 ) else : writing . append ( line . strip ( ) ) loc += 1 if not loc % 10000 : outfile . write ( "\n" . join ( writing ) + "\n" ) writing = [ ] outfile . write ( "\n" . join ( writing ) ) outfile . close ( )
13095	def callback ( self , event ) : if event . mask == 0x00000008 : if event . name . endswith ( '.json' ) : print_success ( "Ldapdomaindump file found" ) if event . name in [ 'domain_groups.json' , 'domain_users.json' ] : if event . name == 'domain_groups.json' : self . domain_groups_file = event . pathname if event . name == 'domain_users.json' : self . domain_users_file = event . pathname if self . domain_groups_file and self . domain_users_file : print_success ( "Importing users" ) subprocess . Popen ( [ 'jk-import-domaindump' , self . domain_groups_file , self . domain_users_file ] ) elif event . name == 'domain_computers.json' : print_success ( "Importing computers" ) subprocess . Popen ( [ 'jk-import-domaindump' , event . pathname ] ) self . ldap_strings = [ ] self . write_targets ( ) if event . name . endswith ( '_samhashes.sam' ) : host = event . name . replace ( '_samhashes.sam' , '' ) print_success ( "Secretsdump file, host ip: {}" . format ( host ) ) subprocess . Popen ( [ 'jk-import-secretsdump' , event . pathname ] ) self . ips . remove ( host ) self . write_targets ( )
8140	def invert ( self ) : alpha = self . img . split ( ) [ 3 ] self . img = self . img . convert ( "RGB" ) self . img = ImageOps . invert ( self . img ) self . img = self . img . convert ( "RGBA" ) self . img . putalpha ( alpha )
9512	def gaps ( self , min_length = 1 ) : gaps = [ ] regex = re . compile ( 'N+' , re . IGNORECASE ) for m in regex . finditer ( self . seq ) : if m . span ( ) [ 1 ] - m . span ( ) [ 0 ] + 1 >= min_length : gaps . append ( intervals . Interval ( m . span ( ) [ 0 ] , m . span ( ) [ 1 ] - 1 ) ) return gaps
11076	def load_user_rights ( self , user ) : if user . username in self . admins : user . is_admin = True elif not hasattr ( user , 'is_admin' ) : user . is_admin = False
1240	def _move ( self , index , new_priority ) : item , old_priority = self . _memory [ index ] old_priority = old_priority or 0 self . _memory [ index ] = _SumRow ( item , new_priority ) self . _update_internal_nodes ( index , new_priority - old_priority )
8339	def _findAll ( self , name , attrs , text , limit , generator , ** kwargs ) : "Iterates over a generator looking for things that match." if isinstance ( name , SoupStrainer ) : strainer = name else : strainer = SoupStrainer ( name , attrs , text , ** kwargs ) results = ResultSet ( strainer ) g = generator ( ) while True : try : i = g . next ( ) except StopIteration : break if i : found = strainer . search ( i ) if found : results . append ( found ) if limit and len ( results ) >= limit : break return results
167	def is_fully_within_image ( self , image , default = False ) : if len ( self . coords ) == 0 : return default return np . all ( self . get_pointwise_inside_image_mask ( image ) )
9382	def aggregate_count_over_time ( self , metric_store , line_data , transaction_list , aggregate_timestamp ) : for transaction in transaction_list : if line_data . get ( 's' ) == 'true' : all_qps = metric_store [ 'qps' ] else : all_qps = metric_store [ 'eqps' ] qps = all_qps [ transaction ] if aggregate_timestamp in qps : qps [ aggregate_timestamp ] += 1 else : qps [ aggregate_timestamp ] = 1 return None
7520	def write_usnps ( data , sidx , pnames ) : tmparrs = os . path . join ( data . dirs . outfiles , "tmp-{}.h5" . format ( data . name ) ) with h5py . File ( tmparrs , 'r' ) as io5 : bisarr = io5 [ "bisarr" ] end = np . where ( np . all ( bisarr [ : ] == "" , axis = 0 ) ) [ 0 ] if np . any ( end ) : end = end . min ( ) else : end = bisarr . shape [ 1 ] with open ( data . outfiles . usnpsphy , 'w' ) as out : out . write ( "{} {}\n" . format ( bisarr . shape [ 0 ] , end ) ) for idx , name in enumerate ( pnames ) : out . write ( "{}{}\n" . format ( name , "" . join ( bisarr [ idx , : end ] ) ) )
4039	def _cache ( self , response , key ) : thetime = datetime . datetime . utcnow ( ) . replace ( tzinfo = pytz . timezone ( "GMT" ) ) self . templates [ key ] = { "tmplt" : response . json ( ) , "updated" : thetime } return copy . deepcopy ( response . json ( ) )
13110	def lookup ( cls , key , get = False ) : if get : item = cls . _item_dict . get ( key ) return item . name if item else key return cls . _item_dict [ key ] . name
2947	def deserialize ( cls , serializer , wf_spec , s_state , ** kwargs ) : return serializer . deserialize_trigger ( wf_spec , s_state , ** kwargs )
5509	def get_permissions ( self , path ) : path = pathlib . PurePosixPath ( path ) parents = filter ( lambda p : p . is_parent ( path ) , self . permissions ) perm = min ( parents , key = lambda p : len ( path . relative_to ( p . path ) . parts ) , default = Permission ( ) , ) return perm
9640	def require_template_debug ( f ) : def _ ( * args , ** kwargs ) : TEMPLATE_DEBUG = getattr ( settings , 'TEMPLATE_DEBUG' , False ) return f ( * args , ** kwargs ) if TEMPLATE_DEBUG else '' return _
1792	def INC ( cpu , dest ) : arg0 = dest . read ( ) res = dest . write ( arg0 + 1 ) res &= ( 1 << dest . size ) - 1 SIGN_MASK = 1 << ( dest . size - 1 ) cpu . AF = ( ( arg0 ^ 1 ) ^ res ) & 0x10 != 0 cpu . ZF = res == 0 cpu . SF = ( res & SIGN_MASK ) != 0 cpu . OF = res == SIGN_MASK cpu . PF = cpu . _calculate_parity_flag ( res )
3877	async def _on_event ( self , event_ ) : conv_id = event_ . conversation_id . id try : conv = await self . _get_or_fetch_conversation ( conv_id ) except exceptions . NetworkError : logger . warning ( 'Failed to fetch conversation for event notification: %s' , conv_id ) else : self . _sync_timestamp = parsers . from_timestamp ( event_ . timestamp ) conv_event = conv . add_event ( event_ ) if conv_event is not None : await self . on_event . fire ( conv_event ) await conv . on_event . fire ( conv_event )
9081	def find ( self , query , ** kwargs ) : if 'providers' not in kwargs : providers = self . get_providers ( ) else : pargs = kwargs [ 'providers' ] if isinstance ( pargs , list ) : providers = self . get_providers ( ids = pargs ) else : providers = self . get_providers ( ** pargs ) kwarguments = { } if 'language' in kwargs : kwarguments [ 'language' ] = kwargs [ 'language' ] return [ { 'id' : p . get_vocabulary_id ( ) , 'concepts' : p . find ( query , ** kwarguments ) } for p in providers ]
3983	def get_same_container_repos_from_spec ( app_or_library_spec ) : repos = set ( ) app_or_lib_repo = get_repo_of_app_or_library ( app_or_library_spec . name ) if app_or_lib_repo is not None : repos . add ( app_or_lib_repo ) for dependent_name in app_or_library_spec [ 'depends' ] [ 'libs' ] : repos . add ( get_repo_of_app_or_library ( dependent_name ) ) return repos
5360	def execute_batch_tasks ( self , tasks_cls , big_delay = 0 , small_delay = 0 , wait_for_threads = True ) : def _split_tasks ( tasks_cls ) : backend_t = [ ] global_t = [ ] for t in tasks_cls : if t . is_backend_task ( t ) : backend_t . append ( t ) else : global_t . append ( t ) return backend_t , global_t backend_tasks , global_tasks = _split_tasks ( tasks_cls ) logger . debug ( 'backend_tasks = %s' % ( backend_tasks ) ) logger . debug ( 'global_tasks = %s' % ( global_tasks ) ) threads = [ ] stopper = threading . Event ( ) if len ( backend_tasks ) > 0 : repos_backend = self . _get_repos_by_backend ( ) for backend in repos_backend : t = TasksManager ( backend_tasks , backend , stopper , self . config , small_delay ) threads . append ( t ) t . start ( ) if len ( global_tasks ) > 0 : gt = TasksManager ( global_tasks , "Global tasks" , stopper , self . config , big_delay ) threads . append ( gt ) gt . start ( ) if big_delay > 0 : when = datetime . now ( ) + timedelta ( seconds = big_delay ) when_str = when . strftime ( '%a, %d %b %Y %H:%M:%S %Z' ) logger . info ( "%s will be executed on %s" % ( global_tasks , when_str ) ) if wait_for_threads : time . sleep ( 1 ) stopper . set ( ) for t in threads : t . join ( ) self . __check_queue_for_errors ( ) logger . debug ( "[thread:main] All threads (and their tasks) are finished" )
3820	async def create_conversation ( self , create_conversation_request ) : response = hangouts_pb2 . CreateConversationResponse ( ) await self . _pb_request ( 'conversations/createconversation' , create_conversation_request , response ) return response
121	def get_batch ( self ) : if self . all_finished ( ) : return None batch_str = self . queue_result . get ( ) batch = pickle . loads ( batch_str ) if batch is not None : return batch else : self . nb_workers_finished += 1 if self . nb_workers_finished >= self . nb_workers : try : self . queue_source . get ( timeout = 0.001 ) except QueueEmpty : pass return None else : return self . get_batch ( )
7132	def prepare_docset ( source , dest , name , index_page , enable_js , online_redirect_url ) : resources = os . path . join ( dest , "Contents" , "Resources" ) docs = os . path . join ( resources , "Documents" ) os . makedirs ( resources ) db_conn = sqlite3 . connect ( os . path . join ( resources , "docSet.dsidx" ) ) db_conn . row_factory = sqlite3 . Row db_conn . execute ( "CREATE TABLE searchIndex(id INTEGER PRIMARY KEY, name TEXT, " "type TEXT, path TEXT)" ) db_conn . commit ( ) plist_path = os . path . join ( dest , "Contents" , "Info.plist" ) plist_cfg = { "CFBundleIdentifier" : name , "CFBundleName" : name , "DocSetPlatformFamily" : name . lower ( ) , "DashDocSetFamily" : "python" , "isDashDocset" : True , "isJavaScriptEnabled" : enable_js , } if index_page is not None : plist_cfg [ "dashIndexFilePath" ] = index_page if online_redirect_url is not None : plist_cfg [ "DashDocSetFallbackURL" ] = online_redirect_url write_plist ( plist_cfg , plist_path ) shutil . copytree ( source , docs ) return DocSet ( path = dest , docs = docs , plist = plist_path , db_conn = db_conn )
566	def createEncoder ( ) : consumption_encoder = ScalarEncoder ( 21 , 0.0 , 100.0 , n = 50 , name = "consumption" , clipInput = True ) time_encoder = DateEncoder ( timeOfDay = ( 21 , 9.5 ) , name = "timestamp_timeOfDay" ) encoder = MultiEncoder ( ) encoder . addEncoder ( "consumption" , consumption_encoder ) encoder . addEncoder ( "timestamp" , time_encoder ) return encoder
8966	def which ( command , path = None , verbose = 0 , exts = None ) : matched = whichgen ( command , path , verbose , exts ) try : match = next ( matched ) except StopIteration : raise WhichError ( "Could not find '%s' on the path." % command ) else : return match
11579	def system_reset ( self ) : data = chr ( self . SYSTEM_RESET ) self . pymata . transport . write ( data ) with self . pymata . data_lock : for _ in range ( len ( self . digital_response_table ) ) : self . digital_response_table . pop ( ) for _ in range ( len ( self . analog_response_table ) ) : self . analog_response_table . pop ( ) for pin in range ( 0 , self . total_pins_discovered ) : response_entry = [ self . pymata . INPUT , 0 , None ] self . digital_response_table . append ( response_entry ) for pin in range ( 0 , self . number_of_analog_pins_discovered ) : response_entry = [ self . pymata . INPUT , 0 , None ] self . analog_response_table . append ( response_entry )
8850	def setup_editor ( self , editor ) : editor . cursorPositionChanged . connect ( self . on_cursor_pos_changed ) try : m = editor . modes . get ( modes . GoToAssignmentsMode ) except KeyError : pass else : assert isinstance ( m , modes . GoToAssignmentsMode ) m . out_of_doc . connect ( self . on_goto_out_of_doc )
7473	def singlecat ( data , sample , bseeds , sidx , nloci ) : LOGGER . info ( "in single cat here" ) isref = 'reference' in data . paramsdict [ "assembly_method" ] with h5py . File ( bseeds , 'r' ) as io5 : hits = io5 [ "uarr" ] [ : ] hits = hits [ hits [ : , 1 ] == sidx , : ] seeds = io5 [ "seedsarr" ] [ : ] seeds = seeds [ seeds [ : , 1 ] == sidx , : ] full = np . concatenate ( ( seeds , hits ) ) full = full [ full [ : , 0 ] . argsort ( ) ] maxlen = data . _hackersonly [ "max_fragment_length" ] + 20 ocatg = np . zeros ( ( nloci , maxlen , 4 ) , dtype = np . uint32 ) onall = np . zeros ( nloci , dtype = np . uint8 ) ochrom = np . zeros ( ( nloci , 3 ) , dtype = np . int64 ) if not sample . files . database : raise IPyradWarningExit ( "missing catg file - {}" . format ( sample . name ) ) with h5py . File ( sample . files . database , 'r' ) as io5 : catarr = io5 [ "catg" ] [ : ] tmp = catarr [ full [ : , 2 ] , : maxlen , : ] del catarr ocatg [ full [ : , 0 ] , : tmp . shape [ 1 ] , : ] = tmp del tmp nall = io5 [ "nalleles" ] [ : ] onall [ full [ : , 0 ] ] = nall [ full [ : , 2 ] ] del nall if isref : chrom = io5 [ "chroms" ] [ : ] ochrom [ full [ : , 0 ] ] = chrom [ full [ : , 2 ] ] del chrom ipath = os . path . join ( data . dirs . across , data . name + ".tmp.indels.hdf5" ) with h5py . File ( ipath , 'r' ) as ih5 : indels = ih5 [ "indels" ] [ sidx , : , : maxlen ] newcatg = inserted_indels ( indels , ocatg ) del ocatg , indels smpio = os . path . join ( data . dirs . across , sample . name + '.tmp.h5' ) with h5py . File ( smpio , 'w' ) as oh5 : oh5 . create_dataset ( "icatg" , data = newcatg , dtype = np . uint32 ) oh5 . create_dataset ( "inall" , data = onall , dtype = np . uint8 ) if isref : oh5 . create_dataset ( "ichrom" , data = ochrom , dtype = np . int64 )
2204	def find_exe ( name , multi = False , path = None ) : candidates = find_path ( name , path = path , exact = True ) mode = os . X_OK | os . F_OK results = ( fpath for fpath in candidates if os . access ( fpath , mode ) and not isdir ( fpath ) ) if not multi : for fpath in results : return fpath else : return list ( results )
13871	def CanonicalPath ( path ) : path = os . path . normpath ( path ) path = os . path . abspath ( path ) path = os . path . normcase ( path ) return path
3819	async def add_user ( self , add_user_request ) : response = hangouts_pb2 . AddUserResponse ( ) await self . _pb_request ( 'conversations/adduser' , add_user_request , response ) return response
1368	def register_on_message ( self , msg_builder ) : message = msg_builder ( ) Log . debug ( "In register_on_message(): %s" % message . DESCRIPTOR . full_name ) self . registered_message_map [ message . DESCRIPTOR . full_name ] = msg_builder
5552	def clip_bounds ( bounds = None , clip = None ) : bounds = Bounds ( * bounds ) clip = Bounds ( * clip ) return Bounds ( max ( bounds . left , clip . left ) , max ( bounds . bottom , clip . bottom ) , min ( bounds . right , clip . right ) , min ( bounds . top , clip . top ) )
430	def read_images ( img_list , path = '' , n_threads = 10 , printable = True ) : imgs = [ ] for idx in range ( 0 , len ( img_list ) , n_threads ) : b_imgs_list = img_list [ idx : idx + n_threads ] b_imgs = tl . prepro . threading_data ( b_imgs_list , fn = read_image , path = path ) imgs . extend ( b_imgs ) if printable : tl . logging . info ( 'read %d from %s' % ( len ( imgs ) , path ) ) return imgs
3369	def set_objective ( model , value , additive = False ) : interface = model . problem reverse_value = model . solver . objective . expression reverse_value = interface . Objective ( reverse_value , direction = model . solver . objective . direction , sloppy = True ) if isinstance ( value , dict ) : if not model . objective . is_Linear : raise ValueError ( 'can only update non-linear objectives ' 'additively using object of class ' 'model.problem.Objective, not %s' % type ( value ) ) if not additive : model . solver . objective = interface . Objective ( Zero , direction = model . solver . objective . direction ) for reaction , coef in value . items ( ) : model . solver . objective . set_linear_coefficients ( { reaction . forward_variable : coef , reaction . reverse_variable : - coef } ) elif isinstance ( value , ( Basic , optlang . interface . Objective ) ) : if isinstance ( value , Basic ) : value = interface . Objective ( value , direction = model . solver . objective . direction , sloppy = False ) if not _valid_atoms ( model , value . expression ) : value = interface . Objective . clone ( value , model = model . solver ) if not additive : model . solver . objective = value else : model . solver . objective += value . expression else : raise TypeError ( '%r is not a valid objective for %r.' % ( value , model . solver ) ) context = get_context ( model ) if context : def reset ( ) : model . solver . objective = reverse_value model . solver . objective . direction = reverse_value . direction context ( reset )
12377	def make_response ( self , data = None ) : if data is not None : data = self . prepare ( data ) self . response . write ( data , serialize = True )
9198	def pop ( self , key , default = _sentinel ) : if default is not _sentinel : tup = self . _data . pop ( key . lower ( ) , default ) else : tup = self . _data . pop ( key . lower ( ) ) if tup is not default : return tup [ 1 ] else : return default
11836	def h ( self , node ) : "h function is straight-line distance from a node's state to goal." locs = getattr ( self . graph , 'locations' , None ) if locs : return int ( distance ( locs [ node . state ] , locs [ self . goal ] ) ) else : return infinity
5080	def parse_course_key ( course_identifier ) : try : course_run_key = CourseKey . from_string ( course_identifier ) except InvalidKeyError : return course_identifier return quote_plus ( ' ' . join ( [ course_run_key . org , course_run_key . course ] ) )
2610	def _restore_buffers ( obj , buffers ) : if isinstance ( obj , CannedObject ) and obj . buffers : for i , buf in enumerate ( obj . buffers ) : if buf is None : obj . buffers [ i ] = buffers . pop ( 0 )
5754	def bootstrap_paginate ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) < 2 : raise TemplateSyntaxError ( "'%s' takes at least one argument" " (Page object reference)" % bits [ 0 ] ) page = parser . compile_filter ( bits [ 1 ] ) kwargs = { } bits = bits [ 2 : ] kwarg_re = re . compile ( r'(\w+)=(.+)' ) if len ( bits ) : for bit in bits : match = kwarg_re . match ( bit ) if not match : raise TemplateSyntaxError ( "Malformed arguments to bootstrap_pagination paginate tag" ) name , value = match . groups ( ) kwargs [ name ] = parser . compile_filter ( value ) return BootstrapPaginationNode ( page , kwargs )
3795	def setup_a_alpha_and_derivatives ( self , i , T = None ) : r if not hasattr ( self , 'kappas' ) : self . kappas = [ kappa0 + kappa1 * ( 1 + ( T / Tc ) ** 0.5 ) * ( 0.7 - ( T / Tc ) ) for kappa0 , kappa1 , Tc in zip ( self . kappa0s , self . kappa1s , self . Tcs ) ] self . a , self . kappa , self . kappa0 , self . kappa1 , self . Tc = self . ais [ i ] , self . kappas [ i ] , self . kappa0s [ i ] , self . kappa1s [ i ] , self . Tcs [ i ]
828	def getFieldDescription ( self , fieldName ) : description = self . getDescription ( ) + [ ( "end" , self . getWidth ( ) ) ] for i in xrange ( len ( description ) ) : ( name , offset ) = description [ i ] if ( name == fieldName ) : break if i >= len ( description ) - 1 : raise RuntimeError ( "Field name %s not found in this encoder" % fieldName ) return ( offset , description [ i + 1 ] [ 1 ] - offset )
5492	def write_config ( self ) : with open ( self . config_file , "w" ) as config_file : self . cfg . write ( config_file )
4784	def contains_ignoring_case ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) if isinstance ( self . val , str_types ) : if len ( items ) == 1 : if not isinstance ( items [ 0 ] , str_types ) : raise TypeError ( 'given arg must be a string' ) if items [ 0 ] . lower ( ) not in self . val . lower ( ) : self . _err ( 'Expected <%s> to case-insensitive contain item <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : missing = [ ] for i in items : if not isinstance ( i , str_types ) : raise TypeError ( 'given args must all be strings' ) if i . lower ( ) not in self . val . lower ( ) : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to case-insensitive contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) elif isinstance ( self . val , Iterable ) : missing = [ ] for i in items : if not isinstance ( i , str_types ) : raise TypeError ( 'given args must all be strings' ) found = False for v in self . val : if not isinstance ( v , str_types ) : raise TypeError ( 'val items must all be strings' ) if i . lower ( ) == v . lower ( ) : found = True break if not found : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to case-insensitive contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
10964	def trigger_update ( self , params , values ) : if self . _parent : self . _parent . trigger_update ( params , values ) else : self . update ( params , values )
5388	def _task_sort_function ( task ) : return ( task . get_field ( 'create-time' ) , int ( task . get_field ( 'task-id' , 0 ) ) , int ( task . get_field ( 'task-attempt' , 0 ) ) )
2522	def p_file_chk_sum ( self , f_term , predicate ) : try : for _s , _p , checksum in self . graph . triples ( ( f_term , predicate , None ) ) : for _ , _ , value in self . graph . triples ( ( checksum , self . spdx_namespace [ 'checksumValue' ] , None ) ) : self . builder . set_file_chksum ( self . doc , six . text_type ( value ) ) except CardinalityError : self . more_than_one_error ( 'File checksum' )
5261	def parse_10qk ( self , response ) : loader = ReportItemLoader ( response = response ) item = loader . load_item ( ) if 'doc_type' in item : doc_type = item [ 'doc_type' ] if doc_type in ( '10-Q' , '10-K' ) : return item return None
13788	def open ( name = None , fileobj = None , closefd = True ) : return Guesser ( ) . open ( name = name , fileobj = fileobj , closefd = closefd )
10530	def get_project ( project_id ) : try : res = _pybossa_req ( 'get' , 'project' , project_id ) if res . get ( 'id' ) : return Project ( res ) else : return res except : raise
7751	def process_message ( self , stanza ) : stanza_type = stanza . stanza_type if stanza_type is None : stanza_type = "normal" if self . __try_handlers ( self . _message_handlers , stanza , stanza_type = stanza_type ) : return True if stanza_type not in ( "error" , "normal" ) : return self . __try_handlers ( self . _message_handlers , stanza , stanza_type = "normal" ) return False
3077	def has_credentials ( self ) : if not self . credentials : return False elif ( self . credentials . access_token_expired and not self . credentials . refresh_token ) : return False else : return True
11015	def publish ( context ) : header ( 'Recording changes...' ) run ( 'git add -A' ) header ( 'Displaying changes...' ) run ( 'git -c color.status=always status' ) if not click . confirm ( '\nContinue publishing' ) : run ( 'git reset HEAD --' ) abort ( context ) header ( 'Saving changes...' ) try : run ( 'git commit -m "{message}"' . format ( message = 'Publishing {}' . format ( choose_commit_emoji ( ) ) ) , capture = True ) except subprocess . CalledProcessError as e : if 'nothing to commit' not in e . stdout : raise else : click . echo ( 'Nothing to commit.' ) header ( 'Pushing to GitHub...' ) branch = get_branch ( ) run ( 'git push origin {branch}:{branch}' . format ( branch = branch ) ) pr_link = get_pr_link ( branch ) if pr_link : click . launch ( pr_link )
9018	def new_pattern ( self , id_ , name , rows = None ) : if rows is None : rows = self . new_row_collection ( ) return self . _spec . new_pattern ( id_ , name , rows , self )
3541	def do_apply ( mutation_pk , dict_synonyms , backup ) : filename , mutation_id = filename_and_mutation_id_from_pk ( int ( mutation_pk ) ) update_line_numbers ( filename ) context = Context ( mutation_id = mutation_id , filename = filename , dict_synonyms = dict_synonyms , ) mutate_file ( backup = backup , context = context , ) if context . number_of_performed_mutations == 0 : raise RuntimeError ( 'No mutations performed.' )
12451	def deref ( self , data ) : deref = copy . deepcopy ( jsonref . JsonRef . replace_refs ( data ) ) self . write_template ( deref , filename = 'swagger.json' ) return deref
8526	def find_match ( self ) : for pattern , callback in self . rules : match = pattern . match ( self . source , pos = self . pos ) if not match : continue try : node = callback ( match ) except IgnoredMatchException : pass else : self . seen . append ( node ) return match raise NoMatchException ( 'None of the known patterns match for {}' '' . format ( self . source [ self . pos : ] ) )
1526	def add_context ( self , err_context , succ_context = None ) : self . err_context = err_context self . succ_context = succ_context
3835	async def set_active_client ( self , set_active_client_request ) : response = hangouts_pb2 . SetActiveClientResponse ( ) await self . _pb_request ( 'clients/setactiveclient' , set_active_client_request , response ) return response
13125	def id_to_object ( self , line ) : user = User . get ( line , ignore = 404 ) if not user : user = User ( username = line ) user . save ( ) return user
12645	def set_aad_cache ( token , cache ) : set_config_value ( 'aad_token' , jsonpickle . encode ( token ) ) set_config_value ( 'aad_cache' , jsonpickle . encode ( cache ) )
13211	def _parse_revision_date ( self ) : r doc_datetime = None if not self . is_draft : date_command = LatexCommand ( 'date' , { 'name' : 'content' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( date_command . parse ( self . _tex ) ) command_content = parsed [ 'content' ] . strip ( ) except StopIteration : command_content = None self . _logger . warning ( 'lsstdoc has no date command' ) if command_content is not None and command_content != r'\today' : try : doc_datetime = datetime . datetime . strptime ( command_content , '%Y-%m-%d' ) project_tz = timezone ( 'US/Pacific' ) localized_datetime = project_tz . localize ( doc_datetime ) doc_datetime = localized_datetime . astimezone ( pytz . utc ) self . _revision_datetime_source = 'tex' except ValueError : self . _logger . warning ( 'Could not parse a datetime from ' 'lsstdoc date command: %r' , command_content ) if doc_datetime is None : content_extensions = ( 'tex' , 'bib' , 'pdf' , 'png' , 'jpg' ) try : doc_datetime = get_content_commit_date ( content_extensions , root_dir = self . _root_dir ) self . _revision_datetime_source = 'git' except RuntimeError : self . _logger . warning ( 'Could not get a datetime from the Git ' 'repository at %r' , self . _root_dir ) if doc_datetime is None : doc_datetime = pytz . utc . localize ( datetime . datetime . now ( ) ) self . _revision_datetime_source = 'now' self . _datetime = doc_datetime
12491	def as_float_array ( X , copy = True , force_all_finite = True ) : if isinstance ( X , np . matrix ) or ( not isinstance ( X , np . ndarray ) and not sp . issparse ( X ) ) : return check_array ( X , [ 'csr' , 'csc' , 'coo' ] , dtype = np . float64 , copy = copy , force_all_finite = force_all_finite , ensure_2d = False ) elif sp . issparse ( X ) and X . dtype in [ np . float32 , np . float64 ] : return X . copy ( ) if copy else X elif X . dtype in [ np . float32 , np . float64 ] : return X . copy ( 'F' if X . flags [ 'F_CONTIGUOUS' ] else 'C' ) if copy else X else : return X . astype ( np . float32 if X . dtype == np . int32 else np . float64 )
4845	def _load_data ( self , resource , default = DEFAULT_VALUE_SAFEGUARD , ** kwargs ) : default_val = default if default != self . DEFAULT_VALUE_SAFEGUARD else { } try : return get_edx_api_data ( api_config = CatalogIntegration . current ( ) , resource = resource , api = self . client , ** kwargs ) or default_val except ( SlumberBaseException , ConnectionError , Timeout ) as exc : LOGGER . exception ( 'Failed to load data from resource [%s] with kwargs [%s] due to: [%s]' , resource , kwargs , str ( exc ) ) return default_val
5041	def enroll_users_in_program ( cls , enterprise_customer , program_details , course_mode , emails , cohort = None ) : existing_users , unregistered_emails = cls . get_users_by_email ( emails ) course_ids = get_course_runs_from_program ( program_details ) successes = [ ] pending = [ ] failures = [ ] for user in existing_users : succeeded = cls . enroll_user ( enterprise_customer , user , course_mode , * course_ids ) if succeeded : successes . append ( user ) else : failures . append ( user ) for email in unregistered_emails : pending_user = enterprise_customer . enroll_user_pending_registration ( email , course_mode , * course_ids , cohort = cohort ) pending . append ( pending_user ) return successes , pending , failures
8234	def complementary ( clr ) : clr = color ( clr ) colors = colorlist ( clr ) c = clr . copy ( ) if clr . brightness > 0.4 : c . brightness = 0.1 + c . brightness * 0.25 else : c . brightness = 1.0 - c . brightness * 0.25 colors . append ( c ) c = clr . copy ( ) c . brightness = 0.3 + c . brightness c . saturation = 0.1 + c . saturation * 0.3 colors . append ( c ) clr = clr . complement c = clr . copy ( ) if clr . brightness > 0.3 : c . brightness = 0.1 + clr . brightness * 0.25 else : c . brightness = 1.0 - c . brightness * 0.25 colors . append ( c ) colors . append ( clr ) c = clr . copy ( ) c . brightness = 0.3 + c . brightness c . saturation = 0.1 + c . saturation * 0.25 colors . append ( c ) return colors
8887	def fit ( self , x , y = None ) : x = iter2array ( x , dtype = ( MoleculeContainer , CGRContainer ) ) if self . __head_less : warn ( f'{self.__class__.__name__} configured to head less mode. fit unusable' ) return self self . _reset ( ) self . __prepare ( x ) return self
3236	def list_objects_in_bucket ( ** kwargs ) : bucket = get_bucket ( ** kwargs ) if bucket : return [ o for o in bucket . list_blobs ( ) ] else : return None
2408	def extract_features_and_generate_model ( essays , algorithm = util_functions . AlgorithmTypes . regression ) : f = feature_extractor . FeatureExtractor ( ) f . initialize_dictionaries ( essays ) train_feats = f . gen_feats ( essays ) set_score = numpy . asarray ( essays . _score , dtype = numpy . int ) if len ( util_functions . f7 ( list ( set_score ) ) ) > 5 : algorithm = util_functions . AlgorithmTypes . regression else : algorithm = util_functions . AlgorithmTypes . classification clf , clf2 = get_algorithms ( algorithm ) cv_error_results = get_cv_error ( clf2 , train_feats , essays . _score ) try : clf . fit ( train_feats , set_score ) except ValueError : log . exception ( "Not enough classes (0,1,etc) in sample." ) set_score [ 0 ] = 1 set_score [ 1 ] = 0 clf . fit ( train_feats , set_score ) return f , clf , cv_error_results
13564	def get_field_names ( obj , ignore_auto = True , ignore_relations = True , exclude = [ ] ) : from django . db . models import ( AutoField , ForeignKey , ManyToManyField , ManyToOneRel , OneToOneField , OneToOneRel ) for field in obj . _meta . get_fields ( ) : if ignore_auto and isinstance ( field , AutoField ) : continue if ignore_relations and ( isinstance ( field , ForeignKey ) or isinstance ( field , ManyToManyField ) or isinstance ( field , ManyToOneRel ) or isinstance ( field , OneToOneRel ) or isinstance ( field , OneToOneField ) ) : a = 1 a continue if field . name in exclude : continue yield field . name
13158	def insert ( cls , cur , table : str , values : dict ) : keys = cls . _COMMA . join ( values . keys ( ) ) value_place_holder = cls . _PLACEHOLDER * len ( values ) query = cls . _insert_string . format ( table , keys , value_place_holder [ : - 1 ] ) yield from cur . execute ( query , tuple ( values . values ( ) ) ) return ( yield from cur . fetchone ( ) )
13318	def launch ( module_name , * args , ** kwargs ) : r = resolve ( module_name ) r . activate ( ) mod = r . resolved [ 0 ] mod . launch ( * args , ** kwargs )
71	def clip_out_of_image ( self ) : bbs_cut = [ bb . clip_out_of_image ( self . shape ) for bb in self . bounding_boxes if bb . is_partly_within_image ( self . shape ) ] return BoundingBoxesOnImage ( bbs_cut , shape = self . shape )
3509	def constraint_matrices ( model , array_type = 'dense' , include_vars = False , zero_tol = 1e-6 ) : if array_type not in ( 'DataFrame' , 'dense' ) and not dok_matrix : raise ValueError ( 'Sparse matrices require scipy' ) array_builder = { 'dense' : np . array , 'dok' : dok_matrix , 'lil' : lil_matrix , 'DataFrame' : pd . DataFrame , } [ array_type ] Problem = namedtuple ( "Problem" , [ "equalities" , "b" , "inequalities" , "bounds" , "variable_fixed" , "variable_bounds" ] ) equality_rows = [ ] inequality_rows = [ ] inequality_bounds = [ ] b = [ ] for const in model . constraints : lb = - np . inf if const . lb is None else const . lb ub = np . inf if const . ub is None else const . ub equality = ( ub - lb ) < zero_tol coefs = const . get_linear_coefficients ( model . variables ) coefs = [ coefs [ v ] for v in model . variables ] if equality : b . append ( lb if abs ( lb ) > zero_tol else 0.0 ) equality_rows . append ( coefs ) else : inequality_rows . append ( coefs ) inequality_bounds . append ( [ lb , ub ] ) var_bounds = np . array ( [ [ v . lb , v . ub ] for v in model . variables ] ) fixed = var_bounds [ : , 1 ] - var_bounds [ : , 0 ] < zero_tol results = Problem ( equalities = array_builder ( equality_rows ) , b = np . array ( b ) , inequalities = array_builder ( inequality_rows ) , bounds = array_builder ( inequality_bounds ) , variable_fixed = np . array ( fixed ) , variable_bounds = array_builder ( var_bounds ) ) return results
11048	def dataReceived ( self , data ) : self . resetTimeout ( ) lines = ( self . _buffer + data ) . splitlines ( ) if data . endswith ( b'\n' ) or data . endswith ( b'\r' ) : self . _buffer = b'' else : self . _buffer = lines . pop ( - 1 ) for line in lines : if self . transport . disconnecting : return if len ( line ) > self . _max_length : self . lineLengthExceeded ( line ) return else : self . lineReceived ( line ) if len ( self . _buffer ) > self . _max_length : self . lineLengthExceeded ( self . _buffer ) return
406	def retrieve_seq_length_op3 ( data , pad_val = 0 ) : data_shape_size = data . get_shape ( ) . ndims if data_shape_size == 3 : return tf . reduce_sum ( tf . cast ( tf . reduce_any ( tf . not_equal ( data , pad_val ) , axis = 2 ) , dtype = tf . int32 ) , 1 ) elif data_shape_size == 2 : return tf . reduce_sum ( tf . cast ( tf . not_equal ( data , pad_val ) , dtype = tf . int32 ) , 1 ) elif data_shape_size == 1 : raise ValueError ( "retrieve_seq_length_op3: data has wrong shape!" ) else : raise ValueError ( "retrieve_seq_length_op3: handling data_shape_size %s hasn't been implemented!" % ( data_shape_size ) )
7736	def map ( self , data ) : result = [ ] for char in data : ret = None for lookup in self . mapping : ret = lookup ( char ) if ret is not None : break if ret is not None : result . append ( ret ) else : result . append ( char ) return result
11638	def write_data ( data , filename ) : name , ext = get_file_extension ( filename ) func = json_write_data if ext == '.json' else yaml_write_data return func ( data , filename )
11270	def safe_substitute ( prev , * args , ** kw ) : template_obj = string . Template ( * args , ** kw ) for data in prev : yield template_obj . safe_substitute ( data )
9179	def _validate_license ( model ) : license_mapping = obtain_licenses ( ) try : license_url = model . metadata [ 'license_url' ] except KeyError : raise exceptions . MissingRequiredMetadata ( 'license_url' ) try : license = license_mapping [ license_url ] except KeyError : raise exceptions . InvalidLicense ( license_url ) if not license [ 'is_valid_for_publication' ] : raise exceptions . InvalidLicense ( license_url )
10358	def shuffle_node_data ( graph : BELGraph , key : str , percentage : Optional [ float ] = None ) -> BELGraph : percentage = percentage or 0.3 assert 0 < percentage <= 1 n = graph . number_of_nodes ( ) swaps = int ( percentage * n * ( n - 1 ) / 2 ) result : BELGraph = graph . copy ( ) for _ in range ( swaps ) : s , t = random . sample ( result . node , 2 ) result . nodes [ s ] [ key ] , result . nodes [ t ] [ key ] = result . nodes [ t ] [ key ] , result . nodes [ s ] [ key ] return result
13084	def add_tag ( ) : if len ( sys . argv ) > 1 : tag = sys . argv [ 1 ] doc_mapper = DocMapper ( ) if doc_mapper . is_pipe : count = 0 for obj in doc_mapper . get_pipe ( ) : obj . add_tag ( tag ) obj . update ( tags = obj . tags ) count += 1 print_success ( "Added tag '{}' to {} object(s)" . format ( tag , count ) ) else : print_error ( "Please use this script with pipes" ) else : print_error ( "Usage: jk-add-tag <tag>" ) sys . exit ( )
3304	def _run_paste ( app , config , mode ) : from paste import httpserver version = "WsgiDAV/{} {} Python {}" . format ( __version__ , httpserver . WSGIHandler . server_version , util . PYTHON_VERSION ) _logger . info ( "Running {}..." . format ( version ) ) server = httpserver . serve ( app , host = config [ "host" ] , port = config [ "port" ] , server_version = version , protocol_version = "HTTP/1.1" , start_loop = False , ) if config [ "verbose" ] >= 5 : __handle_one_request = server . RequestHandlerClass . handle_one_request def handle_one_request ( self ) : __handle_one_request ( self ) if self . close_connection == 1 : _logger . debug ( "HTTP Connection : close" ) else : _logger . debug ( "HTTP Connection : continue" ) server . RequestHandlerClass . handle_one_request = handle_one_request server . RequestHandlerClass . handle_one_request = handle_one_request host , port = server . server_address if host == "0.0.0.0" : _logger . info ( "Serving on 0.0.0.0:{} view at {}://127.0.0.1:{}" . format ( port , "http" , port ) ) else : _logger . info ( "Serving on {}://{}:{}" . format ( "http" , host , port ) ) try : server . serve_forever ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
1279	def parse_lheading ( self , m ) : self . tokens . append ( { 'type' : 'heading' , 'level' : 1 if m . group ( 2 ) == '=' else 2 , 'text' : m . group ( 1 ) , } )
12548	def icc_img_to_zscore ( icc , center_image = False ) : vol = read_img ( icc ) . get_data ( ) v2 = vol [ vol != 0 ] if center_image : v2 = detrend ( v2 , axis = 0 ) vstd = np . linalg . norm ( v2 , ord = 2 ) / np . sqrt ( np . prod ( v2 . shape ) - 1 ) eps = np . finfo ( vstd . dtype ) . eps vol /= ( eps + vstd ) return vol
12957	def _get_key_for_index ( self , indexedField , val ) : if hasattr ( indexedField , 'toIndex' ) : val = indexedField . toIndex ( val ) else : val = self . fields [ indexedField ] . toIndex ( val ) return '' . join ( [ INDEXED_REDIS_PREFIX , self . keyName , ':idx:' , indexedField , ':' , val ] )
11687	def changeset_info ( changeset ) : keys = [ tag . attrib . get ( 'k' ) for tag in changeset . getchildren ( ) ] keys += [ 'id' , 'user' , 'uid' , 'bbox' , 'created_at' ] values = [ tag . attrib . get ( 'v' ) for tag in changeset . getchildren ( ) ] values += [ changeset . get ( 'id' ) , changeset . get ( 'user' ) , changeset . get ( 'uid' ) , get_bounds ( changeset ) , changeset . get ( 'created_at' ) ] return dict ( zip ( keys , values ) )
7840	def get_name ( self ) : var = self . xmlnode . prop ( "name" ) if not var : var = "" return var . decode ( "utf-8" )
3834	async def send_offnetwork_invitation ( self , send_offnetwork_invitation_request ) : response = hangouts_pb2 . SendOffnetworkInvitationResponse ( ) await self . _pb_request ( 'devices/sendoffnetworkinvitation' , send_offnetwork_invitation_request , response ) return response
3529	def get_user_from_context ( context ) : try : return context [ 'user' ] except KeyError : pass try : request = context [ 'request' ] return request . user except ( KeyError , AttributeError ) : pass return None
1990	def rm ( self , key ) : path = os . path . join ( self . uri , key ) os . remove ( path )
5826	def _validate_search_query ( self , returning_query ) : start_index = returning_query . from_index or 0 size = returning_query . size or 0 if start_index < 0 : raise CitrinationClientError ( "start_index cannot be negative. Please enter a value greater than or equal to zero" ) if size < 0 : raise CitrinationClientError ( "Size cannot be negative. Please enter a value greater than or equal to zero" ) if start_index + size > MAX_QUERY_DEPTH : raise CitrinationClientError ( "Citrination does not support pagination past the {0}th result. Please reduce either the from_index and/or size such that their sum is below {0}" . format ( MAX_QUERY_DEPTH ) )
12159	def abfGroupFiles ( groups , folder ) : assert os . path . exists ( folder ) files = os . listdir ( folder ) group2 = { } for parent in groups . keys ( ) : if not parent in group2 . keys ( ) : group2 [ parent ] = [ ] for ID in groups [ parent ] : for fname in [ x . lower ( ) for x in files if ID in x . lower ( ) ] : group2 [ parent ] . extend ( [ fname ] ) return group2
9015	def _row ( self , values ) : row_id = self . _to_id ( values [ ID ] ) row = self . _spec . new_row ( row_id , values , self ) if SAME_AS in values : self . _delay_inheritance ( row , self . _to_id ( values [ SAME_AS ] ) ) self . _delay_instructions ( row ) self . _id_cache [ row_id ] = row return row
4881	def delete_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . filter ( name = ENTERPRISE_ROLE_BASED_ACCESS_CONTROL_SWITCH ) . delete ( )
7037	def xmatch_search ( lcc_server , file_to_upload , xmatch_dist_arcsec = 3.0 , result_visibility = 'unlisted' , email_when_done = False , collections = None , columns = None , filters = None , sortspec = None , limitspec = None , samplespec = None , download_data = True , outdir = None , maxtimeout = 300.0 , refresh = 15.0 ) : with open ( file_to_upload ) as infd : xmq = infd . read ( ) xmqlines = len ( xmq . split ( '\n' ) [ : - 1 ] ) if xmqlines > 5000 : LOGERROR ( 'you have more than 5000 lines in the file to upload: %s' % file_to_upload ) return None , None , None params = { 'xmq' : xmq , 'xmd' : xmatch_dist_arcsec } if collections : params [ 'collections' ] = collections if columns : params [ 'columns' ] = columns if filters : params [ 'filters' ] = filters if sortspec : params [ 'sortspec' ] = json . dumps ( [ sortspec ] ) if samplespec : params [ 'samplespec' ] = int ( samplespec ) if limitspec : params [ 'limitspec' ] = int ( limitspec ) params [ 'visibility' ] = result_visibility params [ 'emailwhendone' ] = email_when_done if email_when_done : download_data = False have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) api_url = '%s/api/xmatch' % lcc_server searchresult = submit_post_searchquery ( api_url , params , apikey ) status = searchresult [ 0 ] if download_data : if status == 'ok' : LOGINFO ( 'query complete, downloading associated data...' ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if pkl : return searchresult [ 1 ] , csv , lczip , pkl else : return searchresult [ 1 ] , csv , lczip elif status == 'background' : LOGINFO ( 'query is not yet complete, ' 'waiting up to %.1f minutes, ' 'updates every %s seconds (hit Ctrl+C to cancel)...' % ( maxtimeout / 60.0 , refresh ) ) timewaited = 0.0 while timewaited < maxtimeout : try : time . sleep ( refresh ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if ( csv and os . path . exists ( csv ) and lczip and os . path . exists ( lczip ) ) : LOGINFO ( 'all dataset products collected' ) return searchresult [ 1 ] , csv , lczip timewaited = timewaited + refresh except KeyboardInterrupt : LOGWARNING ( 'abandoned wait for downloading data' ) return searchresult [ 1 ] , None , None LOGERROR ( 'wait timed out.' ) return searchresult [ 1 ] , None , None else : LOGERROR ( 'could not download the data for this query result' ) return searchresult [ 1 ] , None , None else : return searchresult [ 1 ] , None , None
1748	def _get_offset ( self , index ) : if not self . _in_range ( index ) : raise IndexError ( 'Map index out of range' ) if isinstance ( index , slice ) : index = slice ( index . start - self . start , index . stop - self . start ) else : index -= self . start return index
4128	def readwav ( filename ) : from scipy . io . wavfile import read as readwav samplerate , signal = readwav ( filename ) return signal , samplerate
12576	def set_mask ( self , mask_img ) : mask = load_mask ( mask_img , allow_empty = True ) check_img_compatibility ( self . img , mask , only_check_3d = True ) self . mask = mask
13032	def serve_forever ( self , poll_interval = 0.5 ) : logger . info ( 'Starting server on {}:{}...' . format ( self . server_name , self . server_port ) ) while True : try : self . poll_once ( poll_interval ) except ( KeyboardInterrupt , SystemExit ) : break self . handle_close ( ) logger . info ( 'Server stopped.' )
7754	def process_stanza ( self , stanza ) : self . fix_in_stanza ( stanza ) to_jid = stanza . to_jid if not self . process_all_stanzas and to_jid and ( to_jid != self . me and to_jid . bare ( ) != self . me . bare ( ) ) : return self . route_stanza ( stanza ) try : if isinstance ( stanza , Iq ) : if self . process_iq ( stanza ) : return True elif isinstance ( stanza , Message ) : if self . process_message ( stanza ) : return True elif isinstance ( stanza , Presence ) : if self . process_presence ( stanza ) : return True except ProtocolError , err : typ = stanza . stanza_type if typ != 'error' and ( typ != 'result' or stanza . stanza_type != 'iq' ) : response = stanza . make_error_response ( err . xmpp_name ) self . send ( response ) err . log_reported ( ) else : err . log_ignored ( ) return logger . debug ( "Unhandled %r stanza: %r" % ( stanza . stanza_type , stanza . serialize ( ) ) ) return False
1210	def table ( self , header , body ) : table = '\n.. list-table::\n' if header and not header . isspace ( ) : table = ( table + self . indent + ':header-rows: 1\n\n' + self . _indent_block ( header ) + '\n' ) else : table = table + '\n' table = table + self . _indent_block ( body ) + '\n\n' return table
5570	def profile ( self ) : with rasterio . open ( self . path , "r" ) as src : return deepcopy ( src . meta )
5895	def formfield ( self , ** kwargs ) : defaults = { 'form_class' : RichTextFormField , 'config' : self . config , } defaults . update ( kwargs ) return super ( RichTextField , self ) . formfield ( ** defaults )
13552	def _post_resource ( self , url , body ) : headers = { "Content-Type" : "application/json" , "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = WhenIWork_DAO ( ) . postURL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 204 ) : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
739	def cPrint ( self , level , message , * args , ** kw ) : if level > self . consolePrinterVerbosity : return if len ( kw ) > 1 : raise KeyError ( "Invalid keywords for cPrint: %s" % str ( kw . keys ( ) ) ) newline = kw . get ( "newline" , True ) if len ( kw ) == 1 and 'newline' not in kw : raise KeyError ( "Invalid keyword for cPrint: %s" % kw . keys ( ) [ 0 ] ) if len ( args ) == 0 : if newline : print message else : print message , else : if newline : print message % args else : print message % args ,
6904	def hms_to_decimal ( hours , minutes , seconds , returndeg = True ) : if hours > 24 : return None else : dec_hours = fabs ( hours ) + fabs ( minutes ) / 60.0 + fabs ( seconds ) / 3600.0 if returndeg : dec_deg = dec_hours * 15.0 if dec_deg < 0 : dec_deg = dec_deg + 360.0 dec_deg = dec_deg % 360.0 return dec_deg else : return dec_hours
3245	def get_inline_policies ( group , ** conn ) : policy_list = list_group_policies ( group [ 'GroupName' ] ) policy_documents = { } for policy in policy_list : policy_documents [ policy ] = get_group_policy_document ( group [ 'GroupName' ] , policy , ** conn ) return policy_documents
133	def clip_out_of_image ( self , image ) : import shapely . geometry if self . is_out_of_image ( image , fully = True , partly = False ) : return [ ] h , w = image . shape [ 0 : 2 ] if ia . is_np_array ( image ) else image [ 0 : 2 ] poly_shapely = self . to_shapely_polygon ( ) poly_image = shapely . geometry . Polygon ( [ ( 0 , 0 ) , ( w , 0 ) , ( w , h ) , ( 0 , h ) ] ) multipoly_inter_shapely = poly_shapely . intersection ( poly_image ) if not isinstance ( multipoly_inter_shapely , shapely . geometry . MultiPolygon ) : ia . do_assert ( isinstance ( multipoly_inter_shapely , shapely . geometry . Polygon ) ) multipoly_inter_shapely = shapely . geometry . MultiPolygon ( [ multipoly_inter_shapely ] ) polygons = [ ] for poly_inter_shapely in multipoly_inter_shapely . geoms : polygons . append ( Polygon . from_shapely ( poly_inter_shapely , label = self . label ) ) polygons_reordered = [ ] for polygon in polygons : found = False for x , y in self . exterior : closest_idx , dist = polygon . find_closest_point_index ( x = x , y = y , return_distance = True ) if dist < 1e-6 : polygon_reordered = polygon . change_first_point_by_index ( closest_idx ) polygons_reordered . append ( polygon_reordered ) found = True break ia . do_assert ( found ) return polygons_reordered
3353	def _replace_on_id ( self , new_object ) : the_id = new_object . id the_index = self . _dict [ the_id ] list . __setitem__ ( self , the_index , new_object )
6205	def populations_slices ( particles , num_pop_list ) : slices = [ ] i_prev = 0 for num_pop in num_pop_list : slices . append ( slice ( i_prev , i_prev + num_pop ) ) i_prev += num_pop return slices
3948	def _timezone_format ( value ) : return timezone . make_aware ( value , timezone . get_current_timezone ( ) ) if getattr ( settings , 'USE_TZ' , False ) else value
11593	def _rc_msetnx ( self , mapping ) : for k in iterkeys ( mapping ) : if self . exists ( k ) : return False return self . _rc_mset ( mapping )
8181	def remove_node ( self , id ) : if self . has_key ( id ) : n = self [ id ] self . nodes . remove ( n ) del self [ id ] for e in list ( self . edges ) : if n in ( e . node1 , e . node2 ) : if n in e . node1 . links : e . node1 . links . remove ( n ) if n in e . node2 . links : e . node2 . links . remove ( n ) self . edges . remove ( e )
4756	def postprocess ( trun ) : plog = [ ] plog . append ( ( "trun" , process_trun ( trun ) ) ) for tsuite in trun [ "testsuites" ] : plog . append ( ( "tsuite" , process_tsuite ( tsuite ) ) ) for tcase in tsuite [ "testcases" ] : plog . append ( ( "tcase" , process_tcase ( tcase ) ) ) for task , success in plog : if not success : cij . err ( "rprtr::postprocess: FAILED for %r" % task ) return sum ( ( success for task , success in plog ) )
9660	def merge_from_store_and_in_mems ( from_store , in_mem_shas , dont_update_shas_of ) : if not from_store : for item in dont_update_shas_of : if item in in_mem_shas [ 'files' ] : del in_mem_shas [ 'files' ] [ item ] return in_mem_shas for key in from_store [ 'files' ] : if key not in in_mem_shas [ 'files' ] and key not in dont_update_shas_of : in_mem_shas [ 'files' ] [ key ] = from_store [ 'files' ] [ key ] for item in dont_update_shas_of : if item in in_mem_shas [ 'files' ] : del in_mem_shas [ 'files' ] [ item ] return in_mem_shas
12468	def save_traceback ( err ) : dirname = safe_path ( os . path . expanduser ( os . path . join ( '~' , '.{0}' . format ( __script__ ) ) ) ) if not os . path . isdir ( dirname ) : os . mkdir ( dirname ) filename = os . path . join ( dirname , '{0}.log' . format ( __script__ ) ) with open ( filename , 'a+' ) as handler : traceback . print_exc ( file = handler ) message = ( 'User aborted workflow' if isinstance ( err , KeyboardInterrupt ) else 'Unexpected error catched' ) print_error ( message ) print_error ( 'Full log stored to {0}' . format ( filename ) , False ) return True
13538	def get_location ( self , location_id ) : url = "/2/locations/%s" % location_id return self . location_from_json ( self . _get_resource ( url ) [ "location" ] )
697	def bestModelIdAndErrScore ( self , swarmId = None , genIdx = None ) : if swarmId is None : return ( self . _bestModelID , self . _bestResult ) else : if swarmId not in self . _swarmBestOverall : return ( None , numpy . inf ) genScores = self . _swarmBestOverall [ swarmId ] bestModelId = None bestScore = numpy . inf for ( i , ( modelId , errScore ) ) in enumerate ( genScores ) : if genIdx is not None and i > genIdx : break if errScore < bestScore : bestScore = errScore bestModelId = modelId return ( bestModelId , bestScore )
649	def generateSimpleCoincMatrix ( nCoinc = 10 , length = 500 , activity = 50 ) : assert nCoinc * activity <= length , "can't generate non-overlapping coincidences" coincMatrix = SM32 ( 0 , length ) coinc = numpy . zeros ( length , dtype = 'int32' ) for i in xrange ( nCoinc ) : coinc [ : ] = 0 coinc [ i * activity : ( i + 1 ) * activity ] = 1 coincMatrix . addRow ( coinc ) return coincMatrix
3700	def solubility_parameter ( T = 298.15 , Hvapm = None , Vml = None , CASRN = '' , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if T and Hvapm and Vml : methods . append ( DEFINITION ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == DEFINITION : if ( not Hvapm ) or ( not T ) or ( not Vml ) : delta = None else : if Hvapm < R * T or Vml < 0 : delta = None else : delta = ( ( Hvapm - R * T ) / Vml ) ** 0.5 elif Method == NONE : delta = None else : raise Exception ( 'Failure in in function' ) return delta
478	def word_to_id ( self , word ) : if word in self . _vocab : return self . _vocab [ word ] else : return self . _unk_id
7678	def pitch_contour ( annotation , ** kwargs ) : ax = kwargs . pop ( 'ax' , None ) ax = mir_eval . display . __get_axes ( ax = ax ) [ 0 ] times , values = annotation . to_interval_values ( ) indices = np . unique ( [ v [ 'index' ] for v in values ] ) for idx in indices : rows = [ i for ( i , v ) in enumerate ( values ) if v [ 'index' ] == idx ] freqs = np . asarray ( [ values [ r ] [ 'frequency' ] for r in rows ] ) unvoiced = ~ np . asarray ( [ values [ r ] [ 'voiced' ] for r in rows ] ) freqs [ unvoiced ] *= - 1 ax = mir_eval . display . pitch ( times [ rows , 0 ] , freqs , unvoiced = True , ax = ax , ** kwargs ) return ax
7793	def set_fetcher ( self , fetcher_class ) : self . _lock . acquire ( ) try : self . _fetcher = fetcher_class finally : self . _lock . release ( )
466	def generate_skip_gram_batch ( data , batch_size , num_skips , skip_window , data_index = 0 ) : if batch_size % num_skips != 0 : raise Exception ( "batch_size should be able to be divided by num_skips." ) if num_skips > 2 * skip_window : raise Exception ( "num_skips <= 2 * skip_window" ) batch = np . ndarray ( shape = ( batch_size ) , dtype = np . int32 ) labels = np . ndarray ( shape = ( batch_size , 1 ) , dtype = np . int32 ) span = 2 * skip_window + 1 buffer = collections . deque ( maxlen = span ) for _ in range ( span ) : buffer . append ( data [ data_index ] ) data_index = ( data_index + 1 ) % len ( data ) for i in range ( batch_size // num_skips ) : target = skip_window targets_to_avoid = [ skip_window ] for j in range ( num_skips ) : while target in targets_to_avoid : target = random . randint ( 0 , span - 1 ) targets_to_avoid . append ( target ) batch [ i * num_skips + j ] = buffer [ skip_window ] labels [ i * num_skips + j , 0 ] = buffer [ target ] buffer . append ( data [ data_index ] ) data_index = ( data_index + 1 ) % len ( data ) return batch , labels , data_index
3503	def add_loopless ( model , zero_cutoff = None ) : zero_cutoff = normalize_cutoff ( model , zero_cutoff ) internal = [ i for i , r in enumerate ( model . reactions ) if not r . boundary ] s_int = create_stoichiometric_matrix ( model ) [ : , numpy . array ( internal ) ] n_int = nullspace ( s_int ) . T max_bound = max ( max ( abs ( b ) for b in r . bounds ) for r in model . reactions ) prob = model . problem to_add = [ ] for i in internal : rxn = model . reactions [ i ] indicator = prob . Variable ( "indicator_" + rxn . id , type = "binary" ) on_off_constraint = prob . Constraint ( rxn . flux_expression - max_bound * indicator , lb = - max_bound , ub = 0 , name = "on_off_" + rxn . id ) delta_g = prob . Variable ( "delta_g_" + rxn . id ) delta_g_range = prob . Constraint ( delta_g + ( max_bound + 1 ) * indicator , lb = 1 , ub = max_bound , name = "delta_g_range_" + rxn . id ) to_add . extend ( [ indicator , on_off_constraint , delta_g , delta_g_range ] ) model . add_cons_vars ( to_add ) for i , row in enumerate ( n_int ) : name = "nullspace_constraint_" + str ( i ) nullspace_constraint = prob . Constraint ( Zero , lb = 0 , ub = 0 , name = name ) model . add_cons_vars ( [ nullspace_constraint ] ) coefs = { model . variables [ "delta_g_" + model . reactions [ ridx ] . id ] : row [ i ] for i , ridx in enumerate ( internal ) if abs ( row [ i ] ) > zero_cutoff } model . constraints [ name ] . set_linear_coefficients ( coefs )
11874	def put ( xy , * args ) : cmd = [ TermCursor . save , TermCursor . move ( * xy ) , '' . join ( args ) , TermCursor . restore ] write ( '' . join ( cmd ) )
11597	def _rc_dbsize ( self ) : "Returns the number of keys in the current database" result = 0 for alias , redisent in iteritems ( self . redises ) : if alias . find ( '_slave' ) == - 1 : continue result += redisent . dbsize ( ) return result
10700	def paginate_link_tag ( item ) : a_tag = Page . default_link_tag ( item ) if item [ 'type' ] == 'current_page' : return make_html_tag ( 'li' , a_tag , ** { 'class' : 'blue white-text' } ) return make_html_tag ( 'li' , a_tag )
10368	def find_activations ( graph : BELGraph ) : for u , v , key , data in graph . edges ( keys = True , data = True ) : if u != v : continue bel = graph . edge_to_bel ( u , v , data ) line = data . get ( LINE ) if line is None : continue elif has_protein_modification_increases_activity ( graph , u , v , key ) : print ( line , '- pmod changes -' , bel ) find_related ( graph , v , data ) elif has_degradation_increases_activity ( data ) : print ( line , '- degradation changes -' , bel ) find_related ( graph , v , data ) elif has_translocation_increases_activity ( data ) : print ( line , '- translocation changes -' , bel ) find_related ( graph , v , data ) elif complex_increases_activity ( graph , u , v , key ) : print ( line , '- complex changes - ' , bel ) find_related ( graph , v , data ) elif has_same_subject_object ( graph , u , v , key ) : print ( line , '- same sub/obj -' , bel ) else : print ( line , '- *** - ' , bel )
13667	def execute ( self , command , timeout = None ) : try : self . channel = self . ssh . get_transport ( ) . open_session ( ) except paramiko . SSHException as e : self . unknown ( "Create channel error: %s" % e ) try : self . channel . settimeout ( self . args . timeout if not timeout else timeout ) except socket . timeout as e : self . unknown ( "Settimeout for channel error: %s" % e ) try : self . logger . debug ( "command: {}" . format ( command ) ) self . channel . exec_command ( command ) except paramiko . SSHException as e : self . unknown ( "Execute command error: %s" % e ) try : self . stdin = self . channel . makefile ( 'wb' , - 1 ) self . stderr = map ( string . strip , self . channel . makefile_stderr ( 'rb' , - 1 ) . readlines ( ) ) self . stdout = map ( string . strip , self . channel . makefile ( 'rb' , - 1 ) . readlines ( ) ) except Exception as e : self . unknown ( "Get result error: %s" % e ) try : self . status = self . channel . recv_exit_status ( ) except paramiko . SSHException as e : self . unknown ( "Get return code error: %s" % e ) else : if self . status != 0 : self . unknown ( "Return code: %d , stderr: %s" % ( self . status , self . errors ) ) else : return self . stdout finally : self . logger . debug ( "Execute command finish." )
13428	def get_site ( self , site_id ) : url = "/2/sites/%s" % site_id return self . site_from_json ( self . _get_resource ( url ) [ "site" ] )
6152	def fir_remez_bpf ( f_stop1 , f_pass1 , f_pass2 , f_stop2 , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : n , ff , aa , wts = bandpass_order ( f_stop1 , f_pass1 , f_pass2 , f_stop2 , d_pass , d_stop , fsamp = fs ) N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 ) print ( 'Remez filter taps = %d.' % N_taps ) return b
6301	def get_effect_resources ( self ) -> List [ Any ] : resources = [ ] for package in self . packages : resources . extend ( package . resources ) return resources
2836	def read ( self , length , assert_ss = True , deassert_ss = True ) : if self . _miso is None : raise RuntimeError ( 'Read attempted with no MISO pin specified.' ) if assert_ss and self . _ss is not None : self . _gpio . set_low ( self . _ss ) result = bytearray ( length ) for i in range ( length ) : for j in range ( 8 ) : self . _gpio . output ( self . _sclk , not self . _clock_base ) if self . _read_leading : if self . _gpio . is_high ( self . _miso ) : result [ i ] |= self . _read_shift ( self . _mask , j ) else : result [ i ] &= ~ self . _read_shift ( self . _mask , j ) self . _gpio . output ( self . _sclk , self . _clock_base ) if not self . _read_leading : if self . _gpio . is_high ( self . _miso ) : result [ i ] |= self . _read_shift ( self . _mask , j ) else : result [ i ] &= ~ self . _read_shift ( self . _mask , j ) if deassert_ss and self . _ss is not None : self . _gpio . set_high ( self . _ss ) return result
3935	def get ( self ) : logger . info ( 'Loading refresh_token from %s' , repr ( self . _filename ) ) try : with open ( self . _filename ) as f : return f . read ( ) except IOError as e : logger . info ( 'Failed to load refresh_token: %s' , e )
12844	def execute_undo ( self , message ) : info ( "undoing message: {message}" ) with self . world . _unlock_temporarily ( ) : message . _undo ( self . world ) self . world . _react_to_undo_response ( message ) for actor in self . actors : actor . _react_to_undo_response ( message )
9259	def delete_by_time ( self , issues , older_tag , newer_tag ) : if not older_tag and not newer_tag : return copy . deepcopy ( issues ) newer_tag_time = self . get_time_of_tag ( newer_tag ) older_tag_time = self . get_time_of_tag ( older_tag ) filtered = [ ] for issue in issues : if issue . get ( 'actual_date' ) : rslt = older_tag_time < issue [ 'actual_date' ] <= newer_tag_time if rslt : filtered . append ( copy . deepcopy ( issue ) ) return filtered
1093	def split ( pattern , string , maxsplit = 0 , flags = 0 ) : return _compile ( pattern , flags ) . split ( string , maxsplit )
6638	def getScript ( self , scriptname ) : script = self . description . get ( 'scripts' , { } ) . get ( scriptname , None ) if script is not None : if isinstance ( script , str ) or isinstance ( script , type ( u'unicode string' ) ) : import shlex script = shlex . split ( script ) if len ( script ) and script [ 0 ] . lower ( ) . endswith ( '.py' ) : if not os . path . isabs ( script [ 0 ] ) : absscript = os . path . abspath ( os . path . join ( self . path , script [ 0 ] ) ) logger . debug ( 'rewriting script %s to be absolute path %s' , script [ 0 ] , absscript ) script [ 0 ] = absscript import sys script = [ sys . executable ] + script return script
8226	def _makeInstance ( self , clazz , args , kwargs ) : inst = clazz ( self , * args , ** kwargs ) return inst
3705	def Townsend_Hales ( T , Tc , Vc , omega ) : r Tr = T / Tc return Vc / ( 1 + 0.85 * ( 1 - Tr ) + ( 1.692 + 0.986 * omega ) * ( 1 - Tr ) ** ( 1 / 3. ) )
13718	def _camelcase_to_underscore ( url ) : def upper2underscore ( text ) : for char in text : if char . islower ( ) : yield char else : yield '_' if char . isalpha ( ) : yield char . lower ( ) return '' . join ( upper2underscore ( url ) )
13571	def configure ( server = None , username = None , password = None , tid = None , auto = False ) : if not server and not username and not password and not tid : if Config . has ( ) : if not yn_prompt ( "Override old configuration" , False ) : return False reset_db ( ) if not server : while True : server = input ( "Server url [https://tmc.mooc.fi/mooc/]: " ) . strip ( ) if len ( server ) == 0 : server = "https://tmc.mooc.fi/mooc/" if not server . endswith ( '/' ) : server += '/' if not ( server . startswith ( "http://" ) or server . startswith ( "https://" ) ) : ret = custom_prompt ( "Server should start with http:// or https://\n" + "R: Retry, H: Assume http://, S: Assume https://" , [ "r" , "h" , "s" ] , "r" ) if ret == "r" : continue if "://" in server : server = server . split ( "://" ) [ 1 ] if ret == "h" : server = "http://" + server elif ret == "s" : server = "https://" + server break print ( "Using URL: '{0}'" . format ( server ) ) while True : if not username : username = input ( "Username: " ) if not password : password = getpass ( "Password: " ) token = b64encode ( bytes ( "{0}:{1}" . format ( username , password ) , encoding = 'utf-8' ) ) . decode ( "utf-8" ) try : api . configure ( url = server , token = token , test = True ) except APIError as e : print ( e ) if auto is False and yn_prompt ( "Retry authentication" ) : username = password = None continue return False break if tid : select ( course = True , tid = tid , auto = auto ) else : select ( course = True )
220	async def get_response ( self , path : str , scope : Scope ) -> Response : if scope [ "method" ] not in ( "GET" , "HEAD" ) : return PlainTextResponse ( "Method Not Allowed" , status_code = 405 ) if path . startswith ( ".." ) : return PlainTextResponse ( "Not Found" , status_code = 404 ) full_path , stat_result = await self . lookup_path ( path ) if stat_result and stat . S_ISREG ( stat_result . st_mode ) : return self . file_response ( full_path , stat_result , scope ) elif stat_result and stat . S_ISDIR ( stat_result . st_mode ) and self . html : index_path = os . path . join ( path , "index.html" ) full_path , stat_result = await self . lookup_path ( index_path ) if stat_result is not None and stat . S_ISREG ( stat_result . st_mode ) : if not scope [ "path" ] . endswith ( "/" ) : url = URL ( scope = scope ) url = url . replace ( path = url . path + "/" ) return RedirectResponse ( url = url ) return self . file_response ( full_path , stat_result , scope ) if self . html : full_path , stat_result = await self . lookup_path ( "404.html" ) if stat_result is not None and stat . S_ISREG ( stat_result . st_mode ) : return self . file_response ( full_path , stat_result , scope , status_code = 404 ) return PlainTextResponse ( "Not Found" , status_code = 404 )
13178	def cache_func ( prefix , method = False ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : cache_args = args if method : cache_args = args [ 1 : ] cache_key = get_cache_key ( prefix , * cache_args , ** kwargs ) cached_value = cache . get ( cache_key ) if cached_value is None : cached_value = func ( * args , ** kwargs ) cache . set ( cache_key , cached_value ) return cached_value return wrapper return decorator
10753	def open_archive ( fs_url , archive ) : it = pkg_resources . iter_entry_points ( 'fs.archive.open_archive' ) entry_point = next ( ( ep for ep in it if archive . endswith ( ep . name ) ) , None ) if entry_point is None : raise UnsupportedProtocol ( 'unknown archive extension: {}' . format ( archive ) ) try : archive_opener = entry_point . load ( ) except pkg_resources . DistributionNotFound as df : six . raise_from ( UnsupportedProtocol ( 'extension {} requires {}' . format ( entry_point . name , df . req ) ) , None ) try : binfile = None archive_fs = None fs = open_fs ( fs_url ) if issubclass ( archive_opener , base . ArchiveFS ) : try : binfile = fs . openbin ( archive , 'r+' ) except errors . ResourceNotFound : binfile = fs . openbin ( archive , 'w' ) except errors . ResourceReadOnly : binfile = fs . openbin ( archive , 'r' ) archive_opener = archive_opener . _read_fs_cls elif issubclass ( archive_opener , base . ArchiveReadFS ) : binfile = fs . openbin ( archive , 'r' ) if not hasattr ( binfile , 'name' ) : binfile . name = basename ( archive ) archive_fs = archive_opener ( binfile ) except Exception : getattr ( archive_fs , 'close' , lambda : None ) ( ) getattr ( binfile , 'close' , lambda : None ) ( ) raise else : return archive_fs
7337	def predict_subsequences ( self , sequence_dict , peptide_lengths = None ) : sequence_dict = check_sequence_dictionary ( sequence_dict ) peptide_lengths = self . _check_peptide_lengths ( peptide_lengths ) binding_predictions = [ ] expected_peptides = set ( [ ] ) normalized_alleles = [ ] for key , amino_acid_sequence in sequence_dict . items ( ) : for l in peptide_lengths : for i in range ( len ( amino_acid_sequence ) - l + 1 ) : expected_peptides . add ( amino_acid_sequence [ i : i + l ] ) self . _check_peptide_inputs ( expected_peptides ) for allele in self . alleles : allele = normalize_allele_name ( allele , omit_dra1 = True ) normalized_alleles . append ( allele ) request = self . _get_iedb_request_params ( amino_acid_sequence , allele ) logger . info ( "Calling IEDB (%s) with request %s" , self . url , request ) response_df = _query_iedb ( request , self . url ) for _ , row in response_df . iterrows ( ) : binding_predictions . append ( BindingPrediction ( source_sequence_name = key , offset = row [ 'start' ] - 1 , allele = row [ 'allele' ] , peptide = row [ 'peptide' ] , affinity = row [ 'ic50' ] , percentile_rank = row [ 'rank' ] , prediction_method_name = "iedb-" + self . prediction_method ) ) self . _check_results ( binding_predictions , alleles = normalized_alleles , peptides = expected_peptides ) return BindingPredictionCollection ( binding_predictions )
10595	def Nu_L ( self , L , theta , Ts , ** statef ) : return self . Nu_x ( L , theta , Ts , ** statef ) / 0.75
1397	def extract_execution_state ( self , topology ) : execution_state = topology . execution_state executionState = { "cluster" : execution_state . cluster , "environ" : execution_state . environ , "role" : execution_state . role , "jobname" : topology . name , "submission_time" : execution_state . submission_time , "submission_user" : execution_state . submission_user , "release_username" : execution_state . release_state . release_username , "release_tag" : execution_state . release_state . release_tag , "release_version" : execution_state . release_state . release_version , "has_physical_plan" : None , "has_tmaster_location" : None , "has_scheduler_location" : None , "extra_links" : [ ] , } for extra_link in self . config . extra_links : link = extra_link . copy ( ) link [ "url" ] = self . config . get_formatted_url ( executionState , link [ EXTRA_LINK_FORMATTER_KEY ] ) executionState [ "extra_links" ] . append ( link ) return executionState
361	def exists_or_mkdir ( path , verbose = True ) : if not os . path . exists ( path ) : if verbose : logging . info ( "[*] creates %s ..." % path ) os . makedirs ( path ) return False else : if verbose : logging . info ( "[!] %s exists ..." % path ) return True
6434	def dist_eudex ( src , tar , weights = 'exponential' , max_length = 8 ) : return Eudex ( ) . dist ( src , tar , weights , max_length )
12616	def is_img ( obj ) : try : get_data = getattr ( obj , 'get_data' ) get_affine = getattr ( obj , 'get_affine' ) return isinstance ( get_data , collections . Callable ) and isinstance ( get_affine , collections . Callable ) except AttributeError : return False
4488	def _iter_children ( self , url , kind , klass , recurse = None ) : children = self . _follow_next ( url ) while children : child = children . pop ( ) kind_ = child [ 'attributes' ] [ 'kind' ] if kind_ == kind : yield klass ( child , self . session ) elif recurse is not None : url = self . _get_attribute ( child , * recurse ) children . extend ( self . _follow_next ( url ) )
4694	def regex_find ( pattern , content ) : find = re . findall ( pattern , content ) if not find : cij . err ( "pattern <%r> is invalid, no matches!" % pattern ) cij . err ( "content: %r" % content ) return '' if len ( find ) >= 2 : cij . err ( "pattern <%r> is too simple, matched more than 2!" % pattern ) cij . err ( "content: %r" % content ) return '' return find [ 0 ]
10913	def find_particles_in_tile ( positions , tile ) : bools = tile . contains ( positions ) return np . arange ( bools . size ) [ bools ]
10617	def get_compound_mass ( self , compound ) : if compound in self . material . compounds : return self . _compound_masses [ self . material . get_compound_index ( compound ) ] else : return 0.0
11208	def get_config ( jid ) : acls = getattr ( settings , 'XMPP_HTTP_UPLOAD_ACCESS' , ( ( '.*' , False ) , ) ) for regex , config in acls : if isinstance ( regex , six . string_types ) : regex = [ regex ] for subex in regex : if re . search ( subex , jid ) : return config return False
9750	def find_x ( path1 ) : libs = os . listdir ( path1 ) for lib_dir in libs : if "doublefann" in lib_dir : return True
1064	def isheader ( self , line ) : i = line . find ( ':' ) if i > - 1 : return line [ : i ] . lower ( ) return None
1693	def map ( self , map_function ) : from heronpy . streamlet . impl . mapbolt import MapStreamlet map_streamlet = MapStreamlet ( map_function , self ) self . _add_child ( map_streamlet ) return map_streamlet
6712	def tunnel ( self , local_port , remote_port ) : r = self . local_renderer r . env . tunnel_local_port = local_port r . env . tunnel_remote_port = remote_port r . local ( ' ssh -i {key_filename} -L {tunnel_local_port}:localhost:{tunnel_remote_port} {user}@{host_string} -N' )
140	def to_keypoints ( self ) : from imgaug . augmentables . kps import Keypoint return [ Keypoint ( x = point [ 0 ] , y = point [ 1 ] ) for point in self . exterior ]
4690	def encode_memo ( priv , pub , nonce , message ) : shared_secret = get_shared_secret ( priv , pub ) aes = init_aes ( shared_secret , nonce ) " Checksum " raw = bytes ( message , "utf8" ) checksum = hashlib . sha256 ( raw ) . digest ( ) raw = checksum [ 0 : 4 ] + raw " Padding " raw = _pad ( raw , 16 ) " Encryption " return hexlify ( aes . encrypt ( raw ) ) . decode ( "ascii" )
13867	def truncate ( when , unit , week_start = mon ) : if is_datetime ( when ) : if unit == millisecond : return when . replace ( microsecond = int ( round ( when . microsecond / 1000.0 ) ) * 1000 ) elif unit == second : return when . replace ( microsecond = 0 ) elif unit == minute : return when . replace ( second = 0 , microsecond = 0 ) elif unit == hour : return when . replace ( minute = 0 , second = 0 , microsecond = 0 ) elif unit == day : return when . replace ( hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == week : weekday = prevweekday ( when , week_start ) return when . replace ( year = weekday . year , month = weekday . month , day = weekday . day , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == month : return when . replace ( day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == year : return when . replace ( month = 1 , day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif is_date ( when ) : if unit == week : return prevweekday ( when , week_start ) elif unit == month : return when . replace ( day = 1 ) elif unit == year : return when . replace ( month = 1 , day = 1 ) elif is_time ( when ) : if unit == millisecond : return when . replace ( microsecond = int ( when . microsecond / 1000.0 ) * 1000 ) elif unit == second : return when . replace ( microsecond = 0 ) elif unit == minute : return when . replace ( second = 0 , microsecond = 0 ) return when
8065	def drawdaisy ( x , y , color = '#fefefe' ) : _ctx . push ( ) _fill = _ctx . fill ( ) _stroke = _ctx . stroke ( ) sc = ( 1.0 / _ctx . HEIGHT ) * float ( y * 0.5 ) * 4.0 _ctx . strokewidth ( sc * 2.0 ) _ctx . stroke ( '#3B240B' ) _ctx . line ( x + ( sin ( x * 0.1 ) * 10.0 ) , y + 80 , x + sin ( _ctx . FRAME * 0.1 ) , y ) _ctx . translate ( - 20 , 0 ) _ctx . scale ( sc ) _ctx . fill ( color ) _ctx . nostroke ( ) for angle in xrange ( 0 , 360 , 45 ) : _ctx . rotate ( degrees = 45 ) _ctx . rect ( x , y , 40 , 8 , 1 ) _ctx . fill ( '#F7FE2E' ) _ctx . ellipse ( x + 15 , y , 10 , 10 ) _ctx . fill ( _fill ) _ctx . stroke ( _stroke ) _ctx . pop ( )
12021	def add_line_error ( self , line_data , error_info , log_level = logging . ERROR ) : if not error_info : return try : line_data [ 'line_errors' ] . append ( error_info ) except KeyError : line_data [ 'line_errors' ] = [ error_info ] except TypeError : pass try : self . logger . log ( log_level , Gff3 . error_format . format ( current_line_num = line_data [ 'line_index' ] + 1 , error_type = error_info [ 'error_type' ] , message = error_info [ 'message' ] , line = line_data [ 'line_raw' ] . rstrip ( ) ) ) except AttributeError : pass
10192	def get_geoip ( ip ) : reader = geolite2 . reader ( ) ip_data = reader . get ( ip ) or { } return ip_data . get ( 'country' , { } ) . get ( 'iso_code' )
13637	def maybe ( f , default = None ) : @ wraps ( f ) def _maybe ( x , * a , ** kw ) : if x is None : return default return f ( x , * a , ** kw ) return _maybe
12919	def save ( self ) : if len ( self ) == 0 : return [ ] mdl = self . getModel ( ) return mdl . saver . save ( self )
4097	def AKICc ( N , rho , k ) : r from numpy import log , array p = k res = log ( rho ) + p / N / ( N - p ) + ( 3. - ( p + 2. ) / N ) * ( p + 1. ) / ( N - p - 2. ) return res
8864	def icon_from_typename ( name , icon_type ) : ICONS = { 'CLASS' : ICON_CLASS , 'IMPORT' : ICON_NAMESPACE , 'STATEMENT' : ICON_VAR , 'FORFLOW' : ICON_VAR , 'FORSTMT' : ICON_VAR , 'WITHSTMT' : ICON_VAR , 'GLOBALSTMT' : ICON_VAR , 'MODULE' : ICON_NAMESPACE , 'KEYWORD' : ICON_KEYWORD , 'PARAM' : ICON_VAR , 'ARRAY' : ICON_VAR , 'INSTANCEELEMENT' : ICON_VAR , 'INSTANCE' : ICON_VAR , 'PARAM-PRIV' : ICON_VAR , 'PARAM-PROT' : ICON_VAR , 'FUNCTION' : ICON_FUNC , 'DEF' : ICON_FUNC , 'FUNCTION-PRIV' : ICON_FUNC_PRIVATE , 'FUNCTION-PROT' : ICON_FUNC_PROTECTED } ret_val = None icon_type = icon_type . upper ( ) if hasattr ( name , "string" ) : name = name . string if icon_type == "FORFLOW" or icon_type == "STATEMENT" : icon_type = "PARAM" if icon_type == "PARAM" or icon_type == "FUNCTION" : if name . startswith ( "__" ) : icon_type += "-PRIV" elif name . startswith ( "_" ) : icon_type += "-PROT" if icon_type in ICONS : ret_val = ICONS [ icon_type ] elif icon_type : _logger ( ) . warning ( "Unimplemented completion icon_type: %s" , icon_type ) return ret_val
8938	def confluence ( ctx , no_publish = False , clean = False , opts = '' ) : cfg = config . load ( ) if clean : ctx . run ( "invoke clean --docs" ) cmd = [ 'sphinx-build' , '-b' , 'confluence' ] cmd . extend ( [ '-E' , '-a' ] ) if opts : cmd . append ( opts ) cmd . extend ( [ '.' , ctx . rituals . docs . build + '_cf' ] ) if no_publish : cmd . extend ( [ '-Dconfluence_publish=False' ] ) notify . info ( "Starting Sphinx build..." ) with pushd ( ctx . rituals . docs . sources ) : ctx . run ( ' ' . join ( cmd ) , pty = True )
11510	def get_item_metadata ( self , item_id , token = None , revision = None ) : parameters = dict ( ) parameters [ 'id' ] = item_id if token : parameters [ 'token' ] = token if revision : parameters [ 'revision' ] = revision response = self . request ( 'midas.item.getmetadata' , parameters ) return response
4433	async def dispatch_event ( self , event ) : log . debug ( 'Dispatching event of type {} to {} hooks' . format ( event . __class__ . __name__ , len ( self . hooks ) ) ) for hook in self . hooks : try : if asyncio . iscoroutinefunction ( hook ) : await hook ( event ) else : hook ( event ) except Exception as e : log . warning ( 'Encountered exception while dispatching an event to hook `{}` ({})' . format ( hook . __name__ , str ( e ) ) ) if isinstance ( event , ( TrackEndEvent , TrackExceptionEvent , TrackStuckEvent ) ) and event . player : await event . player . handle_event ( event )
4176	def window_gaussian ( N , alpha = 2.5 ) : r t = linspace ( - ( N - 1 ) / 2. , ( N - 1 ) / 2. , N ) w = exp ( - 0.5 * ( alpha * t / ( N / 2. ) ) ** 2. ) return w
9773	def statuses ( ctx , page ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) page = page or 1 try : response = PolyaxonClient ( ) . job . get_statuses ( user , project_name , _job , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get status for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Statuses for Job `{}`.' . format ( _job ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No statuses found for job `{}`.' . format ( _job ) ) objects = list_dicts_to_tabulate ( [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) , status_key = 'status' ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Statuses:" ) objects . pop ( 'job' , None ) dict_tabulate ( objects , is_list_dict = True )
13562	def save ( self , * args , ** kwargs ) : rerank = kwargs . pop ( 'rerank' , True ) if rerank : if not self . id : self . _process_new_rank_obj ( ) elif self . rank == self . _rank_at_load : pass else : self . _process_moved_rank_obj ( ) super ( RankedModel , self ) . save ( * args , ** kwargs )
11213	def decode ( secret : Union [ str , bytes ] , token : Union [ str , bytes ] , alg : str = default_alg ) -> Tuple [ dict , dict ] : secret = util . to_bytes ( secret ) token = util . to_bytes ( token ) pre_signature , signature_segment = token . rsplit ( b'.' , 1 ) header_b64 , payload_b64 = pre_signature . split ( b'.' ) try : header_json = util . b64_decode ( header_b64 ) header = json . loads ( util . from_bytes ( header_json ) ) except ( json . decoder . JSONDecodeError , UnicodeDecodeError , ValueError ) : raise InvalidHeaderError ( 'Invalid header' ) try : payload_json = util . b64_decode ( payload_b64 ) payload = json . loads ( util . from_bytes ( payload_json ) ) except ( json . decoder . JSONDecodeError , UnicodeDecodeError , ValueError ) : raise InvalidPayloadError ( 'Invalid payload' ) if not isinstance ( header , dict ) : raise InvalidHeaderError ( 'Invalid header: {}' . format ( header ) ) if not isinstance ( payload , dict ) : raise InvalidPayloadError ( 'Invalid payload: {}' . format ( payload ) ) signature = util . b64_decode ( signature_segment ) calculated_signature = _hash ( secret , pre_signature , alg ) if not compare_signature ( signature , calculated_signature ) : raise InvalidSignatureError ( 'Invalid signature' ) return header , payload
4029	def load ( domain_name = "" ) : cj = http . cookiejar . CookieJar ( ) for cookie_fn in [ chrome , firefox ] : try : for cookie in cookie_fn ( domain_name = domain_name ) : cj . set_cookie ( cookie ) except BrowserCookieError : pass return cj
4226	def _data_root_Linux ( ) : fallback = os . path . expanduser ( '~/.local/share' ) root = os . environ . get ( 'XDG_DATA_HOME' , None ) or fallback return os . path . join ( root , 'python_keyring' )
3630	def add_dividers ( row , divider , padding ) : div = '' . join ( [ padding * ' ' , divider , padding * ' ' ] ) return div . join ( row )
10917	def get_residuals_update_tile ( st , padded_tile ) : inner_tile = st . ishape . intersection ( [ st . ishape , padded_tile ] ) return inner_tile . translate ( - st . pad )
7437	def stats ( self ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) pd . options . display . max_rows = len ( self . samples ) statdat = pd . DataFrame ( [ self . samples [ i ] . stats for i in nameordered ] , index = nameordered ) . dropna ( axis = 1 , how = 'all' ) for column in statdat : if column not in [ "hetero_est" , "error_est" ] : statdat [ column ] = np . nan_to_num ( statdat [ column ] ) . astype ( int ) return statdat
10508	def log ( self , message , level = logging . DEBUG ) : if _ldtp_debug : print ( message ) self . logger . log ( level , str ( message ) ) return 1
3650	def num2hex ( self , num ) : temp = '' for i in range ( 0 , 4 ) : x = self . hexChars [ ( num >> ( i * 8 + 4 ) ) & 0x0F ] y = self . hexChars [ ( num >> ( i * 8 ) ) & 0x0F ] temp += ( x + y ) return temp
1456	def valid_path ( path ) : if path . endswith ( '*' ) : Log . debug ( 'Checking classpath entry suffix as directory: %s' , path [ : - 1 ] ) if os . path . isdir ( path [ : - 1 ] ) : return True return False Log . debug ( 'Checking classpath entry as directory: %s' , path ) if os . path . isdir ( path ) : return True else : Log . debug ( 'Checking classpath entry as file: %s' , path ) if os . path . isfile ( path ) : return True return False
385	def parse_darknet_ann_list_to_cls_box ( annotations ) : class_list = [ ] bbox_list = [ ] for ann in annotations : class_list . append ( ann [ 0 ] ) bbox_list . append ( ann [ 1 : ] ) return class_list , bbox_list
619	def unescape ( s ) : assert isinstance ( s , basestring ) s = s . replace ( '\t' , ',' ) s = s . replace ( '\\,' , ',' ) s = s . replace ( '\\n' , '\n' ) s = s . replace ( '\\\\' , '\\' ) return s
770	def _getGroundTruth ( self , inferenceElement ) : sensorInputElement = InferenceElement . getInputElement ( inferenceElement ) if sensorInputElement is None : return None return getattr ( self . __currentGroundTruth . sensorInput , sensorInputElement )
12364	def transfer ( self , region ) : action = self . post ( type = 'transfer' , region = region ) [ 'action' ] return self . parent . get ( action [ 'resource_id' ] )
1479	def _wait_process_std_out_err ( self , name , process ) : proc . stream_process_stdout ( process , stdout_log_fn ( name ) ) process . wait ( )
174	def draw_points_on_image ( self , image , color = ( 0 , 128 , 0 ) , alpha = 1.0 , size = 3 , copy = True , raise_if_out_of_image = False ) : from . kps import KeypointsOnImage kpsoi = KeypointsOnImage . from_xy_array ( self . coords , shape = image . shape ) image = kpsoi . draw_on_image ( image , color = color , alpha = alpha , size = size , copy = copy , raise_if_out_of_image = raise_if_out_of_image ) return image
7180	def reapply_all ( ast_node , lib2to3_node ) : late_processing = reapply ( ast_node , lib2to3_node ) for lazy_func in reversed ( late_processing ) : lazy_func ( )
10698	def get ( self , key , default = None ) : if self . in_memory : return self . _memory_db . get ( key , default ) else : db = self . _read_file ( ) return db . get ( key , default )
2566	def udp_messenger ( domain_name , UDP_IP , UDP_PORT , sock_timeout , message ) : try : if message is None : raise ValueError ( "message was none" ) encoded_message = bytes ( message , "utf-8" ) if encoded_message is None : raise ValueError ( "utf-8 encoding of message failed" ) if domain_name : try : UDP_IP = socket . gethostbyname ( domain_name ) except Exception : pass if UDP_IP is None : raise Exception ( "UDP_IP is None" ) if UDP_PORT is None : raise Exception ( "UDP_PORT is None" ) sock = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) sock . settimeout ( sock_timeout ) sock . sendto ( bytes ( message , "utf-8" ) , ( UDP_IP , UDP_PORT ) ) sock . close ( ) except socket . timeout : logger . debug ( "Failed to send usage tracking data: socket timeout" ) except OSError as e : logger . debug ( "Failed to send usage tracking data: OSError: {}" . format ( e ) ) except Exception as e : logger . debug ( "Failed to send usage tracking data: Exception: {}" . format ( e ) )
4714	def trun_to_file ( trun , fpath = None ) : if fpath is None : fpath = yml_fpath ( trun [ "conf" ] [ "OUTPUT" ] ) with open ( fpath , 'w' ) as yml_file : data = yaml . dump ( trun , explicit_start = True , default_flow_style = False ) yml_file . write ( data )
11615	def print_read ( self , rid ) : if self . rname is not None : print self . rname [ rid ] print '--' r = self . get_read_data ( rid ) aligned_loci = np . unique ( r . nonzero ( ) [ 1 ] ) for locus in aligned_loci : nzvec = r [ : , locus ] . todense ( ) . transpose ( ) [ 0 ] . A . flatten ( ) if self . lname is not None : print self . lname [ locus ] , else : print locus , print nzvec
12226	def on_pref_update ( * args , ** kwargs ) : Preference . update_prefs ( * args , ** kwargs ) Preference . read_prefs ( get_prefs ( ) )
10885	def oslicer ( self , tile ) : mask = None vecs = tile . coords ( form = 'meshed' ) for v in vecs : v [ self . slicer ] = - 1 mask = mask & ( v > 0 ) if mask is not None else ( v > 0 ) return tuple ( np . array ( i ) . astype ( 'int' ) for i in zip ( * [ v [ mask ] for v in vecs ] ) )
11322	def generate_dirlist_html ( FS , filepath ) : yield '<table class="dirlist">' if filepath == '/' : filepath = '' for name in FS . listdir ( filepath ) : full_path = pathjoin ( filepath , name ) if FS . isdir ( full_path ) : full_path = full_path + '/' yield u'<tr><td><a href="{0}">{0}</a></td></tr>' . format ( cgi . escape ( full_path ) ) yield '</table>'
6692	def invalidate ( self , * paths ) : dj = self . get_satchel ( 'dj' ) if not paths : return _settings = dj . get_settings ( ) if not _settings . AWS_STATIC_BUCKET_NAME : print ( 'No static media bucket set.' ) return if isinstance ( paths , six . string_types ) : paths = paths . split ( ',' ) all_paths = map ( str . strip , paths ) i = 0 while 1 : paths = all_paths [ i : i + 1000 ] if not paths : break c = boto . connect_cloudfront ( ) rs = c . get_all_distributions ( ) target_dist = None for dist in rs : print ( dist . domain_name , dir ( dist ) , dist . __dict__ ) bucket_name = dist . origin . dns_name . replace ( '.s3.amazonaws.com' , '' ) if bucket_name == _settings . AWS_STATIC_BUCKET_NAME : target_dist = dist break if not target_dist : raise Exception ( ( 'Target distribution %s could not be found in the AWS account.' ) % ( settings . AWS_STATIC_BUCKET_NAME , ) ) print ( 'Using distribution %s associated with origin %s.' % ( target_dist . id , _settings . AWS_STATIC_BUCKET_NAME ) ) inval_req = c . create_invalidation_request ( target_dist . id , paths ) print ( 'Issue invalidation request %s.' % ( inval_req , ) ) i += 1000
6051	def map_2d_array_to_masked_1d_array_from_array_2d_and_mask ( mask , array_2d ) : total_image_pixels = mask_util . total_regular_pixels_from_mask ( mask ) array_1d = np . zeros ( shape = total_image_pixels ) index = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : array_1d [ index ] = array_2d [ y , x ] index += 1 return array_1d
11100	def select_by_atime ( self , min_time = 0 , max_time = ts_2100 , recursive = True ) : def filters ( p ) : return min_time <= p . atime <= max_time return self . select_file ( filters , recursive )
1297	def from_config ( config , kwargs = None ) : return util . get_object ( obj = config , predefined = tensorforce . core . optimizers . solvers . solvers , kwargs = kwargs )
11358	def fix_journal_name ( journal , knowledge_base ) : if not journal : return '' , '' if not knowledge_base : return journal , '' if len ( journal ) < 2 : return journal , '' volume = '' if ( journal [ - 1 ] <= 'Z' and journal [ - 1 ] >= 'A' ) and ( journal [ - 2 ] == '.' or journal [ - 2 ] == ' ' ) : volume += journal [ - 1 ] journal = journal [ : - 1 ] journal = journal . strip ( ) if journal . upper ( ) in knowledge_base : journal = knowledge_base [ journal . upper ( ) ] . strip ( ) elif journal in knowledge_base : journal = knowledge_base [ journal ] . strip ( ) elif '.' in journal : journalnodots = journal . replace ( '. ' , ' ' ) journalnodots = journalnodots . replace ( '.' , ' ' ) . strip ( ) . upper ( ) if journalnodots in knowledge_base : journal = knowledge_base [ journalnodots ] . strip ( ) journal = journal . replace ( '. ' , '.' ) return journal , volume
9153	def get_exif_info ( self ) : _dict = { } for tag in _EXIF_TAGS : ret = self . img . attribute ( "EXIF:%s" % tag ) if ret and ret != 'unknown' : _dict [ tag ] = ret return _dict
12932	def nav_to_vcf_dir ( ftp , build ) : if build == 'b37' : ftp . cwd ( DIR_CLINVAR_VCF_B37 ) elif build == 'b38' : ftp . cwd ( DIR_CLINVAR_VCF_B38 ) else : raise IOError ( "Genome build not recognized." )
9953	def _get_node ( name : str , args : str ) : obj = get_object ( name ) args = ast . literal_eval ( args ) if not isinstance ( args , tuple ) : args = ( args , ) return obj . node ( * args )
1414	def get_pplan ( self , topologyName , callback = None ) : isWatching = False ret = { "result" : None } if callback : isWatching = True else : def callback ( data ) : ret [ "result" ] = data self . _get_pplan_with_watch ( topologyName , callback , isWatching ) return ret [ "result" ]
10041	def deposit_minter ( record_uuid , data ) : provider = DepositProvider . create ( object_type = 'rec' , object_uuid = record_uuid , pid_value = uuid . uuid4 ( ) . hex , ) data [ '_deposit' ] = { 'id' : provider . pid . pid_value , 'status' : 'draft' , } return provider . pid
7762	def make_error_response ( self , cond ) : if self . stanza_type == "error" : raise ValueError ( "Errors may not be generated in response" " to errors" ) msg = Message ( stanza_type = "error" , from_jid = self . to_jid , to_jid = self . from_jid , stanza_id = self . stanza_id , error_cond = cond , subject = self . _subject , body = self . _body , thread = self . _thread ) if self . _payload is None : self . decode_payload ( ) for payload in self . _payload : msg . add_payload ( payload . copy ( ) ) return msg
1923	def binary_arch ( binary ) : with open ( binary , 'rb' ) as f : elffile = ELFFile ( f ) if elffile [ 'e_machine' ] == 'EM_X86_64' : return True else : return False
2786	def create_from_snapshot ( self , * args , ** kwargs ) : data = self . get_data ( 'volumes/' , type = POST , params = { 'name' : self . name , 'snapshot_id' : self . snapshot_id , 'region' : self . region , 'size_gigabytes' : self . size_gigabytes , 'description' : self . description , 'filesystem_type' : self . filesystem_type , 'filesystem_label' : self . filesystem_label } ) if data : self . id = data [ 'volume' ] [ 'id' ] self . created_at = data [ 'volume' ] [ 'created_at' ] return self
9589	def init ( self ) : resp = self . _execute ( Command . NEW_SESSION , { 'desiredCapabilities' : self . desired_capabilities } , False ) resp . raise_for_status ( ) self . session_id = str ( resp . session_id ) self . capabilities = resp . value
12628	def get_all_files ( folder ) : for path , dirlist , filelist in os . walk ( folder ) : for fn in filelist : yield op . join ( path , fn )
12432	def create ( self ) : self . create_virtualenv ( ) self . create_project ( ) self . create_uwsgi_script ( ) self . create_nginx_config ( ) self . create_manage_scripts ( ) logging . info ( '** Make sure to set proper permissions for the webserver user account on the var and log directories in the project root' )
7091	def _cpinfo_key_worker ( task ) : cpfile , keyspeclist = task keystoget = [ x [ 0 ] for x in keyspeclist ] nonesubs = [ x [ - 2 ] for x in keyspeclist ] nansubs = [ x [ - 1 ] for x in keyspeclist ] for i , k in enumerate ( keystoget ) : thisk = k . split ( '.' ) if sys . version_info [ : 2 ] < ( 3 , 4 ) : thisk = [ ( int ( x ) if x . isdigit ( ) else x ) for x in thisk ] else : thisk = [ ( int ( x ) if x . isdecimal ( ) else x ) for x in thisk ] keystoget [ i ] = thisk keystoget . insert ( 0 , [ 'objectid' ] ) nonesubs . insert ( 0 , '' ) nansubs . insert ( 0 , '' ) vals = checkplot_infokey_worker ( ( cpfile , keystoget ) ) for val , nonesub , nansub , valind in zip ( vals , nonesubs , nansubs , range ( len ( vals ) ) ) : if val is None : outval = nonesub elif isinstance ( val , float ) and not np . isfinite ( val ) : outval = nansub elif isinstance ( val , ( list , tuple ) ) : outval = ', ' . join ( val ) else : outval = val vals [ valind ] = outval return vals
5241	def market_normal ( self , session , after_open , before_close ) -> Session : logger = logs . get_logger ( self . market_normal ) if session not in self . exch : return SessNA ss = self . exch [ session ] s_time = shift_time ( ss [ 0 ] , int ( after_open ) + 1 ) e_time = shift_time ( ss [ - 1 ] , - int ( before_close ) ) request_cross = pd . Timestamp ( s_time ) >= pd . Timestamp ( e_time ) session_cross = pd . Timestamp ( ss [ 0 ] ) >= pd . Timestamp ( ss [ 1 ] ) if request_cross and ( not session_cross ) : logger . warning ( f'end time {e_time} is earlier than {s_time} ...' ) return SessNA return Session ( s_time , e_time )
6853	def partitions ( device = "" ) : partitions_list = { } with settings ( hide ( 'running' , 'stdout' ) ) : res = run_as_root ( 'sfdisk -d %(device)s' % locals ( ) ) spart = re . compile ( r'(?P<pname>^/.*) : .* Id=(?P<ptypeid>[0-9a-z]+)' ) for line in res . splitlines ( ) : m = spart . search ( line ) if m : partitions_list [ m . group ( 'pname' ) ] = int ( m . group ( 'ptypeid' ) , 16 ) return partitions_list
8866	def make_python_patterns ( additional_keywords = [ ] , additional_builtins = [ ] ) : kw = r"\b" + any ( "keyword" , kwlist + additional_keywords ) + r"\b" kw_namespace = r"\b" + any ( "namespace" , kw_namespace_list ) + r"\b" word_operators = r"\b" + any ( "operator_word" , wordop_list ) + r"\b" builtinlist = [ str ( name ) for name in dir ( builtins ) if not name . startswith ( '_' ) ] + additional_builtins for v in [ 'None' , 'True' , 'False' ] : builtinlist . remove ( v ) builtin = r"([^.'\"\\#]\b|^)" + any ( "builtin" , builtinlist ) + r"\b" builtin_fct = any ( "builtin_fct" , [ r'_{2}[a-zA-Z_]*_{2}' ] ) comment = any ( "comment" , [ r"#[^\n]*" ] ) instance = any ( "instance" , [ r"\bself\b" , r"\bcls\b" ] ) decorator = any ( 'decorator' , [ r'@\w*' , r'.setter' ] ) number = any ( "number" , [ r"\b[+-]?[0-9]+[lLjJ]?\b" , r"\b[+-]?0[xX][0-9A-Fa-f]+[lL]?\b" , r"\b[+-]?0[oO][0-7]+[lL]?\b" , r"\b[+-]?0[bB][01]+[lL]?\b" , r"\b[+-]?[0-9]+(?:\.[0-9]+)?(?:[eE][+-]?[0-9]+)?[jJ]?\b" ] ) sqstring = r"(\b[rRuU])?'[^'\\\n]*(\\.[^'\\\n]*)*'?" dqstring = r'(\b[rRuU])?"[^"\\\n]*(\\.[^"\\\n]*)*"?' uf_sqstring = r"(\b[rRuU])?'[^'\\\n]*(\\.[^'\\\n]*)*(\\)$(?!')$" uf_dqstring = r'(\b[rRuU])?"[^"\\\n]*(\\.[^"\\\n]*)*(\\)$(?!")$' sq3string = r"(\b[rRuU])?)?" dq3string = r'(\b[rRuU])?)?' uf_sq3string = r"(\b[rRuU])?)$" uf_dq3string = r'(\b[rRuU])?)$' string = any ( "string" , [ sq3string , dq3string , sqstring , dqstring ] ) ufstring1 = any ( "uf_sqstring" , [ uf_sqstring ] ) ufstring2 = any ( "uf_dqstring" , [ uf_dqstring ] ) ufstring3 = any ( "uf_sq3string" , [ uf_sq3string ] ) ufstring4 = any ( "uf_dq3string" , [ uf_dq3string ] ) return "|" . join ( [ instance , decorator , kw , kw_namespace , builtin , word_operators , builtin_fct , comment , ufstring1 , ufstring2 , ufstring3 , ufstring4 , string , number , any ( "SYNC" , [ r"\n" ] ) ] )
3638	def clubConsumables ( self , fast = False ) : method = 'GET' url = 'club/consumables/development' rc = self . __request__ ( method , url ) events = [ self . pin . event ( 'page_view' , 'Hub - Club' ) ] self . pin . send ( events , fast = fast ) events = [ self . pin . event ( 'page_view' , 'Club - Consumables' ) ] self . pin . send ( events , fast = fast ) events = [ self . pin . event ( 'page_view' , 'Club - Consumables - List View' ) ] self . pin . send ( events , fast = fast ) return [ itemParse ( i ) for i in rc . get ( 'itemData' , ( ) ) ]
6018	def from_weight_map ( cls , pixel_scale , weight_map ) : np . seterr ( divide = 'ignore' ) noise_map = 1.0 / np . sqrt ( weight_map ) noise_map [ noise_map == np . inf ] = 1.0e8 return NoiseMap ( array = noise_map , pixel_scale = pixel_scale )
10656	def count_with_multiplier ( groups , multiplier ) : counts = collections . defaultdict ( float ) for group in groups : for element , count in group . count ( ) . items ( ) : counts [ element ] += count * multiplier return counts
4357	def remove_namespace ( self , namespace ) : if namespace in self . active_ns : del self . active_ns [ namespace ] if len ( self . active_ns ) == 0 and self . connected : self . kill ( detach = True )
13285	def clone ( src , dst_path , skip_globals , skip_dimensions , skip_variables ) : if os . path . exists ( dst_path ) : os . unlink ( dst_path ) dst = netCDF4 . Dataset ( dst_path , 'w' ) for attname in src . ncattrs ( ) : if attname not in skip_globals : setattr ( dst , attname , getattr ( src , attname ) ) unlimdim = None unlimdimname = False for dimname , dim in src . dimensions . items ( ) : if dimname in skip_dimensions : continue if dim . isunlimited ( ) : unlimdim = dim unlimdimname = dimname dst . createDimension ( dimname , None ) else : dst . createDimension ( dimname , len ( dim ) ) for varname , ncvar in src . variables . items ( ) : if varname in skip_variables : continue hasunlimdim = False if unlimdimname and unlimdimname in ncvar . dimensions : hasunlimdim = True filler = None if hasattr ( ncvar , '_FillValue' ) : filler = ncvar . _FillValue if ncvar . chunking == "contiguous" : var = dst . createVariable ( varname , ncvar . dtype , ncvar . dimensions , fill_value = filler ) else : var = dst . createVariable ( varname , ncvar . dtype , ncvar . dimensions , fill_value = filler , chunksizes = ncvar . chunking ( ) ) for attname in ncvar . ncattrs ( ) : if attname == '_FillValue' : continue else : setattr ( var , attname , getattr ( ncvar , attname ) ) nchunk = 1000 if hasunlimdim : if nchunk : start = 0 stop = len ( unlimdim ) step = nchunk if step < 1 : step = 1 for n in range ( start , stop , step ) : nmax = n + nchunk if nmax > len ( unlimdim ) : nmax = len ( unlimdim ) idata = ncvar [ n : nmax ] var [ n : nmax ] = idata else : idata = ncvar [ : ] var [ 0 : len ( unlimdim ) ] = idata else : idata = ncvar [ : ] var [ : ] = idata dst . sync ( ) src . close ( ) dst . close ( )
1604	def run_metrics ( command , parser , cl_args , unknown_args ) : cluster , role , env = cl_args [ 'cluster' ] , cl_args [ 'role' ] , cl_args [ 'environ' ] topology = cl_args [ 'topology-name' ] try : result = tracker_access . get_topology_info ( cluster , env , topology , role ) spouts = result [ 'physical_plan' ] [ 'spouts' ] . keys ( ) bolts = result [ 'physical_plan' ] [ 'bolts' ] . keys ( ) components = spouts + bolts cname = cl_args [ 'component' ] if cname : if cname in components : components = [ cname ] else : Log . error ( 'Unknown component: \'%s\'' % cname ) raise except Exception : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False cresult = [ ] for comp in components : try : metrics = tracker_access . get_component_metrics ( comp , cluster , env , topology , role ) except : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False stat , header = to_table ( metrics ) cresult . append ( ( comp , stat , header ) ) for i , ( comp , stat , header ) in enumerate ( cresult ) : if i != 0 : print ( '' ) print ( '\'%s\' metrics:' % comp ) print ( tabulate ( stat , headers = header ) ) return True
9299	def apply_filters ( self , query , filters ) : assert isinstance ( query , peewee . Query ) assert isinstance ( filters , dict )
4731	def terminate ( self ) : if self . __thread : cmd = [ "who am i" ] status , output , _ = cij . util . execute ( cmd , shell = True , echo = True ) if status : cij . warn ( "cij.dmesg.terminate: who am i failed" ) return 1 tty = output . split ( ) [ 1 ] cmd = [ "pkill -f '{}' -t '{}'" . format ( " " . join ( self . __prefix ) , tty ) ] status , _ , _ = cij . util . execute ( cmd , shell = True , echo = True ) if status : cij . warn ( "cij.dmesg.terminate: pkill failed" ) return 1 self . __thread . join ( ) self . __thread = None return 0
5746	def asn ( self , ip , announce_date = None ) : assignations , announce_date , _ = self . run ( ip , announce_date ) return next ( ( assign for assign in assignations if assign is not None ) , None ) , announce_date
12691	def write_table_pair_potential ( func , dfunc = None , bounds = ( 1.0 , 10.0 ) , samples = 1000 , tollerance = 1e-6 , keyword = 'PAIR' ) : r_min , r_max = bounds if dfunc is None : dfunc = lambda r : ( func ( r + tollerance ) - func ( r - tollerance ) ) / ( 2 * tollerance ) i = np . arange ( 1 , samples + 1 ) r = np . linspace ( r_min , r_max , samples ) forces = func ( r ) energies = dfunc ( r ) lines = [ '%d %f %f %f\n' % ( index , radius , force , energy ) for index , radius , force , energy in zip ( i , r , forces , energies ) ] return "%s\nN %d\n\n" % ( keyword , samples ) + '' . join ( lines )
7614	def get_deck_link ( self , deck : BaseAttrDict ) : deck_link = 'https://link.clashroyale.com/deck/en?deck=' for i in deck : card = self . get_card_info ( i . name ) deck_link += '{0.id};' . format ( card ) return deck_link
8580	def create_server ( self , datacenter_id , server ) : data = json . dumps ( self . _create_server_dict ( server ) ) response = self . _perform_request ( url = '/datacenters/%s/servers' % ( datacenter_id ) , method = 'POST' , data = data ) return response
12181	def api_subclass_factory ( name , docstring , remove_methods , base = SlackApi ) : methods = deepcopy ( base . API_METHODS ) for parent , to_remove in remove_methods . items ( ) : if to_remove is ALL : del methods [ parent ] else : for method in to_remove : del methods [ parent ] [ method ] return type ( name , ( base , ) , dict ( API_METHODS = methods , __doc__ = docstring ) )
4338	def phaser ( self , gain_in = 0.8 , gain_out = 0.74 , delay = 3 , decay = 0.4 , speed = 0.5 , modulation_shape = 'sinusoidal' ) : if not is_number ( gain_in ) or gain_in <= 0 or gain_in > 1 : raise ValueError ( "gain_in must be a number between 0 and 1." ) if not is_number ( gain_out ) or gain_out <= 0 or gain_out > 1 : raise ValueError ( "gain_out must be a number between 0 and 1." ) if not is_number ( delay ) or delay <= 0 or delay > 5 : raise ValueError ( "delay must be a positive number." ) if not is_number ( decay ) or decay < 0.1 or decay > 0.5 : raise ValueError ( "decay must be a number between 0.1 and 0.5." ) if not is_number ( speed ) or speed < 0.1 or speed > 2 : raise ValueError ( "speed must be a positive number." ) if modulation_shape not in [ 'sinusoidal' , 'triangular' ] : raise ValueError ( "modulation_shape must be one of 'sinusoidal', 'triangular'." ) effect_args = [ 'phaser' , '{:f}' . format ( gain_in ) , '{:f}' . format ( gain_out ) , '{:f}' . format ( delay ) , '{:f}' . format ( decay ) , '{:f}' . format ( speed ) ] if modulation_shape == 'sinusoidal' : effect_args . append ( '-s' ) elif modulation_shape == 'triangular' : effect_args . append ( '-t' ) self . effects . extend ( effect_args ) self . effects_log . append ( 'phaser' ) return self
3975	def _env_vars_from_file ( filename ) : def split_env ( env ) : if '=' in env : return env . split ( '=' , 1 ) else : return env , None env = { } for line in open ( filename , 'r' ) : line = line . strip ( ) if line and not line . startswith ( '#' ) : k , v = split_env ( line ) env [ k ] = v return env
4461	def transform ( self , jam ) : yield jam for jam_out in self . transformer . transform ( jam ) : yield jam_out
10327	def statistics ( graph , ps , spanning_cluster = True , model = 'bond' , alpha = alpha_1sigma , runs = 40 ) : my_microcanonical_averages = microcanonical_averages ( graph = graph , runs = runs , spanning_cluster = spanning_cluster , model = model , alpha = alpha ) my_microcanonical_averages_arrays = microcanonical_averages_arrays ( my_microcanonical_averages ) return canonical_averages ( ps , my_microcanonical_averages_arrays )
3939	def get_chunks ( self , new_data_bytes ) : self . _buf += new_data_bytes while True : buf_decoded = _best_effort_decode ( self . _buf ) buf_utf16 = buf_decoded . encode ( 'utf-16' ) [ 2 : ] length_str_match = LEN_REGEX . match ( buf_decoded ) if length_str_match is None : break else : length_str = length_str_match . group ( 1 ) length = int ( length_str ) * 2 length_length = len ( ( length_str + '\n' ) . encode ( 'utf-16' ) [ 2 : ] ) if len ( buf_utf16 ) - length_length < length : break submission = buf_utf16 [ length_length : length_length + length ] yield submission . decode ( 'utf-16' ) drop_length = ( len ( ( length_str + '\n' ) . encode ( ) ) + len ( submission . decode ( 'utf-16' ) . encode ( ) ) ) self . _buf = self . _buf [ drop_length : ]
2064	def migrate ( self , expression , name_migration_map = None ) : if name_migration_map is None : name_migration_map = { } object_migration_map = { } foreign_vars = itertools . filterfalse ( self . is_declared , get_variables ( expression ) ) for foreign_var in foreign_vars : if foreign_var . name in name_migration_map : migrated_name = name_migration_map [ foreign_var . name ] native_var = self . get_variable ( migrated_name ) assert native_var is not None , "name_migration_map contains a variable that does not exist in this ConstraintSet" object_migration_map [ foreign_var ] = native_var else : migrated_name = foreign_var . name if migrated_name in self . _declarations : migrated_name = self . _make_unique_name ( f'{foreign_var.name}_migrated' ) if isinstance ( foreign_var , Bool ) : new_var = self . new_bool ( name = migrated_name ) elif isinstance ( foreign_var , BitVec ) : new_var = self . new_bitvec ( foreign_var . size , name = migrated_name ) elif isinstance ( foreign_var , Array ) : new_var = self . new_array ( index_max = foreign_var . index_max , index_bits = foreign_var . index_bits , value_bits = foreign_var . value_bits , name = migrated_name ) . array else : raise NotImplemented ( f"Unknown expression type {type(var)} encountered during expression migration" ) object_migration_map [ foreign_var ] = new_var name_migration_map [ foreign_var . name ] = new_var . name migrated_expression = replace ( expression , object_migration_map ) return migrated_expression
7771	def payload_class_for_element_name ( element_name ) : logger . debug ( " looking up payload class for element: {0!r}" . format ( element_name ) ) logger . debug ( " known: {0!r}" . format ( STANZA_PAYLOAD_CLASSES ) ) if element_name in STANZA_PAYLOAD_CLASSES : return STANZA_PAYLOAD_CLASSES [ element_name ] else : return XMLPayload
1329	def has_gradient ( self ) : try : self . __model . gradient self . __model . predictions_and_gradient except AttributeError : return False else : return True
9844	def __refill_tokenbuffer ( self ) : if len ( self . tokens ) == 0 : self . __tokenize ( self . dxfile . readline ( ) )
3224	def _build_google_client ( service , api_version , http_auth ) : client = build ( service , api_version , http = http_auth ) return client
10580	def calculate ( self , ** state ) : super ( ) . calculate ( ** state ) return self . mm * self . P / R / state [ "T" ]
8677	def ssh ( key_name , no_tunnel , stash , passphrase , backend ) : def execute ( command ) : try : click . echo ( 'Executing: {0}' . format ( ' ' . join ( command ) ) ) subprocess . check_call ( ' ' . join ( command ) , shell = True ) except subprocess . CalledProcessError : sys . exit ( 1 ) stash = _get_stash ( backend , stash , passphrase ) key = stash . get ( key_name ) if key : _assert_is_ssh_type_key ( key ) else : sys . exit ( 'Key `{0}` not found' . format ( key_name ) ) conn_info = key [ 'value' ] ssh_key_path = conn_info . get ( 'ssh_key_path' ) ssh_key = conn_info . get ( 'ssh_key' ) proxy_key_path = conn_info . get ( 'proxy_key_path' ) proxy_key = conn_info . get ( 'proxy_key' ) id_file = _write_tmp ( ssh_key ) if ssh_key else ssh_key_path conn_info [ 'ssh_key_path' ] = id_file if conn_info . get ( 'proxy' ) : proxy_id_file = _write_tmp ( proxy_key ) if proxy_key else proxy_key_path conn_info [ 'proxy_key_path' ] = proxy_id_file ssh_command = _build_ssh_command ( conn_info , no_tunnel ) try : execute ( ssh_command ) finally : if id_file != ssh_key_path : click . echo ( 'Removing temp ssh key file: {0}...' . format ( id_file ) ) os . remove ( id_file ) if conn_info . get ( 'proxy' ) and proxy_id_file != proxy_key_path : click . echo ( 'Removing temp proxy key file: {0}...' . format ( proxy_id_file ) ) os . remove ( proxy_id_file )
8262	def repeat ( self , n = 2 , oscillate = False , callback = None ) : colorlist = ColorList ( ) colors = ColorList . copy ( self ) for i in _range ( n ) : colorlist . extend ( colors ) if oscillate : colors = colors . reverse ( ) if callback : colors = callback ( colors ) return colorlist
3949	def execute ( self , using = None ) : if not using : using = self . get_connection ( ) inserted_entities = { } for klass in self . orders : number = self . quantities [ klass ] if klass not in inserted_entities : inserted_entities [ klass ] = [ ] for i in range ( 0 , number ) : entity = self . entities [ klass ] . execute ( using , inserted_entities ) inserted_entities [ klass ] . append ( entity ) return inserted_entities
12283	def lookup ( self , username = None , reponame = None , key = None ) : if key is None : key = self . key ( username , reponame ) if key not in self . repos : raise UnknownRepository ( ) return self . repos [ key ]
10597	def h_L ( self , L , theta , Ts , ** statef ) : Nu_L = self . Nu_L ( L , theta , Ts , ** statef ) k = self . _fluid . k ( T = self . Tr ) return Nu_L * k / L
13557	def get_all_images ( self ) : self_imgs = self . image_set . all ( ) update_ids = self . update_set . values_list ( 'id' , flat = True ) u_images = UpdateImage . objects . filter ( update__id__in = update_ids ) return list ( chain ( self_imgs , u_images ) )
4544	def _add_redundant_arguments ( parser ) : parser . add_argument ( '-a' , '--animation' , default = None , help = 'Default animation type if no animation is specified' ) if deprecated . allowed ( ) : parser . add_argument ( '--dimensions' , '--dim' , default = None , help = 'DEPRECATED: x, (x, y) or (x, y, z) dimensions for project' ) parser . add_argument ( '--shape' , default = None , help = 'x, (x, y) or (x, y, z) dimensions for project' ) parser . add_argument ( '-l' , '--layout' , default = None , help = 'Default layout class if no layout is specified' ) parser . add_argument ( '--numbers' , '-n' , default = 'python' , choices = NUMBER_TYPES , help = NUMBERS_HELP ) parser . add_argument ( '-p' , '--path' , default = None , help = PATH_HELP )
3281	def compute_digest_response ( self , realm , user_name , method , uri , nonce , cnonce , qop , nc , environ ) : def md5h ( data ) : return md5 ( compat . to_bytes ( data ) ) . hexdigest ( ) def md5kd ( secret , data ) : return md5h ( secret + ":" + data ) A1 = self . domain_controller . digest_auth_user ( realm , user_name , environ ) if not A1 : return False A2 = method + ":" + uri if qop : res = md5kd ( A1 , nonce + ":" + nc + ":" + cnonce + ":" + qop + ":" + md5h ( A2 ) ) else : res = md5kd ( A1 , nonce + ":" + md5h ( A2 ) ) return res
10557	def login ( self , oauth_filename = "oauth" , uploader_id = None ) : cls_name = type ( self ) . __name__ oauth_cred = os . path . join ( os . path . dirname ( OAUTH_FILEPATH ) , oauth_filename + '.cred' ) try : if not self . api . login ( oauth_credentials = oauth_cred , uploader_id = uploader_id ) : try : self . api . perform_oauth ( storage_filepath = oauth_cred ) except OSError : logger . exception ( "\nUnable to login with specified oauth code." ) self . api . login ( oauth_credentials = oauth_cred , uploader_id = uploader_id ) except ( OSError , ValueError ) : logger . exception ( "{} authentication failed." . format ( cls_name ) ) return False if not self . is_authenticated : logger . warning ( "{} authentication failed." . format ( cls_name ) ) return False logger . info ( "{} authentication succeeded.\n" . format ( cls_name ) ) return True
11906	def to_permutation_matrix ( matches ) : n = len ( matches ) P = np . zeros ( ( n , n ) ) P [ list ( zip ( * ( matches . items ( ) ) ) ) ] = 1 return P
1941	def map_memory_callback ( self , address , size , perms , name , offset , result ) : logger . info ( ' ' . join ( ( "Mapping Memory @" , hex ( address ) if type ( address ) is int else "0x??" , hr_size ( size ) , "-" , perms , "-" , f"{name}:{hex(offset) if name else ''}" , "->" , hex ( result ) ) ) ) self . _emu . mem_map ( address , size , convert_permissions ( perms ) ) self . copy_memory ( address , size )
7634	def __load_jams_schema ( ) : schema_file = os . path . join ( SCHEMA_DIR , 'jams_schema.json' ) jams_schema = None with open ( resource_filename ( __name__ , schema_file ) , mode = 'r' ) as fdesc : jams_schema = json . load ( fdesc ) if jams_schema is None : raise JamsError ( 'Unable to load JAMS schema' ) return jams_schema
9055	def fast_scan ( self , M , verbose = True ) : from tqdm import tqdm if M . ndim != 2 : raise ValueError ( "`M` array must be bidimensional." ) p = M . shape [ 1 ] lmls = empty ( p ) effsizes0 = empty ( ( p , self . _XTQ [ 0 ] . shape [ 0 ] ) ) effsizes0_se = empty ( ( p , self . _XTQ [ 0 ] . shape [ 0 ] ) ) effsizes1 = empty ( p ) effsizes1_se = empty ( p ) scales = empty ( p ) if verbose : nchunks = min ( p , 30 ) else : nchunks = min ( p , 1 ) chunk_size = ( p + nchunks - 1 ) // nchunks for i in tqdm ( range ( nchunks ) , desc = "Scanning" , disable = not verbose ) : start = i * chunk_size stop = min ( start + chunk_size , M . shape [ 1 ] ) r = self . _fast_scan_chunk ( M [ : , start : stop ] ) lmls [ start : stop ] = r [ "lml" ] effsizes0 [ start : stop , : ] = r [ "effsizes0" ] effsizes0_se [ start : stop , : ] = r [ "effsizes0_se" ] effsizes1 [ start : stop ] = r [ "effsizes1" ] effsizes1_se [ start : stop ] = r [ "effsizes1_se" ] scales [ start : stop ] = r [ "scale" ] return { "lml" : lmls , "effsizes0" : effsizes0 , "effsizes0_se" : effsizes0_se , "effsizes1" : effsizes1 , "effsizes1_se" : effsizes1_se , "scale" : scales , }
6836	def base_boxes ( self ) : return sorted ( list ( set ( [ name for name , provider in self . _box_list ( ) ] ) ) )
8929	def pylint ( ctx , skip_tests = False , skip_root = False , reports = False ) : cfg = config . load ( ) add_dir2pypath ( cfg . project_root ) if not os . path . exists ( cfg . testjoin ( '__init__.py' ) ) : add_dir2pypath ( cfg . testjoin ( ) ) namelist = set ( ) for package in cfg . project . get ( 'packages' , [ ] ) : if '.' not in package : namelist . add ( cfg . srcjoin ( package ) ) for module in cfg . project . get ( 'py_modules' , [ ] ) : namelist . add ( module + '.py' ) if not skip_tests : test_py = antglob . FileSet ( cfg . testdir , '**/*.py' ) test_py = [ cfg . testjoin ( i ) for i in test_py ] if test_py : namelist |= set ( test_py ) if not skip_root : root_py = antglob . FileSet ( '.' , '*.py' ) if root_py : namelist |= set ( root_py ) namelist = set ( [ i [ len ( os . getcwd ( ) ) + 1 : ] if i . startswith ( os . getcwd ( ) + os . sep ) else i for i in namelist ] ) cmd = 'pylint' cmd += ' "{}"' . format ( '" "' . join ( sorted ( namelist ) ) ) cmd += ' --reports={0}' . format ( 'y' if reports else 'n' ) for cfgfile in ( '.pylintrc' , 'pylint.rc' , 'pylint.cfg' , 'project.d/pylint.cfg' ) : if os . path . exists ( cfgfile ) : cmd += ' --rcfile={0}' . format ( cfgfile ) break try : shell . run ( cmd , report_error = False , runner = ctx . run ) notify . info ( "OK - No problems found by pylint." ) except exceptions . Failure as exc : if exc . result . return_code & 32 : notify . error ( "Usage error, bad arguments in {}?!" . format ( repr ( cmd ) ) ) raise else : bits = { 1 : "fatal" , 2 : "error" , 4 : "warning" , 8 : "refactor" , 16 : "convention" , } notify . warning ( "Some messages of type {} issued by pylint." . format ( ", " . join ( [ text for bit , text in bits . items ( ) if exc . result . return_code & bit ] ) ) ) if exc . result . return_code & 3 : notify . error ( "Exiting due to fatal / error message." ) raise
13282	def _parse_command ( self , source , start_index ) : parsed_elements = [ ] running_index = start_index for element in self . elements : opening_bracket = element [ 'bracket' ] closing_bracket = self . _brackets [ opening_bracket ] element_start = None element_end = None for i , c in enumerate ( source [ running_index : ] , start = running_index ) : if c == element [ 'bracket' ] : element_start = i break elif c == '\n' : if element [ 'required' ] is True : content = self . _parse_whitespace_argument ( source [ running_index : ] , self . name ) return ParsedCommand ( self . name , [ { 'index' : element [ 'index' ] , 'name' : element [ 'name' ] , 'content' : content . strip ( ) } ] , start_index , source [ start_index : i ] ) else : break if element_start is None and element [ 'required' ] is False : continue elif element_start is None and element [ 'required' ] is True : message = ( 'Parsing command {0} at index {1:d}, ' 'did not detect element {2:d}' . format ( self . name , start_index , element [ 'index' ] ) ) raise CommandParserError ( message ) balance = 1 for i , c in enumerate ( source [ element_start + 1 : ] , start = element_start + 1 ) : if c == opening_bracket : balance += 1 elif c == closing_bracket : balance -= 1 if balance == 0 : element_end = i break if balance > 0 : message = ( 'Parsing command {0} at index {1:d}, ' 'did not find closing bracket for required ' 'command element {2:d}' . format ( self . name , start_index , element [ 'index' ] ) ) raise CommandParserError ( message ) element_content = source [ element_start + 1 : element_end ] parsed_element = { 'index' : element [ 'index' ] , 'name' : element [ 'name' ] , 'content' : element_content . strip ( ) } parsed_elements . append ( parsed_element ) running_index = element_end + 1 command_source = source [ start_index : running_index ] parsed_command = ParsedCommand ( self . name , parsed_elements , start_index , command_source ) return parsed_command
6390	def _sb_has_vowel ( self , term ) : for letter in term : if letter in self . _vowels : return True return False
11161	def autopep8 ( self , ** kwargs ) : self . assert_is_dir_and_exists ( ) for p in self . select_by_ext ( ".py" ) : with open ( p . abspath , "rb" ) as f : code = f . read ( ) . decode ( "utf-8" ) formatted_code = autopep8 . fix_code ( code , ** kwargs ) with open ( p . abspath , "wb" ) as f : f . write ( formatted_code . encode ( "utf-8" ) )
8467	def path ( self , value ) : if not value . endswith ( '/' ) : self . _path = '{v}/' . format ( v = value ) else : self . _path = value
2271	def _win32_symlink ( path , link , verbose = 0 ) : from ubelt import util_cmd if os . path . isdir ( path ) : if verbose : print ( '... as directory symlink' ) command = 'mklink /D "{}" "{}"' . format ( link , path ) else : if verbose : print ( '... as file symlink' ) command = 'mklink "{}" "{}"' . format ( link , path ) if command is not None : info = util_cmd . cmd ( command , shell = True ) if info [ 'ret' ] != 0 : from ubelt import util_format permission_msg = 'You do not have sufficient privledges' if permission_msg not in info [ 'err' ] : print ( 'Failed command:' ) print ( info [ 'command' ] ) print ( util_format . repr2 ( info , nl = 1 ) ) raise OSError ( str ( info ) ) return link
2731	def create ( self ) : data = { "name" : self . name , "ip_address" : self . ip_address , } domain = self . get_data ( "domains" , type = POST , params = data ) return domain
6246	def draw ( self , time : float , frametime : float , target : moderngl . Framebuffer ) : raise NotImplementedError ( "draw() is not implemented" )
6795	def syncdb ( self , site = None , all = 0 , database = None , ignore_errors = 1 ) : r = self . local_renderer ignore_errors = int ( ignore_errors ) post_south = self . version_tuple >= ( 1 , 7 , 0 ) use_run_syncdb = self . version_tuple >= ( 1 , 9 , 0 ) r . env . db_syncdb_all_flag = '--all' if int ( all ) else '' r . env . db_syncdb_database = '' if database : r . env . db_syncdb_database = ' --database=%s' % database if self . is_local : r . env . project_dir = r . env . local_project_dir site = site or self . genv . SITE for _site , site_data in r . iter_unique_databases ( site = site ) : r . env . SITE = _site with self . settings ( warn_only = ignore_errors ) : if post_south : if use_run_syncdb : r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} migrate --run-syncdb --noinput {db_syncdb_database}' ) else : r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} migrate --noinput {db_syncdb_database}' ) else : r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} syncdb --noinput {db_syncdb_all_flag} {db_syncdb_database}' )
9479	def node ( self , node ) : if node == self . node1 : return self . node2 elif node == self . node2 : return self . node1 else : return None
1701	def join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . INNER , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
8026	def getPaths ( roots , ignores = None ) : paths , count , ignores = [ ] , 0 , ignores or [ ] ignore_re = multiglob_compile ( ignores , prefix = False ) for root in roots : root = os . path . realpath ( root ) if os . path . isfile ( root ) : paths . append ( root ) continue for fldr in os . walk ( root ) : out . write ( "Gathering file paths to compare... (%d files examined)" % count ) for subdir in fldr [ 1 ] : dirpath = os . path . join ( fldr [ 0 ] , subdir ) if ignore_re . match ( dirpath ) : fldr [ 1 ] . remove ( subdir ) for filename in fldr [ 2 ] : filepath = os . path . join ( fldr [ 0 ] , filename ) if ignore_re . match ( filepath ) : continue paths . append ( filepath ) count += 1 out . write ( "Found %s files to be compared for duplication." % ( len ( paths ) ) , newline = True ) return paths
13738	def _real_time_thread ( self ) : while self . ws_client . connected ( ) : if self . die : break if self . pause : sleep ( 5 ) continue message = self . ws_client . receive ( ) if message is None : break message_type = message [ 'type' ] if message_type == 'error' : continue if message [ 'sequence' ] <= self . sequence : continue if message_type == 'open' : self . _handle_open ( message ) elif message_type == 'match' : self . _handle_match ( message ) elif message_type == 'done' : self . _handle_done ( message ) elif message_type == 'change' : self . _handle_change ( message ) else : continue self . ws_client . disconnect ( )
415	def save_dataset ( self , dataset = None , dataset_name = None , ** kwargs ) : self . _fill_project_info ( kwargs ) if dataset_name is None : raise Exception ( "dataset_name is None, please give a dataset name" ) kwargs . update ( { 'dataset_name' : dataset_name } ) s = time . time ( ) try : dataset_id = self . dataset_fs . put ( self . _serialization ( dataset ) ) kwargs . update ( { 'dataset_id' : dataset_id , 'time' : datetime . utcnow ( ) } ) self . db . Dataset . insert_one ( kwargs ) print ( "[Database] Save dataset: SUCCESS, took: {}s" . format ( round ( time . time ( ) - s , 2 ) ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) print ( "[Database] Save dataset: FAIL" ) return False
12261	def gradient_optimizer ( coro ) : class GradientOptimizer ( Optimizer ) : @ wraps ( coro ) def __init__ ( self , * args , ** kwargs ) : self . algorithm = coro ( * args , ** kwargs ) self . algorithm . send ( None ) self . operators = [ ] def set_transform ( self , func ) : self . transform = compose ( destruct , func , self . restruct ) def minimize ( self , f_df , x0 , display = sys . stdout , maxiter = 1e3 ) : self . display = display self . theta = x0 xk = self . algorithm . send ( destruct ( x0 ) . copy ( ) ) store = defaultdict ( list ) runtimes = [ ] if len ( self . operators ) == 0 : self . operators = [ proxops . identity ( ) ] obj , grad = wrap ( f_df , x0 ) transform = compose ( destruct , * reversed ( self . operators ) , self . restruct ) self . optional_print ( tp . header ( [ 'Iteration' , 'Objective' , '||Grad||' , 'Runtime' ] ) ) try : for k in count ( ) : tstart = perf_counter ( ) f = obj ( xk ) df = grad ( xk ) xk = transform ( self . algorithm . send ( df ) ) runtimes . append ( perf_counter ( ) - tstart ) store [ 'f' ] . append ( f ) self . optional_print ( tp . row ( [ k , f , np . linalg . norm ( destruct ( df ) ) , tp . humantime ( runtimes [ - 1 ] ) ] ) ) if k >= maxiter : break except KeyboardInterrupt : pass self . optional_print ( tp . bottom ( 4 ) ) self . optional_print ( u'\u279b Final objective: {}' . format ( store [ 'f' ] [ - 1 ] ) ) self . optional_print ( u'\u279b Total runtime: {}' . format ( tp . humantime ( sum ( runtimes ) ) ) ) self . optional_print ( u'\u279b Per iteration runtime: {} +/- {}' . format ( tp . humantime ( np . mean ( runtimes ) ) , tp . humantime ( np . std ( runtimes ) ) , ) ) return OptimizeResult ( { 'x' : self . restruct ( xk ) , 'f' : f , 'df' : self . restruct ( df ) , 'k' : k , 'obj' : np . array ( store [ 'f' ] ) , } ) return GradientOptimizer
4310	def _build_input_format_list ( input_filepath_list , input_volumes = None , input_format = None ) : n_inputs = len ( input_filepath_list ) input_format_list = [ ] for _ in range ( n_inputs ) : input_format_list . append ( [ ] ) if input_volumes is None : vols = [ 1 ] * n_inputs else : n_volumes = len ( input_volumes ) if n_volumes < n_inputs : logger . warning ( 'Volumes were only specified for %s out of %s files.' 'The last %s files will remain at their original volumes.' , n_volumes , n_inputs , n_inputs - n_volumes ) vols = input_volumes + [ 1 ] * ( n_inputs - n_volumes ) elif n_volumes > n_inputs : logger . warning ( '%s volumes were specified but only %s input files exist.' 'The last %s volumes will be ignored.' , n_volumes , n_inputs , n_volumes - n_inputs ) vols = input_volumes [ : n_inputs ] else : vols = [ v for v in input_volumes ] if input_format is None : fmts = [ [ ] for _ in range ( n_inputs ) ] else : n_fmts = len ( input_format ) if n_fmts < n_inputs : logger . warning ( 'Input formats were only specified for %s out of %s files.' 'The last %s files will remain unformatted.' , n_fmts , n_inputs , n_inputs - n_fmts ) fmts = [ f for f in input_format ] fmts . extend ( [ [ ] for _ in range ( n_inputs - n_fmts ) ] ) elif n_fmts > n_inputs : logger . warning ( '%s Input formats were specified but only %s input files exist' '. The last %s formats will be ignored.' , n_fmts , n_inputs , n_fmts - n_inputs ) fmts = input_format [ : n_inputs ] else : fmts = [ f for f in input_format ] for i , ( vol , fmt ) in enumerate ( zip ( vols , fmts ) ) : input_format_list [ i ] . extend ( [ '-v' , '{}' . format ( vol ) ] ) input_format_list [ i ] . extend ( fmt ) return input_format_list
6616	def receive ( self ) : pkgidx_result_pairs = self . receive_all ( ) if pkgidx_result_pairs is None : return results = [ r for _ , r in pkgidx_result_pairs ] return results
11231	def replace ( self , ** kwargs ) : new_kwargs = { "interval" : self . _interval , "count" : self . _count , "dtstart" : self . _dtstart , "freq" : self . _freq , "until" : self . _until , "wkst" : self . _wkst , "cache" : False if self . _cache is None else True } new_kwargs . update ( self . _original_rule ) new_kwargs . update ( kwargs ) return rrule ( ** new_kwargs )
6798	def set_root_login ( self , r ) : try : r . env . db_root_username = r . env . root_username except AttributeError : pass try : r . env . db_root_password = r . env . root_password except AttributeError : pass key = r . env . get ( 'db_host' ) if self . verbose : print ( 'db.set_root_login.key:' , key ) print ( 'db.set_root_logins:' , r . env . root_logins ) if key in r . env . root_logins : data = r . env . root_logins [ key ] if 'username' in data : r . env . db_root_username = data [ 'username' ] r . genv . db_root_username = data [ 'username' ] if 'password' in data : r . env . db_root_password = data [ 'password' ] r . genv . db_root_password = data [ 'password' ] else : msg = 'Warning: No root login entry found for host %s in role %s.' % ( r . env . get ( 'db_host' ) , self . genv . get ( 'ROLE' ) ) print ( msg , file = sys . stderr )
9794	def _ignore_path ( cls , path , ignore_list = None , white_list = None ) : ignore_list = ignore_list or [ ] white_list = white_list or [ ] return ( cls . _matches_patterns ( path , ignore_list ) and not cls . _matches_patterns ( path , white_list ) )
2221	def _rectify_hasher ( hasher ) : if xxhash is not None : if hasher in { 'xxh32' , 'xx32' , 'xxhash' } : return xxhash . xxh32 if hasher in { 'xxh64' , 'xx64' } : return xxhash . xxh64 if hasher is NoParam or hasher == 'default' : hasher = DEFAULT_HASHER elif isinstance ( hasher , six . string_types ) : if hasher not in hashlib . algorithms_available : raise KeyError ( 'unknown hasher: {}' . format ( hasher ) ) else : hasher = getattr ( hashlib , hasher ) elif isinstance ( hasher , HASH ) : return lambda : hasher return hasher
1734	def remove_objects ( code , count = 1 ) : replacements = { } br = bracket_split ( code , [ '{}' , '[]' ] ) res = '' last = '' for e in br : if e [ 0 ] == '{' : n , temp_rep , cand_count = remove_objects ( e [ 1 : - 1 ] , count ) if is_object ( n , last ) : res += ' ' + OBJECT_LVAL % count replacements [ OBJECT_LVAL % count ] = e count += 1 else : res += '{%s}' % n count = cand_count replacements . update ( temp_rep ) elif e [ 0 ] == '[' : if is_array ( last ) : res += e else : n , rep , count = remove_objects ( e [ 1 : - 1 ] , count ) res += '[%s]' % n replacements . update ( rep ) else : res += e last = e return res , replacements , count
12866	def startup ( self , app ) : self . database . init_async ( app . loop ) if not self . cfg . connection_manual : app . middlewares . insert ( 0 , self . _middleware )
8261	def reverse ( self ) : colors = ColorList . copy ( self ) _list . reverse ( colors ) return colors
6703	def enter_password_change ( self , username = None , old_password = None ) : from fabric . state import connections from fabric . network import disconnect_all r = self . local_renderer r . genv . user = r . genv . user or username r . pc ( 'Changing password for user {user} via interactive prompts.' ) r . env . old_password = r . env . default_passwords [ self . genv . user ] r . env . new_password = self . env . passwords [ self . genv . user ] if old_password : r . env . old_password = old_password prompts = { '(current) UNIX password: ' : r . env . old_password , 'Enter new UNIX password: ' : r . env . new_password , 'Retype new UNIX password: ' : r . env . new_password , } print ( 'prompts:' , prompts ) r . env . password = r . env . old_password with self . settings ( warn_only = True ) : ret = r . _local ( "sshpass -p '{password}' ssh -o StrictHostKeyChecking=no {user}@{host_string} echo hello" , capture = True ) if ret . return_code in ( 1 , 6 ) or 'hello' in ret : self . genv . password = r . env . old_password elif self . genv . user in self . genv . user_passwords : self . genv . password = r . env . new_password else : self . genv . password = None print ( 'using password:' , self . genv . password ) with self . settings ( prompts = prompts ) : ret = r . _run ( 'echo checking for expired password' ) print ( 'ret:[%s]' % ret ) do_disconnect = 'passwd: password updated successfully' in ret print ( 'do_disconnect:' , do_disconnect ) if do_disconnect : disconnect_all ( ) self . genv . password = r . env . new_password
4402	def remove_node ( self , node ) : self . nodes . remove ( node ) for x in xrange ( self . replicas ) : ring_key = self . hash_method ( b ( "%s:%d" % ( node , x ) ) ) self . ring . pop ( ring_key ) self . sorted_keys . remove ( ring_key )
5543	def clip ( self , array , geometries , inverted = False , clip_buffer = 0 ) : return commons_clip . clip_array_with_vector ( array , self . tile . affine , geometries , inverted = inverted , clip_buffer = clip_buffer * self . tile . pixel_x_size )
5089	def dropHistoricalTable ( apps , schema_editor ) : table_name = 'sap_success_factors_historicalsapsuccessfactorsenterprisecus80ad' if table_name in connection . introspection . table_names ( ) : migrations . DeleteModel ( name = table_name , )
2835	def write ( self , data , assert_ss = True , deassert_ss = True ) : if self . _mosi is None : raise RuntimeError ( 'Write attempted with no MOSI pin specified.' ) if assert_ss and self . _ss is not None : self . _gpio . set_low ( self . _ss ) for byte in data : for i in range ( 8 ) : if self . _write_shift ( byte , i ) & self . _mask : self . _gpio . set_high ( self . _mosi ) else : self . _gpio . set_low ( self . _mosi ) self . _gpio . output ( self . _sclk , not self . _clock_base ) self . _gpio . output ( self . _sclk , self . _clock_base ) if deassert_ss and self . _ss is not None : self . _gpio . set_high ( self . _ss )
12834	def on_update_stage ( self , dt ) : for actor in self . actors : actor . on_update_game ( dt ) self . forum . on_update_game ( ) with self . world . _unlock_temporarily ( ) : self . world . on_update_game ( dt ) if self . world . has_game_ended ( ) : self . exit_stage ( )
3747	def calculate ( self , T , P , zs , ws , method ) : r if method == MIXING_LOG_MOLAR : mus = [ i ( T , P ) for i in self . ViscosityLiquids ] return mixing_logarithmic ( zs , mus ) elif method == MIXING_LOG_MASS : mus = [ i ( T , P ) for i in self . ViscosityLiquids ] return mixing_logarithmic ( ws , mus ) elif method == LALIBERTE_MU : ws = list ( ws ) ws . pop ( self . index_w ) return Laliberte_viscosity ( T , ws , self . wCASs ) else : raise Exception ( 'Method not valid' )
8123	def textpath ( self , txt , x , y , width = None , height = 1000000 , enableRendering = False , ** kwargs ) : txt = self . Text ( txt , x , y , width , height , ** kwargs ) path = txt . path if draw : path . draw ( ) return path
1994	def save_state ( self , state , state_id = None ) : assert isinstance ( state , StateBase ) if state_id is None : state_id = self . _get_id ( ) else : self . rm_state ( state_id ) self . _store . save_state ( state , f'{self._prefix}{state_id:08x}{self._suffix}' ) return state_id
8765	def opt_args_decorator ( func ) : @ wraps ( func ) def wrapped_dec ( * args , ** kwargs ) : if len ( args ) == 1 and len ( kwargs ) == 0 and callable ( args [ 0 ] ) : return func ( args [ 0 ] ) else : return lambda realf : func ( realf , * args , ** kwargs ) return wrapped_dec
10782	def _feature_guess ( im , rad , minmass = None , use_tp = False , trim_edge = False ) : if minmass is None : minmass = rad ** 3 * 4 / 3. * np . pi * 0.01 if use_tp : diameter = np . ceil ( 2 * rad ) diameter += 1 - ( diameter % 2 ) df = peri . trackpy . locate ( im , int ( diameter ) , minmass = minmass ) npart = np . array ( df [ 'mass' ] ) . size guess = np . zeros ( [ npart , 3 ] ) guess [ : , 0 ] = df [ 'z' ] guess [ : , 1 ] = df [ 'y' ] guess [ : , 2 ] = df [ 'x' ] mass = df [ 'mass' ] else : guess , mass = initializers . local_max_featuring ( im , radius = rad , minmass = minmass , trim_edge = trim_edge ) npart = guess . shape [ 0 ] inds = np . argsort ( mass ) [ : : - 1 ] return guess [ inds ] . copy ( ) , npart
10113	def filter_rows_as_dict ( fname , filter_ , ** kw ) : filter_ = DictFilter ( filter_ ) rewrite ( fname , filter_ , ** kw ) return filter_ . removed
8767	def _validate_allocation_pools ( self ) : ip_pools = self . _alloc_pools subnet_cidr = self . _subnet_cidr LOG . debug ( _ ( "Performing IP validity checks on allocation pools" ) ) ip_sets = [ ] for ip_pool in ip_pools : try : start_ip = netaddr . IPAddress ( ip_pool [ 'start' ] ) end_ip = netaddr . IPAddress ( ip_pool [ 'end' ] ) except netaddr . AddrFormatError : LOG . info ( _ ( "Found invalid IP address in pool: " "%(start)s - %(end)s:" ) , { 'start' : ip_pool [ 'start' ] , 'end' : ip_pool [ 'end' ] } ) raise n_exc_ext . InvalidAllocationPool ( pool = ip_pool ) if ( start_ip . version != self . _subnet_cidr . version or end_ip . version != self . _subnet_cidr . version ) : LOG . info ( _ ( "Specified IP addresses do not match " "the subnet IP version" ) ) raise n_exc_ext . InvalidAllocationPool ( pool = ip_pool ) if end_ip < start_ip : LOG . info ( _ ( "Start IP (%(start)s) is greater than end IP " "(%(end)s)" ) , { 'start' : ip_pool [ 'start' ] , 'end' : ip_pool [ 'end' ] } ) raise n_exc_ext . InvalidAllocationPool ( pool = ip_pool ) if ( start_ip < self . _subnet_first_ip or end_ip > self . _subnet_last_ip ) : LOG . info ( _ ( "Found pool larger than subnet " "CIDR:%(start)s - %(end)s" ) , { 'start' : ip_pool [ 'start' ] , 'end' : ip_pool [ 'end' ] } ) raise n_exc_ext . OutOfBoundsAllocationPool ( pool = ip_pool , subnet_cidr = subnet_cidr ) ip_sets . append ( netaddr . IPSet ( netaddr . IPRange ( ip_pool [ 'start' ] , ip_pool [ 'end' ] ) . cidrs ( ) ) ) LOG . debug ( _ ( "Checking for overlaps among allocation pools " "and gateway ip" ) ) ip_ranges = ip_pools [ : ] for l_cursor in xrange ( len ( ip_sets ) ) : for r_cursor in xrange ( l_cursor + 1 , len ( ip_sets ) ) : if ip_sets [ l_cursor ] & ip_sets [ r_cursor ] : l_range = ip_ranges [ l_cursor ] r_range = ip_ranges [ r_cursor ] LOG . info ( _ ( "Found overlapping ranges: %(l_range)s and " "%(r_range)s" ) , { 'l_range' : l_range , 'r_range' : r_range } ) raise n_exc_ext . OverlappingAllocationPools ( pool_1 = l_range , pool_2 = r_range , subnet_cidr = subnet_cidr )
9265	def fetch_and_filter_tags ( self ) : self . all_tags = self . fetcher . get_all_tags ( ) self . filtered_tags = self . get_filtered_tags ( self . all_tags ) self . fetch_tags_dates ( )
9745	def connection_made ( self , transport ) : self . transport = transport sock = transport . get_extra_info ( "socket" ) self . port = sock . getsockname ( ) [ 1 ]
9220	def parse_args ( self , args , scope ) : arguments = list ( zip ( args , [ ' ' ] * len ( args ) ) ) if args and args [ 0 ] else None zl = itertools . zip_longest if sys . version_info [ 0 ] == 3 else itertools . izip_longest if self . args : parsed = [ v if hasattr ( v , 'parse' ) else v for v in copy . copy ( self . args ) ] args = args if isinstance ( args , list ) else [ args ] vars = [ self . _parse_arg ( var , arg , scope ) for arg , var in zl ( [ a for a in args ] , parsed ) ] for var in vars : if var : var . parse ( scope ) if not arguments : arguments = [ v . value for v in vars if v ] if not arguments : arguments = '' Variable ( [ '@arguments' , None , arguments ] ) . parse ( scope )
7575	def detect_cpus ( ) : if hasattr ( os , "sysconf" ) : if os . sysconf_names . has_key ( "SC_NPROCESSORS_ONLN" ) : ncpus = os . sysconf ( "SC_NPROCESSORS_ONLN" ) if isinstance ( ncpus , int ) and ncpus > 0 : return ncpus else : return int ( os . popen2 ( "sysctl -n hw.ncpu" ) [ 1 ] . read ( ) ) if os . environ . has_key ( "NUMBER_OF_PROCESSORS" ) : ncpus = int ( os . environ [ "NUMBER_OF_PROCESSORS" ] ) if ncpus > 0 : return ncpus return 1
2874	def get_process_parser ( self , process_id_or_name ) : if process_id_or_name in self . process_parsers_by_name : return self . process_parsers_by_name [ process_id_or_name ] else : return self . process_parsers [ process_id_or_name ]
1496	def get_sub_parts ( self , query ) : parts = [ ] num_open_braces = 0 delimiter = ',' last_starting_index = 0 for i in range ( len ( query ) ) : if query [ i ] == '(' : num_open_braces += 1 elif query [ i ] == ')' : num_open_braces -= 1 elif query [ i ] == delimiter and num_open_braces == 0 : parts . append ( query [ last_starting_index : i ] . strip ( ) ) last_starting_index = i + 1 parts . append ( query [ last_starting_index : ] . strip ( ) ) return parts
12023	def check_phase ( self ) : plus_minus = set ( [ '+' , '-' ] ) for k , g in groupby ( sorted ( [ line for line in self . lines if line [ 'line_type' ] == 'feature' and line [ 'type' ] == 'CDS' and 'Parent' in line [ 'attributes' ] ] , key = lambda x : x [ 'attributes' ] [ 'Parent' ] ) , key = lambda x : x [ 'attributes' ] [ 'Parent' ] ) : cds_list = list ( g ) strand_set = list ( set ( [ line [ 'strand' ] for line in cds_list ] ) ) if len ( strand_set ) != 1 : for line in cds_list : self . add_line_error ( line , { 'message' : 'Inconsistent CDS strand with parent: {0:s}' . format ( k ) , 'error_type' : 'STRAND' } ) continue if len ( cds_list ) == 1 : if cds_list [ 0 ] [ 'phase' ] != 0 : self . add_line_error ( cds_list [ 0 ] , { 'message' : 'Wrong phase {0:d}, should be {1:d}' . format ( cds_list [ 0 ] [ 'phase' ] , 0 ) , 'error_type' : 'PHASE' } ) continue strand = strand_set [ 0 ] if strand not in plus_minus : continue if strand == '-' : sorted_cds_list = sorted ( cds_list , key = lambda x : x [ 'end' ] , reverse = True ) else : sorted_cds_list = sorted ( cds_list , key = lambda x : x [ 'start' ] ) phase = 0 for line in sorted_cds_list : if line [ 'phase' ] != phase : self . add_line_error ( line , { 'message' : 'Wrong phase {0:d}, should be {1:d}' . format ( line [ 'phase' ] , phase ) , 'error_type' : 'PHASE' } ) phase = ( 3 - ( ( line [ 'end' ] - line [ 'start' ] + 1 - phase ) % 3 ) ) % 3
2600	def unset_logging ( self ) : if self . logger_flag is True : return root_logger = logging . getLogger ( ) for hndlr in root_logger . handlers : if hndlr not in self . prior_loghandlers : hndlr . setLevel ( logging . ERROR ) self . logger_flag = True
2542	def set_file_chksum ( self , doc , chk_sum ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_chksum_set : self . file_chksum_set = True self . file ( doc ) . chk_sum = checksum . Algorithm ( 'SHA1' , chk_sum ) return True else : raise CardinalityError ( 'File::CheckSum' ) else : raise OrderError ( 'File::CheckSum' )
12194	def _validate_first_message ( cls , msg ) : data = cls . _unpack_message ( msg ) logger . debug ( data ) if data != cls . RTM_HANDSHAKE : raise SlackApiError ( 'Unexpected response: {!r}' . format ( data ) ) logger . info ( 'Joined real-time messaging.' )
11902	def serve_dir ( dir_path ) : print ( 'Performing first pass index file generation' ) created_files = _create_index_files ( dir_path , True ) if ( PIL_ENABLED ) : print ( 'Performing PIL-enchanced optimised index file generation in background' ) background_indexer = BackgroundIndexFileGenerator ( dir_path ) background_indexer . run ( ) _run_server ( ) _clean_up ( created_files )
10301	def count_defaultdict ( dict_of_lists : Mapping [ X , List [ Y ] ] ) -> Mapping [ X , typing . Counter [ Y ] ] : return { k : Counter ( v ) for k , v in dict_of_lists . items ( ) }
9706	def get_field_settings ( self ) : field_settings = None if self . field_settings : if isinstance ( self . field_settings , six . string_types ) : profiles = settings . CONFIG . get ( self . PROFILE_KEY , { } ) field_settings = profiles . get ( self . field_settings ) else : field_settings = self . field_settings return field_settings
6982	def _get_legendre_deg_ctd ( npts ) : from scipy . interpolate import interp1d degs = nparray ( [ 4 , 5 , 6 , 10 , 15 ] ) pts = nparray ( [ 1e2 , 3e2 , 5e2 , 1e3 , 3e3 ] ) fn = interp1d ( pts , degs , kind = 'linear' , bounds_error = False , fill_value = ( min ( degs ) , max ( degs ) ) ) legendredeg = int ( npfloor ( fn ( npts ) ) ) return legendredeg
2627	def show_summary ( self ) : self . get_instance_state ( ) status_string = "EC2 Summary:\n\tVPC IDs: {}\n\tSubnet IDs: \{}\n\tSecurity Group ID: {}\n\tRunning Instance IDs: {}\n" . format ( self . vpc_id , self . sn_ids , self . sg_id , self . instances ) status_string += "\tInstance States:\n\t\t" self . get_instance_state ( ) for state in self . instance_states . keys ( ) : status_string += "Instance ID: {} State: {}\n\t\t" . format ( state , self . instance_states [ state ] ) status_string += "\n" logger . info ( status_string ) return status_string
1901	def get_value ( self , constraints , expression ) : if not issymbolic ( expression ) : return expression assert isinstance ( expression , ( Bool , BitVec , Array ) ) with constraints as temp_cs : if isinstance ( expression , Bool ) : var = temp_cs . new_bool ( ) elif isinstance ( expression , BitVec ) : var = temp_cs . new_bitvec ( expression . size ) elif isinstance ( expression , Array ) : var = [ ] result = [ ] for i in range ( expression . index_max ) : subvar = temp_cs . new_bitvec ( expression . value_bits ) var . append ( subvar ) temp_cs . add ( subvar == simplify ( expression [ i ] ) ) self . _reset ( temp_cs ) if not self . _is_sat ( ) : raise SolverError ( 'Model is not available' ) for i in range ( expression . index_max ) : self . _send ( '(get-value (%s))' % var [ i ] . name ) ret = self . _recv ( ) assert ret . startswith ( '((' ) and ret . endswith ( '))' ) pattern , base = self . _get_value_fmt m = pattern . match ( ret ) expr , value = m . group ( 'expr' ) , m . group ( 'value' ) result . append ( int ( value , base ) ) return bytes ( result ) temp_cs . add ( var == expression ) self . _reset ( temp_cs ) if not self . _is_sat ( ) : raise SolverError ( 'Model is not available' ) self . _send ( '(get-value (%s))' % var . name ) ret = self . _recv ( ) if not ( ret . startswith ( '((' ) and ret . endswith ( '))' ) ) : raise SolverError ( 'SMTLIB error parsing response: %s' % ret ) if isinstance ( expression , Bool ) : return { 'true' : True , 'false' : False } [ ret [ 2 : - 2 ] . split ( ' ' ) [ 1 ] ] if isinstance ( expression , BitVec ) : pattern , base = self . _get_value_fmt m = pattern . match ( ret ) expr , value = m . group ( 'expr' ) , m . group ( 'value' ) return int ( value , base ) raise NotImplementedError ( "get_value only implemented for Bool and BitVec" )
7927	def reorder_srv ( records ) : records = list ( records ) records . sort ( ) ret = [ ] tmp = [ ] for rrecord in records : if not tmp or rrecord . priority == tmp [ 0 ] . priority : tmp . append ( rrecord ) continue ret += shuffle_srv ( tmp ) tmp = [ rrecord ] if tmp : ret += shuffle_srv ( tmp ) return ret
13907	def create_commands ( self , commands , parser ) : self . apply_defaults ( commands ) def create_single_command ( command ) : keys = command [ 'keys' ] del command [ 'keys' ] kwargs = { } for item in command : kwargs [ item ] = command [ item ] parser . add_argument ( * keys , ** kwargs ) if len ( commands ) > 1 : for command in commands : create_single_command ( command ) else : create_single_command ( commands [ 0 ] )
12118	def abfinfo ( self , printToo = False , returnDict = False ) : info = "\n### ABF INFO ###\n" d = { } for thingName in sorted ( dir ( self ) ) : if thingName in [ 'cm' , 'evIs' , 'colormap' , 'dataX' , 'dataY' , 'protoX' , 'protoY' ] : continue if "_" in thingName : continue thing = getattr ( self , thingName ) if type ( thing ) is list and len ( thing ) > 5 : continue thingType = str ( type ( thing ) ) . split ( "'" ) [ 1 ] if "method" in thingType or "neo." in thingType : continue if thingName in [ "header" , "MT" ] : continue info += "%s <%s> %s\n" % ( thingName , thingType , thing ) d [ thingName ] = thing if printToo : print ( ) for line in info . split ( "\n" ) : if len ( line ) < 3 : continue print ( " " , line ) print ( ) if returnDict : return d return info
3484	def _create_bound ( model , reaction , bound_type , f_replace , units = None , flux_udef = None ) : value = getattr ( reaction , bound_type ) if value == config . lower_bound : return LOWER_BOUND_ID elif value == 0 : return ZERO_BOUND_ID elif value == config . upper_bound : return UPPER_BOUND_ID elif value == - float ( "Inf" ) : return BOUND_MINUS_INF elif value == float ( "Inf" ) : return BOUND_PLUS_INF else : rid = reaction . id if f_replace and F_REACTION_REV in f_replace : rid = f_replace [ F_REACTION_REV ] ( rid ) pid = rid + "_" + bound_type _create_parameter ( model , pid = pid , value = value , sbo = SBO_FLUX_BOUND , units = units , flux_udef = flux_udef ) return pid
5628	def hook ( self , event_type = 'push' ) : def decorator ( func ) : self . _hooks [ event_type ] . append ( func ) return func return decorator
10259	def get_modifications_count ( graph : BELGraph ) -> Mapping [ str , int ] : return remove_falsy_values ( { 'Translocations' : len ( get_translocated ( graph ) ) , 'Degradations' : len ( get_degradations ( graph ) ) , 'Molecular Activities' : len ( get_activities ( graph ) ) , } )
4821	def redirect_if_blocked ( course_run_ids , user = None , ip_address = None , url = None ) : for course_run_id in course_run_ids : redirect_url = embargo_api . redirect_if_blocked ( CourseKey . from_string ( course_run_id ) , user = user , ip_address = ip_address , url = url ) if redirect_url : return redirect_url
8709	def write_lines ( self , data ) : lines = data . replace ( '\r' , '' ) . split ( '\n' ) for line in lines : self . __exchange ( line )
9977	def fix_lamdaline ( source ) : strio = io . StringIO ( source ) gen = tokenize . generate_tokens ( strio . readline ) tkns = [ ] try : for t in gen : tkns . append ( t ) except tokenize . TokenError : pass lambda_pos = [ ( t . type , t . string ) for t in tkns ] . index ( ( tokenize . NAME , "lambda" ) ) tkns = tkns [ lambda_pos : ] lastop_pos = ( len ( tkns ) - 1 - [ t . type for t in tkns [ : : - 1 ] ] . index ( tokenize . OP ) ) lastop = tkns [ lastop_pos ] fiedlineno = lastop . start [ 0 ] fixedline = lastop . line [ : lastop . start [ 1 ] ] + lastop . line [ lastop . end [ 1 ] : ] tkns = tkns [ : lastop_pos ] fixedlines = "" last_lineno = 0 for t in tkns : if last_lineno == t . start [ 0 ] : continue elif t . start [ 0 ] == fiedlineno : fixedlines += fixedline last_lineno = t . start [ 0 ] else : fixedlines += t . line last_lineno = t . start [ 0 ] return fixedlines
4645	def delete ( self , key ) : query = ( "DELETE FROM {} WHERE {}=?" . format ( self . __tablename__ , self . __key__ ) , ( key , ) , ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( * query ) connection . commit ( )
1108	def get_grouped_opcodes ( self , n = 3 ) : codes = self . get_opcodes ( ) if not codes : codes = [ ( "equal" , 0 , 1 , 0 , 1 ) ] if codes [ 0 ] [ 0 ] == 'equal' : tag , i1 , i2 , j1 , j2 = codes [ 0 ] codes [ 0 ] = tag , max ( i1 , i2 - n ) , i2 , max ( j1 , j2 - n ) , j2 if codes [ - 1 ] [ 0 ] == 'equal' : tag , i1 , i2 , j1 , j2 = codes [ - 1 ] codes [ - 1 ] = tag , i1 , min ( i2 , i1 + n ) , j1 , min ( j2 , j1 + n ) nn = n + n group = [ ] for tag , i1 , i2 , j1 , j2 in codes : if tag == 'equal' and i2 - i1 > nn : group . append ( ( tag , i1 , min ( i2 , i1 + n ) , j1 , min ( j2 , j1 + n ) ) ) yield group group = [ ] i1 , j1 = max ( i1 , i2 - n ) , max ( j1 , j2 - n ) group . append ( ( tag , i1 , i2 , j1 , j2 ) ) if group and not ( len ( group ) == 1 and group [ 0 ] [ 0 ] == 'equal' ) : yield group
2071	def col_transform ( self , col , digits ) : if col is None or float ( col ) < 0.0 : return None else : col = self . number_to_base ( int ( col ) , self . base , digits ) if len ( col ) == digits : return col else : return [ 0 for _ in range ( digits - len ( col ) ) ] + col
3142	def create ( self , data ) : if 'name' not in data : raise KeyError ( 'The file must have a name' ) if 'file_data' not in data : raise KeyError ( 'The file must have file_data' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . file_id = response [ 'id' ] else : self . file_id = None return response
5057	def build_notification_message ( template_context , template_configuration = None ) : if ( template_configuration is not None and template_configuration . html_template and template_configuration . plaintext_template ) : plain_msg , html_msg = template_configuration . render_all_templates ( template_context ) else : plain_msg = render_to_string ( 'enterprise/emails/user_notification.txt' , template_context ) html_msg = render_to_string ( 'enterprise/emails/user_notification.html' , template_context ) return plain_msg , html_msg
5325	def measure_memory ( cls , obj , seen = None ) : size = sys . getsizeof ( obj ) if seen is None : seen = set ( ) obj_id = id ( obj ) if obj_id in seen : return 0 seen . add ( obj_id ) if isinstance ( obj , dict ) : size += sum ( [ cls . measure_memory ( v , seen ) for v in obj . values ( ) ] ) size += sum ( [ cls . measure_memory ( k , seen ) for k in obj . keys ( ) ] ) elif hasattr ( obj , '__dict__' ) : size += cls . measure_memory ( obj . __dict__ , seen ) elif hasattr ( obj , '__iter__' ) and not isinstance ( obj , ( str , bytes , bytearray ) ) : size += sum ( [ cls . measure_memory ( i , seen ) for i in obj ] ) return size
4538	def single ( method ) : @ functools . wraps ( method ) def single ( self , address , value = None ) : address = urllib . parse . unquote_plus ( address ) try : error = NO_PROJECT_ERROR if not self . project : raise ValueError error = BAD_ADDRESS_ERROR ed = editor . Editor ( address , self . project ) if value is None : error = BAD_GETTER_ERROR result = method ( self , ed ) else : error = BAD_SETTER_ERROR result = method ( self , ed , value ) result = { 'value' : result } except Exception as e : traceback . print_exc ( ) msg = '%s\n%s' % ( error . format ( ** locals ( ) ) , e ) result = { 'error' : msg } return flask . jsonify ( result ) return single
7471	def build_tmp_h5 ( data , samples ) : snames = [ i . name for i in samples ] snames . sort ( ) uhandle = os . path . join ( data . dirs . across , data . name + ".utemp.sort" ) bseeds = os . path . join ( data . dirs . across , data . name + ".tmparrs.h5" ) get_seeds_and_hits ( uhandle , bseeds , snames )
2468	def set_file_license_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_license_comment_set : self . file_license_comment_set = True if validations . validate_file_lics_comment ( text ) : self . file ( doc ) . license_comment = str_from_text ( text ) else : raise SPDXValueError ( 'File::LicenseComment' ) else : raise CardinalityError ( 'File::LicenseComment' ) else : raise OrderError ( 'File::LicenseComment' )
12483	def filter_list ( lst , pattern ) : if is_fnmatch_regex ( pattern ) and not is_regex ( pattern ) : log . info ( 'Using fnmatch for {0}' . format ( pattern ) ) filst = fnmatch . filter ( lst , pattern ) else : log . info ( 'Using regex match for {0}' . format ( pattern ) ) filst = match_list ( lst , pattern ) if filst : filst . sort ( ) return filst
9618	def UnPlug ( self , force = False ) : if force : _xinput . UnPlugForce ( c_uint ( self . id ) ) else : _xinput . UnPlug ( c_uint ( self . id ) ) while self . id not in self . available_ids ( ) : if self . id == 0 : break
13696	def parse_int ( s ) : try : val = int ( s ) except ValueError : print_err ( '\nInvalid integer: {}' . format ( s ) ) sys . exit ( 1 ) return val
681	def addValuesToField ( self , i , numValues ) : assert ( len ( self . fields ) > i ) values = [ self . addValueToField ( i ) for n in range ( numValues ) ] return values
11883	def scanProcessForOpenFile ( pid , searchPortion , isExactMatch = True , ignoreCase = False ) : try : try : pid = int ( pid ) except ValueError as e : sys . stderr . write ( 'Expected an integer, got %s for pid.\n' % ( str ( type ( pid ) ) , ) ) raise e prefixDir = "/proc/%d/fd" % ( pid , ) processFDs = os . listdir ( prefixDir ) matchedFDs = [ ] matchedFilenames = [ ] if isExactMatch is True : if ignoreCase is False : isMatch = lambda searchFor , totalPath : bool ( searchFor == totalPath ) else : isMatch = lambda searchFor , totalPath : bool ( searchFor . lower ( ) == totalPath . lower ( ) ) else : if ignoreCase is False : isMatch = lambda searchFor , totalPath : bool ( searchFor in totalPath ) else : isMatch = lambda searchFor , totalPath : bool ( searchFor . lower ( ) in totalPath . lower ( ) ) for fd in processFDs : fdPath = os . readlink ( prefixDir + '/' + fd ) if isMatch ( searchPortion , fdPath ) : matchedFDs . append ( fd ) matchedFilenames . append ( fdPath ) if len ( matchedFDs ) == 0 : return None cmdline = getProcessCommandLineStr ( pid ) owner = getProcessOwnerStr ( pid ) return { 'searchPortion' : searchPortion , 'pid' : pid , 'owner' : owner , 'cmdline' : cmdline , 'fds' : matchedFDs , 'filenames' : matchedFilenames , } except OSError : return None except IOError : return None except FileNotFoundError : return None except PermissionError : return None
12134	def write_log ( log_path , data , allow_append = True ) : append = os . path . isfile ( log_path ) islist = isinstance ( data , list ) if append and not allow_append : raise Exception ( 'Appending has been disabled' ' and file %s exists' % log_path ) if not ( islist or isinstance ( data , Args ) ) : raise Exception ( 'Can only write Args objects or dictionary' ' lists to log file.' ) specs = data if islist else data . specs if not all ( isinstance ( el , dict ) for el in specs ) : raise Exception ( 'List elements must be dictionaries.' ) log_file = open ( log_path , 'r+' ) if append else open ( log_path , 'w' ) start = int ( log_file . readlines ( ) [ - 1 ] . split ( ) [ 0 ] ) + 1 if append else 0 ascending_indices = range ( start , start + len ( data ) ) log_str = '\n' . join ( [ '%d %s' % ( tid , json . dumps ( el ) ) for ( tid , el ) in zip ( ascending_indices , specs ) ] ) log_file . write ( "\n" + log_str if append else log_str ) log_file . close ( )
8321	def parse_important ( self , markup ) : important = [ ] table_titles = [ table . title for table in self . tables ] m = re . findall ( self . re [ "bold" ] , markup ) for bold in m : bold = self . plain ( bold ) if not bold in table_titles : important . append ( bold . lower ( ) ) return important
1192	def translate ( pat ) : i , n = 0 , len ( pat ) res = '' while i < n : c = pat [ i ] i = i + 1 if c == '*' : res = res + '.*' elif c == '?' : res = res + '.' elif c == '[' : j = i if j < n and pat [ j ] == '!' : j = j + 1 if j < n and pat [ j ] == ']' : j = j + 1 while j < n and pat [ j ] != ']' : j = j + 1 if j >= n : res = res + '\\[' else : stuff = pat [ i : j ] . replace ( '\\' , '\\\\' ) i = j + 1 if stuff [ 0 ] == '!' : stuff = '^' + stuff [ 1 : ] elif stuff [ 0 ] == '^' : stuff = '\\' + stuff res = '%s[%s]' % ( res , stuff ) else : res = res + re . escape ( c ) return res + '\Z(?ms)'
3423	def resettable ( f ) : def wrapper ( self , new_value ) : context = get_context ( self ) if context : old_value = getattr ( self , f . __name__ ) if old_value == new_value : return context ( partial ( f , self , old_value ) ) f ( self , new_value ) return wrapper
6711	def disk ( self ) : r = self . local_renderer r . run ( r . env . disk_usage_command )
7048	def bls_stats_singleperiod ( times , mags , errs , period , magsarefluxes = False , sigclip = 10.0 , perioddeltapercent = 10 , nphasebins = 200 , mintransitduration = 0.01 , maxtransitduration = 0.4 , ingressdurationfraction = 0.1 , verbose = True ) : stimes , smags , serrs = sigclip_magseries ( times , mags , errs , magsarefluxes = magsarefluxes , sigclip = sigclip ) if len ( stimes ) > 9 and len ( smags ) > 9 and len ( serrs ) > 9 : startp = period - perioddeltapercent * period / 100.0 if startp < 0 : startp = period endp = period + perioddeltapercent * period / 100.0 blsres = bls_serial_pfind ( stimes , smags , serrs , verbose = verbose , startp = startp , endp = endp , nphasebins = nphasebins , mintransitduration = mintransitduration , maxtransitduration = maxtransitduration , magsarefluxes = magsarefluxes , get_stats = False , sigclip = None ) if ( not blsres or 'blsresult' not in blsres or blsres [ 'blsresult' ] is None ) : LOGERROR ( "BLS failed during a period-search " "performed around the input best period: %.6f. " "Can't continue. " % period ) return None thistransdepth = blsres [ 'blsresult' ] [ 'transdepth' ] thistransduration = blsres [ 'blsresult' ] [ 'transduration' ] thisbestperiod = blsres [ 'bestperiod' ] thistransingressbin = blsres [ 'blsresult' ] [ 'transingressbin' ] thistransegressbin = blsres [ 'blsresult' ] [ 'transegressbin' ] thisnphasebins = nphasebins stats = _get_bls_stats ( stimes , smags , serrs , thistransdepth , thistransduration , ingressdurationfraction , nphasebins , thistransingressbin , thistransegressbin , thisbestperiod , thisnphasebins , magsarefluxes = magsarefluxes , verbose = verbose ) return stats else : LOGERROR ( 'no good detections for these times and mags, skipping...' ) return None
8170	def separation ( self , r = 10 ) : vx = vy = vz = 0 for b in self . boids : if b != self : if abs ( self . x - b . x ) < r : vx += ( self . x - b . x ) if abs ( self . y - b . y ) < r : vy += ( self . y - b . y ) if abs ( self . z - b . z ) < r : vz += ( self . z - b . z ) return vx , vy , vz
1718	def fix_js_args ( func ) : fcode = six . get_function_code ( func ) fargs = fcode . co_varnames [ fcode . co_argcount - 2 : fcode . co_argcount ] if fargs == ( 'this' , 'arguments' ) or fargs == ( 'arguments' , 'var' ) : return func code = append_arguments ( six . get_function_code ( func ) , ( 'this' , 'arguments' ) ) return types . FunctionType ( code , six . get_function_globals ( func ) , func . __name__ , closure = six . get_function_closure ( func ) )
938	def _getModelPickleFilePath ( saveModelDir ) : path = os . path . join ( saveModelDir , "model.pkl" ) path = os . path . abspath ( path ) return path
1377	def check_java_home_set ( ) : if "JAVA_HOME" not in os . environ : Log . error ( "JAVA_HOME not set" ) return False java_path = get_java_path ( ) if os . path . isfile ( java_path ) and os . access ( java_path , os . X_OK ) : return True Log . error ( "JAVA_HOME/bin/java either does not exist or not an executable" ) return False
12476	def ux_file_len ( filepath ) : p = subprocess . Popen ( [ 'wc' , '-l' , filepath ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) result , err = p . communicate ( ) if p . returncode != 0 : raise IOError ( err ) l = result . strip ( ) l = int ( l . split ( ) [ 0 ] ) return l
11038	def maybe_key_vault ( client , mount_path ) : d = client . read_kv2 ( 'client_key' , mount_path = mount_path ) def get_or_create_key ( client_key ) : if client_key is not None : key_data = client_key [ 'data' ] [ 'data' ] key = _load_pem_private_key_bytes ( key_data [ 'key' ] . encode ( 'utf-8' ) ) return JWKRSA ( key = key ) else : key = generate_private_key ( u'rsa' ) key_data = { 'key' : _dump_pem_private_key_bytes ( key ) . decode ( 'utf-8' ) } d = client . create_or_update_kv2 ( 'client_key' , key_data , mount_path = mount_path ) return d . addCallback ( lambda _result : JWKRSA ( key = key ) ) return d . addCallback ( get_or_create_key )
1058	def remove_extension ( module , name , code ) : key = ( module , name ) if ( _extension_registry . get ( key ) != code or _inverted_registry . get ( code ) != key ) : raise ValueError ( "key %s is not registered with code %s" % ( key , code ) ) del _extension_registry [ key ] del _inverted_registry [ code ] if code in _extension_cache : del _extension_cache [ code ]
6669	def task_or_dryrun ( * args , ** kwargs ) : invoked = bool ( not args or kwargs ) task_class = kwargs . pop ( "task_class" , WrappedCallableTask ) func , args = args [ 0 ] , ( ) def wrapper ( func ) : return task_class ( func , * args , ** kwargs ) wrapper . is_task_or_dryrun = True wrapper . wrapped = func return wrapper if invoked else wrapper ( func )
5942	def _get_gmx_docs ( self ) : if self . _doc_cache is not None : return self . _doc_cache try : logging . disable ( logging . CRITICAL ) rc , header , docs = self . run ( 'h' , stdout = PIPE , stderr = PIPE , use_input = False ) except : logging . critical ( "Invoking command {0} failed when determining its doc string. Proceed with caution" . format ( self . command_name ) ) self . _doc_cache = "(No Gromacs documentation available)" return self . _doc_cache finally : logging . disable ( logging . NOTSET ) m = re . match ( self . doc_pattern , docs , re . DOTALL ) if m is None : m = re . match ( self . doc_pattern , header , re . DOTALL ) if m is None : self . _doc_cache = "(No Gromacs documentation available)" return self . _doc_cache self . _doc_cache = m . group ( 'DOCS' ) return self . _doc_cache
12512	def niftilist_to_array ( img_filelist , outdtype = None ) : try : first_img = img_filelist [ 0 ] vol = get_img_data ( first_img ) except IndexError as ie : raise Exception ( 'Error getting the first item of img_filelis: {}' . format ( repr_imgs ( img_filelist [ 0 ] ) ) ) from ie if not outdtype : outdtype = vol . dtype outmat = np . zeros ( ( len ( img_filelist ) , np . prod ( vol . shape ) ) , dtype = outdtype ) try : for i , img_file in enumerate ( img_filelist ) : vol = get_img_data ( img_file ) outmat [ i , : ] = vol . flatten ( ) except Exception as exc : raise Exception ( 'Error on reading file {0}.' . format ( img_file ) ) from exc return outmat , vol . shape
7427	def refmap_init ( data , sample , force ) : sample . files . unmapped_reads = os . path . join ( data . dirs . edits , "{}-refmap_derep.fastq" . format ( sample . name ) ) sample . files . mapped_reads = os . path . join ( data . dirs . refmapping , "{}-mapped-sorted.bam" . format ( sample . name ) )
11506	def create_item ( self , token , name , parent_id , ** kwargs ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'name' ] = name parameters [ 'parentid' ] = parent_id optional_keys = [ 'description' , 'uuid' , 'privacy' ] for key in optional_keys : if key in kwargs : parameters [ key ] = kwargs [ key ] response = self . request ( 'midas.item.create' , parameters ) return response
2151	def list ( self , all_pages = False , ** kwargs ) : self . _separate ( kwargs ) return super ( Resource , self ) . list ( all_pages = all_pages , ** kwargs )
1721	def limited ( func ) : def f ( standard = False , ** args ) : insert_pos = len ( inline_stack . names ) res = func ( ** args ) if len ( res ) > LINE_LEN_LIMIT : name = inline_stack . require ( 'LONG' ) inline_stack . names . pop ( ) inline_stack . names . insert ( insert_pos , name ) res = 'def %s(var=var):\n return %s\n' % ( name , res ) inline_stack . define ( name , res ) return name + '()' else : return res f . __dict__ [ 'standard' ] = func return f
8320	def parse_categories ( self , markup ) : categories = [ ] m = re . findall ( self . re [ "category" ] , markup ) for category in m : category = category . split ( "|" ) page = category [ 0 ] . strip ( ) display = u"" if len ( category ) > 1 : display = category [ 1 ] . strip ( ) if not page in categories : categories . append ( page ) return categories
5529	def _get_zoom_level ( zoom , process ) : if zoom is None : return reversed ( process . config . zoom_levels ) if isinstance ( zoom , int ) : return [ zoom ] elif len ( zoom ) == 2 : return reversed ( range ( min ( zoom ) , max ( zoom ) + 1 ) ) elif len ( zoom ) == 1 : return zoom
5956	def load_v4_tools ( ) : logger . debug ( "Loading v4 tools..." ) names = config . get_tool_names ( ) if len ( names ) == 0 and 'GMXBIN' in os . environ : names = find_executables ( os . environ [ 'GMXBIN' ] ) if len ( names ) == 0 or len ( names ) > len ( V4TOOLS ) * 4 : names = list ( V4TOOLS ) names . extend ( config . get_extra_tool_names ( ) ) tools = { } for name in names : fancy = make_valid_identifier ( name ) tools [ fancy ] = tool_factory ( fancy , name , None ) if not tools : errmsg = "Failed to load v4 tools" logger . debug ( errmsg ) raise GromacsToolLoadingError ( errmsg ) logger . debug ( "Loaded {0} v4 tools successfully!" . format ( len ( tools ) ) ) return tools
5223	def ccy_pair ( local , base = 'USD' ) -> CurrencyPair : ccy_param = param . load_info ( cat = 'ccy' ) if f'{local}{base}' in ccy_param : info = ccy_param [ f'{local}{base}' ] elif f'{base}{local}' in ccy_param : info = ccy_param [ f'{base}{local}' ] info [ 'factor' ] = 1. / info . get ( 'factor' , 1. ) info [ 'power' ] = - info . get ( 'power' , 1 ) elif base . lower ( ) == local . lower ( ) : info = dict ( ticker = '' ) info [ 'factor' ] = 1. if base [ - 1 ] . lower ( ) == base [ - 1 ] : info [ 'factor' ] /= 100. if local [ - 1 ] . lower ( ) == local [ - 1 ] : info [ 'factor' ] *= 100. else : logger = logs . get_logger ( ccy_pair ) logger . error ( f'incorrect currency - local {local} / base {base}' ) return CurrencyPair ( ticker = '' , factor = 1. , power = 1 ) if 'factor' not in info : info [ 'factor' ] = 1. if 'power' not in info : info [ 'power' ] = 1 return CurrencyPair ( ** info )
2090	def _disassoc ( self , url_fragment , me , other ) : url = self . endpoint + '%d/%s/' % ( me , url_fragment ) r = client . get ( url , params = { 'id' : other } ) . json ( ) if r [ 'count' ] == 0 : return { 'changed' : False } r = client . post ( url , data = { 'disassociate' : True , 'id' : other } ) return { 'changed' : True }
3667	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : Cpgms = [ i ( T ) for i in self . HeatCapacityGases ] return mixing_simple ( zs , Cpgms ) else : raise Exception ( 'Method not valid' )
6249	def get_effect_class ( self , effect_name : str , package_name : str = None ) -> Type [ 'Effect' ] : return self . _project . get_effect_class ( effect_name , package_name = package_name )
12044	def where_cross ( data , threshold ) : Is = np . where ( data > threshold ) [ 0 ] Is = np . concatenate ( ( [ 0 ] , Is ) ) Ds = Is [ : - 1 ] - Is [ 1 : ] + 1 return Is [ np . where ( Ds ) [ 0 ] + 1 ]
11454	def from_source ( cls , source ) : bibrecs = BibRecordPackage ( source ) bibrecs . parse ( ) for bibrec in bibrecs . get_records ( ) : yield cls ( bibrec )
10571	def template_to_filepath ( template , metadata , template_patterns = None ) : if template_patterns is None : template_patterns = TEMPLATE_PATTERNS metadata = metadata if isinstance ( metadata , dict ) else _mutagen_fields_to_single_value ( metadata ) assert isinstance ( metadata , dict ) suggested_filename = get_suggested_filename ( metadata ) . replace ( '.mp3' , '' ) if template == os . getcwd ( ) or template == '%suggested%' : filepath = suggested_filename else : t = template . replace ( '%suggested%' , suggested_filename ) filepath = _replace_template_patterns ( t , metadata , template_patterns ) return filepath
2330	def computeGaussKernel ( x ) : xnorm = np . power ( euclidean_distances ( x , x ) , 2 ) return np . exp ( - xnorm / ( 2.0 ) )
8511	def load ( self ) : from pylearn2 . config import yaml_parse from pylearn2 . datasets import Dataset dataset = yaml_parse . load ( self . yaml_string ) assert isinstance ( dataset , Dataset ) data = dataset . iterator ( mode = 'sequential' , num_batches = 1 , data_specs = dataset . data_specs , return_tuple = True ) . next ( ) if len ( data ) == 2 : X , y = data y = np . squeeze ( y ) if self . one_hot : y = np . argmax ( y , axis = 1 ) else : X = data y = None return X , y
11122	def add_directory ( self , relativePath , info = None ) : path = os . path . normpath ( relativePath ) currentDir = self . path currentDict = self if path in ( "" , "." ) : return currentDict save = False for dir in path . split ( os . sep ) : dirPath = os . path . join ( currentDir , dir ) if not os . path . exists ( dirPath ) : os . mkdir ( dirPath ) currentDict = dict . __getitem__ ( currentDict , "directories" ) if currentDict . get ( dir , None ) is None : save = True currentDict [ dir ] = { "directories" : { } , "files" : { } , "timestamp" : datetime . utcnow ( ) , "id" : str ( uuid . uuid1 ( ) ) , "info" : info } currentDict = currentDict [ dir ] currentDir = dirPath if save : self . save ( ) return currentDict
9935	def find ( self , path , all = False ) : matches = [ ] for prefix , root in self . locations : if root not in searched_locations : searched_locations . append ( root ) matched_path = self . find_location ( root , path , prefix ) if matched_path : if not all : return matched_path matches . append ( matched_path ) return matches
7287	def set_form_fields ( self , form_field_dict , parent_key = None , field_type = None ) : for form_key , field_value in form_field_dict . items ( ) : form_key = make_key ( parent_key , form_key ) if parent_key is not None else form_key if isinstance ( field_value , tuple ) : set_list_class = False base_key = form_key if ListField in ( field_value . field_type , field_type ) : if parent_key is None or ListField == field_value . field_type : if field_type != EmbeddedDocumentField : field_value . widget . attrs [ 'class' ] += ' listField {0}' . format ( form_key ) set_list_class = True else : field_value . widget . attrs [ 'class' ] += ' listField' list_keys = [ field_key for field_key in self . form . fields . keys ( ) if has_digit ( field_key ) ] key_int = 0 while form_key in list_keys : key_int += 1 form_key = make_key ( form_key , key_int ) if parent_key is not None : valid_base_keys = [ model_key for model_key in self . model_map_dict . keys ( ) if not model_key . startswith ( "_" ) ] while base_key not in valid_base_keys and base_key : base_key = make_key ( base_key , exclude_last_string = True ) embedded_key_class = None if set_list_class : field_value . widget . attrs [ 'class' ] += " listField" . format ( base_key ) embedded_key_class = make_key ( field_key , exclude_last_string = True ) field_value . widget . attrs [ 'class' ] += " embeddedField" if base_key == parent_key : field_value . widget . attrs [ 'class' ] += ' {0}' . format ( base_key ) else : field_value . widget . attrs [ 'class' ] += ' {0} {1}' . format ( base_key , parent_key ) if embedded_key_class is not None : field_value . widget . attrs [ 'class' ] += ' {0}' . format ( embedded_key_class ) default_value = self . get_field_value ( form_key ) if isinstance ( default_value , list ) and len ( default_value ) > 0 : key_index = int ( form_key . split ( "_" ) [ - 1 ] ) new_base_key = make_key ( form_key , exclude_last_string = True ) for list_value in default_value : list_widget = deepcopy ( field_value . widget ) new_key = make_key ( new_base_key , six . text_type ( key_index ) ) list_widget . attrs [ 'class' ] += " {0}" . format ( make_key ( base_key , key_index ) ) self . set_form_field ( list_widget , field_value . document_field , new_key , list_value ) key_index += 1 else : self . set_form_field ( field_value . widget , field_value . document_field , form_key , default_value ) elif isinstance ( field_value , dict ) : self . set_form_fields ( field_value , form_key , field_value . get ( "_field_type" , None ) )
11402	def create_records ( marcxml , verbose = CFG_BIBRECORD_DEFAULT_VERBOSE_LEVEL , correct = CFG_BIBRECORD_DEFAULT_CORRECT , parser = '' , keep_singletons = CFG_BIBRECORD_KEEP_SINGLETONS ) : regex = re . compile ( '<record.*?>.*?</record>' , re . DOTALL ) record_xmls = regex . findall ( marcxml ) return [ create_record ( record_xml , verbose = verbose , correct = correct , parser = parser , keep_singletons = keep_singletons ) for record_xml in record_xmls ]
4837	def get_catalog_courses ( self , catalog_id ) : return self . _load_data ( self . CATALOGS_COURSES_ENDPOINT . format ( catalog_id ) , default = [ ] )
358	def load_npy_to_any ( path = '' , name = 'file.npy' ) : file_path = os . path . join ( path , name ) try : return np . load ( file_path ) . item ( ) except Exception : return np . load ( file_path ) raise Exception ( "[!] Fail to load %s" % file_path )
1673	def PrintCategories ( ) : sys . stderr . write ( '' . join ( ' %s\n' % cat for cat in _ERROR_CATEGORIES ) ) sys . exit ( 0 )
13888	def DeleteDirectory ( directory , skip_on_error = False ) : _AssertIsLocal ( directory ) import shutil def OnError ( fn , path , excinfo ) : if IsLink ( path ) : return if fn is os . remove and os . access ( path , os . W_OK ) : raise import stat os . chmod ( path , stat . S_IWRITE ) fn ( path ) try : if not os . path . isdir ( directory ) : if skip_on_error : return from . _exceptions import DirectoryNotFoundError raise DirectoryNotFoundError ( directory ) shutil . rmtree ( directory , onerror = OnError ) except : if not skip_on_error : raise
13531	def ancestors ( self ) : ancestors = set ( [ ] ) self . _depth_ascend ( self , ancestors ) try : ancestors . remove ( self ) except KeyError : pass return list ( ancestors )
8710	def __write_chunk ( self , chunk ) : log . debug ( 'writing %d bytes chunk' , len ( chunk ) ) data = BLOCK_START + chr ( len ( chunk ) ) + chunk if len ( chunk ) < 128 : padding = 128 - len ( chunk ) log . debug ( 'pad with %d characters' , padding ) data = data + ( ' ' * padding ) log . debug ( "packet size %d" , len ( data ) ) self . __write ( data ) self . _port . flush ( ) return self . __got_ack ( )
2640	def runner ( incoming_q , outgoing_q ) : logger . debug ( "[RUNNER] Starting" ) def execute_task ( bufs ) : user_ns = locals ( ) user_ns . update ( { '__builtins__' : __builtins__ } ) f , args , kwargs = unpack_apply_message ( bufs , user_ns , copy = False ) fname = getattr ( f , '__name__' , 'f' ) prefix = "parsl_" fname = prefix + "f" argname = prefix + "args" kwargname = prefix + "kwargs" resultname = prefix + "result" user_ns . update ( { fname : f , argname : args , kwargname : kwargs , resultname : resultname } ) code = "{0} = {1}(*{2}, **{3})" . format ( resultname , fname , argname , kwargname ) try : logger . debug ( "[RUNNER] Executing: {0}" . format ( code ) ) exec ( code , user_ns , user_ns ) except Exception as e : logger . warning ( "Caught exception; will raise it: {}" . format ( e ) ) raise e else : logger . debug ( "[RUNNER] Result: {0}" . format ( user_ns . get ( resultname ) ) ) return user_ns . get ( resultname ) while True : try : msg = incoming_q . get ( block = True , timeout = 10 ) except queue . Empty : logger . debug ( "[RUNNER] Queue is empty" ) except IOError as e : logger . debug ( "[RUNNER] Broken pipe: {}" . format ( e ) ) try : outgoing_q . put ( None ) except Exception : pass break except Exception as e : logger . debug ( "[RUNNER] Caught unknown exception: {}" . format ( e ) ) else : if not msg : logger . debug ( "[RUNNER] Received exit request" ) outgoing_q . put ( None ) break else : logger . debug ( "[RUNNER] Got a valid task with ID {}" . format ( msg [ "task_id" ] ) ) try : response_obj = execute_task ( msg [ 'buffer' ] ) response = { "task_id" : msg [ "task_id" ] , "result" : serialize_object ( response_obj ) } logger . debug ( "[RUNNER] Returing result: {}" . format ( deserialize_object ( response [ "result" ] ) ) ) except Exception as e : logger . debug ( "[RUNNER] Caught task exception: {}" . format ( e ) ) response = { "task_id" : msg [ "task_id" ] , "exception" : serialize_object ( e ) } outgoing_q . put ( response ) logger . debug ( "[RUNNER] Terminating" )
7985	def submit_registration_form ( self , form ) : self . lock . acquire ( ) try : if form and form . type != "cancel" : self . registration_form = form iq = Iq ( stanza_type = "set" ) iq . set_content ( self . __register . submit_form ( form ) ) self . set_response_handlers ( iq , self . registration_success , self . registration_error ) self . send ( iq ) else : self . __register = None finally : self . lock . release ( )
6285	def toggle_pause ( self ) : self . controller . playing = not self . controller . playing self . music . toggle_pause ( )
12400	def add ( self , requirements , required = None ) : if isinstance ( requirements , RequirementsManager ) : requirements = list ( requirements ) elif not isinstance ( requirements , list ) : requirements = [ requirements ] for req in requirements : name = req . project_name if not isinstance ( req , BumpRequirement ) : req = BumpRequirement ( req , required = required ) elif required is not None : req . required = required add = True if name in self . requirements : for existing_req in self . requirements [ name ] : if req == existing_req : add = False break replace = False if ( req . specs and req . specs [ 0 ] [ 0 ] == '==' and existing_req . specs and existing_req . specs [ 0 ] [ 0 ] == '==' ) : if pkg_resources . parse_version ( req . specs [ 0 ] [ 1 ] ) < pkg_resources . parse_version ( existing_req . specs [ 0 ] [ 1 ] ) : req . requirement = existing_req . requirement replace = True if not ( req . specs and existing_req . specs ) : if existing_req . specs : req . requirement = existing_req . requirement replace = True if replace : req . required |= existing_req . required if existing_req . required_by and not req . required_by : req . required_by = existing_req . required_by self . requirements [ name ] . remove ( existing_req ) break if add : self . requirements [ name ] . append ( req )
10634	def get_compound_afr ( self , compound ) : index = self . material . get_compound_index ( compound ) return stoich . amount ( compound , self . _compound_mfrs [ index ] )
6653	def start ( self , builddir , program , forward_args ) : child = None try : prog_path = self . findProgram ( builddir , program ) if prog_path is None : return start_env , start_vars = self . buildProgEnvAndVars ( prog_path , builddir ) if self . getScript ( 'start' ) : cmd = [ os . path . expandvars ( string . Template ( x ) . safe_substitute ( ** start_vars ) ) for x in self . getScript ( 'start' ) ] + forward_args else : cmd = shlex . split ( './' + prog_path ) + forward_args logger . debug ( 'starting program: %s' , cmd ) child = subprocess . Popen ( cmd , cwd = builddir , env = start_env ) child . wait ( ) if child . returncode : return "process exited with status %s" % child . returncode child = None except OSError as e : import errno if e . errno == errno . ENOEXEC : return ( "the program %s cannot be run (perhaps your target " + "needs to define a 'start' script to start it on its " "intended execution target?)" ) % prog_path finally : if child is not None : _tryTerminate ( child )
7148	def with_payment_id ( self , payment_id = 0 ) : payment_id = numbers . PaymentID ( payment_id ) if not payment_id . is_short ( ) : raise TypeError ( "Payment ID {0} has more than 64 bits and cannot be integrated" . format ( payment_id ) ) prefix = 54 if self . is_testnet ( ) else 25 if self . is_stagenet ( ) else 19 data = bytearray ( [ prefix ] ) + self . _decoded [ 1 : 65 ] + struct . pack ( '>Q' , int ( payment_id ) ) checksum = bytearray ( keccak_256 ( data ) . digest ( ) [ : 4 ] ) return IntegratedAddress ( base58 . encode ( hexlify ( data + checksum ) ) )
13676	def add_prepare_handler ( self , prepare_handlers ) : if not isinstance ( prepare_handlers , static_bundle . BUNDLE_ITERABLE_TYPES ) : prepare_handlers = [ prepare_handlers ] if self . prepare_handlers_chain is None : self . prepare_handlers_chain = [ ] for handler in prepare_handlers : self . prepare_handlers_chain . append ( handler )
9383	def parse ( self ) : file_status = True for infile in self . infile_list : file_status = file_status and naarad . utils . is_valid_file ( infile ) if not file_status : return False status = self . parse_xml_jtl ( self . aggregation_granularity ) gc . collect ( ) return status
6090	def transform_grid ( func ) : @ wraps ( func ) def wrapper ( profile , grid , * args , ** kwargs ) : if not isinstance ( grid , TransformedGrid ) : return func ( profile , profile . transform_grid_to_reference_frame ( grid ) , * args , ** kwargs ) else : return func ( profile , grid , * args , ** kwargs ) return wrapper
10828	def delete ( cls , group , user ) : with db . session . begin_nested ( ) : cls . query . filter_by ( group = group , user_id = user . get_id ( ) ) . delete ( )
10108	def normalize_name ( s ) : s = s . replace ( '-' , '_' ) . replace ( '.' , '_' ) . replace ( ' ' , '_' ) if s in keyword . kwlist : return s + '_' s = '_' . join ( slug ( ss , lowercase = False ) for ss in s . split ( '_' ) ) if not s : s = '_' if s [ 0 ] not in string . ascii_letters + '_' : s = '_' + s return s
9505	def intersects ( self , i ) : return self . start <= i . end and i . start <= self . end
3451	def find_blocked_reactions ( model , reaction_list = None , zero_cutoff = None , open_exchanges = False , processes = None ) : zero_cutoff = normalize_cutoff ( model , zero_cutoff ) with model : if open_exchanges : for reaction in model . exchanges : reaction . bounds = ( min ( reaction . lower_bound , - 1000 ) , max ( reaction . upper_bound , 1000 ) ) if reaction_list is None : reaction_list = model . reactions model . slim_optimize ( ) solution = get_solution ( model , reactions = reaction_list ) reaction_list = solution . fluxes [ solution . fluxes . abs ( ) < zero_cutoff ] . index . tolist ( ) flux_span = flux_variability_analysis ( model , fraction_of_optimum = 0. , reaction_list = reaction_list , processes = processes ) return flux_span [ flux_span . abs ( ) . max ( axis = 1 ) < zero_cutoff ] . index . tolist ( )
12054	def inspectABF ( abf = exampleABF , saveToo = False , justPlot = False ) : pylab . close ( 'all' ) print ( " ~~ inspectABF()" ) if type ( abf ) is str : abf = swhlab . ABF ( abf ) swhlab . plot . new ( abf , forceNewFigure = True ) if abf . sweepInterval * abf . sweeps < 60 * 5 : pylab . subplot ( 211 ) pylab . title ( "%s [%s]" % ( abf . ID , abf . protoComment ) ) swhlab . plot . sweep ( abf , 'all' ) pylab . subplot ( 212 ) swhlab . plot . sweep ( abf , 'all' , continuous = True ) swhlab . plot . comments ( abf ) else : print ( " -- plotting as long recording" ) swhlab . plot . sweep ( abf , 'all' , continuous = True , minutes = True ) swhlab . plot . comments ( abf , minutes = True ) pylab . title ( "%s [%s]" % ( abf . ID , abf . protoComment ) ) swhlab . plot . annotate ( abf ) if justPlot : return if saveToo : path = os . path . split ( abf . fname ) [ 0 ] basename = os . path . basename ( abf . fname ) pylab . savefig ( os . path . join ( path , "_" + basename . replace ( ".abf" , ".png" ) ) ) pylab . show ( ) return
12634	def transform ( self ) : if self . dcmf1 is None or self . dcmf2 is None : return np . inf for field_name in self . field_weights : if ( str ( getattr ( self . dcmf1 , field_name , '' ) ) != str ( getattr ( self . dcmf2 , field_name , '' ) ) ) : return False return True
252	def _groupby_consecutive ( txn , max_delta = pd . Timedelta ( '8h' ) ) : def vwap ( transaction ) : if transaction . amount . sum ( ) == 0 : warnings . warn ( 'Zero transacted shares, setting vwap to nan.' ) return np . nan return ( transaction . amount * transaction . price ) . sum ( ) / transaction . amount . sum ( ) out = [ ] for sym , t in txn . groupby ( 'symbol' ) : t = t . sort_index ( ) t . index . name = 'dt' t = t . reset_index ( ) t [ 'order_sign' ] = t . amount > 0 t [ 'block_dir' ] = ( t . order_sign . shift ( 1 ) != t . order_sign ) . astype ( int ) . cumsum ( ) t [ 'block_time' ] = ( ( t . dt . sub ( t . dt . shift ( 1 ) ) ) > max_delta ) . astype ( int ) . cumsum ( ) grouped_price = ( t . groupby ( ( 'block_dir' , 'block_time' ) ) . apply ( vwap ) ) grouped_price . name = 'price' grouped_rest = t . groupby ( ( 'block_dir' , 'block_time' ) ) . agg ( { 'amount' : 'sum' , 'symbol' : 'first' , 'dt' : 'first' } ) grouped = grouped_rest . join ( grouped_price ) out . append ( grouped ) out = pd . concat ( out ) out = out . set_index ( 'dt' ) return out
9963	def get_impls ( interfaces ) : if interfaces is None : return None elif isinstance ( interfaces , Mapping ) : return { name : interfaces [ name ] . _impl for name in interfaces } elif isinstance ( interfaces , Sequence ) : return [ interfaces . _impl for interfaces in interfaces ] else : return interfaces . _impl
8083	def transform ( self , mode = None ) : if mode : self . _canvas . mode = mode return self . _canvas . mode
6542	def on_tool_finish ( self , tool ) : with self . _lock : if tool in self . current_tools : self . current_tools . remove ( tool ) self . completed_tools . append ( tool )
1401	def extract_packing_plan ( self , topology ) : packingPlan = { "id" : "" , "container_plans" : [ ] } if not topology . packing_plan : return packingPlan container_plans = topology . packing_plan . container_plans containers = [ ] for container_plan in container_plans : instances = [ ] for instance_plan in container_plan . instance_plans : instance_resources = { "cpu" : instance_plan . resource . cpu , "ram" : instance_plan . resource . ram , "disk" : instance_plan . resource . disk } instance = { "component_name" : instance_plan . component_name , "task_id" : instance_plan . task_id , "component_index" : instance_plan . component_index , "instance_resources" : instance_resources } instances . append ( instance ) required_resource = { "cpu" : container_plan . requiredResource . cpu , "ram" : container_plan . requiredResource . ram , "disk" : container_plan . requiredResource . disk } scheduled_resource = { } if container_plan . scheduledResource : scheduled_resource = { "cpu" : container_plan . scheduledResource . cpu , "ram" : container_plan . scheduledResource . ram , "disk" : container_plan . scheduledResource . disk } container = { "id" : container_plan . id , "instances" : instances , "required_resources" : required_resource , "scheduled_resources" : scheduled_resource } containers . append ( container ) packingPlan [ "id" ] = topology . packing_plan . id packingPlan [ "container_plans" ] = containers return json . dumps ( packingPlan )
12946	def copy ( self , copyPrimaryKey = False , copyValues = False ) : cpy = self . __class__ ( ** self . asDict ( copyPrimaryKey , forStorage = False ) ) if copyValues is True : for fieldName in cpy . FIELDS : setattr ( cpy , fieldName , copy . deepcopy ( getattr ( cpy , fieldName ) ) ) return cpy
9069	def _df ( self ) : if not self . _restricted : return self . nsamples return self . nsamples - self . _X [ "tX" ] . shape [ 1 ]
4602	def merge ( * projects ) : result = { } for project in projects : for name , section in ( project or { } ) . items ( ) : if name not in PROJECT_SECTIONS : raise ValueError ( UNKNOWN_SECTION_ERROR % name ) if section is None : result [ name ] = type ( result [ name ] ) ( ) continue if name in NOT_MERGEABLE + SPECIAL_CASE : result [ name ] = section continue if section and not isinstance ( section , ( dict , str ) ) : cname = section . __class__ . __name__ raise ValueError ( SECTION_ISNT_DICT_ERROR % ( name , cname ) ) if name == 'animation' : adesc = load . load_if_filename ( section ) if adesc : section = adesc . get ( 'animation' , { } ) section [ 'run' ] = adesc . get ( 'run' , { } ) result_section = result . setdefault ( name , { } ) section = construct . to_type ( section ) for k , v in section . items ( ) : if v is None : result_section . pop ( k , None ) else : result_section [ k ] = v return result
2859	def transfer ( self , data ) : if ( len ( data ) > 65536 ) : print ( 'the FTDI chip is limited to 65536 bytes (64 KB) of input/output per command!' ) print ( 'use for loops for larger reads' ) exit ( 1 ) command = 0x30 | ( self . lsbfirst << 3 ) | ( self . read_clock_ve << 2 ) | self . write_clock_ve logger . debug ( 'SPI transfer with command {0:2X}.' . format ( command ) ) data1 = data [ : len ( data ) / 2 ] data2 = data [ len ( data ) / 2 : ] len_low1 = ( len ( data1 ) - 1 ) & 0xFF len_high1 = ( ( len ( data1 ) - 1 ) >> 8 ) & 0xFF len_low2 = ( len ( data2 ) - 1 ) & 0xFF len_high2 = ( ( len ( data2 ) - 1 ) >> 8 ) & 0xFF payload1 = '' payload2 = '' self . _assert_cs ( ) if len ( data1 ) > 0 : self . _ft232h . _write ( str ( bytearray ( ( command , len_low1 , len_high1 ) ) ) ) self . _ft232h . _write ( str ( bytearray ( data1 ) ) ) payload1 = self . _ft232h . _poll_read ( len ( data1 ) ) if len ( data2 ) > 0 : self . _ft232h . _write ( str ( bytearray ( ( command , len_low2 , len_high2 ) ) ) ) self . _ft232h . _write ( str ( bytearray ( data2 ) ) ) payload2 = self . _ft232h . _poll_read ( len ( data2 ) ) self . _deassert_cs ( ) return bytearray ( payload1 + payload2 )
7731	def make_join_request ( self , password = None , history_maxchars = None , history_maxstanzas = None , history_seconds = None , history_since = None ) : self . clear_muc_child ( ) self . muc_child = MucX ( parent = self . xmlnode ) if ( history_maxchars is not None or history_maxstanzas is not None or history_seconds is not None or history_since is not None ) : history = HistoryParameters ( history_maxchars , history_maxstanzas , history_seconds , history_since ) self . muc_child . set_history ( history ) if password is not None : self . muc_child . set_password ( password )
13105	def cmpToDataStore_uri ( base , ds1 , ds2 ) : ret = difflib . get_close_matches ( base . uri , [ ds1 . uri , ds2 . uri ] , 1 , cutoff = 0.5 ) if len ( ret ) <= 0 : return 0 if ret [ 0 ] == ds1 . uri : return - 1 return 1
7722	def set_password ( self , password ) : for child in xml_element_iter ( self . xmlnode . children ) : if get_node_ns_uri ( child ) == MUC_NS and child . name == "password" : child . unlinkNode ( ) child . freeNode ( ) break if password is not None : self . xmlnode . newTextChild ( self . xmlnode . ns ( ) , "password" , to_utf8 ( password ) )
12047	def msgDict ( d , matching = None , sep1 = "=" , sep2 = "\n" , sort = True , cantEndWith = None ) : msg = "" if "record" in str ( type ( d ) ) : keys = d . dtype . names else : keys = d . keys ( ) if sort : keys = sorted ( keys ) for key in keys : if key [ 0 ] == "_" : continue if matching : if not key in matching : continue if cantEndWith and key [ - len ( cantEndWith ) ] == cantEndWith : continue if 'float' in str ( type ( d [ key ] ) ) : s = "%.02f" % d [ key ] else : s = str ( d [ key ] ) if "object" in s : s = '<object>' msg += key + sep1 + s + sep2 return msg . strip ( )
3207	def delete ( self , batch_id ) : self . batch_id = batch_id self . operation_status = None return self . _mc_client . _delete ( url = self . _build_path ( batch_id ) )
1163	def wait ( self , timeout = None ) : with self . __cond : if not self . __flag : self . __cond . wait ( timeout ) return self . __flag
7883	def _make_prefixed ( self , name , is_element , declared_prefixes , declarations ) : namespace , name = self . _split_qname ( name , is_element ) if namespace is None : prefix = None elif namespace in declared_prefixes : prefix = declared_prefixes [ namespace ] elif namespace in self . _prefixes : prefix = self . _prefixes [ namespace ] declarations [ namespace ] = prefix declared_prefixes [ namespace ] = prefix else : if is_element : prefix = None else : prefix = self . _make_prefix ( declared_prefixes ) declarations [ namespace ] = prefix declared_prefixes [ namespace ] = prefix if prefix : return prefix + u":" + name else : return name
8488	def load ( self , prefix = None , depth = None ) : prefix = prefix or self . prefix prefix = '/' + prefix . strip ( '/' ) + '/' if depth is None : depth = self . inherit_depth if not self . configured : log . debug ( "etcd not available" ) return if self . watching : log . info ( "Starting watcher for %r" , prefix ) self . start_watching ( ) log . info ( "Loading from etcd %r" , prefix ) try : result = self . client . get ( prefix ) except self . module . EtcdKeyNotFound : result = None if not result : log . info ( "No configuration found" ) return { } update = { } for item in result . children : key = item . key value = item . value try : value = pytool . json . from_json ( value ) except : pass if not self . case_sensitive : key = key . lower ( ) if key . startswith ( prefix ) : key = key [ len ( prefix ) : ] update [ key ] = value inherited = Config ( ) . settings . get ( self . inherit_key , update . get ( self . inherit_key , None ) ) if depth > 0 and inherited : log . info ( " ... inheriting ..." ) inherited = self . load ( inherited , depth - 1 ) or { } inherited . update ( update ) update = inherited return update
1202	def execute ( self , action ) : adjusted_action = list ( ) for action_spec in self . level . action_spec ( ) : if action_spec [ 'min' ] == - 1 and action_spec [ 'max' ] == 1 : adjusted_action . append ( action [ action_spec [ 'name' ] ] - 1 ) else : adjusted_action . append ( action [ action_spec [ 'name' ] ] ) action = np . array ( adjusted_action , dtype = np . intc ) reward = self . level . step ( action = action , num_steps = self . repeat_action ) state = self . level . observations ( ) [ 'RGB_INTERLACED' ] terminal = not self . level . is_running ( ) return state , terminal , reward
1618	def IsCppString ( line ) : line = line . replace ( r'\\' , 'XX' ) return ( ( line . count ( '"' ) - line . count ( r'\"' ) - line . count ( "'\"'" ) ) & 1 ) == 1
784	def jobCancelAllRunningJobs ( self ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET cancel=TRUE WHERE status<>%%s ' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . STATUS_COMPLETED ] ) return
3817	async def _pb_request ( self , endpoint , request_pb , response_pb ) : logger . debug ( 'Sending Protocol Buffer request %s:\n%s' , endpoint , request_pb ) res = await self . _base_request ( 'https://clients6.google.com/chat/v1/{}' . format ( endpoint ) , 'application/x-protobuf' , 'proto' , request_pb . SerializeToString ( ) ) try : response_pb . ParseFromString ( base64 . b64decode ( res . body ) ) except binascii . Error as e : raise exceptions . NetworkError ( 'Failed to decode base64 response: {}' . format ( e ) ) except google . protobuf . message . DecodeError as e : raise exceptions . NetworkError ( 'Failed to decode Protocol Buffer response: {}' . format ( e ) ) logger . debug ( 'Received Protocol Buffer response:\n%s' , response_pb ) status = response_pb . response_header . status if status != hangouts_pb2 . RESPONSE_STATUS_OK : description = response_pb . response_header . error_description raise exceptions . NetworkError ( 'Request failed with status {}: \'{}\'' . format ( status , description ) )
5532	def batch_process ( self , zoom = None , tile = None , multi = cpu_count ( ) , max_chunksize = 1 ) : list ( self . batch_processor ( zoom , tile , multi , max_chunksize ) )
12098	def show ( self , args , file_handle = None , ** kwargs ) : "Write to file_handle if supplied, othewise print output" full_string = '' info = { 'root_directory' : '<root_directory>' , 'batch_name' : '<batch_name>' , 'batch_tag' : '<batch_tag>' , 'batch_description' : '<batch_description>' , 'launcher' : '<launcher>' , 'timestamp_format' : '<timestamp_format>' , 'timestamp' : tuple ( time . localtime ( ) ) , 'varying_keys' : args . varying_keys , 'constant_keys' : args . constant_keys , 'constant_items' : args . constant_items } quoted_cmds = [ subprocess . list2cmdline ( [ el for el in self ( self . _formatter ( s ) , '<tid>' , info ) ] ) for s in args . specs ] cmd_lines = [ '%d: %s\n' % ( i , qcmds ) for ( i , qcmds ) in enumerate ( quoted_cmds ) ] full_string += '' . join ( cmd_lines ) if file_handle : file_handle . write ( full_string ) file_handle . flush ( ) else : print ( full_string )
10343	def overlay_type_data ( graph : BELGraph , data : Mapping [ str , float ] , func : str , namespace : str , label : Optional [ str ] = None , overwrite : bool = False , impute : Optional [ float ] = None , ) -> None : new_data = { node : data . get ( node [ NAME ] , impute ) for node in filter_nodes ( graph , function_namespace_inclusion_builder ( func , namespace ) ) } overlay_data ( graph , new_data , label = label , overwrite = overwrite )
6732	def add_class_methods_as_module_level_functions_for_fabric ( instance , module_name , method_name , module_alias = None ) : import imp from . decorators import task_or_dryrun module_obj = sys . modules [ module_name ] module_alias = re . sub ( '[^a-zA-Z0-9]+' , '' , module_alias or '' ) method_obj = getattr ( instance , method_name ) if not method_name . startswith ( '_' ) : func = getattr ( instance , method_name ) if not hasattr ( func , 'is_task_or_dryrun' ) : func = task_or_dryrun ( func ) if module_name == module_alias or ( module_name . startswith ( 'satchels.' ) and module_name . endswith ( module_alias ) ) : setattr ( module_obj , method_name , func ) else : _module_obj = module_obj module_obj = create_module ( module_alias ) setattr ( module_obj , method_name , func ) post_import_modules . add ( module_alias ) fabric_name = '%s.%s' % ( module_alias or module_name , method_name ) func . wrapped . __func__ . fabric_name = fabric_name return func
13388	def ttl ( self , response ) : if response . code != 200 : return 0 if not self . request . method in [ 'GET' , 'HEAD' , 'OPTIONS' ] : return 0 try : pragma = self . request . headers [ 'pragma' ] if pragma == 'no-cache' : return 0 except KeyError : pass try : cache_control = self . request . headers [ 'cache-control' ] for option in [ 'private' , 'no-cache' , 'no-store' , 'must-revalidate' , 'proxy-revalidate' ] : if cache_control . find ( option ) : return 0 options = parse_cache_control ( cache_control ) try : return int ( options [ 's-maxage' ] ) except KeyError : pass try : return int ( options [ 'max-age' ] ) except KeyError : pass if 's-maxage' in options : max_age = options [ 's-maxage' ] if max_age < ttl : ttl = max_age if 'max-age' in options : max_age = options [ 'max-age' ] if max_age < ttl : ttl = max_age return ttl except KeyError : pass try : expires = self . request . headers [ 'expires' ] return time . mktime ( time . strptime ( expires , '%a, %d %b %Y %H:%M:%S' ) ) - time . time ( ) except KeyError : pass
3884	def from_conv_part_data ( conv_part_data , self_user_id ) : user_id = UserID ( chat_id = conv_part_data . id . chat_id , gaia_id = conv_part_data . id . gaia_id ) return User ( user_id , conv_part_data . fallback_name , None , None , [ ] , ( self_user_id == user_id ) or ( self_user_id is None ) )
3265	def build_url ( base , seg , query = None ) : def clean_segment ( segment ) : segment = segment . strip ( '/' ) if isinstance ( segment , basestring ) : segment = segment . encode ( 'utf-8' ) return segment seg = ( quote ( clean_segment ( s ) ) for s in seg ) if query is None or len ( query ) == 0 : query_string = '' else : query_string = "?" + urlencode ( query ) path = '/' . join ( seg ) + query_string adjusted_base = base . rstrip ( '/' ) + '/' return urljoin ( str ( adjusted_base ) , str ( path ) )
11370	def convert_date_from_iso_to_human ( value ) : try : year , month , day = value . split ( "-" ) except ValueError : try : year , month , day = value . split ( " " ) except ValueError : return value try : date_object = datetime ( int ( year ) , int ( month ) , int ( day ) ) except TypeError : return value return date_object . strftime ( "%d %b %Y" )
2166	def list_commands ( self , ctx ) : commands = set ( self . list_resource_commands ( ) ) commands . union ( set ( self . list_misc_commands ( ) ) ) return sorted ( commands )
11577	def send_sysex ( self , sysex_command , sysex_data = None ) : if not sysex_data : sysex_data = [ ] sysex_message = chr ( self . START_SYSEX ) sysex_message += chr ( sysex_command ) if len ( sysex_data ) : for d in sysex_data : sysex_message += chr ( d ) sysex_message += chr ( self . END_SYSEX ) for data in sysex_message : self . pymata . transport . write ( data )
7973	def start ( self , daemon = False ) : self . daemon = daemon self . io_threads = [ ] self . event_thread = EventDispatcherThread ( self . event_dispatcher , daemon = daemon , exc_queue = self . exc_queue ) self . event_thread . start ( ) for handler in self . io_handlers : self . _run_io_threads ( handler ) for handler in self . timeout_handlers : self . _run_timeout_threads ( handler )
13350	def add_file ( self , file , ** kwargs ) : if os . access ( file , os . F_OK ) : if file in self . f_repository : raise DuplicationError ( "file already added." ) self . f_repository . append ( file ) else : raise IOError ( "file not found." )
7168	def remove_intent ( self , name ) : self . intents . remove ( name ) self . padaos . remove_intent ( name ) self . must_train = True
4061	def _attachment ( self , payload , parentid = None ) : attachment = Zupload ( self , payload , parentid ) res = attachment . upload ( ) return res
10221	def preprocessing_br_projection_excel ( path : str ) -> pd . DataFrame : if not os . path . exists ( path ) : raise ValueError ( "Error: %s file not found" % path ) return pd . read_excel ( path , sheetname = 0 , header = 0 )
2197	def log_part ( self ) : self . cap_stdout . seek ( self . _pos ) text = self . cap_stdout . read ( ) self . _pos = self . cap_stdout . tell ( ) self . parts . append ( text ) self . text = text
12160	def parent ( groups , ID ) : if ID in groups . keys ( ) : return ID if not ID in groups . keys ( ) : for actualParent in groups . keys ( ) : if ID in groups [ actualParent ] : return actualParent return None
5692	def update ( self , new_labels , departure_time_backup = None ) : if self . _closed : raise RuntimeError ( "Profile is closed, no updates can be made" ) try : departure_time = next ( iter ( new_labels ) ) . departure_time except StopIteration : departure_time = departure_time_backup self . _check_dep_time_is_valid ( departure_time ) for new_label in new_labels : assert ( new_label . departure_time == departure_time ) dep_time_index = self . dep_times_to_index [ departure_time ] if dep_time_index > 0 : mod_prev_labels = [ label . get_copy_with_specified_departure_time ( departure_time ) for label in self . _label_bags [ dep_time_index - 1 ] ] else : mod_prev_labels = list ( ) mod_prev_labels += self . _label_bags [ dep_time_index ] walk_label = self . _get_label_to_target ( departure_time ) if walk_label : new_labels = new_labels + [ walk_label ] new_frontier = merge_pareto_frontiers ( new_labels , mod_prev_labels ) self . _label_bags [ dep_time_index ] = new_frontier return True
2460	def set_pkg_desc ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_desc_set : self . package_desc_set = True if validations . validate_pkg_desc ( text ) : doc . package . description = str_from_text ( text ) else : raise SPDXValueError ( 'Package::Description' ) else : raise CardinalityError ( 'Package::Description' )
8591	def create_snapshot ( self , datacenter_id , volume_id , name = None , description = None ) : data = { 'name' : name , 'description' : description } response = self . _perform_request ( '/datacenters/%s/volumes/%s/create-snapshot' % ( datacenter_id , volume_id ) , method = 'POST-ACTION-JSON' , data = urlencode ( data ) ) return response
1208	def output_image_link ( self , m ) : return self . renderer . image_link ( m . group ( 'url' ) , m . group ( 'target' ) , m . group ( 'alt' ) )
1472	def main ( ) : shell_env = os . environ . copy ( ) shell_env [ "PEX_ROOT" ] = os . path . join ( os . path . abspath ( '.' ) , ".pex" ) executor = HeronExecutor ( sys . argv , shell_env ) executor . initialize ( ) start ( executor )
1150	def _show_warning ( message , category , filename , lineno , file = None , line = None ) : if file is None : file = sys . stderr if file is None : return try : file . write ( formatwarning ( message , category , filename , lineno , line ) ) except ( IOError , UnicodeError ) : pass
13249	def get_url_from_entry ( entry ) : if 'url' in entry . fields : return entry . fields [ 'url' ] elif entry . type . lower ( ) == 'docushare' : return 'https://ls.st/' + entry . fields [ 'handle' ] elif 'adsurl' in entry . fields : return entry . fields [ 'adsurl' ] elif 'doi' in entry . fields : return 'https://doi.org/' + entry . fields [ 'doi' ] else : raise NoEntryUrlError ( )
4518	def fillTriangle ( self , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : md . fill_triangle ( self . set , x0 , y0 , x1 , y1 , x2 , y2 , color , aa )
8059	def do_vars ( self , line ) : if self . bot . _vars : max_name_len = max ( [ len ( name ) for name in self . bot . _vars ] ) for i , ( name , v ) in enumerate ( self . bot . _vars . items ( ) ) : keep = i < len ( self . bot . _vars ) - 1 self . print_response ( "%s = %s" % ( name . ljust ( max_name_len ) , v . value ) , keep = keep ) else : self . print_response ( "No vars" )
13008	def path ( self ) : path = super ( WindowsPath2 , self ) . path if path . startswith ( "\\\\?\\" ) : return path [ 4 : ] return path
8358	def shoebot_example ( ** shoebot_kwargs ) : def decorator ( f ) : def run ( ) : from shoebot import ShoebotInstallError print ( " Shoebot - %s:" % f . __name__ . replace ( "_" , " " ) ) try : import shoebot outputfile = "/tmp/shoebot-%s.png" % f . __name__ bot = shoebot . create_bot ( outputfile = outputfile ) f ( bot ) bot . finish ( ) print ( ' [passed] : %s' % outputfile ) print ( '' ) except ShoebotInstallError as e : print ( ' [failed]' , e . args [ 0 ] ) print ( '' ) except Exception : print ( ' [failed] - traceback:' ) for line in traceback . format_exc ( ) . splitlines ( ) : print ( ' %s' % line ) print ( '' ) return run return decorator
9054	def posteriori_covariance ( self ) : r K = GLMM . covariance ( self ) tau = self . _ep . _posterior . tau return pinv ( pinv ( K ) + diag ( 1 / tau ) )
11300	def unregister ( self , provider_class ) : if not issubclass ( provider_class , BaseProvider ) : raise TypeError ( '%s must be a subclass of BaseProvider' % provider_class . __name__ ) if provider_class not in self . _registered_providers : raise NotRegistered ( '%s is not registered' % provider_class . __name__ ) self . _registered_providers . remove ( provider_class ) self . invalidate_providers ( )
7328	def _get_base_url ( base_url , api , version ) : format_args = { } if "{api}" in base_url : if api == "" : base_url = base_url . replace ( '{api}.' , '' ) else : format_args [ 'api' ] = api if "{version}" in base_url : if version == "" : base_url = base_url . replace ( '/{version}' , '' ) else : format_args [ 'version' ] = version return base_url . format ( api = api , version = version )
12594	def select_arg_verify ( endpoint , cert , key , pem , ca , aad , no_verify ) : if not ( endpoint . lower ( ) . startswith ( 'http' ) or endpoint . lower ( ) . startswith ( 'https' ) ) : raise CLIError ( 'Endpoint must be HTTP or HTTPS' ) usage = ( 'Valid syntax : --endpoint [ [ --key --cert | --pem | --aad] ' '[ --ca | --no-verify ] ]' ) if ca and not ( pem or all ( [ key , cert ] ) ) : raise CLIError ( usage ) if no_verify and not ( pem or all ( [ key , cert ] ) or aad ) : raise CLIError ( usage ) if no_verify and ca : raise CLIError ( usage ) if any ( [ cert , key ] ) and not all ( [ cert , key ] ) : raise CLIError ( usage ) if aad and any ( [ pem , cert , key ] ) : raise CLIError ( usage ) if pem and any ( [ cert , key ] ) : raise CLIError ( usage )
6341	def sim ( self , src , tar ) : if src == tar : return 1.0 if not src or not tar : return 0.0 min_word , max_word = ( src , tar ) if len ( src ) < len ( tar ) else ( tar , src ) min_len = len ( min_word ) for i in range ( min_len , 0 , - 1 ) : if min_word [ : i ] == max_word [ : i ] : return i / min_len return 0.0
1514	def wait_for_job_to_start ( single_master , job ) : i = 0 while True : try : r = requests . get ( "http://%s:4646/v1/job/%s" % ( single_master , job ) ) if r . status_code == 200 and r . json ( ) [ "Status" ] == "running" : break else : raise RuntimeError ( ) except : Log . debug ( sys . exc_info ( ) [ 0 ] ) Log . info ( "Waiting for %s to come up... %s" % ( job , i ) ) time . sleep ( 1 ) if i > 20 : Log . error ( "Failed to start Nomad Cluster!" ) sys . exit ( - 1 ) i = i + 1
7761	def xml_elements_equal ( element1 , element2 , ignore_level1_cdata = False ) : if None in ( element1 , element2 ) or element1 . tag != element2 . tag : return False attrs1 = element1 . items ( ) attrs1 . sort ( ) attrs2 = element2 . items ( ) attrs2 . sort ( ) if not ignore_level1_cdata : if element1 . text != element2 . text : return False if attrs1 != attrs2 : return False if len ( element1 ) != len ( element2 ) : return False for child1 , child2 in zip ( element1 , element2 ) : if child1 . tag != child2 . tag : return False if not ignore_level1_cdata : if element1 . text != element2 . text : return False if not xml_elements_equal ( child1 , child2 ) : return False return True
6718	def virtualenv_exists ( self , virtualenv_dir = None ) : r = self . local_renderer ret = True with self . settings ( warn_only = True ) : ret = r . run_or_local ( 'ls {virtualenv_dir}' ) or '' ret = 'cannot access' not in ret . strip ( ) . lower ( ) if self . verbose : if ret : print ( 'Yes' ) else : print ( 'No' ) return ret
3256	def mosaic_coverages ( self , store ) : params = dict ( ) url = build_url ( self . service_url , [ "workspaces" , store . workspace . name , "coveragestores" , store . name , "coverages.json" ] , params ) headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http_request ( url , headers = headers ) if resp . status_code != 200 : FailedRequestError ( 'Failed to get mosaic coverages {} : {}, {}' . format ( store , resp . status_code , resp . text ) ) self . _cache . clear ( ) return resp . json ( )
10340	def spia_matrices_to_tsvs ( spia_matrices : Mapping [ str , pd . DataFrame ] , directory : str ) -> None : os . makedirs ( directory , exist_ok = True ) for relation , df in spia_matrices . items ( ) : df . to_csv ( os . path . join ( directory , f'{relation}.tsv' ) , index = True )
7084	def _make_magseries_plot ( axes , stimes , smags , serrs , magsarefluxes = False , ms = 2.0 ) : scaledplottime = stimes - npmin ( stimes ) axes . plot ( scaledplottime , smags , marker = 'o' , ms = ms , ls = 'None' , mew = 0 , color = 'green' , rasterized = True ) if not magsarefluxes : plot_ylim = axes . get_ylim ( ) axes . set_ylim ( ( plot_ylim [ 1 ] , plot_ylim [ 0 ] ) ) axes . set_xlim ( ( npmin ( scaledplottime ) - 1.0 , npmax ( scaledplottime ) + 1.0 ) ) axes . grid ( color = '#a9a9a9' , alpha = 0.9 , zorder = 0 , linewidth = 1.0 , linestyle = ':' ) plot_xlabel = 'JD - %.3f' % npmin ( stimes ) if magsarefluxes : plot_ylabel = 'flux' else : plot_ylabel = 'magnitude' axes . set_xlabel ( plot_xlabel ) axes . set_ylabel ( plot_ylabel ) axes . get_yaxis ( ) . get_major_formatter ( ) . set_useOffset ( False ) axes . get_xaxis ( ) . get_major_formatter ( ) . set_useOffset ( False )
12563	def get_rois_centers_of_mass ( vol ) : from scipy . ndimage . measurements import center_of_mass roisvals = np . unique ( vol ) roisvals = roisvals [ roisvals != 0 ] rois_centers = OrderedDict ( ) for r in roisvals : rois_centers [ r ] = center_of_mass ( vol , vol , r ) return rois_centers
4005	def streaming_to_client ( ) : for handler in client_logger . handlers : if hasattr ( handler , 'append_newlines' ) : break else : handler = None old_propagate = client_logger . propagate client_logger . propagate = False if handler is not None : old_append = handler . append_newlines handler . append_newlines = False yield client_logger . propagate = old_propagate if handler is not None : handler . append_newlines = old_append
9560	def _apply_assert_methods ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'assert' ) : rdict = self . _as_dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except AssertionError as e : code = ASSERT_CHECK_FAILED message = MESSAGES [ ASSERT_CHECK_FAILED ] if len ( e . args ) > 0 : custom = e . args [ 0 ] if isinstance ( custom , ( list , tuple ) ) : if len ( custom ) > 0 : code = custom [ 0 ] if len ( custom ) > 1 : message = custom [ 1 ] else : code = custom p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . __name__ , f . __doc__ ) if context is not None : p [ 'context' ] = context yield p
7113	def predict ( self , X ) : x = X if not isinstance ( X , list ) : x = [ X ] y = self . estimator . predict ( x ) y = [ item [ 0 ] for item in y ] y = [ self . _remove_prefix ( label ) for label in y ] if not isinstance ( X , list ) : y = y [ 0 ] return y
3808	def charge_from_formula ( formula ) : r negative = '-' in formula positive = '+' in formula if positive and negative : raise ValueError ( 'Both negative and positive signs were found in the formula; only one sign is allowed' ) elif not ( positive or negative ) : return 0 multiplier , sign = ( - 1 , '-' ) if negative else ( 1 , '+' ) hit = False if '(' in formula : hit = bracketed_charge_re . findall ( formula ) if hit : formula = hit [ - 1 ] . replace ( '(' , '' ) . replace ( ')' , '' ) count = formula . count ( sign ) if count == 1 : splits = formula . split ( sign ) if splits [ 1 ] == '' or splits [ 1 ] == ')' : return multiplier else : return multiplier * int ( splits [ 1 ] ) else : return multiplier * count
2449	def set_pkg_supplier ( self , doc , entity ) : self . assert_package_exists ( ) if not self . package_supplier_set : self . package_supplier_set = True if validations . validate_pkg_supplier ( entity ) : doc . package . supplier = entity return True else : raise SPDXValueError ( 'Package::Supplier' ) else : raise CardinalityError ( 'Package::Supplier' )
6751	def unregister ( self ) : for k in list ( env . keys ( ) ) : if k . startswith ( self . env_prefix ) : del env [ k ] try : del all_satchels [ self . name . upper ( ) ] except KeyError : pass try : del manifest_recorder [ self . name ] except KeyError : pass try : del manifest_deployers [ self . name . upper ( ) ] except KeyError : pass try : del manifest_deployers_befores [ self . name . upper ( ) ] except KeyError : pass try : del required_system_packages [ self . name . upper ( ) ] except KeyError : pass
2107	def config ( key = None , value = None , scope = 'user' , global_ = False , unset = False ) : if global_ : scope = 'global' warnings . warn ( 'The `--global` option is deprecated and will be ' 'removed. Use `--scope=global` to get the same effect.' , DeprecationWarning ) if not key : seen = set ( ) parser_desc = { 'runtime' : 'Runtime options.' , 'environment' : 'Options from environment variables.' , 'local' : 'Local options (set with `tower-cli config ' '--scope=local`; stored in .tower_cli.cfg of this ' 'directory or a parent)' , 'user' : 'User options (set with `tower-cli config`; stored in ' '~/.tower_cli.cfg).' , 'global' : 'Global options (set with `tower-cli config ' '--scope=global`, stored in /etc/tower/tower_cli.cfg).' , 'defaults' : 'Defaults.' , } click . echo ( '' ) for name , parser in zip ( settings . _parser_names , settings . _parsers ) : will_echo = False for option in parser . options ( 'general' ) : if option in seen : continue will_echo = True if will_echo : secho ( '# %s' % parser_desc [ name ] , fg = 'green' , bold = True ) for option in parser . options ( 'general' ) : if option in seen : continue _echo_setting ( option ) seen . add ( option ) if will_echo : click . echo ( '' ) return if not hasattr ( settings , key ) : raise exc . TowerCLIError ( 'Invalid configuration option "%s".' % key ) if value and unset : raise exc . UsageError ( 'Cannot provide both a value and --unset.' ) if key and not value and not unset : _echo_setting ( key ) return filename = os . path . expanduser ( '~/.tower_cli.cfg' ) if scope == 'global' : if not os . path . isdir ( '/etc/tower/' ) : raise exc . TowerCLIError ( '/etc/tower/ does not exist, and this ' 'command cowardly declines to create it.' ) filename = '/etc/tower/tower_cli.cfg' elif scope == 'local' : filename = '.tower_cli.cfg' parser = Parser ( ) parser . add_section ( 'general' ) parser . read ( filename ) if unset : parser . remove_option ( 'general' , key ) else : parser . set ( 'general' , key , value ) with open ( filename , 'w' ) as config_file : parser . write ( config_file ) try : os . chmod ( filename , stat . S_IRUSR | stat . S_IWUSR ) except Exception as e : warnings . warn ( 'Unable to set permissions on {0} - {1} ' . format ( filename , e ) , UserWarning ) click . echo ( 'Configuration updated successfully.' )
7954	def starttls ( self , ** kwargs ) : with self . lock : self . event ( TLSConnectingEvent ( ) ) self . _write_queue . append ( StartTLS ( ** kwargs ) ) self . _write_queue_cond . notify ( )
8538	def run ( self , * args , ** kwargs ) : while True : try : timestamp , ip_p = self . _queue . popleft ( ) src_ip = get_ip ( ip_p , ip_p . src ) dst_ip = get_ip ( ip_p , ip_p . dst ) src = intern ( '%s:%s' % ( src_ip , ip_p . data . sport ) ) dst = intern ( '%s:%s' % ( dst_ip , ip_p . data . dport ) ) key = intern ( '%s<->%s' % ( src , dst ) ) stream = self . _streams . get ( key ) if stream is None : stream = Stream ( src , dst ) self . _streams [ key ] = stream setattr ( ip_p , 'timestamp' , timestamp ) pushed = stream . push ( ip_p ) if not pushed : continue for handler in self . _handlers : try : handler ( stream ) except Exception as ex : print ( 'handler exception: %s' % ex ) except Exception : time . sleep ( 0.00001 )
10472	def _addKeyToQueue ( self , keychr , modFlags = 0 , globally = False ) : if not keychr : return if not hasattr ( self , 'keyboard' ) : self . keyboard = AXKeyboard . loadKeyboard ( ) if keychr in self . keyboard [ 'upperSymbols' ] and not modFlags : self . _sendKeyWithModifiers ( keychr , [ AXKeyCodeConstants . SHIFT ] , globally ) return if keychr . isupper ( ) and not modFlags : self . _sendKeyWithModifiers ( keychr . lower ( ) , [ AXKeyCodeConstants . SHIFT ] , globally ) return if keychr not in self . keyboard : self . _clearEventQueue ( ) raise ValueError ( 'Key %s not found in keyboard layout' % keychr ) keyDown = Quartz . CGEventCreateKeyboardEvent ( None , self . keyboard [ keychr ] , True ) keyUp = Quartz . CGEventCreateKeyboardEvent ( None , self . keyboard [ keychr ] , False ) Quartz . CGEventSetFlags ( keyDown , modFlags ) Quartz . CGEventSetFlags ( keyUp , modFlags ) if not globally : macVer , _ , _ = platform . mac_ver ( ) macVer = int ( macVer . split ( '.' ) [ 1 ] ) if macVer > 10 : appPid = self . _getPid ( ) self . _queueEvent ( Quartz . CGEventPostToPid , ( appPid , keyDown ) ) self . _queueEvent ( Quartz . CGEventPostToPid , ( appPid , keyUp ) ) else : appPsn = self . _getPsnForPid ( self . _getPid ( ) ) self . _queueEvent ( Quartz . CGEventPostToPSN , ( appPsn , keyDown ) ) self . _queueEvent ( Quartz . CGEventPostToPSN , ( appPsn , keyUp ) ) else : self . _queueEvent ( Quartz . CGEventPost , ( 0 , keyDown ) ) self . _queueEvent ( Quartz . CGEventPost , ( 0 , keyUp ) )
10166	def get_md_device ( self , line , personalities = [ ] ) : ret = { } splitted = split ( '\W+' , line ) ret [ 'status' ] = splitted [ 1 ] if splitted [ 2 ] in personalities : ret [ 'type' ] = splitted [ 2 ] ret [ 'components' ] = self . get_components ( line , with_type = True ) else : ret [ 'type' ] = None ret [ 'components' ] = self . get_components ( line , with_type = False ) return ret
1976	def sys_receive ( self , cpu , fd , buf , count , rx_bytes ) : data = '' if count != 0 : if not self . _is_open ( fd ) : logger . info ( "RECEIVE: Not valid file descriptor on receive. Returning EBADF" ) return Decree . CGC_EBADF if buf not in cpu . memory : logger . info ( "RECEIVE: buf points to invalid address. Returning EFAULT" ) return Decree . CGC_EFAULT if fd > 2 and self . files [ fd ] . is_empty ( ) : cpu . PC -= cpu . instruction . size self . wait ( [ fd ] , [ ] , None ) raise RestartSyscall ( ) data = self . files [ fd ] . receive ( count ) self . syscall_trace . append ( ( "_receive" , fd , data ) ) cpu . write_bytes ( buf , data ) self . signal_receive ( fd ) if rx_bytes : if rx_bytes not in cpu . memory : logger . info ( "RECEIVE: Not valid file descriptor on receive. Returning EFAULT" ) return Decree . CGC_EFAULT cpu . write_int ( rx_bytes , len ( data ) , 32 ) logger . info ( "RECEIVE(%d, 0x%08x, %d, 0x%08x) -> <%s> (size:%d)" % ( fd , buf , count , rx_bytes , repr ( data ) [ : min ( count , 10 ) ] , len ( data ) ) ) return 0
721	def getAllMetrics ( self ) : result = self . getReportMetrics ( ) result . update ( self . getOptimizationMetrics ( ) ) return result
2155	def set_or_reset_runtime_param ( self , key , value ) : if self . _runtime . has_option ( 'general' , key ) : self . _runtime = self . _new_parser ( ) if value is None : return settings . _runtime . set ( 'general' , key . replace ( 'tower_' , '' ) , six . text_type ( value ) )
4216	def get_credential ( self , service , username ) : if username is not None : password = self . get_password ( service , username ) if password is not None : return credentials . SimpleCredential ( username , password , ) return None
9899	def _data ( self ) : if self . is_caching : return self . cache with open ( self . path , "r" ) as f : return json . load ( f )
6294	def transform ( self , program : moderngl . Program , buffer : moderngl . Buffer , mode = None , vertices = - 1 , first = 0 , instances = 1 ) : vao = self . instance ( program ) if mode is None : mode = self . mode vao . transform ( buffer , mode = mode , vertices = vertices , first = first , instances = instances )
7728	def add_item ( self , item ) : if not isinstance ( item , MucItemBase ) : raise TypeError ( "Bad item type for muc#user" ) item . as_xml ( self . xmlnode )
8648	def reject_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'reject' , } endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotRejectedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
1154	def remove ( self , value ) : if value not in self : raise KeyError ( value ) self . discard ( value )
10631	def clear ( self ) : self . _compound_mfrs = self . _compound_mfrs * 0.0 self . _P = 1.0 self . _T = 25.0 self . _H = 0.0
2498	def handle_package_has_file ( self , package , package_node ) : file_nodes = map ( self . handle_package_has_file_helper , package . files ) triples = [ ( package_node , self . spdx_namespace . hasFile , node ) for node in file_nodes ] for triple in triples : self . graph . add ( triple )
9974	def get_mro ( self , space ) : seqs = [ self . get_mro ( base ) for base in self . get_bases ( space ) ] + [ list ( self . get_bases ( space ) ) ] res = [ ] while True : non_empty = list ( filter ( None , seqs ) ) if not non_empty : res . insert ( 0 , space ) return res for seq in non_empty : candidate = seq [ 0 ] not_head = [ s for s in non_empty if candidate in s [ 1 : ] ] if not_head : candidate = None else : break if not candidate : raise TypeError ( "inconsistent hierarchy, no C3 MRO is possible" ) res . append ( candidate ) for seq in non_empty : if seq [ 0 ] == candidate : del seq [ 0 ]
9038	def bounding_box ( self ) : min_x , min_y , max_x , max_y = zip ( * list ( self . walk_rows ( lambda row : row . bounding_box ) ) ) return min ( min_x ) , min ( min_y ) , max ( max_x ) , max ( max_y )
7044	def lightcurve_flux_measures ( ftimes , fmags , ferrs , magsarefluxes = False ) : ndet = len ( fmags ) if ndet > 9 : if magsarefluxes : series_fluxes = fmags else : series_fluxes = 10.0 ** ( - 0.4 * fmags ) series_flux_median = npmedian ( series_fluxes ) series_flux_percent_amplitude = ( npmax ( npabs ( series_fluxes ) ) / series_flux_median ) series_flux_percentiles = nppercentile ( series_fluxes , [ 5.0 , 10 , 17.5 , 25 , 32.5 , 40 , 60 , 67.5 , 75 , 82.5 , 90 , 95 ] ) series_frat_595 = ( series_flux_percentiles [ - 1 ] - series_flux_percentiles [ 0 ] ) series_frat_1090 = ( series_flux_percentiles [ - 2 ] - series_flux_percentiles [ 1 ] ) series_frat_175825 = ( series_flux_percentiles [ - 3 ] - series_flux_percentiles [ 2 ] ) series_frat_2575 = ( series_flux_percentiles [ - 4 ] - series_flux_percentiles [ 3 ] ) series_frat_325675 = ( series_flux_percentiles [ - 5 ] - series_flux_percentiles [ 4 ] ) series_frat_4060 = ( series_flux_percentiles [ - 6 ] - series_flux_percentiles [ 5 ] ) series_flux_percentile_ratio_mid20 = series_frat_4060 / series_frat_595 series_flux_percentile_ratio_mid35 = series_frat_325675 / series_frat_595 series_flux_percentile_ratio_mid50 = series_frat_2575 / series_frat_595 series_flux_percentile_ratio_mid65 = series_frat_175825 / series_frat_595 series_flux_percentile_ratio_mid80 = series_frat_1090 / series_frat_595 series_percent_difference_flux_percentile = ( series_frat_595 / series_flux_median ) series_percentile_magdiff = - 2.5 * nplog10 ( series_percent_difference_flux_percentile ) return { 'flux_median' : series_flux_median , 'flux_percent_amplitude' : series_flux_percent_amplitude , 'flux_percentiles' : series_flux_percentiles , 'flux_percentile_ratio_mid20' : series_flux_percentile_ratio_mid20 , 'flux_percentile_ratio_mid35' : series_flux_percentile_ratio_mid35 , 'flux_percentile_ratio_mid50' : series_flux_percentile_ratio_mid50 , 'flux_percentile_ratio_mid65' : series_flux_percentile_ratio_mid65 , 'flux_percentile_ratio_mid80' : series_flux_percentile_ratio_mid80 , 'percent_difference_flux_percentile' : series_percentile_magdiff , } else : LOGERROR ( 'not enough detections in this magseries ' 'to calculate flux measures' ) return None
9511	def replace_bases ( self , old , new ) : self . seq = self . seq . replace ( old , new )
